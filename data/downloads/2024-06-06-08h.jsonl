{"created":"2024-06-05 17:59:40","title":"Wings: Learning Multimodal LLMs without Text-only Forgetting","abstract":"Multimodal large language models (MLLMs), initiated with a trained LLM, first align images with text and then fine-tune on multimodal mixed inputs. However, the MLLM catastrophically forgets the text-only instructions, which do not include images and can be addressed within the initial LLM. In this paper, we present Wings, a novel MLLM that excels in both text-only dialogues and multimodal comprehension. Analyzing MLLM attention in multimodal instructions reveals that text-only forgetting is related to the attention shifts from pre-image to post-image text. From that, we construct extra modules that act as the boosted learner to compensate for the attention shift. The complementary visual and textual learners, like \"wings\" on either side, are connected in parallel within each layer's attention block. Initially, image and text inputs are aligned with visual learners operating alongside the main attention, balancing focus on visual elements. Textual learners are later collaboratively integrated with attention-based routing to blend the outputs of the visual and textual learners. We design the Low-Rank Residual Attention (LoRRA) to guarantee high efficiency for learners. Our experimental results demonstrate that Wings outperforms equally-scaled MLLMs in both text-only and visual question-answering tasks. On a newly constructed Interleaved Image-Text (IIT) benchmark, Wings exhibits superior performance from text-only-rich to multimodal-rich question-answering tasks.","sentences":["Multimodal large language models (MLLMs), initiated with a trained LLM, first align images with text and then fine-tune on multimodal mixed inputs.","However, the MLLM catastrophically forgets the text-only instructions, which do not include images and can be addressed within the initial LLM.","In this paper, we present Wings, a novel MLLM that excels in both text-only dialogues and multimodal comprehension.","Analyzing MLLM attention in multimodal instructions reveals that text-only forgetting is related to the attention shifts from pre-image to post-image text.","From that, we construct extra modules that act as the boosted learner to compensate for the attention shift.","The complementary visual and textual learners, like \"wings\" on either side, are connected in parallel within each layer's attention block.","Initially, image and text inputs are aligned with visual learners operating alongside the main attention, balancing focus on visual elements.","Textual learners are later collaboratively integrated with attention-based routing to blend the outputs of the visual and textual learners.","We design the Low-Rank Residual Attention (LoRRA) to guarantee high efficiency for learners.","Our experimental results demonstrate that Wings outperforms equally-scaled MLLMs in both text-only and visual question-answering tasks.","On a newly constructed Interleaved Image-Text (IIT) benchmark, Wings exhibits superior performance from text-only-rich to multimodal-rich question-answering tasks."],"url":"http://arxiv.org/abs/2406.03496v1","category":"cs.CL"}
{"created":"2024-06-05 17:59:35","title":"Grokking Modular Polynomials","abstract":"Neural networks readily learn a subset of the modular arithmetic tasks, while failing to generalize on the rest. This limitation remains unmoved by the choice of architecture and training strategies. On the other hand, an analytical solution for the weights of Multi-layer Perceptron (MLP) networks that generalize on the modular addition task is known in the literature. In this work, we (i) extend the class of analytical solutions to include modular multiplication as well as modular addition with many terms. Additionally, we show that real networks trained on these datasets learn similar solutions upon generalization (grokking). (ii) We combine these \"expert\" solutions to construct networks that generalize on arbitrary modular polynomials. (iii) We hypothesize a classification of modular polynomials into learnable and non-learnable via neural networks training; and provide experimental evidence supporting our claims.","sentences":["Neural networks readily learn a subset of the modular arithmetic tasks, while failing to generalize on the rest.","This limitation remains unmoved by the choice of architecture and training strategies.","On the other hand, an analytical solution for the weights of Multi-layer Perceptron (MLP) networks that generalize on the modular addition task is known in the literature.","In this work, we (i) extend the class of analytical solutions to include modular multiplication as well as modular addition with many terms.","Additionally, we show that real networks trained on these datasets learn similar solutions upon generalization (grokking).","(ii) We combine these \"expert\" solutions to construct networks that generalize on arbitrary modular polynomials.","(iii) We hypothesize a classification of modular polynomials into learnable and non-learnable via neural networks training; and provide experimental evidence supporting our claims."],"url":"http://arxiv.org/abs/2406.03495v1","category":"cs.LG"}
{"created":"2024-06-05 17:58:21","title":"Stout smearing and Wilson flow in lattice perturbation theory","abstract":"We present the expansion of stout smearing and the Wilson flow in lattice perturbation theory to order $g_0^3$, which is suitable for one-loop calculations. As the Wilson flow is generated by infinitesimal stout smearing steps, the results are related to each other by taking the appropriate limits. We show how to apply perturbative stout smearing or Wilson flow to the Feynman rules of any lattice fermion action and and illustrate them by calculating the self-energy of the clover-improved Wilson fermion.","sentences":["We present the expansion of stout smearing and the Wilson flow in lattice perturbation theory to order $g_0^3$, which is suitable for one-loop calculations.","As the Wilson flow is generated by infinitesimal stout smearing steps, the results are related to each other by taking the appropriate limits.","We show how to apply perturbative stout smearing or Wilson flow to the Feynman rules of any lattice fermion action and and illustrate them by calculating the self-energy of the clover-improved Wilson fermion."],"url":"http://arxiv.org/abs/2406.03493v1","category":"hep-lat"}
{"created":"2024-06-05 17:57:58","title":"The Logarithmic Memristor-Based Bayesian Machine","abstract":"The demand for explainable and energy-efficient artificial intelligence (AI) systems for edge computing has led to significant interest in electronic systems dedicated to Bayesian inference. Traditional designs of such systems often rely on stochastic computing, which offers high energy efficiency but suffers from latency issues and struggles with low-probability values. In this paper, we introduce the logarithmic memristor-based Bayesian machine, an innovative design that leverages the unique properties of memristors and logarithmic computing as an alternative to stochastic computing. We present a prototype machine fabricated in a hybrid CMOS/hafnium-oxide memristor process. We validate the versatility and robustness of our system through experimental validation and extensive simulations in two distinct applications: gesture recognition and sleep stage classification. The logarithmic approach simplifies the computational model by converting multiplications into additions and enhances the handling of low-probability events, which are crucial in time-dependent tasks. Our results demonstrate that the logarithmic Bayesian machine achieves superior performance in terms of accuracy and energy efficiency compared to its stochastic counterpart, particularly in scenarios involving complex probabilistic models. This work paves the way for the deployment of advanced AI capabilities in edge devices, where power efficiency and reliability are paramount.","sentences":["The demand for explainable and energy-efficient artificial intelligence (AI) systems for edge computing has led to significant interest in electronic systems dedicated to Bayesian inference.","Traditional designs of such systems often rely on stochastic computing, which offers high energy efficiency but suffers from latency issues and struggles with low-probability values.","In this paper, we introduce the logarithmic memristor-based Bayesian machine, an innovative design that leverages the unique properties of memristors and logarithmic computing as an alternative to stochastic computing.","We present a prototype machine fabricated in a hybrid CMOS/hafnium-oxide memristor process.","We validate the versatility and robustness of our system through experimental validation and extensive simulations in two distinct applications: gesture recognition and sleep stage classification.","The logarithmic approach simplifies the computational model by converting multiplications into additions and enhances the handling of low-probability events, which are crucial in time-dependent tasks.","Our results demonstrate that the logarithmic Bayesian machine achieves superior performance in terms of accuracy and energy efficiency compared to its stochastic counterpart, particularly in scenarios involving complex probabilistic models.","This work paves the way for the deployment of advanced AI capabilities in edge devices, where power efficiency and reliability are paramount."],"url":"http://arxiv.org/abs/2406.03492v1","category":"cs.ET"}
{"created":"2024-06-05 17:56:13","title":"Detecting Phase Coherence of 2D Bose Gases via Noise Correlations","abstract":"We measure the noise correlations of two-dimensional (2D) Bose gases after free expansion, which allows us to characterize the in-situ phase coherence across the Berezinskii-Kosterlitz-Thouless (BKT) transition. The noise correlation function features a characteristic spatial oscillatory behavior in the superfluid phase, which gives direct access to the superfluid exponent. This oscillatory behavior vanishes above the BKT critical point, as we demonstrate for both single-layer and decoupled bilayer 2D Bose gases. Our work establishes noise interferometry as a general tool to probe and identify many-body states of bilayer quantum gases.","sentences":["We measure the noise correlations of two-dimensional (2D)","Bose gases after free expansion, which allows us to characterize the in-situ phase coherence across the Berezinskii-Kosterlitz-Thouless (BKT) transition.","The noise correlation function features a characteristic spatial oscillatory behavior in the superfluid phase, which gives direct access to the superfluid exponent.","This oscillatory behavior vanishes above the BKT critical point, as we demonstrate for both single-layer and decoupled bilayer 2D Bose gases.","Our work establishes noise interferometry as a general tool to probe and identify many-body states of bilayer quantum gases."],"url":"http://arxiv.org/abs/2406.03491v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-05 17:54:24","title":"Simultaneous retrieval of orbital phase resolved JWST/MIRI emission spectra of the hot Jupiter WASP-43b: evidence of water, ammonia and carbon monoxide","abstract":"Spectroscopic phase curves of hot Jupiters measure their emission spectra at multiple orbital phases, thus enabling detailed characterisation of their atmospheres. Precise constraints on the atmospheric composition of these exoplanets offer insights into their formation and evolution. We analyse four phase-resolved emission spectra of the hot Jupiter WASP-43b, generated from a phase curve observed with the MIRI/LRS onboard the JWST, to retrieve its atmospheric properties. Using a parametric 2D temperature model and assuming a chemically homogeneous atmosphere within the observed pressure region, we simultaneously fit the four spectra to constrain the abundances of atmospheric constituents, thereby yielding more precise constraints than previous work that analysed each spectrum independently. Our analysis reveals statistically significant evidence of NH3 (4$\\sigma$) in a hot Jupiter's emission spectra for the first time, along with evidence of H2O (6.5$\\sigma$), CO (3.1$\\sigma$), and a non-detection of CH4. With our abundance constraints, we tentatively estimate the metallicity of WASP-43b at 0.6-6.5$\\times$solar and its C/O ratio at 0.6-0.9. Our findings offer vital insights into the atmospheric conditions and formation history of WASP-43b by simultaneously constraining the abundances of carbon, oxygen, and nitrogen-bearing species.","sentences":["Spectroscopic phase curves of hot Jupiters measure their emission spectra at multiple orbital phases, thus enabling detailed characterisation of their atmospheres.","Precise constraints on the atmospheric composition of these exoplanets offer insights into their formation and evolution.","We analyse four phase-resolved emission spectra of the hot Jupiter WASP-43b, generated from a phase curve observed with the MIRI/LRS onboard the JWST, to retrieve its atmospheric properties.","Using a parametric 2D temperature model and assuming a chemically homogeneous atmosphere within the observed pressure region, we simultaneously fit the four spectra to constrain the abundances of atmospheric constituents, thereby yielding more precise constraints than previous work that analysed each spectrum independently.","Our analysis reveals statistically significant evidence of NH3 (4$\\sigma$) in a hot Jupiter's emission spectra for the first time, along with evidence of H2O (6.5$\\sigma$), CO (3.1$\\sigma$), and a non-detection of CH4.","With our abundance constraints, we tentatively estimate the metallicity of WASP-43b at 0.6-6.5$\\times$solar and its C/O ratio at 0.6-0.9.","Our findings offer vital insights into the atmospheric conditions and formation history of WASP-43b by simultaneously constraining the abundances of carbon, oxygen, and nitrogen-bearing species."],"url":"http://arxiv.org/abs/2406.03490v1","category":"astro-ph.EP"}
{"created":"2024-06-05 17:49:47","title":"Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends","abstract":"Recent advancements in large language models (LLMs) have considerably advanced the capabilities of summarization systems. However, they continue to face concerns about hallucinations. While prior work has evaluated LLMs extensively in news domains, most evaluation of dialogue summarization has focused on BART-based models, leaving a gap in our understanding of their faithfulness. Our work benchmarks the faithfulness of LLMs for dialogue summarization, using human annotations and focusing on identifying and categorizing span-level inconsistencies. Specifically, we focus on two prominent LLMs: GPT-4 and Alpaca-13B. Our evaluation reveals subtleties as to what constitutes a hallucination: LLMs often generate plausible inferences, supported by circumstantial evidence in the conversation, that lack direct evidence, a pattern that is less prevalent in older models. We propose a refined taxonomy of errors, coining the category of \"Circumstantial Inference\" to bucket these LLM behaviors and release the dataset. Using our taxonomy, we compare the behavioral differences between LLMs and older fine-tuned models. Additionally, we systematically assess the efficacy of automatic error detection methods on LLM summaries and find that they struggle to detect these nuanced errors. To address this, we introduce two prompt-based approaches for fine-grained error detection that outperform existing metrics, particularly for identifying \"Circumstantial Inference.\"","sentences":["Recent advancements in large language models (LLMs) have considerably advanced the capabilities of summarization systems.","However, they continue to face concerns about hallucinations.","While prior work has evaluated LLMs extensively in news domains, most evaluation of dialogue summarization has focused on BART-based models, leaving a gap in our understanding of their faithfulness.","Our work benchmarks the faithfulness of LLMs for dialogue summarization, using human annotations and focusing on identifying and categorizing span-level inconsistencies.","Specifically, we focus on two prominent LLMs: GPT-4 and Alpaca-13B. Our evaluation reveals subtleties as to what constitutes a hallucination: LLMs often generate plausible inferences, supported by circumstantial evidence in the conversation, that lack direct evidence, a pattern that is less prevalent in older models.","We propose a refined taxonomy of errors, coining the category of \"Circumstantial Inference\" to bucket these LLM behaviors and release the dataset.","Using our taxonomy, we compare the behavioral differences between LLMs and older fine-tuned models.","Additionally, we systematically assess the efficacy of automatic error detection methods on LLM summaries and find that they struggle to detect these nuanced errors.","To address this, we introduce two prompt-based approaches for fine-grained error detection that outperform existing metrics, particularly for identifying \"Circumstantial Inference.\""],"url":"http://arxiv.org/abs/2406.03487v1","category":"cs.CL"}
{"created":"2024-06-05 17:49:24","title":"BIPED: Pedagogically Informed Tutoring System for ESL Education","abstract":"Large Language Models (LLMs) have a great potential to serve as readily available and cost-efficient Conversational Intelligent Tutoring Systems (CITS) for teaching L2 learners of English. Existing CITS, however, are designed to teach only simple concepts or lack the pedagogical depth necessary to address diverse learning strategies. To develop a more pedagogically informed CITS capable of teaching complex concepts, we construct a BIlingual PEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human English tutoring interactions. Through post-hoc analysis of the tutoring interactions, we come up with a lexicon of dialogue acts (34 tutor acts and 9 student acts), which we use to further annotate the collected dataset. Based on a two-step framework of first predicting the appropriate tutor act then generating the corresponding response, we implemented two CITS models using GPT-4 and SOLAR-KO, respectively. We experimentally demonstrate that the implemented models not only replicate the style of human teachers but also employ diverse and contextually appropriate pedagogical strategies.","sentences":["Large Language Models (LLMs) have a great potential to serve as readily available and cost-efficient Conversational Intelligent Tutoring Systems (CITS) for teaching L2 learners of English.","Existing CITS, however, are designed to teach only simple concepts or lack the pedagogical depth necessary to address diverse learning strategies.","To develop a more pedagogically informed CITS capable of teaching complex concepts, we construct a BIlingual PEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human English tutoring interactions.","Through post-hoc analysis of the tutoring interactions, we come up with a lexicon of dialogue acts (34 tutor acts and 9 student acts), which we use to further annotate the collected dataset.","Based on a two-step framework of first predicting the appropriate tutor act then generating the corresponding response, we implemented two CITS models using GPT-4 and SOLAR-KO, respectively.","We experimentally demonstrate that the implemented models not only replicate the style of human teachers but also employ diverse and contextually appropriate pedagogical strategies."],"url":"http://arxiv.org/abs/2406.03486v1","category":"cs.CL"}
{"created":"2024-06-05 17:46:26","title":"Highway Value Iteration Networks","abstract":"Value iteration networks (VINs) enable end-to-end learning for planning tasks by employing a differentiable \"planning module\" that approximates the value iteration algorithm. However, long-term planning remains a challenge because training very deep VINs is difficult. To address this problem, we embed highway value iteration -- a recent algorithm designed to facilitate long-term credit assignment -- into the structure of VINs. This improvement augments the \"planning module\" of the VIN with three additional components: 1) an \"aggregate gate,\" which constructs skip connections to improve information flow across many layers; 2) an \"exploration module,\" crafted to increase the diversity of information and gradient flow in spatial dimensions; 3) a \"filter gate\" designed to ensure safe exploration. The resulting novel highway VIN can be trained effectively with hundreds of layers using standard backpropagation. In long-term planning tasks requiring hundreds of planning steps, deep highway VINs outperform both traditional VINs and several advanced, very deep NNs.","sentences":["Value iteration networks (VINs) enable end-to-end learning for planning tasks by employing a differentiable \"planning module\" that approximates the value iteration algorithm.","However, long-term planning remains a challenge because training very deep VINs is difficult.","To address this problem, we embed highway value iteration -- a recent algorithm designed to facilitate long-term credit assignment -- into the structure of VINs.","This improvement augments the \"planning module\" of the VIN with three additional components: 1) an \"aggregate gate,\" which constructs skip connections to improve information flow across many layers; 2) an \"exploration module,\" crafted to increase the diversity of information and gradient flow in spatial dimensions; 3) a \"filter gate\" designed to ensure safe exploration.","The resulting novel highway VIN can be trained effectively with hundreds of layers using standard backpropagation.","In long-term planning tasks requiring hundreds of planning steps, deep highway VINs outperform both traditional VINs and several advanced, very deep NNs."],"url":"http://arxiv.org/abs/2406.03485v1","category":"cs.LG"}
{"created":"2024-06-05 17:44:19","title":"Raman effects in Quantum Frequency Conversion using Bragg Scattering","abstract":"We present a quantum-mechanical model that describes fiber-based frequency conversion by four-wave-mixing Bragg scattering in the presence of Raman interactions. In the case of continuous-wave pumps we find closed-form expressions for the conversion efficiency and photon statistics, characterized by the second-order correlation function. For pulsed pumps, we derive a highly general model based on Green functions, and provide a numerical solution method using a split-step scheme. In both cases, we find that noise from spontaneous Raman scattering can pose a serious challenge to this type of frequency conversion if the pumps are less than 30 THz from the quantum fields. However, this impact can be mitigated with crosspolarized pumps and on the anti-Stokes side, through cooling of the fiber.","sentences":["We present a quantum-mechanical model that describes fiber-based frequency conversion by four-wave-mixing Bragg scattering in the presence of Raman interactions.","In the case of continuous-wave pumps we find closed-form expressions for the conversion efficiency and photon statistics, characterized by the second-order correlation function.","For pulsed pumps, we derive a highly general model based on Green functions, and provide a numerical solution method using a split-step scheme.","In both cases, we find that noise from spontaneous Raman scattering can pose a serious challenge to this type of frequency conversion if the pumps are less than 30 THz from the quantum fields.","However, this impact can be mitigated with crosspolarized pumps and on the anti-Stokes side, through cooling of the fiber."],"url":"http://arxiv.org/abs/2406.03484v1","category":"quant-ph"}
{"created":"2024-06-05 17:43:23","title":"A Low Duty Cycle Pulsed UV Technique for Spectroscopy of Aluminum Monochloride","abstract":"We present a novel technique to minimize UV-induced damage in experiments that employ second-harmonic generation cavities. The principle of our approach is to reduce the duty cycle of the UV light as much as possible to prolong the lifetime of the used optics. The low duty cycle is achieved by ramping the cavity into resonance for a short time during the experimental cycle when the light is used and tuning it to an off-resonant state otherwise. The necessary fast ramp and length-stabilization control of the cavity is implemented with the FPGA-based STEMlab platform. We demonstrate the utility of this method by measuring the isotope shift of the electronic transition ($X^1\\Sigma \\leftarrow A^1\\Pi$) in AlCl at 261.5 nm in a pulsed molecular beam experiment.","sentences":["We present a novel technique to minimize UV-induced damage in experiments that employ second-harmonic generation cavities.","The principle of our approach is to reduce the duty cycle of the UV light as much as possible to prolong the lifetime of the used optics.","The low duty cycle is achieved by ramping the cavity into resonance for a short time during the experimental cycle when the light is used and tuning it to an off-resonant state otherwise.","The necessary fast ramp and length-stabilization control of the cavity is implemented with the FPGA-based STEMlab platform.","We demonstrate the utility of this method by measuring the isotope shift of the electronic transition ($X^1\\Sigma \\leftarrow A^1\\Pi$) in AlCl at 261.5 nm in a pulsed molecular beam experiment."],"url":"http://arxiv.org/abs/2406.03483v1","category":"physics.atom-ph"}
{"created":"2024-06-05 17:42:05","title":"QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead","abstract":"Serving LLMs requires substantial memory due to the storage requirements of Key-Value (KV) embeddings in the KV cache, which grows with sequence length. An effective approach to compress KV cache is quantization. However, traditional quantization methods face significant memory overhead due to the need to store quantization constants (at least a zero point and a scale) in full precision per data block. Depending on the block size, this overhead can add 1 or 2 bits per quantized number. We introduce QJL, a new quantization approach that consists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit quantization. In contrast to existing methods, QJL eliminates memory overheads by removing the need for storing quantization constants. We propose an asymmetric estimator for the inner product of two vectors and demonstrate that applying QJL to one vector and a standard JL transform without quantization to the other provides an unbiased estimator with minimal distortion. We have developed an efficient implementation of the QJL sketch and its corresponding inner product estimator, incorporating a lightweight CUDA kernel for optimized computation. When applied across various LLMs and NLP tasks to quantize the KV cache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV cache memory usage without compromising accuracy, all while achieving faster runtime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.","sentences":["Serving LLMs requires substantial memory due to the storage requirements of Key-Value (KV) embeddings in the KV cache, which grows with sequence length.","An effective approach to compress KV cache is quantization.","However, traditional quantization methods face significant memory overhead due to the need to store quantization constants (at least a zero point and a scale) in full precision per data block.","Depending on the block size, this overhead can add 1 or 2 bits per quantized number.","We introduce QJL, a new quantization approach that consists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit quantization.","In contrast to existing methods, QJL eliminates memory overheads by removing the need for storing quantization constants.","We propose an asymmetric estimator for the inner product of two vectors and demonstrate that applying QJL to one vector and a standard JL transform without quantization to the other provides an unbiased estimator with minimal distortion.","We have developed an efficient implementation of the QJL sketch and its corresponding inner product estimator, incorporating a lightweight CUDA kernel for optimized computation.","When applied across various LLMs and NLP tasks to quantize the KV cache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV cache memory usage without compromising accuracy, all while achieving faster runtime.","Codes are available at \\url{https://github.com/amirzandieh/QJL}."],"url":"http://arxiv.org/abs/2406.03482v1","category":"cs.LG"}
{"created":"2024-06-05 17:37:21","title":"Unpacking Approaches to Learning and Teaching Machine Learning in K-12 Education: Transparency, Ethics, and Design Activities","abstract":"In this conceptual paper, we review existing literature on artificial intelligence/machine learning (AI/ML) education to identify three approaches to how learning and teaching ML could be conceptualized. One of them, a data-driven approach, emphasizes providing young people with opportunities to create data sets, train, and test models. A second approach, learning algorithm-driven, prioritizes learning about how the learning algorithms or engines behind how ML models work. In addition, we identify efforts within a third approach that integrates the previous two. In our review, we focus on how the approaches: (1) glassbox and blackbox different aspects of ML, (2) build on learner interests and provide opportunities for designing applications, (3) integrate ethics and justice. In the discussion, we address the challenges and opportunities of current approaches and suggest future directions for the design of learning activities.","sentences":["In this conceptual paper, we review existing literature on artificial intelligence/machine learning (AI/ML) education to identify three approaches to how learning and teaching ML could be conceptualized.","One of them, a data-driven approach, emphasizes providing young people with opportunities to create data sets, train, and test models.","A second approach, learning algorithm-driven, prioritizes learning about how the learning algorithms or engines behind how ML models work.","In addition, we identify efforts within a third approach that integrates the previous two.","In our review, we focus on how the approaches: (1) glassbox and blackbox different aspects of ML, (2) build on learner interests and provide opportunities for designing applications, (3) integrate ethics and justice.","In the discussion, we address the challenges and opportunities of current approaches and suggest future directions for the design of learning activities."],"url":"http://arxiv.org/abs/2406.03480v1","category":"cs.CY"}
{"created":"2024-06-05 17:32:28","title":"MODABS: Multi-Objective Learning for Dynamic Aspect-Based Summarization","abstract":"The rapid proliferation of online content necessitates effective summarization methods, among which dynamic aspect-based summarization stands out. Unlike its traditional counterpart, which assumes a fixed set of known aspects, this approach adapts to the varied aspects of the input text. We introduce a novel multi-objective learning framework employing a Longformer-Encoder-Decoder for this task. The framework optimizes aspect number prediction, minimizes disparity between generated and reference summaries for each aspect, and maximizes dissimilarity across aspect-specific summaries. Extensive experiments show our method significantly outperforms baselines on three diverse datasets, largely due to the effective alignment of generated and reference aspect counts without sacrificing single-aspect summarization quality.","sentences":["The rapid proliferation of online content necessitates effective summarization methods, among which dynamic aspect-based summarization stands out.","Unlike its traditional counterpart, which assumes a fixed set of known aspects, this approach adapts to the varied aspects of the input text.","We introduce a novel multi-objective learning framework employing a Longformer-Encoder-Decoder for this task.","The framework optimizes aspect number prediction, minimizes disparity between generated and reference summaries for each aspect, and maximizes dissimilarity across aspect-specific summaries.","Extensive experiments show our method significantly outperforms baselines on three diverse datasets, largely due to the effective alignment of generated and reference aspect counts without sacrificing single-aspect summarization quality."],"url":"http://arxiv.org/abs/2406.03479v1","category":"cs.CL"}
{"created":"2024-06-05 17:29:15","title":"Does your data spark joy? Performance gains from domain upsampling at the end of training","abstract":"Pretraining datasets for large language models (LLMs) have grown to trillions of tokens composed of large amounts of CommonCrawl (CC) web scrape along with smaller, domain-specific datasets. It is expensive to understand the impact of these domain-specific datasets on model capabilities as training at large FLOP scales is required to reveal significant changes to difficult and emergent benchmarks. Given the increasing cost of experimenting with pretraining data, how does one determine the optimal balance between the diversity in general web scrapes and the information density of domain specific data? In this work, we show how to leverage the smaller domain specific datasets by upsampling them relative to CC at the end of training to drive performance improvements on difficult benchmarks. This simple technique allows us to improve up to 6.90 pp on MMLU, 8.26 pp on GSM8K, and 6.17 pp on HumanEval relative to the base data mix for a 7B model trained for 1 trillion (T) tokens, thus rivaling Llama-2 (7B)$\\unicode{x2014}$a model trained for twice as long. We experiment with ablating the duration of domain upsampling from 5% to 30% of training and find that 10% to 20% percent is optimal for navigating the tradeoff between general language modeling capabilities and targeted benchmarks. We also use domain upsampling to characterize at scale the utility of individual datasets for improving various benchmarks by removing them during this final phase of training. This tool opens up the ability to experiment with the impact of different pretraining datasets at scale, but at an order of magnitude lower cost compared to full pretraining runs.","sentences":["Pretraining datasets for large language models (LLMs) have grown to trillions of tokens composed of large amounts of CommonCrawl (CC) web scrape along with smaller, domain-specific datasets.","It is expensive to understand the impact of these domain-specific datasets on model capabilities as training at large FLOP scales is required to reveal significant changes to difficult and emergent benchmarks.","Given the increasing cost of experimenting with pretraining data, how does one determine the optimal balance between the diversity in general web scrapes and the information density of domain specific data?","In this work, we show how to leverage the smaller domain specific datasets by upsampling them relative to CC at the end of training to drive performance improvements on difficult benchmarks.","This simple technique allows us to improve up to 6.90 pp on MMLU, 8.26 pp on GSM8K, and 6.17 pp on HumanEval relative to the base data mix for a 7B model trained for 1 trillion (T) tokens, thus rivaling Llama-2 (7B)$\\unicode{x2014}$a model trained for twice as long.","We experiment with ablating the duration of domain upsampling from 5% to 30% of training and find that 10% to 20% percent is optimal for navigating the tradeoff between general language modeling capabilities and targeted benchmarks.","We also use domain upsampling to characterize at scale the utility of individual datasets for improving various benchmarks by removing them during this final phase of training.","This tool opens up the ability to experiment with the impact of different pretraining datasets at scale, but at an order of magnitude lower cost compared to full pretraining runs."],"url":"http://arxiv.org/abs/2406.03476v1","category":"cs.LG"}
{"created":"2024-06-05 17:25:46","title":"AD-H: Autonomous Driving with Hierarchical Agents","abstract":"Due to the impressive capabilities of multimodal large language models (MLLMs), recent works have focused on employing MLLM-based agents for autonomous driving in large-scale and dynamic environments. However, prevalent approaches often directly translate high-level instructions into low-level vehicle control signals, which deviates from the inherent language generation paradigm of MLLMs and fails to fully harness their emergent powers. As a result, the generalizability of these methods is highly restricted by autonomous driving datasets used during fine-tuning. To tackle this challenge, we propose to connect high-level instructions and low-level control signals with mid-level language-driven commands, which are more fine-grained than high-level instructions but more universal and explainable than control signals, and thus can effectively bridge the gap in between. We implement this idea through a hierarchical multi-agent driving system named AD-H, including a MLLM planner for high-level reasoning and a lightweight controller for low-level execution. The hierarchical design liberates the MLLM from low-level control signal decoding and therefore fully releases their emergent capability in high-level perception, reasoning, and planning. We build a new dataset with action hierarchy annotations. Comprehensive closed-loop evaluations demonstrate several key advantages of our proposed AD-H system. First, AD-H can notably outperform state-of-the-art methods in achieving exceptional driving performance, even exhibiting self-correction capabilities during vehicle operation, a scenario not encountered in the training dataset. Second, AD-H demonstrates superior generalization under long-horizon instructions and novel environmental conditions, significantly surpassing current state-of-the-art methods. We will make our data and code publicly accessible at https://github.com/zhangzaibin/AD-H","sentences":["Due to the impressive capabilities of multimodal large language models (MLLMs), recent works have focused on employing MLLM-based agents for autonomous driving in large-scale and dynamic environments.","However, prevalent approaches often directly translate high-level instructions into low-level vehicle control signals, which deviates from the inherent language generation paradigm of MLLMs and fails to fully harness their emergent powers.","As a result, the generalizability of these methods is highly restricted by autonomous driving datasets used during fine-tuning.","To tackle this challenge, we propose to connect high-level instructions and low-level control signals with mid-level language-driven commands, which are more fine-grained than high-level instructions but more universal and explainable than control signals, and thus can effectively bridge the gap in between.","We implement this idea through a hierarchical multi-agent driving system named AD-H, including a MLLM planner for high-level reasoning and a lightweight controller for low-level execution.","The hierarchical design liberates the MLLM from low-level control signal decoding and therefore fully releases their emergent capability in high-level perception, reasoning, and planning.","We build a new dataset with action hierarchy annotations.","Comprehensive closed-loop evaluations demonstrate several key advantages of our proposed AD-H system.","First, AD-H can notably outperform state-of-the-art methods in achieving exceptional driving performance, even exhibiting self-correction capabilities during vehicle operation, a scenario not encountered in the training dataset.","Second, AD-H demonstrates superior generalization under long-horizon instructions and novel environmental conditions, significantly surpassing current state-of-the-art methods.","We will make our data and code publicly accessible at https://github.com/zhangzaibin/AD-H"],"url":"http://arxiv.org/abs/2406.03474v1","category":"cs.CV"}
{"created":"2024-06-05 17:25:33","title":"Mapping dynamical systems into chemical reactions","abstract":"Dynamical systems with polynomials on the right-hand side can model a wide range of physical processes. A subset of such dynamical systems that can model chemical reactions under mass-action kinetics are called chemical systems. A central problem in synthetic biology is to map general polynomial dynamical systems into dynamically similar chemical ones. In this paper, we present a novel map, called the quasi-chemical map, that can systematically solve this problem. The quasi-chemical map introduces suitable state-dependent perturbations into any given polynomial dynamical system which then becomes chemical under suitably large translation of variables. We prove that this map preserves robust dynamical features, such as generic equilibria and limit cycles, as well as temporal properties, such as periods of oscillations. Furthermore, the resulting chemical systems are of only at most one degree higher than the original dynamical systems. We demonstrate the quasi-chemical map by designing relatively simple chemical systems with exotic dynamics and pre-defined bifurcation structures.","sentences":["Dynamical systems with polynomials on the right-hand side can model a wide range of physical processes.","A subset of such dynamical systems that can model chemical reactions under mass-action kinetics are called chemical systems.","A central problem in synthetic biology is to map general polynomial dynamical systems into dynamically similar chemical ones.","In this paper, we present a novel map, called the quasi-chemical map, that can systematically solve this problem.","The quasi-chemical map introduces suitable state-dependent perturbations into any given polynomial dynamical system which then becomes chemical under suitably large translation of variables.","We prove that this map preserves robust dynamical features, such as generic equilibria and limit cycles, as well as temporal properties, such as periods of oscillations.","Furthermore, the resulting chemical systems are of only at most one degree higher than the original dynamical systems.","We demonstrate the quasi-chemical map by designing relatively simple chemical systems with exotic dynamics and pre-defined bifurcation structures."],"url":"http://arxiv.org/abs/2406.03473v1","category":"q-bio.MN"}
{"created":"2024-06-05 17:24:59","title":"Non-trivial copies of N*","abstract":"We show that it is consistent to have regular closed non-clopen copies of $\\mathbb N^*$ within $\\mathbb N^*$ and a non-trivial self-map of $\\mathbb N^*$ even if all autohomeomorphisms of $\\mathbb N^*$ are trivial.","sentences":["We show that it is consistent to have regular closed non-clopen copies of $\\mathbb N^*$ within $\\mathbb N^*$ and a non-trivial self-map of $\\mathbb N^*$ even if all autohomeomorphisms of $\\mathbb N^*$ are trivial."],"url":"http://arxiv.org/abs/2406.03471v1","category":"math.GN"}
{"created":"2024-06-05 17:24:07","title":"SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN","abstract":"Spiking neural network (SNN) has attracted great attention due to its characteristic of high efficiency and accuracy. Currently, the ANN-to-SNN conversion methods can obtain ANN on-par accuracy SNN with ultra-low latency (8 time-steps) in CNN structure on computer vision (CV) tasks. However, as Transformer-based networks have achieved prevailing precision on both CV and natural language processing (NLP), the Transformer-based SNNs are still encounting the lower accuracy w.r.t the ANN counterparts. In this work, we introduce a novel ANN-to-SNN conversion method called SpikeZIP-TF, where ANN and SNN are exactly equivalent, thus incurring no accuracy degradation. SpikeZIP-TF achieves 83.82% accuracy on CV dataset (ImageNet) and 93.79% accuracy on NLP dataset (SST-2), which are higher than SOTA Transformer-based SNNs. The code is available in GitHub: https://github.com/Intelligent-Computing-Research-Group/SpikeZIP_transformer","sentences":["Spiking neural network (SNN) has attracted great attention due to its characteristic of high efficiency and accuracy.","Currently, the ANN-to-SNN conversion methods can obtain ANN on-par accuracy SNN with ultra-low latency (8 time-steps) in CNN structure on computer vision (CV) tasks.","However, as Transformer-based networks have achieved prevailing precision on both CV and natural language processing (NLP), the Transformer-based SNNs are still encounting the lower accuracy w.r.t the ANN counterparts.","In this work, we introduce a novel ANN-to-SNN conversion method called SpikeZIP-TF, where ANN and SNN are exactly equivalent, thus incurring no accuracy degradation.","SpikeZIP-TF achieves 83.82% accuracy on CV dataset (ImageNet) and 93.79% accuracy on NLP dataset (SST-2), which are higher than SOTA Transformer-based SNNs.","The code is available in GitHub: https://github.com/Intelligent-Computing-Research-Group/SpikeZIP_transformer"],"url":"http://arxiv.org/abs/2406.03470v1","category":"cs.NE"}
{"created":"2024-06-05 17:16:54","title":"Topological defects and tensionless holography","abstract":"We study topological defect lines in the symmetric-product orbifolds $\\mathrm{Sym}^N(X)$ for a generic seed CFT $X$. We focus on the defects which preserve the maximum of the $S_N$ symmetry. When $X$ is taken to describe the free theory of four fermions and four bosons on a $\\mathbb{T}^4$, we construct holographically dual backgrounds describing propagation of tensionless closed strings in the presence of spacetime defects wrapping the conformal boundary. We find a precise match between the spectra of local on-shell closed-string vertex operators in the bulk and the spectra of non-local disorder fields in the spacetime theory.","sentences":["We study topological defect lines in the symmetric-product orbifolds $\\mathrm{Sym}^N(X)$ for a generic seed CFT $X$. We focus on the defects which preserve the maximum of the $S_N$ symmetry.","When $X$ is taken to describe the free theory of four fermions and four bosons on a $\\mathbb{T}^4$, we construct holographically dual backgrounds describing propagation of tensionless closed strings in the presence of spacetime defects wrapping the conformal boundary.","We find a precise match between the spectra of local on-shell closed-string vertex operators in the bulk and the spectra of non-local disorder fields in the spacetime theory."],"url":"http://arxiv.org/abs/2406.03467v1","category":"hep-th"}
{"created":"2024-06-05 17:16:07","title":"Parallel Quantum Computing Simulations via Quantum Accelerator Platform Virtualization","abstract":"Quantum circuit execution is the central task in quantum computation. Due to inherent quantum-mechanical constraints, quantum computing workflows often involve a considerable number of independent measurements over a large set of slightly different quantum circuits. Here we discuss a simple model for parallelizing simulation of such quantum circuit executions that is based on introducing a large array of virtual quantum processing units, mapped to classical HPC nodes, as a parallel quantum computing platform. Implemented within the XACC framework, the model can readily take advantage of its backend-agnostic features, enabling parallel quantum circuit execution over any target backend supported by XACC. We illustrate the performance of this approach by demonstrating strong scaling in two pertinent domain science problems, namely in computing the gradients for the multi-contracted variational quantum eigensolver and in data-driven quantum circuit learning, where we vary the number of qubits and the number of circuit layers. The latter (classical) simulation leverages the cuQuantum SDK library to run efficiently on GPU-accelerated HPC platforms.","sentences":["Quantum circuit execution is the central task in quantum computation.","Due to inherent quantum-mechanical constraints, quantum computing workflows often involve a considerable number of independent measurements over a large set of slightly different quantum circuits.","Here we discuss a simple model for parallelizing simulation of such quantum circuit executions that is based on introducing a large array of virtual quantum processing units, mapped to classical HPC nodes, as a parallel quantum computing platform.","Implemented within the XACC framework, the model can readily take advantage of its backend-agnostic features, enabling parallel quantum circuit execution over any target backend supported by XACC.","We illustrate the performance of this approach by demonstrating strong scaling in two pertinent domain science problems, namely in computing the gradients for the multi-contracted variational quantum eigensolver and in data-driven quantum circuit learning, where we vary the number of qubits and the number of circuit layers.","The latter (classical) simulation leverages the cuQuantum SDK library to run efficiently on GPU-accelerated HPC platforms."],"url":"http://arxiv.org/abs/2406.03466v1","category":"quant-ph"}
{"created":"2024-06-05 17:03:47","title":"Distributional Adversarial Loss","abstract":"A major challenge in defending against adversarial attacks is the enormous space of possible attacks that even a simple adversary might perform. To address this, prior work has proposed a variety of defenses that effectively reduce the size of this space. These include randomized smoothing methods that add noise to the input to take away some of the adversary's impact. Another approach is input discretization which limits the adversary's possible number of actions.   Motivated by these two approaches, we introduce a new notion of adversarial loss which we call distributional adversarial loss, to unify these two forms of effectively weakening an adversary. In this notion, we assume for each original example, the allowed adversarial perturbation set is a family of distributions (e.g., induced by a smoothing procedure), and the adversarial loss over each example is the maximum loss over all the associated distributions. The goal is to minimize the overall adversarial loss.   We show generalization guarantees for our notion of adversarial loss in terms of the VC-dimension of the hypothesis class and the size of the set of allowed adversarial distributions associated with each input. We also investigate the role of randomness in achieving robustness against adversarial attacks in the methods described above. We show a general derandomization technique that preserves the extent of a randomized classifier's robustness against adversarial attacks. We corroborate the procedure experimentally via derandomizing the Random Projection Filters framework of \\cite{dong2023adversarial}. Our procedure also improves the robustness of the model against various adversarial attacks.","sentences":["A major challenge in defending against adversarial attacks is the enormous space of possible attacks that even a simple adversary might perform.","To address this, prior work has proposed a variety of defenses that effectively reduce the size of this space.","These include randomized smoothing methods that add noise to the input to take away some of the adversary's impact.","Another approach is input discretization which limits the adversary's possible number of actions.   ","Motivated by these two approaches, we introduce a new notion of adversarial loss which we call distributional adversarial loss, to unify these two forms of effectively weakening an adversary.","In this notion, we assume for each original example, the allowed adversarial perturbation set is a family of distributions (e.g., induced by a smoothing procedure), and the adversarial loss over each example is the maximum loss over all the associated distributions.","The goal is to minimize the overall adversarial loss.   ","We show generalization guarantees for our notion of adversarial loss in terms of the VC-dimension of the hypothesis class and the size of the set of allowed adversarial distributions associated with each input.","We also investigate the role of randomness in achieving robustness against adversarial attacks in the methods described above.","We show a general derandomization technique that preserves the extent of a randomized classifier's robustness against adversarial attacks.","We corroborate the procedure experimentally via derandomizing the Random Projection Filters framework of \\cite{dong2023adversarial}.","Our procedure also improves the robustness of the model against various adversarial attacks."],"url":"http://arxiv.org/abs/2406.03458v1","category":"cs.LG"}
{"created":"2024-06-05 17:01:55","title":"Gravitational radiation from inspiralling compact binaries to N$^3$LO in the Effective Field Theory approach","abstract":"Within the context of the Effective Field Theory (EFT) framework to gravitational dynamics, we compute the Hamiltonian, source quadrupole moment, and gravitational-wave energy flux for (non-spinning) inspiralling compact binaries at next-to-next-to-next-to leading order (N$^3$LO) in the Post-Newtonian (PN) expansion. We use the recently developed $d$-dimensional multipole-expanded effective theory, and explicitly perform the matching to the (pseudo-) stress-energy tensor. The calculation involves Feynman integrals up to three- (conservative) and two-loop (radiative) orders, evaluated within dimensional regularization. Our (ambiguity-free) results confirm (for the first time) the value of the gravitational-wave flux for quasi-circular orbits at 3PN order, while paving the way forward to the inclusion of spin effects as well as higher order computations.","sentences":["Within the context of the Effective Field Theory (EFT) framework to gravitational dynamics, we compute the Hamiltonian, source quadrupole moment, and gravitational-wave energy flux for (non-spinning) inspiralling compact binaries at next-to-next-to-next-to leading order (N$^3$LO) in the Post-Newtonian (PN) expansion.","We use the recently developed $d$-dimensional multipole-expanded effective theory, and explicitly perform the matching to the (pseudo-) stress-energy tensor.","The calculation involves Feynman integrals up to three- (conservative) and two-loop (radiative) orders, evaluated within dimensional regularization.","Our (ambiguity-free) results confirm (for the first time) the value of the gravitational-wave flux for quasi-circular orbits at 3PN order, while paving the way forward to the inclusion of spin effects as well as higher order computations."],"url":"http://arxiv.org/abs/2406.03457v1","category":"gr-qc"}
{"created":"2024-06-05 17:00:16","title":"Recurrent neural chemical reaction networks that approximate arbitrary dynamics","abstract":"Many important phenomena in chemistry and biology are realized via dynamical features such as multi-stability, oscillations, and chaos. Construction of novel chemical systems with such finely-tuned dynamics is a challenging problem central to the growing field of synthetic biology. In this paper, we address this problem by putting forward a molecular version of a recurrent artificial neural network, which we call a recurrent neural chemical reaction network (RNCRN). We prove that the RNCRN, with sufficiently many auxiliary chemical species and suitable fast reactions, can be systematically trained to achieve any dynamics. This approximation ability is shown to hold independent of the initial conditions for the auxiliary species, making the RNCRN more experimentally feasible. To demonstrate the results, we present a number of relatively simple RNCRNs trained to display a variety of biologically-important dynamical features.","sentences":["Many important phenomena in chemistry and biology are realized via dynamical features such as multi-stability, oscillations, and chaos.","Construction of novel chemical systems with such finely-tuned dynamics is a challenging problem central to the growing field of synthetic biology.","In this paper, we address this problem by putting forward a molecular version of a recurrent artificial neural network, which we call a recurrent neural chemical reaction network (RNCRN).","We prove that the RNCRN, with sufficiently many auxiliary chemical species and suitable fast reactions, can be systematically trained to achieve any dynamics.","This approximation ability is shown to hold independent of the initial conditions for the auxiliary species, making the RNCRN more experimentally feasible.","To demonstrate the results, we present a number of relatively simple RNCRNs trained to display a variety of biologically-important dynamical features."],"url":"http://arxiv.org/abs/2406.03456v1","category":"q-bio.MN"}
{"created":"2024-06-05 16:57:57","title":"Mission Design for Unmanned Aerial Vehicles using Hybrid Probabilistic Logic Program","abstract":"Advanced Air Mobility (AAM) is a growing field that demands a deep understanding of legal, spatial and temporal concepts in navigation. Hence, any implementation of AAM is forced to deal with the inherent uncertainties of human-inhabited spaces. Enabling growth and innovation requires the creation of a system for safe and robust mission design, i.e., the way we formalize intentions and decide their execution as trajectories for the Unmanned Aerial Vehicle (UAV). Although legal frameworks have emerged to govern urban air spaces, their full integration into the decision process of autonomous agents and operators remains an open task. In this work we present ProMis, a system architecture for probabilistic mission design. It links the data available from various static and dynamic data sources with legal text and operator requirements by following principles of formal verification and probabilistic modeling. Hereby, ProMis enables the combination of low-level perception and high-level rules in AAM to infer validity over the UAV's state-space. To this end, we employ Hybrid Probabilistic Logic Programs (HPLP) as a unifying, intermediate representation between perception and action-taking. Furthermore, we present methods to connect ProMis with crowd-sourced map data by generating HPLP atoms that represent spatial relations in a probabilistic fashion. Our claims of the utility and generality of ProMis are supported by experiments on a diverse set of scenarios and a discussion of the computational demands associated with probabilistic missions.","sentences":["Advanced Air Mobility (AAM) is a growing field that demands a deep understanding of legal, spatial and temporal concepts in navigation.","Hence, any implementation of AAM is forced to deal with the inherent uncertainties of human-inhabited spaces.","Enabling growth and innovation requires the creation of a system for safe and robust mission design, i.e., the way we formalize intentions and decide their execution as trajectories for the Unmanned Aerial Vehicle (UAV).","Although legal frameworks have emerged to govern urban air spaces, their full integration into the decision process of autonomous agents and operators remains an open task.","In this work we present ProMis, a system architecture for probabilistic mission design.","It links the data available from various static and dynamic data sources with legal text and operator requirements by following principles of formal verification and probabilistic modeling.","Hereby, ProMis enables the combination of low-level perception and high-level rules in AAM to infer validity over the UAV's state-space.","To this end, we employ Hybrid Probabilistic Logic Programs (HPLP) as a unifying, intermediate representation between perception and action-taking.","Furthermore, we present methods to connect ProMis with crowd-sourced map data by generating HPLP atoms that represent spatial relations in a probabilistic fashion.","Our claims of the utility and generality of ProMis are supported by experiments on a diverse set of scenarios and a discussion of the computational demands associated with probabilistic missions."],"url":"http://arxiv.org/abs/2406.03454v1","category":"cs.RO"}
{"created":"2024-06-05 16:52:21","title":"Using Synchronic Definitions and Semantic Relations to Classify Semantic Change Types","abstract":"There is abundant evidence of the fact that the way words change their meaning can be classified in different types of change, highlighting the relationship between the old and new meanings (among which generalization, specialization and co-hyponymy transfer). In this paper, we present a way of detecting these types of change by constructing a model that leverages information both from synchronic lexical relations and definitions of word meanings. Specifically, we use synset definitions and hierarchy information from WordNet and test it on a digitized version of Blank's (1997) dataset of semantic change types. Finally, we show how the sense relationships can improve models for both approximation of human judgments of semantic relatedness as well as binary Lexical Semantic Change Detection.","sentences":["There is abundant evidence of the fact that the way words change their meaning can be classified in different types of change, highlighting the relationship between the old and new meanings (among which generalization, specialization and co-hyponymy transfer).","In this paper, we present a way of detecting these types of change by constructing a model that leverages information both from synchronic lexical relations and definitions of word meanings.","Specifically, we use synset definitions and hierarchy information from WordNet and test it on a digitized version of Blank's (1997) dataset of semantic change types.","Finally, we show how the sense relationships can improve models for both approximation of human judgments of semantic relatedness as well as binary Lexical Semantic Change Detection."],"url":"http://arxiv.org/abs/2406.03452v1","category":"cs.CL"}
{"created":"2024-06-05 16:48:26","title":"What is the Best Way for ChatGPT to Translate Poetry?","abstract":"Machine translation (MT) has historically faced significant challenges when applied to literary works, particularly in the domain of poetry translation. The advent of Large Language Models such as ChatGPT holds potential for innovation in this field. This study examines ChatGPT's capabilities in English-Chinese poetry translation tasks, utilizing targeted prompts and small sample scenarios to ascertain optimal performance. Despite promising outcomes, our analysis reveals persistent issues in the translations generated by ChatGPT that warrant attention. To address these shortcomings, we propose an Explanation-Assisted Poetry Machine Translation (EAPMT) method, which leverages monolingual poetry explanation as a guiding information for the translation process. Furthermore, we refine existing evaluation criteria to better suit the nuances of modern poetry translation. We engaged a panel of professional poets for assessments, complemented evaluations by using GPT-4. The results from both human and machine evaluations demonstrate that our EAPMT method outperforms traditional translation methods of ChatGPT and the existing online systems. This paper validates the efficacy of our method and contributes a novel perspective to machine-assisted literary translation.","sentences":["Machine translation (MT) has historically faced significant challenges when applied to literary works, particularly in the domain of poetry translation.","The advent of Large Language Models such as ChatGPT holds potential for innovation in this field.","This study examines ChatGPT's capabilities in English-Chinese poetry translation tasks, utilizing targeted prompts and small sample scenarios to ascertain optimal performance.","Despite promising outcomes, our analysis reveals persistent issues in the translations generated by ChatGPT that warrant attention.","To address these shortcomings, we propose an Explanation-Assisted Poetry Machine Translation (EAPMT) method, which leverages monolingual poetry explanation as a guiding information for the translation process.","Furthermore, we refine existing evaluation criteria to better suit the nuances of modern poetry translation.","We engaged a panel of professional poets for assessments, complemented evaluations by using GPT-4.","The results from both human and machine evaluations demonstrate that our EAPMT method outperforms traditional translation methods of ChatGPT and the existing online systems.","This paper validates the efficacy of our method and contributes a novel perspective to machine-assisted literary translation."],"url":"http://arxiv.org/abs/2406.03450v1","category":"cs.CL"}
{"created":"2024-06-05 16:47:37","title":"Global fermionic mode optimization via swap gates","abstract":"We propose a general approach to find an optimal representation of a quantum many body wave function for a given error margin via global fermionic mode optimization. The stationary point on a fixed rank matrix product state manifold is obtained via a joint optimization on the Grassman manifold [Phys. Rev. Lett. 117, 210402] together with swap gates controlled permutations. The minimization of the global quantity, the block entropy area, guarantees that the method fulfills all criteria with respect to partial derivatives. Numerical results via large scale density matrix renormalization group simulations on strongly correlated molecular systems and two-dimensional fermionic lattice models are discussed.","sentences":["We propose a general approach to find an optimal representation of a quantum many body wave function for a given error margin via global fermionic mode optimization.","The stationary point on a fixed rank matrix product state manifold is obtained via a joint optimization on the Grassman manifold [Phys. Rev. Lett.","117, 210402] together with swap gates controlled permutations.","The minimization of the global quantity, the block entropy area, guarantees that the method fulfills all criteria with respect to partial derivatives.","Numerical results via large scale density matrix renormalization group simulations on strongly correlated molecular systems and two-dimensional fermionic lattice models are discussed."],"url":"http://arxiv.org/abs/2406.03449v1","category":"cond-mat.str-el"}
{"created":"2024-06-05 16:44:06","title":"FILS: Self-Supervised Video Feature Prediction In Semantic Language Space","abstract":"This paper demonstrates a self-supervised approach for learning semantic video representations. Recent vision studies show that a masking strategy for vision and natural language supervision has contributed to developing transferable visual pretraining. Our goal is to achieve a more semantic video representation by leveraging the text related to the video content during the pretraining in a fully self-supervised manner. To this end, we present FILS, a novel self-supervised video Feature prediction In semantic Language Space (FILS). The vision model can capture valuable structured information by correctly predicting masked feature semantics in language space. It is learned using a patch-wise video-text contrastive strategy, in which the text representations act as prototypes for transforming vision features into a language space, which are then used as targets for semantically meaningful feature prediction using our masked encoder-decoder structure. FILS demonstrates remarkable transferability on downstream action recognition tasks, achieving state-of-the-art on challenging egocentric datasets, like Epic-Kitchens, Something-SomethingV2, Charades-Ego, and EGTEA, using ViT-Base. Our efficient method requires less computation and smaller batches compared to previous works.","sentences":["This paper demonstrates a self-supervised approach for learning semantic video representations.","Recent vision studies show that a masking strategy for vision and natural language supervision has contributed to developing transferable visual pretraining.","Our goal is to achieve a more semantic video representation by leveraging the text related to the video content during the pretraining in a fully self-supervised manner.","To this end, we present FILS, a novel self-supervised video Feature prediction In semantic Language Space (FILS).","The vision model can capture valuable structured information by correctly predicting masked feature semantics in language space.","It is learned using a patch-wise video-text contrastive strategy, in which the text representations act as prototypes for transforming vision features into a language space, which are then used as targets for semantically meaningful feature prediction using our masked encoder-decoder structure.","FILS demonstrates remarkable transferability on downstream action recognition tasks, achieving state-of-the-art on challenging egocentric datasets, like Epic-Kitchens, Something-SomethingV2, Charades-Ego, and EGTEA, using ViT-Base.","Our efficient method requires less computation and smaller batches compared to previous works."],"url":"http://arxiv.org/abs/2406.03447v1","category":"cs.CV"}
{"created":"2024-06-05 16:43:41","title":"Fixed point results for contractions of polynomial type","abstract":"We introduce two new classes of single-valued contractions of polynomial type defined on a metric space. For the first one, called the class of polynomial contractions, we establish two fixed point theorems. Namely, we first consider the case when the mapping is continuous. Next, we weaken the continuity condition. In particular, we recover Banach's fixed point theorem. The second class, called the class of almost polynomial contractions, includes the class of almost contractions introduced by Berinde [Nonlinear Analysis Forum. 9(1) (2004) 43--53]. A fixed point theorem is established for almost polynomial contractions. The obtained result generalizes that derived by Berinde in the above reference. Several examples showing that our generalizations are significant, are provided.","sentences":["We introduce two new classes of single-valued contractions of polynomial type defined on a metric space.","For the first one, called the class of polynomial contractions, we establish two fixed point theorems.","Namely, we first consider the case when the mapping is continuous.","Next, we weaken the continuity condition.","In particular, we recover Banach's fixed point theorem.","The second class, called the class of almost polynomial contractions, includes the class of almost contractions introduced by Berinde [Nonlinear Analysis Forum.","9(1) (2004) 43--53].","A fixed point theorem is established for almost polynomial contractions.","The obtained result generalizes that derived by Berinde in the above reference.","Several examples showing that our generalizations are significant, are provided."],"url":"http://arxiv.org/abs/2406.03446v1","category":"math.GN"}
{"created":"2024-06-05 16:36:21","title":"Are language models rational? The case of coherence norms and belief revision","abstract":"Do norms of rationality apply to machine learning models, in particular language models? In this paper we investigate this question by focusing on a special subset of rational norms: coherence norms. We consider both logical coherence norms as well as coherence norms tied to the strength of belief. To make sense of the latter, we introduce the Minimal Assent Connection (MAC) and propose a new account of credence, which captures the strength of belief in language models. This proposal uniformly assigns strength of belief simply on the basis of model internal next token probabilities. We argue that rational norms tied to coherence do apply to some language models, but not to others. This issue is significant since rationality is closely tied to predicting and explaining behavior, and thus it is connected to considerations about AI safety and alignment, as well as understanding model behavior more generally.","sentences":["Do norms of rationality apply to machine learning models, in particular language models?","In this paper we investigate this question by focusing on a special subset of rational norms: coherence norms.","We consider both logical coherence norms as well as coherence norms tied to the strength of belief.","To make sense of the latter, we introduce the Minimal Assent Connection (MAC) and propose a new account of credence, which captures the strength of belief in language models.","This proposal uniformly assigns strength of belief simply on the basis of model internal next token probabilities.","We argue that rational norms tied to coherence do apply to some language models, but not to others.","This issue is significant since rationality is closely tied to predicting and explaining behavior, and thus it is connected to considerations about AI safety and alignment, as well as understanding model behavior more generally."],"url":"http://arxiv.org/abs/2406.03442v1","category":"cs.CL"}
{"created":"2024-06-05 16:35:30","title":"Cycles of Thought: Measuring LLM Confidence through Stable Explanations","abstract":"In many high-risk machine learning applications it is essential for a model to indicate when it is uncertain about a prediction. While large language models (LLMs) can reach and even surpass human-level accuracy on a variety of benchmarks, their overconfidence in incorrect responses is still a well-documented failure mode. Traditional methods for ML uncertainty quantification can be difficult to directly adapt to LLMs due to the computational cost of implementation and closed-source nature of many models. A variety of black-box methods have recently been proposed, but these often rely on heuristics such as self-verbalized confidence. We instead propose a framework for measuring an LLM's uncertainty with respect to the distribution of generated explanations for an answer. While utilizing explanations is not a new idea in and of itself, by interpreting each possible model+explanation pair as a test-time classifier we can calculate a posterior answer distribution over the most likely of these classifiers. We demonstrate how a specific instance of this framework using explanation entailment as our classifier likelihood improves confidence score metrics (in particular AURC and AUROC) over baselines across five different datasets. We believe these results indicate that our framework is both a well-principled and effective way of quantifying uncertainty in LLMs.","sentences":["In many high-risk machine learning applications it is essential for a model to indicate when it is uncertain about a prediction.","While large language models (LLMs) can reach and even surpass human-level accuracy on a variety of benchmarks, their overconfidence in incorrect responses is still a well-documented failure mode.","Traditional methods for ML uncertainty quantification can be difficult to directly adapt to LLMs due to the computational cost of implementation and closed-source nature of many models.","A variety of black-box methods have recently been proposed, but these often rely on heuristics such as self-verbalized confidence.","We instead propose a framework for measuring an LLM's uncertainty with respect to the distribution of generated explanations for an answer.","While utilizing explanations is not a new idea in and of itself, by interpreting each possible model+explanation pair as a test-time classifier we can calculate a posterior answer distribution over the most likely of these classifiers.","We demonstrate how a specific instance of this framework using explanation entailment as our classifier likelihood improves confidence score metrics (in particular AURC and AUROC) over baselines across five different datasets.","We believe these results indicate that our framework is both a well-principled and effective way of quantifying uncertainty in LLMs."],"url":"http://arxiv.org/abs/2406.03441v1","category":"cs.CL"}
{"created":"2024-06-05 16:34:12","title":"Text-to-Events: Synthetic Event Camera Streams from Conditional Text Input","abstract":"Event cameras are advantageous for tasks that require vision sensors with low-latency and sparse output responses. However, the development of deep network algorithms using event cameras has been slow because of the lack of large labelled event camera datasets for network training. This paper reports a method for creating new labelled event datasets by using a text-to-X model, where X is one or multiple output modalities, in the case of this work, events. Our proposed text-to-events model produces synthetic event frames directly from text prompts. It uses an autoencoder which is trained to produce sparse event frames representing event camera outputs. By combining the pretrained autoencoder with a diffusion model architecture, the new text-to-events model is able to generate smooth synthetic event streams of moving objects. The autoencoder was first trained on an event camera dataset of diverse scenes. In the combined training with the diffusion model, the DVS gesture dataset was used. We demonstrate that the model can generate realistic event sequences of human gestures prompted by different text statements. The classification accuracy of the generated sequences, using a classifier trained on the real dataset, ranges between 42% to 92%, depending on the gesture group. The results demonstrate the capability of this method in synthesizing event datasets.","sentences":["Event cameras are advantageous for tasks that require vision sensors with low-latency and sparse output responses.","However, the development of deep network algorithms using event cameras has been slow because of the lack of large labelled event camera datasets for network training.","This paper reports a method for creating new labelled event datasets by using a text-to-X model, where X is one or multiple output modalities, in the case of this work, events.","Our proposed text-to-events model produces synthetic event frames directly from text prompts.","It uses an autoencoder which is trained to produce sparse event frames representing event camera outputs.","By combining the pretrained autoencoder with a diffusion model architecture, the new text-to-events model is able to generate smooth synthetic event streams of moving objects.","The autoencoder was first trained on an event camera dataset of diverse scenes.","In the combined training with the diffusion model, the DVS gesture dataset was used.","We demonstrate that the model can generate realistic event sequences of human gestures prompted by different text statements.","The classification accuracy of the generated sequences, using a classifier trained on the real dataset, ranges between 42% to 92%, depending on the gesture group.","The results demonstrate the capability of this method in synthesizing event datasets."],"url":"http://arxiv.org/abs/2406.03439v1","category":"cs.CV"}
{"created":"2024-06-05 16:33:35","title":"CSI-GPT: Integrating Generative Pre-Trained Transformer with Federated-Tuning to Acquire Downlink Massive MIMO Channels","abstract":"In massive multiple-input multiple-output (MIMO) systems, how to reliably acquire downlink channel state information (CSI) with low overhead is challenging. In this work, by integrating the generative pre-trained Transformer (GPT) with federated-tuning, we propose a CSI-GPT approach to realize efficient downlink CSI acquisition. Specifically, we first propose a Swin Transformer-based channel acquisition network (SWTCAN) to acquire downlink CSI, where pilot signals, downlink channel estimation, and uplink CSI feedback are jointly designed. Furthermore, to solve the problem of insufficient training data, we propose a variational auto-encoder-based channel sample generator (VAE-CSG), which can generate sufficient CSI samples based on a limited number of high-quality CSI data obtained from the current cell. The CSI dataset generated from VAE-CSG will be used for pre-training SWTCAN. To fine-tune the pre-trained SWTCAN for improved performance, we propose an online federated-tuning method, where only a small amount of SWTCAN parameters are unfrozen and updated using over-the-air computation, avoiding the high communication overhead caused by aggregating the complete CSI samples from user equipment (UEs) to the BS for centralized fine-tuning. Simulation results verify the advantages of the proposed SWTCAN and the communication efficiency of the proposed federated-tuning method.","sentences":["In massive multiple-input multiple-output (MIMO) systems, how to reliably acquire downlink channel state information (CSI) with low overhead is challenging.","In this work, by integrating the generative pre-trained Transformer (GPT) with federated-tuning, we propose a CSI-GPT approach to realize efficient downlink CSI acquisition.","Specifically, we first propose a Swin Transformer-based channel acquisition network (SWTCAN) to acquire downlink CSI, where pilot signals, downlink channel estimation, and uplink CSI feedback are jointly designed.","Furthermore, to solve the problem of insufficient training data, we propose a variational auto-encoder-based channel sample generator (VAE-CSG), which can generate sufficient CSI samples based on a limited number of high-quality CSI data obtained from the current cell.","The CSI dataset generated from VAE-CSG will be used for pre-training SWTCAN.","To fine-tune the pre-trained SWTCAN for improved performance, we propose an online federated-tuning method, where only a small amount of SWTCAN parameters are unfrozen and updated using over-the-air computation, avoiding the high communication overhead caused by aggregating the complete CSI samples from user equipment (UEs) to the BS for centralized fine-tuning.","Simulation results verify the advantages of the proposed SWTCAN and the communication efficiency of the proposed federated-tuning method."],"url":"http://arxiv.org/abs/2406.03438v1","category":"cs.IT"}
{"created":"2024-06-05 16:32:20","title":"Stabilizing massless fields with fluxes in Landau-Ginzburg models","abstract":"Recent work on flux compactifications suggests that the tadpole constraint generically allows only a limited number of complex structure moduli to become massive, i.e., be stabilized at quadratic order in the spacetime superpotential. We study the effects of higher-order terms systematically around the Fermat point in the $1^9$ Landau-Ginzburg model. This model lives at strong coupling and features no K\\\"ahler moduli. We show that, depending on the flux, several massless fields can indeed be stabilized in this fashion, and argue that this paves the way to explicit ${\\mathcal N}=1$ Minkowski vacua without flat directions. Along the way, we complete the classification of integral flux vectors with small tadpole contribution. Thereby we are closing in on a future complete understanding of all possible flux configurations in the $1^9$ Landau-Ginzburg model.","sentences":["Recent work on flux compactifications suggests that the tadpole constraint generically allows only a limited number of complex structure moduli to become massive, i.e., be stabilized at quadratic order in the spacetime superpotential.","We study the effects of higher-order terms systematically around the Fermat point in the $1^9$ Landau-Ginzburg model.","This model lives at strong coupling and features no K\\\"ahler moduli.","We show that, depending on the flux, several massless fields can indeed be stabilized in this fashion, and argue that this paves the way to explicit ${\\mathcal N}=1$ Minkowski vacua without flat directions.","Along the way, we complete the classification of integral flux vectors with small tadpole contribution.","Thereby we are closing in on a future complete understanding of all possible flux configurations in the $1^9$ Landau-Ginzburg model."],"url":"http://arxiv.org/abs/2406.03435v1","category":"hep-th"}
{"created":"2024-06-05 16:32:14","title":"Unified PAC-Bayesian Study of Pessimism for Offline Policy Learning with Regularized Importance Sampling","abstract":"Off-policy learning (OPL) often involves minimizing a risk estimator based on importance weighting to correct bias from the logging policy used to collect data. However, this method can produce an estimator with a high variance. A common solution is to regularize the importance weights and learn the policy by minimizing an estimator with penalties derived from generalization bounds specific to the estimator. This approach, known as pessimism, has gained recent attention but lacks a unified framework for analysis. To address this gap, we introduce a comprehensive PAC-Bayesian framework to examine pessimism with regularized importance weighting. We derive a tractable PAC-Bayesian generalization bound that universally applies to common importance weight regularizations, enabling their comparison within a single framework. Our empirical results challenge common understanding, demonstrating the effectiveness of standard IW regularization techniques.","sentences":["Off-policy learning (OPL) often involves minimizing a risk estimator based on importance weighting to correct bias from the logging policy used to collect data.","However, this method can produce an estimator with a high variance.","A common solution is to regularize the importance weights and learn the policy by minimizing an estimator with penalties derived from generalization bounds specific to the estimator.","This approach, known as pessimism, has gained recent attention but lacks a unified framework for analysis.","To address this gap, we introduce a comprehensive PAC-Bayesian framework to examine pessimism with regularized importance weighting.","We derive a tractable PAC-Bayesian generalization bound that universally applies to common importance weight regularizations, enabling their comparison within a single framework.","Our empirical results challenge common understanding, demonstrating the effectiveness of standard IW regularization techniques."],"url":"http://arxiv.org/abs/2406.03434v1","category":"cs.LG"}
{"created":"2024-06-05 16:30:30","title":"Cell divisions imprint long lasting elastic strain fields in epithelial tissues","abstract":"A hallmark of biological tissues, viewed as complex cellular materials, is the active generation of mechanical stresses by cellular processes, such as cell divisions. Each cellular event generates a force dipole that deforms the surrounding tissue. Therefore, a quantitative description of these force dipoles, and their consequences on tissue mechanics, is one of the central problems in understanding the overall tissue mechanics. In this work we analyze previously published experimental data on fruit fly \\textit{D. melanogaster} wing epithelia to quantitatively describe the deformation fields induced by a cell-scale force dipole. We find that the measured deformation field can be explained by a simple model of fly epithelium as a linearly elastic sheet. This fact allows us to use measurements of the strain field around cellular events, such as cell divisions, to infer the magnitude and dynamics of the mechanical forces they generate. In particular, we find that cell divisions exert a transient isotropic force dipole field, corresponding to the temporary localisation of the cell nucleus to the tissue surface during the division, and traceless-symmetric force dipole field that remains detectable from the tissue strain field for up to about $3.5$ hours after the division. This is the timescale on which elastic strains are erased by other mechanical processes and therefore it corresponds to the tissue fluidization timescale. In summary, we have developed a method to infer force dipoles induced by cell divisions, by observing the strain field in the surrounding tissues. Using this method we quantitatively characterize mechanical forces generated during a cell division, and their effects on the tissue mechanics.","sentences":["A hallmark of biological tissues, viewed as complex cellular materials, is the active generation of mechanical stresses by cellular processes, such as cell divisions.","Each cellular event generates a force dipole that deforms the surrounding tissue.","Therefore, a quantitative description of these force dipoles, and their consequences on tissue mechanics, is one of the central problems in understanding the overall tissue mechanics.","In this work we analyze previously published experimental data on fruit fly \\textit{D. melanogaster} wing epithelia to quantitatively describe the deformation fields induced by a cell-scale force dipole.","We find that the measured deformation field can be explained by a simple model of fly epithelium as a linearly elastic sheet.","This fact allows us to use measurements of the strain field around cellular events, such as cell divisions, to infer the magnitude and dynamics of the mechanical forces they generate.","In particular, we find that cell divisions exert a transient isotropic force dipole field, corresponding to the temporary localisation of the cell nucleus to the tissue surface during the division, and traceless-symmetric force dipole field that remains detectable from the tissue strain field for up to about $3.5$ hours after the division.","This is the timescale on which elastic strains are erased by other mechanical processes and therefore it corresponds to the tissue fluidization timescale.","In summary, we have developed a method to infer force dipoles induced by cell divisions, by observing the strain field in the surrounding tissues.","Using this method we quantitatively characterize mechanical forces generated during a cell division, and their effects on the tissue mechanics."],"url":"http://arxiv.org/abs/2406.03433v1","category":"physics.bio-ph"}
{"created":"2024-06-05 16:26:55","title":"Quantitative metastability of the Tikhonov-Mann iteration for countable families of mappings","abstract":"In this paper, we obtain rates of metastability for the Tikhonov-Mann iteration for countable families of mappings in CAT(0) spaces. This iteration was recently defined by the author in the setting of W-hyperbolic spaces as a generalization of the strongly convergent version of the Krasnoselskii-Mann iteration introduced by Bot and Meier for finding common fixed points of families of nonexpansive mappings in Hilbert spaces, and as an extension of the Tikhonov-Mann iteration for single mappings, for which Leustean and the author obtained rates of asymptotic regularity in W-hyperbolic spaces.","sentences":["In this paper, we obtain rates of metastability for the Tikhonov-Mann iteration for countable families of mappings in CAT(0) spaces.","This iteration was recently defined by the author in the setting of W-hyperbolic spaces as a generalization of the strongly convergent version of the Krasnoselskii-Mann iteration introduced by Bot and Meier for finding common fixed points of families of nonexpansive mappings in Hilbert spaces, and as an extension of the Tikhonov-Mann iteration for single mappings, for which Leustean and the author obtained rates of asymptotic regularity in W-hyperbolic spaces."],"url":"http://arxiv.org/abs/2406.03429v1","category":"math.OC"}
{"created":"2024-06-05 16:25:57","title":"HelloFresh: LLM Evaluations on Streams of Real-World Human Editorial Actions across X Community Notes and Wikipedia edits","abstract":"Benchmarks have been essential for driving progress in machine learning. A better understanding of LLM capabilities on real world tasks is vital for safe development. Designing adequate LLM benchmarks is challenging: Data from real-world tasks is hard to collect, public availability of static evaluation data results in test data contamination and benchmark overfitting, and periodically generating new evaluation data is tedious and may result in temporally inconsistent results. We introduce HelloFresh, based on continuous streams of real-world data generated by intrinsically motivated human labelers. It covers recent events from X (formerly Twitter) community notes and edits of Wikipedia pages, mitigating the risk of test data contamination and benchmark overfitting. Any X user can propose an X note to add additional context to a misleading post (formerly tweet); if the community classifies it as helpful, it is shown with the post. Similarly, Wikipedia relies on community-based consensus, allowing users to edit articles or revert edits made by other users. Verifying whether an X note is helpful or whether a Wikipedia edit should be accepted are hard tasks that require grounding by querying the web. We backtest state-of-the-art LLMs supplemented with simple web search access and find that HelloFresh yields a temporally consistent ranking. To enable continuous evaluation on HelloFresh, we host a public leaderboard and periodically updated evaluation data at https://tinyurl.com/hello-fresh-LLM.","sentences":["Benchmarks have been essential for driving progress in machine learning.","A better understanding of LLM capabilities on real world tasks is vital for safe development.","Designing adequate LLM benchmarks is challenging: Data from real-world tasks is hard to collect, public availability of static evaluation data results in test data contamination and benchmark overfitting, and periodically generating new evaluation data is tedious and may result in temporally inconsistent results.","We introduce HelloFresh, based on continuous streams of real-world data generated by intrinsically motivated human labelers.","It covers recent events from X (formerly Twitter) community notes and edits of Wikipedia pages, mitigating the risk of test data contamination and benchmark overfitting.","Any X user can propose an X note to add additional context to a misleading post (formerly tweet); if the community classifies it as helpful, it is shown with the post.","Similarly, Wikipedia relies on community-based consensus, allowing users to edit articles or revert edits made by other users.","Verifying whether an X note is helpful or whether a Wikipedia edit should be accepted are hard tasks that require grounding by querying the web.","We backtest state-of-the-art LLMs supplemented with simple web search access and find that HelloFresh yields a temporally consistent ranking.","To enable continuous evaluation on HelloFresh, we host a public leaderboard and periodically updated evaluation data at https://tinyurl.com/hello-fresh-LLM."],"url":"http://arxiv.org/abs/2406.03428v1","category":"cs.LG"}
{"created":"2024-06-05 16:25:45","title":"The strong data processing inequality under the heat flow","abstract":"Let $\\nu$ and $\\mu$ be probability distributions on $\\mathbb{R}^n$, and $\\nu_s,\\mu_s$ be their evolution under the heat flow, that is, the probability distributions resulting from convolving their density with the density of an isotropic Gaussian random vector with variance $s$ in each entry. This paper studies the rate of decay of $s\\mapsto D(\\nu_s\\|\\mu_s)$ for various divergences, including the $\\chi^2$ and Kullback-Leibler (KL) divergences. We prove upper and lower bounds on the strong data-processing inequality (SDPI) coefficients corresponding to the source $\\mu$ and the Gaussian channel. We also prove generalizations of de Brujin's identity, and Costa's result on the concavity in $s$ of the differential entropy of $\\nu_s$. As a byproduct of our analysis, we obtain new lower bounds on the mutual information between $X$ and $Y=X+\\sqrt{s} Z$, where $Z$ is a standard Gaussian vector in $\\mathbb{R}^n$, independent of $X$, and on the minimum mean-square error (MMSE) in estimating $X$ from $Y$, in terms of the Poincar\\'e constant of $X$.","sentences":["Let $\\nu$ and $\\mu$ be probability distributions on $\\mathbb{R}^n$, and $\\nu_s,\\mu_s$ be their evolution under the heat flow, that is, the probability distributions resulting from convolving their density with the density of an isotropic Gaussian random vector with variance $s$ in each entry.","This paper studies the rate of decay of $s\\mapsto D(\\nu_s\\|\\mu_s)$ for various divergences, including the $\\chi^2$ and Kullback-Leibler (KL) divergences.","We prove upper and lower bounds on the strong data-processing inequality (SDPI) coefficients corresponding to the source $\\mu$ and the Gaussian channel.","We also prove generalizations of de Brujin's identity, and Costa's result on the concavity in $s$ of the differential entropy of $\\nu_s$. As a byproduct of our analysis, we obtain new lower bounds on the mutual information between $X$ and $Y=X+\\sqrt{s} Z$, where $Z$ is a standard Gaussian vector in $\\mathbb{R}^n$, independent of $X$, and on the minimum mean-square error (MMSE) in estimating $X$ from $Y$, in terms of the Poincar\\'e constant of $X$."],"url":"http://arxiv.org/abs/2406.03427v1","category":"cs.IT"}
{"created":"2024-06-05 16:25:21","title":"Vertex Representation of Hyperbolic Tensor Networks","abstract":"We propose a vertex representation of the tensor network (TN) in the anti-de Sitter space (AdS$_{2+0}$) that we model on a subset of hyperbolic lattices. The tensors form a network of regular $p$-sided polygons ($p>4$) with the coordination number four. The response to multi-state spin systems on the hyperbolic TN is analyzed for their entire parameter space. We show that entanglement entropy is sensitive to distinguish various hyperbolic geometries whereas other thermodynamic quantities are not. We test the numerical accuracy of vertex TNs in the phase transitions of the first, second, and infinite order at the point of maximal entanglement entropy. The hyperbolic structure of TNs induces non-critical properties in the bulk although boundary conditions significantly affect the total free energy in the thermodynamic limit. Thus developed vertex-type TN can be applied to study low-energy quantum states on AdS.","sentences":["We propose a vertex representation of the tensor network (TN) in the anti-de Sitter space (AdS$_{2+0}$) that we model on a subset of hyperbolic lattices.","The tensors form a network of regular $p$-sided polygons ($p>4$) with the coordination number four.","The response to multi-state spin systems on the hyperbolic TN is analyzed for their entire parameter space.","We show that entanglement entropy is sensitive to distinguish various hyperbolic geometries whereas other thermodynamic quantities are not.","We test the numerical accuracy of vertex TNs in the phase transitions of the first, second, and infinite order at the point of maximal entanglement entropy.","The hyperbolic structure of TNs induces non-critical properties in the bulk although boundary conditions significantly affect the total free energy in the thermodynamic limit.","Thus developed vertex-type TN can be applied to study low-energy quantum states on AdS."],"url":"http://arxiv.org/abs/2406.03426v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-05 16:19:24","title":"Improving Users' Passwords with DPAR: a Data-driven Password Recommendation System","abstract":"Passwords are the primary authentication method online, but even with password policies and meters, users still find it hard to create strong and memorable passwords. In this paper, we propose DPAR: a Data-driven PAssword Recommendation system based on a dataset of 905 million leaked passwords. DPAR generates password recommendations by analyzing the user's given password and suggesting specific tweaks that would make it stronger while still keeping it memorable and similar to the original password. We conducted two studies to evaluate our approach: verifying the memorability of generated passwords (n=317), and evaluating the strength and recall of DPAR recommendations against password meters (n=441). In a randomized experiment, we show that DPAR increased password strength by 34.8 bits on average and did not significantly affect the ability to recall their password. Furthermore, 36.6% of users accepted DPAR's recommendations verbatim. We discuss our findings and their implications for enhancing password management with recommendation systems.","sentences":["Passwords are the primary authentication method online, but even with password policies and meters, users still find it hard to create strong and memorable passwords.","In this paper, we propose DPAR: a Data-driven PAssword Recommendation system based on a dataset of 905 million leaked passwords.","DPAR generates password recommendations by analyzing the user's given password and suggesting specific tweaks that would make it stronger while still keeping it memorable and similar to the original password.","We conducted two studies to evaluate our approach: verifying the memorability of generated passwords (n=317), and evaluating the strength and recall of DPAR recommendations against password meters (n=441).","In a randomized experiment, we show that DPAR increased password strength by 34.8 bits on average and did not significantly affect the ability to recall their password.","Furthermore, 36.6% of users accepted DPAR's recommendations verbatim.","We discuss our findings and their implications for enhancing password management with recommendation systems."],"url":"http://arxiv.org/abs/2406.03423v1","category":"cs.CR"}
{"created":"2024-06-05 16:16:45","title":"Causal Inference from Competing Treatments","abstract":"Many applications of RCTs involve the presence of multiple treatment administrators -- from field experiments to online advertising -- that compete for the subjects' attention. In the face of competition, estimating a causal effect becomes difficult, as the position at which a subject sees a treatment influences their response, and thus the treatment effect. In this paper, we build a game-theoretic model of agents who wish to estimate causal effects in the presence of competition, through a bidding system and a utility function that minimizes estimation error. Our main technical result establishes an approximation with a tractable objective that maximizes the sample value obtained through strategically allocating budget on subjects. This allows us to find an equilibrium in our model: we show that the tractable objective has a pure Nash equilibrium, and that any Nash equilibrium is an approximate equilibrium for our general objective that minimizes estimation error under broad conditions. Conceptually, our work successfully combines elements from causal inference and game theory to shed light on the equilibrium behavior of experimentation under competition.","sentences":["Many applications of RCTs involve the presence of multiple treatment administrators -- from field experiments to online advertising -- that compete for the subjects' attention.","In the face of competition, estimating a causal effect becomes difficult, as the position at which a subject sees a treatment influences their response, and thus the treatment effect.","In this paper, we build a game-theoretic model of agents who wish to estimate causal effects in the presence of competition, through a bidding system and a utility function that minimizes estimation error.","Our main technical result establishes an approximation with a tractable objective that maximizes the sample value obtained through strategically allocating budget on subjects.","This allows us to find an equilibrium in our model: we show that the tractable objective has a pure Nash equilibrium, and that any Nash equilibrium is an approximate equilibrium for our general objective that minimizes estimation error under broad conditions.","Conceptually, our work successfully combines elements from causal inference and game theory to shed light on the equilibrium behavior of experimentation under competition."],"url":"http://arxiv.org/abs/2406.03422v1","category":"cs.GT"}
{"created":"2024-06-05 16:12:24","title":"Superconductivity in twisted bilayer WSe$_2$","abstract":"The discovery of superconductivity in twisted bilayer and twisted trilayer graphene has generated tremendous interest. The key feature of these systems is an interplay between interlayer coupling and a moir\\'e superlattice that gives rise to low-energy flat bands with strong correlations. Flat bands can also be induced by moir\\'e patterns in lattice-mismatched and or twisted heterostructures of other two-dimensional materials such as transition metal dichalcogenides (TMDs). Although a wide range of correlated phenomenon have indeed been observed in the moir\\'e TMDs, robust demonstration of superconductivity has remained absent. Here we report superconductivity in 5 degree twisted bilayer WSe$_2$ (tWSe$_2$) with a maximum critical temperature of 426 mK. The superconducting state appears in a limited region of displacement field and density that is adjacent to a metallic state with Fermi surface reconstruction believed to arise from antiferromagnetic order. A sharp boundary is observed between the superconducting and magnetic phases at low temperature, reminiscent of spin-fluctuation mediated superconductivity. Our results establish that moir\\'e flat-band superconductivity extends beyond graphene structures. Material properties that are absent in graphene but intrinsic among the TMDs such as a native band gap, large spin-orbit coupling, spin-valley locking, and magnetism offer the possibility to access a broader superconducting parameter space than graphene-only structures.","sentences":["The discovery of superconductivity in twisted bilayer and twisted trilayer graphene has generated tremendous interest.","The key feature of these systems is an interplay between interlayer coupling and a moir\\'e superlattice that gives rise to low-energy flat bands with strong correlations.","Flat bands can also be induced by moir\\'e patterns in lattice-mismatched and or twisted heterostructures of other two-dimensional materials such as transition metal dichalcogenides (TMDs).","Although a wide range of correlated phenomenon have indeed been observed in the moir\\'e TMDs, robust demonstration of superconductivity has remained absent.","Here we report superconductivity in 5 degree twisted bilayer WSe$_2$ (tWSe$_2$) with a maximum critical temperature of 426 mK. The superconducting state appears in a limited region of displacement field and density that is adjacent to a metallic state with Fermi surface reconstruction believed to arise from antiferromagnetic order.","A sharp boundary is observed between the superconducting and magnetic phases at low temperature, reminiscent of spin-fluctuation mediated superconductivity.","Our results establish that moir\\'e flat-band superconductivity extends beyond graphene structures.","Material properties that are absent in graphene but intrinsic among the TMDs such as a native band gap, large spin-orbit coupling, spin-valley locking, and magnetism offer the possibility to access a broader superconducting parameter space than graphene-only structures."],"url":"http://arxiv.org/abs/2406.03418v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-05 16:11:15","title":"RemixTape: Enriching Narratives about Metrics with Semantic Alignment and Contextual Recommendation","abstract":"The temporal dynamics of quantitative metrics or key performance indicators (KPIs) are central to decision making within enterprise organizations. Recently, major business intelligence providers have introduced new infrastructure for defining, sharing, and monitoring metric values. However, these values are often presented in isolation and appropriate context is seldom externalized. In this design study, we present RemixTape, an application for constructing structured narratives around metrics. With design imperatives grounded in an formative interview study, RemixTape provides a hierarchical canvas for collecting and coordinating sequences of line chart representations of metrics, along with the ability to externalize situational context around them. RemixTape incorporates affordances to semantically align and annotate juxtaposed charts and text, as well as recommendations of complementary charts based on metrics already present on the canvas. We evaluated RemixTape in a user study in which six enterprise data professionals reproduced and extended partial narratives, with participants appreciating RemixTape as a novel alternative to dashboards, galleries, and slide presentations for supporting conversations about metrics. We conclude with a reflection on how aspects of RemixTape could generalize beyond metrics, with a call to define a conceptual foundation for remixing in the context of visualization.","sentences":["The temporal dynamics of quantitative metrics or key performance indicators (KPIs) are central to decision making within enterprise organizations.","Recently, major business intelligence providers have introduced new infrastructure for defining, sharing, and monitoring metric values.","However, these values are often presented in isolation and appropriate context is seldom externalized.","In this design study, we present RemixTape, an application for constructing structured narratives around metrics.","With design imperatives grounded in an formative interview study, RemixTape provides a hierarchical canvas for collecting and coordinating sequences of line chart representations of metrics, along with the ability to externalize situational context around them.","RemixTape incorporates affordances to semantically align and annotate juxtaposed charts and text, as well as recommendations of complementary charts based on metrics already present on the canvas.","We evaluated RemixTape in a user study in which six enterprise data professionals reproduced and extended partial narratives, with participants appreciating RemixTape as a novel alternative to dashboards, galleries, and slide presentations for supporting conversations about metrics.","We conclude with a reflection on how aspects of RemixTape could generalize beyond metrics, with a call to define a conceptual foundation for remixing in the context of visualization."],"url":"http://arxiv.org/abs/2406.03415v1","category":"cs.HC"}
{"created":"2024-06-05 16:09:54","title":"Nonlocal chiral contributions to generalized parton distributions of the proton at nonzero skewness","abstract":"We compute the one-loop contributions to spin-averaged generalized parton distributions (GPDs) in the proton from pseudoscalar mesons with intermediate octet and decuplet baryon states at nonzero skewness. Our framework is based on nonlocal covariant chiral effective theory, with ultraviolet divergences regularized by introducing a relativistic regulator derived consistently from the nonlocal Lagrangian. Using the splitting functions calculated from the nonlocal Lagrangian, we find the nonzero skewness GPDs from meson loops by convoluting with the phenomenological pion GPD and the generalized distribution amplitude, and verify that these satisfy the correct polynomiality properties. We also compute the lowest two moments of GPDs to quantify the meson loop effects on the Dirac, Pauli and gravitational form factors of the proton.","sentences":["We compute the one-loop contributions to spin-averaged generalized parton distributions (GPDs) in the proton from pseudoscalar mesons with intermediate octet and decuplet baryon states at nonzero skewness.","Our framework is based on nonlocal covariant chiral effective theory, with ultraviolet divergences regularized by introducing a relativistic regulator derived consistently from the nonlocal Lagrangian.","Using the splitting functions calculated from the nonlocal Lagrangian, we find the nonzero skewness GPDs from meson loops by convoluting with the phenomenological pion GPD and the generalized distribution amplitude, and verify that these satisfy the correct polynomiality properties.","We also compute the lowest two moments of GPDs to quantify the meson loop effects on the Dirac, Pauli and gravitational form factors of the proton."],"url":"http://arxiv.org/abs/2406.03412v1","category":"hep-ph"}
{"created":"2024-06-05 16:09:01","title":"Interactive Text-to-Image Retrieval with Large Language Models: A Plug-and-Play Approach","abstract":"In this paper, we primarily address the issue of dialogue-form context query within the interactive text-to-image retrieval task. Our methodology, PlugIR, actively utilizes the general instruction-following capability of LLMs in two ways. First, by reformulating the dialogue-form context, we eliminate the necessity of fine-tuning a retrieval model on existing visual dialogue data, thereby enabling the use of any arbitrary black-box model. Second, we construct the LLM questioner to generate non-redundant questions about the attributes of the target image, based on the information of retrieval candidate images in the current context. This approach mitigates the issues of noisiness and redundancy in the generated questions. Beyond our methodology, we propose a novel evaluation metric, Best log Rank Integral (BRI), for a comprehensive assessment of the interactive retrieval system. PlugIR demonstrates superior performance compared to both zero-shot and fine-tuned baselines in various benchmarks. Additionally, the two methodologies comprising PlugIR can be flexibly applied together or separately in various situations. Our codes are available at https://github.com/Saehyung-Lee/PlugIR.","sentences":["In this paper, we primarily address the issue of dialogue-form context query within the interactive text-to-image retrieval task.","Our methodology, PlugIR, actively utilizes the general instruction-following capability of LLMs in two ways.","First, by reformulating the dialogue-form context, we eliminate the necessity of fine-tuning a retrieval model on existing visual dialogue data, thereby enabling the use of any arbitrary black-box model.","Second, we construct the LLM questioner to generate non-redundant questions about the attributes of the target image, based on the information of retrieval candidate images in the current context.","This approach mitigates the issues of noisiness and redundancy in the generated questions.","Beyond our methodology, we propose a novel evaluation metric, Best log Rank Integral (BRI), for a comprehensive assessment of the interactive retrieval system.","PlugIR demonstrates superior performance compared to both zero-shot and fine-tuned baselines in various benchmarks.","Additionally, the two methodologies comprising PlugIR can be flexibly applied together or separately in various situations.","Our codes are available at https://github.com/Saehyung-Lee/PlugIR."],"url":"http://arxiv.org/abs/2406.03411v1","category":"cs.CV"}
{"created":"2024-06-05 15:55:08","title":"Methods for Class-Imbalanced Learning with Support Vector Machines: A Review and an Empirical Evaluation","abstract":"This paper presents a review on methods for class-imbalanced learning with the Support Vector Machine (SVM) and its variants. We first explain the structure of SVM and its variants and discuss their inefficiency in learning with class-imbalanced data sets. We introduce a hierarchical categorization of SVM-based models with respect to class-imbalanced learning. Specifically, we categorize SVM-based models into re-sampling, algorithmic, and fusion methods, and discuss the principles of the representative models in each category. In addition, we conduct a series of empirical evaluations to compare the performances of various representative SVM-based models in each category using benchmark imbalanced data sets, ranging from low to high imbalanced ratios. Our findings reveal that while algorithmic methods are less time-consuming owing to no data pre-processing requirements, fusion methods, which combine both re-sampling and algorithmic approaches, generally perform the best, but with a higher computational load. A discussion on research gaps and future research directions is provided.","sentences":["This paper presents a review on methods for class-imbalanced learning with the Support Vector Machine (SVM) and its variants.","We first explain the structure of SVM and its variants and discuss their inefficiency in learning with class-imbalanced data sets.","We introduce a hierarchical categorization of SVM-based models with respect to class-imbalanced learning.","Specifically, we categorize SVM-based models into re-sampling, algorithmic, and fusion methods, and discuss the principles of the representative models in each category.","In addition, we conduct a series of empirical evaluations to compare the performances of various representative SVM-based models in each category using benchmark imbalanced data sets, ranging from low to high imbalanced ratios.","Our findings reveal that while algorithmic methods are less time-consuming owing to no data pre-processing requirements, fusion methods, which combine both re-sampling and algorithmic approaches, generally perform the best, but with a higher computational load.","A discussion on research gaps and future research directions is provided."],"url":"http://arxiv.org/abs/2406.03398v1","category":"cs.LG"}
{"created":"2024-06-05 15:54:50","title":"Automating Turkish Educational Quiz Generation Using Large Language Models","abstract":"Crafting quizzes from educational content is a pivotal activity that benefits both teachers and students by reinforcing learning and evaluating understanding. In this study, we introduce a novel approach to generate quizzes from Turkish educational texts, marking a pioneering endeavor in educational technology specifically tailored to the Turkish educational context. We present a specialized dataset, named the Turkish-Quiz-Instruct, comprising an extensive collection of Turkish educational texts accompanied by multiple-choice and short-answer quizzes. This research leverages the capabilities of Large Language Models (LLMs), including GPT-4-Turbo, GPT-3.5-Turbo, Llama-2-7b-chat-hf, and Llama-2-13b-chat-hf, to automatically generate quiz questions and answers from the Turkish educational content. Our work delineates the methodology for employing these LLMs in the context of Turkish educational material, thereby opening new avenues for automated Turkish quiz generation. The study not only demonstrates the efficacy of using such models for generating coherent and relevant quiz content but also sets a precedent for future research in the domain of automated educational content creation for languages other than English. The Turkish-Quiz-Instruct dataset is introduced as a valuable resource for researchers and practitioners aiming to explore the boundaries of educational technology and language-specific applications of LLMs in Turkish. By addressing the challenges of quiz generation in a non-English context specifically Turkish, this study contributes significantly to the field of Turkish educational technology, providing insights into the potential of leveraging LLMs for educational purposes across diverse linguistic landscapes.","sentences":["Crafting quizzes from educational content is a pivotal activity that benefits both teachers and students by reinforcing learning and evaluating understanding.","In this study, we introduce a novel approach to generate quizzes from Turkish educational texts, marking a pioneering endeavor in educational technology specifically tailored to the Turkish educational context.","We present a specialized dataset, named the Turkish-Quiz-Instruct, comprising an extensive collection of Turkish educational texts accompanied by multiple-choice and short-answer quizzes.","This research leverages the capabilities of Large Language Models (LLMs), including GPT-4-Turbo, GPT-3.5-Turbo, Llama-2-7b-chat-hf, and Llama-2-13b-chat-hf, to automatically generate quiz questions and answers from the Turkish educational content.","Our work delineates the methodology for employing these LLMs in the context of Turkish educational material, thereby opening new avenues for automated Turkish quiz generation.","The study not only demonstrates the efficacy of using such models for generating coherent and relevant quiz content but also sets a precedent for future research in the domain of automated educational content creation for languages other than English.","The Turkish-Quiz-Instruct dataset is introduced as a valuable resource for researchers and practitioners aiming to explore the boundaries of educational technology and language-specific applications of LLMs in Turkish.","By addressing the challenges of quiz generation in a non-English context specifically Turkish, this study contributes significantly to the field of Turkish educational technology, providing insights into the potential of leveraging LLMs for educational purposes across diverse linguistic landscapes."],"url":"http://arxiv.org/abs/2406.03397v1","category":"cs.CL"}
{"created":"2024-06-05 15:46:54","title":"Can recent DESI BAO measurements accommodate a negative cosmological constant?","abstract":"Anti-de Sitter vacuum, which correspond to a negative cosmological constant (CC), is theoretically important and well-motivated. It is interesting to see whether current data can allow the existence of a negative CC not. In this paper, we perform the MCMC analysis for the $w_0w_a$CDM+CC model using recent DESI BAO measurements combined with Planck CMB and Pantheon Plus dataset. The results reveal that the bestfit energy density of CC is $\\Omega_\\Lambda\\sim -0.3$ and the fitting to DESI is slightly improved, while $\\Omega_\\Lambda=0$ is also $1\\sigma$ consistent.","sentences":["Anti-de Sitter vacuum, which correspond to a negative cosmological constant (CC), is theoretically important and well-motivated.","It is interesting to see whether current data can allow the existence of a negative CC not.","In this paper, we perform the MCMC analysis for the $w_0w_a$CDM+CC model using recent DESI BAO measurements combined with Planck CMB and Pantheon Plus dataset.","The results reveal that the bestfit energy density of CC is $\\Omega_\\Lambda\\sim -0.3$ and the fitting to DESI is slightly improved, while $\\Omega_\\Lambda=0$ is also $1\\sigma$ consistent."],"url":"http://arxiv.org/abs/2406.03395v1","category":"astro-ph.CO"}
{"created":"2024-06-05 15:43:15","title":"Censorship in Democracy","abstract":"The spread of propaganda, misinformation, and biased narratives from autocratic regimes, especially on social media, is a growing concern in many democracies. Can censorship be an effective tool to curb the spread of such slanted narratives? In this paper, we study the European Union's ban on Russian state-led news outlets after the 2022 Russian invasion of Ukraine. We analyze 775,616 tweets from 133,276 users on Twitter/X, employing a difference-in-differences strategy. We show that the ban reduced pro-Russian slant among users who had previously directly interacted with banned outlets. The impact is most pronounced among users with the highest pre-ban slant levels. However, this effect was short-lived, with slant returning to its pre-ban levels within two weeks post-enforcement. Additionally, we find a detectable albeit less pronounced indirect effect on users who had not directly interacted with the outlets before the ban. We provide evidence that other suppliers of propaganda may have actively sought to mitigate the ban's influence by intensifying their activity, effectively counteracting the persistence and reach of the ban.","sentences":["The spread of propaganda, misinformation, and biased narratives from autocratic regimes, especially on social media, is a growing concern in many democracies.","Can censorship be an effective tool to curb the spread of such slanted narratives?","In this paper, we study the European Union's ban on Russian state-led news outlets after the 2022 Russian invasion of Ukraine.","We analyze 775,616 tweets from 133,276 users on Twitter/X, employing a difference-in-differences strategy.","We show that the ban reduced pro-Russian slant among users who had previously directly interacted with banned outlets.","The impact is most pronounced among users with the highest pre-ban slant levels.","However, this effect was short-lived, with slant returning to its pre-ban levels within two weeks post-enforcement.","Additionally, we find a detectable albeit less pronounced indirect effect on users who had not directly interacted with the outlets before the ban.","We provide evidence that other suppliers of propaganda may have actively sought to mitigate the ban's influence by intensifying their activity, effectively counteracting the persistence and reach of the ban."],"url":"http://arxiv.org/abs/2406.03393v1","category":"econ.GN"}
{"created":"2024-06-05 15:42:38","title":"Joint Association, Beamforming, and Resource Allocation for Multi-IRS Enabled MU-MISO Systems With RSMA","abstract":"Intelligent reflecting surface (IRS) and rate-splitting multiple access (RSMA) technologies are at the forefront of enhancing spectrum and energy efficiency in the next generation multi-antenna communication systems. This paper explores a RSMA system with multiple IRSs, and proposes two purpose-driven scheduling schemes, i.e., the exhaustive IRS-aided (EIA) and opportunistic IRS-aided (OIA) schemes. The aim is to optimize the system weighted energy efficiency (EE) under the above two schemes, respectively. Specifically, the Dinkelbach, branch and bound, successive convex approximation, and the semidefinite relaxation methods are exploited within the alternating optimization framework to obtain effective solutions to the considered problems. The numerical findings indicate that the EIA scheme exhibits better performance compared to the OIA scheme in diverse scenarios when considering the weighted EE, and the proposed algorithm demonstrates superior performance in comparison to the baseline algorithms.","sentences":["Intelligent reflecting surface (IRS) and rate-splitting multiple access (RSMA) technologies are at the forefront of enhancing spectrum and energy efficiency in the next generation multi-antenna communication systems.","This paper explores a RSMA system with multiple IRSs, and proposes two purpose-driven scheduling schemes, i.e., the exhaustive IRS-aided (EIA) and opportunistic IRS-aided (OIA) schemes.","The aim is to optimize the system weighted energy efficiency (EE) under the above two schemes, respectively.","Specifically, the Dinkelbach, branch and bound, successive convex approximation, and the semidefinite relaxation methods are exploited within the alternating optimization framework to obtain effective solutions to the considered problems.","The numerical findings indicate that the EIA scheme exhibits better performance compared to the OIA scheme in diverse scenarios when considering the weighted EE, and the proposed algorithm demonstrates superior performance in comparison to the baseline algorithms."],"url":"http://arxiv.org/abs/2406.03391v1","category":"eess.SP"}
{"created":"2024-06-05 15:41:02","title":"Author, Content or Sharers? Estimating Spread Dynamics with Bayesian Mixture Hawkes","abstract":"The spread of content on social media is shaped by intertwining factors on three levels: the source, the content itself, and the pathways of content spread. At the lowest level, the popularity of the sharing user determines its eventual reach. However, higher-level factors such as the nature of the online item and the credibility of its source also play crucial roles in determining how widely and rapidly the online item spreads. In this work, we propose the Bayesian Mixture Hawkes (BMH) model to jointly learn the influence of source, content and spread. We formulate the BMH model as a hierarchical mixture model of separable Hawkes processes, accommodating different classes of Hawkes dynamics and the influence of feature sets on these classes. We test the BMH model on two learning tasks, cold-start popularity prediction and temporal profile generalization performance, applying to two real-world retweet cascade datasets referencing articles from controversial and traditional media publishers. The BMH model outperforms the state-of-the-art models and predictive baselines on both datasets and utilizes cascade- and item-level information better than the alternatives. Lastly, we perform a counter-factual analysis where we apply the trained publisher-level BMH models to a set of article headlines and show that effectiveness of headline writing style (neutral, clickbait, inflammatory) varies across publishers. The BMH model unveils differences in style effectiveness between controversial and reputable publishers, where we find clickbait to be notably more effective for reputable publishers as opposed to controversial ones, which links to the latter's overuse of clickbait.","sentences":["The spread of content on social media is shaped by intertwining factors on three levels: the source, the content itself, and the pathways of content spread.","At the lowest level, the popularity of the sharing user determines its eventual reach.","However, higher-level factors such as the nature of the online item and the credibility of its source also play crucial roles in determining how widely and rapidly the online item spreads.","In this work, we propose the Bayesian Mixture Hawkes (BMH) model to jointly learn the influence of source, content and spread.","We formulate the BMH model as a hierarchical mixture model of separable Hawkes processes, accommodating different classes of Hawkes dynamics and the influence of feature sets on these classes.","We test the BMH model on two learning tasks, cold-start popularity prediction and temporal profile generalization performance, applying to two real-world retweet cascade datasets referencing articles from controversial and traditional media publishers.","The BMH model outperforms the state-of-the-art models and predictive baselines on both datasets and utilizes cascade- and item-level information better than the alternatives.","Lastly, we perform a counter-factual analysis where we apply the trained publisher-level BMH models to a set of article headlines and show that effectiveness of headline writing style (neutral, clickbait, inflammatory) varies across publishers.","The BMH model unveils differences in style effectiveness between controversial and reputable publishers, where we find clickbait to be notably more effective for reputable publishers as opposed to controversial ones, which links to the latter's overuse of clickbait."],"url":"http://arxiv.org/abs/2406.03390v1","category":"cs.LG"}
{"created":"2024-06-05 15:39:51","title":"Hot Schr\u00f6dinger Cat States","abstract":"The observation of quantum phenomena often necessitates sufficiently pure states, a requirement that can be challenging to achieve. In this study, our goal is to prepare a non-classical state originating from a mixed state, utilizing dynamics that preserve the initial low purity of the state. We generate a quantum superposition of displaced thermal states within a microwave cavity using only unitary interactions with a transmon qubit. We measure the Wigner functions of these ``hot'' Schr\\\"odinger cat states for an initial purity as low as 0.06. This corresponds to a cavity mode temperature of up to 1.8 Kelvin, sixty times hotter than the cavity's physical environment. Our realization of highly mixed quantum superposition states could be implemented with other continuous-variable systems e.g. nanomechanical oscillators, for which ground-state cooling remains challenging.","sentences":["The observation of quantum phenomena often necessitates sufficiently pure states, a requirement that can be challenging to achieve.","In this study, our goal is to prepare a non-classical state originating from a mixed state, utilizing dynamics that preserve the initial low purity of the state.","We generate a quantum superposition of displaced thermal states within a microwave cavity using only unitary interactions with a transmon qubit.","We measure the Wigner functions of these ``hot'' Schr\\\"odinger cat states for an initial purity as low as 0.06.","This corresponds to a cavity mode temperature of up to 1.8 Kelvin, sixty times hotter than the cavity's physical environment.","Our realization of highly mixed quantum superposition states could be implemented with other continuous-variable systems e.g. nanomechanical oscillators, for which ground-state cooling remains challenging."],"url":"http://arxiv.org/abs/2406.03389v1","category":"quant-ph"}
{"created":"2024-06-05 15:38:02","title":"SelfReDepth: Self-Supervised Real-Time Depth Restoration for Consumer-Grade Sensors","abstract":"Depth maps produced by consumer-grade sensors suffer from inaccurate measurements and missing data from either system or scene-specific sources. Data-driven denoising algorithms can mitigate such problems. However, they require vast amounts of ground truth depth data. Recent research has tackled this limitation using self-supervised learning techniques, but it requires multiple RGB-D sensors. Moreover, most existing approaches focus on denoising single isolated depth maps or specific subjects of interest, highlighting a need for methods to effectively denoise depth maps in real-time dynamic environments. This paper extends state-of-the-art approaches for depth-denoising commodity depth devices, proposing SelfReDepth, a self-supervised deep learning technique for depth restoration, via denoising and hole-filling by inpainting full-depth maps captured with RGB-D sensors. The algorithm targets depth data in video streams, utilizing multiple sequential depth frames coupled with color data to achieve high-quality depth videos with temporal coherence. Finally, SelfReDepth is designed to be compatible with various RGB-D sensors and usable in real-time scenarios as a pre-processing step before applying other depth-dependent algorithms. Our results demonstrate our approach's real-time performance on real-world datasets. They show that it outperforms state-of-the-art denoising and restoration performance at over 30fps on Commercial Depth Cameras, with potential benefits for augmented and mixed-reality applications.","sentences":["Depth maps produced by consumer-grade sensors suffer from inaccurate measurements and missing data from either system or scene-specific sources.","Data-driven denoising algorithms can mitigate such problems.","However, they require vast amounts of ground truth depth data.","Recent research has tackled this limitation using self-supervised learning techniques, but it requires multiple RGB-D sensors.","Moreover, most existing approaches focus on denoising single isolated depth maps or specific subjects of interest, highlighting a need for methods to effectively denoise depth maps in real-time dynamic environments.","This paper extends state-of-the-art approaches for depth-denoising commodity depth devices, proposing SelfReDepth, a self-supervised deep learning technique for depth restoration, via denoising and hole-filling by inpainting full-depth maps captured with RGB-D sensors.","The algorithm targets depth data in video streams, utilizing multiple sequential depth frames coupled with color data to achieve high-quality depth videos with temporal coherence.","Finally, SelfReDepth is designed to be compatible with various RGB-D sensors and usable in real-time scenarios as a pre-processing step before applying other depth-dependent algorithms.","Our results demonstrate our approach's real-time performance on real-world datasets.","They show that it outperforms state-of-the-art denoising and restoration performance at over 30fps on Commercial Depth Cameras, with potential benefits for augmented and mixed-reality applications."],"url":"http://arxiv.org/abs/2406.03388v1","category":"cs.CV"}
{"created":"2024-06-05 15:33:47","title":"Self-improving boundedness of the maximal operator on quasi-Banach lattices over spaces of homogeneous type","abstract":"We prove the self-improvement property of the Hardy--Littlewood maximal operator on quasi-Banach lattices with the Fatou property in the setting of spaces of homogeneous type. Our result is a generalization of the boundedness criterion obtained in 2010 by Lerner and Ombrosi for maximal operators on quasi-Banach function spaces over Euclidean spaces. The specialty of the proof for spaces of homogeneous type lies in using adjacent grids of Hyt\\\"onen--Kairema dyadic cubes and studying the maximal operator alongside its dyadic version. Then we apply the obtained result to variable Lebesgue spaces over spaces of homogeneous type.","sentences":["We prove the self-improvement property of the Hardy--Littlewood maximal operator on quasi-Banach lattices with the Fatou property in the setting of spaces of homogeneous type.","Our result is a generalization of the boundedness criterion obtained in 2010 by Lerner and Ombrosi for maximal operators on quasi-Banach function spaces over Euclidean spaces.","The specialty of the proof for spaces of homogeneous type lies in using adjacent grids of Hyt\\\"onen--Kairema dyadic cubes and studying the maximal operator alongside its dyadic version.","Then we apply the obtained result to variable Lebesgue spaces over spaces of homogeneous type."],"url":"http://arxiv.org/abs/2406.03382v1","category":"math.CA"}
{"created":"2024-06-05 15:32:15","title":"How to Construct Quantum FHE, Generically","abstract":"We construct a (compact) quantum fully homomorphic encryption (QFHE) scheme starting from (compact) classical fully homomorphic encryption scheme with decryption in $\\mathsf{NC}^{1}$, together with a dual-mode trapdoor function family. Compared to previous constructions (Mahadev, FOCS 2018; Brakerski, CRYPTO 2018) which made non-black-box use of similar underlying primitives, our construction provides a pathway to instantiations from different assumptions. Our construction uses the techniques of Dulek, Schaffner and Speelman (CRYPTO 2016) and shows how to make the client in their QFHE scheme classical using dual-mode trapdoor functions. As an additional contribution, we show a new instantiation of dual-mode trapdoor functions from group actions.","sentences":["We construct a (compact) quantum fully homomorphic encryption (QFHE) scheme starting from (compact) classical fully homomorphic encryption scheme with decryption in $\\mathsf{NC}^{1}$, together with a dual-mode trapdoor function family.","Compared to previous constructions (Mahadev, FOCS 2018; Brakerski, CRYPTO 2018) which made non-black-box use of similar underlying primitives, our construction provides a pathway to instantiations from different assumptions.","Our construction uses the techniques of Dulek, Schaffner and Speelman (CRYPTO 2016) and shows how to make the client in their QFHE scheme classical using dual-mode trapdoor functions.","As an additional contribution, we show a new instantiation of dual-mode trapdoor functions from group actions."],"url":"http://arxiv.org/abs/2406.03379v1","category":"quant-ph"}
{"created":"2024-06-05 15:32:12","title":"On the use of quantality in nuclei and many-body systems","abstract":"The use of quantality is discussed in the case of nuclei and other many-body systems such as atomic electrons. This dimensionless quantity is known to indicate when a many-body system behaves like a crystal or a quantum liquid. Its role is further analyzed, showing the emergence of a fundamental lengthscale, the limit radius, which corresponds to the hard-core of the nucleon-nucleon interaction in the case of nucleons, and to a value close to the Bohr radius in the case of atomic electrons. The occurrence of a cluster phase in nuclei is analyzed using the quantality through its relation to the localization parameter, allowing for the identification of both the number of nucleons and the density as control parameters for the occurrence of this phase. The relation of the quantality to the magnitude of the interaction also exhibits a third dimensionless parameter, monitoring the magnitude of the spin-orbit effect in finite systems, through the realization of the pseudo-spin symmetry. The impact of quantality on the spin-orbit effect is compared in various many-body systems. The role of quantality in the relative effect of the binding energy and the shell one is also analyzed in nuclei. Finally, additional dimensionless quantities are proposed from the generalization of the quantality. Nuclei are found to be exceptional systems because all their dimensionless quantities are close to the order of unity, at variance with other many-body systems.","sentences":["The use of quantality is discussed in the case of nuclei and other many-body systems such as atomic electrons.","This dimensionless quantity is known to indicate when a many-body system behaves like a crystal or a quantum liquid.","Its role is further analyzed, showing the emergence of a fundamental lengthscale, the limit radius, which corresponds to the hard-core of the nucleon-nucleon interaction in the case of nucleons, and to a value close to the Bohr radius in the case of atomic electrons.","The occurrence of a cluster phase in nuclei is analyzed using the quantality through its relation to the localization parameter, allowing for the identification of both the number of nucleons and the density as control parameters for the occurrence of this phase.","The relation of the quantality to the magnitude of the interaction also exhibits a third dimensionless parameter, monitoring the magnitude of the spin-orbit effect in finite systems, through the realization of the pseudo-spin symmetry.","The impact of quantality on the spin-orbit effect is compared in various many-body systems.","The role of quantality in the relative effect of the binding energy and the shell one is also analyzed in nuclei.","Finally, additional dimensionless quantities are proposed from the generalization of the quantality.","Nuclei are found to be exceptional systems because all their dimensionless quantities are close to the order of unity, at variance with other many-body systems."],"url":"http://arxiv.org/abs/2406.03378v1","category":"nucl-th"}
{"created":"2024-06-05 15:31:43","title":"Log Parsing with Self-Generated In-Context Learning and Self-Correction","abstract":"Log parsing transforms log messages into structured formats, serving as a crucial step for log analysis. Despite a variety of log parsing methods that have been proposed, their performance on evolving log data remains unsatisfactory due to reliance on human-crafted rules or learning-based models with limited training data. The recent emergence of large language models (LLMs) has demonstrated strong abilities in understanding natural language and code, making it promising to apply LLMs for log parsing. Consequently, several studies have proposed LLM-based log parsers. However, LLMs may produce inaccurate templates, and existing LLM-based log parsers directly use the template generated by the LLM as the parsing result, hindering the accuracy of log parsing. Furthermore, these log parsers depend heavily on historical log data as demonstrations, which poses challenges in maintaining accuracy when dealing with scarce historical log data or evolving log data. To address these challenges, we propose AdaParser, an effective and adaptive log parsing framework using LLMs with self-generated in-context learning (SG-ICL) and self-correction. To facilitate accurate log parsing, AdaParser incorporates a novel component, a template corrector, which utilizes the LLM to correct potential parsing errors in the templates it generates. In addition, AdaParser maintains a dynamic candidate set composed of previously generated templates as demonstrations to adapt evolving log data. Extensive experiments on public large-scale datasets show that AdaParser outperforms state-of-the-art methods across all metrics, even in zero-shot scenarios. Moreover, when integrated with different LLMs, AdaParser consistently enhances the performance of the utilized LLMs by a large margin.","sentences":["Log parsing transforms log messages into structured formats, serving as a crucial step for log analysis.","Despite a variety of log parsing methods that have been proposed, their performance on evolving log data remains unsatisfactory due to reliance on human-crafted rules or learning-based models with limited training data.","The recent emergence of large language models (LLMs) has demonstrated strong abilities in understanding natural language and code, making it promising to apply LLMs for log parsing.","Consequently, several studies have proposed LLM-based log parsers.","However, LLMs may produce inaccurate templates, and existing LLM-based log parsers directly use the template generated by the LLM as the parsing result, hindering the accuracy of log parsing.","Furthermore, these log parsers depend heavily on historical log data as demonstrations, which poses challenges in maintaining accuracy when dealing with scarce historical log data or evolving log data.","To address these challenges, we propose AdaParser, an effective and adaptive log parsing framework using LLMs with self-generated in-context learning (SG-ICL) and self-correction.","To facilitate accurate log parsing, AdaParser incorporates a novel component, a template corrector, which utilizes the LLM to correct potential parsing errors in the templates it generates.","In addition, AdaParser maintains a dynamic candidate set composed of previously generated templates as demonstrations to adapt evolving log data.","Extensive experiments on public large-scale datasets show that AdaParser outperforms state-of-the-art methods across all metrics, even in zero-shot scenarios.","Moreover, when integrated with different LLMs, AdaParser consistently enhances the performance of the utilized LLMs by a large margin."],"url":"http://arxiv.org/abs/2406.03376v1","category":"cs.SE"}
{"created":"2024-06-05 15:30:10","title":"A field-level emulator for modified gravity","abstract":"Stage IV dark energy surveys such as the Vera C. Rubin Observatory and Euclid present a unique opportunity to shed light on the nature of dark energy. However, the full constraining power of the new data cannot be unlocked unless accurate predictions are available at all observable scales. Currently, only the linear regime is well understood in models beyond $\\Lambda$CDM: on the nonlinear scales, expensive numerical simulations become necessary, making their direct use impractical in the analyses of large datasets. Recently, machine learning techniques have shown potential to break this impasse: by training emulators, we can reproduce complex data fields in a fraction of the time it takes to produce them.   In this work, we present a field-level emulator capable of turning a $\\Lambda$CDM N-body simulation into one evolved under $f(R)$ gravity. To achieve this, we build on the map2map neural network, using the strength of modified gravity $|f_{R_0}|$ as style parameter. We find that our emulator correctly estimates the changes it needs to apply to the positions and velocities of the input N-body particles to produce the target simulation.   We test the performance of our network against several summary statistics, finding $1\\%$ agreement in the power spectrum up to $k \\sim 1$ $h/$Mpc, and $1.5\\%$ agreement against the independent boost emulator eMantis. Although the algorithm is trained on fixed cosmological parameters, we find it can extrapolate to models it was not trained on. Coupled with available field-level emulators and simulations suites for $\\Lambda$CDM, our algorithm can be used to constrain modified gravity in the large-scale structure using full information available at the field-level.","sentences":["Stage IV dark energy surveys such as the Vera C. Rubin Observatory and Euclid present a unique opportunity to shed light on the nature of dark energy.","However, the full constraining power of the new data cannot be unlocked unless accurate predictions are available at all observable scales.","Currently, only the linear regime is well understood in models beyond $\\Lambda$CDM: on the nonlinear scales, expensive numerical simulations become necessary, making their direct use impractical in the analyses of large datasets.","Recently, machine learning techniques have shown potential to break this impasse: by training emulators, we can reproduce complex data fields in a fraction of the time it takes to produce them.   ","In this work, we present a field-level emulator capable of turning a $\\Lambda$CDM N-body simulation into one evolved under $f(R)$ gravity.","To achieve this, we build on the map2map neural network, using the strength of modified gravity $|f_{R_0}|$ as style parameter.","We find that our emulator correctly estimates the changes it needs to apply to the positions and velocities of the input N-body particles to produce the target simulation.   ","We test the performance of our network against several summary statistics, finding $1\\%$ agreement in the power spectrum up to $k \\sim 1$ $h/$Mpc, and $1.5\\%$ agreement against the independent boost emulator eMantis.","Although the algorithm is trained on fixed cosmological parameters, we find it can extrapolate to models it was not trained on.","Coupled with available field-level emulators and simulations suites for $\\Lambda$CDM, our algorithm can be used to constrain modified gravity in the large-scale structure using full information available at the field-level."],"url":"http://arxiv.org/abs/2406.03374v1","category":"astro-ph.CO"}
{"created":"2024-06-05 15:25:16","title":"Orbits of particles with magnetic dipole moment around magnetized Schwarzschild black holes: Applications to S2 star orbit","abstract":"This study provides a comprehensive analytical investigation of the bound and unbound motion of magnetized particles orbiting a Schwarzschild black hole immersed in an external asymptotically uniform magnetic field, which includes all conceivable types of bounded and unbounded orbits. In particular, for planetary orbits, we perform a comparative analysis of our findings with the observed position of the S2 star carrying magnetic dipole moment around Sagittarius A* (Sgr A*). We found maximum and minimum values for the parameter of magnetic interaction between the magnetic dipole of the star and the external magnetic field, as well as the energy and angular momentum of the S2 star. As a result, we obtain estimations of the magnetic dipole of the star in order of $10^6 \\rm \\ G\\cdot cm^{3}$. Additionally, we explore deflecting trajectories akin to gravitational Rutherford scattering. In obtaining the solutions for the orbital equations, we articulate the elliptic integrals and Jacobi elliptic functions, and our study is augmented by illustrative figures and simulations.","sentences":["This study provides a comprehensive analytical investigation of the bound and unbound motion of magnetized particles orbiting a Schwarzschild black hole immersed in an external asymptotically uniform magnetic field, which includes all conceivable types of bounded and unbounded orbits.","In particular, for planetary orbits, we perform a comparative analysis of our findings with the observed position of the S2 star carrying magnetic dipole moment around Sagittarius A* (Sgr A*).","We found maximum and minimum values for the parameter of magnetic interaction between the magnetic dipole of the star and the external magnetic field, as well as the energy and angular momentum of the S2 star.","As a result, we obtain estimations of the magnetic dipole of the star in order of $10^6 \\rm \\ G\\cdot","cm^{3}$. Additionally, we explore deflecting trajectories akin to gravitational Rutherford scattering.","In obtaining the solutions for the orbital equations, we articulate the elliptic integrals and Jacobi elliptic functions, and our study is augmented by illustrative figures and simulations."],"url":"http://arxiv.org/abs/2406.03371v1","category":"gr-qc"}
{"created":"2024-06-05 15:23:08","title":"IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models","abstract":"Despite the widespread adoption of Large language models (LLMs), their remarkable capabilities remain limited to a few high-resource languages. Additionally, many low-resource languages (e.g. African languages) are often evaluated only on basic text classification tasks due to the lack of appropriate or comprehensive benchmarks outside of high-resource languages. In this paper, we introduce IrokoBench -- a human-translated benchmark dataset for 16 typologically-diverse low-resource African languages covering three tasks: natural language inference~(AfriXNLI), mathematical reasoning~(AfriMGSM), and multi-choice knowledge-based QA~(AfriMMLU). We use IrokoBench to evaluate zero-shot, few-shot, and translate-test settings~(where test sets are translated into English) across 10 open and four proprietary LLMs. Our evaluation reveals a significant performance gap between high-resource languages~(such as English and French) and low-resource African languages. We observe a significant performance gap between open and proprietary models, with the highest performing open model, Aya-101 only at 58\\% of the best-performing proprietary model GPT-4o performance. Machine translating the test set to English before evaluation helped to close the gap for larger models that are English-centric, like LLaMa 3 70B. These findings suggest that more efforts are needed to develop and adapt LLMs for African languages.","sentences":["Despite the widespread adoption of Large language models (LLMs), their remarkable capabilities remain limited to a few high-resource languages.","Additionally, many low-resource languages (e.g. African languages) are often evaluated only on basic text classification tasks due to the lack of appropriate or comprehensive benchmarks outside of high-resource languages.","In this paper, we introduce IrokoBench -- a human-translated benchmark dataset for 16 typologically-diverse low-resource African languages covering three tasks: natural language inference~(AfriXNLI), mathematical reasoning~(AfriMGSM), and multi-choice knowledge-based QA~(AfriMMLU).","We use IrokoBench to evaluate zero-shot, few-shot, and translate-test settings~(where test sets are translated into English) across 10 open and four proprietary LLMs.","Our evaluation reveals a significant performance gap between high-resource languages~(such as English and French) and low-resource African languages.","We observe a significant performance gap between open and proprietary models, with the highest performing open model, Aya-101 only at 58\\% of the best-performing proprietary model GPT-4o performance.","Machine translating the test set to English before evaluation helped to close the gap for larger models that are English-centric, like LLaMa 3 70B. These findings suggest that more efforts are needed to develop and adapt LLMs for African languages."],"url":"http://arxiv.org/abs/2406.03368v1","category":"cs.CL"}
{"created":"2024-06-05 15:21:44","title":"CLMASP: Coupling Large Language Models with Answer Set Programming for Robotic Task Planning","abstract":"Large Language Models (LLMs) possess extensive foundational knowledge and moderate reasoning abilities, making them suitable for general task planning in open-world scenarios. However, it is challenging to ground a LLM-generated plan to be executable for the specified robot with certain restrictions. This paper introduces CLMASP, an approach that couples LLMs with Answer Set Programming (ASP) to overcome the limitations, where ASP is a non-monotonic logic programming formalism renowned for its capacity to represent and reason about a robot's action knowledge. CLMASP initiates with a LLM generating a basic skeleton plan, which is subsequently tailored to the specific scenario using a vector database. This plan is then refined by an ASP program with a robot's action knowledge, which integrates implementation details into the skeleton, grounding the LLM's abstract outputs in practical robot contexts. Our experiments conducted on the VirtualHome platform demonstrate CLMASP's efficacy. Compared to the baseline executable rate of under 2% with LLM approaches, CLMASP significantly improves this to over 90%.","sentences":["Large Language Models (LLMs) possess extensive foundational knowledge and moderate reasoning abilities, making them suitable for general task planning in open-world scenarios.","However, it is challenging to ground a LLM-generated plan to be executable for the specified robot with certain restrictions.","This paper introduces CLMASP, an approach that couples LLMs with Answer Set Programming (ASP) to overcome the limitations, where ASP is a non-monotonic logic programming formalism renowned for its capacity to represent and reason about a robot's action knowledge.","CLMASP initiates with a LLM generating a basic skeleton plan, which is subsequently tailored to the specific scenario using a vector database.","This plan is then refined by an ASP program with a robot's action knowledge, which integrates implementation details into the skeleton, grounding the LLM's abstract outputs in practical robot contexts.","Our experiments conducted on the VirtualHome platform demonstrate CLMASP's efficacy.","Compared to the baseline executable rate of under 2% with LLM approaches, CLMASP significantly improves this to over 90%."],"url":"http://arxiv.org/abs/2406.03367v1","category":"cs.AI"}
{"created":"2024-06-05 15:14:58","title":"What Matters in Hierarchical Search for Combinatorial Reasoning Problems?","abstract":"Efficiently tackling combinatorial reasoning problems, particularly the notorious NP-hard tasks, remains a significant challenge for AI research. Recent efforts have sought to enhance planning by incorporating hierarchical high-level search strategies, known as subgoal methods. While promising, their performance against traditional low-level planners is inconsistent, raising questions about their application contexts. In this study, we conduct an in-depth exploration of subgoal-planning methods for combinatorial reasoning. We identify the attributes pivotal for leveraging the advantages of high-level search: hard-to-learn value functions, complex action spaces, presence of dead ends in the environment, or using data collected from diverse experts. We propose a consistent evaluation methodology to achieve meaningful comparisons between methods and reevaluate the state-of-the-art algorithms.","sentences":["Efficiently tackling combinatorial reasoning problems, particularly the notorious NP-hard tasks, remains a significant challenge for AI research.","Recent efforts have sought to enhance planning by incorporating hierarchical high-level search strategies, known as subgoal methods.","While promising, their performance against traditional low-level planners is inconsistent, raising questions about their application contexts.","In this study, we conduct an in-depth exploration of subgoal-planning methods for combinatorial reasoning.","We identify the attributes pivotal for leveraging the advantages of high-level search: hard-to-learn value functions, complex action spaces, presence of dead ends in the environment, or using data collected from diverse experts.","We propose a consistent evaluation methodology to achieve meaningful comparisons between methods and reevaluate the state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2406.03361v1","category":"cs.LG"}
{"created":"2024-06-05 15:14:40","title":"On determinantal point processes with nonsymmetric kernels","abstract":"Determinantal point processes (DPPs for short) are a class of repulsive point processes. They have found some statistical applications to model spatial point pattern datasets with repulsion between close points. In the case of DPPs on finite sets, they are defined by a matrix called the DPP kernel which is usually assumed to be symmetric. While there are a few known examples of DPPs with nonsymmetric kernels, not much is known on how this affects their usual properties. In this paper, we demonstrate how to adapt the results on $P_0$ matrices to the DPP setting in order to get necessary and sufficient conditions for the well-definedness of DPPs with nonsymmetric kernels. We also generalize various common results on DPPs. We then show how to use these results to construct attractive couplings of regular DPPs with symmetric kernels in order to model spatial marked point patterns with repulsion between points of the same mark and attraction between points of different marks.","sentences":["Determinantal point processes (DPPs for short) are a class of repulsive point processes.","They have found some statistical applications to model spatial point pattern datasets with repulsion between close points.","In the case of DPPs on finite sets, they are defined by a matrix called the DPP kernel which is usually assumed to be symmetric.","While there are a few known examples of DPPs with nonsymmetric kernels, not much is known on how this affects their usual properties.","In this paper, we demonstrate how to adapt the results on $P_0$ matrices to the DPP setting in order to get necessary and sufficient conditions for the well-definedness of DPPs with nonsymmetric kernels.","We also generalize various common results on DPPs.","We then show how to use these results to construct attractive couplings of regular DPPs with symmetric kernels in order to model spatial marked point patterns with repulsion between points of the same mark and attraction between points of different marks."],"url":"http://arxiv.org/abs/2406.03360v1","category":"math.ST"}
{"created":"2024-06-05 15:14:07","title":"Bayesian Quantile Estimation and Regression with Martingale Posteriors","abstract":"Quantile estimation and regression within the Bayesian framework is challenging as the choice of likelihood and prior is not obvious. In this paper, we introduce a novel Bayesian nonparametric method for quantile estimation and regression based on the recently introduced martingale posterior (MP) framework. The core idea of the MP is that posterior sampling is equivalent to predictive imputation, which allows us to break free of the stringent likelihood-prior specification. We demonstrate that a recursive estimate of a smooth quantile function, subject to a martingale condition, is entirely sufficient for full nonparametric Bayesian inference. We term the resulting posterior distribution as the quantile martingale posterior (QMP), which arises from an implicit generative predictive distribution. Associated with the QMP is an expedient, MCMC-free and parallelizable posterior computation scheme, which can be further accelerated with an asymptotic approximation based on a Gaussian process. Furthermore, the well-known issue of monotonicity in quantile estimation is naturally alleviated through increasing rearrangement due to the connections to the Bayesian bootstrap. Finally, the QMP has a particularly tractable form that allows for comprehensive theoretical study, which forms a main focus of the work. We demonstrate the ease of posterior computation in simulations and real data experiments.","sentences":["Quantile estimation and regression within the Bayesian framework is challenging as the choice of likelihood and prior is not obvious.","In this paper, we introduce a novel Bayesian nonparametric method for quantile estimation and regression based on the recently introduced martingale posterior (MP) framework.","The core idea of the MP is that posterior sampling is equivalent to predictive imputation, which allows us to break free of the stringent likelihood-prior specification.","We demonstrate that a recursive estimate of a smooth quantile function, subject to a martingale condition, is entirely sufficient for full nonparametric Bayesian inference.","We term the resulting posterior distribution as the quantile martingale posterior (QMP), which arises from an implicit generative predictive distribution.","Associated with the QMP is an expedient, MCMC-free and parallelizable posterior computation scheme, which can be further accelerated with an asymptotic approximation based on a Gaussian process.","Furthermore, the well-known issue of monotonicity in quantile estimation is naturally alleviated through increasing rearrangement due to the connections to the Bayesian bootstrap.","Finally, the QMP has a particularly tractable form that allows for comprehensive theoretical study, which forms a main focus of the work.","We demonstrate the ease of posterior computation in simulations and real data experiments."],"url":"http://arxiv.org/abs/2406.03358v1","category":"stat.ME"}
{"created":"2024-06-05 15:12:29","title":"Cooperative learning of Pl@ntNet's Artificial Intelligence algorithm: how does it work and how can we improve it?","abstract":"Deep learning models for plant species identification rely on large annotated datasets. The PlantNet system enables global data collection by allowing users to upload and annotate plant observations, leading to noisy labels due to diverse user skills. Achieving consensus is crucial for training, but the vast scale of collected data makes traditional label aggregation strategies challenging. Existing methods either retain all observations, resulting in noisy training data or selectively keep those with sufficient votes, discarding valuable information. Additionally, as many species are rarely observed, user expertise can not be evaluated as an inter-user agreement: otherwise, botanical experts would have a lower weight in the AI training step than the average user. Our proposed label aggregation strategy aims to cooperatively train plant identification AI models. This strategy estimates user expertise as a trust score per user based on their ability to identify plant species from crowdsourced data. The trust score is recursively estimated from correctly identified species given the current estimated labels. This interpretable score exploits botanical experts' knowledge and the heterogeneity of users. Subsequently, our strategy removes unreliable observations but retains those with limited trusted annotations, unlike other approaches. We evaluate PlantNet's strategy on a released large subset of the PlantNet database focused on European flora, comprising over 6M observations and 800K users. We demonstrate that estimating users' skills based on the diversity of their expertise enhances labeling performance. Our findings emphasize the synergy of human annotation and data filtering in improving AI performance for a refined dataset. We explore incorporating AI-based votes alongside human input. This can further enhance human-AI interactions to detect unreliable observations.","sentences":["Deep learning models for plant species identification rely on large annotated datasets.","The PlantNet system enables global data collection by allowing users to upload and annotate plant observations, leading to noisy labels due to diverse user skills.","Achieving consensus is crucial for training, but the vast scale of collected data makes traditional label aggregation strategies challenging.","Existing methods either retain all observations, resulting in noisy training data or selectively keep those with sufficient votes, discarding valuable information.","Additionally, as many species are rarely observed, user expertise can not be evaluated as an inter-user agreement: otherwise, botanical experts would have a lower weight in the AI training step than the average user.","Our proposed label aggregation strategy aims to cooperatively train plant identification AI models.","This strategy estimates user expertise as a trust score per user based on their ability to identify plant species from crowdsourced data.","The trust score is recursively estimated from correctly identified species given the current estimated labels.","This interpretable score exploits botanical experts' knowledge and the heterogeneity of users.","Subsequently, our strategy removes unreliable observations but retains those with limited trusted annotations, unlike other approaches.","We evaluate PlantNet's strategy on a released large subset of the PlantNet database focused on European flora, comprising over 6M observations and 800K users.","We demonstrate that estimating users' skills based on the diversity of their expertise enhances labeling performance.","Our findings emphasize the synergy of human annotation and data filtering in improving AI performance for a refined dataset.","We explore incorporating AI-based votes alongside human input.","This can further enhance human-AI interactions to detect unreliable observations."],"url":"http://arxiv.org/abs/2406.03356v1","category":"cs.LG"}
{"created":"2024-06-05 15:10:29","title":"Can Social Media Platforms Transcend Political Labels? An Analysis of Neutral Conservations on Truth Social","abstract":"There is a prevailing perception that content on a social media platform generally have the same political leaning. These platforms are often viewed as ideologically congruent entities, reflecting the majority opinion of their users; a prime example of this is Truth Social. While this perception may exist, it is essential to verify the platform's credibility, acknowledging that such platforms contain meaningful insights with neutral stances. To this end, we examine the dissemination of Wikipedia links on the alt-right platform, Truth Social. Wikipedia is recognized for enforcing content neutrality and serves as a unique lens to analyze the objectivity of user-generated content on Truth Social. By scrutinizing Truths with and without Wikipedia links, identifying toxicity trends & recognizing coordinated networks, we observe a lower level of engagement and a tendency for Truths shared on Truth Social to cover more neutral topics when it includes Wikipedia links (Wiki Truths). Given the significantly different engagement and nature of content shared of Wiki Truths against Non-Wiki Truths, we emphasize that we should not generalize the techno-political affiliation of a social media platform, but rather should investigate the content closely.","sentences":["There is a prevailing perception that content on a social media platform generally have the same political leaning.","These platforms are often viewed as ideologically congruent entities, reflecting the majority opinion of their users; a prime example of this is Truth Social.","While this perception may exist, it is essential to verify the platform's credibility, acknowledging that such platforms contain meaningful insights with neutral stances.","To this end, we examine the dissemination of Wikipedia links on the alt-right platform, Truth Social.","Wikipedia is recognized for enforcing content neutrality and serves as a unique lens to analyze the objectivity of user-generated content on Truth Social.","By scrutinizing Truths with and without Wikipedia links, identifying toxicity trends & recognizing coordinated networks, we observe a lower level of engagement and a tendency for Truths shared on Truth Social to cover more neutral topics when it includes Wikipedia links (Wiki Truths).","Given the significantly different engagement and nature of content shared of Wiki Truths against Non-Wiki Truths, we emphasize that we should not generalize the techno-political affiliation of a social media platform, but rather should investigate the content closely."],"url":"http://arxiv.org/abs/2406.03354v1","category":"cs.SI"}
{"created":"2024-06-05 15:10:22","title":"Quasinormal Modes in Noncommutative Schwarzschild Black Holes: A Spectral Analysis","abstract":"We present a comprehensive analysis of quasinormal modes (QNMs) for noncommutative geometry-inspired Schwarzschild black holes, encompassing both non-extreme and extreme cases. By employing a spectral method, we calculate the QNMs in the context of scalar, electromagnetic, and gravitational perturbations. Our findings not only challenge previous claims in the literature regarding the instability of these black holes but also reveal remarkable stability for both non-extreme and extreme Schwarzschild black holes under various perturbations.","sentences":["We present a comprehensive analysis of quasinormal modes (QNMs) for noncommutative geometry-inspired Schwarzschild black holes, encompassing both non-extreme and extreme cases.","By employing a spectral method, we calculate the QNMs in the context of scalar, electromagnetic, and gravitational perturbations.","Our findings not only challenge previous claims in the literature regarding the instability of these black holes but also reveal remarkable stability for both non-extreme and extreme Schwarzschild black holes under various perturbations."],"url":"http://arxiv.org/abs/2406.03353v1","category":"gr-qc"}
{"created":"2024-06-05 15:09:20","title":"Joint Modelling of Astrophysical Systematics for Cosmology with LSST","abstract":"We present a novel framework for jointly modelling the weak lensing source galaxy redshift distribution and the intrinsic alignment of galaxies via a shared luminosity function (LF). Considering this framework within the context of a Rubin Observatory's Legacy Survey of Space and Time (LSST) Year 1 and Year 10 cosmic shear analysis, we first demonstrate the substantial impact of the LF on both source galaxy redshift distributions and the intrinsic alignment contamination. We establish how the individual parameters of a Schechter LF model impact the redshift distribution of a magnitude-limited sample, and we demonstrate the effect of marginalising over the LF parameters as incorporated in the intrinsic alignment modelling of a standard cosmic shear analysis set-up. We forecast the impact of our joint modelling framework on cosmological parameter constraints. Our preliminary results are promising, indicating that this framework can yield cosmological constraints consistent with those expected from standard analyses, enhanced by the flexibility of not fixing LF parameters. We plan to further validate these findings with comprehensive Markov chain Monte Carlo simulations to robustly quantify bias avoidance and underscore the framework's efficacy. Taking advantage of our forecasting results and the parameter degeneracies, we identify the specific impact of the shape of the LF of source galaxies on the cosmic shear data vector. We also discuss the potential of this method in providing a way to model generic selection functions in redshift distribution estimation, as well as its possibilities for extension to a 3x2pt analysis, particularly with respect to incorporating galaxy bias in this luminosity-function-based framework. Although we consider the context of LSST cosmic shear in this work, the proposed joint modelling framework is generically applicable to weak lensing surveys.","sentences":["We present a novel framework for jointly modelling the weak lensing source galaxy redshift distribution and the intrinsic alignment of galaxies via a shared luminosity function (LF).","Considering this framework within the context of a Rubin Observatory's Legacy Survey of Space and Time (LSST)","Year 1 and Year 10 cosmic shear analysis, we first demonstrate the substantial impact of the LF on both source galaxy redshift distributions and the intrinsic alignment contamination.","We establish how the individual parameters of a Schechter LF model impact the redshift distribution of a magnitude-limited sample, and we demonstrate the effect of marginalising over the LF parameters as incorporated in the intrinsic alignment modelling of a standard cosmic shear analysis set-up.","We forecast the impact of our joint modelling framework on cosmological parameter constraints.","Our preliminary results are promising, indicating that this framework can yield cosmological constraints consistent with those expected from standard analyses, enhanced by the flexibility of not fixing LF parameters.","We plan to further validate these findings with comprehensive Markov chain Monte Carlo simulations to robustly quantify bias avoidance and underscore the framework's efficacy.","Taking advantage of our forecasting results and the parameter degeneracies, we identify the specific impact of the shape of the LF of source galaxies on the cosmic shear data vector.","We also discuss the potential of this method in providing a way to model generic selection functions in redshift distribution estimation, as well as its possibilities for extension to a 3x2pt analysis, particularly with respect to incorporating galaxy bias in this luminosity-function-based framework.","Although we consider the context of LSST cosmic shear in this work, the proposed joint modelling framework is generically applicable to weak lensing surveys."],"url":"http://arxiv.org/abs/2406.03352v1","category":"astro-ph.CO"}
{"created":"2024-06-05 15:07:06","title":"Resonant phenomena in finite motions of test particles in oscillating dark matter configurations","abstract":"Nonlinear differential equations are derived that describe the time evolution of the test particle coordinates during finite motions in the gravitational field of oscillating dark matter. It is shown that in the weak field approximation, the radial oscillations of a test particle and oscillations in orbital motion are described by the Hill equation and the nonhomogeneous Hill equation, respectively. In the case of scalar dark matter with a logarithmic self-interactions, these equations are integrated numerically, and the solutions are compared with the corresponding solutions of the original nonlinear system to identify possible resonance effects.","sentences":["Nonlinear differential equations are derived that describe the time evolution of the test particle coordinates during finite motions in the gravitational field of oscillating dark matter.","It is shown that in the weak field approximation, the radial oscillations of a test particle and oscillations in orbital motion are described by the Hill equation and the nonhomogeneous Hill equation, respectively.","In the case of scalar dark matter with a logarithmic self-interactions, these equations are integrated numerically, and the solutions are compared with the corresponding solutions of the original nonlinear system to identify possible resonance effects."],"url":"http://arxiv.org/abs/2406.03351v1","category":"gr-qc"}
{"created":"2024-06-05 15:04:28","title":"Normalizing Flows for Conformal Regression","abstract":"Conformal Prediction (CP) algorithms estimate the uncertainty of a prediction model by calibrating its outputs on labeled data. The same calibration scheme usually applies to any model and data without modifications. The obtained prediction intervals are valid by construction but could be inefficient, i.e. unnecessarily big, if the prediction errors are not uniformly distributed over the input space.   We present a general scheme to localize the intervals by training the calibration process. The standard prediction error is replaced by an optimized distance metric that depends explicitly on the object attributes. Learning the optimal metric is equivalent to training a Normalizing Flow that acts on the joint distribution of the errors and the inputs. Unlike the Error Re-weighting CP algorithm of Papadopoulos et al. (2008), the framework allows estimating the gap between nominal and empirical conditional validity. The approach is compatible with existing locally-adaptive CP strategies based on re-weighting the calibration samples and applies to any point-prediction model without retraining.","sentences":["Conformal Prediction (CP) algorithms estimate the uncertainty of a prediction model by calibrating its outputs on labeled data.","The same calibration scheme usually applies to any model and data without modifications.","The obtained prediction intervals are valid by construction but could be inefficient, i.e. unnecessarily big, if the prediction errors are not uniformly distributed over the input space.   ","We present a general scheme to localize the intervals by training the calibration process.","The standard prediction error is replaced by an optimized distance metric that depends explicitly on the object attributes.","Learning the optimal metric is equivalent to training a Normalizing Flow that acts on the joint distribution of the errors and the inputs.","Unlike the Error Re-weighting CP algorithm of Papadopoulos et al. (2008), the framework allows estimating the gap between nominal and empirical conditional validity.","The approach is compatible with existing locally-adaptive CP strategies based on re-weighting the calibration samples and applies to any point-prediction model without retraining."],"url":"http://arxiv.org/abs/2406.03346v1","category":"cs.LG"}
{"created":"2024-06-05 15:04:27","title":"Feature Contamination: Neural Networks Learn Uncorrelated Features and Fail to Generalize","abstract":"Learning representations that generalize under distribution shifts is critical for building robust machine learning models. However, despite significant efforts in recent years, algorithmic advances in this direction have been limited. In this work, we seek to understand the fundamental difficulty of out-of-distribution generalization with deep neural networks. We first empirically show that perhaps surprisingly, even allowing a neural network to explicitly fit the representations obtained from a teacher network that can generalize out-of-distribution is insufficient for the generalization of the student network. Then, by a theoretical study of two-layer ReLU networks optimized by stochastic gradient descent (SGD) under a structured feature model, we identify a fundamental yet unexplored feature learning proclivity of neural networks, feature contamination: neural networks can learn uncorrelated features together with predictive features, resulting in generalization failure under distribution shifts. Notably, this mechanism essentially differs from the prevailing narrative in the literature that attributes the generalization failure to spurious correlations. Overall, our results offer new insights into the non-linear feature learning dynamics of neural networks and highlight the necessity of considering inductive biases in out-of-distribution generalization.","sentences":["Learning representations that generalize under distribution shifts is critical for building robust machine learning models.","However, despite significant efforts in recent years, algorithmic advances in this direction have been limited.","In this work, we seek to understand the fundamental difficulty of out-of-distribution generalization with deep neural networks.","We first empirically show that perhaps surprisingly, even allowing a neural network to explicitly fit the representations obtained from a teacher network that can generalize out-of-distribution is insufficient for the generalization of the student network.","Then, by a theoretical study of two-layer ReLU networks optimized by stochastic gradient descent (SGD) under a structured feature model, we identify a fundamental yet unexplored feature learning proclivity of neural networks, feature contamination: neural networks can learn uncorrelated features together with predictive features, resulting in generalization failure under distribution shifts.","Notably, this mechanism essentially differs from the prevailing narrative in the literature that attributes the generalization failure to spurious correlations.","Overall, our results offer new insights into the non-linear feature learning dynamics of neural networks and highlight the necessity of considering inductive biases in out-of-distribution generalization."],"url":"http://arxiv.org/abs/2406.03345v1","category":"cs.LG"}
{"created":"2024-06-05 15:00:59","title":"Audio Mamba: Bidirectional State Space Model for Audio Representation Learning","abstract":"Transformers have rapidly become the preferred choice for audio classification, surpassing methods based on CNNs. However, Audio Spectrogram Transformers (ASTs) exhibit quadratic scaling due to self-attention. The removal of this quadratic self-attention cost presents an appealing direction. Recently, state space models (SSMs), such as Mamba, have demonstrated potential in language and vision tasks in this regard. In this study, we explore whether reliance on self-attention is necessary for audio classification tasks. By introducing Audio Mamba (AuM), the first self-attention-free, purely SSM-based model for audio classification, we aim to address this question. We evaluate AuM on various audio datasets - comprising six different benchmarks - where it achieves comparable or better performance compared to well-established AST model.","sentences":["Transformers have rapidly become the preferred choice for audio classification, surpassing methods based on CNNs.","However, Audio Spectrogram Transformers (ASTs) exhibit quadratic scaling due to self-attention.","The removal of this quadratic self-attention cost presents an appealing direction.","Recently, state space models (SSMs), such as Mamba, have demonstrated potential in language and vision tasks in this regard.","In this study, we explore whether reliance on self-attention is necessary for audio classification tasks.","By introducing Audio Mamba (AuM), the first self-attention-free, purely SSM-based model for audio classification, we aim to address this question.","We evaluate AuM on various audio datasets - comprising six different benchmarks - where it achieves comparable or better performance compared to well-established AST model."],"url":"http://arxiv.org/abs/2406.03344v1","category":"cs.SD"}
{"created":"2024-06-05 14:59:58","title":"Relative-belief inference in quantum information theory","abstract":"We introduce the framework of Bayesian relative belief that directly evaluates whether or not the experimental data at hand supports a given hypothesis regarding a quantum system by directly comparing the prior and posterior probabilities for the hypothesis. In model-dimension certification tasks, we show that the relative belief procedure typically chooses Hilbert spaces that are never smaller in dimension than those selected from optimizing a broad class of information criteria, including Akaike's criterion. As a concrete and focused exposition of this powerful evidence-based technique, we apply the relative belief procedure to an important application: state reconstruction of imperfect quantum sources. In particular, just by comparing prior and posterior probabilities based on data, we demonstrate its capability of tracking multiphoton emissions using (realistically lossy) single-photon detectors in order to assess the actual quality of photon sources without making ad hoc assumptions, thereby reliably safeguarding source integrity for general quantum-information and communication tasks with Bayesian reasoning. Finally, we discuss how relative belief can be exploited to carry out parametric model certification and estimate the total dimension of the quantum state for the combined (measured) physical and interacting external systems described by the Tavis--Cummings model.","sentences":["We introduce the framework of Bayesian relative belief that directly evaluates whether or not the experimental data at hand supports a given hypothesis regarding a quantum system by directly comparing the prior and posterior probabilities for the hypothesis.","In model-dimension certification tasks, we show that the relative belief procedure typically chooses Hilbert spaces that are never smaller in dimension than those selected from optimizing a broad class of information criteria, including Akaike's criterion.","As a concrete and focused exposition of this powerful evidence-based technique, we apply the relative belief procedure to an important application: state reconstruction of imperfect quantum sources.","In particular, just by comparing prior and posterior probabilities based on data, we demonstrate its capability of tracking multiphoton emissions using (realistically lossy) single-photon detectors in order to assess the actual quality of photon sources without making ad hoc assumptions, thereby reliably safeguarding source integrity for general quantum-information and communication tasks with Bayesian reasoning.","Finally, we discuss how relative belief can be exploited to carry out parametric model certification and estimate the total dimension of the quantum state for the combined (measured) physical and interacting external systems described by the Tavis--Cummings model."],"url":"http://arxiv.org/abs/2406.03343v1","category":"quant-ph"}
{"created":"2024-06-05 14:58:32","title":"Tackling GenAI Copyright Issues: Originality Estimation and Genericization","abstract":"The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. While some studies explore methods to mitigate copyright risks by steering the outputs of generative models away from those resembling copyrighted data, little attention has been paid to the question of how much of a resemblance is undesirable; more original or unique data are afforded stronger protection, and the threshold level of resemblance for constituting infringement correspondingly lower. Here, leveraging this principle, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to infringe copyright. To achieve this, we introduce a metric for quantifying the level of originality of data in a manner that is consistent with the legal framework. This metric can be practically estimated by drawing samples from a generative model, which is then used for the genericization process. Experiments demonstrate that our genericization method successfully modifies the output of a text-to-image generative model so that it produces more generic, copyright-compliant images.","sentences":["The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers.","While some studies explore methods to mitigate copyright risks by steering the outputs of generative models away from those resembling copyrighted data, little attention has been paid to the question of how much of a resemblance is undesirable; more original or unique data are afforded stronger protection, and the threshold level of resemblance for constituting infringement correspondingly lower.","Here, leveraging this principle, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to infringe copyright.","To achieve this, we introduce a metric for quantifying the level of originality of data in a manner that is consistent with the legal framework.","This metric can be practically estimated by drawing samples from a generative model, which is then used for the genericization process.","Experiments demonstrate that our genericization method successfully modifies the output of a text-to-image generative model so that it produces more generic, copyright-compliant images."],"url":"http://arxiv.org/abs/2406.03341v1","category":"cs.LG"}
{"created":"2024-06-05 14:55:10","title":"The Challenges of Evaluating LLM Applications: An Analysis of Automated, Human, and LLM-Based Approaches","abstract":"Chatbots have been an interesting application of natural language generation since its inception. With novel transformer based Generative AI methods, building chatbots have become trivial. Chatbots which are targeted at specific domains such as medicine, psychology, and general information retrieval are implemented rapidly. This, however, should not distract from the need to evaluate the chatbot responses. Especially because the natural language generation community does not entirely agree upon how to effectively evaluate such applications. With this work we discuss the issue further with the increasingly popular LLM based evaluations and how they correlate with human evaluations. Additionally, we introduce a comprehensive factored evaluation mechanism that can be utilized in conjunction with both human and LLM-based evaluations.   We present the results of an experimental evaluation conducted using this scheme in one of our chatbot implementations, and subsequently compare automated, traditional human evaluation, factored human evaluation, and factored LLM evaluation. Results show that factor based evaluation produces better insights on which aspects need to be improved in LLM applications and further strengthens the argument to use human evaluation in critical spaces where main functionality is not direct retrieval.","sentences":["Chatbots have been an interesting application of natural language generation since its inception.","With novel transformer based Generative AI methods, building chatbots have become trivial.","Chatbots which are targeted at specific domains such as medicine, psychology, and general information retrieval are implemented rapidly.","This, however, should not distract from the need to evaluate the chatbot responses.","Especially because the natural language generation community does not entirely agree upon how to effectively evaluate such applications.","With this work we discuss the issue further with the increasingly popular LLM based evaluations and how they correlate with human evaluations.","Additionally, we introduce a comprehensive factored evaluation mechanism that can be utilized in conjunction with both human and LLM-based evaluations.   ","We present the results of an experimental evaluation conducted using this scheme in one of our chatbot implementations, and subsequently compare automated, traditional human evaluation, factored human evaluation, and factored LLM evaluation.","Results show that factor based evaluation produces better insights on which aspects need to be improved in LLM applications and further strengthens the argument to use human evaluation in critical spaces where main functionality is not direct retrieval."],"url":"http://arxiv.org/abs/2406.03339v1","category":"cs.CL"}
{"created":"2024-06-05 14:52:43","title":"Identifying latent state transition in non-linear dynamical systems","abstract":"This work aims to improve generalization and interpretability of dynamical systems by recovering the underlying lower-dimensional latent states and their time evolutions. Previous work on disentangled representation learning within the realm of dynamical systems focused on the latent states, possibly with linear transition approximations. As such, they cannot identify nonlinear transition dynamics, and hence fail to reliably predict complex future behavior. Inspired by the advances in nonlinear ICA, we propose a state-space modeling framework in which we can identify not just the latent states but also the unknown transition function that maps the past states to the present. We introduce a practical algorithm based on variational auto-encoders and empirically demonstrate in realistic synthetic settings that we can (i) recover latent state dynamics with high accuracy, (ii) correspondingly achieve high future prediction accuracy, and (iii) adapt fast to new environments.","sentences":["This work aims to improve generalization and interpretability of dynamical systems by recovering the underlying lower-dimensional latent states and their time evolutions.","Previous work on disentangled representation learning within the realm of dynamical systems focused on the latent states, possibly with linear transition approximations.","As such, they cannot identify nonlinear transition dynamics, and hence fail to reliably predict complex future behavior.","Inspired by the advances in nonlinear ICA, we propose a state-space modeling framework in which we can identify not just the latent states but also the unknown transition function that maps the past states to the present.","We introduce a practical algorithm based on variational auto-encoders and empirically demonstrate in realistic synthetic settings that we can (i) recover latent state dynamics with high accuracy, (ii) correspondingly achieve high future prediction accuracy, and (iii) adapt fast to new environments."],"url":"http://arxiv.org/abs/2406.03337v1","category":"cs.LG"}
{"created":"2024-06-05 14:45:07","title":"Interpreting Mass and Radius Measurements of Neutron Stars with Dark Matter Halos","abstract":"The high densities of neutron stars (NSs) could provide astrophysical locations for dark matter (DM) to accumulate. Depending on the DM model, these DM admixed NSs (DANSs) could have significantly different properties than pure baryonic NSs, accessible through X-ray observations of rotation-powered pulsars. We adopt the two-fluid formalism in general relativity to numerically simulate stable configurations of DANSs, assuming a fermionic equation of state (EOS) for the DM with repulsive self-interaction. The distribution of DM in the DANS as a halo affects the path of X-rays emitted from hot spots on the visible baryonic surface causing notable changes in the pulse profile observed by telescopes such as NICER, compared to pure baryonic NSs. We explore how various DM models affect the DM mass distribution, leading to different types of dark halos. We quantify the deviation in observed X-ray flux from stars with each of these halos and explain how to interpret mass and radius measurements of NSs inferred from electromagnetic radiation if these dark halos exist.","sentences":["The high densities of neutron stars (NSs) could provide astrophysical locations for dark matter (DM) to accumulate.","Depending on the DM model, these DM admixed NSs (DANSs) could have significantly different properties than pure baryonic NSs, accessible through X-ray observations of rotation-powered pulsars.","We adopt the two-fluid formalism in general relativity to numerically simulate stable configurations of DANSs, assuming a fermionic equation of state (EOS) for the DM with repulsive self-interaction.","The distribution of DM in the DANS as a halo affects the path of X-rays emitted from hot spots on the visible baryonic surface causing notable changes in the pulse profile observed by telescopes such as NICER, compared to pure baryonic NSs.","We explore how various DM models affect the DM mass distribution, leading to different types of dark halos.","We quantify the deviation in observed X-ray flux from stars with each of these halos and explain how to interpret mass and radius measurements of NSs inferred from electromagnetic radiation if these dark halos exist."],"url":"http://arxiv.org/abs/2406.03332v1","category":"astro-ph.HE"}
{"created":"2024-06-05 14:38:30","title":"EngineBench: Flow Reconstruction in the Transparent Combustion Chamber III Optical Engine","abstract":"We present EngineBench, the first machine learning (ML) oriented database to use high quality experimental data for the study of turbulent flows inside combustion machinery. Prior datasets for ML in fluid mechanics are synthetic or use overly simplistic geometries. EngineBench is comprised of real-world particle image velocimetry (PIV) data that captures the turbulent airflow patterns in a specially-designed optical engine. However, in PIV data from internal flows, such as from engines, it is often challenging to achieve a full field of view and large occlusions can be present. In order to design optimal combustion systems, insight into the turbulent flows in these obscured areas is needed, which can be provided via inpainting models. Here we propose a novel inpainting task using random edge gaps, a technique that emphasises realism by introducing occlusions at random sizes and orientations at the edges of the PIV images. We test five ML methods on random edge gaps using pixel-wise, vector-based, and multi-scale performance metrics. We find that UNet-based models are more accurate than the industry-norm non-parametric approach and the context encoder at this task on both small and large gap sizes. The dataset and inpainting task presented in this paper support the development of more general-purpose pre-trained ML models for engine design problems. The method comparisons allow for more informed selection of ML models for problems in experimental flow diagnostics. All data and code are publicly available at https://eng.ox.ac.uk/tpsrg/research/enginebench/.","sentences":["We present EngineBench, the first machine learning (ML) oriented database to use high quality experimental data for the study of turbulent flows inside combustion machinery.","Prior datasets for ML in fluid mechanics are synthetic or use overly simplistic geometries.","EngineBench is comprised of real-world particle image velocimetry (PIV) data that captures the turbulent airflow patterns in a specially-designed optical engine.","However, in PIV data from internal flows, such as from engines, it is often challenging to achieve a full field of view and large occlusions can be present.","In order to design optimal combustion systems, insight into the turbulent flows in these obscured areas is needed, which can be provided via inpainting models.","Here we propose a novel inpainting task using random edge gaps, a technique that emphasises realism by introducing occlusions at random sizes and orientations at the edges of the PIV images.","We test five ML methods on random edge gaps using pixel-wise, vector-based, and multi-scale performance metrics.","We find that UNet-based models are more accurate than the industry-norm non-parametric approach and the context encoder at this task on both small and large gap sizes.","The dataset and inpainting task presented in this paper support the development of more general-purpose pre-trained ML models for engine design problems.","The method comparisons allow for more informed selection of ML models for problems in experimental flow diagnostics.","All data and code are publicly available at https://eng.ox.ac.uk/tpsrg/research/enginebench/."],"url":"http://arxiv.org/abs/2406.03325v1","category":"physics.flu-dyn"}
{"created":"2024-06-05 14:26:45","title":"Reproducibility study of FairAC","abstract":"This work aims to reproduce the findings of the paper \"Fair Attribute Completion on Graph with Missing Attributes\" written by Guo, Chu, and Li arXiv:2302.12977 by investigating the claims made in the paper. This paper suggests that the results of the original paper are reproducible and thus, the claims hold. However, the claim that FairAC is a generic framework for many downstream tasks is very broad and could therefore only be partially tested. Moreover, we show that FairAC is generalizable to various datasets and sensitive attributes and show evidence that the improvement in group fairness of the FairAC framework does not come at the expense of individual fairness. Lastly, the codebase of FairAC has been refactored and is now easily applicable for various datasets and models.","sentences":["This work aims to reproduce the findings of the paper \"Fair Attribute Completion on Graph with Missing Attributes\" written by Guo, Chu, and Li arXiv:2302.12977 by investigating the claims made in the paper.","This paper suggests that the results of the original paper are reproducible and thus, the claims hold.","However, the claim that FairAC is a generic framework for many downstream tasks is very broad and could therefore only be partially tested.","Moreover, we show that FairAC is generalizable to various datasets and sensitive attributes and show evidence that the improvement in group fairness of the FairAC framework does not come at the expense of individual fairness.","Lastly, the codebase of FairAC has been refactored and is now easily applicable for various datasets and models."],"url":"http://arxiv.org/abs/2406.03314v1","category":"cs.LG"}
{"created":"2024-06-05 14:22:33","title":"Forward-backward algorithms devised by graphs","abstract":"In this work, we present a methodology for devising forward-backward methods for finding zeros in the sum of a finite number of maximally monotone operators. We extend the framework and techniques from [SIAM J. Optim., 34 (2024), pp. 1569-1594] to cover the case involving a finite number of cocoercive operators, which should be directly evaluated instead of computing their resolvent. The algorithms are induced by three graphs that determine how the algorithm variables interact with each other and how they are combined to compute each resolvent. The hypotheses on these graphs ensure that the algorithms obtained have minimal lifting and are frugal, meaning that the ambient space of the underlying fixed point operator has minimal dimension and that each resolvent and each cocoercive operator is evaluated only once per iteration. This framework not only allows to recover some known methods, but also to generate new ones, as the forward-backward algorithm induced by a complete graph. We conclude with a numerical experiment showing how the choice of graphs influences the performance of the algorithms.","sentences":["In this work, we present a methodology for devising forward-backward methods for finding zeros in the sum of a finite number of maximally monotone operators.","We extend the framework and techniques from [SIAM J. Optim., 34 (2024), pp. 1569-1594] to cover the case involving a finite number of cocoercive operators, which should be directly evaluated instead of computing their resolvent.","The algorithms are induced by three graphs that determine how the algorithm variables interact with each other and how they are combined to compute each resolvent.","The hypotheses on these graphs ensure that the algorithms obtained have minimal lifting and are frugal, meaning that the ambient space of the underlying fixed point operator has minimal dimension and that each resolvent and each cocoercive operator is evaluated only once per iteration.","This framework not only allows to recover some known methods, but also to generate new ones, as the forward-backward algorithm induced by a complete graph.","We conclude with a numerical experiment showing how the choice of graphs influences the performance of the algorithms."],"url":"http://arxiv.org/abs/2406.03309v1","category":"math.OC"}
{"created":"2024-06-05 14:18:45","title":"Multi-Patch Isogeometric Convolution Hierarchical Deep-learning Neural Network","abstract":"A seamless integration of neural networks with Isogeometric Analysis (IGA) was first introduced in [1] under the name of Hierarchical Deep-learning Neural Network (HiDeNN) and has systematically evolved into Isogeometric Convolution HiDeNN (in short, C-IGA) [2]. C-IGA achieves higher order approximations without increasing the degree of freedom. Due to the Kronecker delta property of C-IGA shape functions, one can refine the mesh in the physical domain like standard finite element method (FEM) while maintaining the exact geometrical mapping of IGA. In this article, C-IGA theory is generalized for multi-CAD-patch systems with a mathematical investigation of the compatibility conditions at patch interfaces and convergence of error estimates. Two compatibility conditions (nodal compatibility and G^0 (i.e., global C^0) compatibility) are presented and validated through numerical examples.","sentences":["A seamless integration of neural networks with Isogeometric Analysis (IGA) was first introduced in [1] under the name of Hierarchical Deep-learning Neural Network (HiDeNN) and has systematically evolved into Isogeometric Convolution HiDeNN (in short, C-IGA)","[2].","C-IGA achieves higher order approximations without increasing the degree of freedom.","Due to the Kronecker delta property of C-IGA shape functions, one can refine the mesh in the physical domain like standard finite element method (FEM) while maintaining the exact geometrical mapping of IGA.","In this article, C-IGA theory is generalized for multi-CAD-patch systems with a mathematical investigation of the compatibility conditions at patch interfaces and convergence of error estimates.","Two compatibility conditions (nodal compatibility and G^0 (i.e., global C^0) compatibility) are presented and validated through numerical examples."],"url":"http://arxiv.org/abs/2406.03307v1","category":"math.NA"}
{"created":"2024-06-05 14:16:47","title":"Heisenberg-limited adaptive gradient estimation for multiple observables","abstract":"In quantum mechanics, measuring the expectation value of a general observable has an inherent statistical uncertainty that is quantified by variance or mean squared error of measurement outcome. While the uncertainty can be reduced by averaging several samples, the number of samples should be minimized when each sample is very costly. This is especially the case for fault-tolerant quantum computing that involves measurement of multiple observables of non-trivial states in large quantum systems that exceed the capabilities of classical computers. In this work, we provide an adaptive quantum algorithm for estimating the expectation values of $M$ general observables within root mean squared error $\\varepsilon$ simultaneously, using $\\mathcal{O}(\\varepsilon^{-1}\\sqrt{M}\\log M)$ queries to a state preparation oracle of a target state. This remarkably achieves the scaling of Heisenberg limit $1/\\varepsilon$, a fundamental bound on the estimation precision in terms of mean squared error, together with the sublinear scaling of the number of observables $M$. The proposed method is an adaptive version of the quantum gradient estimation algorithm and has a resource-efficient implementation due to its adaptiveness. Specifically, the space overhead in the proposed method is $\\mathcal{O}(M)$ which is independent from the estimation precision $\\varepsilon$ unlike non-iterative algorithms. In addition, our method can avoid the numerical instability problem for constructing quantum circuits in a large-scale task (e.g., $\\varepsilon\\ll 1$ in our case), which appears in the actual implementation of many algorithms relying on quantum signal processing techniques. Our method paves a new way to precisely understand and predict various physical properties in complicated quantum systems using quantum computers.","sentences":["In quantum mechanics, measuring the expectation value of a general observable has an inherent statistical uncertainty that is quantified by variance or mean squared error of measurement outcome.","While the uncertainty can be reduced by averaging several samples, the number of samples should be minimized when each sample is very costly.","This is especially the case for fault-tolerant quantum computing that involves measurement of multiple observables of non-trivial states in large quantum systems that exceed the capabilities of classical computers.","In this work, we provide an adaptive quantum algorithm for estimating the expectation values of $M$ general observables within root mean squared error $\\varepsilon$ simultaneously, using $\\mathcal{O}(\\varepsilon^{-1}\\sqrt{M}\\log M)$ queries to a state preparation oracle of a target state.","This remarkably achieves the scaling of Heisenberg limit $1/\\varepsilon$, a fundamental bound on the estimation precision in terms of mean squared error, together with the sublinear scaling of the number of observables $M$. The proposed method is an adaptive version of the quantum gradient estimation algorithm and has a resource-efficient implementation due to its adaptiveness.","Specifically, the space overhead in the proposed method is $\\mathcal{O}(M)$ which is independent from the estimation precision $\\varepsilon$ unlike non-iterative algorithms.","In addition, our method can avoid the numerical instability problem for constructing quantum circuits in a large-scale task (e.g., $\\varepsilon\\ll 1$ in our case), which appears in the actual implementation of many algorithms relying on quantum signal processing techniques.","Our method paves a new way to precisely understand and predict various physical properties in complicated quantum systems using quantum computers."],"url":"http://arxiv.org/abs/2406.03306v1","category":"quant-ph"}
{"created":"2024-06-05 14:16:09","title":"Parametric Study of a Bladeless Fan Geometry: Investigating the Influence of Geometry Parameters on Discharge Ratio and Thrust Force","abstract":"The innovative bladeless design of the AirMultiplier, patented by Dyson in 2009, has generated significant interest due to its unique approach to air circulation that leverages the Coanda effect. In this study, we present a comprehensive parametric analysis of a simplified bladeless fan geometry, with a specific focus on the discharge ratio and generated thrust force for small to medium radii (5 to 200mm) and a range of flow rates spanning from 1 to 100g/s. In addition to the radii and mass flow rates, this study also addresses the effect of the slit nozzle thickness on the discharge ratio and thrust force of the AirMultiplier. By exploring the influence of this additional parameter, we aim to provide a more complete understanding of the performance characteristics of the geometry and to offer insights that could inform the design and optimization of similar bladeless geometries.","sentences":["The innovative bladeless design of the AirMultiplier, patented by Dyson in 2009, has generated significant interest due to its unique approach to air circulation that leverages the Coanda effect.","In this study, we present a comprehensive parametric analysis of a simplified bladeless fan geometry, with a specific focus on the discharge ratio and generated thrust force for small to medium radii (5 to 200mm) and a range of flow rates spanning from 1 to 100g/s.","In addition to the radii and mass flow rates, this study also addresses the effect of the slit nozzle thickness on the discharge ratio and thrust force of the AirMultiplier.","By exploring the influence of this additional parameter, we aim to provide a more complete understanding of the performance characteristics of the geometry and to offer insights that could inform the design and optimization of similar bladeless geometries."],"url":"http://arxiv.org/abs/2406.03305v1","category":"physics.flu-dyn"}
{"created":"2024-06-05 14:08:54","title":"The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games","abstract":"Behavior study experiments are an important part of society modeling and understanding human interactions. In practice, many behavioral experiments encounter challenges related to internal and external validity, reproducibility, and social bias due to the complexity of social interactions and cooperation in human user studies. Recent advances in Large Language Models (LLMs) have provided researchers with a new promising tool for the simulation of human behavior. However, existing LLM-based simulations operate under the unproven hypothesis that LLM agents behave similarly to humans as well as ignore a crucial factor in human decision-making: emotions.   In this paper, we introduce a novel methodology and the framework to study both, the decision-making of LLMs and their alignment with human behavior under emotional states. Experiments with GPT-3.5 and GPT-4 on four games from two different classes of behavioral game theory showed that emotions profoundly impact the performance of LLMs, leading to the development of more optimal strategies. While there is a strong alignment between the behavioral responses of GPT-3.5 and human participants, particularly evident in bargaining games, GPT-4 exhibits consistent behavior, ignoring induced emotions for rationality decisions. Surprisingly, emotional prompting, particularly with `anger' emotion, can disrupt the \"superhuman\" alignment of GPT-4, resembling human emotional responses.","sentences":["Behavior study experiments are an important part of society modeling and understanding human interactions.","In practice, many behavioral experiments encounter challenges related to internal and external validity, reproducibility, and social bias due to the complexity of social interactions and cooperation in human user studies.","Recent advances in Large Language Models (LLMs) have provided researchers with a new promising tool for the simulation of human behavior.","However, existing LLM-based simulations operate under the unproven hypothesis that LLM agents behave similarly to humans as well as ignore a crucial factor in human decision-making: emotions.   ","In this paper, we introduce a novel methodology and the framework to study both, the decision-making of LLMs and their alignment with human behavior under emotional states.","Experiments with GPT-3.5 and GPT-4 on four games from two different classes of behavioral game theory showed that emotions profoundly impact the performance of LLMs, leading to the development of more optimal strategies.","While there is a strong alignment between the behavioral responses of GPT-3.5 and human participants, particularly evident in bargaining games, GPT-4 exhibits consistent behavior, ignoring induced emotions for rationality decisions.","Surprisingly, emotional prompting, particularly with `anger' emotion, can disrupt the \"superhuman\" alignment of GPT-4, resembling human emotional responses."],"url":"http://arxiv.org/abs/2406.03299v1","category":"cs.AI"}
{"created":"2024-06-05 14:02:31","title":"Text-to-Image Rectified Flow as Plug-and-Play Priors","abstract":"Large-scale diffusion models have achieved remarkable performance in generative tasks. Beyond their initial training applications, these models have proven their ability to function as versatile plug-and-play priors. For instance, 2D diffusion models can serve as loss functions to optimize 3D implicit models. Rectified flow, a novel class of generative models, enforces a linear progression from the source to the target distribution and has demonstrated superior performance across various domains. Compared to diffusion-based methods, rectified flow approaches surpass in terms of generation quality and efficiency, requiring fewer inference steps. In this work, we present theoretical and experimental evidence demonstrating that rectified flow based methods offer similar functionalities to diffusion models - they can also serve as effective priors. Besides the generative capabilities of diffusion priors, motivated by the unique time-symmetry properties of rectified flow models, a variant of our method can additionally perform image inversion. Experimentally, our rectified flow-based priors outperform their diffusion counterparts - the SDS and VSD losses - in text-to-3D generation. Our method also displays competitive performance in image inversion and editing.","sentences":["Large-scale diffusion models have achieved remarkable performance in generative tasks.","Beyond their initial training applications, these models have proven their ability to function as versatile plug-and-play priors.","For instance, 2D diffusion models can serve as loss functions to optimize 3D implicit models.","Rectified flow, a novel class of generative models, enforces a linear progression from the source to the target distribution and has demonstrated superior performance across various domains.","Compared to diffusion-based methods, rectified flow approaches surpass in terms of generation quality and efficiency, requiring fewer inference steps.","In this work, we present theoretical and experimental evidence demonstrating that rectified flow based methods offer similar functionalities to diffusion models - they can also serve as effective priors.","Besides the generative capabilities of diffusion priors, motivated by the unique time-symmetry properties of rectified flow models, a variant of our method can additionally perform image inversion.","Experimentally, our rectified flow-based priors outperform their diffusion counterparts - the SDS and VSD losses - in text-to-3D generation.","Our method also displays competitive performance in image inversion and editing."],"url":"http://arxiv.org/abs/2406.03293v1","category":"cs.CV"}
{"created":"2024-06-05 14:00:46","title":"Evaluating AI fairness in credit scoring with the BRIO tool","abstract":"We present a method for quantitative, in-depth analyses of fairness issues in AI systems with an application to credit scoring. To this aim we use BRIO, a tool for the evaluation of AI systems with respect to social unfairness and, more in general, ethically undesirable behaviours. It features a model-agnostic bias detection module, presented in \\cite{DBLP:conf/beware/CoragliaDGGPPQ23}, to which a full-fledged unfairness risk evaluation module is added. As a case study, we focus on the context of credit scoring, analysing the UCI German Credit Dataset \\cite{misc_statlog_(german_credit_data)_144}. We apply the BRIO fairness metrics to several, socially sensitive attributes featured in the German Credit Dataset, quantifying fairness across various demographic segments, with the aim of identifying potential sources of bias and discrimination in a credit scoring model. We conclude by combining our results with a revenue analysis.","sentences":["We present a method for quantitative, in-depth analyses of fairness issues in AI systems with an application to credit scoring.","To this aim we use BRIO, a tool for the evaluation of AI systems with respect to social unfairness and, more in general, ethically undesirable behaviours.","It features a model-agnostic bias detection module, presented in \\cite{DBLP:conf/beware/CoragliaDGGPPQ23}, to which a full-fledged unfairness risk evaluation module is added.","As a case study, we focus on the context of credit scoring, analysing the UCI German Credit Dataset \\cite{misc_statlog_(german_credit_data)_144}.","We apply the BRIO fairness metrics to several, socially sensitive attributes featured in the German Credit Dataset, quantifying fairness across various demographic segments, with the aim of identifying potential sources of bias and discrimination in a credit scoring model.","We conclude by combining our results with a revenue analysis."],"url":"http://arxiv.org/abs/2406.03292v1","category":"cs.AI"}
{"created":"2024-06-05 13:59:05","title":"Embarrassingly Parallel GFlowNets","abstract":"GFlowNets are a promising alternative to MCMC sampling for discrete compositional random variables. Training GFlowNets requires repeated evaluations of the unnormalized target distribution or reward function. However, for large-scale posterior sampling, this may be prohibitive since it incurs traversing the data several times. Moreover, if the data are distributed across clients, employing standard GFlowNets leads to intensive client-server communication. To alleviate both these issues, we propose embarrassingly parallel GFlowNet (EP-GFlowNet). EP-GFlowNet is a provably correct divide-and-conquer method to sample from product distributions of the form $R(\\cdot) \\propto R_1(\\cdot) ... R_N(\\cdot)$ -- e.g., in parallel or federated Bayes, where each $R_n$ is a local posterior defined on a data partition. First, in parallel, we train a local GFlowNet targeting each $R_n$ and send the resulting models to the server. Then, the server learns a global GFlowNet by enforcing our newly proposed \\emph{aggregating balance} condition, requiring a single communication step. Importantly, EP-GFlowNets can also be applied to multi-objective optimization and model reuse. Our experiments illustrate the EP-GFlowNets's effectiveness on many tasks, including parallel Bayesian phylogenetics, multi-objective multiset, sequence generation, and federated Bayesian structure learning.","sentences":["GFlowNets are a promising alternative to MCMC sampling for discrete compositional random variables.","Training GFlowNets requires repeated evaluations of the unnormalized target distribution or reward function.","However, for large-scale posterior sampling, this may be prohibitive since it incurs traversing the data several times.","Moreover, if the data are distributed across clients, employing standard GFlowNets leads to intensive client-server communication.","To alleviate both these issues, we propose embarrassingly parallel GFlowNet (EP-GFlowNet).","EP-GFlowNet is a provably correct divide-and-conquer method to sample from product distributions of the form $R(\\cdot) \\propto R_1(\\cdot) ...","R_N(\\cdot)$ -- e.g., in parallel or federated Bayes, where each $R_n$ is a local posterior defined on a data partition.","First, in parallel, we train a local GFlowNet targeting each $R_n$ and send the resulting models to the server.","Then, the server learns a global GFlowNet by enforcing our newly proposed \\emph{aggregating balance} condition, requiring a single communication step.","Importantly, EP-GFlowNets can also be applied to multi-objective optimization and model reuse.","Our experiments illustrate the EP-GFlowNets's effectiveness on many tasks, including parallel Bayesian phylogenetics, multi-objective multiset, sequence generation, and federated Bayesian structure learning."],"url":"http://arxiv.org/abs/2406.03288v1","category":"cs.LG"}
{"created":"2024-06-05 13:59:03","title":"SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms","abstract":"Towards energy-efficient artificial intelligence similar to the human brain, the bio-inspired spiking neural networks (SNNs) have advantages of biological plausibility, event-driven sparsity, and binary activation. Recently, large-scale language models exhibit promising generalization capability, making it a valuable issue to explore more general spike-driven models. However, the binary spikes in existing SNNs fail to encode adequate semantic information, placing technological challenges for generalization. This work proposes the first fully spiking mechanism for general language tasks, including both discriminative and generative ones. Different from previous spikes with {0,1} levels, we propose a more general spike formulation with bi-directional, elastic amplitude, and elastic frequency encoding, while still maintaining the addition nature of SNNs. In a single time step, the spike is enhanced by direction and amplitude information; in spike frequency, a strategy to control spike firing rate is well designed. We plug this elastic bi-spiking mechanism in language modeling, named SpikeLM. It is the first time to handle general language tasks with fully spike-driven models, which achieve much higher accuracy than previously possible. SpikeLM also greatly bridges the performance gap between SNNs and ANNs in language modeling. Our code is available at https://github.com/Xingrun-Xing/SpikeLM.","sentences":["Towards energy-efficient artificial intelligence similar to the human brain, the bio-inspired spiking neural networks (SNNs) have advantages of biological plausibility, event-driven sparsity, and binary activation.","Recently, large-scale language models exhibit promising generalization capability, making it a valuable issue to explore more general spike-driven models.","However, the binary spikes in existing SNNs fail to encode adequate semantic information, placing technological challenges for generalization.","This work proposes the first fully spiking mechanism for general language tasks, including both discriminative and generative ones.","Different from previous spikes with {0,1} levels, we propose a more general spike formulation with bi-directional, elastic amplitude, and elastic frequency encoding, while still maintaining the addition nature of SNNs.","In a single time step, the spike is enhanced by direction and amplitude information; in spike frequency, a strategy to control spike firing rate is well designed.","We plug this elastic bi-spiking mechanism in language modeling, named SpikeLM.","It is the first time to handle general language tasks with fully spike-driven models, which achieve much higher accuracy than previously possible.","SpikeLM also greatly bridges the performance gap between SNNs and ANNs in language modeling.","Our code is available at https://github.com/Xingrun-Xing/SpikeLM."],"url":"http://arxiv.org/abs/2406.03287v1","category":"cs.NE"}
{"created":"2024-06-05 13:57:50","title":"Exponential Consensus Formation in Time-Varying Multiagent Systems via Compactification Methods","abstract":"In this article, we establish exponential contraction results for the diameter and variance of general first-order multiagent systems. Our approach is based on compactification techniques, and works under rather mild assumptions. Namely, we posit that either the scrambling coefficient, or the algebraic connectivity of the averaged interaction graphs of the system over all time windows of a given length are uniformly positive.","sentences":["In this article, we establish exponential contraction results for the diameter and variance of general first-order multiagent systems.","Our approach is based on compactification techniques, and works under rather mild assumptions.","Namely, we posit that either the scrambling coefficient, or the algebraic connectivity of the averaged interaction graphs of the system over all time windows of a given length are uniformly positive."],"url":"http://arxiv.org/abs/2406.03286v1","category":"math.OC"}
{"created":"2024-06-05 13:56:43","title":"Geometric inequalities for quasi-Einstein manifolds","abstract":"In this article, we investigate some geometric inequalities for quasi-Einstein manifolds. We use the generalized Reilly's formulas by Qiu-Xia and Li-Xia to establish new boundary estimates and an isoperimetric type inequality for compact quasi-Einstein manifolds with boundary. Boundary estimates in terms of the first eigenvalue of the Jacobi operator and the Hawking mass are also established. In particular, we present a Heintze-Karcher type inequality for a compact domain on a quasi-Einstein manifold.","sentences":["In this article, we investigate some geometric inequalities for quasi-Einstein manifolds.","We use the generalized Reilly's formulas by Qiu-Xia and Li-Xia to establish new boundary estimates and an isoperimetric type inequality for compact quasi-Einstein manifolds with boundary.","Boundary estimates in terms of the first eigenvalue of the Jacobi operator and the Hawking mass are also established.","In particular, we present a Heintze-Karcher type inequality for a compact domain on a quasi-Einstein manifold."],"url":"http://arxiv.org/abs/2406.03284v1","category":"math.DG"}
{"created":"2024-06-05 13:56:42","title":"Enhancing Repository-Level Code Generation with Integrated Contextual Information","abstract":"Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, repository-level code generation presents unique challenges, particularly due to the need to utilize information spread across multiple files within a repository. Existing retrieval-based approaches sometimes fall short as they are limited in obtaining a broader and deeper repository context. In this paper, we present CatCoder, a novel code generation framework designed for statically typed programming languages. CatCoder enhances repository-level code generation by integrating relevant code and type context. Specifically, it leverages static analyzers to extract type dependencies and merges this information with retrieved code to create comprehensive prompts for LLMs. To evaluate the effectiveness of CatCoder, we adapt and construct benchmarks that include 199 Java tasks and 90 Rust tasks. The results show that CatCoder outperforms the RepoCoder baseline by up to 17.35%, in terms of pass@k score. Furthermore, the generalizability of CatCoder is assessed using various LLMs, including both code-specialized models and general-purpose models. Our findings indicate consistent performance improvements across all models, which underlines the practicality of CatCoder.","sentences":["Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks.","However, repository-level code generation presents unique challenges, particularly due to the need to utilize information spread across multiple files within a repository.","Existing retrieval-based approaches sometimes fall short as they are limited in obtaining a broader and deeper repository context.","In this paper, we present CatCoder, a novel code generation framework designed for statically typed programming languages.","CatCoder enhances repository-level code generation by integrating relevant code and type context.","Specifically, it leverages static analyzers to extract type dependencies and merges this information with retrieved code to create comprehensive prompts for LLMs.","To evaluate the effectiveness of CatCoder, we adapt and construct benchmarks that include 199 Java tasks and 90 Rust tasks.","The results show that CatCoder outperforms the RepoCoder baseline by up to 17.35%, in terms of pass@k score.","Furthermore, the generalizability of CatCoder is assessed using various LLMs, including both code-specialized models and general-purpose models.","Our findings indicate consistent performance improvements across all models, which underlines the practicality of CatCoder."],"url":"http://arxiv.org/abs/2406.03283v1","category":"cs.SE"}
{"created":"2024-06-05 13:54:28","title":"FusionBench: A Comprehensive Benchmark of Deep Model Fusion","abstract":"Deep model fusion is an emerging technique that unifies the predictions or parameters of several deep neural networks into a single model in a cost-effective and data-efficient manner. This enables the unified model to take advantage of the original models' strengths, potentially exceeding their performance. Although a variety of deep model fusion techniques have been introduced, their evaluations tend to be inconsistent and often inadequate to validate their effectiveness and robustness against distribution shifts. To address this issue, we introduce FusionBench, which is the first comprehensive benchmark dedicated to deep model fusion. FusionBench covers a wide range of tasks, including open-vocabulary image classification, text classification, and text-to-text generation. Each category includes up to eight tasks with corresponding task-specific models, featuring both full fine-tuning and LoRA fine-tuning, as well as models of different sizes, to ensure fair and balanced comparisons of various multi-task model fusion techniques across different tasks, model scales, and fine-tuning strategies. We implement and evaluate a broad spectrum of deep model fusion techniques. These techniques range from model ensemble methods, which combine the predictions to improve the overall performance, to model merging, which integrates different models into a single one, and model mixing methods, which upscale or recombine the components of the original models. FusionBench now contains 26 distinct tasks, 74 fine-tuned models, and 16 fusion techniques, and we are committed to consistently expanding the benchmark with more tasks, models, and fusion techniques. In addition, we offer a well-documented set of resources and guidelines to aid researchers in understanding and replicating the benchmark results. Homepage https://tanganke.github.io/fusion_bench/","sentences":["Deep model fusion is an emerging technique that unifies the predictions or parameters of several deep neural networks into a single model in a cost-effective and data-efficient manner.","This enables the unified model to take advantage of the original models' strengths, potentially exceeding their performance.","Although a variety of deep model fusion techniques have been introduced, their evaluations tend to be inconsistent and often inadequate to validate their effectiveness and robustness against distribution shifts.","To address this issue, we introduce FusionBench, which is the first comprehensive benchmark dedicated to deep model fusion.","FusionBench covers a wide range of tasks, including open-vocabulary image classification, text classification, and text-to-text generation.","Each category includes up to eight tasks with corresponding task-specific models, featuring both full fine-tuning and LoRA fine-tuning, as well as models of different sizes, to ensure fair and balanced comparisons of various multi-task model fusion techniques across different tasks, model scales, and fine-tuning strategies.","We implement and evaluate a broad spectrum of deep model fusion techniques.","These techniques range from model ensemble methods, which combine the predictions to improve the overall performance, to model merging, which integrates different models into a single one, and model mixing methods, which upscale or recombine the components of the original models.","FusionBench now contains 26 distinct tasks, 74 fine-tuned models, and 16 fusion techniques, and we are committed to consistently expanding the benchmark with more tasks, models, and fusion techniques.","In addition, we offer a well-documented set of resources and guidelines to aid researchers in understanding and replicating the benchmark results.","Homepage https://tanganke.github.io/fusion_bench/"],"url":"http://arxiv.org/abs/2406.03280v1","category":"cs.LG"}
{"created":"2024-06-05 13:54:22","title":"Effective Quantum Gravitational Collapse in a Polymer Framework","abstract":"We study how the presence of an area gap, different than zero, affects the gravitational collapse of a dust ball. The implementation of such discreteness is achieved through the framework of polymer quantization, a scheme inspired by loop quantum gravity (LQG). We study the collapse using variables which represent the area, in order to impose the non-zero area gap condition. The collapse is analyzed for both the flat and spherical Oppenheimer-Snyder models. In both scenarios the formation of the singularity is avoided, due to the inversion of the velocity at finite values of the sphere surface. This happens due to the presence of a negative pressure, with origins at a quantum level. When the inversion happens inside the black hole event horizon, we achieve a geometry transition to a white hole. When the inversion happens outside the event horizon, we find a new possible astrophysical object. A characterization of such hypothetical object is done. Some constraints on the value for the area gap are also imposed in order to maintain the link with our already established physical theories.","sentences":["We study how the presence of an area gap, different than zero, affects the gravitational collapse of a dust ball.","The implementation of such discreteness is achieved through the framework of polymer quantization, a scheme inspired by loop quantum gravity (LQG).","We study the collapse using variables which represent the area, in order to impose the non-zero area gap condition.","The collapse is analyzed for both the flat and spherical Oppenheimer-Snyder models.","In both scenarios the formation of the singularity is avoided, due to the inversion of the velocity at finite values of the sphere surface.","This happens due to the presence of a negative pressure, with origins at a quantum level.","When the inversion happens inside the black hole event horizon, we achieve a geometry transition to a white hole.","When the inversion happens outside the event horizon, we find a new possible astrophysical object.","A characterization of such hypothetical object is done.","Some constraints on the value for the area gap are also imposed in order to maintain the link with our already established physical theories."],"url":"http://arxiv.org/abs/2406.03279v1","category":"gr-qc"}
{"created":"2024-06-05 13:53:47","title":"Using GNN property predictors as molecule generators","abstract":"Graph neural networks (GNNs) have emerged as powerful tools to accurately predict materials and molecular properties in computational discovery pipelines. In this article, we exploit the invertible nature of these neural networks to directly generate molecular structures with desired electronic properties. Starting from a random graph or an existing molecule, we perform a gradient ascent while holding the GNN weights fixed in order to optimize its input, the molecular graph, towards the target property. Valence rules are enforced strictly through a judicious graph construction. The method relies entirely on the property predictor; no additional training is required on molecular structures. We demonstrate the application of this method by generating molecules with specific DFT-verified energy gaps and octanol-water partition coefficients (logP). Our approach hits target properties with rates comparable to or better than state-of-the-art generative models while consistently generating more diverse molecules.","sentences":["Graph neural networks (GNNs) have emerged as powerful tools to accurately predict materials and molecular properties in computational discovery pipelines.","In this article, we exploit the invertible nature of these neural networks to directly generate molecular structures with desired electronic properties.","Starting from a random graph or an existing molecule, we perform a gradient ascent while holding the GNN weights fixed in order to optimize its input, the molecular graph, towards the target property.","Valence rules are enforced strictly through a judicious graph construction.","The method relies entirely on the property predictor; no additional training is required on molecular structures.","We demonstrate the application of this method by generating molecules with specific DFT-verified energy gaps and octanol-water partition coefficients (logP).","Our approach hits target properties with rates comparable to or better than state-of-the-art generative models while consistently generating more diverse molecules."],"url":"http://arxiv.org/abs/2406.03278v1","category":"cs.LG"}
{"created":"2024-06-05 13:53:32","title":"Turbulent relaxation patterns in SOL plasma","abstract":"Relaxations of localized over-density in a plane transverse to the magnetic field are numericallyinvestigated under the effect of drift-wave and interchange drives in SOL conditions. Such a controlleddeparture from thermodynamic equilibrium allows the investigation of fundamental processesat play in cross-field transport. Interchange instabilities generate ballistic outward radial flux withlow amplitude zonal flow patterns, whereas drift-wave instabilities result in symmetric radial fluxwith large amplitude zonal flow patterns. When both instabilities are considered, the combinedeffects tend to favor drift-waves, leading to a weaker outward flux with larger zonal flow patterns.","sentences":["Relaxations of localized over-density in a plane transverse to the magnetic field are numericallyinvestigated under the effect of drift-wave and interchange drives in SOL conditions.","Such a controlleddeparture from thermodynamic equilibrium allows the investigation of fundamental processesat play in cross-field transport.","Interchange instabilities generate ballistic outward radial flux withlow amplitude zonal flow patterns, whereas drift-wave instabilities result in symmetric radial fluxwith large amplitude zonal flow patterns.","When both instabilities are considered, the combinedeffects tend to favor drift-waves, leading to a weaker outward flux with larger zonal flow patterns."],"url":"http://arxiv.org/abs/2406.03277v1","category":"physics.plasm-ph"}
{"created":"2024-06-05 13:53:20","title":"Revisiting Scalable Hessian Diagonal Approximations for Applications in Reinforcement Learning","abstract":"Second-order information is valuable for many applications but challenging to compute. Several works focus on computing or approximating Hessian diagonals, but even this simplification introduces significant additional costs compared to computing a gradient. In the absence of efficient exact computation schemes for Hessian diagonals, we revisit an early approximation scheme proposed by Becker and LeCun (1989, BL89), which has a cost similar to gradients and appears to have been overlooked by the community. We introduce HesScale, an improvement over BL89, which adds negligible extra computation. On small networks, we find that this improvement is of higher quality than all alternatives, even those with theoretical guarantees, such as unbiasedness, while being much cheaper to compute. We use this insight in reinforcement learning problems where small networks are used and demonstrate HesScale in second-order optimization and scaling the step-size parameter. In our experiments, HesScale optimizes faster than existing methods and improves stability through step-size scaling. These findings are promising for scaling second-order methods in larger models in the future.","sentences":["Second-order information is valuable for many applications but challenging to compute.","Several works focus on computing or approximating Hessian diagonals, but even this simplification introduces significant additional costs compared to computing a gradient.","In the absence of efficient exact computation schemes for Hessian diagonals, we revisit an early approximation scheme proposed by Becker and LeCun (1989, BL89), which has a cost similar to gradients and appears to have been overlooked by the community.","We introduce HesScale, an improvement over BL89, which adds negligible extra computation.","On small networks, we find that this improvement is of higher quality than all alternatives, even those with theoretical guarantees, such as unbiasedness, while being much cheaper to compute.","We use this insight in reinforcement learning problems where small networks are used and demonstrate HesScale in second-order optimization and scaling the step-size parameter.","In our experiments, HesScale optimizes faster than existing methods and improves stability through step-size scaling.","These findings are promising for scaling second-order methods in larger models in the future."],"url":"http://arxiv.org/abs/2406.03276v1","category":"cs.LG"}
{"created":"2024-06-05 13:52:55","title":"Enhancing CTC-based speech recognition with diverse modeling units","abstract":"In recent years, the evolution of end-to-end (E2E) automatic speech recognition (ASR) models has been remarkable, largely due to advances in deep learning architectures like transformer. On top of E2E systems, researchers have achieved substantial accuracy improvement by rescoring E2E model's N-best hypotheses with a phoneme-based model. This raises an interesting question about where the improvements come from other than the system combination effect. We examine the underlying mechanisms driving these gains and propose an efficient joint training approach, where E2E models are trained jointly with diverse modeling units. This methodology does not only align the strengths of both phoneme and grapheme-based models but also reveals that using these diverse modeling units in a synergistic way can significantly enhance model accuracy. Our findings offer new insights into the optimal integration of heterogeneous modeling units in the development of more robust and accurate ASR systems.","sentences":["In recent years, the evolution of end-to-end (E2E) automatic speech recognition (ASR) models has been remarkable, largely due to advances in deep learning architectures like transformer.","On top of E2E systems, researchers have achieved substantial accuracy improvement by rescoring E2E model's N-best hypotheses with a phoneme-based model.","This raises an interesting question about where the improvements come from other than the system combination effect.","We examine the underlying mechanisms driving these gains and propose an efficient joint training approach, where E2E models are trained jointly with diverse modeling units.","This methodology does not only align the strengths of both phoneme and grapheme-based models but also reveals that using these diverse modeling units in a synergistic way can significantly enhance model accuracy.","Our findings offer new insights into the optimal integration of heterogeneous modeling units in the development of more robust and accurate ASR systems."],"url":"http://arxiv.org/abs/2406.03274v1","category":"eess.AS"}
{"created":"2024-06-05 13:50:59","title":"Multi-Microphone Speech Emotion Recognition using the Hierarchical Token-semantic Audio Transformer Architecture","abstract":"Most emotion recognition systems fail in real-life situations (in the wild scenarios) where the audio is contaminated by reverberation. Our study explores new methods to alleviate the performance degradation of Speech Emotion Recognition (SER) algorithms and develop a more robust system for adverse conditions. We propose processing multi-microphone signals to address these challenges and improve emotion classification accuracy. We adopt a state-of-the-art transformer model, the Hierarchical Token-semantic Audio Transformer (HTS-AT), to handle multi-channel audio inputs. We evaluate two strategies: averaging mel-spectrograms across channels and summing patch-embedded representations. Our multimicrophone model achieves superior performance compared to single-channel baselines when tested on real-world reverberant environments.","sentences":["Most emotion recognition systems fail in real-life situations (in the wild scenarios) where the audio is contaminated by reverberation.","Our study explores new methods to alleviate the performance degradation of Speech Emotion Recognition (SER) algorithms and develop a more robust system for adverse conditions.","We propose processing multi-microphone signals to address these challenges and improve emotion classification accuracy.","We adopt a state-of-the-art transformer model, the Hierarchical Token-semantic Audio Transformer (HTS-AT), to handle multi-channel audio inputs.","We evaluate two strategies: averaging mel-spectrograms across channels and summing patch-embedded representations.","Our multimicrophone model achieves superior performance compared to single-channel baselines when tested on real-world reverberant environments."],"url":"http://arxiv.org/abs/2406.03272v1","category":"eess.AS"}
{"created":"2024-06-05 13:50:29","title":"Image Copy-Move Forgery Detection and Localization Scheme: How to Avoid Missed Detection and False Alarm","abstract":"Image copy-move is an operation that replaces one part of the image with another part of the same image, which can be used for illegal purposes due to the potential semantic changes. Recent studies have shown that keypoint-based algorithms achieved excellent and robust localization performance even when small or smooth tampered areas were involved. However, when the input image is low-resolution, most existing keypoint-based algorithms are difficult to generate sufficient keypoints, resulting in more missed detections. In addition, existing algorithms are usually unable to distinguish between Similar but Genuine Objects (SGO) images and tampered images, resulting in more false alarms. This is mainly due to the lack of further verification of local homography matrix in forgery localization stage. To tackle these problems, this paper firstly proposes an excessive keypoint extraction strategy to overcome missed detection. Subsequently, a group matching algorithm is used to speed up the matching of excessive keypoints. Finally, a new iterative forgery localization algorithm is introduced to quickly form pixel-level localization results while ensuring a lower false alarm. Extensive experimental results show that our scheme has superior performance than state-of-the-art algorithms in overcoming missed detection and false alarm. Our code is available at https://github.com/LUZW1998/CMFDL.","sentences":["Image copy-move is an operation that replaces one part of the image with another part of the same image, which can be used for illegal purposes due to the potential semantic changes.","Recent studies have shown that keypoint-based algorithms achieved excellent and robust localization performance even when small or smooth tampered areas were involved.","However, when the input image is low-resolution, most existing keypoint-based algorithms are difficult to generate sufficient keypoints, resulting in more missed detections.","In addition, existing algorithms are usually unable to distinguish between Similar but Genuine Objects (SGO) images and tampered images, resulting in more false alarms.","This is mainly due to the lack of further verification of local homography matrix in forgery localization stage.","To tackle these problems, this paper firstly proposes an excessive keypoint extraction strategy to overcome missed detection.","Subsequently, a group matching algorithm is used to speed up the matching of excessive keypoints.","Finally, a new iterative forgery localization algorithm is introduced to quickly form pixel-level localization results while ensuring a lower false alarm.","Extensive experimental results show that our scheme has superior performance than state-of-the-art algorithms in overcoming missed detection and false alarm.","Our code is available at https://github.com/LUZW1998/CMFDL."],"url":"http://arxiv.org/abs/2406.03271v1","category":"cs.CV"}
{"created":"2024-06-05 13:49:18","title":"A Successive Gap Constraint Linearization Method for Optimal Control Problems with Equilibrium Constraints","abstract":"In this study, we propose a novel gap-constraint-based reformulation for optimal control problems with equilibrium constraints (OCPECs). We show that the proposed reformulation generates a new constraint system equivalent to the original one but more concise and with favorable differentiability. Moreover, constraint regularity can be recovered by a relaxation strategy. We show that the gap constraint and its gradient can be evaluated efficiently. We then propose a successive gap constraint linearization method to solve the discretized OCPEC. We also provide an intuitive geometric interpretation of the gap constraint. Numerical experiments validate the effectiveness of the proposed reformulation and solution method.","sentences":["In this study, we propose a novel gap-constraint-based reformulation for optimal control problems with equilibrium constraints (OCPECs).","We show that the proposed reformulation generates a new constraint system equivalent to the original one but more concise and with favorable differentiability.","Moreover, constraint regularity can be recovered by a relaxation strategy.","We show that the gap constraint and its gradient can be evaluated efficiently.","We then propose a successive gap constraint linearization method to solve the discretized OCPEC.","We also provide an intuitive geometric interpretation of the gap constraint.","Numerical experiments validate the effectiveness of the proposed reformulation and solution method."],"url":"http://arxiv.org/abs/2406.03270v1","category":"math.OC"}
{"created":"2024-06-05 13:41:26","title":"No-Regret Algorithms for Safe Bayesian Optimization with Monotonicity Constraints","abstract":"We consider the problem of sequentially maximizing an unknown function $f$ over a set of actions of the form $(s,\\mathbf{x})$, where the selected actions must satisfy a safety constraint with respect to an unknown safety function $g$. We model $f$ and $g$ as lying in a reproducing kernel Hilbert space (RKHS), which facilitates the use of Gaussian process methods. While existing works for this setting have provided algorithms that are guaranteed to identify a near-optimal safe action, the problem of attaining low cumulative regret has remained largely unexplored, with a key challenge being that expanding the safe region can incur high regret. To address this challenge, we show that if $g$ is monotone with respect to just the single variable $s$ (with no such constraint on $f$), sublinear regret becomes achievable with our proposed algorithm. In addition, we show that a modified version of our algorithm is able to attain sublinear regret (for suitably defined notions of regret) for the task of finding a near-optimal $s$ corresponding to every $\\mathbf{x}$, as opposed to only finding the global safe optimum. Our findings are supported with empirical evaluations on various objective and safety functions.","sentences":["We consider the problem of sequentially maximizing an unknown function $f$ over a set of actions of the form $(s,\\mathbf{x})$, where the selected actions must satisfy a safety constraint with respect to an unknown safety function $g$. We model $f$ and $g$ as lying in a reproducing kernel Hilbert space (RKHS), which facilitates the use of Gaussian process methods.","While existing works for this setting have provided algorithms that are guaranteed to identify a near-optimal safe action, the problem of attaining low cumulative regret has remained largely unexplored, with a key challenge being that expanding the safe region can incur high regret.","To address this challenge, we show that if $g$ is monotone with respect to just the single variable $s$ (with no such constraint on $f$), sublinear regret becomes achievable with our proposed algorithm.","In addition, we show that a modified version of our algorithm is able to attain sublinear regret (for suitably defined notions of regret) for the task of finding a near-optimal $s$ corresponding to every $\\mathbf{x}$, as opposed to only finding the global safe optimum.","Our findings are supported with empirical evaluations on various objective and safety functions."],"url":"http://arxiv.org/abs/2406.03264v1","category":"stat.ML"}
{"created":"2024-06-05 13:41:09","title":"Deep Generative Models for Proton Zero Degree Calorimeter Simulations in ALICE, CERN","abstract":"Simulating detector responses is a crucial part of understanding the inner-workings of particle collisions in the Large Hadron Collider at CERN. The current reliance on statistical Monte-Carlo simulations strains CERN's computational grid, underscoring the urgency for more efficient alternatives. Addressing these challenges, recent proposals advocate for generative machine learning methods. In this study, we present an innovative deep learning simulation approach tailored for the proton Zero Degree Calorimeter in the ALICE experiment. Leveraging a Generative Adversarial Network model with Selective Diversity Increase loss, we directly simulate calorimeter responses. To enhance its capabilities in modeling a broad range of calorimeter response intensities, we expand the SDI-GAN architecture with additional regularization. Moreover, to improve the spatial fidelity of the generated data, we introduce an auxiliary regressor network. Our method offers a significant speedup when comparing to the traditional Monte-Carlo based approaches.","sentences":["Simulating detector responses is a crucial part of understanding the inner-workings of particle collisions in the Large Hadron Collider at CERN.","The current reliance on statistical Monte-Carlo simulations strains CERN's computational grid, underscoring the urgency for more efficient alternatives.","Addressing these challenges, recent proposals advocate for generative machine learning methods.","In this study, we present an innovative deep learning simulation approach tailored for the proton Zero Degree Calorimeter in the ALICE experiment.","Leveraging a Generative Adversarial Network model with Selective Diversity Increase loss, we directly simulate calorimeter responses.","To enhance its capabilities in modeling a broad range of calorimeter response intensities, we expand the SDI-GAN architecture with additional regularization.","Moreover, to improve the spatial fidelity of the generated data, we introduce an auxiliary regressor network.","Our method offers a significant speedup when comparing to the traditional Monte-Carlo based approaches."],"url":"http://arxiv.org/abs/2406.03263v1","category":"cs.LG"}
{"created":"2024-06-05 13:38:59","title":"Baryogenesis via QCD preheating with nonadiabatic baryon chemical potential","abstract":"The chiral phase transition in QCD can be supercooled in the thermal history of the universe to be instantaneously out-of equilibrium, if QCD is coupled to a dark QCD sector exhibiting the dark chiral phase transition of the first order. In that case the QCD sigma meson field (as the chiral order parameter, or the light quark condensate) starts to roll in a nonadiabatic way down to the true QCD vacuum. Meanwhile a dynamic baryonic chemical potential can be generated solely within QCD, which is governed by the dynamic motion of the QCD sigma meson field, analogously to the spontaneous baryogenesis or the leptogenesis via the Higgs or axionlike relaxation scenario. When QCD is further allowed to communicate with a dark fermion with mass of order of 1 GeV and the baryon number violating coupling to neutron, the nonadiabatic QCD sigma motion along with the nonadiabatic baryon chemical potential can trigger the preheating and produce the baryon number asymmetry. We discuss this scenario in details to find that the QCD-induced dynamic baryon chemical potential plays a significant role for the QCD preheating and the baryogenesis, which yields the desired amount of the asymmetry today consistently with current astrophysical, cosmological, and terrestrial experimental constraints. Cosmological and phenomenological consequences characteristic to the present scenario are also addressed.","sentences":["The chiral phase transition in QCD can be supercooled in the thermal history of the universe to be instantaneously out-of equilibrium, if QCD is coupled to a dark QCD sector exhibiting the dark chiral phase transition of the first order.","In that case the QCD sigma meson field (as the chiral order parameter, or the light quark condensate) starts to roll in a nonadiabatic way down to the true QCD vacuum.","Meanwhile a dynamic baryonic chemical potential can be generated solely within QCD, which is governed by the dynamic motion of the QCD sigma meson field, analogously to the spontaneous baryogenesis or the leptogenesis via the Higgs or axionlike relaxation scenario.","When QCD is further allowed to communicate with a dark fermion with mass of order of 1 GeV and the baryon number violating coupling to neutron, the nonadiabatic QCD sigma motion along with the nonadiabatic baryon chemical potential can trigger the preheating and produce the baryon number asymmetry.","We discuss this scenario in details to find that the QCD-induced dynamic baryon chemical potential plays a significant role for the QCD preheating and the baryogenesis, which yields the desired amount of the asymmetry today consistently with current astrophysical, cosmological, and terrestrial experimental constraints.","Cosmological and phenomenological consequences characteristic to the present scenario are also addressed."],"url":"http://arxiv.org/abs/2406.03261v1","category":"hep-ph"}
{"created":"2024-06-05 13:36:06","title":"Quantum Sensing from Gravity as Universal Dephasing Channel for Qubits","abstract":"We investigate the interaction of a transmon qubit with a classical gravitational field. Exploiting the generic phenomena of the gravitational redshift and Aharonov-Bohm phase, we show that entangled quantum states dephase with a universal rate. The gravitational phase shift is expressed in terms of a quantum computing noise channel. We give a measurement protocol based on a modified phase estimation algorithm which is linear in the phase drift, which is optimal for measuring the small phase that is acquired from the gravitation channel. Additionally, we propose qubit-based platforms as quantum sensors for precision gravitometers and mechanical strain gauges as an example of this phenomenon's utility. We estimate a sensitivity for measuring the local gravitational acceleration to be $\\delta g/g \\sim 10^{-7}$. This paper demonstrates that classical gravitation has a non-trivial influence on quantum computing hardware, and provides an illustration of how quantum computing hardware may be utilized for purposes other than computation. While we focus on superconducting qubits, we point the universal nature of gravitational phase effects for all quantum platforms.","sentences":["We investigate the interaction of a transmon qubit with a classical gravitational field.","Exploiting the generic phenomena of the gravitational redshift and Aharonov-Bohm phase, we show that entangled quantum states dephase with a universal rate.","The gravitational phase shift is expressed in terms of a quantum computing noise channel.","We give a measurement protocol based on a modified phase estimation algorithm which is linear in the phase drift, which is optimal for measuring the small phase that is acquired from the gravitation channel.","Additionally, we propose qubit-based platforms as quantum sensors for precision gravitometers and mechanical strain gauges as an example of this phenomenon's utility.","We estimate a sensitivity for measuring the local gravitational acceleration to be $\\delta g/g \\sim 10^{-7}$.","This paper demonstrates that classical gravitation has a non-trivial influence on quantum computing hardware, and provides an illustration of how quantum computing hardware may be utilized for purposes other than computation.","While we focus on superconducting qubits, we point the universal nature of gravitational phase effects for all quantum platforms."],"url":"http://arxiv.org/abs/2406.03256v1","category":"quant-ph"}
{"created":"2024-06-05 13:31:30","title":"Exploring Higher Order Structures in Graph Explanantions","abstract":"Recent advancements in graph learning contributed to explaining predictions generated by Graph Neural Networks. However, existing methodologies often fall short when applied to real-world datasets. We introduce HOGE, a framework to capture higher-order structures using cell complexes, which excel at modeling higher-order relationships. In the real world, higher-order structures are ubiquitous like in molecules or social networks, thus our work significantly enhances the practical applicability of graph explanations. HOGE produces clearer and more accurate explanations compared to prior methods. Our method can be integrated with all existing graph explainers, ensuring seamless integration into current frameworks. We evaluate on GraphXAI benchmark datasets, HOGE achieves improved or comparable performance with minimal computational overhead. Ablation studies show that the performance gain observed can be attributed to the higher-order structures that come from introducing cell complexes.","sentences":["Recent advancements in graph learning contributed to explaining predictions generated by Graph Neural Networks.","However, existing methodologies often fall short when applied to real-world datasets.","We introduce HOGE, a framework to capture higher-order structures using cell complexes, which excel at modeling higher-order relationships.","In the real world, higher-order structures are ubiquitous like in molecules or social networks, thus our work significantly enhances the practical applicability of graph explanations.","HOGE produces clearer and more accurate explanations compared to prior methods.","Our method can be integrated with all existing graph explainers, ensuring seamless integration into current frameworks.","We evaluate on GraphXAI benchmark datasets, HOGE achieves improved or comparable performance with minimal computational overhead.","Ablation studies show that the performance gain observed can be attributed to the higher-order structures that come from introducing cell complexes."],"url":"http://arxiv.org/abs/2406.03253v1","category":"cs.LG"}
{"created":"2024-06-05 13:28:28","title":"ASoBO: Attentive Beamformer Selection for Distant Speaker Diarization in Meetings","abstract":"Speaker Diarization (SD) aims at grouping speech segments that belong to the same speaker. This task is required in many speech-processing applications, such as rich meeting transcription. In this context, distant microphone arrays usually capture the audio signal. Beamforming, i.e., spatial filtering, is a common practice to process multi-microphone audio data. However, it often requires an explicit localization of the active source to steer the filter. This paper proposes a self-attention-based algorithm to select the output of a bank of fixed spatial filters. This method serves as a feature extractor for joint Voice Activity (VAD) and Overlapped Speech Detection (OSD). The speaker diarization is then inferred from the detected segments. The approach shows convincing distant VAD, OSD, and SD performance, e.g. 14.5% DER on the AISHELL-4 dataset. The analysis of the self-attention weights demonstrates their explainability, as they correlate with the speaker's angular locations.","sentences":["Speaker Diarization (SD) aims at grouping speech segments that belong to the same speaker.","This task is required in many speech-processing applications, such as rich meeting transcription.","In this context, distant microphone arrays usually capture the audio signal.","Beamforming, i.e., spatial filtering, is a common practice to process multi-microphone audio data.","However, it often requires an explicit localization of the active source to steer the filter.","This paper proposes a self-attention-based algorithm to select the output of a bank of fixed spatial filters.","This method serves as a feature extractor for joint Voice Activity (VAD) and Overlapped Speech Detection (OSD).","The speaker diarization is then inferred from the detected segments.","The approach shows convincing distant VAD, OSD, and SD performance, e.g. 14.5% DER on the AISHELL-4 dataset.","The analysis of the self-attention weights demonstrates their explainability, as they correlate with the speaker's angular locations."],"url":"http://arxiv.org/abs/2406.03251v1","category":"cs.SD"}
{"created":"2024-06-05 13:26:30","title":"Prompt-based Visual Alignment for Zero-shot Policy Transfer","abstract":"Overfitting in RL has become one of the main obstacles to applications in reinforcement learning(RL). Existing methods do not provide explicit semantic constrain for the feature extractor, hindering the agent from learning a unified cross-domain representation and resulting in performance degradation on unseen domains. Besides, abundant data from multiple domains are needed. To address these issues, in this work, we propose prompt-based visual alignment (PVA), a robust framework to mitigate the detrimental domain bias in the image for zero-shot policy transfer. Inspired that Visual-Language Model (VLM) can serve as a bridge to connect both text space and image space, we leverage the semantic information contained in a text sequence as an explicit constraint to train a visual aligner. Thus, the visual aligner can map images from multiple domains to a unified domain and achieve good generalization performance. To better depict semantic information, prompt tuning is applied to learn a sequence of learnable tokens. With explicit constraints of semantic information, PVA can learn unified cross-domain representation under limited access to cross-domain data and achieves great zero-shot generalization ability in unseen domains. We verify PVA on a vision-based autonomous driving task with CARLA simulator. Experiments show that the agent generalizes well on unseen domains under limited access to multi-domain data.","sentences":["Overfitting in RL has become one of the main obstacles to applications in reinforcement learning(RL).","Existing methods do not provide explicit semantic constrain for the feature extractor, hindering the agent from learning a unified cross-domain representation and resulting in performance degradation on unseen domains.","Besides, abundant data from multiple domains are needed.","To address these issues, in this work, we propose prompt-based visual alignment (PVA), a robust framework to mitigate the detrimental domain bias in the image for zero-shot policy transfer.","Inspired that Visual-Language Model (VLM) can serve as a bridge to connect both text space and image space, we leverage the semantic information contained in a text sequence as an explicit constraint to train a visual aligner.","Thus, the visual aligner can map images from multiple domains to a unified domain and achieve good generalization performance.","To better depict semantic information, prompt tuning is applied to learn a sequence of learnable tokens.","With explicit constraints of semantic information, PVA can learn unified cross-domain representation under limited access to cross-domain data and achieves great zero-shot generalization ability in unseen domains.","We verify PVA on a vision-based autonomous driving task with CARLA simulator.","Experiments show that the agent generalizes well on unseen domains under limited access to multi-domain data."],"url":"http://arxiv.org/abs/2406.03250v1","category":"cs.CV"}
{"created":"2024-06-05 13:22:09","title":"Genuine-Focused Learning using Mask AutoEncoder for Generalized Fake Audio Detection","abstract":"The generalization of Fake Audio Detection (FAD) is critical due to the emergence of new spoofing techniques. Traditional FAD methods often focus solely on distinguishing between genuine and known spoofed audio. We propose a Genuine-Focused Learning (GFL) framework guided, aiming for highly generalized FAD, called GFL-FAD. This method incorporates a Counterfactual Reasoning Enhanced Representation (CRER) based on audio reconstruction using the Mask AutoEncoder (MAE) architecture to accurately model genuine audio features. To reduce the influence of spoofed audio during training, we introduce a genuine audio reconstruction loss, maintaining the focus on learning genuine data features. In addition, content-related bottleneck (BN) features are extracted from the MAE to supplement the knowledge of the original audio. These BN features are adaptively fused with CRER to further improve robustness. Our method achieves state-of-the-art performance with an EER of 0.25% on ASVspoof2019 LA.","sentences":["The generalization of Fake Audio Detection (FAD) is critical due to the emergence of new spoofing techniques.","Traditional FAD methods often focus solely on distinguishing between genuine and known spoofed audio.","We propose a Genuine-Focused Learning (GFL) framework guided, aiming for highly generalized FAD, called GFL-FAD.","This method incorporates a Counterfactual Reasoning Enhanced Representation (CRER) based on audio reconstruction using the Mask AutoEncoder (MAE) architecture to accurately model genuine audio features.","To reduce the influence of spoofed audio during training, we introduce a genuine audio reconstruction loss, maintaining the focus on learning genuine data features.","In addition, content-related bottleneck (BN) features are extracted from the MAE to supplement the knowledge of the original audio.","These BN features are adaptively fused with CRER to further improve robustness.","Our method achieves state-of-the-art performance with an EER of 0.25% on ASVspoof2019 LA."],"url":"http://arxiv.org/abs/2406.03247v1","category":"cs.SD"}
{"created":"2024-06-05 13:21:46","title":"Reconfiguring Participatory Design to Resist AI Realism","abstract":"The growing trend of artificial intelligence (AI) as a solution to social and technical problems reinforces AI Realism -- the belief that AI is an inevitable and natural order. In response, this paper argues that participatory design (PD), with its focus on democratic values and processes, can play a role in questioning and resisting AI Realism. I examine three concerning aspects of AI Realism: the facade of democratization that lacks true empowerment, demands for human adaptability in contrast to AI systems' inflexibility, and the obfuscation of essential human labor enabling the AI system. I propose resisting AI Realism by reconfiguring PD to continue engaging with value-centered visions, increasing its exploration of non-AI alternatives, and making the essential human labor underpinning AI systems visible. I position PD as a means to generate friction against AI Realism and open space for alternative futures centered on human needs and values.","sentences":["The growing trend of artificial intelligence (AI) as a solution to social and technical problems reinforces AI Realism -- the belief that AI is an inevitable and natural order.","In response, this paper argues that participatory design (PD), with its focus on democratic values and processes, can play a role in questioning and resisting AI Realism.","I examine three concerning aspects of AI Realism: the facade of democratization that lacks true empowerment, demands for human adaptability in contrast to AI systems' inflexibility, and the obfuscation of essential human labor enabling the AI system.","I propose resisting AI Realism by reconfiguring PD to continue engaging with value-centered visions, increasing its exploration of non-AI alternatives, and making the essential human labor underpinning AI systems visible.","I position PD as a means to generate friction against AI Realism and open space for alternative futures centered on human needs and values."],"url":"http://arxiv.org/abs/2406.03245v1","category":"cs.HC"}
{"created":"2024-06-05 13:21:40","title":"Electromagnetic (high-frequency) gravitational wave detectors: Interferometers revisited","abstract":"Increased interest in pushing the frontier of gravitational wave searches to higher frequencies (kHz and beyond) has resulted in a variety of different proposed experimental concepts. A significant fraction of them are based on the coupling between classical electromagnetism and gravity. We highlight some differences and similarities between different approaches, showcasing the rich phenomenology arising from this coupling. We use the opportunity to re-derive the response function of interferometers as the low-frequency limit of a broader picture. This article was prepared as proceedings for Moriond Cosmology 2024.","sentences":["Increased interest in pushing the frontier of gravitational wave searches to higher frequencies (kHz and beyond) has resulted in a variety of different proposed experimental concepts.","A significant fraction of them are based on the coupling between classical electromagnetism and gravity.","We highlight some differences and similarities between different approaches, showcasing the rich phenomenology arising from this coupling.","We use the opportunity to re-derive the response function of interferometers as the low-frequency limit of a broader picture.","This article was prepared as proceedings for Moriond Cosmology 2024."],"url":"http://arxiv.org/abs/2406.03244v1","category":"gr-qc"}
{"created":"2024-06-05 13:18:55","title":"Variational Pseudo Marginal Methods for Jet Reconstruction in Particle Physics","abstract":"Reconstructing jets, which provide vital insights into the properties and histories of subatomic particles produced in high-energy collisions, is a main problem in data analyses in collider physics. This intricate task deals with estimating the latent structure of a jet (binary tree) and involves parameters such as particle energy, momentum, and types. While Bayesian methods offer a natural approach for handling uncertainty and leveraging prior knowledge, they face significant challenges due to the super-exponential growth of potential jet topologies as the number of observed particles increases. To address this, we introduce a Combinatorial Sequential Monte Carlo approach for inferring jet latent structures. As a second contribution, we leverage the resulting estimator to develop a variational inference algorithm for parameter learning. Building on this, we introduce a variational family using a pseudo-marginal framework for a fully Bayesian treatment of all variables, unifying the generative model with the inference process. We illustrate our method's effectiveness through experiments using data generated with a collider physics generative model, highlighting superior speed and accuracy across a range of tasks.","sentences":["Reconstructing jets, which provide vital insights into the properties and histories of subatomic particles produced in high-energy collisions, is a main problem in data analyses in collider physics.","This intricate task deals with estimating the latent structure of a jet (binary tree) and involves parameters such as particle energy, momentum, and types.","While Bayesian methods offer a natural approach for handling uncertainty and leveraging prior knowledge, they face significant challenges due to the super-exponential growth of potential jet topologies as the number of observed particles increases.","To address this, we introduce a Combinatorial Sequential Monte Carlo approach for inferring jet latent structures.","As a second contribution, we leverage the resulting estimator to develop a variational inference algorithm for parameter learning.","Building on this, we introduce a variational family using a pseudo-marginal framework for a fully Bayesian treatment of all variables, unifying the generative model with the inference process.","We illustrate our method's effectiveness through experiments using data generated with a collider physics generative model, highlighting superior speed and accuracy across a range of tasks."],"url":"http://arxiv.org/abs/2406.03242v1","category":"cs.LG"}
{"created":"2024-06-05 13:16:55","title":"Generalized Source Tracing: Detecting Novel Audio Deepfake Algorithm with Real Emphasis and Fake Dispersion strategy","abstract":"With the proliferation of deepfake audio, there is an urgent need to investigate their attribution. Current source tracing methods can effectively distinguish in-distribution (ID) categories. However, the rapid evolution of deepfake algorithms poses a critical challenge in the accurate identification of out-of-distribution (OOD) novel deepfake algorithms. In this paper, we propose Real Emphasis and Fake Dispersion (REFD) strategy for audio deepfake algorithm recognition, demonstrating its effectiveness in discriminating ID samples while identifying OOD samples. For effective OOD detection, we first explore current post-hoc OOD methods and propose NSD, a novel OOD approach in identifying novel deepfake algorithms through the similarity consideration of both feature and logits scores. REFD achieves 86.83% F1-score as a single system in Audio Deepfake Detection Challenge 2023 Track3, showcasing its state-of-the-art performance.","sentences":["With the proliferation of deepfake audio, there is an urgent need to investigate their attribution.","Current source tracing methods can effectively distinguish in-distribution (ID) categories.","However, the rapid evolution of deepfake algorithms poses a critical challenge in the accurate identification of out-of-distribution (OOD) novel deepfake algorithms.","In this paper, we propose Real Emphasis and Fake Dispersion (REFD) strategy for audio deepfake algorithm recognition, demonstrating its effectiveness in discriminating ID samples while identifying OOD samples.","For effective OOD detection, we first explore current post-hoc OOD methods and propose NSD, a novel OOD approach in identifying novel deepfake algorithms through the similarity consideration of both feature and logits scores.","REFD achieves 86.83% F1-score as a single system in Audio Deepfake Detection Challenge 2023 Track3, showcasing its state-of-the-art performance."],"url":"http://arxiv.org/abs/2406.03240v1","category":"cs.SD"}
{"created":"2024-06-05 13:16:44","title":"The parity of Lusztig's restriction functor and Green's formula for a quiver with automorphism","abstract":"In [8], Fang-Lan-Xiao proved a formula about Lusztig's induction and restriction functors which can induce Green's formula for the path algebra of a quiver over a finite field via the trace map. In this paper, we generalize their formula to that for the mixed semisimple perverse sheaves for a quiver with an automorphism. By applying the trace map, we obtain Green's formula for any finite-dimensional hereditary algebra over a finite field.","sentences":["In [8], Fang-Lan-Xiao proved a formula about Lusztig's induction and restriction functors which can induce Green's formula for the path algebra of a quiver over a finite field via the trace map.","In this paper, we generalize their formula to that for the mixed semisimple perverse sheaves for a quiver with an automorphism.","By applying the trace map, we obtain Green's formula for any finite-dimensional hereditary algebra over a finite field."],"url":"http://arxiv.org/abs/2406.03238v1","category":"math.RT"}
{"created":"2024-06-05 13:16:31","title":"Generalized Fake Audio Detection via Deep Stable Learning","abstract":"Although current fake audio detection approaches have achieved remarkable success on specific datasets, they often fail when evaluated with datasets from different distributions. Previous studies typically address distribution shift by focusing on using extra data or applying extra loss restrictions during training. However, these methods either require a substantial amount of data or complicate the training process. In this work, we propose a stable learning-based training scheme that involves a Sample Weight Learning (SWL) module, addressing distribution shift by decorrelating all selected features via learning weights from training samples. The proposed portable plug-in-like SWL is easy to apply to multiple base models and generalizes them without using extra data during training. Experiments conducted on the ASVspoof datasets clearly demonstrate the effectiveness of SWL in generalizing different models across three evaluation datasets from different distributions.","sentences":["Although current fake audio detection approaches have achieved remarkable success on specific datasets, they often fail when evaluated with datasets from different distributions.","Previous studies typically address distribution shift by focusing on using extra data or applying extra loss restrictions during training.","However, these methods either require a substantial amount of data or complicate the training process.","In this work, we propose a stable learning-based training scheme that involves a Sample Weight Learning (SWL) module, addressing distribution shift by decorrelating all selected features via learning weights from training samples.","The proposed portable plug-in-like SWL is easy to apply to multiple base models and generalizes them without using extra data during training.","Experiments conducted on the ASVspoof datasets clearly demonstrate the effectiveness of SWL in generalizing different models across three evaluation datasets from different distributions."],"url":"http://arxiv.org/abs/2406.03237v1","category":"cs.SD"}
{"created":"2024-06-05 13:15:37","title":"Error-preserving Automatic Speech Recognition of Young English Learners' Language","abstract":"One of the central skills that language learners need to practice is speaking the language. Currently, students in school do not get enough speaking opportunities and lack conversational practice. Recent advances in speech technology and natural language processing allow for the creation of novel tools to practice their speaking skills. In this work, we tackle the first component of such a pipeline, namely, the automated speech recognition module (ASR), which faces a number of challenges: first, state-of-the-art ASR models are often trained on adult read-aloud data by native speakers and do not transfer well to young language learners' speech. Second, most ASR systems contain a powerful language model, which smooths out errors made by the speakers. To give corrective feedback, which is a crucial part of language learning, the ASR systems in our setting need to preserve the errors made by the language learners. In this work, we build an ASR system that satisfies these requirements: it works on spontaneous speech by young language learners and preserves their errors. For this, we collected a corpus containing around 85 hours of English audio spoken by learners in Switzerland from grades 4 to 6 on different language learning tasks, which we used to train an ASR model. Our experiments show that our model benefits from direct fine-tuning on children's voices and has a much higher error preservation rate than other models.","sentences":["One of the central skills that language learners need to practice is speaking the language.","Currently, students in school do not get enough speaking opportunities and lack conversational practice.","Recent advances in speech technology and natural language processing allow for the creation of novel tools to practice their speaking skills.","In this work, we tackle the first component of such a pipeline, namely, the automated speech recognition module (ASR), which faces a number of challenges: first, state-of-the-art ASR models are often trained on adult read-aloud data by native speakers and do not transfer well to young language learners' speech.","Second, most ASR systems contain a powerful language model, which smooths out errors made by the speakers.","To give corrective feedback, which is a crucial part of language learning, the ASR systems in our setting need to preserve the errors made by the language learners.","In this work, we build an ASR system that satisfies these requirements: it works on spontaneous speech by young language learners and preserves their errors.","For this, we collected a corpus containing around 85 hours of English audio spoken by learners in Switzerland from grades 4 to 6 on different language learning tasks, which we used to train an ASR model.","Our experiments show that our model benefits from direct fine-tuning on children's voices and has a much higher error preservation rate than other models."],"url":"http://arxiv.org/abs/2406.03235v1","category":"cs.CL"}
{"created":"2024-06-05 13:13:58","title":"Fine-Grained Causal Dynamics Learning with Quantization for Improving Robustness in Reinforcement Learning","abstract":"Causal dynamics learning has recently emerged as a promising approach to enhancing robustness in reinforcement learning (RL). Typically, the goal is to build a dynamics model that makes predictions based on the causal relationships among the entities. Despite the fact that causal connections often manifest only under certain contexts, existing approaches overlook such fine-grained relationships and lack a detailed understanding of the dynamics. In this work, we propose a novel dynamics model that infers fine-grained causal structures and employs them for prediction, leading to improved robustness in RL. The key idea is to jointly learn the dynamics model with a discrete latent variable that quantizes the state-action space into subgroups. This leads to recognizing meaningful context that displays sparse dependencies, where causal structures are learned for each subgroup throughout the training. Experimental results demonstrate the robustness of our method to unseen states and locally spurious correlations in downstream tasks where fine-grained causal reasoning is crucial. We further illustrate the effectiveness of our subgroup-based approach with quantization in discovering fine-grained causal relationships compared to prior methods.","sentences":["Causal dynamics learning has recently emerged as a promising approach to enhancing robustness in reinforcement learning (RL).","Typically, the goal is to build a dynamics model that makes predictions based on the causal relationships among the entities.","Despite the fact that causal connections often manifest only under certain contexts, existing approaches overlook such fine-grained relationships and lack a detailed understanding of the dynamics.","In this work, we propose a novel dynamics model that infers fine-grained causal structures and employs them for prediction, leading to improved robustness in RL.","The key idea is to jointly learn the dynamics model with a discrete latent variable that quantizes the state-action space into subgroups.","This leads to recognizing meaningful context that displays sparse dependencies, where causal structures are learned for each subgroup throughout the training.","Experimental results demonstrate the robustness of our method to unseen states and locally spurious correlations in downstream tasks where fine-grained causal reasoning is crucial.","We further illustrate the effectiveness of our subgroup-based approach with quantization in discovering fine-grained causal relationships compared to prior methods."],"url":"http://arxiv.org/abs/2406.03234v1","category":"cs.LG"}
{"created":"2024-06-05 13:11:53","title":"Generative Diffusion Models for Fast Simulations of Particle Collisions at CERN","abstract":"In High Energy Physics simulations play a crucial role in unraveling the complexities of particle collision experiments within CERN's Large Hadron Collider. Machine learning simulation methods have garnered attention as promising alternatives to traditional approaches. While existing methods mainly employ Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs), recent advancements highlight the efficacy of diffusion models as state-of-the-art generative machine learning methods. We present the first simulation for Zero Degree Calorimeter (ZDC) at the ALICE experiment based on diffusion models, achieving the highest fidelity compared to existing baselines. We perform an analysis of trade-offs between generation times and the simulation quality. The results indicate a significant potential of latent diffusion model due to its rapid generation time.","sentences":["In High Energy Physics simulations play a crucial role in unraveling the complexities of particle collision experiments within CERN's Large Hadron Collider.","Machine learning simulation methods have garnered attention as promising alternatives to traditional approaches.","While existing methods mainly employ Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs), recent advancements highlight the efficacy of diffusion models as state-of-the-art generative machine learning methods.","We present the first simulation for Zero Degree Calorimeter (ZDC) at the ALICE experiment based on diffusion models, achieving the highest fidelity compared to existing baselines.","We perform an analysis of trade-offs between generation times and the simulation quality.","The results indicate a significant potential of latent diffusion model due to its rapid generation time."],"url":"http://arxiv.org/abs/2406.03233v1","category":"physics.data-an"}
{"created":"2024-06-05 13:06:52","title":"CommonPower: Supercharging Machine Learning for Smart Grids","abstract":"The growing complexity of power system management has led to an increased interest in the use of reinforcement learning (RL). However, no tool for comprehensive and realistic benchmarking of RL in smart grids exists. One prerequisite for such a comparison is a safeguarding mechanism since vanilla RL controllers can not guarantee the satisfaction of system constraints. Other central requirements include flexible modeling of benchmarking scenarios, credible baselines, and the possibility to investigate the impact of forecast uncertainties. Our Python tool CommonPower is the first modular framework addressing these needs. CommonPower offers a unified interface for single-agent and multi-agent RL training algorithms and includes a built-in model predictive control approach based on a symbolic representation of the system equations. This makes it possible to combine model predictive controllers with RL controllers in the same system. Leveraging the symbolic system model, CommonPower facilitates the study of safeguarding strategies via the flexible formulation of safety layers. Furthermore equipped with a generic forecasting interface, CommonPower constitutes a versatile tool significantly augmenting the exploration of safe RL controllers in smart grids on several dimensions.","sentences":["The growing complexity of power system management has led to an increased interest in the use of reinforcement learning (RL).","However, no tool for comprehensive and realistic benchmarking of RL in smart grids exists.","One prerequisite for such a comparison is a safeguarding mechanism since vanilla RL controllers can not guarantee the satisfaction of system constraints.","Other central requirements include flexible modeling of benchmarking scenarios, credible baselines, and the possibility to investigate the impact of forecast uncertainties.","Our Python tool CommonPower is the first modular framework addressing these needs.","CommonPower offers a unified interface for single-agent and multi-agent RL training algorithms and includes a built-in model predictive control approach based on a symbolic representation of the system equations.","This makes it possible to combine model predictive controllers with RL controllers in the same system.","Leveraging the symbolic system model, CommonPower facilitates the study of safeguarding strategies via the flexible formulation of safety layers.","Furthermore equipped with a generic forecasting interface, CommonPower constitutes a versatile tool significantly augmenting the exploration of safe RL controllers in smart grids on several dimensions."],"url":"http://arxiv.org/abs/2406.03231v1","category":"eess.SY"}
{"created":"2024-06-05 13:06:17","title":"Global Clipper: Enhancing Safety and Reliability of Transformer-based Object Detection Models","abstract":"As transformer-based object detection models progress, their impact in critical sectors like autonomous vehicles and aviation is expected to grow. Soft errors causing bit flips during inference have significantly impacted DNN performance, altering predictions. Traditional range restriction solutions for CNNs fall short for transformers. This study introduces the Global Clipper and Global Hybrid Clipper, effective mitigation strategies specifically designed for transformer-based models. It significantly enhances their resilience to soft errors and reduces faulty inferences to ~ 0\\%. We also detail extensive testing across over 64 scenarios involving two transformer models (DINO-DETR and Lite-DETR) and two CNN models (YOLOv3 and SSD) using three datasets, totalling approximately 3.3 million inferences, to assess model robustness comprehensively. Moreover, the paper explores unique aspects of attention blocks in transformers and their operational differences from CNNs.","sentences":["As transformer-based object detection models progress, their impact in critical sectors like autonomous vehicles and aviation is expected to grow.","Soft errors causing bit flips during inference have significantly impacted DNN performance, altering predictions.","Traditional range restriction solutions for CNNs fall short for transformers.","This study introduces the Global Clipper and Global Hybrid Clipper, effective mitigation strategies specifically designed for transformer-based models.","It significantly enhances their resilience to soft errors and reduces faulty inferences to ~ 0\\%.","We also detail extensive testing across over 64 scenarios involving two transformer models (DINO-DETR and Lite-DETR) and two CNN models (YOLOv3 and SSD) using three datasets, totalling approximately 3.3 million inferences, to assess model robustness comprehensively.","Moreover, the paper explores unique aspects of attention blocks in transformers and their operational differences from CNNs."],"url":"http://arxiv.org/abs/2406.03229v1","category":"cs.CV"}
{"created":"2024-06-05 13:05:42","title":"Reference Channel Selection by Multi-Channel Masking for End-to-End Multi-Channel Speech Enhancement","abstract":"In end-to-end multi-channel speech enhancement, the traditional approach of designating one microphone signal as the reference for processing may not always yield optimal results. The limitation is particularly in scenarios with large distributed microphone arrays with varying speaker-to-microphone distances or compact, highly directional microphone arrays where speaker or microphone positions change over time. Current mask-based methods often fix the reference channel during training, which makes it not possible to adaptively select the reference channel for optimal performance. To address this problem, we introduce an adaptive approach for selecting the optimal reference channel. Our method leverages a multi-channel masking-based scheme, where multiple masked signals are combined to generate a single-channel output signal. This enhanced signal is then used for loss calculation, while the reference clean speech is adjusted based on the highest scale-invariant signal-to-distortion ratio (SI-SDR). The experimental results on the Spear challenge simulated dataset D4 demonstrate the superiority of our proposed method over the conventional approach of using a fixed reference channel with single-channel masking","sentences":["In end-to-end multi-channel speech enhancement, the traditional approach of designating one microphone signal as the reference for processing may not always yield optimal results.","The limitation is particularly in scenarios with large distributed microphone arrays with varying speaker-to-microphone distances or compact, highly directional microphone arrays where speaker or microphone positions change over time.","Current mask-based methods often fix the reference channel during training, which makes it not possible to adaptively select the reference channel for optimal performance.","To address this problem, we introduce an adaptive approach for selecting the optimal reference channel.","Our method leverages a multi-channel masking-based scheme, where multiple masked signals are combined to generate a single-channel output signal.","This enhanced signal is then used for loss calculation, while the reference clean speech is adjusted based on the highest scale-invariant signal-to-distortion ratio (SI-SDR).","The experimental results on the Spear challenge simulated dataset D4 demonstrate the superiority of our proposed method over the conventional approach of using a fixed reference channel with single-channel masking"],"url":"http://arxiv.org/abs/2406.03228v1","category":"eess.AS"}
{"created":"2024-06-05 12:55:27","title":"A biobjective Home Care Scheduling Problem with dynamic breaks","abstract":"This paper presents a multiobjective Home Care Scheduling Problem (from now on multiobjective HCSP) related to a home care company for elderly and dependent people located in the North of Spain. In particular, a biobjective problem is considered, with the following two conflicting objectives: the welfare of users and the cost of schedules. To tackle the problem, a custom metaheuristic algorithm based on the Multi-Directional Local Search (MDLS) was designed, obtaining good approximations of the Pareto frontier in efficient computational times. This biobjective algorithm can be divided into three steps: initializing the set of non dominated solutions, generating solutions composed by different routes and obtaining non dominated solutions. The performance of the biobjective algorithm was analyzed by implementing two other well known methods in the literature: the exact method AUGMECON2, which is just an improved version of the Epsilon Constraint approach, and an NSGA-II-based algorithm. Finally, an extensive computational study was developed to compare the three methods over a set of instances from the literature, where the biobjective algorithm exhibited a superior behaviour. Furthermore, the algorithm was also applied to real instances providing solutions to the company with a good trade-off between the two objectives.","sentences":["This paper presents a multiobjective Home Care Scheduling Problem (from now on multiobjective HCSP) related to a home care company for elderly and dependent people located in the North of Spain.","In particular, a biobjective problem is considered, with the following two conflicting objectives: the welfare of users and the cost of schedules.","To tackle the problem, a custom metaheuristic algorithm based on the Multi-Directional Local Search (MDLS) was designed, obtaining good approximations of the Pareto frontier in efficient computational times.","This biobjective algorithm can be divided into three steps: initializing the set of non dominated solutions, generating solutions composed by different routes and obtaining non dominated solutions.","The performance of the biobjective algorithm was analyzed by implementing two other well known methods in the literature: the exact method AUGMECON2, which is just an improved version of the Epsilon Constraint approach, and an NSGA-II-based algorithm.","Finally, an extensive computational study was developed to compare the three methods over a set of instances from the literature, where the biobjective algorithm exhibited a superior behaviour.","Furthermore, the algorithm was also applied to real instances providing solutions to the company with a good trade-off between the two objectives."],"url":"http://arxiv.org/abs/2406.03217v1","category":"math.OC"}
{"created":"2024-06-05 12:53:37","title":"Choice of PEFT Technique in Continual Learning: Prompt Tuning is Not All You Need","abstract":"Recent Continual Learning (CL) methods have combined pretrained Transformers with prompt tuning, a parameter-efficient fine-tuning (PEFT) technique. We argue that the choice of prompt tuning in prior works was an undefended and unablated decision, which has been uncritically adopted by subsequent research, but warrants further research to understand its implications. In this paper, we conduct this research and find that the choice of prompt tuning as a PEFT method hurts the overall performance of the CL system. To illustrate this, we replace prompt tuning with LoRA in two state-of-the-art continual learning methods: Learning to Prompt and S-Prompts. These variants consistently achieve higher accuracy across a wide range of domain-incremental and class-incremental benchmarks, while being competitive in inference speed. Our work highlights a crucial argument: unexamined choices can hinder progress in the field, and rigorous ablations, such as the PEFT method, are required to drive meaningful adoption of CL techniques in real-world applications.","sentences":["Recent Continual Learning (CL) methods have combined pretrained Transformers with prompt tuning, a parameter-efficient fine-tuning (PEFT) technique.","We argue that the choice of prompt tuning in prior works was an undefended and unablated decision, which has been uncritically adopted by subsequent research, but warrants further research to understand its implications.","In this paper, we conduct this research and find that the choice of prompt tuning as a PEFT method hurts the overall performance of the CL system.","To illustrate this, we replace prompt tuning with LoRA in two state-of-the-art continual learning methods: Learning to Prompt and S-Prompts.","These variants consistently achieve higher accuracy across a wide range of domain-incremental and class-incremental benchmarks, while being competitive in inference speed.","Our work highlights a crucial argument: unexamined choices can hinder progress in the field, and rigorous ablations, such as the PEFT method, are required to drive meaningful adoption of CL techniques in real-world applications."],"url":"http://arxiv.org/abs/2406.03216v1","category":"cs.LG"}
{"created":"2024-06-05 12:53:28","title":"Searching Priors Makes Text-to-Video Synthesis Better","abstract":"Significant advancements in video diffusion models have brought substantial progress to the field of text-to-video (T2V) synthesis. However, existing T2V synthesis model struggle to accurately generate complex motion dynamics, leading to a reduction in video realism. One possible solution is to collect massive data and train the model on it, but this would be extremely expensive. To alleviate this problem, in this paper, we reformulate the typical T2V generation process as a search-based generation pipeline. Instead of scaling up the model training, we employ existing videos as the motion prior database. Specifically, we divide T2V generation process into two steps: (i) For a given prompt input, we search existing text-video datasets to find videos with text labels that closely match the prompt motions. We propose a tailored search algorithm that emphasizes object motion features. (ii) Retrieved videos are processed and distilled into motion priors to fine-tune a pre-trained base T2V model, followed by generating desired videos using input prompt. By utilizing the priors gleaned from the searched videos, we enhance the realism of the generated videos' motion. All operations can be finished on a single NVIDIA RTX 4090 GPU. We validate our method against state-of-the-art T2V models across diverse prompt inputs. The code will be public.","sentences":["Significant advancements in video diffusion models have brought substantial progress to the field of text-to-video (T2V) synthesis.","However, existing T2V synthesis model struggle to accurately generate complex motion dynamics, leading to a reduction in video realism.","One possible solution is to collect massive data and train the model on it, but this would be extremely expensive.","To alleviate this problem, in this paper, we reformulate the typical T2V generation process as a search-based generation pipeline.","Instead of scaling up the model training, we employ existing videos as the motion prior database.","Specifically, we divide T2V generation process into two steps: (i) For a given prompt input, we search existing text-video datasets to find videos with text labels that closely match the prompt motions.","We propose a tailored search algorithm that emphasizes object motion features.","(ii) Retrieved videos are processed and distilled into motion priors to fine-tune a pre-trained base T2V model, followed by generating desired videos using input prompt.","By utilizing the priors gleaned from the searched videos, we enhance the realism of the generated videos' motion.","All operations can be finished on a single NVIDIA RTX 4090 GPU.","We validate our method against state-of-the-art T2V models across diverse prompt inputs.","The code will be public."],"url":"http://arxiv.org/abs/2406.03215v1","category":"cs.CV"}
{"created":"2024-06-05 12:53:06","title":"Sum-Frequency Generation Spectro-Microscopy in the Reststrahlen Band of Wurtzite-type Aluminum Nitride","abstract":"Nonlinear-optical microscopy and spectroscopy provide detailed spatial and spectroscopic contrast, specifically sensitive to structural symmetry and order. Ferroics, in particular, have been widely studied using second harmonic generation imaging, which provides detailed information on domain structures but typically lacks spectroscopic detail. In contrast, infrared-visible sum-frequency generation (SFG) spectroscopy reveals details of the atomic structure and bonding via vibrational resonances, but conventionally lacks spatial information. In this work, we combine the benefits of nonlinear optical imaging and SFG spectroscopy by employing SFG spectro-microscopy using an infrared free-electron laser. Specifically, we demonstrate the feasibility of SFG spectro-microscopy for spectroscopy using in-plane anisotropic wurtzite-type aluminum nitride as a model system. We find the experimental spectra to agree well with our theoretical calculations and we show the potential of our microscope to provide spatially resolved spectroscopic information in inhomogeneous systems such as ferroics and their domains in the near future.","sentences":["Nonlinear-optical microscopy and spectroscopy provide detailed spatial and spectroscopic contrast, specifically sensitive to structural symmetry and order.","Ferroics, in particular, have been widely studied using second harmonic generation imaging, which provides detailed information on domain structures but typically lacks spectroscopic detail.","In contrast, infrared-visible sum-frequency generation (SFG) spectroscopy reveals details of the atomic structure and bonding via vibrational resonances, but conventionally lacks spatial information.","In this work, we combine the benefits of nonlinear optical imaging and SFG spectroscopy by employing SFG spectro-microscopy using an infrared free-electron laser.","Specifically, we demonstrate the feasibility of SFG spectro-microscopy for spectroscopy using in-plane anisotropic wurtzite-type aluminum nitride as a model system.","We find the experimental spectra to agree well with our theoretical calculations and we show the potential of our microscope to provide spatially resolved spectroscopic information in inhomogeneous systems such as ferroics and their domains in the near future."],"url":"http://arxiv.org/abs/2406.03214v1","category":"physics.optics"}
{"created":"2024-06-05 12:45:23","title":"Challenges and Considerations in the Evaluation of Bayesian Causal Discovery","abstract":"Representing uncertainty in causal discovery is a crucial component for experimental design, and more broadly, for safe and reliable causal decision making. Bayesian Causal Discovery (BCD) offers a principled approach to encapsulating this uncertainty. Unlike non-Bayesian causal discovery, which relies on a single estimated causal graph and model parameters for assessment, evaluating BCD presents challenges due to the nature of its inferred quantity - the posterior distribution. As a result, the research community has proposed various metrics to assess the quality of the approximate posterior. However, there is, to date, no consensus on the most suitable metric(s) for evaluation. In this work, we reexamine this question by dissecting various metrics and understanding their limitations. Through extensive empirical evaluation, we find that many existing metrics fail to exhibit a strong correlation with the quality of approximation to the true posterior, especially in scenarios with low sample sizes where BCD is most desirable. We highlight the suitability (or lack thereof) of these metrics under two distinct factors: the identifiability of the underlying causal model and the quantity of available data. Both factors affect the entropy of the true posterior, indicating that the current metrics are less fitting in settings of higher entropy. Our findings underline the importance of a more nuanced evaluation of new methods by taking into account the nature of the true posterior, as well as guide and motivate the development of new evaluation procedures for this challenge.","sentences":["Representing uncertainty in causal discovery is a crucial component for experimental design, and more broadly, for safe and reliable causal decision making.","Bayesian Causal Discovery (BCD) offers a principled approach to encapsulating this uncertainty.","Unlike non-Bayesian causal discovery, which relies on a single estimated causal graph and model parameters for assessment, evaluating BCD presents challenges due to the nature of its inferred quantity - the posterior distribution.","As a result, the research community has proposed various metrics to assess the quality of the approximate posterior.","However, there is, to date, no consensus on the most suitable metric(s) for evaluation.","In this work, we reexamine this question by dissecting various metrics and understanding their limitations.","Through extensive empirical evaluation, we find that many existing metrics fail to exhibit a strong correlation with the quality of approximation to the true posterior, especially in scenarios with low sample sizes where BCD is most desirable.","We highlight the suitability (or lack thereof) of these metrics under two distinct factors: the identifiability of the underlying causal model and the quantity of available data.","Both factors affect the entropy of the true posterior, indicating that the current metrics are less fitting in settings of higher entropy.","Our findings underline the importance of a more nuanced evaluation of new methods by taking into account the nature of the true posterior, as well as guide and motivate the development of new evaluation procedures for this challenge."],"url":"http://arxiv.org/abs/2406.03209v1","category":"cs.LG"}
{"created":"2024-06-05 12:45:02","title":"Fuzzing Frameworks for Server-side Web Applications: A Survey","abstract":"There are around 5.3 billion Internet users, amounting to 65.7% of the global population, and web technology is the backbone of the services delivered via the Internet. To ensure web applications are free from security-related bugs, web developers test the server-side web applications before deploying them to production. The tests are commonly conducted through the interfaces (i.e., Web API) that the applications expose since they are the entry points to the application. Fuzzing is one of the most promising automated software testing techniques suitable for this task; however, the research on (server-side) web application fuzzing has been rather limited compared to binary fuzzing which is researched extensively. This study reviews the state-of-the-art fuzzing frameworks for testing web applications through web API, identifies open challenges, and gives potential future research. We collect papers from seven online repositories of peer-reviewed articles over the last ten years. Compared to other similar studies, our review focuses more deeply on revealing prior work strategies in generating valid HTTP requests, utilising feedback from the Web Under Tests (WUTs), and expanding input spaces. The findings of this survey indicate that several crucial challenges need to be solved, such as the ineffectiveness of web instrumentation and the complexity of handling microservice applications. Furthermore, some potential research directions are also provided, such as fuzzing for web client programming. Ultimately, this paper aims to give a good starting point for developing a better web fuzzing framework.","sentences":["There are around 5.3 billion Internet users, amounting to 65.7% of the global population, and web technology is the backbone of the services delivered via the Internet.","To ensure web applications are free from security-related bugs, web developers test the server-side web applications before deploying them to production.","The tests are commonly conducted through the interfaces (i.e., Web API) that the applications expose since they are the entry points to the application.","Fuzzing is one of the most promising automated software testing techniques suitable for this task; however, the research on (server-side) web application fuzzing has been rather limited compared to binary fuzzing which is researched extensively.","This study reviews the state-of-the-art fuzzing frameworks for testing web applications through web API, identifies open challenges, and gives potential future research.","We collect papers from seven online repositories of peer-reviewed articles over the last ten years.","Compared to other similar studies, our review focuses more deeply on revealing prior work strategies in generating valid HTTP requests, utilising feedback from the Web Under Tests (WUTs), and expanding input spaces.","The findings of this survey indicate that several crucial challenges need to be solved, such as the ineffectiveness of web instrumentation and the complexity of handling microservice applications.","Furthermore, some potential research directions are also provided, such as fuzzing for web client programming.","Ultimately, this paper aims to give a good starting point for developing a better web fuzzing framework."],"url":"http://arxiv.org/abs/2406.03208v1","category":"cs.SE"}
{"created":"2024-06-05 12:40:38","title":"Approximating dynamical correlation functions with constant depth quantum circuits","abstract":"One of the most important quantities characterizing the microscopic properties of quantum systems are dynamical correlation functions. These correlations are obtained by time-evolving a perturbation of an eigenstate of the system, typically the ground state. In this work, we study approximations of these correlation functions that do not require time dynamics. We show that having access to a circuit that prepares an eigenstate of the Hamiltonian, it is possible to approximate the dynamical correlation functions up to exponential accuracy in the complex frequency domain $\\omega=\\Re(\\omega)+i\\Im(\\omega)$, on a strip above the real line $\\Im(\\omega)=0$. We achieve this by exploiting the continued fraction representation of the dynamical correlation functions as functions of frequency $\\omega$, where the level $k$ approximant can be obtained by measuring a weight $O(k)$ operator on the eigenstate of interest. In the complex $\\omega$ plane, we show how this approach allows to determine approximations to correlation functions with accuracy that increases exponentially with $k$.   We analyse two algorithms to generate the continuous fraction representation in scalar or matrix form, starting from either one or many initial operators. We prove that these algorithms generate an exponentially accurate approximation of the dynamical correlation functions on a region sufficiently far away from the real frequency axis. We present numerical evidence of these theoretical results through simulations of small lattice systems. We comment on the stability of these algorithms with respect to sampling noise in the context of quantum simulation using quantum computers.","sentences":["One of the most important quantities characterizing the microscopic properties of quantum systems are dynamical correlation functions.","These correlations are obtained by time-evolving a perturbation of an eigenstate of the system, typically the ground state.","In this work, we study approximations of these correlation functions that do not require time dynamics.","We show that having access to a circuit that prepares an eigenstate of the Hamiltonian, it is possible to approximate the dynamical correlation functions up to exponential accuracy in the complex frequency domain $\\omega=\\Re(\\omega)+i\\Im(\\omega)$, on a strip above the real line $\\Im(\\omega)=0$. We achieve this by exploiting the continued fraction representation of the dynamical correlation functions as functions of frequency $\\omega$, where the level $k$ approximant can be obtained by measuring a weight $O(k)$ operator on the eigenstate of interest.","In the complex $\\omega$ plane, we show how this approach allows to determine approximations to correlation functions with accuracy that increases exponentially with $k$.   We analyse two algorithms to generate the continuous fraction representation in scalar or matrix form, starting from either one or many initial operators.","We prove that these algorithms generate an exponentially accurate approximation of the dynamical correlation functions on a region sufficiently far away from the real frequency axis.","We present numerical evidence of these theoretical results through simulations of small lattice systems.","We comment on the stability of these algorithms with respect to sampling noise in the context of quantum simulation using quantum computers."],"url":"http://arxiv.org/abs/2406.03204v1","category":"quant-ph"}
{"created":"2024-06-05 12:35:00","title":"ChatLang-8: An LLM-Based Synthetic Data Generation Framework for Grammatical Error Correction","abstract":"We explore and improve the capabilities of LLMs to generate data for grammatical error correction (GEC). When merely producing parallel sentences, their patterns are too simplistic to be valuable as a corpus. To address this issue, we propose an automated framework that includes a Subject Selector, Grammar Selector, Prompt Manager, and Evaluator. Additionally, we introduce a new dataset for GEC tasks, named \\textbf{ChatLang-8}, which encompasses eight types of subject nouns and 23 types of grammar. It consists of 1 million pairs featuring human-like grammatical errors. Our experiments reveal that ChatLang-8 exhibits a more uniform pattern composition compared to existing GEC datasets. Furthermore, we observe improved model performance when using ChatLang-8 instead of existing GEC datasets. The experimental results suggest that our framework and ChatLang-8 are valuable resources for enhancing ChatGPT's data generation capabilities.","sentences":["We explore and improve the capabilities of LLMs to generate data for grammatical error correction (GEC).","When merely producing parallel sentences, their patterns are too simplistic to be valuable as a corpus.","To address this issue, we propose an automated framework that includes a Subject Selector, Grammar Selector, Prompt Manager, and Evaluator.","Additionally, we introduce a new dataset for GEC tasks, named \\textbf{ChatLang-8}, which encompasses eight types of subject nouns and 23 types of grammar.","It consists of 1 million pairs featuring human-like grammatical errors.","Our experiments reveal that ChatLang-8 exhibits a more uniform pattern composition compared to existing GEC datasets.","Furthermore, we observe improved model performance when using ChatLang-8 instead of existing GEC datasets.","The experimental results suggest that our framework and ChatLang-8 are valuable resources for enhancing ChatGPT's data generation capabilities."],"url":"http://arxiv.org/abs/2406.03202v1","category":"cs.CL"}
{"created":"2024-06-05 12:33:27","title":"Characteristic ideal of the fine Selmer group and results on $\u03bc$-invariance under isogeny in the function field case","abstract":"Consider a function field $K$ with characteristic $p>0$. We investigate the $\\Lambda$-module structure of the Mordell-Weil group of an abelian variety over $\\mathbb{Z}_p$-extensions of $K$, generalizing results due to Lee. Next, we study the algebraic structure and prove a control theorem for the S-fine Mordell-Weil groups, the function field analogue for Wuthrich's fine Mordell-Weil groups, over a $\\mathbb{Z}_p$-extension of $K$. In case of unramified $\\mathbb{Z}_p$-extension, $K_\\infty$, we compute the characteristic ideal of the Pontryagin dual of the S-fine Mordell group. This provides an answer to an analogue of Greenberg's question for the characteristic ideal of the dual fine Selmer group in the function field setup. In the $\\ell\\neq p$ case, we prove the triviality of the $\\mu$-invariant for the Selmer group (same as the fine Selmer group in this case) of an elliptic curve over a non-commutative $GL_2(\\mathbb{Z}_\\ell)$-extension of $K$ and thus extending Conjecture A. In the $\\ell=p$ case, we compute the change of $\\mu$-invariants of the dual Selmer groups of elliptic curves under isogeny, giving a lower bound for the $\\mu$-invariant.","sentences":["Consider a function field $K$ with characteristic $p>0$. We investigate the $\\Lambda$-module structure of the Mordell-Weil group of an abelian variety over $\\mathbb{Z}_p$-extensions of $K$, generalizing results due to Lee.","Next, we study the algebraic structure and prove a control theorem for the S-fine Mordell-Weil groups, the function field analogue for Wuthrich's fine Mordell-Weil groups, over a $\\mathbb{Z}_p$-extension of $K$. In case of unramified $\\mathbb{Z}_p$-extension, $K_\\infty$, we compute the characteristic ideal of the Pontryagin dual of the S-fine Mordell group.","This provides an answer to an analogue of Greenberg's question for the characteristic ideal of the dual fine Selmer group in the function field setup.","In the $\\ell\\neq p$ case, we prove the triviality of the $\\mu$-invariant for the Selmer group (same as the fine Selmer group in this case) of an elliptic curve over a non-commutative $GL_2(\\mathbb{Z}_\\ell)$-extension of $K$ and thus extending Conjecture A.","In the $\\ell=p$ case, we compute the change of $\\mu$-invariants of the dual Selmer groups of elliptic curves under isogeny, giving a lower bound for the $\\mu$-invariant."],"url":"http://arxiv.org/abs/2406.03201v1","category":"math.NT"}
{"created":"2024-06-05 12:33:11","title":"Adaptive Distance Functions via Kelvin Transformation","abstract":"The term safety in robotics is often understood as a synonym for avoidance. Although this perspective has led to progress in path planning and reactive control, a generalization of this perspective is necessary to include task semantics relevant to contact-rich manipulation tasks, especially during teleoperation and to ensure the safety of learned policies.   We introduce the semantics-aware distance function and a corresponding computational method based on the Kelvin Transformation. The semantics-aware distance generalizes signed distance functions by allowing the zero level set to lie inside of the object in regions where contact is allowed, effectively incorporating task semantics -- such as object affordances and user intent -- in an adaptive implicit representation of safe sets. In validation experiments we show the capability of our method to adapt to time-varying semantic information, and to perform queries in sub-microsecond, enabling applications in reinforcement learning, trajectory optimization, and motion planning.","sentences":["The term safety in robotics is often understood as a synonym for avoidance.","Although this perspective has led to progress in path planning and reactive control, a generalization of this perspective is necessary to include task semantics relevant to contact-rich manipulation tasks, especially during teleoperation and to ensure the safety of learned policies.   ","We introduce the semantics-aware distance function and a corresponding computational method based on the Kelvin Transformation.","The semantics-aware distance generalizes signed distance functions by allowing the zero level set to lie inside of the object in regions where contact is allowed, effectively incorporating task semantics -- such as object affordances and user intent -- in an adaptive implicit representation of safe sets.","In validation experiments we show the capability of our method to adapt to time-varying semantic information, and to perform queries in sub-microsecond, enabling applications in reinforcement learning, trajectory optimization, and motion planning."],"url":"http://arxiv.org/abs/2406.03200v1","category":"cs.RO"}
{"created":"2024-06-05 12:25:50","title":"Self-gravitating anisotropic fluid. III: Relativistic theory","abstract":"This is the third and final entry in a sequence of papers devoted to the formulation of a theory of self-gravitating anisotropic fluids in Newtonian gravity and general relativity. In this third paper we elevate the Newtonian theory of the second paper to general relativity, and apply it to the construction of relativistic stellar models. The relativistic theory is crafted by promoting the fluid variables to a curved spacetime, and promoting the gravitational potential to the spacetime metric. The Newtonian action is then generalized in a direct and natural way, and dynamical equations for all the relevant variables are once more obtained through a variational principle. We specialize our relativistic theory of a self-gravitating anisotropic fluid to static and spherically symmetric configurations, and thus obtain models of anisotropic stars in general relativity. As in the Newtonian setting, the models feature a transition from an anisotropic phase at high density to an isotropic phase at low density. Our survey of stellar models reveals that for the same equations of state and the same central density, anisotropic stars are always less compact than isotropic stars.","sentences":["This is the third and final entry in a sequence of papers devoted to the formulation of a theory of self-gravitating anisotropic fluids in Newtonian gravity and general relativity.","In this third paper we elevate the Newtonian theory of the second paper to general relativity, and apply it to the construction of relativistic stellar models.","The relativistic theory is crafted by promoting the fluid variables to a curved spacetime, and promoting the gravitational potential to the spacetime metric.","The Newtonian action is then generalized in a direct and natural way, and dynamical equations for all the relevant variables are once more obtained through a variational principle.","We specialize our relativistic theory of a self-gravitating anisotropic fluid to static and spherically symmetric configurations, and thus obtain models of anisotropic stars in general relativity.","As in the Newtonian setting, the models feature a transition from an anisotropic phase at high density to an isotropic phase at low density.","Our survey of stellar models reveals that for the same equations of state and the same central density, anisotropic stars are always less compact than isotropic stars."],"url":"http://arxiv.org/abs/2406.03196v1","category":"gr-qc"}
{"created":"2024-06-05 12:23:17","title":"Writing Order Recovery in Complex and Long Static Handwriting","abstract":"The order in which the trajectory is executed is a powerful source of information for recognizers. However, there is still no general approach for recovering the trajectory of complex and long handwriting from static images. Complex specimens can result in multiple pen-downs and in a high number of trajectory crossings yielding agglomerations of pixels (also known as clusters). While the scientific literature describes a wide range of approaches for recovering the writing order in handwriting, these approaches nevertheless lack a common evaluation metric. In this paper, we introduce a new system to estimate the order recovery of thinned static trajectories, which allows to effectively resolve the clusters and select the order of the executed pen-downs. We evaluate how knowing the starting points of the pen-downs affects the quality of the recovered writing. Once the stability and sensitivity of the system is analyzed, we describe a series of experiments with three publicly available databases, showing competitive results in all cases. We expect the proposed system, whose code is made publicly available to the research community, to reduce potential confusion when the order of complex trajectories are recovered, and this will in turn make the trajectories recovered to be viable for further applications, such as velocity estimation.","sentences":["The order in which the trajectory is executed is a powerful source of information for recognizers.","However, there is still no general approach for recovering the trajectory of complex and long handwriting from static images.","Complex specimens can result in multiple pen-downs and in a high number of trajectory crossings yielding agglomerations of pixels (also known as clusters).","While the scientific literature describes a wide range of approaches for recovering the writing order in handwriting, these approaches nevertheless lack a common evaluation metric.","In this paper, we introduce a new system to estimate the order recovery of thinned static trajectories, which allows to effectively resolve the clusters and select the order of the executed pen-downs.","We evaluate how knowing the starting points of the pen-downs affects the quality of the recovered writing.","Once the stability and sensitivity of the system is analyzed, we describe a series of experiments with three publicly available databases, showing competitive results in all cases.","We expect the proposed system, whose code is made publicly available to the research community, to reduce potential confusion when the order of complex trajectories are recovered, and this will in turn make the trajectories recovered to be viable for further applications, such as velocity estimation."],"url":"http://arxiv.org/abs/2406.03194v1","category":"cs.CV"}
{"created":"2024-06-05 12:21:11","title":"Self-gravitating anisotropic fluid. II: Newtonian theory","abstract":"This paper is the second in a sequence of three devoted to the formulation of a theory of self-gravitating anisotropic fluids in both Newtonian gravity and general relativity. In this second paper we develop the Newtonian theory, inspired by a real-life example of an anisotropic fluid, the (nematic) liquid crystal. We apply the theory to the construction of static and spherical stellar models. In addition to the usual fluid variables (mass density, velocity field), the Newtonian theory features a director vector field, whose length provides a local measure of the size of the anisotropy, and whose direction gives the local direction of anisotropy. The theory is defined in terms of a Lagrangian which implicates all the relevant forms of energy: kinetic energy (with contributions from the velocity field and the time derivative of the director vector), internal energy (with isotropic and anisotropic contributions), gravitational interaction energy, and gravitational-field energy. This Lagrangian is easy to motivate, and it provides an excellent starting point for a relativistic generalization in the third paper. The equations of motion for the fluid, and Poisson's equation for the gravitational potential, follow from a variation of the action functional, given by the time integral of the Lagrangian. Because our stellar models feature a transition from an anisotropic phase at high density to an isotropic phase at low density, a substantial part of the paper is devoted to the development of a mechanics for the interface fluid, which mediates the phase transition.","sentences":["This paper is the second in a sequence of three devoted to the formulation of a theory of self-gravitating anisotropic fluids in both Newtonian gravity and general relativity.","In this second paper we develop the Newtonian theory, inspired by a real-life example of an anisotropic fluid, the (nematic) liquid crystal.","We apply the theory to the construction of static and spherical stellar models.","In addition to the usual fluid variables (mass density, velocity field), the Newtonian theory features a director vector field, whose length provides a local measure of the size of the anisotropy, and whose direction gives the local direction of anisotropy.","The theory is defined in terms of a Lagrangian which implicates all the relevant forms of energy: kinetic energy (with contributions from the velocity field and the time derivative of the director vector), internal energy (with isotropic and anisotropic contributions), gravitational interaction energy, and gravitational-field energy.","This Lagrangian is easy to motivate, and it provides an excellent starting point for a relativistic generalization in the third paper.","The equations of motion for the fluid, and Poisson's equation for the gravitational potential, follow from a variation of the action functional, given by the time integral of the Lagrangian.","Because our stellar models feature a transition from an anisotropic phase at high density to an isotropic phase at low density, a substantial part of the paper is devoted to the development of a mechanics for the interface fluid, which mediates the phase transition."],"url":"http://arxiv.org/abs/2406.03191v1","category":"gr-qc"}
{"created":"2024-06-05 12:20:58","title":"Matter coupled to 3d Quantum Gravity: One-loop Unitarity","abstract":"We expect quantum field theories for matter to acquire intricate corrections due to their coupling to quantum fluctuations of the gravitational field. This can be precisely worked out in 3d quantum gravity: after integrating out quantum gravity, matter fields are effectively described as noncommutative quantum field theories, with quantum-deformed Lorentz symmetries. An open question remains: Are such theories unitary or not? On the one hand, since these are effective field theories obtained after integrating out high energy degrees of freedom, we may expect the loss of unitarity. On the other hand, as rigorously defined field theories built with Lorentz symmetries and standing on their own, we naturally expect the conservation of unitarity. In an effort to settle this issue, we explicitly check unitarity for a scalar field at one-loop level in both Euclidean and Lorentzian space-time signatures. We find that unitarity requires adding an extra-term to the propagator of the noncommutative theory, corresponding to a massless mode and given by a representation with vanishing Plancherel measure, thus usually ignored in spinfoam path integrals for quantum gravity. This indicates that the inclusion of matter in spinfoam models, and more generally in quantum gravity, might be more subtle than previously thought.","sentences":["We expect quantum field theories for matter to acquire intricate corrections due to their coupling to quantum fluctuations of the gravitational field.","This can be precisely worked out in 3d quantum gravity: after integrating out quantum gravity, matter fields are effectively described as noncommutative quantum field theories, with quantum-deformed Lorentz symmetries.","An open question remains: Are such theories unitary or not?","On the one hand, since these are effective field theories obtained after integrating out high energy degrees of freedom, we may expect the loss of unitarity.","On the other hand, as rigorously defined field theories built with Lorentz symmetries and standing on their own, we naturally expect the conservation of unitarity.","In an effort to settle this issue, we explicitly check unitarity for a scalar field at one-loop level in both Euclidean and Lorentzian space-time signatures.","We find that unitarity requires adding an extra-term to the propagator of the noncommutative theory, corresponding to a massless mode and given by a representation with vanishing Plancherel measure, thus usually ignored in spinfoam path integrals for quantum gravity.","This indicates that the inclusion of matter in spinfoam models, and more generally in quantum gravity, might be more subtle than previously thought."],"url":"http://arxiv.org/abs/2406.03190v1","category":"hep-th"}
{"created":"2024-06-05 12:20:36","title":"Situation Monitor: Diversity-Driven Zero-Shot Out-of-Distribution Detection using Budding Ensemble Architecture for Object Detection","abstract":"We introduce Situation Monitor, a novel zero-shot Out-of-Distribution (OOD) detection approach for transformer-based object detection models to enhance reliability in safety-critical machine learning applications such as autonomous driving. The Situation Monitor utilizes the Diversity-based Budding Ensemble Architecture (DBEA) and increases the OOD performance by integrating a diversity loss into the training process on top of the budding ensemble architecture, detecting Far-OOD samples and minimizing false positives on Near-OOD samples. Moreover, utilizing the resulting DBEA increases the model's OOD performance and improves the calibration of confidence scores, particularly concerning the intersection over union of the detected objects. The DBEA model achieves these advancements with a 14% reduction in trainable parameters compared to the vanilla model. This signifies a substantial improvement in efficiency without compromising the model's ability to detect OOD instances and calibrate the confidence scores accurately.","sentences":["We introduce Situation Monitor, a novel zero-shot Out-of-Distribution (OOD) detection approach for transformer-based object detection models to enhance reliability in safety-critical machine learning applications such as autonomous driving.","The Situation Monitor utilizes the Diversity-based Budding Ensemble Architecture (DBEA) and increases the OOD performance by integrating a diversity loss into the training process on top of the budding ensemble architecture, detecting Far-OOD samples and minimizing false positives on Near-OOD samples.","Moreover, utilizing the resulting DBEA increases the model's OOD performance and improves the calibration of confidence scores, particularly concerning the intersection over union of the detected objects.","The DBEA model achieves these advancements with a 14% reduction in trainable parameters compared to the vanilla model.","This signifies a substantial improvement in efficiency without compromising the model's ability to detect OOD instances and calibrate the confidence scores accurately."],"url":"http://arxiv.org/abs/2406.03188v1","category":"cs.CV"}
{"created":"2024-06-05 12:15:34","title":"Self-gravitating anisotropic fluids. I: Context and overview","abstract":"This paper is the first in a sequence of three devoted to the formulation of a theory of self-gravitating anisotropic fluids in both Newtonian and relativistic gravity. In this first paper we set the stage, place our work in the context of a vast literature on anisotropic stars in general relativity, and provide an overview of the results obtained in the remaining two papers. In both cases, Newtonian and relativistic, the state of the fluid is described by the familiar variables of an isotropic fluid (such as mass density and velocity field), to which we adjoin a director vector, which defines a locally preferred direction within the fluid. Both the Newtonian and relativistic theories are defined in terms of an action functional. While each theory is formulated in complete generality, in these papers we apply them to the construction of stellar models by restricting the fluid configurations to be static and spherically symmetric. We find that the equations of anisotropic stellar structure are generically singular at the stellar surface. To avoid a singularity, we postulate the existence of a phase transition at a critical value of the mass density; the fluid is anisotropic at high densities, and goes to an isotropic phase at low densities. In the case of Newtonian stars, we find that sequences of equilibrium configurations terminate at a maximum value of the central density; beyond this maximum the density profile becomes multi-valued within the star, and the model therefore becomes unphysical. In the case of relativistic stars, this phenomenon typically occurs beyond the point at which the stellar mass achieves a maximum. Also in the case of relativistic stars, we find that for a given equation of state and a given assignment of central density, anisotropic stellar models are always less compact than isotropic models.","sentences":["This paper is the first in a sequence of three devoted to the formulation of a theory of self-gravitating anisotropic fluids in both Newtonian and relativistic gravity.","In this first paper we set the stage, place our work in the context of a vast literature on anisotropic stars in general relativity, and provide an overview of the results obtained in the remaining two papers.","In both cases, Newtonian and relativistic, the state of the fluid is described by the familiar variables of an isotropic fluid (such as mass density and velocity field), to which we adjoin a director vector, which defines a locally preferred direction within the fluid.","Both the Newtonian and relativistic theories are defined in terms of an action functional.","While each theory is formulated in complete generality, in these papers we apply them to the construction of stellar models by restricting the fluid configurations to be static and spherically symmetric.","We find that the equations of anisotropic stellar structure are generically singular at the stellar surface.","To avoid a singularity, we postulate the existence of a phase transition at a critical value of the mass density; the fluid is anisotropic at high densities, and goes to an isotropic phase at low densities.","In the case of Newtonian stars, we find that sequences of equilibrium configurations terminate at a maximum value of the central density; beyond this maximum the density profile becomes multi-valued within the star, and the model therefore becomes unphysical.","In the case of relativistic stars, this phenomenon typically occurs beyond the point at which the stellar mass achieves a maximum.","Also in the case of relativistic stars, we find that for a given equation of state and a given assignment of central density, anisotropic stellar models are always less compact than isotropic models."],"url":"http://arxiv.org/abs/2406.03185v1","category":"gr-qc"}
{"created":"2024-06-05 12:15:22","title":"Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion","abstract":"Existing single image-to-3D creation methods typically involve a two-stage process, first generating multi-view images, and then using these images for 3D reconstruction. However, training these two stages separately leads to significant data bias in the inference phase, thus affecting the quality of reconstructed results. We introduce a unified 3D generation framework, named Ouroboros3D, which integrates diffusion-based multi-view image generation and 3D reconstruction into a recursive diffusion process. In our framework, these two modules are jointly trained through a self-conditioning mechanism, allowing them to adapt to each other's characteristics for robust inference. During the multi-view denoising process, the multi-view diffusion model uses the 3D-aware maps rendered by the reconstruction module at the previous timestep as additional conditions. The recursive diffusion framework with 3D-aware feedback unites the entire process and improves geometric consistency.Experiments show that our framework outperforms separation of these two stages and existing methods that combine them at the inference phase. Project page: https://costwen.github.io/Ouroboros3D/","sentences":["Existing single image-to-3D creation methods typically involve a two-stage process, first generating multi-view images, and then using these images for 3D reconstruction.","However, training these two stages separately leads to significant data bias in the inference phase, thus affecting the quality of reconstructed results.","We introduce a unified 3D generation framework, named Ouroboros3D, which integrates diffusion-based multi-view image generation and 3D reconstruction into a recursive diffusion process.","In our framework, these two modules are jointly trained through a self-conditioning mechanism, allowing them to adapt to each other's characteristics for robust inference.","During the multi-view denoising process, the multi-view diffusion model uses the 3D-aware maps rendered by the reconstruction module at the previous timestep as additional conditions.","The recursive diffusion framework with 3D-aware feedback unites the entire process and improves geometric consistency.","Experiments show that our framework outperforms separation of these two stages and existing methods that combine them at the inference phase.","Project page: https://costwen.github.io/Ouroboros3D/"],"url":"http://arxiv.org/abs/2406.03184v1","category":"cs.CV"}
{"created":"2024-06-05 12:13:25","title":"Geometric Localization of Homology Cycles","abstract":"Computing an optimal cycle in a given homology class, also referred to as the homology localization problem, is known to be an NP-hard problem in general. Furthermore, there is currently no known optimality criterion that localizes classes geometrically and admits a stability property under the setting of persistent homology. We present a geometric optimization of the cycles that is computable in polynomial time and is stable in an approximate sense. Tailoring our search criterion to different settings, we obtain various optimization problems like optimal homologous cycle, minimum homology basis, and minimum persistent homology basis. In practice, the (trivial) exact algorithm is computationally expensive despite having a worst case polynomial runtime. Therefore, we design approximation algorithms for the above problems and study their performance experimentally. These algorithms have reasonable runtimes for moderate sized datasets and the cycles computed by these algorithms are consistently of high quality as demonstrated via experiments on multiple datasets.","sentences":["Computing an optimal cycle in a given homology class, also referred to as the homology localization problem, is known to be an NP-hard problem in general.","Furthermore, there is currently no known optimality criterion that localizes classes geometrically and admits a stability property under the setting of persistent homology.","We present a geometric optimization of the cycles that is computable in polynomial time and is stable in an approximate sense.","Tailoring our search criterion to different settings, we obtain various optimization problems like optimal homologous cycle, minimum homology basis, and minimum persistent homology basis.","In practice, the (trivial) exact algorithm is computationally expensive despite having a worst case polynomial runtime.","Therefore, we design approximation algorithms for the above problems and study their performance experimentally.","These algorithms have reasonable runtimes for moderate sized datasets and the cycles computed by these algorithms are consistently of high quality as demonstrated via experiments on multiple datasets."],"url":"http://arxiv.org/abs/2406.03183v1","category":"cs.CG"}
{"created":"2024-06-05 12:08:58","title":"Sub-diffraction estimation, discrimination and learning of quantum states of light","abstract":"The resolution of optical imaging is classically limited by the width of the point-spread function, which in turn is determined by the Rayleigh length. Recently, spatial-mode demultiplexing (SPADE) has been proposed as a method to achieve sub-Rayleigh estimation and discrimination of natural, incoherent sources. Here we show that SPADE is optimal in the broader context of machine learning. To this goal, we introduce a hybrid quantum-classical image classifier that achieves sub-Rayleigh resolution. The algorithm includes a quantum and a classical part. In the quantum part, a physical device (demultiplexer) is used to sort the transverse field, followed by mode-wise photon detection. This part of the algorithm implements a physical pre-processing of the quantum field that cannot be classically simulated without essentially reducing the signal-to-noise ratio. In the classical part of the algorithm, the collected data are fed into an artificial neural network for training and classification. As a case study, we classify images from the MNIST dataset after severe blurring due to diffraction. Our numerical experiments demonstrate the ability to learn highly blurred images that would be otherwise indistinguishable by direct imaging without the physical pre-processing of the quantum field.","sentences":["The resolution of optical imaging is classically limited by the width of the point-spread function, which in turn is determined by the Rayleigh length.","Recently, spatial-mode demultiplexing (SPADE) has been proposed as a method to achieve sub-Rayleigh estimation and discrimination of natural, incoherent sources.","Here we show that SPADE is optimal in the broader context of machine learning.","To this goal, we introduce a hybrid quantum-classical image classifier that achieves sub-Rayleigh resolution.","The algorithm includes a quantum and a classical part.","In the quantum part, a physical device (demultiplexer) is used to sort the transverse field, followed by mode-wise photon detection.","This part of the algorithm implements a physical pre-processing of the quantum field that cannot be classically simulated without essentially reducing the signal-to-noise ratio.","In the classical part of the algorithm, the collected data are fed into an artificial neural network for training and classification.","As a case study, we classify images from the MNIST dataset after severe blurring due to diffraction.","Our numerical experiments demonstrate the ability to learn highly blurred images that would be otherwise indistinguishable by direct imaging without the physical pre-processing of the quantum field."],"url":"http://arxiv.org/abs/2406.03179v1","category":"quant-ph"}
{"created":"2024-06-05 12:07:58","title":"MMCL: Boosting Deformable DETR-Based Detectors with Multi-Class Min-Margin Contrastive Learning for Superior Prohibited Item Detection","abstract":"Prohibited Item detection in X-ray images is one of the most effective security inspection methods.However, differing from natural light images, the unique overlapping phenomena in X-ray images lead to the coupling of foreground and background features, thereby lowering the accuracy of general object detectors.Therefore, we propose a Multi-Class Min-Margin Contrastive Learning (MMCL) method that, by clarifying the category semantic information of content queries under the deformable DETR architecture, aids the model in extracting specific category foreground information from coupled features.Specifically, after grouping content queries by the number of categories, we employ the Multi-Class Inter-Class Exclusion (MIE) loss to push apart content queries from different groups. Concurrently, the Intra-Class Min-Margin Clustering (IMC) loss is utilized to attract content queries within the same group, while ensuring the preservation of necessary disparity. As training, the inherent Hungarian matching of the model progressively strengthens the alignment between each group of queries and the semantic features of their corresponding category of objects. This evolving coherence ensures a deep-seated grasp of category characteristics, consequently bolstering the anti-overlapping detection capabilities of models.MMCL is versatile and can be easily plugged into any deformable DETR-based model with dozens of lines of code. Extensive experiments on the PIXray and OPIXray datasets demonstrate that MMCL significantly enhances the performance of various state-of-the-art models without increasing complexity. The code has been released at https://github.com/anonymity0403/MMCL.","sentences":["Prohibited Item detection in X-ray images is one of the most effective security inspection methods.","However, differing from natural light images, the unique overlapping phenomena in X-ray images lead to the coupling of foreground and background features, thereby lowering the accuracy of general object detectors.","Therefore, we propose a Multi-Class Min-Margin Contrastive Learning (MMCL) method that, by clarifying the category semantic information of content queries under the deformable DETR architecture, aids the model in extracting specific category foreground information from coupled features.","Specifically, after grouping content queries by the number of categories, we employ the Multi-Class Inter-Class Exclusion (MIE) loss to push apart content queries from different groups.","Concurrently, the Intra-Class Min-Margin Clustering (IMC) loss is utilized to attract content queries within the same group, while ensuring the preservation of necessary disparity.","As training, the inherent Hungarian matching of the model progressively strengthens the alignment between each group of queries and the semantic features of their corresponding category of objects.","This evolving coherence ensures a deep-seated grasp of category characteristics, consequently bolstering the anti-overlapping detection capabilities of models.","MMCL is versatile and can be easily plugged into any deformable DETR-based model with dozens of lines of code.","Extensive experiments on the PIXray and OPIXray datasets demonstrate that MMCL significantly enhances the performance of various state-of-the-art models without increasing complexity.","The code has been released at https://github.com/anonymity0403/MMCL."],"url":"http://arxiv.org/abs/2406.03176v1","category":"cs.CV"}
{"created":"2024-06-05 12:03:19","title":"StatBot.Swiss: Bilingual Open Data Exploration in Natural Language","abstract":"The potential for improvements brought by Large Language Models (LLMs) in Text-to-SQL systems is mostly assessed on monolingual English datasets. However, LLMs' performance for other languages remains vastly unexplored. In this work, we release the StatBot.Swiss dataset, the first bilingual benchmark for evaluating Text-to-SQL systems based on real-world applications. The StatBot.Swiss dataset contains 455 natural language/SQL-pairs over 35 big databases with varying level of complexity for both English and German.   We evaluate the performance of state-of-the-art LLMs such as GPT-3.5-Turbo and mixtral-8x7b-instruct for the Text-to-SQL translation task using an in-context learning approach. Our experimental analysis illustrates that current LLMs struggle to generalize well in generating SQL queries on our novel bilingual dataset.","sentences":["The potential for improvements brought by Large Language Models (LLMs) in Text-to-SQL systems is mostly assessed on monolingual English datasets.","However, LLMs' performance for other languages remains vastly unexplored.","In this work, we release the StatBot.","Swiss dataset, the first bilingual benchmark for evaluating Text-to-SQL systems based on real-world applications.","The StatBot.","Swiss dataset contains 455 natural language/SQL-pairs over 35 big databases with varying level of complexity for both English and German.   ","We evaluate the performance of state-of-the-art LLMs such as GPT-3.5-Turbo and mixtral-8x7b-instruct for the Text-to-SQL translation task using an in-context learning approach.","Our experimental analysis illustrates that current LLMs struggle to generalize well in generating SQL queries on our novel bilingual dataset."],"url":"http://arxiv.org/abs/2406.03170v1","category":"cs.CL"}
{"created":"2024-06-05 11:56:54","title":"Topological Neural Networks go Persistent, Equivariant, and Continuous","abstract":"Topological Neural Networks (TNNs) incorporate higher-order relational information beyond pairwise interactions, enabling richer representations than Graph Neural Networks (GNNs). Concurrently, topological descriptors based on persistent homology (PH) are being increasingly employed to augment the GNNs. We investigate the benefits of integrating these two paradigms. Specifically, we introduce TopNets as a broad framework that subsumes and unifies various methods in the intersection of GNNs/TNNs and PH such as (generalizations of) RePHINE and TOGL. TopNets can also be readily adapted to handle (symmetries in) geometric complexes, extending the scope of TNNs and PH to spatial settings. Theoretically, we show that PH descriptors can provably enhance the expressivity of simplicial message-passing networks. Empirically, (continuous and E(n)-equivariant extensions of) TopNets achieve strong performance across diverse tasks, including antibody design, molecular dynamics simulation, and drug property prediction.","sentences":["Topological Neural Networks (TNNs) incorporate higher-order relational information beyond pairwise interactions, enabling richer representations than Graph Neural Networks (GNNs).","Concurrently, topological descriptors based on persistent homology (PH) are being increasingly employed to augment the GNNs.","We investigate the benefits of integrating these two paradigms.","Specifically, we introduce TopNets as a broad framework that subsumes and unifies various methods in the intersection of GNNs/TNNs and PH such as (generalizations of) RePHINE and TOGL.","TopNets can also be readily adapted to handle (symmetries in) geometric complexes, extending the scope of TNNs and PH to spatial settings.","Theoretically, we show that PH descriptors can provably enhance the expressivity of simplicial message-passing networks.","Empirically, (continuous and E(n)-equivariant extensions of) TopNets achieve strong performance across diverse tasks, including antibody design, molecular dynamics simulation, and drug property prediction."],"url":"http://arxiv.org/abs/2406.03164v1","category":"cs.LG"}
{"created":"2024-06-05 11:49:17","title":"The Curse of Beam-Squint in ISAC: Causes, Implications, and Mitigation Strategies","abstract":"Integrated sensing and communications (ISAC) has emerged as a means to efficiently utilize spectrum and thereby save cost and power. At the higher end of the spectrum, ISAC systems operate at wideband using large antenna arrays to meet the stringent demands for high-resolution sensing and enhanced communications capacity. However, the wideband implementation entails beam-squint, that is, deviations in the generated beam directions because of the narrowband assumption in the analog components. This causes significant degradation in the communications capacity, target detection, and parameter estimation. This article presents the design challenges caused by beam-squint and its mitigation in ISAC systems. In this context, we also discuss several ISAC design perspectives including far-/near-field beamforming, channel/direction estimation, sparse array design, and index modulation. There are also several research opportunities in waveform design, beam training, and array processing to adequately address beam-squint in ISAC.","sentences":["Integrated sensing and communications (ISAC) has emerged as a means to efficiently utilize spectrum and thereby save cost and power.","At the higher end of the spectrum, ISAC systems operate at wideband using large antenna arrays to meet the stringent demands for high-resolution sensing and enhanced communications capacity.","However, the wideband implementation entails beam-squint, that is, deviations in the generated beam directions because of the narrowband assumption in the analog components.","This causes significant degradation in the communications capacity, target detection, and parameter estimation.","This article presents the design challenges caused by beam-squint and its mitigation in ISAC systems.","In this context, we also discuss several ISAC design perspectives including far-/near-field beamforming, channel/direction estimation, sparse array design, and index modulation.","There are also several research opportunities in waveform design, beam training, and array processing to adequately address beam-squint in ISAC."],"url":"http://arxiv.org/abs/2406.03162v1","category":"eess.SP"}
{"created":"2024-06-05 11:37:45","title":"Hurry: Dynamic Collaborative Framework For Low-orbit Mega-Constellation Data Downloading","abstract":"Low-orbit mega-constellation network, which utilize thousands of satellites to provide a variety of network services and collect a wide range of space information, is a rapidly growing field. Each satellite collects TB-level data daily, including delay-sensitive data used for crucial tasks, such as military surveillance, natural disaster monitoring, and weather forecasting. According to NASA's statement, these data need to be downloaded to the ground for processing within 3 to 5 hours. To reduce the time required for satellite data downloads, the state-of-the-art solution known as CoDld, which is only available for small constellations, uses an iterative method for cooperative downloads via inter-satellite links. However, in LMCN, the time required to download the same amount of data using CoDld will exponentially increase compared to downloading the same amount of data in a small constellation. We have identified and analyzed the reasons for this degradation phenomenon and propose a new satellite data download framework, named Hurry. By modeling and mapping satellite topology changes and data transmission to Time-Expanded Graphs, we implement our algorithm within the Hurry framework to avoid degradation effects. In the fixed data volume download evaluation, Hurry achieves 100% completion of the download task while the CoDld only reached 44% of download progress. In continuous data generation evaluation, the Hurry flow algorithm improves throughput from 11% to 66% compared to the CoDld in different scenarios.","sentences":["Low-orbit mega-constellation network, which utilize thousands of satellites to provide a variety of network services and collect a wide range of space information, is a rapidly growing field.","Each satellite collects TB-level data daily, including delay-sensitive data used for crucial tasks, such as military surveillance, natural disaster monitoring, and weather forecasting.","According to NASA's statement, these data need to be downloaded to the ground for processing within 3 to 5 hours.","To reduce the time required for satellite data downloads, the state-of-the-art solution known as CoDld, which is only available for small constellations, uses an iterative method for cooperative downloads via inter-satellite links.","However, in LMCN, the time required to download the same amount of data using CoDld will exponentially increase compared to downloading the same amount of data in a small constellation.","We have identified and analyzed the reasons for this degradation phenomenon and propose a new satellite data download framework, named Hurry.","By modeling and mapping satellite topology changes and data transmission to Time-Expanded Graphs, we implement our algorithm within the Hurry framework to avoid degradation effects.","In the fixed data volume download evaluation, Hurry achieves 100% completion of the download task while the CoDld only reached 44% of download progress.","In continuous data generation evaluation, the Hurry flow algorithm improves throughput from 11% to 66% compared to the CoDld in different scenarios."],"url":"http://arxiv.org/abs/2406.03159v1","category":"cs.NI"}
{"created":"2024-06-05 11:35:44","title":"CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs","abstract":"Despite the impressive capability of large language models (LLMs), knowing when to trust their generations remains an open challenge. The recent literature on uncertainty quantification of natural language generation (NLG) utilises a conventional natural language inference (NLI) classifier to measure the semantic dispersion of LLMs responses. These studies employ logits of NLI classifier for semantic clustering to estimate uncertainty. However, logits represent the probability of the predicted class and barely contain feature information for potential clustering. Alternatively, CLIP (Contrastive Language-Image Pre-training) performs impressively in extracting image-text pair features and measuring their similarity. To extend its usability, we propose Contrastive Semantic Similarity, the CLIP-based feature extraction module to obtain similarity features for measuring uncertainty for text pairs. We apply this method to selective NLG, which detects and rejects unreliable generations for better trustworthiness of LLMs. We conduct extensive experiments with three LLMs on several benchmark question-answering datasets with comprehensive evaluation metrics. Results show that our proposed method performs better in estimating reliable responses of LLMs than comparable baselines. Results show that our proposed method performs better in estimating reliable responses of LLMs than comparable baselines. The code are available at \\url{https://github.com/AoShuang92/css_uq_llms}.","sentences":["Despite the impressive capability of large language models (LLMs), knowing when to trust their generations remains an open challenge.","The recent literature on uncertainty quantification of natural language generation (NLG) utilises a conventional natural language inference (NLI) classifier to measure the semantic dispersion of LLMs responses.","These studies employ logits of NLI classifier for semantic clustering to estimate uncertainty.","However, logits represent the probability of the predicted class and barely contain feature information for potential clustering.","Alternatively, CLIP (Contrastive Language-Image Pre-training) performs impressively in extracting image-text pair features and measuring their similarity.","To extend its usability, we propose Contrastive Semantic Similarity, the CLIP-based feature extraction module to obtain similarity features for measuring uncertainty for text pairs.","We apply this method to selective NLG, which detects and rejects unreliable generations for better trustworthiness of LLMs.","We conduct extensive experiments with three LLMs on several benchmark question-answering datasets with comprehensive evaluation metrics.","Results show that our proposed method performs better in estimating reliable responses of LLMs than comparable baselines.","Results show that our proposed method performs better in estimating reliable responses of LLMs than comparable baselines.","The code are available at \\url{https://github.com/AoShuang92/css_uq_llms}."],"url":"http://arxiv.org/abs/2406.03158v1","category":"cs.CL"}
{"created":"2024-06-05 11:35:38","title":"A Combination Model Based on Sequential General Variational Mode Decomposition Method for Time Series Prediction","abstract":"Accurate prediction of financial time series is a key concern for market economy makers and investors. The article selects online store sales and Australian beer sales as representatives of non-stationary, trending, and seasonal financial time series, and constructs a new SGVMD-ARIMA combination model in a non-linear combination way to predict financial time series. The ARIMA model, LSTM model, and other classic decomposition prediction models are used as control models to compare the accuracy of different models. The empirical results indicate that the constructed combination prediction model has universal advantages over the single prediction model and linear combination prediction model of the control group. Within the prediction interval, our proposed combination model has improved advantages over traditional decomposition prediction control group models.","sentences":["Accurate prediction of financial time series is a key concern for market economy makers and investors.","The article selects online store sales and Australian beer sales as representatives of non-stationary, trending, and seasonal financial time series, and constructs a new SGVMD-ARIMA combination model in a non-linear combination way to predict financial time series.","The ARIMA model, LSTM model, and other classic decomposition prediction models are used as control models to compare the accuracy of different models.","The empirical results indicate that the constructed combination prediction model has universal advantages over the single prediction model and linear combination prediction model of the control group.","Within the prediction interval, our proposed combination model has improved advantages over traditional decomposition prediction control group models."],"url":"http://arxiv.org/abs/2406.03157v1","category":"eess.SP"}
{"created":"2024-06-05 11:30:16","title":"Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks: An Extended Investigation","abstract":"Recent advances in probabilistic deep learning enable efficient amortized Bayesian inference in settings where the likelihood function is only implicitly defined by a simulation program (simulation-based inference; SBI). But how faithful is such inference if the simulation represents reality somewhat inaccurately, that is, if the true system behavior at test time deviates from the one seen during training? We conceptualize the types of such model misspecification arising in SBI and systematically investigate how the performance of neural posterior approximators gradually deteriorates as a consequence, making inference results less and less trustworthy. To notify users about this problem, we propose a new misspecification measure that can be trained in an unsupervised fashion (i.e., without training data from the true distribution) and reliably detects model misspecification at test time. Our experiments clearly demonstrate the utility of our new measure both on toy examples with an analytical ground-truth and on representative scientific tasks in cell biology, cognitive decision making, disease outbreak dynamics, and computer vision. We show how the proposed misspecification test warns users about suspicious outputs, raises an alarm when predictions are not trustworthy, and guides model designers in their search for better simulators.","sentences":["Recent advances in probabilistic deep learning enable efficient amortized Bayesian inference in settings where the likelihood function is only implicitly defined by a simulation program (simulation-based inference; SBI).","But how faithful is such inference if the simulation represents reality somewhat inaccurately, that is, if the true system behavior at test time deviates from the one seen during training?","We conceptualize the types of such model misspecification arising in SBI and systematically investigate how the performance of neural posterior approximators gradually deteriorates as a consequence, making inference results less and less trustworthy.","To notify users about this problem, we propose a new misspecification measure that can be trained in an unsupervised fashion (i.e., without training data from the true distribution) and reliably detects model misspecification at test time.","Our experiments clearly demonstrate the utility of our new measure both on toy examples with an analytical ground-truth and on representative scientific tasks in cell biology, cognitive decision making, disease outbreak dynamics, and computer vision.","We show how the proposed misspecification test warns users about suspicious outputs, raises an alarm when predictions are not trustworthy, and guides model designers in their search for better simulators."],"url":"http://arxiv.org/abs/2406.03154v1","category":"cs.LG"}
{"created":"2024-06-05 11:15:45","title":"Which Side Are You On? A Multi-task Dataset for End-to-End Argument Summarisation and Evaluation","abstract":"With the recent advances of large language models (LLMs), it is no longer infeasible to build an automated debate system that helps people to synthesise persuasive arguments. Previous work attempted this task by integrating multiple components. In our work, we introduce an argument mining dataset that captures the end-to-end process of preparing an argumentative essay for a debate, which covers the tasks of claim and evidence identification (Task 1 ED), evidence convincingness ranking (Task 2 ECR), argumentative essay summarisation and human preference ranking (Task 3 ASR) and metric learning for automated evaluation of resulting essays, based on human feedback along argument quality dimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are fully annotated with the various properties supporting the aforementioned tasks. We evaluate multiple generative baselines for each of these tasks, including representative LLMs. We find, that while they show promising results on individual tasks in our benchmark, their end-to-end performance on all four tasks in succession deteriorates significantly, both in automated measures as well as in human-centred evaluation. This challenge presented by our proposed dataset motivates future research on end-to-end argument mining and summarisation. The repository of this project is available at https://github.com/HarrywillDr/ArgSum-Datatset","sentences":["With the recent advances of large language models (LLMs), it is no longer infeasible to build an automated debate system that helps people to synthesise persuasive arguments.","Previous work attempted this task by integrating multiple components.","In our work, we introduce an argument mining dataset that captures the end-to-end process of preparing an argumentative essay for a debate, which covers the tasks of claim and evidence identification (Task 1 ED), evidence convincingness ranking (Task 2 ECR), argumentative essay summarisation and human preference ranking (Task 3 ASR) and metric learning for automated evaluation of resulting essays, based on human feedback along argument quality dimensions (Task 4 SQE).","Our dataset contains 14k examples of claims that are fully annotated with the various properties supporting the aforementioned tasks.","We evaluate multiple generative baselines for each of these tasks, including representative LLMs.","We find, that while they show promising results on individual tasks in our benchmark, their end-to-end performance on all four tasks in succession deteriorates significantly, both in automated measures as well as in human-centred evaluation.","This challenge presented by our proposed dataset motivates future research on end-to-end argument mining and summarisation.","The repository of this project is available at https://github.com/HarrywillDr/ArgSum-Datatset"],"url":"http://arxiv.org/abs/2406.03151v1","category":"cs.CL"}
{"created":"2024-06-05 11:15:43","title":"Sample-specific Masks for Visual Reprogramming-based Prompting","abstract":"Visual reprogramming (VR) is a prompting technique that aims to re-purpose a pre-trained model (e.g., a classifier on ImageNet) to target tasks (e.g., medical data prediction) by learning a small-scale pattern added into input images instead of tuning considerable parameters within the model. The location of the pattern within input samples is usually determined by a pre-defined mask shared across all samples. In this paper, we show that the shared mask potentially limits VR's generalization and increases its approximation error due to the lack of sample-level adaptation. Motivated by this finding, we design a new framework for VR called sample-specific multi-channel masks (SMM). Specifically, SMM employs a lightweight ConvNet and patch-wise interpolation to generate sample-specific three-channel masks instead of a shared and pre-defined mask. Since we generate different masks for individual samples, SMM is theoretically shown to reduce approximation error for the target tasks compared with existing state-of-the-art VR methods. We also empirically demonstrate its performance gain on both ResNet and ViT. The success of SMM further highlights the broader applicability of VR in leveraging the latent knowledge of pre-trained models for various target tasks. Our code is available at https://github.com/tmlr-group/SMM.","sentences":["Visual reprogramming (VR) is a prompting technique that aims to re-purpose a pre-trained model (e.g., a classifier on ImageNet) to target tasks (e.g., medical data prediction) by learning a small-scale pattern added into input images instead of tuning considerable parameters within the model.","The location of the pattern within input samples is usually determined by a pre-defined mask shared across all samples.","In this paper, we show that the shared mask potentially limits VR's generalization and increases its approximation error due to the lack of sample-level adaptation.","Motivated by this finding, we design a new framework for VR called sample-specific multi-channel masks (SMM).","Specifically, SMM employs a lightweight ConvNet and patch-wise interpolation to generate sample-specific three-channel masks instead of a shared and pre-defined mask.","Since we generate different masks for individual samples, SMM is theoretically shown to reduce approximation error for the target tasks compared with existing state-of-the-art VR methods.","We also empirically demonstrate its performance gain on both ResNet and ViT.","The success of SMM further highlights the broader applicability of VR in leveraging the latent knowledge of pre-trained models for various target tasks.","Our code is available at https://github.com/tmlr-group/SMM."],"url":"http://arxiv.org/abs/2406.03150v1","category":"cs.LG"}
{"created":"2024-06-05 11:01:42","title":"Tiny models from tiny data: Textual and null-text inversion for few-shot distillation","abstract":"Few-shot image classification involves classifying images using very few training examples. Recent vision foundation models show excellent few-shot transfer abilities, but are large and slow at inference. Using knowledge distillation, the capabilities of high-performing but slow models can be transferred to tiny, efficient models. However, common distillation methods require a large set of unlabeled data, which is not available in the few-shot setting. To overcome this lack of data, there has been a recent interest in using synthetic data.   We expand on this work by presenting a novel diffusion model inversion technique (TINT) combining the diversity of textual inversion with the specificity of null-text inversion. Using this method in a few-shot distillation pipeline leads to state-of-the-art accuracy among small student models on popular benchmarks, while being significantly faster than prior work. This allows us to push even tiny models to high accuracy using only a tiny application-specific dataset, albeit relying on extra data for pre-training.   Popular few-shot benchmarks involve evaluation over a large number of episodes, which is computationally cumbersome for methods involving synthetic data generation. Therefore, we also present a theoretical analysis on how the variance of the accuracy estimator depends on the number of episodes and query examples, and use these results to lower the computational effort required for method evaluation. In addition, to further motivate the use of generative models in few-shot distillation, we demonstrate that our method performs better compared to training on real data mined from the dataset used to train the diffusion model.   Source code will be made available at https://github.com/pixwse/tiny2.","sentences":["Few-shot image classification involves classifying images using very few training examples.","Recent vision foundation models show excellent few-shot transfer abilities, but are large and slow at inference.","Using knowledge distillation, the capabilities of high-performing but slow models can be transferred to tiny, efficient models.","However, common distillation methods require a large set of unlabeled data, which is not available in the few-shot setting.","To overcome this lack of data, there has been a recent interest in using synthetic data.   ","We expand on this work by presenting a novel diffusion model inversion technique (TINT) combining the diversity of textual inversion with the specificity of null-text inversion.","Using this method in a few-shot distillation pipeline leads to state-of-the-art accuracy among small student models on popular benchmarks, while being significantly faster than prior work.","This allows us to push even tiny models to high accuracy using only a tiny application-specific dataset, albeit relying on extra data for pre-training.   ","Popular few-shot benchmarks involve evaluation over a large number of episodes, which is computationally cumbersome for methods involving synthetic data generation.","Therefore, we also present a theoretical analysis on how the variance of the accuracy estimator depends on the number of episodes and query examples, and use these results to lower the computational effort required for method evaluation.","In addition, to further motivate the use of generative models in few-shot distillation, we demonstrate that our method performs better compared to training on real data mined from the dataset used to train the diffusion model.   ","Source code will be made available at https://github.com/pixwse/tiny2."],"url":"http://arxiv.org/abs/2406.03146v1","category":"cs.CV"}
{"created":"2024-06-05 11:00:27","title":"E(n) Equivariant Message Passing Cellular Networks","abstract":"This paper introduces E(n) Equivariant Message Passing Cellular Networks (EMPCNs), an extension of E(n) Equivariant Graph Neural Networks to CW-complexes. Our approach addresses two aspects of geometric message passing networks: 1) enhancing their expressiveness by incorporating arbitrary cells, and 2) achieving this in a computationally efficient way with a decoupled EMPCNs technique. We demonstrate that EMPCNs achieve close to state-of-the-art performance on multiple tasks without the need for steerability, including many-body predictions and motion capture. Moreover, ablation studies confirm that decoupled EMPCNs exhibit stronger generalization capabilities than their non-topologically informed counterparts. These findings show that EMPCNs can be used as a scalable and expressive framework for higher-order message passing in geometric and topological graphs","sentences":["This paper introduces E(n) Equivariant Message Passing Cellular Networks (EMPCNs), an extension of E(n) Equivariant Graph Neural Networks to CW-complexes.","Our approach addresses two aspects of geometric message passing networks: 1) enhancing their expressiveness by incorporating arbitrary cells, and 2) achieving this in a computationally efficient way with a decoupled EMPCNs technique.","We demonstrate that EMPCNs achieve close to state-of-the-art performance on multiple tasks without the need for steerability, including many-body predictions and motion capture.","Moreover, ablation studies confirm that decoupled EMPCNs exhibit stronger generalization capabilities than their non-topologically informed counterparts.","These findings show that EMPCNs can be used as a scalable and expressive framework for higher-order message passing in geometric and topological graphs"],"url":"http://arxiv.org/abs/2406.03145v1","category":"cs.LG"}
{"created":"2024-06-05 11:00:03","title":"A Combination Model for Time Series Prediction using LSTM via Extracting Dynamic Features Based on Spatial Smoothing and Sequential General Variational Mode Decomposition","abstract":"In order to solve the problems such as difficult to extract effective features and low accuracy of sales volume prediction caused by complex relationships such as market sales volume in time series prediction, we proposed a time series prediction method of market sales volume based on Sequential General VMD and spatial smoothing Long short-term memory neural network (SS-LSTM) combination model. Firstly, the spatial smoothing algorithm is used to decompose and calculate the sample data of related industry sectors affected by the linkage effect of market sectors, extracting modal features containing information via Sequential General VMD on overall market and specific price trends; Then, according to the background of different Market data sets, LSTM network is used to model and predict the price of fundamental data and modal characteristics. The experimental results of data prediction with seasonal and periodic trends show that this method can achieve higher price prediction accuracy and more accurate accuracy in specific market contexts compared to traditional prediction methods Describe the changes in market sales volume.","sentences":["In order to solve the problems such as difficult to extract effective features and low accuracy of sales volume prediction caused by complex relationships such as market sales volume in time series prediction, we proposed a time series prediction method of market sales volume based on Sequential General VMD and spatial smoothing Long short-term memory neural network (SS-LSTM) combination model.","Firstly, the spatial smoothing algorithm is used to decompose and calculate the sample data of related industry sectors affected by the linkage effect of market sectors, extracting modal features containing information via Sequential General VMD on overall market and specific price trends; Then, according to the background of different Market data sets, LSTM network is used to model and predict the price of fundamental data and modal characteristics.","The experimental results of data prediction with seasonal and periodic trends show that this method can achieve higher price prediction accuracy and more accurate accuracy in specific market contexts compared to traditional prediction methods Describe the changes in market sales volume."],"url":"http://arxiv.org/abs/2406.03144v1","category":"eess.SP"}
{"created":"2024-06-05 10:58:15","title":"ZeroPur: Succinct Training-Free Adversarial Purification","abstract":"Adversarial purification is a kind of defense technique that can defend various unseen adversarial attacks without modifying the victim classifier. Existing methods often depend on external generative models or cooperation between auxiliary functions and victim classifiers. However, retraining generative models, auxiliary functions, or victim classifiers relies on the domain of the fine-tuned dataset and is computation-consuming. In this work, we suppose that adversarial images are outliers of the natural image manifold and the purification process can be considered as returning them to this manifold. Following this assumption, we present a simple adversarial purification method without further training to purify adversarial images, called ZeroPur. ZeroPur contains two steps: given an adversarial example, Guided Shift obtains the shifted embedding of the adversarial example by the guidance of its blurred counterparts; after that, Adaptive Projection constructs a directional vector by this shifted embedding to provide momentum, projecting adversarial images onto the manifold adaptively. ZeroPur is independent of external models and requires no retraining of victim classifiers or auxiliary functions, relying solely on victim classifiers themselves to achieve purification. Extensive experiments on three datasets (CIFAR-10, CIFAR-100, and ImageNet-1K) using various classifier architectures (ResNet, WideResNet) demonstrate that our method achieves state-of-the-art robust performance. The code will be publicly available.","sentences":["Adversarial purification is a kind of defense technique that can defend various unseen adversarial attacks without modifying the victim classifier.","Existing methods often depend on external generative models or cooperation between auxiliary functions and victim classifiers.","However, retraining generative models, auxiliary functions, or victim classifiers relies on the domain of the fine-tuned dataset and is computation-consuming.","In this work, we suppose that adversarial images are outliers of the natural image manifold and the purification process can be considered as returning them to this manifold.","Following this assumption, we present a simple adversarial purification method without further training to purify adversarial images, called ZeroPur.","ZeroPur contains two steps: given an adversarial example, Guided Shift obtains the shifted embedding of the adversarial example by the guidance of its blurred counterparts; after that, Adaptive Projection constructs a directional vector by this shifted embedding to provide momentum, projecting adversarial images onto the manifold adaptively.","ZeroPur is independent of external models and requires no retraining of victim classifiers or auxiliary functions, relying solely on victim classifiers themselves to achieve purification.","Extensive experiments on three datasets (CIFAR-10, CIFAR-100, and ImageNet-1K) using various classifier architectures (ResNet, WideResNet) demonstrate that our method achieves state-of-the-art robust performance.","The code will be publicly available."],"url":"http://arxiv.org/abs/2406.03143v1","category":"cs.CV"}
{"created":"2024-06-05 10:54:18","title":"Floating Anchor Diffusion Model for Multi-motif Scaffolding","abstract":"Motif scaffolding seeks to design scaffold structures for constructing proteins with functions derived from the desired motif, which is crucial for the design of vaccines and enzymes. Previous works approach the problem by inpainting or conditional generation. Both of them can only scaffold motifs with fixed positions, and the conditional generation cannot guarantee the presence of motifs. However, prior knowledge of the relative motif positions in a protein is not readily available, and constructing a protein with multiple functions in one protein is more general and significant because of the synergies between functions. We propose a Floating Anchor Diffusion (FADiff) model. FADiff allows motifs to float rigidly and independently in the process of diffusion, which guarantees the presence of motifs and automates the motif position design. Our experiments demonstrate the efficacy of FADiff with high success rates and designable novel scaffolds. To the best of our knowledge, FADiff is the first work to tackle the challenge of scaffolding multiple motifs without relying on the expertise of relative motif positions in the protein. Code is available at https://github.com/aim-uofa/FADiff.","sentences":["Motif scaffolding seeks to design scaffold structures for constructing proteins with functions derived from the desired motif, which is crucial for the design of vaccines and enzymes.","Previous works approach the problem by inpainting or conditional generation.","Both of them can only scaffold motifs with fixed positions, and the conditional generation cannot guarantee the presence of motifs.","However, prior knowledge of the relative motif positions in a protein is not readily available, and constructing a protein with multiple functions in one protein is more general and significant because of the synergies between functions.","We propose a Floating Anchor Diffusion (FADiff) model.","FADiff allows motifs to float rigidly and independently in the process of diffusion, which guarantees the presence of motifs and automates the motif position design.","Our experiments demonstrate the efficacy of FADiff with high success rates and designable novel scaffolds.","To the best of our knowledge, FADiff is the first work to tackle the challenge of scaffolding multiple motifs without relying on the expertise of relative motif positions in the protein.","Code is available at https://github.com/aim-uofa/FADiff."],"url":"http://arxiv.org/abs/2406.03141v1","category":"q-bio.BM"}
{"created":"2024-06-05 10:44:08","title":"Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models","abstract":"We study the computational limits of Low-Rank Adaptation (LoRA) update for finetuning transformer-based models using fine-grained complexity theory. Our key observation is that the existence of low-rank decompositions within the gradient computation of LoRA adaptation leads to possible algorithmic speedup. This allows us to (i) identify a phase transition behavior and (ii) prove the existence of nearly linear algorithms by controlling the LoRA update computation term by term, assuming the Strong Exponential Time Hypothesis (SETH). For the former, we identify a sharp transition in the efficiency of all possible rank-$r$ LoRA update algorithms for transformers, based on specific norms resulting from the multiplications of the input sequence $\\mathbf{X}$, pretrained weights $\\mathbf{W^\\star}$, and adapter matrices $\\alpha \\mathbf{B} \\mathbf{A} / r$. Specifically, we derive a shared upper bound threshold for such norms and show that efficient (sub-quadratic) approximation algorithms of LoRA exist only below this threshold. For the latter, we prove the existence of nearly linear approximation algorithms for LoRA adaptation by utilizing the hierarchical low-rank structures of LoRA gradients and approximating the gradients with a series of chained low-rank approximations. To showcase our theory, we consider two practical scenarios: partial (e.g., only $\\mathbf{W}_V$ and $\\mathbf{W}_Q$) and full adaptations (e.g., $\\mathbf{W}_Q$, $\\mathbf{W}_V$, and $\\mathbf{W}_K$) of weights in attention heads.","sentences":["We study the computational limits of Low-Rank Adaptation (LoRA) update for finetuning transformer-based models using fine-grained complexity theory.","Our key observation is that the existence of low-rank decompositions within the gradient computation of LoRA adaptation leads to possible algorithmic speedup.","This allows us to (i) identify a phase transition behavior and (ii) prove the existence of nearly linear algorithms by controlling the LoRA update computation term by term, assuming the Strong Exponential Time Hypothesis (SETH).","For the former, we identify a sharp transition in the efficiency of all possible rank-$r$ LoRA update algorithms for transformers, based on specific norms resulting from the multiplications of the input sequence $\\mathbf{X}$, pretrained weights $\\mathbf{W^\\star}$, and adapter matrices $\\alpha \\mathbf{B} \\mathbf{A} / r$.","Specifically, we derive a shared upper bound threshold for such norms and show that efficient (sub-quadratic) approximation algorithms of LoRA exist only below this threshold.","For the latter, we prove the existence of nearly linear approximation algorithms for LoRA adaptation by utilizing the hierarchical low-rank structures of LoRA gradients and approximating the gradients with a series of chained low-rank approximations.","To showcase our theory, we consider two practical scenarios: partial (e.g., only $\\mathbf{W}_V$ and $\\mathbf{W}_Q$) and full adaptations (e.g., $\\mathbf{W}_Q$, $\\mathbf{W}_V$, and $\\mathbf{W}_K$) of weights in attention heads."],"url":"http://arxiv.org/abs/2406.03136v1","category":"cs.LG"}
{"created":"2024-06-05 10:24:00","title":"Enhanced Automotive Object Detection via RGB-D Fusion in a DiffusionDet Framework","abstract":"Vision-based autonomous driving requires reliable and efficient object detection. This work proposes a DiffusionDet-based framework that exploits data fusion from the monocular camera and depth sensor to provide the RGB and depth (RGB-D) data. Within this framework, ground truth bounding boxes are randomly reshaped as part of the training phase, allowing the model to learn the reverse diffusion process of noise addition. The system methodically enhances a randomly generated set of boxes at the inference stage, guiding them toward accurate final detections. By integrating the textural and color features from RGB images with the spatial depth information from the LiDAR sensors, the proposed framework employs a feature fusion that substantially enhances object detection of automotive targets. The $2.3$ AP gain in detecting automotive targets is achieved through comprehensive experiments using the KITTI dataset. Specifically, the improved performance of the proposed approach in detecting small objects is demonstrated.","sentences":["Vision-based autonomous driving requires reliable and efficient object detection.","This work proposes a DiffusionDet-based framework that exploits data fusion from the monocular camera and depth sensor to provide the RGB and depth (RGB-D) data.","Within this framework, ground truth bounding boxes are randomly reshaped as part of the training phase, allowing the model to learn the reverse diffusion process of noise addition.","The system methodically enhances a randomly generated set of boxes at the inference stage, guiding them toward accurate final detections.","By integrating the textural and color features from RGB images with the spatial depth information from the LiDAR sensors, the proposed framework employs a feature fusion that substantially enhances object detection of automotive targets.","The $2.3$ AP gain in detecting automotive targets is achieved through comprehensive experiments using the KITTI dataset.","Specifically, the improved performance of the proposed approach in detecting small objects is demonstrated."],"url":"http://arxiv.org/abs/2406.03129v1","category":"cs.CV"}
{"created":"2024-06-05 10:22:27","title":"Towards Real-world Scenario: Imbalanced New Intent Discovery","abstract":"New Intent Discovery (NID) aims at detecting known and previously undefined categories of user intent by utilizing limited labeled and massive unlabeled data. Most prior works often operate under the unrealistic assumption that the distribution of both familiar and new intent classes is uniform, overlooking the skewed and long-tailed distributions frequently encountered in real-world scenarios. To bridge the gap, our work introduces the imbalanced new intent discovery (i-NID) task, which seeks to identify familiar and novel intent categories within long-tailed distributions. A new benchmark (ImbaNID-Bench) comprised of three datasets is created to simulate the real-world long-tail distributions. ImbaNID-Bench ranges from broad cross-domain to specific single-domain intent categories, providing a thorough representation of practical use cases. Besides, a robust baseline model ImbaNID is proposed to achieve cluster-friendly intent representations. It includes three stages: model pre-training, generation of reliable pseudo-labels, and robust representation learning that strengthens the model performance to handle the intricacies of real-world data distributions. Our extensive experiments on previous benchmarks and the newly established benchmark demonstrate the superior performance of ImbaNID in addressing the i-NID task, highlighting its potential as a powerful baseline for uncovering and categorizing user intents in imbalanced and long-tailed distributions\\footnote{\\url{https://github.com/Zkdc/i-NID}}.","sentences":["New Intent Discovery (NID) aims at detecting known and previously undefined categories of user intent by utilizing limited labeled and massive unlabeled data.","Most prior works often operate under the unrealistic assumption that the distribution of both familiar and new intent classes is uniform, overlooking the skewed and long-tailed distributions frequently encountered in real-world scenarios.","To bridge the gap, our work introduces the imbalanced new intent discovery (i-NID) task, which seeks to identify familiar and novel intent categories within long-tailed distributions.","A new benchmark (ImbaNID-Bench) comprised of three datasets is created to simulate the real-world long-tail distributions.","ImbaNID-Bench ranges from broad cross-domain to specific single-domain intent categories, providing a thorough representation of practical use cases.","Besides, a robust baseline model ImbaNID is proposed to achieve cluster-friendly intent representations.","It includes three stages: model pre-training, generation of reliable pseudo-labels, and robust representation learning that strengthens the model performance to handle the intricacies of real-world data distributions.","Our extensive experiments on previous benchmarks and the newly established benchmark demonstrate the superior performance of ImbaNID in addressing the i-NID task, highlighting its potential as a powerful baseline for uncovering and categorizing user intents in imbalanced and long-tailed distributions\\footnote{\\url{https://github.com/Zkdc/i-NID}}."],"url":"http://arxiv.org/abs/2406.03127v1","category":"cs.CL"}
{"created":"2024-06-05 10:15:17","title":"Propagation of singularities for anharmonic Schr\u00f6dinger equations","abstract":"We consider evolution equations for two classes of generalized anharmonic oscillators and the associated initial value problem in the space of tempered distributions. We prove that the Cauchy problem is well posed in anisotropic Shubin--Sobolev modulation spaces of Hilbert type, and we investigate propagation of suitable notions of singularities.","sentences":["We consider evolution equations for two classes of generalized anharmonic oscillators and the associated initial value problem in the space of tempered distributions.","We prove that the Cauchy problem is well posed in anisotropic Shubin--Sobolev modulation spaces of Hilbert type, and we investigate propagation of suitable notions of singularities."],"url":"http://arxiv.org/abs/2406.03122v1","category":"math.AP"}
{"created":"2024-06-05 10:12:47","title":"Automated Verification of Silq Quantum Programs using SMT Solvers","abstract":"We present SilVer (Silq Verification), an automated tool for verifying behaviors of quantum programs written in Silq, which is a high-level programming language for quantum computing. The goal of the verification is to ensure correctness of the Silq quantum program against user-defined specifications using SMT solvers. We introduce a programming model that is based on a quantum RAM-style computer as an interface between Silq programs and SMT proof obligations, allowing for control of quantum operations using both classical and quantum conditions. Additionally, users can employ measurement flags within the specification to easily specify conditions that measurement results require to satisfy for being a valid behavior. We provide case studies on the verification of generating entangled states and multiple oracle-based algorithms.","sentences":["We present SilVer (Silq Verification), an automated tool for verifying behaviors of quantum programs written in Silq, which is a high-level programming language for quantum computing.","The goal of the verification is to ensure correctness of the Silq quantum program against user-defined specifications using SMT solvers.","We introduce a programming model that is based on a quantum RAM-style computer as an interface between Silq programs and SMT proof obligations, allowing for control of quantum operations using both classical and quantum conditions.","Additionally, users can employ measurement flags within the specification to easily specify conditions that measurement results require to satisfy for being a valid behavior.","We provide case studies on the verification of generating entangled states and multiple oracle-based algorithms."],"url":"http://arxiv.org/abs/2406.03119v1","category":"quant-ph"}
{"created":"2024-06-05 10:10:03","title":"VQUNet: Vector Quantization U-Net for Defending Adversarial Atacks by Regularizing Unwanted Noise","abstract":"Deep Neural Networks (DNN) have become a promising paradigm when developing Artificial Intelligence (AI) and Machine Learning (ML) applications. However, DNN applications are vulnerable to fake data that are crafted with adversarial attack algorithms. Under adversarial attacks, the prediction accuracy of DNN applications suffers, making them unreliable. In order to defend against adversarial attacks, we introduce a novel noise-reduction procedure, Vector Quantization U-Net (VQUNet), to reduce adversarial noise and reconstruct data with high fidelity. VQUNet features a discrete latent representation learning through a multi-scale hierarchical structure for both noise reduction and data reconstruction. The empirical experiments show that the proposed VQUNet provides better robustness to the target DNN models, and it outperforms other state-of-the-art noise-reduction-based defense methods under various adversarial attacks for both Fashion-MNIST and CIFAR10 datasets. When there is no adversarial attack, the defense method has less than 1% accuracy degradation for both datasets.","sentences":["Deep Neural Networks (DNN) have become a promising paradigm when developing Artificial Intelligence (AI) and Machine Learning (ML) applications.","However, DNN applications are vulnerable to fake data that are crafted with adversarial attack algorithms.","Under adversarial attacks, the prediction accuracy of DNN applications suffers, making them unreliable.","In order to defend against adversarial attacks, we introduce a novel noise-reduction procedure, Vector Quantization U-Net (VQUNet), to reduce adversarial noise and reconstruct data with high fidelity.","VQUNet features a discrete latent representation learning through a multi-scale hierarchical structure for both noise reduction and data reconstruction.","The empirical experiments show that the proposed VQUNet provides better robustness to the target DNN models, and it outperforms other state-of-the-art noise-reduction-based defense methods under various adversarial attacks for both Fashion-MNIST and CIFAR10 datasets.","When there is no adversarial attack, the defense method has less than 1% accuracy degradation for both datasets."],"url":"http://arxiv.org/abs/2406.03117v1","category":"cs.CV"}
{"created":"2024-06-05 10:10:00","title":"GET: A Generative EEG Transformer for continuous context-based neural","abstract":"Generating continuous electroencephalography (EEG) signals through advanced artificial neural networks presents a novel opportunity to enhance brain-computer interface (BCI) technology. This capability has the potential to significantly enhance applications ranging from simulating dynamic brain activity and data augmentation to improving real-time epilepsy detection and BCI inference. By harnessing generative transformer neural networks, specifically designed for EEG signal generation, we can revolutionize the interpretation and interaction with neural data. Generative AI has demonstrated significant success across various domains, from natural language processing (NLP) and computer vision to content creation in visual arts and music. It distinguishes itself by using large-scale datasets to construct context windows during pre-training, a technique that has proven particularly effective in NLP, where models are fine-tuned for specific downstream tasks after extensive foundational training. However, the application of generative AI in the field of BCIs, particularly through the development of continuous, context-rich neural signal generators, has been limited. To address this, we introduce the Generative EEG Transformer (GET), a model leveraging transformer architecture tailored for EEG data. The GET model is pre-trained on diverse EEG datasets, including motor imagery and alpha wave datasets, enabling it to produce high-fidelity neural signals that maintain contextual integrity. Our empirical findings indicate that GET not only faithfully reproduces the frequency spectrum of the training data and input prompts but also robustly generates continuous neural signals. By adopting the successful training strategies of the NLP domain for BCIs, the GET sets a new standard for the development and application of neural signal generation technologies.","sentences":["Generating continuous electroencephalography (EEG) signals through advanced artificial neural networks presents a novel opportunity to enhance brain-computer interface (BCI) technology.","This capability has the potential to significantly enhance applications ranging from simulating dynamic brain activity and data augmentation to improving real-time epilepsy detection and BCI inference.","By harnessing generative transformer neural networks, specifically designed for EEG signal generation, we can revolutionize the interpretation and interaction with neural data.","Generative AI has demonstrated significant success across various domains, from natural language processing (NLP) and computer vision to content creation in visual arts and music.","It distinguishes itself by using large-scale datasets to construct context windows during pre-training, a technique that has proven particularly effective in NLP, where models are fine-tuned for specific downstream tasks after extensive foundational training.","However, the application of generative AI in the field of BCIs, particularly through the development of continuous, context-rich neural signal generators, has been limited.","To address this, we introduce the Generative EEG Transformer (GET), a model leveraging transformer architecture tailored for EEG data.","The GET model is pre-trained on diverse EEG datasets, including motor imagery and alpha wave datasets, enabling it to produce high-fidelity neural signals that maintain contextual integrity.","Our empirical findings indicate that GET not only faithfully reproduces the frequency spectrum of the training data and input prompts but also robustly generates continuous neural signals.","By adopting the successful training strategies of the NLP domain for BCIs, the GET sets a new standard for the development and application of neural signal generation technologies."],"url":"http://arxiv.org/abs/2406.03115v1","category":"q-bio.NC"}
{"created":"2024-06-05 09:50:00","title":"Reproducing Kernel Thesis of Hankel Operators on Weighted Hardy Spaces","abstract":"We study the boundedness of Hankel operators between two weighted spaces, with Muckenhoupt weights. In particular, we consider whether the Reproducing Kernel Thesis for Hankel operators generalizes to the case of two different weights. There, Hankel operators are bounded on the Hardy space if and only if they are bounded when tested on reproducing kernel functions. The supremum of testing the Hankel operator on this special class of functions is called the Garsia norm of the symbol of the Hankel operator, known to be equivalent to the BMO norm of the operator. We formulate a two-weight version of the Garsia norm and prove that testing the Hankel operator on the same reproducing kernel functions and measuring the norm in the two-weight setting is sufficient to prove that the operator is bounded. In the process,we prove that the Garsia norm is equivalent to the weighted Garsia norm, when two weights are the same, and we prove the two-weight version of the Carleson embedding theorem.","sentences":["We study the boundedness of Hankel operators between two weighted spaces, with Muckenhoupt weights.","In particular, we consider whether the Reproducing Kernel Thesis for Hankel operators generalizes to the case of two different weights.","There, Hankel operators are bounded on the Hardy space if and only if they are bounded when tested on reproducing kernel functions.","The supremum of testing the Hankel operator on this special class of functions is called the Garsia norm of the symbol of the Hankel operator, known to be equivalent to the BMO norm of the operator.","We formulate a two-weight version of the Garsia norm and prove that testing the Hankel operator on the same reproducing kernel functions and measuring the norm in the two-weight setting is sufficient to prove that the operator is bounded.","In the process,we prove that the Garsia norm is equivalent to the weighted Garsia norm, when two weights are the same, and we prove the two-weight version of the Carleson embedding theorem."],"url":"http://arxiv.org/abs/2406.03106v1","category":"math.CA"}
{"created":"2024-06-05 09:45:56","title":"EpidermaQuant: Unsupervised detection and quantification of epidermal differentiation markers on H-DAB-stained images of reconstructed human epidermis","abstract":"The integrity of the reconstructed human epidermis generated in vitro could be assessed using histological analyses combined with immunohistochemical staining of keratinocyte differentiation markers. Computer-based analysis of scanned tissue saves the expert time and may improve the accuracy of quantification by eliminating interrater reliability issues. However, technical differences during the preparation and capture of stained images and the presence of multiple artifacts may influence the outcome of computational methods. Using a dataset with 598 unannotated images showing cross-sections of in vitro reconstructed human epidermis stained with DAB-based immunohistochemistry reaction to visualize 4 different keratinocyte differentiation marker proteins (filaggrin, keratin 10, Ki67, HSPA2) and counterstained with hematoxylin, we developed an unsupervised method for the detection and quantification of immunohistochemical staining. The proposed pipeline includes the following steps: (i) color normalization to reduce the variability of pixel intensity values in different samples; (ii) color deconvolution to acquire color channels of the stains used; (iii) morphological operations to find the background area of the image; (iv) automatic image rotation; and (v) finding markers of human epidermal differentiation with clustering. Also, we created a method to exclude images without DAB-stained areas. The most effective combination of methods includes: (i) Reinhard's normalization; (ii) Ruifrok and Johnston color deconvolution method; (iii) proposed image rotation method based on boundary distribution of image intensity; (iv) k-means clustering using DAB stain intensity. These results should enhance the performance of quantitative analysis of protein markers in reconstructed human epidermis samples and enable comparison of their spatial distribution between different experimental conditions.","sentences":["The integrity of the reconstructed human epidermis generated in vitro could be assessed using histological analyses combined with immunohistochemical staining of keratinocyte differentiation markers.","Computer-based analysis of scanned tissue saves the expert time and may improve the accuracy of quantification by eliminating interrater reliability issues.","However, technical differences during the preparation and capture of stained images and the presence of multiple artifacts may influence the outcome of computational methods.","Using a dataset with 598 unannotated images showing cross-sections of in vitro reconstructed human epidermis stained with DAB-based immunohistochemistry reaction to visualize 4 different keratinocyte differentiation marker proteins (filaggrin, keratin 10, Ki67, HSPA2) and counterstained with hematoxylin, we developed an unsupervised method for the detection and quantification of immunohistochemical staining.","The proposed pipeline includes the following steps: (i) color normalization to reduce the variability of pixel intensity values in different samples; (ii) color deconvolution to acquire color channels of the stains used; (iii) morphological operations to find the background area of the image; (iv) automatic image rotation; and (v) finding markers of human epidermal differentiation with clustering.","Also, we created a method to exclude images without DAB-stained areas.","The most effective combination of methods includes: (i) Reinhard's normalization; (ii) Ruifrok and Johnston color deconvolution method; (iii) proposed image rotation method based on boundary distribution of image intensity; (iv) k-means clustering using DAB stain intensity.","These results should enhance the performance of quantitative analysis of protein markers in reconstructed human epidermis samples and enable comparison of their spatial distribution between different experimental conditions."],"url":"http://arxiv.org/abs/2406.03103v1","category":"eess.IV"}
{"created":"2024-06-05 09:45:26","title":"DEER: A Delay-Resilient Framework for Reinforcement Learning with Variable Delays","abstract":"Classic reinforcement learning (RL) frequently confronts challenges in tasks involving delays, which cause a mismatch between received observations and subsequent actions, thereby deviating from the Markov assumption. Existing methods usually tackle this issue with end-to-end solutions using state augmentation. However, these black-box approaches often involve incomprehensible processes and redundant information in the information states, causing instability and potentially undermining the overall performance. To alleviate the delay challenges in RL, we propose $\\textbf{DEER (Delay-resilient Encoder-Enhanced RL)}$, a framework designed to effectively enhance the interpretability and address the random delay issues. DEER employs a pretrained encoder to map delayed states, along with their variable-length past action sequences resulting from different delays, into hidden states, which is trained on delay-free environment datasets. In a variety of delayed scenarios, the trained encoder can seamlessly integrate with standard RL algorithms without requiring additional modifications and enhance the delay-solving capability by simply adapting the input dimension of the original algorithms. We evaluate DEER through extensive experiments on Gym and Mujoco environments. The results confirm that DEER is superior to state-of-the-art RL algorithms in both constant and random delay settings.","sentences":["Classic reinforcement learning (RL) frequently confronts challenges in tasks involving delays, which cause a mismatch between received observations and subsequent actions, thereby deviating from the Markov assumption.","Existing methods usually tackle this issue with end-to-end solutions using state augmentation.","However, these black-box approaches often involve incomprehensible processes and redundant information in the information states, causing instability and potentially undermining the overall performance.","To alleviate the delay challenges in RL, we propose $\\textbf{DEER (Delay-resilient Encoder-Enhanced RL)}$, a framework designed to effectively enhance the interpretability and address the random delay issues.","DEER employs a pretrained encoder to map delayed states, along with their variable-length past action sequences resulting from different delays, into hidden states, which is trained on delay-free environment datasets.","In a variety of delayed scenarios, the trained encoder can seamlessly integrate with standard RL algorithms without requiring additional modifications and enhance the delay-solving capability by simply adapting the input dimension of the original algorithms.","We evaluate DEER through extensive experiments on Gym and Mujoco environments.","The results confirm that DEER is superior to state-of-the-art RL algorithms in both constant and random delay settings."],"url":"http://arxiv.org/abs/2406.03102v1","category":"cs.LG"}
{"created":"2024-06-05 09:44:06","title":"Anisotropic Durgapal-Fuloria Neutron Stars in $f(\\mathcal{R},\\mathrm{T}^{2})$ Gravity","abstract":"The main purpose of this paper is to obtain physically stable stellar models coupled with anisotropic matter distribution in the context of $f(\\mathcal{R},\\mathrm{T}^{2})$ theory. For this, we consider a static spherical geometry and formulate modified field equations containing various unknowns such as matter determinants and metric potentials. We then obtain a unique solution to these equations by employing Durgapal-Fuloria ansatz possessing a constant doublet. We also use matching criteria to calculate the values of these constants by considering the Schwarzschild exterior spacetime. Two different viable models of this modified theory are adopted to analyze the behavior of effective matter variables, anisotropy, energy conditions, compactness and redshift in the interiors of Her X-1, PSR J0348-0432, LMC X-4, SMC X-1, Cen X-3, and SAX J 1808.4-3658 star candidates. We also check the stability of these models by using three different physical tests. It is concluded that our considered stars satisfy all the physical requirements and are stable in this modified gravity for the considered parametric values.","sentences":["The main purpose of this paper is to obtain physically stable stellar models coupled with anisotropic matter distribution in the context of $f(\\mathcal{R},\\mathrm{T}^{2})$ theory.","For this, we consider a static spherical geometry and formulate modified field equations containing various unknowns such as matter determinants and metric potentials.","We then obtain a unique solution to these equations by employing Durgapal-Fuloria ansatz possessing a constant doublet.","We also use matching criteria to calculate the values of these constants by considering the Schwarzschild exterior spacetime.","Two different viable models of this modified theory are adopted to analyze the behavior of effective matter variables, anisotropy, energy conditions, compactness and redshift in the interiors of Her X-1, PSR J0348-0432, LMC X-4, SMC X-1, Cen X-3, and SAX J 1808.4-3658 star candidates.","We also check the stability of these models by using three different physical tests.","It is concluded that our considered stars satisfy all the physical requirements and are stable in this modified gravity for the considered parametric values."],"url":"http://arxiv.org/abs/2406.03101v1","category":"gr-qc"}
{"created":"2024-06-05 09:42:43","title":"Graph Convolutional Branch and Bound","abstract":"This article demonstrates the effectiveness of employing a deep learning model in an optimization pipeline. Specifically, in a generic exact algorithm for a NP problem, multiple heuristic criteria are usually used to guide the search of the optimum within the set of all feasible solutions. In this context, neural networks can be leveraged to rapidly acquire valuable information, enabling the identification of a more expedient path in this vast space. So, after the explanation of the tackled traveling salesman problem, the implemented branch and bound for its classical resolution is described. This algorithm is then compared with its hybrid version termed \"graph convolutional branch and bound\" that integrates the previous branch and bound with a graph convolutional neural network. The empirical results obtained highlight the efficacy of this approach, leading to conclusive findings and suggesting potential directions for future research.","sentences":["This article demonstrates the effectiveness of employing a deep learning model in an optimization pipeline.","Specifically, in a generic exact algorithm for a NP problem, multiple heuristic criteria are usually used to guide the search of the optimum within the set of all feasible solutions.","In this context, neural networks can be leveraged to rapidly acquire valuable information, enabling the identification of a more expedient path in this vast space.","So, after the explanation of the tackled traveling salesman problem, the implemented branch and bound for its classical resolution is described.","This algorithm is then compared with its hybrid version termed \"graph convolutional branch and bound\" that integrates the previous branch and bound with a graph convolutional neural network.","The empirical results obtained highlight the efficacy of this approach, leading to conclusive findings and suggesting potential directions for future research."],"url":"http://arxiv.org/abs/2406.03099v1","category":"cs.LG"}
{"created":"2024-06-05 09:40:56","title":"A Data and Model-Driven Deep Learning Approach to Robust Downlink Beamforming Optimization","abstract":"This paper investigates the optimization of the long-standing probabilistically robust transmit beamforming problem with channel uncertainties in the multiuser multiple-input single-output (MISO) downlink transmission. This problem poses significant analytical and computational challenges. Currently, the state-of-the-art optimization method relies on convex restrictions as tractable approximations to ensure robustness against Gaussian channel uncertainties. However, this method not only exhibits high computational complexity and suffers from the rank relaxation issue but also yields conservative solutions. In this paper, we propose an unsupervised deep learning-based approach that incorporates the sampling of channel uncertainties in the training process to optimize the probabilistic system performance. We introduce a model-driven learning approach that defines a new beamforming structure with trainable parameters to account for channel uncertainties. Additionally, we employ a graph neural network to efficiently infer the key beamforming parameters. We successfully apply this approach to the minimum rate quantile maximization problem subject to outage and total power constraints. Furthermore, we propose a bisection search method to address the more challenging power minimization problem with probabilistic rate constraints by leveraging the aforementioned approach. Numerical results confirm that our approach achieves non-conservative robust performance, higher data rates, greater power efficiency, and faster execution compared to state-of-the-art optimization methods.","sentences":["This paper investigates the optimization of the long-standing probabilistically robust transmit beamforming problem with channel uncertainties in the multiuser multiple-input single-output (MISO) downlink transmission.","This problem poses significant analytical and computational challenges.","Currently, the state-of-the-art optimization method relies on convex restrictions as tractable approximations to ensure robustness against Gaussian channel uncertainties.","However, this method not only exhibits high computational complexity and suffers from the rank relaxation issue but also yields conservative solutions.","In this paper, we propose an unsupervised deep learning-based approach that incorporates the sampling of channel uncertainties in the training process to optimize the probabilistic system performance.","We introduce a model-driven learning approach that defines a new beamforming structure with trainable parameters to account for channel uncertainties.","Additionally, we employ a graph neural network to efficiently infer the key beamforming parameters.","We successfully apply this approach to the minimum rate quantile maximization problem subject to outage and total power constraints.","Furthermore, we propose a bisection search method to address the more challenging power minimization problem with probabilistic rate constraints by leveraging the aforementioned approach.","Numerical results confirm that our approach achieves non-conservative robust performance, higher data rates, greater power efficiency, and faster execution compared to state-of-the-art optimization methods."],"url":"http://arxiv.org/abs/2406.03098v1","category":"cs.IT"}
{"created":"2024-06-05 09:40:08","title":"Enhancing the Resilience of Graph Neural Networks to Topological Perturbations in Sparse Graphs","abstract":"Graph neural networks (GNNs) have been extensively employed in node classification. Nevertheless, recent studies indicate that GNNs are vulnerable to topological perturbations, such as adversarial attacks and edge disruptions. Considerable efforts have been devoted to mitigating these challenges. For example, pioneering Bayesian methodologies, including GraphSS and LlnDT, incorporate Bayesian label transitions and topology-based label sampling to strengthen the robustness of GNNs. However, GraphSS is hindered by slow convergence, while LlnDT faces challenges in sparse graphs. To overcome these limitations, we propose a novel label inference framework, TraTopo, which combines topology-driven label propagation, Bayesian label transitions, and link analysis via random walks. TraTopo significantly surpasses its predecessors on sparse graphs by utilizing random walk sampling, specifically targeting isolated nodes for link prediction, thus enhancing its effectiveness in topological sampling contexts. Additionally, TraTopo employs a shortest-path strategy to refine link prediction, thereby reducing predictive overhead and improving label inference accuracy. Empirical evaluations highlight TraTopo's superiority in node classification, significantly exceeding contemporary GCN models in accuracy.","sentences":["Graph neural networks (GNNs) have been extensively employed in node classification.","Nevertheless, recent studies indicate that GNNs are vulnerable to topological perturbations, such as adversarial attacks and edge disruptions.","Considerable efforts have been devoted to mitigating these challenges.","For example, pioneering Bayesian methodologies, including GraphSS and LlnDT, incorporate Bayesian label transitions and topology-based label sampling to strengthen the robustness of GNNs.","However, GraphSS is hindered by slow convergence, while LlnDT faces challenges in sparse graphs.","To overcome these limitations, we propose a novel label inference framework, TraTopo, which combines topology-driven label propagation, Bayesian label transitions, and link analysis via random walks.","TraTopo significantly surpasses its predecessors on sparse graphs by utilizing random walk sampling, specifically targeting isolated nodes for link prediction, thus enhancing its effectiveness in topological sampling contexts.","Additionally, TraTopo employs a shortest-path strategy to refine link prediction, thereby reducing predictive overhead and improving label inference accuracy.","Empirical evaluations highlight TraTopo's superiority in node classification, significantly exceeding contemporary GCN models in accuracy."],"url":"http://arxiv.org/abs/2406.03097v1","category":"cs.LG"}
{"created":"2024-06-05 09:36:15","title":"EgoSurgery-Tool: A Dataset of Surgical Tool and Hand Detection from Egocentric Open Surgery Videos","abstract":"Surgical tool detection is a fundamental task for understanding egocentric open surgery videos. However, detecting surgical tools presents significant challenges due to their highly imbalanced class distribution, similar shapes and similar textures, and heavy occlusion. The lack of a comprehensive large-scale dataset compounds these challenges. In this paper, we introduce EgoSurgery-Tool, an extension of the existing EgoSurgery-Phase dataset, which contains real open surgery videos captured using an egocentric camera attached to the surgeon's head, along with phase annotations. EgoSurgery-Tool has been densely annotated with surgical tools and comprises over 49K surgical tool bounding boxes across 15 categories, constituting a large-scale surgical tool detection dataset. EgoSurgery-Tool also provides annotations for hand detection with over 46K hand-bounding boxes, capturing hand-object interactions that are crucial for understanding activities in egocentric open surgery. EgoSurgery-Tool is superior to existing datasets due to its larger scale, greater variety of surgical tools, more annotations, and denser scenes. We conduct a comprehensive analysis of EgoSurgery-Tool using nine popular object detectors to assess their effectiveness in both surgical tool and hand detection. The dataset will be released at https://github.com/Fujiry0/EgoSurgery.","sentences":["Surgical tool detection is a fundamental task for understanding egocentric open surgery videos.","However, detecting surgical tools presents significant challenges due to their highly imbalanced class distribution, similar shapes and similar textures, and heavy occlusion.","The lack of a comprehensive large-scale dataset compounds these challenges.","In this paper, we introduce EgoSurgery-Tool, an extension of the existing EgoSurgery-Phase dataset, which contains real open surgery videos captured using an egocentric camera attached to the surgeon's head, along with phase annotations.","EgoSurgery-Tool has been densely annotated with surgical tools and comprises over 49K surgical tool bounding boxes across 15 categories, constituting a large-scale surgical tool detection dataset.","EgoSurgery-Tool also provides annotations for hand detection with over 46K hand-bounding boxes, capturing hand-object interactions that are crucial for understanding activities in egocentric open surgery.","EgoSurgery-Tool is superior to existing datasets due to its larger scale, greater variety of surgical tools, more annotations, and denser scenes.","We conduct a comprehensive analysis of EgoSurgery-Tool using nine popular object detectors to assess their effectiveness in both surgical tool and hand detection.","The dataset will be released at https://github.com/Fujiry0/EgoSurgery."],"url":"http://arxiv.org/abs/2406.03095v1","category":"cs.CV"}
{"created":"2024-06-05 09:33:48","title":"Modelling the propagation of slow magneto-acoustic waves in a multi-stranded coronal loop","abstract":"We study the propagation properties of slow magneto-acoustic waves in a multi-thermal coronal loop using a 3D MHD model, for the first time. A bundle of 33 vertical cylinders, each of 100{\\,}km radius, randomly distributed over a circular region of radius 1{\\,}Mm is considered to represent the coronal loop. The slow waves are driven by perturbing the vertical velocity ($v_z$) at the base of the loop. We apply forward modelling to the simulation results to generate synthetic images in the coronal channels of SDO/AIA. Furthermore, we add appropriate data noise to enable direct comparison with the real observations. It is found that the synthetic images at the instrument resolution show non-cospatial features in different temperature channels in agreement with previous observations. Time-distance maps are constructed from the synthetic data to study the propagation properties. The results indicate that the oscillations are only visible in specific channels depending on the temperature range of plasma existing within the loop. Additionally, the propagation speed of slow waves is also found to be sensitive to the available temperature range. Overall, we propose that the cross-field thermal properties of coronal structures can be inferred using a combination of numerical simulations and observations of slow magneto-acoustic waves.","sentences":["We study the propagation properties of slow magneto-acoustic waves in a multi-thermal coronal loop using a 3D MHD model, for the first time.","A bundle of 33 vertical cylinders, each of 100{\\,}km radius, randomly distributed over a circular region of radius 1{\\,}Mm is considered to represent the coronal loop.","The slow waves are driven by perturbing the vertical velocity ($v_z$) at the base of the loop.","We apply forward modelling to the simulation results to generate synthetic images in the coronal channels of SDO/AIA.","Furthermore, we add appropriate data noise to enable direct comparison with the real observations.","It is found that the synthetic images at the instrument resolution show non-cospatial features in different temperature channels in agreement with previous observations.","Time-distance maps are constructed from the synthetic data to study the propagation properties.","The results indicate that the oscillations are only visible in specific channels depending on the temperature range of plasma existing within the loop.","Additionally, the propagation speed of slow waves is also found to be sensitive to the available temperature range.","Overall, we propose that the cross-field thermal properties of coronal structures can be inferred using a combination of numerical simulations and observations of slow magneto-acoustic waves."],"url":"http://arxiv.org/abs/2406.03093v1","category":"astro-ph.SR"}
{"created":"2024-06-05 09:31:37","title":"FragRel: Exploiting Fragment-level Relations in the External Memory of Large Language Models","abstract":"To process contexts with unlimited length using Large Language Models (LLMs), recent studies explore hierarchically managing the long text. Only several text fragments are taken from the external memory and passed into the temporary working memory, i.e., LLM's context window. However, existing approaches isolatedly handle the text fragments without considering their structural connections, thereby suffering limited capability on texts with intensive inter-relations, e.g., coherent stories and code repositories. This work attempts to resolve this by exploiting the fragment-level relations in external memory. First, we formulate the fragment-level relations and present several instantiations for different text types. Next, we introduce a relation-aware fragment assessment criteria upon previous independent fragment assessment. Finally, we present the fragment-connected Hierarchical Memory based LLM. We validate the benefits of involving these relations on long story understanding, repository-level code generation, and long-term chatting.","sentences":["To process contexts with unlimited length using Large Language Models (LLMs), recent studies explore hierarchically managing the long text.","Only several text fragments are taken from the external memory and passed into the temporary working memory, i.e., LLM's context window.","However, existing approaches isolatedly handle the text fragments without considering their structural connections, thereby suffering limited capability on texts with intensive inter-relations, e.g., coherent stories and code repositories.","This work attempts to resolve this by exploiting the fragment-level relations in external memory.","First, we formulate the fragment-level relations and present several instantiations for different text types.","Next, we introduce a relation-aware fragment assessment criteria upon previous independent fragment assessment.","Finally, we present the fragment-connected Hierarchical Memory based LLM.","We validate the benefits of involving these relations on long story understanding, repository-level code generation, and long-term chatting."],"url":"http://arxiv.org/abs/2406.03092v1","category":"cs.CL"}
{"created":"2024-06-05 09:30:48","title":"Improving Plan Execution Flexibility using Block-Substitution","abstract":"Partial-order plans in AI planning facilitate execution flexibility due to their less-constrained nature. Maximizing plan flexibility has been studied through the notions of plan deordering, and plan reordering. Plan deordering removes unnecessary action orderings within a plan, while plan reordering modifies them arbitrarily to minimize action orderings. This study, in contrast with traditional plan deordering and reordering strategies, improves a plan's flexibility by substituting its subplans with actions outside the plan for a planning problem. We exploit block deordering, which eliminates orderings in a POP by encapsulating coherent actions in blocks, to construct action blocks as candidate subplans for substitutions. In addition, this paper introduces a pruning technique for eliminating redundant actions within a BDPO plan. We also evaluate our approach when combined with MaxSAT-based reorderings. Our experimental result demonstrates a significant improvement in plan execution flexibility on the benchmark problems from International Planning Competitions (IPC), maintaining good coverage and execution time.","sentences":["Partial-order plans in AI planning facilitate execution flexibility due to their less-constrained nature.","Maximizing plan flexibility has been studied through the notions of plan deordering, and plan reordering.","Plan deordering removes unnecessary action orderings within a plan, while plan reordering modifies them arbitrarily to minimize action orderings.","This study, in contrast with traditional plan deordering and reordering strategies, improves a plan's flexibility by substituting its subplans with actions outside the plan for a planning problem.","We exploit block deordering, which eliminates orderings in a POP by encapsulating coherent actions in blocks, to construct action blocks as candidate subplans for substitutions.","In addition, this paper introduces a pruning technique for eliminating redundant actions within a BDPO plan.","We also evaluate our approach when combined with MaxSAT-based reorderings.","Our experimental result demonstrates a significant improvement in plan execution flexibility on the benchmark problems from International Planning Competitions (IPC), maintaining good coverage and execution time."],"url":"http://arxiv.org/abs/2406.03091v1","category":"cs.AI"}
{"created":"2024-06-05 09:29:26","title":"The Optimal Production Transport: Model and Algorithm","abstract":"In this paper, we propose the optimal production transport model, which is an extension of the classical optimal transport model. We observe in economics, the production of the factories can always be adjusted within a certain range, while the classical optimal transport does not take this situation into account. Therefore, differing from the classical optimal transport, one of the marginals is allowed to vary within a certain range in our proposed model. To address this, we introduce a multiple relaxation optimal production transport model and propose the generalized alternating Sinkhorn algorithms, inspired by the Sinkhorn algorithm and the double regularization method. By incorporating multiple relaxation variables and multiple regularization terms, the inequality and capacity constraints in the optimal production transport model are naturally satisfied. Alternating iteration algorithms are derived based on the duality of the regularized model. We also provide a theoretical analysis to guarantee the convergence of our proposed algorithms. Numerical results indicate significant advantages in terms of accuracy and efficiency. Furthermore, we apply the optimal production transport model to the coal production and transport problem. Numerical simulation demonstrates that our proposed model can save the production and transport cost by 13.17%.","sentences":["In this paper, we propose the optimal production transport model, which is an extension of the classical optimal transport model.","We observe in economics, the production of the factories can always be adjusted within a certain range, while the classical optimal transport does not take this situation into account.","Therefore, differing from the classical optimal transport, one of the marginals is allowed to vary within a certain range in our proposed model.","To address this, we introduce a multiple relaxation optimal production transport model and propose the generalized alternating Sinkhorn algorithms, inspired by the Sinkhorn algorithm and the double regularization method.","By incorporating multiple relaxation variables and multiple regularization terms, the inequality and capacity constraints in the optimal production transport model are naturally satisfied.","Alternating iteration algorithms are derived based on the duality of the regularized model.","We also provide a theoretical analysis to guarantee the convergence of our proposed algorithms.","Numerical results indicate significant advantages in terms of accuracy and efficiency.","Furthermore, we apply the optimal production transport model to the coal production and transport problem.","Numerical simulation demonstrates that our proposed model can save the production and transport cost by 13.17%."],"url":"http://arxiv.org/abs/2406.03090v1","category":"math.OC"}
{"created":"2024-06-05 09:24:10","title":"Lossless Image Compression Using Multi-level Dictionaries: Binary Images","abstract":"Lossless image compression is required in various applications to reduce storage or transmission costs of images, while requiring the reconstructed images to have zero information loss compared to the original. Existing lossless image compression methods either have simple design but poor compression performance, or complex design, better performance, but with no performance guarantees. In our endeavor to develop a lossless image compression method with low complexity and guaranteed performance, we argue that compressibility of a color image is essentially derived from the patterns in its spatial structure, intensity variations, and color variations. Thus, we divide the overall design of a lossless image compression scheme into three parts that exploit corresponding redundancies. We further argue that the binarized version of an image captures its fundamental spatial structure and in this work, we propose a scheme for lossless compression of binary images.   The proposed scheme first learns dictionaries of $16\\times16$, $8\\times8$, $4\\times4$, and $2\\times 2$ square pixel patterns from various datasets of binary images. It then uses these dictionaries to encode binary images. These dictionaries have various interesting properties that are further exploited to construct an efficient scheme. Our preliminary results show that the proposed scheme consistently outperforms existing conventional and learning based lossless compression approaches, and provides, on average, as much as $1.5\\times$ better performance than a common general purpose lossless compression scheme (WebP), more than $3\\times$ better performance than a state of the art learning based scheme, and better performance than a specialized scheme for binary image compression (JBIG2).","sentences":["Lossless image compression is required in various applications to reduce storage or transmission costs of images, while requiring the reconstructed images to have zero information loss compared to the original.","Existing lossless image compression methods either have simple design but poor compression performance, or complex design, better performance, but with no performance guarantees.","In our endeavor to develop a lossless image compression method with low complexity and guaranteed performance, we argue that compressibility of a color image is essentially derived from the patterns in its spatial structure, intensity variations, and color variations.","Thus, we divide the overall design of a lossless image compression scheme into three parts that exploit corresponding redundancies.","We further argue that the binarized version of an image captures its fundamental spatial structure and in this work, we propose a scheme for lossless compression of binary images.   ","The proposed scheme first learns dictionaries of $16\\times16$, $8\\times8$, $4\\times4$, and $2\\times 2$ square pixel patterns from various datasets of binary images.","It then uses these dictionaries to encode binary images.","These dictionaries have various interesting properties that are further exploited to construct an efficient scheme.","Our preliminary results show that the proposed scheme consistently outperforms existing conventional and learning based lossless compression approaches, and provides, on average, as much as $1.5\\times$ better performance than a common general purpose lossless compression scheme (WebP), more than $3\\times$ better performance than a state of the art learning based scheme, and better performance than a specialized scheme for binary image compression (JBIG2)."],"url":"http://arxiv.org/abs/2406.03087v1","category":"cs.IT"}
{"created":"2024-06-05 09:22:19","title":"Task-Oriented Wireless Communications for Collaborative Perception in Intelligent Unmanned Systems","abstract":"Collaborative Perception (CP) has shown great potential to achieve more holistic and reliable environmental perception in intelligent unmanned systems (IUSs). However, implementing CP still faces key challenges due to the characteristics of the CP task and the dynamics of wireless channels. In this article, a task-oriented wireless communication framework is proposed to jointly optimize the communication scheme and the CP procedure. We first propose channel-adaptive compression and robust fusion approaches to extract and exploit the most valuable semantic information under wireless communication constraints. We then propose a task-oriented distributed scheduling algorithm to identify the best collaborators for CP under dynamic environments. The main idea is learning while scheduling, where the collaboration utility is effectively learned with low computation and communication overhead. Case studies are carried out in connected autonomous driving scenarios to verify the proposed framework. Finally, we identify several future research directions.","sentences":["Collaborative Perception (CP) has shown great potential to achieve more holistic and reliable environmental perception in intelligent unmanned systems (IUSs).","However, implementing CP still faces key challenges due to the characteristics of the CP task and the dynamics of wireless channels.","In this article, a task-oriented wireless communication framework is proposed to jointly optimize the communication scheme and the CP procedure.","We first propose channel-adaptive compression and robust fusion approaches to extract and exploit the most valuable semantic information under wireless communication constraints.","We then propose a task-oriented distributed scheduling algorithm to identify the best collaborators for CP under dynamic environments.","The main idea is learning while scheduling, where the collaboration utility is effectively learned with low computation and communication overhead.","Case studies are carried out in connected autonomous driving scenarios to verify the proposed framework.","Finally, we identify several future research directions."],"url":"http://arxiv.org/abs/2406.03086v1","category":"cs.MA"}
{"created":"2024-06-05 09:19:54","title":"Exploring User Retrieval Integration towards Large Language Models for Cross-Domain Sequential Recommendation","abstract":"Cross-Domain Sequential Recommendation (CDSR) aims to mine and transfer users' sequential preferences across different domains to alleviate the long-standing cold-start issue. Traditional CDSR models capture collaborative information through user and item modeling while overlooking valuable semantic information. Recently, Large Language Model (LLM) has demonstrated powerful semantic reasoning capabilities, motivating us to introduce them to better capture semantic information. However, introducing LLMs to CDSR is non-trivial due to two crucial issues: seamless information integration and domain-specific generation. To this end, we propose a novel framework named URLLM, which aims to improve the CDSR performance by exploring the User Retrieval approach and domain grounding on LLM simultaneously. Specifically, we first present a novel dual-graph sequential model to capture the diverse information, along with an alignment and contrastive learning method to facilitate domain knowledge transfer. Subsequently, a user retrieve-generation model is adopted to seamlessly integrate the structural information into LLM, fully harnessing its emergent inferencing ability. Furthermore, we propose a domain-specific strategy and a refinement module to prevent out-of-domain generation. Extensive experiments on Amazon demonstrated the information integration and domain-specific generation ability of URLLM in comparison to state-of-the-art baselines. Our code is available at https://github.com/TingJShen/URLLM","sentences":["Cross-Domain Sequential Recommendation (CDSR) aims to mine and transfer users' sequential preferences across different domains to alleviate the long-standing cold-start issue.","Traditional CDSR models capture collaborative information through user and item modeling while overlooking valuable semantic information.","Recently, Large Language Model (LLM) has demonstrated powerful semantic reasoning capabilities, motivating us to introduce them to better capture semantic information.","However, introducing LLMs to CDSR is non-trivial due to two crucial issues: seamless information integration and domain-specific generation.","To this end, we propose a novel framework named URLLM, which aims to improve the CDSR performance by exploring the User Retrieval approach and domain grounding on LLM simultaneously.","Specifically, we first present a novel dual-graph sequential model to capture the diverse information, along with an alignment and contrastive learning method to facilitate domain knowledge transfer.","Subsequently, a user retrieve-generation model is adopted to seamlessly integrate the structural information into LLM, fully harnessing its emergent inferencing ability.","Furthermore, we propose a domain-specific strategy and a refinement module to prevent out-of-domain generation.","Extensive experiments on Amazon demonstrated the information integration and domain-specific generation ability of URLLM in comparison to state-of-the-art baselines.","Our code is available at https://github.com/TingJShen/URLLM"],"url":"http://arxiv.org/abs/2406.03085v1","category":"cs.LG"}
{"created":"2024-06-05 09:13:01","title":"Holographic image features of an AdS black hole in Einstein-power-Yang-Mills gravity","abstract":"By utilizing the AdS/CFT correspondence, we investigate the holographic image of an AdS black hole in Einstein-power-Yang-Mills gravity. The AdS boundary hosts a Gaussian oscillation source, which induces a lensed response on the opposite side of the boundary during propagation through bulk spacetime. The optical system assists observers at the north pole to continuously capture holographic images that show an axisymmetric bright ring known as the Einstein ring. As the observation position shifted, the bright ring gradually transformed into a luminous arc and eventually transitioned into a light point. Simultaneously, we examine the impact of variations in relevant physical quantity on the ring, and present the corresponding brightness curve. The results indicate that as the temperature $T$ and nonlinear Yang Mills charge parameter $q$ increase, the ring radius also increases, while an increase in the chemical potential $u$ leads to a decrease. However, the peak brightness curve of the ring invariably decreases as the values of $T$, $u$, and $q$ increase, albeit to varying degrees. Upon comparing the outcomes of geometric optics, it can be observed that the position of the ring in holography images is consistent with that of the photon sphere.","sentences":["By utilizing the AdS/CFT correspondence, we investigate the holographic image of an AdS black hole in Einstein-power-Yang-Mills gravity.","The AdS boundary hosts a Gaussian oscillation source, which induces a lensed response on the opposite side of the boundary during propagation through bulk spacetime.","The optical system assists observers at the north pole to continuously capture holographic images that show an axisymmetric bright ring known as the Einstein ring.","As the observation position shifted, the bright ring gradually transformed into a luminous arc and eventually transitioned into a light point.","Simultaneously, we examine the impact of variations in relevant physical quantity on the ring, and present the corresponding brightness curve.","The results indicate that as the temperature $T$ and nonlinear Yang Mills charge parameter $q$ increase, the ring radius also increases, while an increase in the chemical potential $u$ leads to a decrease.","However, the peak brightness curve of the ring invariably decreases as the values of $T$, $u$, and $q$ increase, albeit to varying degrees.","Upon comparing the outcomes of geometric optics, it can be observed that the position of the ring in holography images is consistent with that of the photon sphere."],"url":"http://arxiv.org/abs/2406.03083v1","category":"hep-th"}
{"created":"2024-06-05 09:11:46","title":"Learning Solutions of Stochastic Optimization Problems with Bayesian Neural Networks","abstract":"Mathematical solvers use parametrized Optimization Problems (OPs) as inputs to yield optimal decisions. In many real-world settings, some of these parameters are unknown or uncertain. Recent research focuses on predicting the value of these unknown parameters using available contextual features, aiming to decrease decision regret by adopting end-to-end learning approaches. However, these approaches disregard prediction uncertainty and therefore make the mathematical solver susceptible to provide erroneous decisions in case of low-confidence predictions. We propose a novel framework that models prediction uncertainty with Bayesian Neural Networks (BNNs) and propagates this uncertainty into the mathematical solver with a Stochastic Programming technique. The differentiable nature of BNNs and differentiable mathematical solvers allow for two different learning approaches: In the Decoupled learning approach, we update the BNN weights to increase the quality of the predictions' distribution of the OP parameters, while in the Combined learning approach, we update the weights aiming to directly minimize the expected OP's cost function in a stochastic end-to-end fashion. We do an extensive evaluation using synthetic data with various noise properties and a real dataset, showing that decisions regret are generally lower (better) with both proposed methods.","sentences":["Mathematical solvers use parametrized Optimization Problems (OPs) as inputs to yield optimal decisions.","In many real-world settings, some of these parameters are unknown or uncertain.","Recent research focuses on predicting the value of these unknown parameters using available contextual features, aiming to decrease decision regret by adopting end-to-end learning approaches.","However, these approaches disregard prediction uncertainty and therefore make the mathematical solver susceptible to provide erroneous decisions in case of low-confidence predictions.","We propose a novel framework that models prediction uncertainty with Bayesian Neural Networks (BNNs) and propagates this uncertainty into the mathematical solver with a Stochastic Programming technique.","The differentiable nature of BNNs and differentiable mathematical solvers allow for two different learning approaches: In the Decoupled learning approach, we update the BNN weights to increase the quality of the predictions' distribution of the OP parameters, while in the Combined learning approach, we update the weights aiming to directly minimize the expected OP's cost function in a stochastic end-to-end fashion.","We do an extensive evaluation using synthetic data with various noise properties and a real dataset, showing that decisions regret are generally lower (better) with both proposed methods."],"url":"http://arxiv.org/abs/2406.03082v1","category":"cs.LG"}
{"created":"2024-06-05 09:10:00","title":"A Priori Estimation of the Approximation, Optimization and Generalization Error of Random Neural Networks for Solving Partial Differential Equations","abstract":"In recent years, there are numerous methods involving neural networks for solving partial differential equations (PDEs), such as Physics informed neural networks (PINNs), Deep Ritz method (DRM) and others. However, the optimization problems are typically non-convex, which makes these methods lead to unsatisfactory solutions. With weights sampled from some distribution, applying random neural networks to solve PDEs yields least squares problems that are easily solvable. In this paper, we focus on Barron type functions and demonstrate the approximation, optimization and generalization of random neural networks for solving PDEs.","sentences":["In recent years, there are numerous methods involving neural networks for solving partial differential equations (PDEs), such as Physics informed neural networks (PINNs), Deep Ritz method (DRM) and others.","However, the optimization problems are typically non-convex, which makes these methods lead to unsatisfactory solutions.","With weights sampled from some distribution, applying random neural networks to solve PDEs yields least squares problems that are easily solvable.","In this paper, we focus on Barron type functions and demonstrate the approximation, optimization and generalization of random neural networks for solving PDEs."],"url":"http://arxiv.org/abs/2406.03080v1","category":"math.NA"}
{"created":"2024-06-05 09:09:32","title":"Cryptocurrency Frauds for Dummies: How ChatGPT introduces us to fraud?","abstract":"Recent advances in the field of large language models (LLMs), particularly the ChatGPT family, have given rise to a powerful and versatile machine interlocutor, packed with knowledge and challenging our understanding of learning. This interlocutor is a double-edged sword: it can be harnessed for a wide variety of beneficial tasks, but it can also be used to cause harm. This study explores the complicated interaction between ChatGPT and the growing problem of cryptocurrency fraud. Although ChatGPT is known for its adaptability and ethical considerations when used for harmful purposes, we highlight the deep connection that may exist between ChatGPT and fraudulent actions in the volatile cryptocurrency ecosystem. Based on our categorization of cryptocurrency frauds, we show how to influence outputs, bypass ethical terms, and achieve specific fraud goals by manipulating ChatGPT prompts. Furthermore, our findings emphasize the importance of realizing that ChatGPT could be a valuable instructor even for novice fraudsters, as well as understanding and safely deploying complex language models, particularly in the context of cryptocurrency frauds. Finally, our study underlines the importance of using LLMs responsibly and ethically in the digital currency sector, identifying potential risks and resolving ethical issues. It should be noted that our work is not intended to encourage and promote fraud, but rather to raise awareness of the risks of fraud associated with the use of ChatGPT.","sentences":["Recent advances in the field of large language models (LLMs), particularly the ChatGPT family, have given rise to a powerful and versatile machine interlocutor, packed with knowledge and challenging our understanding of learning.","This interlocutor is a double-edged sword: it can be harnessed for a wide variety of beneficial tasks, but it can also be used to cause harm.","This study explores the complicated interaction between ChatGPT and the growing problem of cryptocurrency fraud.","Although ChatGPT is known for its adaptability and ethical considerations when used for harmful purposes, we highlight the deep connection that may exist between ChatGPT and fraudulent actions in the volatile cryptocurrency ecosystem.","Based on our categorization of cryptocurrency frauds, we show how to influence outputs, bypass ethical terms, and achieve specific fraud goals by manipulating ChatGPT prompts.","Furthermore, our findings emphasize the importance of realizing that ChatGPT could be a valuable instructor even for novice fraudsters, as well as understanding and safely deploying complex language models, particularly in the context of cryptocurrency frauds.","Finally, our study underlines the importance of using LLMs responsibly and ethically in the digital currency sector, identifying potential risks and resolving ethical issues.","It should be noted that our work is not intended to encourage and promote fraud, but rather to raise awareness of the risks of fraud associated with the use of ChatGPT."],"url":"http://arxiv.org/abs/2406.03079v1","category":"cs.CL"}
{"created":"2024-06-05 09:05:55","title":"Towards Federated Domain Unlearning: Verification Methodologies and Challenges","abstract":"Federated Learning (FL) has evolved as a powerful tool for collaborative model training across multiple entities, ensuring data privacy in sensitive sectors such as healthcare and finance. However, the introduction of the Right to Be Forgotten (RTBF) poses new challenges, necessitating federated unlearning to delete data without full model retraining. Traditional FL unlearning methods, not originally designed with domain specificity in mind, inadequately address the complexities of multi-domain scenarios, often affecting the accuracy of models in non-targeted domains or leading to uniform forgetting across all domains. Our work presents the first comprehensive empirical study on Federated Domain Unlearning, analyzing the characteristics and challenges of current techniques in multi-domain contexts. We uncover that these methods falter, particularly because they neglect the nuanced influences of domain-specific data, which can lead to significant performance degradation and inaccurate model behavior. Our findings reveal that unlearning disproportionately affects the model's deeper layers, erasing critical representational subspaces acquired during earlier training phases. In response, we propose novel evaluation methodologies tailored for Federated Domain Unlearning, aiming to accurately assess and verify domain-specific data erasure without compromising the model's overall integrity and performance. This investigation not only highlights the urgent need for domain-centric unlearning strategies in FL but also sets a new precedent for evaluating and implementing these techniques effectively.","sentences":["Federated Learning (FL) has evolved as a powerful tool for collaborative model training across multiple entities, ensuring data privacy in sensitive sectors such as healthcare and finance.","However, the introduction of the Right to Be Forgotten (RTBF) poses new challenges, necessitating federated unlearning to delete data without full model retraining.","Traditional FL unlearning methods, not originally designed with domain specificity in mind, inadequately address the complexities of multi-domain scenarios, often affecting the accuracy of models in non-targeted domains or leading to uniform forgetting across all domains.","Our work presents the first comprehensive empirical study on Federated Domain Unlearning, analyzing the characteristics and challenges of current techniques in multi-domain contexts.","We uncover that these methods falter, particularly because they neglect the nuanced influences of domain-specific data, which can lead to significant performance degradation and inaccurate model behavior.","Our findings reveal that unlearning disproportionately affects the model's deeper layers, erasing critical representational subspaces acquired during earlier training phases.","In response, we propose novel evaluation methodologies tailored for Federated Domain Unlearning, aiming to accurately assess and verify domain-specific data erasure without compromising the model's overall integrity and performance.","This investigation not only highlights the urgent need for domain-centric unlearning strategies in FL but also sets a new precedent for evaluating and implementing these techniques effectively."],"url":"http://arxiv.org/abs/2406.03078v1","category":"cs.LG"}
{"created":"2024-06-05 08:59:47","title":"Fourier integral operators on Hardy spaces with Hormander class","abstract":"In this note, we consider a Fourier integral operator defined by   \\begin{align*}   T_{\\phi,a}f(x)=\\int_{\\mathbb{R}^{n}}e^{i\\phi(x,\\xi)}a(x,\\xi)\\widehat{f}(\\xi)d\\xi,   \\end{align*} where $a$ is the amplitude, and $\\phi$ is the phase.   Let $0\\leq\\rho\\leq 1,n\\geq 2$ or $0\\leq\\rho<1,n=1$ and   $$m_p=\\frac{\\rho-n}{p}+(n-1)\\min\\{\\frac 12,\\rho\\}.$$   If $a$ belongs to the forbidden H\\\"{o}rmander class $S^{m_p}_{\\rho,1}$ and $\\phi\\in \\Phi^{2}$ satisfies the strong non-degeneracy condition, then for any $\\frac {n}{n+1}<p\\leq 1$, we can show that the Fourier integral operator $T_{\\phi,a}$ is bounded from the local Hardy space $h^p$ to $L^p$.   Furthermore, if $a$ has compact support in variable $x$, then we can extend this result to $0<p\\leq 1$. As $S^{m_p}_{\\rho,\\delta}\\subset S^{m_p}_{\\rho,1}$ for any $0\\leq \\delta\\leq 1$, our result supplements and improves upon recent theorems proved by Staubach and his collaborators for $a\\in S^{m}_{\\rho,\\delta}$ when $\\delta$ is close to 1.   As an important special case, when $n\\geq 2$, we show that $T_{\\phi,a}$ is bounded from $H^1$ to $L^1$ if $a\\in S^{(1-n)/2}_{1,1}$ which is a generalization of the well-known Seeger-Sogge-Stein theorem for $a\\in S^{(1-n)/2}_{1,0}$. This result is false when $n=1$ and $a\\in S^{0}_{1,1}$.","sentences":["In this note, we consider a Fourier integral operator defined by   \\begin{align*}   T_{\\phi,a}f(x)=\\int_{\\mathbb{R}^{n}}e^{i\\phi(x,\\xi)}a(x,\\xi)\\widehat{f}(\\xi)d\\xi,   \\end{align*} where $a$ is the amplitude, and $\\phi$ is the phase.   ","Let $0\\leq\\rho\\leq 1,n\\geq 2$ or $0\\leq\\rho<1,n=1$ and   $$m_p=\\frac{\\rho-n}{p}+(n-1)\\min\\{\\frac 12,\\rho\\}.$$   If $a$ belongs to the forbidden H\\\"{o}rmander class $S^{m_p}_{\\rho,1}$ and $\\phi\\in \\Phi^{2}$ satisfies the strong non-degeneracy condition, then for any $\\frac {n}{n+1}<p\\leq 1$, we can show that the Fourier integral operator $T_{\\phi,a}$ is bounded from the local Hardy space $h^p$ to $L^p$.   ","Furthermore, if $a$ has compact support in variable $x$, then we can extend this result to $0<p\\leq 1$.","As $S^{m_p}_{\\rho,\\delta}\\subset S^{m_p}_{\\rho,1}$ for any $0\\leq \\delta\\leq 1$, our result supplements and improves upon recent theorems proved by Staubach and his collaborators for $a\\in S^{m}_{\\rho,\\delta}$ when $\\delta$ is close to 1.   ","As an important special case, when $n\\geq 2$, we show that $T_{\\phi,a}$ is bounded from $H^1$ to $L^1$ if $a\\in S^{(1-n)/2}_{1,1}$ which is a generalization of the well-known Seeger-Sogge-Stein theorem for $a\\in S^{(1-n)/2}_{1,0}$.","This result is false when $n=1$ and $a\\in S^{0}_{1,1}$."],"url":"http://arxiv.org/abs/2406.03076v1","category":"math.DG"}
{"created":"2024-06-05 08:59:45","title":"Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework","abstract":"The advent of large language models (LLMs) has facilitated the development of natural language text generation. It also poses unprecedented challenges, with content hallucination emerging as a significant concern. Existing solutions often involve expensive and complex interventions during the training process. Moreover, some approaches emphasize problem disassembly while neglecting the crucial validation process, leading to performance degradation or limited applications. To overcome these limitations, we propose a Markov Chain-based multi-agent debate verification framework to enhance hallucination detection accuracy in concise claims. Our method integrates the fact-checking process, including claim detection, evidence retrieval, and multi-agent verification. In the verification stage, we deploy multiple agents through flexible Markov Chain-based debates to validate individual claims, ensuring meticulous verification outcomes. Experimental results across three generative tasks demonstrate that our approach achieves significant improvements over baselines.","sentences":["The advent of large language models (LLMs) has facilitated the development of natural language text generation.","It also poses unprecedented challenges, with content hallucination emerging as a significant concern.","Existing solutions often involve expensive and complex interventions during the training process.","Moreover, some approaches emphasize problem disassembly while neglecting the crucial validation process, leading to performance degradation or limited applications.","To overcome these limitations, we propose a Markov Chain-based multi-agent debate verification framework to enhance hallucination detection accuracy in concise claims.","Our method integrates the fact-checking process, including claim detection, evidence retrieval, and multi-agent verification.","In the verification stage, we deploy multiple agents through flexible Markov Chain-based debates to validate individual claims, ensuring meticulous verification outcomes.","Experimental results across three generative tasks demonstrate that our approach achieves significant improvements over baselines."],"url":"http://arxiv.org/abs/2406.03075v1","category":"cs.CL"}
{"created":"2024-06-05 08:57:47","title":"Joint distribution of Hecke eigenforms","abstract":"In this paper, we formulate conjectures on the joint distribution of several Hecke eigenforms. We prove an asymptotic formula of the joint mass of two Hecke eigenforms under the generalized Riemann Hypothesis (GRH) and the generalized Ramanujan conjecture (GRC). We also show that a higher decorrelation of two Hecke eigenforms asymptotically vanishes under GRH. As a consequence, we prove an asymptotic formula for the first moment of the triple product $L$-functions under GRH and GRC.","sentences":["In this paper, we formulate conjectures on the joint distribution of several Hecke eigenforms.","We prove an asymptotic formula of the joint mass of two Hecke eigenforms under the generalized Riemann Hypothesis (GRH) and the generalized Ramanujan conjecture (GRC).","We also show that a higher decorrelation of two Hecke eigenforms asymptotically vanishes under GRH.","As a consequence, we prove an asymptotic formula for the first moment of the triple product $L$-functions under GRH and GRC."],"url":"http://arxiv.org/abs/2406.03073v1","category":"math.NT"}
{"created":"2024-06-05 08:56:24","title":"Exploiting LMM-based knowledge for image classification tasks","abstract":"In this paper we address image classification tasks leveraging knowledge encoded in Large Multimodal Models (LMMs). More specifically, we use the MiniGPT-4 model to extract semantic descriptions for the images, in a multimodal prompting fashion. In the current literature, vision language models such as CLIP, among other approaches, are utilized as feature extractors, using only the image encoder, for solving image classification tasks. In this paper, we propose to additionally use the text encoder to obtain the text embeddings corresponding to the MiniGPT-4-generated semantic descriptions. Thus, we use both the image and text embeddings for solving the image classification task. The experimental evaluation on three datasets validates the improved classification performance achieved by exploiting LMM-based knowledge.","sentences":["In this paper we address image classification tasks leveraging knowledge encoded in Large Multimodal Models (LMMs).","More specifically, we use the MiniGPT-4 model to extract semantic descriptions for the images, in a multimodal prompting fashion.","In the current literature, vision language models such as CLIP, among other approaches, are utilized as feature extractors, using only the image encoder, for solving image classification tasks.","In this paper, we propose to additionally use the text encoder to obtain the text embeddings corresponding to the MiniGPT-4-generated semantic descriptions.","Thus, we use both the image and text embeddings for solving the image classification task.","The experimental evaluation on three datasets validates the improved classification performance achieved by exploiting LMM-based knowledge."],"url":"http://arxiv.org/abs/2406.03071v1","category":"cs.CV"}
{"created":"2024-06-05 08:55:02","title":"A-Bench: Are LMMs Masters at Evaluating AI-generated Images?","abstract":"How to accurately and efficiently assess AI-generated images (AIGIs) remains a critical challenge for generative models. Given the high costs and extensive time commitments required for user studies, many researchers have turned towards employing large multi-modal models (LMMs) as AIGI evaluators, the precision and validity of which are still questionable. Furthermore, traditional benchmarks often utilize mostly natural-captured content rather than AIGIs to test the abilities of LMMs, leading to a noticeable gap for AIGIs. Therefore, we introduce A-Bench in this paper, a benchmark designed to diagnose whether LMMs are masters at evaluating AIGIs. Specifically, A-Bench is organized under two key principles: 1) Emphasizing both high-level semantic understanding and low-level visual quality perception to address the intricate demands of AIGIs. 2) Various generative models are utilized for AIGI creation, and various LMMs are employed for evaluation, which ensures a comprehensive validation scope. Ultimately, 2,864 AIGIs from 16 text-to-image models are sampled, each paired with question-answers annotated by human experts, and tested across 18 leading LMMs. We hope that A-Bench will significantly enhance the evaluation process and promote the generation quality for AIGIs. The benchmark is available at https://github.com/Q-Future/A-Bench.","sentences":["How to accurately and efficiently assess AI-generated images (AIGIs) remains a critical challenge for generative models.","Given the high costs and extensive time commitments required for user studies, many researchers have turned towards employing large multi-modal models (LMMs) as AIGI evaluators, the precision and validity of which are still questionable.","Furthermore, traditional benchmarks often utilize mostly natural-captured content rather than AIGIs to test the abilities of LMMs, leading to a noticeable gap for AIGIs.","Therefore, we introduce A-Bench in this paper, a benchmark designed to diagnose whether LMMs are masters at evaluating AIGIs.","Specifically, A-Bench is organized under two key principles: 1) Emphasizing both high-level semantic understanding and low-level visual quality perception to address the intricate demands of AIGIs. 2) Various generative models are utilized for AIGI creation, and various LMMs are employed for evaluation, which ensures a comprehensive validation scope.","Ultimately, 2,864 AIGIs from 16 text-to-image models are sampled, each paired with question-answers annotated by human experts, and tested across 18 leading LMMs.","We hope that A-Bench will significantly enhance the evaluation process and promote the generation quality for AIGIs.","The benchmark is available at https://github.com/Q-Future/A-Bench."],"url":"http://arxiv.org/abs/2406.03070v1","category":"cs.CV"}
{"created":"2024-06-05 08:52:21","title":"\"Give Me an Example Like This\": Episodic Active Reinforcement Learning from Demonstrations","abstract":"Reinforcement Learning (RL) has achieved great success in sequential decision-making problems, but often at the cost of a large number of agent-environment interactions. To improve sample efficiency, methods like Reinforcement Learning from Expert Demonstrations (RLED) introduce external expert demonstrations to facilitate agent exploration during the learning process. In practice, these demonstrations, which are often collected from human users, are costly and hence often constrained to a limited amount. How to select the best set of human demonstrations that is most beneficial for learning therefore becomes a major concern. This paper presents EARLY (Episodic Active Learning from demonstration querY), an algorithm that enables a learning agent to generate optimized queries of expert demonstrations in a trajectory-based feature space. Based on a trajectory-level estimate of uncertainty in the agent's current policy, EARLY determines the optimized timing and content for feature-based queries. By querying episodic demonstrations as opposed to isolated state-action pairs, EARLY improves the human teaching experience and achieves better learning performance. We validate the effectiveness of our method in three simulated navigation tasks of increasing difficulty. The results show that our method is able to achieve expert-level performance for all three tasks with convergence over 30\\% faster than other baseline methods when demonstrations are generated by simulated oracle policies. The results of a follow-up pilot user study (N=18) further validate that our method can still maintain a significantly better convergence in the case of human expert demonstrators while achieving a better user experience in perceived task load and consuming significantly less human time.","sentences":["Reinforcement Learning (RL) has achieved great success in sequential decision-making problems, but often at the cost of a large number of agent-environment interactions.","To improve sample efficiency, methods like Reinforcement Learning from Expert Demonstrations (RLED) introduce external expert demonstrations to facilitate agent exploration during the learning process.","In practice, these demonstrations, which are often collected from human users, are costly and hence often constrained to a limited amount.","How to select the best set of human demonstrations that is most beneficial for learning therefore becomes a major concern.","This paper presents EARLY (Episodic Active Learning from demonstration querY), an algorithm that enables a learning agent to generate optimized queries of expert demonstrations in a trajectory-based feature space.","Based on a trajectory-level estimate of uncertainty in the agent's current policy, EARLY determines the optimized timing and content for feature-based queries.","By querying episodic demonstrations as opposed to isolated state-action pairs, EARLY improves the human teaching experience and achieves better learning performance.","We validate the effectiveness of our method in three simulated navigation tasks of increasing difficulty.","The results show that our method is able to achieve expert-level performance for all three tasks with convergence over 30\\% faster than other baseline methods when demonstrations are generated by simulated oracle policies.","The results of a follow-up pilot user study (N=18) further validate that our method can still maintain a significantly better convergence in the case of human expert demonstrators while achieving a better user experience in perceived task load and consuming significantly less human time."],"url":"http://arxiv.org/abs/2406.03069v1","category":"cs.AI"}
{"created":"2024-06-05 08:51:08","title":"How Truncating Weights Improves Reasoning in Language Models","abstract":"In addition to the ability to generate fluent text in various languages, large language models have been successful at tasks that involve basic forms of logical \"reasoning\" over their context. Recent work found that selectively removing certain components from weight matrices in pre-trained models can improve such reasoning capabilities. We investigate this phenomenon further by carefully studying how certain global associations tend to be stored in specific weight components or Transformer blocks, in particular feed-forward layers. Such associations may hurt predictions in reasoning tasks, and removing the corresponding components may then improve performance. We analyze how this arises during training, both empirically and theoretically, on a two-layer Transformer trained on a basic reasoning task with noise, a toy associative memory model, and on the Pythia family of pre-trained models tested on simple reasoning tasks.","sentences":["In addition to the ability to generate fluent text in various languages, large language models have been successful at tasks that involve basic forms of logical \"reasoning\" over their context.","Recent work found that selectively removing certain components from weight matrices in pre-trained models can improve such reasoning capabilities.","We investigate this phenomenon further by carefully studying how certain global associations tend to be stored in specific weight components or Transformer blocks, in particular feed-forward layers.","Such associations may hurt predictions in reasoning tasks, and removing the corresponding components may then improve performance.","We analyze how this arises during training, both empirically and theoretically, on a two-layer Transformer trained on a basic reasoning task with noise, a toy associative memory model, and on the Pythia family of pre-trained models tested on simple reasoning tasks."],"url":"http://arxiv.org/abs/2406.03068v1","category":"cs.LG"}
{"created":"2024-06-05 08:50:08","title":"Automatically detecting scientific political science texts from a large general document index","abstract":"This technical report outlines the filtering approach applied to the collection of the Bielefeld Academic Search Engine (BASE) data to extract articles from the political science domain. We combined hard and soft filters to address entries with different available metadata, e.g. title, abstract or keywords. The hard filter is a weighted keyword-based filter approach. The soft filter uses a multilingual BERT-based classification model, trained to detect scientific articles from the political science domain. We evaluated both approaches using an annotated dataset, consisting of scientific articles from different scientific domains. The weighted keyword-based approach achieved the highest total accuracy of 0.88. The multilingual BERT-based classification model was fine-tuned using a dataset of 14,178 abstracts from scientific articles and reached the highest total accuracy of 0.98.","sentences":["This technical report outlines the filtering approach applied to the collection of the Bielefeld Academic Search Engine (BASE) data to extract articles from the political science domain.","We combined hard and soft filters to address entries with different available metadata, e.g. title, abstract or keywords.","The hard filter is a weighted keyword-based filter approach.","The soft filter uses a multilingual BERT-based classification model, trained to detect scientific articles from the political science domain.","We evaluated both approaches using an annotated dataset, consisting of scientific articles from different scientific domains.","The weighted keyword-based approach achieved the highest total accuracy of 0.88.","The multilingual BERT-based classification model was fine-tuned using a dataset of 14,178 abstracts from scientific articles and reached the highest total accuracy of 0.98."],"url":"http://arxiv.org/abs/2406.03067v1","category":"cs.DL"}
{"created":"2024-06-05 08:49:51","title":"Decision Boundary-aware Knowledge Consolidation Generates Better Instance-Incremental Learner","abstract":"Instance-incremental learning (IIL) focuses on learning continually with data of the same classes. Compared to class-incremental learning (CIL), the IIL is seldom explored because IIL suffers less from catastrophic forgetting (CF). However, besides retaining knowledge, in real-world deployment scenarios where the class space is always predefined, continual and cost-effective model promotion with the potential unavailability of previous data is a more essential demand. Therefore, we first define a new and more practical IIL setting as promoting the model's performance besides resisting CF with only new observations. Two issues have to be tackled in the new IIL setting: 1) the notorious catastrophic forgetting because of no access to old data, and 2) broadening the existing decision boundary to new observations because of concept drift. To tackle these problems, our key insight is to moderately broaden the decision boundary to fail cases while retain old boundary. Hence, we propose a novel decision boundary-aware distillation method with consolidating knowledge to teacher to ease the student learning new knowledge. We also establish the benchmarks on existing datasets Cifar-100 and ImageNet. Notably, extensive experiments demonstrate that the teacher model can be a better incremental learner than the student model, which overturns previous knowledge distillation-based methods treating student as the main role.","sentences":["Instance-incremental learning (IIL) focuses on learning continually with data of the same classes.","Compared to class-incremental learning (CIL), the IIL is seldom explored because IIL suffers less from catastrophic forgetting (CF).","However, besides retaining knowledge, in real-world deployment scenarios where the class space is always predefined, continual and cost-effective model promotion with the potential unavailability of previous data is a more essential demand.","Therefore, we first define a new and more practical IIL setting as promoting the model's performance besides resisting CF with only new observations.","Two issues have to be tackled in the new IIL setting: 1) the notorious catastrophic forgetting because of no access to old data, and 2) broadening the existing decision boundary to new observations because of concept drift.","To tackle these problems, our key insight is to moderately broaden the decision boundary to fail cases while retain old boundary.","Hence, we propose a novel decision boundary-aware distillation method with consolidating knowledge to teacher to ease the student learning new knowledge.","We also establish the benchmarks on existing datasets Cifar-100 and ImageNet.","Notably, extensive experiments demonstrate that the teacher model can be a better incremental learner than the student model, which overturns previous knowledge distillation-based methods treating student as the main role."],"url":"http://arxiv.org/abs/2406.03065v1","category":"cs.LG"}
{"created":"2024-06-05 08:47:30","title":"Path-Specific Causal Reasoning for Fairness-aware Cognitive Diagnosis","abstract":"Cognitive Diagnosis~(CD), which leverages students and exercise data to predict students' proficiency levels on different knowledge concepts, is one of fundamental components in Intelligent Education. Due to the scarcity of student-exercise interaction data, most existing methods focus on making the best use of available data, such as exercise content and student information~(e.g., educational context). Despite the great progress, the abuse of student sensitive information has not been paid enough attention. Due to the important position of CD in Intelligent Education, employing sensitive information when making diagnosis predictions will cause serious social issues. Moreover, data-driven neural networks are easily misled by the shortcut between input data and output prediction, exacerbating this problem. Therefore, it is crucial to eliminate the negative impact of sensitive information in CD models. In response, we argue that sensitive attributes of students can also provide useful information, and only the shortcuts directly related to the sensitive information should be eliminated from the diagnosis process. Thus, we employ causal reasoning and design a novel Path-Specific Causal Reasoning Framework (PSCRF) to achieve this goal. Specifically, we first leverage an encoder to extract features and generate embeddings for general information and sensitive information of students. Then, we design a novel attribute-oriented predictor to decouple the sensitive attributes, in which fairness-related sensitive features will be eliminated and other useful information will be retained. Finally, we designed a multi-factor constraint to ensure the performance of fairness and diagnosis performance simultaneously. Extensive experiments over real-world datasets (e.g., PISA dataset) demonstrate the effectiveness of our proposed PSCRF.","sentences":["Cognitive Diagnosis~(CD), which leverages students and exercise data to predict students' proficiency levels on different knowledge concepts, is one of fundamental components in Intelligent Education.","Due to the scarcity of student-exercise interaction data, most existing methods focus on making the best use of available data, such as exercise content and student information~(e.g., educational context).","Despite the great progress, the abuse of student sensitive information has not been paid enough attention.","Due to the important position of CD in Intelligent Education, employing sensitive information when making diagnosis predictions will cause serious social issues.","Moreover, data-driven neural networks are easily misled by the shortcut between input data and output prediction, exacerbating this problem.","Therefore, it is crucial to eliminate the negative impact of sensitive information in CD models.","In response, we argue that sensitive attributes of students can also provide useful information, and only the shortcuts directly related to the sensitive information should be eliminated from the diagnosis process.","Thus, we employ causal reasoning and design a novel Path-Specific Causal Reasoning Framework (PSCRF) to achieve this goal.","Specifically, we first leverage an encoder to extract features and generate embeddings for general information and sensitive information of students.","Then, we design a novel attribute-oriented predictor to decouple the sensitive attributes, in which fairness-related sensitive features will be eliminated and other useful information will be retained.","Finally, we designed a multi-factor constraint to ensure the performance of fairness and diagnosis performance simultaneously.","Extensive experiments over real-world datasets (e.g., PISA dataset) demonstrate the effectiveness of our proposed PSCRF."],"url":"http://arxiv.org/abs/2406.03064v1","category":"cs.LG"}
{"created":"2024-06-05 08:43:11","title":"RadBARTsum: Domain Specific Adaption of Denoising Sequence-to-Sequence Models for Abstractive Radiology Report Summarization","abstract":"Radiology report summarization is a crucial task that can help doctors quickly identify clinically significant findings without the need to review detailed sections of reports. This study proposes RadBARTsum, a domain-specific and ontology facilitated adaptation of the BART model for abstractive radiology report summarization. The approach involves two main steps: 1) re-training the BART model on a large corpus of radiology reports using a novel entity masking strategy to improving biomedical domain knowledge learning, and 2) fine-tuning the model for the summarization task using the Findings and Background sections to predict the Impression section. Experiments are conducted using different masking strategies. Results show that the re-training process with domain knowledge facilitated masking improves performances consistently across various settings. This work contributes a domain-specific generative language model for radiology report summarization and a method for utilising medical knowledge to realise entity masking language model. The proposed approach demonstrates a promising direction of enhancing the efficiency of language models by deepening its understanding of clinical knowledge in radiology reports.","sentences":["Radiology report summarization is a crucial task that can help doctors quickly identify clinically significant findings without the need to review detailed sections of reports.","This study proposes RadBARTsum, a domain-specific and ontology facilitated adaptation of the BART model for abstractive radiology report summarization.","The approach involves two main steps: 1) re-training the BART model on a large corpus of radiology reports using a novel entity masking strategy to improving biomedical domain knowledge learning, and 2) fine-tuning the model for the summarization task using the Findings and Background sections to predict the Impression section.","Experiments are conducted using different masking strategies.","Results show that the re-training process with domain knowledge facilitated masking improves performances consistently across various settings.","This work contributes a domain-specific generative language model for radiology report summarization and a method for utilising medical knowledge to realise entity masking language model.","The proposed approach demonstrates a promising direction of enhancing the efficiency of language models by deepening its understanding of clinical knowledge in radiology reports."],"url":"http://arxiv.org/abs/2406.03062v1","category":"cs.CL"}
{"created":"2024-06-05 08:30:11","title":"Spiking representation learning for associative memories","abstract":"Networks of interconnected neurons communicating through spiking signals offer the bedrock of neural computations. Our brains spiking neural networks have the computational capacity to achieve complex pattern recognition and cognitive functions effortlessly. However, solving real-world problems with artificial spiking neural networks (SNNs) has proved to be difficult for a variety of reasons. Crucially, scaling SNNs to large networks and processing large-scale real-world datasets have been challenging, especially when compared to their non-spiking deep learning counterparts. The critical operation that is needed of SNNs is the ability to learn distributed representations from data and use these representations for perceptual, cognitive and memory operations. In this work, we introduce a novel SNN that performs unsupervised representation learning and associative memory operations leveraging Hebbian synaptic and activity-dependent structural plasticity coupled with neuron-units modelled as Poisson spike generators with sparse firing (~1 Hz mean and ~100 Hz maximum firing rate). Crucially, the architecture of our model derives from the neocortical columnar organization and combines feedforward projections for learning hidden representations and recurrent projections for forming associative memories. We evaluated the model on properties relevant for attractor-based associative memories such as pattern completion, perceptual rivalry, distortion resistance, and prototype extraction.","sentences":["Networks of interconnected neurons communicating through spiking signals offer the bedrock of neural computations.","Our brains spiking neural networks have the computational capacity to achieve complex pattern recognition and cognitive functions effortlessly.","However, solving real-world problems with artificial spiking neural networks (SNNs) has proved to be difficult for a variety of reasons.","Crucially, scaling SNNs to large networks and processing large-scale real-world datasets have been challenging, especially when compared to their non-spiking deep learning counterparts.","The critical operation that is needed of SNNs is the ability to learn distributed representations from data and use these representations for perceptual, cognitive and memory operations.","In this work, we introduce a novel SNN that performs unsupervised representation learning and associative memory operations leveraging Hebbian synaptic and activity-dependent structural plasticity coupled with neuron-units modelled as Poisson spike generators with sparse firing (~1 Hz mean and ~100","Hz maximum firing rate).","Crucially, the architecture of our model derives from the neocortical columnar organization and combines feedforward projections for learning hidden representations and recurrent projections for forming associative memories.","We evaluated the model on properties relevant for attractor-based associative memories such as pattern completion, perceptual rivalry, distortion resistance, and prototype extraction."],"url":"http://arxiv.org/abs/2406.03054v1","category":"cs.NE"}
{"created":"2024-06-05 08:26:44","title":"Adapter-X: A Novel General Parameter-Efficient Fine-Tuning Framework for Vision","abstract":"Parameter-efficient fine-tuning (PEFT) has become increasingly important as foundation models continue to grow in both popularity and size. Adapter has been particularly well-received due to their potential for parameter reduction and adaptability across diverse tasks. However, striking a balance between high efficiency and robust generalization across tasks remains a challenge for adapter-based methods. We analyze existing methods and find that: 1) parameter sharing is the key to reducing redundancy; 2) more tunable parameters, dynamic allocation, and block-specific design are keys to improving performance. Unfortunately, no previous work considers all these factors. Inspired by this insight, we introduce a novel framework named Adapter-X. First, a Sharing Mixture of Adapters (SMoA) module is proposed to fulfill token-level dynamic allocation, increased tunable parameters, and inter-block sharing at the same time. Second, some block-specific designs like Prompt Generator (PG) are introduced to further enhance the ability of adaptation. Extensive experiments across 2D image and 3D point cloud modalities demonstrate that Adapter-X represents a significant milestone as it is the first to outperform full fine-tuning in both 2D image and 3D point cloud modalities with significantly fewer parameters, i.e., only 0.20% and 1.88% of original trainable parameters for 2D and 3D classification tasks. Our code will be publicly available.","sentences":["Parameter-efficient fine-tuning (PEFT) has become increasingly important as foundation models continue to grow in both popularity and size.","Adapter has been particularly well-received due to their potential for parameter reduction and adaptability across diverse tasks.","However, striking a balance between high efficiency and robust generalization across tasks remains a challenge for adapter-based methods.","We analyze existing methods and find that: 1) parameter sharing is the key to reducing redundancy; 2) more tunable parameters, dynamic allocation, and block-specific design are keys to improving performance.","Unfortunately, no previous work considers all these factors.","Inspired by this insight, we introduce a novel framework named Adapter-X. First, a Sharing Mixture of Adapters (SMoA) module is proposed to fulfill token-level dynamic allocation, increased tunable parameters, and inter-block sharing at the same time.","Second, some block-specific designs like Prompt Generator (PG) are introduced to further enhance the ability of adaptation.","Extensive experiments across 2D image and 3D point cloud modalities demonstrate that Adapter-X represents a significant milestone as it is the first to outperform full fine-tuning in both 2D image and 3D point cloud modalities with significantly fewer parameters, i.e., only 0.20% and 1.88% of original trainable parameters for 2D and 3D classification tasks.","Our code will be publicly available."],"url":"http://arxiv.org/abs/2406.03051v1","category":"cs.CV"}
{"created":"2024-06-05 08:24:27","title":"On a parity result for the symmetric square of modular forms with congruent residual representations","abstract":"The parity of Selmer ranks for elliptic curves defined over the rational numbers $\\mathbb{Q}$ with good ordinary reduction at an odd prime $p$ has been studied by Shekhar. The proof of Shekhar relies on proving a parity result for the $\\lambda$-invariants of Selmer groups over the cyclotomic $\\mathbb{Z}_p$-extension $\\mathbb{Q}_\\infty$ of $\\mathbb{Q}$.   This has been further generalized for elliptic curves with supersingular reduction at $p$ by Hatley and for modular forms by Hatley--Lei. In this paper, we prove a parity result for the $\\lambda$-invariants of Selmer groups over $\\mathbb{Q}_\\infty$ for the symmetric square representations associated to two modular forms with congruent residual Galois representations. We treat both the ordinary and the non-ordinary cases.","sentences":["The parity of Selmer ranks for elliptic curves defined over the rational numbers $\\mathbb{Q}$ with good ordinary reduction at an odd prime $p$ has been studied by Shekhar.","The proof of Shekhar relies on proving a parity result for the $\\lambda$-invariants of Selmer groups over the cyclotomic $\\mathbb{Z}_p$-extension $\\mathbb{Q}_\\infty$ of $\\mathbb{Q}$.   This has been further generalized for elliptic curves with supersingular reduction at $p$ by Hatley and for modular forms by Hatley--Lei.","In this paper, we prove a parity result for the $\\lambda$-invariants of Selmer groups over $\\mathbb{Q}_\\infty$ for the symmetric square representations associated to two modular forms with congruent residual Galois representations.","We treat both the ordinary and the non-ordinary cases."],"url":"http://arxiv.org/abs/2406.03050v1","category":"math.NT"}
{"created":"2024-06-05 08:24:22","title":"StreamSpeech: Simultaneous Speech-to-Speech Translation with Multi-task Learning","abstract":"Simultaneous speech-to-speech translation (Simul-S2ST, a.k.a streaming speech translation) outputs target speech while receiving streaming speech inputs, which is critical for real-time communication. Beyond accomplishing translation between speech, Simul-S2ST requires a policy to control the model to generate corresponding target speech at the opportune moment within speech inputs, thereby posing a double challenge of translation and policy. In this paper, we propose StreamSpeech, a direct Simul-S2ST model that jointly learns translation and simultaneous policy in a unified framework of multi-task learning. Adhering to a multi-task learning approach, StreamSpeech can perform offline and simultaneous speech recognition, speech translation and speech synthesis via an \"All-in-One\" seamless model. Experiments on CVSS benchmark demonstrate that StreamSpeech achieves state-of-the-art performance in both offline S2ST and Simul-S2ST tasks. Besides, StreamSpeech is able to present high-quality intermediate results (i.e., ASR or translation results) during simultaneous translation process, offering a more comprehensive real-time communication experience.","sentences":["Simultaneous speech-to-speech translation (Simul-S2ST, a.k.a streaming speech translation) outputs target speech while receiving streaming speech inputs, which is critical for real-time communication.","Beyond accomplishing translation between speech, Simul-S2ST requires a policy to control the model to generate corresponding target speech at the opportune moment within speech inputs, thereby posing a double challenge of translation and policy.","In this paper, we propose StreamSpeech, a direct Simul-S2ST model that jointly learns translation and simultaneous policy in a unified framework of multi-task learning.","Adhering to a multi-task learning approach, StreamSpeech can perform offline and simultaneous speech recognition, speech translation and speech synthesis via an \"All-in-One\" seamless model.","Experiments on CVSS benchmark demonstrate that StreamSpeech achieves state-of-the-art performance in both offline S2ST and Simul-S2ST tasks.","Besides, StreamSpeech is able to present high-quality intermediate results (i.e., ASR or translation results) during simultaneous translation process, offering a more comprehensive real-time communication experience."],"url":"http://arxiv.org/abs/2406.03049v1","category":"cs.CL"}
{"created":"2024-06-05 08:21:55","title":"When Spiking neural networks meet temporal attention image decoding and adaptive spiking neuron","abstract":"Spiking Neural Networks (SNNs) are capable of encoding and processing temporal information in a biologically plausible way. However, most existing SNN-based methods for image tasks do not fully exploit this feature. Moreover, they often overlook the role of adaptive threshold in spiking neurons, which can enhance their dynamic behavior and learning ability. To address these issues, we propose a novel method for image decoding based on temporal attention (TAID) and an adaptive Leaky-Integrate-and-Fire (ALIF) neuron model. Our method leverages the temporal information of SNN outputs to generate high-quality images that surpass the state-of-the-art (SOTA) in terms of Inception score, Fr\\'echet Inception Distance, and Fr\\'echet Autoencoder Distance. Furthermore, our ALIF neuron model achieves remarkable classification accuracy on MNIST (99.78\\%) and CIFAR-10 (93.89\\%) datasets, demonstrating the effectiveness of learning adaptive thresholds for spiking neurons. The code is available at https://github.com/bollossom/ICLR_TINY_SNN.","sentences":["Spiking Neural Networks (SNNs) are capable of encoding and processing temporal information in a biologically plausible way.","However, most existing SNN-based methods for image tasks do not fully exploit this feature.","Moreover, they often overlook the role of adaptive threshold in spiking neurons, which can enhance their dynamic behavior and learning ability.","To address these issues, we propose a novel method for image decoding based on temporal attention (TAID) and an adaptive Leaky-Integrate-and-Fire (ALIF) neuron model.","Our method leverages the temporal information of SNN outputs to generate high-quality images that surpass the state-of-the-art (SOTA) in terms of Inception score, Fr\\'echet Inception Distance, and Fr\\'echet Autoencoder Distance.","Furthermore, our ALIF neuron model achieves remarkable classification accuracy on MNIST (99.78\\%) and CIFAR-10 (93.89\\%) datasets, demonstrating the effectiveness of learning adaptive thresholds for spiking neurons.","The code is available at https://github.com/bollossom/ICLR_TINY_SNN."],"url":"http://arxiv.org/abs/2406.03046v1","category":"cs.NE"}
{"created":"2024-06-05 08:03:18","title":"Follow-Your-Pose v2: Multiple-Condition Guided Character Image Animation for Stable Pose Control","abstract":"Pose-controllable character video generation is in high demand with extensive applications for fields such as automatic advertising and content creation on social media platforms. While existing character image animation methods using pose sequences and reference images have shown promising performance, they tend to struggle with incoherent animation in complex scenarios, such as multiple character animation and body occlusion. Additionally, current methods request large-scale high-quality videos with stable backgrounds and temporal consistency as training datasets, otherwise, their performance will greatly deteriorate. These two issues hinder the practical utilization of character image animation tools. In this paper, we propose a practical and robust framework Follow-Your-Pose v2, which can be trained on noisy open-sourced videos readily available on the internet. Multi-condition guiders are designed to address the challenges of background stability, body occlusion in multi-character generation, and consistency of character appearance. Moreover, to fill the gap of fair evaluation of multi-character pose animation, we propose a new benchmark comprising approximately 4,000 frames. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods by a margin of over 35\\% across 2 datasets and on 7 metrics. Meanwhile, qualitative assessments reveal a significant improvement in the quality of generated video, particularly in scenarios involving complex backgrounds and body occlusion of multi-character, suggesting the superiority of our approach.","sentences":["Pose-controllable character video generation is in high demand with extensive applications for fields such as automatic advertising and content creation on social media platforms.","While existing character image animation methods using pose sequences and reference images have shown promising performance, they tend to struggle with incoherent animation in complex scenarios, such as multiple character animation and body occlusion.","Additionally, current methods request large-scale high-quality videos with stable backgrounds and temporal consistency as training datasets, otherwise, their performance will greatly deteriorate.","These two issues hinder the practical utilization of character image animation tools.","In this paper, we propose a practical and robust framework Follow-Your-Pose v2, which can be trained on noisy open-sourced videos readily available on the internet.","Multi-condition guiders are designed to address the challenges of background stability, body occlusion in multi-character generation, and consistency of character appearance.","Moreover, to fill the gap of fair evaluation of multi-character pose animation, we propose a new benchmark comprising approximately 4,000 frames.","Extensive experiments demonstrate that our approach outperforms state-of-the-art methods by a margin of over 35\\% across 2 datasets and on 7 metrics.","Meanwhile, qualitative assessments reveal a significant improvement in the quality of generated video, particularly in scenarios involving complex backgrounds and body occlusion of multi-character, suggesting the superiority of our approach."],"url":"http://arxiv.org/abs/2406.03035v1","category":"cs.CV"}
{"created":"2024-06-05 16:24:32","title":"Field Theory Approach to Classical $N$-Particle Systems In and Out of Equilibrium","abstract":"We present an approach to solving the evolution of a classical $N$-particle ensemble based on the path integral approach to classical mechanics. This formulation provides a perturbative solution to the Liouville equation in terms of a propagator which can be expanded in a Dyson series. We show that this perturbative expansion exactly corresponds to an iterative solution of the BBGKY-hierarchy in orders of the interaction potential. Using the path integral formulation, we perform a Hubbard-Stratonovich transformation (HST) to obtain an effective field theoretic description in terms of macroscopic fields, which contains the full microscopic dynamics of the system in its vertices. Naturally, the HST leads to a new perturbative expansion scheme which contains an infinite order of microscopic interactions already at the lowest order of the perturbative expansion. Our approach can be applied to in and out of equilibrium systems with arbitrary interaction potentials and initial conditions. We show how (unequal-time) cumulants of the Klimontovich phase space densities can be computed within this framework and derive results for density and momentum correlations for a spatially homogeneous system. Under the explicit assumptions for the interaction potential and initial conditions, we show that well-known results related to plasma oscillations and the Jeans instability criterion for gravitational collapse can be recovered in the lowest order perturbative expansion and that both are the effect of the same collective behaviour of the many-body system.","sentences":["We present an approach to solving the evolution of a classical $N$-particle ensemble based on the path integral approach to classical mechanics.","This formulation provides a perturbative solution to the Liouville equation in terms of a propagator which can be expanded in a Dyson series.","We show that this perturbative expansion exactly corresponds to an iterative solution of the BBGKY-hierarchy in orders of the interaction potential.","Using the path integral formulation, we perform a Hubbard-Stratonovich transformation (HST) to obtain an effective field theoretic description in terms of macroscopic fields, which contains the full microscopic dynamics of the system in its vertices.","Naturally, the HST leads to a new perturbative expansion scheme which contains an infinite order of microscopic interactions already at the lowest order of the perturbative expansion.","Our approach can be applied to in and out of equilibrium systems with arbitrary interaction potentials and initial conditions.","We show how (unequal-time) cumulants of the Klimontovich phase space densities can be computed within this framework and derive results for density and momentum correlations for a spatially homogeneous system.","Under the explicit assumptions for the interaction potential and initial conditions, we show that well-known results related to plasma oscillations and the Jeans instability criterion for gravitational collapse can be recovered in the lowest order perturbative expansion and that both are the effect of the same collective behaviour of the many-body system."],"url":"http://arxiv.org/abs/2406.03425v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-05 15:37:30","title":"Measurement of the branching fraction ratios $R(D^{+})$ and $R(D^{*+})$ using muonic $\u03c4$ decays","abstract":"The branching fraction ratios of $\\overline{B}^0\\to D^+\\tau^-\\overline{\\nu}_{\\tau}$ and $\\overline{B}^0\\to D^{*+}\\tau^-\\overline{\\nu}_{\\tau}$ decays are measured with respect to their muonic counterparts, using a data sample corresponding to an integrated luminosity of 2.0 fb$^{-1}$ collected by the LHCb experiment in proton-proton collisions at $\\sqrt{s} = 13$ TeV. The reconstructed final states are formed by combining $D^+$ mesons with $\\tau^-\\to\\mu^-\\overline{\\nu}_{\\mu}\\nu_{\\tau}$ candidates, where the $D^+$ is reconstructed via the $D^+\\to K^-\\pi^+\\pi^+$ decay. The results are   \\begin{align*}   R(D^{+}) &= 0.249 \\pm 0.043 \\pm 0.047,   R(D^{*+}) &= 0.402 \\pm 0.081\\pm 0.085,   \\end{align*} where the first uncertainties are statistical and the second systematic. The two measurements have a correlation coefficient of $-0.39$ and are compatible with the Standard Model.","sentences":["The branching fraction ratios of $\\overline{B}^0\\to D^+\\tau^-\\overline{\\nu}_{\\tau}$ and $\\overline{B}^0\\to D^{*+}\\tau^-\\overline{\\nu}_{\\tau}$ decays are measured with respect to their muonic counterparts, using a data sample corresponding to an integrated luminosity of 2.0 fb$^{-1}$ collected by the LHCb experiment in proton-proton collisions at $\\sqrt{s} = 13$ TeV.","The reconstructed final states are formed by combining $D^+$ mesons with $\\tau^-\\to\\mu^-\\overline{\\nu}_{\\mu}\\nu_{\\tau}$ candidates, where the $D^+$ is reconstructed via the $D^+\\to K^-\\pi^+\\pi^+$ decay.","The results are   \\begin{align*}   R(D^{+}) &= 0.249 \\pm 0.043 \\pm 0.047,   R(D^{*+}) &= 0.402 \\pm 0.081\\pm 0.085,   \\end{align*} where the first uncertainties are statistical and the second systematic.","The two measurements have a correlation coefficient of $-0.39$ and are compatible with the Standard Model."],"url":"http://arxiv.org/abs/2406.03387v1","category":"hep-ex"}
{"created":"2024-06-05 15:36:55","title":"Discrete Autoregressive Switching Processes in Sparse Graphical Modeling of Multivariate Time Series Data","abstract":"We propose a flexible Bayesian approach for sparse Gaussian graphical modeling of multivariate time series. We account for temporal correlation in the data by assuming that observations are characterized by an underlying and unobserved hidden discrete autoregressive process. We assume multivariate Gaussian emission distributions and capture spatial dependencies by modeling the state-specific precision matrices via graphical horseshoe priors. We characterize the mixing probabilities of the hidden process via a cumulative shrinkage prior that accommodates zero-inflated parameters for non-active components, and further incorporate a sparsity-inducing Dirichlet prior to estimate the effective number of states from the data. For posterior inference, we develop a sampling procedure that allows estimation of the number of discrete autoregressive lags and the number of states, and that cleverly avoids having to deal with the changing dimensions of the parameter space. We thoroughly investigate performance of our proposed methodology through several simulation studies. We further illustrate the use of our approach for the estimation of dynamic brain connectivity based on fMRI data collected on a subject performing a task-based experiment on latent learning","sentences":["We propose a flexible Bayesian approach for sparse Gaussian graphical modeling of multivariate time series.","We account for temporal correlation in the data by assuming that observations are characterized by an underlying and unobserved hidden discrete autoregressive process.","We assume multivariate Gaussian emission distributions and capture spatial dependencies by modeling the state-specific precision matrices via graphical horseshoe priors.","We characterize the mixing probabilities of the hidden process via a cumulative shrinkage prior that accommodates zero-inflated parameters for non-active components, and further incorporate a sparsity-inducing Dirichlet prior to estimate the effective number of states from the data.","For posterior inference, we develop a sampling procedure that allows estimation of the number of discrete autoregressive lags and the number of states, and that cleverly avoids having to deal with the changing dimensions of the parameter space.","We thoroughly investigate performance of our proposed methodology through several simulation studies.","We further illustrate the use of our approach for the estimation of dynamic brain connectivity based on fMRI data collected on a subject performing a task-based experiment on latent learning"],"url":"http://arxiv.org/abs/2406.03385v1","category":"stat.ME"}
{"created":"2024-06-05 14:36:33","title":"Comparative Benchmarking of Failure Detection Methods in Medical Image Segmentation: Unveiling the Role of Confidence Aggregation","abstract":"Semantic segmentation is an essential component of medical image analysis research, with recent deep learning algorithms offering out-of-the-box applicability across diverse datasets. Despite these advancements, segmentation failures remain a significant concern for real-world clinical applications, necessitating reliable detection mechanisms. This paper introduces a comprehensive benchmarking framework aimed at evaluating failure detection methodologies within medical image segmentation. Through our analysis, we identify the strengths and limitations of current failure detection metrics, advocating for the risk-coverage analysis as a holistic evaluation approach. Utilizing a collective dataset comprising five public 3D medical image collections, we assess the efficacy of various failure detection strategies under realistic test-time distribution shifts. Our findings highlight the importance of pixel confidence aggregation and we observe superior performance of the pairwise Dice score (Roy et al., 2019) between ensemble predictions, positioning it as a simple and robust baseline for failure detection in medical image segmentation. To promote ongoing research, we make the benchmarking framework available to the community.","sentences":["Semantic segmentation is an essential component of medical image analysis research, with recent deep learning algorithms offering out-of-the-box applicability across diverse datasets.","Despite these advancements, segmentation failures remain a significant concern for real-world clinical applications, necessitating reliable detection mechanisms.","This paper introduces a comprehensive benchmarking framework aimed at evaluating failure detection methodologies within medical image segmentation.","Through our analysis, we identify the strengths and limitations of current failure detection metrics, advocating for the risk-coverage analysis as a holistic evaluation approach.","Utilizing a collective dataset comprising five public 3D medical image collections, we assess the efficacy of various failure detection strategies under realistic test-time distribution shifts.","Our findings highlight the importance of pixel confidence aggregation and we observe superior performance of the pairwise Dice score (Roy et al., 2019) between ensemble predictions, positioning it as a simple and robust baseline for failure detection in medical image segmentation.","To promote ongoing research, we make the benchmarking framework available to the community."],"url":"http://arxiv.org/abs/2406.03323v1","category":"cs.CV"}
{"created":"2024-06-05 13:53:05","title":"Improved stability for the size and structure of sumsets","abstract":"Let $A \\subset \\mathbb{Z}^d$ be a finite set. It is known that the sumset $NA$ has predictable size ($\\vert NA\\vert = P_A(N)$ for some $P_A(X) \\in \\mathbb{Q}[X]$) and structure (all of the lattice points in some finite cone other than all of the lattice points in a finite collection of exceptional subcones), once $N$ is larger than some threshold. In previous work, joint with Shakan, the first and third named authors established the first effective bounds for both of these thresholds for an arbitrary set $A$. In this article we substantially improve each of these bounds, coming much closer to the corresponding lower bounds known.","sentences":["Let $A \\subset \\mathbb{Z}^d$ be a finite set.","It is known that the sumset $NA$ has predictable size ($\\vert NA\\vert","= P_A(N)$ for some $P_A(X) \\in \\mathbb{Q}[X]$) and structure (all of the lattice points in some finite cone other than all of the lattice points in a finite collection of exceptional subcones), once $N$ is larger than some threshold.","In previous work, joint with Shakan, the first and third named authors established the first effective bounds for both of these thresholds for an arbitrary set $A$.","In this article we substantially improve each of these bounds, coming much closer to the corresponding lower bounds known."],"url":"http://arxiv.org/abs/2406.03275v1","category":"math.CO"}
{"created":"2024-06-05 13:52:45","title":"VWise: A novel benchmark for evaluating scene classification for vehicular applications","abstract":"Current datasets for vehicular applications are mostly collected in North America or Europe. Models trained or evaluated on these datasets might suffer from geographical bias when deployed in other regions. Specifically, for scene classification, a highway in a Latin American country differs drastically from an Autobahn, for example, both in design and maintenance levels. We propose VWise, a novel benchmark for road-type classification and scene classification tasks, in addition to tasks focused on external contexts related to vehicular applications in LatAm. We collected over 520 video clips covering diverse urban and rural environments across Latin American countries, annotated with six classes of road types. We also evaluated several state-of-the-art classification models in baseline experiments, obtaining over 84% accuracy. With this dataset, we aim to enhance research on vehicular tasks in Latin America.","sentences":["Current datasets for vehicular applications are mostly collected in North America or Europe.","Models trained or evaluated on these datasets might suffer from geographical bias when deployed in other regions.","Specifically, for scene classification, a highway in a Latin American country differs drastically from an Autobahn, for example, both in design and maintenance levels.","We propose VWise, a novel benchmark for road-type classification and scene classification tasks, in addition to tasks focused on external contexts related to vehicular applications in LatAm.","We collected over 520 video clips covering diverse urban and rural environments across Latin American countries, annotated with six classes of road types.","We also evaluated several state-of-the-art classification models in baseline experiments, obtaining over 84% accuracy.","With this dataset, we aim to enhance research on vehicular tasks in Latin America."],"url":"http://arxiv.org/abs/2406.03273v1","category":"cs.CV"}
{"created":"2024-06-05 13:23:23","title":"Large Language Models as Evaluators for Recommendation Explanations","abstract":"The explainability of recommender systems has attracted significant attention in academia and industry. Many efforts have been made for explainable recommendations, yet evaluating the quality of the explanations remains a challenging and unresolved issue. In recent years, leveraging LLMs as evaluators presents a promising avenue in Natural Language Processing tasks (e.g., sentiment classification, information extraction), as they perform strong capabilities in instruction following and common-sense reasoning. However, evaluating recommendation explanatory texts is different from these NLG tasks, as its criteria are related to human perceptions and are usually subjective. In this paper, we investigate whether LLMs can serve as evaluators of recommendation explanations. To answer the question, we utilize real user feedback on explanations given from previous work and additionally collect third-party annotations and LLM evaluations. We design and apply a 3-level meta evaluation strategy to measure the correlation between evaluator labels and the ground truth provided by users. Our experiments reveal that LLMs, such as GPT4, can provide comparable evaluations with appropriate prompts and settings. We also provide further insights into combining human labels with the LLM evaluation process and utilizing ensembles of multiple heterogeneous LLM evaluators to enhance the accuracy and stability of evaluations. Our study verifies that utilizing LLMs as evaluators can be an accurate, reproducible and cost-effective solution for evaluating recommendation explanation texts. Our code is available at https://github.com/Xiaoyu-SZ/LLMasEvaluator.","sentences":["The explainability of recommender systems has attracted significant attention in academia and industry.","Many efforts have been made for explainable recommendations, yet evaluating the quality of the explanations remains a challenging and unresolved issue.","In recent years, leveraging LLMs as evaluators presents a promising avenue in Natural Language Processing tasks (e.g., sentiment classification, information extraction), as they perform strong capabilities in instruction following and common-sense reasoning.","However, evaluating recommendation explanatory texts is different from these NLG tasks, as its criteria are related to human perceptions and are usually subjective.","In this paper, we investigate whether LLMs can serve as evaluators of recommendation explanations.","To answer the question, we utilize real user feedback on explanations given from previous work and additionally collect third-party annotations and LLM evaluations.","We design and apply a 3-level meta evaluation strategy to measure the correlation between evaluator labels and the ground truth provided by users.","Our experiments reveal that LLMs, such as GPT4, can provide comparable evaluations with appropriate prompts and settings.","We also provide further insights into combining human labels with the LLM evaluation process and utilizing ensembles of multiple heterogeneous LLM evaluators to enhance the accuracy and stability of evaluations.","Our study verifies that utilizing LLMs as evaluators can be an accurate, reproducible and cost-effective solution for evaluating recommendation explanation texts.","Our code is available at https://github.com/Xiaoyu-SZ/LLMasEvaluator."],"url":"http://arxiv.org/abs/2406.03248v1","category":"cs.IR"}
{"created":"2024-06-05 13:22:04","title":"Intrinsic permeability of heterogeneous porous media","abstract":"Providing a sound appraisal of the nature of the relationship between flow $(Q)$ and pressure drop $(\\Delta P)$ for porous media is a long-standing fundamental research challenge. A wide variety of environmental, societal and industrial issues, ranging, e.g., from water-soil system remediation to subsurface energy optimization, is affected by this critical issue. While such dependence is well represented by the Kozeny-Carman formulation for homogeneous media, the fundamental nature of such a relationship ($Q$ vs $\\Delta P$) within heterogeneous porous systems characterized by a broad range of pore sizes is still not fully understood. We design a set of controlled and complex porous structures and quantify their intrinsic permeability through detailed high quality microfluidics experiments. We synthesize the results upon deriving an original analytical formulation relating the overall intrinsic permeability of the porous structure and their key features. Our formulation explicitly embeds the spatial variability of pore sizes into the medium permeability through a conceptualization of the system as a collection of smaller scale porous media arranged in series. The resulting analytical formulation yields permeability values matching their experimentally-based counterparts without the need of additional tunable parameters. Our study then documents and supports the strong role played by the micro-structure on the overall medium permeability.","sentences":["Providing a sound appraisal of the nature of the relationship between flow $(Q)$ and pressure drop $(\\Delta P)$ for porous media is a long-standing fundamental research challenge.","A wide variety of environmental, societal and industrial issues, ranging, e.g., from water-soil system remediation to subsurface energy optimization, is affected by this critical issue.","While such dependence is well represented by the Kozeny-Carman formulation for homogeneous media, the fundamental nature of such a relationship ($Q$ vs $\\Delta P$) within heterogeneous porous systems characterized by a broad range of pore sizes is still not fully understood.","We design a set of controlled and complex porous structures and quantify their intrinsic permeability through detailed high quality microfluidics experiments.","We synthesize the results upon deriving an original analytical formulation relating the overall intrinsic permeability of the porous structure and their key features.","Our formulation explicitly embeds the spatial variability of pore sizes into the medium permeability through a conceptualization of the system as a collection of smaller scale porous media arranged in series.","The resulting analytical formulation yields permeability values matching their experimentally-based counterparts without the need of additional tunable parameters.","Our study then documents and supports the strong role played by the micro-structure on the overall medium permeability."],"url":"http://arxiv.org/abs/2406.03246v1","category":"physics.flu-dyn"}
{"created":"2024-06-05 13:04:01","title":"Electron Confinement-Induced Plasmonic Breakdown in Metals","abstract":"Plasmon resonance in metals represents the collective oscillation of the free electron gas density and enables enhanced light-matter interactions in nanoscale dimensions. Traditionally, the classical Drude model describes the plasmonic excitation, wherein the plasma frequency exhibits no spatial dispersion. Here, we show conclusive experimental evidence of the breakdown of the plasmon resonance and a consequent photonic metal-insulator transition in an ultrathin archetypal refractory plasmonic material, hafnium nitride (HfN). Epitaxial HfN thick films exhibit a low-loss and high-quality Drude-like plasmon resonance in the visible spectral range. However, as the film thickness is reduced to nanoscale dimensions, the Coulomb interaction among electrons increases due to the electron confinement, leading to the spatial dispersion of the plasma frequency. Importantly, with the further decrease in thickness, electrons lose their ability to shield the incident electric field, turning the medium into a dielectric. The breakdown of the plasmon resonance in epitaxial ultrathin metals could be useful for fundamental physics studies in transdimensional regimes and novel photonic device applications.","sentences":["Plasmon resonance in metals represents the collective oscillation of the free electron gas density and enables enhanced light-matter interactions in nanoscale dimensions.","Traditionally, the classical Drude model describes the plasmonic excitation, wherein the plasma frequency exhibits no spatial dispersion.","Here, we show conclusive experimental evidence of the breakdown of the plasmon resonance and a consequent photonic metal-insulator transition in an ultrathin archetypal refractory plasmonic material, hafnium nitride (HfN).","Epitaxial HfN thick films exhibit a low-loss and high-quality Drude-like plasmon resonance in the visible spectral range.","However, as the film thickness is reduced to nanoscale dimensions, the Coulomb interaction among electrons increases due to the electron confinement, leading to the spatial dispersion of the plasma frequency.","Importantly, with the further decrease in thickness, electrons lose their ability to shield the incident electric field, turning the medium into a dielectric.","The breakdown of the plasmon resonance in epitaxial ultrathin metals could be useful for fundamental physics studies in transdimensional regimes and novel photonic device applications."],"url":"http://arxiv.org/abs/2406.03226v1","category":"physics.optics"}
{"created":"2024-06-05 13:00:04","title":"Linking Named Entities in Diderot's \\textit{Encyclop\u00e9die} to Wikidata","abstract":"Diderot's \\textit{Encyclop\\'edie} is a reference work from XVIIIth century in Europe that aimed at collecting the knowledge of its era. \\textit{Wikipedia} has the same ambition with a much greater scope. However, the lack of digital connection between the two encyclopedias may hinder their comparison and the study of how knowledge has evolved. A key element of \\textit{Wikipedia} is Wikidata that backs the articles with a graph of structured data. In this paper, we describe the annotation of more than 10,300 of the \\textit{Encyclop\\'edie} entries with Wikidata identifiers enabling us to connect these entries to the graph. We considered geographic and human entities. The \\textit{Encyclop\\'edie} does not contain biographic entries as they mostly appear as subentries of locations. We extracted all the geographic entries and we completely annotated all the entries containing a description of human entities. This represents more than 2,600 links referring to locations or human entities. In addition, we annotated more than 9,500 entries having a geographic content only. We describe the annotation process as well as application examples. This resource is available at https://github.com/pnugues/encyclopedie_1751","sentences":["Diderot's \\textit{Encyclop\\'edie} is a reference work from XVIIIth century in Europe that aimed at collecting the knowledge of its era. \\textit{Wikipedia} has the same ambition with a much greater scope.","However, the lack of digital connection between the two encyclopedias may hinder their comparison and the study of how knowledge has evolved.","A key element of \\textit{Wikipedia} is Wikidata that backs the articles with a graph of structured data.","In this paper, we describe the annotation of more than 10,300 of the \\textit{Encyclop\\'edie} entries with Wikidata identifiers enabling us to connect these entries to the graph.","We considered geographic and human entities.","The \\textit{Encyclop\\'edie} does not contain biographic entries as they mostly appear as subentries of locations.","We extracted all the geographic entries and we completely annotated all the entries containing a description of human entities.","This represents more than 2,600 links referring to locations or human entities.","In addition, we annotated more than 9,500 entries having a geographic content only.","We describe the annotation process as well as application examples.","This resource is available at https://github.com/pnugues/encyclopedie_1751"],"url":"http://arxiv.org/abs/2406.03221v1","category":"cs.CL"}
{"created":"2024-06-05 10:42:04","title":"Very-long-baseline interferometry study of the flaring blazar TXS 1508+572 in the early Universe","abstract":"High-redshift blazars provide valuable input to studies of the evolution of active galactic nuclei (AGN) jets and provide constraints on cosmological models. Detections at high energies ($0.1<\\mathrm{E}<100$ GeV) of these distant sources are rare, but when they exhibit bright gamma-ray flares, we are able to study them. However, contemporaneous multi-wavelength observations of high-redshift objects ($z>4$) during their different periods of activity have not been carried out so far. An excellent opportunity for such a study arose when the blazar TXS 1508+572 ($z=4.31$) exhibited a $\\gamma$-ray flare in 2022 February in the $0.1-300$ GeV range with a flux 25 times brighter than the one reported in the in the fourth catalog of the \\textit{Fermi} Large Area Telescope. Our goal is to monitor the morphological changes, spectral index and opacity variations that could be associated with the preceding $\\gamma$-ray flare in TXS 1508+572 to find the origin of the high-energy emission in this source. We also plan to compare the source characteristics in the radio band to the blazars in the local Universe ($z<0.1$). In addition, we aim to collect quasi-simultaneous data to our multi-wavelength observations of the object, making TXS 1508+572 the first blazar in the early Universe ($z>4$) with contemporaneous multi-frequency data available in its high state. In order to study the parsec-scale structure of the source, we performed three epochs of very-long-baseline interferometry (VLBI) follow-up observations with the Very Long Baseline Array (VLBA) supplemented with the Effelsberg 100-m Telescope at 15, 22, and 43 GHz, which corresponds to 80, 117, and 228 GHz in the rest frame of TXS 1508+572. In addition, one 86 GHz (456 GHz) measurement was performed by the VLBA and the Green Bank Telescope during the first epoch.","sentences":["High-redshift blazars provide valuable input to studies of the evolution of active galactic nuclei (AGN) jets and provide constraints on cosmological models.","Detections at high energies ($0.1<\\mathrm{E}<100$ GeV) of these distant sources are rare, but when they exhibit bright gamma-ray flares, we are able to study them.","However, contemporaneous multi-wavelength observations of high-redshift objects ($z>4$) during their different periods of activity have not been carried out so far.","An excellent opportunity for such a study arose when the blazar TXS 1508+572 ($z=4.31$) exhibited a $\\gamma$-ray flare in 2022 February in the $0.1-300$ GeV range with a flux 25 times brighter than the one reported in the in the fourth catalog of the \\textit{Fermi} Large Area Telescope.","Our goal is to monitor the morphological changes, spectral index and opacity variations that could be associated with the preceding $\\gamma$-ray flare in TXS 1508+572 to find the origin of the high-energy emission in this source.","We also plan to compare the source characteristics in the radio band to the blazars in the local Universe ($z<0.1$).","In addition, we aim to collect quasi-simultaneous data to our multi-wavelength observations of the object, making TXS 1508+572 the first blazar in the early Universe ($z>4$) with contemporaneous multi-frequency data available in its high state.","In order to study the parsec-scale structure of the source, we performed three epochs of very-long-baseline interferometry (VLBI) follow-up observations with the Very Long Baseline Array (VLBA) supplemented with the Effelsberg 100-m Telescope at 15, 22, and 43 GHz, which corresponds to 80, 117, and 228 GHz in the rest frame of TXS 1508+572.","In addition, one 86 GHz (456 GHz) measurement was performed by the VLBA and the Green Bank Telescope during the first epoch."],"url":"http://arxiv.org/abs/2406.03135v1","category":"astro-ph.HE"}
{"created":"2024-06-05 08:39:10","title":"Predicting unobserved climate time series data at distant areas via spatial correlation using reservoir computing","abstract":"Collecting time series data spatially distributed in many locations is often important for analyzing climate change and its impacts on ecosystems. However, comprehensive spatial data collection is not always feasible, requiring us to predict climate variables at some locations. This study focuses on a prediction of climatic elements, specifically near-surface temperature and pressure, at a target location apart from a data observation point. Our approach uses two prediction methods: reservoir computing (RC), known as a machine learning framework with low computational requirements, and vector autoregression models (VAR), recognized as a statistical method for analyzing time series data. Our results show that the accuracy of the predictions degrades with the distance between the observation and target locations. We quantitatively estimate the distance in which effective predictions are possible. We also find that in the context of climate data, a geographical distance is associated with data correlation, and a strong data correlation significantly improves the prediction accuracy with RC. In particular, RC outperforms VAR in predicting highly correlated data within the predictive range. These findings suggest that machine learning-based methods can be used more effectively to predict climatic elements in remote locations by assessing the distance to them from the data observation point in advance. Our study on low-cost and accurate prediction of climate variables has significant value for climate change strategies.","sentences":["Collecting time series data spatially distributed in many locations is often important for analyzing climate change and its impacts on ecosystems.","However, comprehensive spatial data collection is not always feasible, requiring us to predict climate variables at some locations.","This study focuses on a prediction of climatic elements, specifically near-surface temperature and pressure, at a target location apart from a data observation point.","Our approach uses two prediction methods: reservoir computing (RC), known as a machine learning framework with low computational requirements, and vector autoregression models (VAR), recognized as a statistical method for analyzing time series data.","Our results show that the accuracy of the predictions degrades with the distance between the observation and target locations.","We quantitatively estimate the distance in which effective predictions are possible.","We also find that in the context of climate data, a geographical distance is associated with data correlation, and a strong data correlation significantly improves the prediction accuracy with RC.","In particular, RC outperforms VAR in predicting highly correlated data within the predictive range.","These findings suggest that machine learning-based methods can be used more effectively to predict climatic elements in remote locations by assessing the distance to them from the data observation point in advance.","Our study on low-cost and accurate prediction of climate variables has significant value for climate change strategies."],"url":"http://arxiv.org/abs/2406.03061v1","category":"cs.LG"}
{"created":"2024-06-05 08:10:27","title":"Theory and Modeling of Transport in Nanoporous Materials: From Microscopic to Coarse-Grained Descriptions","abstract":"This review presents the state of the art on the theory, molecular simulation, and coarse-grained strategies applied to the transport of gases and liquids in nanoporous materials (pore size in the range 1 - 100 nm). Recent advances in the understanding of molecular diffusion and transport under thermodynamic gradients in nanoporous adsorbents are discussed with special emphasis on small molecules in zeolites, active carbons, metal organic frameworks, but also in nanoporous materials with larger pores such as ordered and disordered mesoporous oxides. We provide a description of the fundamentals and principles of the different atom-scale and mesoscopic methods as well as of the theoretical formalisms that can be used to address such an important problem. Special attention is paid to the investigation of different molecular transport coefficients - including the so-called self, collective and transport diffusivities - but also to the determination of free energy barriers and their role in overall adsorption/separation process rates. We also introduce other available approaches such as hierarchical simulations and upscaling strategies.","sentences":["This review presents the state of the art on the theory, molecular simulation, and coarse-grained strategies applied to the transport of gases and liquids in nanoporous materials (pore size in the range 1 - 100 nm).","Recent advances in the understanding of molecular diffusion and transport under thermodynamic gradients in nanoporous adsorbents are discussed with special emphasis on small molecules in zeolites, active carbons, metal organic frameworks, but also in nanoporous materials with larger pores such as ordered and disordered mesoporous oxides.","We provide a description of the fundamentals and principles of the different atom-scale and mesoscopic methods as well as of the theoretical formalisms that can be used to address such an important problem.","Special attention is paid to the investigation of different molecular transport coefficients - including the so-called self, collective and transport diffusivities - but also to the determination of free energy barriers and their role in overall adsorption/separation process rates.","We also introduce other available approaches such as hierarchical simulations and upscaling strategies."],"url":"http://arxiv.org/abs/2406.03039v1","category":"cond-mat.soft"}
{"created":"2024-06-05 07:34:39","title":"Puzzle Pieces Picker: Deciphering Ancient Chinese Characters with Radical Reconstruction","abstract":"Oracle Bone Inscriptions is one of the oldest existing forms of writing in the world. However, due to the great antiquity of the era, a large number of Oracle Bone Inscriptions (OBI) remain undeciphered, making it one of the global challenges in the field of paleography today. This paper introduces a novel approach, namely Puzzle Pieces Picker (P$^3$), to decipher these enigmatic characters through radical reconstruction. We deconstruct OBI into foundational strokes and radicals, then employ a Transformer model to reconstruct them into their modern (conterpart)\\textcolor{blue}{counterparts}, offering a groundbreaking solution to ancient script analysis. To further this endeavor, a new Ancient Chinese Character Puzzles (ACCP) dataset was developed, comprising an extensive collection of character images from seven key historical stages, annotated with detailed radical sequences. The experiments have showcased considerable promising insights, underscoring the potential and effectiveness of our approach in deciphering the intricacies of ancient Chinese scripts. Through this novel dataset and methodology, we aim to bridge the gap between traditional philology and modern document analysis techniques, offering new insights into the rich history of Chinese linguistic heritage.","sentences":["Oracle Bone Inscriptions is one of the oldest existing forms of writing in the world.","However, due to the great antiquity of the era, a large number of Oracle Bone Inscriptions (OBI) remain undeciphered, making it one of the global challenges in the field of paleography today.","This paper introduces a novel approach, namely Puzzle Pieces Picker (P$^3$), to decipher these enigmatic characters through radical reconstruction.","We deconstruct OBI into foundational strokes and radicals, then employ a Transformer model to reconstruct them into their modern (conterpart)\\textcolor{blue}{counterparts}, offering a groundbreaking solution to ancient script analysis.","To further this endeavor, a new Ancient Chinese Character Puzzles (ACCP) dataset was developed, comprising an extensive collection of character images from seven key historical stages, annotated with detailed radical sequences.","The experiments have showcased considerable promising insights, underscoring the potential and effectiveness of our approach in deciphering the intricacies of ancient Chinese scripts.","Through this novel dataset and methodology, we aim to bridge the gap between traditional philology and modern document analysis techniques, offering new insights into the rich history of Chinese linguistic heritage."],"url":"http://arxiv.org/abs/2406.03019v1","category":"cs.CV"}
{"created":"2024-06-05 07:20:06","title":"Analyzing the Influence of Training Samples on Explanations","abstract":"EXplainable AI (XAI) constitutes a popular method to analyze the reasoning of AI systems by explaining their decision-making, e.g. providing a counterfactual explanation of how to achieve recourse. However, in cases such as unexpected explanations, the user might be interested in learning about the cause of this explanation -- e.g. properties of the utilized training data that are responsible for the observed explanation. Under the umbrella of data valuation, first approaches have been proposed that estimate the influence of data samples on a given model. In this work, we take a slightly different stance, as we are interested in the influence of single samples on a model explanation rather than the model itself. Hence, we propose the novel problem of identifying training data samples that have a high influence on a given explanation (or related quantity) and investigate the particular case of differences in the cost of the recourse between protected groups. For this, we propose an algorithm that identifies such influential training samples.","sentences":["EXplainable AI (XAI) constitutes a popular method to analyze the reasoning of AI systems by explaining their decision-making, e.g. providing a counterfactual explanation of how to achieve recourse.","However, in cases such as unexpected explanations, the user might be interested in learning about the cause of this explanation -- e.g. properties of the utilized training data that are responsible for the observed explanation.","Under the umbrella of data valuation, first approaches have been proposed that estimate the influence of data samples on a given model.","In this work, we take a slightly different stance, as we are interested in the influence of single samples on a model explanation rather than the model itself.","Hence, we propose the novel problem of identifying training data samples that have a high influence on a given explanation (or related quantity) and investigate the particular case of differences in the cost of the recourse between protected groups.","For this, we propose an algorithm that identifies such influential training samples."],"url":"http://arxiv.org/abs/2406.03012v1","category":"cs.LG"}
{"created":"2024-06-05 07:18:56","title":"Huygens-Fresnel Model Based Position-Aided Phase Configuration for 1-Bit RIS Assisted Wireless Communication","abstract":"Reconfigurable intelligent surface (RIS), composed of nearly passive elements, is regarded as one of the potential paradigms to support multi-gigabit data in real-time. However, in traditional CSI (channel state information) driven frame, the training overhead of channel estimation greatly increases as the number of RIS elements increases to intelligently manipulate the reflected signals. To conveniently use the reflected signal without complex CSI feedback, in this paper we propose a position-aided phase configuration scheme based on the property of Fresnel zone. In particular, we design the impedance based discrete RIS elements with joint absorption mode and reflection mode considering the fabrication complexities, which integrated the property of the Fresnel zone to resist the impact of position error. Then, with joint absorption and 1-bit reflection mode elements, we develop the two-step position-aided ON/OFF states judgement (TPOSJ) scheme and the frame structure to control the ON/OFF state of RIS, followed by analyzing the impacts of mobility and position error on our proposed scheme. Also, we derive the Helmholtz-Kirchhoff integral theorem based power flow. Simulations show that the proposed scheme can manipulate the ON/OFF state intelligently without complex CSI, thus verifying the practical application of our proposed scheme.","sentences":["Reconfigurable intelligent surface (RIS), composed of nearly passive elements, is regarded as one of the potential paradigms to support multi-gigabit data in real-time.","However, in traditional CSI (channel state information) driven frame, the training overhead of channel estimation greatly increases as the number of RIS elements increases to intelligently manipulate the reflected signals.","To conveniently use the reflected signal without complex CSI feedback, in this paper we propose a position-aided phase configuration scheme based on the property of Fresnel zone.","In particular, we design the impedance based discrete RIS elements with joint absorption mode and reflection mode considering the fabrication complexities, which integrated the property of the Fresnel zone to resist the impact of position error.","Then, with joint absorption and 1-bit reflection mode elements, we develop the two-step position-aided ON/OFF states judgement (TPOSJ) scheme and the frame structure to control the ON/OFF state of RIS, followed by analyzing the impacts of mobility and position error on our proposed scheme.","Also, we derive the Helmholtz-Kirchhoff integral theorem based power flow.","Simulations show that the proposed scheme can manipulate the ON/OFF state intelligently without complex CSI, thus verifying the practical application of our proposed scheme."],"url":"http://arxiv.org/abs/2406.03011v1","category":"cs.IT"}
{"created":"2024-06-05 07:16:51","title":"Unveiling Selection Biases: Exploring Order and Token Sensitivity in Large Language Models","abstract":"In this paper, we investigate the phenomena of \"selection biases\" in Large Language Models (LLMs), focusing on problems where models are tasked with choosing the optimal option from an ordered sequence. We delve into biases related to option order and token usage, which significantly impact LLMs' decision-making processes. We also quantify the impact of these biases through an extensive empirical analysis across multiple models and tasks. Furthermore, we propose mitigation strategies to enhance model performance. Our key contributions are threefold: 1) Precisely quantifying the influence of option order and token on LLMs, 2) Developing strategies to mitigate the impact of token and order sensitivity to enhance robustness, and 3) Offering a detailed analysis of sensitivity across models and tasks, which informs the creation of more stable and reliable LLM applications for selection problems.","sentences":["In this paper, we investigate the phenomena of \"selection biases\" in Large Language Models (LLMs), focusing on problems where models are tasked with choosing the optimal option from an ordered sequence.","We delve into biases related to option order and token usage, which significantly impact LLMs' decision-making processes.","We also quantify the impact of these biases through an extensive empirical analysis across multiple models and tasks.","Furthermore, we propose mitigation strategies to enhance model performance.","Our key contributions are threefold: 1) Precisely quantifying the influence of option order and token on LLMs, 2) Developing strategies to mitigate the impact of token and order sensitivity to enhance robustness, and 3) Offering a detailed analysis of sensitivity across models and tasks, which informs the creation of more stable and reliable LLM applications for selection problems."],"url":"http://arxiv.org/abs/2406.03009v1","category":"cs.CL"}
{"created":"2024-06-05 07:14:44","title":"DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences","abstract":"Recent advancements in foundation models (FMs) have unlocked new prospects in autonomous driving, yet the experimental settings of these studies are preliminary, over-simplified, and fail to capture the complexity of real-world driving scenarios in human environments. It remains under-explored whether FM agents can handle long-horizon navigation tasks with free-from dialogue and deal with unexpected situations caused by environmental dynamics or task changes. To explore the capabilities and boundaries of FMs faced with the challenges above, we introduce DriVLMe, a video-language-model-based agent to facilitate natural and effective communication between humans and autonomous vehicles that perceive the environment and navigate. We develop DriVLMe from both embodied experiences in a simulated environment and social experiences from real human dialogue. While DriVLMe demonstrates competitive performance in both open-loop benchmarks and closed-loop human studies, we reveal several limitations and challenges, including unacceptable inference time, imbalanced training data, limited visual understanding, challenges with multi-turn interactions, simplified language generation from robotic experiences, and difficulties in handling on-the-fly unexpected situations like environmental dynamics and task changes.","sentences":["Recent advancements in foundation models (FMs) have unlocked new prospects in autonomous driving, yet the experimental settings of these studies are preliminary, over-simplified, and fail to capture the complexity of real-world driving scenarios in human environments.","It remains under-explored whether FM agents can handle long-horizon navigation tasks with free-from dialogue and deal with unexpected situations caused by environmental dynamics or task changes.","To explore the capabilities and boundaries of FMs faced with the challenges above, we introduce DriVLMe, a video-language-model-based agent to facilitate natural and effective communication between humans and autonomous vehicles that perceive the environment and navigate.","We develop DriVLMe from both embodied experiences in a simulated environment and social experiences from real human dialogue.","While DriVLMe demonstrates competitive performance in both open-loop benchmarks and closed-loop human studies, we reveal several limitations and challenges, including unacceptable inference time, imbalanced training data, limited visual understanding, challenges with multi-turn interactions, simplified language generation from robotic experiences, and difficulties in handling on-the-fly unexpected situations like environmental dynamics and task changes."],"url":"http://arxiv.org/abs/2406.03008v1","category":"cs.CV"}
{"created":"2024-06-05 07:14:28","title":"BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents","abstract":"With the prosperity of large language models (LLMs), powerful LLM-based intelligent agents have been developed to provide customized services with a set of user-defined tools. State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task. However, we show that such methods are vulnerable to our proposed backdoor attacks named BadAgent on various agent tasks, where a backdoor can be embedded by fine-tuning on the backdoor data. At test time, the attacker can manipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment. To our surprise, our proposed attack methods are extremely robust even after fine-tuning on trustworthy data. Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools. Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data. Our code is public at https://github.com/DPamK/BadAgent","sentences":["With the prosperity of large language models (LLMs), powerful LLM-based intelligent agents have been developed to provide customized services with a set of user-defined tools.","State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task.","However, we show that such methods are vulnerable to our proposed backdoor attacks named BadAgent on various agent tasks, where a backdoor can be embedded by fine-tuning on the backdoor data.","At test time, the attacker can manipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment.","To our surprise, our proposed attack methods are extremely robust even after fine-tuning on trustworthy data.","Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools.","Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data.","Our code is public at https://github.com/DPamK/BadAgent"],"url":"http://arxiv.org/abs/2406.03007v1","category":"cs.CL"}
{"created":"2024-06-05 07:11:56","title":"Evaluation of data inconsistency for multi-modal sentiment analysis","abstract":"Emotion semantic inconsistency is an ubiquitous challenge in multi-modal sentiment analysis (MSA). MSA involves analyzing sentiment expressed across various modalities like text, audio, and videos. Each modality may convey distinct aspects of sentiment, due to subtle and nuanced expression of human beings, leading to inconsistency, which may hinder the prediction of artificial agents. In this work, we introduce a modality conflicting test set and assess the performance of both traditional multi-modal sentiment analysis models and multi-modal large language models (MLLMs). Our findings reveal significant performance degradation across traditional models when confronted with semantically conflicting data and point out the drawbacks of MLLMs when handling multi-modal emotion analysis. Our research presents a new challenge and offer valuable insights for the future development of sentiment analysis systems.","sentences":["Emotion semantic inconsistency is an ubiquitous challenge in multi-modal sentiment analysis (MSA).","MSA involves analyzing sentiment expressed across various modalities like text, audio, and videos.","Each modality may convey distinct aspects of sentiment, due to subtle and nuanced expression of human beings, leading to inconsistency, which may hinder the prediction of artificial agents.","In this work, we introduce a modality conflicting test set and assess the performance of both traditional multi-modal sentiment analysis models and multi-modal large language models (MLLMs).","Our findings reveal significant performance degradation across traditional models when confronted with semantically conflicting data and point out the drawbacks of MLLMs when handling multi-modal emotion analysis.","Our research presents a new challenge and offer valuable insights for the future development of sentiment analysis systems."],"url":"http://arxiv.org/abs/2406.03004v1","category":"cs.CL"}
{"created":"2024-06-05 07:06:26","title":"EdgeSync: Faster Edge-model Updating via Adaptive Continuous Learning for Video Data Drift","abstract":"Real-time video analytics systems typically place models with fewer weights on edge devices to reduce latency. The distribution of video content features may change over time for various reasons (i.e. light and weather change) , leading to accuracy degradation of existing models, to solve this problem, recent work proposes a framework that uses a remote server to continually train and adapt the lightweight model at edge with the help of complex model. However, existing analytics approaches leave two challenges untouched: firstly, retraining task is compute-intensive, resulting in large model update delays; secondly, new model may not fit well enough with the data distribution of the current video stream. To address these challenges, in this paper, we present EdgeSync, EdgeSync filters the samples by considering both timeliness and inference results to make training samples more relevant to the current video content as well as reduce the update delay, to improve the quality of training, EdgeSync also designs a training management module that can efficiently adjusts the model training time and training order on the runtime. By evaluating real datasets with complex scenes, our method improves about 3.4% compared to existing methods and about 10% compared to traditional means.","sentences":["Real-time video analytics systems typically place models with fewer weights on edge devices to reduce latency.","The distribution of video content features may change over time for various reasons (i.e. light and weather change) , leading to accuracy degradation of existing models, to solve this problem, recent work proposes a framework that uses a remote server to continually train and adapt the lightweight model at edge with the help of complex model.","However, existing analytics approaches leave two challenges untouched: firstly, retraining task is compute-intensive, resulting in large model update delays; secondly, new model may not fit well enough with the data distribution of the current video stream.","To address these challenges, in this paper, we present EdgeSync, EdgeSync filters the samples by considering both timeliness and inference results to make training samples more relevant to the current video content as well as reduce the update delay, to improve the quality of training, EdgeSync also designs a training management module that can efficiently adjusts the model training time and training order on the runtime.","By evaluating real datasets with complex scenes, our method improves about 3.4% compared to existing methods and about 10% compared to traditional means."],"url":"http://arxiv.org/abs/2406.03001v1","category":"cs.CV"}
{"created":"2024-06-05 07:05:52","title":"Simplification of Risk Averse POMDPs with Performance Guarantees","abstract":"Risk averse decision making under uncertainty in partially observable domains is a fundamental problem in AI and essential for reliable autonomous agents. In our case, the problem is modeled using partially observable Markov decision processes (POMDPs), when the value function is the conditional value at risk (CVaR) of the return. Calculating an optimal solution for POMDPs is computationally intractable in general. In this work we develop a simplification framework to speedup the evaluation of the value function, while providing performance guarantees. We consider as simplification a computationally cheaper belief-MDP transition model, that can correspond, e.g., to cheaper observation or transition models. Our contributions include general bounds for CVaR that allow bounding the CVaR of a random variable X, using a random variable Y, by assuming bounds between their cumulative distributions. We then derive bounds for the CVaR value function in a POMDP setting, and show how to bound the value function using the computationally cheaper belief-MDP transition model and without accessing the computationally expensive model in real-time. Then, we provide theoretical performance guarantees for the estimated bounds. Our results apply for a general simplification of a belief-MDP transition model and support simplification of both the observation and state transition models simultaneously.","sentences":["Risk averse decision making under uncertainty in partially observable domains is a fundamental problem in AI and essential for reliable autonomous agents.","In our case, the problem is modeled using partially observable Markov decision processes (POMDPs), when the value function is the conditional value at risk (CVaR) of the return.","Calculating an optimal solution for POMDPs is computationally intractable in general.","In this work we develop a simplification framework to speedup the evaluation of the value function, while providing performance guarantees.","We consider as simplification a computationally cheaper belief-MDP transition model, that can correspond, e.g., to cheaper observation or transition models.","Our contributions include general bounds for CVaR that allow bounding the CVaR of a random variable X, using a random variable Y, by assuming bounds between their cumulative distributions.","We then derive bounds for the CVaR value function in a POMDP setting, and show how to bound the value function using the computationally cheaper belief-MDP transition model and without accessing the computationally expensive model in real-time.","Then, we provide theoretical performance guarantees for the estimated bounds.","Our results apply for a general simplification of a belief-MDP transition model and support simplification of both the observation and state transition models simultaneously."],"url":"http://arxiv.org/abs/2406.03000v1","category":"cs.AI"}
{"created":"2024-06-05 06:40:04","title":"Learning Semantic Traversability with Egocentric Video and Automated Annotation Strategy","abstract":"For reliable autonomous robot navigation in urban settings, the robot must have the ability to identify semantically traversable terrains in the image based on the semantic understanding of the scene. This reasoning ability is based on semantic traversability, which is frequently achieved using semantic segmentation models fine-tuned on the testing domain. This fine-tuning process often involves manual data collection with the target robot and annotation by human labelers which is prohibitively expensive and unscalable. In this work, we present an effective methodology for training a semantic traversability estimator using egocentric videos and an automated annotation process. Egocentric videos are collected from a camera mounted on a pedestrian's chest. The dataset for training the semantic traversability estimator is then automatically generated by extracting semantically traversable regions in each video frame using a recent foundation model in image segmentation and its prompting technique. Extensive experiments with videos taken across several countries and cities, covering diverse urban scenarios, demonstrate the high scalability and generalizability of the proposed annotation method. Furthermore, performance analysis and real-world deployment for autonomous robot navigation showcase that the trained semantic traversability estimator is highly accurate, able to handle diverse camera viewpoints, computationally light, and real-world applicable. The summary video is available at https://youtu.be/EUVoH-wA-lA.","sentences":["For reliable autonomous robot navigation in urban settings, the robot must have the ability to identify semantically traversable terrains in the image based on the semantic understanding of the scene.","This reasoning ability is based on semantic traversability, which is frequently achieved using semantic segmentation models fine-tuned on the testing domain.","This fine-tuning process often involves manual data collection with the target robot and annotation by human labelers which is prohibitively expensive and unscalable.","In this work, we present an effective methodology for training a semantic traversability estimator using egocentric videos and an automated annotation process.","Egocentric videos are collected from a camera mounted on a pedestrian's chest.","The dataset for training the semantic traversability estimator is then automatically generated by extracting semantically traversable regions in each video frame using a recent foundation model in image segmentation and its prompting technique.","Extensive experiments with videos taken across several countries and cities, covering diverse urban scenarios, demonstrate the high scalability and generalizability of the proposed annotation method.","Furthermore, performance analysis and real-world deployment for autonomous robot navigation showcase that the trained semantic traversability estimator is highly accurate, able to handle diverse camera viewpoints, computationally light, and real-world applicable.","The summary video is available at https://youtu.be/EUVoH-wA-lA."],"url":"http://arxiv.org/abs/2406.02989v1","category":"cs.RO"}
{"created":"2024-06-05 06:26:15","title":"FREA: Feasibility-Guided Generation of Safety-Critical Scenarios with Reasonable Adversariality","abstract":"Generating safety-critical scenarios, which are essential yet difficult to collect at scale, offers an effective method to evaluate the robustness of autonomous vehicles (AVs). Existing methods focus on optimizing adversariality while preserving the naturalness of scenarios, aiming to achieve a balance through data-driven approaches. However, without an appropriate upper bound for adversariality, the scenarios might exhibit excessive adversariality, potentially leading to unavoidable collisions. In this paper, we introduce FREA, a novel safety-critical scenarios generation method that incorporates the Largest Feasible Region (LFR) of AV as guidance to ensure the reasonableness of the adversarial scenarios. Concretely, FREA initially pre-calculates the LFR of AV from offline datasets. Subsequently, it learns a reasonable adversarial policy that controls critical background vehicles (CBVs) in the scene to generate adversarial yet AV-feasible scenarios by maximizing a novel feasibility-dependent objective function. Extensive experiments illustrate that FREA can effectively generate safety-critical scenarios, yielding considerable near-miss events while ensuring AV's feasibility. Generalization analysis also confirms the robustness of FREA in AV testing across various surrogate AV methods and traffic environments.","sentences":["Generating safety-critical scenarios, which are essential yet difficult to collect at scale, offers an effective method to evaluate the robustness of autonomous vehicles (AVs).","Existing methods focus on optimizing adversariality while preserving the naturalness of scenarios, aiming to achieve a balance through data-driven approaches.","However, without an appropriate upper bound for adversariality, the scenarios might exhibit excessive adversariality, potentially leading to unavoidable collisions.","In this paper, we introduce FREA, a novel safety-critical scenarios generation method that incorporates the Largest Feasible Region (LFR) of AV as guidance to ensure the reasonableness of the adversarial scenarios.","Concretely, FREA initially pre-calculates the LFR of AV from offline datasets.","Subsequently, it learns a reasonable adversarial policy that controls critical background vehicles (CBVs) in the scene to generate adversarial yet AV-feasible scenarios by maximizing a novel feasibility-dependent objective function.","Extensive experiments illustrate that FREA can effectively generate safety-critical scenarios, yielding considerable near-miss events while ensuring AV's feasibility.","Generalization analysis also confirms the robustness of FREA in AV testing across various surrogate AV methods and traffic environments."],"url":"http://arxiv.org/abs/2406.02983v1","category":"cs.RO"}
{"created":"2024-06-05 06:23:11","title":"Tensor Polynomial Additive Model","abstract":"Additive models can be used for interpretable machine learning for their clarity and simplicity. However, In the classical models for high-order data, the vectorization operation disrupts the data structure, which may lead to degenerated accuracy and increased computational complexity. To deal with these problems, we propose the tensor polynomial addition model (TPAM). It retains the multidimensional structure information of high-order inputs with tensor representation. The model parameter compression is achieved using a hierarchical and low-order symmetric tensor approximation. In this way, complex high-order feature interactions can be captured with fewer parameters. Moreover, The TPAM preserves the inherent interpretability of additive models, facilitating transparent decision-making and the extraction of meaningful feature values. Additionally, leveraging TPAM's transparency and ability to handle higher-order features, it is used as a post-processing module for other interpretation models by introducing two variants for class activation maps. Experimental results on a series of datasets demonstrate that TPAM can enhance accuracy by up to 30\\%, and compression rate by up to 5 times, while maintaining a good interpretability.","sentences":["Additive models can be used for interpretable machine learning for their clarity and simplicity.","However, In the classical models for high-order data, the vectorization operation disrupts the data structure, which may lead to degenerated accuracy and increased computational complexity.","To deal with these problems, we propose the tensor polynomial addition model (TPAM).","It retains the multidimensional structure information of high-order inputs with tensor representation.","The model parameter compression is achieved using a hierarchical and low-order symmetric tensor approximation.","In this way, complex high-order feature interactions can be captured with fewer parameters.","Moreover, The TPAM preserves the inherent interpretability of additive models, facilitating transparent decision-making and the extraction of meaningful feature values.","Additionally, leveraging TPAM's transparency and ability to handle higher-order features, it is used as a post-processing module for other interpretation models by introducing two variants for class activation maps.","Experimental results on a series of datasets demonstrate that TPAM can enhance accuracy by up to 30\\%, and compression rate by up to 5 times, while maintaining a good interpretability."],"url":"http://arxiv.org/abs/2406.02980v1","category":"cs.LG"}
{"created":"2024-06-05 06:22:11","title":"Efficient User Sequence Learning for Online Services via Compressed Graph Neural Networks","abstract":"Learning representations of user behavior sequences is crucial for various online services, such as online fraudulent transaction detection mechanisms. Graph Neural Networks (GNNs) have been extensively applied to model sequence relationships, and extract information from similar sequences. While user behavior sequence data volume is usually huge for online applications, directly applying GNN models may lead to substantial computational overhead during both the training and inference stages and make it challenging to meet real-time requirements for online services. In this paper, we leverage graph compression techniques to alleviate the efficiency issue. Specifically, we propose a novel unified framework called ECSeq, to introduce graph compression techniques into relation modeling for user sequence representation learning. The key module of ECSeq is sequence relation modeling, which explores relationships among sequences to enhance sequence representation learning, and employs graph compression algorithms to achieve high efficiency and scalability. ECSeq also exhibits plug-and-play characteristics, seamlessly augmenting pre-trained sequence representation models without modifications. Empirical experiments on both sequence classification and regression tasks demonstrate the effectiveness of ECSeq. Specifically, with an additional training time of tens of seconds in total on 100,000+ sequences and inference time preserved within $10^{-4}$ seconds/sample, ECSeq improves the prediction R@P$_{0.9}$ of the widely used LSTM by $\\sim 5\\%$.","sentences":["Learning representations of user behavior sequences is crucial for various online services, such as online fraudulent transaction detection mechanisms.","Graph Neural Networks (GNNs) have been extensively applied to model sequence relationships, and extract information from similar sequences.","While user behavior sequence data volume is usually huge for online applications, directly applying GNN models may lead to substantial computational overhead during both the training and inference stages and make it challenging to meet real-time requirements for online services.","In this paper, we leverage graph compression techniques to alleviate the efficiency issue.","Specifically, we propose a novel unified framework called ECSeq, to introduce graph compression techniques into relation modeling for user sequence representation learning.","The key module of ECSeq is sequence relation modeling, which explores relationships among sequences to enhance sequence representation learning, and employs graph compression algorithms to achieve high efficiency and scalability.","ECSeq also exhibits plug-and-play characteristics, seamlessly augmenting pre-trained sequence representation models without modifications.","Empirical experiments on both sequence classification and regression tasks demonstrate the effectiveness of ECSeq.","Specifically, with an additional training time of tens of seconds in total on 100,000+ sequences and inference time preserved within $10^{-4}$ seconds/sample, ECSeq improves the prediction R@P$_{0.9}$ of the widely used LSTM by $\\sim 5\\%$."],"url":"http://arxiv.org/abs/2406.02979v1","category":"cs.LG"}
{"created":"2024-06-05 06:18:03","title":"DA-Flow: Dual Attention Normalizing Flow for Skeleton-based Video Anomaly Detection","abstract":"Cooperation between temporal convolutional networks (TCN) and graph convolutional networks (GCN) as a processing module has shown promising results in skeleton-based video anomaly detection (SVAD). However, to maintain a lightweight model with low computational and storage complexity, shallow GCN and TCN blocks are constrained by small receptive fields and a lack of cross-dimension interaction capture. To tackle this limitation, we propose a lightweight module called the Dual Attention Module (DAM) for capturing cross-dimension interaction relationships in spatio-temporal skeletal data. It employs the frame attention mechanism to identify the most significant frames and the skeleton attention mechanism to capture broader relationships across fixed partitions with minimal parameters and flops. Furthermore, the proposed Dual Attention Normalizing Flow (DA-Flow) integrates the DAM as a post-processing unit after GCN within the normalizing flow framework. Simulations show that the proposed model is robust against noise and negative samples. Experimental results show that DA-Flow reaches competitive or better performance than the existing state-of-the-art (SOTA) methods in terms of the micro AUC metric with the fewest number of parameters. Moreover, we found that even without training, simply using random projection without dimensionality reduction on skeleton data enables substantial anomaly detection capabilities.","sentences":["Cooperation between temporal convolutional networks (TCN) and graph convolutional networks (GCN) as a processing module has shown promising results in skeleton-based video anomaly detection (SVAD).","However, to maintain a lightweight model with low computational and storage complexity, shallow GCN and TCN blocks are constrained by small receptive fields and a lack of cross-dimension interaction capture.","To tackle this limitation, we propose a lightweight module called the Dual Attention Module (DAM) for capturing cross-dimension interaction relationships in spatio-temporal skeletal data.","It employs the frame attention mechanism to identify the most significant frames and the skeleton attention mechanism to capture broader relationships across fixed partitions with minimal parameters and flops.","Furthermore, the proposed Dual Attention Normalizing Flow (DA-Flow) integrates the DAM as a post-processing unit after GCN within the normalizing flow framework.","Simulations show that the proposed model is robust against noise and negative samples.","Experimental results show that DA-Flow reaches competitive or better performance than the existing state-of-the-art (SOTA) methods in terms of the micro AUC metric with the fewest number of parameters.","Moreover, we found that even without training, simply using random projection without dimensionality reduction on skeleton data enables substantial anomaly detection capabilities."],"url":"http://arxiv.org/abs/2406.02976v1","category":"cs.CV"}
{"created":"2024-06-05 06:16:04","title":"A Shared-Aperture Dual-Band sub-6 GHz and mmWave Reconfigurable Intelligent Surface With Independent Operation","abstract":"A novel dual-band reconfigurable intelligent surface (DBI-RIS) design that combines the functionalities of millimeter-wave (mmWave) and sub-6 GHz bands within a single aperture is proposed. This design aims to bridge the gap between current single-band reconfigurable intelligent surfaces (RISs) and wireless systems utilizing sub-6 GHz and mmWave bands that require RIS with independently reconfigurable dual-band operation. The mmWave element is realized by a double-layer patch antenna loaded with 1-bit phase shifters, providing two reconfigurable states. An 8x8 mmWave element array is selectively interconnected using three RF switches to form a reconfigurable sub-6 GHz element at 3.5 GHz. A suspended electromagnetic band gap (EBG) structure is proposed to suppress surface waves and ensure sufficient geometric space for the phase shifter and control networks in the mmWave element. A low-cost planar spiral inductor (PSI) is carefully optimized to connect mmWave elements, enabling the sub-6 GHz function without affecting mmWave operation. Finally, prototypes of the DBI-RIS are fabricated, and experimental verification is conducted using two separate measurement testbeds. The fabricated sub-6 GHz RIS successfully achieves beam steering within the range of -35 to 35 degrees for DBI-RIS with 4x4 sub-6 GHz elements, while the mmWave RIS demonstrates beam steering between -30 to 30 degrees for DBI-RIS with 8x8 mmWave elements, and have good agreement with simulation results.","sentences":["A novel dual-band reconfigurable intelligent surface (DBI-RIS) design that combines the functionalities of millimeter-wave (mmWave) and sub-6 GHz bands within a single aperture is proposed.","This design aims to bridge the gap between current single-band reconfigurable intelligent surfaces (RISs) and wireless systems utilizing sub-6 GHz and mmWave bands that require RIS with independently reconfigurable dual-band operation.","The mmWave element is realized by a double-layer patch antenna loaded with 1-bit phase shifters, providing two reconfigurable states.","An 8x8 mmWave element array is selectively interconnected using three RF switches to form a reconfigurable sub-6 GHz element at 3.5 GHz.","A suspended electromagnetic band gap (EBG) structure is proposed to suppress surface waves and ensure sufficient geometric space for the phase shifter and control networks in the mmWave element.","A low-cost planar spiral inductor (PSI) is carefully optimized to connect mmWave elements, enabling the sub-6 GHz function without affecting mmWave operation.","Finally, prototypes of the DBI-RIS are fabricated, and experimental verification is conducted using two separate measurement testbeds.","The fabricated sub-6 GHz RIS successfully achieves beam steering within the range of -35 to 35 degrees for DBI-RIS with 4x4 sub-6 GHz elements, while the mmWave RIS demonstrates beam steering between -30 to 30 degrees for DBI-RIS with 8x8 mmWave elements, and have good agreement with simulation results."],"url":"http://arxiv.org/abs/2406.02975v1","category":"eess.SP"}
{"created":"2024-06-05 05:53:50","title":"Filtered not Mixed: Stochastic Filtering-Based Online Gating for Mixture of Large Language Models","abstract":"We propose MoE-F -- a formalised mechanism for combining $N$ pre-trained expert Large Language Models (LLMs) in online time-series prediction tasks by adaptively forecasting the best weighting of LLM predictions at every time step. Our mechanism leverages the conditional information in each expert's running performance to forecast the best combination of LLMs for predicting the time series in its next step. Diverging from static (learned) Mixture of Experts (MoE) methods, MoE-F employs time-adaptive stochastic filtering techniques to combine experts. By framing the expert selection problem as a finite state-space, continuous-time Hidden Markov model (HMM), we can leverage the Wohman-Shiryaev filter. Our approach first constructs $N$ parallel filters corresponding to each of the $N$ individual LLMs. Each filter proposes its best combination of LLMs, given the information that they have access to. Subsequently, the $N$ filter outputs are aggregated to optimize a lower bound for the loss of the aggregated LLMs, which can be optimized in closed-form, thus generating our ensemble predictor. Our contributions here are: (I) the MoE-F algorithm -- deployable as a plug-and-play filtering harness, (II) theoretical optimality guarantees of the proposed filtering-based gating algorithm, and (III) empirical evaluation and ablative results using state of the art foundational and MoE LLMs on a real-world Financial Market Movement task where MoE-F attains a remarkable 17% absolute and 48.5% relative F1 measure improvement over the next best performing individual LLM expert.","sentences":["We propose MoE-F -- a formalised mechanism for combining $N$ pre-trained expert Large Language Models (LLMs) in online time-series prediction tasks by adaptively forecasting the best weighting of LLM predictions at every time step.","Our mechanism leverages the conditional information in each expert's running performance to forecast the best combination of LLMs for predicting the time series in its next step.","Diverging from static (learned) Mixture of Experts (MoE) methods, MoE-F employs time-adaptive stochastic filtering techniques to combine experts.","By framing the expert selection problem as a finite state-space, continuous-time Hidden Markov model (HMM), we can leverage the Wohman-Shiryaev filter.","Our approach first constructs $N$ parallel filters corresponding to each of the $N$ individual LLMs.","Each filter proposes its best combination of LLMs, given the information that they have access to.","Subsequently, the $N$ filter outputs are aggregated to optimize a lower bound for the loss of the aggregated LLMs, which can be optimized in closed-form, thus generating our ensemble predictor.","Our contributions here are: (I) the MoE-F algorithm -- deployable as a plug-and-play filtering harness, (II) theoretical optimality guarantees of the proposed filtering-based gating algorithm, and (III) empirical evaluation and ablative results using state of the art foundational and MoE LLMs on a real-world Financial Market Movement task where MoE-F attains a remarkable 17% absolute and 48.5% relative F1 measure improvement over the next best performing individual LLM expert."],"url":"http://arxiv.org/abs/2406.02969v1","category":"cs.LG"}
{"created":"2024-06-05 05:43:55","title":"Generative AI and Digital Neocolonialism in Global Education: Towards an Equitable Framework","abstract":"This paper critically discusses how Generative Artificial Intelligence (GenAI) might impose Western ideologies on non-Western societies, perpetuating digital neocolonialism in education through its inherent biases and further suggests strategies to mitigate these effects. Our discussions demonstrated that GenAI can foster cultural imperialism by generating content that primarily incorporates cultural references and examples relevant to Western students, thereby alienating students from non-Western backgrounds. Also, the predominant use of Western languages by GenAI can marginalize non-dominant languages, making educational content less accessible to speakers of indigenous languages and potentially impacting their ability to learn in their first language. Additionally, GenAI often generates content and curricula that reflect the perspectives of technologically dominant countries, overshadowing marginalized indigenous knowledge and practices. Moreover, the cost of access to GenAI intensifies educational inequality and the control of GenAI data could lead to commercial exploitation without benefiting local students and their communities. We propose human-centric reforms to prioritize cultural diversity and equity in GenAI development; a liberatory design to empower educators and students to identify and dismantle the oppressive structures within GenAI applications; foresight by design to create an adjustable GenAI systems to meet future educational needs, and finally, effective prompting skills to reduces the retrieval of neocolonial outputs.","sentences":["This paper critically discusses how Generative Artificial Intelligence (GenAI) might impose Western ideologies on non-Western societies, perpetuating digital neocolonialism in education through its inherent biases and further suggests strategies to mitigate these effects.","Our discussions demonstrated that GenAI can foster cultural imperialism by generating content that primarily incorporates cultural references and examples relevant to Western students, thereby alienating students from non-Western backgrounds.","Also, the predominant use of Western languages by GenAI can marginalize non-dominant languages, making educational content less accessible to speakers of indigenous languages and potentially impacting their ability to learn in their first language.","Additionally, GenAI often generates content and curricula that reflect the perspectives of technologically dominant countries, overshadowing marginalized indigenous knowledge and practices.","Moreover, the cost of access to GenAI intensifies educational inequality and the control of GenAI data could lead to commercial exploitation without benefiting local students and their communities.","We propose human-centric reforms to prioritize cultural diversity and equity in GenAI development; a liberatory design to empower educators and students to identify and dismantle the oppressive structures within GenAI applications; foresight by design to create an adjustable GenAI systems to meet future educational needs, and finally, effective prompting skills to reduces the retrieval of neocolonial outputs."],"url":"http://arxiv.org/abs/2406.02966v1","category":"cs.CY"}
{"created":"2024-06-05 05:35:59","title":"Docs2KG: Unified Knowledge Graph Construction from Heterogeneous Documents Assisted by Large Language Models","abstract":"Even for a conservative estimate, 80% of enterprise data reside in unstructured files, stored in data lakes that accommodate heterogeneous formats. Classical search engines can no longer meet information seeking needs, especially when the task is to browse and explore for insight formulation. In other words, there are no obvious search keywords to use. Knowledge graphs, due to their natural visual appeals that reduce the human cognitive load, become the winning candidate for heterogeneous data integration and knowledge representation.   In this paper, we introduce Docs2KG, a novel framework designed to extract multimodal information from diverse and heterogeneous unstructured documents, including emails, web pages, PDF files, and Excel files. Dynamically generates a unified knowledge graph that represents the extracted key information, Docs2KG enables efficient querying and exploration of document data lakes. Unlike existing approaches that focus on domain-specific data sources or pre-designed schemas, Docs2KG offers a flexible and extensible solution that can adapt to various document structures and content types. The proposed framework unifies data processing supporting a multitude of downstream tasks with improved domain interpretability. Docs2KG is publicly accessible at https://docs2kg.ai4wa.com, and a demonstration video is available at https://docs2kg.ai4wa.com/Video.","sentences":["Even for a conservative estimate, 80% of enterprise data reside in unstructured files, stored in data lakes that accommodate heterogeneous formats.","Classical search engines can no longer meet information seeking needs, especially when the task is to browse and explore for insight formulation.","In other words, there are no obvious search keywords to use.","Knowledge graphs, due to their natural visual appeals that reduce the human cognitive load, become the winning candidate for heterogeneous data integration and knowledge representation.   ","In this paper, we introduce Docs2KG, a novel framework designed to extract multimodal information from diverse and heterogeneous unstructured documents, including emails, web pages, PDF files, and Excel files.","Dynamically generates a unified knowledge graph that represents the extracted key information, Docs2KG enables efficient querying and exploration of document data lakes.","Unlike existing approaches that focus on domain-specific data sources or pre-designed schemas, Docs2KG offers a flexible and extensible solution that can adapt to various document structures and content types.","The proposed framework unifies data processing supporting a multitude of downstream tasks with improved domain interpretability.","Docs2KG is publicly accessible at https://docs2kg.ai4wa.com, and a demonstration video is available at https://docs2kg.ai4wa.com/Video."],"url":"http://arxiv.org/abs/2406.02962v1","category":"cs.CL"}
{"created":"2024-06-05 05:27:02","title":"PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs","abstract":"On-device training is currently the most common approach for training machine learning (ML) models on private, distributed user data. Despite this, on-device training has several drawbacks: (1) most user devices are too small to train large models on-device, (2) on-device training is communication- and computation-intensive, and (3) on-device training can be difficult to debug and deploy. To address these problems, we propose Private Evolution-Text (PrE-Text), a method for generating differentially private (DP) synthetic textual data. First, we show that across multiple datasets, training small models (models that fit on user devices) with PrE-Text synthetic data outperforms small models trained on-device under practical privacy regimes ($\\epsilon=1.29$, $\\epsilon=7.58$). We achieve these results while using 9$\\times$ fewer rounds, 6$\\times$ less client computation per round, and 100$\\times$ less communication per round. Second, finetuning large models on PrE-Text's DP synthetic data improves large language model (LLM) performance on private data across the same range of privacy budgets. Altogether, these results suggest that training on DP synthetic data can be a better option than training a model on-device on private distributed data. Code is available at https://github.com/houcharlie/PrE-Text.","sentences":["On-device training is currently the most common approach for training machine learning (ML) models on private, distributed user data.","Despite this, on-device training has several drawbacks: (1) most user devices are too small to train large models on-device, (2) on-device training is communication- and computation-intensive, and (3) on-device training can be difficult to debug and deploy.","To address these problems, we propose Private Evolution-Text (PrE-Text), a method for generating differentially private (DP) synthetic textual data.","First, we show that across multiple datasets, training small models (models that fit on user devices) with PrE-Text synthetic data outperforms small models trained on-device under practical privacy regimes ($\\epsilon=1.29$, $\\epsilon=7.58$).","We achieve these results while using 9$\\times$ fewer rounds, 6$\\times$ less client computation per round, and 100$\\times$ less communication per round.","Second, finetuning large models on PrE-Text's DP synthetic data improves large language model (LLM) performance on private data across the same range of privacy budgets.","Altogether, these results suggest that training on DP synthetic data can be a better option than training a model on-device on private distributed data.","Code is available at https://github.com/houcharlie/PrE-Text."],"url":"http://arxiv.org/abs/2406.02958v1","category":"cs.LG"}
{"created":"2024-06-05 05:22:32","title":"GraphAlign: Pretraining One Graph Neural Network on Multiple Graphs via Feature Alignment","abstract":"Graph self-supervised learning (SSL) holds considerable promise for mining and learning with graph-structured data. Yet, a significant challenge in graph SSL lies in the feature discrepancy among graphs across different domains. In this work, we aim to pretrain one graph neural network (GNN) on a varied collection of graphs endowed with rich node features and subsequently apply the pretrained GNN to unseen graphs. We present a general GraphAlign method that can be seamlessly integrated into the existing graph SSL framework. To align feature distributions across disparate graphs, GraphAlign designs alignment strategies of feature encoding, normalization, alongside a mixture-of-feature-expert module. Extensive experiments show that GraphAlign empowers existing graph SSL frameworks to pretrain a unified and powerful GNN across multiple graphs, showcasing performance superiority on both in-domain and out-of-domain graphs.","sentences":["Graph self-supervised learning (SSL) holds considerable promise for mining and learning with graph-structured data.","Yet, a significant challenge in graph SSL lies in the feature discrepancy among graphs across different domains.","In this work, we aim to pretrain one graph neural network (GNN) on a varied collection of graphs endowed with rich node features and subsequently apply the pretrained GNN to unseen graphs.","We present a general GraphAlign method that can be seamlessly integrated into the existing graph SSL framework.","To align feature distributions across disparate graphs, GraphAlign designs alignment strategies of feature encoding, normalization, alongside a mixture-of-feature-expert module.","Extensive experiments show that GraphAlign empowers existing graph SSL frameworks to pretrain a unified and powerful GNN across multiple graphs, showcasing performance superiority on both in-domain and out-of-domain graphs."],"url":"http://arxiv.org/abs/2406.02953v1","category":"cs.LG"}
{"created":"2024-06-05 05:05:41","title":"The Task-oriented Queries Benchmark (ToQB)","abstract":"Task-oriented queries (e.g., one-shot queries to play videos, order food, or call a taxi) are crucial for assessing the quality of virtual assistants, chatbots, and other large language model (LLM)-based services. However, a standard benchmark for task-oriented queries is not yet available, as existing benchmarks in the relevant NLP (Natural Language Processing) fields have primarily focused on task-oriented dialogues. Thus, we present a new methodology for efficiently generating the Task-oriented Queries Benchmark (ToQB) using existing task-oriented dialogue datasets and an LLM service. Our methodology involves formulating the underlying NLP task to summarize the original intent of a speaker in each dialogue, detailing the key steps to perform the devised NLP task using an LLM service, and outlining a framework for automating a major part of the benchmark generation process. Through a case study encompassing three domains (i.e., two single-task domains and one multi-task domain), we demonstrate how to customize the LLM prompts (e.g., omitting system utterances or speaker labels) for those three domains and characterize the generated task-oriented queries. The generated ToQB dataset is made available to the public. We further discuss new domains that can be added to ToQB by community contributors and its practical applications.","sentences":["Task-oriented queries (e.g., one-shot queries to play videos, order food, or call a taxi) are crucial for assessing the quality of virtual assistants, chatbots, and other large language model (LLM)-based services.","However, a standard benchmark for task-oriented queries is not yet available, as existing benchmarks in the relevant NLP (Natural Language Processing) fields have primarily focused on task-oriented dialogues.","Thus, we present a new methodology for efficiently generating the Task-oriented Queries Benchmark (ToQB) using existing task-oriented dialogue datasets and an LLM service.","Our methodology involves formulating the underlying NLP task to summarize the original intent of a speaker in each dialogue, detailing the key steps to perform the devised NLP task using an LLM service, and outlining a framework for automating a major part of the benchmark generation process.","Through a case study encompassing three domains (i.e., two single-task domains and one multi-task domain), we demonstrate how to customize the LLM prompts (e.g., omitting system utterances or speaker labels) for those three domains and characterize the generated task-oriented queries.","The generated ToQB dataset is made available to the public.","We further discuss new domains that can be added to ToQB by community contributors and its practical applications."],"url":"http://arxiv.org/abs/2406.02943v1","category":"cs.IR"}
{"created":"2024-06-05 04:47:15","title":"TOP detector for particle identification at Belle II","abstract":"The Time-Of-Propagation (TOP) counter is a ring-imaging Cherenkov detector designed to identify the charged hadrons in the barrel region of the Belle II detector. The Belle II experiment collected data delivered by the SuperKEKB accelerator from March 2019 to June 2022. After the first run period (Run1) a long shutdown (LS1) was dedicated to implement several accelerator and detector upgrades. The most important upgrade was the completion of the vertex detector. The second collision period (Run2), started in January 2024. The components of the TOP detector and the performance during the Run1 will be reported, the detector upgrade during LS1 and the future upgrades will be described.","sentences":["The Time-Of-Propagation (TOP) counter is a ring-imaging Cherenkov detector designed to identify the charged hadrons in the barrel region of the Belle II detector.","The Belle II experiment collected data delivered by the SuperKEKB accelerator from March 2019 to June 2022.","After the first run period (Run1) a long shutdown (LS1) was dedicated to implement several accelerator and detector upgrades.","The most important upgrade was the completion of the vertex detector.","The second collision period (Run2), started in January 2024.","The components of the TOP detector and the performance during the Run1 will be reported, the detector upgrade during LS1 and the future upgrades will be described."],"url":"http://arxiv.org/abs/2406.02935v1","category":"physics.ins-det"}
{"created":"2024-06-05 04:47:13","title":"Estimating Disease-Free Life Expectancy based on Clinical Data from the French Hospital Discharge Database","abstract":"The development of health indicators to measure healthy life expectancy (HLE) is an active field of research aimed at summarizing the health of a population. Although many health indicators have emerged in the literature as critical metrics in public health assessments, the methods and data to conduct this evaluation vary considerably in nature and quality. Traditionally, health data collection relies on population surveys. However, these studies, typically of limited size, encompass only a small yet representative segment of the population. This limitation can necessitate the separate estimation of incidence and mortality rates, significantly restricting the available analysis methods. In this article, we leverage an extract from the French National Hospital Discharge database to define health indicators. Our analysis focuses on the resulting Disease-Free Life Expectancy (Dis-FLE) indicator, which provides insights based on the hospital trajectory of each patient admitted to hospital in France during 2008-13. Through this research, we illustrate the advantages and disadvantages of employing large clinical datasets as the foundation for more robust health indicators. We shed light on the opportunities that such data offer for a more comprehensive understanding of the health status of a population. In particular, we estimate age-dependent hazard rates associated with sex, alcohol abuse, tobacco consumption, and obesity, as well as geographic location. Simultaneously, we delve into the challenges and limitations that arise when adopting such a data-driven approach.","sentences":["The development of health indicators to measure healthy life expectancy (HLE) is an active field of research aimed at summarizing the health of a population.","Although many health indicators have emerged in the literature as critical metrics in public health assessments, the methods and data to conduct this evaluation vary considerably in nature and quality.","Traditionally, health data collection relies on population surveys.","However, these studies, typically of limited size, encompass only a small yet representative segment of the population.","This limitation can necessitate the separate estimation of incidence and mortality rates, significantly restricting the available analysis methods.","In this article, we leverage an extract from the French National Hospital Discharge database to define health indicators.","Our analysis focuses on the resulting Disease-Free Life Expectancy (Dis-FLE) indicator, which provides insights based on the hospital trajectory of each patient admitted to hospital in France during 2008-13.","Through this research, we illustrate the advantages and disadvantages of employing large clinical datasets as the foundation for more robust health indicators.","We shed light on the opportunities that such data offer for a more comprehensive understanding of the health status of a population.","In particular, we estimate age-dependent hazard rates associated with sex, alcohol abuse, tobacco consumption, and obesity, as well as geographic location.","Simultaneously, we delve into the challenges and limitations that arise when adopting such a data-driven approach."],"url":"http://arxiv.org/abs/2406.02934v1","category":"stat.AP"}
{"created":"2024-06-05 04:28:57","title":"Multivariate Physics-Informed Convolutional Autoencoder for Anomaly Detection in Power Distribution Systems with High Penetration of DERs","abstract":"Despite the relentless progress of deep learning models in analyzing the system conditions under cyber-physical events, their abilities are limited in the power system domain due to data availability issues, cost of data acquisition, and lack of interpretation and extrapolation for the data beyond the training windows. In addition, the integration of distributed energy resources (DERs) such as wind and solar generations increases the complexities and nonlinear nature of power systems. Therefore, an interpretable and reliable methodology is of utmost need to increase the confidence of power system operators and their situational awareness for making reliable decisions. This has led to the development of physics-informed neural network (PINN) models as more interpretable, trustworthy, and robust models where the underlying principled laws are integrated into the training process of neural network models to achieve improved performance. This paper proposes a multivariate physics-informed convolutional autoencoder (PIConvAE) model to detect cyber anomalies in power distribution systems with unbalanced configurations and high penetration of DERs. The physical laws are integrated through a customized loss function that embeds the underlying Kirchhoff's circuit laws into the training process of the autoencoder. The performance of the multivariate PIConvAE model is evaluated on two unbalanced power distribution grids, IEEE 123-bus system and a real-world feeder in Riverside, CA. The results show the exceptional performance of the proposed method in detecting various cyber anomalies in both systems. In addition, the model's effectiveness is evaluated in data scarcity scenarios with different training data ratios. Finally, the model's performance is compared with existing machine learning models where the PIConvAE model surpasses other models with considerably higher detection metrics.","sentences":["Despite the relentless progress of deep learning models in analyzing the system conditions under cyber-physical events, their abilities are limited in the power system domain due to data availability issues, cost of data acquisition, and lack of interpretation and extrapolation for the data beyond the training windows.","In addition, the integration of distributed energy resources (DERs) such as wind and solar generations increases the complexities and nonlinear nature of power systems.","Therefore, an interpretable and reliable methodology is of utmost need to increase the confidence of power system operators and their situational awareness for making reliable decisions.","This has led to the development of physics-informed neural network (PINN) models as more interpretable, trustworthy, and robust models where the underlying principled laws are integrated into the training process of neural network models to achieve improved performance.","This paper proposes a multivariate physics-informed convolutional autoencoder (PIConvAE) model to detect cyber anomalies in power distribution systems with unbalanced configurations and high penetration of DERs.","The physical laws are integrated through a customized loss function that embeds the underlying Kirchhoff's circuit laws into the training process of the autoencoder.","The performance of the multivariate PIConvAE model is evaluated on two unbalanced power distribution grids, IEEE 123-bus system and a real-world feeder in Riverside, CA.","The results show the exceptional performance of the proposed method in detecting various cyber anomalies in both systems.","In addition, the model's effectiveness is evaluated in data scarcity scenarios with different training data ratios.","Finally, the model's performance is compared with existing machine learning models where the PIConvAE model surpasses other models with considerably higher detection metrics."],"url":"http://arxiv.org/abs/2406.02927v1","category":"eess.SY"}
{"created":"2024-06-05 04:25:56","title":"SYN2REAL: Leveraging Task Arithmetic for Mitigating Synthetic-Real Discrepancies in ASR Domain Adaptation","abstract":"Recent advancements in large language models (LLMs) have introduced the 'task vector' concept, which has significantly impacted various domains but remains underexplored in speech recognition. This paper presents a novel 'SYN2REAL' task vector for domain adaptation in automatic speech recognition (ASR), specifically targeting text-only domains. Traditional fine-tuning on synthetic speech often results in performance degradation due to acoustic mismatches. To address this issue, we propose creating a 'SYN2REAL' vector by subtracting the parameter differences between models fine-tuned on real and synthetic speech. This vector effectively bridges the gap between the two domains. Experiments on the SLURP dataset demonstrate that our approach yields an average improvement of 11.15% in word error rate for unseen target domains, highlighting the potential of task vectors in enhancing speech domain adaptation.","sentences":["Recent advancements in large language models (LLMs) have introduced the 'task vector' concept, which has significantly impacted various domains but remains underexplored in speech recognition.","This paper presents a novel 'SYN2REAL' task vector for domain adaptation in automatic speech recognition (ASR), specifically targeting text-only domains.","Traditional fine-tuning on synthetic speech often results in performance degradation due to acoustic mismatches.","To address this issue, we propose creating a 'SYN2REAL' vector by subtracting the parameter differences between models fine-tuned on real and synthetic speech.","This vector effectively bridges the gap between the two domains.","Experiments on the SLURP dataset demonstrate that our approach yields an average improvement of 11.15% in word error rate for unseen target domains, highlighting the potential of task vectors in enhancing speech domain adaptation."],"url":"http://arxiv.org/abs/2406.02925v1","category":"eess.AS"}
{"created":"2024-06-05 04:20:17","title":"Text Injection for Neural Contextual Biasing","abstract":"Neural contextual biasing effectively improves automatic speech recognition (ASR) for crucial phrases within a speaker's context, particularly those that are infrequent in the training data. This work proposes contextual text injection (CTI) to enhance contextual ASR. CTI leverages not only the paired speech-text data, but also a much larger corpus of unpaired text to optimize the ASR model and its biasing component. Unpaired text is converted into speech-like representations and used to guide the model's attention towards relevant bias phrases. Moreover, we introduce a contextual text-injected (CTI) minimum word error rate (MWER) training, which minimizes the expected WER caused by contextual biasing when unpaired text is injected into the model. Experiments show that CTI with 100 billion text sentences can achieve up to 43.3% relative WER reduction from a strong neural biasing model. CTI-MWER provides a further relative improvement of 23.5%.","sentences":["Neural contextual biasing effectively improves automatic speech recognition (ASR) for crucial phrases within a speaker's context, particularly those that are infrequent in the training data.","This work proposes contextual text injection (CTI) to enhance contextual ASR.","CTI leverages not only the paired speech-text data, but also a much larger corpus of unpaired text to optimize the ASR model and its biasing component.","Unpaired text is converted into speech-like representations and used to guide the model's attention towards relevant bias phrases.","Moreover, we introduce a contextual text-injected (CTI) minimum word error rate (MWER) training, which minimizes the expected WER caused by contextual biasing when unpaired text is injected into the model.","Experiments show that CTI with 100 billion text sentences can achieve up to 43.3% relative WER reduction from a strong neural biasing model.","CTI-MWER provides a further relative improvement of 23.5%."],"url":"http://arxiv.org/abs/2406.02921v1","category":"cs.CL"}
{"created":"2024-06-05 04:07:35","title":"Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity","abstract":"Zeroth-order optimization (ZO) is a memory-efficient strategy for fine-tuning Large Language Models using only forward passes. However, the application of ZO fine-tuning in memory-constrained settings such as mobile phones and laptops is still challenging since full precision forward passes are infeasible. In this study, we address this limitation by integrating sparsity and quantization into ZO fine-tuning of LLMs. Specifically, we investigate the feasibility of fine-tuning an extremely small subset of LLM parameters using ZO. This approach allows the majority of un-tuned parameters to be quantized to accommodate the constraint of limited device memory. Our findings reveal that the pre-training process can identify a set of \"sensitive parameters\" that can guide the ZO fine-tuning of LLMs on downstream tasks. Our results demonstrate that fine-tuning 0.1% sensitive parameters in the LLM with ZO can outperform the full ZO fine-tuning performance, while offering wall-clock time speedup. Additionally, we show that ZO fine-tuning targeting these 0.1% sensitive parameters, combined with 4 bit quantization, enables efficient ZO fine-tuning of an Llama2-7B model on a GPU device with less than 8 GiB of memory and notably reduced latency.","sentences":["Zeroth-order optimization (ZO) is a memory-efficient strategy for fine-tuning Large Language Models using only forward passes.","However, the application of ZO fine-tuning in memory-constrained settings such as mobile phones and laptops is still challenging since full precision forward passes are infeasible.","In this study, we address this limitation by integrating sparsity and quantization into ZO fine-tuning of LLMs.","Specifically, we investigate the feasibility of fine-tuning an extremely small subset of LLM parameters using ZO.","This approach allows the majority of un-tuned parameters to be quantized to accommodate the constraint of limited device memory.","Our findings reveal that the pre-training process can identify a set of \"sensitive parameters\" that can guide the ZO fine-tuning of LLMs on downstream tasks.","Our results demonstrate that fine-tuning 0.1% sensitive parameters in the LLM with ZO can outperform the full ZO fine-tuning performance, while offering wall-clock time speedup.","Additionally, we show that ZO fine-tuning targeting these 0.1% sensitive parameters, combined with 4 bit quantization, enables efficient ZO fine-tuning of an Llama2-7B model on a GPU device with less than 8 GiB of memory and notably reduced latency."],"url":"http://arxiv.org/abs/2406.02913v1","category":"cs.LG"}
{"created":"2024-06-05 03:41:37","title":"Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms","abstract":"Reinforcement Learning from Human Feedback (RLHF) has been crucial to the recent success of Large Language Models (LLMs), however, it is often a complex and brittle process. In the classical RLHF framework, a reward model is first trained to represent human preferences, which is in turn used by an online reinforcement learning (RL) algorithm to optimize the LLM. A prominent issue with such methods is \\emph{reward over-optimization} or \\emph{reward hacking}, where performance as measured by the learned proxy reward model increases, but true quality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs) like Direct Preference Optimization have emerged as alternatives to the classical RLHF pipeline by circumventing the reward modeling phase. However, although DAAs do not use a separate proxy reward model, they still commonly deteriorate from over-optimization. While the so-called reward hacking phenomenon is not well-defined for DAAs, we still uncover similar trends: at higher KL budgets, DAA algorithms exhibit similar degradation patterns to their classic RLHF counterparts. In particular, we find that DAA methods deteriorate not only across a wide range of KL budgets but also often before even a single epoch of the dataset is completed. Through extensive empirical experimentation, this work formulates and formalizes the reward over-optimization or hacking problem for DAAs and explores its consequences across objectives, training regimes, and model scales.","sentences":["Reinforcement Learning from Human Feedback (RLHF) has been crucial to the recent success of Large Language Models (LLMs), however, it is often a complex and brittle process.","In the classical RLHF framework, a reward model is first trained to represent human preferences, which is in turn used by an online reinforcement learning (RL) algorithm to optimize the LLM.","A prominent issue with such methods is \\emph{reward over-optimization} or \\emph{reward hacking}, where performance as measured by the learned proxy reward model increases, but true quality plateaus or even deteriorates.","Direct Alignment Algorithms (DDAs) like Direct Preference Optimization have emerged as alternatives to the classical RLHF pipeline by circumventing the reward modeling phase.","However, although DAAs do not use a separate proxy reward model, they still commonly deteriorate from over-optimization.","While the so-called reward hacking phenomenon is not well-defined for DAAs, we still uncover similar trends: at higher KL budgets, DAA algorithms exhibit similar degradation patterns to their classic RLHF counterparts.","In particular, we find that DAA methods deteriorate not only across a wide range of KL budgets but also often before even a single epoch of the dataset is completed.","Through extensive empirical experimentation, this work formulates and formalizes the reward over-optimization or hacking problem for DAAs and explores its consequences across objectives, training regimes, and model scales."],"url":"http://arxiv.org/abs/2406.02900v1","category":"cs.LG"}
{"created":"2024-06-05 03:36:38","title":"Location-Driven Beamforming for RIS-Assisted Near-Field Communications","abstract":"Future wireless communications are promising to support ubiquitous connections and high data rates with cost-effective devices. Benefiting from the energy-efficient elements with low cost, reconfigurable intelligent surface (RIS) emerges as a potential solution to fulfill such demands, which has the capability to flexibly manipulate the wireless signals with a tunable phase. Recently, as the operational frequency ascends to the sub-terahertz (THz) bands or higher bands for wireless communications in six-generation (6G), it becomes imperative to consider the near-field propagation in RIS-assisted communications. The challenging acquisition of channel parameters is an inherent issue for near-field RIS-assisted communications, where the complex design is essential to acquire the informative near-field channel embedded with both the angle and distance information. Hence, in this paper we systematically investigate the potential of exploiting location information for near-field RIS-assisted communications. Firstly, we present the progresses in the near-field RIS-assisted communications, which are compatible with existing wireless communications and show the potential to achieve the fine-grained localization accuracy to support location-driven scheme. Then, the Fresnel zone based model is introduced, with which the location-driven beamforming scheme and corresponding frame structure are developed. Also, we elaborate on four unique advantages for leveraging location information in RIS-assisted communications, followed by numerical simulations. Finally, several key challenges and corresponding potential solutions are pointed out.","sentences":["Future wireless communications are promising to support ubiquitous connections and high data rates with cost-effective devices.","Benefiting from the energy-efficient elements with low cost, reconfigurable intelligent surface (RIS) emerges as a potential solution to fulfill such demands, which has the capability to flexibly manipulate the wireless signals with a tunable phase.","Recently, as the operational frequency ascends to the sub-terahertz (THz) bands or higher bands for wireless communications in six-generation (6G), it becomes imperative to consider the near-field propagation in RIS-assisted communications.","The challenging acquisition of channel parameters is an inherent issue for near-field RIS-assisted communications, where the complex design is essential to acquire the informative near-field channel embedded with both the angle and distance information.","Hence, in this paper we systematically investigate the potential of exploiting location information for near-field RIS-assisted communications.","Firstly, we present the progresses in the near-field RIS-assisted communications, which are compatible with existing wireless communications and show the potential to achieve the fine-grained localization accuracy to support location-driven scheme.","Then, the Fresnel zone based model is introduced, with which the location-driven beamforming scheme and corresponding frame structure are developed.","Also, we elaborate on four unique advantages for leveraging location information in RIS-assisted communications, followed by numerical simulations.","Finally, several key challenges and corresponding potential solutions are pointed out."],"url":"http://arxiv.org/abs/2406.02898v1","category":"cs.IT"}
{"created":"2024-06-05 03:11:44","title":"Representation Learning For Efficient Deep Multi-Agent Reinforcement Learning","abstract":"Sample efficiency remains a key challenge in multi-agent reinforcement learning (MARL). A promising approach is to learn a meaningful latent representation space through auxiliary learning objectives alongside the MARL objective to aid in learning a successful control policy. In our work, we present MAPO-LSO (Multi-Agent Policy Optimization with Latent Space Optimization) which applies a form of comprehensive representation learning devised to supplement MARL training. Specifically, MAPO-LSO proposes a multi-agent extension of transition dynamics reconstruction and self-predictive learning that constructs a latent state optimization scheme that can be trivially extended to current state-of-the-art MARL algorithms. Empirical results demonstrate MAPO-LSO to show notable improvements in sample efficiency and learning performance compared to its vanilla MARL counterpart without any additional MARL hyperparameter tuning on a diverse suite of MARL tasks.","sentences":["Sample efficiency remains a key challenge in multi-agent reinforcement learning (MARL).","A promising approach is to learn a meaningful latent representation space through auxiliary learning objectives alongside the MARL objective to aid in learning a successful control policy.","In our work, we present MAPO-LSO (Multi-Agent Policy Optimization with Latent Space Optimization) which applies a form of comprehensive representation learning devised to supplement MARL training.","Specifically, MAPO-LSO proposes a multi-agent extension of transition dynamics reconstruction and self-predictive learning that constructs a latent state optimization scheme that can be trivially extended to current state-of-the-art MARL algorithms.","Empirical results demonstrate MAPO-LSO to show notable improvements in sample efficiency and learning performance compared to its vanilla MARL counterpart without any additional MARL hyperparameter tuning on a diverse suite of MARL tasks."],"url":"http://arxiv.org/abs/2406.02890v1","category":"cs.MA"}
{"created":"2024-06-05 03:08:46","title":"HYDRA: Model Factorization Framework for Black-Box LLM Personalization","abstract":"Personalization has emerged as a critical research area in modern intelligent systems, focusing on mining users' behavioral history and adapting to their preferences for delivering tailored experiences. Despite the remarkable few-shot capabilities exhibited by black-box large language models (LLMs), the inherent opacity of their model parameters presents significant challenges in aligning the generated output with individual expectations. Existing solutions have primarily focused on prompt design to incorporate user-specific profiles and behaviors; however, such approaches often struggle to generalize effectively due to their inability to capture shared knowledge among all users. To address these challenges, we propose HYDRA, a model factorization framework that captures both user-specific behavior patterns from historical data and shared general knowledge among all users to deliver personalized generation. In order to capture user-specific behavior patterns, we first train a reranker to prioritize the most useful information from top-retrieved relevant historical records. By combining the prioritized history with the corresponding query, we train an adapter to align the output with individual user-specific preferences, eliminating the reliance on access to inherent model parameters of black-box LLMs. Both the reranker and the adapter can be decomposed into a base model with multiple user-specific heads, resembling a hydra. The base model maintains shared knowledge across users, while the multiple personal heads capture user-specific preferences. Experimental results demonstrate that HYDRA outperforms existing state-of-the-art prompt-based methods by an average relative improvement of 9.01% across five diverse personalization tasks in the LaMP benchmark. Our implementation is available at https://github.com/night-chen/HYDRA.","sentences":["Personalization has emerged as a critical research area in modern intelligent systems, focusing on mining users' behavioral history and adapting to their preferences for delivering tailored experiences.","Despite the remarkable few-shot capabilities exhibited by black-box large language models (LLMs), the inherent opacity of their model parameters presents significant challenges in aligning the generated output with individual expectations.","Existing solutions have primarily focused on prompt design to incorporate user-specific profiles and behaviors; however, such approaches often struggle to generalize effectively due to their inability to capture shared knowledge among all users.","To address these challenges, we propose HYDRA, a model factorization framework that captures both user-specific behavior patterns from historical data and shared general knowledge among all users to deliver personalized generation.","In order to capture user-specific behavior patterns, we first train a reranker to prioritize the most useful information from top-retrieved relevant historical records.","By combining the prioritized history with the corresponding query, we train an adapter to align the output with individual user-specific preferences, eliminating the reliance on access to inherent model parameters of black-box LLMs.","Both the reranker and the adapter can be decomposed into a base model with multiple user-specific heads, resembling a hydra.","The base model maintains shared knowledge across users, while the multiple personal heads capture user-specific preferences.","Experimental results demonstrate that HYDRA outperforms existing state-of-the-art prompt-based methods by an average relative improvement of 9.01% across five diverse personalization tasks in the LaMP benchmark.","Our implementation is available at https://github.com/night-chen/HYDRA."],"url":"http://arxiv.org/abs/2406.02888v1","category":"cs.CL"}
{"created":"2024-06-05 03:08:25","title":"PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs","abstract":"Large Language Models (LLMs) have exhibited impressive capabilities in various tasks, yet their vast parameter sizes restrict their applicability in resource-constrained settings. Knowledge distillation (KD) offers a viable solution by transferring expertise from large teacher models to compact student models. However, traditional KD techniques face specific challenges when applied to LLMs, including restricted access to LLM outputs, significant teacher-student capacity gaps, and the inherited mis-calibration issue. In this work, we present PLaD, a novel preference-based LLM distillation framework. PLaD exploits the teacher-student capacity discrepancy to generate pseudo-preference pairs where teacher outputs are preferred over student outputs. Then, PLaD leverages a ranking loss to re-calibrate student's estimation of sequence likelihood, which steers the student's focus towards understanding the relative quality of outputs instead of simply imitating the teacher. PLaD bypasses the need for access to teacher LLM's internal states, tackles the student's expressivity limitations, and mitigates the student mis-calibration issue. Through extensive experiments on two sequence generation tasks and with various LLMs, we demonstrate the effectiveness of our proposed PLaD framework.","sentences":["Large Language Models (LLMs) have exhibited impressive capabilities in various tasks, yet their vast parameter sizes restrict their applicability in resource-constrained settings.","Knowledge distillation (KD) offers a viable solution by transferring expertise from large teacher models to compact student models.","However, traditional KD techniques face specific challenges when applied to LLMs, including restricted access to LLM outputs, significant teacher-student capacity gaps, and the inherited mis-calibration issue.","In this work, we present PLaD, a novel preference-based LLM distillation framework.","PLaD exploits the teacher-student capacity discrepancy to generate pseudo-preference pairs where teacher outputs are preferred over student outputs.","Then, PLaD leverages a ranking loss to re-calibrate student's estimation of sequence likelihood, which steers the student's focus towards understanding the relative quality of outputs instead of simply imitating the teacher.","PLaD bypasses the need for access to teacher LLM's internal states, tackles the student's expressivity limitations, and mitigates the student mis-calibration issue.","Through extensive experiments on two sequence generation tasks and with various LLMs, we demonstrate the effectiveness of our proposed PLaD framework."],"url":"http://arxiv.org/abs/2406.02886v1","category":"cs.CL"}
{"created":"2024-06-05 03:08:00","title":"Homotopic Path Set Planning for Robot Manipulation and Navigation","abstract":"This paper addresses path set planning that yields important applications in robot manipulation and navigation such as path generation for deformable object keypoints and swarms. A path set refers to the collection of finite agent paths to represent the overall spatial path of a group of keypoints or a swarm, whose collective properties meet spatial and topological constraints. As opposed to planning a single path, simultaneously planning multiple paths with constraints poses nontrivial challenges in complex environments. This paper presents a systematic planning pipeline for homotopic path sets, a widely applicable path set class in robotics. An extended visibility check condition is first proposed to attain a sparse passage distribution amidst dense obstacles. Passage-aware optimal path planning compatible with sampling-based planners is then designed for single path planning with adjustable costs. Large accessible free space for path set accommodation can be achieved by the planned path while having a sufficiently short path length. After specifying the homotopic properties of path sets, path set generation based on deformable path transfer is proposed in an efficient centralized manner. The effectiveness of these methods is validated by extensive simulated and experimental results.","sentences":["This paper addresses path set planning that yields important applications in robot manipulation and navigation such as path generation for deformable object keypoints and swarms.","A path set refers to the collection of finite agent paths to represent the overall spatial path of a group of keypoints or a swarm, whose collective properties meet spatial and topological constraints.","As opposed to planning a single path, simultaneously planning multiple paths with constraints poses nontrivial challenges in complex environments.","This paper presents a systematic planning pipeline for homotopic path sets, a widely applicable path set class in robotics.","An extended visibility check condition is first proposed to attain a sparse passage distribution amidst dense obstacles.","Passage-aware optimal path planning compatible with sampling-based planners is then designed for single path planning with adjustable costs.","Large accessible free space for path set accommodation can be achieved by the planned path while having a sufficiently short path length.","After specifying the homotopic properties of path sets, path set generation based on deformable path transfer is proposed in an efficient centralized manner.","The effectiveness of these methods is validated by extensive simulated and experimental results."],"url":"http://arxiv.org/abs/2406.02885v1","category":"cs.RO"}
{"created":"2024-06-05 03:00:47","title":"Nonlinear Transformations Against Unlearnable Datasets","abstract":"Automated scraping stands out as a common method for collecting data in deep learning models without the authorization of data owners. Recent studies have begun to tackle the privacy concerns associated with this data collection method. Notable approaches include Deepconfuse, error-minimizing, error-maximizing (also known as adversarial poisoning), Neural Tangent Generalization Attack, synthetic, autoregressive, One-Pixel Shortcut, Self-Ensemble Protection, Entangled Features, Robust Error-Minimizing, Hypocritical, and TensorClog. The data generated by those approaches, called \"unlearnable\" examples, are prevented \"learning\" by deep learning models. In this research, we investigate and devise an effective nonlinear transformation framework and conduct extensive experiments to demonstrate that a deep neural network can effectively learn from the data/examples traditionally considered unlearnable produced by the above twelve approaches. The resulting approach improves the ability to break unlearnable data compared to the linear separable technique recently proposed by researchers. Specifically, our extensive experiments show that the improvement ranges from 0.34% to 249.59% for the unlearnable CIFAR10 datasets generated by those twelve data protection approaches, except for One-Pixel Shortcut. Moreover, the proposed framework achieves over 100% improvement of test accuracy for Autoregressive and REM approaches compared to the linear separable technique. Our findings suggest that these approaches are inadequate in preventing unauthorized uses of data in machine learning models. There is an urgent need to develop more robust protection mechanisms that effectively thwart an attacker from accessing data without proper authorization from the owners.","sentences":["Automated scraping stands out as a common method for collecting data in deep learning models without the authorization of data owners.","Recent studies have begun to tackle the privacy concerns associated with this data collection method.","Notable approaches include Deepconfuse, error-minimizing, error-maximizing (also known as adversarial poisoning), Neural Tangent Generalization Attack, synthetic, autoregressive, One-Pixel Shortcut, Self-Ensemble Protection, Entangled Features, Robust Error-Minimizing, Hypocritical, and TensorClog.","The data generated by those approaches, called \"unlearnable\" examples, are prevented \"learning\" by deep learning models.","In this research, we investigate and devise an effective nonlinear transformation framework and conduct extensive experiments to demonstrate that a deep neural network can effectively learn from the data/examples traditionally considered unlearnable produced by the above twelve approaches.","The resulting approach improves the ability to break unlearnable data compared to the linear separable technique recently proposed by researchers.","Specifically, our extensive experiments show that the improvement ranges from 0.34% to 249.59% for the unlearnable CIFAR10 datasets generated by those twelve data protection approaches, except for One-Pixel Shortcut.","Moreover, the proposed framework achieves over 100% improvement of test accuracy for Autoregressive and REM approaches compared to the linear separable technique.","Our findings suggest that these approaches are inadequate in preventing unauthorized uses of data in machine learning models.","There is an urgent need to develop more robust protection mechanisms that effectively thwart an attacker from accessing data without proper authorization from the owners."],"url":"http://arxiv.org/abs/2406.02883v1","category":"cs.LG"}
{"created":"2024-06-05 03:00:15","title":"Outdated Issue Aware Decoding for Factual Knowledge Editing","abstract":"Recently, Knowledge Editing has received increasing attention, since it could update the specific knowledge from outdated ones in pretrained models without re-training. However, as pointed out by recent studies, existing related methods tend to merely memorize the superficial word composition of the edited knowledge, rather than truly learning and absorbing it. Consequently, on the reasoning questions, we discover that existing methods struggle to utilize the edited knowledge to reason the new answer, and tend to retain outdated responses, which are generated by the original models utilizing original knowledge. Nevertheless, the outdated responses are unexpected for the correct answers to reasoning questions, which we named as the outdated issue. To alleviate this issue, in this paper, we propose a simple yet effective decoding strategy, i.e., outDated ISsue aware deCOding (DISCO), to enhance the performance of edited models on reasoning questions. Specifically, we capture the difference in the probability distribution between the original and edited models. Further, we amplify the difference of the token prediction in the edited model to alleviate the outdated issue, and thus enhance the model performance w.r.t the edited knowledge. Experimental results suggest that applying DISCO could enhance edited models to reason, e.g., on reasoning questions, DISCO outperforms the prior SOTA method by 12.99 F1 scores, and reduces the ratio of the outdated issue to 5.78% on the zsRE dataset.","sentences":["Recently, Knowledge Editing has received increasing attention, since it could update the specific knowledge from outdated ones in pretrained models without re-training.","However, as pointed out by recent studies, existing related methods tend to merely memorize the superficial word composition of the edited knowledge, rather than truly learning and absorbing it.","Consequently, on the reasoning questions, we discover that existing methods struggle to utilize the edited knowledge to reason the new answer, and tend to retain outdated responses, which are generated by the original models utilizing original knowledge.","Nevertheless, the outdated responses are unexpected for the correct answers to reasoning questions, which we named as the outdated issue.","To alleviate this issue, in this paper, we propose a simple yet effective decoding strategy, i.e., outDated ISsue aware deCOding (DISCO), to enhance the performance of edited models on reasoning questions.","Specifically, we capture the difference in the probability distribution between the original and edited models.","Further, we amplify the difference of the token prediction in the edited model to alleviate the outdated issue, and thus enhance the model performance w.r.t the edited knowledge.","Experimental results suggest that applying DISCO could enhance edited models to reason, e.g., on reasoning questions, DISCO outperforms the prior SOTA method by 12.99 F1 scores, and reduces the ratio of the outdated issue to 5.78% on the zsRE dataset."],"url":"http://arxiv.org/abs/2406.02882v1","category":"cs.CL"}
{"created":"2024-06-05 02:54:46","title":"Controllable Talking Face Generation by Implicit Facial Keypoints Editing","abstract":"Audio-driven talking face generation has garnered significant interest within the domain of digital human research. Existing methods are encumbered by intricate model architectures that are intricately dependent on each other, complicating the process of re-editing image or video inputs. In this work, we present ControlTalk, a talking face generation method to control face expression deformation based on driven audio, which can construct the head pose and facial expression including lip motion for both single image or sequential video inputs in a unified manner. By utilizing a pre-trained video synthesis renderer and proposing the lightweight adaptation, ControlTalk achieves precise and naturalistic lip synchronization while enabling quantitative control over mouth opening shape. Our experiments show that our method is superior to state-of-the-art performance on widely used benchmarks, including HDTF and MEAD. The parameterized adaptation demonstrates remarkable generalization capabilities, effectively handling expression deformation across same-ID and cross-ID scenarios, and extending its utility to out-of-domain portraits, regardless of languages.","sentences":["Audio-driven talking face generation has garnered significant interest within the domain of digital human research.","Existing methods are encumbered by intricate model architectures that are intricately dependent on each other, complicating the process of re-editing image or video inputs.","In this work, we present ControlTalk, a talking face generation method to control face expression deformation based on driven audio, which can construct the head pose and facial expression including lip motion for both single image or sequential video inputs in a unified manner.","By utilizing a pre-trained video synthesis renderer and proposing the lightweight adaptation, ControlTalk achieves precise and naturalistic lip synchronization while enabling quantitative control over mouth opening shape.","Our experiments show that our method is superior to state-of-the-art performance on widely used benchmarks, including HDTF and MEAD.","The parameterized adaptation demonstrates remarkable generalization capabilities, effectively handling expression deformation across same-ID and cross-ID scenarios, and extending its utility to out-of-domain portraits, regardless of languages."],"url":"http://arxiv.org/abs/2406.02880v1","category":"cs.CV"}
{"created":"2024-06-05 02:52:17","title":"LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation","abstract":"Multilingual neural machine translation models generally distinguish translation directions by the language tag (LT) in front of the source or target sentences. However, current LT strategies cannot indicate the desired target language as expected on zero-shot translation, i.e., the off-target issue. Our analysis reveals that the indication of the target language is sensitive to the placement of the target LT. For example, when placing the target LT on the decoder side, the indication would rapidly degrade along with decoding steps, while placing the target LT on the encoder side would lead to copying or paraphrasing the source input. To address the above issues, we propose a simple yet effective strategy named Language Converter Strategy (LCS). By introducing the target language embedding into the top encoder layers, LCS mitigates confusion in the encoder and ensures stable language indication for the decoder. Experimental results on MultiUN, TED, and OPUS-100 datasets demonstrate that LCS could significantly mitigate the off-target issue, with language accuracy up to 95.28%, 96.21%, and 85.35% meanwhile outperforming the vanilla LT strategy by 3.07, 3,3, and 7.93 BLEU scores on zero-shot translation, respectively.","sentences":["Multilingual neural machine translation models generally distinguish translation directions by the language tag (LT) in front of the source or target sentences.","However, current LT strategies cannot indicate the desired target language as expected on zero-shot translation, i.e., the off-target issue.","Our analysis reveals that the indication of the target language is sensitive to the placement of the target LT.","For example, when placing the target LT on the decoder side, the indication would rapidly degrade along with decoding steps, while placing the target LT on the encoder side would lead to copying or paraphrasing the source input.","To address the above issues, we propose a simple yet effective strategy named Language Converter Strategy (LCS).","By introducing the target language embedding into the top encoder layers, LCS mitigates confusion in the encoder and ensures stable language indication for the decoder.","Experimental results on MultiUN, TED, and OPUS-100 datasets demonstrate that LCS could significantly mitigate the off-target issue, with language accuracy up to 95.28%, 96.21%, and 85.35% meanwhile outperforming the vanilla LT strategy by 3.07, 3,3, and 7.93 BLEU scores on zero-shot translation, respectively."],"url":"http://arxiv.org/abs/2406.02876v1","category":"cs.CL"}
{"created":"2024-06-05 02:43:41","title":"Combinatorial Optimization with Automated Graph Neural Networks","abstract":"In recent years, graph neural networks (GNNs) have become increasingly popular for solving NP-hard combinatorial optimization (CO) problems, such as maximum cut and maximum independent set. The core idea behind these methods is to represent a CO problem as a graph and then use GNNs to learn the node/graph embedding with combinatorial information. Although these methods have achieved promising results, given a specific CO problem, the design of GNN architectures still requires heavy manual work with domain knowledge. Existing automated GNNs are mostly focused on traditional graph learning problems, which is inapplicable to solving NP-hard CO problems. To this end, we present a new class of \\textbf{AUTO}mated \\textbf{G}NNs for solving \\textbf{NP}-hard problems, namely \\textbf{AutoGNP}. We represent CO problems by GNNs and focus on two specific problems, i.e., mixed integer linear programming and quadratic unconstrained binary optimization. The idea of AutoGNP is to use graph neural architecture search algorithms to automatically find the best GNNs for a given NP-hard combinatorial optimization problem. Compared with existing graph neural architecture search algorithms, AutoGNP utilizes two-hop operators in the architecture search space. Moreover, AutoGNP utilizes simulated annealing and a strict early stopping policy to avoid local optimal solutions. Empirical results on benchmark combinatorial problems demonstrate the superiority of our proposed model.","sentences":["In recent years, graph neural networks (GNNs) have become increasingly popular for solving NP-hard combinatorial optimization (CO) problems, such as maximum cut and maximum independent set.","The core idea behind these methods is to represent a CO problem as a graph and then use GNNs to learn the node/graph embedding with combinatorial information.","Although these methods have achieved promising results, given a specific CO problem, the design of GNN architectures still requires heavy manual work with domain knowledge.","Existing automated GNNs are mostly focused on traditional graph learning problems, which is inapplicable to solving NP-hard CO problems.","To this end, we present a new class of \\textbf{AUTO}mated \\textbf{G}NNs for solving \\textbf{NP}-hard problems, namely \\textbf{AutoGNP}.","We represent CO problems by GNNs and focus on two specific problems, i.e., mixed integer linear programming and quadratic unconstrained binary optimization.","The idea of AutoGNP is to use graph neural architecture search algorithms to automatically find the best GNNs for a given NP-hard combinatorial optimization problem.","Compared with existing graph neural architecture search algorithms, AutoGNP utilizes two-hop operators in the architecture search space.","Moreover, AutoGNP utilizes simulated annealing and a strict early stopping policy to avoid local optimal solutions.","Empirical results on benchmark combinatorial problems demonstrate the superiority of our proposed model."],"url":"http://arxiv.org/abs/2406.02872v1","category":"cs.LG"}
{"created":"2024-06-05 02:33:50","title":"Sound Heuristic Search Value Iteration for Undiscounted POMDPs with Reachability Objectives","abstract":"Partially Observable Markov Decision Processes (POMDPs) are powerful models for sequential decision making under transition and observation uncertainties. This paper studies the challenging yet important problem in POMDPs known as the (indefinite-horizon) Maximal Reachability Probability Problem (MRPP), where the goal is to maximize the probability of reaching some target states. This is also a core problem in model checking with logical specifications and is naturally undiscounted (discount factor is one). Inspired by the success of point-based methods developed for discounted problems, we study their extensions to MRPP. Specifically, we focus on trial-based heuristic search value iteration techniques and present a novel algorithm that leverages the strengths of these techniques for efficient exploration of the belief space (informed search via value bounds) while addressing their drawbacks in handling loops for indefinite-horizon problems. The algorithm produces policies with two-sided bounds on optimal reachability probabilities. We prove convergence to an optimal policy from below under certain conditions. Experimental evaluations on a suite of benchmarks show that our algorithm outperforms existing methods in almost all cases in both probability guarantees and computation time.","sentences":["Partially Observable Markov Decision Processes (POMDPs) are powerful models for sequential decision making under transition and observation uncertainties.","This paper studies the challenging yet important problem in POMDPs known as the (indefinite-horizon) Maximal Reachability Probability Problem (MRPP), where the goal is to maximize the probability of reaching some target states.","This is also a core problem in model checking with logical specifications and is naturally undiscounted (discount factor is one).","Inspired by the success of point-based methods developed for discounted problems, we study their extensions to MRPP.","Specifically, we focus on trial-based heuristic search value iteration techniques and present a novel algorithm that leverages the strengths of these techniques for efficient exploration of the belief space (informed search via value bounds) while addressing their drawbacks in handling loops for indefinite-horizon problems.","The algorithm produces policies with two-sided bounds on optimal reachability probabilities.","We prove convergence to an optimal policy from below under certain conditions.","Experimental evaluations on a suite of benchmarks show that our algorithm outperforms existing methods in almost all cases in both probability guarantees and computation time."],"url":"http://arxiv.org/abs/2406.02871v1","category":"cs.AI"}
{"created":"2024-06-05 02:30:55","title":"Mesoscopic Bayesian Inference by Solvable Models","abstract":"The rapid advancement of data science and artificial intelligence has influenced physics in numerous ways, including the application of Bayesian inference. Our group has proposed Bayesian measurement, a framework that applies Bayesian inference to measurement science and is applicable across various natural sciences. This framework enables the determination of posterior probability distributions for system parameters, model selection, and the integration of multiple measurement datasets. However, a theoretical framework to address fluctuations in these results due to finite measurement data (N) is still needed. In this paper, we suggest a mesoscopic theoretical framework for the components of Bayesian measurement-parameter estimation, model selection, and Bayesian integration-within the mesoscopic region where (N) is finite. We develop a solvable theory for linear regression with Gaussian noise, which is practical for real-world measurements and as an approximation for nonlinear models with large (N). By utilizing mesoscopic Gaussian and chi-squared distributions, we aim to analytically evaluate the three components of Bayesian measurement. Our results offer a novel approach to understanding fluctuations in Bayesian measurement outcomes.","sentences":["The rapid advancement of data science and artificial intelligence has influenced physics in numerous ways, including the application of Bayesian inference.","Our group has proposed Bayesian measurement, a framework that applies Bayesian inference to measurement science and is applicable across various natural sciences.","This framework enables the determination of posterior probability distributions for system parameters, model selection, and the integration of multiple measurement datasets.","However, a theoretical framework to address fluctuations in these results due to finite measurement data (N) is still needed.","In this paper, we suggest a mesoscopic theoretical framework for the components of Bayesian measurement-parameter estimation, model selection, and Bayesian integration-within the mesoscopic region where (N) is finite.","We develop a solvable theory for linear regression with Gaussian noise, which is practical for real-world measurements and as an approximation for nonlinear models with large (N).","By utilizing mesoscopic Gaussian and chi-squared distributions, we aim to analytically evaluate the three components of Bayesian measurement.","Our results offer a novel approach to understanding fluctuations in Bayesian measurement outcomes."],"url":"http://arxiv.org/abs/2406.02869v1","category":"physics.data-an"}
{"created":"2024-06-05 02:30:29","title":"Oscillations enhance time-series prediction in reservoir computing with feedback","abstract":"Reservoir computing, a machine learning framework used for modeling the brain, can predict temporal data with little observations and minimal computational resources. However, it is difficult to accurately reproduce the long-term target time series because the reservoir system becomes unstable. This predictive capability is required for a wide variety of time-series processing, including predictions of motor timing and chaotic dynamical systems. This study proposes oscillation-driven reservoir computing (ODRC) with feedback, where oscillatory signals are fed into a reservoir network to stabilize the network activity and induce complex reservoir dynamics. The ODRC can reproduce long-term target time series more accurately than conventional reservoir computing methods in a motor timing and chaotic time-series prediction tasks. Furthermore, it generates a time series similar to the target in the unexperienced period, that is, it can learn the abstract generative rules from limited observations. Given these significant improvements made by the simple and computationally inexpensive implementation, the ODRC would serve as a practical model of various time series data. Moreover, we will discuss biological implications of the ODRC, considering it as a model of neural oscillations and their cerebellar processors.","sentences":["Reservoir computing, a machine learning framework used for modeling the brain, can predict temporal data with little observations and minimal computational resources.","However, it is difficult to accurately reproduce the long-term target time series because the reservoir system becomes unstable.","This predictive capability is required for a wide variety of time-series processing, including predictions of motor timing and chaotic dynamical systems.","This study proposes oscillation-driven reservoir computing (ODRC) with feedback, where oscillatory signals are fed into a reservoir network to stabilize the network activity and induce complex reservoir dynamics.","The ODRC can reproduce long-term target time series more accurately than conventional reservoir computing methods in a motor timing and chaotic time-series prediction tasks.","Furthermore, it generates a time series similar to the target in the unexperienced period, that is, it can learn the abstract generative rules from limited observations.","Given these significant improvements made by the simple and computationally inexpensive implementation, the ODRC would serve as a practical model of various time series data.","Moreover, we will discuss biological implications of the ODRC, considering it as a model of neural oscillations and their cerebellar processors."],"url":"http://arxiv.org/abs/2406.02867v1","category":"cs.LG"}
{"created":"2024-06-05 02:26:14","title":"NUMCoT: Numerals and Units of Measurement in Chain-of-Thought Reasoning using Large Language Models","abstract":"Numeral systems and units of measurement are two conjoined topics in activities of human beings and have mutual effects with the languages expressing them. Currently, the evaluation of Large Language Models (LLMs) often involves mathematical reasoning, yet little attention is given to how minor changes in numbers or units can drastically alter the complexity of problems and the performance of LLMs. In this paper, we scrutinize existing LLMs on processing of numerals and units of measurement by constructing datasets with perturbations. We first anatomize the reasoning of math word problems to different sub-procedures like numeral conversions from language to numbers and measurement conversions based on units. Then we further annotate math word problems from ancient Chinese arithmetic works which are challenging in numerals and units of measurement. Experiments on perturbed datasets demonstrate that LLMs still encounter difficulties in handling numeral and measurement conversions.","sentences":["Numeral systems and units of measurement are two conjoined topics in activities of human beings and have mutual effects with the languages expressing them.","Currently, the evaluation of Large Language Models (LLMs) often involves mathematical reasoning, yet little attention is given to how minor changes in numbers or units can drastically alter the complexity of problems and the performance of LLMs.","In this paper, we scrutinize existing LLMs on processing of numerals and units of measurement by constructing datasets with perturbations.","We first anatomize the reasoning of math word problems to different sub-procedures like numeral conversions from language to numbers and measurement conversions based on units.","Then we further annotate math word problems from ancient Chinese arithmetic works which are challenging in numerals and units of measurement.","Experiments on perturbed datasets demonstrate that LLMs still encounter difficulties in handling numeral and measurement conversions."],"url":"http://arxiv.org/abs/2406.02864v1","category":"cs.CL"}
{"created":"2024-06-05 02:15:55","title":"TSPDiffuser: Diffusion Models as Learned Samplers for Traveling Salesperson Path Planning Problems","abstract":"This paper presents TSPDiffuser, a novel data-driven path planner for traveling salesperson path planning problems (TSPPPs) in environments rich with obstacles. Given a set of destinations within obstacle maps, our objective is to efficiently find the shortest possible collision-free path that visits all the destinations. In TSPDiffuser, we train a diffusion model on a large collection of TSPPP instances and their respective solutions to generate plausible paths for unseen problem instances. The model can then be employed as a learned sampler to construct a roadmap that contains potential solutions with a small number of nodes and edges. This approach enables efficient and accurate estimation of traveling costs between destinations, effectively addressing the primary computational challenge in solving TSPPPs. Experimental evaluations with diverse synthetic and real-world indoor/outdoor environments demonstrate the effectiveness of TSPDiffuser over existing methods in terms of the trade-off between solution quality and computational time requirements.","sentences":["This paper presents TSPDiffuser, a novel data-driven path planner for traveling salesperson path planning problems (TSPPPs) in environments rich with obstacles.","Given a set of destinations within obstacle maps, our objective is to efficiently find the shortest possible collision-free path that visits all the destinations.","In TSPDiffuser, we train a diffusion model on a large collection of TSPPP instances and their respective solutions to generate plausible paths for unseen problem instances.","The model can then be employed as a learned sampler to construct a roadmap that contains potential solutions with a small number of nodes and edges.","This approach enables efficient and accurate estimation of traveling costs between destinations, effectively addressing the primary computational challenge in solving TSPPPs.","Experimental evaluations with diverse synthetic and real-world indoor/outdoor environments demonstrate the effectiveness of TSPDiffuser over existing methods in terms of the trade-off between solution quality and computational time requirements."],"url":"http://arxiv.org/abs/2406.02858v1","category":"cs.RO"}
{"created":"2024-06-05 02:12:06","title":"Xmodel-LM Technical Report","abstract":"We introduce Xmodel-LM, a compact and efficient 1.1B language model pre-trained on over 2 trillion tokens. Trained on our self-built dataset (Xdata), which balances Chinese and English corpora based on downstream task optimization, Xmodel-LM exhibits remarkable performance despite its smaller size. It notably surpasses existing open-source language models of similar scale. Our model checkpoints and code are publicly accessible on GitHub at https://github.com/XiaoduoAILab/XmodelLM.","sentences":["We introduce Xmodel-LM, a compact and efficient 1.1B language model pre-trained on over 2 trillion tokens.","Trained on our self-built dataset (Xdata), which balances Chinese and English corpora based on downstream task optimization, Xmodel-LM exhibits remarkable performance despite its smaller size.","It notably surpasses existing open-source language models of similar scale.","Our model checkpoints and code are publicly accessible on GitHub at https://github.com/XiaoduoAILab/XmodelLM."],"url":"http://arxiv.org/abs/2406.02856v1","category":"cs.CL"}
{"created":"2024-06-05 01:31:50","title":"Conditional Idempotent Generative Networks","abstract":"We propose Conditional Idempotent Generative Networks (CIGN), a novel approach that expands upon Idempotent Generative Networks (IGN) to enable conditional generation. While IGNs offer efficient single-pass generation, they lack the ability to control the content of the generated data. CIGNs address this limitation by incorporating conditioning mechanisms, allowing users to steer the generation process towards specific types of data.   We establish the theoretical foundations for CIGNs, outlining their scope, loss function design, and evaluation metrics. We then present two potential architectures for implementing CIGNs: channel conditioning and filter conditioning. Finally, we discuss experimental results on the MNIST dataset, demonstrating the effectiveness of both approaches. Our findings pave the way for further exploration of CIGNs on larger datasets and with more powerful computing resources to determine the optimal implementation strategy.","sentences":["We propose Conditional Idempotent Generative Networks (CIGN), a novel approach that expands upon Idempotent Generative Networks (IGN) to enable conditional generation.","While IGNs offer efficient single-pass generation, they lack the ability to control the content of the generated data.","CIGNs address this limitation by incorporating conditioning mechanisms, allowing users to steer the generation process towards specific types of data.   ","We establish the theoretical foundations for CIGNs, outlining their scope, loss function design, and evaluation metrics.","We then present two potential architectures for implementing CIGNs: channel conditioning and filter conditioning.","Finally, we discuss experimental results on the MNIST dataset, demonstrating the effectiveness of both approaches.","Our findings pave the way for further exploration of CIGNs on larger datasets and with more powerful computing resources to determine the optimal implementation strategy."],"url":"http://arxiv.org/abs/2406.02841v1","category":"cs.LG"}
{"created":"2024-06-05 00:13:38","title":"Stochastic Diffusion: A Diffusion Probabilistic Model for Stochastic Time Series Forecasting","abstract":"Recent innovations in diffusion probabilistic models have paved the way for significant progress in image, text and audio generation, leading to their applications in generative time series forecasting. However, leveraging such abilities to model highly stochastic time series data remains a challenge. In this paper, we propose a novel Stochastic Diffusion (StochDiff) model which learns data-driven prior knowledge at each time step by utilizing the representational power of the stochastic latent spaces to model the variability of the multivariate time series data. The learnt prior knowledge helps the model to capture complex temporal dynamics and the inherent uncertainty of the data. This improves its ability to model highly stochastic time series data. Through extensive experiments on real-world datasets, we demonstrate the effectiveness of our proposed model on stochastic time series forecasting. Additionally, we showcase an application of our model for real-world surgical guidance, highlighting its potential to benefit the medical community.","sentences":["Recent innovations in diffusion probabilistic models have paved the way for significant progress in image, text and audio generation, leading to their applications in generative time series forecasting.","However, leveraging such abilities to model highly stochastic time series data remains a challenge.","In this paper, we propose a novel Stochastic Diffusion (StochDiff) model which learns data-driven prior knowledge at each time step by utilizing the representational power of the stochastic latent spaces to model the variability of the multivariate time series data.","The learnt prior knowledge helps the model to capture complex temporal dynamics and the inherent uncertainty of the data.","This improves its ability to model highly stochastic time series data.","Through extensive experiments on real-world datasets, we demonstrate the effectiveness of our proposed model on stochastic time series forecasting.","Additionally, we showcase an application of our model for real-world surgical guidance, highlighting its potential to benefit the medical community."],"url":"http://arxiv.org/abs/2406.02827v1","category":"cs.LG"}
{"created":"2024-06-05 00:11:20","title":"Exploring Robustness in Doctor-Patient Conversation Summarization: An Analysis of Out-of-Domain SOAP Notes","abstract":"Summarizing medical conversations poses unique challenges due to the specialized domain and the difficulty of collecting in-domain training data. In this study, we investigate the performance of state-of-the-art doctor-patient conversation generative summarization models on the out-of-domain data. We divide the summarization model of doctor-patient conversation into two configurations: (1) a general model, without specifying subjective (S), objective (O), and assessment (A) and plan (P) notes; (2) a SOAP-oriented model that generates a summary with SOAP sections. We analyzed the limitations and strengths of the fine-tuning language model-based methods and GPTs on both configurations. We also conducted a Linguistic Inquiry and Word Count analysis to compare the SOAP notes from different datasets. The results exhibit a strong correlation for reference notes across different datasets, indicating that format mismatch (i.e., discrepancies in word distribution) is not the main cause of performance decline on out-of-domain data. Lastly, a detailed analysis of SOAP notes is included to provide insights into missing information and hallucinations introduced by the models.","sentences":["Summarizing medical conversations poses unique challenges due to the specialized domain and the difficulty of collecting in-domain training data.","In this study, we investigate the performance of state-of-the-art doctor-patient conversation generative summarization models on the out-of-domain data.","We divide the summarization model of doctor-patient conversation into two configurations: (1) a general model, without specifying subjective (S), objective (O), and assessment (A) and plan (P) notes; (2) a SOAP-oriented model that generates a summary with SOAP sections.","We analyzed the limitations and strengths of the fine-tuning language model-based methods and GPTs on both configurations.","We also conducted a Linguistic Inquiry and Word Count analysis to compare the SOAP notes from different datasets.","The results exhibit a strong correlation for reference notes across different datasets, indicating that format mismatch (i.e., discrepancies in word distribution) is not the main cause of performance decline on out-of-domain data.","Lastly, a detailed analysis of SOAP notes is included to provide insights into missing information and hallucinations introduced by the models."],"url":"http://arxiv.org/abs/2406.02826v1","category":"cs.CL"}
{"created":"2024-06-04 23:39:08","title":"ORACLE: Leveraging Mutual Information for Consistent Character Generation with LoRAs in Diffusion Models","abstract":"Text-to-image diffusion models have recently taken center stage as pivotal tools in promoting visual creativity across an array of domains such as comic book artistry, children's literature, game development, and web design. These models harness the power of artificial intelligence to convert textual descriptions into vivid images, thereby enabling artists and creators to bring their imaginative concepts to life with unprecedented ease. However, one of the significant hurdles that persist is the challenge of maintaining consistency in character generation across diverse contexts. Variations in textual prompts, even if minor, can yield vastly different visual outputs, posing a considerable problem in projects that require a uniform representation of characters throughout. In this paper, we introduce a novel framework designed to produce consistent character representations from a single text prompt across diverse settings. Through both quantitative and qualitative analyses, we demonstrate that our framework outperforms existing methods in generating characters with consistent visual identities, underscoring its potential to transform creative industries. By addressing the critical challenge of character consistency, we not only enhance the practical utility of these models but also broaden the horizons for artistic and creative expression.","sentences":["Text-to-image diffusion models have recently taken center stage as pivotal tools in promoting visual creativity across an array of domains such as comic book artistry, children's literature, game development, and web design.","These models harness the power of artificial intelligence to convert textual descriptions into vivid images, thereby enabling artists and creators to bring their imaginative concepts to life with unprecedented ease.","However, one of the significant hurdles that persist is the challenge of maintaining consistency in character generation across diverse contexts.","Variations in textual prompts, even if minor, can yield vastly different visual outputs, posing a considerable problem in projects that require a uniform representation of characters throughout.","In this paper, we introduce a novel framework designed to produce consistent character representations from a single text prompt across diverse settings.","Through both quantitative and qualitative analyses, we demonstrate that our framework outperforms existing methods in generating characters with consistent visual identities, underscoring its potential to transform creative industries.","By addressing the critical challenge of character consistency, we not only enhance the practical utility of these models but also broaden the horizons for artistic and creative expression."],"url":"http://arxiv.org/abs/2406.02820v1","category":"cs.CV"}
{"created":"2024-06-04 22:08:24","title":"$\\texttt{ACCORD}$: Closing the Commonsense Measurability Gap","abstract":"We present $\\texttt{ACCORD}$, a framework and benchmark suite for disentangling the commonsense grounding and reasoning abilities of large language models (LLMs) through controlled, multi-hop counterfactuals. $\\texttt{ACCORD}$ introduces formal elements to commonsense reasoning to explicitly control and quantify reasoning complexity beyond the typical 1 or 2 hops. Uniquely, $\\texttt{ACCORD}$ can automatically generate benchmarks of arbitrary reasoning complexity, and so it scales with future LLM improvements. Benchmarking state-of-the-art LLMs -- including GPT-4o (2024-05-13), Llama-3-70B-Instruct, and Mixtral-8x22B-Instruct-v0.1 -- shows performance degrading to random chance with only moderate scaling, leaving substantial headroom for improvement. We release a leaderboard of the benchmark suite tested in this work, as well as code for automatically generating more complex benchmarks.","sentences":["We present $\\texttt{ACCORD}$, a framework and benchmark suite for disentangling the commonsense grounding and reasoning abilities of large language models (LLMs) through controlled, multi-hop counterfactuals.","$\\texttt{ACCORD}$ introduces formal elements to commonsense reasoning to explicitly control and quantify reasoning complexity beyond the typical 1 or 2 hops.","Uniquely, $\\texttt{ACCORD}$ can automatically generate benchmarks of arbitrary reasoning complexity, and so it scales with future LLM improvements.","Benchmarking state-of-the-art LLMs -- including GPT-4o (2024-05-13), Llama-3-70B-Instruct, and Mixtral-8x22B-Instruct-v0.1 -- shows performance degrading to random chance with only moderate scaling, leaving substantial headroom for improvement.","We release a leaderboard of the benchmark suite tested in this work, as well as code for automatically generating more complex benchmarks."],"url":"http://arxiv.org/abs/2406.02804v1","category":"cs.AI"}
{"created":"2024-06-04 22:01:26","title":"SenTopX: Benchmark for User Sentiment on Various Topics","abstract":"Toxic sentiment analysis on Twitter (X) often focuses on specific topics and events such as politics and elections. Datasets of toxic users in such research are typically gathered through lexicon-based techniques, providing only a cross-sectional view. his approach has a tight confine for studying toxic user behavior and effective platform moderation. To identify users consistently spreading toxicity, a longitudinal analysis of their tweets is essential. However, such datasets currently do not exist.   This study addresses this gap by collecting a longitudinal dataset from 143K Twitter users, covering the period from 2007 to 2021, amounting to a total of 293 million tweets. Using topic modeling, we extract all topics discussed by each user and categorize users into eight groups based on the predominant topic in their timelines. We then analyze the sentiments of each group using 16 toxic scores. Our research demonstrates that examining users longitudinally reveals a distinct perspective on their comprehensive personality traits and their overall impact on the platform. Our comprehensive dataset is accessible to researchers for additional analysis.","sentences":["Toxic sentiment analysis on Twitter (X) often focuses on specific topics and events such as politics and elections.","Datasets of toxic users in such research are typically gathered through lexicon-based techniques, providing only a cross-sectional view.","his approach has a tight confine for studying toxic user behavior and effective platform moderation.","To identify users consistently spreading toxicity, a longitudinal analysis of their tweets is essential.","However, such datasets currently do not exist.   ","This study addresses this gap by collecting a longitudinal dataset from 143K Twitter users, covering the period from 2007 to 2021, amounting to a total of 293 million tweets.","Using topic modeling, we extract all topics discussed by each user and categorize users into eight groups based on the predominant topic in their timelines.","We then analyze the sentiments of each group using 16 toxic scores.","Our research demonstrates that examining users longitudinally reveals a distinct perspective on their comprehensive personality traits and their overall impact on the platform.","Our comprehensive dataset is accessible to researchers for additional analysis."],"url":"http://arxiv.org/abs/2406.02801v1","category":"cs.SI"}
{"created":"2024-06-04 21:29:56","title":"Language Models can Infer Action Semantics for Classical Planners from Environment Feedback","abstract":"Classical planning approaches guarantee finding a set of actions that can achieve a given goal state when possible, but require an expert to specify logical action semantics that govern the dynamics of the environment. Researchers have shown that Large Language Models (LLMs) can be used to directly infer planning steps based on commonsense knowledge and minimal domain information alone, but such plans often fail on execution. We bring together the strengths of classical planning and LLM commonsense inference to perform domain induction, learning and validating action pre- and post-conditions based on closed-loop interactions with the environment itself. We propose PSALM, which leverages LLM inference to heuristically complete partial plans emitted by a classical planner given partial domain knowledge, as well as to infer the semantic rules of the domain in a logical language based on environment feedback after execution. Our analysis on 7 environments shows that with just one expert-curated example plans, using LLMs as heuristic planners and rule predictors achieves lower environment execution steps and environment resets than random exploration while simultaneously recovering the underlying ground truth action semantics of the domain.","sentences":["Classical planning approaches guarantee finding a set of actions that can achieve a given goal state when possible, but require an expert to specify logical action semantics that govern the dynamics of the environment.","Researchers have shown that Large Language Models (LLMs) can be used to directly infer planning steps based on commonsense knowledge and minimal domain information alone, but such plans often fail on execution.","We bring together the strengths of classical planning and LLM commonsense inference to perform domain induction, learning and validating action pre- and post-conditions based on closed-loop interactions with the environment itself.","We propose PSALM, which leverages LLM inference to heuristically complete partial plans emitted by a classical planner given partial domain knowledge, as well as to infer the semantic rules of the domain in a logical language based on environment feedback after execution.","Our analysis on 7 environments shows that with just one expert-curated example plans, using LLMs as heuristic planners and rule predictors achieves lower environment execution steps and environment resets than random exploration while simultaneously recovering the underlying ground truth action semantics of the domain."],"url":"http://arxiv.org/abs/2406.02791v1","category":"cs.AI"}
{"created":"2024-06-04 21:25:06","title":"Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities","abstract":"This study intends to systematically disentangle pure logic reasoning and text understanding by investigating the contrast across abstract and contextualized logical problems from a comprehensive set of domains. We explore whether LLMs demonstrate genuine reasoning capabilities across various domains when the underlying logical structure remains constant. We focus on two main questions (1) Can abstract logical problems alone accurately benchmark an LLM's reasoning ability in real-world scenarios, disentangled from contextual support in practical settings? (2) Does fine-tuning LLMs on abstract logic problem generalize to contextualized logic problems and vice versa? To investigate these questions, we focus on standard propositional logic, specifically propositional deductive and abductive logic reasoning. In particular, we construct instantiated datasets for deductive and abductive reasoning with 4 levels of difficulty, encompassing 12 distinct categories or domains based on the categorization of Wikipedia. Our experiments aim to provide insights into disentangling context in logical reasoning and the true reasoning capabilities of LLMs and their generalization potential. The code and dataset are available at: https://github.com/agiresearch/ContextHub.","sentences":["This study intends to systematically disentangle pure logic reasoning and text understanding by investigating the contrast across abstract and contextualized logical problems from a comprehensive set of domains.","We explore whether LLMs demonstrate genuine reasoning capabilities across various domains when the underlying logical structure remains constant.","We focus on two main questions (1) Can abstract logical problems alone accurately benchmark an LLM's reasoning ability in real-world scenarios, disentangled from contextual support in practical settings?","(2) Does fine-tuning LLMs on abstract logic problem generalize to contextualized logic problems and vice versa?","To investigate these questions, we focus on standard propositional logic, specifically propositional deductive and abductive logic reasoning.","In particular, we construct instantiated datasets for deductive and abductive reasoning with 4 levels of difficulty, encompassing 12 distinct categories or domains based on the categorization of Wikipedia.","Our experiments aim to provide insights into disentangling context in logical reasoning and the true reasoning capabilities of LLMs and their generalization potential.","The code and dataset are available at: https://github.com/agiresearch/ContextHub."],"url":"http://arxiv.org/abs/2406.02787v1","category":"cs.CL"}
{"created":"2024-06-04 21:04:34","title":"Feasibility of State Space Models for Network Traffic Generation","abstract":"Many problems in computer networking rely on parsing collections of network traces (e.g., traffic prioritization, intrusion detection). Unfortunately, the availability and utility of these collections is limited due to privacy concerns, data staleness, and low representativeness. While methods for generating data to augment collections exist, they often fall short in replicating the quality of real-world traffic In this paper, we i) survey the evolution of traffic simulators/generators and ii) propose the use of state-space models, specifically Mamba, for packet-level, synthetic network trace generation by modeling it as an unsupervised sequence generation problem. Early evaluation shows that state-space models can generate synthetic network traffic with higher statistical similarity to real traffic than the state-of-the-art. Our approach thus has the potential to reliably generate realistic, informative synthetic network traces for downstream tasks.","sentences":["Many problems in computer networking rely on parsing collections of network traces (e.g., traffic prioritization, intrusion detection).","Unfortunately, the availability and utility of these collections is limited due to privacy concerns, data staleness, and low representativeness.","While methods for generating data to augment collections exist, they often fall short in replicating the quality of real-world traffic In this paper, we i) survey the evolution of traffic simulators/generators and ii) propose the use of state-space models, specifically Mamba, for packet-level, synthetic network trace generation by modeling it as an unsupervised sequence generation problem.","Early evaluation shows that state-space models can generate synthetic network traffic with higher statistical similarity to real traffic than the state-of-the-art.","Our approach thus has the potential to reliably generate realistic, informative synthetic network traces for downstream tasks."],"url":"http://arxiv.org/abs/2406.02784v1","category":"cs.NI"}
{"created":"2024-06-04 20:51:04","title":"LADI v2: Multi-label Dataset and Classifiers for Low-Altitude Disaster Imagery","abstract":"ML-based computer vision models are promising tools for supporting emergency management operations following natural disasters. Arial photographs taken from small manned and unmanned aircraft can be available soon after a disaster and provide valuable information from multiple perspectives for situational awareness and damage assessment applications. However, emergency managers often face challenges finding the most relevant photos among the tens of thousands that may be taken after an incident. While ML-based solutions could enable more effective use of aerial photographs, there is still a lack of training data for imagery of this type from multiple perspectives and for multiple hazard types. To address this, we present the LADI v2 (Low Altitude Disaster Imagery version 2) dataset, a curated set of about 10,000 disaster images captured in the United States by the Civil Air Patrol (CAP) in response to federally-declared emergencies (2015-2023) and annotated for multi-label classification by trained CAP volunteers. We also provide two pretrained baseline classifiers and compare their performance to state-of-the-art vision-language models in multi-label classification. The data and code are released publicly to support the development of computer vision models for emergency management research and applications.","sentences":["ML-based computer vision models are promising tools for supporting emergency management operations following natural disasters.","Arial photographs taken from small manned and unmanned aircraft can be available soon after a disaster and provide valuable information from multiple perspectives for situational awareness and damage assessment applications.","However, emergency managers often face challenges finding the most relevant photos among the tens of thousands that may be taken after an incident.","While ML-based solutions could enable more effective use of aerial photographs, there is still a lack of training data for imagery of this type from multiple perspectives and for multiple hazard types.","To address this, we present the LADI v2 (Low Altitude Disaster Imagery version 2) dataset, a curated set of about 10,000 disaster images captured in the United States by the Civil Air Patrol (CAP) in response to federally-declared emergencies (2015-2023) and annotated for multi-label classification by trained CAP volunteers.","We also provide two pretrained baseline classifiers and compare their performance to state-of-the-art vision-language models in multi-label classification.","The data and code are released publicly to support the development of computer vision models for emergency management research and applications."],"url":"http://arxiv.org/abs/2406.02780v1","category":"cs.CV"}
{"created":"2024-06-04 20:45:53","title":"MeshVPR: Citywide Visual Place Recognition Using 3D Meshes","abstract":"Mesh-based scene representation offers a promising direction for simplifying large-scale hierarchical visual localization pipelines, combining a visual place recognition step based on global features (retrieval) and a visual localization step based on local features. While existing work demonstrates the viability of meshes for visual localization, the impact of using synthetic databases rendered from them in visual place recognition remains largely unexplored. In this work we investigate using dense 3D textured meshes for large-scale Visual Place Recognition (VPR) and identify a significant performance drop when using synthetic mesh-based databases compared to real-world images for retrieval. To address this, we propose MeshVPR, a novel VPR pipeline that utilizes a lightweight features alignment framework to bridge the gap between real-world and synthetic domains. MeshVPR leverages pre-trained VPR models and it is efficient and scalable for city-wide deployments. We introduce novel datasets with freely available 3D meshes and manually collected queries from Berlin, Paris, and Melbourne. Extensive evaluations demonstrate that MeshVPR achieves competitive performance with standard VPR pipelines, paving the way for mesh-based localization systems. Our contributions include the new task of citywide mesh-based VPR, the new benchmark datasets, MeshVPR, and a thorough analysis of open challenges. Data, code, and interactive visualizations are available at https://mesh-vpr.github.io","sentences":["Mesh-based scene representation offers a promising direction for simplifying large-scale hierarchical visual localization pipelines, combining a visual place recognition step based on global features (retrieval) and a visual localization step based on local features.","While existing work demonstrates the viability of meshes for visual localization, the impact of using synthetic databases rendered from them in visual place recognition remains largely unexplored.","In this work we investigate using dense 3D textured meshes for large-scale Visual Place Recognition (VPR) and identify a significant performance drop when using synthetic mesh-based databases compared to real-world images for retrieval.","To address this, we propose MeshVPR, a novel VPR pipeline that utilizes a lightweight features alignment framework to bridge the gap between real-world and synthetic domains.","MeshVPR leverages pre-trained VPR models and it is efficient and scalable for city-wide deployments.","We introduce novel datasets with freely available 3D meshes and manually collected queries from Berlin, Paris, and Melbourne.","Extensive evaluations demonstrate that MeshVPR achieves competitive performance with standard VPR pipelines, paving the way for mesh-based localization systems.","Our contributions include the new task of citywide mesh-based VPR, the new benchmark datasets, MeshVPR, and a thorough analysis of open challenges.","Data, code, and interactive visualizations are available at https://mesh-vpr.github.io"],"url":"http://arxiv.org/abs/2406.02776v1","category":"cs.CV"}
{"created":"2024-06-04 20:45:20","title":"Diagnostic Digital Twin for Anomaly Detection in Floating Offshore Wind Energy","abstract":"The demand for condition-based and predictive maintenance is rising across industries, especially for remote, high-value, and high-risk assets. In this article, the diagnostic digital twin concept is introduced, discussed, and implemented for a floating offshore turbine. A diagnostic digital twin is a virtual representation of an asset that combines real-time data and models to monitor damage, detect anomalies, and diagnose failures, thereby enabling condition-based and predictive maintenance. By applying diagnostic digital twins to offshore assets, unexpected failures can be alleviated, but the implementation can prove challenging. Here, a diagnostic digital twin is implemented for an operational floating offshore wind turbine. The asset is monitored through measurements. Unsupervised learning methods are employed to build a normal operation model, detect anomalies, and provide a fault diagnosis. Warnings and diagnoses are sent through text messages, and a more detailed diagnosis can be accessed in a virtual reality interface. The diagnostic digital twin successfully detected an anomaly with high confidence hours before a failure occurred. The paper concludes by discussing diagnostic digital twins in the broader context of offshore engineering. The presented approach can be generalized to other offshore assets to improve maintenance and increase the lifetime, efficiency, and sustainability of offshore assets.","sentences":["The demand for condition-based and predictive maintenance is rising across industries, especially for remote, high-value, and high-risk assets.","In this article, the diagnostic digital twin concept is introduced, discussed, and implemented for a floating offshore turbine.","A diagnostic digital twin is a virtual representation of an asset that combines real-time data and models to monitor damage, detect anomalies, and diagnose failures, thereby enabling condition-based and predictive maintenance.","By applying diagnostic digital twins to offshore assets, unexpected failures can be alleviated, but the implementation can prove challenging.","Here, a diagnostic digital twin is implemented for an operational floating offshore wind turbine.","The asset is monitored through measurements.","Unsupervised learning methods are employed to build a normal operation model, detect anomalies, and provide a fault diagnosis.","Warnings and diagnoses are sent through text messages, and a more detailed diagnosis can be accessed in a virtual reality interface.","The diagnostic digital twin successfully detected an anomaly with high confidence hours before a failure occurred.","The paper concludes by discussing diagnostic digital twins in the broader context of offshore engineering.","The presented approach can be generalized to other offshore assets to improve maintenance and increase the lifetime, efficiency, and sustainability of offshore assets."],"url":"http://arxiv.org/abs/2406.02775v1","category":"cs.LG"}
{"created":"2024-06-04 20:39:14","title":"Improved context-sensitive transformer model for inland vessel trajectory prediction","abstract":"Physics-related and model-based vessel trajectory prediction is highly accurate but requires specific knowledge of the vessel under consideration which is not always practical. Machine learning-based trajectory prediction models do not require expert knowledge, but rely on the implicit knowledge extracted from massive amounts of data. Several deep learning (DL) methods for vessel trajectory prediction have recently been suggested. The DL models developed typically only process information about the (dis)location of vessels defined with respect to a global reference system. In the context of inland navigation, this can be problematic, since without knowledge of the limited navigable space, irrealistic trajectories are likely to be determined. If spatial constraintes are introduced, e.g., by implementing an additional submodule to process map data, however, overall complexity increases. Instead of processing the vessel displacement information on the one hand and the spatial information on the other hand, the paper proposes the merging of both information. Here, fairway-related and navigation-related displacement information are used directly. In this way, the previously proposed context-sensitive Classification Transformer (CSCT) shows an improved spatial awareness. Additionally, the CSCT is adapted to assess the model uncertainty by enabling dropout during inference. This approach is trained on different inland waterways to analyze its generalizability. As the improved CSCT obtains lower prediction errors and enables to estimate the trustworthiness of each prediction, it is more suitable for safety-critical applications in inland navigation than previously developed models.","sentences":["Physics-related and model-based vessel trajectory prediction is highly accurate but requires specific knowledge of the vessel under consideration which is not always practical.","Machine learning-based trajectory prediction models do not require expert knowledge, but rely on the implicit knowledge extracted from massive amounts of data.","Several deep learning (DL) methods for vessel trajectory prediction have recently been suggested.","The DL models developed typically only process information about the (dis)location of vessels defined with respect to a global reference system.","In the context of inland navigation, this can be problematic, since without knowledge of the limited navigable space, irrealistic trajectories are likely to be determined.","If spatial constraintes are introduced, e.g., by implementing an additional submodule to process map data, however, overall complexity increases.","Instead of processing the vessel displacement information on the one hand and the spatial information on the other hand, the paper proposes the merging of both information.","Here, fairway-related and navigation-related displacement information are used directly.","In this way, the previously proposed context-sensitive Classification Transformer (CSCT) shows an improved spatial awareness.","Additionally, the CSCT is adapted to assess the model uncertainty by enabling dropout during inference.","This approach is trained on different inland waterways to analyze its generalizability.","As the improved CSCT obtains lower prediction errors and enables to estimate the trustworthiness of each prediction, it is more suitable for safety-critical applications in inland navigation than previously developed models."],"url":"http://arxiv.org/abs/2406.02771v1","category":"cs.LG"}
{"created":"2024-06-04 20:37:30","title":"Short-term Inland Vessel Trajectory Prediction with Encoder-Decoder Models","abstract":"Accurate vessel trajectory prediction is necessary for save and efficient navigation. Deep learning-based prediction models, esp. encoder-decoders, are rarely applied to inland navigation specifically. Approaches from the maritime domain cannot directly be transferred to river navigation due to specific driving behavior influencing factors. Different encoder-decoder architectures, including a transformer encoder-decoder, are compared herein for predicting the next positions of inland vessels, given not only spatio-temporal information from AIS, but also river specific features. The results show that the reformulation of the regression task as classification problem and the inclusion of river specific features yield the lowest displacement errors. The standard LSTM encoder-decoder outperforms the transformer encoder-decoder for the data considered, but is computationally more expensive. In this study for the first time a transformer-based encoder-decoder model is applied to the problem of predicting the ship trajectory. Here, a feature vector using the river-specific context of navigation input parameters is established. Future studies can built on the proposed models, investigate the improvement of the computationally more efficient transformer, e.g. through further hyper-parameter optimization, and use additional river-specific information in the context representation to further increase prediction accuracy.","sentences":["Accurate vessel trajectory prediction is necessary for save and efficient navigation.","Deep learning-based prediction models, esp.","encoder-decoders, are rarely applied to inland navigation specifically.","Approaches from the maritime domain cannot directly be transferred to river navigation due to specific driving behavior influencing factors.","Different encoder-decoder architectures, including a transformer encoder-decoder, are compared herein for predicting the next positions of inland vessels, given not only spatio-temporal information from AIS, but also river specific features.","The results show that the reformulation of the regression task as classification problem and the inclusion of river specific features yield the lowest displacement errors.","The standard LSTM encoder-decoder outperforms the transformer encoder-decoder for the data considered, but is computationally more expensive.","In this study for the first time a transformer-based encoder-decoder model is applied to the problem of predicting the ship trajectory.","Here, a feature vector using the river-specific context of navigation input parameters is established.","Future studies can built on the proposed models, investigate the improvement of the computationally more efficient transformer, e.g. through further hyper-parameter optimization, and use additional river-specific information in the context representation to further increase prediction accuracy."],"url":"http://arxiv.org/abs/2406.02770v1","category":"cs.LG"}
{"created":"2024-06-04 20:33:29","title":"Discovering Dynamic Symbolic Policies with Genetic Programming","abstract":"Artificial intelligence (AI) techniques are increasingly being applied to solve control problems. However, control systems developed in AI are often black-box methods, in that it is not clear how and why they generate their outputs. A lack of transparency can be problematic for control tasks in particular, because it complicates the identification of biases or errors, which in turn negatively influences the user's confidence in the system. To improve the interpretability and transparency in control systems, the black-box structure can be replaced with white-box symbolic policies described by mathematical expressions. Genetic programming offers a gradient-free method to optimise the structure of non-differentiable mathematical expressions. In this paper, we show that genetic programming can be used to discover symbolic control systems. This is achieved by learning a symbolic representation of a function that transforms observations into control signals. We consider both systems that implement static control policies without memory and systems that implement dynamic memory-based control policies. In case of the latter, the discovered function becomes the state equation of a differential equation, which allows for evidence integration. Our results show that symbolic policies are discovered that perform comparably with black-box policies on a variety of control tasks. Furthermore, the additional value of the memory capacity in the dynamic policies is demonstrated on experiments where static policies fall short. Overall, we demonstrate that white-box symbolic policies can be optimised with genetic programming, while offering interpretability and transparency that lacks in black-box models.","sentences":["Artificial intelligence (AI) techniques are increasingly being applied to solve control problems.","However, control systems developed in AI are often black-box methods, in that it is not clear how and why they generate their outputs.","A lack of transparency can be problematic for control tasks in particular, because it complicates the identification of biases or errors, which in turn negatively influences the user's confidence in the system.","To improve the interpretability and transparency in control systems, the black-box structure can be replaced with white-box symbolic policies described by mathematical expressions.","Genetic programming offers a gradient-free method to optimise the structure of non-differentiable mathematical expressions.","In this paper, we show that genetic programming can be used to discover symbolic control systems.","This is achieved by learning a symbolic representation of a function that transforms observations into control signals.","We consider both systems that implement static control policies without memory and systems that implement dynamic memory-based control policies.","In case of the latter, the discovered function becomes the state equation of a differential equation, which allows for evidence integration.","Our results show that symbolic policies are discovered that perform comparably with black-box policies on a variety of control tasks.","Furthermore, the additional value of the memory capacity in the dynamic policies is demonstrated on experiments where static policies fall short.","Overall, we demonstrate that white-box symbolic policies can be optimised with genetic programming, while offering interpretability and transparency that lacks in black-box models."],"url":"http://arxiv.org/abs/2406.02765v1","category":"cs.NE"}
{"created":"2024-06-04 20:33:22","title":"Adaptive Preference Scaling for Reinforcement Learning with Human Feedback","abstract":"Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data. Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs. In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength. By incorporating an adaptive scaling parameter into the loss for each pair, our method increases the flexibility of the reward function. Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards. Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm. Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO). Our experiments with robotic control and natural language generation with large language models (LLMs) show that our method not only improves policy performance but also aligns reward function selection more closely with policy optimization, simplifying the hyperparameter tuning process.","sentences":["Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data.","Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs.","In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength.","By incorporating an adaptive scaling parameter into the loss for each pair, our method increases the flexibility of the reward function.","Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards.","Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm.","Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO).","Our experiments with robotic control and natural language generation with large language models (LLMs) show that our method not only improves policy performance but also aligns reward function selection more closely with policy optimization, simplifying the hyperparameter tuning process."],"url":"http://arxiv.org/abs/2406.02764v1","category":"cs.LG"}
{"created":"2024-06-04 20:28:02","title":"Multi-layer Learnable Attention Mask for Multimodal Tasks","abstract":"While the Self-Attention mechanism in the Transformer model has proven to be effective in many domains, we observe that it is less effective in more diverse settings (e.g. multimodality) due to the varying granularity of each token and the high computational demands of lengthy sequences. To address the challenges, we introduce the Learnable Attention Mask (LAM), strategically designed to globally regulate attention maps and prioritize critical tokens within the sequence. Leveraging the Self-Attention module in a BERT-like transformer network, our approach adeptly captures associations between tokens. The extension of the LAM to a multi-layer version accommodates the varied information aspects embedded at each layer of the Transformer network. Comprehensive experimental validation on various datasets, such as MADv2, QVHighlights, ImageNet 1K, and MSRVTT, demonstrates the efficacy of the LAM, exemplifying its ability to enhance model performance while mitigating redundant computations. This pioneering approach presents a significant advancement in enhancing the understanding of complex scenarios, such as in movie understanding.","sentences":["While the Self-Attention mechanism in the Transformer model has proven to be effective in many domains, we observe that it is less effective in more diverse settings (e.g. multimodality) due to the varying granularity of each token and the high computational demands of lengthy sequences.","To address the challenges, we introduce the Learnable Attention Mask (LAM), strategically designed to globally regulate attention maps and prioritize critical tokens within the sequence.","Leveraging the Self-Attention module in a BERT-like transformer network, our approach adeptly captures associations between tokens.","The extension of the LAM to a multi-layer version accommodates the varied information aspects embedded at each layer of the Transformer network.","Comprehensive experimental validation on various datasets, such as MADv2, QVHighlights, ImageNet 1K, and MSRVTT, demonstrates the efficacy of the LAM, exemplifying its ability to enhance model performance while mitigating redundant computations.","This pioneering approach presents a significant advancement in enhancing the understanding of complex scenarios, such as in movie understanding."],"url":"http://arxiv.org/abs/2406.02761v1","category":"cs.CV"}
{"created":"2024-06-04 20:21:45","title":"Aligning Large Language Models via Fine-grained Supervision","abstract":"Pre-trained large-scale language models (LLMs) excel at producing coherent articles, yet their outputs may be untruthful, toxic, or fail to align with user expectations. Current approaches focus on using reinforcement learning with human feedback (RLHF) to improve model alignment, which works by transforming coarse human preferences of LLM outputs into a feedback signal that guides the model learning process. However, because this approach operates on sequence-level feedback, it lacks the precision to identify the exact parts of the output affecting user preferences. To address this gap, we propose a method to enhance LLM alignment through fine-grained token-level supervision. Specifically, we ask annotators to minimally edit less preferred responses within the standard reward modeling dataset to make them more favorable, ensuring changes are made only where necessary while retaining most of the original content. The refined dataset is used to train a token-level reward model, which is then used for training our fine-grained Proximal Policy Optimization (PPO) model. Our experiment results demonstrate that this approach can achieve up to an absolute improvement of $5.1\\%$ in LLM performance, in terms of win rate against the reference model, compared with the traditional PPO model.","sentences":["Pre-trained large-scale language models (LLMs) excel at producing coherent articles, yet their outputs may be untruthful, toxic, or fail to align with user expectations.","Current approaches focus on using reinforcement learning with human feedback (RLHF) to improve model alignment, which works by transforming coarse human preferences of LLM outputs into a feedback signal that guides the model learning process.","However, because this approach operates on sequence-level feedback, it lacks the precision to identify the exact parts of the output affecting user preferences.","To address this gap, we propose a method to enhance LLM alignment through fine-grained token-level supervision.","Specifically, we ask annotators to minimally edit less preferred responses within the standard reward modeling dataset to make them more favorable, ensuring changes are made only where necessary while retaining most of the original content.","The refined dataset is used to train a token-level reward model, which is then used for training our fine-grained Proximal Policy Optimization (PPO) model.","Our experiment results demonstrate that this approach can achieve up to an absolute improvement of $5.1\\%$ in LLM performance, in terms of win rate against the reference model, compared with the traditional PPO model."],"url":"http://arxiv.org/abs/2406.02756v1","category":"cs.CL"}
{"created":"2024-06-04 20:07:58","title":"Story Generation from Visual Inputs: Techniques, Related Tasks, and Challenges","abstract":"Creating engaging narratives from visual data is crucial for automated digital media consumption, assistive technologies, and interactive entertainment. This survey covers methodologies used in the generation of these narratives, focusing on their principles, strengths, and limitations.   The survey also covers tasks related to automatic story generation, such as image and video captioning, and visual question answering, as well as story generation without visual inputs. These tasks share common challenges with visual story generation and have served as inspiration for the techniques used in the field. We analyze the main datasets and evaluation metrics, providing a critical perspective on their limitations.","sentences":["Creating engaging narratives from visual data is crucial for automated digital media consumption, assistive technologies, and interactive entertainment.","This survey covers methodologies used in the generation of these narratives, focusing on their principles, strengths, and limitations.   ","The survey also covers tasks related to automatic story generation, such as image and video captioning, and visual question answering, as well as story generation without visual inputs.","These tasks share common challenges with visual story generation and have served as inspiration for the techniques used in the field.","We analyze the main datasets and evaluation metrics, providing a critical perspective on their limitations."],"url":"http://arxiv.org/abs/2406.02748v1","category":"cs.CV"}
{"created":"2024-06-04 19:06:49","title":"Predicting AI Agent Behavior through Approximation of the Perron-Frobenius Operator","abstract":"Predicting the behavior of AI-driven agents is particularly challenging without a preexisting model. In our paper, we address this by treating AI agents as nonlinear dynamical systems and adopting a probabilistic perspective to predict their statistical behavior using the Perron-Frobenius (PF) operator. We formulate the approximation of the PF operator as an entropy minimization problem, which can be solved by leveraging the Markovian property of the operator and decomposing its spectrum. Our data-driven methodology simultaneously approximates the PF operator to perform prediction of the evolution of the agents and also predicts the terminal probability density of AI agents, such as robotic systems and generative models. We demonstrate the effectiveness of our prediction model through extensive experiments on practical systems driven by AI algorithms.","sentences":["Predicting the behavior of AI-driven agents is particularly challenging without a preexisting model.","In our paper, we address this by treating AI agents as nonlinear dynamical systems and adopting a probabilistic perspective to predict their statistical behavior using the Perron-Frobenius (PF) operator.","We formulate the approximation of the PF operator as an entropy minimization problem, which can be solved by leveraging the Markovian property of the operator and decomposing its spectrum.","Our data-driven methodology simultaneously approximates the PF operator to perform prediction of the evolution of the agents and also predicts the terminal probability density of AI agents, such as robotic systems and generative models.","We demonstrate the effectiveness of our prediction model through extensive experiments on practical systems driven by AI algorithms."],"url":"http://arxiv.org/abs/2406.02723v1","category":"cs.AI"}
{"created":"2024-06-04 19:05:10","title":"Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller","abstract":"We propose Self-Control, a novel method utilizing suffix gradients to control the behavior of large language models (LLMs) without explicit human annotations. Given a guideline expressed in suffix string and the model's self-assessment of adherence, Self-Control computes the gradient of this self-judgment concerning the model's hidden states, directly influencing the auto-regressive generation process towards desired behaviors. To enhance efficiency, we introduce Self-Control_{prefix}, a compact module that encapsulates the learned representations from suffix gradients into a Prefix Controller, facilitating inference-time control for various LLM behaviors. Our experiments demonstrate Self-Control's efficacy across multiple domains, including emotional modulation, ensuring harmlessness, and enhancing complex reasoning. Especially, Self-Control_{prefix} enables a plug-and-play control and jointly controls multiple attributes, improving model outputs without altering model parameters or increasing inference-time costs.","sentences":["We propose Self-Control, a novel method utilizing suffix gradients to control the behavior of large language models (LLMs) without explicit human annotations.","Given a guideline expressed in suffix string and the model's self-assessment of adherence, Self-Control computes the gradient of this self-judgment concerning the model's hidden states, directly influencing the auto-regressive generation process towards desired behaviors.","To enhance efficiency, we introduce Self-Control_{prefix}, a compact module that encapsulates the learned representations from suffix gradients into a Prefix Controller, facilitating inference-time control for various LLM behaviors.","Our experiments demonstrate Self-Control's efficacy across multiple domains, including emotional modulation, ensuring harmlessness, and enhancing complex reasoning.","Especially, Self-Control_{prefix} enables a plug-and-play control and jointly controls multiple attributes, improving model outputs without altering model parameters or increasing inference-time costs."],"url":"http://arxiv.org/abs/2406.02721v1","category":"cs.CL"}
{"created":"2024-06-04 17:59:49","title":"Neural Representations of Dynamic Visual Stimuli","abstract":"Humans experience the world through constantly changing visual stimuli, where scenes can shift and move, change in appearance, and vary in distance. The dynamic nature of visual perception is a fundamental aspect of our daily lives, yet the large majority of research on object and scene processing, particularly using fMRI, has focused on static stimuli. While studies of static image perception are attractive due to their computational simplicity, they impose a strong non-naturalistic constraint on our investigation of human vision. In contrast, dynamic visual stimuli offer a more ecologically-valid approach but present new challenges due to the interplay between spatial and temporal information, making it difficult to disentangle the representations of stable image features and motion. To overcome this limitation -- given dynamic inputs, we explicitly decouple the modeling of static image representations and motion representations in the human brain. Three results demonstrate the feasibility of this approach. First, we show that visual motion information as optical flow can be predicted (or decoded) from brain activity as measured by fMRI. Second, we show that this predicted motion can be used to realistically animate static images using a motion-conditioned video diffusion model (where the motion is driven by fMRI brain activity). Third, we show prediction in the reverse direction: existing video encoders can be fine-tuned to predict fMRI brain activity from video imagery, and can do so more effectively than image encoders. This foundational work offers a novel, extensible framework for interpreting how the human brain processes dynamic visual information.","sentences":["Humans experience the world through constantly changing visual stimuli, where scenes can shift and move, change in appearance, and vary in distance.","The dynamic nature of visual perception is a fundamental aspect of our daily lives, yet the large majority of research on object and scene processing, particularly using fMRI, has focused on static stimuli.","While studies of static image perception are attractive due to their computational simplicity, they impose a strong non-naturalistic constraint on our investigation of human vision.","In contrast, dynamic visual stimuli offer a more ecologically-valid approach but present new challenges due to the interplay between spatial and temporal information, making it difficult to disentangle the representations of stable image features and motion.","To overcome this limitation -- given dynamic inputs, we explicitly decouple the modeling of static image representations and motion representations in the human brain.","Three results demonstrate the feasibility of this approach.","First, we show that visual motion information as optical flow can be predicted (or decoded) from brain activity as measured by fMRI.","Second, we show that this predicted motion can be used to realistically animate static images using a motion-conditioned video diffusion model (where the motion is driven by fMRI brain activity).","Third, we show prediction in the reverse direction: existing video encoders can be fine-tuned to predict fMRI brain activity from video imagery, and can do so more effectively than image encoders.","This foundational work offers a novel, extensible framework for interpreting how the human brain processes dynamic visual information."],"url":"http://arxiv.org/abs/2406.02659v1","category":"q-bio.NC"}
{"created":"2024-06-04 17:45:26","title":"Block Transformer: Global-to-Local Language Modeling for Fast Inference","abstract":"This paper presents the Block Transformer architecture which adopts hierarchical global-to-local modeling to autoregressive transformers to mitigate the inference bottlenecks of self-attention. To apply self-attention, the key-value (KV) cache of all previous sequences must be retrieved from memory at every decoding step. Thereby, this KV cache IO becomes a significant bottleneck in batch inference. We notice that these costs stem from applying self-attention on the global context, therefore we isolate the expensive bottlenecks of global modeling to lower layers and apply fast local modeling in upper layers. To mitigate the remaining costs in the lower layers, we aggregate input tokens into fixed size blocks and then apply self-attention at this coarse level. Context information is aggregated into a single embedding to enable upper layers to decode the next block of tokens, without global attention. Free of global attention bottlenecks, the upper layers can fully utilize the compute hardware to maximize inference throughput. By leveraging global and local modules, the Block Transformer architecture demonstrates 10-20x gains in inference throughput compared to vanilla transformers with equivalent perplexity. Our work introduces a new approach to optimize language model inference through novel application of global-to-local modeling. Code is available at https://github.com/itsnamgyu/block-transformer.","sentences":["This paper presents the Block Transformer architecture which adopts hierarchical global-to-local modeling to autoregressive transformers to mitigate the inference bottlenecks of self-attention.","To apply self-attention, the key-value (KV) cache of all previous sequences must be retrieved from memory at every decoding step.","Thereby, this KV cache IO becomes a significant bottleneck in batch inference.","We notice that these costs stem from applying self-attention on the global context, therefore we isolate the expensive bottlenecks of global modeling to lower layers and apply fast local modeling in upper layers.","To mitigate the remaining costs in the lower layers, we aggregate input tokens into fixed size blocks and then apply self-attention at this coarse level.","Context information is aggregated into a single embedding to enable upper layers to decode the next block of tokens, without global attention.","Free of global attention bottlenecks, the upper layers can fully utilize the compute hardware to maximize inference throughput.","By leveraging global and local modules, the Block Transformer architecture demonstrates 10-20x gains in inference throughput compared to vanilla transformers with equivalent perplexity.","Our work introduces a new approach to optimize language model inference through novel application of global-to-local modeling.","Code is available at https://github.com/itsnamgyu/block-transformer."],"url":"http://arxiv.org/abs/2406.02657v1","category":"cs.CL"}
{"created":"2024-06-04 16:39:02","title":"kNN Classification of Malware Data Dependency Graph Features","abstract":"Feature resolution impacts the ability of classifiers to make explainable inferences when applied to malware classification. We explore classification based on features constructed from data dependency graphs, and present results from k-Nearest Neighbors (kNN) classifiers. Our study demonstrates that classification based on a novel feature representation not only yields high accuracy, but also increases explainability in inference, as features of data dependency are directly representative of program behavior. We present classification results using the Microsoft Kaggle 2015 malware dataset which was processed with a novel approach to feature extraction and representation. We show that non-parametric approaches to classification in the metric space are able to obtain classification accuracy of 87.5\\% when applied to multi-class classification in the Kaggle malware dataset. Additionally, similarity in the metric space can be calculated directly without prior training. Our results provide evidence that data dependency graphs accurately capture both semantic and structural information.","sentences":["Feature resolution impacts the ability of classifiers to make explainable inferences when applied to malware classification.","We explore classification based on features constructed from data dependency graphs, and present results from k-Nearest Neighbors (kNN) classifiers.","Our study demonstrates that classification based on a novel feature representation not only yields high accuracy, but also increases explainability in inference, as features of data dependency are directly representative of program behavior.","We present classification results using the Microsoft Kaggle 2015 malware dataset which was processed with a novel approach to feature extraction and representation.","We show that non-parametric approaches to classification in the metric space are able to obtain classification accuracy of 87.5\\% when applied to multi-class classification in the Kaggle malware dataset.","Additionally, similarity in the metric space can be calculated directly without prior training.","Our results provide evidence that data dependency graphs accurately capture both semantic and structural information."],"url":"http://arxiv.org/abs/2406.02654v1","category":"cs.CR"}
{"created":"2024-06-04 16:38:11","title":"Pancreatic Tumor Segmentation as Anomaly Detection in CT Images Using Denoising Diffusion Models","abstract":"Despite the advances in medicine, cancer has remained a formidable challenge. Particularly in the case of pancreatic tumors, characterized by their diversity and late diagnosis, early detection poses a significant challenge crucial for effective treatment. The advancement of deep learning techniques, particularly supervised algorithms, has significantly propelled pancreatic tumor detection in the medical field. However, supervised deep learning approaches necessitate extensive labeled medical images for training, yet acquiring such annotations is both limited and costly. Conversely, weakly supervised anomaly detection methods, requiring only image-level annotations, have garnered interest. Existing methodologies predominantly hinge on generative adversarial networks (GANs) or autoencoder models, which can pose complexity in training and, these models may face difficulties in accurately preserving fine image details. This research presents a novel approach to pancreatic tumor detection, employing weak supervision anomaly detection through denoising diffusion algorithms. By incorporating a deterministic iterative process of adding and removing noise along with classifier guidance, the method enables seamless translation of images between diseased and healthy subjects, resulting in detailed anomaly maps without requiring complex training protocols and segmentation masks. This study explores denoising diffusion models as a recent advancement over traditional generative models like GANs, contributing to the field of pancreatic tumor detection. Recognizing the low survival rates of pancreatic cancer, this study emphasizes the need for continued research to leverage diffusion models' efficiency in medical segmentation tasks.","sentences":["Despite the advances in medicine, cancer has remained a formidable challenge.","Particularly in the case of pancreatic tumors, characterized by their diversity and late diagnosis, early detection poses a significant challenge crucial for effective treatment.","The advancement of deep learning techniques, particularly supervised algorithms, has significantly propelled pancreatic tumor detection in the medical field.","However, supervised deep learning approaches necessitate extensive labeled medical images for training, yet acquiring such annotations is both limited and costly.","Conversely, weakly supervised anomaly detection methods, requiring only image-level annotations, have garnered interest.","Existing methodologies predominantly hinge on generative adversarial networks (GANs) or autoencoder models, which can pose complexity in training and, these models may face difficulties in accurately preserving fine image details.","This research presents a novel approach to pancreatic tumor detection, employing weak supervision anomaly detection through denoising diffusion algorithms.","By incorporating a deterministic iterative process of adding and removing noise along with classifier guidance, the method enables seamless translation of images between diseased and healthy subjects, resulting in detailed anomaly maps without requiring complex training protocols and segmentation masks.","This study explores denoising diffusion models as a recent advancement over traditional generative models like GANs, contributing to the field of pancreatic tumor detection.","Recognizing the low survival rates of pancreatic cancer, this study emphasizes the need for continued research to leverage diffusion models' efficiency in medical segmentation tasks."],"url":"http://arxiv.org/abs/2406.02653v1","category":"eess.IV"}
{"created":"2024-06-04 16:14:19","title":"RepCNN: Micro-sized, Mighty Models for Wakeword Detection","abstract":"Always-on machine learning models require a very low memory and compute footprint. Their restricted parameter count limits the model's capacity to learn, and the effectiveness of the usual training algorithms to find the best parameters. Here we show that a small convolutional model can be better trained by first refactoring its computation into a larger redundant multi-branched architecture. Then, for inference, we algebraically re-parameterize the trained model into the single-branched form with fewer parameters for a lower memory footprint and compute cost. Using this technique, we show that our always-on wake-word detector model, RepCNN, provides a good trade-off between latency and accuracy during inference. RepCNN re-parameterized models are 43% more accurate than a uni-branch convolutional model while having the same runtime. RepCNN also meets the accuracy of complex architectures like BC-ResNet, while having 2x lesser peak memory usage and 10x faster runtime.","sentences":["Always-on machine learning models require a very low memory and compute footprint.","Their restricted parameter count limits the model's capacity to learn, and the effectiveness of the usual training algorithms to find the best parameters.","Here we show that a small convolutional model can be better trained by first refactoring its computation into a larger redundant multi-branched architecture.","Then, for inference, we algebraically re-parameterize the trained model into the single-branched form with fewer parameters for a lower memory footprint and compute cost.","Using this technique, we show that our always-on wake-word detector model, RepCNN, provides a good trade-off between latency and accuracy during inference.","RepCNN re-parameterized models are 43% more accurate than a uni-branch convolutional model while having the same runtime.","RepCNN also meets the accuracy of complex architectures like BC-ResNet, while having 2x lesser peak memory usage and 10x faster runtime."],"url":"http://arxiv.org/abs/2406.02652v1","category":"eess.AS"}
{"created":"2024-06-04 15:39:41","title":"RoutePlacer: An End-to-End Routability-Aware Placer with Graph Neural Network","abstract":"Placement is a critical and challenging step of modern chip design, with routability being an essential indicator of placement quality. Current routability-oriented placers typically apply an iterative two-stage approach, wherein the first stage generates a placement solution, and the second stage provides non-differentiable routing results to heuristically improve the solution quality. This method hinders jointly optimizing the routability aspect during placement. To address this problem, this work introduces RoutePlacer, an end-to-end routability-aware placement method. It trains RouteGNN, a customized graph neural network, to efficiently and accurately predict routability by capturing and fusing geometric and topological representations of placements. Well-trained RouteGNN then serves as a differentiable approximation of routability, enabling end-to-end gradient-based routability optimization. In addition, RouteGNN can improve two-stage placers as a plug-and-play alternative to external routers. Our experiments on DREAMPlace, an open-source AI4EDA platform, show that RoutePlacer can reduce Total Overflow by up to 16% while maintaining routed wirelength, compared to the state-of-the-art; integrating RouteGNN within two-stage placers leads to a 44% reduction in Total Overflow without compromising wirelength.","sentences":["Placement is a critical and challenging step of modern chip design, with routability being an essential indicator of placement quality.","Current routability-oriented placers typically apply an iterative two-stage approach, wherein the first stage generates a placement solution, and the second stage provides non-differentiable routing results to heuristically improve the solution quality.","This method hinders jointly optimizing the routability aspect during placement.","To address this problem, this work introduces RoutePlacer, an end-to-end routability-aware placement method.","It trains RouteGNN, a customized graph neural network, to efficiently and accurately predict routability by capturing and fusing geometric and topological representations of placements.","Well-trained RouteGNN then serves as a differentiable approximation of routability, enabling end-to-end gradient-based routability optimization.","In addition, RouteGNN can improve two-stage placers as a plug-and-play alternative to external routers.","Our experiments on DREAMPlace, an open-source AI4EDA platform, show that RoutePlacer can reduce Total Overflow by up to 16% while maintaining routed wirelength, compared to the state-of-the-art; integrating RouteGNN within two-stage placers leads to a 44% reduction in Total Overflow without compromising wirelength."],"url":"http://arxiv.org/abs/2406.02651v1","category":"cs.LG"}
{"created":"2024-06-04 15:37:14","title":"Structure-based Drug Design Benchmark: Do 3D Methods Really Dominate?","abstract":"Currently, the field of structure-based drug design is dominated by three main types of algorithms: search-based algorithms, deep generative models, and reinforcement learning. While existing works have typically focused on comparing models within a single algorithmic category, cross-algorithm comparisons remain scarce. In this paper, to fill the gap, we establish a benchmark to evaluate the performance of sixteen models across these different algorithmic foundations by assessing the pharmaceutical properties of the generated molecules and their docking affinities with specified target proteins. We highlight the unique advantages of each algorithmic approach and offer recommendations for the design of future SBDD models. We emphasize that 1D/2D ligand-centric drug design methods can be used in SBDD by treating the docking function as a black-box oracle, which is typically neglected. The empirical results show that 1D/2D methods achieve competitive performance compared with 3D-based methods that use the 3D structure of the target protein explicitly. Also, AutoGrow4, a 2D molecular graph-based genetic algorithm, dominates SBDD in terms of optimization ability. The relevant code is available in https://github.com/zkysfls/2024-sbdd-benchmark.","sentences":["Currently, the field of structure-based drug design is dominated by three main types of algorithms: search-based algorithms, deep generative models, and reinforcement learning.","While existing works have typically focused on comparing models within a single algorithmic category, cross-algorithm comparisons remain scarce.","In this paper, to fill the gap, we establish a benchmark to evaluate the performance of sixteen models across these different algorithmic foundations by assessing the pharmaceutical properties of the generated molecules and their docking affinities with specified target proteins.","We highlight the unique advantages of each algorithmic approach and offer recommendations for the design of future SBDD models.","We emphasize that 1D/2D ligand-centric drug design methods can be used in SBDD by treating the docking function as a black-box oracle, which is typically neglected.","The empirical results show that 1D/2D methods achieve competitive performance compared with 3D-based methods that use the 3D structure of the target protein explicitly.","Also, AutoGrow4, a 2D molecular graph-based genetic algorithm, dominates SBDD in terms of optimization ability.","The relevant code is available in https://github.com/zkysfls/2024-sbdd-benchmark."],"url":"http://arxiv.org/abs/2406.03403v1","category":"cs.LG"}
{"created":"2024-06-04 15:35:08","title":"By Fair Means or Foul: Quantifying Collusion in a Market Simulation with Deep Reinforcement Learning","abstract":"In the rapidly evolving landscape of eCommerce, Artificial Intelligence (AI) based pricing algorithms, particularly those utilizing Reinforcement Learning (RL), are becoming increasingly prevalent. This rise has led to an inextricable pricing situation with the potential for market collusion. Our research employs an experimental oligopoly model of repeated price competition, systematically varying the environment to cover scenarios from basic economic theory to subjective consumer demand preferences. We also introduce a novel demand framework that enables the implementation of various demand models, allowing for a weighted blending of different models. In contrast to existing research in this domain, we aim to investigate the strategies and emerging pricing patterns developed by the agents, which may lead to a collusive outcome. Furthermore, we investigate a scenario where agents cannot observe their competitors' prices. Finally, we provide a comprehensive legal analysis across all scenarios. Our findings indicate that RL-based AI agents converge to a collusive state characterized by the charging of supracompetitive prices, without necessarily requiring inter-agent communication. Implementing alternative RL algorithms, altering the number of agents or simulation settings, and restricting the scope of the agents' observation space does not significantly impact the collusive market outcome behavior.","sentences":["In the rapidly evolving landscape of eCommerce, Artificial Intelligence (AI) based pricing algorithms, particularly those utilizing Reinforcement Learning (RL), are becoming increasingly prevalent.","This rise has led to an inextricable pricing situation with the potential for market collusion.","Our research employs an experimental oligopoly model of repeated price competition, systematically varying the environment to cover scenarios from basic economic theory to subjective consumer demand preferences.","We also introduce a novel demand framework that enables the implementation of various demand models, allowing for a weighted blending of different models.","In contrast to existing research in this domain, we aim to investigate the strategies and emerging pricing patterns developed by the agents, which may lead to a collusive outcome.","Furthermore, we investigate a scenario where agents cannot observe their competitors' prices.","Finally, we provide a comprehensive legal analysis across all scenarios.","Our findings indicate that RL-based AI agents converge to a collusive state characterized by the charging of supracompetitive prices, without necessarily requiring inter-agent communication.","Implementing alternative RL algorithms, altering the number of agents or simulation settings, and restricting the scope of the agents' observation space does not significantly impact the collusive market outcome behavior."],"url":"http://arxiv.org/abs/2406.02650v1","category":"cs.LG"}
{"created":"2024-06-05 17:52:21","title":"A self-aligning recirculated crossed optical dipole trap for lithium atoms","abstract":"Crossed optical dipole traps (ODTs) provide three-dimensional confinement of cold atoms and other optically trappable particles. However, the need to maintain the intersection of the two trapping beams poses strict requirements on alignment stability, and limits the ability to move the trap. Here we demonstrate a novel crossed ODT design that features inherent stability of the beam crossing, allowing the trap to move and remain aligned. The trap consists of a single high-power laser beam, imaged back onto itself at an angle to form a crossed trap. Self-aligning behavior results from employing an imaging system with positive magnification tuned precisely to unity. We employ laser-cooled samples of $^6$Li atoms to demonstrate that the trap remains well-aligned over a 4.3 mm travel range along an axis approximately perpendicular to the plane containing the crossed beams. Our scheme can be applied to bring an atomic cloud held in a crossed ODT close to a surface or field source for various applications in quantum simulation, sensing, and information processing.","sentences":["Crossed optical dipole traps (ODTs) provide three-dimensional confinement of cold atoms and other optically trappable particles.","However, the need to maintain the intersection of the two trapping beams poses strict requirements on alignment stability, and limits the ability to move the trap.","Here we demonstrate a novel crossed ODT design that features inherent stability of the beam crossing, allowing the trap to move and remain aligned.","The trap consists of a single high-power laser beam, imaged back onto itself at an angle to form a crossed trap.","Self-aligning behavior results from employing an imaging system with positive magnification tuned precisely to unity.","We employ laser-cooled samples of $^6$Li atoms to demonstrate that the trap remains well-aligned over a 4.3 mm travel range along an axis approximately perpendicular to the plane containing the crossed beams.","Our scheme can be applied to bring an atomic cloud held in a crossed ODT close to a surface or field source for various applications in quantum simulation, sensing, and information processing."],"url":"http://arxiv.org/abs/2406.03489v1","category":"physics.atom-ph"}
{"created":"2024-06-05 17:25:29","title":"Solving Differential Equations using Physics-Informed Deep Equilibrium Models","abstract":"This paper introduces Physics-Informed Deep Equilibrium Models (PIDEQs) for solving initial value problems (IVPs) of ordinary differential equations (ODEs). Leveraging recent advancements in deep equilibrium models (DEQs) and physics-informed neural networks (PINNs), PIDEQs combine the implicit output representation of DEQs with physics-informed training techniques. We validate PIDEQs using the Van der Pol oscillator as a benchmark problem, demonstrating their efficiency and effectiveness in solving IVPs. Our analysis includes key hyperparameter considerations for optimizing PIDEQ performance. By bridging deep learning and physics-based modeling, this work advances computational techniques for solving IVPs, with implications for scientific computing and engineering applications.","sentences":["This paper introduces Physics-Informed Deep Equilibrium Models (PIDEQs) for solving initial value problems (IVPs) of ordinary differential equations (ODEs).","Leveraging recent advancements in deep equilibrium models (DEQs) and physics-informed neural networks (PINNs), PIDEQs combine the implicit output representation of DEQs with physics-informed training techniques.","We validate PIDEQs using the Van der Pol oscillator as a benchmark problem, demonstrating their efficiency and effectiveness in solving IVPs.","Our analysis includes key hyperparameter considerations for optimizing PIDEQ performance.","By bridging deep learning and physics-based modeling, this work advances computational techniques for solving IVPs, with implications for scientific computing and engineering applications."],"url":"http://arxiv.org/abs/2406.03472v1","category":"cs.LG"}
{"created":"2024-06-05 17:23:34","title":"Unifying atoms and colloids near the glass transition through bond-order topology","abstract":"In this combined experimental and simulation study, we utilize bond-order topology to quantitatively match particle volume fraction in mechanically uniformly compressed colloidal suspensions with temperature in atomistic simulations. The obtained mapping temperature is above the dynamical glass transition temperature, indicating that the colloidal systems examined are structurally most like simulated undercooled liquids. Furthermore, the structural mapping procedure offers a unifying framework for quantifying relaxation in arrested colloidal systems.","sentences":["In this combined experimental and simulation study, we utilize bond-order topology to quantitatively match particle volume fraction in mechanically uniformly compressed colloidal suspensions with temperature in atomistic simulations.","The obtained mapping temperature is above the dynamical glass transition temperature, indicating that the colloidal systems examined are structurally most like simulated undercooled liquids.","Furthermore, the structural mapping procedure offers a unifying framework for quantifying relaxation in arrested colloidal systems."],"url":"http://arxiv.org/abs/2406.03469v1","category":"cond-mat.soft"}
{"created":"2024-06-05 17:12:38","title":"Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach","abstract":"Graph Neural Networks (GNNs) have proven to be highly effective for node classification tasks across diverse graph structural patterns. Traditionally, GNNs employ a uniform global filter, typically a low-pass filter for homophilic graphs and a high-pass filter for heterophilic graphs. However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, rendering a single global filter approach suboptimal. In this work, we theoretically demonstrate that a global filter optimized for one pattern can adversely affect performance on nodes with differing patterns. To address this, we introduce a novel GNN framework Node-MoE that utilizes a mixture of experts to adaptively select the appropriate filters for different nodes. Extensive experiments demonstrate the effectiveness of Node-MoE on both homophilic and heterophilic graphs.","sentences":["Graph Neural Networks (GNNs) have proven to be highly effective for node classification tasks across diverse graph structural patterns.","Traditionally, GNNs employ a uniform global filter, typically a low-pass filter for homophilic graphs and a high-pass filter for heterophilic graphs.","However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, rendering a single global filter approach suboptimal.","In this work, we theoretically demonstrate that a global filter optimized for one pattern can adversely affect performance on nodes with differing patterns.","To address this, we introduce a novel GNN framework Node-MoE that utilizes a mixture of experts to adaptively select the appropriate filters for different nodes.","Extensive experiments demonstrate the effectiveness of Node-MoE on both homophilic and heterophilic graphs."],"url":"http://arxiv.org/abs/2406.03464v1","category":"cs.LG"}
{"created":"2024-06-05 17:07:24","title":"LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection","abstract":"In this paper, we present a light-weight detection transformer, LW-DETR, which outperforms YOLOs for real-time object detection. The architecture is a simple stack of a ViT encoder, a projector, and a shallow DETR decoder. Our approach leverages recent advanced techniques, such as training-effective techniques, e.g., improved loss and pretraining, and interleaved window and global attentions for reducing the ViT encoder complexity. We improve the ViT encoder by aggregating multi-level feature maps, and the intermediate and final feature maps in the ViT encoder, forming richer feature maps, and introduce window-major feature map organization for improving the efficiency of interleaved attention computation. Experimental results demonstrate that the proposed approach is superior over existing real-time detectors, e.g., YOLO and its variants, on COCO and other benchmark datasets. Code and models are available at (https://github.com/Atten4Vis/LW-DETR).","sentences":["In this paper, we present a light-weight detection transformer, LW-DETR, which outperforms YOLOs for real-time object detection.","The architecture is a simple stack of a ViT encoder, a projector, and a shallow DETR decoder.","Our approach leverages recent advanced techniques, such as training-effective techniques, e.g., improved loss and pretraining, and interleaved window and global attentions for reducing the ViT encoder complexity.","We improve the ViT encoder by aggregating multi-level feature maps, and the intermediate and final feature maps in the ViT encoder, forming richer feature maps, and introduce window-major feature map organization for improving the efficiency of interleaved attention computation.","Experimental results demonstrate that the proposed approach is superior over existing real-time detectors, e.g., YOLO and its variants, on COCO and other benchmark datasets.","Code and models are available at (https://github.com/Atten4Vis/LW-DETR)."],"url":"http://arxiv.org/abs/2406.03459v1","category":"cs.CV"}
{"created":"2024-06-05 16:29:03","title":"Computation-Efficient Era: A Comprehensive Survey of State Space Models in Medical Image Analysis","abstract":"Sequence modeling plays a vital role across various domains, with recurrent neural networks being historically the predominant method of performing these tasks. However, the emergence of transformers has altered this paradigm due to their superior performance. Built upon these advances, transformers have conjoined CNNs as two leading foundational models for learning visual representations. However, transformers are hindered by the $\\mathcal{O}(N^2)$ complexity of their attention mechanisms, while CNNs lack global receptive fields and dynamic weight allocation. State Space Models (SSMs), specifically the \\textit{\\textbf{Mamba}} model with selection mechanisms and hardware-aware architecture, have garnered immense interest lately in sequential modeling and visual representation learning, challenging the dominance of transformers by providing infinite context lengths and offering substantial efficiency maintaining linear complexity in the input sequence. Capitalizing on the advances in computer vision, medical imaging has heralded a new epoch with Mamba models. Intending to help researchers navigate the surge, this survey seeks to offer an encyclopedic review of Mamba models in medical imaging. Specifically, we start with a comprehensive theoretical review forming the basis of SSMs, including Mamba architecture and its alternatives for sequence modeling paradigms in this context. Next, we offer a structured classification of Mamba models in the medical field and introduce a diverse categorization scheme based on their application, imaging modalities, and targeted organs. Finally, we summarize key challenges, discuss different future research directions of the SSMs in the medical domain, and propose several directions to fulfill the demands of this field. In addition, we have compiled the studies discussed in this paper along with their open-source implementations on our GitHub repository.","sentences":["Sequence modeling plays a vital role across various domains, with recurrent neural networks being historically the predominant method of performing these tasks.","However, the emergence of transformers has altered this paradigm due to their superior performance.","Built upon these advances, transformers have conjoined CNNs as two leading foundational models for learning visual representations.","However, transformers are hindered by the $\\mathcal{O}(N^2)$ complexity of their attention mechanisms, while CNNs lack global receptive fields and dynamic weight allocation.","State Space Models (SSMs), specifically the \\textit{\\textbf{Mamba}} model with selection mechanisms and hardware-aware architecture, have garnered immense interest lately in sequential modeling and visual representation learning, challenging the dominance of transformers by providing infinite context lengths and offering substantial efficiency maintaining linear complexity in the input sequence.","Capitalizing on the advances in computer vision, medical imaging has heralded a new epoch with Mamba models.","Intending to help researchers navigate the surge, this survey seeks to offer an encyclopedic review of Mamba models in medical imaging.","Specifically, we start with a comprehensive theoretical review forming the basis of SSMs, including Mamba architecture and its alternatives for sequence modeling paradigms in this context.","Next, we offer a structured classification of Mamba models in the medical field and introduce a diverse categorization scheme based on their application, imaging modalities, and targeted organs.","Finally, we summarize key challenges, discuss different future research directions of the SSMs in the medical domain, and propose several directions to fulfill the demands of this field.","In addition, we have compiled the studies discussed in this paper along with their open-source implementations on our GitHub repository."],"url":"http://arxiv.org/abs/2406.03430v1","category":"eess.IV"}
{"created":"2024-06-05 16:20:52","title":"Computational lower bounds for multi-frequency group synchronization","abstract":"We consider a group synchronization problem with multiple frequencies which involves observing pairwise relative measurements of group elements on multiple frequency channels, corrupted by Gaussian noise. We study the computational phase transition in the problem of detecting whether a structured signal is present in such observations by analyzing low-degree polynomial algorithms. We show that, assuming the low-degree conjecture, in synchronization models over arbitrary finite groups as well as over the circle group $SO(2)$, a simple spectral algorithm is optimal among algorithms of runtime $\\exp(\\tilde{\\Omega}(n^{1/3}))$ for detection from an observation including a constant number of frequencies. Combined with an upper bound for the statistical threshold shown in Perry et al., our results indicate the presence of a statistical-to-computational gap in such models with a sufficiently large number of frequencies.","sentences":["We consider a group synchronization problem with multiple frequencies which involves observing pairwise relative measurements of group elements on multiple frequency channels, corrupted by Gaussian noise.","We study the computational phase transition in the problem of detecting whether a structured signal is present in such observations by analyzing low-degree polynomial algorithms.","We show that, assuming the low-degree conjecture, in synchronization models over arbitrary finite groups as well as over the circle group $SO(2)$, a simple spectral algorithm is optimal among algorithms of runtime $\\exp(\\tilde{\\Omega}(n^{1/3}))$ for detection from an observation including a constant number of frequencies.","Combined with an upper bound for the statistical threshold shown in Perry et al., our results indicate the presence of a statistical-to-computational gap in such models with a sufficiently large number of frequencies."],"url":"http://arxiv.org/abs/2406.03424v1","category":"math.ST"}
{"created":"2024-06-05 16:15:53","title":"Dynamic properties of a class of van der Pol-Duffing oscillators","abstract":"In this paper, we study the existence of bifurcation of a van der Pol-Duffing oscillator with quintic terms and its quasi-periodic solutions by means of qualitative and bifurcation theories. Firstly, we analyze the autonomous system and find that it has two kinds of local bifurcations and a global bifurcation: pitchfork bifurcation, Hopf bifurcation, homoclinic bifurcation. It is worth noting that the disappearance of the homoclinic orbit is synchronized with the emergence of a large limit cycle. Then, by discussing the stability of equilibria at infinity and the orientation of the trajectory, the existence and stability of limit circles of the autonomous system are analyzed by combining the Poincar\\'{e}-Bendixson theorem and the index theory. The global phase portrait and the numerical simulation of the autonomous system in different parameter values are given. Finally, the existence of periodic and quasi-periodic solutions to periodic forced system is proved by a KAM theorem.","sentences":["In this paper, we study the existence of bifurcation of a van der Pol-Duffing oscillator with quintic terms and its quasi-periodic solutions by means of qualitative and bifurcation theories.","Firstly, we analyze the autonomous system and find that it has two kinds of local bifurcations and a global bifurcation: pitchfork bifurcation, Hopf bifurcation, homoclinic bifurcation.","It is worth noting that the disappearance of the homoclinic orbit is synchronized with the emergence of a large limit cycle.","Then, by discussing the stability of equilibria at infinity and the orientation of the trajectory, the existence and stability of limit circles of the autonomous system are analyzed by combining the Poincar\\'{e}-Bendixson theorem and the index theory.","The global phase portrait and the numerical simulation of the autonomous system in different parameter values are given.","Finally, the existence of periodic and quasi-periodic solutions to periodic forced system is proved by a KAM theorem."],"url":"http://arxiv.org/abs/2406.03420v1","category":"math.DS"}
{"created":"2024-06-05 16:12:19","title":"CoFie: Learning Compact Neural Surface Representations with Coordinate Fields","abstract":"This paper introduces CoFie, a novel local geometry-aware neural surface representation. CoFie is motivated by the theoretical analysis of local SDFs with quadratic approximation. We find that local shapes are highly compressive in an aligned coordinate frame defined by the normal and tangent directions of local shapes. Accordingly, we introduce Coordinate Field, which is a composition of coordinate frames of all local shapes. The Coordinate Field is optimizable and is used to transform the local shapes from the world coordinate frame to the aligned shape coordinate frame. It largely reduces the complexity of local shapes and benefits the learning of MLP-based implicit representations. Moreover, we introduce quadratic layers into the MLP to enhance expressiveness concerning local shape geometry. CoFie is a generalizable surface representation. It is trained on a curated set of 3D shapes and works on novel shape instances during testing. When using the same amount of parameters with prior works, CoFie reduces the shape error by 48% and 56% on novel instances of both training and unseen shape categories. Moreover, CoFie demonstrates comparable performance to prior works when using only 70% fewer parameters.","sentences":["This paper introduces CoFie, a novel local geometry-aware neural surface representation.","CoFie is motivated by the theoretical analysis of local SDFs with quadratic approximation.","We find that local shapes are highly compressive in an aligned coordinate frame defined by the normal and tangent directions of local shapes.","Accordingly, we introduce Coordinate Field, which is a composition of coordinate frames of all local shapes.","The Coordinate Field is optimizable and is used to transform the local shapes from the world coordinate frame to the aligned shape coordinate frame.","It largely reduces the complexity of local shapes and benefits the learning of MLP-based implicit representations.","Moreover, we introduce quadratic layers into the MLP to enhance expressiveness concerning local shape geometry.","CoFie is a generalizable surface representation.","It is trained on a curated set of 3D shapes and works on novel shape instances during testing.","When using the same amount of parameters with prior works, CoFie reduces the shape error by 48% and 56% on novel instances of both training and unseen shape categories.","Moreover, CoFie demonstrates comparable performance to prior works when using only 70% fewer parameters."],"url":"http://arxiv.org/abs/2406.03417v1","category":"cs.CV"}
{"created":"2024-06-05 16:12:00","title":"Hubbard and Heisenberg models on hyperbolic lattices -- Metal-insulator transitions, global antiferromagnetism and enhanced boundary fluctuations","abstract":"We study the Hubbard and Heisenberg models on hyperbolic lattices with open boundary conditions by means of mean-field approximations, spin-wave theory and quantum Monte Carlo (QMC) simulations. For the Hubbard model we use the auxiliary-field approach and for Heisenberg systems the stochastic series expansion algorithm. We concentrate on bipartite lattices where the QMC simulations are free of the negative sign problem. The considered lattices are characterized by a Dirac like density of states, Schl\\\"afli indices $\\{p,q\\}=\\{10,3\\}$ and $\\{8,3\\}$, as well as by flat bands, $\\{8,8\\}$. The Dirac density of states cuts off the logarithmic divergence of the staggered spin susceptibility and allows for a finite $U$ semi-metal to insulator transition. This transition has the same mean-field exponents as for the Euclidean counterpart. In the presence of flat bands we observe the onset of magnetic ordering at any finite $U$. The magnetic state at intermediate coupling can be described as a global antiferromagnet. It breaks the $C_p$ rotational and time reversal symmetries but remains invariant under combined $C_p \\mathcal{T}$ transformations. The state is characterized by macroscopic ferromagnetic moments, that globally cancel. We observe that fluctuations on the boundary of the system are greatly enhanced: while spin wave calculations predict the breakdown of antiferromagnetism on the boundary but not in the bulk, QMC simulations show a marked reduction of the staggered moment on the edge of the system.","sentences":["We study the Hubbard and Heisenberg models on hyperbolic lattices with open boundary conditions by means of mean-field approximations, spin-wave theory and quantum Monte Carlo (QMC) simulations.","For the Hubbard model we use the auxiliary-field approach and for Heisenberg systems the stochastic series expansion algorithm.","We concentrate on bipartite lattices where the QMC simulations are free of the negative sign problem.","The considered lattices are characterized by a Dirac like density of states, Schl\\\"afli indices $\\{p,q\\}=\\{10,3\\}$ and $\\{8,3\\}$, as well as by flat bands, $\\{8,8\\}$. The Dirac density of states cuts off the logarithmic divergence of the staggered spin susceptibility and allows for a finite $U$ semi-metal to insulator transition.","This transition has the same mean-field exponents as for the Euclidean counterpart.","In the presence of flat bands we observe the onset of magnetic ordering at any finite $U$. The magnetic state at intermediate coupling can be described as a global antiferromagnet.","It breaks the $C_p$ rotational and time reversal symmetries but remains invariant under combined $C_p \\mathcal{T}$ transformations.","The state is characterized by macroscopic ferromagnetic moments, that globally cancel.","We observe that fluctuations on the boundary of the system are greatly enhanced: while spin wave calculations predict the breakdown of antiferromagnetism on the boundary but not in the bulk, QMC simulations show a marked reduction of the staggered moment on the edge of the system."],"url":"http://arxiv.org/abs/2406.03416v1","category":"cond-mat.str-el"}
{"created":"2024-06-05 16:10:29","title":"UnWave-Net: Unrolled Wavelet Network for Compton Tomography Image Reconstruction","abstract":"Computed tomography (CT) is a widely used medical imaging technique to scan internal structures of a body, typically involving collimation and mechanical rotation. Compton scatter tomography (CST) presents an interesting alternative to conventional CT by leveraging Compton physics instead of collimation to gather information from multiple directions. While CST introduces new imaging opportunities with several advantages such as high sensitivity, compactness, and entirely fixed systems, image reconstruction remains an open problem due to the mathematical challenges of CST modeling. In contrast, deep unrolling networks have demonstrated potential in CT image reconstruction, despite their computationally intensive nature. In this study, we investigate the efficiency of unrolling networks for CST image reconstruction. To address the important computational cost required for training, we propose UnWave-Net, a novel unrolled wavelet-based reconstruction network. This architecture includes a non-local regularization term based on wavelets, which captures long-range dependencies within images and emphasizes the multi-scale components of the wavelet transform. We evaluate our approach using a CST of circular geometry which stays completely static during data acquisition, where UnWave-Net facilitates image reconstruction in the absence of a specific reconstruction formula. Our method outperforms existing approaches and achieves state-of-the-art performance in terms of SSIM and PSNR, and offers an improved computational efficiency compared to traditional unrolling networks.","sentences":["Computed tomography (CT) is a widely used medical imaging technique to scan internal structures of a body, typically involving collimation and mechanical rotation.","Compton scatter tomography (CST) presents an interesting alternative to conventional CT by leveraging Compton physics instead of collimation to gather information from multiple directions.","While CST introduces new imaging opportunities with several advantages such as high sensitivity, compactness, and entirely fixed systems, image reconstruction remains an open problem due to the mathematical challenges of CST modeling.","In contrast, deep unrolling networks have demonstrated potential in CT image reconstruction, despite their computationally intensive nature.","In this study, we investigate the efficiency of unrolling networks for CST image reconstruction.","To address the important computational cost required for training, we propose UnWave-Net, a novel unrolled wavelet-based reconstruction network.","This architecture includes a non-local regularization term based on wavelets, which captures long-range dependencies within images and emphasizes the multi-scale components of the wavelet transform.","We evaluate our approach using a CST of circular geometry which stays completely static during data acquisition, where UnWave-Net facilitates image reconstruction in the absence of a specific reconstruction formula.","Our method outperforms existing approaches and achieves state-of-the-art performance in terms of SSIM and PSNR, and offers an improved computational efficiency compared to traditional unrolling networks."],"url":"http://arxiv.org/abs/2406.03413v1","category":"eess.IV"}
{"created":"2024-06-05 15:44:54","title":"Gaussian Representation for Deformable Image Registration","abstract":"Deformable image registration (DIR) is a fundamental task in radiotherapy, with existing methods often struggling to balance computational efficiency, registration accuracy, and speed effectively. We introduce a novel DIR approach employing parametric 3D Gaussian control points achieving a better tradeoff. It provides an explicit and flexible representation for spatial deformation fields between 3D volumetric medical images, producing a displacement vector field (DVF) across all volumetric positions. The movement of individual voxels is derived using linear blend skinning (LBS) through localized interpolation of transformations associated with neighboring Gaussians. This interpolation strategy not only simplifies the determination of voxel motions but also acts as an effective regularization technique. Our approach incorporates a unified optimization process through backpropagation, enabling iterative learning of both the parameters of the 3D Gaussians and their transformations. Additionally, the density of Gaussians is adjusted adaptively during the learning phase to accommodate varying degrees of motion complexity. We validated our approach on the 4D-CT lung DIR-Lab and cardiac ACDC datasets, achieving an average target registration error (TRE) of 1.06 mm within a much-improved processing time of 2.43 seconds for the DIR-Lab dataset over existing methods, demonstrating significant advancements in both accuracy and efficiency.","sentences":["Deformable image registration (DIR) is a fundamental task in radiotherapy, with existing methods often struggling to balance computational efficiency, registration accuracy, and speed effectively.","We introduce a novel DIR approach employing parametric 3D Gaussian control points achieving a better tradeoff.","It provides an explicit and flexible representation for spatial deformation fields between 3D volumetric medical images, producing a displacement vector field (DVF) across all volumetric positions.","The movement of individual voxels is derived using linear blend skinning (LBS) through localized interpolation of transformations associated with neighboring Gaussians.","This interpolation strategy not only simplifies the determination of voxel motions but also acts as an effective regularization technique.","Our approach incorporates a unified optimization process through backpropagation, enabling iterative learning of both the parameters of the 3D Gaussians and their transformations.","Additionally, the density of Gaussians is adjusted adaptively during the learning phase to accommodate varying degrees of motion complexity.","We validated our approach on the 4D-CT lung DIR-Lab and cardiac ACDC datasets, achieving an average target registration error (TRE) of 1.06 mm within a much-improved processing time of 2.43 seconds for the DIR-Lab dataset over existing methods, demonstrating significant advancements in both accuracy and efficiency."],"url":"http://arxiv.org/abs/2406.03394v1","category":"cs.CV"}
{"created":"2024-06-05 15:32:38","title":"Paths towards time evolution with larger neural-network quantum states","abstract":"In recent years, the neural-network quantum states method has been investigated to study the ground state and the time evolution of many-body quantum systems. Here we expand on the investigation and consider a quantum quench from the paramagnetic to the anti-ferromagnetic phase in the tilted Ising model. We use two types of neural networks, a restricted Boltzmann machine and a feed-forward neural network. We show that for both types of networks, the projected time-dependent variational Monte Carlo (p-tVMC) method performs better than the non-projected approach. We further demonstrate that one can use K-FAC or minSR in conjunction with p-tVMC to reduce the computational complexity of the stochastic reconfiguration approach, thus allowing the use of these techniques for neural networks with more parameters.","sentences":["In recent years, the neural-network quantum states method has been investigated to study the ground state and the time evolution of many-body quantum systems.","Here we expand on the investigation and consider a quantum quench from the paramagnetic to the anti-ferromagnetic phase in the tilted Ising model.","We use two types of neural networks, a restricted Boltzmann machine and a feed-forward neural network.","We show that for both types of networks, the projected time-dependent variational Monte Carlo (p-tVMC) method performs better than the non-projected approach.","We further demonstrate that one can use K-FAC or minSR in conjunction with p-tVMC to reduce the computational complexity of the stochastic reconfiguration approach, thus allowing the use of these techniques for neural networks with more parameters."],"url":"http://arxiv.org/abs/2406.03381v1","category":"quant-ph"}
{"created":"2024-06-05 15:32:35","title":"Maximal information at the edge of stability in excitatory-inhibitory neural populations","abstract":"Understanding how the complex connectivity structure of the brain shapes its information-processing capabilities is a long-standing question. Here, by focusing on a paradigmatic architecture, we study how the neural activity of excitatory and inhibitory populations encodes information on external signals. We show that information is maximized at the edge of stability, where excitation is balanced by inhibition. When the input switches among different stimuli, this maximum corresponds to the entropy of the external switching dynamics. By analyzing the case of a prolonged stimulus, we find that stronger inhibition is needed to maximize the instantaneous sensitivity, revealing an intrinsic trade-off between short-time responses and long-time accuracy. In agreement with recent experimental findings, our results open the avenue for a complete information-theoretic understanding of how and why inhibition strength should be tuned to optimize information-processing capabilities.","sentences":["Understanding how the complex connectivity structure of the brain shapes its information-processing capabilities is a long-standing question.","Here, by focusing on a paradigmatic architecture, we study how the neural activity of excitatory and inhibitory populations encodes information on external signals.","We show that information is maximized at the edge of stability, where excitation is balanced by inhibition.","When the input switches among different stimuli, this maximum corresponds to the entropy of the external switching dynamics.","By analyzing the case of a prolonged stimulus, we find that stronger inhibition is needed to maximize the instantaneous sensitivity, revealing an intrinsic trade-off between short-time responses and long-time accuracy.","In agreement with recent experimental findings, our results open the avenue for a complete information-theoretic understanding of how and why inhibition strength should be tuned to optimize information-processing capabilities."],"url":"http://arxiv.org/abs/2406.03380v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-05 15:28:04","title":"Training of Physical Neural Networks","abstract":"Physical neural networks (PNNs) are a class of neural-like networks that leverage the properties of physical systems to perform computation. While PNNs are so far a niche research area with small-scale laboratory demonstrations, they are arguably one of the most underappreciated important opportunities in modern AI. Could we train AI models 1000x larger than current ones? Could we do this and also have them perform inference locally and privately on edge devices, such as smartphones or sensors? Research over the past few years has shown that the answer to all these questions is likely \"yes, with enough research\": PNNs could one day radically change what is possible and practical for AI systems. To do this will however require rethinking both how AI models work, and how they are trained - primarily by considering the problems through the constraints of the underlying hardware physics. To train PNNs at large scale, many methods including backpropagation-based and backpropagation-free approaches are now being explored. These methods have various trade-offs, and so far no method has been shown to scale to the same scale and performance as the backpropagation algorithm widely used in deep learning today. However, this is rapidly changing, and a diverse ecosystem of training techniques provides clues for how PNNs may one day be utilized to create both more efficient realizations of current-scale AI models, and to enable unprecedented-scale models.","sentences":["Physical neural networks (PNNs) are a class of neural-like networks that leverage the properties of physical systems to perform computation.","While PNNs are so far a niche research area with small-scale laboratory demonstrations, they are arguably one of the most underappreciated important opportunities in modern AI.","Could we train AI models 1000x larger than current ones?","Could we do this and also have them perform inference locally and privately on edge devices, such as smartphones or sensors?","Research over the past few years has shown that the answer to all these questions is likely \"yes, with enough research\": PNNs could one day radically change what is possible and practical for AI systems.","To do this will however require rethinking both how AI models work, and how they are trained - primarily by considering the problems through the constraints of the underlying hardware physics.","To train PNNs at large scale, many methods including backpropagation-based and backpropagation-free approaches are now being explored.","These methods have various trade-offs, and so far no method has been shown to scale to the same scale and performance as the backpropagation algorithm widely used in deep learning today.","However, this is rapidly changing, and a diverse ecosystem of training techniques provides clues for how PNNs may one day be utilized to create both more efficient realizations of current-scale AI models, and to enable unprecedented-scale models."],"url":"http://arxiv.org/abs/2406.03372v1","category":"physics.app-ph"}
{"created":"2024-06-05 15:19:53","title":"Computational Supremacy of Quantum Eigensolver by Extension of Optimized Binary Configurations","abstract":"We developed a quantum eigensolver (QE) which is based on an extension of optimized binary configurations measured by quantum annealing (QA) on a D-Wave Quantum Annealer (D-Wave QA). This approach performs iterative QA measurements to optimize the eigenstates $\\vert \\psi \\rangle$ without the derivation of a classical computer. The computational cost is $\\eta M L$ for full eigenvalues $E$ and $\\vert \\psi \\rangle$ of the Hamiltonian $\\hat{H}$ of size $L \\times L$, where $M$ and $\\eta$ are the number of QA measurements required to reach the converged $\\vert \\psi \\rangle$ and the total annealing time of many QA shots, respectively. Unlike the exact diagonalized (ED) algorithm with $L^3$ iterations on a classical computer, the computation cost is not significantly affected by $L$ and $M$ because $\\eta$ represents a very short time within $10^{-2}$ seconds on the D-Wave QA. We selected the tight-binding $\\hat{H}$ that contains the exact $E$ values of all energy states in two systems with metallic and insulating phases. We confirmed that the proposed QE algorithm provides exact solutions within the errors of $5 \\times 10^{-3}$. The QE algorithm will not only show computational supremacy over the ED approach on a classical computer but will also be widely used for various applications such as material and drug design.","sentences":["We developed a quantum eigensolver (QE) which is based on an extension of optimized binary configurations measured by quantum annealing (QA) on a D-Wave Quantum Annealer (D-Wave QA).","This approach performs iterative QA measurements to optimize the eigenstates $\\vert \\psi \\rangle$ without the derivation of a classical computer.","The computational cost is $\\eta M L$ for full eigenvalues $E$ and $\\vert \\psi \\rangle$ of the Hamiltonian $\\hat{H}$ of size $L \\times L$, where $M$ and $\\eta$ are the number of QA measurements required to reach the converged $\\vert \\psi \\rangle$ and the total annealing time of many QA shots, respectively.","Unlike the exact diagonalized (ED) algorithm with $L^3$ iterations on a classical computer, the computation cost is not significantly affected by $L$ and $M$ because $\\eta$ represents a very short time within $10^{-2}$ seconds on the D-Wave QA.","We selected the tight-binding $\\hat{H}$ that contains the exact $E$ values of all energy states in two systems with metallic and insulating phases.","We confirmed that the proposed QE algorithm provides exact solutions within the errors of $5 \\times 10^{-3}$.","The QE algorithm will not only show computational supremacy over the ED approach on a classical computer but will also be widely used for various applications such as material and drug design."],"url":"http://arxiv.org/abs/2406.03366v1","category":"quant-ph"}
{"created":"2024-06-05 15:18:24","title":"Optical read and write of spin states in organic diradicals","abstract":"Optical control and read-out of the ground state spin structure has been demonstrated for defect states in crystalline semiconductors, including the diamond NV- center, and these are promising systems for quantum technologies. Molecular organic semiconductors offer synthetic control of spin placement, in contrast to current limitations in these crystalline systems. Here we report the discovery of spin-optical addressability in a diradical molecule that comprises two trityl radical groups coupled via a fluorene bridge. We demonstrate the three important properties that enable operation as a spin-photon interface: (i) triplet and singlet spin states show photoluminescence peaked at 640 and 700 nm respectively; this allows easy optical measurement of ground state spin. (ii) the ground state spin exchange is small (~60 {\\mu}eV) that allows preparation of ground state spin population. This can be achieved by spin-selective excited state intersystem crossing, and we report up to 8% microwave-driven contrast in photoluminescence. (iii) both singlet and triplet manifolds have near-unity photoluminescence quantum yield, which is in contrast to the near-zero quantum yields in prior reports of molecular diradicals. Our results establish these tuneable open-shell organic molecules as a platform to engineer tailor-made spin-optical interfaces.","sentences":["Optical control and read-out of the ground state spin structure has been demonstrated for defect states in crystalline semiconductors, including the diamond NV- center, and these are promising systems for quantum technologies.","Molecular organic semiconductors offer synthetic control of spin placement, in contrast to current limitations in these crystalline systems.","Here we report the discovery of spin-optical addressability in a diradical molecule that comprises two trityl radical groups coupled via a fluorene bridge.","We demonstrate the three important properties that enable operation as a spin-photon interface: (i) triplet and singlet spin states show photoluminescence peaked at 640 and 700 nm respectively; this allows easy optical measurement of ground state spin.","(ii) the ground state spin exchange is small (~60 {\\mu}eV) that allows preparation of ground state spin population.","This can be achieved by spin-selective excited state intersystem crossing, and we report up to 8% microwave-driven contrast in photoluminescence.","(iii) both singlet and triplet manifolds have near-unity photoluminescence quantum yield, which is in contrast to the near-zero quantum yields in prior reports of molecular diradicals.","Our results establish these tuneable open-shell organic molecules as a platform to engineer tailor-made spin-optical interfaces."],"url":"http://arxiv.org/abs/2406.03365v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-05 15:12:34","title":"Nonreciprocal synchronization of active quantum spins","abstract":"Active agents are capable of exerting nonreciprocal forces upon one another. For instance, one agent may attract another one, which in turn is repelled. These antagonistic nonreciprocal interactions have been extensively studied in classical systems, revealing a wealth of exciting phenomena such as novel phase transitions and traveling-wave states. Whether these phenomena can originate in quantum many-body systems is an open issue, and proposals for their realization are lacking. In this work, we present a model of two species of quantum spins that interact in an antagonistic nonreciprocal way of the attraction-repulsion type. We propose an implementation based on two atomic ensembles coupled via chiral waveguides featuring both braided and non-braided geometries. The spins are active due to the presence of local gain, which allows them to synchronize. We show that nonreciprocal interactions deeply affect their synchronization dynamics. In the thermodynamic limit, this results in a nonreciprocal phase transition to nonstationary traveling-wave states, associated with spontaneous breaking of parity-time symmetry. We establish how this symmetry emerges from the microscopic quantum model. For a finite number of spins, we discuss how traveling-wave states and nonreciprocal phase transitions are revealed by spin correlations measurable via the output field of the waveguides. Our work opens a new avenue to explore nonreciprocal interactions in active quantum matter.","sentences":["Active agents are capable of exerting nonreciprocal forces upon one another.","For instance, one agent may attract another one, which in turn is repelled.","These antagonistic nonreciprocal interactions have been extensively studied in classical systems, revealing a wealth of exciting phenomena such as novel phase transitions and traveling-wave states.","Whether these phenomena can originate in quantum many-body systems is an open issue, and proposals for their realization are lacking.","In this work, we present a model of two species of quantum spins that interact in an antagonistic nonreciprocal way of the attraction-repulsion type.","We propose an implementation based on two atomic ensembles coupled via chiral waveguides featuring both braided and non-braided geometries.","The spins are active due to the presence of local gain, which allows them to synchronize.","We show that nonreciprocal interactions deeply affect their synchronization dynamics.","In the thermodynamic limit, this results in a nonreciprocal phase transition to nonstationary traveling-wave states, associated with spontaneous breaking of parity-time symmetry.","We establish how this symmetry emerges from the microscopic quantum model.","For a finite number of spins, we discuss how traveling-wave states and nonreciprocal phase transitions are revealed by spin correlations measurable via the output field of the waveguides.","Our work opens a new avenue to explore nonreciprocal interactions in active quantum matter."],"url":"http://arxiv.org/abs/2406.03357v1","category":"quant-ph"}
{"created":"2024-06-05 15:05:24","title":"Position: A Call to Action for a Human-Centered AutoML Paradigm","abstract":"Automated machine learning (AutoML) was formed around the fundamental objectives of automatically and efficiently configuring machine learning (ML) workflows, aiding the research of new ML algorithms, and contributing to the democratization of ML by making it accessible to a broader audience. Over the past decade, commendable achievements in AutoML have primarily focused on optimizing predictive performance. This focused progress, while substantial, raises questions about how well AutoML has met its broader, original goals. In this position paper, we argue that a key to unlocking AutoML's full potential lies in addressing the currently underexplored aspect of user interaction with AutoML systems, including their diverse roles, expectations, and expertise. We envision a more human-centered approach in future AutoML research, promoting the collaborative design of ML systems that tightly integrates the complementary strengths of human expertise and AutoML methodologies.","sentences":["Automated machine learning (AutoML) was formed around the fundamental objectives of automatically and efficiently configuring machine learning (ML) workflows, aiding the research of new ML algorithms, and contributing to the democratization of ML by making it accessible to a broader audience.","Over the past decade, commendable achievements in AutoML have primarily focused on optimizing predictive performance.","This focused progress, while substantial, raises questions about how well AutoML has met its broader, original goals.","In this position paper, we argue that a key to unlocking AutoML's full potential lies in addressing the currently underexplored aspect of user interaction with AutoML systems, including their diverse roles, expectations, and expertise.","We envision a more human-centered approach in future AutoML research, promoting the collaborative design of ML systems that tightly integrates the complementary strengths of human expertise and AutoML methodologies."],"url":"http://arxiv.org/abs/2406.03348v1","category":"cs.LG"}
{"created":"2024-06-05 15:04:39","title":"Minimal submanifolds and stability in Einstein manifolds","abstract":"In this paper, we compute the index and nullity for minimal submanifolds of some complex Einstein spaces. We investigate the stability of these minimal submanifolds and suggest a criterion for instability for some cases. We also compute some higher eigenvalues for the Laplacian of the Berger spheres and provide with an algorithm.","sentences":["In this paper, we compute the index and nullity for minimal submanifolds of some complex Einstein spaces.","We investigate the stability of these minimal submanifolds and suggest a criterion for instability for some cases.","We also compute some higher eigenvalues for the Laplacian of the Berger spheres and provide with an algorithm."],"url":"http://arxiv.org/abs/2406.03347v1","category":"math.DG"}
{"created":"2024-06-05 14:59:40","title":"Understanding and measuring software engineer behavior: What can we learn from the behavioral sciences?","abstract":"This paper explores the intricate challenge of understanding and measuring software engineer behavior. More specifically, we revolve around a central question: How can we enhance our understanding of software engineer behavior? Grounded in the nuanced complexities addressed within Behavioral Software Engineering (BSE), we advocate for holistic methods that integrate quantitative measures, such as psychometric instruments, and qualitative data from diverse sources. Furthermore, we delve into the relevance of this challenge within national and international contexts, highlighting the increasing interest in understanding software engineer behavior. Real-world initiatives and academic endeavors are also examined to underscore the potential for advancing this research agenda and, consequently, refining software engineering practices based on behavioral aspects. Lastly, this paper addresses different ways to evaluate the progress of this challenge by leveraging methodological skills derived from behavioral sciences, ultimately contributing to a deeper understanding of software engineer behavior and software engineering practices.","sentences":["This paper explores the intricate challenge of understanding and measuring software engineer behavior.","More specifically, we revolve around a central question: How can we enhance our understanding of software engineer behavior?","Grounded in the nuanced complexities addressed within Behavioral Software Engineering (BSE), we advocate for holistic methods that integrate quantitative measures, such as psychometric instruments, and qualitative data from diverse sources.","Furthermore, we delve into the relevance of this challenge within national and international contexts, highlighting the increasing interest in understanding software engineer behavior.","Real-world initiatives and academic endeavors are also examined to underscore the potential for advancing this research agenda and, consequently, refining software engineering practices based on behavioral aspects.","Lastly, this paper addresses different ways to evaluate the progress of this challenge by leveraging methodological skills derived from behavioral sciences, ultimately contributing to a deeper understanding of software engineer behavior and software engineering practices."],"url":"http://arxiv.org/abs/2406.03342v1","category":"cs.SE"}
{"created":"2024-06-05 14:52:50","title":"Strength of Kitaev Interaction in Na$_3$Co$_2$SbO$_6$ and Na$_3$Ni$_2$BiO$_6$","abstract":"Kitaev spin liquid is proposed to be promisingly realized in low spin-orbit coupling $3d$ systems, represented by Na$_3$Co$_2$SbO$_6$ and Na$_3$Ni$_2$BiO$_6$. However, the existence of Kitaev interaction is still debatable among experiments, and obtaining the strength of Kitaev interaction from first-principles calculations is also challenging. Here, we report the state-dependent anisotropy of Kitaev interaction, based on which a convenient method is developed to rapidly determine the strength of Kitaev interaction. Applying such method and density functional theory calculations, it is found that Na$_3$Co$_2$SbO$_6$ with $3d^7$ configuration exhibits considerable ferromagnetic Kitaev interaction. Moreover, by further applying the symmetry-adapted cluster expansion method, a realistic spin model is determined for Na$_3$Ni$_2$BiO$_6$ with $3d^8$ configuration. Such model indicates negligible small Kitaev interaction, but it predicts many properties, such as ground states and field effects, which are well consistent with measurements. Furthermore, we demonstrate that the heavy elements, Sb or Bi, located at the hollow sites of honeycomb lattice, do not contribute to emergence of Kitaev interaction through proximity, contradictory to common belief. The presently developed anisotropy method will be beneficial not only for computations but also for measurements.","sentences":["Kitaev spin liquid is proposed to be promisingly realized in low spin-orbit coupling $3d$ systems, represented by Na$_3$Co$_2$SbO$_6$ and Na$_3$Ni$_2$BiO$_6$.","However, the existence of Kitaev interaction is still debatable among experiments, and obtaining the strength of Kitaev interaction from first-principles calculations is also challenging.","Here, we report the state-dependent anisotropy of Kitaev interaction, based on which a convenient method is developed to rapidly determine the strength of Kitaev interaction.","Applying such method and density functional theory calculations, it is found that Na$_3$Co$_2$SbO$_6$ with $3d^7$ configuration exhibits considerable ferromagnetic Kitaev interaction.","Moreover, by further applying the symmetry-adapted cluster expansion method, a realistic spin model is determined for Na$_3$Ni$_2$BiO$_6$ with $3d^8$ configuration.","Such model indicates negligible small Kitaev interaction, but it predicts many properties, such as ground states and field effects, which are well consistent with measurements.","Furthermore, we demonstrate that the heavy elements, Sb or Bi, located at the hollow sites of honeycomb lattice, do not contribute to emergence of Kitaev interaction through proximity, contradictory to common belief.","The presently developed anisotropy method will be beneficial not only for computations but also for measurements."],"url":"http://arxiv.org/abs/2406.03338v1","category":"cond-mat.str-el"}
{"created":"2024-06-05 14:49:53","title":"Entangled states are typically incomparable","abstract":"Consider a bipartite quantum system, where Alice and Bob jointly possess a pure state $|\\psi\\rangle$. Using local quantum operations on their respective subsystems, and unlimited classical communication, Alice and Bob may be able to transform $|\\psi\\rangle$ into another state $|\\phi\\rangle$. Famously, Nielsen's theorem [Phys. Rev. Lett., 1999] provides a necessary and sufficient algebraic criterion for such a transformation to be possible (namely, the local spectrum of $|\\phi\\rangle$ should majorise the local spectrum of $|\\psi\\rangle$).   In the paper where Nielsen proved this theorem, he conjectured that in the limit of large dimensionality, for almost all pairs of states $|\\psi\\rangle, |\\phi\\rangle$ (according to the natural unitary invariant measure) such a transformation is not possible. That is to say, typical pairs of quantum states $|\\psi\\rangle, |\\phi\\rangle$ are entangled in fundamentally different ways, that cannot be converted to each other via local operations and classical communication.   Via Nielsen's theorem, this conjecture can be equivalently stated as a conjecture about majorisation of spectra of random matrices from the so-called trace-normalised complex Wishart-Laguerre ensemble. Concretely, let $X$ and $Y$ be independent $n \\times m$ random matrices whose entries are i.i.d. standard complex Gaussians; then Nielsen's conjecture says that the probability that the spectrum of $X X^\\dagger / \\operatorname{tr}(X X^\\dagger)$ majorises the spectrum of $Y Y^\\dagger / \\operatorname{tr}(Y Y^\\dagger)$ tends to zero as both $n$ and $m$ grow large. We prove this conjecture, and we also confirm some related predictions of Cunden, Facchi, Florio and Gramegna [J. Phys. A., 2020; Phys. Rev. A., 2021].","sentences":["Consider a bipartite quantum system, where Alice and Bob jointly possess a pure state $|\\psi\\rangle$. Using local quantum operations on their respective subsystems, and unlimited classical communication, Alice and Bob may be able to transform $|\\psi\\rangle$ into another state $|\\phi\\rangle$. Famously, Nielsen's theorem [Phys. Rev. Lett., 1999] provides a necessary and sufficient algebraic criterion for such a transformation to be possible (namely, the local spectrum of $|\\phi\\rangle$ should majorise the local spectrum of $|\\psi\\rangle$).   ","In the paper where Nielsen proved this theorem, he conjectured that in the limit of large dimensionality, for almost all pairs of states $|\\psi\\rangle, |\\phi\\rangle$ (according to the natural unitary invariant measure) such a transformation is not possible.","That is to say, typical pairs of quantum states $|\\psi\\rangle, |\\phi\\rangle$ are entangled in fundamentally different ways, that cannot be converted to each other via local operations and classical communication.   ","Via Nielsen's theorem",", this conjecture can be equivalently stated as a conjecture about majorisation of spectra of random matrices from the so-called trace-normalised complex Wishart-Laguerre ensemble.","Concretely, let $X$ and $Y$ be independent $n \\times m$ random matrices whose entries are i.i.d. standard complex Gaussians; then Nielsen's conjecture says that the probability that the spectrum of $X X^\\dagger / \\operatorname{tr}(X X^\\dagger)$ majorises the spectrum of $Y Y^\\dagger / \\operatorname{tr}(Y Y^\\dagger)$ tends to zero as both $n$ and $m$ grow large.","We prove this conjecture, and we also confirm some related predictions of Cunden, Facchi, Florio and Gramegna [J. Phys. A., 2020; Phys.","Rev. A., 2021]."],"url":"http://arxiv.org/abs/2406.03335v1","category":"quant-ph"}
{"created":"2024-06-05 14:44:19","title":"Rethinking Programming Paradigms in the QC-HPC Context","abstract":"Programming for today's quantum computers is making significant strides toward modern workflows compatible with high performance computing (HPC), but fundamental challenges still remain in the integration of these vastly different technologies. Quantum computing (QC) programming languages share some common ground, as well as their emerging runtimes and algorithmic modalities. In this short paper, we explore avenues of refinement for the quantum processing unit (QPU) in the context of many-tasks management, asynchronous or otherwise, in order to understand the value it can play in linking QC with HPC. Through examples, we illustrate how its potential for scientific discovery might be realized.","sentences":["Programming for today's quantum computers is making significant strides toward modern workflows compatible with high performance computing (HPC), but fundamental challenges still remain in the integration of these vastly different technologies.","Quantum computing (QC) programming languages share some common ground, as well as their emerging runtimes and algorithmic modalities.","In this short paper, we explore avenues of refinement for the quantum processing unit (QPU) in the context of many-tasks management, asynchronous or otherwise, in order to understand the value it can play in linking QC with HPC.","Through examples, we illustrate how its potential for scientific discovery might be realized."],"url":"http://arxiv.org/abs/2406.03330v1","category":"quant-ph"}
{"created":"2024-06-05 14:33:51","title":"Decision synthesis in monetary policy","abstract":"The macroeconomy is a sophisticated dynamic system involving significant uncertainties that complicate modelling. In response, decision makers consider multiple models that provide different predictions and policy recommendations which are then synthesized into a policy decision. In this setting, we introduce and develop Bayesian predictive decision synthesis (BPDS) to formalize monetary policy decision processes. BPDS draws on recent developments in model combination and statistical decision theory that yield new opportunities in combining multiple models, emphasizing the integration of decision goals, expectations and outcomes into the model synthesis process. Our case study concerns central bank policy decisions about target interest rates with a focus on implications for multi-step macroeconomic forecasting.","sentences":["The macroeconomy is a sophisticated dynamic system involving significant uncertainties that complicate modelling.","In response, decision makers consider multiple models that provide different predictions and policy recommendations which are then synthesized into a policy decision.","In this setting, we introduce and develop Bayesian predictive decision synthesis (BPDS) to formalize monetary policy decision processes.","BPDS draws on recent developments in model combination and statistical decision theory that yield new opportunities in combining multiple models, emphasizing the integration of decision goals, expectations and outcomes into the model synthesis process.","Our case study concerns central bank policy decisions about target interest rates with a focus on implications for multi-step macroeconomic forecasting."],"url":"http://arxiv.org/abs/2406.03321v1","category":"stat.ME"}
{"created":"2024-06-05 14:33:31","title":"Detection of extended gamma-ray emission in the vicinity of Cl Danks 1 and 2","abstract":"We report the detection of high-energy gamma-ray emission towards the G305 star-forming region. Using almost 15 years of observation data from {\\sl Fermi} Large Area Telescope, we detected an extended gamma-ray source in this region with a significance of $\\sim 13 \\sigma$. The gamma-ray radiation reveals a clear pion-bump feature and can be fitted with the power law parent proton spectrum with an index of $-2.5$. The total cosmic ray (CR) proton energy in the gamma-ray production region is estimated to be the order of $10^{49}\\ \\rm erg$. We further derived the CR radial distribution from both the gamma-ray emission and gas distribution and found it roughly obeys the $1/r$ type profile, which is consistent with other similar systems and expected from the continuous injection of CRs by the central powerful young massive star cluster Danks 1 or Danks 2 in this region. Together with former detections of similar gamma-ray structures, such as Cygnus cocoon, Westerlund 1, Westerlund 2, NGC 3603, and W40, the detection supports the hypothesis that young massive star clusters are CR accelerators.","sentences":["We report the detection of high-energy gamma-ray emission towards the G305 star-forming region.","Using almost 15 years of observation data from {\\sl Fermi} Large Area Telescope, we detected an extended gamma-ray source in this region with a significance of $\\sim 13 \\sigma$.","The gamma-ray radiation reveals a clear pion-bump feature and can be fitted with the power law parent proton spectrum with an index of $-2.5$. The total cosmic ray (CR) proton energy in the gamma-ray production region is estimated to be the order of $10^{49}\\ \\rm erg$.","We further derived the CR radial distribution from both the gamma-ray emission and gas distribution and found it roughly obeys the $1/r$ type profile, which is consistent with other similar systems and expected from the continuous injection of CRs by the central powerful young massive star cluster Danks 1 or Danks 2 in this region.","Together with former detections of similar gamma-ray structures, such as Cygnus cocoon, Westerlund 1, Westerlund 2, NGC 3603, and W40, the detection supports the hypothesis that young massive star clusters are CR accelerators."],"url":"http://arxiv.org/abs/2406.03320v1","category":"astro-ph.HE"}
{"created":"2024-06-05 14:33:18","title":"Synchronized Optimal Transport for Joint Modeling of Dynamics Across Multiple Spaces","abstract":"Optimal transport has been an essential tool for reconstructing dynamics from complex data. With the increasingly available multifaceted data, a system can often be characterized across multiple spaces. Therefore, it is crucial to maintain coherence in the dynamics across these diverse spaces. To address this challenge, we introduce Synchronized Optimal Transport (SyncOT), a novel approach to jointly model dynamics that represent the same system through multiple spaces. With given correspondence between the spaces, SyncOT minimizes the aggregated cost of the dynamics induced across all considered spaces. The problem is discretized into a finite-dimensional convex problem using a staggered grid. Primal-dual algorithm-based approaches are then developed to solve the discretized problem. Various numerical experiments demonstrate the capabilities and properties of SyncOT and validate the effectiveness of the proposed algorithms.","sentences":["Optimal transport has been an essential tool for reconstructing dynamics from complex data.","With the increasingly available multifaceted data, a system can often be characterized across multiple spaces.","Therefore, it is crucial to maintain coherence in the dynamics across these diverse spaces.","To address this challenge, we introduce Synchronized Optimal Transport (SyncOT), a novel approach to jointly model dynamics that represent the same system through multiple spaces.","With given correspondence between the spaces, SyncOT minimizes the aggregated cost of the dynamics induced across all considered spaces.","The problem is discretized into a finite-dimensional convex problem using a staggered grid.","Primal-dual algorithm-based approaches are then developed to solve the discretized problem.","Various numerical experiments demonstrate the capabilities and properties of SyncOT and validate the effectiveness of the proposed algorithms."],"url":"http://arxiv.org/abs/2406.03319v1","category":"math.OC"}
{"created":"2024-06-05 14:31:41","title":"Single-cycle, 643-mW average power THz source based on tilted pulse front in lithium niobate","abstract":"We present, to the best of our knowledge, the highest average power from a laser-driven single-cycle THz source demonstrated so far, using optical rectification in the titled pulse-front geometry in cryogenically cooled lithium niobate, pumped by a commercially available 500 W ultrafast thin-disk Yb-amplifier. We study repetition rate dependent effects in our setup at 100 kHz and 40 kHz at this high average power, revealing different optimal fluence conditions for efficient conversion. The demonstrated sources with multi-100 mW average power at these high repetition rates combine high THz pulse energies and high repetition rate and is thus ideally suited for nonlinear THz spectroscopy experiments with significantly reduced measurement times. The presented result is a first benchmark for high average power THz time domain spectroscopy systems for nonlinear spectroscopy, driven by very high average power ultrafast Yb lasers.","sentences":["We present, to the best of our knowledge, the highest average power from a laser-driven single-cycle THz source demonstrated so far, using optical rectification in the titled pulse-front geometry in cryogenically cooled lithium niobate, pumped by a commercially available 500 W ultrafast thin-disk Yb-amplifier.","We study repetition rate dependent effects in our setup at 100 kHz and 40 kHz at this high average power, revealing different optimal fluence conditions for efficient conversion.","The demonstrated sources with multi-100 mW average power at these high repetition rates combine high THz pulse energies and high repetition rate and is thus ideally suited for nonlinear THz spectroscopy experiments with significantly reduced measurement times.","The presented result is a first benchmark for high average power THz time domain spectroscopy systems for nonlinear spectroscopy, driven by very high average power ultrafast Yb lasers."],"url":"http://arxiv.org/abs/2406.03318v1","category":"physics.optics"}
{"created":"2024-06-05 14:29:44","title":"Save It for the \"Hot\" Day: An LLM-Empowered Visual Analytics System for Heat Risk Management","abstract":"The escalating frequency and intensity of heat-related climate events, particularly heatwaves, emphasize the pressing need for advanced heat risk management strategies. Current approaches, primarily relying on numerical models, face challenges in spatial-temporal resolution and in capturing the dynamic interplay of environmental, social, and behavioral factors affecting heat risks. This has led to difficulties in translating risk assessments into effective mitigation actions. Recognizing these problems, we introduce a novel approach leveraging the burgeoning capabilities of Large Language Models (LLMs) to extract rich and contextual insights from news reports. We hence propose an LLM-empowered visual analytics system, Havior, that integrates the precise, data-driven insights of numerical models with nuanced news report information. This hybrid approach enables a more comprehensive assessment of heat risks and better identification, assessment, and mitigation of heat-related threats. The system incorporates novel visualization designs, such as \"thermoglyph\" and news glyph, enhancing intuitive understanding and analysis of heat risks. The integration of LLM-based techniques also enables advanced information retrieval and semantic knowledge extraction that can be guided by experts' analytics needs. Our case studies on two cities that faced significant heatwave events and interviews with five experts have demonstrated the usefulness of our system in providing in-depth and actionable insights for heat risk management.","sentences":["The escalating frequency and intensity of heat-related climate events, particularly heatwaves, emphasize the pressing need for advanced heat risk management strategies.","Current approaches, primarily relying on numerical models, face challenges in spatial-temporal resolution and in capturing the dynamic interplay of environmental, social, and behavioral factors affecting heat risks.","This has led to difficulties in translating risk assessments into effective mitigation actions.","Recognizing these problems, we introduce a novel approach leveraging the burgeoning capabilities of Large Language Models (LLMs) to extract rich and contextual insights from news reports.","We hence propose an LLM-empowered visual analytics system, Havior, that integrates the precise, data-driven insights of numerical models with nuanced news report information.","This hybrid approach enables a more comprehensive assessment of heat risks and better identification, assessment, and mitigation of heat-related threats.","The system incorporates novel visualization designs, such as \"thermoglyph\" and news glyph, enhancing intuitive understanding and analysis of heat risks.","The integration of LLM-based techniques also enables advanced information retrieval and semantic knowledge extraction that can be guided by experts' analytics needs.","Our case studies on two cities that faced significant heatwave events and interviews with five experts have demonstrated the usefulness of our system in providing in-depth and actionable insights for heat risk management."],"url":"http://arxiv.org/abs/2406.03317v1","category":"cs.HC"}
{"created":"2024-06-05 14:27:18","title":"Correlated states controlled by tunable van Hove singularity in moir\u00e9 WSe2","abstract":"Twisted bilayers of transition metal dichalcogenide (TMD) semiconductors have emerged as a highly tunable system for the studies of correlated and topological states of matter, such as superconductivity, ferromagnetism, correlated insulators, and topological and Chern insulators. However, the connection between these symmetry-breaking ground states and the underlying band structure singularity in these materials remains largely unexplored. Here, by combining exciton sensing and magnetic circular dichroism (MCD) measurements, we demonstrate how the magnetic properties and the correlated insulating states are controlled by the gate tunable van Hove singularity (VHS) in the band structure of twisted bilayer WSe2 (tWSe2). In particular, we demonstrate how the location of the VHS in the tWSe2 band structure can influence 1) the stability of Stoner ferromagnetism, 2) the valley polarizability and the stability of Chern insulators, as well as 3) the layer polarizability and the associated metal-insulator transition. The results are supported by continuum model band structure calculations. Our work highlights an important ingredient for understanding the electronic phase diagram in twisted bilayer TMDs.","sentences":["Twisted bilayers of transition metal dichalcogenide (TMD) semiconductors have emerged as a highly tunable system for the studies of correlated and topological states of matter, such as superconductivity, ferromagnetism, correlated insulators, and topological and Chern insulators.","However, the connection between these symmetry-breaking ground states and the underlying band structure singularity in these materials remains largely unexplored.","Here, by combining exciton sensing and magnetic circular dichroism (MCD) measurements, we demonstrate how the magnetic properties and the correlated insulating states are controlled by the gate tunable van Hove singularity (VHS) in the band structure of twisted bilayer WSe2 (tWSe2).","In particular, we demonstrate how the location of the VHS in the tWSe2 band structure can influence 1) the stability of Stoner ferromagnetism, 2) the valley polarizability and the stability of Chern insulators, as well as 3) the layer polarizability and the associated metal-insulator transition.","The results are supported by continuum model band structure calculations.","Our work highlights an important ingredient for understanding the electronic phase diagram in twisted bilayer TMDs."],"url":"http://arxiv.org/abs/2406.03315v1","category":"cond-mat.str-el"}
{"created":"2024-06-05 14:23:37","title":"Inhomogeneous SU(2) gluon matter under rotation","abstract":"In this work a rotating SU(2) gluon system have been studied with the dyon ensemble in dilute limit. By solving the rotation-modified Yang-Mills equation we have obtained rotational corrections to the so-called dyon solutions with arbitrary centers to $\\mathcal{O}(\\omega^2)$ order and the corresponding semi-classical potential. The radial position dependent deconfinement temperature have been obtained by minimizing the semi-classical potential in both real and imaginary angular velocity cases. Although without the $\\omega$-dependent coupling constant the critical temperature behaves different from the lattice simulation at each radial position as the rotation goes faster, its radial dependence is qualitatively the same as the lattice. That is in the real velocity case the outer layer will deconfine more difficult while the reverse is true in the imaginary velocity case.","sentences":["In this work a rotating SU(2) gluon system have been studied with the dyon ensemble in dilute limit.","By solving the rotation-modified Yang-Mills equation we have obtained rotational corrections to the so-called dyon solutions with arbitrary centers to $\\mathcal{O}(\\omega^2)$ order and the corresponding semi-classical potential.","The radial position dependent deconfinement temperature have been obtained by minimizing the semi-classical potential in both real and imaginary angular velocity cases.","Although without the $\\omega$-dependent coupling constant the critical temperature behaves different from the lattice simulation at each radial position as the rotation goes faster, its radial dependence is qualitatively the same as the lattice.","That is in the real velocity case the outer layer will deconfine more difficult while the reverse is true in the imaginary velocity case."],"url":"http://arxiv.org/abs/2406.03311v1","category":"nucl-th"}
{"created":"2024-06-05 14:11:30","title":"Effects of Mosaic Crystal Instrument Functions on X-ray Thomson Scattering Diagnostics","abstract":"Mosaic crystals, with their high integrated reflectivities, are widely-employed in spectrometers used to diagnose high energy density systems. X-ray Thomson scattering (XRTS) has emerged as a powerful diagnostic tool of these systems, providing in principle direct access to important properties such as the temperature via detailed balance. However, the measured XRTS spectrum is broadened by the spectrometer instrument function (IF), and without careful consideration of the IF one risks misdiagnosing system conditions. Here, we consider in detail the IF of mosaic crystals and how the broadening varies across the spectrometer. Notably, we find a strong asymmetry in the shape of the IF towards higher energies. As an example, we consider the effect on the inferred temperature, and find that it can be overestimated if an approximate symmetric IF is used. We therefore expect a detailed consideration of the full IF will have an important impact on system properties inferred via XRTS in both forward modelling and model-free approaches.","sentences":["Mosaic crystals, with their high integrated reflectivities, are widely-employed in spectrometers used to diagnose high energy density systems.","X-ray Thomson scattering (XRTS) has emerged as a powerful diagnostic tool of these systems, providing in principle direct access to important properties such as the temperature via detailed balance.","However, the measured XRTS spectrum is broadened by the spectrometer instrument function (IF), and without careful consideration of the IF one risks misdiagnosing system conditions.","Here, we consider in detail the IF of mosaic crystals and how the broadening varies across the spectrometer.","Notably, we find a strong asymmetry in the shape of the IF towards higher energies.","As an example, we consider the effect on the inferred temperature, and find that it can be overestimated if an approximate symmetric IF is used.","We therefore expect a detailed consideration of the full IF will have an important impact on system properties inferred via XRTS in both forward modelling and model-free approaches."],"url":"http://arxiv.org/abs/2406.03301v1","category":"physics.plasm-ph"}
{"created":"2024-06-05 14:09:15","title":"Form factor bootstrap in the thermally perturbed tricritical Ising model","abstract":"We derive a systematic construction for form factors of relevant fields in the thermal perturbation of the tricritical Ising model, an integrable model with scattering amplitudes described by the $E_7$ bootstrap. We find a new type of recursive structure encoding the information in the bound state fusion structure, which fully determines the form factors of the perturbing field and the order/disorder fields, for which we present a mathematical proof. Knowledge of these form factors enables the systematic computation of correlation functions and dynamical structure factors in systems whose dynamics is governed by the vicinity of a fixed point in the tricritical Ising universality class.","sentences":["We derive a systematic construction for form factors of relevant fields in the thermal perturbation of the tricritical Ising model, an integrable model with scattering amplitudes described by the $E_7$ bootstrap.","We find a new type of recursive structure encoding the information in the bound state fusion structure, which fully determines the form factors of the perturbing field and the order/disorder fields, for which we present a mathematical proof.","Knowledge of these form factors enables the systematic computation of correlation functions and dynamical structure factors in systems whose dynamics is governed by the vicinity of a fixed point in the tricritical Ising universality class."],"url":"http://arxiv.org/abs/2406.03300v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-05 14:00:22","title":"Partial regularity and $L^3$-norm concentration effects around possible blow-up points for the micropolar fluid equations","abstract":"The micropolar fluid system is a model based on the Navier-Stokes equations which considers two coupled variables: the velocity field $\\vec u$ and the microrotation field $\\vec\\omega$. Assuming an additional condition over the variable $\\vec u$ we will first prove that weak solutions $(\\vec u, \\vec\\omega)$ of this system are smooth. Then, we will present a concentration effect of the $L^3_x$ norm of the velocity field $\\vec u$ near a possible singular time.","sentences":["The micropolar fluid system is a model based on the Navier-Stokes equations which considers two coupled variables: the velocity field $\\vec u$ and the microrotation field $\\vec\\omega$.","Assuming an additional condition over the variable $\\vec u$ we will first prove that weak solutions $(\\vec u, \\vec\\omega)$ of this system are smooth.","Then, we will present a concentration effect of the $L^3_x$ norm of the velocity field $\\vec u$ near a possible singular time."],"url":"http://arxiv.org/abs/2406.03291v1","category":"math.AP"}
{"created":"2024-06-05 13:48:05","title":"Advancing Mathematical Epidemic Modeling via synergies with Chemical Reaction Network Theory and Lagrange-Hamilton Geometry","abstract":"This essay reviews some key concepts in mathematical epidemiology and examines the intersection of this field with related scientific disciplines, such as chemical reaction network theory and Lagrange-Hamilton geometry. Through a synthesis of theoretical insights and practical perspectives, we underscore the significance of essentially non-negative kinetic systems in the development and implementation of robust epidemiological models. Our purpose is to make the case that currently mathematical modeling of epidemiology is focusing too much on simple particular cases, and maybe not enough on more complex models, whose challenges would require cooperation with scientific computing experts and with researchers in the \"sister disciplines\" involving essentially nonnegative kinetic systems (like virology, ecology, chemical reaction networks, population dynamics, etc).","sentences":["This essay reviews some key concepts in mathematical epidemiology and examines the intersection of this field with related scientific disciplines, such as chemical reaction network theory and Lagrange-Hamilton geometry.","Through a synthesis of theoretical insights and practical perspectives, we underscore the significance of essentially non-negative kinetic systems in the development and implementation of robust epidemiological models.","Our purpose is to make the case that currently mathematical modeling of epidemiology is focusing too much on simple particular cases, and maybe not enough on more complex models, whose challenges would require cooperation with scientific computing experts and with researchers in the \"sister disciplines\" involving essentially nonnegative kinetic systems (like virology, ecology, chemical reaction networks, population dynamics, etc)."],"url":"http://arxiv.org/abs/2406.03269v1","category":"math.DS"}
{"created":"2024-06-05 13:46:50","title":"Relative Entropy for the Numerical Diffusive Limit of the Linear Jin-Xin System","abstract":"This paper deals with the diffusive limit of the Jin and Xin model and its approximation by an asymptotic preserving finite volume scheme. At the continuous level, we determine a convergence rate to the diffusive limit by means of a relative entropy method. Considering a semi-discrete approximation (discrete in space and continuous in time), we adapt the method to this semi-discrete framework and establish that the approximated solutions converge towards the discrete convection-diffusion limit with the same convergence rate.","sentences":["This paper deals with the diffusive limit of the Jin and Xin model and its approximation by an asymptotic preserving finite volume scheme.","At the continuous level, we determine a convergence rate to the diffusive limit by means of a relative entropy method.","Considering a semi-discrete approximation (discrete in space and continuous in time), we adapt the method to this semi-discrete framework and establish that the approximated solutions converge towards the discrete convection-diffusion limit with the same convergence rate."],"url":"http://arxiv.org/abs/2406.03268v1","category":"math.NA"}
{"created":"2024-06-05 13:41:33","title":"Unification with Simple Variable Restrictions and Admissibility of $\u03a0_{2}$-rules","abstract":"We develop a method to recognize admissibility of $\\Pi_{2}$-rules, relating this problem to a specific instance of the unification problem with linear constants restriction, called here \"unification with simple variable restriction\". It is shown that for logical systems enjoying an appropriate algebraic semantics and a finite approximation of left uniform interpolation, this unification with simple variable restriction can be reduced to standard unification. As a corollary, we obtain the decidability of admissibility of $\\Pi_{2}$-rules for many logical systems.","sentences":["We develop a method to recognize admissibility of $\\Pi_{2}$-rules, relating this problem to a specific instance of the unification problem with linear constants restriction, called here \"unification with simple variable restriction\".","It is shown that for logical systems enjoying an appropriate algebraic semantics and a finite approximation of left uniform interpolation, this unification with simple variable restriction can be reduced to standard unification.","As a corollary, we obtain the decidability of admissibility of $\\Pi_{2}$-rules for many logical systems."],"url":"http://arxiv.org/abs/2406.03265v1","category":"math.LO"}
{"created":"2024-06-05 13:40:07","title":"ADer: A Comprehensive Benchmark for Multi-class Visual Anomaly Detection","abstract":"Visual anomaly detection aims to identify anomalous regions in images through unsupervised learning paradigms, with increasing application demand and value in fields such as industrial inspection and medical lesion detection. Despite significant progress in recent years, there is a lack of comprehensive benchmarks to adequately evaluate the performance of various mainstream methods across different datasets under the practical multi-class setting. The absence of standardized experimental setups can lead to potential biases in training epochs, resolution, and metric results, resulting in erroneous conclusions. This paper addresses this issue by proposing a comprehensive visual anomaly detection benchmark, \\textbf{\\textit{ADer}}, which is a modular framework that is highly extensible for new methods. The benchmark includes multiple datasets from industrial and medical domains, implementing fifteen state-of-the-art methods and nine comprehensive metrics. Additionally, we have open-sourced the GPU-assisted \\href{https://pypi.org/project/ADEval}{ADEval} package to address the slow evaluation problem of metrics like time-consuming mAU-PRO on large-scale data, significantly reducing evaluation time by more than \\textit{1000-fold}. Through extensive experimental results, we objectively reveal the strengths and weaknesses of different methods and provide insights into the challenges and future directions of multi-class visual anomaly detection. We hope that \\textbf{\\textit{ADer}} will become a valuable resource for researchers and practitioners in the field, promoting the development of more robust and generalizable anomaly detection systems. Full codes have been attached in Appendix and open-sourced at \\url{https://github.com/zhangzjn/ader}.","sentences":["Visual anomaly detection aims to identify anomalous regions in images through unsupervised learning paradigms, with increasing application demand and value in fields such as industrial inspection and medical lesion detection.","Despite significant progress in recent years, there is a lack of comprehensive benchmarks to adequately evaluate the performance of various mainstream methods across different datasets under the practical multi-class setting.","The absence of standardized experimental setups can lead to potential biases in training epochs, resolution, and metric results, resulting in erroneous conclusions.","This paper addresses this issue by proposing a comprehensive visual anomaly detection benchmark, \\textbf{\\textit{ADer}}, which is a modular framework that is highly extensible for new methods.","The benchmark includes multiple datasets from industrial and medical domains, implementing fifteen state-of-the-art methods and nine comprehensive metrics.","Additionally, we have open-sourced the GPU-assisted \\href{https://pypi.org/project/ADEval}{ADEval} package to address the slow evaluation problem of metrics like time-consuming mAU-PRO on large-scale data, significantly reducing evaluation time by more than \\textit{1000-fold}.","Through extensive experimental results, we objectively reveal the strengths and weaknesses of different methods and provide insights into the challenges and future directions of multi-class visual anomaly detection.","We hope that \\textbf{\\textit{ADer}} will become a valuable resource for researchers and practitioners in the field, promoting the development of more robust and generalizable anomaly detection systems.","Full codes have been attached in Appendix and open-sourced at \\url{https://github.com/zhangzjn/ader}."],"url":"http://arxiv.org/abs/2406.03262v1","category":"cs.CV"}
{"created":"2024-06-05 13:37:42","title":"Feature learning in finite-width Bayesian deep linear networks with multiple outputs and convolutional layers","abstract":"Deep linear networks have been extensively studied, as they provide simplified models of deep learning. However, little is known in the case of finite-width architectures with multiple outputs and convolutional layers. In this manuscript, we provide rigorous results for the statistics of functions implemented by the aforementioned class of networks, thus moving closer to a complete characterization of feature learning in the Bayesian setting. Our results include: (i) an exact and elementary non-asymptotic integral representation for the joint prior distribution over the outputs, given in terms of a mixture of Gaussians; (ii) an analytical formula for the posterior distribution in the case of squared error loss function (Gaussian likelihood); (iii) a quantitative description of the feature learning infinite-width regime, using large deviation theory. From a physical perspective, deep architectures with multiple outputs or convolutional layers represent different manifestations of kernel shape renormalization, and our work provides a dictionary that translates this physics intuition and terminology into rigorous Bayesian statistics.","sentences":["Deep linear networks have been extensively studied, as they provide simplified models of deep learning.","However, little is known in the case of finite-width architectures with multiple outputs and convolutional layers.","In this manuscript, we provide rigorous results for the statistics of functions implemented by the aforementioned class of networks, thus moving closer to a complete characterization of feature learning in the Bayesian setting.","Our results include: (i) an exact and elementary non-asymptotic integral representation for the joint prior distribution over the outputs, given in terms of a mixture of Gaussians; (ii) an analytical formula for the posterior distribution in the case of squared error loss function (Gaussian likelihood); (iii) a quantitative description of the feature learning infinite-width regime, using large deviation theory.","From a physical perspective, deep architectures with multiple outputs or convolutional layers represent different manifestations of kernel shape renormalization, and our work provides a dictionary that translates this physics intuition and terminology into rigorous Bayesian statistics."],"url":"http://arxiv.org/abs/2406.03260v1","category":"stat.ML"}
{"created":"2024-06-05 13:36:25","title":"The Heavier the Faster: A Sub-population of Heavy, Rapidly Spinning and Quickly Evolving Binary Black Holes","abstract":"The spins of binary black holes (BBHs) measured from gravitational waves carry notable information of the formation pathways. Here we propose a quantity \"dimensionless net spin\" ($\\chi_{\\rm N}$), which is related to the sum of angular momentum of component black holes in the system, to provide a novel perspective to study the origin(s) of BBHs. By performing hierarchical Bayesian inference on $\\chi_{\\rm N}$, we find strong evidence that the marginal distribution of this quantity can be better fitted by two Gaussian components rather than one: there is a narrow peak at $\\chi_{\\rm N} \\sim 0.15$ and another extended peak at $\\chi_{\\rm N} \\sim 0.47$. We also find that the rapidly spinning systems likely dominate the high-mass end of the population and they evolve with redshift much quicker. These findings bring new challenges to the field binary scenario, and suggest that dynamical process should plays a key role in forming high total mass BBHs.","sentences":["The spins of binary black holes (BBHs) measured from gravitational waves carry notable information of the formation pathways.","Here we propose a quantity \"dimensionless net spin\" ($\\chi_{\\rm N}$), which is related to the sum of angular momentum of component black holes in the system, to provide a novel perspective to study the origin(s) of BBHs.","By performing hierarchical Bayesian inference on $\\chi_{\\rm N}$, we find strong evidence that the marginal distribution of this quantity can be better fitted by two Gaussian components rather than one: there is a narrow peak at $\\chi_{\\rm N} \\sim 0.15$ and another extended peak at $\\chi_{\\rm N} \\sim 0.47$.","We also find that the rapidly spinning systems likely dominate the high-mass end of the population and they evolve with redshift much quicker.","These findings bring new challenges to the field binary scenario, and suggest that dynamical process should plays a key role in forming high total mass BBHs."],"url":"http://arxiv.org/abs/2406.03257v1","category":"astro-ph.HE"}
{"created":"2024-06-05 13:35:48","title":"On the Maximal Local Disparity of Fairness-Aware Classifiers","abstract":"Fairness has become a crucial aspect in the development of trustworthy machine learning algorithms. Current fairness metrics to measure the violation of demographic parity have the following drawbacks: (i) the average difference of model predictions on two groups cannot reflect their distribution disparity, and (ii) the overall calculation along all possible predictions conceals the extreme local disparity at or around certain predictions. In this work, we propose a novel fairness metric called Maximal Cumulative ratio Disparity along varying Predictions' neighborhood (MCDP), for measuring the maximal local disparity of the fairness-aware classifiers. To accurately and efficiently calculate the MCDP, we develop a provably exact and an approximate calculation algorithm that greatly reduces the computational complexity with low estimation error. We further propose a bi-level optimization algorithm using a differentiable approximation of the MCDP for improving the algorithmic fairness. Extensive experiments on both tabular and image datasets validate that our fair training algorithm can achieve superior fairness-accuracy trade-offs.","sentences":["Fairness has become a crucial aspect in the development of trustworthy machine learning algorithms.","Current fairness metrics to measure the violation of demographic parity have the following drawbacks: (i) the average difference of model predictions on two groups cannot reflect their distribution disparity, and (ii) the overall calculation along all possible predictions conceals the extreme local disparity at or around certain predictions.","In this work, we propose a novel fairness metric called Maximal Cumulative ratio Disparity along varying Predictions' neighborhood (MCDP), for measuring the maximal local disparity of the fairness-aware classifiers.","To accurately and efficiently calculate the MCDP, we develop a provably exact and an approximate calculation algorithm that greatly reduces the computational complexity with low estimation error.","We further propose a bi-level optimization algorithm using a differentiable approximation of the MCDP for improving the algorithmic fairness.","Extensive experiments on both tabular and image datasets validate that our fair training algorithm can achieve superior fairness-accuracy trade-offs."],"url":"http://arxiv.org/abs/2406.03255v1","category":"cs.LG"}
{"created":"2024-06-05 13:26:25","title":"Near-field Beamforming for Extremely Large-scale MIMO Based on Unsupervised Deep Learning","abstract":"Extremely Large-scale Array (ELAA) is considered a frontier technology for future communication systems, pivotal in improving wireless systems' rate and spectral efficiency. However, as ELAA employs a multitude of antennas operating at higher frequencies, users are typically situated in the near-field region where the spherical wavefront propagates. This inevitably leads to a significant increase in the overhead of beam training, requiring complex two-dimensional beam searching in both the angle domain and the distance domain. To address this problem, we propose a near-field beamforming method based on unsupervised deep learning. Our convolutional neural network efficiently extracts complex channel state information features by strategically selecting padding and kernel size. We optimize the beamformers to maximize achievable rates in a multi-user network without relying on predefined custom codebooks. Upon deployment, the model requires solely the input of pre-estimated channel state information to derive the optimal beamforming vector. Simulation results show that our proposed scheme can obtain stable beamforming gain compared with the baseline scheme. Furthermore, owing to the inherent traits of deep learning methodologies, this approach substantially diminishes the beam training costs in near-field regions.","sentences":["Extremely Large-scale Array (ELAA) is considered a frontier technology for future communication systems, pivotal in improving wireless systems' rate and spectral efficiency.","However, as ELAA employs a multitude of antennas operating at higher frequencies, users are typically situated in the near-field region where the spherical wavefront propagates.","This inevitably leads to a significant increase in the overhead of beam training, requiring complex two-dimensional beam searching in both the angle domain and the distance domain.","To address this problem, we propose a near-field beamforming method based on unsupervised deep learning.","Our convolutional neural network efficiently extracts complex channel state information features by strategically selecting padding and kernel size.","We optimize the beamformers to maximize achievable rates in a multi-user network without relying on predefined custom codebooks.","Upon deployment, the model requires solely the input of pre-estimated channel state information to derive the optimal beamforming vector.","Simulation results show that our proposed scheme can obtain stable beamforming gain compared with the baseline scheme.","Furthermore, owing to the inherent traits of deep learning methodologies, this approach substantially diminishes the beam training costs in near-field regions."],"url":"http://arxiv.org/abs/2406.03249v1","category":"cs.LG"}
{"created":"2024-06-05 13:20:18","title":"Llumnix: Dynamic Scheduling for Large Language Model Serving","abstract":"Inference serving for large language models (LLMs) is the key to unleashing their potential in people's daily lives. However, efficient LLM serving remains challenging today because the requests are inherently heterogeneous and unpredictable in terms of resource and latency requirements, as a result of the diverse applications and the dynamic execution nature of LLMs. Existing systems are fundamentally limited in handling these characteristics and cause problems such as severe queuing delays, poor tail latencies, and SLO violations.   We introduce Llumnix, an LLM serving system that reacts to such heterogeneous and unpredictable requests by runtime rescheduling across multiple model instances. Similar to context switching across CPU cores in modern operating systems, Llumnix reschedules requests to improve load balancing and isolation, mitigate resource fragmentation, and differentiate request priorities and SLOs. Llumnix implements the rescheduling with an efficient and scalable live migration mechanism for requests and their in-memory states, and exploits it in a dynamic scheduling policy that unifies the multiple rescheduling scenarios elegantly. Our evaluations show that Llumnix improves tail latencies by an order of magnitude, accelerates high-priority requests by up to 1.5x, and delivers up to 36% cost savings while achieving similar tail latencies, compared against state-of-the-art LLM serving systems. Llumnix is publicly available at https://github.com/AlibabaPAI/llumnix.","sentences":["Inference serving for large language models (LLMs) is the key to unleashing their potential in people's daily lives.","However, efficient LLM serving remains challenging today because the requests are inherently heterogeneous and unpredictable in terms of resource and latency requirements, as a result of the diverse applications and the dynamic execution nature of LLMs.","Existing systems are fundamentally limited in handling these characteristics and cause problems such as severe queuing delays, poor tail latencies, and SLO violations.   ","We introduce Llumnix, an LLM serving system that reacts to such heterogeneous and unpredictable requests by runtime rescheduling across multiple model instances.","Similar to context switching across CPU cores in modern operating systems, Llumnix reschedules requests to improve load balancing and isolation, mitigate resource fragmentation, and differentiate request priorities and SLOs.","Llumnix implements the rescheduling with an efficient and scalable live migration mechanism for requests and their in-memory states, and exploits it in a dynamic scheduling policy that unifies the multiple rescheduling scenarios elegantly.","Our evaluations show that Llumnix improves tail latencies by an order of magnitude, accelerates high-priority requests by up to 1.5x, and delivers up to 36% cost savings while achieving similar tail latencies, compared against state-of-the-art LLM serving systems.","Llumnix is publicly available at https://github.com/AlibabaPAI/llumnix."],"url":"http://arxiv.org/abs/2406.03243v1","category":"cs.AR"}
{"created":"2024-06-05 13:17:08","title":"Temperature Illusions in Mixed Reality using Color and Dynamic Graphics","abstract":"Sensory illusions - where a sensory stimulus causes people to perceive effects that are altered by a different sensory stimulus - have the potential to enrich mixed-reality based interactions. The well-known colour-temperature illusion is a sensory illusion that causes people to, somewhat counterintuitively, perceive blue objects to feel warmer and red objects to feel colder. There is currently little information about whether this illusion can be recreated in mixed reality (MR). Additionally, it is unknown whether dynamic graphical effects made possible by mixed-reality systems could create a similar or potentially stronger effect to the color-temperature illusion. The results of our study (n=30) support that the color-temperature illusion can be recreated in MR and that dynamic graphics can create a new temperature-sensory illusion. Our dynamic-graphics-temperature illusion creates a stronger effect than the color-temperature illusion and has more intuitive relationship between the stimulus and the effect: cold graphical effects (a virtual ice ball) are perceived as colder and hot graphical effects (a virtual fire ball) as hotter. Our results demonstrate that mixed reality has the potential to create novel and stronger temperature-based illusions and encourage further investigation into graphical effects to shape user perception.","sentences":["Sensory illusions - where a sensory stimulus causes people to perceive effects that are altered by a different sensory stimulus - have the potential to enrich mixed-reality based interactions.","The well-known colour-temperature illusion is a sensory illusion that causes people to, somewhat counterintuitively, perceive blue objects to feel warmer and red objects to feel colder.","There is currently little information about whether this illusion can be recreated in mixed reality (MR).","Additionally, it is unknown whether dynamic graphical effects made possible by mixed-reality systems could create a similar or potentially stronger effect to the color-temperature illusion.","The results of our study (n=30) support that the color-temperature illusion can be recreated in MR and that dynamic graphics can create a new temperature-sensory illusion.","Our dynamic-graphics-temperature illusion creates a stronger effect than the color-temperature illusion and has more intuitive relationship between the stimulus and the effect: cold graphical effects (a virtual ice ball) are perceived as colder and hot graphical effects (a virtual fire ball) as hotter.","Our results demonstrate that mixed reality has the potential to create novel and stronger temperature-based illusions and encourage further investigation into graphical effects to shape user perception."],"url":"http://arxiv.org/abs/2406.03241v1","category":"cs.HC"}
{"created":"2024-06-05 13:07:16","title":"Higher Order Lipschitz Greedy Recombination Interpolation Method (HOLGRIM)","abstract":"In this paper we introduce the Higher Order Lipschitz Greedy Recombination Interpolation Method (HOLGRIM) for finding sparse approximations of Lip$(\\gamma)$ functions, in the sense of Stein, given as a linear combination of a (large) number of simpler Lip$(\\gamma)$ functions. HOLGRIM is developed as a refinement of the Greedy Recombination Interpolation Method (GRIM) in the setting of Lip$(\\gamma)$ functions. HOLGRIM combines dynamic growth-based interpolation techniques with thinning-based reduction techniques in a data-driven fashion. The dynamic growth is driven by a greedy selection algorithm in which multiple new points may be selected at each step. The thinning reduction is carried out by recombination, the linear algebra technique utilised by GRIM. We establish that the number of non-zero weights for the approximation returned by HOLGRIM is controlled by a particular packing number of the data. The level of data concentration required to guarantee that HOLGRIM returns a good sparse approximation is decreasing with respect to the regularity parameter $\\gamma > 0$. Further, we establish complexity cost estimates verifying that implementing HOLGRIM is feasible.","sentences":["In this paper we introduce the Higher Order Lipschitz Greedy Recombination Interpolation Method (HOLGRIM) for finding sparse approximations of Lip$(\\gamma)$ functions, in the sense of Stein, given as a linear combination of a (large) number of simpler Lip$(\\gamma)$ functions.","HOLGRIM is developed as a refinement of the Greedy Recombination Interpolation Method (GRIM) in the setting of Lip$(\\gamma)$ functions.","HOLGRIM combines dynamic growth-based interpolation techniques with thinning-based reduction techniques in a data-driven fashion.","The dynamic growth is driven by a greedy selection algorithm in which multiple new points may be selected at each step.","The thinning reduction is carried out by recombination, the linear algebra technique utilised by GRIM.","We establish that the number of non-zero weights for the approximation returned by HOLGRIM is controlled by a particular packing number of the data.","The level of data concentration required to guarantee that HOLGRIM returns a good sparse approximation is decreasing with respect to the regularity parameter $\\gamma > 0$.","Further, we establish complexity cost estimates verifying that implementing HOLGRIM is feasible."],"url":"http://arxiv.org/abs/2406.03232v1","category":"math.NA"}
{"created":"2024-06-05 13:01:55","title":"Exponentially Stable Projector-based Control of Lagrangian Systems with Gaussian Processes","abstract":"Designing accurate yet robust tracking controllers with tight performance guarantees for Lagrangian systems is challenging due to nonlinear modeling uncertainties and conservative stability criteria. This article proposes a structure-preserving projector-based tracking control law for uncertain Euler-Lagrange (EL) systems using physically consistent Lagrangian-Gaussian Processes (L-GPs). We leverage the uncertainty quantification of the L-GP for adaptive feedforward-feedback balancing. In particular, an accurate probabilistic guarantee for exponential stability is derived by leveraging matrix analysis results and contraction theory, where the benefit of the proposed controller is proven and shown in the closed-form expressions for convergence rate and radius. Extensive numerical simulations not only demonstrate the controller's efficacy based on a two-link and a soft robotic manipulator but also all theoretical results are explicitly analyzed and validated.","sentences":["Designing accurate yet robust tracking controllers with tight performance guarantees for Lagrangian systems is challenging due to nonlinear modeling uncertainties and conservative stability criteria.","This article proposes a structure-preserving projector-based tracking control law for uncertain Euler-Lagrange (EL) systems using physically consistent Lagrangian-Gaussian Processes (L-GPs).","We leverage the uncertainty quantification of the L-GP for adaptive feedforward-feedback balancing.","In particular, an accurate probabilistic guarantee for exponential stability is derived by leveraging matrix analysis results and contraction theory, where the benefit of the proposed controller is proven and shown in the closed-form expressions for convergence rate and radius.","Extensive numerical simulations not only demonstrate the controller's efficacy based on a two-link and a soft robotic manipulator but also all theoretical results are explicitly analyzed and validated."],"url":"http://arxiv.org/abs/2406.03224v1","category":"eess.SY"}
{"created":"2024-06-05 13:01:36","title":"Object Manipulation in Marine Environments using Reinforcement Learning","abstract":"Performing intervention tasks in the maritime domain is crucial for safety and operational efficiency. The unpredictable and dynamic marine environment makes the intervention tasks such as object manipulation extremely challenging. This study proposes a robust solution for object manipulation from a dock in the presence of disturbances caused by sea waves. To tackle this challenging problem, we apply a deep reinforcement learning (DRL) based algorithm called Soft. Actor-Critic (SAC). SAC employs an actor-critic framework; the actors learn a policy that minimizes an objective function while the critic evaluates the learned policy and provides feedback to guide the actor-learning process. We trained the agent using the PyBullet dynamic simulator and tested it in a realistic simulation environment called MBZIRC maritime simulator. This simulator allows the simulation of different wave conditions according to the World Meteorological Organization (WMO) sea state code. Simulation results demonstrate a high success rate in retrieving the objects from the dock. The trained agent achieved an 80 percent success rate when applied in the simulation environment in the presence of waves characterized by sea state 2, according to the WMO sea state code","sentences":["Performing intervention tasks in the maritime domain is crucial for safety and operational efficiency.","The unpredictable and dynamic marine environment makes the intervention tasks such as object manipulation extremely challenging.","This study proposes a robust solution for object manipulation from a dock in the presence of disturbances caused by sea waves.","To tackle this challenging problem, we apply a deep reinforcement learning (DRL) based algorithm called Soft.","Actor-Critic (SAC).","SAC employs an actor-critic framework; the actors learn a policy that minimizes an objective function while the critic evaluates the learned policy and provides feedback to guide the actor-learning process.","We trained the agent using the PyBullet dynamic simulator and tested it in a realistic simulation environment called MBZIRC maritime simulator.","This simulator allows the simulation of different wave conditions according to the World Meteorological Organization (WMO) sea state code.","Simulation results demonstrate a high success rate in retrieving the objects from the dock.","The trained agent achieved an 80 percent success rate when applied in the simulation environment in the presence of waves characterized by sea state 2, according to the WMO sea state code"],"url":"http://arxiv.org/abs/2406.03223v1","category":"cs.RO"}
{"created":"2024-06-05 13:00:22","title":"Solving Sharp Bounded-error Quantum Polynomial Time Problem by Evolution methods","abstract":"Counting ground state degeneracy of a $k$-local Hamiltonian is important in many fields of physics. Its complexity belongs to the problem of sharp bounded-error quantum polynomial time (#BQP) class and few methods are known for its solution. Finding ground states of a $k$-local Hamiltonian, on the other hand, is an easier problem of Quantum Merlin Arthur (QMA) class, for which many efficient methods exist. In this work, we propose an algorithm of mapping a #BQP problem into one of finding a special ground state of a $k$-local Hamiltonian. We prove that all traditional methods, which solve the QMA problem by evolution under a function of a Hamiltonian, can be used to find the special ground state from a well-designed initial state, thus can solve the #BQP problem. We combine our algorithm with power method, Lanczos method, and quantum imaginary time evolution method for different systems to illustrate the detection of phase boundaries, competition between frustration and quantum fluctuation, and potential implementations with quantum circuits.","sentences":["Counting ground state degeneracy of a $k$-local Hamiltonian is important in many fields of physics.","Its complexity belongs to the problem of sharp bounded-error quantum polynomial time (#BQP) class and few methods are known for its solution.","Finding ground states of a $k$-local Hamiltonian, on the other hand, is an easier problem of Quantum Merlin Arthur (QMA) class, for which many efficient methods exist.","In this work, we propose an algorithm of mapping a #BQP problem into one of finding a special ground state of a $k$-local Hamiltonian.","We prove that all traditional methods, which solve the QMA problem by evolution under a function of a Hamiltonian, can be used to find the special ground state from a well-designed initial state, thus can solve the #BQP problem.","We combine our algorithm with power method, Lanczos method, and quantum imaginary time evolution method for different systems to illustrate the detection of phase boundaries, competition between frustration and quantum fluctuation, and potential implementations with quantum circuits."],"url":"http://arxiv.org/abs/2406.03222v1","category":"quant-ph"}
{"created":"2024-06-05 12:51:20","title":"Inferring the time-varying coupling of dynamical systems with temporal convolutional autoencoders","abstract":"Most approaches for assessing causality in complex dynamical systems fail when the interactions between variables are inherently non-linear and non-stationary. Here we introduce Temporal Autoencoders for Causal Inference (TACI), a methodology that combines a new surrogate data metric for assessing causal interactions with a novel two-headed machine learning architecture to identify and measure the direction and strength of time-varying causal interactions. Through tests on both synthetic and real-world datasets, we demonstrate TACI's ability to accurately quantify dynamic causal interactions across a variety of systems. Our findings display the method's effectiveness compared to existing approaches and also highlight our approach's potential to build a deeper understanding of the mechanisms that underlie time-varying interactions in physical and biological systems.","sentences":["Most approaches for assessing causality in complex dynamical systems fail when the interactions between variables are inherently non-linear and non-stationary.","Here we introduce Temporal Autoencoders for Causal Inference (TACI), a methodology that combines a new surrogate data metric for assessing causal interactions with a novel two-headed machine learning architecture to identify and measure the direction and strength of time-varying causal interactions.","Through tests on both synthetic and real-world datasets, we demonstrate TACI's ability to accurately quantify dynamic causal interactions across a variety of systems.","Our findings display the method's effectiveness compared to existing approaches and also highlight our approach's potential to build a deeper understanding of the mechanisms that underlie time-varying interactions in physical and biological systems."],"url":"http://arxiv.org/abs/2406.03212v1","category":"cs.LG"}
{"created":"2024-06-05 12:43:33","title":"CoLLAB: A Collaborative Approach for Multilingual Abuse Detection","abstract":"In this study, we investigate representations from paralingual Pre-Trained model (PTM) for Audio Abuse Detection (AAD), which has not been explored for AAD. Our results demonstrate their superiority compared to other PTM representations on the ADIMA benchmark. Furthermore, combining PTM representations enhances AAD performance. Despite these improvements, challenges with cross-lingual generalizability still remain, and certain languages require training in the same language. This demands individual models for different languages, leading to scalability, maintenance, and resource allocation issues and hindering the practical deployment of AAD systems in linguistically diverse real-world environments. To address this, we introduce CoLLAB, a novel framework that doesn't require training and allows seamless merging of models trained in different languages through weight-averaging. This results in a unified model with competitive AAD performance across multiple languages.","sentences":["In this study, we investigate representations from paralingual Pre-Trained model (PTM) for Audio Abuse Detection (AAD), which has not been explored for AAD.","Our results demonstrate their superiority compared to other PTM representations on the ADIMA benchmark.","Furthermore, combining PTM representations enhances AAD performance.","Despite these improvements, challenges with cross-lingual generalizability still remain, and certain languages require training in the same language.","This demands individual models for different languages, leading to scalability, maintenance, and resource allocation issues and hindering the practical deployment of AAD systems in linguistically diverse real-world environments.","To address this, we introduce CoLLAB, a novel framework that doesn't require training and allows seamless merging of models trained in different languages through weight-averaging.","This results in a unified model with competitive AAD performance across multiple languages."],"url":"http://arxiv.org/abs/2406.03205v1","category":"eess.AS"}
{"created":"2024-06-05 12:37:21","title":"Fokas-Lenells Derivative nonlinear Schr\u00f6dinger equation its associated soliton surfaces and Gaussian curvature","abstract":"One of the most important tasks in mathematics and physics is to connect differential geometry and nonlinear differential equations. In the study of nonlinear optics, integrable nonlinear differential equations such as the nonlinear Schr\\\"odinger equation (NLSE) and higher-order NLSE (HNLSE) play crucial roles. Because of the medium's balance between dispersion and nonlinearity, all of these systems display soliton solutions. The soliton surfaces, or manifolds, connected to these integrable systems hold significance in numerous areas of mathematics and physics. We examine the use of soliton theory in differential geometry in this paper. We build the two-dimensional soliton surface in the three-dimensional Euclidean space by taking into account the Fokas-Lenells Derivative nonlinear Schr\\\"odinger equation (also known as the gauged Fokas-Lenells equation). The same is constructed by us using the Sym-Tafel formula. The first and second fundamental forms, surface area, and Gaussian curvature are obtained using a Lax representation of the gauged FLE.","sentences":["One of the most important tasks in mathematics and physics is to connect differential geometry and nonlinear differential equations.","In the study of nonlinear optics, integrable nonlinear differential equations such as the nonlinear Schr\\\"odinger equation (NLSE) and higher-order NLSE (HNLSE) play crucial roles.","Because of the medium's balance between dispersion and nonlinearity, all of these systems display soliton solutions.","The soliton surfaces, or manifolds, connected to these integrable systems hold significance in numerous areas of mathematics and physics.","We examine the use of soliton theory in differential geometry in this paper.","We build the two-dimensional soliton surface in the three-dimensional Euclidean space by taking into account the Fokas-Lenells Derivative nonlinear Schr\\\"odinger equation (also known as the gauged Fokas-Lenells equation).","The same is constructed by us using the Sym-Tafel formula.","The first and second fundamental forms, surface area, and Gaussian curvature are obtained using a Lax representation of the gauged FLE."],"url":"http://arxiv.org/abs/2406.03203v1","category":"nlin.SI"}
{"created":"2024-06-05 12:22:58","title":"Highly sensitive AuNCs@GSH/Ch-PtNPs metal nanoprobes for fluorescent and colorimetric dual-mode detection of ascorbic acid in drink","abstract":"Fluorescence detection is a commonly used analytical method with the advantages of fast response, good selectivity and low destructiveness. However, fluorescence detection, a single-mode detection method, has some limitations, such as background interference that affects the accuracy of the fluorescence signal, lack of visualization of the detection results, and low sensitivity for detecting low-concentration samples. In order to overcome the shortcomings of fluorescence single-mode detection, we used the dual-mode method of fluorescence and colorimetry to detect ascorbic acid.   The dual-mode detection of AA by fluorescence and colorimetry in the probe system enhances the specificity and accuracy of the detection. This bimodal detection method solved the problem of low detection sensitivity in the low concentration range of the analytes to be tested, and was linear in the lower (0-50 {\\mu}M) and higher (50-350 {\\mu}M) concentration ranges, respectively, and had a lower detection limit (0.034 {\\mu}M). This glutathione-based gold cluster assay is characterized by simplicity, rapidity and accuracy, and provides a new way for the quantitative analysis of ascorbic acid. In addition, the method was validated during the determination of AA in beverages, which has the advantages of high sensitivity and fast response time.","sentences":["Fluorescence detection is a commonly used analytical method with the advantages of fast response, good selectivity and low destructiveness.","However, fluorescence detection, a single-mode detection method, has some limitations, such as background interference that affects the accuracy of the fluorescence signal, lack of visualization of the detection results, and low sensitivity for detecting low-concentration samples.","In order to overcome the shortcomings of fluorescence single-mode detection, we used the dual-mode method of fluorescence and colorimetry to detect ascorbic acid.   ","The dual-mode detection of AA by fluorescence and colorimetry in the probe system enhances the specificity and accuracy of the detection.","This bimodal detection method solved the problem of low detection sensitivity in the low concentration range of the analytes to be tested, and was linear in the lower (0-50 {\\mu}M) and higher (50-350 {\\mu}M) concentration ranges, respectively, and had a lower detection limit (0.034 {\\mu}M).","This glutathione-based gold cluster assay is characterized by simplicity, rapidity and accuracy, and provides a new way for the quantitative analysis of ascorbic acid.","In addition, the method was validated during the determination of AA in beverages, which has the advantages of high sensitivity and fast response time."],"url":"http://arxiv.org/abs/2406.03192v1","category":"physics.optics"}
{"created":"2024-06-05 12:20:57","title":"Novel Atmospheric Dynamics Shape Inner Edge of Habitable Zone Around White Dwarfs","abstract":"White dwarfs offer a unique opportunity to search nearby stellar systems for signs of life, but the habitable zone around these stars is still poorly understood. Since white dwarfs are compact stars with low luminosity, any planets in their habitable zone should be tidally locked, like planets around M-dwarfs. Unlike planets around M-dwarfs, however, habitable white dwarf planets have to rotate very rapidly, with orbital periods ranging from hours to several days. Here we use the ExoCAM Global Climate Model (GCM) to investigate the inner edge of the habitable zone (HZ) around white dwarfs. Our simulations show habitable planets with ultrashort orbital periods ($P\\lesssim$1 day) enter a ``bat rotation\" regime, which differs from typical atmospheric circulation regimes around M dwarfs. Bat rotators feature mean equatorial subrotation and a displacement of the surface's hottest regions from the equator towards the midlatitudes. We qualitatively explain the onset of bat rotation using shallow water theory. The resulting circulation shifts increase dayside cloud cover and decrease stratospheric water vapor, expanding the white dwarf habitable zone by $\\sim$50\\% compared to estimates based on 1D models. The James Webb Space Telescope (JWST) should be able to quickly characterize bat rotators around nearby white dwarfs thanks to their distinct thermal phase curves. Our work underlines that tidally locked planets on ultrashort orbits may exhibit unique atmospheric dynamics, and guides future habitability studies of white dwarf systems.","sentences":["White dwarfs offer a unique opportunity to search nearby stellar systems for signs of life, but the habitable zone around these stars is still poorly understood.","Since white dwarfs are compact stars with low luminosity, any planets in their habitable zone should be tidally locked, like planets around M-dwarfs.","Unlike planets around M-dwarfs, however, habitable white dwarf planets have to rotate very rapidly, with orbital periods ranging from hours to several days.","Here we use the ExoCAM Global Climate Model (GCM) to investigate the inner edge of the habitable zone (HZ) around white dwarfs.","Our simulations show habitable planets with ultrashort orbital periods ($P\\lesssim$1 day) enter a ``bat rotation\" regime, which differs from typical atmospheric circulation regimes around M dwarfs.","Bat rotators feature mean equatorial subrotation and a displacement of the surface's hottest regions from the equator towards the midlatitudes.","We qualitatively explain the onset of bat rotation using shallow water theory.","The resulting circulation shifts increase dayside cloud cover and decrease stratospheric water vapor, expanding the white dwarf habitable zone by $\\sim$50\\% compared to estimates based on 1D models.","The James Webb Space Telescope (JWST) should be able to quickly characterize bat rotators around nearby white dwarfs thanks to their distinct thermal phase curves.","Our work underlines that tidally locked planets on ultrashort orbits may exhibit unique atmospheric dynamics, and guides future habitability studies of white dwarf systems."],"url":"http://arxiv.org/abs/2406.03189v1","category":"astro-ph.EP"}
{"created":"2024-06-05 12:11:10","title":"Missci: Reconstructing Fallacies in Misrepresented Science","abstract":"Health-related misinformation on social networks can lead to poor decision-making and real-world dangers. Such misinformation often misrepresents scientific publications and cites them as \"proof\" to gain perceived credibility. To effectively counter such claims automatically, a system must explain how the claim was falsely derived from the cited publication. Current methods for automated fact-checking or fallacy detection neglect to assess the (mis)used evidence in relation to misinformation claims, which is required to detect the mismatch between them. To address this gap, we introduce Missci, a novel argumentation theoretical model for fallacious reasoning together with a new dataset for real-world misinformation detection that misrepresents biomedical publications. Unlike previous fallacy detection datasets, Missci (i) focuses on implicit fallacies between the relevant content of the cited publication and the inaccurate claim, and (ii) requires models to verbalize the fallacious reasoning in addition to classifying it. We present Missci as a dataset to test the critical reasoning abilities of large language models (LLMs), that are required to reconstruct real-world fallacious arguments, in a zero-shot setting. We evaluate two representative LLMs and the impact of different levels of detail about the fallacy classes provided to the LLM via prompts. Our experiments and human evaluation show promising results for GPT 4, while also demonstrating the difficulty of this task.","sentences":["Health-related misinformation on social networks can lead to poor decision-making and real-world dangers.","Such misinformation often misrepresents scientific publications and cites them as \"proof\" to gain perceived credibility.","To effectively counter such claims automatically, a system must explain how the claim was falsely derived from the cited publication.","Current methods for automated fact-checking or fallacy detection neglect to assess the (mis)used evidence in relation to misinformation claims, which is required to detect the mismatch between them.","To address this gap, we introduce Missci, a novel argumentation theoretical model for fallacious reasoning together with a new dataset for real-world misinformation detection that misrepresents biomedical publications.","Unlike previous fallacy detection datasets, Missci (i) focuses on implicit fallacies between the relevant content of the cited publication and the inaccurate claim, and (ii) requires models to verbalize the fallacious reasoning in addition to classifying it.","We present Missci as a dataset to test the critical reasoning abilities of large language models (LLMs), that are required to reconstruct real-world fallacious arguments, in a zero-shot setting.","We evaluate two representative LLMs and the impact of different levels of detail about the fallacy classes provided to the LLM via prompts.","Our experiments and human evaluation show promising results for GPT 4, while also demonstrating the difficulty of this task."],"url":"http://arxiv.org/abs/2406.03181v1","category":"cs.CL"}
{"created":"2024-06-05 12:08:18","title":"Identification of PK-PD Insulin Models using Experimental GIR Data","abstract":"We present a method to estimate parameters in pharmacokinetic (PK) and pharmacodynamic (PD) models for glucose insulin dynamics in humans. The method combines 1) experimental glucose infusion rate (GIR) data from glucose clamp studies and 2) a PK-PD model to estimate parameters such that the model fits the data. Assuming that the glucose clamp is perfect, we do not need to know the details of the controller in the clamp, and the GIR can be computed directly from the PK-PD model. To illustrate the procedure, we use the glucoregulatory model developed by Hovorka and modify it to have a smooth non-negative endogeneous glucose production (EGP) term. We estimate PK-PD parameters for rapid-acting insulin analogs (Fiasp and NovoRapid). We use these PK-PD parameters to illustrate GIR for insulin analogs with 30% and 50% faster absorption time than currently available rapid-acting insulin analogs. We discuss the role of system identification using GIR data from glucose clamp studies and how such identified models can be used in automated insulin dosing (AID) systems with ultra rapid-acting insulin.","sentences":["We present a method to estimate parameters in pharmacokinetic (PK) and pharmacodynamic (PD) models for glucose insulin dynamics in humans.","The method combines 1) experimental glucose infusion rate (GIR) data from glucose clamp studies and 2) a PK-PD model to estimate parameters such that the model fits the data.","Assuming that the glucose clamp is perfect, we do not need to know the details of the controller in the clamp, and the GIR can be computed directly from the PK-PD model.","To illustrate the procedure, we use the glucoregulatory model developed by Hovorka and modify it to have a smooth non-negative endogeneous glucose production (EGP) term.","We estimate PK-PD parameters for rapid-acting insulin analogs (Fiasp and NovoRapid).","We use these PK-PD parameters to illustrate GIR for insulin analogs with 30% and 50% faster absorption time than currently available rapid-acting insulin analogs.","We discuss the role of system identification using GIR data from glucose clamp studies and how such identified models can be used in automated insulin dosing (AID) systems with ultra rapid-acting insulin."],"url":"http://arxiv.org/abs/2406.03178v1","category":"q-bio.QM"}
{"created":"2024-06-05 12:08:01","title":"FAPNet: An Effective Frequency Adaptive Point-based Eye Tracker","abstract":"Eye tracking is crucial for human-computer interaction in different domains. Conventional cameras encounter challenges such as power consumption and image quality during different eye movements, prompting the need for advanced solutions with ultra-fast, low-power, and accurate eye trackers. Event cameras, fundamentally designed to capture information about moving objects, exhibit low power consumption and high temporal resolution. This positions them as an alternative to traditional cameras in the realm of eye tracking. Nevertheless, existing event-based eye tracking networks neglect the pivotal sparse and fine-grained temporal information in events, resulting in unsatisfactory performance. Moreover, the energy-efficient features are further compromised by the use of excessively complex models, hindering efficient deployment on edge devices. In this paper, we utilize Point Cloud as the event representation to harness the high temporal resolution and sparse characteristics of events in eye tracking tasks. We rethink the point-based architecture PEPNet with preprocessing the long-term relationships between samples, leading to the innovative design of FAPNet. A frequency adaptive mechanism is designed to realize adaptive tracking according to the speed of the pupil movement and the Inter Sample LSTM module is introduced to utilize the temporal correlation between samples. In the Event-based Eye Tracking Challenge, we utilize vanilla PEPNet, which is the former work to achieve the $p_{10}$ accuracy of 97.95\\%. On the SEET synthetic dataset, FAPNet can achieve state-of-the-art while consuming merely 10\\% of the PEPNet's computational resources. Notably, the computational demand of FAPNet is independent of the sensor's spatial resolution, enhancing its applicability on resource-limited edge devices.","sentences":["Eye tracking is crucial for human-computer interaction in different domains.","Conventional cameras encounter challenges such as power consumption and image quality during different eye movements, prompting the need for advanced solutions with ultra-fast, low-power, and accurate eye trackers.","Event cameras, fundamentally designed to capture information about moving objects, exhibit low power consumption and high temporal resolution.","This positions them as an alternative to traditional cameras in the realm of eye tracking.","Nevertheless, existing event-based eye tracking networks neglect the pivotal sparse and fine-grained temporal information in events, resulting in unsatisfactory performance.","Moreover, the energy-efficient features are further compromised by the use of excessively complex models, hindering efficient deployment on edge devices.","In this paper, we utilize Point Cloud as the event representation to harness the high temporal resolution and sparse characteristics of events in eye tracking tasks.","We rethink the point-based architecture PEPNet with preprocessing the long-term relationships between samples, leading to the innovative design of FAPNet.","A frequency adaptive mechanism is designed to realize adaptive tracking according to the speed of the pupil movement and the Inter Sample LSTM module is introduced to utilize the temporal correlation between samples.","In the Event-based Eye Tracking Challenge, we utilize vanilla PEPNet, which is the former work to achieve the $p_{10}$ accuracy of 97.95\\%.","On the SEET synthetic dataset, FAPNet can achieve state-of-the-art while consuming merely 10\\% of the PEPNet's computational resources.","Notably, the computational demand of FAPNet is independent of the sensor's spatial resolution, enhancing its applicability on resource-limited edge devices."],"url":"http://arxiv.org/abs/2406.03177v1","category":"cs.CV"}
{"created":"2024-06-05 11:57:02","title":"Floorplanning with I/O assignment via feasibility-seeking and superiorization methods","abstract":"The feasibility-seeking approach offers a systematic framework for managing and resolving intricate constraints in continuous problems, making it a promising avenue to explore in the context of floorplanning problems with increasingly heterogeneous constraints. The classic legality constraints can be expressed as the union of convex sets. In implementation, we introduce a resetting strategy aimed at effectively reducing the problem of algorithmic divergence in the projection-based method used for the feasibility-seeking formulation. Furthermore, we introduce the novel application of the superiorization method (SM) to floorplanning, which bridges the gap between feasibility-seeking and constrained optimization. The SM employs perturbations to steer the iterations of the feasibility-seeking algorithm towards feasible solutions with reduced (not necessarily minimal) total wirelength. To evaluate the performance of Per-RMAP, we conduct comprehensive experiments on the MCNC benchmarks and GSRC benchmarks. The results demonstrate that we can obtain legal floorplanning results 166 times faster than the branch-and-bound (B&B) method while incurring only a 5% wirelength increase compared to the optimal results. Furthermore, we evaluate the effectiveness of the algorithmic flow that considers the I/O assignment constraints, which achieves an 6% improvement in wirelength. Besides, considering the soft modules with a larger feasible solution space, we obtain 15% improved runtime compared with PeF, the state-of-the-art analytical method. Moreover, we compared our method with Parquet-4 and Fast-SA on GSRC benchmarks which include larger-scale instances. The results highlight the ability of our approach to maintain a balance between floorplanning quality and efficiency.","sentences":["The feasibility-seeking approach offers a systematic framework for managing and resolving intricate constraints in continuous problems, making it a promising avenue to explore in the context of floorplanning problems with increasingly heterogeneous constraints.","The classic legality constraints can be expressed as the union of convex sets.","In implementation, we introduce a resetting strategy aimed at effectively reducing the problem of algorithmic divergence in the projection-based method used for the feasibility-seeking formulation.","Furthermore, we introduce the novel application of the superiorization method (SM) to floorplanning, which bridges the gap between feasibility-seeking and constrained optimization.","The SM employs perturbations to steer the iterations of the feasibility-seeking algorithm towards feasible solutions with reduced (not necessarily minimal) total wirelength.","To evaluate the performance of Per-RMAP, we conduct comprehensive experiments on the MCNC benchmarks and GSRC benchmarks.","The results demonstrate that we can obtain legal floorplanning results 166 times faster than the branch-and-bound (B&B) method while incurring only a 5% wirelength increase compared to the optimal results.","Furthermore, we evaluate the effectiveness of the algorithmic flow that considers the I/O assignment constraints, which achieves an 6% improvement in wirelength.","Besides, considering the soft modules with a larger feasible solution space, we obtain 15% improved runtime compared with PeF, the state-of-the-art analytical method.","Moreover, we compared our method with Parquet-4 and Fast-SA on GSRC benchmarks which include larger-scale instances.","The results highlight the ability of our approach to maintain a balance between floorplanning quality and efficiency."],"url":"http://arxiv.org/abs/2406.03165v1","category":"math.OC"}
{"created":"2024-06-05 11:32:13","title":"Once more Diarization: Improving meeting transcription systems through segment-level speaker reassignment","abstract":"Diarization is a crucial component in meeting transcription systems to ease the challenges of speech enhancement and attribute the transcriptions to the correct speaker. Particularly in the presence of overlapping or noisy speech, these systems have problems reliably assigning the correct speaker labels, leading to a significant amount of speaker confusion errors. We propose to add segment-level speaker reassignment to address this issue. By revisiting, after speech enhancement, the speaker attribution for each segment, speaker confusion errors from the initial diarization stage are significantly reduced. Through experiments across different system configurations and datasets, we further demonstrate the effectiveness and applicability in various domains. Our results show that segment-level speaker reassignment successfully rectifies at least 40% of speaker confusion word errors, highlighting its potential for enhancing diarization accuracy in meeting transcription systems.","sentences":["Diarization is a crucial component in meeting transcription systems to ease the challenges of speech enhancement and attribute the transcriptions to the correct speaker.","Particularly in the presence of overlapping or noisy speech, these systems have problems reliably assigning the correct speaker labels, leading to a significant amount of speaker confusion errors.","We propose to add segment-level speaker reassignment to address this issue.","By revisiting, after speech enhancement, the speaker attribution for each segment, speaker confusion errors from the initial diarization stage are significantly reduced.","Through experiments across different system configurations and datasets, we further demonstrate the effectiveness and applicability in various domains.","Our results show that segment-level speaker reassignment successfully rectifies at least 40% of speaker confusion word errors, highlighting its potential for enhancing diarization accuracy in meeting transcription systems."],"url":"http://arxiv.org/abs/2406.03155v1","category":"eess.AS"}
{"created":"2024-06-05 11:06:33","title":"Aligning Transformers with Weisfeiler-Leman","abstract":"Graph neural network architectures aligned with the $k$-dimensional Weisfeiler--Leman ($k$-WL) hierarchy offer theoretically well-understood expressive power. However, these architectures often fail to deliver state-of-the-art predictive performance on real-world graphs, limiting their practical utility. While recent works aligning graph transformer architectures with the $k$-WL hierarchy have shown promising empirical results, employing transformers for higher orders of $k$ remains challenging due to a prohibitive runtime and memory complexity of self-attention as well as impractical architectural assumptions, such as an infeasible number of attention heads. Here, we advance the alignment of transformers with the $k$-WL hierarchy, showing stronger expressivity results for each $k$, making them more feasible in practice. In addition, we develop a theoretical framework that allows the study of established positional encodings such as Laplacian PEs and SPE. We evaluate our transformers on the large-scale PCQM4Mv2 dataset, showing competitive predictive performance with the state-of-the-art and demonstrating strong downstream performance when fine-tuning them on small-scale molecular datasets. Our code is available at https://github.com/luis-mueller/wl-transformers.","sentences":["Graph neural network architectures aligned with the $k$-dimensional Weisfeiler--Leman ($k$-WL) hierarchy offer theoretically well-understood expressive power.","However, these architectures often fail to deliver state-of-the-art predictive performance on real-world graphs, limiting their practical utility.","While recent works aligning graph transformer architectures with the $k$-WL hierarchy have shown promising empirical results, employing transformers for higher orders of $k$ remains challenging due to a prohibitive runtime and memory complexity of self-attention as well as impractical architectural assumptions, such as an infeasible number of attention heads.","Here, we advance the alignment of transformers with the $k$-WL hierarchy, showing stronger expressivity results for each $k$, making them more feasible in practice.","In addition, we develop a theoretical framework that allows the study of established positional encodings such as Laplacian PEs and SPE.","We evaluate our transformers on the large-scale PCQM4Mv2 dataset, showing competitive predictive performance with the state-of-the-art and demonstrating strong downstream performance when fine-tuning them on small-scale molecular datasets.","Our code is available at https://github.com/luis-mueller/wl-transformers."],"url":"http://arxiv.org/abs/2406.03148v1","category":"cs.LG"}
{"created":"2024-06-05 10:39:22","title":"Sensitivity-Based Distributed Model Predictive Control for Nonlinear Systems under Inexact Optimization","abstract":"This paper presents a distributed model predictive control (DMPC) scheme for nonlinear continuous-time systems. The underlying distributed optimal control problem is cooperatively solved in parallel via a sensitivity-based algorithm. The algorithm is fully distributed in the sense that only one neighbor-to-neighbor communication step per iteration is necessary and that all computations are performed locally. Sufficient conditions are derived for the algorithm to converge towards the central solution. Based on this result, stability is shown for the suboptimal DMPC scheme under inexact minimization with the sensitivity-based algorithm and verified with numerical simulations. In particular, stability can be guaranteed with either a suitable stopping criterion or a fixed number of algorithm iterations in each MPC sampling step which allows for a real-time capable implementation.","sentences":["This paper presents a distributed model predictive control (DMPC) scheme for nonlinear continuous-time systems.","The underlying distributed optimal control problem is cooperatively solved in parallel via a sensitivity-based algorithm.","The algorithm is fully distributed in the sense that only one neighbor-to-neighbor communication step per iteration is necessary and that all computations are performed locally.","Sufficient conditions are derived for the algorithm to converge towards the central solution.","Based on this result, stability is shown for the suboptimal DMPC scheme under inexact minimization with the sensitivity-based algorithm and verified with numerical simulations.","In particular, stability can be guaranteed with either a suitable stopping criterion or a fixed number of algorithm iterations in each MPC sampling step which allows for a real-time capable implementation."],"url":"http://arxiv.org/abs/2406.03134v1","category":"math.OC"}
{"created":"2024-06-05 10:33:04","title":"The Harder You Try, The Harder You Fail: The KeyTrap Denial-of-Service Algorithmic Complexity Attacks on DNSSEC","abstract":"Availability is a major concern in the design of DNSSEC. To ensure availability, DNSSEC follows Postel's Law [RFC1123]: \"Be liberal in what you accept, and conservative in what you send.\" Hence, nameservers should send not just one matching key for a record set, but all the relevant cryptographic material, e.g., all the keys for all the ciphers that they support and all the corresponding signatures. This ensures that validation succeeds, and hence availability, even if some of the DNSSEC keys are misconfigured, incorrect or correspond to unsupported ciphers.   We show that this design of DNSSEC is flawed. Exploiting vulnerable recommendations in the DNSSEC standards, we develop a new class of DNSSEC-based algorithmic complexity attacks on DNS, we dub KeyTrap attacks. All popular DNS implementations and services are vulnerable. With just a single DNS packet, the KeyTrap attacks lead to a 2.000.000x spike in CPU instruction count in vulnerable DNS resolvers, stalling some for as long as 16 hours. This devastating effect prompted major DNS vendors to refer to KeyTrap as the worst attack on DNS ever discovered. Exploiting KeyTrap, an attacker could effectively disable Internet access in any system utilizing a DNSSEC-validating resolver.   We disclosed KeyTrap to vendors and operators on November 2, 2023, confidentially reporting the vulnerabilities to a closed group of DNS experts, operators and developers from the industry. Since then we have been working with all major vendors to mitigate KeyTrap, repeatedly discovering and assisting in closing weaknesses in proposed patches. Following our disclosure, the industry-wide umbrella CVE-2023-50387 has been assigned, covering the DNSSEC protocol vulnerabilities we present in this work.","sentences":["Availability is a major concern in the design of DNSSEC.","To ensure availability, DNSSEC follows Postel's Law","[RFC1123]: \"Be liberal in what you accept, and conservative in what you send.\"","Hence, nameservers should send not just one matching key for a record set, but all the relevant cryptographic material, e.g., all the keys for all the ciphers that they support and all the corresponding signatures.","This ensures that validation succeeds, and hence availability, even if some of the DNSSEC keys are misconfigured, incorrect or correspond to unsupported ciphers.   ","We show that this design of DNSSEC is flawed.","Exploiting vulnerable recommendations in the DNSSEC standards, we develop a new class of DNSSEC-based algorithmic complexity attacks on DNS, we dub KeyTrap attacks.","All popular DNS implementations and services are vulnerable.","With just a single DNS packet, the KeyTrap attacks lead to a 2.000.000x spike in CPU instruction count in vulnerable DNS resolvers, stalling some for as long as 16 hours.","This devastating effect prompted major DNS vendors to refer to KeyTrap as the worst attack on DNS ever discovered.","Exploiting KeyTrap, an attacker could effectively disable Internet access in any system utilizing a DNSSEC-validating resolver.   ","We disclosed KeyTrap to vendors and operators on November 2, 2023, confidentially reporting the vulnerabilities to a closed group of DNS experts, operators and developers from the industry.","Since then we have been working with all major vendors to mitigate KeyTrap, repeatedly discovering and assisting in closing weaknesses in proposed patches.","Following our disclosure, the industry-wide umbrella CVE-2023-50387 has been assigned, covering the DNSSEC protocol vulnerabilities we present in this work."],"url":"http://arxiv.org/abs/2406.03133v1","category":"cs.CR"}
{"created":"2024-06-05 10:30:43","title":"Comprehensive Measurement of Three-Dimensional Thermal Conductivity Tensor Using a Beam-Offset Square-Pulsed Source (BO-SPS) Approach","abstract":"Accurately measuring the three-dimensional thermal conductivity tensor is essential for understanding and engineering the thermal behavior of anisotropic materials. Existing methods often struggle to isolate individual tensor elements, leading to large measurement uncertainties and time-consuming iterative fitting procedures. In this study, we introduce the Beam-Offset Square-Pulsed Source (BO-SPS) method for comprehensive measurements of three-dimensional anisotropic thermal conductivity tensors. This method uses square-pulsed heating and precise temperature rise measurements to achieve high signal-to-noise ratios, even with large beam offsets and low modulation frequencies, enabling the isolation of thermal conductivity tensor elements. We demonstrate and validate the BO-SPS method by measuring X-cut and AT-cut quartz samples. For X-cut quartz, with a known relationship between in-plane and cross-plane thermal conductivities, we can determine the full thermal conductivity tensor and heat capacity simultaneously. For AT-cut quartz, assuming a known heat capacity, we can determine the entire anisotropic thermal conductivity tensor, even with finite off-diagonal terms. Our results yield consistent principal thermal conductivity values for both quartz types, demonstrating the method's reliability and accuracy. This research highlights the BO-SPS method's potential to advance the understanding of thermal behavior in complex materials.","sentences":["Accurately measuring the three-dimensional thermal conductivity tensor is essential for understanding and engineering the thermal behavior of anisotropic materials.","Existing methods often struggle to isolate individual tensor elements, leading to large measurement uncertainties and time-consuming iterative fitting procedures.","In this study, we introduce the Beam-Offset Square-Pulsed Source (BO-SPS) method for comprehensive measurements of three-dimensional anisotropic thermal conductivity tensors.","This method uses square-pulsed heating and precise temperature rise measurements to achieve high signal-to-noise ratios, even with large beam offsets and low modulation frequencies, enabling the isolation of thermal conductivity tensor elements.","We demonstrate and validate the BO-SPS method by measuring X-cut and AT-cut quartz samples.","For X-cut quartz, with a known relationship between in-plane and cross-plane thermal conductivities, we can determine the full thermal conductivity tensor and heat capacity simultaneously.","For AT-cut quartz, assuming a known heat capacity, we can determine the entire anisotropic thermal conductivity tensor, even with finite off-diagonal terms.","Our results yield consistent principal thermal conductivity values for both quartz types, demonstrating the method's reliability and accuracy.","This research highlights the BO-SPS method's potential to advance the understanding of thermal behavior in complex materials."],"url":"http://arxiv.org/abs/2406.03131v1","category":"physics.app-ph"}
{"created":"2024-06-05 10:30:40","title":"Ordinal Mixed-Effects Random Forest","abstract":"We propose an innovative statistical method, called Ordinal Mixed-Effect Random Forest (OMERF), that extends the use of random forest to the analysis of hierarchical data and ordinal responses. The model preserves the flexibility and ability of modeling complex patterns of both categorical and continuous variables, typical of tree-based ensemble methods, and, at the same time, takes into account the structure of hierarchical data, modeling the dependence structure induced by the grouping and allowing statistical inference at all data levels. A simulation study is conducted to validate the performance of the proposed method and to compare it to the one of other state-of-the art models. The application of OMERF is exemplified in a case study focusing on predicting students performances using data from the Programme for International Student Assessment (PISA) 2022. The model identifies discriminating student characteristics and estimates the school-effect.","sentences":["We propose an innovative statistical method, called Ordinal Mixed-Effect Random Forest (OMERF), that extends the use of random forest to the analysis of hierarchical data and ordinal responses.","The model preserves the flexibility and ability of modeling complex patterns of both categorical and continuous variables, typical of tree-based ensemble methods, and, at the same time, takes into account the structure of hierarchical data, modeling the dependence structure induced by the grouping and allowing statistical inference at all data levels.","A simulation study is conducted to validate the performance of the proposed method and to compare it to the one of other state-of-the art models.","The application of OMERF is exemplified in a case study focusing on predicting students performances using data from the Programme for International Student Assessment (PISA) 2022.","The model identifies discriminating student characteristics and estimates the school-effect."],"url":"http://arxiv.org/abs/2406.03130v1","category":"stat.ME"}
{"created":"2024-06-05 10:19:39","title":"Taylor-Fourier integration","abstract":"In this paper we introduce an algorithm which provides approximate solutions to semi-linear ordinary differential equations with highly oscillatory solutions that after an appropriate change of variables can be written as a non-autonomous system with $(2\\pi/\\omega)$-periodic dependence on $t$. The proposed approximate solutions are written in closed form as functions $X(t,\\omega\\, t)$ where $X(t,\\theta)$ is, (i) a truncated Fourier series in $\\theta$ for fixed $t$, and (ii) a truncated Taylor series in $t$ for fixed $\\theta$ (that is the reason for the name of the proposed integrators). Such approximations are intended to be uniformly accurate in $\\omega$ (in the sense that their accuracy is not deteriorated as $\\omega\\to \\infty$). This feature implies that Taylor-Fourier approximations become more efficient than the application of standard numerical integrators for sufficiently high basic frequency $\\omega$. The main goal of the paper is to propose a procedure to efficiently compute such approximations by combining power series arithmetic techniques and the FFT algorithm. We present numerical experiments that demonstrate the effectiveness of our approximation method through its application to well-known problems of interest.","sentences":["In this paper we introduce an algorithm which provides approximate solutions to semi-linear ordinary differential equations with highly oscillatory solutions that after an appropriate change of variables can be written as a non-autonomous system with $(2\\pi/\\omega)$-periodic dependence on $t$. The proposed approximate solutions are written in closed form as functions $X(t,\\omega\\, t)$ where $X(t,\\theta)$ is, (i) a truncated Fourier series in $\\theta$ for fixed $t$, and (ii) a truncated Taylor series in $t$ for fixed $\\theta$ (that is the reason for the name of the proposed integrators).","Such approximations are intended to be uniformly accurate in $\\omega$ (in the sense that their accuracy is not deteriorated as $\\omega\\to \\infty$).","This feature implies that Taylor-Fourier approximations become more efficient than the application of standard numerical integrators for sufficiently high basic frequency $\\omega$. The main goal of the paper is to propose a procedure to efficiently compute such approximations by combining power series arithmetic techniques and the FFT algorithm.","We present numerical experiments that demonstrate the effectiveness of our approximation method through its application to well-known problems of interest."],"url":"http://arxiv.org/abs/2406.03124v1","category":"math.NA"}
{"created":"2024-06-05 10:15:16","title":"MESS: Modern Electronic Structure Simulations","abstract":"Electronic structure simulation (ESS) has been used for decades to provide quantitative scientific insights on an atomistic scale, enabling advances in chemistry, biology, and materials science, among other disciplines. Following standard practice in scientific computing, the software packages driving these studies have been implemented in compiled languages such as FORTRAN and C. However, the recent introduction of machine learning (ML) into these domains has meant that ML models must be coded in these languages, or that complex software bridges have to be built between ML models in Python and these large compiled software systems. This is in contrast with recent progress in modern ML frameworks which aim to optimise both ease of use and high performance by harnessing hardware acceleration of tensor programs defined in Python. We introduce MESS: a modern electronic structure simulation package implemented in JAX; porting the ESS code to the ML world. We outline the costs and benefits of following the software development practices used in ML for this important scientific workload. MESS shows significant speedups n widely available hardware accelerators and simultaneously opens a clear pathway towards combining ESS with ML. MESS is available at https://github.com/graphcore-research/mess.","sentences":["Electronic structure simulation (ESS) has been used for decades to provide quantitative scientific insights on an atomistic scale, enabling advances in chemistry, biology, and materials science, among other disciplines.","Following standard practice in scientific computing, the software packages driving these studies have been implemented in compiled languages such as FORTRAN and C. However, the recent introduction of machine learning (ML) into these domains has meant that ML models must be coded in these languages, or that complex software bridges have to be built between ML models in Python and these large compiled software systems.","This is in contrast with recent progress in modern ML frameworks which aim to optimise both ease of use and high performance by harnessing hardware acceleration of tensor programs defined in Python.","We introduce MESS: a modern electronic structure simulation package implemented in JAX; porting the ESS code to the ML world.","We outline the costs and benefits of following the software development practices used in ML for this important scientific workload.","MESS shows significant speedups n widely available hardware accelerators and simultaneously opens a clear pathway towards combining ESS with ML.","MESS is available at https://github.com/graphcore-research/mess."],"url":"http://arxiv.org/abs/2406.03121v1","category":"cs.LG"}
{"created":"2024-06-05 10:03:24","title":"Magnetization without spin: effective Lagrangian of itinerant electrons","abstract":"Effective Lagrangian of itinerant electron system of finite density at finite magnetic field is found to include Chern-Simons term of electromagnetic potentials of lower scale dimension than those studied before. This term has an origin in many-body wave function and unique topological property that is independent of a spin degree of freedom. The coupling strength is proportional to $\\frac{\\rho}{eB}$, which is singular at $B=0$ for a constant charge density. The effective Lagrangian at a finite $B$ represents physical effects at $ B \\neq 0$ properly. A universal shift of the magnetic field known as Slater-Pauling curve is derived from the effective Lagrangian.","sentences":["Effective Lagrangian of itinerant electron system of finite density at finite magnetic field is found to include Chern-Simons term of electromagnetic potentials of lower scale dimension than those studied before.","This term has an origin in many-body wave function and unique topological property that is independent of a spin degree of freedom.","The coupling strength is proportional to $\\frac{\\rho}{eB}$, which is singular at $B=0$ for a constant charge density.","The effective Lagrangian at a finite $B$ represents physical effects at $ B \\neq 0$ properly.","A universal shift of the magnetic field known as Slater-Pauling curve is derived from the effective Lagrangian."],"url":"http://arxiv.org/abs/2406.03112v1","category":"cond-mat.str-el"}
{"created":"2024-06-05 09:58:15","title":"Optimal Control of Semilinear Elliptic Partial Differential Equations with Non-Lipschitzian Nonlinearities","abstract":"We study optimal control problems that are governed by semilinear elliptic partial differential equations that involve non-Lipschitzian nonlinearities. It is shown that, for a certain class of such PDEs, the solution map is Fr\\'{e}chet differentiable even though the differential operator contains a nondifferentiable term. We exploit this effect to establish first-order necessary optimality conditions for minimizers of the considered control problems. The resulting KKT-conditions take the form of coupled PDE-systems that are posed in non-Muckenhoupt weighted Sobolev spaces and raise interesting questions regarding the regularity of optimal controls, the derivation of second-order optimality conditions, and the analysis of finite element discretizations.","sentences":["We study optimal control problems that are governed by semilinear elliptic partial differential equations that involve non-Lipschitzian nonlinearities.","It is shown that, for a certain class of such PDEs, the solution map is Fr\\'{e}chet differentiable even though the differential operator contains a nondifferentiable term.","We exploit this effect to establish first-order necessary optimality conditions for minimizers of the considered control problems.","The resulting KKT-conditions take the form of coupled PDE-systems that are posed in non-Muckenhoupt weighted Sobolev spaces and raise interesting questions regarding the regularity of optimal controls, the derivation of second-order optimality conditions, and the analysis of finite element discretizations."],"url":"http://arxiv.org/abs/2406.03110v1","category":"math.OC"}
{"created":"2024-06-05 09:57:58","title":"CAPRI-FAIR: Integration of Multi-sided Fairness in Contextual POI Recommendation Framework","abstract":"Point-of-interest (POI) recommendation, a form of context-aware recommendation, takes into account spatio-temporal constraints and contexts like distance, peak business hours, and previous user check-ins. Given the ability of these kinds of systems to influence not just the consumer's travel experience, but also the POI's business, it is important to consider fairness from multiple perspectives. Unfortunately, these systems tend to provide less accurate recommendations to inactive users, and less exposure to unpopular POIs. The goal of this paper is to develop a post-filter methodology that incorporates provider and consumer fairness factors into pre-existing recommendation models, to satisfy fairness metrics like item exposure, and performance metrics like precision and distance, making the system more sustainable to both consumers and providers. Experiments have shown that using a linear scoring model for provider fairness in re-scoring recommended items yields the best tradeoff between performance and long-tail exposure, in some cases without a significant decrease in precision. When attempting to address consumer fairness by recommending more popular POIs to inactive users, the result was an increase in precision for only some recommendation models and datasets. Finally, when considering the tradeoff between both parameters, the combinations that reached the Pareto front of consumer and provider fairness, unfortunately, achieved the lowest precision values. We find that the nature of this tradeoff depends heavily on the model and the dataset.","sentences":["Point-of-interest (POI) recommendation, a form of context-aware recommendation, takes into account spatio-temporal constraints and contexts like distance, peak business hours, and previous user check-ins.","Given the ability of these kinds of systems to influence not just the consumer's travel experience, but also the POI's business, it is important to consider fairness from multiple perspectives.","Unfortunately, these systems tend to provide less accurate recommendations to inactive users, and less exposure to unpopular POIs.","The goal of this paper is to develop a post-filter methodology that incorporates provider and consumer fairness factors into pre-existing recommendation models, to satisfy fairness metrics like item exposure, and performance metrics like precision and distance, making the system more sustainable to both consumers and providers.","Experiments have shown that using a linear scoring model for provider fairness in re-scoring recommended items yields the best tradeoff between performance and long-tail exposure, in some cases without a significant decrease in precision.","When attempting to address consumer fairness by recommending more popular POIs to inactive users, the result was an increase in precision for only some recommendation models and datasets.","Finally, when considering the tradeoff between both parameters, the combinations that reached the Pareto front of consumer and provider fairness, unfortunately, achieved the lowest precision values.","We find that the nature of this tradeoff depends heavily on the model and the dataset."],"url":"http://arxiv.org/abs/2406.03109v1","category":"cs.IR"}
{"created":"2024-06-05 09:48:37","title":"Dark state transport between unitary Fermi superfluids","abstract":"The formation of dark states is an important concept in quantum sciences, but its compatibility with strong interparticle interactions, for example, in a quantum degenerate gas is hardly explored. Here, we realize a dark state in one of the spins of a two-component, resonantly-interacting Fermi gas using a $\\Lambda$ system within the $D_2$ transitions of $^6$Li at high magnetic field. The dark state is created in a micrometer-sized region within a one-dimensional channel connecting two superfluid reservoirs. The particle transport between the reservoirs is used as a probe. We observe that atoms are transported in the dark state and the superfluid-assisted fast current is preserved. If the dark state resonant condition is not met, the transport is suppressed by the spontaneous emission. We also uncover an asymmetry in the transport timescale across the two-photon resonance, which is absent in the non-interacting regime. This work raises questions on the interplay of dark states with interparticle interactions and opens up perspectives for optical manipulation of fermionic pairing.","sentences":["The formation of dark states is an important concept in quantum sciences, but its compatibility with strong interparticle interactions, for example, in a quantum degenerate gas is hardly explored.","Here, we realize a dark state in one of the spins of a two-component, resonantly-interacting Fermi gas using a $\\Lambda$ system within the $D_2$ transitions of $^6$Li at high magnetic field.","The dark state is created in a micrometer-sized region within a one-dimensional channel connecting two superfluid reservoirs.","The particle transport between the reservoirs is used as a probe.","We observe that atoms are transported in the dark state and the superfluid-assisted fast current is preserved.","If the dark state resonant condition is not met, the transport is suppressed by the spontaneous emission.","We also uncover an asymmetry in the transport timescale across the two-photon resonance, which is absent in the non-interacting regime.","This work raises questions on the interplay of dark states with interparticle interactions and opens up perspectives for optical manipulation of fermionic pairing."],"url":"http://arxiv.org/abs/2406.03104v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-05 09:34:43","title":"BEBOP V. Homogeneous Stellar Analysis of Potential Circumbinary Planet Hosts","abstract":"Planets orbiting binary systems are relatively unexplored compared to those around single stars. Detections of circumbinary planets and planetary systems offer a first detailed view into our understanding of circumbinary planet formation and dynamical evolution. The BEBOP (Binaries Escorted by Orbiting Planets) radial velocity survey plays a special role in this adventure as it focuses on eclipsing single-lined binaries with an FGK dwarf primary and M dwarf secondary allowing for the highest-radial velocity precision using the HARPS and SOPHIE spectrographs. We obtained 4512 high-resolution spectra for the 179 targets in the BEBOP survey which we used to derive the stellar atmospheric parameters using both equivalent widths and spectral synthesis. We furthermore derive stellar masses, radii, and ages for all targets. With this work, we present the first homogeneous catalogue of precise stellar parameters for these eclipsing single-lined binaries.","sentences":["Planets orbiting binary systems are relatively unexplored compared to those around single stars.","Detections of circumbinary planets and planetary systems offer a first detailed view into our understanding of circumbinary planet formation and dynamical evolution.","The BEBOP (Binaries Escorted by Orbiting Planets) radial velocity survey plays a special role in this adventure as it focuses on eclipsing single-lined binaries with an FGK dwarf primary and M dwarf secondary allowing for the highest-radial velocity precision using the HARPS and SOPHIE spectrographs.","We obtained 4512 high-resolution spectra for the 179 targets in the BEBOP survey which we used to derive the stellar atmospheric parameters using both equivalent widths and spectral synthesis.","We furthermore derive stellar masses, radii, and ages for all targets.","With this work, we present the first homogeneous catalogue of precise stellar parameters for these eclipsing single-lined binaries."],"url":"http://arxiv.org/abs/2406.03094v1","category":"astro-ph.SR"}
{"created":"2024-06-05 09:14:19","title":"Efficient weighted-ensemble network simulations of the SIS model of epidemics","abstract":"The presence of erratic or unstable paths in standard kinetic Monte Carlo simulations significantly undermines the accurate simulation and sampling of transition pathways. While typically reliable methods, such as the Gillespie algorithm, are employed to simulate such paths, they encounter challenges in efficiently identifying rare events due to their sequential nature and reliance on exact Monte Carlo sampling. In contrast, the weighted ensemble method effectively samples rare events and accelerates the exploration of complex reaction pathways by distributing computational resources among multiple replicas, where each replica is assigned a weight reflecting its importance, and evolves independently from the others. Here, we implement the highly efficient and robust weighted ensemble method to model susceptible-infected-susceptible (SIS) dynamics on large heterogeneous population networks. In particular, we explore the interplay between stochasticity and contact heterogeneity which gives rise to large fluctuations, leading to extinction (spontaneous clearance of infection). By studying a wide variety of networks characterized by fat-tailed degree distributions, we are able to compute the mean time to extinction as function of the various network and epidemic parameters. Importantly, this method allows exploring previously-inaccessible parameter regimes.","sentences":["The presence of erratic or unstable paths in standard kinetic Monte Carlo simulations significantly undermines the accurate simulation and sampling of transition pathways.","While typically reliable methods, such as the Gillespie algorithm, are employed to simulate such paths, they encounter challenges in efficiently identifying rare events due to their sequential nature and reliance on exact Monte Carlo sampling.","In contrast, the weighted ensemble method effectively samples rare events and accelerates the exploration of complex reaction pathways by distributing computational resources among multiple replicas, where each replica is assigned a weight reflecting its importance, and evolves independently from the others.","Here, we implement the highly efficient and robust weighted ensemble method to model susceptible-infected-susceptible (SIS) dynamics on large heterogeneous population networks.","In particular, we explore the interplay between stochasticity and contact heterogeneity which gives rise to large fluctuations, leading to extinction (spontaneous clearance of infection).","By studying a wide variety of networks characterized by fat-tailed degree distributions, we are able to compute the mean time to extinction as function of the various network and epidemic parameters.","Importantly, this method allows exploring previously-inaccessible parameter regimes."],"url":"http://arxiv.org/abs/2406.03084v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-05 09:10:11","title":"A Quantum Neural Network-Based Approach to Power Quality Disturbances Detection and Recognition","abstract":"Power quality disturbances (PQDs) significantly impact the stability and reliability of power systems, necessitating accurate and efficient detection and recognition methods. While numerous classical algorithms for PQDs detection and recognition have been extensively studied and applied, related work in the quantum domain is still in its infancy. In this paper, an improved quantum neural networks (QNN) model for PQDs detection and recognition is proposed. Specifically, the model constructs a quantum circuit comprising data qubits and ancilla qubits. Classical data is transformed into quantum data by embedding it into data qubits via the encoding layer. Subsequently, parametric quantum gates are utilized to form the variational layer, which facilitates qubit information transformation, thereby extracting essential feature information for detection and recognition. The expected value is obtained by measuring ancilla qubits, enabling the completion of disturbance classification based on this expected value. An analysis reveals that the runtime and space complexities of the QNN are $O\\left ( poly\\left ( N \\right ) \\right )$ and $O\\left ( N \\right )$, respectively. Extensive experiments validate the feasibility and superiority of the proposed model in PQD detection and recognition. The model achieves accuracies of 99.75\\%, 97.85\\% and 95.5\\% in experiments involving the detection of disturbances, recognition of seven single disturbances, and recognition of ten mixed disturbances, respectively. Additionally, noise simulation and comparative experiments demonstrate that the proposed model exhibits robust anti-noise capabilities, requires few training parameters, and maintains high accuracy.","sentences":["Power quality disturbances (PQDs) significantly impact the stability and reliability of power systems, necessitating accurate and efficient detection and recognition methods.","While numerous classical algorithms for PQDs detection and recognition have been extensively studied and applied, related work in the quantum domain is still in its infancy.","In this paper, an improved quantum neural networks (QNN) model for PQDs detection and recognition is proposed.","Specifically, the model constructs a quantum circuit comprising data qubits and ancilla qubits.","Classical data is transformed into quantum data by embedding it into data qubits via the encoding layer.","Subsequently, parametric quantum gates are utilized to form the variational layer, which facilitates qubit information transformation, thereby extracting essential feature information for detection and recognition.","The expected value is obtained by measuring ancilla qubits, enabling the completion of disturbance classification based on this expected value.","An analysis reveals that the runtime and space complexities of the QNN are $O\\left ( poly\\left ( N \\right ) \\right )$ and $O\\left ( N \\right )$, respectively.","Extensive experiments validate the feasibility and superiority of the proposed model in PQD detection and recognition.","The model achieves accuracies of 99.75\\%, 97.85\\% and 95.5\\% in experiments involving the detection of disturbances, recognition of seven single disturbances, and recognition of ten mixed disturbances, respectively.","Additionally, noise simulation and comparative experiments demonstrate that the proposed model exhibits robust anti-noise capabilities, requires few training parameters, and maintains high accuracy."],"url":"http://arxiv.org/abs/2406.03081v1","category":"quant-ph"}
{"created":"2024-06-05 08:37:41","title":"Efficient Exploration of the Rashomon Set of Rule Set Models","abstract":"Today, as increasingly complex predictive models are developed, simple rule sets remain a crucial tool to obtain interpretable predictions and drive high-stakes decision making. However, a single rule set provides a partial representation of a learning task. An emerging paradigm in interpretable machine learning aims at exploring the Rashomon set of all models exhibiting near-optimal performance. Existing work on Rashomon-set exploration focuses on exhaustive search of the Rashomon set for particular classes of models, which can be a computationally challenging task. On the other hand, exhaustive enumeration leads to redundancy that often is not necessary, and a representative sample or an estimate of the size of the Rashomon set is sufficient for many applications. In this work, we propose, for the first time, efficient methods to explore the Rashomon set of rule set models with or without exhaustive search. Extensive experiments demonstrate the effectiveness of the proposed methods in a variety of scenarios.","sentences":["Today, as increasingly complex predictive models are developed, simple rule sets remain a crucial tool to obtain interpretable predictions and drive high-stakes decision making.","However, a single rule set provides a partial representation of a learning task.","An emerging paradigm in interpretable machine learning aims at exploring the Rashomon set of all models exhibiting near-optimal performance.","Existing work on Rashomon-set exploration focuses on exhaustive search of the Rashomon set for particular classes of models, which can be a computationally challenging task.","On the other hand, exhaustive enumeration leads to redundancy that often is not necessary, and a representative sample or an estimate of the size of the Rashomon set is sufficient for many applications.","In this work, we propose, for the first time, efficient methods to explore the Rashomon set of rule set models with or without exhaustive search.","Extensive experiments demonstrate the effectiveness of the proposed methods in a variety of scenarios."],"url":"http://arxiv.org/abs/2406.03059v1","category":"cs.LG"}
{"created":"2024-06-05 08:30:49","title":"A Computer-Supported Collaborative Learning Environment for Computer Science Education","abstract":"Skills in the field of computer science (CS) are increasingly in demand. Often traditional teaching approaches are not sufficient to teach complex computational concepts. Interactive and digital learning experiences have been shown as valuable tools to support learners in understanding. However, the missing social interaction affects the quality of the learning experience. Adding collaborative and competitive elements can make the virtual learning environment even more social, engaging, and motivating for learners. In this paper, we explore the potential of collaborative and competitive elements in an interactive virtual laboratory environment with a focus on computer science education. In an AB study with 35 CS students, we investigated the effectiveness of collaborative and competitive elements in a virtual laboratory using interactive visualizations of sorting algorithms.","sentences":["Skills in the field of computer science (CS) are increasingly in demand.","Often traditional teaching approaches are not sufficient to teach complex computational concepts.","Interactive and digital learning experiences have been shown as valuable tools to support learners in understanding.","However, the missing social interaction affects the quality of the learning experience.","Adding collaborative and competitive elements can make the virtual learning environment even more social, engaging, and motivating for learners.","In this paper, we explore the potential of collaborative and competitive elements in an interactive virtual laboratory environment with a focus on computer science education.","In an AB study with 35 CS students, we investigated the effectiveness of collaborative and competitive elements in a virtual laboratory using interactive visualizations of sorting algorithms."],"url":"http://arxiv.org/abs/2406.03055v1","category":"cs.ET"}
{"created":"2024-06-05 08:08:19","title":"Study on layout of double rotated serpentine springs for vertical-comb-driven torsional micromirror","abstract":"The combination of double rotated serpentine springs (RSS) and vertical comb-drive is a suitbale solution for the development of torsional micromirror with high fill factor, low fabrication difficulty and good performance. However, the alignment error between upper and lower comb set caused by fabrication can induce force with unexpected direction. And the cross-axis coupled spring constants in double rotated serpentine springs (DRSSs) makes micromirror more susceptible to this alignment error. Herein, in order to minimize the unexpected deflection caused by alignment error of vertical-comb-driven micromirror, this paper, for the first time, studies the effect of layout (centrosymmetrically-arranged and axisymmetrically-arranged) of DRSSs on cross-axis coupled spring constants. Both of theoretical analysis and finite element analysis (FEA) simulation are conducted to reveal this phenomenon. With an example, centrosymmetrically-arranged DRSSs are proved to be more resistant to pull-in of two comb sets. Finally, the relationship between key structure parameters and cross-axis coupled spring constants of centrosymmetrically -arranged DRSSs are presented.","sentences":["The combination of double rotated serpentine springs (RSS) and vertical comb-drive is a suitbale solution for the development of torsional micromirror with high fill factor, low fabrication difficulty and good performance.","However, the alignment error between upper and lower comb set caused by fabrication can induce force with unexpected direction.","And the cross-axis coupled spring constants in double rotated serpentine springs (DRSSs) makes micromirror more susceptible to this alignment error.","Herein, in order to minimize the unexpected deflection caused by alignment error of vertical-comb-driven micromirror, this paper, for the first time, studies the effect of layout (centrosymmetrically-arranged and axisymmetrically-arranged) of DRSSs on cross-axis coupled spring constants.","Both of theoretical analysis and finite element analysis (FEA) simulation are conducted to reveal this phenomenon.","With an example, centrosymmetrically-arranged DRSSs are proved to be more resistant to pull-in of two comb sets.","Finally, the relationship between key structure parameters and cross-axis coupled spring constants of centrosymmetrically -arranged DRSSs are presented."],"url":"http://arxiv.org/abs/2406.03038v1","category":"eess.SY"}
{"created":"2024-06-05 08:07:48","title":"The Phase Transition of the Voter Model on Dynamic Scale-Free Networks","abstract":"The voter model is a classical interacting particle system explaining consensus formation on a social network. Real social networks feature not only a heterogeneous degree distribution but also connections changing over time. We study the voter model on a rank one scale-free network evolving in time by each vertex \\emph{updating} (refreshing its edge neighbourhood) at any rate $\\kappa=\\kappa(N)$.   We find the dynamic giant component phase transition in the consensus time of the voter model: when $\\kappa\\ll \\tfrac{1}{N}$, the subcritical graph parameters are slower by a factor of $\\tfrac{N}{\\log N}$. Conversely, when $\\kappa \\gg 1$ the effect of the giant is removed completely and so for either graph parameter case we see consensus time on the same order as in the static supercritical case (up to polylogarithmic corrections). The intermediate dynamic speeds produce consensus time for subcritical network parameters longer not by the previous factor $\\tfrac{N}{\\log N}$, but by the factor $\\tfrac{1}{\\kappa}$.","sentences":["The voter model is a classical interacting particle system explaining consensus formation on a social network.","Real social networks feature not only a heterogeneous degree distribution but also connections changing over time.","We study the voter model on a rank one scale-free network evolving in time by each vertex \\emph{updating} (refreshing its edge neighbourhood) at any rate $\\kappa=\\kappa(N)$.   ","We find the dynamic giant component phase transition in the consensus time of the voter model: when $\\kappa\\ll \\tfrac{1}{N}$, the subcritical graph parameters are slower by a factor of $\\tfrac{N}{\\log N}$. Conversely,","when $\\kappa \\gg 1$ the effect of the giant is removed completely and so for either graph parameter case we see consensus time on the same order as in the static supercritical case (up to polylogarithmic corrections).","The intermediate dynamic speeds produce consensus time for subcritical network parameters longer not by the previous factor $\\tfrac{N}{\\log N}$, but by the factor $\\tfrac{1}{\\kappa}$."],"url":"http://arxiv.org/abs/2406.03037v1","category":"math.PR"}
{"created":"2024-06-05 08:02:40","title":"Optimal Multi-Fidelity Best-Arm Identification","abstract":"In bandit best-arm identification, an algorithm is tasked with finding the arm with highest mean reward with a specified accuracy as fast as possible. We study multi-fidelity best-arm identification, in which the algorithm can choose to sample an arm at a lower fidelity (less accurate mean estimate) for a lower cost. Several methods have been proposed for tackling this problem, but their optimality remain elusive, notably due to loose lower bounds on the total cost needed to identify the best arm. Our first contribution is a tight, instance-dependent lower bound on the cost complexity. The study of the optimization problem featured in the lower bound provides new insights to devise computationally efficient algorithms, and leads us to propose a gradient-based approach with asymptotically optimal cost complexity. We demonstrate the benefits of the new algorithm compared to existing methods in experiments. Our theoretical and empirical findings also shed light on an intriguing concept of optimal fidelity for each arm.","sentences":["In bandit best-arm identification, an algorithm is tasked with finding the arm with highest mean reward with a specified accuracy as fast as possible.","We study multi-fidelity best-arm identification, in which the algorithm can choose to sample an arm at a lower fidelity (less accurate mean estimate) for a lower cost.","Several methods have been proposed for tackling this problem, but their optimality remain elusive, notably due to loose lower bounds on the total cost needed to identify the best arm.","Our first contribution is a tight, instance-dependent lower bound on the cost complexity.","The study of the optimization problem featured in the lower bound provides new insights to devise computationally efficient algorithms, and leads us to propose a gradient-based approach with asymptotically optimal cost complexity.","We demonstrate the benefits of the new algorithm compared to existing methods in experiments.","Our theoretical and empirical findings also shed light on an intriguing concept of optimal fidelity for each arm."],"url":"http://arxiv.org/abs/2406.03033v1","category":"cs.LG"}
{"created":"2024-06-05 07:51:58","title":"Dynamical topology of chiral and nonreciprocal state transfers in a non-Hermitian quantum system","abstract":"The fundamental concept underlying topological phenomena posits the geometric phase associated with eigenstates. In contrast to this prevailing notion, theoretical studies on time-varying Hamiltonians allow for a new type of topological phenomenon, known as topological dynamics, where the evolution process allows a hidden topological invariant associated with continuous flows. To validate this conjecture, we study topological chiral and nonreciprocal dynamics by encircling the exceptional points (EPs) of non-Hermitian Hamiltonians in a trapped ion system. These dynamics are topologically robust against external perturbations even in the presence dissipation-induced nonadiabatic processes. Our findings indicate that they are protected by dynamical vorticity -- an emerging topological invariant associated with the energy dispersion of non-Hermitian band structures in a parallel transported eigenbasis. The symmetry breaking and other key features of topological dynamics are directly observed through quantum state tomography. Our results mark a significant step towards exploring topological properties of open quantum systems.","sentences":["The fundamental concept underlying topological phenomena posits the geometric phase associated with eigenstates.","In contrast to this prevailing notion, theoretical studies on time-varying Hamiltonians allow for a new type of topological phenomenon, known as topological dynamics, where the evolution process allows a hidden topological invariant associated with continuous flows.","To validate this conjecture, we study topological chiral and nonreciprocal dynamics by encircling the exceptional points (EPs) of non-Hermitian Hamiltonians in a trapped ion system.","These dynamics are topologically robust against external perturbations even in the presence dissipation-induced nonadiabatic processes.","Our findings indicate that they are protected by dynamical vorticity -- an emerging topological invariant associated with the energy dispersion of non-Hermitian band structures in a parallel transported eigenbasis.","The symmetry breaking and other key features of topological dynamics are directly observed through quantum state tomography.","Our results mark a significant step towards exploring topological properties of open quantum systems."],"url":"http://arxiv.org/abs/2406.03026v1","category":"quant-ph"}
{"created":"2024-06-05 07:31:51","title":"A Multi-Technique Study of C2H4 Adsorption on a Model Single-Atom Rh1 Catalyst","abstract":"Single-atom catalysts are potentially ideal model systems to investigate structure-function relationships in catalysis, if the active sites can be uniquely determined. In this work, we study the interaction of C2H4 with a model Rh/Fe3O4(001) catalyst that features 2-, 5-, and 6-fold coordinated Rh adatoms, as well as Rh clusters. Using multiple surface-sensitive techniques in combination with calculations of density functional theory (DFT), we follow the thermal evolution of the system and disentangle the behavior of the different species. C2H4 adsorption is strongest at the 2-fold coordinated Rh1 with a DFT-determined adsorption energy of -2.26 eV. However, desorption occurs at lower temperatures than expected because the Rh migrates into substitutional sites within the support, where the molecule is more weakly bound. Adsorption at the 5-fold coordinated Rh sites is predicated to -1.49 eV, but the superposition of this signal with that from small Rh clusters and additional heterogeneity leads to a broad C2H4 desorption shoulder in TPD above room temperature.","sentences":["Single-atom catalysts are potentially ideal model systems to investigate structure-function relationships in catalysis, if the active sites can be uniquely determined.","In this work, we study the interaction of C2H4 with a model Rh/Fe3O4(001) catalyst that features 2-, 5-, and 6-fold coordinated Rh adatoms, as well as Rh clusters.","Using multiple surface-sensitive techniques in combination with calculations of density functional theory (DFT), we follow the thermal evolution of the system and disentangle the behavior of the different species.","C2H4 adsorption is strongest at the 2-fold coordinated Rh1 with a DFT-determined adsorption energy of -2.26 eV. However, desorption occurs at lower temperatures than expected because the Rh migrates into substitutional sites within the support, where the molecule is more weakly bound.","Adsorption at the 5-fold coordinated Rh sites is predicated to -1.49 eV, but the superposition of this signal with that from small Rh clusters and additional heterogeneity leads to a broad C2H4 desorption shoulder in TPD above room temperature."],"url":"http://arxiv.org/abs/2406.03016v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-05 07:26:23","title":"Enhancing Critical Current Density in Thin Superconductor Layers by Moir\u00e9 Pinning Centers","abstract":"One important factor affecting the critical current density in type-II superconductors is the formation of artificial pinning centers. Hence, the engineering of pinning centers in superconducting systems has garnered considerable attention. In this study, the effect of moir\\'e patterned pinning centers on the critical current density of superconducting tapes is investigated. The Langevin equation is solved by taking into account the prominent forces within the superconductor medium, using the appropriate boundary conditions for vortices. The vortices dynamics are investigated by performing molecular dynamics simulations, which are used to calculate the corresponding critical current densities. Results show a significant enhancement in the critical current density at particular angles of the relative rotation of the primary lattices. It is also revealed that for stronger pinning forces, the calculated critical current densities are higher in the moir\\'e lattices compared to the primary lattices of pinning centers.","sentences":["One important factor affecting the critical current density in type-II superconductors is the formation of artificial pinning centers.","Hence, the engineering of pinning centers in superconducting systems has garnered considerable attention.","In this study, the effect of moir\\'e patterned pinning centers on the critical current density of superconducting tapes is investigated.","The Langevin equation is solved by taking into account the prominent forces within the superconductor medium, using the appropriate boundary conditions for vortices.","The vortices dynamics are investigated by performing molecular dynamics simulations, which are used to calculate the corresponding critical current densities.","Results show a significant enhancement in the critical current density at particular angles of the relative rotation of the primary lattices.","It is also revealed that for stronger pinning forces, the calculated critical current densities are higher in the moir\\'e lattices compared to the primary lattices of pinning centers."],"url":"http://arxiv.org/abs/2406.03013v1","category":"cond-mat.supr-con"}
{"created":"2024-06-05 07:18:28","title":"Simplification of tensor updates toward performance-complexity balanced quantum computer simulation","abstract":"Tensor network methods have evolved from solving optimization problems in quantum many-body spin systems. While the tensor network is now regarded as a powerful tool in quantum computer simulation, there still exists a complexity issue in updating the tensors. This work studies the tensor updates simplification in the context of the tensor network based quantum computer simulation. According to the numerical simulations, a method called simple update, also originated in quantum many-body spin systems, shows a good balance of the fidelity and the computational complexity.","sentences":["Tensor network methods have evolved from solving optimization problems in quantum many-body spin systems.","While the tensor network is now regarded as a powerful tool in quantum computer simulation, there still exists a complexity issue in updating the tensors.","This work studies the tensor updates simplification in the context of the tensor network based quantum computer simulation.","According to the numerical simulations, a method called simple update, also originated in quantum many-body spin systems, shows a good balance of the fidelity and the computational complexity."],"url":"http://arxiv.org/abs/2406.03010v1","category":"quant-ph"}
{"created":"2024-06-05 07:13:52","title":"Quantum Algorithms and Lower Bounds for Finite-Sum Optimization","abstract":"Finite-sum optimization has wide applications in machine learning, covering important problems such as support vector machines, regression, etc. In this paper, we initiate the study of solving finite-sum optimization problems by quantum computing. Specifically, let $f_1,\\ldots,f_n\\colon\\mathbb{R}^d\\to\\mathbb{R}$ be $\\ell$-smooth convex functions and $\\psi\\colon\\mathbb{R}^d\\to\\mathbb{R}$ be a $\\mu$-strongly convex proximal function. The goal is to find an $\\epsilon$-optimal point for $F(\\mathbf{x})=\\frac{1}{n}\\sum_{i=1}^n f_i(\\mathbf{x})+\\psi(\\mathbf{x})$. We give a quantum algorithm with complexity $\\tilde{O}\\big(n+\\sqrt{d}+\\sqrt{\\ell/\\mu}\\big(n^{1/3}d^{1/3}+n^{-2/3}d^{5/6}\\big)\\big)$, improving the classical tight bound $\\tilde{\\Theta}\\big(n+\\sqrt{n\\ell/\\mu}\\big)$. We also prove a quantum lower bound $\\tilde{\\Omega}(n+n^{3/4}(\\ell/\\mu)^{1/4})$ when $d$ is large enough. Both our quantum upper and lower bounds can extend to the cases where $\\psi$ is not necessarily strongly convex, or each $f_i$ is Lipschitz but not necessarily smooth. In addition, when $F$ is nonconvex, our quantum algorithm can find an $\\epsilon$-critial point using $\\tilde{O}(n+\\ell(d^{1/3}n^{1/3}+\\sqrt{d})/\\epsilon^2)$ queries.","sentences":["Finite-sum optimization has wide applications in machine learning, covering important problems such as support vector machines, regression, etc.","In this paper, we initiate the study of solving finite-sum optimization problems by quantum computing.","Specifically, let $f_1,\\ldots,f_n\\colon\\mathbb{R}^d\\to\\mathbb{R}$ be $\\ell$-smooth convex functions and $\\psi\\colon\\mathbb{R}^d\\to\\mathbb{R}$ be a $\\mu$-strongly convex proximal function.","The goal is to find an $\\epsilon$-optimal point for $F(\\mathbf{x})=\\frac{1}{n}\\sum_{i=1}^n f_i(\\mathbf{x})+\\psi(\\mathbf{x})$.","We give a quantum algorithm with complexity $\\tilde{O}\\big(n+\\sqrt{d}+\\sqrt{\\ell/\\mu}\\big(n^{1/3}d^{1/3}+n^{-2/3}d^{5/6}\\big)\\big)$, improving the classical tight bound $\\tilde{\\Theta}\\big(n+\\sqrt{n\\ell/\\mu}\\big)$. We also prove a quantum lower bound $\\tilde{\\Omega}(n+n^{3/4}(\\ell/\\mu)^{1/4})$ when $d$ is large enough.","Both our quantum upper and lower bounds can extend to the cases where $\\psi$ is not necessarily strongly convex, or each $f_i$ is Lipschitz but not necessarily smooth.","In addition, when $F$ is nonconvex, our quantum algorithm can find an $\\epsilon$-critial point using $\\tilde{O}(n+\\ell(d^{1/3}n^{1/3}+\\sqrt{d})/\\epsilon^2)$ queries."],"url":"http://arxiv.org/abs/2406.03006v1","category":"quant-ph"}
{"created":"2024-06-05 07:13:44","title":"Nonlinearity in spin dynamics of frustrated Kagom\u00e9 lattice system under harmonic perturbation","abstract":"In this study, we investigate the spin dynamics of a frustrated Kagom\\'e lattice system, focusing on the nonlinearity of spin oscillations induced by a harmonic magnetic field under varying strengths of Dzyaloshinskii-Moriya interaction (DMI), exchange field, and anisotropy energy. We have utilized Poincar\\'e Surface Sections (PSS) and Power Spectra (PS) for different DMI and anisotropy energy to study the spin dynamics. Our findings reveal that when the DMI strength, external field, anisotropy, and applied magnetic field are weak, the oscillations are quasi-periodic, mostly dominated by the exchange field. With the increase in the DMI strength, the oscillation of the system becomes highly aperiodic. Strong anisotropy tends to induce periodic oscillations, but increasing DMI eventually leads to chaotic behaviour. Additionally, the external magnetic field destabilizes the periodicity of oscillations in systems with weak easy-axis anisotropy, but the systems with strong anisotropy, the oscillations remain unaffected by the external field's strength. Our analysis of magnon dispersion and magnetic resonance (MR) spectra reveals multiple resonance peaks at higher DMI strengths, indicating a complex interplay between spin wave excitation and system parameters. These results underscore the importance of understanding the inherent DMI and anisotropy in the Kagom\\'e lattice during fabrication for various applications. Moreover, our comprehensive analysis of spin dynamics in a Kagom\\'e lattice system demonstrates a clear transition from quasi-periodic to chaotic oscillations with the increase in DMI strength.","sentences":["In this study, we investigate the spin dynamics of a frustrated Kagom\\'e lattice system, focusing on the nonlinearity of spin oscillations induced by a harmonic magnetic field under varying strengths of Dzyaloshinskii-Moriya interaction (DMI), exchange field, and anisotropy energy.","We have utilized Poincar\\'e Surface Sections (PSS) and Power Spectra (PS) for different DMI and anisotropy energy to study the spin dynamics.","Our findings reveal that when the DMI strength, external field, anisotropy, and applied magnetic field are weak, the oscillations are quasi-periodic, mostly dominated by the exchange field.","With the increase in the DMI strength, the oscillation of the system becomes highly aperiodic.","Strong anisotropy tends to induce periodic oscillations, but increasing DMI eventually leads to chaotic behaviour.","Additionally, the external magnetic field destabilizes the periodicity of oscillations in systems with weak easy-axis anisotropy, but the systems with strong anisotropy, the oscillations remain unaffected by the external field's strength.","Our analysis of magnon dispersion and magnetic resonance (MR) spectra reveals multiple resonance peaks at higher DMI strengths, indicating a complex interplay between spin wave excitation and system parameters.","These results underscore the importance of understanding the inherent DMI and anisotropy in the Kagom\\'e lattice during fabrication for various applications.","Moreover, our comprehensive analysis of spin dynamics in a Kagom\\'e lattice system demonstrates a clear transition from quasi-periodic to chaotic oscillations with the increase in DMI strength."],"url":"http://arxiv.org/abs/2406.03005v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-05 06:48:16","title":"Dark magnetohydrodynamics: Black hole accretion in superradiant dark photon clouds","abstract":"Black holes threaded by massive vector fields can be subject to a superradiant instability, growing a cloud of massive vector particles around it. In this work, we consider what happens if such a dark matter candidate field mimicking a dark photon interacts with an accretion flow onto the black hole. By including a kinetic mixing term with the standard model photon, we extend the commonly used equations of general-relativistic magnetohydrodynamics to a dark photon constituent. The coupling to the dark photon then appears as an effective dynamo term together with a dark Lorentz force acting on the accreting matter. We numerically study the interactions between the superradiant dark photon cloud and the inner accretion flow by solving the coupled system in full numerical relativity. By parameterically varying the mixing parameter between dark and standard model sector, we provide a first investigation of how the accretion flow could be modified. Depending on the coupling strength, our solutions exhibit increased wind launching, as well as oscillation modes in the disk.","sentences":["Black holes threaded by massive vector fields can be subject to a superradiant instability, growing a cloud of massive vector particles around it.","In this work, we consider what happens if such a dark matter candidate field mimicking a dark photon interacts with an accretion flow onto the black hole.","By including a kinetic mixing term with the standard model photon, we extend the commonly used equations of general-relativistic magnetohydrodynamics to a dark photon constituent.","The coupling to the dark photon then appears as an effective dynamo term together with a dark Lorentz force acting on the accreting matter.","We numerically study the interactions between the superradiant dark photon cloud and the inner accretion flow by solving the coupled system in full numerical relativity.","By parameterically varying the mixing parameter between dark and standard model sector, we provide a first investigation of how the accretion flow could be modified.","Depending on the coupling strength, our solutions exhibit increased wind launching, as well as oscillation modes in the disk."],"url":"http://arxiv.org/abs/2406.02992v1","category":"astro-ph.HE"}
{"created":"2024-06-05 06:23:49","title":"Local vs. Global Interpretability: A Computational Complexity Perspective","abstract":"The local and global interpretability of various ML models has been studied extensively in recent years. However, despite significant progress in the field, many known results remain informal or lack sufficient mathematical rigor. We propose a framework for bridging this gap, by using computational complexity theory to assess local and global perspectives of interpreting ML models. We begin by proposing proofs for two novel insights that are essential for our analysis: (1) a duality between local and global forms of explanations; and (2) the inherent uniqueness of certain global explanation forms. We then use these insights to evaluate the complexity of computing explanations, across three model types representing the extremes of the interpretability spectrum: (1) linear models; (2) decision trees; and (3) neural networks. Our findings offer insights into both the local and global interpretability of these models. For instance, under standard complexity assumptions such as P != NP, we prove that selecting global sufficient subsets in linear models is computationally harder than selecting local subsets. Interestingly, with neural networks and decision trees, the opposite is true: it is harder to carry out this task locally than globally. We believe that our findings demonstrate how examining explainability through a computational complexity lens can help us develop a more rigorous grasp of the inherent interpretability of ML models.","sentences":["The local and global interpretability of various ML models has been studied extensively in recent years.","However, despite significant progress in the field, many known results remain informal or lack sufficient mathematical rigor.","We propose a framework for bridging this gap, by using computational complexity theory to assess local and global perspectives of interpreting ML models.","We begin by proposing proofs for two novel insights that are essential for our analysis: (1) a duality between local and global forms of explanations; and (2) the inherent uniqueness of certain global explanation forms.","We then use these insights to evaluate the complexity of computing explanations, across three model types representing the extremes of the interpretability spectrum: (1) linear models; (2) decision trees; and (3) neural networks.","Our findings offer insights into both the local and global interpretability of these models.","For instance, under standard complexity assumptions such as P != NP, we prove that selecting global sufficient subsets in linear models is computationally harder than selecting local subsets.","Interestingly, with neural networks and decision trees, the opposite is true: it is harder to carry out this task locally than globally.","We believe that our findings demonstrate how examining explainability through a computational complexity lens can help us develop a more rigorous grasp of the inherent interpretability of ML models."],"url":"http://arxiv.org/abs/2406.02981v1","category":"cs.LG"}
{"created":"2024-06-05 06:21:48","title":"Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices","abstract":"As robotics and augmented reality applications increasingly rely on precise and efficient 6D object pose estimation, real-time performance on edge devices is required for more interactive and responsive systems. Our proposed Sparse Color-Code Net (SCCN) embodies a clear and concise pipeline design to effectively address this requirement. SCCN performs pixel-level predictions on the target object in the RGB image, utilizing the sparsity of essential object geometry features to speed up the Perspective-n-Point (PnP) computation process. Additionally, it introduces a novel pixel-level geometry-based object symmetry representation that seamlessly integrates with the initial pose predictions, effectively addressing symmetric object ambiguities. SCCN notably achieves an estimation rate of 19 frames per second (FPS) and 6 FPS on the benchmark LINEMOD dataset and the Occlusion LINEMOD dataset, respectively, for an NVIDIA Jetson AGX Xavier, while consistently maintaining high estimation accuracy at these rates.","sentences":["As robotics and augmented reality applications increasingly rely on precise and efficient 6D object pose estimation, real-time performance on edge devices is required for more interactive and responsive systems.","Our proposed Sparse Color-Code Net (SCCN) embodies a clear and concise pipeline design to effectively address this requirement.","SCCN performs pixel-level predictions on the target object in the RGB image, utilizing the sparsity of essential object geometry features to speed up the Perspective-n-Point (PnP) computation process.","Additionally, it introduces a novel pixel-level geometry-based object symmetry representation that seamlessly integrates with the initial pose predictions, effectively addressing symmetric object ambiguities.","SCCN notably achieves an estimation rate of 19 frames per second (FPS) and 6 FPS on the benchmark LINEMOD dataset and the Occlusion LINEMOD dataset, respectively, for an NVIDIA Jetson AGX Xavier, while consistently maintaining high estimation accuracy at these rates."],"url":"http://arxiv.org/abs/2406.02977v1","category":"cs.CV"}
{"created":"2024-06-05 05:39:26","title":"Real-Time Small-Signal Security Assessment Using Graph Neural Networks","abstract":"Security assessment is one of the most crucial functions of a power system operator. However, growing complexity and unpredictability make this an increasingly complex and computationally difficult task. In recent times, machine learning methods have gained attention for their ability to handle complex modeling applications. Some methods proposed include deep learning using convolutional neural networks, decision trees, etc. While these methods generate promising results, most methods still require long training times and computational resources. This paper proposes a graph neural network (GNN) approach to the small-signal security assessment problem using data from Phasor Measurement Units (PMUs). Leveraging the inherently graphical structure of the power grid using GNNs, training times can be reduced and efficiency improved for real-time application. Also, using graph properties, optimal PMU placement is determined and the proposed method is shown to perform efficiently under partial observability with limited PMU data. Case studies with simulated data from the IEEE 68-bus system and the NPCC 140-bus system are used to verify the effectiveness of the proposed method.","sentences":["Security assessment is one of the most crucial functions of a power system operator.","However, growing complexity and unpredictability make this an increasingly complex and computationally difficult task.","In recent times, machine learning methods have gained attention for their ability to handle complex modeling applications.","Some methods proposed include deep learning using convolutional neural networks, decision trees, etc.","While these methods generate promising results, most methods still require long training times and computational resources.","This paper proposes a graph neural network (GNN) approach to the small-signal security assessment problem using data from Phasor Measurement Units (PMUs).","Leveraging the inherently graphical structure of the power grid using GNNs, training times can be reduced and efficiency improved for real-time application.","Also, using graph properties, optimal PMU placement is determined and the proposed method is shown to perform efficiently under partial observability with limited PMU data.","Case studies with simulated data from the IEEE 68-bus system and the NPCC 140-bus system are used to verify the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2406.02964v1","category":"eess.SY"}
{"created":"2024-06-05 05:31:58","title":"Carbon Isotope Fractionation of Complex Organic Molecules in Star-Forming Cores","abstract":"Recent high-resolution and sensitivity ALMA observations have unveiled the carbon isotope ratios ($^{12}$C/$^{13}$C) of Complex Organic Molecules (COMs) in a low-mass protostellar source. To understand the $^{12}$C/$^{13}$C ratios of COMs, we investigated the carbon isotope fractionation of COMs from prestellar cores to protostellar cores with a gas-grain chemical network model. We confirmed that the $^{12}$C/$^{13}$C ratios of small molecules are bimodal in the prestellar phase: CO and species formed from CO (e.g., CH$_{3}$OH) are slightly enriched in $^{13}$C compared to the local ISM (by $\\sim$ 10 $\\%$), while those from C and C$^{+}$ are depleted in $^{13}$C owing to isotope exchange reactions. COMs are mainly formed on the grain surface and in the hot gas ($>$ 100 K) in the protostellar phase. The $^{12}$C/$^{13}$C ratios of COMs depend on which molecules the COMs are formed from. In our base model, some COMs in the hot gas are depleted in $^{13}$C compared to the observations. Thus, We additionally incorporate reactions between gaseous atomic C and H$_{2}$O ice or CO ice on the grain surface to form H$_2$CO ice or \\ce{C2O} ice, as suggested by recent laboratory studies. The direct C-atom addition reactions open pathways to form \\ce{^13C}-enriched COMs from atomic C and CO ice. We find that these direct C-atom addition reactions mitigate $^{13}$C-depletion of COMs, and the model with the direct C-atom addition reactions better reproduces the observations than our base model. We also discuss the impact of the cosmic ray ionization rate on the $^{12}$C/$^{13}$C ratio of COMs.","sentences":["Recent high-resolution and sensitivity ALMA observations have unveiled the carbon isotope ratios ($^{12}$C/$^{13}$C) of Complex Organic Molecules (COMs) in a low-mass protostellar source.","To understand the $^{12}$C/$^{13}$C ratios of COMs, we investigated the carbon isotope fractionation of COMs from prestellar cores to protostellar cores with a gas-grain chemical network model.","We confirmed that the $^{12}$C/$^{13}$C ratios of small molecules are bimodal in the prestellar phase: CO and species formed from CO (e.g., CH$_{3}$OH) are slightly enriched in $^{13}$C compared to the local ISM (by $\\sim$ 10 $\\%$), while those from C and C$^{+}$ are depleted in $^{13}$C owing to isotope exchange reactions.","COMs are mainly formed on the grain surface and in the hot gas ($>$ 100 K) in the protostellar phase.","The $^{12}$C/$^{13}$C ratios of COMs depend on which molecules the COMs are formed from.","In our base model, some COMs in the hot gas are depleted in $^{13}$C compared to the observations.","Thus, We additionally incorporate reactions between gaseous atomic C and H$_{2}$O ice or CO ice on the grain surface to form H$_2$CO ice or \\ce{C2O} ice, as suggested by recent laboratory studies.","The direct C-atom addition reactions open pathways to form \\ce{^13C}-enriched COMs from atomic C and CO ice.","We find that these direct C-atom addition reactions mitigate $^{13}$C-depletion of COMs, and the model with the direct C-atom addition reactions better reproduces the observations than our base model.","We also discuss the impact of the cosmic ray ionization rate on the $^{12}$C/$^{13}$C ratio of COMs."],"url":"http://arxiv.org/abs/2406.02961v1","category":"astro-ph.SR"}
{"created":"2024-06-05 05:09:27","title":"CAMEL. II. A 3D Coronal Mass Ejection Catalog Based on Coronal Mass Ejection Automatic Detection with Deep Learning","abstract":"Coronal mass ejections (CMEs) are major drivers of geomagnetic storms, which may cause severe space weather effects. Automating the detection, tracking, and three-dimensional (3D) reconstruction of CMEs is important for operational predictions of CME arrivals. The COR1 coronagraphs on board the Solar Terrestrial Relations Observatory spacecraft have facilitated extensive polarization observations, which are very suitable for the establishment of a 3D CME system. We have developed such a 3D system comprising four modules: classification, segmentation, tracking, and 3D reconstructions. We generalize our previously pretrained classification model to classify COR1 coronagraph images. Subsequently, as there are no publicly available CME segmentation data sets, we manually annotate the structural regions of CMEs using Large Angle and Spectrometric Coronagraph C2 observations. Leveraging transformer-based models, we achieve state-of-the-art results in CME segmentation. Furthermore, we improve the tracking algorithm to solve the difficult separation task of multiple CMEs. In the final module, tracking results, combined with the polarization ratio technique are used to develop the first single-view 3D CME catalog without requiring manual mask annotation. Our method provides higher precision in automatic 2D CME catalog and more reliable physical parameters of CMEs, including 3D propagation direction and speed. The aforementioned 3D CME system can be applied to any coronagraph data with the capability of polarization measurements.","sentences":["Coronal mass ejections (CMEs) are major drivers of geomagnetic storms, which may cause severe space weather effects.","Automating the detection, tracking, and three-dimensional (3D) reconstruction of CMEs is important for operational predictions of CME arrivals.","The COR1 coronagraphs on board the Solar Terrestrial Relations Observatory spacecraft have facilitated extensive polarization observations, which are very suitable for the establishment of a 3D CME system.","We have developed such a 3D system comprising four modules: classification, segmentation, tracking, and 3D reconstructions.","We generalize our previously pretrained classification model to classify COR1 coronagraph images.","Subsequently, as there are no publicly available CME segmentation data sets, we manually annotate the structural regions of CMEs using Large Angle and Spectrometric Coronagraph C2 observations.","Leveraging transformer-based models, we achieve state-of-the-art results in CME segmentation.","Furthermore, we improve the tracking algorithm to solve the difficult separation task of multiple CMEs.","In the final module, tracking results, combined with the polarization ratio technique are used to develop the first single-view 3D CME catalog without requiring manual mask annotation.","Our method provides higher precision in automatic 2D CME catalog and more reliable physical parameters of CMEs, including 3D propagation direction and speed.","The aforementioned 3D CME system can be applied to any coronagraph data with the capability of polarization measurements."],"url":"http://arxiv.org/abs/2406.02946v1","category":"astro-ph.SR"}
{"created":"2024-06-05 05:02:01","title":"Light-induced large and tunable valley-selective Hall effect in a centrosymmetric system","abstract":"We propose that a large and tunable valley-selective Hall effect can be realized in a centrosymmetric system via light-induced breaking of inversion and time-reversal symmetries. This is demonstrated in graphene driven by bicircularly polarized light, which consists of a linear combination of left- and right-handed circularly polarized light with different frequencies. We also show that our Hall conductivity is two orders of magnitude larger than the maximum value obtained in noncentrosymmetric systems, and that the main valley can be switched by tuning a phase difference between the left- and right-handed circularly polarized light. Our results will enable us to generate and control the valley-selective Hall effect in centrosymmetric systems.","sentences":["We propose that a large and tunable valley-selective Hall effect can be realized in a centrosymmetric system via light-induced breaking of inversion and time-reversal symmetries.","This is demonstrated in graphene driven by bicircularly polarized light, which consists of a linear combination of left- and right-handed circularly polarized light with different frequencies.","We also show that our Hall conductivity is two orders of magnitude larger than the maximum value obtained in noncentrosymmetric systems, and that the main valley can be switched by tuning a phase difference between the left- and right-handed circularly polarized light.","Our results will enable us to generate and control the valley-selective Hall effect in centrosymmetric systems."],"url":"http://arxiv.org/abs/2406.02942v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-05 04:43:26","title":"Microscopic theory of spin Nernst effect","abstract":"We present the microscopic theory of the spin Nernst effect, which is a transverse spin current directly induced by a temperature gradient, employing the linear response theory with Luttinger's gravitational potential method. We consider a generic, non-interacting electron system with randomly distributed impurities and evaluate the spin current response to the gravitational potential. Our theory takes into account a contribution of the local equilibrium current modified by Luttinger's gravitational potential and is thus consistent with the thermodynamic principle that thermal responses should vanish. The Ward-Takahashi identities ensure that the spin Nernst current is well-behaved at low temperatures in any order of the random impurity potentials. Furthermore, we microscopically derive the spin-current version of Mott's formula, which associates the spin Nernst coefficient with the spin Hall conductivity. The spin-current version of the St\\v{r}eda formula is also discussed. To demonstrate these findings, the spin Nernst current of three-dimensional Dirac electrons is computed. Our theory is general and can therefore be extended to interacting electron systems, where Mott's formula no longer holds.","sentences":["We present the microscopic theory of the spin Nernst effect, which is a transverse spin current directly induced by a temperature gradient, employing the linear response theory with Luttinger's gravitational potential method.","We consider a generic, non-interacting electron system with randomly distributed impurities and evaluate the spin current response to the gravitational potential.","Our theory takes into account a contribution of the local equilibrium current modified by Luttinger's gravitational potential and is thus consistent with the thermodynamic principle that thermal responses should vanish.","The Ward-Takahashi identities ensure that the spin Nernst current is well-behaved at low temperatures in any order of the random impurity potentials.","Furthermore, we microscopically derive the spin-current version of Mott's formula, which associates the spin Nernst coefficient with the spin Hall conductivity.","The spin-current version of the St\\v{r}eda formula is also discussed.","To demonstrate these findings, the spin Nernst current of three-dimensional Dirac electrons is computed.","Our theory is general and can therefore be extended to interacting electron systems, where Mott's formula no longer holds."],"url":"http://arxiv.org/abs/2406.02932v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-05 04:38:45","title":"P2PFormer: A Primitive-to-polygon Method for Regular Building Contour Extraction from Remote Sensing Images","abstract":"Extracting building contours from remote sensing imagery is a significant challenge due to buildings' complex and diverse shapes, occlusions, and noise. Existing methods often struggle with irregular contours, rounded corners, and redundancy points, necessitating extensive post-processing to produce regular polygonal building contours. To address these challenges, we introduce a novel, streamlined pipeline that generates regular building contours without post-processing. Our approach begins with the segmentation of generic geometric primitives (which can include vertices, lines, and corners), followed by the prediction of their sequence. This allows for the direct construction of regular building contours by sequentially connecting the segmented primitives. Building on this pipeline, we developed P2PFormer, which utilizes a transformer-based architecture to segment geometric primitives and predict their order. To enhance the segmentation of primitives, we introduce a unique representation called group queries. This representation comprises a set of queries and a singular query position, which improve the focus on multiple midpoints of primitives and their efficient linkage. Furthermore, we propose an innovative implicit update strategy for the query position embedding aimed at sharpening the focus of queries on the correct positions and, consequently, enhancing the quality of primitive segmentation. Our experiments demonstrate that P2PFormer achieves new state-of-the-art performance on the WHU, CrowdAI, and WHU-Mix datasets, surpassing the previous SOTA PolyWorld by a margin of 2.7 AP and 6.5 AP75 on the largest CrowdAI dataset. We intend to make the code and trained weights publicly available to promote their use and facilitate further research.","sentences":["Extracting building contours from remote sensing imagery is a significant challenge due to buildings' complex and diverse shapes, occlusions, and noise.","Existing methods often struggle with irregular contours, rounded corners, and redundancy points, necessitating extensive post-processing to produce regular polygonal building contours.","To address these challenges, we introduce a novel, streamlined pipeline that generates regular building contours without post-processing.","Our approach begins with the segmentation of generic geometric primitives (which can include vertices, lines, and corners), followed by the prediction of their sequence.","This allows for the direct construction of regular building contours by sequentially connecting the segmented primitives.","Building on this pipeline, we developed P2PFormer, which utilizes a transformer-based architecture to segment geometric primitives and predict their order.","To enhance the segmentation of primitives, we introduce a unique representation called group queries.","This representation comprises a set of queries and a singular query position, which improve the focus on multiple midpoints of primitives and their efficient linkage.","Furthermore, we propose an innovative implicit update strategy for the query position embedding aimed at sharpening the focus of queries on the correct positions and, consequently, enhancing the quality of primitive segmentation.","Our experiments demonstrate that P2PFormer achieves new state-of-the-art performance on the WHU, CrowdAI, and WHU-Mix datasets, surpassing the previous SOTA PolyWorld by a margin of 2.7 AP and 6.5 AP75 on the largest CrowdAI dataset.","We intend to make the code and trained weights publicly available to promote their use and facilitate further research."],"url":"http://arxiv.org/abs/2406.02930v1","category":"cs.CV"}
{"created":"2024-06-05 04:32:51","title":"Unveiling a Family of Dimerized Quantum Magnets in Ternary Metal Borides","abstract":"Dimerized quantum magnets are exotic crystalline materials where Bose-Einstein condensation of magnetic excitations can happen. However, known dimerized quantum magnets are limited to only a few oxides and halides. Here, we unveil 9 dimerized quantum magnets and 11 conventional antiferromagnets in ternary metal borides MTB$_4$ (M = Sc, Y, La, Ce, Lu, Mg, Ca, Al; T = V, Cr, Mn, Fe, Co, Ni). In this type of structure, 3d transition-metal atoms T are arranged in dimers. Quantum magnetism in these compounds is dominated by strong antiferromagnetic interactions between Cr (both Cr and Mn for M = Mg and Ca) atoms within the structural dimers, with much weaker interactions between the dimers. These systems are proposed to be close to a quantum critical point between a disordered singlet spin-dimer phase, with a spin gap, and the ordered conventional N\\'eel antiferromagnetic phase. This new family of dimerized quantum magnets greatly enriches the materials inventory that allows investigations of the spin-gap phase. All the quantum-, conventionally-, and non-magnetic systems identified, together with experimental synthesis methods of a phase suitable for characterization, provide a platform with abundant possibilities to tune the magnetic exchange coupling by doping and study this unconventional type of quantum phase transition. This work opens up new avenues for studying the quantum magnetism of spin dimers in borides and establishes a theoretical workflow for future searches for dimerized quantum magnets in other families or types of materials.","sentences":["Dimerized quantum magnets are exotic crystalline materials where Bose-Einstein condensation of magnetic excitations can happen.","However, known dimerized quantum magnets are limited to only a few oxides and halides.","Here, we unveil 9 dimerized quantum magnets and 11 conventional antiferromagnets in ternary metal borides MTB$_4$ (M = Sc, Y, La, Ce, Lu, Mg, Ca, Al; T = V, Cr, Mn, Fe, Co, Ni).","In this type of structure, 3d transition-metal atoms T are arranged in dimers.","Quantum magnetism in these compounds is dominated by strong antiferromagnetic interactions between Cr (both Cr and Mn for M = Mg and Ca) atoms within the structural dimers, with much weaker interactions between the dimers.","These systems are proposed to be close to a quantum critical point between a disordered singlet spin-dimer phase, with a spin gap, and the ordered conventional N\\'eel antiferromagnetic phase.","This new family of dimerized quantum magnets greatly enriches the materials inventory that allows investigations of the spin-gap phase.","All the quantum-, conventionally-, and non-magnetic systems identified, together with experimental synthesis methods of a phase suitable for characterization, provide a platform with abundant possibilities to tune the magnetic exchange coupling by doping and study this unconventional type of quantum phase transition.","This work opens up new avenues for studying the quantum magnetism of spin dimers in borides and establishes a theoretical workflow for future searches for dimerized quantum magnets in other families or types of materials."],"url":"http://arxiv.org/abs/2406.02928v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-05 04:20:50","title":"Saturated de Rham-Witt complexes with unit-root coefficients","abstract":"The saturated de Rham-Witt complex, introduced by Bhatt-Lurie-Mathew, is a variant of the classical de Rham-Witt complex which provides a conceptual simplification of the construction and which is expected to produce better results for non-smooth varieties. In this paper, we introduce a generalization of the saturated de Rham-Witt complex which allows coefficients in a unit-root $F$-crystal. We define our complex by a universal property in a category of so-called de Rham-Witt modules. We prove a number of results about it, including existence, quasicoherence, and comparisons to the de Rham-Witt complex of Bhatt-Lurie-Mathew and (in the smooth case) to crystalline cohomology and the classical de Rham-Witt complex with coefficients.","sentences":["The saturated de Rham-Witt complex, introduced by Bhatt-Lurie-Mathew, is a variant of the classical de Rham-Witt complex which provides a conceptual simplification of the construction and which is expected to produce better results for non-smooth varieties.","In this paper, we introduce a generalization of the saturated de Rham-Witt complex which allows coefficients in a unit-root $F$-crystal.","We define our complex by a universal property in a category of so-called de Rham-Witt modules.","We prove a number of results about it, including existence, quasicoherence, and comparisons to the de Rham-Witt complex of Bhatt-Lurie-Mathew and (in the smooth case) to crystalline cohomology and the classical de Rham-Witt complex with coefficients."],"url":"http://arxiv.org/abs/2406.02922v1","category":"math.AG"}
{"created":"2024-06-05 04:10:29","title":"Real-time Motion Planning for autonomous vehicles in dynamic environments","abstract":"Recent advancements in self-driving car technologies have enabled them to navigate autonomously through various environments. However, one of the critical challenges in autonomous vehicle operation is trajectory planning, especially in dynamic environments with moving obstacles. This research aims to tackle this challenge by proposing a robust algorithm tailored for autonomous cars operating in dynamic environments with moving obstacles. The algorithm introduces two main innovations. Firstly, it defines path density by adjusting the number of waypoints along the trajectory, optimizing their distribution for accuracy in curved areas and reducing computational complexity in straight sections. Secondly, it integrates hierarchical motion planning algorithms, combining global planning with an enhanced $A^*$ graph-based method and local planning using the time elastic band algorithm with moving obstacle detection considering different motion models. The proposed algorithm is adaptable for different vehicle types and mobile robots, making it versatile for real-world applications. Simulation results demonstrate its effectiveness across various conditions, promising safer and more efficient navigation for autonomous vehicles in dynamic environments. These modifications significantly improve trajectory planning capabilities, addressing a crucial aspect of autonomous vehicle technology.","sentences":["Recent advancements in self-driving car technologies have enabled them to navigate autonomously through various environments.","However, one of the critical challenges in autonomous vehicle operation is trajectory planning, especially in dynamic environments with moving obstacles.","This research aims to tackle this challenge by proposing a robust algorithm tailored for autonomous cars operating in dynamic environments with moving obstacles.","The algorithm introduces two main innovations.","Firstly, it defines path density by adjusting the number of waypoints along the trajectory, optimizing their distribution for accuracy in curved areas and reducing computational complexity in straight sections.","Secondly, it integrates hierarchical motion planning algorithms, combining global planning with an enhanced $A^*$ graph-based method and local planning using the time elastic band algorithm with moving obstacle detection considering different motion models.","The proposed algorithm is adaptable for different vehicle types and mobile robots, making it versatile for real-world applications.","Simulation results demonstrate its effectiveness across various conditions, promising safer and more efficient navigation for autonomous vehicles in dynamic environments.","These modifications significantly improve trajectory planning capabilities, addressing a crucial aspect of autonomous vehicle technology."],"url":"http://arxiv.org/abs/2406.02916v1","category":"cs.RO"}
{"created":"2024-06-05 04:07:37","title":"A Self-Supervised Denoising Strategy for Underwater Acoustic Camera Imageries","abstract":"In low-visibility marine environments characterized by turbidity and darkness, acoustic cameras serve as visual sensors capable of generating high-resolution 2D sonar images. However, acoustic camera images are interfered with by complex noise and are difficult to be directly ingested by downstream visual algorithms. This paper introduces a novel strategy for denoising acoustic camera images using deep learning techniques, which comprises two principal components: a self-supervised denoising framework and a fine feature-guided block. Additionally, the study explores the relationship between the level of image denoising and the improvement in feature-matching performance. Experimental results show that the proposed denoising strategy can effectively filter acoustic camera images without prior knowledge of the noise model. The denoising process is nearly end-to-end without complex parameter tuning and post-processing. It successfully removes noise while preserving fine feature details, thereby enhancing the performance of local feature matching.","sentences":["In low-visibility marine environments characterized by turbidity and darkness, acoustic cameras serve as visual sensors capable of generating high-resolution 2D sonar images.","However, acoustic camera images are interfered with by complex noise and are difficult to be directly ingested by downstream visual algorithms.","This paper introduces a novel strategy for denoising acoustic camera images using deep learning techniques, which comprises two principal components: a self-supervised denoising framework and a fine feature-guided block.","Additionally, the study explores the relationship between the level of image denoising and the improvement in feature-matching performance.","Experimental results show that the proposed denoising strategy can effectively filter acoustic camera images without prior knowledge of the noise model.","The denoising process is nearly end-to-end without complex parameter tuning and post-processing.","It successfully removes noise while preserving fine feature details, thereby enhancing the performance of local feature matching."],"url":"http://arxiv.org/abs/2406.02914v1","category":"cs.CV"}
{"created":"2024-06-05 04:05:05","title":"Equivariant vector bundles on complexity-one T-varieties and Bruhat-Tits buildings","abstract":"We give a combinatorial classification of torus equivariant vector bundles on a (normal) projective T-variety of complexity-one. This extends the classification of equivariant line bundles on complexity-one T-varieties by Petersen-S\\\"uss on one hand, and Klyachko's classification of equivariant vector bundles on toric varieties on the other hand. A main ingredient in our classification is the classification of torus equivariant vector bundles on toric schemes over a DVR in terms of piecewise affine maps to the (extended) Bruhat-Tits building of the general linear group.","sentences":["We give a combinatorial classification of torus equivariant vector bundles on a (normal) projective T-variety of complexity-one.","This extends the classification of equivariant line bundles on complexity-one T-varieties by Petersen-S\\\"uss on one hand, and Klyachko's classification of equivariant vector bundles on toric varieties on the other hand.","A main ingredient in our classification is the classification of torus equivariant vector bundles on toric schemes over a DVR in terms of piecewise affine maps to the (extended) Bruhat-Tits building of the general linear group."],"url":"http://arxiv.org/abs/2406.02912v1","category":"math.AG"}
{"created":"2024-06-05 04:01:49","title":"The HS-CMU Dataset for Diagnosing Benign and Malignant Diseases through Hysteroscopy","abstract":"Hysteroscopy enables direct visualization of morphological changes in the endometrium, serving as an important means for screening, diagnosing, and treating intrauterine lesions. Accurate identification of the benign or malignant nature of diseases is crucial. However, the complexity and variability of uterine morphology increase the difficulty of identification, leading to missed diagnoses and misdiagnoses, often requiring the expertise of experienced gynecologists and pathologists. Here, we provide the video and image dataset of hysteroscopic examinations conducted at Beijing Chaoyang Hospital, Capital Medical University (named the HS-CMU dataset), recording videos of 175 patients undergoing hysteroscopic surgery to explore the uterine cavity. These data were obtained using corresponding supporting software. From these videos, 3385 high-quality images from 8 categories were selected to form the HS-CMU dataset. These images were annotated by two experienced obstetricians and gynecologists using lableme software. We hope that this dataset can be used as an auxiliary tool for the diagnosis of intrauterine benign and malignant diseases.","sentences":["Hysteroscopy enables direct visualization of morphological changes in the endometrium, serving as an important means for screening, diagnosing, and treating intrauterine lesions.","Accurate identification of the benign or malignant nature of diseases is crucial.","However, the complexity and variability of uterine morphology increase the difficulty of identification, leading to missed diagnoses and misdiagnoses, often requiring the expertise of experienced gynecologists and pathologists.","Here, we provide the video and image dataset of hysteroscopic examinations conducted at Beijing Chaoyang Hospital, Capital Medical University (named the HS-CMU dataset), recording videos of 175 patients undergoing hysteroscopic surgery to explore the uterine cavity.","These data were obtained using corresponding supporting software.","From these videos, 3385 high-quality images from 8 categories were selected to form the HS-CMU dataset.","These images were annotated by two experienced obstetricians and gynecologists using lableme software.","We hope that this dataset can be used as an auxiliary tool for the diagnosis of intrauterine benign and malignant diseases."],"url":"http://arxiv.org/abs/2406.02908v1","category":"physics.med-ph"}
{"created":"2024-06-05 03:44:35","title":"S$^2$GSL: Incorporating Segment to Syntactic Enhanced Graph Structure Learning for Aspect-based Sentiment Analysis","abstract":"Previous graph-based approaches in Aspect based Sentiment Analysis(ABSA) have demonstrated impressive performance by utilizing graph neural networks and attention mechanisms to learn structures of static dependency trees and dynamic latent trees. However, incorporating both semantic and syntactic information simultaneously within complex global structures can introduce irrelevant contexts and syntactic dependencies during the process of graph structure learning, potentially resulting in inaccurate predictions. In order to address the issues above, we propose S$^2$GSL, incorporating Segment to Syntactic enhanced Graph Structure Learning for ABSA. Specifically,S$^2$GSL is featured with a segment-aware semantic graph learning and a syntax-based latent graph learning enabling the removal of irrelevant contexts and dependencies, respectively. We further propose a self-adaptive aggregation network that facilitates the fusion of two graph learning branches, thereby achieving complementarity across diverse structures. Experimental results on four benchmarks demonstrate the effectiveness of our framework.","sentences":["Previous graph-based approaches in Aspect based Sentiment Analysis(ABSA) have demonstrated impressive performance by utilizing graph neural networks and attention mechanisms to learn structures of static dependency trees and dynamic latent trees.","However, incorporating both semantic and syntactic information simultaneously within complex global structures can introduce irrelevant contexts and syntactic dependencies during the process of graph structure learning, potentially resulting in inaccurate predictions.","In order to address the issues above, we propose S$^2$GSL, incorporating Segment to Syntactic enhanced Graph Structure Learning for ABSA.","Specifically,S$^2$GSL is featured with a segment-aware semantic graph learning and a syntax-based latent graph learning enabling the removal of irrelevant contexts and dependencies, respectively.","We further propose a self-adaptive aggregation network that facilitates the fusion of two graph learning branches, thereby achieving complementarity across diverse structures.","Experimental results on four benchmarks demonstrate the effectiveness of our framework."],"url":"http://arxiv.org/abs/2406.02902v1","category":"cs.CL"}
{"created":"2024-06-05 03:42:57","title":"Functions with image in a strip","abstract":"We consider holomorphic functions on the unit disc whose images are contained in a strip of the complex plane and show that these are constants. We also consider appropriate operator valued versions. Applications are found to the theory of semigroups.","sentences":["We consider holomorphic functions on the unit disc whose images are contained in a strip of the complex plane and show that these are constants.","We also consider appropriate operator valued versions.","Applications are found to the theory of semigroups."],"url":"http://arxiv.org/abs/2406.02901v1","category":"math.FA"}
{"created":"2024-06-05 03:39:36","title":"Entanglement and Thermal Transitions from Singularities","abstract":"We study holographic entanglement entropy and revisit thermodynamics and confinement in the dilaton-gravity system. Our analysis focuses on a solvable class of backgrounds that includes AdS and linear dilaton spacetimes as particular cases, with some results extended to general warped metrics. A general lesson is that the behavior of the holographic theory is tied to the bulk curvature singularities. We find that a singular background is confining if and only if i) the singularity coincides with a boundary or ii) it is the linear dilaton. In the former case, for which the singularity cuts off spacetime, we demonstrate that both entanglement entropy and thermodynamics exhibit a first order phase transition. In the linear dilaton case we find instead that both entanglement entropy and thermal phase transitions are of second order. Additionally, along the process we thoroughly derive the radion effective action at quadratic order.","sentences":["We study holographic entanglement entropy and revisit thermodynamics and confinement in the dilaton-gravity system.","Our analysis focuses on a solvable class of backgrounds that includes AdS and linear dilaton spacetimes as particular cases, with some results extended to general warped metrics.","A general lesson is that the behavior of the holographic theory is tied to the bulk curvature singularities.","We find that a singular background is confining if and only if i) the singularity coincides with a boundary or ii) it is the linear dilaton.","In the former case, for which the singularity cuts off spacetime, we demonstrate that both entanglement entropy and thermodynamics exhibit a first order phase transition.","In the linear dilaton case we find instead that both entanglement entropy and thermal phase transitions are of second order.","Additionally, along the process we thoroughly derive the radion effective action at quadratic order."],"url":"http://arxiv.org/abs/2406.02899v1","category":"hep-th"}
{"created":"2024-06-05 03:05:52","title":"PosterLLaVa: Constructing a Unified Multi-modal Layout Generator with LLM","abstract":"Layout generation is the keystone in achieving automated graphic design, requiring arranging the position and size of various multi-modal design elements in a visually pleasing and constraint-following manner. Previous approaches are either inefficient for large-scale applications or lack flexibility for varying design requirements. Our research introduces a unified framework for automated graphic layout generation, leveraging the multi-modal large language model (MLLM) to accommodate diverse design tasks. In contrast, our data-driven method employs structured text (JSON format) and visual instruction tuning to generate layouts under specific visual and textual constraints, including user-defined natural language specifications. We conducted extensive experiments and achieved state-of-the-art (SOTA) performance on public multi-modal layout generation benchmarks, demonstrating the effectiveness of our method. Moreover, recognizing existing datasets' limitations in capturing the complexity of real-world graphic designs, we propose two new datasets for much more challenging tasks (user-constrained generation and complicated poster), further validating our model's utility in real-life settings. Marking by its superior accessibility and adaptability, this approach further automates large-scale graphic design tasks. The code and datasets will be publicly available on https://github.com/posterllava/PosterLLaVA.","sentences":["Layout generation is the keystone in achieving automated graphic design, requiring arranging the position and size of various multi-modal design elements in a visually pleasing and constraint-following manner.","Previous approaches are either inefficient for large-scale applications or lack flexibility for varying design requirements.","Our research introduces a unified framework for automated graphic layout generation, leveraging the multi-modal large language model (MLLM) to accommodate diverse design tasks.","In contrast, our data-driven method employs structured text (JSON format) and visual instruction tuning to generate layouts under specific visual and textual constraints, including user-defined natural language specifications.","We conducted extensive experiments and achieved state-of-the-art (SOTA) performance on public multi-modal layout generation benchmarks, demonstrating the effectiveness of our method.","Moreover, recognizing existing datasets' limitations in capturing the complexity of real-world graphic designs, we propose two new datasets for much more challenging tasks (user-constrained generation and complicated poster), further validating our model's utility in real-life settings.","Marking by its superior accessibility and adaptability, this approach further automates large-scale graphic design tasks.","The code and datasets will be publicly available on https://github.com/posterllava/PosterLLaVA."],"url":"http://arxiv.org/abs/2406.02884v1","category":"cs.CV"}
{"created":"2024-06-05 02:50:27","title":"Leveraging KANs For Enhanced Deep Koopman Operator Discovery","abstract":"Multi-layer perceptrons (MLP's) have been extensively utilized in discovering Deep Koopman operators for linearizing nonlinear dynamics. With the emergence of Kolmogorov-Arnold Networks (KANs) as a more efficient and accurate alternative to the MLP Neural Network, we propose a comparison of the performance of each network type in the context of learning Koopman operators with control.In this work, we propose a KANs-based deep Koopman framework with applications to an orbital Two-Body Problem (2BP) and the pendulum for data-driven discovery of linear system dynamics. KANs were found to be superior in nearly all aspects of training; learning 31 times faster, being 15 times more parameter efficiency, and predicting 1.25 times more accurately as compared to the MLP Deep Neural Networks (DNNs) in the case of the 2BP. Thus, KANs shows potential for being an efficient tool in the development of Deep Koopman Theory.","sentences":["Multi-layer perceptrons (MLP's) have been extensively utilized in discovering Deep Koopman operators for linearizing nonlinear dynamics.","With the emergence of Kolmogorov-Arnold Networks (KANs) as a more efficient and accurate alternative to the MLP Neural Network, we propose a comparison of the performance of each network type in the context of learning Koopman operators with control.","In this work, we propose a KANs-based deep Koopman framework with applications to an orbital Two-Body Problem (2BP) and the pendulum for data-driven discovery of linear system dynamics.","KANs were found to be superior in nearly all aspects of training; learning 31 times faster, being 15 times more parameter efficiency, and predicting 1.25 times more accurately as compared to the MLP Deep Neural Networks (DNNs) in the case of the 2BP.","Thus, KANs shows potential for being an efficient tool in the development of Deep Koopman Theory."],"url":"http://arxiv.org/abs/2406.02875v1","category":"cs.LG"}
{"created":"2024-06-05 02:44:14","title":"Prediction-powered Generalization of Causal Inferences","abstract":"Causal inferences from a randomized controlled trial (RCT) may not pertain to a target population where some effect modifiers have a different distribution. Prior work studies generalizing the results of a trial to a target population with no outcome but covariate data available. We show how the limited size of trials makes generalization a statistically infeasible task, as it requires estimating complex nuisance functions. We develop generalization algorithms that supplement the trial data with a prediction model learned from an additional observational study (OS), without making any assumptions on the OS. We theoretically and empirically show that our methods facilitate better generalization when the OS is high-quality, and remain robust when it is not, and e.g., have unmeasured confounding.","sentences":["Causal inferences from a randomized controlled trial (RCT) may not pertain to a target population where some effect modifiers have a different distribution.","Prior work studies generalizing the results of a trial to a target population with no outcome but covariate data available.","We show how the limited size of trials makes generalization a statistically infeasible task, as it requires estimating complex nuisance functions.","We develop generalization algorithms that supplement the trial data with a prediction model learned from an additional observational study (OS), without making any assumptions on the OS.","We theoretically and empirically show that our methods facilitate better generalization when the OS is high-quality, and remain robust when it is not, and e.g., have unmeasured confounding."],"url":"http://arxiv.org/abs/2406.02873v1","category":"stat.ML"}
{"created":"2024-06-05 02:22:22","title":"Diffusive Limit of the One-species Vlasov-Maxwell-Boltzmann System for Cutoff Hard Potentials","abstract":"Diffusive limit of the one-species Vlasov-Maxwell-Boltzmann system in perturbation framework still remains unsolved, due to the weaker time decay rate compared with the two-species Vlasov-Maxwell-Boltzmann system. By employing the weighted energy method with two newly introduced weight functions and some novel treatments, we solve this problem for the full range of cutoff hard potentials $0\\leq \\gamma \\leq 1$. Uniform estimate with respect to the Knudsen number $\\varepsilon\\in (0,1]$ is established globally in time, which eventually leads to the global existence of solutions to the one-species Vlasov-Maxwell-Boltzmann system and hydrodynamic limit to the incompressible Navier-Stokes-Fourier-Maxwell system. To the best of our knowledge, this is the first result on diffusive limit of the one-species Vlasov-Maxwell-Boltzmann system in perturbation framework.","sentences":["Diffusive limit of the one-species Vlasov-Maxwell-Boltzmann system in perturbation framework still remains unsolved, due to the weaker time decay rate compared with the two-species Vlasov-Maxwell-Boltzmann system.","By employing the weighted energy method with two newly introduced weight functions and some novel treatments, we solve this problem for the full range of cutoff hard potentials $0\\leq \\gamma \\leq 1$. Uniform estimate with respect to the Knudsen number $\\varepsilon\\in (0,1]$ is established globally in time, which eventually leads to the global existence of solutions to the one-species Vlasov-Maxwell-Boltzmann system and hydrodynamic limit to the incompressible Navier-Stokes-Fourier-Maxwell system.","To the best of our knowledge, this is the first result on diffusive limit of the one-species Vlasov-Maxwell-Boltzmann system in perturbation framework."],"url":"http://arxiv.org/abs/2406.02861v1","category":"math.AP"}
{"created":"2024-06-05 02:07:22","title":"Development of an underwater inductive coupling communication system with power carrier technology","abstract":"Inductive coupling communication is one of the main methods of underwater communication systems due to its excellent comprehensive performance. However, the data transmission distance and operational power consumption need to be further enhanced. In this paper, an underwater induction coupling communication scheme based on power carrier technology is proposed to improve the transmission speed and reduce the bit error rate. The microcontroller of STM32L series with ultra-low power consumption was employed as the core of the system. Through the construction and simulation of the communication channel, the optimal parameters were determined. According to the circuit model of the power carrier communication, the effect of different modulation and demodulation methods to the signal transmission quality were discussed, which demonstrates the superiority of Differential Phase Shift Keying (DPSK). With the system-level low power design and onboard communication quality optimization, the device was developed. The test results in the laboratory environment show that the system can achieve efficient data communication with a rate of 115200bps and static power consumption as low as 660{\\mu}A in the 700m channel. This study provides a practical design approach for the high-speed communication and Low-power operation of underwater communication systems.","sentences":["Inductive coupling communication is one of the main methods of underwater communication systems due to its excellent comprehensive performance.","However, the data transmission distance and operational power consumption need to be further enhanced.","In this paper, an underwater induction coupling communication scheme based on power carrier technology is proposed to improve the transmission speed and reduce the bit error rate.","The microcontroller of STM32L series with ultra-low power consumption was employed as the core of the system.","Through the construction and simulation of the communication channel, the optimal parameters were determined.","According to the circuit model of the power carrier communication, the effect of different modulation and demodulation methods to the signal transmission quality were discussed, which demonstrates the superiority of Differential Phase Shift Keying (DPSK).","With the system-level low power design and onboard communication quality optimization, the device was developed.","The test results in the laboratory environment show that the system can achieve efficient data communication with a rate of 115200bps and static power consumption as low as 660{\\mu}A in the 700m channel.","This study provides a practical design approach for the high-speed communication and Low-power operation of underwater communication systems."],"url":"http://arxiv.org/abs/2406.02854v1","category":"eess.SY"}
{"created":"2024-06-05 02:01:31","title":"Isolated anions induced high ionic conductivity","abstract":"One of the key materials in solid-state lithium batteries is fast ion conductors. However, the Li+ ion transport in inorganic crystals involves complex factors, making it a mystery to find and design ion conductors with low migration barriers. In this work, a distinctive structural characteristic involving isolated anions has been discovered to enhance high ionic conductivity in crystals. It is an effective way to create a smooth energy potential landscape and construct local pathways for lithium ion migration. By adjusting the spacing and arrangement of the isolated anions, these local pathways can connect with each other, leading to high ion conductivity. By designing different space groups and local environments of the Se2- anions in the Li8SiSe6 composition, combined with the ion transport properties obtained from AIMD simulations, we define isolated anions and find that local environment with higher point group symmetry promotes the formation of cage-like local transport channels. Additionally, the appropriate distance between neighboring isolated anions can create coplanar connections between adjacent cage-like channels. Furthermore, different types of isolated anions can be used to control the distribution of cage-like channels in the lattice. Based on the structural characteristic of isolated anions, we discovered compounds with isolated N3-, Cl-, I-, and S2- features from the crystal structure databases. The confirmation of ion transport in these structures validates the proposed design method of using isolated anions as structural features for fast ion conductors and leads to the discovery of several new fast ion conductor materials.","sentences":["One of the key materials in solid-state lithium batteries is fast ion conductors.","However, the Li+ ion transport in inorganic crystals involves complex factors, making it a mystery to find and design ion conductors with low migration barriers.","In this work, a distinctive structural characteristic involving isolated anions has been discovered to enhance high ionic conductivity in crystals.","It is an effective way to create a smooth energy potential landscape and construct local pathways for lithium ion migration.","By adjusting the spacing and arrangement of the isolated anions, these local pathways can connect with each other, leading to high ion conductivity.","By designing different space groups and local environments of the Se2- anions in the Li8SiSe6 composition, combined with the ion transport properties obtained from AIMD simulations, we define isolated anions and find that local environment with higher point group symmetry promotes the formation of cage-like local transport channels.","Additionally, the appropriate distance between neighboring isolated anions can create coplanar connections between adjacent cage-like channels.","Furthermore, different types of isolated anions can be used to control the distribution of cage-like channels in the lattice.","Based on the structural characteristic of isolated anions, we discovered compounds with isolated N3-, Cl-, I-, and S2- features from the crystal structure databases.","The confirmation of ion transport in these structures validates the proposed design method of using isolated anions as structural features for fast ion conductors and leads to the discovery of several new fast ion conductor materials."],"url":"http://arxiv.org/abs/2406.02852v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-05 17:32:22","title":"Convolutional Neural Networks and Vision Transformers for Fashion MNIST Classification: A Literature Review","abstract":"Our review explores the comparative analysis between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) in the domain of image classification, with a particular focus on clothing classification within the e-commerce sector. Utilizing the Fashion MNIST dataset, we delve into the unique attributes of CNNs and ViTs. While CNNs have long been the cornerstone of image classification, ViTs introduce an innovative self-attention mechanism enabling nuanced weighting of different input data components. Historically, transformers have primarily been associated with Natural Language Processing (NLP) tasks. Through a comprehensive examination of existing literature, our aim is to unveil the distinctions between ViTs and CNNs in the context of image classification. Our analysis meticulously scrutinizes state-of-the-art methodologies employing both architectures, striving to identify the factors influencing their performance. These factors encompass dataset characteristics, image dimensions, the number of target classes, hardware infrastructure, and the specific architectures along with their respective top results. Our key goal is to determine the most appropriate architecture between ViT and CNN for classifying images in the Fashion MNIST dataset within the e-commerce industry, while taking into account specific conditions and needs. We highlight the importance of combining these two architectures with different forms to enhance overall performance. By uniting these architectures, we can take advantage of their unique strengths, which may lead to more precise and reliable models for e-commerce applications. CNNs are skilled at recognizing local patterns, while ViTs are effective at grasping overall context, making their combination a promising strategy for boosting image classification performance.","sentences":["Our review explores the comparative analysis between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) in the domain of image classification, with a particular focus on clothing classification within the e-commerce sector.","Utilizing the Fashion MNIST dataset, we delve into the unique attributes of CNNs and ViTs.","While CNNs have long been the cornerstone of image classification, ViTs introduce an innovative self-attention mechanism enabling nuanced weighting of different input data components.","Historically, transformers have primarily been associated with Natural Language Processing (NLP) tasks.","Through a comprehensive examination of existing literature, our aim is to unveil the distinctions between ViTs and CNNs in the context of image classification.","Our analysis meticulously scrutinizes state-of-the-art methodologies employing both architectures, striving to identify the factors influencing their performance.","These factors encompass dataset characteristics, image dimensions, the number of target classes, hardware infrastructure, and the specific architectures along with their respective top results.","Our key goal is to determine the most appropriate architecture between ViT and CNN for classifying images in the Fashion MNIST dataset within the e-commerce industry, while taking into account specific conditions and needs.","We highlight the importance of combining these two architectures with different forms to enhance overall performance.","By uniting these architectures, we can take advantage of their unique strengths, which may lead to more precise and reliable models for e-commerce applications.","CNNs are skilled at recognizing local patterns, while ViTs are effective at grasping overall context, making their combination a promising strategy for boosting image classification performance."],"url":"http://arxiv.org/abs/2406.03478v1","category":"cs.CV"}
{"created":"2024-06-05 17:29:22","title":"Lagrangian filtering for wave-mean flow decomposition","abstract":"Geophysical flows are typically composed of wave and mean motions with a wide range of overlapping temporal scales, making separation between the two types of motion in wave-resolving numerical simulations challenging. Lagrangian filtering - whereby a temporal filter is applied in the frame of the flow - is an effective way to overcome this challenge, allowing clean separation of waves from mean flow based on frequency separation in a Lagrangian frame. Previous implementations of Lagrangian filtering have used particle tracking approaches, which are subject to large memory requirements or difficulties with particle clustering. Kafiabad and Vanneste (2023, KV23) recently proposed a novel method for finding Lagrangian means without particle tracking by solving a set of partial differential equations alongside the governing equations of the flow. In this work, we adapt the approach of KV23 to develop a flexible, on-the-fly, PDE-based method for Lagrangian filtering using arbitrary convolutional filters. We present several different wave-mean decompositions, demonstrating that our Lagrangian methods are capable of recovering a clean wave-field from a nonlinear simulation of geostrophic turbulence interacting with Poincar\\'e waves.","sentences":["Geophysical flows are typically composed of wave and mean motions with a wide range of overlapping temporal scales, making separation between the two types of motion in wave-resolving numerical simulations challenging.","Lagrangian filtering - whereby a temporal filter is applied in the frame of the flow - is an effective way to overcome this challenge, allowing clean separation of waves from mean flow based on frequency separation in a Lagrangian frame.","Previous implementations of Lagrangian filtering have used particle tracking approaches, which are subject to large memory requirements or difficulties with particle clustering.","Kafiabad and Vanneste (2023, KV23) recently proposed a novel method for finding Lagrangian means without particle tracking by solving a set of partial differential equations alongside the governing equations of the flow.","In this work, we adapt the approach of KV23 to develop a flexible, on-the-fly, PDE-based method for Lagrangian filtering using arbitrary convolutional filters.","We present several different wave-mean decompositions, demonstrating that our Lagrangian methods are capable of recovering a clean wave-field from a nonlinear simulation of geostrophic turbulence interacting with Poincar\\'e waves."],"url":"http://arxiv.org/abs/2406.03477v1","category":"physics.flu-dyn"}
{"created":"2024-06-05 17:11:59","title":"Gaussian Copula Models for Nonignorable Missing Data Using Auxiliary Marginal Quantiles","abstract":"We present an approach for modeling and imputation of nonignorable missing data under Gaussian copulas. The analyst posits a set of quantiles of the marginal distributions of the study variables, for example, reflecting information from external data sources or elicited expert opinion. When these quantiles are accurately specified, we prove it is possible to consistently estimate the copula correlation and perform multiple imputation in the presence of nonignorable missing data. We develop algorithms for estimation and imputation that are computationally efficient, which we evaluate in simulation studies of multiple imputation inferences. We apply the model to analyze associations between lead exposure levels and end-of-grade test scores for 170,000 students in North Carolina. These measurements are not missing at random, as children deemed at-risk for high lead exposure are more likely to be measured. We construct plausible marginal quantiles for lead exposure using national statistics provided by the Centers for Disease Control and Prevention. Complete cases and missing at random analyses appear to underestimate the relationships between certain variables and end-of-grade test scores, while multiple imputation inferences under our model support stronger adverse associations between lead exposure and educational outcomes.","sentences":["We present an approach for modeling and imputation of nonignorable missing data under Gaussian copulas.","The analyst posits a set of quantiles of the marginal distributions of the study variables, for example, reflecting information from external data sources or elicited expert opinion.","When these quantiles are accurately specified, we prove it is possible to consistently estimate the copula correlation and perform multiple imputation in the presence of nonignorable missing data.","We develop algorithms for estimation and imputation that are computationally efficient, which we evaluate in simulation studies of multiple imputation inferences.","We apply the model to analyze associations between lead exposure levels and end-of-grade test scores for 170,000 students in North Carolina.","These measurements are not missing at random, as children deemed at-risk for high lead exposure are more likely to be measured.","We construct plausible marginal quantiles for lead exposure using national statistics provided by the Centers for Disease Control and Prevention.","Complete cases and missing at random analyses appear to underestimate the relationships between certain variables and end-of-grade test scores, while multiple imputation inferences under our model support stronger adverse associations between lead exposure and educational outcomes."],"url":"http://arxiv.org/abs/2406.03463v1","category":"stat.ME"}
{"created":"2024-06-05 16:45:49","title":"Even Integer Quantum Hall Effect in Materials with Hidden Spin Texture","abstract":"Because spin-orbit coupling (SOC) is invisible in the band structure when inversion symmetry exists, whether spins are trivially degenerate or strongly coupled to momentum due to SOC is presumed to make little difference in transport measurements, such as magnetoresistance and quantum oscillations. In this work, however, we show that hidden Rashba SOC in a centrosymmetric two-dimensional material can lead to the quantum Hall effect with only even-integer plateaus, unlike a spinless electron gas. Here, two Rashba layers that are degenerate but with opposite SOC due to inversion symmetry, hybridize with each other and create two doubly-degenerate bands with hidden spin texture. Correspondingly, two branches of Landau levels interact, resulting in significant suppression of spin splitting due to the balancing of intralayer SOC and interlayer hybridization. Furthermore, we show that breaking inversion symmetry restores the ordinary quantum Hall fluid by introducing spin-split Fermi surfaces. Our theory can apply to centrosymmetric materials with strong SOC, as demonstrated in a recent experiment on the two-dimensional semiconductor Bi$_2$O$_2$Se.","sentences":["Because spin-orbit coupling (SOC) is invisible in the band structure when inversion symmetry exists, whether spins are trivially degenerate or strongly coupled to momentum due to SOC is presumed to make little difference in transport measurements, such as magnetoresistance and quantum oscillations.","In this work, however, we show that hidden Rashba SOC in a centrosymmetric two-dimensional material can lead to the quantum Hall effect with only even-integer plateaus, unlike a spinless electron gas.","Here, two Rashba layers that are degenerate but with opposite SOC due to inversion symmetry, hybridize with each other and create two doubly-degenerate bands with hidden spin texture.","Correspondingly, two branches of Landau levels interact, resulting in significant suppression of spin splitting due to the balancing of intralayer SOC and interlayer hybridization.","Furthermore, we show that breaking inversion symmetry restores the ordinary quantum Hall fluid by introducing spin-split Fermi surfaces.","Our theory can apply to centrosymmetric materials with strong SOC, as demonstrated in a recent experiment on the two-dimensional semiconductor Bi$_2$O$_2$Se."],"url":"http://arxiv.org/abs/2406.03448v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-05 16:29:43","title":"Bayesian inference for scale mixtures of skew-normal linear models under the centered parameterization","abstract":"In many situations we are interested in modeling real data where the response distribution, even conditionally on the covariates, presents asymmetry and/or heavy/light tails. In these situations, it is more suitable to consider models based on the skewed and/or heavy/light tailed distributions, such as the class of scale mixtures of skew-normal distributions. The classical parameterization of this distributions may not be good due to the some inferential issues when the skewness parameter is in a neighborhood of 0, then, the centered parameterization becomes more appropriate. In this paper, we developed a class of scale mixtures of skew-normal distributions under the centered parameterization, also a linear regression model based on them was proposed. We explore a hierarchical representation and set up a MCMC scheme for parameter estimation. Furthermore, we developed residuals and influence analysis tools. A Monte Carlo experiment is conducted to evaluate the performance of the MCMC algorithm and the behavior of the residual distribution. The methodology is illustrated with the analysis of a real data set.","sentences":["In many situations we are interested in modeling real data where the response distribution, even conditionally on the covariates, presents asymmetry and/or heavy/light tails.","In these situations, it is more suitable to consider models based on the skewed and/or heavy/light tailed distributions, such as the class of scale mixtures of skew-normal distributions.","The classical parameterization of this distributions may not be good due to the some inferential issues when the skewness parameter is in a neighborhood of 0, then, the centered parameterization becomes more appropriate.","In this paper, we developed a class of scale mixtures of skew-normal distributions under the centered parameterization, also a linear regression model based on them was proposed.","We explore a hierarchical representation and set up a MCMC scheme for parameter estimation.","Furthermore, we developed residuals and influence analysis tools.","A Monte Carlo experiment is conducted to evaluate the performance of the MCMC algorithm and the behavior of the residual distribution.","The methodology is illustrated with the analysis of a real data set."],"url":"http://arxiv.org/abs/2406.03432v1","category":"stat.ME"}
{"created":"2024-06-05 16:16:03","title":"Post-hoc Part-prototype Networks","abstract":"Post-hoc explainability methods such as Grad-CAM are popular because they do not influence the performance of a trained model. However, they mainly reveal \"where\" a model looks at for a given input, fail to explain \"what\" the model looks for (e.g., what is important to classify a bird image to a Scott Oriole?). Existing part-prototype networks leverage part-prototypes (e.g., characteristic Scott Oriole's wing and head) to answer both \"where\" and \"what\", but often under-perform their black box counterparts in the accuracy. Therefore, a natural question is: can one construct a network that answers both \"where\" and \"what\" in a post-hoc manner to guarantee the model's performance? To this end, we propose the first post-hoc part-prototype network via decomposing the classification head of a trained model into a set of interpretable part-prototypes. Concretely, we propose an unsupervised prototype discovery and refining strategy to obtain prototypes that can precisely reconstruct the classification head, yet being interpretable. Besides guaranteeing the performance, we show that our network offers more faithful explanations qualitatively and yields even better part-prototypes quantitatively than prior part-prototype networks.","sentences":["Post-hoc explainability methods such as Grad-CAM are popular because they do not influence the performance of a trained model.","However, they mainly reveal \"where\" a model looks at for a given input, fail to explain \"what\" the model looks for (e.g., what is important to classify a bird image to a Scott Oriole?).","Existing part-prototype networks leverage part-prototypes (e.g., characteristic Scott Oriole's wing and head) to answer both \"where\" and \"what\", but often under-perform their black box counterparts in the accuracy.","Therefore, a natural question is: can one construct a network that answers both \"where\" and \"what\" in a post-hoc manner to guarantee the model's performance?","To this end, we propose the first post-hoc part-prototype network via decomposing the classification head of a trained model into a set of interpretable part-prototypes.","Concretely, we propose an unsupervised prototype discovery and refining strategy to obtain prototypes that can precisely reconstruct the classification head, yet being interpretable.","Besides guaranteeing the performance, we show that our network offers more faithful explanations qualitatively and yields even better part-prototypes quantitatively than prior part-prototype networks."],"url":"http://arxiv.org/abs/2406.03421v1","category":"cs.CV"}
{"created":"2024-06-05 16:14:11","title":"Logistic growth in seasonally changing environments","abstract":"We consider a parameter dependent periodic-logistic problem with a logistic term involving a degeneracy that replicates time dependent refuges in the habitat of a population. Working under no or very minimal assumptions on the boundary regularity of the domain we show the existence of a time-periodic solution which bifurcates with respect to the parameter and show their stability. We show that under suitable assumptions that the periodic solution blows up on part of the domain and remains finite on other parts when the parameter approaches a critical value.","sentences":["We consider a parameter dependent periodic-logistic problem with a logistic term involving a degeneracy that replicates time dependent refuges in the habitat of a population.","Working under no or very minimal assumptions on the boundary regularity of the domain we show the existence of a time-periodic solution which bifurcates with respect to the parameter and show their stability.","We show that under suitable assumptions that the periodic solution blows up on part of the domain and remains finite on other parts when the parameter approaches a critical value."],"url":"http://arxiv.org/abs/2406.03419v1","category":"math.AP"}
{"created":"2024-06-05 16:10:57","title":"The polarized photon distribution function","abstract":"We employ the LuxQED approach to compute the polarized photon PDF (photon pPDF). This approach expresses the pPDF in terms of the structure functions $g_1$ and $g_2$. Different models for the structure functions are employed according to the parameter space region. The resulting pPDF is approximately of the order of $x$ times the unpolarized PDF. The relative uncertainty in the photon pPDF reaches up to $50\\%$ for $x \\sim 10^{-3}$, decreasing to approximately $10\\%$ for higher values of $x$. The computation of the photon pPDF will be essential for improving the precision of polarized calculations and will have fundamental implications for studies at the future Electron-Ion Collider (EIC).","sentences":["We employ the LuxQED approach to compute the polarized photon PDF (photon pPDF).","This approach expresses the pPDF in terms of the structure functions $g_1$ and $g_2$. Different models for the structure functions are employed according to the parameter space region.","The resulting pPDF is approximately of the order of $x$ times the unpolarized PDF.","The relative uncertainty in the photon pPDF reaches up to $50\\%$ for $x \\sim 10^{-3}$, decreasing to approximately $10\\%$ for higher values of $x$. The computation of the photon pPDF will be essential for improving the precision of polarized calculations and will have fundamental implications for studies at the future Electron-Ion Collider (EIC)."],"url":"http://arxiv.org/abs/2406.03414v1","category":"hep-ph"}
{"created":"2024-06-05 16:03:40","title":"CROSSCON: Cross-platform Open Security Stack for Connected Devices","abstract":"The proliferation of Internet of Things (IoT) embedded devices is expected to reach 30 billion by 2030, creating a dynamic landscape where diverse devices must coexist. This presents challenges due to the rapid expansion of different architectures and platforms. Addressing these challenges requires a unifi ed solution capable of accommodating various devices while offering a broad range of services to connect them to the Internet effectively. This white paper introduces CROSSCON, a three-year Research and Innovation Action funded under Horizon Europe. CROSSCON aims to tackle current IoT challenges by developing a new open, modular, and universally compatible IoT security stack. This stack is designed to be highly portable and vendor-independent, enabling its deployment across different devices with heterogeneous embedded hardware architectures, including ARM and RISC-V. The CROSSCON consortium consists of 11 partners spanning 8 European countries. This consortium includes 4 academic institutions, 1 major industrial partner, and 5 small to medium-sized enterprises (SMEs).","sentences":["The proliferation of Internet of Things (IoT) embedded devices is expected to reach 30 billion by 2030, creating a dynamic landscape where diverse devices must coexist.","This presents challenges due to the rapid expansion of different architectures and platforms.","Addressing these challenges requires a unifi ed solution capable of accommodating various devices while offering a broad range of services to connect them to the Internet effectively.","This white paper introduces CROSSCON, a three-year Research and Innovation Action funded under Horizon Europe.","CROSSCON aims to tackle current IoT challenges by developing a new open, modular, and universally compatible IoT security stack.","This stack is designed to be highly portable and vendor-independent, enabling its deployment across different devices with heterogeneous embedded hardware architectures, including ARM and RISC-V.","The CROSSCON consortium consists of 11 partners spanning 8 European countries.","This consortium includes 4 academic institutions, 1 major industrial partner, and 5 small to medium-sized enterprises (SMEs)."],"url":"http://arxiv.org/abs/2406.03401v1","category":"cs.CR"}
{"created":"2024-06-05 15:59:30","title":"Elliptic curves over Hasse pairs","abstract":"We call a pair of distinct prime powers $(q_1,q_2) = (p_1^{a_1},p_2^{a_2})$ a Hasse pair if $|\\sqrt{q_1}-\\sqrt{q_2}| \\leq 1$. For such pairs, we study the relation between the set $\\mathcal{E}_1$ of isomorphism classes of elliptic curves defined over $\\mathbb{F}_{q_1}$ with $q_2$ points, and the set $\\mathcal{E}_2$ of isomorphism classes of elliptic curves over $\\mathbb{F}_{q_2}$ with $q_1$ points. When both families $\\mathcal{E}_i$ contain only ordinary elliptic curves, we prove that their isogeny graphs are isomorphic. When supersingular curves are involved, we describe which curves might belong to these sets. We also show that if both the $q_i$'s are odd and $\\mathcal{E}_1 \\cup \\mathcal{E}_2 \\neq \\emptyset$, then $\\mathcal{E}_1 \\cup \\mathcal{E}_2$ always contains an ordinary elliptic curve. Conversely, if $q_1$ is even, then $\\mathcal{E}_1 \\cup \\mathcal{E}_2$ may contain only supersingular curves precisely when $q_2$ is a given power of a Fermat or a Mersenne prime. In the case of odd Hasse pairs, we could not rule out the possibility of an empty union $\\mathcal{E}_1 \\cup \\mathcal{E}_2$, but we give necessary conditions for such a case to exist. In an appendix, Moree and Sofos consider how frequently Hasse pairs occur using analytic number theory, making a connection with Andrica's conjecture on the difference between consecutive primes.","sentences":["We call a pair of distinct prime powers $(q_1,q_2) = (p_1^{a_1},p_2^{a_2})$ a Hasse pair if $|\\sqrt{q_1}-\\sqrt{q_2}| \\leq 1$. For such pairs, we study the relation between the set $\\mathcal{E}_1$ of isomorphism classes of elliptic curves defined over $\\mathbb{F}_{q_1}$ with $q_2$ points, and the set $\\mathcal{E}_2$ of isomorphism classes of elliptic curves over $\\mathbb{F}_{q_2}$ with $q_1$ points.","When both families $\\mathcal{E}_i$ contain only ordinary elliptic curves, we prove that their isogeny graphs are isomorphic.","When supersingular curves are involved, we describe which curves might belong to these sets.","We also show that if both the $q_i$'s are odd and $\\mathcal{E}_1 \\cup \\mathcal{E}_2 \\neq \\emptyset$, then $\\mathcal{E}_1 \\cup \\mathcal{E}_2$ always contains an ordinary elliptic curve.","Conversely, if $q_1$ is even, then $\\mathcal{E}_1 \\cup \\mathcal{E}_2$ may contain only supersingular curves precisely when $q_2$ is a given power of a Fermat or a Mersenne prime.","In the case of odd Hasse pairs, we could not rule out the possibility of an empty union $\\mathcal{E}_1 \\cup \\mathcal{E}_2$, but we give necessary conditions for such a case to exist.","In an appendix, Moree and Sofos consider how frequently Hasse pairs occur using analytic number theory, making a connection with Andrica's conjecture on the difference between consecutive primes."],"url":"http://arxiv.org/abs/2406.03399v1","category":"math.NT"}
{"created":"2024-06-05 15:36:39","title":"A mathematical analysis of IPT-DMFT","abstract":"We provide a mathematical analysis of the Dynamical Mean-Field Theory, a celebrated representative of a class of approximations in quantum mechanics known as embedding methods. We start by a pedagogical and self-contained mathematical formulation of the Dynamical Mean-Field Theory equations for the finite Hubbard model. After recalling the definition and properties of one-body time-ordered Green's functions and self-energies, and the mathematical structure of the Hubbard and Anderson impurity models, we describe a specific impurity solver, namely the Iterated Perturbation Theory solver, which can be conveniently formulated using Matsubara's Green's functions. Within this framework, we prove under certain assumptions that the Dynamical Mean-Field Theory equations admit a solution for any set of physical parameters. Moreover, we establish some properties of the solution(s).","sentences":["We provide a mathematical analysis of the Dynamical Mean-Field Theory, a celebrated representative of a class of approximations in quantum mechanics known as embedding methods.","We start by a pedagogical and self-contained mathematical formulation of the Dynamical Mean-Field Theory equations for the finite Hubbard model.","After recalling the definition and properties of one-body time-ordered Green's functions and self-energies, and the mathematical structure of the Hubbard and Anderson impurity models, we describe a specific impurity solver, namely the Iterated Perturbation Theory solver, which can be conveniently formulated using Matsubara's Green's functions.","Within this framework, we prove under certain assumptions that the Dynamical Mean-Field Theory equations admit a solution for any set of physical parameters.","Moreover, we establish some properties of the solution(s)."],"url":"http://arxiv.org/abs/2406.03384v1","category":"math-ph"}
{"created":"2024-06-05 14:57:29","title":"Analyzing and Estimating Support for U.S. Presidential Candidates in Twitter Polls","abstract":"Polls posted on social media have emerged in recent years as an important tool for estimating public opinion, e.g., to gauge public support for business decisions and political candidates in national elections. Here, we examine nearly two thousand Twitter polls gauging support for U.S. presidential candidates during the 2016 and 2020 election campaigns. First, we describe the rapidly emerging prevalence of social polls. Second, we characterize social polls in terms of their heterogeneity and response options. Third, leveraging machine learning models for user attribute inference, we describe the demographics, political leanings, and other characteristics of the users who author and interact with social polls. Finally, we study the relationship between social poll results, their attributes, and the characteristics of users interacting with them. Our findings reveal that Twitter polls are biased in various ways, starting from the position of the presidential candidates among the poll options to biases in demographic attributes and poll results. The 2016 and 2020 polls were predominantly crafted by older males and manifested a pronounced bias favoring candidate Donald Trump, in contrast to traditional surveys, which favored Democratic candidates. We further identify and explore the potential reasons for such biases in social polling and discuss their potential repercussions. Finally, we show that biases in social media polls can be corrected via regression and poststratification. The errors of the resulting election estimates can be as low as 1%-2%, suggesting that social media polls can become a promising source of information about public opinion.","sentences":["Polls posted on social media have emerged in recent years as an important tool for estimating public opinion, e.g., to gauge public support for business decisions and political candidates in national elections.","Here, we examine nearly two thousand Twitter polls gauging support for U.S. presidential candidates during the 2016 and 2020 election campaigns.","First, we describe the rapidly emerging prevalence of social polls.","Second, we characterize social polls in terms of their heterogeneity and response options.","Third, leveraging machine learning models for user attribute inference, we describe the demographics, political leanings, and other characteristics of the users who author and interact with social polls.","Finally, we study the relationship between social poll results, their attributes, and the characteristics of users interacting with them.","Our findings reveal that Twitter polls are biased in various ways, starting from the position of the presidential candidates among the poll options to biases in demographic attributes and poll results.","The 2016 and 2020 polls were predominantly crafted by older males and manifested a pronounced bias favoring candidate Donald Trump, in contrast to traditional surveys, which favored Democratic candidates.","We further identify and explore the potential reasons for such biases in social polling and discuss their potential repercussions.","Finally, we show that biases in social media polls can be corrected via regression and poststratification.","The errors of the resulting election estimates can be as low as 1%-2%, suggesting that social media polls can become a promising source of information about public opinion."],"url":"http://arxiv.org/abs/2406.03340v1","category":"cs.SI"}
{"created":"2024-06-05 14:51:34","title":"Griddy-Gibbs sampling for Bayesian P-splines models with Poisson data","abstract":"P-splines are appealing for smoothing Poisson distributed counts. They provide a flexible setting for modeling nonlinear model components based on a discretized penalty structure with a relatively simple computational backbone. Under a Bayesian inferential process relying on Markov chain Monte Carlo, estimates of spline coefficients are typically obtained by means of Metropolis-type algorithms, which may suffer from convergence issues if the proposal distribution is not properly chosen. To avoid such a sensitive calibration choice, we extend the Griddy-Gibbs sampler to Bayesian P-splines models with a Poisson response variable. In this model class, conditional posterior distributions of spline components are shown to have attractive mathematical properties. Despite their non-conjugate nature, conditional posteriors of spline coefficients can be efficiently explored with a Gibbs sampling scheme by relying on grid-based approximations. The proposed Griddy-Gibbs sampler for Bayesian P-splines (GGSBPS) algorithm is an interesting calibration-free tool for density estimation and histogram smoothing that is made available in a compact and user-friendly routine. The performance of our approach is assessed in different simulation settings and the GGSBPS algorithm is illustrated on two real datasets.","sentences":["P-splines are appealing for smoothing Poisson distributed counts.","They provide a flexible setting for modeling nonlinear model components based on a discretized penalty structure with a relatively simple computational backbone.","Under a Bayesian inferential process relying on Markov chain Monte Carlo, estimates of spline coefficients are typically obtained by means of Metropolis-type algorithms, which may suffer from convergence issues if the proposal distribution is not properly chosen.","To avoid such a sensitive calibration choice, we extend the Griddy-Gibbs sampler to Bayesian P-splines models with a Poisson response variable.","In this model class, conditional posterior distributions of spline components are shown to have attractive mathematical properties.","Despite their non-conjugate nature, conditional posteriors of spline coefficients can be efficiently explored with a Gibbs sampling scheme by relying on grid-based approximations.","The proposed Griddy-Gibbs sampler for Bayesian P-splines (GGSBPS) algorithm is an interesting calibration-free tool for density estimation and histogram smoothing that is made available in a compact and user-friendly routine.","The performance of our approach is assessed in different simulation settings and the GGSBPS algorithm is illustrated on two real datasets."],"url":"http://arxiv.org/abs/2406.03336v1","category":"stat.ME"}
{"created":"2024-06-05 14:49:15","title":"Reparameterization invariance in approximate Bayesian inference","abstract":"Current approximate posteriors in Bayesian neural networks (BNNs) exhibit a crucial limitation: they fail to maintain invariance under reparameterization, i.e. BNNs assign different posterior densities to different parametrizations of identical functions. This creates a fundamental flaw in the application of Bayesian principles as it breaks the correspondence between uncertainty over the parameters with uncertainty over the parametrized function. In this paper, we investigate this issue in the context of the increasingly popular linearized Laplace approximation. Specifically, it has been observed that linearized predictives alleviate the common underfitting problems of the Laplace approximation. We develop a new geometric view of reparametrizations from which we explain the success of linearization. Moreover, we demonstrate that these reparameterization invariance properties can be extended to the original neural network predictive using a Riemannian diffusion process giving a straightforward algorithm for approximate posterior sampling, which empirically improves posterior fit.","sentences":["Current approximate posteriors in Bayesian neural networks (BNNs) exhibit a crucial limitation: they fail to maintain invariance under reparameterization, i.e. BNNs assign different posterior densities to different parametrizations of identical functions.","This creates a fundamental flaw in the application of Bayesian principles as it breaks the correspondence between uncertainty over the parameters with uncertainty over the parametrized function.","In this paper, we investigate this issue in the context of the increasingly popular linearized Laplace approximation.","Specifically, it has been observed that linearized predictives alleviate the common underfitting problems of the Laplace approximation.","We develop a new geometric view of reparametrizations from which we explain the success of linearization.","Moreover, we demonstrate that these reparameterization invariance properties can be extended to the original neural network predictive using a Riemannian diffusion process giving a straightforward algorithm for approximate posterior sampling, which empirically improves posterior fit."],"url":"http://arxiv.org/abs/2406.03334v1","category":"cs.LG"}
{"created":"2024-06-05 14:11:40","title":"Combining an experimental study with external data: study designs and identification strategies","abstract":"There is increasing interest in combining information from experimental studies, including randomized and single-group trials, with information from external experimental or observational data sources. Such efforts are usually motivated by the desire to compare treatments evaluated in different studies -- for instance, through the introduction of external treatment groups -- or to estimate treatment effects with greater precision. Proposals to combine experimental studies with external data were made at least as early as the 1970s, but in recent years have come under increasing consideration by regulatory agencies involved in drug and device evaluation, particularly with the increasing availability of rich observational data. In this paper, we describe basic templates of study designs and data structures for combining information from experimental studies with external data, and use the potential (counterfactual) outcomes framework to elaborate identification strategies for potential outcome means and average treatment effects in these designs. In formalizing designs and identification strategies for combining information from experimental studies with external data, we hope to provide a conceptual foundation to support the systematic use and evaluation of such efforts.","sentences":["There is increasing interest in combining information from experimental studies, including randomized and single-group trials, with information from external experimental or observational data sources.","Such efforts are usually motivated by the desire to compare treatments evaluated in different studies -- for instance, through the introduction of external treatment groups -- or to estimate treatment effects with greater precision.","Proposals to combine experimental studies with external data were made at least as early as the 1970s, but in recent years have come under increasing consideration by regulatory agencies involved in drug and device evaluation, particularly with the increasing availability of rich observational data.","In this paper, we describe basic templates of study designs and data structures for combining information from experimental studies with external data, and use the potential (counterfactual) outcomes framework to elaborate identification strategies for potential outcome means and average treatment effects in these designs.","In formalizing designs and identification strategies for combining information from experimental studies with external data, we hope to provide a conceptual foundation to support the systematic use and evaluation of such efforts."],"url":"http://arxiv.org/abs/2406.03302v1","category":"stat.ME"}
{"created":"2024-06-05 13:59:58","title":"Production of $ B \\bar{B}$ bound state via $\u03a5(4S)$ radiative decays","abstract":"Motivated by recent theoretical predictions about the existence of a $ B \\bar{B}$ bound state (also denoted as $ X(10550) $), in this work we estimate the production of the $S$-wave $ B^+ B^-$ molecule via $\\Upsilon (4S)$ radiative decays. In particular, we make use of effective Lagrangian approach and the compositeness condition to calculate the $ X(10550) $ production rate via $\\Upsilon(4S)\\rightarrow \\gamma X(10550)$ decays employing triangle diagrams. Our results show that the partial decay width of this reaction is of the order of $0.5 - 192 \\ \\keV $ for a respective binding energy of $1 - 100 $ MeV, corresponding to a branching fraction of $ 10^{-5} - 10^{-3}$. These findings suggest that the existence of the $ X(10550) $ might be checked via the analysis of the mentioned decay in present and future experiments.","sentences":["Motivated by recent theoretical predictions about the existence of a $ B \\bar{B}$ bound state (also denoted as $ X(10550) $), in this work we estimate the production of the $S$-wave $ B^+ B^-$ molecule via $\\Upsilon (4S)$ radiative decays.","In particular, we make use of effective Lagrangian approach and the compositeness condition to calculate the $ X(10550) $ production rate via $\\Upsilon(4S)\\rightarrow \\gamma X(10550)$ decays employing triangle diagrams.","Our results show that the partial decay width of this reaction is of the order of $0.5 - 192 \\ \\keV $ for a respective binding energy of $1 - 100 $ MeV, corresponding to a branching fraction of $ 10^{-5} - 10^{-3}$.","These findings suggest that the existence of the $ X(10550) $ might be checked via the analysis of the mentioned decay in present and future experiments."],"url":"http://arxiv.org/abs/2406.03289v1","category":"hep-ph"}
{"created":"2024-06-05 13:45:34","title":"Nonlinear Ion-Acoustic Waves with Landau Damping in Non-Maxwellian Space Plasmas","abstract":"The dynamics of nonlinear ion-acoustic solitary waves in the presence of kinetic (Landau type) damping have been investigated in a collisionless, non-magnetized electron-ion plasma. A cold ion fluid model, coupled to a Vlasov-type kinetic equation for the electron dynamics, has been adopted as a starting point. The electron population was assumed to be in a kappa-distributed state, in account of the non-Maxwellian behavior of energetic (suprathermal) electrons often observed in Space. A multiscale perturbation technique has led to an evolution equation for the electrostatic potential, in the form of a modified Korteweg-de Vries (KdV) equation, incorporating a non-local term accounting for Landau damping (associated with the electron statistics). Exact analytical solutions have been obtained, representing solitary waves undergoing amplitude decay over time. The combined effect of Landau damping and non-Maxwellian electron statistics (via the kappa parameter) on the characteristics of IASWs has been examined. Numerical integration of the evolution equation has been undertaken, to elucidate the importance of kinetic Landau damping on a shock-shaped initial condition. The results of this investigation aim to improve our understanding of the dynamics of nonlinear electrostatic waves under the influence of Landau damping in various space plasma environments.","sentences":["The dynamics of nonlinear ion-acoustic solitary waves in the presence of kinetic (Landau type) damping have been investigated in a collisionless, non-magnetized electron-ion plasma.","A cold ion fluid model, coupled to a Vlasov-type kinetic equation for the electron dynamics, has been adopted as a starting point.","The electron population was assumed to be in a kappa-distributed state, in account of the non-Maxwellian behavior of energetic (suprathermal) electrons often observed in Space.","A multiscale perturbation technique has led to an evolution equation for the electrostatic potential, in the form of a modified Korteweg-de Vries (KdV) equation, incorporating a non-local term accounting for Landau damping (associated with the electron statistics).","Exact analytical solutions have been obtained, representing solitary waves undergoing amplitude decay over time.","The combined effect of Landau damping and non-Maxwellian electron statistics (via the kappa parameter) on the characteristics of IASWs has been examined.","Numerical integration of the evolution equation has been undertaken, to elucidate the importance of kinetic Landau damping on a shock-shaped initial condition.","The results of this investigation aim to improve our understanding of the dynamics of nonlinear electrostatic waves under the influence of Landau damping in various space plasma environments."],"url":"http://arxiv.org/abs/2406.03267v1","category":"physics.plasm-ph"}
{"created":"2024-06-05 13:44:33","title":"Technological Perspective on Digital Sovereignty","abstract":"This report for the attention of the Federal Department of Foreign Affairs (FDFA) makes a scientific contribution in the context of postulate 22.4411 \"Digital Sovereignty Strategy for Switzerland\" by Councillor of States Heidi Z'graggen. The report shows what digital sovereignty means from a technological perspective and what activities are currently being carried out in this regard in Switzerland and abroad. It also provides strategic directions and specific recommendations for a future \"Swiss Digital Sovereignty Strategy\".","sentences":["This report for the attention of the Federal Department of Foreign Affairs (FDFA) makes a scientific contribution in the context of postulate 22.4411 \"Digital Sovereignty Strategy for Switzerland\" by Councillor of States Heidi Z'graggen.","The report shows what digital sovereignty means from a technological perspective and what activities are currently being carried out in this regard in Switzerland and abroad.","It also provides strategic directions and specific recommendations for a future \"Swiss Digital Sovereignty Strategy\"."],"url":"http://arxiv.org/abs/2406.03266v1","category":"cs.CY"}
{"created":"2024-06-05 13:36:41","title":"Empirical calibration for helium abundance determinations in Active Galactic Nuclei","abstract":"For the first time, a calibration between the HeI $\\lambda5876$/H$\\beta$ emission line ratio and the helium abundance $y$=12+log(He/H) for Narrow line regions (NLRs) of Seyfert~2 Active Galactic Nuclei (AGN) is proposed. In this context, observational data (taken from the SDSS-DR15 and from the literature) and direct abundance estimates (via the $T_{\\rm e}$-method) for a sample of 65 local ($z \\: < \\: 0.2$) Seyfert~2 nuclei are considered. The resulting calibration estimates the $y$ abundance with an average uncertainty of 0.02 dex. Applying our calibration to spectroscopic data containing only strong emission lines, it yields a helium abundance distribution similar to that obtained via the $T_{\\rm e}$-method. Some cautions must be considered to apply our calibration for Seyfert~2 nuclei with high values of electron temperature ($\\gtrsim\\: 20\\,000$ K) or ionization parameter ($\\log U > -2.0$).","sentences":["For the first time, a calibration between the HeI $\\lambda5876$/H$\\beta$ emission line ratio and the helium abundance $y$=12+log(He/H) for Narrow line regions (NLRs) of Seyfert~2 Active Galactic Nuclei (AGN) is proposed.","In this context, observational data (taken from the SDSS-DR15 and from the literature) and direct abundance estimates (via the $T_{\\rm e}$-method) for a sample of 65 local ($z \\: < \\: 0.2$) Seyfert~2 nuclei are considered.","The resulting calibration estimates the $y$ abundance with an average uncertainty of 0.02 dex.","Applying our calibration to spectroscopic data containing only strong emission lines, it yields a helium abundance distribution similar to that obtained via the $T_{\\rm e}$-method.","Some cautions must be considered to apply our calibration for Seyfert~2 nuclei with high values of electron temperature ($\\gtrsim\\: 20\\,000$ K) or ionization parameter ($\\log U > -2.0$)."],"url":"http://arxiv.org/abs/2406.03259v1","category":"astro-ph.GA"}
{"created":"2024-06-05 13:36:38","title":"Relaxed Quantile Regression: Prediction Intervals for Asymmetric Noise","abstract":"Constructing valid prediction intervals rather than point estimates is a well-established approach for uncertainty quantification in the regression setting. Models equipped with this capacity output an interval of values in which the ground truth target will fall with some prespecified probability. This is an essential requirement in many real-world applications where simple point predictions' inability to convey the magnitude and frequency of errors renders them insufficient for high-stakes decisions. Quantile regression is a leading approach for obtaining such intervals via the empirical estimation of quantiles in the (non-parametric) distribution of outputs. This method is simple, computationally inexpensive, interpretable, assumption-free, and effective. However, it does require that the specific quantiles being learned are chosen a priori. This results in (a) intervals that are arbitrarily symmetric around the median which is sub-optimal for realistic skewed distributions, or (b) learning an excessive number of intervals. In this work, we propose Relaxed Quantile Regression (RQR), a direct alternative to quantile regression based interval construction that removes this arbitrary constraint whilst maintaining its strengths. We demonstrate that this added flexibility results in intervals with an improvement in desirable qualities (e.g. mean width) whilst retaining the essential coverage guarantees of quantile regression.","sentences":["Constructing valid prediction intervals rather than point estimates is a well-established approach for uncertainty quantification in the regression setting.","Models equipped with this capacity output an interval of values in which the ground truth target will fall with some prespecified probability.","This is an essential requirement in many real-world applications where simple point predictions' inability to convey the magnitude and frequency of errors renders them insufficient for high-stakes decisions.","Quantile regression is a leading approach for obtaining such intervals via the empirical estimation of quantiles in the (non-parametric) distribution of outputs.","This method is simple, computationally inexpensive, interpretable, assumption-free, and effective.","However, it does require that the specific quantiles being learned are chosen a priori.","This results in (a) intervals that are arbitrarily symmetric around the median which is sub-optimal for realistic skewed distributions, or (b) learning an excessive number of intervals.","In this work, we propose Relaxed Quantile Regression (RQR), a direct alternative to quantile regression based interval construction that removes this arbitrary constraint whilst maintaining its strengths.","We demonstrate that this added flexibility results in intervals with an improvement in desirable qualities (e.g. mean width) whilst retaining the essential coverage guarantees of quantile regression."],"url":"http://arxiv.org/abs/2406.03258v1","category":"stat.ML"}
{"created":"2024-06-05 13:15:58","title":"Toward the measurement of neutrino masses: Performance of cosmic magnification with submillimeter galaxies","abstract":"The phenomenon of magnification bias can induce a non-negligible angular correlation between two samples of galaxies with nonoverlapping redshift distributions. This signal is particularly clear when background submillimeter galaxies are used, and has been shown to constitute an independent cosmological probe. This work extends prior studies on the submillimeter galaxy magnification bias to the massive neutrino scenario, with the aim being to assess its sensitivity as a cosmological observable to the sum of neutrino masses. The measurements of the angular cross-correlation function between moderate redshift GAMA galaxies and high-redshift submillimeter H-ATLAS galaxies are fit to the weak lensing prediction down to the arcmin scale. The signal is interpreted under the halo model, which is modified to accommodate massive neutrinos. We discuss the impact of the choice of cosmological parametrization on the sensitivity to neutrino masses. The currently available data on the magnification bias affecting submillimeter galaxies are sensitive to neutrino masses when a cosmological parametrization in terms of the primordial amplitude of the power spectrum $(A_s$) is chosen over the local root mean square of smoothed linear density perturbations $(\\sigma_8$). A clear upper limit on the sum of neutrino masses can be derived if the value of $A_s$ is either fixed or assigned a narrow Gaussian prior, a behavior that is robust against changes to the chosen value.","sentences":["The phenomenon of magnification bias can induce a non-negligible angular correlation between two samples of galaxies with nonoverlapping redshift distributions.","This signal is particularly clear when background submillimeter galaxies are used, and has been shown to constitute an independent cosmological probe.","This work extends prior studies on the submillimeter galaxy magnification bias to the massive neutrino scenario, with the aim being to assess its sensitivity as a cosmological observable to the sum of neutrino masses.","The measurements of the angular cross-correlation function between moderate redshift GAMA galaxies and high-redshift submillimeter H-ATLAS galaxies are fit to the weak lensing prediction down to the arcmin scale.","The signal is interpreted under the halo model, which is modified to accommodate massive neutrinos.","We discuss the impact of the choice of cosmological parametrization on the sensitivity to neutrino masses.","The currently available data on the magnification bias affecting submillimeter galaxies are sensitive to neutrino masses when a cosmological parametrization in terms of the primordial amplitude of the power spectrum $(A_s$) is chosen over the local root mean square of smoothed linear density perturbations $(\\sigma_8$).","A clear upper limit on the sum of neutrino masses can be derived if the value of $A_s$ is either fixed or assigned a narrow Gaussian prior, a behavior that is robust against changes to the chosen value."],"url":"http://arxiv.org/abs/2406.03236v1","category":"astro-ph.CO"}
{"created":"2024-06-05 12:23:02","title":"Graph Neural Network Explanations are Fragile","abstract":"Explainable Graph Neural Network (GNN) has emerged recently to foster the trust of using GNNs. Existing GNN explainers are developed from various perspectives to enhance the explanation performance. We take the first step to study GNN explainers under adversarial attack--We found that an adversary slightly perturbing graph structure can ensure GNN model makes correct predictions, but the GNN explainer yields a drastically different explanation on the perturbed graph. Specifically, we first formulate the attack problem under a practical threat model (i.e., the adversary has limited knowledge about the GNN explainer and a restricted perturbation budget). We then design two methods (i.e., one is loss-based and the other is deduction-based) to realize the attack. We evaluate our attacks on various GNN explainers and the results show these explainers are fragile.","sentences":["Explainable Graph Neural Network (GNN) has emerged recently to foster the trust of using GNNs.","Existing GNN explainers are developed from various perspectives to enhance the explanation performance.","We take the first step to study GNN explainers under adversarial attack--We found that an adversary slightly perturbing graph structure can ensure GNN model makes correct predictions, but the GNN explainer yields a drastically different explanation on the perturbed graph.","Specifically, we first formulate the attack problem under a practical threat model (i.e., the adversary has limited knowledge about the GNN explainer and a restricted perturbation budget).","We then design two methods (i.e., one is loss-based and the other is deduction-based) to realize the attack.","We evaluate our attacks on various GNN explainers and the results show these explainers are fragile."],"url":"http://arxiv.org/abs/2406.03193v1","category":"cs.CR"}
{"created":"2024-06-05 12:13:18","title":"Reconstructing training data from document understanding models","abstract":"Document understanding models are increasingly employed by companies to supplant humans in processing sensitive documents, such as invoices, tax notices, or even ID cards. However, the robustness of such models to privacy attacks remains vastly unexplored. This paper presents CDMI, the first reconstruction attack designed to extract sensitive fields from the training data of these models. We attack LayoutLM and BROS architectures, demonstrating that an adversary can perfectly reconstruct up to 4.1% of the fields of the documents used for fine-tuning, including some names, dates, and invoice amounts up to six-digit numbers. When our reconstruction attack is combined with a membership inference attack, our attack accuracy escalates to 22.5%. In addition, we introduce two new end-to-end metrics and evaluate our approach under various conditions: unimodal or bimodal data, LayoutLM or BROS backbones, four fine-tuning tasks, and two public datasets (FUNSD and SROIE). We also investigate the interplay between overfitting, predictive performance, and susceptibility to our attack. We conclude with a discussion on possible defenses against our attack and potential future research directions to construct robust document understanding models.","sentences":["Document understanding models are increasingly employed by companies to supplant humans in processing sensitive documents, such as invoices, tax notices, or even ID cards.","However, the robustness of such models to privacy attacks remains vastly unexplored.","This paper presents CDMI, the first reconstruction attack designed to extract sensitive fields from the training data of these models.","We attack LayoutLM and BROS architectures, demonstrating that an adversary can perfectly reconstruct up to 4.1% of the fields of the documents used for fine-tuning, including some names, dates, and invoice amounts up to six-digit numbers.","When our reconstruction attack is combined with a membership inference attack, our attack accuracy escalates to 22.5%.","In addition, we introduce two new end-to-end metrics and evaluate our approach under various conditions: unimodal or bimodal data, LayoutLM or BROS backbones, four fine-tuning tasks, and two public datasets (FUNSD and SROIE).","We also investigate the interplay between overfitting, predictive performance, and susceptibility to our attack.","We conclude with a discussion on possible defenses against our attack and potential future research directions to construct robust document understanding models."],"url":"http://arxiv.org/abs/2406.03182v1","category":"cs.CR"}
{"created":"2024-06-05 12:09:21","title":"The FLAMINGO Project: A comparison of galaxy cluster samples selected on mass, X-ray luminosity, Compton-Y parameter, or galaxy richness","abstract":"Galaxy clusters provide an avenue to expand our knowledge of cosmology and galaxy evolution. Because it is difficult to accurately measure the total mass of a large number of individual clusters, cluster samples are typically selected using an observable proxy for mass. Selection effects are therefore a key problem in understanding galaxy cluster statistics. We make use of the $(2.8~\\rm{Gpc})^3$ FLAMINGO hydrodynamical simulation to investigate how selection based on X-ray luminosity, thermal Sunyaev-Zeldovich effect or galaxy richness influences the halo mass distribution. We define our selection cuts based on the median value of the observable at a fixed mass and compare the resulting samples to a mass-selected sample. We find that all samples are skewed towards lower mass haloes. For X-ray luminosity and richness cuts below a critical value, scatter dominates over the trend with mass and the median mass becomes biased increasingly low with respect to a mass-selected sample. At $z\\leq0.5$, observable cuts corresponding to median halo masses between $M_\\text{500c}=10^{14}$ and $10^{15}~\\rm{M_{\\odot}}$ give nearly unbiased median masses for all selection methods, but X-ray selection results in biased medians for higher masses. For cuts corresponding to median masses $<10^{14}$ at $z\\leq0.5$ and for all masses at $z\\geq1$, only Compton-Y selection yields nearly unbiased median masses. Importantly, even when the median mass is unbiased, the scatter is not because for each selection the sample is skewed towards lower masses than a mass-selected sample. Each selection leads to a different bias in secondary quantities like cool-core fraction, temperature and gas fraction.","sentences":["Galaxy clusters provide an avenue to expand our knowledge of cosmology and galaxy evolution.","Because it is difficult to accurately measure the total mass of a large number of individual clusters, cluster samples are typically selected using an observable proxy for mass.","Selection effects are therefore a key problem in understanding galaxy cluster statistics.","We make use of the $(2.8~\\rm{Gpc})^3$ FLAMINGO hydrodynamical simulation to investigate how selection based on X-ray luminosity, thermal Sunyaev-Zeldovich effect or galaxy richness influences the halo mass distribution.","We define our selection cuts based on the median value of the observable at a fixed mass and compare the resulting samples to a mass-selected sample.","We find that all samples are skewed towards lower mass haloes.","For X-ray luminosity and richness cuts below a critical value, scatter dominates over the trend with mass and the median mass becomes biased increasingly low with respect to a mass-selected sample.","At $z\\leq0.5$, observable cuts corresponding to median halo masses between $M_\\text{500c}=10^{14}$ and $10^{15}~\\rm{M_{\\odot}}$ give nearly unbiased median masses for all selection methods, but X-ray selection results in biased medians for higher masses.","For cuts corresponding to median masses $<10^{14}$ at $z\\leq0.5$ and for all masses at $z\\geq1$, only Compton-Y selection yields nearly unbiased median masses.","Importantly, even when the median mass is unbiased, the scatter is not because for each selection the sample is skewed towards lower masses than a mass-selected sample.","Each selection leads to a different bias in secondary quantities like cool-core fraction, temperature and gas fraction."],"url":"http://arxiv.org/abs/2406.03180v1","category":"astro-ph.CO"}
{"created":"2024-06-05 12:03:27","title":"High-Dimensional Kernel Methods under Covariate Shift: Data-Dependent Implicit Regularization","abstract":"This paper studies kernel ridge regression in high dimensions under covariate shifts and analyzes the role of importance re-weighting. We first derive the asymptotic expansion of high dimensional kernels under covariate shifts. By a bias-variance decomposition, we theoretically demonstrate that the re-weighting strategy allows for decreasing the variance. For bias, we analyze the regularization of the arbitrary or well-chosen scale, showing that the bias can behave very differently under different regularization scales. In our analysis, the bias and variance can be characterized by the spectral decay of a data-dependent regularized kernel: the original kernel matrix associated with an additional re-weighting matrix, and thus the re-weighting strategy can be regarded as a data-dependent regularization for better understanding. Besides, our analysis provides asymptotic expansion of kernel functions/vectors under covariate shift, which has its own interest.","sentences":["This paper studies kernel ridge regression in high dimensions under covariate shifts and analyzes the role of importance re-weighting.","We first derive the asymptotic expansion of high dimensional kernels under covariate shifts.","By a bias-variance decomposition, we theoretically demonstrate that the re-weighting strategy allows for decreasing the variance.","For bias, we analyze the regularization of the arbitrary or well-chosen scale, showing that the bias can behave very differently under different regularization scales.","In our analysis, the bias and variance can be characterized by the spectral decay of a data-dependent regularized kernel: the original kernel matrix associated with an additional re-weighting matrix, and thus the re-weighting strategy can be regarded as a data-dependent regularization for better understanding.","Besides, our analysis provides asymptotic expansion of kernel functions/vectors under covariate shift, which has its own interest."],"url":"http://arxiv.org/abs/2406.03171v1","category":"stat.ML"}
{"created":"2024-06-05 11:59:40","title":"On semi-transitive orientability of circulant graphs","abstract":"A graph $G = (V, E)$ is said to be word-representable if a word $w$ can be formed using the letters of the alphabet $V$ such that for every pair of vertices $x$ and $y$, $xy \\in E$ if and only if $x$ and $y$ alternate in $w$. A \\textit{semi-transitive} orientation is an acyclic directed graph where for any directed path $v_0 \\rightarrow v_1 \\rightarrow \\ldots \\rightarrow v_m$, $m \\ge 2$ either there is no arc between $v_0$ and $v_m$ or for all $1 \\le i < j \\le m$ there is an arc between $v_i$ and $v_j$. An undirected graph is semi-transitive if it admits a semi-transitive orientation. For given positive integers $n, a_1, a_2, \\ldots, a_k$, we consider the undirected circulant graph with set of vertices $\\{0, 1, 2, \\ldots, n-1\\}$ and the set of edges$\\{ij ~ | ~ (i - j) \\pmod n$ or $(j-i) \\pmod n$ are in $\\{a_1, a_2, \\ldots, a_k\\}\\}$, where $ 0 < a_1 < a_2 < \\ldots < a_k < (n+1)/2$. Recently, Kitaev and Pyatkin have shown that every $4$-regular circulant graph is semi-transitive. Further, they have posed an open problem regarding the semi-transitive orientability of circulant graphs for which the elements of the set $\\{a_1, a_2, \\ldots, a_k\\}$ are consecutive positive integers.   In this paper, we solve the problem mentioned above. In addition, we show that under certain assumptions, some $k(\\ge5)$-regular circulant graphs are semi-transitive, and some are not. Moreover, since a semi-transitive orientation is a characterisation of word-representability, we give some upper bound for the representation number of certain $k$-regular circulant graphs.","sentences":["A graph $G = (V, E)$ is said to be word-representable if a word $w$ can be formed using the letters of the alphabet $V$ such that for every pair of vertices $x$ and $y$, $xy \\in E$ if and only if $x$ and $y$ alternate in $w$. A \\textit{semi-transitive} orientation is an acyclic directed graph where for any directed path $v_0 \\rightarrow v_1 \\rightarrow \\ldots \\rightarrow v_m$, $m \\ge 2$ either there is no arc between $v_0$ and $v_m$ or for all $1 \\le i <","j \\le m$ there is an arc between $v_i$ and $v_j$. An undirected graph is semi-transitive if it admits a semi-transitive orientation.","For given positive integers $n, a_1, a_2, \\ldots, a_k$, we consider the undirected circulant graph with set of vertices $\\{0, 1, 2, \\ldots, n-1\\}$ and the set of edges$\\{ij ~ | ~","(i - j) \\pmod n$ or $(j-i) \\pmod n$ are in $\\{a_1, a_2, \\ldots, a_k\\}\\}$, where $ 0 < a_1 < a_2 <","\\ldots < a_k < (n+1)/2$. Recently, Kitaev and Pyatkin have shown that every $4$-regular circulant graph is semi-transitive.","Further, they have posed an open problem regarding the semi-transitive orientability of circulant graphs for which the elements of the set $\\{a_1, a_2, \\ldots, a_k\\}$ are consecutive positive integers.   ","In this paper, we solve the problem mentioned above.","In addition, we show that under certain assumptions, some $k(\\ge5)$-regular circulant graphs are semi-transitive, and some are not.","Moreover, since a semi-transitive orientation is a characterisation of word-representability, we give some upper bound for the representation number of certain $k$-regular circulant graphs."],"url":"http://arxiv.org/abs/2406.03168v1","category":"math.CO"}
{"created":"2024-06-05 11:16:55","title":"Dynamic Spectral Clustering with Provable Approximation Guarantee","abstract":"This paper studies clustering algorithms for dynamically evolving graphs $\\{G_t\\}$, in which new edges (and potential new vertices) are added into a graph, and the underlying cluster structure of the graph can gradually change. The paper proves that, under some mild condition on the cluster-structure, the clusters of the final graph $G_T$ of $n_T$ vertices at time $T$ can be well approximated by a dynamic variant of the spectral clustering algorithm. The algorithm runs in amortised update time $O(1)$ and query time $o(n_T)$. Experimental studies on both synthetic and real-world datasets further confirm the practicality of our designed algorithm.","sentences":["This paper studies clustering algorithms for dynamically evolving graphs $\\{G_t\\}$, in which new edges (and potential new vertices) are added into a graph, and the underlying cluster structure of the graph can gradually change.","The paper proves that, under some mild condition on the cluster-structure, the clusters of the final graph $G_T$ of $n_T$ vertices at time $T$ can be well approximated by a dynamic variant of the spectral clustering algorithm.","The algorithm runs in amortised update time $O(1)$ and query time $o(n_T)$. Experimental studies on both synthetic and real-world datasets further confirm the practicality of our designed algorithm."],"url":"http://arxiv.org/abs/2406.03152v1","category":"cs.DS"}
{"created":"2024-06-05 11:02:47","title":"A dynamical implementation of canonical second quantization on a quantum computer","abstract":"We develop theoretical methods for the implementation of creation and destruction operators in separate registers of a quantum computer, allowing for a transparent and dynamical creation and destruction of particle modes in second quantization in problems with variable particle number. We establish theorems for the commutation (anticommutation) relations on a finite memory bank and provide the needed symmetrizing and antisymmetrizing operators. Finally, we provide formulae in terms of these operators for unitary evolution under conventional two- and four-body Hamiltonian terms, as well as terms varying the particle number. In this formalism, the number of qubits needed to codify $n$ particles with $N_p$ modes each is of order $n\\log_2 N_p$. Such scaling is more efficient than the Jordan-Wigner transformation which requires $O(N_p)$ qubits, whenever there are a modest number of particles with a large number of states available to each (and less advantageous for a large number of particles with few states available to each). And although less efficient, it is also less cumbersome than compact encoding.","sentences":["We develop theoretical methods for the implementation of creation and destruction operators in separate registers of a quantum computer, allowing for a transparent and dynamical creation and destruction of particle modes in second quantization in problems with variable particle number.","We establish theorems for the commutation (anticommutation) relations on a finite memory bank and provide the needed symmetrizing and antisymmetrizing operators.","Finally, we provide formulae in terms of these operators for unitary evolution under conventional two-","and","four-body Hamiltonian terms, as well as terms varying the particle number.","In this formalism, the number of qubits needed to codify $n$ particles with $N_p$ modes each is of order $n\\log_2 N_p$. Such scaling is more efficient than the Jordan-Wigner transformation which requires $O(N_p)$ qubits, whenever there are a modest number of particles with a large number of states available to each (and less advantageous for a large number of particles with few states available to each).","And although less efficient, it is also less cumbersome than compact encoding."],"url":"http://arxiv.org/abs/2406.03147v1","category":"hep-th"}
{"created":"2024-06-05 10:51:17","title":"Continual Traffic Forecasting via Mixture of Experts","abstract":"The real-world traffic networks undergo expansion through the installation of new sensors, implying that the traffic patterns continually evolve over time. Incrementally training a model on the newly added sensors would make the model forget the past knowledge, i.e., catastrophic forgetting, while retraining the model on the entire network to capture these changes is highly inefficient. To address these challenges, we propose a novel Traffic Forecasting Mixture of Experts (TFMoE) for traffic forecasting under evolving networks. The main idea is to segment the traffic flow into multiple homogeneous groups, and assign an expert model responsible for a specific group. This allows each expert model to concentrate on learning and adapting to a specific set of patterns, while minimizing interference between the experts during training, thereby preventing the dilution or replacement of prior knowledge, which is a major cause of catastrophic forgetting. Through extensive experiments on a real-world long-term streaming network dataset, PEMSD3-Stream, we demonstrate the effectiveness and efficiency of TFMoE. Our results showcase superior performance and resilience in the face of catastrophic forgetting, underscoring the effectiveness of our approach in dealing with continual learning for traffic flow forecasting in long-term streaming networks.","sentences":["The real-world traffic networks undergo expansion through the installation of new sensors, implying that the traffic patterns continually evolve over time.","Incrementally training a model on the newly added sensors would make the model forget the past knowledge, i.e., catastrophic forgetting, while retraining the model on the entire network to capture these changes is highly inefficient.","To address these challenges, we propose a novel Traffic Forecasting Mixture of Experts (TFMoE) for traffic forecasting under evolving networks.","The main idea is to segment the traffic flow into multiple homogeneous groups, and assign an expert model responsible for a specific group.","This allows each expert model to concentrate on learning and adapting to a specific set of patterns, while minimizing interference between the experts during training, thereby preventing the dilution or replacement of prior knowledge, which is a major cause of catastrophic forgetting.","Through extensive experiments on a real-world long-term streaming network dataset, PEMSD3-Stream, we demonstrate the effectiveness and efficiency of TFMoE. Our results showcase superior performance and resilience in the face of catastrophic forgetting, underscoring the effectiveness of our approach in dealing with continual learning for traffic flow forecasting in long-term streaming networks."],"url":"http://arxiv.org/abs/2406.03140v1","category":"cs.LG"}
{"created":"2024-06-05 10:18:05","title":"Fully printed flexible perovskite solar modules with improved energy alignment by tin oxide surface modification","abstract":"Fully printed flexible perovskite solar cells (f-PSCs) show great potential for the commercialization of perovskite photovoltaics owing to their compatibility with high-throughput roll-to-roll (R2R) production. However, the challenge remains in the deficiency in controlling interfacial recombination losses of the functional layer, causing remarkable loss of power conversion efficiency (PCE) in industrial production. Here, a fullerene-substituted alkylphosphonic acid dipole layer is introduced between the R2R-printed tin oxide electron transport layer and the perovskite active layer to reduce the energetic barrier and to suppress surface recombination at the buried interface. The resulting f-PSCs exhibit a PCE of 17.0% with negligible hysteresis, retain 95% of their initial PCE over 3000 bending cycles and achieve a T95 lifetime of 1200 h under 1 sun and 65 degreeC in nitrogen atmosphere. Moreover, the fully printed flexible perovskite solar mini-modules (f-PSMs) with a 20.25 cm2 aperture area achieve a PCE of 11.6%. The encapsulated f-PSMs retain 90% of their initial PCE after 500 h damp-heat testing at 65 degreeC and 85% relative humidity (ISOS-D3). This work marks an important progress toward the realization of efficient and stable flexible perovskite photovoltaics for commercialization.","sentences":["Fully printed flexible perovskite solar cells (f-PSCs) show great potential for the commercialization of perovskite photovoltaics owing to their compatibility with high-throughput roll-to-roll (R2R) production.","However, the challenge remains in the deficiency in controlling interfacial recombination losses of the functional layer, causing remarkable loss of power conversion efficiency (PCE) in industrial production.","Here, a fullerene-substituted alkylphosphonic acid dipole layer is introduced between the R2R-printed tin oxide electron transport layer and the perovskite active layer to reduce the energetic barrier and to suppress surface recombination at the buried interface.","The resulting f-PSCs exhibit a PCE of 17.0% with negligible hysteresis, retain 95% of their initial PCE over 3000 bending cycles and achieve a T95 lifetime of 1200 h under 1 sun and 65 degreeC in nitrogen atmosphere.","Moreover, the fully printed flexible perovskite solar mini-modules (f-PSMs) with a 20.25 cm2 aperture area achieve a PCE of 11.6%.","The encapsulated f-PSMs retain 90% of their initial PCE after 500 h damp-heat testing at 65 degreeC and 85% relative humidity (ISOS-D3).","This work marks an important progress toward the realization of efficient and stable flexible perovskite photovoltaics for commercialization."],"url":"http://arxiv.org/abs/2406.03123v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-05 08:57:41","title":"Local to Global: Learning Dynamics and Effect of Initialization for Transformers","abstract":"In recent years, transformer-based models have revolutionized deep learning, particularly in sequence modeling. To better understand this phenomenon, there is a growing interest in using Markov input processes to study transformers. However, our current understanding in this regard remains limited with many fundamental questions about how transformers learn Markov chains still unanswered. In this paper, we address this by focusing on first-order Markov chains and single-layer transformers, providing a comprehensive characterization of the learning dynamics in this context. Specifically, we prove that transformer parameters trained on next-token prediction loss can either converge to global or local minima, contingent on the initialization and the Markovian data properties, and we characterize the precise conditions under which this occurs. To the best of our knowledge, this is the first result of its kind highlighting the role of initialization. We further demonstrate that our theoretical findings are corroborated by empirical evidence. Based on these insights, we provide guidelines for the initialization of transformer parameters and demonstrate their effectiveness. Finally, we outline several open problems in this arena. Code is available at: \\url{https://anonymous.4open.science/r/Local-to-Global-C70B/}.","sentences":["In recent years, transformer-based models have revolutionized deep learning, particularly in sequence modeling.","To better understand this phenomenon, there is a growing interest in using Markov input processes to study transformers.","However, our current understanding in this regard remains limited with many fundamental questions about how transformers learn Markov chains still unanswered.","In this paper, we address this by focusing on first-order Markov chains and single-layer transformers, providing a comprehensive characterization of the learning dynamics in this context.","Specifically, we prove that transformer parameters trained on next-token prediction loss can either converge to global or local minima, contingent on the initialization and the Markovian data properties, and we characterize the precise conditions under which this occurs.","To the best of our knowledge, this is the first result of its kind highlighting the role of initialization.","We further demonstrate that our theoretical findings are corroborated by empirical evidence.","Based on these insights, we provide guidelines for the initialization of transformer parameters and demonstrate their effectiveness.","Finally, we outline several open problems in this arena.","Code is available at: \\url{https://anonymous.4open.science/r/Local-to-Global-C70B/}."],"url":"http://arxiv.org/abs/2406.03072v1","category":"cs.LG"}
{"created":"2024-06-05 08:38:41","title":"Whistler waves in the quasi-parallel and quasi-perpendicular magnetosheath","abstract":"In the Earth's magnetosheath (MSH), several processes contribute to energy dissipation and plasma heating, one of which is wave-particle interactions between whistler waves and electrons. However, the overall impact of whistlers on electron dynamics in the MSH remains to be quantified. We analyze 18 hours of burst-mode measurements from the Magnetospheric Multiscale (MMS) mission, including data from the unbiased magnetosheath campaign during February-March 2023. We present a statistical study of 34,409 whistler waves found using automatic detection. We compare wave occurrence in the different MSH geometries and find three times higher occurrence in the quasi-perpendicular MSH compared to the quasi-parallel case. We also study the wave properties and find that the waves propagate quasi-parallel to the background magnetic field, have a median frequency of 0.2 times the electron cyclotron frequency, median amplitude of 0.03-0.06 nT (30-60 pT), and median duration of a few tens of wave periods. The whistler waves are preferentially observed in local magnetic dips and density peaks and are not associated with an increased temperature anisotropy. Also, almost no whistlers are observed in regions with parallel electron plasma beta lower than 0.1. Importantly, when estimating pitch-angle diffusion times we find that the whistler waves cause significant pitch-angle scattering of electrons in the MSH.","sentences":["In the Earth's magnetosheath (MSH), several processes contribute to energy dissipation and plasma heating, one of which is wave-particle interactions between whistler waves and electrons.","However, the overall impact of whistlers on electron dynamics in the MSH remains to be quantified.","We analyze 18 hours of burst-mode measurements from the Magnetospheric Multiscale (MMS) mission, including data from the unbiased magnetosheath campaign during February-March 2023.","We present a statistical study of 34,409 whistler waves found using automatic detection.","We compare wave occurrence in the different MSH geometries and find three times higher occurrence in the quasi-perpendicular MSH compared to the quasi-parallel case.","We also study the wave properties and find that the waves propagate quasi-parallel to the background magnetic field, have a median frequency of 0.2 times the electron cyclotron frequency, median amplitude of 0.03-0.06","nT","(30-60 pT), and median duration of a few tens of wave periods.","The whistler waves are preferentially observed in local magnetic dips and density peaks and are not associated with an increased temperature anisotropy.","Also, almost no whistlers are observed in regions with parallel electron plasma beta lower than 0.1.","Importantly, when estimating pitch-angle diffusion times we find that the whistler waves cause significant pitch-angle scattering of electrons in the MSH."],"url":"http://arxiv.org/abs/2406.03060v1","category":"physics.space-ph"}
{"created":"2024-06-05 08:31:01","title":"Sparse two-stage Bayesian meta-analysis for individualized treatments","abstract":"Individualized treatment rules tailor treatments to patients based on clinical, demographic, and other characteristics. Estimation of individualized treatment rules requires the identification of individuals who benefit most from the particular treatments and thus the detection of variability in treatment effects. To develop an effective individualized treatment rule, data from multisite studies may be required due to the low power provided by smaller datasets for detecting the often small treatment-covariate interactions. However, sharing of individual-level data is sometimes constrained. Furthermore, sparsity may arise in two senses: different data sites may recruit from different populations, making it infeasible to estimate identical models or all parameters of interest at all sites, and the number of non-zero parameters in the model for the treatment rule may be small. To address these issues, we adopt a two-stage Bayesian meta-analysis approach to estimate individualized treatment rules which optimize expected patient outcomes using multisite data without disclosing individual-level data beyond the sites. Simulation results demonstrate that our approach can provide consistent estimates of the parameters which fully characterize the optimal individualized treatment rule. We estimate the optimal Warfarin dose strategy using data from the International Warfarin Pharmacogenetics Consortium, where data sparsity and small treatment-covariate interaction effects pose additional statistical challenges.","sentences":["Individualized treatment rules tailor treatments to patients based on clinical, demographic, and other characteristics.","Estimation of individualized treatment rules requires the identification of individuals who benefit most from the particular treatments and thus the detection of variability in treatment effects.","To develop an effective individualized treatment rule, data from multisite studies may be required due to the low power provided by smaller datasets for detecting the often small treatment-covariate interactions.","However, sharing of individual-level data is sometimes constrained.","Furthermore, sparsity may arise in two senses: different data sites may recruit from different populations, making it infeasible to estimate identical models or all parameters of interest at all sites, and the number of non-zero parameters in the model for the treatment rule may be small.","To address these issues, we adopt a two-stage Bayesian meta-analysis approach to estimate individualized treatment rules which optimize expected patient outcomes using multisite data without disclosing individual-level data beyond the sites.","Simulation results demonstrate that our approach can provide consistent estimates of the parameters which fully characterize the optimal individualized treatment rule.","We estimate the optimal Warfarin dose strategy using data from the International Warfarin Pharmacogenetics Consortium, where data sparsity and small treatment-covariate interaction effects pose additional statistical challenges."],"url":"http://arxiv.org/abs/2406.03056v1","category":"stat.ME"}
{"created":"2024-06-05 08:26:53","title":"Are Your Models Still Fair? Fairness Attacks on Graph Neural Networks via Node Injections","abstract":"Despite the remarkable capabilities demonstrated by Graph Neural Networks (GNNs) in graph-related tasks, recent research has revealed the fairness vulnerabilities in GNNs when facing malicious adversarial attacks. However, all existing fairness attacks require manipulating the connectivity between existing nodes, which may be prohibited in reality. To this end, we introduce a Node Injection-based Fairness Attack (NIFA), exploring the vulnerabilities of GNN fairness in such a more realistic setting. In detail, NIFA first designs two insightful principles for node injection operations, namely the uncertainty-maximization principle and homophily-increase principle, and then optimizes injected nodes' feature matrix to further ensure the effectiveness of fairness attacks. Comprehensive experiments on three real-world datasets consistently demonstrate that NIFA can significantly undermine the fairness of mainstream GNNs, even including fairness-aware GNNs, by injecting merely 1% of nodes. We sincerely hope that our work can stimulate increasing attention from researchers on the vulnerability of GNN fairness, and encourage the development of corresponding defense mechanisms.","sentences":["Despite the remarkable capabilities demonstrated by Graph Neural Networks (GNNs) in graph-related tasks, recent research has revealed the fairness vulnerabilities in GNNs when facing malicious adversarial attacks.","However, all existing fairness attacks require manipulating the connectivity between existing nodes, which may be prohibited in reality.","To this end, we introduce a Node Injection-based Fairness Attack (NIFA), exploring the vulnerabilities of GNN fairness in such a more realistic setting.","In detail, NIFA first designs two insightful principles for node injection operations, namely the uncertainty-maximization principle and homophily-increase principle, and then optimizes injected nodes' feature matrix to further ensure the effectiveness of fairness attacks.","Comprehensive experiments on three real-world datasets consistently demonstrate that NIFA can significantly undermine the fairness of mainstream GNNs, even including fairness-aware GNNs, by injecting merely 1% of nodes.","We sincerely hope that our work can stimulate increasing attention from researchers on the vulnerability of GNN fairness, and encourage the development of corresponding defense mechanisms."],"url":"http://arxiv.org/abs/2406.03052v1","category":"cs.LG"}
{"created":"2024-06-05 08:14:41","title":"Ramsey numbers and extremal structures in polar spaces","abstract":"We use $p$-rank bounds on partial ovoids and the classical bounds on Ramsey numbers to obtain various upper bounds on partial $m$-ovoids in finite polar spaces. These bounds imply non-existence of $m$-ovoids for various new families of polar spaces. We give a probabilistic construction of large partial $m$-ovoids when $m$ grows linearly with the rank of the polar space.   In the special case of the symplectic spaces over the binary field, we show an equivalence between partial $m$-ovoids and a generalisation of the Oddtown theorem from extremal set theory that has been studied under the name of nearly $m$-orthogonal sets over finite fields. We give new constructions for partial $m$-ovoids in these spaces and thus $m$-nearly orthogonal sets, for small values of $m$. These constructions use triangle-free graphs whose complements have low $\\mathbb{F}_2$-rank and we give an asymptotic improvement over the state of the art. We also prove new lower bounds in the recently introduced rank-Ramsey problem for triangles vs cliques","sentences":["We use $p$-rank bounds on partial ovoids and the classical bounds on Ramsey numbers to obtain various upper bounds on partial $m$-ovoids in finite polar spaces.","These bounds imply non-existence of $m$-ovoids for various new families of polar spaces.","We give a probabilistic construction of large partial $m$-ovoids when $m$ grows linearly with the rank of the polar space.   ","In the special case of the symplectic spaces over the binary field, we show an equivalence between partial $m$-ovoids and a generalisation of the Oddtown theorem from extremal set theory that has been studied under the name of nearly $m$-orthogonal sets over finite fields.","We give new constructions for partial $m$-ovoids in these spaces and thus $m$-nearly orthogonal sets, for small values of $m$. These constructions use triangle-free graphs whose complements have low $\\mathbb{F}_2$-rank","and we give an asymptotic improvement over the state of the art.","We also prove new lower bounds in the recently introduced rank-Ramsey problem for triangles vs cliques"],"url":"http://arxiv.org/abs/2406.03043v1","category":"math.CO"}
{"created":"2024-06-05 07:59:48","title":"Instructing Prompt-to-Prompt Generation for Zero-Shot Learning","abstract":"Zero-shot learning (ZSL) aims to explore the semantic-visual interactions to discover comprehensive knowledge transferred from seen categories to classify unseen categories. Recently, prompt engineering has emerged in ZSL, demonstrating impressive potential as it enables the zero-shot transfer of diverse visual concepts to downstream tasks. However, these methods are still not well generalized to broad unseen domains. A key reason is that the fixed adaption of learnable prompts on seen domains makes it tend to over-emphasize the primary visual features observed during training. In this work, we propose a \\textbf{P}rompt-to-\\textbf{P}rompt generation methodology (\\textbf{P2P}), which addresses this issue by further embracing the instruction-following technique to distill instructive visual prompts for comprehensive transferable knowledge discovery. The core of P2P is to mine semantic-related instruction from prompt-conditioned visual features and text instruction on modal-sharing semantic concepts and then inversely rectify the visual representations with the guidance of the learned instruction prompts. This enforces the compensation for missing visual details to primary contexts and further eliminates the cross-modal disparity, endowing unseen domain generalization. Through extensive experimental results, we demonstrate the efficacy of P2P in achieving superior performance over state-of-the-art methods.","sentences":["Zero-shot learning (ZSL) aims to explore the semantic-visual interactions to discover comprehensive knowledge transferred from seen categories to classify unseen categories.","Recently, prompt engineering has emerged in ZSL, demonstrating impressive potential as it enables the zero-shot transfer of diverse visual concepts to downstream tasks.","However, these methods are still not well generalized to broad unseen domains.","A key reason is that the fixed adaption of learnable prompts on seen domains makes it tend to over-emphasize the primary visual features observed during training.","In this work, we propose a \\textbf{P}rompt-to-\\textbf{P}rompt generation methodology (\\textbf{P2P}), which addresses this issue by further embracing the instruction-following technique to distill instructive visual prompts for comprehensive transferable knowledge discovery.","The core of P2P is to mine semantic-related instruction from prompt-conditioned visual features and text instruction on modal-sharing semantic concepts and then inversely rectify the visual representations with the guidance of the learned instruction prompts.","This enforces the compensation for missing visual details to primary contexts and further eliminates the cross-modal disparity, endowing unseen domain generalization.","Through extensive experimental results, we demonstrate the efficacy of P2P in achieving superior performance over state-of-the-art methods."],"url":"http://arxiv.org/abs/2406.03032v1","category":"cs.CV"}
{"created":"2024-06-05 07:59:18","title":"Galactic Cirri at High Galactic Latitudes: I. Investigating Scatter in Slopes between Optical and far-Infrared Intensities","abstract":"Based on the slopes between DESI $g,r$ and IRAS 100 $\\mu m$ intensities, specifically $k_{g}$ and $k_{r}$, we have constructed a substantial sample of Galactic cirri. This sample covers 561.25 deg$^2$ at high Galactic latitudes (|b| $\\geq$ 30$^{\\circ}$), allowing for a systematic study of the physical parameters of the Galactic cirrus on a large scale, such as $g-r$ color, dust temperature, asymmetry factor and albedo. The ratio of $k_{g}$ and $k_{r}$ is consistent with the diffuse Galactic starlight model, suggesting that the diffuse starlight within our own Galaxy serves as the primary illumination source for the cirrus. Both $k_{g}$ and $k_{r}$ decrease slowly with increasing Galactic latitudes and IRAS 100 $\\mu m$ intensities, while they do not have a correlation with Galactic longitudes. The distribution of $k_{g}$ and $k_{r}$ confirms a significant scatter in the slopes, reaching a factor of 4-5. Such large scatter cannot be explained by the weak correlation between the slopes and Galactic latitudes and IRAS 100 $\\mu m$ intensities. Instead, it is attributed to substantial variations in the intrinsic properties of the dust, e.g., asymmetry factor and albedo. We propose that the properties of dust particles play a critical role in the observed scatter in slopes, making them the primary contributing factors. Moreover, the variations in dust properties within the cirrus are localized rather than exhibiting large-scale gradients.","sentences":["Based on the slopes between DESI $g,r$ and IRAS 100 $\\mu m$ intensities, specifically $k_{g}$ and $k_{r}$, we have constructed a substantial sample of Galactic cirri.","This sample covers 561.25 deg$^2$ at high Galactic latitudes (|b| $\\geq$ 30$^{\\circ}$), allowing for a systematic study of the physical parameters of the Galactic cirrus on a large scale, such as $g-r$ color, dust temperature, asymmetry factor and albedo.","The ratio of $k_{g}$ and $k_{r}$ is consistent with the diffuse Galactic starlight model, suggesting that the diffuse starlight within our own Galaxy serves as the primary illumination source for the cirrus.","Both $k_{g}$ and $k_{r}$ decrease slowly with increasing Galactic latitudes and IRAS 100 $\\mu m$ intensities, while they do not have a correlation with Galactic longitudes.","The distribution of $k_{g}$ and $k_{r}$ confirms a significant scatter in the slopes, reaching a factor of 4-5.","Such large scatter cannot be explained by the weak correlation between the slopes and Galactic latitudes and IRAS 100","$\\mu m$ intensities.","Instead, it is attributed to substantial variations in the intrinsic properties of the dust, e.g., asymmetry factor and albedo.","We propose that the properties of dust particles play a critical role in the observed scatter in slopes, making them the primary contributing factors.","Moreover, the variations in dust properties within the cirrus are localized rather than exhibiting large-scale gradients."],"url":"http://arxiv.org/abs/2406.03031v1","category":"astro-ph.GA"}
{"created":"2024-06-05 07:48:07","title":"Refined Horton-Strahler numbers I: a discrete bijection","abstract":"The Horton-Strahler number of a rooted tree $T$ is the height of the tallest complete binary tree that can be homeomorphically embedded in $T$. The number of full binary trees with $n$ internal vertices and Horton-Strahler number $s$ is known to be the same as the number of Dyck paths of length $2n$ whose height $h$ satisfies $\\lfloor \\log_2(1+h)\\rfloor=s$.   In this paper, we present a new bijective proof of the above result, that in fact strengthens and refines it as follows. We introduce a sequence of trees $(\\tau_i,i \\ge 0)$ which \"interpolates\" the complete binary trees, in the sense that $\\tau_{2^h-1}$ is the complete binary tree of height $h$ for all $h \\ge 0$, and $\\tau_{i+1}$ strictly contains $\\tau_i$ for all $i \\ge 0$. Defining $\\mathcal{S}(T)$ to be the largest $i$ for which $\\tau_i$ can be homeomorphically embedded in $T$, we then show that the number of full binary trees $T$ with $n$ internal vertices and with $\\mathcal{S}(T)=h$ is the same as the number of Dyck paths of length $2n$ with height $h$. (We call $\\mathcal{S}(T)$ the refined Horton-Strahler number of $T$.)   Our proof is bijective and relies on a recursive decomposition of binary trees (resp. Dyck paths) into subtrees with strictly smaller refined Horton-Strahler number (resp. subpaths with strictly smaller height). In a subsequent paper, we will show that the bijection has a continuum analogue, which transforms a Brownian continuum random tree into a Brownian excursion and under which (a continuous analogue of) the refined Horton-Strahler number of the tree becomes the height of the excursion.","sentences":["The Horton-Strahler number of a rooted tree $T$ is the height of the tallest complete binary tree that can be homeomorphically embedded in $T$. The number of full binary trees with $n$ internal vertices and Horton-Strahler number $s$ is known to be the same as the number of Dyck paths of length $2n$ whose height $h$ satisfies $\\lfloor \\log_2(1+h)\\rfloor=s$.   In this paper, we present a new bijective proof of the above result, that in fact strengthens and refines it as follows.","We introduce a sequence of trees $(\\tau_i,i \\ge 0)$ which \"interpolates\" the complete binary trees, in the sense that $\\tau_{2^h-1}$ is the complete binary tree of height $h$ for all $h \\ge 0$, and $\\tau_{i+1}$ strictly contains $\\tau_i$ for all $i \\ge 0$.","Defining $\\mathcal{S}(T)$ to be the largest $i$ for which $\\tau_i$ can be homeomorphically embedded in $T$, we then show that the number of full binary trees $T$ with $n$ internal vertices and with $\\mathcal{S}(T)=h$ is the same as the number of Dyck paths of length $2n$ with height $h$. (We call $\\mathcal{S}(T)$ the refined Horton-Strahler number of $T$.)   ","Our proof is bijective and relies on a recursive decomposition of binary trees (resp.","Dyck paths) into subtrees with strictly smaller refined Horton-Strahler number (resp.","subpaths with strictly smaller height).","In a subsequent paper, we will show that the bijection has a continuum analogue, which transforms a Brownian continuum random tree into a Brownian excursion and under which (a continuous analogue of) the refined Horton-Strahler number of the tree becomes the height of the excursion."],"url":"http://arxiv.org/abs/2406.03025v1","category":"math.CO"}
{"created":"2024-06-05 07:10:30","title":"Verified Code Transpilation with LLMs","abstract":"Domain-specific languages (DSLs) are integral to various software workflows. Such languages offer domain-specific optimizations and abstractions that improve code readability and maintainability. However, leveraging these languages requires developers to rewrite existing code using the specific DSL's API. While large language models (LLMs) have shown some success in automatic code transpilation, none of them provide any functional correctness guarantees on the transpiled code. Another approach for automating this task is verified lifting, which relies on program synthesis to find programs in the target language that are functionally equivalent to the source language program. While several verified lifting tools have been developed for various application domains, they are specialized for specific source-target languages or require significant expertise in domain knowledge to make the search efficient. In this paper, leveraging recent advances in LLMs, we propose an LLM-based approach (LLMLift) to building verified lifting tools. We use the LLM's capabilities to reason about programs to translate a given program into its corresponding equivalent in the target language. Additionally, we use LLMs to generate proofs for functional equivalence. We develop lifting-based compilers for {\\em four different} DSLs targeting different application domains. Our approach not only outperforms previous symbolic-based tools in both the number of benchmarks transpiled and transpilation time, but also requires significantly less effort to build.","sentences":["Domain-specific languages (DSLs) are integral to various software workflows.","Such languages offer domain-specific optimizations and abstractions that improve code readability and maintainability.","However, leveraging these languages requires developers to rewrite existing code using the specific DSL's API.","While large language models (LLMs) have shown some success in automatic code transpilation, none of them provide any functional correctness guarantees on the transpiled code.","Another approach for automating this task is verified lifting, which relies on program synthesis to find programs in the target language that are functionally equivalent to the source language program.","While several verified lifting tools have been developed for various application domains, they are specialized for specific source-target languages or require significant expertise in domain knowledge to make the search efficient.","In this paper, leveraging recent advances in LLMs, we propose an LLM-based approach (LLMLift) to building verified lifting tools.","We use the LLM's capabilities to reason about programs to translate a given program into its corresponding equivalent in the target language.","Additionally, we use LLMs to generate proofs for functional equivalence.","We develop lifting-based compilers for {\\em four different} DSLs targeting different application domains.","Our approach not only outperforms previous symbolic-based tools in both the number of benchmarks transpiled and transpilation time, but also requires significantly less effort to build."],"url":"http://arxiv.org/abs/2406.03003v1","category":"cs.PL"}
{"created":"2024-06-05 06:50:17","title":"The metric dimension of the total graph of a semiring","abstract":"We calculate the metric dimension of the total graph of a direct product of finite commutative antinegative semirings with their sets of zero-divisors closed under addition.","sentences":["We calculate the metric dimension of the total graph of a direct product of finite commutative antinegative semirings with their sets of zero-divisors closed under addition."],"url":"http://arxiv.org/abs/2406.02994v1","category":"math.RA"}
{"created":"2024-06-05 06:21:54","title":"Self-Supervised Skeleton Action Representation Learning: A Benchmark and Beyond","abstract":"Self-supervised learning (SSL), which aims to learn meaningful prior representations from unlabeled data, has been proven effective for label-efficient skeleton-based action understanding. Different from the image domain, skeleton data possesses sparser spatial structures and diverse representation forms, with the absence of background clues and the additional temporal dimension. This presents the new challenges for the pretext task design of spatial-temporal motion representation learning. Recently, many endeavors have been made for skeleton-based SSL and remarkable progress has been achieved. However, a systematic and thorough review is still lacking. In this paper, we conduct, for the first time, a comprehensive survey on self-supervised skeleton-based action representation learning, where various literature is organized according to their pre-training pretext task methodologies. Following the taxonomy of context-based, generative learning, and contrastive learning approaches, we make a thorough review and benchmark of existing works and shed light on the future possible directions. Our investigation demonstrates that most SSL works rely on the single paradigm, learning representations of a single level, and are evaluated on the action recognition task solely, which leaves the generalization power of skeleton SSL models under-explored. To this end, a novel and effective SSL method for skeleton is further proposed, which integrates multiple pretext tasks to jointly learn versatile representations of different granularity, substantially boosting the generalization capacity for different downstream tasks. Extensive experiments under three large-scale datasets demonstrate that the proposed method achieves the superior generalization performance on various downstream tasks, including recognition, retrieval, detection, and few-shot learning.","sentences":["Self-supervised learning (SSL), which aims to learn meaningful prior representations from unlabeled data, has been proven effective for label-efficient skeleton-based action understanding.","Different from the image domain, skeleton data possesses sparser spatial structures and diverse representation forms, with the absence of background clues and the additional temporal dimension.","This presents the new challenges for the pretext task design of spatial-temporal motion representation learning.","Recently, many endeavors have been made for skeleton-based SSL and remarkable progress has been achieved.","However, a systematic and thorough review is still lacking.","In this paper, we conduct, for the first time, a comprehensive survey on self-supervised skeleton-based action representation learning, where various literature is organized according to their pre-training pretext task methodologies.","Following the taxonomy of context-based, generative learning, and contrastive learning approaches, we make a thorough review and benchmark of existing works and shed light on the future possible directions.","Our investigation demonstrates that most SSL works rely on the single paradigm, learning representations of a single level, and are evaluated on the action recognition task solely, which leaves the generalization power of skeleton SSL models under-explored.","To this end, a novel and effective SSL method for skeleton is further proposed, which integrates multiple pretext tasks to jointly learn versatile representations of different granularity, substantially boosting the generalization capacity for different downstream tasks.","Extensive experiments under three large-scale datasets demonstrate that the proposed method achieves the superior generalization performance on various downstream tasks, including recognition, retrieval, detection, and few-shot learning."],"url":"http://arxiv.org/abs/2406.02978v1","category":"cs.CV"}
{"created":"2024-06-05 06:13:39","title":"How precisely are solute clusters in RPV steels characterized by atom probe experiments?","abstract":"Atom probe tomography (APT) is a powerful microscopy technique to characterize nano-sized clusters of the alloying elements in the bulk of reactor pressure vessel (RPV) steels. These clusters are known to dominantly determine the evolution of mechanical properties under irradiation. The results are conventionally summarized as the overall number density N and the average diameter D of the solute clusters identified in the material. Here, we demonstrate that these descriptors are intrinsically imprecise because they are steered by the parameters involved in the measurement and data processing, some of which are directly under the control of the operators, but some others not. Consequently, a direct comparison between data derived at different laboratories is compromised, and key trends such as the evolution with dose, are masked. This study relies on a state-of-the-art physical model for neutron irradiation in steels to make reliable estimates of the true microstructure before the measurement is performed, which allows the prediction of the population of solute clusters that are not seen by APT. We mimic APT measurements from simulated microstructures, performing a detailed study of the effects of the parameters of the analysis. We show that the values of N and D reported in the scientific literature can be matched by the predictions of our theoretical model only if specific sets of parameters are used for each laboratory that issued the measurements. We also show that if, on the contrary, all studied cases are analyzed in a consistent way, the scatter of N and D values is reduced.","sentences":["Atom probe tomography (APT) is a powerful microscopy technique to characterize nano-sized clusters of the alloying elements in the bulk of reactor pressure vessel (RPV) steels.","These clusters are known to dominantly determine the evolution of mechanical properties under irradiation.","The results are conventionally summarized as the overall number density N and the average diameter D of the solute clusters identified in the material.","Here, we demonstrate that these descriptors are intrinsically imprecise because they are steered by the parameters involved in the measurement and data processing, some of which are directly under the control of the operators, but some others not.","Consequently, a direct comparison between data derived at different laboratories is compromised, and key trends such as the evolution with dose, are masked.","This study relies on a state-of-the-art physical model for neutron irradiation in steels to make reliable estimates of the true microstructure before the measurement is performed, which allows the prediction of the population of solute clusters that are not seen by APT.","We mimic APT measurements from simulated microstructures, performing a detailed study of the effects of the parameters of the analysis.","We show that the values of N and D reported in the scientific literature can be matched by the predictions of our theoretical model only if specific sets of parameters are used for each laboratory that issued the measurements.","We also show that if, on the contrary, all studied cases are analyzed in a consistent way, the scatter of N and D values is reduced."],"url":"http://arxiv.org/abs/2406.02973v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-05 06:06:03","title":"Event3DGS: Event-based 3D Gaussian Splatting for Fast Egomotion","abstract":"The recent emergence of 3D Gaussian splatting (3DGS) leverages the advantage of explicit point-based representations, which significantly improves the rendering speed and quality of novel-view synthesis. However, 3D radiance field rendering in environments with high-dynamic motion or challenging illumination condition remains problematic in real-world robotic tasks. The reason is that fast egomotion is prevalent real-world robotic tasks, which induces motion blur, leading to inaccuracies and artifacts in the reconstructed structure. To alleviate this problem, we propose Event3DGS, the first method that learns Gaussian Splatting solely from raw event streams. By exploiting the high temporal resolution of event cameras and explicit point-based representation, Event3DGS can reconstruct high-fidelity 3D structures solely from the event streams under fast egomotion. Our sparsity-aware sampling and progressive training approaches allow for better reconstruction quality and consistency. To further enhance the fidelity of appearance, we explicitly incorporate the motion blur formation process into a differentiable rasterizer, which is used with a limited set of blurred RGB images to refine the appearance. Extensive experiments on multiple datasets validate the superior rendering quality of Event3DGS compared with existing approaches, with over 95% lower training time and faster rendering speed in orders of magnitude.","sentences":["The recent emergence of 3D Gaussian splatting (3DGS) leverages the advantage of explicit point-based representations, which significantly improves the rendering speed and quality of novel-view synthesis.","However, 3D radiance field rendering in environments with high-dynamic motion or challenging illumination condition remains problematic in real-world robotic tasks.","The reason is that fast egomotion is prevalent real-world robotic tasks, which induces motion blur, leading to inaccuracies and artifacts in the reconstructed structure.","To alleviate this problem, we propose Event3DGS, the first method that learns Gaussian Splatting solely from raw event streams.","By exploiting the high temporal resolution of event cameras and explicit point-based representation, Event3DGS can reconstruct high-fidelity 3D structures solely from the event streams under fast egomotion.","Our sparsity-aware sampling and progressive training approaches allow for better reconstruction quality and consistency.","To further enhance the fidelity of appearance, we explicitly incorporate the motion blur formation process into a differentiable rasterizer, which is used with a limited set of blurred RGB images to refine the appearance.","Extensive experiments on multiple datasets validate the superior rendering quality of Event3DGS compared with existing approaches, with over 95% lower training time and faster rendering speed in orders of magnitude."],"url":"http://arxiv.org/abs/2406.02972v1","category":"cs.CV"}
{"created":"2024-06-05 05:23:59","title":"Prominence and coronal rain formation by steady versus stochastic heating and how we can relate it to observations","abstract":"Prominences and coronal rain are two forms of coronal condensations for which we still lack satisfactory details on the formation pathways and conditions under which the two come to exist. We compared prominences that formed via a steady versus stochastic type of heating. We performed 2.5D simulations using the open-source MPI-AMRVAC code. To further extend the work and allow for future direct comparison with observations, we used Lightweaver to form spectra of the filament view of our steady case prominence. With that, we analysed a reconnection event that shares certain characteristics with nanojets. We show how different forms of localised heating that induce thermal instability result in prominences with different properties. The steady form of heating results in prominence with a clear vertical structure stretching across the magnetic field lines. On the other hand, stochastic heating produces many threads that predominantly have a horizontal motion along the field lines. In the steady heating case, the prominence is relatively static; however, there is evidence of reconnection happening almost the entire time the prominence is present. In the case of stochastic heating, the threads are highly dynamic, with them also exhibiting a form of transverse oscillation (strongly resembling the decayless type). The fact that the threads in the stochastic heating case are constantly moving along the field lines suppresses any conditions for reconnection. It, therefore, appears that, to first order, the choice of heating prescription defines whether the prominence-internal dynamics are oriented vertically or horizontally. We closely inspected a sample reconnection event and computed the synthetic optically thick radiation using the open-source Lightweaver radiative transfer framework.","sentences":["Prominences and coronal rain are two forms of coronal condensations for which we still lack satisfactory details on the formation pathways and conditions under which the two come to exist.","We compared prominences that formed via a steady versus stochastic type of heating.","We performed 2.5D simulations using the open-source MPI-AMRVAC code.","To further extend the work and allow for future direct comparison with observations, we used Lightweaver to form spectra of the filament view of our steady case prominence.","With that, we analysed a reconnection event that shares certain characteristics with nanojets.","We show how different forms of localised heating that induce thermal instability result in prominences with different properties.","The steady form of heating results in prominence with a clear vertical structure stretching across the magnetic field lines.","On the other hand, stochastic heating produces many threads that predominantly have a horizontal motion along the field lines.","In the steady heating case, the prominence is relatively static; however, there is evidence of reconnection happening almost the entire time the prominence is present.","In the case of stochastic heating, the threads are highly dynamic, with them also exhibiting a form of transverse oscillation (strongly resembling the decayless type).","The fact that the threads in the stochastic heating case are constantly moving along the field lines suppresses any conditions for reconnection.","It, therefore, appears that, to first order, the choice of heating prescription defines whether the prominence-internal dynamics are oriented vertically or horizontally.","We closely inspected a sample reconnection event and computed the synthetic optically thick radiation using the open-source Lightweaver radiative transfer framework."],"url":"http://arxiv.org/abs/2406.02955v1","category":"astro-ph.SR"}
{"created":"2024-06-05 05:21:39","title":"Lagrangian formulation for perfect fluid equations with the l-conformal Galilei symmetry","abstract":"Lagrangian formulation for perfect fluid equations which hold invariant under the $\\ell$-conformal Galilei group with half-integer $\\ell$ is proposed. It is based on a Clebsch-type parametrization and reproduces Lagrangian description of the Euler fluid equations for $\\ell=\\frac12$. The transition from the Lagrangian formulation to the Hamiltonian one is analyzed in detail.","sentences":["Lagrangian formulation for perfect fluid equations which hold invariant under the $\\ell$-conformal Galilei group with half-integer $\\ell$ is proposed.","It is based on a Clebsch-type parametrization and reproduces Lagrangian description of the Euler fluid equations for $\\ell=\\frac12$. The transition from the Lagrangian formulation to the Hamiltonian one is analyzed in detail."],"url":"http://arxiv.org/abs/2406.02952v1","category":"hep-th"}
{"created":"2024-06-05 05:14:37","title":"Copula-based semiparametric nonnormal transformed linear model for survival data with dependent censoring","abstract":"Although the independent censoring assumption is commonly used in survival analysis, it can be violated when the censoring time is related to the survival time, which often happens in many practical applications. To address this issue, we propose a flexible semiparametric method for dependent censored data. Our approach involves fitting the survival time and the censoring time with a joint transformed linear model, where the transformed function is unspecified. This allows for a very general class of models that can account for possible covariate effects, while also accommodating administrative censoring. We assume that the transformed variables have a bivariate nonnormal distribution based on parametric copulas and parametric marginals, which further enhances the flexibility of our method. We demonstrate the identifiability of the proposed model and establish the consistency and asymptotic normality of the model parameters under appropriate regularity conditions and assumptions. Furthermore, we evaluate the performance of our method through extensive simulation studies, and provide a real data example for illustration.","sentences":["Although the independent censoring assumption is commonly used in survival analysis, it can be violated when the censoring time is related to the survival time, which often happens in many practical applications.","To address this issue, we propose a flexible semiparametric method for dependent censored data.","Our approach involves fitting the survival time and the censoring time with a joint transformed linear model, where the transformed function is unspecified.","This allows for a very general class of models that can account for possible covariate effects, while also accommodating administrative censoring.","We assume that the transformed variables have a bivariate nonnormal distribution based on parametric copulas and parametric marginals, which further enhances the flexibility of our method.","We demonstrate the identifiability of the proposed model and establish the consistency and asymptotic normality of the model parameters under appropriate regularity conditions and assumptions.","Furthermore, we evaluate the performance of our method through extensive simulation studies, and provide a real data example for illustration."],"url":"http://arxiv.org/abs/2406.02948v1","category":"stat.ME"}
{"created":"2024-06-05 04:54:36","title":"Robots Have Been Seen and Not Heard: Effects of Consequential Sounds on Human-Perception of Robots","abstract":"Many people expect robots to move fairly quietly, or make pleasant \"beep boop\" sounds or jingles similar to what they have observed in videos of robots. Unfortunately, this expectation of quietness does not match reality, as robots make machine sounds, known as 'consequential sounds', as they move and operate. As robots become more prevalent within society, understanding the sounds produced by robots and how these sounds are perceived by people is becoming increasingly important for positive human robot interactions (HRI). This paper investigates how people respond to the consequential sounds of robots, specifically how robots make a participant feel, how much they like the robot, would be distracted by the robot, and a person's desire to colocate with robots. Participants were shown 5 videos of different robots and asked their opinions on the robots and the sounds they made. This was compared with a control condition of completely silent videos. The results in this paper demonstrate with data from 182 participants (858 trials) that consequential sounds produced by robots have a significant negative effect on human perceptions of robots. Firstly there were increased negative 'associated affects' of the participants, such as making them feel more uncomfortable or agitated around the robot. Secondly, the presence of consequential sounds correlated with participants feeling more distracted and less able to focus. Thirdly participants reported being less likely to want to colocate in a shared environment with robots.","sentences":["Many people expect robots to move fairly quietly, or make pleasant \"beep boop\" sounds or jingles similar to what they have observed in videos of robots.","Unfortunately, this expectation of quietness does not match reality, as robots make machine sounds, known as 'consequential sounds', as they move and operate.","As robots become more prevalent within society, understanding the sounds produced by robots and how these sounds are perceived by people is becoming increasingly important for positive human robot interactions (HRI).","This paper investigates how people respond to the consequential sounds of robots, specifically how robots make a participant feel, how much they like the robot, would be distracted by the robot, and a person's desire to colocate with robots.","Participants were shown 5 videos of different robots and asked their opinions on the robots and the sounds they made.","This was compared with a control condition of completely silent videos.","The results in this paper demonstrate with data from 182 participants (858 trials) that consequential sounds produced by robots have a significant negative effect on human perceptions of robots.","Firstly there were increased negative 'associated affects' of the participants, such as making them feel more uncomfortable or agitated around the robot.","Secondly, the presence of consequential sounds correlated with participants feeling more distracted and less able to focus.","Thirdly participants reported being less likely to want to colocate in a shared environment with robots."],"url":"http://arxiv.org/abs/2406.02938v1","category":"cs.RO"}
{"created":"2024-06-05 04:39:51","title":"Measurements of the branching fractions of the $P$-wave charmonium spin-singlet state $h_c(^1P_1) \\to h^+ h^-\u03c0^0/\u03b7$","abstract":"Based on $(2712.4\\pm 14.3)\\times10^{6}$ $\\psi(3686)$ events, we investigate four hadronic decay modes of the $P$-wave charmonium spin-singlet state $h_c(^1P_1) \\to h^+ h^- \\pi^0/\\eta$ ($h=\\pi$ or $K$) via the process $\\psi(3686) \\to \\pi^{0}h_c$ at BESIII. The $h_c \\to \\pi^+ \\pi^- \\pi^0$ decay is observed with a significance of 9.6$\\sigma$ after taking into account systematic uncertainties. Evidences for $h_c \\to K^+ K^- \\pi^0$ and $h_c \\to K^+ K^- \\eta$ are found with significances of $3.5\\sigma$ and $3.3\\sigma$, respectively, after considering the systematic uncertainties. The branching fractions of these decays are measured to be $\\mathcal{B}(h_c \\to \\pi^+ \\pi^- \\pi^0)=(1.36\\pm0.16\\pm0.14)\\times10^{-3}$, $\\mathcal{B}(h_c \\to K^+ K^- \\pi^0)=(3.26\\pm0.84\\pm0.36)\\times10^{-4}$, and $\\mathcal{B}(h_c \\to K^+ K^- \\eta)=(3.13\\pm1.08\\pm0.38)\\times10^{-4}$, where the first uncertainties are statistical and the second are systematic. No significant signal of $h_c\\to\\pi^+\\pi^-\\eta$ is found, and the upper limit of its decay branching fraction is determined to be $\\mathcal{B}(h_c\\to\\pi^+\\pi^-\\eta) < 4.0 \\times 10^{-4}$ at 90% confidence level.","sentences":["Based on $(2712.4\\pm 14.3)\\times10^{6}$ $\\psi(3686)$ events, we investigate four hadronic decay modes of the $P$-wave charmonium spin-singlet state $h_c(^1P_1) \\to h^+ h^- \\pi^0/\\eta$ ($h=\\pi$ or $K$) via the process $\\psi(3686) \\to \\pi^{0}h_c$ at BESIII.","The $h_c \\to \\pi^+ \\pi^- \\pi^0$ decay is observed with a significance of 9.6$\\sigma$ after taking into account systematic uncertainties.","Evidences for $h_c \\to K^+ K^- \\pi^0$ and $h_c \\to K^+ K^- \\eta$ are found with significances of $3.5\\sigma$ and $3.3\\sigma$, respectively, after considering the systematic uncertainties.","The branching fractions of these decays are measured to be $\\mathcal{B}(h_c \\to \\pi^+ \\pi^- \\pi^0)=(1.36\\pm0.16\\pm0.14)\\times10^{-3}$, $\\mathcal{B}(h_c \\to K^+ K^- \\pi^0)=(3.26\\pm0.84\\pm0.36)\\times10^{-4}$, and $\\mathcal{B}(h_c \\to K^+ K^- \\eta)=(3.13\\pm1.08\\pm0.38)\\times10^{-4}$, where the first uncertainties are statistical and the second are systematic.","No significant signal of $h_c\\to\\pi^+\\pi^-\\eta$ is found, and the upper limit of its decay branching fraction is determined to be $\\mathcal{B}(h_c\\to\\pi^+\\pi^-\\eta) < 4.0 \\times 10^{-4}$ at 90% confidence level."],"url":"http://arxiv.org/abs/2406.02931v1","category":"hep-ex"}
{"created":"2024-06-05 04:23:11","title":"Rethinking Spiking Neural Networks as State Space Models","abstract":"Spiking neural networks (SNNs) are posited as a biologically plausible alternative to conventional neural architectures, with their core computational framework resting on the extensively studied leaky integrate-and-fire (LIF) neuron design. The stateful nature of LIF neurons has spurred ongoing discussions about the ability of SNNs to process sequential data, akin to recurrent neural networks (RNNs). Despite this, there remains a significant gap in the exploration of current SNNs within the realm of long-range dependency tasks. In this study, to extend the analysis of neuronal dynamics beyond simplistic LIF mechanism, we present a novel class of stochastic spiking neuronal model grounded in state space models. We expand beyond the scalar hidden state representation of LIF neurons, which traditionally comprises only the membrane potential, by proposing an n-dimensional hidden state. Additionally, we enable fine-tuned formulation of neuronal dynamics across each layer by introducing learnable parameters, as opposed to the fixed dynamics in LIF neurons. We also develop a robust framework for scaling these neuronal models to deep SNN-based architectures, ensuring efficient parallel training while also adeptly addressing the challenge of non-differentiability of stochastic spiking operation during the backward phase. Our models attain state-of-the-art performance among SNN models across diverse long-range dependency tasks, encompassing the Long Range Arena benchmark, permuted sequential MNIST, and the Speech Command dataset. Moreover, we provide an analysis of the energy efficiency advantages, emphasizing the sparse activity pattern intrinsic to this spiking model.","sentences":["Spiking neural networks (SNNs) are posited as a biologically plausible alternative to conventional neural architectures, with their core computational framework resting on the extensively studied leaky integrate-and-fire (LIF) neuron design.","The stateful nature of LIF neurons has spurred ongoing discussions about the ability of SNNs to process sequential data, akin to recurrent neural networks (RNNs).","Despite this, there remains a significant gap in the exploration of current SNNs within the realm of long-range dependency tasks.","In this study, to extend the analysis of neuronal dynamics beyond simplistic LIF mechanism, we present a novel class of stochastic spiking neuronal model grounded in state space models.","We expand beyond the scalar hidden state representation of LIF neurons, which traditionally comprises only the membrane potential, by proposing an n-dimensional hidden state.","Additionally, we enable fine-tuned formulation of neuronal dynamics across each layer by introducing learnable parameters, as opposed to the fixed dynamics in LIF neurons.","We also develop a robust framework for scaling these neuronal models to deep SNN-based architectures, ensuring efficient parallel training while also adeptly addressing the challenge of non-differentiability of stochastic spiking operation during the backward phase.","Our models attain state-of-the-art performance among SNN models across diverse long-range dependency tasks, encompassing the Long Range Arena benchmark, permuted sequential MNIST, and the Speech Command dataset.","Moreover, we provide an analysis of the energy efficiency advantages, emphasizing the sparse activity pattern intrinsic to this spiking model."],"url":"http://arxiv.org/abs/2406.02923v1","category":"cs.NE"}
{"created":"2024-06-05 04:13:03","title":"U-KAN Makes Strong Backbone for Medical Image Segmentation and Generation","abstract":"U-Net has become a cornerstone in various visual applications such as image segmentation and diffusion probability models. While numerous innovative designs and improvements have been introduced by incorporating transformers or MLPs, the networks are still limited to linearly modeling patterns as well as the deficient interpretability. To address these challenges, our intuition is inspired by the impressive results of the Kolmogorov-Arnold Networks (KANs) in terms of accuracy and interpretability, which reshape the neural network learning via the stack of non-linear learnable activation functions derived from the Kolmogorov-Anold representation theorem. Specifically, in this paper, we explore the untapped potential of KANs in improving backbones for vision tasks. We investigate, modify and re-design the established U-Net pipeline by integrating the dedicated KAN layers on the tokenized intermediate representation, termed U-KAN. Rigorous medical image segmentation benchmarks verify the superiority of U-KAN by higher accuracy even with less computation cost. We further delved into the potential of U-KAN as an alternative U-Net noise predictor in diffusion models, demonstrating its applicability in generating task-oriented model architectures. These endeavours unveil valuable insights and sheds light on the prospect that with U-KAN, you can make strong backbone for medical image segmentation and generation. Project page: https://yes-ukan.github.io/","sentences":["U-Net has become a cornerstone in various visual applications such as image segmentation and diffusion probability models.","While numerous innovative designs and improvements have been introduced by incorporating transformers or MLPs, the networks are still limited to linearly modeling patterns as well as the deficient interpretability.","To address these challenges, our intuition is inspired by the impressive results of the Kolmogorov-Arnold Networks (KANs) in terms of accuracy and interpretability, which reshape the neural network learning via the stack of non-linear learnable activation functions derived from the Kolmogorov-Anold representation theorem.","Specifically, in this paper, we explore the untapped potential of KANs in improving backbones for vision tasks.","We investigate, modify and re-design the established U-Net pipeline by integrating the dedicated KAN layers on the tokenized intermediate representation, termed U-KAN.","Rigorous medical image segmentation benchmarks verify the superiority of U-KAN by higher accuracy even with less computation cost.","We further delved into the potential of U-KAN as an alternative U-Net noise predictor in diffusion models, demonstrating its applicability in generating task-oriented model architectures.","These endeavours unveil valuable insights and sheds light on the prospect that with U-KAN, you can make strong backbone for medical image segmentation and generation.","Project page: https://yes-ukan.github.io/"],"url":"http://arxiv.org/abs/2406.02918v1","category":"eess.IV"}
{"created":"2024-06-05 04:03:39","title":"An iterative constraint energy minimizing generalized multiscale finite element method for contact problem","abstract":"This work presents an Iterative Constraint Energy Minimizing Generalized Multiscale Finite Element Method (ICEM-GMsFEM) for solving the contact problem with high contrast coefficients. The model problem can be characterized by a variational inequality, where we add a penalty term to convert this problem into a non-smooth and non-linear unconstrained minimizing problem. The characterization of the minimizer satisfies the variational form of a mixed Dirilect-Neumann-Robin boundary value problem. So we apply CEM-GMsFEM iteratively and introduce special boundary correctors along with multiscale spaces to achieve an optimal convergence rate. Numerical results are conducted for different highly heterogeneous permeability fields, validating the fast convergence of the CEM-GMsFEM iteration in handling the contact boundary and illustrating the stability of the proposed method with different sets of parameters. We also prove the fast convergence of the proposed iterative CEM-GMsFEM method and provide an error estimate of the multiscale solution under a mild assumption.","sentences":["This work presents an Iterative Constraint Energy Minimizing Generalized Multiscale Finite Element Method (ICEM-GMsFEM) for solving the contact problem with high contrast coefficients.","The model problem can be characterized by a variational inequality, where we add a penalty term to convert this problem into a non-smooth and non-linear unconstrained minimizing problem.","The characterization of the minimizer satisfies the variational form of a mixed Dirilect-Neumann-Robin boundary value problem.","So we apply CEM-GMsFEM iteratively and introduce special boundary correctors along with multiscale spaces to achieve an optimal convergence rate.","Numerical results are conducted for different highly heterogeneous permeability fields, validating the fast convergence of the CEM-GMsFEM iteration in handling the contact boundary and illustrating the stability of the proposed method with different sets of parameters.","We also prove the fast convergence of the proposed iterative CEM-GMsFEM method and provide an error estimate of the multiscale solution under a mild assumption."],"url":"http://arxiv.org/abs/2406.02909v1","category":"math.NA"}
{"created":"2024-06-05 03:46:52","title":"Open Grounded Planning: Challenges and Benchmark Construction","abstract":"The emergence of large language models (LLMs) has increasingly drawn attention to the use of LLMs for human-like planning. Existing work on LLM-based planning either focuses on leveraging the inherent language generation capabilities of LLMs to produce free-style plans, or employs reinforcement learning approaches to learn decision-making for a limited set of actions within restricted environments. However, both approaches exhibit significant discrepancies from the open and executable requirements in real-world planning. In this paper, we propose a new planning task--open grounded planning. The primary objective of open grounded planning is to ask the model to generate an executable plan based on a variable action set, thereby ensuring the executability of the produced plan. To this end, we establishes a benchmark for open grounded planning spanning a wide range of domains. Then we test current state-of-the-art LLMs along with five planning approaches, revealing that existing LLMs and methods still struggle to address the challenges posed by grounded planning in open domains. The outcomes of this paper define and establish a foundational dataset for open grounded planning, and shed light on the potential challenges and future directions of LLM-based planning.","sentences":["The emergence of large language models (LLMs) has increasingly drawn attention to the use of LLMs for human-like planning.","Existing work on LLM-based planning either focuses on leveraging the inherent language generation capabilities of LLMs to produce free-style plans, or employs reinforcement learning approaches to learn decision-making for a limited set of actions within restricted environments.","However, both approaches exhibit significant discrepancies from the open and executable requirements in real-world planning.","In this paper, we propose a new planning task--open grounded planning.","The primary objective of open grounded planning is to ask the model to generate an executable plan based on a variable action set, thereby ensuring the executability of the produced plan.","To this end, we establishes a benchmark for open grounded planning spanning a wide range of domains.","Then we test current state-of-the-art LLMs along with five planning approaches, revealing that existing LLMs and methods still struggle to address the challenges posed by grounded planning in open domains.","The outcomes of this paper define and establish a foundational dataset for open grounded planning, and shed light on the potential challenges and future directions of LLM-based planning."],"url":"http://arxiv.org/abs/2406.02903v1","category":"cs.CL"}
{"created":"2024-06-05 03:31:49","title":"A computationally efficient queue-based algorithm for simulating volume-controlled drainage under the influence of gravity on volumetric images","abstract":"Simulating non-wetting fluid invasion in volumetric images of porous materials is of broad interest in applications as diverse as electrochemical devices and CO2 sequestration. Among available methods, image-based algorithms offer much lower computational cost compared to direct numerical simulations. Recent work has extended image-based method to incorporate more physics such as gravity and volume-controlled invasion. The present work combines these two developments to develop an image-based invasion percolation algorithm that incorporates the effect of gravity. Additionally, the presented algorithm was developed using a priority queue algorithm to drastically reduce the computational cost of the simulation. The priority queue-based method was validated against previous image-based methods both with and without the effect of gravity, showing identical results. It was also shown that the new method provides a speedup of 20X over the previous image-based methods. Finally, comparison with experimental results at three Bond numbers showed that the model can predict the real invasion process with a high accuracy with and without gravitational effects.","sentences":["Simulating non-wetting fluid invasion in volumetric images of porous materials is of broad interest in applications as diverse as electrochemical devices and CO2 sequestration.","Among available methods, image-based algorithms offer much lower computational cost compared to direct numerical simulations.","Recent work has extended image-based method to incorporate more physics such as gravity and volume-controlled invasion.","The present work combines these two developments to develop an image-based invasion percolation algorithm that incorporates the effect of gravity.","Additionally, the presented algorithm was developed using a priority queue algorithm to drastically reduce the computational cost of the simulation.","The priority queue-based method was validated against previous image-based methods both with and without the effect of gravity, showing identical results.","It was also shown that the new method provides a speedup of 20X over the previous image-based methods.","Finally, comparison with experimental results at three Bond numbers showed that the model can predict the real invasion process with a high accuracy with and without gravitational effects."],"url":"http://arxiv.org/abs/2406.02895v1","category":"physics.flu-dyn"}
{"created":"2024-06-05 03:18:30","title":"Investigation of boulder distribution in (1) Ceres and insight into its surface evolution","abstract":"The surface conditions of terrestrial bodies strongly reflect their geological evolutionary processes and vary among various terrestrial bodies. This diversity is attributed to variations in the timescales of boulder formation through processes such as impact cratering, rockfalls from crater walls, seismic motion, and boulder fragmentation caused by micrometeoroid impacts and thermal stress. In this study, we examined boulders on Ceres using high-resolution images with a resolution of approximately 5 m/px obtained during the Ceres Extended Mission 2 Orbit 7 of the Dawn mission. Almost all boulders were present around impact craters, even at a resolution of 5 m/px, thus indicating that the boulders on Ceres were created by impact cratering alone. The maximum boulder size on Ceres is approximately 200 m, even around large craters, which may indicate the upper size limit determined by the mechanical strength of the boulders, such as the tensile strength, the scale effect, and/or shattering strength. The slope of the size-frequency distribution of boulders on Ceres varied significantly across the range of boulder sizes, thus making it difficult to describe it using a single function of a power-law relationship; in particular, it changed at approximately 100 m, thus indicating that destructive or formation mechanisms may be different for large boulders > 100 m and for small boulders < 100 m. There may also be a subsurface structure that prevents the formation of small boulders, although this is difficult to argue conclusively. We estimated that the lifetime of boulders larger than 50 m was equivalent to or shorter than 100 Myr. This lifetime is consistent with a theoretical estimation assuming that micrometeoroid impacts are the primary destructive mechanism.","sentences":["The surface conditions of terrestrial bodies strongly reflect their geological evolutionary processes and vary among various terrestrial bodies.","This diversity is attributed to variations in the timescales of boulder formation through processes such as impact cratering, rockfalls from crater walls, seismic motion, and boulder fragmentation caused by micrometeoroid impacts and thermal stress.","In this study, we examined boulders on Ceres using high-resolution images with a resolution of approximately 5 m/px obtained during the Ceres Extended Mission 2 Orbit 7 of the Dawn mission.","Almost all boulders were present around impact craters, even at a resolution of 5 m/px, thus indicating that the boulders on Ceres were created by impact cratering alone.","The maximum boulder size on Ceres is approximately 200 m, even around large craters, which may indicate the upper size limit determined by the mechanical strength of the boulders, such as the tensile strength, the scale effect, and/or shattering strength.","The slope of the size-frequency distribution of boulders on Ceres varied significantly across the range of boulder sizes, thus making it difficult to describe it using a single function of a power-law relationship; in particular, it changed at approximately 100 m, thus indicating that destructive or formation mechanisms may be different for large boulders > 100 m and for small boulders < 100 m. There may also be a subsurface structure that prevents the formation of small boulders, although this is difficult to argue conclusively.","We estimated that the lifetime of boulders larger than 50 m was equivalent to or shorter than 100 Myr.","This lifetime is consistent with a theoretical estimation assuming that micrometeoroid impacts are the primary destructive mechanism."],"url":"http://arxiv.org/abs/2406.02892v1","category":"astro-ph.EP"}
{"created":"2024-06-05 03:08:37","title":"USM RNN-T model weights binarization","abstract":"Large-scale universal speech models (USM) are already used in production. However, as the model size grows, the serving cost grows too. Serving cost of large models is dominated by model size that is why model size reduction is an important research topic. In this work we are focused on model size reduction using weights only quantization. We present the weights binarization of USM Recurrent Neural Network Transducer (RNN-T) and show that its model size can be reduced by 15.9x times at cost of word error rate (WER) increase by only 1.9% in comparison to the float32 model. It makes it attractive for practical applications.","sentences":["Large-scale universal speech models (USM) are already used in production.","However, as the model size grows, the serving cost grows too.","Serving cost of large models is dominated by model size that is why model size reduction is an important research topic.","In this work we are focused on model size reduction using weights only quantization.","We present the weights binarization of USM Recurrent Neural Network Transducer (RNN-T) and show that its model size can be reduced by 15.9x times at cost of word error rate (WER) increase by only 1.9% in comparison to the float32 model.","It makes it attractive for practical applications."],"url":"http://arxiv.org/abs/2406.02887v1","category":"eess.AS"}
{"created":"2024-06-05 02:53:02","title":"The test of investors' behavioral bias through the price discovery process in cryptoasset exchange\" Transactional-level evidence from Thailand","abstract":"Analyzing investors' trading behavior in cryptoasset markets provides new evidence supporting the theory that retail investors likely exhibit behavioral biases. We investigate the price discovery process between Thailand's most highly liquid exchange and the global exchange. Under the no-arbitrage assumption, bid-offer quotes in the local exchange should quickly move to match the new price levels in the global exchange, as the price process of cryptoassets in the local exchange does not contain new information that can lead the price dynamic in the global exchange. We analyze intraday bid-offer quotes and investors' portfolio positions and find that investors exhibit the disposition effect by attempting to sell their profitable positions during market upturns. The rate of bid-offer movement is significantly slow to match a new global price level only in situations when most investors in the market are in profit. These insights are crucial as they suggest that the risk-return characteristics of asset prices between the bull and bear market may differ, resulting from investors' behavioral biases.","sentences":["Analyzing investors' trading behavior in cryptoasset markets provides new evidence supporting the theory that retail investors likely exhibit behavioral biases.","We investigate the price discovery process between Thailand's most highly liquid exchange and the global exchange.","Under the no-arbitrage assumption, bid-offer quotes in the local exchange should quickly move to match the new price levels in the global exchange, as the price process of cryptoassets in the local exchange does not contain new information that can lead the price dynamic in the global exchange.","We analyze intraday bid-offer quotes and investors' portfolio positions and find that investors exhibit the disposition effect by attempting to sell their profitable positions during market upturns.","The rate of bid-offer movement is significantly slow to match a new global price level only in situations when most investors in the market are in profit.","These insights are crucial as they suggest that the risk-return characteristics of asset prices between the bull and bear market may differ, resulting from investors' behavioral biases."],"url":"http://arxiv.org/abs/2406.02878v1","category":"econ.GN"}
{"created":"2024-06-05 02:49:31","title":"Giant enhancement of hole mobility for 4H-silicon carbide through suppressing interband electron-phonon scattering","abstract":"4H-Silicon Carbide (4H-SiC) possesses a high Baliga figure of merit, making it a promising material for power electronics. However, its applications are limited by its low hole mobility. Herein, we found that the hole mobility of 4H-SiC is mainly limited by the strong interband electron-phonon scattering using mode-level first-principles calculations. Our research indicates that applying compressive strain can reverse the sign of crystal-field splitting and change the ordering of electron bands close to the valence band maximum. Therefore, the interband electron-phonon scattering is severely suppressed, and the out-of-plane hole mobility of 4H-SiC can be enhanced by 200% with 2% uniaxial compressive strain applied. This work provides new insights into the electron transport mechanisms in semiconductors and suggests a strategy to improve hole mobility that could be applied to other semiconductors with hexagonal crystalline geometries.","sentences":["4H-Silicon Carbide (4H-SiC) possesses a high Baliga figure of merit, making it a promising material for power electronics.","However, its applications are limited by its low hole mobility.","Herein, we found that the hole mobility of 4H-SiC is mainly limited by the strong interband electron-phonon scattering using mode-level first-principles calculations.","Our research indicates that applying compressive strain can reverse the sign of crystal-field splitting and change the ordering of electron bands close to the valence band maximum.","Therefore, the interband electron-phonon scattering is severely suppressed, and the out-of-plane hole mobility of 4H-SiC can be enhanced by 200% with 2% uniaxial compressive strain applied.","This work provides new insights into the electron transport mechanisms in semiconductors and suggests a strategy to improve hole mobility that could be applied to other semiconductors with hexagonal crystalline geometries."],"url":"http://arxiv.org/abs/2406.02874v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-05 02:30:45","title":"Bayesian Adaptive Trials for Social Policy","abstract":"This paper proposes Bayesian Adaptive Trials (BAT) as both an efficient method to conduct trials and a unifying framework for evaluation social policy interventions, addressing limitations inherent in traditional methods such as Randomized Controlled Trials (RCT). Recognizing the crucial need for evidence-based approaches in public policy, the proposal aims to lower barriers to the adoption of evidence-based methods and align evaluation processes more closely with the dynamic nature of policy cycles. BATs, grounded in decision theory, offer a dynamic, ``learning as we go'' approach, enabling the integration of diverse information types and facilitating a continuous, iterative process of policy evaluation. BATs' adaptive nature is particularly advantageous in policy settings, allowing for more timely and context-sensitive decisions. Moreover, BATs' ability to value potential future information sources positions it as an optimal strategy for sequential data acquisition during policy implementation. While acknowledging the assumptions and models intrinsic to BATs, such as prior distributions and likelihood functions, the paper argues that these are advantageous for decision-makers in social policy, effectively merging the best features of various methodologies.","sentences":["This paper proposes Bayesian Adaptive Trials (BAT) as both an efficient method to conduct trials and a unifying framework for evaluation social policy interventions, addressing limitations inherent in traditional methods such as Randomized Controlled Trials (RCT).","Recognizing the crucial need for evidence-based approaches in public policy, the proposal aims to lower barriers to the adoption of evidence-based methods and align evaluation processes more closely with the dynamic nature of policy cycles.","BATs, grounded in decision theory, offer a dynamic, ``learning as we go'' approach, enabling the integration of diverse information types and facilitating a continuous, iterative process of policy evaluation.","BATs' adaptive nature is particularly advantageous in policy settings, allowing for more timely and context-sensitive decisions.","Moreover, BATs' ability to value potential future information sources positions it as an optimal strategy for sequential data acquisition during policy implementation.","While acknowledging the assumptions and models intrinsic to BATs, such as prior distributions and likelihood functions, the paper argues that these are advantageous for decision-makers in social policy, effectively merging the best features of various methodologies."],"url":"http://arxiv.org/abs/2406.02868v1","category":"stat.AP"}
{"created":"2024-06-05 02:22:54","title":"Rethinking Guidance Information to Utilize Unlabeled Samples:A Label Encoding Perspective","abstract":"Empirical Risk Minimization (ERM) is fragile in scenarios with insufficient labeled samples. A vanilla extension of ERM to unlabeled samples is Entropy Minimization (EntMin), which employs the soft-labels of unlabeled samples to guide their learning. However, EntMin emphasizes prediction discriminability while neglecting prediction diversity. To alleviate this issue, in this paper, we rethink the guidance information to utilize unlabeled samples. By analyzing the learning objective of ERM, we find that the guidance information for labeled samples in a specific category is the corresponding label encoding. Inspired by this finding, we propose a Label-Encoding Risk Minimization (LERM). It first estimates the label encodings through prediction means of unlabeled samples and then aligns them with their corresponding ground-truth label encodings. As a result, the LERM ensures both prediction discriminability and diversity, and it can be integrated into existing methods as a plugin. Theoretically, we analyze the relationships between LERM and ERM as well as EntMin. Empirically, we verify the superiority of the LERM under several label insufficient scenarios. The codes are available at https://github.com/zhangyl660/LERM.","sentences":["Empirical Risk Minimization (ERM) is fragile in scenarios with insufficient labeled samples.","A vanilla extension of ERM to unlabeled samples is Entropy Minimization (EntMin), which employs the soft-labels of unlabeled samples to guide their learning.","However, EntMin emphasizes prediction discriminability while neglecting prediction diversity.","To alleviate this issue, in this paper, we rethink the guidance information to utilize unlabeled samples.","By analyzing the learning objective of ERM, we find that the guidance information for labeled samples in a specific category is the corresponding label encoding.","Inspired by this finding, we propose a Label-Encoding Risk Minimization (LERM).","It first estimates the label encodings through prediction means of unlabeled samples and then aligns them with their corresponding ground-truth label encodings.","As a result, the LERM ensures both prediction discriminability and diversity, and it can be integrated into existing methods as a plugin.","Theoretically, we analyze the relationships between LERM and ERM as well as EntMin.","Empirically, we verify the superiority of the LERM under several label insufficient scenarios.","The codes are available at https://github.com/zhangyl660/LERM."],"url":"http://arxiv.org/abs/2406.02862v1","category":"cs.CV"}
{"created":"2024-06-05 02:21:17","title":"Towards Interactive Autonomous Vehicle Testing: Vehicle-Under-Test-Centered Traffic Simulation","abstract":"The simulation-based testing is essential for safely implementing autonomous vehicles (AVs) on roads, necessitating simulated traffic environments that dynamically interact with the Vehicle Under Test (VUT). This study introduces a VUT-Centered environmental Dynamics Inference (VCDI) model for realistic, interactive, and diverse background traffic simulation. VCDI is built on a Transformer-based trajectory inference model to generate trajectories for background objects. Serving the purpose of AV testing, VCDI additionally considers VUT-centered interactivity and scenario diversity using a conditional inference framework. First, the VUT future motion is taken as an augmented model input to bridge the interaction between VUT and background objects. Second, to enrich the scenario diversity, a Bayesian-network-based cost function module is designed. The module, learned in a distributional form, captures the uncertainty of the VUT's strategy, triggering various scenario evolution. Experimental results validate VCDI's trajectory-level simulation precision which outperforms the state-of-the-art trajectory prediction work. The flexibility of the distributional cost function allows VCDI to provide diverse-yet-realistic scenarios for AV testing. We demonstrate such capability by modifying the anticipation to VUT's cost-based strategy and thus achieve multiple testing scenarios with explainable background traffic evolution.","sentences":["The simulation-based testing is essential for safely implementing autonomous vehicles (AVs) on roads, necessitating simulated traffic environments that dynamically interact with the Vehicle Under Test (VUT).","This study introduces a VUT-Centered environmental Dynamics Inference (VCDI) model for realistic, interactive, and diverse background traffic simulation.","VCDI is built on a Transformer-based trajectory inference model to generate trajectories for background objects.","Serving the purpose of AV testing, VCDI additionally considers VUT-centered interactivity and scenario diversity using a conditional inference framework.","First, the VUT future motion is taken as an augmented model input to bridge the interaction between VUT and background objects.","Second, to enrich the scenario diversity, a Bayesian-network-based cost function module is designed.","The module, learned in a distributional form, captures the uncertainty of the VUT's strategy, triggering various scenario evolution.","Experimental results validate VCDI's trajectory-level simulation precision which outperforms the state-of-the-art trajectory prediction work.","The flexibility of the distributional cost function allows VCDI to provide diverse-yet-realistic scenarios for AV testing.","We demonstrate such capability by modifying the anticipation to VUT's cost-based strategy and thus achieve multiple testing scenarios with explainable background traffic evolution."],"url":"http://arxiv.org/abs/2406.02860v1","category":"cs.RO"}
{"created":"2024-06-05 02:07:34","title":"Fluorine Abundances in Local Stellar Populations","abstract":"We present the first fluorine measurements in 12 normal giants belonging to the Galactic thin and thick disks using spectra obtained with the Phoenix infrared spectrometer on the 2.1m telescope at Kitt Peak. Abundances are determined from the (1-0) R9 2.3358 micron feature of the molecule HF. Additionally, sodium abundances are derived in 25 giants in the thin disk, thick disk, and halo using the Na I line at 2.3379 microns. We report fluorine abundances for thin and thick disk stars in the metallicity range -0.7 < [Fe/H] < 0. We add two abundance measurements for stars with [Fe/H] < 0.5 dex which are at a critical metallicity range to constrain models. We find a larger dispersion in fluorine abundances than sodium abundances despite both species having similar overall uncertainties due to atmospheric parameters, suggesting this dispersion is real and not observational. The dispersion is slightly larger in the thick disk than the thin. The thin and thick disk average [F/Fe] for our sample of stars combined with the literature differ by 0.03 dex. The observations are compared to available chemical evolution models.","sentences":["We present the first fluorine measurements in 12 normal giants belonging to the Galactic thin and thick disks using spectra obtained with the Phoenix infrared spectrometer on the 2.1m telescope at Kitt Peak.","Abundances are determined from the (1-0) R9 2.3358 micron feature of the molecule HF.","Additionally, sodium abundances are derived in 25 giants in the thin disk, thick disk, and halo using the Na","I line at 2.3379 microns.","We report fluorine abundances for thin and thick disk stars in the metallicity range -0.7 <","[Fe/H] < 0.","We add two abundance measurements for stars with [Fe/H] < 0.5 dex which are at a critical metallicity range to constrain models.","We find a larger dispersion in fluorine abundances than sodium abundances despite both species having similar overall uncertainties due to atmospheric parameters, suggesting this dispersion is real and not observational.","The dispersion is slightly larger in the thick disk than the thin.","The thin and thick disk average [F/Fe] for our sample of stars combined with the literature differ by 0.03 dex.","The observations are compared to available chemical evolution models."],"url":"http://arxiv.org/abs/2406.02855v1","category":"astro-ph.GA"}
{"created":"2024-06-05 01:27:31","title":"General-Relativistic Gauge-Invariant Magnetic Helicity Transport: Basic Formulation and Application to Neutron Star Mergers","abstract":"Dynamo processes are ubiquitous in astrophysical systems. In relativistic astrophysical systems, such as accretion disks around black holes or neutron stars, they may critically affect the launching of winds and jets that can power electromagnetic emission. Dynamo processes are governed by several microscopic parameters, one of them being magnetic helicity. As a conserved quantity in nonresistive plasmas, magnetic helicity is transported across the system. One important implication of helicity conservation is, that in the absence of helicity fluxes some mean-field dynamos can be quenched, potentially affecting the large-scale evolution of the magnetic field. One of the major challenges in computing magnetic helicity is the need to fix a meaningful electromagnetic gauge. We here present a fully covariant formulation of magnetic helicity transport in general-relativistic plasmas based on the concept of relative helicity by Berger & Field and Finn & Antonsen. This formulation is separately invariant under gauge-transformation of the Maxwell and Einstein equations. As an application of this new formalism we present the first analysis of magnetic helicity transport in the merger of two neutron stars. We demonstrate the presence of global helicity fluxes into the outer layers of the stellar merger remnant, which may impact subsequent large-scale dynamo amplification in these regions.","sentences":["Dynamo processes are ubiquitous in astrophysical systems.","In relativistic astrophysical systems, such as accretion disks around black holes or neutron stars, they may critically affect the launching of winds and jets that can power electromagnetic emission.","Dynamo processes are governed by several microscopic parameters, one of them being magnetic helicity.","As a conserved quantity in nonresistive plasmas, magnetic helicity is transported across the system.","One important implication of helicity conservation is, that in the absence of helicity fluxes some mean-field dynamos can be quenched, potentially affecting the large-scale evolution of the magnetic field.","One of the major challenges in computing magnetic helicity is the need to fix a meaningful electromagnetic gauge.","We here present a fully covariant formulation of magnetic helicity transport in general-relativistic plasmas based on the concept of relative helicity by Berger & Field and Finn & Antonsen.","This formulation is separately invariant under gauge-transformation of the Maxwell and Einstein equations.","As an application of this new formalism we present the first analysis of magnetic helicity transport in the merger of two neutron stars.","We demonstrate the presence of global helicity fluxes into the outer layers of the stellar merger remnant, which may impact subsequent large-scale dynamo amplification in these regions."],"url":"http://arxiv.org/abs/2406.02837v1","category":"astro-ph.HE"}
{"created":"2024-06-05 01:19:44","title":"DREW : Towards Robust Data Provenance by Leveraging Error-Controlled Watermarking","abstract":"Identifying the origin of data is crucial for data provenance, with applications including data ownership protection, media forensics, and detecting AI-generated content. A standard approach involves embedding-based retrieval techniques that match query data with entries in a reference dataset. However, this method is not robust against benign and malicious edits. To address this, we propose Data Retrieval with Error-corrected codes and Watermarking (DREW). DREW randomly clusters the reference dataset, injects unique error-controlled watermark keys into each cluster, and uses these keys at query time to identify the appropriate cluster for a given sample. After locating the relevant cluster, embedding vector similarity retrieval is performed within the cluster to find the most accurate matches. The integration of error control codes (ECC) ensures reliable cluster assignments, enabling the method to perform retrieval on the entire dataset in case the ECC algorithm cannot detect the correct cluster with high confidence. This makes DREW maintain baseline performance, while also providing opportunities for performance improvements due to the increased likelihood of correctly matching queries to their origin when performing retrieval on a smaller subset of the dataset. Depending on the watermark technique used, DREW can provide substantial improvements in retrieval accuracy (up to 40\\% for some datasets and modification types) across multiple datasets and state-of-the-art embedding models (e.g., DinoV2, CLIP), making our method a promising solution for secure and reliable source identification. The code is available at https://github.com/mehrdadsaberi/DREW","sentences":["Identifying the origin of data is crucial for data provenance, with applications including data ownership protection, media forensics, and detecting AI-generated content.","A standard approach involves embedding-based retrieval techniques that match query data with entries in a reference dataset.","However, this method is not robust against benign and malicious edits.","To address this, we propose Data Retrieval with Error-corrected codes and Watermarking (DREW).","DREW randomly clusters the reference dataset, injects unique error-controlled watermark keys into each cluster, and uses these keys at query time to identify the appropriate cluster for a given sample.","After locating the relevant cluster, embedding vector similarity retrieval is performed within the cluster to find the most accurate matches.","The integration of error control codes (ECC) ensures reliable cluster assignments, enabling the method to perform retrieval on the entire dataset in case the ECC algorithm cannot detect the correct cluster with high confidence.","This makes DREW maintain baseline performance, while also providing opportunities for performance improvements due to the increased likelihood of correctly matching queries to their origin when performing retrieval on a smaller subset of the dataset.","Depending on the watermark technique used, DREW can provide substantial improvements in retrieval accuracy (up to 40\\% for some datasets and modification types) across multiple datasets and state-of-the-art embedding models (e.g., DinoV2, CLIP), making our method a promising solution for secure and reliable source identification.","The code is available at https://github.com/mehrdadsaberi/DREW"],"url":"http://arxiv.org/abs/2406.02836v1","category":"cs.CR"}
{"created":"2024-06-05 01:14:45","title":"Asymptotic inference with flexible covariate adjustment under rerandomization and stratified rerandomization","abstract":"Rerandomization is an effective treatment allocation procedure to control for baseline covariate imbalance. For estimating the average treatment effect, rerandomization has been previously shown to improve the precision of the unadjusted and the linearly-adjusted estimators over simple randomization without compromising consistency. However, it remains unclear whether such results apply more generally to the class of M-estimators, including the g-computation formula with generalized linear regression and doubly-robust methods, and more broadly, to efficient estimators with data-adaptive machine learners. In this paper, using a super-population framework, we develop the asymptotic theory for a more general class of covariate-adjusted estimators under rerandomization and its stratified extension. We prove that the asymptotic linearity and the influence function remain identical for any M-estimator under simple randomization and rerandomization, but rerandomization may lead to a non-Gaussian asymptotic distribution. We further explain, drawing examples from several common M-estimators, that asymptotic normality can be achieved if rerandomization variables are appropriately adjusted for in the final estimator. These results are extended to stratified rerandomization. Finally, we study the asymptotic theory for efficient estimators based on data-adaptive machine learners, and prove their efficiency optimality under rerandomization and stratified rerandomization. Our results are demonstrated via simulations and re-analyses of a cluster-randomized experiment that used stratified rerandomization.","sentences":["Rerandomization is an effective treatment allocation procedure to control for baseline covariate imbalance.","For estimating the average treatment effect, rerandomization has been previously shown to improve the precision of the unadjusted and the linearly-adjusted estimators over simple randomization without compromising consistency.","However, it remains unclear whether such results apply more generally to the class of M-estimators, including the g-computation formula with generalized linear regression and doubly-robust methods, and more broadly, to efficient estimators with data-adaptive machine learners.","In this paper, using a super-population framework, we develop the asymptotic theory for a more general class of covariate-adjusted estimators under rerandomization and its stratified extension.","We prove that the asymptotic linearity and the influence function remain identical for any M-estimator under simple randomization and rerandomization, but rerandomization may lead to a non-Gaussian asymptotic distribution.","We further explain, drawing examples from several common M-estimators, that asymptotic normality can be achieved if rerandomization variables are appropriately adjusted for in the final estimator.","These results are extended to stratified rerandomization.","Finally, we study the asymptotic theory for efficient estimators based on data-adaptive machine learners, and prove their efficiency optimality under rerandomization and stratified rerandomization.","Our results are demonstrated via simulations and re-analyses of a cluster-randomized experiment that used stratified rerandomization."],"url":"http://arxiv.org/abs/2406.02834v1","category":"stat.ME"}
{"created":"2024-06-05 00:27:50","title":"Approximation properties of torsion classes","abstract":"We clarify some results of Bagaria and Magidor \\cite{MR3152715} about the relationship between large cardinals and torsion classes of abelian groups, and prove that (1) the Maximum Deconstructibility principle introduced in \\cite{Cox_MaxDecon} requires large cardinals; it sits, implication-wise, between Vop\\v{e}nka's Principle and the existence of an $\\omega_1$-strongly compact cardinal. (2) While deconstructibility of a class of modules always implies the precovering property by \\cite{MR2822215}, the concepts are (consistently) non-equivalent, even for classes of abelian groups closed under extensions, homomorphic images, and colimits.","sentences":["We clarify some results of Bagaria and Magidor \\cite{MR3152715} about the relationship between large cardinals and torsion classes of abelian groups, and prove that (1) the Maximum Deconstructibility principle introduced in \\cite{Cox_MaxDecon} requires large cardinals; it sits, implication-wise, between Vop\\v{e}nka's Principle and the existence of an $\\omega_1$-strongly compact cardinal.","(2) While deconstructibility of a class of modules always implies the precovering property by \\cite{MR2822215}, the concepts are (consistently) non-equivalent, even for classes of abelian groups closed under extensions, homomorphic images, and colimits."],"url":"http://arxiv.org/abs/2406.02829v1","category":"math.LO"}
{"created":"2024-06-04 23:38:24","title":"Versatile polymer method to dry-flip two-dimensional moir\u00e9 hetero structures for nanoscale surface characterization","abstract":"The recent discovery of magic angle twisted bilayer graphene (MATBG), in which two sheets of monolayer graphene are precisely stacked to a specific angle, has opened up a plethora of new opportunities in the field of topology, superconductivity, and other strongly correlated effects. Most conventional ways of preparing twisted bilayer devices require the use of high process temperatures and solvents and are not well-suited for preparing samples which need to be flipped to be compatible with characterization techniques like STM, ARPES, PFM, SThM etc. Here, we demonstrate a very simple polymer-based method using Polyvinyl Chloride (PVC), which can be used for making flipped twisted bilayer graphene devices. This allowed us to produce flipped twisted samples without the need of any solvents and with high quality as confirmed by Piezoresponse Force Microscopy. We believe that this dry flip technique can be readily extended to twist 2D materials beyond graphene, especially air-sensitive materials which require operation under inert atmosphere, where often solvents cannot be used.","sentences":["The recent discovery of magic angle twisted bilayer graphene (MATBG), in which two sheets of monolayer graphene are precisely stacked to a specific angle, has opened up a plethora of new opportunities in the field of topology, superconductivity, and other strongly correlated effects.","Most conventional ways of preparing twisted bilayer devices require the use of high process temperatures and solvents and are not well-suited for preparing samples which need to be flipped to be compatible with characterization techniques like STM, ARPES, PFM, SThM etc.","Here, we demonstrate a very simple polymer-based method using Polyvinyl Chloride (PVC), which can be used for making flipped twisted bilayer graphene devices.","This allowed us to produce flipped twisted samples without the need of any solvents and with high quality as confirmed by Piezoresponse Force Microscopy.","We believe that this dry flip technique can be readily extended to twist 2D materials beyond graphene, especially air-sensitive materials which require operation under inert atmosphere, where often solvents cannot be used."],"url":"http://arxiv.org/abs/2406.02819v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-04 22:22:41","title":"Collision-Affording Point Trees: SIMD-Amenable Nearest Neighbors for Fast Collision Checking","abstract":"Motion planning against sensor data is often a critical bottleneck in real-time robot control. For sampling-based motion planners, which are effective for high-dimensional systems such as manipulators, the most time-intensive component is collision checking. We present a novel spatial data structure, the collision-affording point tree (CAPT): an exact representation of point clouds that accelerates collision-checking queries between robots and point clouds by an order of magnitude, with an average query time of less than 10 nanoseconds on 3D scenes comprising thousands of points. With the CAPT, sampling-based planners can generate valid, high-quality paths in under a millisecond, with total end-to-end computation time faster than 60 FPS, on a single thread of a consumer-grade CPU. We also present a point cloud filtering algorithm, based on space-filling curves, which reduces the number of points in a point cloud while preserving structure. Our approach enables robots to plan at real-time speeds in sensed environments, opening up potential uses of planning for high-dimensional systems in dynamic, changing, and unmodeled environments.","sentences":["Motion planning against sensor data is often a critical bottleneck in real-time robot control.","For sampling-based motion planners, which are effective for high-dimensional systems such as manipulators, the most time-intensive component is collision checking.","We present a novel spatial data structure, the collision-affording point tree (CAPT): an exact representation of point clouds that accelerates collision-checking queries between robots and point clouds by an order of magnitude, with an average query time of less than 10 nanoseconds on 3D scenes comprising thousands of points.","With the CAPT, sampling-based planners can generate valid, high-quality paths in under a millisecond, with total end-to-end computation time faster than 60 FPS, on a single thread of a consumer-grade CPU.","We also present a point cloud filtering algorithm, based on space-filling curves, which reduces the number of points in a point cloud while preserving structure.","Our approach enables robots to plan at real-time speeds in sensed environments, opening up potential uses of planning for high-dimensional systems in dynamic, changing, and unmodeled environments."],"url":"http://arxiv.org/abs/2406.02807v1","category":"cs.RO"}
{"created":"2024-06-04 21:48:30","title":"Auditing Privacy Mechanisms via Label Inference Attacks","abstract":"We propose reconstruction advantage measures to audit label privatization mechanisms. A reconstruction advantage measure quantifies the increase in an attacker's ability to infer the true label of an unlabeled example when provided with a private version of the labels in a dataset (e.g., aggregate of labels from different users or noisy labels output by randomized response), compared to an attacker that only observes the feature vectors, but may have prior knowledge of the correlation between features and labels. We consider two such auditing measures: one additive, and one multiplicative. These incorporate previous approaches taken in the literature on empirical auditing and differential privacy. The measures allow us to place a variety of proposed privatization schemes -- some differentially private, some not -- on the same footing. We analyze these measures theoretically under a distributional model which encapsulates reasonable adversarial settings. We also quantify their behavior empirically on real and simulated prediction tasks. Across a range of experimental settings, we find that differentially private schemes dominate or match the privacy-utility tradeoff of more heuristic approaches.","sentences":["We propose reconstruction advantage measures to audit label privatization mechanisms.","A reconstruction advantage measure quantifies the increase in an attacker's ability to infer the true label of an unlabeled example when provided with a private version of the labels in a dataset (e.g., aggregate of labels from different users or noisy labels output by randomized response), compared to an attacker that only observes the feature vectors, but may have prior knowledge of the correlation between features and labels.","We consider two such auditing measures: one additive, and one multiplicative.","These incorporate previous approaches taken in the literature on empirical auditing and differential privacy.","The measures allow us to place a variety of proposed privatization schemes -- some differentially private, some not -- on the same footing.","We analyze these measures theoretically under a distributional model which encapsulates reasonable adversarial settings.","We also quantify their behavior empirically on real and simulated prediction tasks.","Across a range of experimental settings, we find that differentially private schemes dominate or match the privacy-utility tradeoff of more heuristic approaches."],"url":"http://arxiv.org/abs/2406.02797v1","category":"cs.LG"}
{"created":"2024-06-04 21:43:56","title":"ArguMentor: Augmenting User Experiences with Counter-Perspectives","abstract":"Opinion pieces often represent only one side of any story, which can influence users and make them susceptible to confirmation bias and echo chambers in society. Moreover, humans are also bad at reading long articles -- often indulging in idle reading and re-reading. To solve this, we design ArguMentor, an end-to-end system that highlights claims in opinion pieces, generates counter-arguments for them using an LLM, and generates a context-based summary of the passage based on current events. It further enhances user interaction and understanding through additional features like Q&A bot, DebateMe and highlighting trigger windows. Our survey and results show that users can generate more counterarguments and on an average have more neutralized views after engaging with the system.","sentences":["Opinion pieces often represent only one side of any story, which can influence users and make them susceptible to confirmation bias and echo chambers in society.","Moreover, humans are also bad at reading long articles -- often indulging in idle reading and re-reading.","To solve this, we design ArguMentor, an end-to-end system that highlights claims in opinion pieces, generates counter-arguments for them using an LLM, and generates a context-based summary of the passage based on current events.","It further enhances user interaction and understanding through additional features like Q&A bot, DebateMe and highlighting trigger windows.","Our survey and results show that users can generate more counterarguments and on an average have more neutralized views after engaging with the system."],"url":"http://arxiv.org/abs/2406.02795v1","category":"cs.HC"}
{"created":"2024-06-04 21:43:49","title":"PriME: Privacy-aware Membership profile Estimation in networks","abstract":"This paper presents a novel approach to estimating community membership probabilities for network vertices generated by the Degree Corrected Mixed Membership Stochastic Block Model while preserving individual edge privacy. Operating within the $\\varepsilon$-edge local differential privacy framework, we introduce an optimal private algorithm based on a symmetric edge flip mechanism and spectral clustering for accurate estimation of vertex community memberships. We conduct a comprehensive analysis of the estimation risk and establish the optimality of our procedure by providing matching lower bounds to the minimax risk under privacy constraints. To validate our approach, we demonstrate its performance through numerical simulations and its practical application to real-world data. This work represents a significant step forward in balancing accurate community membership estimation with stringent privacy preservation in network data analysis.","sentences":["This paper presents a novel approach to estimating community membership probabilities for network vertices generated by the Degree Corrected Mixed Membership Stochastic Block Model while preserving individual edge privacy.","Operating within the $\\varepsilon$-edge local differential privacy framework, we introduce an optimal private algorithm based on a symmetric edge flip mechanism and spectral clustering for accurate estimation of vertex community memberships.","We conduct a comprehensive analysis of the estimation risk and establish the optimality of our procedure by providing matching lower bounds to the minimax risk under privacy constraints.","To validate our approach, we demonstrate its performance through numerical simulations and its practical application to real-world data.","This work represents a significant step forward in balancing accurate community membership estimation with stringent privacy preservation in network data analysis."],"url":"http://arxiv.org/abs/2406.02794v1","category":"stat.ME"}
{"created":"2024-06-04 21:27:43","title":"Building Socially-Equitable Public Models","abstract":"Public models offer predictions to a variety of downstream tasks and have played a crucial role in various AI applications, showcasing their proficiency in accurate predictions. However, the exclusive emphasis on prediction accuracy may not align with the diverse end objectives of downstream agents. Recognizing the public model's predictions as a service, we advocate for integrating the objectives of downstream agents into the optimization process. Concretely, to address performance disparities and foster fairness among heterogeneous agents in training, we propose a novel Equitable Objective. This objective, coupled with a policy gradient algorithm, is crafted to train the public model to produce a more equitable/uniform performance distribution across downstream agents, each with their unique concerns. Both theoretical analysis and empirical case studies have proven the effectiveness of our method in advancing performance equity across diverse downstream agents utilizing the public model for their decision-making. Codes and datasets are released at https://github.com/Ren-Research/Socially-Equitable-Public-Models.","sentences":["Public models offer predictions to a variety of downstream tasks and have played a crucial role in various AI applications, showcasing their proficiency in accurate predictions.","However, the exclusive emphasis on prediction accuracy may not align with the diverse end objectives of downstream agents.","Recognizing the public model's predictions as a service, we advocate for integrating the objectives of downstream agents into the optimization process.","Concretely, to address performance disparities and foster fairness among heterogeneous agents in training, we propose a novel Equitable Objective.","This objective, coupled with a policy gradient algorithm, is crafted to train the public model to produce a more equitable/uniform performance distribution across downstream agents, each with their unique concerns.","Both theoretical analysis and empirical case studies have proven the effectiveness of our method in advancing performance equity across diverse downstream agents utilizing the public model for their decision-making.","Codes and datasets are released at https://github.com/Ren-Research/Socially-Equitable-Public-Models."],"url":"http://arxiv.org/abs/2406.02790v1","category":"cs.LG"}
{"created":"2024-06-04 21:26:29","title":"Private Stochastic Convex Optimization with Heavy Tails: Near-Optimality from Simple Reductions","abstract":"We study the problem of differentially private stochastic convex optimization (DP-SCO) with heavy-tailed gradients, where we assume a $k^{\\text{th}}$-moment bound on the Lipschitz constants of sample functions rather than a uniform bound. We propose a new reduction-based approach that enables us to obtain the first optimal rates (up to logarithmic factors) in the heavy-tailed setting, achieving error $G_2 \\cdot \\frac 1 {\\sqrt n} + G_k \\cdot (\\frac{\\sqrt d}{n\\epsilon})^{1 - \\frac 1 k}$ under $(\\epsilon, \\delta)$-approximate differential privacy, up to a mild $\\textup{polylog}(\\frac{1}{\\delta})$ factor, where $G_2^2$ and $G_k^k$ are the $2^{\\text{nd}}$ and $k^{\\text{th}}$ moment bounds on sample Lipschitz constants, nearly-matching a lower bound of [Lowy and Razaviyayn 2023].   We further give a suite of private algorithms in the heavy-tailed setting which improve upon our basic result under additional assumptions, including an optimal algorithm under a known-Lipschitz constant assumption, a near-linear time algorithm for smooth functions, and an optimal linear time algorithm for smooth generalized linear models.","sentences":["We study the problem of differentially private stochastic convex optimization (DP-SCO) with heavy-tailed gradients, where we assume a $k^{\\text{th}}$-moment bound on the Lipschitz constants of sample functions rather than a uniform bound.","We propose a new reduction-based approach that enables us to obtain the first optimal rates (up to logarithmic factors) in the heavy-tailed setting, achieving error $G_2 \\cdot \\frac 1 {\\sqrt n} + G_k \\cdot (\\frac{\\sqrt d}{n\\epsilon})^{1 - \\frac 1 k}$ under $(\\epsilon, \\delta)$-approximate differential privacy, up to a mild $\\textup{polylog}(\\frac{1}{\\delta})$ factor, where $G_2^2$ and $G_k^k$ are the $2^{\\text{nd}}$ and $k^{\\text{th}}$ moment bounds on sample Lipschitz constants, nearly-matching a lower bound of [Lowy and Razaviyayn 2023].   ","We further give a suite of private algorithms in the heavy-tailed setting which improve upon our basic result under additional assumptions, including an optimal algorithm under a known-Lipschitz constant assumption, a near-linear time algorithm for smooth functions, and an optimal linear time algorithm for smooth generalized linear models."],"url":"http://arxiv.org/abs/2406.02789v1","category":"cs.DS"}
{"created":"2024-06-04 21:08:07","title":"Event-horizon-scale Imaging of M87* under Different Assumptions via Deep Generative Image Priors","abstract":"Reconstructing images from the Event Horizon Telescope (EHT) observations of M87*, the supermassive black hole at the center of the galaxy M87, depends on a prior to impose desired image statistics. However, given the impossibility of directly observing black holes, there is no clear choice for a prior. We present a framework for flexibly designing a range of priors, each bringing different biases to the image reconstruction. These priors can be weak (e.g., impose only basic natural-image statistics) or strong (e.g., impose assumptions of black-hole structure). Our framework uses Bayesian inference with score-based priors, which are data-driven priors arising from a deep generative model that can learn complicated image distributions. Using our Bayesian imaging approach with sophisticated data-driven priors, we can assess how visual features and uncertainty of reconstructed images change depending on the prior. In addition to simulated data, we image the real EHT M87* data and discuss how recovered features are influenced by the choice of prior.","sentences":["Reconstructing images from the Event Horizon Telescope (EHT) observations of M87*, the supermassive black hole at the center of the galaxy M87, depends on a prior to impose desired image statistics.","However, given the impossibility of directly observing black holes, there is no clear choice for a prior.","We present a framework for flexibly designing a range of priors, each bringing different biases to the image reconstruction.","These priors can be weak (e.g., impose only basic natural-image statistics) or strong (e.g., impose assumptions of black-hole structure).","Our framework uses Bayesian inference with score-based priors, which are data-driven priors arising from a deep generative model that can learn complicated image distributions.","Using our Bayesian imaging approach with sophisticated data-driven priors, we can assess how visual features and uncertainty of reconstructed images change depending on the prior.","In addition to simulated data, we image the real EHT M87* data and discuss how recovered features are influenced by the choice of prior."],"url":"http://arxiv.org/abs/2406.02785v1","category":"astro-ph.IM"}
{"created":"2024-06-04 20:57:59","title":"KPZ scaling from the Krylov space","abstract":"Recently, a superdiffusion exhibiting the Kardar-Parisi-Zhang (KPZ) scaling in late-time correlators and autocorrelators of certain interacting many-body systems has been reported. Inspired by these results, we explore the KPZ scaling in correlation functions using their realization in the Krylov operator basis. We focus on the Heisenberg time scale, which approximately corresponds to the ramp--plateau transition for the Krylov complexity in systems with a large but finite number degrees of freedom. Two frameworks are under consideration: i) the system with growing Lanczos coefficients and an artificial cut-off, and ii) the system with the finite Hilbert space. In both cases via numerical analysis, we observe the transition from Gaussian to KPZ-like scaling at the critical Euclidean time $t_{E}^*=c_{cr}K$, for the Krylov chain of finite length $K$, and $c_{cr}=O(1)$. In particular, we find a scaling $\\sim K^{1/3}$ for fluctuations in the one-point correlation function and a dynamical scaling $\\sim K^{-2/3}$ associated with the return probability (Loschmidt echo) corresponding to autocorrelators in physical space. In the first case, the transition is of the 3rd order and can be considered as an example of dynamical quantum phase transition (DQPT), while in the second, it is a crossover. For case ii), utilizing the relationship between the spectrum of tridiagonal matrices at the spectral edge and the spectrum of the stochastic Airy operator, we demonstrate analytically the origin of the KPZ scaling for the particular Krylov chain using the results of the probability theory. We argue that there is some outcome of our study for the double scaling limit of matrix models. For the case of topological gravity, the white noise $O(\\frac{1}{N})$ term is identified, which should be taken into account in the controversial issue of ensemble averaging in 2D/1D holography.","sentences":["Recently, a superdiffusion exhibiting the Kardar-Parisi-Zhang (KPZ) scaling in late-time correlators and autocorrelators of certain interacting many-body systems has been reported.","Inspired by these results, we explore the KPZ scaling in correlation functions using their realization in the Krylov operator basis.","We focus on the Heisenberg time scale, which approximately corresponds to the ramp--plateau transition for the Krylov complexity in systems with a large but finite number degrees of freedom.","Two frameworks are under consideration: i) the system with growing Lanczos coefficients and an artificial cut-off, and ii) the system with the finite Hilbert space.","In both cases via numerical analysis, we observe the transition from Gaussian to KPZ-like scaling at the critical Euclidean time $t_{E}^*=c_{cr}K$, for the Krylov chain of finite length $K$, and $c_{cr}=O(1)$. In particular, we find a scaling $\\sim K^{1/3}$ for fluctuations in the one-point correlation function and a dynamical scaling $\\sim K^{-2/3}$ associated with the return probability (Loschmidt echo) corresponding to autocorrelators in physical space.","In the first case, the transition is of the 3rd order and can be considered as an example of dynamical quantum phase transition (DQPT), while in the second, it is a crossover.","For case ii), utilizing the relationship between the spectrum of tridiagonal matrices at the spectral edge and the spectrum of the stochastic Airy operator, we demonstrate analytically the origin of the KPZ scaling for the particular Krylov chain using the results of the probability theory.","We argue that there is some outcome of our study for the double scaling limit of matrix models.","For the case of topological gravity, the white noise $O(\\frac{1}{N})$ term is identified, which should be taken into account in the controversial issue of ensemble averaging in 2D/1D holography."],"url":"http://arxiv.org/abs/2406.02782v1","category":"hep-th"}
{"created":"2024-06-04 20:36:21","title":"Lightweight CNN-BiLSTM based Intrusion Detection Systems for Resource-Constrained IoT Devices","abstract":"Intrusion Detection Systems (IDSs) have played a significant role in detecting and preventing cyber-attacks within traditional computing systems. It is not surprising that the same technology is being applied to secure Internet of Things (IoT) networks from cyber threats. The limited computational resources available on IoT devices make it challenging to deploy conventional computing-based IDSs. The IDSs designed for IoT environments must also demonstrate high classification performance, utilize low-complexity models, and be of a small size. Despite significant progress in IoT-based intrusion detection, developing models that both achieve high classification performance and maintain reduced complexity remains challenging. In this study, we propose a hybrid CNN architecture composed of a lightweight CNN and bidirectional LSTM (BiLSTM) to enhance the performance of IDS on the UNSW-NB15 dataset. The proposed model is specifically designed to run onboard resource-constrained IoT devices and meet their computation capability requirements. Despite the complexity of designing a model that fits the requirements of IoT devices and achieves higher accuracy, our proposed model outperforms the existing research efforts in the literature by achieving an accuracy of 97.28\\% for binary classification and 96.91\\% for multiclassification.","sentences":["Intrusion Detection Systems (IDSs) have played a significant role in detecting and preventing cyber-attacks within traditional computing systems.","It is not surprising that the same technology is being applied to secure Internet of Things (IoT) networks from cyber threats.","The limited computational resources available on IoT devices make it challenging to deploy conventional computing-based IDSs.","The IDSs designed for IoT environments must also demonstrate high classification performance, utilize low-complexity models, and be of a small size.","Despite significant progress in IoT-based intrusion detection, developing models that both achieve high classification performance and maintain reduced complexity remains challenging.","In this study, we propose a hybrid CNN architecture composed of a lightweight CNN and bidirectional LSTM (BiLSTM) to enhance the performance of IDS on the UNSW-NB15 dataset.","The proposed model is specifically designed to run onboard resource-constrained IoT devices and meet their computation capability requirements.","Despite the complexity of designing a model that fits the requirements of IoT devices and achieves higher accuracy, our proposed model outperforms the existing research efforts in the literature by achieving an accuracy of 97.28\\% for binary classification and 96.91\\% for multiclassification."],"url":"http://arxiv.org/abs/2406.02768v1","category":"cs.CR"}
{"created":"2024-06-04 20:24:58","title":"Non-linear resolvents of holomorphically accretive mappings","abstract":"In this paper, we present new results on holomorphically accretive mappings and their resolvents defined on the open unit ball of a complex Banach space.   We employ a unified approach to examine various properties of non-linear resolvents by applying a distortion theorem we have established. This method enables us to prove a covering result and to establish the accretivity of resolvents along with estimates of the squeezing ratio. Furthermore, we prove that under certain mild conditions, a non-linear resolvent is a starlike mapping of a specified order.   As a key tool, we first introduce a refined version of the inverse function theorem for mappings satisfying so-called one-sided estimates.","sentences":["In this paper, we present new results on holomorphically accretive mappings and their resolvents defined on the open unit ball of a complex Banach space.   ","We employ a unified approach to examine various properties of non-linear resolvents by applying a distortion theorem we have established.","This method enables us to prove a covering result and to establish the accretivity of resolvents along with estimates of the squeezing ratio.","Furthermore, we prove that under certain mild conditions, a non-linear resolvent is a starlike mapping of a specified order.   ","As a key tool, we first introduce a refined version of the inverse function theorem for mappings satisfying so-called one-sided estimates."],"url":"http://arxiv.org/abs/2406.02758v1","category":"math.CV"}
{"created":"2024-06-04 20:16:21","title":"Towards improved property prediction of two-dimensional (2D) materials using many-body Quantum Monte Carlo methods","abstract":"The field of two-dimensional (2D) materials has grown dramatically in the last two decades. 2D materials can be utilized for a variety of next-generation optoelectronic, spintronic, clean energy, and quantum computation applications. These 2D structures, which are often exfoliated from layered van der Waals (vdW) materials, possess highly inhomogeneous electron densities and can possess short- and long-range electron correlations. The complexities of 2D materials make them challenging to study with standard mean-field electronic structure methods such as density functional theory (DFT), which relies on approximations for the unknown exchange-correlation functional. In order to overcome the limitations of DFT, highly accurate many-body electronic structure approaches such as Diffusion Monte Carlo (DMC) can be utilized. In the past decade, DMC has been used to calculate accurate magnetic, electronic, excitonic, and topological properties in addition to accurately capturing interlayer interactions and cohesion and adsorption energetics of 2D materials. This approach has been applied to 2D systems of wide interest including graphene, phosphorene, MoS$_2$, CrI$_3$, VSe$_2$, GaSe, GeSe, borophene, and several others. In this review article, we highlight some successful recent applications of DMC to 2D systems for improved property predictions beyond standard DFT.","sentences":["The field of two-dimensional (2D) materials has grown dramatically in the last two decades.","2D materials can be utilized for a variety of next-generation optoelectronic, spintronic, clean energy, and quantum computation applications.","These 2D structures, which are often exfoliated from layered van der Waals (vdW) materials, possess highly inhomogeneous electron densities and can possess short- and long-range electron correlations.","The complexities of 2D materials make them challenging to study with standard mean-field electronic structure methods such as density functional theory (DFT), which relies on approximations for the unknown exchange-correlation functional.","In order to overcome the limitations of DFT, highly accurate many-body electronic structure approaches such as Diffusion Monte Carlo (DMC) can be utilized.","In the past decade, DMC has been used to calculate accurate magnetic, electronic, excitonic, and topological properties in addition to accurately capturing interlayer interactions and cohesion and adsorption energetics of 2D materials.","This approach has been applied to 2D systems of wide interest including graphene, phosphorene, MoS$_2$, CrI$_3$, VSe$_2$, GaSe, GeSe, borophene, and several others.","In this review article, we highlight some successful recent applications of DMC to 2D systems for improved property predictions beyond standard DFT."],"url":"http://arxiv.org/abs/2406.02753v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-04 20:14:30","title":"Multidimensional analogs of the Fekete--Szeg\u00f6 functional","abstract":"In this paper we introduce the Fekete--Szeg\\\"{o} type mapping in the open unit ball of a complex Banach space. % and study its geometric and analytical properties. All previously studied modifications of the Fekete--Szeg\\\"{o} functional are either special cases or `components' of the mapping we introduce.   The study involves the examination of transforms of the Fekete--Szeg\\\"o mapping under specific transformations applied to given holomorphic mappings.   We show that for a mapping $f$, the third order Fr\\'eshet derivative of the inverse mapping $f^{-1}$ and of elements of the semigroup generated by $f$ can be expressed in terms of the Fekete--Szeg\\\"{o} mapping. Estimates of the Fekete--Szeg\\\"o mapping over some subclasses of semigroup generators and of starlike mappings are also presented.","sentences":["In this paper we introduce the Fekete--Szeg\\\"{o} type mapping in the open unit ball of a complex Banach space.","% and study its geometric and analytical properties.","All previously studied modifications of the Fekete--Szeg\\\"{o} functional are either special cases or `components' of the mapping we introduce.   ","The study involves the examination of transforms of the Fekete--Szeg\\\"o mapping under specific transformations applied to given holomorphic mappings.   ","We show that for a mapping $f$, the third order Fr\\'eshet derivative of the inverse mapping $f^{-1}$ and of elements of the semigroup generated by $f$ can be expressed in terms of the Fekete--Szeg\\\"{o} mapping.","Estimates of the Fekete--Szeg\\\"o mapping over some subclasses of semigroup generators and of starlike mappings are also presented."],"url":"http://arxiv.org/abs/2406.02752v1","category":"math.CV"}
{"created":"2024-06-04 20:02:52","title":"RATT: AThought Structure for Coherent and Correct LLMReasoning","abstract":"Large Language Models (LLMs) gain substantial reasoning and decision-making capabilities from thought structures. However, existing methods such as Tree of Thought and Retrieval Augmented Thoughts often fall short in complex tasks due to the limitations of insufficient local retrieval of factual knowledge and inadequate global selection of strategies. These limitations make it challenging for these methods to balance factual accuracy and comprehensive logical optimization effectively. To address these limitations, we introduce the Retrieval Augmented Thought Tree (RATT), a novel thought structure that considers both overall logical soundness and factual correctness at each step of the thinking process. Specifically, at every point of a thought branch, RATT performs planning and lookahead to explore and evaluate multiple potential reasoning steps, and integrate the fact-checking ability of Retrieval-Augmented Generation (RAG) with LLM's ability to assess overall strategy. Through this combination of factual knowledge and strategic feasibility, the RATT adjusts and integrates the thought tree structure to search for the most promising branches within the search space. This thought structure significantly enhances the model's coherence in logical inference and efficiency in decision-making, and thus increases the limit of the capacity of LLM to generate reliable inferences and decisions based on thought structures. A broad range of experiments on different types of tasks showcases that the RATT structure significantly outperforms existing methods in factual correctness and logical coherence.","sentences":["Large Language Models (LLMs) gain substantial reasoning and decision-making capabilities from thought structures.","However, existing methods such as Tree of Thought and Retrieval Augmented Thoughts often fall short in complex tasks due to the limitations of insufficient local retrieval of factual knowledge and inadequate global selection of strategies.","These limitations make it challenging for these methods to balance factual accuracy and comprehensive logical optimization effectively.","To address these limitations, we introduce the Retrieval Augmented Thought Tree (RATT), a novel thought structure that considers both overall logical soundness and factual correctness at each step of the thinking process.","Specifically, at every point of a thought branch, RATT performs planning and lookahead to explore and evaluate multiple potential reasoning steps, and integrate the fact-checking ability of Retrieval-Augmented Generation (RAG) with LLM's ability to assess overall strategy.","Through this combination of factual knowledge and strategic feasibility, the RATT adjusts and integrates the thought tree structure to search for the most promising branches within the search space.","This thought structure significantly enhances the model's coherence in logical inference and efficiency in decision-making, and thus increases the limit of the capacity of LLM to generate reliable inferences and decisions based on thought structures.","A broad range of experiments on different types of tasks showcases that the RATT structure significantly outperforms existing methods in factual correctness and logical coherence."],"url":"http://arxiv.org/abs/2406.02746v1","category":"cs.CL"}
{"created":"2024-06-04 20:01:39","title":"Measuring Stochastic Data Complexity with Boltzmann Influence Functions","abstract":"Estimating the uncertainty of a model's prediction on a test point is a crucial part of ensuring reliability and calibration under distribution shifts. A minimum description length approach to this problem uses the predictive normalized maximum likelihood (pNML) distribution, which considers every possible label for a data point, and decreases confidence in a prediction if other labels are also consistent with the model and training data. In this work we propose IF-COMP, a scalable and efficient approximation of the pNML distribution that linearizes the model with a temperature-scaled Boltzmann influence function. IF-COMP can be used to produce well-calibrated predictions on test points as well as measure complexity in both labelled and unlabelled settings. We experimentally validate IF-COMP on uncertainty calibration, mislabel detection, and OOD detection tasks, where it consistently matches or beats strong baseline methods.","sentences":["Estimating the uncertainty of a model's prediction on a test point is a crucial part of ensuring reliability and calibration under distribution shifts.","A minimum description length approach to this problem uses the predictive normalized maximum likelihood (pNML) distribution, which considers every possible label for a data point, and decreases confidence in a prediction if other labels are also consistent with the model and training data.","In this work we propose IF-COMP, a scalable and efficient approximation of the pNML distribution that linearizes the model with a temperature-scaled Boltzmann influence function.","IF-COMP can be used to produce well-calibrated predictions on test points as well as measure complexity in both labelled and unlabelled settings.","We experimentally validate IF-COMP on uncertainty calibration, mislabel detection, and OOD detection tasks, where it consistently matches or beats strong baseline methods."],"url":"http://arxiv.org/abs/2406.02745v1","category":"cs.LG"}
{"created":"2024-06-04 19:57:34","title":"Democratizing Propensity Score Matching Using Web Application","abstract":"Traditionally, data scientists use exploratory data analysis techniques such as correlation analysis, summary statistics, and regression analysis for identifying the most product enhancements and roadmap planning. However, these conventional approaches often yield biased conclusions and suboptimal solutions, leading to a waste of valuable time and missed opportunities for higher-value outcomes. In contrast, there are alternative techniques that involve the use of causal inference methods. However, these methods suffer from issues of limited accessibility, as they are not easily understandable or effectively utilized by inexperienced practitioners. Additionally, their implementation necessitates a substantial investment of time and effort. To this end, this paper tackles these challenges by democratizing one of the causal inference methods called Propensity Score Matching (PSM) and enhancing its accessibility for less technically inclined users through the automation of the entire workflow using a web application. Our approach not only fills this accessibility gap but also contributes to the existing literature by introducing a more rigorous model selection process and an enhanced sensitivity analysis. By overcoming the limitations of traditional exploratory data analysis methods, our web application has empowered data scientists at Booking.com to make better use of PSM, thereby improving the overall efficacy of their analyses.","sentences":["Traditionally, data scientists use exploratory data analysis techniques such as correlation analysis, summary statistics, and regression analysis for identifying the most product enhancements and roadmap planning.","However, these conventional approaches often yield biased conclusions and suboptimal solutions, leading to a waste of valuable time and missed opportunities for higher-value outcomes.","In contrast, there are alternative techniques that involve the use of causal inference methods.","However, these methods suffer from issues of limited accessibility, as they are not easily understandable or effectively utilized by inexperienced practitioners.","Additionally, their implementation necessitates a substantial investment of time and effort.","To this end, this paper tackles these challenges by democratizing one of the causal inference methods called Propensity Score Matching (PSM) and enhancing its accessibility for less technically inclined users through the automation of the entire workflow using a web application.","Our approach not only fills this accessibility gap but also contributes to the existing literature by introducing a more rigorous model selection process and an enhanced sensitivity analysis.","By overcoming the limitations of traditional exploratory data analysis methods, our web application has empowered data scientists at Booking.com to make better use of PSM, thereby improving the overall efficacy of their analyses."],"url":"http://arxiv.org/abs/2406.02743v1","category":"stat.AP"}
{"created":"2024-06-04 19:50:05","title":"Tolerant Algorithms for Learning with Arbitrary Covariate Shift","abstract":"We study the problem of learning under arbitrary distribution shift, where the learner is trained on a labeled set from one distribution but evaluated on a different, potentially adversarially generated test distribution. We focus on two frameworks: PQ learning [Goldwasser, A. Kalai, Y. Kalai, Montasser NeurIPS 2020], allowing abstention on adversarially generated parts of the test distribution, and TDS learning [Klivans, Stavropoulos, Vasilyan COLT 2024], permitting abstention on the entire test distribution if distribution shift is detected. All prior known algorithms either rely on learning primitives that are computationally hard even for simple function classes, or end up abstaining entirely even in the presence of a tiny amount of distribution shift.   We address both these challenges for natural function classes, including intersections of halfspaces and decision trees, and standard training distributions, including Gaussians. For PQ learning, we give efficient learning algorithms, while for TDS learning, our algorithms can tolerate moderate amounts of distribution shift. At the core of our approach is an improved analysis of spectral outlier-removal techniques from learning with nasty noise. Our analysis can (1) handle arbitrarily large fraction of outliers, which is crucial for handling arbitrary distribution shifts, and (2) obtain stronger bounds on polynomial moments of the distribution after outlier removal, yielding new insights into polynomial regression under distribution shifts. Lastly, our techniques lead to novel results for tolerant testable learning [Rubinfeld and Vasilyan STOC 2023], and learning with nasty noise.","sentences":["We study the problem of learning under arbitrary distribution shift, where the learner is trained on a labeled set from one distribution but evaluated on a different, potentially adversarially generated test distribution.","We focus on two frameworks: PQ learning [Goldwasser, A. Kalai, Y. Kalai, Montasser NeurIPS 2020], allowing abstention on adversarially generated parts of the test distribution, and TDS learning [Klivans, Stavropoulos, Vasilyan COLT 2024], permitting abstention on the entire test distribution if distribution shift is detected.","All prior known algorithms either rely on learning primitives that are computationally hard even for simple function classes, or end up abstaining entirely even in the presence of a tiny amount of distribution shift.   ","We address both these challenges for natural function classes, including intersections of halfspaces and decision trees, and standard training distributions, including Gaussians.","For PQ learning, we give efficient learning algorithms, while for TDS learning, our algorithms can tolerate moderate amounts of distribution shift.","At the core of our approach is an improved analysis of spectral outlier-removal techniques from learning with nasty noise.","Our analysis can (1) handle arbitrarily large fraction of outliers, which is crucial for handling arbitrary distribution shifts, and (2) obtain stronger bounds on polynomial moments of the distribution after outlier removal, yielding new insights into polynomial regression under distribution shifts.","Lastly, our techniques lead to novel results for tolerant testable learning","[Rubinfeld and Vasilyan STOC 2023], and learning with nasty noise."],"url":"http://arxiv.org/abs/2406.02742v1","category":"cs.DS"}
{"created":"2024-06-04 19:46:51","title":"Sampling From Multiscale Densities With Delayed Rejection Generalized Hamiltonian Monte Carlo","abstract":"With the increasing prevalence of probabilistic programming languages, Hamiltonian Monte Carlo (HMC) has become the mainstay of applied Bayesian inference. However HMC still struggles to sample from densities with multiscale geometry: a large step size is needed to efficiently explore low curvature regions while a small step size is needed to accurately explore high curvature regions. We introduce the delayed rejection generalized HMC (DR-G-HMC) sampler that overcomes this challenge by employing dynamic step size selection, inspired by differential equation solvers. In a single sampling iteration, DR-G-HMC sequentially makes proposals with geometrically decreasing step sizes if necessary. This simulates Hamiltonian dynamics with increasing fidelity that, in high curvature regions, generates proposals with a higher chance of acceptance. DR-G-HMC also makes generalized HMC competitive by decreasing the number of rejections which otherwise cause inefficient backtracking and prevents directed movement. We present experiments to demonstrate that DR-G-HMC (1) correctly samples from multiscale densities, (2) makes generalized HMC methods competitive with the state of the art No-U-Turn sampler, and (3) is robust to tuning parameters.","sentences":["With the increasing prevalence of probabilistic programming languages, Hamiltonian Monte Carlo (HMC) has become the mainstay of applied Bayesian inference.","However HMC still struggles to sample from densities with multiscale geometry: a large step size is needed to efficiently explore low curvature regions while a small step size is needed to accurately explore high curvature regions.","We introduce the delayed rejection generalized HMC (DR-G-HMC) sampler that overcomes this challenge by employing dynamic step size selection, inspired by differential equation solvers.","In a single sampling iteration, DR-G-HMC sequentially makes proposals with geometrically decreasing step sizes if necessary.","This simulates Hamiltonian dynamics with increasing fidelity that, in high curvature regions, generates proposals with a higher chance of acceptance.","DR-G-HMC also makes generalized HMC competitive by decreasing the number of rejections which otherwise cause inefficient backtracking and prevents directed movement.","We present experiments to demonstrate that DR-G-HMC (1) correctly samples from multiscale densities, (2) makes generalized HMC methods competitive with the state of the art No-U-Turn sampler, and (3) is robust to tuning parameters."],"url":"http://arxiv.org/abs/2406.02741v1","category":"stat.CO"}
{"created":"2024-06-04 19:24:19","title":"Existence and nonexistence of minimizers for classical capillarity problems in presence of nonlocal repulsion and gravity","abstract":"We investigate, under a volume constraint and among sets contained in a Euclidean half-space, the minimization problem of an energy functional given by the sum of a capillarity perimeter, a nonlocal repulsive term and a gravitational potential energy. The capillarity perimeter assigns a constant weight to the portion of the boundary touching the boundary of the half-space. The nonlocal term is represented by a double integral of a positive kernel $g$, while the gravitational term is represented by the integral of a positive potential $G$.   We first establish existence of volume-constrained minimizers in the small mass regime, together with several qualitative properties of minimizers. The existence result holds even for rather general choices of kernels in the nonlocal term, including attractive-repulsive ones. When the nonlocal kernel $g(x)=1/|x|^\\beta$ with $\\beta \\in (0,2]$, we also obtain nonexistence of volume constrained minimizers in the large mass regime. Finally, we prove a generalized existence result of minimizers holding for all masses, meaning that the infimum of the problem is realized by a finite disjoint union of sets thought located at \"infinite distance\" one from the other.   These results stem from an application of quantitative isoperimetric inequalities for the capillarity problem in a half-space.","sentences":["We investigate, under a volume constraint and among sets contained in a Euclidean half-space, the minimization problem of an energy functional given by the sum of a capillarity perimeter, a nonlocal repulsive term and a gravitational potential energy.","The capillarity perimeter assigns a constant weight to the portion of the boundary touching the boundary of the half-space.","The nonlocal term is represented by a double integral of a positive kernel $g$, while the gravitational term is represented by the integral of a positive potential $G$.   We first establish existence of volume-constrained minimizers in the small mass regime, together with several qualitative properties of minimizers.","The existence result holds even for rather general choices of kernels in the nonlocal term, including attractive-repulsive ones.","When the nonlocal kernel $g(x)=1/|x|^\\beta$ with $\\beta \\in (0,2]$, we also obtain nonexistence of volume constrained minimizers in the large mass regime.","Finally, we prove a generalized existence result of minimizers holding for all masses, meaning that the infimum of the problem is realized by a finite disjoint union of sets thought located at \"infinite distance\" one from the other.   ","These results stem from an application of quantitative isoperimetric inequalities for the capillarity problem in a half-space."],"url":"http://arxiv.org/abs/2406.02735v1","category":"math.AP"}
{"created":"2024-06-04 19:23:17","title":"Mukai lifting of self-dual points in $\\mathbb{P}^6$","abstract":"A set of $2n$ points in $\\mathbb{P}^{n-1}$ is self-dual if it is invariant under the Gale transform. Motivated by Mukai's work on canonical curves, Petrakiev showed that a general self-dual set of $14$ points in $\\mathbb{P}^6$ arises as the intersection of the Grassmannian ${\\rm Gr}(2,6)$ in its Pl\\\"ucker embedding in $\\mathbb{P}^{14}$ with a linear space of dimension $6$. In this paper we focus on the inverse problem of recovering such a linear space associated to a general self-dual set of points. We use numerical homotopy continuation to approach the problem and implement an algorithm in Julia to solve it. Along the way we also implement the forward problem of slicing Grassmannians and use it to experimentally study the real solutions to this problem.","sentences":["A set of $2n$ points in $\\mathbb{P}^{n-1}$ is self-dual if it is invariant under the Gale transform.","Motivated by Mukai's work on canonical curves, Petrakiev showed that a general self-dual set of $14$ points in $\\mathbb{P}^6$ arises as the intersection of the Grassmannian ${\\rm Gr}(2,6)$ in its Pl\\\"ucker embedding in $\\mathbb{P}^{14}$ with a linear space of dimension $6$.","In this paper we focus on the inverse problem of recovering such a linear space associated to a general self-dual set of points.","We use numerical homotopy continuation to approach the problem and implement an algorithm in Julia to solve it.","Along the way we also implement the forward problem of slicing Grassmannians and use it to experimentally study the real solutions to this problem."],"url":"http://arxiv.org/abs/2406.02734v1","category":"math.AG"}
{"created":"2024-06-04 19:18:05","title":"GEFL: Extended Filtration Learning for Graph Classification","abstract":"Extended persistence is a technique from topological data analysis to obtain global multiscale topological information from a graph. This includes information about connected components and cycles that are captured by the so-called persistence barcodes. We introduce extended persistence into a supervised learning framework for graph classification. Global topological information, in the form of a barcode with four different types of bars and their explicit cycle representatives, is combined into the model by the readout function which is computed by extended persistence. The entire model is end-to-end differentiable. We use a link-cut tree data structure and parallelism to lower the complexity of computing extended persistence, obtaining a speedup of more than 60x over the state-of-the-art for extended persistence computation. This makes extended persistence feasible for machine learning. We show that, under certain conditions, extended persistence surpasses both the WL[1] graph isomorphism test and 0-dimensional barcodes in terms of expressivity because it adds more global (topological) information. In particular, arbitrarily long cycles can be represented, which is difficult for finite receptive field message passing graph neural networks. Furthermore, we show the effectiveness of our method on real world datasets compared to many existing recent graph representation learning methods.","sentences":["Extended persistence is a technique from topological data analysis to obtain global multiscale topological information from a graph.","This includes information about connected components and cycles that are captured by the so-called persistence barcodes.","We introduce extended persistence into a supervised learning framework for graph classification.","Global topological information, in the form of a barcode with four different types of bars and their explicit cycle representatives, is combined into the model by the readout function which is computed by extended persistence.","The entire model is end-to-end differentiable.","We use a link-cut tree data structure and parallelism to lower the complexity of computing extended persistence, obtaining a speedup of more than 60x over the state-of-the-art for extended persistence computation.","This makes extended persistence feasible for machine learning.","We show that, under certain conditions, extended persistence surpasses both the WL[1] graph isomorphism test and 0-dimensional barcodes in terms of expressivity because it adds more global (topological) information.","In particular, arbitrarily long cycles can be represented, which is difficult for finite receptive field message passing graph neural networks.","Furthermore, we show the effectiveness of our method on real world datasets compared to many existing recent graph representation learning methods."],"url":"http://arxiv.org/abs/2406.02732v1","category":"cs.LG"}
{"created":"2024-06-04 19:07:45","title":"The LiteBIRD mission to explore cosmic inflation","abstract":"\\textit{LiteBIRD}, the next-generation cosmic microwave background (CMB) experiment, aims for a launch in Japan's fiscal year 2032, marking a major advancement in the exploration of primordial cosmology and fundamental physics. Orbiting the Sun-Earth Lagrangian point L2, this JAXA-led strategic L-class mission will conduct a comprehensive mapping of the CMB polarization across the entire sky. During its 3-year mission, \\textit{LiteBIRD} will employ three telescopes within 15 unique frequency bands (ranging from 34 through 448 GHz), targeting a sensitivity of 2.2\\,$\\mu$K-arcmin and a resolution of 0.5$^\\circ$ at 100\\,GHz. Its primary goal is to measure the tensor-to-scalar ratio $r$ with an uncertainty $\\delta r = 0.001$, including systematic errors and margin. If $r \\geq 0.01$, \\textit{LiteBIRD} expects to achieve a $>5\\sigma$ detection in the $\\ell=$2-10 and $\\ell=$11-200 ranges separately, providing crucial insight into the early Universe. We describe \\textit{LiteBIRD}'s scientific objectives, the application of systems engineering to mission requirements, the anticipated scientific impact, and the operations and scanning strategies vital to minimizing systematic effects. We will also highlight \\textit{LiteBIRD}'s synergies with concurrent CMB projects.","sentences":["\\textit{LiteBIRD}, the next-generation cosmic microwave background (CMB) experiment, aims for a launch in Japan's fiscal year 2032, marking a major advancement in the exploration of primordial cosmology and fundamental physics.","Orbiting the Sun-Earth Lagrangian point L2, this JAXA-led strategic L-class mission will conduct a comprehensive mapping of the CMB polarization across the entire sky.","During its 3-year mission, \\textit{LiteBIRD} will employ three telescopes within 15 unique frequency bands (ranging from 34 through 448 GHz), targeting a sensitivity of 2.2\\,$\\mu$K-arcmin and a resolution of 0.5$^\\circ$ at 100\\,GHz.","Its primary goal is to measure the tensor-to-scalar ratio $r$ with an uncertainty $\\delta r = 0.001$, including systematic errors and margin.","If $r \\geq 0.01$, \\textit{LiteBIRD} expects to achieve a $>5\\sigma$ detection in the $\\ell=$2-10 and $\\ell=$11-200 ranges separately, providing crucial insight into the early Universe.","We describe \\textit{LiteBIRD}'s scientific objectives, the application of systems engineering to mission requirements, the anticipated scientific impact, and the operations and scanning strategies vital to minimizing systematic effects.","We will also highlight \\textit{LiteBIRD}'s synergies with concurrent CMB projects."],"url":"http://arxiv.org/abs/2406.02724v1","category":"astro-ph.IM"}
{"created":"2024-06-04 19:03:13","title":"Analysis of the blowout plasma wakefields produced by drive beams with elliptical symmetry","abstract":"In the underdense (blowout) regime of plasma wakefield acceleration (PWFA), the particle beam is denser than the plasma. Under these conditions, the plasma electrons are nearly completely rarefacted from the beam channel, resulting in a nominally uniform ion column. Extensive investigations of this interaction assuming axisymmetry have been undertaken. However, the plasma blowout produced by a transversely asymmetric driver possesses quite different characteristics. They create an asymmetric plasma rarefaction region (bubble) which leads to asymmetric focusing in the two transverse planes. This is also accompanied by an undesired non-uniform accelerating gradient. The asymmetric blowout cross-section is found through simulation to be elliptical, and treating it as such permits a simple extension of the symmetric theory. In particular, focusing fields linear in both transverse directions exist in the bubble. The form of the wake potential and the concomitant matching conditions in this elliptical cavity are discussed in this paper. We also discuss bubble boundary estimation in the long driver limit and applications of the asymmetric features of the wakefield.","sentences":["In the underdense (blowout) regime of plasma wakefield acceleration (PWFA), the particle beam is denser than the plasma.","Under these conditions, the plasma electrons are nearly completely rarefacted from the beam channel, resulting in a nominally uniform ion column.","Extensive investigations of this interaction assuming axisymmetry have been undertaken.","However, the plasma blowout produced by a transversely asymmetric driver possesses quite different characteristics.","They create an asymmetric plasma rarefaction region (bubble) which leads to asymmetric focusing in the two transverse planes.","This is also accompanied by an undesired non-uniform accelerating gradient.","The asymmetric blowout cross-section is found through simulation to be elliptical, and treating it as such permits a simple extension of the symmetric theory.","In particular, focusing fields linear in both transverse directions exist in the bubble.","The form of the wake potential and the concomitant matching conditions in this elliptical cavity are discussed in this paper.","We also discuss bubble boundary estimation in the long driver limit and applications of the asymmetric features of the wakefield."],"url":"http://arxiv.org/abs/2406.02719v1","category":"physics.acc-ph"}
{"created":"2024-06-04 18:58:36","title":"Gravitational-wave data analysis with high-precision numerical relativity simulations of boson star mergers","abstract":"Gravitational-wave signals detected to date are commonly interpreted under the paradigm that they originate from pairs of black holes or neutron stars. Here, we explore the alternative scenario of boson-star signals being present in the data stream. We perform accurate and long ($\\sim 20$ orbits) numerical simulations of boson-star binaries and inject the resulting strain into LIGO noise. Our Bayesian inference reveals that some boson-star signals. are degenerate with current approximants, albeit with biased parameters, while others exhibit smoking-gun signatures leaving behind conspicuous residuals.","sentences":["Gravitational-wave signals detected to date are commonly interpreted under the paradigm that they originate from pairs of black holes or neutron stars.","Here, we explore the alternative scenario of boson-star signals being present in the data stream.","We perform accurate and long ($\\sim 20$ orbits) numerical simulations of boson-star binaries and inject the resulting strain into LIGO noise.","Our Bayesian inference reveals that some boson-star signals.","are degenerate with current approximants, albeit with biased parameters, while others exhibit smoking-gun signatures leaving behind conspicuous residuals."],"url":"http://arxiv.org/abs/2406.02715v1","category":"gr-qc"}
{"created":"2024-06-04 18:53:43","title":"Eccentricity evolution of PTA sources from cosmological initial conditions","abstract":"Recent results from pulsar timing arrays (PTAs) show evidence for a gravitational wave background (GWB) consistent with a population of unresolved supermassive black hole (SMBH) binaries (BHBs). While the data do not yet constrain the slope of the spectrum, this appears to flatten at the lowest frequencies, deviating from the power-law shape expected for circular binaries evolving solely due to gravitational wave (GW) emission. Interestingly, such flattening can be explained with a population of eccentric rather than circular binaries. The eccentricity of BHBs is notoriously difficult to predict based simply on the parameters of the host galaxies and the initial galactic orbit, as it is subject to stochastic effects. We study the evolution of the eccentricity of BHBs formed in galactic mergers with cosmological initial conditions from pairing to coalescence, with a focus on potential PTA sources. We select galactic mergers from the IllustrisTNG100-1 simulation and re-simulate them at high resolution with the N-body code Griffin down to binary separations of the order of a parsec. We then estimate coalescence timescales with a semi-analytical model of the evolution under the effects of GW emission and stellar hardening. We find that most mergers in IllustrisTNG100-1 occur on highly eccentric orbits, and that the eccentricity of BHBs at binary formation correlates with the initial eccentricity of the merger, if this is no larger than approximately 0.9. For extremely eccentric mergers, the binaries tend to form with modest eccentricities. We discuss the implications of these results on the interpretation of the observed GWB.","sentences":["Recent results from pulsar timing arrays (PTAs) show evidence for a gravitational wave background (GWB) consistent with a population of unresolved supermassive black hole (SMBH) binaries (BHBs).","While the data do not yet constrain the slope of the spectrum, this appears to flatten at the lowest frequencies, deviating from the power-law shape expected for circular binaries evolving solely due to gravitational wave (GW) emission.","Interestingly, such flattening can be explained with a population of eccentric rather than circular binaries.","The eccentricity of BHBs is notoriously difficult to predict based simply on the parameters of the host galaxies and the initial galactic orbit, as it is subject to stochastic effects.","We study the evolution of the eccentricity of BHBs formed in galactic mergers with cosmological initial conditions from pairing to coalescence, with a focus on potential PTA sources.","We select galactic mergers from the IllustrisTNG100-1 simulation and re-simulate them at high resolution with the N-body code Griffin down to binary separations of the order of a parsec.","We then estimate coalescence timescales with a semi-analytical model of the evolution under the effects of GW emission and stellar hardening.","We find that most mergers in IllustrisTNG100-1 occur on highly eccentric orbits, and that the eccentricity of BHBs at binary formation correlates with the initial eccentricity of the merger, if this is no larger than approximately 0.9.","For extremely eccentric mergers, the binaries tend to form with modest eccentricities.","We discuss the implications of these results on the interpretation of the observed GWB."],"url":"http://arxiv.org/abs/2406.02710v1","category":"astro-ph.GA"}
{"created":"2024-06-04 18:52:24","title":"Constructive Safety-Critical Control: Synthesizing Control Barrier Functions for Partially Feedback Linearizable Systems","abstract":"Certifying the safety of nonlinear systems, through the lens of set invariance and control barrier functions (CBFs), offers a powerful method for controller synthesis, provided a CBF can be constructed. This paper draws connections between partial feedback linearization and CBF synthesis. We illustrate that when a control affine system is input-output linearizable with respect to a smooth output function, then, under mild regularity conditions, one may extend any safety constraint defined on the output to a CBF for the full-order dynamics. These more general results are specialized to robotic systems where the conditions required to synthesize CBFs simplify. The CBFs constructed from our approach are applied and verified in simulation and hardware experiments on a quadrotor.","sentences":["Certifying the safety of nonlinear systems, through the lens of set invariance and control barrier functions (CBFs), offers a powerful method for controller synthesis, provided a CBF can be constructed.","This paper draws connections between partial feedback linearization and CBF synthesis.","We illustrate that when a control affine system is input-output linearizable with respect to a smooth output function, then, under mild regularity conditions, one may extend any safety constraint defined on the output to a CBF for the full-order dynamics.","These more general results are specialized to robotic systems where the conditions required to synthesize CBFs simplify.","The CBFs constructed from our approach are applied and verified in simulation and hardware experiments on a quadrotor."],"url":"http://arxiv.org/abs/2406.02709v1","category":"eess.SY"}
{"created":"2024-06-04 18:38:52","title":"Nature of long-lived moir\u00e9 interlayer excitons in electrically tunable MoS$_{2}$/MoSe$_{2}$ heterobilayers","abstract":"Interlayer excitons in transition-metal dichalcogenide heterobilayers combine high binding energy and valley-contrasting physics with long optical lifetime and strong dipolar character. Their permanent electric dipole enables electric-field control of emission energy, lifetime, and location. Device material and geometry impacts the nature of the interlayer excitons via their real- and momentum-space configurations. Here, we show that interlayer excitons in MoS$_{2}$/MoSe$_{2}$ heterobilayers are formed by charge carriers residing at the Brillouin zone edges, with negligible interlayer hybridization. We find that the moir\\'e superlattice leads to the reversal of the valley-dependent optical selection rules, yielding a positively valued g-factor and cross-polarized photoluminescence. Time-resolved photoluminescence measurements reveal that the interlayer exciton population retains the optically induced valley polarization throughout its microsecond-long lifetime. The combination of long optical lifetime and valley polarization retention makes MoS$_{2}$/MoSe$_{2}$ heterobilayers a promising platform for studying fundamental bosonic interactions and developing excitonic circuits for optical information processing.","sentences":["Interlayer excitons in transition-metal dichalcogenide heterobilayers combine high binding energy and valley-contrasting physics with long optical lifetime and strong dipolar character.","Their permanent electric dipole enables electric-field control of emission energy, lifetime, and location.","Device material and geometry impacts the nature of the interlayer excitons via their real- and momentum-space configurations.","Here, we show that interlayer excitons in MoS$_{2}$/MoSe$_{2}$ heterobilayers are formed by charge carriers residing at the Brillouin zone edges, with negligible interlayer hybridization.","We find that the moir\\'e superlattice leads to the reversal of the valley-dependent optical selection rules, yielding a positively valued g-factor and cross-polarized photoluminescence.","Time-resolved photoluminescence measurements reveal that the interlayer exciton population retains the optically induced valley polarization throughout its microsecond-long lifetime.","The combination of long optical lifetime and valley polarization retention makes MoS$_{2}$/MoSe$_{2}$ heterobilayers a promising platform for studying fundamental bosonic interactions and developing excitonic circuits for optical information processing."],"url":"http://arxiv.org/abs/2406.02708v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-05 15:53:25","title":"Noisy Data Visualization using Functional Data Analysis","abstract":"Data visualization via dimensionality reduction is an important tool in exploratory data analysis. However, when the data are noisy, many existing methods fail to capture the underlying structure of the data. The method called Empirical Intrinsic Geometry (EIG) was previously proposed for performing dimensionality reduction on high dimensional dynamical processes while theoretically eliminating all noise. However, implementing EIG in practice requires the construction of high-dimensional histograms, which suffer from the curse of dimensionality. Here we propose a new data visualization method called Functional Information Geometry (FIG) for dynamical processes that adapts the EIG framework while using approaches from functional data analysis to mitigate the curse of dimensionality. We experimentally demonstrate that the resulting method outperforms a variant of EIG designed for visualization in terms of capturing the true structure, hyperparameter robustness, and computational speed. We then use our method to visualize EEG brain measurements of sleep activity.","sentences":["Data visualization via dimensionality reduction is an important tool in exploratory data analysis.","However, when the data are noisy, many existing methods fail to capture the underlying structure of the data.","The method called Empirical Intrinsic Geometry (EIG) was previously proposed for performing dimensionality reduction on high dimensional dynamical processes while theoretically eliminating all noise.","However, implementing EIG in practice requires the construction of high-dimensional histograms, which suffer from the curse of dimensionality.","Here we propose a new data visualization method called Functional Information Geometry (FIG) for dynamical processes that adapts the EIG framework while using approaches from functional data analysis to mitigate the curse of dimensionality.","We experimentally demonstrate that the resulting method outperforms a variant of EIG designed for visualization in terms of capturing the true structure, hyperparameter robustness, and computational speed.","We then use our method to visualize EEG brain measurements of sleep activity."],"url":"http://arxiv.org/abs/2406.03396v1","category":"cs.LG"}
{"created":"2024-06-05 15:24:20","title":"Posterior and variational inference for deep neural networks with heavy-tailed weights","abstract":"We consider deep neural networks in a Bayesian framework with a prior distribution sampling the network weights at random. Following a recent idea of Agapiou and Castillo (2023), who show that heavy-tailed prior distributions achieve automatic adaptation to smoothness, we introduce a simple Bayesian deep learning prior based on heavy-tailed weights and ReLU activation. We show that the corresponding posterior distribution achieves near-optimal minimax contraction rates, simultaneously adaptive to both intrinsic dimension and smoothness of the underlying function, in a variety of contexts including nonparametric regression, geometric data and Besov spaces. While most works so far need a form of model selection built-in within the prior distribution, a key aspect of our approach is that it does not require to sample hyperparameters to learn the architecture of the network. We also provide variational Bayes counterparts of the results, that show that mean-field variational approximations still benefit from near-optimal theoretical support.","sentences":["We consider deep neural networks in a Bayesian framework with a prior distribution sampling the network weights at random.","Following a recent idea of Agapiou and Castillo (2023), who show that heavy-tailed prior distributions achieve automatic adaptation to smoothness, we introduce a simple Bayesian deep learning prior based on heavy-tailed weights and ReLU activation.","We show that the corresponding posterior distribution achieves near-optimal minimax contraction rates, simultaneously adaptive to both intrinsic dimension and smoothness of the underlying function, in a variety of contexts including nonparametric regression, geometric data and Besov spaces.","While most works so far need a form of model selection built-in within the prior distribution, a key aspect of our approach is that it does not require to sample hyperparameters to learn the architecture of the network.","We also provide variational Bayes counterparts of the results, that show that mean-field variational approximations still benefit from near-optimal theoretical support."],"url":"http://arxiv.org/abs/2406.03369v1","category":"stat.ML"}
{"created":"2024-06-05 14:13:38","title":"Learning Visual Prompts for Guiding the Attention of Vision Transformers","abstract":"Visual prompting infuses visual information into the input image to adapt models toward specific predictions and tasks. Recently, manually crafted markers such as red circles are shown to guide the model to attend to a target region on the image. However, these markers only work on models trained with data containing those markers. Moreover, finding these prompts requires guesswork or prior knowledge of the domain on which the model is trained. This work circumvents manual design constraints by proposing to learn the visual prompts for guiding the attention of vision transformers. The learned visual prompt, added to any input image would redirect the attention of the pre-trained vision transformer to its spatial location on the image. Specifically, the prompt is learned in a self-supervised manner without requiring annotations and without fine-tuning the vision transformer. Our experiments demonstrate the effectiveness of the proposed optimization-based visual prompting strategy across various pre-trained vision encoders.","sentences":["Visual prompting infuses visual information into the input image to adapt models toward specific predictions and tasks.","Recently, manually crafted markers such as red circles are shown to guide the model to attend to a target region on the image.","However, these markers only work on models trained with data containing those markers.","Moreover, finding these prompts requires guesswork or prior knowledge of the domain on which the model is trained.","This work circumvents manual design constraints by proposing to learn the visual prompts for guiding the attention of vision transformers.","The learned visual prompt, added to any input image would redirect the attention of the pre-trained vision transformer to its spatial location on the image.","Specifically, the prompt is learned in a self-supervised manner without requiring annotations and without fine-tuning the vision transformer.","Our experiments demonstrate the effectiveness of the proposed optimization-based visual prompting strategy across various pre-trained vision encoders."],"url":"http://arxiv.org/abs/2406.03303v1","category":"cs.CV"}
{"created":"2024-06-05 14:08:13","title":"L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap Multiview Point Cloud Registration","abstract":"Point cloud registration is a prerequisite for many applications in computer vision and robotics. Most existing methods focus on pairwise registration of two point clouds with high overlap. Although there have been some methods for low overlap cases, they struggle in degraded scenarios. This paper introduces a novel framework named L-PR, designed to register unordered low overlap multiview point clouds leveraging LiDAR fiducial markers. We refer to them as LiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco markers, thin sheets of paper that do not affect the 3D geometry of the environment. We first propose an improved adaptive threshold marker detection method to provide robust detection results when the viewpoints among point clouds change dramatically. Then, we formulate the unordered multiview point cloud registration problem as a maximum a-posteriori (MAP) problem and develop a framework consisting of two levels of graphs to address it. The first-level graph, constructed as a weighted graph, is designed to efficiently and optimally infer initial values of scan poses from the unordered set. The second-level graph is constructed as a factor graph. By globally optimizing the variables on the graph, including scan poses, marker poses, and marker corner positions, we tackle the MAP problem. We conduct qualitative and quantitative experiments to demonstrate that the proposed method exhibits superiority over competitors in four aspects: registration accuracy, instance reconstruction quality, localization accuracy, and robustness to the degraded scene. To benefit the community, we open-source our method and dataset at https://github.com/yorklyb/LiDAR-SFM.","sentences":["Point cloud registration is a prerequisite for many applications in computer vision and robotics.","Most existing methods focus on pairwise registration of two point clouds with high overlap.","Although there have been some methods for low overlap cases, they struggle in degraded scenarios.","This paper introduces a novel framework named L-PR, designed to register unordered low overlap multiview point clouds leveraging LiDAR fiducial markers.","We refer to them as LiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco markers, thin sheets of paper that do not affect the 3D geometry of the environment.","We first propose an improved adaptive threshold marker detection method to provide robust detection results when the viewpoints among point clouds change dramatically.","Then, we formulate the unordered multiview point cloud registration problem as a maximum a-posteriori (MAP) problem and develop a framework consisting of two levels of graphs to address it.","The first-level graph, constructed as a weighted graph, is designed to efficiently and optimally infer initial values of scan poses from the unordered set.","The second-level graph is constructed as a factor graph.","By globally optimizing the variables on the graph, including scan poses, marker poses, and marker corner positions, we tackle the MAP problem.","We conduct qualitative and quantitative experiments to demonstrate that the proposed method exhibits superiority over competitors in four aspects: registration accuracy, instance reconstruction quality, localization accuracy, and robustness to the degraded scene.","To benefit the community, we open-source our method and dataset at https://github.com/yorklyb/LiDAR-SFM."],"url":"http://arxiv.org/abs/2406.03298v1","category":"cs.CV"}
{"created":"2024-06-05 12:45:25","title":"Text-like Encoding of Collaborative Information in Large Language Models for Recommendation","abstract":"When adapting Large Language Models for Recommendation (LLMRec), it is crucial to integrate collaborative information. Existing methods achieve this by learning collaborative embeddings in LLMs' latent space from scratch or by mapping from external models. However, they fail to represent the information in a text-like format, which may not align optimally with LLMs. To bridge this gap, we introduce BinLLM, a novel LLMRec method that seamlessly integrates collaborative information through text-like encoding. BinLLM converts collaborative embeddings from external models into binary sequences -- a specific text format that LLMs can understand and operate on directly, facilitating the direct usage of collaborative information in text-like format by LLMs. Additionally, BinLLM provides options to compress the binary sequence using dot-decimal notation to avoid excessively long lengths. Extensive experiments validate that BinLLM introduces collaborative information in a manner better aligned with LLMs, resulting in enhanced performance. We release our code at https://github.com/zyang1580/BinLLM.","sentences":["When adapting Large Language Models for Recommendation (LLMRec), it is crucial to integrate collaborative information.","Existing methods achieve this by learning collaborative embeddings in LLMs' latent space from scratch or by mapping from external models.","However, they fail to represent the information in a text-like format, which may not align optimally with LLMs.","To bridge this gap, we introduce BinLLM, a novel LLMRec method that seamlessly integrates collaborative information through text-like encoding.","BinLLM converts collaborative embeddings from external models into binary sequences -- a specific text format that LLMs can understand and operate on directly, facilitating the direct usage of collaborative information in text-like format by LLMs.","Additionally, BinLLM provides options to compress the binary sequence using dot-decimal notation to avoid excessively long lengths.","Extensive experiments validate that BinLLM introduces collaborative information in a manner better aligned with LLMs, resulting in enhanced performance.","We release our code at https://github.com/zyang1580/BinLLM."],"url":"http://arxiv.org/abs/2406.03210v1","category":"cs.IR"}
{"created":"2024-06-05 11:53:28","title":"Foundation Models for Geophysics: Reviews and Perspectives","abstract":"Recently, large models, or foundation models have demonstrated outstanding performance and have been applied in a variety of disciplines, such as chemistry, biology, economics, etc. Foundation models, trained on vast amounts of data, can be adapted to a wide range of use cases. The emergence of foundation models has a significant impact on the research paradigms in these fields. Geophysics is a scientific field dedicated to exploring and understanding the Earth's structures and states through the application of physical principles and the analysis of multimodal geophysical data. In the field of geophysics, the processing and interpretation of geophysical data are characterized by three primary features: extensive data volume, multimodality, and dependence on experience. These characteristics provide a suitable environment as well as challenges for the development and breakthrough of foundation models in the field of geophysics. In this perspective, we discuss the potential applications and research directions of geophysical foundation models (GeoFMs), exploring new research paradigms in geophysics in the era of foundation models. Exploration geophysics is the main focus, while the development of foundation models in remote sensing, seismology, and other related sub-disciplines in geophysics is also discussed. In the meantime, we also propose two strategies for constructing GeoFMs and discuss challenges that may arise during the process of development.","sentences":["Recently, large models, or foundation models have demonstrated outstanding performance and have been applied in a variety of disciplines, such as chemistry, biology, economics, etc.","Foundation models, trained on vast amounts of data, can be adapted to a wide range of use cases.","The emergence of foundation models has a significant impact on the research paradigms in these fields.","Geophysics is a scientific field dedicated to exploring and understanding the Earth's structures and states through the application of physical principles and the analysis of multimodal geophysical data.","In the field of geophysics, the processing and interpretation of geophysical data are characterized by three primary features: extensive data volume, multimodality, and dependence on experience.","These characteristics provide a suitable environment as well as challenges for the development and breakthrough of foundation models in the field of geophysics.","In this perspective, we discuss the potential applications and research directions of geophysical foundation models (GeoFMs), exploring new research paradigms in geophysics in the era of foundation models.","Exploration geophysics is the main focus, while the development of foundation models in remote sensing, seismology, and other related sub-disciplines in geophysics is also discussed.","In the meantime, we also propose two strategies for constructing GeoFMs and discuss challenges that may arise during the process of development."],"url":"http://arxiv.org/abs/2406.03163v1","category":"physics.geo-ph"}
{"created":"2024-06-05 10:02:56","title":"Singing Voice Graph Modeling for SingFake Detection","abstract":"Detecting singing voice deepfakes, or SingFake, involves determining the authenticity and copyright of a singing voice. Existing models for speech deepfake detection have struggled to adapt to unseen attacks in this unique singing voice domain of human vocalization. To bridge the gap, we present a groundbreaking SingGraph model. The model synergizes the capabilities of the MERT acoustic music understanding model for pitch and rhythm analysis with the wav2vec2.0 model for linguistic analysis of lyrics. Additionally, we advocate for using RawBoost and beat matching techniques grounded in music domain knowledge for singing voice augmentation, thereby enhancing SingFake detection performance. Our proposed method achieves new state-of-the-art (SOTA) results within the SingFake dataset, surpassing the previous SOTA model across three distinct scenarios: it improves EER relatively for seen singers by 13.2%, for unseen singers by 24.3%, and unseen singers using different codecs by 37.1%.","sentences":["Detecting singing voice deepfakes, or SingFake, involves determining the authenticity and copyright of a singing voice.","Existing models for speech deepfake detection have struggled to adapt to unseen attacks in this unique singing voice domain of human vocalization.","To bridge the gap, we present a groundbreaking SingGraph model.","The model synergizes the capabilities of the MERT acoustic music understanding model for pitch and rhythm analysis with the wav2vec2.0 model for linguistic analysis of lyrics.","Additionally, we advocate for using RawBoost and beat matching techniques grounded in music domain knowledge for singing voice augmentation, thereby enhancing SingFake detection performance.","Our proposed method achieves new state-of-the-art (SOTA) results within the SingFake dataset, surpassing the previous SOTA model across three distinct scenarios: it improves EER relatively for seen singers by 13.2%, for unseen singers by 24.3%, and unseen singers using different codecs by 37.1%."],"url":"http://arxiv.org/abs/2406.03111v1","category":"eess.AS"}
{"created":"2024-06-05 09:54:53","title":"Heavy Particle Clustering in Inertial Subrange of High--Reynolds Number Turbulence","abstract":"Direct numerical simulation of homogeneous isotropic turbulence shows pronounced clustering of inertial particles in the inertial subrange at high Reynolds number, in addition to the clustering typically observed in the near dissipation range. The clustering in the inertial subrange is characterized by the bump in the particle number density spectra and is due to modulation of preferential concentration. The number density spectrum can be modeled by a rational function of the scale-dependent Stokes number.","sentences":["Direct numerical simulation of homogeneous isotropic turbulence shows pronounced clustering of inertial particles in the inertial subrange at high Reynolds number, in addition to the clustering typically observed in the near dissipation range.","The clustering in the inertial subrange is characterized by the bump in the particle number density spectra and is due to modulation of preferential concentration.","The number density spectrum can be modeled by a rational function of the scale-dependent Stokes number."],"url":"http://arxiv.org/abs/2406.03107v1","category":"physics.flu-dyn"}
{"created":"2024-06-05 08:21:49","title":"High-order Discontinuous Galerkin Methods for the Monodomain and Bidomain Models","abstract":"This work aims at presenting a Discontinuous Galerkin (DG) formulation employing a spectral basis for two important models employed in cardiac electrophysiology, namely the monodomain and bidomain models. The use of DG methods is motivated by the characteristic of the mathematical solution of such equations which often corresponds to a highly steep wavefront. Hence, the built-in flexibility of discontinuous methods in developing adaptive approaches, combined with the high-order accuracy, can well represent the underlying physics. The choice of a semi-implicit time integration allows for a fast solution at each time step. The article includes some numerical tests to verify the convergence properties and the physiological behaviour of the numerical solution. Also, a pseudo-realistic simulation turns out to fully reconstruct the propagation of the electric potential, comprising the phases of depolarization and repolarization, by overcoming the typical issues related to the steepness of the wave front.","sentences":["This work aims at presenting a Discontinuous Galerkin (DG) formulation employing a spectral basis for two important models employed in cardiac electrophysiology, namely the monodomain and bidomain models.","The use of DG methods is motivated by the characteristic of the mathematical solution of such equations which often corresponds to a highly steep wavefront.","Hence, the built-in flexibility of discontinuous methods in developing adaptive approaches, combined with the high-order accuracy, can well represent the underlying physics.","The choice of a semi-implicit time integration allows for a fast solution at each time step.","The article includes some numerical tests to verify the convergence properties and the physiological behaviour of the numerical solution.","Also, a pseudo-realistic simulation turns out to fully reconstruct the propagation of the electric potential, comprising the phases of depolarization and repolarization, by overcoming the typical issues related to the steepness of the wave front."],"url":"http://arxiv.org/abs/2406.03045v1","category":"math.NA"}
{"created":"2024-06-05 07:09:19","title":"Phy-Diff: Physics-guided Hourglass Diffusion Model for Diffusion MRI Synthesis","abstract":"Diffusion MRI (dMRI) is an important neuroimaging technique with high acquisition costs. Deep learning approaches have been used to enhance dMRI and predict diffusion biomarkers through undersampled dMRI. To generate more comprehensive raw dMRI, generative adversarial network based methods are proposed to include b-values and b-vectors as conditions, but they are limited by unstable training and less desirable diversity. The emerging diffusion model (DM) promises to improve generative performance. However, it remains challenging to include essential information in conditioning DM for more relevant generation, i.e., the physical principles of dMRI and white matter tract structures. In this study, we propose a physics-guided diffusion model to generate high-quality dMRI. Our model introduces the physical principles of dMRI in the noise evolution in the diffusion process and introduce a query-based conditional mapping within the difussion model. In addition, to enhance the anatomical fine detials of the generation, we introduce the XTRACT atlas as prior of white matter tracts by adopting an adapter technique. Our experiment results show that our method outperforms other state-of-the-art methods and has the potential to advance dMRI enhancement.","sentences":["Diffusion MRI (dMRI) is an important neuroimaging technique with high acquisition costs.","Deep learning approaches have been used to enhance dMRI and predict diffusion biomarkers through undersampled dMRI.","To generate more comprehensive raw dMRI, generative adversarial network based methods are proposed to include b-values and b-vectors as conditions, but they are limited by unstable training and less desirable diversity.","The emerging diffusion model (DM) promises to improve generative performance.","However, it remains challenging to include essential information in conditioning DM for more relevant generation, i.e., the physical principles of dMRI and white matter tract structures.","In this study, we propose a physics-guided diffusion model to generate high-quality dMRI.","Our model introduces the physical principles of dMRI in the noise evolution in the diffusion process and introduce a query-based conditional mapping within the difussion model.","In addition, to enhance the anatomical fine detials of the generation, we introduce the XTRACT atlas as prior of white matter tracts by adopting an adapter technique.","Our experiment results show that our method outperforms other state-of-the-art methods and has the potential to advance dMRI enhancement."],"url":"http://arxiv.org/abs/2406.03002v1","category":"eess.IV"}
{"created":"2024-06-05 06:36:43","title":"Enhancing Multimodal Large Language Models with Multi-instance Visual Prompt Generator for Visual Representation Enrichment","abstract":"Multimodal Large Language Models (MLLMs) have achieved SOTA performance in various visual language tasks by fusing the visual representations with LLMs leveraging some visual adapters. In this paper, we first establish that adapters using query-based Transformers such as Q-former is a simplified Multi-instance Learning method without considering instance heterogeneity/correlation. We then propose a general component termed Multi-instance Visual Prompt Generator (MIVPG) to incorporate enriched visual representations into LLMs by taking advantage of instance correlation between images or patches for the same sample. Quantatitive evaluation on three public vision-language (VL) datasets from different scenarios shows that the proposed MIVPG improves Q-former in main VL tasks.","sentences":["Multimodal Large Language Models (MLLMs) have achieved SOTA performance in various visual language tasks by fusing the visual representations with LLMs leveraging some visual adapters.","In this paper, we first establish that adapters using query-based Transformers such as Q-former is a simplified Multi-instance Learning method without considering instance heterogeneity/correlation.","We then propose a general component termed Multi-instance Visual Prompt Generator (MIVPG) to incorporate enriched visual representations into LLMs by taking advantage of instance correlation between images or patches for the same sample.","Quantatitive evaluation on three public vision-language (VL) datasets from different scenarios shows that the proposed MIVPG improves Q-former in main VL tasks."],"url":"http://arxiv.org/abs/2406.02987v1","category":"cs.CV"}
{"created":"2024-06-05 05:52:20","title":"Adversarial Generation of Hierarchical Gaussians for 3D Generative Model","abstract":"Most advances in 3D Generative Adversarial Networks (3D GANs) largely depend on ray casting-based volume rendering, which incurs demanding rendering costs. One promising alternative is rasterization-based 3D Gaussian Splatting (3D-GS), providing a much faster rendering speed and explicit 3D representation. In this paper, we exploit Gaussian as a 3D representation for 3D GANs by leveraging its efficient and explicit characteristics. However, in an adversarial framework, we observe that a na\\\"ive generator architecture suffers from training instability and lacks the capability to adjust the scale of Gaussians. This leads to model divergence and visual artifacts due to the absence of proper guidance for initialized positions of Gaussians and densification to manage their scales adaptively. To address these issues, we introduce a generator architecture with a hierarchical multi-scale Gaussian representation that effectively regularizes the position and scale of generated Gaussians. Specifically, we design a hierarchy of Gaussians where finer-level Gaussians are parameterized by their coarser-level counterparts; the position of finer-level Gaussians would be located near their coarser-level counterparts, and the scale would monotonically decrease as the level becomes finer, modeling both coarse and fine details of the 3D scene. Experimental results demonstrate that ours achieves a significantly faster rendering speed (x100) compared to state-of-the-art 3D consistent GANs with comparable 3D generation capability. Project page: https://hse1032.github.io/gsgan.","sentences":["Most advances in 3D Generative Adversarial Networks (3D GANs) largely depend on ray casting-based volume rendering, which incurs demanding rendering costs.","One promising alternative is rasterization-based 3D Gaussian Splatting (3D-GS), providing a much faster rendering speed and explicit 3D representation.","In this paper, we exploit Gaussian as a 3D representation for 3D GANs by leveraging its efficient and explicit characteristics.","However, in an adversarial framework, we observe that a na\\\"ive generator architecture suffers from training instability and lacks the capability to adjust the scale of Gaussians.","This leads to model divergence and visual artifacts due to the absence of proper guidance for initialized positions of Gaussians and densification to manage their scales adaptively.","To address these issues, we introduce a generator architecture with a hierarchical multi-scale Gaussian representation that effectively regularizes the position and scale of generated Gaussians.","Specifically, we design a hierarchy of Gaussians where finer-level Gaussians are parameterized by their coarser-level counterparts; the position of finer-level Gaussians would be located near their coarser-level counterparts, and the scale would monotonically decrease as the level becomes finer, modeling both coarse and fine details of the 3D scene.","Experimental results demonstrate that ours achieves a significantly faster rendering speed (x100) compared to state-of-the-art 3D consistent GANs with comparable 3D generation capability.","Project page: https://hse1032.github.io/gsgan."],"url":"http://arxiv.org/abs/2406.02968v1","category":"cs.CV"}
{"created":"2024-06-05 05:42:46","title":"Understanding the Impact of Negative Prompts: When and How Do They Take Effect?","abstract":"The concept of negative prompts, emerging from conditional generation models like Stable Diffusion, allows users to specify what to exclude from the generated images.%, demonstrating significant practical efficacy. Despite the widespread use of negative prompts, their intrinsic mechanisms remain largely unexplored. This paper presents the first comprehensive study to uncover how and when negative prompts take effect. Our extensive empirical analysis identifies two primary behaviors of negative prompts. Delayed Effect: The impact of negative prompts is observed after positive prompts render corresponding content. Deletion Through Neutralization: Negative prompts delete concepts from the generated image through a mutual cancellation effect in latent space with positive prompts. These insights reveal significant potential real-world applications; for example, we demonstrate that negative prompts can facilitate object inpainting with minimal alterations to the background via a simple adaptive algorithm. We believe our findings will offer valuable insights for the community in capitalizing on the potential of negative prompts.","sentences":["The concept of negative prompts, emerging from conditional generation models like Stable Diffusion, allows users to specify what to exclude from the generated images.%, demonstrating significant practical efficacy.","Despite the widespread use of negative prompts, their intrinsic mechanisms remain largely unexplored.","This paper presents the first comprehensive study to uncover how and when negative prompts take effect.","Our extensive empirical analysis identifies two primary behaviors of negative prompts.","Delayed Effect: The impact of negative prompts is observed after positive prompts render corresponding content.","Deletion Through Neutralization:","Negative prompts delete concepts from the generated image through a mutual cancellation effect in latent space with positive prompts.","These insights reveal significant potential real-world applications; for example, we demonstrate that negative prompts can facilitate object inpainting with minimal alterations to the background via a simple adaptive algorithm.","We believe our findings will offer valuable insights for the community in capitalizing on the potential of negative prompts."],"url":"http://arxiv.org/abs/2406.02965v1","category":"cs.CV"}
{"created":"2024-06-05 04:54:36","title":"Achieving Near-Optimal Convergence for Distributed Minimax Optimization with Adaptive Stepsizes","abstract":"In this paper, we show that applying adaptive methods directly to distributed minimax problems can result in non-convergence due to inconsistency in locally computed adaptive stepsizes. To address this challenge, we propose D-AdaST, a Distributed Adaptive minimax method with Stepsize Tracking. The key strategy is to employ an adaptive stepsize tracking protocol involving the transmission of two extra (scalar) variables. This protocol ensures the consistency among stepsizes of nodes, eliminating the steady-state error due to the lack of coordination of stepsizes among nodes that commonly exists in vanilla distributed adaptive methods, and thus guarantees exact convergence. For nonconvex-strongly-concave distributed minimax problems, we characterize the specific transient times that ensure time-scale separation of stepsizes and quasi-independence of networks, leading to a near-optimal convergence rate of $\\tilde{\\mathcal{O}} \\left( \\epsilon ^{-\\left( 4+\\delta \\right)} \\right)$ for any small $\\delta > 0$, matching that of the centralized counterpart. To our best knowledge, D-AdaST is the first distributed adaptive method achieving near-optimal convergence without knowing any problem-dependent parameters for nonconvex minimax problems. Extensive experiments are conducted to validate our theoretical results.","sentences":["In this paper, we show that applying adaptive methods directly to distributed minimax problems can result in non-convergence due to inconsistency in locally computed adaptive stepsizes.","To address this challenge, we propose D-AdaST, a Distributed Adaptive minimax method with Stepsize Tracking.","The key strategy is to employ an adaptive stepsize tracking protocol involving the transmission of two extra (scalar) variables.","This protocol ensures the consistency among stepsizes of nodes, eliminating the steady-state error due to the lack of coordination of stepsizes among nodes that commonly exists in vanilla distributed adaptive methods, and thus guarantees exact convergence.","For nonconvex-strongly-concave distributed minimax problems, we characterize the specific transient times that ensure time-scale separation of stepsizes and quasi-independence of networks, leading to a near-optimal convergence rate of $\\tilde{\\mathcal{O}} \\left( \\epsilon ^{-\\left( 4+\\delta \\right)} \\right)$ for any small $\\delta > 0$, matching that of the centralized counterpart.","To our best knowledge, D-AdaST is the first distributed adaptive method achieving near-optimal convergence without knowing any problem-dependent parameters for nonconvex minimax problems.","Extensive experiments are conducted to validate our theoretical results."],"url":"http://arxiv.org/abs/2406.02939v1","category":"math.OC"}
{"created":"2024-06-05 03:36:11","title":"LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive Modeling of Audio Discrete Codes","abstract":"Prior works have demonstrated zero-shot text-to-speech by using a generative language model on audio tokens obtained via a neural audio codec. It is still challenging, however, to adapt them to low-latency scenarios. In this paper, we present LiveSpeech - a fully autoregressive language model-based approach for zero-shot text-to-speech, enabling low-latency streaming of the output audio. To allow multiple token prediction within a single decoding step, we propose (1) using adaptive codebook loss weights that consider codebook contribution in each frame and focus on hard instances, and (2) grouping codebooks and processing groups in parallel. Experiments show our proposed models achieve competitive results to state-of-the-art baselines in terms of content accuracy, speaker similarity, audio quality, and inference speed while being suitable for low-latency streaming applications.","sentences":["Prior works have demonstrated zero-shot text-to-speech by using a generative language model on audio tokens obtained via a neural audio codec.","It is still challenging, however, to adapt them to low-latency scenarios.","In this paper, we present LiveSpeech - a fully autoregressive language model-based approach for zero-shot text-to-speech, enabling low-latency streaming of the output audio.","To allow multiple token prediction within a single decoding step, we propose (1) using adaptive codebook loss weights that consider codebook contribution in each frame and focus on hard instances, and (2) grouping codebooks and processing groups in parallel.","Experiments show our proposed models achieve competitive results to state-of-the-art baselines in terms of content accuracy, speaker similarity, audio quality, and inference speed while being suitable for low-latency streaming applications."],"url":"http://arxiv.org/abs/2406.02897v1","category":"cs.SD"}
{"created":"2024-06-05 02:59:08","title":"Inv-Adapter: ID Customization Generation via Image Inversion and Lightweight Adapter","abstract":"The remarkable advancement in text-to-image generation models significantly boosts the research in ID customization generation. However, existing personalization methods cannot simultaneously satisfy high fidelity and high-efficiency requirements. Their main bottleneck lies in the prompt image encoder, which produces weak alignment signals with the text-to-image model and significantly increased model size. Towards this end, we propose a lightweight Inv-Adapter, which first extracts diffusion-domain representations of ID images utilizing a pre-trained text-to-image model via DDIM image inversion, without additional image encoder. Benefiting from the high alignment of the extracted ID prompt features and the intermediate features of the text-to-image model, we then embed them efficiently into the base text-to-image model by carefully designing a lightweight attention adapter. We conduct extensive experiments to assess ID fidelity, generation loyalty, speed, and training parameters, all of which show that the proposed Inv-Adapter is highly competitive in ID customization generation and model scale.","sentences":["The remarkable advancement in text-to-image generation models significantly boosts the research in ID customization generation.","However, existing personalization methods cannot simultaneously satisfy high fidelity and high-efficiency requirements.","Their main bottleneck lies in the prompt image encoder, which produces weak alignment signals with the text-to-image model and significantly increased model size.","Towards this end, we propose a lightweight Inv-Adapter, which first extracts diffusion-domain representations of ID images utilizing a pre-trained text-to-image model via DDIM image inversion, without additional image encoder.","Benefiting from the high alignment of the extracted ID prompt features and the intermediate features of the text-to-image model, we then embed them efficiently into the base text-to-image model by carefully designing a lightweight attention adapter.","We conduct extensive experiments to assess ID fidelity, generation loyalty, speed, and training parameters, all of which show that the proposed Inv-Adapter is highly competitive in ID customization generation and model scale."],"url":"http://arxiv.org/abs/2406.02881v1","category":"cs.CV"}
{"created":"2024-06-05 01:52:58","title":"Concentration bounds for stochastic systems with singular kernels","abstract":"This note is concerned with weakly interacting stochastic particle systems with possibly singular pairwise interactions. In this setting, we observe a connection between entropic propagation of chaos and exponential concentration bounds for the empirical measure of the system. In particular, we establish a variational upper bound for the probability of a certain rare event, and then use this upper bound to show that \"controlled\" entropic propagation of chaos implies an exponential concentration bound for the empirical measure. This connection allows us to infer concentration bounds for a class of singular stochastic systems through a simple adaptation of the arguments developed in Jabin and Wang (2018).","sentences":["This note is concerned with weakly interacting stochastic particle systems with possibly singular pairwise interactions.","In this setting, we observe a connection between entropic propagation of chaos and exponential concentration bounds for the empirical measure of the system.","In particular, we establish a variational upper bound for the probability of a certain rare event, and then use this upper bound to show that \"controlled\" entropic propagation of chaos implies an exponential concentration bound for the empirical measure.","This connection allows us to infer concentration bounds for a class of singular stochastic systems through a simple adaptation of the arguments developed in Jabin and Wang (2018)."],"url":"http://arxiv.org/abs/2406.02848v1","category":"math.PR"}
{"created":"2024-06-05 01:47:40","title":"Exact Conversion of In-Context Learning to Model Weights","abstract":"In-Context Learning (ICL) has been a powerful emergent property of large language models that has attracted increasing attention in recent years. In contrast to regular gradient-based learning, ICL is highly interpretable and does not require parameter updates. In this paper, we show that, for linearized transformer networks, ICL can be made explicit and permanent through the inclusion of bias terms. We mathematically demonstrate the equivalence between a model with ICL demonstration prompts and the same model with the additional bias terms. Our algorithm (ICLCA) allows for exact conversion in an inexpensive manner. Existing methods are not exact and require expensive parameter updates. We demonstrate the efficacy of our approach through experiments that show the exact incorporation of ICL tokens into a linear transformer. We further suggest how our method can be adapted to achieve cheap approximate conversion of ICL tokens, even in regular transformer networks that are not linearized. Our experiments on GPT-2 show that, even though the conversion is only approximate, the model still gains valuable context from the included bias terms.","sentences":["In-Context Learning (ICL) has been a powerful emergent property of large language models that has attracted increasing attention in recent years.","In contrast to regular gradient-based learning, ICL is highly interpretable and does not require parameter updates.","In this paper, we show that, for linearized transformer networks, ICL can be made explicit and permanent through the inclusion of bias terms.","We mathematically demonstrate the equivalence between a model with ICL demonstration prompts and the same model with the additional bias terms.","Our algorithm (ICLCA) allows for exact conversion in an inexpensive manner.","Existing methods are not exact and require expensive parameter updates.","We demonstrate the efficacy of our approach through experiments that show the exact incorporation of ICL tokens into a linear transformer.","We further suggest how our method can be adapted to achieve cheap approximate conversion of ICL tokens, even in regular transformer networks that are not linearized.","Our experiments on GPT-2 show that, even though the conversion is only approximate, the model still gains valuable context from the included bias terms."],"url":"http://arxiv.org/abs/2406.02847v1","category":"cs.LG"}
{"created":"2024-06-05 01:05:26","title":"DenoDet: Attention as Deformable Multi-Subspace Feature Denoising for Target Detection in SAR Images","abstract":"Synthetic Aperture Radar (SAR) target detection has long been impeded by inherent speckle noise and the prevalence of diminutive, ambiguous targets. While deep neural networks have advanced SAR target detection, their intrinsic low-frequency bias and static post-training weights falter with coherent noise and preserving subtle details across heterogeneous terrains. Motivated by traditional SAR image denoising, we propose DenoDet, a network aided by explicit frequency domain transform to calibrate convolutional biases and pay more attention to high-frequencies, forming a natural multi-scale subspace representation to detect targets from the perspective of multi-subspace denoising. We design TransDeno, a dynamic frequency domain attention module that performs as a transform domain soft thresholding operation, dynamically denoising across subspaces by preserving salient target signals and attenuating noise. To adaptively adjust the granularity of subspace processing, we also propose a deformable group fully-connected layer (DeGroFC) that dynamically varies the group conditioned on the input features. Without bells and whistles, our plug-and-play TransDeno sets state-of-the-art scores on multiple SAR target detection datasets. The code is available at https://github.com/GrokCV/GrokSAR.","sentences":["Synthetic Aperture Radar (SAR) target detection has long been impeded by inherent speckle noise and the prevalence of diminutive, ambiguous targets.","While deep neural networks have advanced SAR target detection, their intrinsic low-frequency bias and static post-training weights falter with coherent noise and preserving subtle details across heterogeneous terrains.","Motivated by traditional SAR image denoising, we propose DenoDet, a network aided by explicit frequency domain transform to calibrate convolutional biases and pay more attention to high-frequencies, forming a natural multi-scale subspace representation to detect targets from the perspective of multi-subspace denoising.","We design TransDeno, a dynamic frequency domain attention module that performs as a transform domain soft thresholding operation, dynamically denoising across subspaces by preserving salient target signals and attenuating noise.","To adaptively adjust the granularity of subspace processing, we also propose a deformable group fully-connected layer (DeGroFC) that dynamically varies the group conditioned on the input features.","Without bells and whistles, our plug-and-play TransDeno sets state-of-the-art scores on multiple SAR target detection datasets.","The code is available at https://github.com/GrokCV/GrokSAR."],"url":"http://arxiv.org/abs/2406.02833v1","category":"cs.CV"}
{"created":"2024-06-04 19:08:40","title":"Temporal Graph Learning Recurrent Neural Network for Traffic Forecasting","abstract":"Accurate traffic flow forecasting is a crucial research topic in transportation management. However, it is a challenging problem due to rapidly changing traffic conditions, high nonlinearity of traffic flow, and complex spatial and temporal correlations of road networks. Most existing studies either try to capture the spatial dependencies between roads using the same semantic graph over different time steps, or assume all sensors on the roads are equally likely to be connected regardless of the distance between them. However, we observe that the spatial dependencies between roads indeed change over time, and two distant roads are not likely to be helpful to each other when predicting the traffic flow, both of which limit the performance of existing studies. In this paper, we propose Temporal Graph Learning Recurrent Neural Network (TGLRN) to address these problems. More precisely, to effectively model the nature of time series, we leverage Recurrent Neural Networks (RNNs) to dynamically construct a graph at each time step, thereby capturing the time-evolving spatial dependencies between roads (i.e., microscopic view). Simultaneously, we provide the Adaptive Structure Information to the model, ensuring that close and consecutive sensors are considered to be more important for predicting the traffic flow (i.e., macroscopic view). Furthermore, to endow TGLRN with robustness, we introduce an edge sampling strategy when constructing the graph at each time step, which eventually leads to further improvements on the model performance. Experimental results on four commonly used real-world benchmark datasets show the effectiveness of TGLRN.","sentences":["Accurate traffic flow forecasting is a crucial research topic in transportation management.","However, it is a challenging problem due to rapidly changing traffic conditions, high nonlinearity of traffic flow, and complex spatial and temporal correlations of road networks.","Most existing studies either try to capture the spatial dependencies between roads using the same semantic graph over different time steps, or assume all sensors on the roads are equally likely to be connected regardless of the distance between them.","However, we observe that the spatial dependencies between roads indeed change over time, and two distant roads are not likely to be helpful to each other when predicting the traffic flow, both of which limit the performance of existing studies.","In this paper, we propose Temporal Graph Learning Recurrent Neural Network (TGLRN) to address these problems.","More precisely, to effectively model the nature of time series, we leverage Recurrent Neural Networks (RNNs) to dynamically construct a graph at each time step, thereby capturing the time-evolving spatial dependencies between roads (i.e., microscopic view).","Simultaneously, we provide the Adaptive Structure Information to the model, ensuring that close and consecutive sensors are considered to be more important for predicting the traffic flow (i.e., macroscopic view).","Furthermore, to endow TGLRN with robustness, we introduce an edge sampling strategy when constructing the graph at each time step, which eventually leads to further improvements on the model performance.","Experimental results on four commonly used real-world benchmark datasets show the effectiveness of TGLRN."],"url":"http://arxiv.org/abs/2406.02726v1","category":"cs.LG"}
{"created":"2024-06-04 19:08:09","title":"Non-linear microlocal cut-off functors","abstract":"To any conic closed set of a cotangent bundle, one can associate 4 functors on the category of sheaves, which are called non-linear microlocal cut-off functors. Here we explain their relation with the microlocal cut-off functor defined by Kashiwara and Schapira, and prove a microlocal cut-off lemma for non-linear microlocal cut-off functors, adapting inputs from symplectic geometry. We also prove two K\\\"unneth formulas and a functor classification result for categories of sheaves with microsupport conditions.","sentences":["To any conic closed set of a cotangent bundle, one can associate 4 functors on the category of sheaves, which are called non-linear microlocal cut-off functors.","Here we explain their relation with the microlocal cut-off functor defined by Kashiwara and Schapira, and prove a microlocal cut-off lemma for non-linear microlocal cut-off functors, adapting inputs from symplectic geometry.","We also prove two K\\\"unneth formulas and a functor classification result for categories of sheaves with microsupport conditions."],"url":"http://arxiv.org/abs/2406.02725v1","category":"math.SG"}
{"created":"2024-06-04 18:36:11","title":"Window to Wall Ratio Detection using SegFormer","abstract":"Window to Wall Ratios (WWR) are key to assessing the energy, daylight and ventilation performance of buildings. Studies have shown that window area has a large impact on building performance and simulation. However, data to set up these environmental models and simulations is typically not available. Instead, a standard 40% WWR is typically assumed for all buildings. This paper leverages existing computer vision window detection methods to predict WWR of buildings from external street view images using semantic segmentation, demonstrating the potential for adapting established computer vision technique in architectural applications","sentences":["Window to Wall Ratios (WWR) are key to assessing the energy, daylight and ventilation performance of buildings.","Studies have shown that window area has a large impact on building performance and simulation.","However, data to set up these environmental models and simulations is typically not available.","Instead, a standard 40% WWR is typically assumed for all buildings.","This paper leverages existing computer vision window detection methods to predict WWR of buildings from external street view images using semantic segmentation, demonstrating the potential for adapting established computer vision technique in architectural applications"],"url":"http://arxiv.org/abs/2406.02706v1","category":"cs.CV"}
{"created":"2024-06-04 18:34:01","title":"Quantum-enabled continuous microwave-to-optics frequency conversion","abstract":"A quantum interface between microwave and optical photons is essential for entangling remote superconducting quantum processors. To preserve fragile quantum states, a transducer must operate efficiently while generating less than one photon of noise referred to its input. Here, we present a platform that meets these criteria, utilizing a combination of electrostatic and optomechanical interactions in devices made entirely from crystalline silicon. This platform's small mechanical dissipation and low optical absorption enable ground-state radiative cooling, resulting in quantum-enabled operation with a continuous laser drive. Under the optimal settings for high efficiency (low noise), we measure an external efficiency of $2.2\\%$ ($0.47\\%$) and an input-referred added noise of $0.94$ ($0.58$) in microwave-to-optics conversion. We quantify the transducer throughput using the efficiency-bandwidth product, finding it exceeds previous demonstrations with similar noise performance by approximately two orders of magnitude, thereby paving a practical path to interconnecting remote superconducting qubits.","sentences":["A quantum interface between microwave and optical photons is essential for entangling remote superconducting quantum processors.","To preserve fragile quantum states, a transducer must operate efficiently while generating less than one photon of noise referred to its input.","Here, we present a platform that meets these criteria, utilizing a combination of electrostatic and optomechanical interactions in devices made entirely from crystalline silicon.","This platform's small mechanical dissipation and low optical absorption enable ground-state radiative cooling, resulting in quantum-enabled operation with a continuous laser drive.","Under the optimal settings for high efficiency (low noise), we measure an external efficiency of $2.2\\%$ ($0.47\\%$) and an input-referred added noise of $0.94$ ($0.58$) in microwave-to-optics conversion.","We quantify the transducer throughput using the efficiency-bandwidth product, finding it exceeds previous demonstrations with similar noise performance by approximately two orders of magnitude, thereby paving a practical path to interconnecting remote superconducting qubits."],"url":"http://arxiv.org/abs/2406.02704v1","category":"quant-ph"}
{"created":"2024-06-04 18:00:03","title":"Scaling of Disorder Operator and Entanglement Entropy at Easy-Plane Deconfined Quantum Criticalities","abstract":"We systematically investigate the scaling behavior of the disorder operator and the entanglement entropy (EE) of the easy-plane JQ (EPJQ) model at its transition between the antiferromagnetic XY ordered phase (AFXY) and the valence bond solid (VBS) phase. We find $\\mathbf{(1)}$ there exists a finite value of the order parameters at the AFXY-VBS phase transition points of the EPJQ model, and the finite order parameter is strengthened as anisotropy $\\Delta$ varies from the Heisenberg limit ($\\Delta=1$) to the easy-plane limit ($\\Delta=0$); $\\mathbf{(2)}$ Both EE and disorder operator with smooth boundary cut exhibit anomalous scaling behavior at the transition points, resembling the scaling inside the Goldstone model phase, and the anomalous scaling becomes strengthened as the transition becomes more first order; $\\mathbf{(3)}$ First put forward in Ref. [arXiv:2401.12838], with the finite-size corrections in EE for Goldstone phase is properly considered in the fitting form, the anomalous scaling behavior of EE can be adapted with emergent SO(5) symmetry breaking at the Heisenberg limit ($\\Delta=1$). We extend this method in the EPJQ model and observe similar results, which may indicate emergent SO(4) symmetry breaking in the easy-plane regime ($\\Delta<1$) or emergent SO(5) symmetry breaking in the Heisenberg limit ($\\Delta=1$). These observations provide evidence that the N\\'eel-VBS transition in the JQ model setting evolves from weak to prominent first-order transition as the system becomes anisotropic, and the non-local probes such as EE and disorder operator, serve as the sensitive tool to detect such salient yet fundamental features.","sentences":["We systematically investigate the scaling behavior of the disorder operator and the entanglement entropy (EE) of the easy-plane JQ (EPJQ) model at its transition between the antiferromagnetic XY ordered phase (AFXY) and the valence bond solid (VBS) phase.","We find $\\mathbf{(1)}$ there exists a finite value of the order parameters at the AFXY-VBS phase transition points of the EPJQ model, and the finite order parameter is strengthened as anisotropy $\\Delta$ varies from the Heisenberg limit ($\\Delta=1$) to the easy-plane limit ($\\Delta=0$); $\\mathbf{(2)}$ Both EE and disorder operator with smooth boundary cut exhibit anomalous scaling behavior at the transition points, resembling the scaling inside the Goldstone model phase, and the anomalous scaling becomes strengthened as the transition becomes more first order; $\\mathbf{(3)}$ First put forward in Ref.","[arXiv:2401.12838], with the finite-size corrections in EE for Goldstone phase is properly considered in the fitting form, the anomalous scaling behavior of EE can be adapted with emergent SO(5) symmetry breaking at the Heisenberg limit ($\\Delta=1$).","We extend this method in the EPJQ model and observe similar results, which may indicate emergent SO(4) symmetry breaking in the easy-plane regime ($\\Delta<1$) or emergent SO(5) symmetry breaking in the Heisenberg limit ($\\Delta=1$).","These observations provide evidence that the N\\'eel-VBS transition in the JQ model setting evolves from weak to prominent first-order transition as the system becomes anisotropic, and the non-local probes such as EE and disorder operator, serve as the sensitive tool to detect such salient yet fundamental features."],"url":"http://arxiv.org/abs/2406.02681v1","category":"cond-mat.str-el"}
{"created":"2024-06-04 17:29:29","title":"Knotted 4-regular graphs II: Consistent application of the Pachner moves","abstract":"A common choice for the evolution of the knotted graphs in loop quantum gravity is to use the Pachner moves, adapted to graphs from their dual triangulations. Here, we show that the natural way to consistently use these moves is on framed graphs with edge twists, where the Pachner moves can only be performed when the twists, and the vertices the edges are incident on, meet certain criteria. For other twists, one can introduce an algebraic object, which allow any knotted graph with framed edges to be written in terms of a generalized braid group.","sentences":["A common choice for the evolution of the knotted graphs in loop quantum gravity is to use the Pachner moves, adapted to graphs from their dual triangulations.","Here, we show that the natural way to consistently use these moves is on framed graphs with edge twists, where the Pachner moves can only be performed when the twists, and the vertices the edges are incident on, meet certain criteria.","For other twists, one can introduce an algebraic object, which allow any knotted graph with framed edges to be written in terms of a generalized braid group."],"url":"http://arxiv.org/abs/2406.02655v1","category":"gr-qc"}
{"created":"2024-06-04 14:20:38","title":"Keyword-Guided Adaptation of Automatic Speech Recognition","abstract":"Automatic Speech Recognition (ASR) technology has made significant progress in recent years, providing accurate transcription across various domains. However, some challenges remain, especially in noisy environments and specialized jargon. In this paper, we propose a novel approach for improved jargon word recognition by contextual biasing Whisper-based models. We employ a keyword spotting model that leverages the Whisper encoder representation to dynamically generate prompts for guiding the decoder during the transcription process. We introduce two approaches to effectively steer the decoder towards these prompts: KG-Whisper, which is aimed at fine-tuning the Whisper decoder, and KG-Whisper-PT, which learns a prompt prefix. Our results show a significant improvement in the recognition accuracy of specified keywords and in reducing the overall word error rates. Specifically, in unseen language generalization, we demonstrate an average WER improvement of 5.1% over Whisper.","sentences":["Automatic Speech Recognition (ASR) technology has made significant progress in recent years, providing accurate transcription across various domains.","However, some challenges remain, especially in noisy environments and specialized jargon.","In this paper, we propose a novel approach for improved jargon word recognition by contextual biasing Whisper-based models.","We employ a keyword spotting model that leverages the Whisper encoder representation to dynamically generate prompts for guiding the decoder during the transcription process.","We introduce two approaches to effectively steer the decoder towards these prompts: KG-Whisper, which is aimed at fine-tuning the Whisper decoder, and KG-Whisper-PT, which learns a prompt prefix.","Our results show a significant improvement in the recognition accuracy of specified keywords and in reducing the overall word error rates.","Specifically, in unseen language generalization, we demonstrate an average WER improvement of 5.1% over Whisper."],"url":"http://arxiv.org/abs/2406.02649v1","category":"eess.AS"}
{"created":"2024-06-04 13:17:04","title":"Autonomous Adaptive Security Framework for 5G-Enabled IoT","abstract":"In IoT-based critical sectors, 5G can provide more rapid connection speeds, lower latency, faster downloads, and capability to connect more devices due to the introduction of new dynamics such as softwarization and virtualization. 5G-enabled IoT networks increase systems vulnerabilities to security threats due to these dynamics. Consequently, adaptive cybersecurity solutions need to be developed for 5G-enabled IoT applications to protect them against potential cyber-attacks. This task specifies new adaptive strategies of security intelligence with associated scenarios to meet the challenges of 5G-IoT characteristics. In this task we have also developed an autonomous adaptive security framework which can protect 5G-enabaled IoT dynamically and autonomously. The framework is based on a closed feedback loop of advanced analytics to monitor, analyse, and adapt to evolving threats to 5G-enanled IoT applications.","sentences":["In IoT-based critical sectors, 5G can provide more rapid connection speeds, lower latency, faster downloads, and capability to connect more devices due to the introduction of new dynamics such as softwarization and virtualization.","5G-enabled IoT networks increase systems vulnerabilities to security threats due to these dynamics.","Consequently, adaptive cybersecurity solutions need to be developed for 5G-enabled IoT applications to protect them against potential cyber-attacks.","This task specifies new adaptive strategies of security intelligence with associated scenarios to meet the challenges of 5G-IoT characteristics.","In this task we have also developed an autonomous adaptive security framework which can protect 5G-enabaled IoT dynamically and autonomously.","The framework is based on a closed feedback loop of advanced analytics to monitor, analyse, and adapt to evolving threats to 5G-enanled IoT applications."],"url":"http://arxiv.org/abs/2406.03186v1","category":"cs.CR"}
{"created":"2024-06-04 09:15:32","title":"Warm Inflation with Barrow Holographic Dark Energy","abstract":"In this work, we study the warm inflation mechanism in the presence of the Barrow holographic dark energy model. Warm inflation differs from other forms of inflation primarily in that it makes the assumption that radiation and inflaton exist and interact throughout the inflationary process. After the warming process, energy moves from the inflaton to the radiation as a result of the interaction, keeping the cosmos warm. Here we have set up the warm inflationary mechanism using Barrow holographic dark energy as the driving agent. Warm inflation has been explored in a high dissipative regime and interesting results have been obtained. It is seen that the Barrow holographic dark energy can successfully drive a warm inflationary scenario in the early universe. Finally, the model has been compared with the observational data and compliance has been found.","sentences":["In this work, we study the warm inflation mechanism in the presence of the Barrow holographic dark energy model.","Warm inflation differs from other forms of inflation primarily in that it makes the assumption that radiation and inflaton exist and interact throughout the inflationary process.","After the warming process, energy moves from the inflaton to the radiation as a result of the interaction, keeping the cosmos warm.","Here we have set up the warm inflationary mechanism using Barrow holographic dark energy as the driving agent.","Warm inflation has been explored in a high dissipative regime and interesting results have been obtained.","It is seen that the Barrow holographic dark energy can successfully drive a warm inflationary scenario in the early universe.","Finally, the model has been compared with the observational data and compliance has been found."],"url":"http://arxiv.org/abs/2406.02639v1","category":"gr-qc"}
{"created":"2024-06-04 05:36:29","title":"Evidentially Calibrated Source-Free Time-Series Domain Adaptation with Temporal Imputation","abstract":"Source-free domain adaptation (SFDA) aims to adapt a model pre-trained on a labeled source domain to an unlabeled target domain without access to source data, preserving the source domain's privacy. While SFDA is prevalent in computer vision, it remains largely unexplored in time series analysis. Existing SFDA methods, designed for visual data, struggle to capture the inherent temporal dynamics of time series, hindering adaptation performance. This paper proposes MAsk And imPUte (MAPU), a novel and effective approach for time series SFDA. MAPU addresses the critical challenge of temporal consistency by introducing a novel temporal imputation task. This task involves randomly masking time series signals and leveraging a dedicated temporal imputer to recover the original signal within the learned embedding space, bypassing the complexities of noisy raw data. Notably, MAPU is the first method to explicitly address temporal consistency in the context of time series SFDA. Additionally, it offers seamless integration with existing SFDA methods, providing greater flexibility. We further introduce E-MAPU, which incorporates evidential uncertainty estimation to address the overconfidence issue inherent in softmax predictions. To achieve that, we leverage evidential deep learning to obtain a better-calibrated pre-trained model and adapt the target encoder to map out-of-support target samples to a new feature representation closer to the source domain's support. This fosters better alignment, ultimately enhancing adaptation performance. Extensive experiments on five real-world time series datasets demonstrate that both MAPU and E-MAPU achieve significant performance gains compared to existing methods. These results highlight the effectiveness of our proposed approaches for tackling various time series domain adaptation problems.","sentences":["Source-free domain adaptation (SFDA) aims to adapt a model pre-trained on a labeled source domain to an unlabeled target domain without access to source data, preserving the source domain's privacy.","While SFDA is prevalent in computer vision, it remains largely unexplored in time series analysis.","Existing SFDA methods, designed for visual data, struggle to capture the inherent temporal dynamics of time series, hindering adaptation performance.","This paper proposes MAsk And imPUte (MAPU), a novel and effective approach for time series SFDA.","MAPU addresses the critical challenge of temporal consistency by introducing a novel temporal imputation task.","This task involves randomly masking time series signals and leveraging a dedicated temporal imputer to recover the original signal within the learned embedding space, bypassing the complexities of noisy raw data.","Notably, MAPU is the first method to explicitly address temporal consistency in the context of time series SFDA.","Additionally, it offers seamless integration with existing SFDA methods, providing greater flexibility.","We further introduce E-MAPU, which incorporates evidential uncertainty estimation to address the overconfidence issue inherent in softmax predictions.","To achieve that, we leverage evidential deep learning to obtain a better-calibrated pre-trained model and adapt the target encoder to map out-of-support target samples to a new feature representation closer to the source domain's support.","This fosters better alignment, ultimately enhancing adaptation performance.","Extensive experiments on five real-world time series datasets demonstrate that both MAPU and E-MAPU achieve significant performance gains compared to existing methods.","These results highlight the effectiveness of our proposed approaches for tackling various time series domain adaptation problems."],"url":"http://arxiv.org/abs/2406.02635v1","category":"cs.LG"}
{"created":"2024-06-04 03:22:52","title":"Redefining DDoS Attack Detection Using A Dual-Space Prototypical Network-Based Approach","abstract":"Distributed Denial of Service (DDoS) attacks pose an increasingly substantial cybersecurity threat to organizations across the globe. In this paper, we introduce a new deep learning-based technique for detecting DDoS attacks, a paramount cybersecurity challenge with evolving complexity and scale. Specifically, we propose a new dual-space prototypical network that leverages a unique dual-space loss function to enhance detection accuracy for various attack patterns through geometric and angular similarity measures. This approach capitalizes on the strengths of representation learning within the latent space (a lower-dimensional representation of data that captures complex patterns for machine learning analysis), improving the model's adaptability and sensitivity towards varying DDoS attack vectors. Our comprehensive evaluation spans multiple training environments, including offline training, simulated online training, and prototypical network scenarios, to validate the model's robustness under diverse data abundance and scarcity conditions. The Multilayer Perceptron (MLP) with Attention, trained with our dual-space prototypical design over a reduced training set, achieves an average accuracy of 94.85% and an F1-Score of 94.71% across our tests, showcasing its effectiveness in dynamic and constrained real-world scenarios.","sentences":["Distributed Denial of Service (DDoS) attacks pose an increasingly substantial cybersecurity threat to organizations across the globe.","In this paper, we introduce a new deep learning-based technique for detecting DDoS attacks, a paramount cybersecurity challenge with evolving complexity and scale.","Specifically, we propose a new dual-space prototypical network that leverages a unique dual-space loss function to enhance detection accuracy for various attack patterns through geometric and angular similarity measures.","This approach capitalizes on the strengths of representation learning within the latent space (a lower-dimensional representation of data that captures complex patterns for machine learning analysis), improving the model's adaptability and sensitivity towards varying DDoS attack vectors.","Our comprehensive evaluation spans multiple training environments, including offline training, simulated online training, and prototypical network scenarios, to validate the model's robustness under diverse data abundance and scarcity conditions.","The Multilayer Perceptron (MLP) with Attention, trained with our dual-space prototypical design over a reduced training set, achieves an average accuracy of 94.85% and an F1-Score of 94.71% across our tests, showcasing its effectiveness in dynamic and constrained real-world scenarios."],"url":"http://arxiv.org/abs/2406.02632v1","category":"cs.CR"}
{"created":"2024-06-04 00:06:42","title":"Replicability in High Dimensional Statistics","abstract":"The replicability crisis is a major issue across nearly all areas of empirical science, calling for the formal study of replicability in statistics. Motivated in this context, [Impagliazzo, Lei, Pitassi, and Sorrell STOC 2022] introduced the notion of replicable learning algorithms, and gave basic procedures for $1$-dimensional tasks including statistical queries. In this work, we study the computational and statistical cost of replicability for several fundamental high dimensional statistical tasks, including multi-hypothesis testing and mean estimation.   Our main contribution establishes a computational and statistical equivalence between optimal replicable algorithms and high dimensional isoperimetric tilings. As a consequence, we obtain matching sample complexity upper and lower bounds for replicable mean estimation of distributions with bounded covariance, resolving an open problem of [Bun, Gaboardi, Hopkins, Impagliazzo, Lei, Pitassi, Sivakumar, and Sorrell, STOC2023] and for the $N$-Coin Problem, resolving a problem of [Karbasi, Velegkas, Yang, and Zhou, NeurIPS2023] up to log factors.   While our equivalence is computational, allowing us to shave log factors in sample complexity from the best known efficient algorithms, efficient isoperimetric tilings are not known. To circumvent this, we introduce several relaxed paradigms that do allow for sample and computationally efficient algorithms, including allowing pre-processing, adaptivity, and approximate replicability. In these cases we give efficient algorithms matching or beating the best known sample complexity for mean estimation and the coin problem, including a generic procedure that reduces the standard quadratic overhead of replicability to linear in expectation.","sentences":["The replicability crisis is a major issue across nearly all areas of empirical science, calling for the formal study of replicability in statistics.","Motivated in this context, [Impagliazzo, Lei, Pitassi, and Sorrell STOC 2022] introduced the notion of replicable learning algorithms, and gave basic procedures for $1$-dimensional tasks including statistical queries.","In this work, we study the computational and statistical cost of replicability for several fundamental high dimensional statistical tasks, including multi-hypothesis testing and mean estimation.   ","Our main contribution establishes a computational and statistical equivalence between optimal replicable algorithms and high dimensional isoperimetric tilings.","As a consequence, we obtain matching sample complexity upper and lower bounds for replicable mean estimation of distributions with bounded covariance, resolving an open problem of [Bun, Gaboardi, Hopkins, Impagliazzo, Lei, Pitassi, Sivakumar, and Sorrell, STOC2023] and for the $N$-Coin Problem, resolving a problem of [Karbasi, Velegkas, Yang, and Zhou, NeurIPS2023] up to log factors.   ","While our equivalence is computational, allowing us to shave log factors in sample complexity from the best known efficient algorithms, efficient isoperimetric tilings are not known.","To circumvent this, we introduce several relaxed paradigms that do allow for sample and computationally efficient algorithms, including allowing pre-processing, adaptivity, and approximate replicability.","In these cases we give efficient algorithms matching or beating the best known sample complexity for mean estimation and the coin problem, including a generic procedure that reduces the standard quadratic overhead of replicability to linear in expectation."],"url":"http://arxiv.org/abs/2406.02628v1","category":"stat.ML"}
{"created":"2024-06-05 17:59:22","title":"Solving Poisson Equations using Neural Walk-on-Spheres","abstract":"We propose Neural Walk-on-Spheres (NWoS), a novel neural PDE solver for the efficient solution of high-dimensional Poisson equations. Leveraging stochastic representations and Walk-on-Spheres methods, we develop novel losses for neural networks based on the recursive solution of Poisson equations on spheres inside the domain. The resulting method is highly parallelizable and does not require spatial gradients for the loss. We provide a comprehensive comparison against competing methods based on PINNs, the Deep Ritz method, and (backward) stochastic differential equations. In several challenging, high-dimensional numerical examples, we demonstrate the superiority of NWoS in accuracy, speed, and computational costs. Compared to commonly used PINNs, our approach can reduce memory usage and errors by orders of magnitude. Furthermore, we apply NWoS to problems in PDE-constrained optimization and molecular dynamics to show its efficiency in practical applications.","sentences":["We propose Neural Walk-on-Spheres (NWoS), a novel neural PDE solver for the efficient solution of high-dimensional Poisson equations.","Leveraging stochastic representations and Walk-on-Spheres methods, we develop novel losses for neural networks based on the recursive solution of Poisson equations on spheres inside the domain.","The resulting method is highly parallelizable and does not require spatial gradients for the loss.","We provide a comprehensive comparison against competing methods based on PINNs, the Deep Ritz method, and (backward) stochastic differential equations.","In several challenging, high-dimensional numerical examples, we demonstrate the superiority of NWoS in accuracy, speed, and computational costs.","Compared to commonly used PINNs, our approach can reduce memory usage and errors by orders of magnitude.","Furthermore, we apply NWoS to problems in PDE-constrained optimization and molecular dynamics to show its efficiency in practical applications."],"url":"http://arxiv.org/abs/2406.03494v1","category":"cs.LG"}
{"created":"2024-06-05 17:21:25","title":"Fast randomized least-squares solvers can be just as accurate and stable as classical direct solvers","abstract":"One of the greatest success stories of randomized algorithms for linear algebra has been the development of fast, randomized algorithms for highly overdetermined linear least-squares problems. However, none of the existing algorithms is backward stable, preventing them from being deployed as drop-in replacements for existing QR-based solvers. This paper introduces FOSSILS, a fast, provably backward stable randomized least-squares solver. FOSSILS combines iterative refinement with a preconditioned iterative method applied to the normal equations, and it converges at the same rate as existing randomized least-squares solvers. This work offers the promise of incorporating randomized least-squares solvers into existing software libraries while maintaining the same level of accuracy and stability as classical solvers.","sentences":["One of the greatest success stories of randomized algorithms for linear algebra has been the development of fast, randomized algorithms for highly overdetermined linear least-squares problems.","However, none of the existing algorithms is backward stable, preventing them from being deployed as drop-in replacements for existing QR-based solvers.","This paper introduces FOSSILS, a fast, provably backward stable randomized least-squares solver.","FOSSILS combines iterative refinement with a preconditioned iterative method applied to the normal equations, and it converges at the same rate as existing randomized least-squares solvers.","This work offers the promise of incorporating randomized least-squares solvers into existing software libraries while maintaining the same level of accuracy and stability as classical solvers."],"url":"http://arxiv.org/abs/2406.03468v1","category":"math.NA"}
{"created":"2024-06-05 16:33:30","title":"Analytical Survival Analysis of the Non-autonomous Ornstein-Uhlenbeck Process","abstract":"The survival probability for a periodic non-autonomous Ornstein-Uhlenbeck process is calculated analytically using two different methods. The first uses an asymptotic approach. We treat the associated Kolmogorov Backward Equation with an absorbing boundary by dividing the domain into an interior region, centered around the origin, and a \"boundary layer\" near the absorbing boundary. In each region we determine the leading-order analytical solutions, and construct a uniformly valid solution over the entire domain using asymptotic matching. In the second method we examine the integral relationship between the probability density function and the mean first passage time probability density function. These allow us to determine approximate analytical forms for the exit rate. The validity of the solutions derived from both methods is assessed numerically, and we find the asymptotic method to be superior.","sentences":["The survival probability for a periodic non-autonomous Ornstein-Uhlenbeck process is calculated analytically using two different methods.","The first uses an asymptotic approach.","We treat the associated Kolmogorov Backward Equation with an absorbing boundary by dividing the domain into an interior region, centered around the origin, and a \"boundary layer\" near the absorbing boundary.","In each region we determine the leading-order analytical solutions, and construct a uniformly valid solution over the entire domain using asymptotic matching.","In the second method we examine the integral relationship between the probability density function and the mean first passage time probability density function.","These allow us to determine approximate analytical forms for the exit rate.","The validity of the solutions derived from both methods is assessed numerically, and we find the asymptotic method to be superior."],"url":"http://arxiv.org/abs/2406.03436v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-05 16:03:17","title":"Non-stationary Spatio-Temporal Modeling Using the Stochastic Advection-Diffusion Equation","abstract":"We construct flexible spatio-temporal models through stochastic partial differential equations (SPDEs) where both diffusion and advection can be spatially varying. Computations are done through a Gaussian Markov random field approximation of the solution of the SPDE, which is constructed through a finite volume method. The new flexible non-separable model is compared to a flexible separable model both for reconstruction and forecasting and evaluated in terms of root mean square errors and continuous rank probability scores. A simulation study demonstrates that the non-separable model performs better when the data is simulated with non-separable effects such as diffusion and advection. Further, we estimate surrogate models for emulating the output of a ocean model in Trondheimsfjorden, Norway, and simulate observations of autonoumous underwater vehicles. The results show that the flexible non-separable model outperforms the flexible separable model for real-time prediction of unobserved locations.","sentences":["We construct flexible spatio-temporal models through stochastic partial differential equations (SPDEs) where both diffusion and advection can be spatially varying.","Computations are done through a Gaussian Markov random field approximation of the solution of the SPDE, which is constructed through a finite volume method.","The new flexible non-separable model is compared to a flexible separable model both for reconstruction and forecasting and evaluated in terms of root mean square errors and continuous rank probability scores.","A simulation study demonstrates that the non-separable model performs better when the data is simulated with non-separable effects such as diffusion and advection.","Further, we estimate surrogate models for emulating the output of a ocean model in Trondheimsfjorden, Norway, and simulate observations of autonoumous underwater vehicles.","The results show that the flexible non-separable model outperforms the flexible separable model for real-time prediction of unobserved locations."],"url":"http://arxiv.org/abs/2406.03400v1","category":"stat.ME"}
{"created":"2024-06-05 15:43:14","title":"On the embedding between the variable Lebesgue space $L^{p(\\cdot)}(\u03a9)$ and the Orlicz space $L(\\log L)^\u03b1(\u03a9)$","abstract":"We give a sharp sufficient condition on the distribution function, $|\\{x\\in \\Omega :\\,p(x)\\leq 1+\\lambda\\}|$, $\\lambda>0$, of the exponent function $p(\\cdot): \\Omega \\to [1,\\infty)$ that implies the embedding of the variable Lebesgue space $L^{p(\\cdot)}(\\Omega)$ into the Orlicz space $L(\\log L)^{\\alpha}(\\Omega)$, $\\alpha>0$, where $\\Omega$ is an open set with finite Lebesgue measure. As applications of our results, we first give conditions that imply the strong differentiation of integrals of functions in $L^{p(\\cdot)}((0,1)^{n})$, $n>1$. We then consider the integrability of the maximal function on variable Lebesgue spaces, where the exponent function $p(\\cdot)$ approaches $1$ in value on some part of the domain. This result is an improvement of the result in~\\cite{CUF2}.","sentences":["We give a sharp sufficient condition on the distribution function, $|\\{x\\in \\Omega :\\,p(x)\\leq 1+\\lambda\\}|$, $\\lambda>0$, of the exponent function $p(\\cdot): \\Omega","\\to","[1,\\infty)$ that implies the embedding of the variable Lebesgue space $L^{p(\\cdot)}(\\Omega)$ into the Orlicz space $L(\\log L)^{\\alpha}(\\Omega)$, $\\alpha>0$, where $\\Omega$ is an open set with finite Lebesgue measure.","As applications of our results, we first give conditions that imply the strong differentiation of integrals of functions in $L^{p(\\cdot)}((0,1)^{n})$, $n>1$. We then consider the integrability of the maximal function on variable Lebesgue spaces, where the exponent function $p(\\cdot)$ approaches $1$ in value on some part of the domain.","This result is an improvement of the result in~\\cite{CUF2}."],"url":"http://arxiv.org/abs/2406.03392v1","category":"math.CA"}
{"created":"2024-06-05 15:36:57","title":"Learning Long Range Dependencies on Graphs via Random Walks","abstract":"Message-passing graph neural networks (GNNs), while excelling at capturing local relationships, often struggle with long-range dependencies on graphs. Conversely, graph transformers (GTs) enable information exchange between all nodes but oversimplify the graph structure by treating them as a set of fixed-length vectors. This work proposes a novel architecture, NeuralWalker, that overcomes the limitations of both methods by combining random walks with message passing. NeuralWalker achieves this by treating random walks as sequences, allowing for the application of recent advances in sequence models in order to capture long-range dependencies within these walks. Based on this concept, we propose a framework that offers (1) more expressive graph representations through random walk sequences, (2) the ability to utilize any sequence model for capturing long-range dependencies, and (3) the flexibility by integrating various GNN and GT architectures. Our experimental evaluations demonstrate that NeuralWalker achieves significant performance improvements on 19 graph and node benchmark datasets, notably outperforming existing methods by up to 13% on the PascalVoc-SP and COCO-SP datasets. Code is available at https://github.com/BorgwardtLab/NeuralWalker.","sentences":["Message-passing graph neural networks (GNNs), while excelling at capturing local relationships, often struggle with long-range dependencies on graphs.","Conversely, graph transformers (GTs) enable information exchange between all nodes but oversimplify the graph structure by treating them as a set of fixed-length vectors.","This work proposes a novel architecture, NeuralWalker, that overcomes the limitations of both methods by combining random walks with message passing.","NeuralWalker achieves this by treating random walks as sequences, allowing for the application of recent advances in sequence models in order to capture long-range dependencies within these walks.","Based on this concept, we propose a framework that offers (1) more expressive graph representations through random walk sequences, (2) the ability to utilize any sequence model for capturing long-range dependencies, and (3) the flexibility by integrating various GNN and GT architectures.","Our experimental evaluations demonstrate that NeuralWalker achieves significant performance improvements on 19 graph and node benchmark datasets, notably outperforming existing methods by up to 13% on the PascalVoc-SP and COCO-SP datasets.","Code is available at https://github.com/BorgwardtLab/NeuralWalker."],"url":"http://arxiv.org/abs/2406.03386v1","category":"cs.LG"}
{"created":"2024-06-05 15:06:31","title":"Input of the Coulomb law modification to the Lamb shift of the hydrogen atom","abstract":"Radiative corrections which remove accidental degeneracy in the spectrum of the relativistic hydrogen atom and lead to the modification of the Coulomb law, are calculated within the novel approach, based on the exact solution of the Dirac equation with the Coulomb potential. The energy spectrum of the hydrogen atom is obtained with account of these corrections and the Lamb shift is calculated for the lowest energy states.","sentences":["Radiative corrections which remove accidental degeneracy in the spectrum of the relativistic hydrogen atom and lead to the modification of the Coulomb law, are calculated within the novel approach, based on the exact solution of the Dirac equation with the Coulomb potential.","The energy spectrum of the hydrogen atom is obtained with account of these corrections and the Lamb shift is calculated for the lowest energy states."],"url":"http://arxiv.org/abs/2406.03350v1","category":"quant-ph"}
{"created":"2024-06-05 14:44:16","title":"Nonadiabatic Dynamics of Molecules Interacting with Metal Surfaces: Extending the Hierarchical Equations of Motion and Langevin Dynamics Approach to Position-Dependent Metal-Molecule Couplings","abstract":"Electronic friction and Langevin dynamics is a popular mixed quantum-classical method for simulating the nonadiabatic dynamics of molecules interacting with metal surfaces, as it can be computationally more efficient than fully quantum approaches. Previous approaches to calculating the electronic friction and other forces, however, have been limited to either noninteracting molecular models or position-independent metal-molecule couplings. In this work, we extend the theory of electronic friction within the hierarchical equations of motion formalism to models with a position-dependent metal-molecule coupling. We show that the addition of a position-dependent metal-molecule coupling adds new contributions to the electronic friction and other forces, which are highly relevant for many physical processes. Our expressions for the electronic forces within the Langevin equation are valid both in and out of equilibrium and for molecular models containing strong interactions. We demonstrate the approach by applying it to different models of interest.","sentences":["Electronic friction and Langevin dynamics is a popular mixed quantum-classical method for simulating the nonadiabatic dynamics of molecules interacting with metal surfaces, as it can be computationally more efficient than fully quantum approaches.","Previous approaches to calculating the electronic friction and other forces, however, have been limited to either noninteracting molecular models or position-independent metal-molecule couplings.","In this work, we extend the theory of electronic friction within the hierarchical equations of motion formalism to models with a position-dependent metal-molecule coupling.","We show that the addition of a position-dependent metal-molecule coupling adds new contributions to the electronic friction and other forces, which are highly relevant for many physical processes.","Our expressions for the electronic forces within the Langevin equation are valid both in and out of equilibrium and for molecular models containing strong interactions.","We demonstrate the approach by applying it to different models of interest."],"url":"http://arxiv.org/abs/2406.03329v1","category":"physics.chem-ph"}
{"created":"2024-06-05 14:41:58","title":"Neural density functionals: Local learning and pair-correlation matching","abstract":"Recently Dijkman et al. (arxiv:2403.15007) proposed training classical neural density functionals via bulk pair-correlation matching. We show their method to be an efficient regularizer for neural functionals based on local learning of inhomogeneous one-body direct correlations [Samm\\\"uller et al., Proc. Natl. Acad. Sci. 120, e2312484120 (2023)]. We demonstrate that local one-body learning allows flexible neural modelling of the full Mermin-Evans density functional map, but that bulk pair-correlation matching alone does not. Using spatial localization gives access to accurate neural free energy functionals, including convolutional neural networks, that transcend the training box.","sentences":["Recently Dijkman et al. (arxiv:2403.15007) proposed training classical neural density functionals via bulk pair-correlation matching.","We show their method to be an efficient regularizer for neural functionals based on local learning of inhomogeneous one-body direct correlations [Samm\\\"uller et al., Proc.","Natl.","Acad.","Sci. 120, e2312484120 (2023)].","We demonstrate that local one-body learning allows flexible neural modelling of the full Mermin-Evans density functional map, but that bulk pair-correlation matching alone does not.","Using spatial localization gives access to accurate neural free energy functionals, including convolutional neural networks, that transcend the training box."],"url":"http://arxiv.org/abs/2406.03327v1","category":"cond-mat.soft"}
{"created":"2024-06-05 14:41:41","title":"Calibrated absolute optical contrast for high-throughput characterization of horizontally aligned carbon nanotube arrays","abstract":"Horizontally aligned carbon nanotube (HACNT) arrays hold significant potential for various applications in nanoelectronics and material science. However, their high-throughput characterization remains challenging due to the lack of methods with both high efficiency and high accuracy. Here, we present a novel technique, Calibrated Absolute Optical Contrast (CAOC), achieved through the implementation of differential principles to filter out stray signals and high-resolution calibration to endow optical contrast with physical significance. CAOC offers major advantages over previous characterization techniques, providing consistent and reliable measurements of HACNT array density with high throughput and non-destructive assessment. To validate its utility, we demonstrate wafer-scale uniformity assessment by rapid density mapping. This technique not only facilitates the practical evaluation of HACNT arrays but also provides insights into balancing high throughput and high resolution in nanomaterial characterization.","sentences":["Horizontally aligned carbon nanotube (HACNT) arrays hold significant potential for various applications in nanoelectronics and material science.","However, their high-throughput characterization remains challenging due to the lack of methods with both high efficiency and high accuracy.","Here, we present a novel technique, Calibrated Absolute Optical Contrast (CAOC), achieved through the implementation of differential principles to filter out stray signals and high-resolution calibration to endow optical contrast with physical significance.","CAOC offers major advantages over previous characterization techniques, providing consistent and reliable measurements of HACNT array density with high throughput and non-destructive assessment.","To validate its utility, we demonstrate wafer-scale uniformity assessment by rapid density mapping.","This technique not only facilitates the practical evaluation of HACNT arrays but also provides insights into balancing high throughput and high resolution in nanomaterial characterization."],"url":"http://arxiv.org/abs/2406.03326v1","category":"physics.app-ph"}
{"created":"2024-06-05 14:07:38","title":"Functional calculus on weighted Sobolev spaces for the Laplacian on the half-space","abstract":"In this paper, we consider the Laplace operator on the half-space with Dirichlet and Neumann boundary conditions. We prove that this operator admits a bounded $H^\\infty$-calculus on Sobolev spaces with power weights measuring the distance to the boundary. These weights do not necessarily belong to the class of Muckenhoupt $A_p$ weights. We additionally study the corresponding Dirichlet and Neumann heat semigroup. It is shown that these semigroups, in contrast to the $L^p$-case, have polynomial growth. Moreover, maximal regularity results for the heat equation are derived on inhomogeneous and homogeneous weighted Sobolev spaces.","sentences":["In this paper, we consider the Laplace operator on the half-space with Dirichlet and Neumann boundary conditions.","We prove that this operator admits a bounded $H^\\infty$-calculus on Sobolev spaces with power weights measuring the distance to the boundary.","These weights do not necessarily belong to the class of Muckenhoupt $A_p$ weights.","We additionally study the corresponding Dirichlet and Neumann heat semigroup.","It is shown that these semigroups, in contrast to the $L^p$-case, have polynomial growth.","Moreover, maximal regularity results for the heat equation are derived on inhomogeneous and homogeneous weighted Sobolev spaces."],"url":"http://arxiv.org/abs/2406.03297v1","category":"math.FA"}
{"created":"2024-06-05 13:34:11","title":"What excites the optical emission in X-ray-selected galaxies?","abstract":"We present a study of $1347$ galaxies at $z<0.35$ with detected nuclear X-ray emission and optical emission line diagnostics in the Baldwin-Phillips-Terlevich (BPT) diagram. This sample was obtained by cross-matching the X-ray Multi-Mirror Mission Observatory - Newton (XMM-Newton) DR10 catalogue with Sloan Digital Sky Survey (SDSS) DR17 galaxies with well-measured line ratios. The distribution of these sources in the BPT diagram covers all three excitation regimes: Ionized Hydrogen (HII) regions (23\\%), `composites' (30\\%), and Seyfert galaxies with the low ionization nuclear emission line regions (LINERs) (47\\%). In contrast, the fraction of objects classified as active galactic nuclei (AGN) in the SDSS subsample selected for cross-match with XMM-Newton is only 13\\%. This fact illustrates that X-ray emission from galaxies commonly points towards the presence of AGN. Our data show, for the first time, a clear dependence of the BPT position on the ratio of the X-ray to $H\\alpha$ fluxes. Sources dominated by X-ray emission lie in the Seyfert and LINER regimes of the BPT diagram. Most sources with a low X-ray-to-$H\\alpha$-luminosity ratio, $log_{10}(L_X/L_{H\\alpha}) < 1.0$, lie in the HII regime. In our sample, there are even 45 galaxies that have $L^{Star}_{XR}/L^{Total}_{Xray}>0.5$. In contrast, the positions of the sample members in the BPT diagram exhibit {no} dependence on the X-ray hardness ratio. Our finding suggests that the X-ray-to-$H\\alpha$ ratio can help us to differentiate galaxies whose X-ray flux is dominated by an AGN {from galaxies with} central X-ray binaries and other stellar X-ray sources.","sentences":["We present a study of $1347$ galaxies at $z<0.35$ with detected nuclear X-ray emission and optical emission line diagnostics in the Baldwin-Phillips-Terlevich (BPT) diagram.","This sample was obtained by cross-matching the X-ray Multi-Mirror Mission Observatory - Newton (XMM-Newton)","DR10 catalogue with Sloan Digital Sky Survey (SDSS)","DR17 galaxies with well-measured line ratios.","The distribution of these sources in the BPT diagram covers all three excitation regimes: Ionized Hydrogen (HII) regions (23\\%), `composites' (30\\%), and Seyfert galaxies with the low ionization nuclear emission line regions (LINERs) (47\\%).","In contrast, the fraction of objects classified as active galactic nuclei (AGN) in the SDSS subsample selected for cross-match with XMM-Newton is only 13\\%.","This fact illustrates that X-ray emission from galaxies commonly points towards the presence of AGN.","Our data show, for the first time, a clear dependence of the BPT position on the ratio of the X-ray to $H\\alpha$ fluxes.","Sources dominated by X-ray emission lie in the Seyfert and LINER regimes of the BPT diagram.","Most sources with a low X-ray-to-$H\\alpha$-luminosity ratio, $log_{10}(L_X/L_{H\\alpha}) < 1.0$, lie in the HII regime.","In our sample, there are even 45 galaxies that have $L^{Star}_{XR}/L^{Total}_{Xray}>0.5$. In contrast, the positions of the sample members in the BPT diagram exhibit {no} dependence on the X-ray hardness ratio.","Our finding suggests that the X-ray-to-$H\\alpha$ ratio can help us to differentiate galaxies whose X-ray flux is dominated by an AGN {from galaxies with} central X-ray binaries and other stellar X-ray sources."],"url":"http://arxiv.org/abs/2406.03254v1","category":"astro-ph.GA"}
{"created":"2024-06-05 13:30:45","title":"Continuous-time modeling and bootstrap for chain ladder reserving","abstract":"We revisit the famous Mack's model which gives an estimate for the mean square error of prediction of the chain ladder claims reserves. We introduce a stochastic differential equation driven by a Brownian motion to model accumulated total claims amount for the chain ladder method. Within this continuous-time framework, we propose a bootstrap technique for estimating the distribution of claims reserves. It turns out that our approach leads to inherently capturing asymmetry and non-negativity, eliminating the necessity for additional assumptions. We conclude with a case study and comparative analysis against alternative methodologies based on Mack's model.","sentences":["We revisit the famous Mack's model which gives an estimate for the mean square error of prediction of the chain ladder claims reserves.","We introduce a stochastic differential equation driven by a Brownian motion to model accumulated total claims amount for the chain ladder method.","Within this continuous-time framework, we propose a bootstrap technique for estimating the distribution of claims reserves.","It turns out that our approach leads to inherently capturing asymmetry and non-negativity, eliminating the necessity for additional assumptions.","We conclude with a case study and comparative analysis against alternative methodologies based on Mack's model."],"url":"http://arxiv.org/abs/2406.03252v1","category":"stat.ME"}
{"created":"2024-06-05 12:50:07","title":"Study of hybrid stars with nonstrange quark matter cores","abstract":"In this work, we use a modified 2-flavor Nambu-Jona-Lasinio model to study hybrid stars with a hypothesis that the quark matter may not be strange [Phys. Rev. Lett. 120, 222001 (2018)]. To obtain hybrid equation of states, the Maxwell construction is used to describe the first-order confinement-deconfinement phase transition in hybrid stars. With recent measurements on neutron star mass, radius, and tidal deformability, the hybrid equation of states are constrained. The result suggests that pure nonstrange quark matter cores can exist in hybrid stars, possessing 0.014-0.026 solar mass, and the binary neutron stars in GW170817 may be hadron stars.","sentences":["In this work, we use a modified 2-flavor Nambu-Jona-Lasinio model to study hybrid stars with a hypothesis that the quark matter may not be strange [Phys. Rev. Lett.","120, 222001 (2018)].","To obtain hybrid equation of states, the Maxwell construction is used to describe the first-order confinement-deconfinement phase transition in hybrid stars.","With recent measurements on neutron star mass, radius, and tidal deformability, the hybrid equation of states are constrained.","The result suggests that pure nonstrange quark matter cores can exist in hybrid stars, possessing 0.014-0.026 solar mass, and the binary neutron stars in GW170817 may be hadron stars."],"url":"http://arxiv.org/abs/2406.03211v1","category":"nucl-th"}
{"created":"2024-06-05 12:25:14","title":"Chemically Regulated Conical Channel Synapse for Neuromorphic and Sensing Applications","abstract":"Fluidic iontronics offer a unique capability for emulating the chemical processes found in neurons. We extract multiple distinct chemically regulated synaptic features from a single conical microfluidic channel carrying functionalized surface groups, using finite-element calculations of continuum transport equations. Such channels have long been employed for fluidic sensing and are therefore experimentally well established. By modeling a Langmuir-type surface reaction on the channel wall we couple fast voltage-induced volumetric salt accumulation with a long-term channel surface charge modulation by means of fast charging and slow discharging. These nonlinear charging dynamics are understood through an analytic approximation rooted in first-principles. We show how short-and long-term potentiation and depression, frequency-dependent plasticity, and chemical-electrical signal coincidence detection (acting like a chemical-electrical AND logic gate), akin to the NMDA mechanism for Hebbian learning in biological synapses, can all be emulated with a single channel.","sentences":["Fluidic iontronics offer a unique capability for emulating the chemical processes found in neurons.","We extract multiple distinct chemically regulated synaptic features from a single conical microfluidic channel carrying functionalized surface groups, using finite-element calculations of continuum transport equations.","Such channels have long been employed for fluidic sensing and are therefore experimentally well established.","By modeling a Langmuir-type surface reaction on the channel wall we couple fast voltage-induced volumetric salt accumulation with a long-term channel surface charge modulation by means of fast charging and slow discharging.","These nonlinear charging dynamics are understood through an analytic approximation rooted in first-principles.","We show how short-and long-term potentiation and depression, frequency-dependent plasticity, and chemical-electrical signal coincidence detection (acting like a chemical-electrical AND logic gate), akin to the NMDA mechanism for Hebbian learning in biological synapses, can all be emulated with a single channel."],"url":"http://arxiv.org/abs/2406.03195v1","category":"cond-mat.soft"}
{"created":"2024-06-05 12:07:39","title":"Dynamic 3D Gaussian Fields for Urban Areas","abstract":"We present an efficient neural 3D scene representation for novel-view synthesis (NVS) in large-scale, dynamic urban areas. Existing works are not well suited for applications like mixed-reality or closed-loop simulation due to their limited visual quality and non-interactive rendering speeds. Recently, rasterization-based approaches have achieved high-quality NVS at impressive speeds. However, these methods are limited to small-scale, homogeneous data, i.e. they cannot handle severe appearance and geometry variations due to weather, season, and lighting and do not scale to larger, dynamic areas with thousands of images. We propose 4DGF, a neural scene representation that scales to large-scale dynamic urban areas, handles heterogeneous input data, and substantially improves rendering speeds. We use 3D Gaussians as an efficient geometry scaffold while relying on neural fields as a compact and flexible appearance model. We integrate scene dynamics via a scene graph at global scale while modeling articulated motions on a local level via deformations. This decomposed approach enables flexible scene composition suitable for real-world applications. In experiments, we surpass the state-of-the-art by over 3 dB in PSNR and more than 200 times in rendering speed.","sentences":["We present an efficient neural 3D scene representation for novel-view synthesis (NVS) in large-scale, dynamic urban areas.","Existing works are not well suited for applications like mixed-reality or closed-loop simulation due to their limited visual quality and non-interactive rendering speeds.","Recently, rasterization-based approaches have achieved high-quality NVS at impressive speeds.","However, these methods are limited to small-scale, homogeneous data, i.e. they cannot handle severe appearance and geometry variations due to weather, season, and lighting and do not scale to larger, dynamic areas with thousands of images.","We propose 4DGF, a neural scene representation that scales to large-scale dynamic urban areas, handles heterogeneous input data, and substantially improves rendering speeds.","We use 3D Gaussians as an efficient geometry scaffold while relying on neural fields as a compact and flexible appearance model.","We integrate scene dynamics via a scene graph at global scale while modeling articulated motions on a local level via deformations.","This decomposed approach enables flexible scene composition suitable for real-world applications.","In experiments, we surpass the state-of-the-art by over 3 dB in PSNR and more than 200 times in rendering speed."],"url":"http://arxiv.org/abs/2406.03175v1","category":"cs.CV"}
{"created":"2024-06-05 12:06:04","title":"Multi-Task Multi-Scale Contrastive Knowledge Distillation for Efficient Medical Image Segmentation","abstract":"This thesis aims to investigate the feasibility of knowledge transfer between neural networks for medical image segmentation tasks, specifically focusing on the transfer from a larger multi-task \"Teacher\" network to a smaller \"Student\" network. In the context of medical imaging, where the data volumes are often limited, leveraging knowledge from a larger pre-trained network could be useful. The primary objective is to enhance the performance of a smaller student model by incorporating knowledge representations acquired by a teacher model that adopts a multi-task pre-trained architecture trained on CT images, to a more resource-efficient student network, which can essentially be a smaller version of the same, trained on a mere 50% of the data than that of the teacher model.   To facilitate knowledge transfer between the two models, we devised an architecture incorporating multi-scale feature distillation and supervised contrastive learning. Our study aims to improve the student model's performance by integrating knowledge representations from the teacher model. We investigate whether this approach is particularly effective in scenarios with limited computational resources and limited training data availability. To assess the impact of multi-scale feature distillation, we conducted extensive experiments. We also conducted a detailed ablation study to determine whether it is essential to distil knowledge at various scales, including low-level features from encoder layers, for effective knowledge transfer. In addition, we examine different losses in the knowledge distillation process to gain insights into their effects on overall performance.","sentences":["This thesis aims to investigate the feasibility of knowledge transfer between neural networks for medical image segmentation tasks, specifically focusing on the transfer from a larger multi-task \"Teacher\" network to a smaller \"Student\" network.","In the context of medical imaging, where the data volumes are often limited, leveraging knowledge from a larger pre-trained network could be useful.","The primary objective is to enhance the performance of a smaller student model by incorporating knowledge representations acquired by a teacher model that adopts a multi-task pre-trained architecture trained on CT images, to a more resource-efficient student network, which can essentially be a smaller version of the same, trained on a mere 50% of the data than that of the teacher model.   ","To facilitate knowledge transfer between the two models, we devised an architecture incorporating multi-scale feature distillation and supervised contrastive learning.","Our study aims to improve the student model's performance by integrating knowledge representations from the teacher model.","We investigate whether this approach is particularly effective in scenarios with limited computational resources and limited training data availability.","To assess the impact of multi-scale feature distillation, we conducted extensive experiments.","We also conducted a detailed ablation study to determine whether it is essential to distil knowledge at various scales, including low-level features from encoder layers, for effective knowledge transfer.","In addition, we examine different losses in the knowledge distillation process to gain insights into their effects on overall performance."],"url":"http://arxiv.org/abs/2406.03173v1","category":"eess.IV"}
{"created":"2024-06-05 12:03:45","title":"Initialization-enhanced Physics-Informed Neural Network with Domain Decomposition (IDPINN)","abstract":"We propose a new physics-informed neural network framework, IDPINN, based on the enhancement of initialization and domain decomposition to improve prediction accuracy. We train a PINN using a small dataset to obtain an initial network structure, including the weighted matrix and bias, which initializes the PINN for each subdomain. Moreover, we leverage the smoothness condition on the interface to enhance the prediction performance. We numerically evaluated it on several forward problems and demonstrated the benefits of IDPINN in terms of accuracy.","sentences":["We propose a new physics-informed neural network framework, IDPINN, based on the enhancement of initialization and domain decomposition to improve prediction accuracy.","We train a PINN using a small dataset to obtain an initial network structure, including the weighted matrix and bias, which initializes the PINN for each subdomain.","Moreover, we leverage the smoothness condition on the interface to enhance the prediction performance.","We numerically evaluated it on several forward problems and demonstrated the benefits of IDPINN in terms of accuracy."],"url":"http://arxiv.org/abs/2406.03172v1","category":"cs.LG"}
{"created":"2024-06-05 10:30:48","title":"Skew scattering and ratchet effect in photonic graphene","abstract":"In this paper we bring up our investigation on the properties of asymmetric scattering in graphene made with the microcavity exciton-polaritons. It is shown by numerical simulations of Gross-Pitaevskii equation that ratchet phenomena occurs in the polariton graphene due to its structural inhomogeneity. Thus, we suggest a way to induce preferential motion of polaritons by the confinement potential engineering.","sentences":["In this paper we bring up our investigation on the properties of asymmetric scattering in graphene made with the microcavity exciton-polaritons.","It is shown by numerical simulations of Gross-Pitaevskii equation that ratchet phenomena occurs in the polariton graphene due to its structural inhomogeneity.","Thus, we suggest a way to induce preferential motion of polaritons by the confinement potential engineering."],"url":"http://arxiv.org/abs/2406.03132v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-05 09:39:15","title":"Learning to see R-parity violating scalar top decays","abstract":"With this article we introduce recent, improved machine learning methods from computer vision to the problem of event classification in particle physics. Supersymmetric scalar top decays to top quarks and weak scale bino-like neutralinos, where the neutralinos decay via the $UDD$ operator to three quarks, are difficult to search for and therefore weakly constrained. The jet substructure of the boosted decay products can be used to differentiate signal from background events. We apply transformer-based computer vision models CoAtNet and MaxViT to images built from jet constituents and compare the classification performance to a more classical convolutional neural network (CNN). We find that results from computer vision translate well onto physics applications and both transformer-based models perform better than the CNN. By replacing the CNN with MaxViT we find an improvement of $S/\\sqrt{B}$ by a factor of almost 2 for some neutralino masses. We show that combining this classifier with additional features results in a strong separation of background and signal. We also find that replacing a CNN with a MaxViT model in a simple mock analysis can push the 95% C.L. exclusion limit of stop masses by about $100$ GeV and $60$ GeV for neutralino masses of $100$ GeV and $500$ GeV.","sentences":["With this article we introduce recent, improved machine learning methods from computer vision to the problem of event classification in particle physics.","Supersymmetric scalar top decays to top quarks and weak scale bino-like neutralinos, where the neutralinos decay via the $UDD$ operator to three quarks, are difficult to search for and therefore weakly constrained.","The jet substructure of the boosted decay products can be used to differentiate signal from background events.","We apply transformer-based computer vision models CoAtNet and MaxViT to images built from jet constituents and compare the classification performance to a more classical convolutional neural network (CNN).","We find that results from computer vision translate well onto physics applications and both transformer-based models perform better than the CNN.","By replacing the CNN with MaxViT we find an improvement of $S/\\sqrt{B}$ by a factor of almost 2 for some neutralino masses.","We show that combining this classifier with additional features results in a strong separation of background and signal.","We also find that replacing a CNN with a MaxViT model in a simple mock analysis can push the 95% C.L. exclusion limit of stop masses by about $100$ GeV and $60$ GeV for neutralino masses of $100$ GeV and $500$ GeV."],"url":"http://arxiv.org/abs/2406.03096v1","category":"hep-ph"}
{"created":"2024-06-05 09:25:18","title":"HASS: Hardware-Aware Sparsity Search for Dataflow DNN Accelerator","abstract":"Deep Neural Networks (DNNs) excel in learning hierarchical representations from raw data, such as images, audio, and text. To compute these DNN models with high performance and energy efficiency, these models are usually deployed onto customized hardware accelerators. Among various accelerator designs, dataflow architecture has shown promising performance due to its layer-pipelined structure and its scalability in data parallelism.   Exploiting weights and activations sparsity can further enhance memory storage and computation efficiency. However, existing approaches focus on exploiting sparsity in non-dataflow accelerators, which cannot be applied onto dataflow accelerators because of the large hardware design space introduced. As such, this could miss opportunities to find an optimal combination of sparsity features and hardware designs.   In this paper, we propose a novel approach to exploit unstructured weights and activations sparsity for dataflow accelerators, using software and hardware co-optimization. We propose a Hardware-Aware Sparsity Search (HASS) to systematically determine an efficient sparsity solution for dataflow accelerators. Over a set of models, we achieve an efficiency improvement ranging from 1.3$\\times$ to 4.2$\\times$ compared to existing sparse designs, which are either non-dataflow or non-hardware-aware. Particularly, the throughput of MobileNetV3 can be optimized to 4895 images per second. HASS is open-source: \\url{https://github.com/Yu-Zhewen/HASS}","sentences":["Deep Neural Networks (DNNs) excel in learning hierarchical representations from raw data, such as images, audio, and text.","To compute these DNN models with high performance and energy efficiency, these models are usually deployed onto customized hardware accelerators.","Among various accelerator designs, dataflow architecture has shown promising performance due to its layer-pipelined structure and its scalability in data parallelism.   ","Exploiting weights and activations sparsity can further enhance memory storage and computation efficiency.","However, existing approaches focus on exploiting sparsity in non-dataflow accelerators, which cannot be applied onto dataflow accelerators because of the large hardware design space introduced.","As such, this could miss opportunities to find an optimal combination of sparsity features and hardware designs.   ","In this paper, we propose a novel approach to exploit unstructured weights and activations sparsity for dataflow accelerators, using software and hardware co-optimization.","We propose a Hardware-Aware Sparsity Search (HASS) to systematically determine an efficient sparsity solution for dataflow accelerators.","Over a set of models, we achieve an efficiency improvement ranging from 1.3$\\times$ to 4.2$\\times$ compared to existing sparse designs, which are either non-dataflow or non-hardware-aware.","Particularly, the throughput of MobileNetV3 can be optimized to 4895 images per second.","HASS is open-source: \\url{https://github.com/Yu-Zhewen/HASS}"],"url":"http://arxiv.org/abs/2406.03088v1","category":"cs.AR"}
{"created":"2024-06-05 08:33:09","title":"BWS: Best Window Selection Based on Sample Scores for Data Pruning across Broad Ranges","abstract":"Data subset selection aims to find a smaller yet informative subset of a large dataset that can approximate the full-dataset training, addressing challenges associated with training neural networks on large-scale datasets. However, existing methods tend to specialize in either high or low selection ratio regimes, lacking a universal approach that consistently achieves competitive performance across a broad range of selection ratios. We introduce a universal and efficient data subset selection method, Best Window Selection (BWS), by proposing a method to choose the best window subset from samples ordered based on their difficulty scores. This approach offers flexibility by allowing the choice of window intervals that span from easy to difficult samples. Furthermore, we provide an efficient mechanism for selecting the best window subset by evaluating its quality using kernel ridge regression. Our experimental results demonstrate the superior performance of BWS compared to other baselines across a broad range of selection ratios over datasets, including CIFAR-10/100 and ImageNet, and the scenarios involving training from random initialization or fine-tuning of pre-trained models.","sentences":["Data subset selection aims to find a smaller yet informative subset of a large dataset that can approximate the full-dataset training, addressing challenges associated with training neural networks on large-scale datasets.","However, existing methods tend to specialize in either high or low selection ratio regimes, lacking a universal approach that consistently achieves competitive performance across a broad range of selection ratios.","We introduce a universal and efficient data subset selection method, Best Window Selection (BWS), by proposing a method to choose the best window subset from samples ordered based on their difficulty scores.","This approach offers flexibility by allowing the choice of window intervals that span from easy to difficult samples.","Furthermore, we provide an efficient mechanism for selecting the best window subset by evaluating its quality using kernel ridge regression.","Our experimental results demonstrate the superior performance of BWS compared to other baselines across a broad range of selection ratios over datasets, including CIFAR-10/100 and ImageNet, and the scenarios involving training from random initialization or fine-tuning of pre-trained models."],"url":"http://arxiv.org/abs/2406.03057v1","category":"cs.LG"}
{"created":"2024-06-05 08:23:38","title":"Giving each task what it needs -- leveraging structured sparsity for tailored multi-task learning","abstract":"Every task demands distinct feature representations, ranging from low-level to high-level attributes, so it is vital to address the specific needs of each task, especially in the Multi-task Learning (MTL) framework. This work, therefore, introduces Layer-Optimized Multi-Task (LOMT) models that utilize structured sparsity to refine feature selection for individual tasks and enhance the performance of all tasks in a multi-task scenario. Structured or group sparsity systematically eliminates parameters from trivial channels and, eventually, entire layers within a convolution neural network during training. Consequently, the remaining layers provide the most optimal features for a given task. In this two-step approach, we subsequently leverage this sparsity-induced optimal layer information to build the LOMT models by connecting task-specific decoders to these strategically identified layers, deviating from conventional approaches that uniformly connect decoders at the end of the network. This tailored architecture optimizes the network, focusing on essential features while reducing redundancy. We validate the efficacy of the proposed approach on two datasets, ie NYU-v2 and CelebAMask-HD datasets, for multiple heterogeneous tasks. A detailed performance analysis of the LOMT models, in contrast to the conventional MTL models, reveals that the LOMT models outperform for most task combinations. The excellent qualitative and quantitative outcomes highlight the effectiveness of employing structured sparsity for optimal layer (or feature) selection.","sentences":["Every task demands distinct feature representations, ranging from low-level to high-level attributes, so it is vital to address the specific needs of each task, especially in the Multi-task Learning (MTL) framework.","This work, therefore, introduces Layer-Optimized Multi-Task (LOMT) models that utilize structured sparsity to refine feature selection for individual tasks and enhance the performance of all tasks in a multi-task scenario.","Structured or group sparsity systematically eliminates parameters from trivial channels and, eventually, entire layers within a convolution neural network during training.","Consequently, the remaining layers provide the most optimal features for a given task.","In this two-step approach, we subsequently leverage this sparsity-induced optimal layer information to build the LOMT models by connecting task-specific decoders to these strategically identified layers, deviating from conventional approaches that uniformly connect decoders at the end of the network.","This tailored architecture optimizes the network, focusing on essential features while reducing redundancy.","We validate the efficacy of the proposed approach on two datasets, ie NYU-v2 and CelebAMask-HD datasets, for multiple heterogeneous tasks.","A detailed performance analysis of the LOMT models, in contrast to the conventional MTL models, reveals that the LOMT models outperform for most task combinations.","The excellent qualitative and quantitative outcomes highlight the effectiveness of employing structured sparsity for optimal layer (or feature) selection."],"url":"http://arxiv.org/abs/2406.03048v1","category":"cs.CV"}
{"created":"2024-06-05 08:15:09","title":"Population Transformer: Learning Population-level Representations of Intracranial Activity","abstract":"We present a self-supervised framework that learns population-level codes for intracranial neural recordings at scale, unlocking the benefits of representation learning for a key neuroscience recording modality. The Population Transformer (PopT) lowers the amount of data required for decoding experiments, while increasing accuracy, even on never-before-seen subjects and tasks. We address two key challenges in developing PopT: sparse electrode distribution and varying electrode location across patients. PopT stacks on top of pretrained representations and enhances downstream tasks by enabling learned aggregation of multiple spatially-sparse data channels. Beyond decoding, we interpret the pretrained PopT and fine-tuned models to show how it can be used to provide neuroscience insights learned from massive amounts of data. We release a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability, and code is available at https://github.com/czlwang/PopulationTransformer.","sentences":["We present a self-supervised framework that learns population-level codes for intracranial neural recordings at scale, unlocking the benefits of representation learning for a key neuroscience recording modality.","The Population Transformer (PopT) lowers the amount of data required for decoding experiments, while increasing accuracy, even on never-before-seen subjects and tasks.","We address two key challenges in developing PopT: sparse electrode distribution and varying electrode location across patients.","PopT stacks on top of pretrained representations and enhances downstream tasks by enabling learned aggregation of multiple spatially-sparse data channels.","Beyond decoding, we interpret the pretrained PopT and fine-tuned models to show how it can be used to provide neuroscience insights learned from massive amounts of data.","We release a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability, and code is available at https://github.com/czlwang/PopulationTransformer."],"url":"http://arxiv.org/abs/2406.03044v1","category":"cs.LG"}
{"created":"2024-06-05 08:02:45","title":"Towards tree Yang-Mills and Yang-Mills-scalar amplitudes with higher-derivative interactions","abstract":"In our recent works, a new approach for constructing tree amplitudes, based on exploiting soft behaviors, was proposed. In this paper, we extend this approach to effective theories for gluons which incorporate higher-derivative interactions. By applying our method, we construct tree Yang-Mills (YM) and Yang-Mills-scalar (YMS) amplitudes with the single insertion of $F^3$ local operator, as well as the YM amplitudes those receive contributions from both $F^3$ and $F^4$ operators. All results are represented as universal expansions to appropriate basis. We also conjecture a compact general formula for tree YM amplitudes with higher mass dimension, which allows us to generate them from ordinary YM amplitudes, and discuss the consistent factorizations of the conjectured formula.","sentences":["In our recent works, a new approach for constructing tree amplitudes, based on exploiting soft behaviors, was proposed.","In this paper, we extend this approach to effective theories for gluons which incorporate higher-derivative interactions.","By applying our method, we construct tree Yang-Mills (YM) and Yang-Mills-scalar (YMS) amplitudes with the single insertion of $F^3$ local operator, as well as the YM amplitudes those receive contributions from both $F^3$ and $F^4$ operators.","All results are represented as universal expansions to appropriate basis.","We also conjecture a compact general formula for tree YM amplitudes with higher mass dimension, which allows us to generate them from ordinary YM amplitudes, and discuss the consistent factorizations of the conjectured formula."],"url":"http://arxiv.org/abs/2406.03034v1","category":"hep-th"}
{"created":"2024-06-05 07:35:00","title":"An analytical solution of the Schrodinger equation for the neutral ground state Para Helium","abstract":"This report presents the analytical solution of the Schrodinger equation and its corresponding wave function for the neutral para-helium or para-helium-like atoms in the ground state. The state functions of the two electrons for s=0 and l=0 as well as their boundary conditions are examined in detail. Furthermore, a method for describing a generic electron potential consisting of Coulomb and exchange interactions is derived, and the resulting potential function is integrated into the Schrodinger equation as a potential term. In addition, the altered electromagnetic coupling of the electrons due to vacuum polarization effects is investigated and finally the Schrodinger equation for the neutral Para-Helium is solved using Laplace transformations. The energy in the ground state is then determined , and it can be shown that this agrees with the literature values given the fact that the electron can be assumed to be a point-like particle. In the context of these investigations, an upper limit estimation for the spatial dimension of the electron can also be given as well as the existence of a minimal distance of a stable bonding state between two electrons, which can be interpreted as an entangled state; in addition, the chemical inertness of helium with regard to chemical reactions-i.e. the principle of the \"closed\" electron shell-can be made plausible by the quantum mechanical electron configuration and its consequences with regard to binding energy. The wave function found for the helium atom is compared with the known solutions for the hydrogen atom, and essential differences between the two are worked out.","sentences":["This report presents the analytical solution of the Schrodinger equation and its corresponding wave function for the neutral para-helium or para-helium-like atoms in the ground state.","The state functions of the two electrons for s=0 and l=0 as well as their boundary conditions are examined in detail.","Furthermore, a method for describing a generic electron potential consisting of Coulomb and exchange interactions is derived, and the resulting potential function is integrated into the Schrodinger equation as a potential term.","In addition, the altered electromagnetic coupling of the electrons due to vacuum polarization effects is investigated and finally the Schrodinger equation for the neutral Para-Helium is solved using Laplace transformations.","The energy in the ground state is then determined , and it can be shown that this agrees with the literature values given the fact that the electron can be assumed to be a point-like particle.","In the context of these investigations, an upper limit estimation for the spatial dimension of the electron can also be given as well as the existence of a minimal distance of a stable bonding state between two electrons, which can be interpreted as an entangled state; in addition, the chemical inertness of helium with regard to chemical reactions-i.e. the principle of the \"closed\" electron shell-can be made plausible by the quantum mechanical electron configuration and its consequences with regard to binding energy.","The wave function found for the helium atom is compared with the known solutions for the hydrogen atom, and essential differences between the two are worked out."],"url":"http://arxiv.org/abs/2406.03020v1","category":"quant-ph"}
{"created":"2024-06-05 06:53:16","title":"Residual Connections and Normalization Can Provably Prevent Oversmoothing in GNNs","abstract":"Residual connections and normalization layers have become standard design choices for graph neural networks (GNNs), and were proposed as solutions to the mitigate the oversmoothing problem in GNNs. However, how exactly these methods help alleviate the oversmoothing problem from a theoretical perspective is not well understood. In this work, we provide a formal and precise characterization of (linearized) GNNs with residual connections and normalization layers. We establish that (a) for residual connections, the incorporation of the initial features at each layer can prevent the signal from becoming too smooth, and determines the subspace of possible node representations; (b) batch normalization prevents a complete collapse of the output embedding space to a one-dimensional subspace through the individual rescaling of each column of the feature matrix. This results in the convergence of node representations to the top-$k$ eigenspace of the message-passing operator; (c) moreover, we show that the centering step of a normalization layer -- which can be understood as a projection -- alters the graph signal in message-passing in such a way that relevant information can become harder to extract. We therefore introduce a novel, principled normalization layer called GraphNormv2 in which the centering step is learned such that it does not distort the original graph signal in an undesirable way. Experimental results confirm the effectiveness of our method.","sentences":["Residual connections and normalization layers have become standard design choices for graph neural networks (GNNs), and were proposed as solutions to the mitigate the oversmoothing problem in GNNs.","However, how exactly these methods help alleviate the oversmoothing problem from a theoretical perspective is not well understood.","In this work, we provide a formal and precise characterization of (linearized) GNNs with residual connections and normalization layers.","We establish that (a) for residual connections, the incorporation of the initial features at each layer can prevent the signal from becoming too smooth, and determines the subspace of possible node representations; (b) batch normalization prevents a complete collapse of the output embedding space to a one-dimensional subspace through the individual rescaling of each column of the feature matrix.","This results in the convergence of node representations to the top-$k$ eigenspace of the message-passing operator; (c) moreover, we show that the centering step of a normalization layer -- which can be understood as a projection -- alters the graph signal in message-passing in such a way that relevant information can become harder to extract.","We therefore introduce a novel, principled normalization layer called GraphNormv2 in which the centering step is learned such that it does not distort the original graph signal in an undesirable way.","Experimental results confirm the effectiveness of our method."],"url":"http://arxiv.org/abs/2406.02997v1","category":"cs.LG"}
{"created":"2024-06-05 06:48:27","title":"Dual-color Q-switched mode-locking in an Erbium-doped fiber laser","abstract":"Q-switched mode-locking (QML) has been widely observed in various lasers, but its generation mechanism in passive mode-locking remains unclear. In this paper, we build up a dual-color QML Erbium-doped fiber laser and find a bound-state-like envelope on the optical spectrum for the first time. Theoretically, the formation mechanism of QML is numerically investigated using the coupled Ginzburg-Landau equations. In addition, we demonstrated the existence of two QML pulse evolution patterns with gain or polarization state variations in simulation. Our results deepen the understanding of QML pulses in mode-locked fiber lasers and provide a foundation for studying mode-locking nonlinear evolutionary paths.","sentences":["Q-switched mode-locking (QML) has been widely observed in various lasers, but its generation mechanism in passive mode-locking remains unclear.","In this paper, we build up a dual-color QML Erbium-doped fiber laser and find a bound-state-like envelope on the optical spectrum for the first time.","Theoretically, the formation mechanism of QML is numerically investigated using the coupled Ginzburg-Landau equations.","In addition, we demonstrated the existence of two QML pulse evolution patterns with gain or polarization state variations in simulation.","Our results deepen the understanding of QML pulses in mode-locked fiber lasers and provide a foundation for studying mode-locking nonlinear evolutionary paths."],"url":"http://arxiv.org/abs/2406.02993v1","category":"physics.optics"}
{"created":"2024-06-05 06:38:10","title":"Collapse of the Gibbs measure for the dynamical $\u03a6^3_2$-models on the infinite volume","abstract":"We study the $\\Phi^3_2$-measure in the infinite volume limit. This is the invariant measure for several stochastic partial differential equations including the parabolic and hyperbolic $\\Phi^3_2$-models. In the large torus limit, we observe a concentration phenomenon of the $\\Phi^3_2$-measure around zero, which is the single minimizer of the corresponding Hamiltonian for any fixed torus size. From our sharp estimates for the partition function, we obtain a triviality result for the $\\Phi^3_2$-measure on infinite volume: the ensemble collapses onto a delta function on the zero field.","sentences":["We study the $\\Phi^3_2$-measure in the infinite volume limit.","This is the invariant measure for several stochastic partial differential equations including the parabolic and hyperbolic $\\Phi^3_2$-models.","In the large torus limit, we observe a concentration phenomenon of the $\\Phi^3_2$-measure around zero, which is the single minimizer of the corresponding Hamiltonian for any fixed torus size.","From our sharp estimates for the partition function, we obtain a triviality result for the $\\Phi^3_2$-measure on infinite volume: the ensemble collapses onto a delta function on the zero field."],"url":"http://arxiv.org/abs/2406.02988v1","category":"math.PR"}
{"created":"2024-06-05 05:18:20","title":"4D ASR: Joint Beam Search Integrating CTC, Attention, Transducer, and Mask Predict Decoders","abstract":"End-to-end automatic speech recognition (E2E-ASR) can be classified into several network architectures, such as connectionist temporal classification (CTC), recurrent neural network transducer (RNN-T), attention-based encoder-decoder, and mask-predict models. Each network architecture has advantages and disadvantages, leading practitioners to switch between these different models depending on application requirements. Instead of building separate models, we propose a joint modeling scheme where four decoders (CTC, RNN-T, attention, and mask-predict) share the same encoder -- we refer to this as 4D modeling. The 4D model is trained using multitask learning, which will bring model regularization and maximize the model robustness thanks to their complementary properties. To efficiently train the 4D model, we introduce a two-stage training strategy that stabilizes multitask learning. In addition, we propose three novel one-pass beam search algorithms by combining three decoders (CTC, RNN-T, and attention) to further improve performance. These three beam search algorithms differ in which decoder is used as the primary decoder. We carefully evaluate the performance and computational tradeoffs associated with each algorithm. Experimental results demonstrate that the jointly trained 4D model outperforms the E2E-ASR models trained with only one individual decoder. Furthermore, we demonstrate that the proposed one-pass beam search algorithm outperforms the previously proposed CTC/attention decoding.","sentences":["End-to-end automatic speech recognition (E2E-ASR) can be classified into several network architectures, such as connectionist temporal classification (CTC), recurrent neural network transducer (RNN-T), attention-based encoder-decoder, and mask-predict models.","Each network architecture has advantages and disadvantages, leading practitioners to switch between these different models depending on application requirements.","Instead of building separate models, we propose a joint modeling scheme where four decoders (CTC, RNN-T, attention, and mask-predict) share the same encoder -- we refer to this as 4D modeling.","The 4D model is trained using multitask learning, which will bring model regularization and maximize the model robustness thanks to their complementary properties.","To efficiently train the 4D model, we introduce a two-stage training strategy that stabilizes multitask learning.","In addition, we propose three novel one-pass beam search algorithms by combining three decoders (CTC, RNN-T, and attention) to further improve performance.","These three beam search algorithms differ in which decoder is used as the primary decoder.","We carefully evaluate the performance and computational tradeoffs associated with each algorithm.","Experimental results demonstrate that the jointly trained 4D model outperforms the E2E-ASR models trained with only one individual decoder.","Furthermore, we demonstrate that the proposed one-pass beam search algorithm outperforms the previously proposed CTC/attention decoding."],"url":"http://arxiv.org/abs/2406.02950v1","category":"eess.AS"}
{"created":"2024-06-05 04:58:15","title":"Numerical approximation for variable-exponent fractional diffusion-wave equation","abstract":"This work presents two numerical schemes for the variable-exponent fractional diffusion-wave equation, which describes, e.g. the propagation of mechanical diffusive waves in viscoelastic media with varying material properties. The main difficulty we overcome lies in that the variable-exponent Abel kernel may not be positive definite or monotonic, and the stability and error estimate of both schemes are proved, with $\\alpha(0)$-order and second-order accuracy in time, respectively. Numerical experiments are presented to substantiate the theoretical findings.","sentences":["This work presents two numerical schemes for the variable-exponent fractional diffusion-wave equation, which describes, e.g. the propagation of mechanical diffusive waves in viscoelastic media with varying material properties.","The main difficulty we overcome lies in that the variable-exponent Abel kernel may not be positive definite or monotonic, and the stability and error estimate of both schemes are proved, with $\\alpha(0)$-order and second-order accuracy in time, respectively.","Numerical experiments are presented to substantiate the theoretical findings."],"url":"http://arxiv.org/abs/2406.02941v1","category":"math.NA"}
{"created":"2024-06-05 04:25:23","title":"Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models","abstract":"Despite the remarkable capabilities, Large Language Models (LLMs) face deployment challenges due to their extensive size. Pruning methods drop a subset of weights to accelerate, but many of them require retraining, which is prohibitively expensive and computationally demanding. Recently, post-training pruning approaches introduced novel metrics, enabling the pruning of LLMs without retraining. However, these metrics require the involvement of human experts and tedious trial and error. To efficiently identify superior pruning metrics, we develop an automatic framework for searching symbolic pruning metrics using genetic programming. In particular, we devise an elaborate search space encompassing the existing pruning metrics to discover the potential symbolic pruning metric. We propose an opposing operation simplification strategy to increase the diversity of the population. In this way, Pruner-Zero allows auto-generation of symbolic pruning metrics. Based on the searched results, we explore the correlation between pruning metrics and performance after pruning and summarize some principles. Extensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot tasks demonstrate that our Pruner-Zero obtains superior performance than SOTA post-training pruning methods. Code at: \\url{https://github.com/pprp/Pruner-Zero}.","sentences":["Despite the remarkable capabilities, Large Language Models (LLMs) face deployment challenges due to their extensive size.","Pruning methods drop a subset of weights to accelerate, but many of them require retraining, which is prohibitively expensive and computationally demanding.","Recently, post-training pruning approaches introduced novel metrics, enabling the pruning of LLMs without retraining.","However, these metrics require the involvement of human experts and tedious trial and error.","To efficiently identify superior pruning metrics, we develop an automatic framework for searching symbolic pruning metrics using genetic programming.","In particular, we devise an elaborate search space encompassing the existing pruning metrics to discover the potential symbolic pruning metric.","We propose an opposing operation simplification strategy to increase the diversity of the population.","In this way, Pruner-Zero allows auto-generation of symbolic pruning metrics.","Based on the searched results, we explore the correlation between pruning metrics and performance after pruning and summarize some principles.","Extensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot tasks demonstrate that our Pruner-Zero obtains superior performance than SOTA post-training pruning methods.","Code at: \\url{https://github.com/pprp/Pruner-Zero}."],"url":"http://arxiv.org/abs/2406.02924v1","category":"cs.LG"}
{"created":"2024-06-05 04:10:36","title":"A comprehensive and FAIR comparison between MLP and KAN representations for differential equations and operator networks","abstract":"Kolmogorov-Arnold Networks (KANs) were recently introduced as an alternative representation model to MLP. Herein, we employ KANs to construct physics-informed machine learning models (PIKANs) and deep operator models (DeepOKANs) for solving differential equations for forward and inverse problems. In particular, we compare them with physics-informed neural networks (PINNs) and deep operator networks (DeepONets), which are based on the standard MLP representation. We find that although the original KANs based on the B-splines parameterization lack accuracy and efficiency, modified versions based on low-order orthogonal polynomials have comparable performance to PINNs and DeepONet although they still lack robustness as they may diverge for different random seeds or higher order orthogonal polynomials. We visualize their corresponding loss landscapes and analyze their learning dynamics using information bottleneck theory. Our study follows the FAIR principles so that other researchers can use our benchmarks to further advance this emerging topic.","sentences":["Kolmogorov-Arnold Networks (KANs) were recently introduced as an alternative representation model to MLP.","Herein, we employ KANs to construct physics-informed machine learning models (PIKANs) and deep operator models (DeepOKANs) for solving differential equations for forward and inverse problems.","In particular, we compare them with physics-informed neural networks (PINNs) and deep operator networks (DeepONets), which are based on the standard MLP representation.","We find that although the original KANs based on the B-splines parameterization lack accuracy and efficiency, modified versions based on low-order orthogonal polynomials have comparable performance to PINNs and DeepONet although they still lack robustness as they may diverge for different random seeds or higher order orthogonal polynomials.","We visualize their corresponding loss landscapes and analyze their learning dynamics using information bottleneck theory.","Our study follows the FAIR principles so that other researchers can use our benchmarks to further advance this emerging topic."],"url":"http://arxiv.org/abs/2406.02917v1","category":"cs.LG"}
{"created":"2024-06-05 03:35:07","title":"Random fluctuation walk in the boson star formation process","abstract":"We construct the mechanism of formation of the scalar boson star (BS) having a hierarchy of self-similar \"lumps\" of dark matter (DM) built into the probability distribution function of allowed steps. It can ensure to study the dynamics of the BS formation through the \"cross-over\" between the free scalar DM and the compact condensate (CC) phase in one-dimensional model of the random fluctuation walk. The main inputs are the random fluctuating weight $\\lambda < 1$, the fundamental fluctuating length $\\xi (\\lambda)$ and the parameter $a >1$ of the spatial separation between the DM \"lumps\" inside the BS. The solution to the functional equation for the characteristic probability to find the free scalar DM and the CC phase is obtained. The time evolution of the probability to form the DM \"lump\" (growth of the BS) is presented with the power-law cascades expression depending on $\\lambda$ and $a$. The special cases for $\\lambda$ in terms of the energy densities of the BS ($\\rho_\\star$) and the DM ($\\rho_\\odot$), as well as the Bose-Einstein correlation function for two scalar DM particles, are considered. The lower bounds on the critical temperature at the \"cross-over\" depending on $\\rho_\\odot$, $\\rho_\\star$, the average multiplicities of the scalar DM particles and the parameters related to the chaotically behaviour of the particles are obtained.","sentences":["We construct the mechanism of formation of the scalar boson star (BS) having a hierarchy of self-similar \"lumps\" of dark matter (DM) built into the probability distribution function of allowed steps.","It can ensure to study the dynamics of the BS formation through the \"cross-over\" between the free scalar DM and the compact condensate (CC) phase in one-dimensional model of the random fluctuation walk.","The main inputs are the random fluctuating weight $\\lambda < 1$, the fundamental fluctuating length $\\xi (\\lambda)$ and the parameter $a >1$ of the spatial separation between the DM \"lumps\" inside the BS.","The solution to the functional equation for the characteristic probability to find the free scalar DM and the CC phase is obtained.","The time evolution of the probability to form the DM \"lump\" (growth of the BS) is presented with the power-law cascades expression depending on $\\lambda$ and $a$. The special cases for $\\lambda$ in terms of the energy densities of the BS ($\\rho_\\star$) and the DM ($\\rho_\\odot$), as well as the Bose-Einstein correlation function for two scalar DM particles, are considered.","The lower bounds on the critical temperature at the \"cross-over\" depending on $\\rho_\\odot$, $\\rho_\\star$, the average multiplicities of the scalar DM particles and the parameters related to the chaotically behaviour of the particles are obtained."],"url":"http://arxiv.org/abs/2406.02896v1","category":"hep-ph"}
{"created":"2024-06-05 02:53:12","title":"Second-order differential operators, stochastic differential equations and Brownian motions on embedded manifolds","abstract":"We specify the conditions when a manifold M embedded in an inner product space E is an invariant manifold of a stochastic differential equation (SDE) on E, linking it with the notion of second-order differential operators on M. When M is given a Riemannian metric, we derive a simple formula for the Laplace-Beltrami operator in terms of the gradient and Hessian on E and construct the Riemannian Brownian motions on M as solutions of conservative Stratonovich and Ito SDEs on E. We derive explicitly the SDE for Brownian motions on several important manifolds in applications, including left-invariant matrix Lie groups using embedded coordinates. Numerically, we propose three simulation schemes to solve SDEs on manifolds. In addition to the stochastic projection method, to simulate Riemannian Brownian motions, we construct a second-order tangent retraction of the Levi-Civita connection using a given E-tubular retraction. We also propose the retractive Euler-Maruyama method to solve a SDE, taking into account the second-order term of a tangent retraction. We provide software to implement the methods in the paper, including Brownian motions of the manifolds discussed. We verify numerically that on several compact Riemannian manifolds, the long-term limit of Brownian simulation converges to the uniform distributions, suggesting a method to sample Riemannian uniform distributions","sentences":["We specify the conditions when a manifold M embedded in an inner product space E is an invariant manifold of a stochastic differential equation (SDE) on E, linking it with the notion of second-order differential operators on M. When M is given a Riemannian metric, we derive a simple formula for the Laplace-Beltrami operator in terms of the gradient and Hessian on E and construct the Riemannian Brownian motions on M as solutions of conservative Stratonovich and Ito SDEs on E.","We derive explicitly the SDE for Brownian motions on several important manifolds in applications, including left-invariant matrix Lie groups using embedded coordinates.","Numerically, we propose three simulation schemes to solve SDEs on manifolds.","In addition to the stochastic projection method, to simulate Riemannian Brownian motions, we construct a second-order tangent retraction of the Levi-Civita connection using a given E-tubular retraction.","We also propose the retractive Euler-Maruyama method to solve a SDE, taking into account the second-order term of a tangent retraction.","We provide software to implement the methods in the paper, including Brownian motions of the manifolds discussed.","We verify numerically that on several compact Riemannian manifolds, the long-term limit of Brownian simulation converges to the uniform distributions, suggesting a method to sample Riemannian uniform distributions"],"url":"http://arxiv.org/abs/2406.02879v1","category":"math.PR"}
{"created":"2024-06-05 02:32:27","title":"Unipotent quantum coordinate ring and cominuscule prefundamental representations","abstract":"We continue the study of realization of the prefundamental modules $L_{r,a}^{\\pm}$, introduced by Hernandez and Jimbo, in terms of unipotent quantum coordinate rings as in [J-Kwon-Park, Int. Math. Res. Not., 2023]. We show that the ordinary character of $L_{r,a}^{\\pm}$ is equal to that of the unipotent quantum coordinate ring $U_q^-(w_r)$ associated to fundamental $r$-th coweight. When $r$ is cominuscule, we prove that there exists a $U_q(\\mathfrak{b})$-module structure on $U_q^-(w_r)$, which is isomorphic to $L_{r,a\\eta_r}^\\pm$ for some $\\eta_r \\in \\mathbb{C}^\\times$.","sentences":["We continue the study of realization of the prefundamental modules $L_{r,a}^{\\pm}$, introduced by Hernandez and Jimbo, in terms of unipotent quantum coordinate rings as in [J-Kwon-Park, Int.","Math. Res.","Not., 2023].","We show that the ordinary character of $L_{r,a}^{\\pm}$ is equal to that of the unipotent quantum coordinate ring $U_q^-(w_r)$ associated to fundamental $r$-th coweight.","When $r$ is cominuscule, we prove that there exists a $U_q(\\mathfrak{b})$-module structure on $U_q^-(w_r)$, which is isomorphic to $L_{r,a\\eta_r}^\\pm$ for some $\\eta_r \\in \\mathbb{C}^\\times$."],"url":"http://arxiv.org/abs/2406.02870v1","category":"math.RT"}
{"created":"2024-06-05 01:30:47","title":"Scalar auxiliary variable (SAV) stabilization of implicit-explicit (IMEX) time integration schemes for nonlinear structural dynamics","abstract":"Implicit-explicit (IMEX) time integration schemes are well suited for nonlinear structural dynamics because of their low computational cost and high accuracy. However, stability of IMEX schemes cannot be guaranteed for general nonlinear problems. In this article, we present a scalar auxiliary variable (SAV) stabilization of high-order IMEX time integration schemes that leads to unconditional stability. The proposed IMEX-BDFk-SAV schemes treat linear terms implicitly using kth-order backward difference formulas (BDFk) and nonlinear terms explicitly. This eliminates the need for iterations in nonlinear problems and leads to low computational cost. Truncation error analysis of the proposed IMEX-BDFk-SAV schemes confirms that up to kth-order accuracy can be achieved and this is verified through a series of convergence tests. Unlike existing SAV schemes for first-order ordinary differential equations (ODEs), we introduce a novel SAV for the proposed schemes that allows direct solution of the second-order ODEs without transforming them to a system of first-order ODEs. Finally, we demonstrate the performance of the proposed schemes by solving several nonlinear problems in structural dynamics and show that the proposed schemes can achieve high accuracy at a low computational cost while maintaining unconditional stability.","sentences":["Implicit-explicit (IMEX) time integration schemes are well suited for nonlinear structural dynamics because of their low computational cost and high accuracy.","However, stability of IMEX schemes cannot be guaranteed for general nonlinear problems.","In this article, we present a scalar auxiliary variable (SAV) stabilization of high-order IMEX time integration schemes that leads to unconditional stability.","The proposed IMEX-BDFk-SAV schemes treat linear terms implicitly using kth-order backward difference formulas (BDFk) and nonlinear terms explicitly.","This eliminates the need for iterations in nonlinear problems and leads to low computational cost.","Truncation error analysis of the proposed IMEX-BDFk-SAV schemes confirms that up to kth-order accuracy can be achieved and this is verified through a series of convergence tests.","Unlike existing SAV schemes for first-order ordinary differential equations (ODEs), we introduce a novel SAV for the proposed schemes that allows direct solution of the second-order ODEs without transforming them to a system of first-order ODEs.","Finally, we demonstrate the performance of the proposed schemes by solving several nonlinear problems in structural dynamics and show that the proposed schemes can achieve high accuracy at a low computational cost while maintaining unconditional stability."],"url":"http://arxiv.org/abs/2406.02839v1","category":"math.NA"}
{"created":"2024-06-05 00:31:50","title":"Too Big to Fail: Larger Language Models are Disproportionately Resilient to Induction of Dementia-Related Linguistic Anomalies","abstract":"As artificial neural networks grow in complexity, understanding their inner workings becomes increasingly challenging, which is particularly important in healthcare applications. The intrinsic evaluation metrics of autoregressive neural language models (NLMs), perplexity (PPL), can reflect how \"surprised\" an NLM model is at novel input. PPL has been widely used to understand the behavior of NLMs. Previous findings show that changes in PPL when masking attention layers in pre-trained transformer-based NLMs reflect linguistic anomalies associated with Alzheimer's disease dementia. Building upon this, we explore a novel bidirectional attention head ablation method that exhibits properties attributed to the concepts of cognitive and brain reserve in human brain studies, which postulate that people with more neurons in the brain and more efficient processing are more resilient to neurodegeneration. Our results show that larger GPT-2 models require a disproportionately larger share of attention heads to be masked/ablated to display degradation of similar magnitude to masking in smaller models. These results suggest that the attention mechanism in transformer models may present an analogue to the notions of cognitive and brain reserve and could potentially be used to model certain aspects of the progression of neurodegenerative disorders and aging.","sentences":["As artificial neural networks grow in complexity, understanding their inner workings becomes increasingly challenging, which is particularly important in healthcare applications.","The intrinsic evaluation metrics of autoregressive neural language models (NLMs), perplexity (PPL), can reflect how \"surprised\" an NLM model is at novel input.","PPL has been widely used to understand the behavior of NLMs.","Previous findings show that changes in PPL when masking attention layers in pre-trained transformer-based NLMs reflect linguistic anomalies associated with Alzheimer's disease dementia.","Building upon this, we explore a novel bidirectional attention head ablation method that exhibits properties attributed to the concepts of cognitive and brain reserve in human brain studies, which postulate that people with more neurons in the brain and more efficient processing are more resilient to neurodegeneration.","Our results show that larger GPT-2 models require a disproportionately larger share of attention heads to be masked/ablated to display degradation of similar magnitude to masking in smaller models.","These results suggest that the attention mechanism in transformer models may present an analogue to the notions of cognitive and brain reserve and could potentially be used to model certain aspects of the progression of neurodegenerative disorders and aging."],"url":"http://arxiv.org/abs/2406.02830v1","category":"cs.CL"}
{"created":"2024-06-05 00:19:03","title":"3-circle Theorem for Willmore surface I","abstract":"In this paper, we study the blow-up of Willmore surfaces. By using the 3-circle theorem, we prove a decay estimate of the second fundamental form along the neck region. This estimate provides a new perspective and streamlined proofs to a few key results in this field, such as the energy identity(quantization), removable singularities and gap theorem.","sentences":["In this paper, we study the blow-up of Willmore surfaces.","By using the 3-circle theorem, we prove a decay estimate of the second fundamental form along the neck region.","This estimate provides a new perspective and streamlined proofs to a few key results in this field, such as the energy identity(quantization), removable singularities and gap theorem."],"url":"http://arxiv.org/abs/2406.02828v1","category":"math.DG"}
{"created":"2024-06-04 23:05:24","title":"$L^p$-norms for the homogeneous non-cutoff Boltzmann equation with soft potentials","abstract":"We establish a priori estimates showing the propagation and generation of $L^p$-norms for solutions to the non-cutoff spatially homogeneous Boltzmann equation with soft potentials. The singularity of the collision kernel is key to generate regularization and inhomogeneity in the energy estimates of the $L^p$-norms. Our result extends \\cite{Alo19} from the hard potential cases to the soft ones.","sentences":["We establish a priori estimates showing the propagation and generation of $L^p$-norms for solutions to the non-cutoff spatially homogeneous Boltzmann equation with soft potentials.","The singularity of the collision kernel is key to generate regularization and inhomogeneity in the energy estimates of the $L^p$-norms.","Our result extends \\cite{Alo19} from the hard potential cases to the soft ones."],"url":"http://arxiv.org/abs/2406.02813v1","category":"math.AP"}
{"created":"2024-06-04 22:49:54","title":"Generalized symmetries of Burgers equation","abstract":"Despite the number of relevant considerations in the literature, the algebra of generalized symmetries of the Burgers equation has not been exhaustively described. We fill this gap, presenting a basis of this algebra in an explicit form and proving that the two well-known recursion operators of the Burgers equation and two seed generalized symmetries, which are evolution forms of its Lie symmetries, suffice to generate this algebra. The core of the proof is essentially simplified by using the original technique of choosing special coordinates in the associated jet space.","sentences":["Despite the number of relevant considerations in the literature, the algebra of generalized symmetries of the Burgers equation has not been exhaustively described.","We fill this gap, presenting a basis of this algebra in an explicit form and proving that the two well-known recursion operators of the Burgers equation and two seed generalized symmetries, which are evolution forms of its Lie symmetries, suffice to generate this algebra.","The core of the proof is essentially simplified by using the original technique of choosing special coordinates in the associated jet space."],"url":"http://arxiv.org/abs/2406.02809v1","category":"math.AP"}
{"created":"2024-06-04 22:22:39","title":"Randomized Geometric Algebra Methods for Convex Neural Networks","abstract":"We introduce randomized algorithms to Clifford's Geometric Algebra, generalizing randomized linear algebra to hypercomplex vector spaces. This novel approach has many implications in machine learning, including training neural networks to global optimality via convex optimization. Additionally, we consider fine-tuning large language model (LLM) embeddings as a key application area, exploring the intersection of geometric algebra and modern AI techniques. In particular, we conduct a comparative analysis of the robustness of transfer learning via embeddings, such as OpenAI GPT models and BERT, using traditional methods versus our novel approach based on convex optimization. We test our convex optimization transfer learning method across a variety of case studies, employing different embeddings (GPT-4 and BERT embeddings) and different text classification datasets (IMDb, Amazon Polarity Dataset, and GLUE) with a range of hyperparameter settings. Our results demonstrate that convex optimization and geometric algebra not only enhances the performance of LLMs but also offers a more stable and reliable method of transfer learning via embeddings.","sentences":["We introduce randomized algorithms to Clifford's Geometric Algebra, generalizing randomized linear algebra to hypercomplex vector spaces.","This novel approach has many implications in machine learning, including training neural networks to global optimality via convex optimization.","Additionally, we consider fine-tuning large language model (LLM) embeddings as a key application area, exploring the intersection of geometric algebra and modern AI techniques.","In particular, we conduct a comparative analysis of the robustness of transfer learning via embeddings, such as OpenAI GPT models and BERT, using traditional methods versus our novel approach based on convex optimization.","We test our convex optimization transfer learning method across a variety of case studies, employing different embeddings (GPT-4 and BERT embeddings) and different text classification datasets (IMDb, Amazon Polarity Dataset, and GLUE) with a range of hyperparameter settings.","Our results demonstrate that convex optimization and geometric algebra not only enhances the performance of LLMs but also offers a more stable and reliable method of transfer learning via embeddings."],"url":"http://arxiv.org/abs/2406.02806v1","category":"cs.LG"}
{"created":"2024-06-04 21:47:20","title":"Numerical approximation of linear evolution equations revisited","abstract":"We obtain rates of convergence of numerical approximations of abstract linear evolution equations. Our estimates extend known results like Theorem 3.5 in \\cite{thomee} to more general equations and accommodate more advanced numerical approximation techniques. As an example, we consider parabolic equations on surfaces, and surface finite element approximations.","sentences":["We obtain rates of convergence of numerical approximations of abstract linear evolution equations.","Our estimates extend known results like Theorem 3.5 in \\cite{thomee} to more general equations and accommodate more advanced numerical approximation techniques.","As an example, we consider parabolic equations on surfaces, and surface finite element approximations."],"url":"http://arxiv.org/abs/2406.02796v1","category":"math.NA"}
{"created":"2024-06-04 21:43:29","title":"Differential equations driven by Besov-Orlicz paths","abstract":"In the article, the rough path theory is extended to cover paths from the exponential Besov-Orlicz space \\[B^\\alpha_{\\Phi_\\beta,q}\\quad\\mbox{ for }\\quad \\alpha\\in (1/3,1/2],\\,\\quad \\Phi_\\beta(x) \\sim \\mathrm{e}^{x^\\beta}-1\\quad\\mbox{with}\\quad \\beta\\in (0,\\infty), \\quad\\mbox{and}\\quad q\\in (0,\\infty],\\] and the extension is used to treat nonlinear differential equations driven by such paths. The exponential Besov-Orlicz-type spaces, rough paths, and controlled rough paths are defined and analyzed, a sewing lemma for such paths is given, and the existence and uniqueness of the solution to differential equations driven by these paths is proved. The results cover equations driven by paths of continuous local martingales with Lipschitz continuous quadratic variation (e.g.\\ the Wiener process) or by paths of fractionally filtered Hermite processes in the $n$\\textsuperscript{th} Wiener chaos with Hurst parameter $H\\in (1/3,1/2]$ (e.g.\\ the fractional Brownian motion).","sentences":["In the article, the rough path theory is extended to cover paths from the exponential Besov-Orlicz space \\[B^\\alpha_{\\Phi_\\beta,q}\\quad\\mbox{ for }\\quad \\alpha\\in (1/3,1/2],\\,\\quad \\Phi_\\beta(x) \\sim \\mathrm{e}^{x^\\beta}-1\\quad\\mbox{with}\\quad \\beta\\in (0,\\infty), \\quad\\mbox{and}\\quad q\\in (0,\\infty],\\] and the extension is used to treat nonlinear differential equations driven by such paths.","The exponential Besov-Orlicz-type spaces, rough paths, and controlled rough paths are defined and analyzed, a sewing lemma for such paths is given, and the existence and uniqueness of the solution to differential equations driven by these paths is proved.","The results cover equations driven by paths of continuous local martingales with Lipschitz continuous quadratic variation (e.g.\\ the Wiener process) or by paths of fractionally filtered Hermite processes in the $n$\\textsuperscript{th} Wiener chaos with Hurst parameter $H\\in (1/3,1/2]$ (e.g.\\ the fractional Brownian motion)."],"url":"http://arxiv.org/abs/2406.02793v1","category":"math.PR"}
{"created":"2024-06-04 21:08:37","title":"An Existence Theorem for a Model of Temperature Within a Lithium-Ion Battery","abstract":"In this article we investigate a model for the temperature within a Lithium-Ion battery. The model takes the form of a parabolic PDE for the temperature coupled with two elliptic PDE's for the electric potential within the solid and electrolyte phases. The primary difficulty comes from the coupling term, which is given by the Butler-Volmer equation. It features an exponential nonlinearity of both the electric potentials and the reciprocal of the temperature. Another difficulty arising in the temperature equation are the gradients of the electric potentials squared showing up on the right-hand side. Due to the nonlinearity, meaningful estimates for the temperature are currently not known. In spite of this, our investigation reveals the local existence of continuous temperature for the Lithium-Ion Battery.","sentences":["In this article we investigate a model for the temperature within a Lithium-Ion battery.","The model takes the form of a parabolic PDE for the temperature coupled with two elliptic PDE's for the electric potential within the solid and electrolyte phases.","The primary difficulty comes from the coupling term, which is given by the Butler-Volmer equation.","It features an exponential nonlinearity of both the electric potentials and the reciprocal of the temperature.","Another difficulty arising in the temperature equation are the gradients of the electric potentials squared showing up on the right-hand side.","Due to the nonlinearity, meaningful estimates for the temperature are currently not known.","In spite of this, our investigation reveals the local existence of continuous temperature for the Lithium-Ion Battery."],"url":"http://arxiv.org/abs/2406.02786v1","category":"math.AP"}
{"created":"2024-06-04 20:52:33","title":"Bounding Shortest Closed Geodesics with Diameter on compact 2-dimensional Orbifolds Homeomorphic to $S^2$","abstract":"Length bounded sweepouts give a way to bound the length of the shortest closed geodesic of a closed manifold. In this paper, we generalized to the case of compact 2-dimensional orbifolds homeomorphic to $S^2$ as well as compact 2-dimensional orbifolds with finite orbifold fundamental groups. We proved an inequality for the length of the shortest closed orbifold geodesic in terms of the diameter.","sentences":["Length bounded sweepouts give a way to bound the length of the shortest closed geodesic of a closed manifold.","In this paper, we generalized to the case of compact 2-dimensional orbifolds homeomorphic to $S^2$ as well as compact 2-dimensional orbifolds with finite orbifold fundamental groups.","We proved an inequality for the length of the shortest closed orbifold geodesic in terms of the diameter."],"url":"http://arxiv.org/abs/2406.02781v1","category":"math.DG"}
{"created":"2024-06-04 20:40:06","title":"Hyperbolic Benchmarking Unveils Network Topology-Feature Relationship in GNN Performance","abstract":"Graph Neural Networks (GNNs) have excelled in predicting graph properties in various applications ranging from identifying trends in social networks to drug discovery and malware detection. With the abundance of new architectures and increased complexity, GNNs are becoming highly specialized when tested on a few well-known datasets. However, how the performance of GNNs depends on the topological and features properties of graphs is still an open question. In this work, we introduce a comprehensive benchmarking framework for graph machine learning, focusing on the performance of GNNs across varied network structures. Utilizing the geometric soft configuration model in hyperbolic space, we generate synthetic networks with realistic topological properties and node feature vectors. This approach enables us to assess the impact of network properties, such as topology-feature correlation, degree distributions, local density of triangles (or clustering), and homophily, on the effectiveness of different GNN architectures. Our results highlight the dependency of model performance on the interplay between network structure and node features, providing insights for model selection in various scenarios. This study contributes to the field by offering a versatile tool for evaluating GNNs, thereby assisting in developing and selecting suitable models based on specific data characteristics.","sentences":["Graph Neural Networks (GNNs) have excelled in predicting graph properties in various applications ranging from identifying trends in social networks to drug discovery and malware detection.","With the abundance of new architectures and increased complexity, GNNs are becoming highly specialized when tested on a few well-known datasets.","However, how the performance of GNNs depends on the topological and features properties of graphs is still an open question.","In this work, we introduce a comprehensive benchmarking framework for graph machine learning, focusing on the performance of GNNs across varied network structures.","Utilizing the geometric soft configuration model in hyperbolic space, we generate synthetic networks with realistic topological properties and node feature vectors.","This approach enables us to assess the impact of network properties, such as topology-feature correlation, degree distributions, local density of triangles (or clustering), and homophily, on the effectiveness of different GNN architectures.","Our results highlight the dependency of model performance on the interplay between network structure and node features, providing insights for model selection in various scenarios.","This study contributes to the field by offering a versatile tool for evaluating GNNs, thereby assisting in developing and selecting suitable models based on specific data characteristics."],"url":"http://arxiv.org/abs/2406.02772v1","category":"cs.LG"}
{"created":"2024-06-04 20:37:17","title":"Precise asymptotics of reweighted least-squares algorithms for linear diagonal networks","abstract":"The classical iteratively reweighted least-squares (IRLS) algorithm aims to recover an unknown signal from linear measurements by performing a sequence of weighted least squares problems, where the weights are recursively updated at each step. Varieties of this algorithm have been shown to achieve favorable empirical performance and theoretical guarantees for sparse recovery and $\\ell_p$-norm minimization. Recently, some preliminary connections have also been made between IRLS and certain types of non-convex linear neural network architectures that are observed to exploit low-dimensional structure in high-dimensional linear models. In this work, we provide a unified asymptotic analysis for a family of algorithms that encompasses IRLS, the recently proposed lin-RFM algorithm (which was motivated by feature learning in neural networks), and the alternating minimization algorithm on linear diagonal neural networks. Our analysis operates in a \"batched\" setting with i.i.d. Gaussian covariates and shows that, with appropriately chosen reweighting policy, the algorithm can achieve favorable performance in only a handful of iterations. We also extend our results to the case of group-sparse recovery and show that leveraging this structure in the reweighting scheme provably improves test error compared to coordinate-wise reweighting.","sentences":["The classical iteratively reweighted least-squares (IRLS) algorithm aims to recover an unknown signal from linear measurements by performing a sequence of weighted least squares problems, where the weights are recursively updated at each step.","Varieties of this algorithm have been shown to achieve favorable empirical performance and theoretical guarantees for sparse recovery and $\\ell_p$-norm minimization.","Recently, some preliminary connections have also been made between IRLS and certain types of non-convex linear neural network architectures that are observed to exploit low-dimensional structure in high-dimensional linear models.","In this work, we provide a unified asymptotic analysis for a family of algorithms that encompasses IRLS, the recently proposed lin-RFM algorithm (which was motivated by feature learning in neural networks), and the alternating minimization algorithm on linear diagonal neural networks.","Our analysis operates in a \"batched\" setting with i.i.d.","Gaussian covariates and shows that, with appropriately chosen reweighting policy, the algorithm can achieve favorable performance in only a handful of iterations.","We also extend our results to the case of group-sparse recovery and show that leveraging this structure in the reweighting scheme provably improves test error compared to coordinate-wise reweighting."],"url":"http://arxiv.org/abs/2406.02769v1","category":"stat.ML"}
{"created":"2024-06-04 20:17:21","title":"Warped Disk Evolution in Grid-Based Simulations","abstract":"Observations show evidence that a significant fraction of protoplanetary disks contain warps. A warp in a disk evolves in time affecting the appearance of shadows and greatly influencing kinematic signatures. So far, many theoretical studies of warped disks have been conducted using Smoothed Particle Hydrodynamics (SPH) methods. In our approach, we use a grid-based method in spherical coordinates which has notable advantages: the method allows for accurate modelling of low viscosity values and the resolution does not depend on density or mass of the disk, which allows surface structures to be resolved. We perform 3D simulations using FARGO3D to simulate the evolution of a warped disk and compare the results to one-dimensional models using a ring code. Additionally, we extensively investigate the applicability of grid-based methods to misaligned disks and test their dependency on grid resolution as well as disk viscosity. We find that grid-based simulations are capable of simulating disks not aligned to the grid geometry. Our three-dimensional simulation of a warped disk compares well to one-dimensional models in evolution of inclination. However, we find a twist which is not captured in 1D models. After thorough analysis we suspect this to be a physical effect possibly caused by non-linear effects neglected in the one-dimensional equations. Evaluating the internal dynamics, we find sloshing and breathing motions as predicted in local shearing box analysis. They can become supersonic, which may have consequences on kinematic observations of warped disks. Warped disks can be accurately modelled in 3D grid-based simulations when using reasonably good resolution, especially in the $\\theta$-direction. We find good agreement with the linear approximation of the sloshing motion which highlights the reliability of 1D models.","sentences":["Observations show evidence that a significant fraction of protoplanetary disks contain warps.","A warp in a disk evolves in time affecting the appearance of shadows and greatly influencing kinematic signatures.","So far, many theoretical studies of warped disks have been conducted using Smoothed Particle Hydrodynamics (SPH) methods.","In our approach, we use a grid-based method in spherical coordinates which has notable advantages: the method allows for accurate modelling of low viscosity values and the resolution does not depend on density or mass of the disk, which allows surface structures to be resolved.","We perform 3D simulations using FARGO3D to simulate the evolution of a warped disk and compare the results to one-dimensional models using a ring code.","Additionally, we extensively investigate the applicability of grid-based methods to misaligned disks and test their dependency on grid resolution as well as disk viscosity.","We find that grid-based simulations are capable of simulating disks not aligned to the grid geometry.","Our three-dimensional simulation of a warped disk compares well to one-dimensional models in evolution of inclination.","However, we find a twist which is not captured in 1D models.","After thorough analysis we suspect this to be a physical effect possibly caused by non-linear effects neglected in the one-dimensional equations.","Evaluating the internal dynamics, we find sloshing and breathing motions as predicted in local shearing box analysis.","They can become supersonic, which may have consequences on kinematic observations of warped disks.","Warped disks can be accurately modelled in 3D grid-based simulations when using reasonably good resolution, especially in the $\\theta$-direction.","We find good agreement with the linear approximation of the sloshing motion which highlights the reliability of 1D models."],"url":"http://arxiv.org/abs/2406.02754v1","category":"astro-ph.SR"}
{"created":"2024-06-04 19:57:47","title":"DPDR: Gradient Decomposition and Reconstruction for Differentially Private Deep Learning","abstract":"Differentially Private Stochastic Gradients Descent (DP-SGD) is a prominent paradigm for preserving privacy in deep learning. It ensures privacy by perturbing gradients with random noise calibrated to their entire norm at each training step. However, this perturbation suffers from a sub-optimal performance: it repeatedly wastes privacy budget on the general converging direction shared among gradients from different batches, which we refer as common knowledge, yet yields little information gain. Motivated by this, we propose a differentially private training framework with early gradient decomposition and reconstruction (DPDR), which enables more efficient use of the privacy budget. In essence, it boosts model utility by focusing on incremental information protection and recycling the privatized common knowledge learned from previous gradients at early training steps. Concretely, DPDR incorporates three steps. First, it disentangles common knowledge and incremental information in current gradients by decomposing them based on previous noisy gradients. Second, most privacy budget is spent on protecting incremental information for higher information gain. Third, the model is updated with the gradient reconstructed from recycled common knowledge and noisy incremental information. Theoretical analysis and extensive experiments show that DPDR outperforms state-of-the-art baselines on both convergence rate and accuracy.","sentences":["Differentially Private Stochastic Gradients Descent (DP-SGD) is a prominent paradigm for preserving privacy in deep learning.","It ensures privacy by perturbing gradients with random noise calibrated to their entire norm at each training step.","However, this perturbation suffers from a sub-optimal performance: it repeatedly wastes privacy budget on the general converging direction shared among gradients from different batches, which we refer as common knowledge, yet yields little information gain.","Motivated by this, we propose a differentially private training framework with early gradient decomposition and reconstruction (DPDR), which enables more efficient use of the privacy budget.","In essence, it boosts model utility by focusing on incremental information protection and recycling the privatized common knowledge learned from previous gradients at early training steps.","Concretely, DPDR incorporates three steps.","First, it disentangles common knowledge and incremental information in current gradients by decomposing them based on previous noisy gradients.","Second, most privacy budget is spent on protecting incremental information for higher information gain.","Third, the model is updated with the gradient reconstructed from recycled common knowledge and noisy incremental information.","Theoretical analysis and extensive experiments show that DPDR outperforms state-of-the-art baselines on both convergence rate and accuracy."],"url":"http://arxiv.org/abs/2406.02744v1","category":"cs.CR"}
{"created":"2024-06-04 19:42:19","title":"Long Range Propagation on Continuous-Time Dynamic Graphs","abstract":"Learning Continuous-Time Dynamic Graphs (C-TDGs) requires accurately modeling spatio-temporal information on streams of irregularly sampled events. While many methods have been proposed recently, we find that most message passing-, recurrent- or self-attention-based methods perform poorly on long-range tasks. These tasks require correlating information that occurred \"far\" away from the current event, either spatially (higher-order node information) or along the time dimension (events occurred in the past). To address long-range dependencies, we introduce Continuous-Time Graph Anti-Symmetric Network (CTAN). Grounded within the ordinary differential equations framework, our method is designed for efficient propagation of information. In this paper, we show how CTAN's (i) long-range modeling capabilities are substantiated by theoretical findings and how (ii) its empirical performance on synthetic long-range benchmarks and real-world benchmarks is superior to other methods. Our results motivate CTAN's ability to propagate long-range information in C-TDGs as well as the inclusion of long-range tasks as part of temporal graph models evaluation.","sentences":["Learning Continuous-Time Dynamic Graphs (C-TDGs) requires accurately modeling spatio-temporal information on streams of irregularly sampled events.","While many methods have been proposed recently, we find that most message passing-, recurrent- or self-attention-based methods perform poorly on long-range tasks.","These tasks require correlating information that occurred \"far\" away from the current event, either spatially (higher-order node information) or along the time dimension (events occurred in the past).","To address long-range dependencies, we introduce Continuous-Time Graph Anti-Symmetric Network (CTAN).","Grounded within the ordinary differential equations framework, our method is designed for efficient propagation of information.","In this paper, we show how CTAN's (i) long-range modeling capabilities are substantiated by theoretical findings and how (ii) its empirical performance on synthetic long-range benchmarks and real-world benchmarks is superior to other methods.","Our results motivate CTAN's ability to propagate long-range information in C-TDGs as well as the inclusion of long-range tasks as part of temporal graph models evaluation."],"url":"http://arxiv.org/abs/2406.02740v1","category":"cs.LG"}
{"created":"2024-06-04 19:35:44","title":"Synthetic Data Outliers: Navigating Identity Disclosure","abstract":"Multiple synthetic data generation models have emerged, among which deep learning models have become the vanguard due to their ability to capture the underlying characteristics of the original data. However, the resemblance of the synthetic to the original data raises important questions on the protection of individuals' privacy. As synthetic data is perceived as a means to fully protect personal information, most current related work disregards the impact of re-identification risk. In particular, limited attention has been given to exploring outliers, despite their privacy relevance. In this work, we analyze the privacy of synthetic data w.r.t the outliers. Our main findings suggest that outliers re-identification via linkage attack is feasible and easily achieved. Furthermore, additional safeguards such as differential privacy can prevent re-identification, albeit at the expense of the data utility.","sentences":["Multiple synthetic data generation models have emerged, among which deep learning models have become the vanguard due to their ability to capture the underlying characteristics of the original data.","However, the resemblance of the synthetic to the original data raises important questions on the protection of individuals' privacy.","As synthetic data is perceived as a means to fully protect personal information, most current related work disregards the impact of re-identification risk.","In particular, limited attention has been given to exploring outliers, despite their privacy relevance.","In this work, we analyze the privacy of synthetic data w.r.t the outliers.","Our main findings suggest that outliers re-identification via linkage attack is feasible and easily achieved.","Furthermore, additional safeguards such as differential privacy can prevent re-identification, albeit at the expense of the data utility."],"url":"http://arxiv.org/abs/2406.02736v1","category":"cs.LG"}
{"created":"2024-06-04 19:11:54","title":"Vagus nerve stimulation: Laying the groundwork for predictive network-based computer models","abstract":"Vagus Nerve Stimulation (VNS) is an established palliative treatment for drug resistant epilepsy. While effective for many patients, its mechanism of action is incompletely understood. Predicting individuals' response, or optimum stimulation parameters, is challenging. Computational modelling has informed other problems in epilepsy but, to our knowledge, has not been applied to VNS.   We started with an established, four-population neural mass model (NMM), capable of reproducing the seizure-like dynamics of a thalamocortical circuit. We extended this to include 18 further neural populations, representing nine other brain regions relevant to VNS, with connectivity based on existing literature. We modelled stimulated afferent vagal fibres as projecting to the nucleus tractus solitarius (NTS), which receives input from the vagus nerve in vivo.   Bifurcation analysis of a deterministic version of the model showed higher background NTS input made the model monostable at a fixed point (FP), representing normal activity, while lower inputs produce bistability between the FP and a limit cycle (LC), representing the seizure state.   Adding noise produced transitions between seizure and normal states. This stochastic model spent decreasing time in the seizure state with increasing background NTS input, until seizures were abolished, consistent with the deterministic model.   Simulated VNS stimulation, modelled as a 30 Hz square wave, was summed with the background input to the NTS and was found to reduce total seizure duration in a dose-dependent manner, similar to expectations in vivo.   We have successfully produced an in silico model of VNS in epilepsy, capturing behaviour seen in vivo. This may aid understanding therapeutic mechanisms of VNS in epilepsy and provides a starting point to (i) determine which patients might respond best to VNS, and (ii) optimise individuals' treatments.","sentences":["Vagus Nerve Stimulation (VNS) is an established palliative treatment for drug resistant epilepsy.","While effective for many patients, its mechanism of action is incompletely understood.","Predicting individuals' response, or optimum stimulation parameters, is challenging.","Computational modelling has informed other problems in epilepsy but, to our knowledge, has not been applied to VNS.   ","We started with an established, four-population neural mass model (NMM), capable of reproducing the seizure-like dynamics of a thalamocortical circuit.","We extended this to include 18 further neural populations, representing nine other brain regions relevant to VNS, with connectivity based on existing literature.","We modelled stimulated afferent vagal fibres as projecting to the nucleus tractus solitarius (NTS), which receives input from the vagus nerve in vivo.   ","Bifurcation analysis of a deterministic version of the model showed higher background NTS input made the model monostable at a fixed point (FP), representing normal activity, while lower inputs produce bistability between the FP and a limit cycle (LC), representing the seizure state.   ","Adding noise produced transitions between seizure and normal states.","This stochastic model spent decreasing time in the seizure state with increasing background NTS input, until seizures were abolished, consistent with the deterministic model.   ","Simulated VNS stimulation, modelled as a 30 Hz square wave, was summed with the background input to the NTS and was found to reduce total seizure duration in a dose-dependent manner, similar to expectations in vivo.   ","We have successfully produced an in silico model of VNS in epilepsy, capturing behaviour seen in vivo.","This may aid understanding therapeutic mechanisms of VNS in epilepsy and provides a starting point to (i) determine which patients might respond best to VNS, and (ii) optimise individuals' treatments."],"url":"http://arxiv.org/abs/2406.02729v1","category":"q-bio.NC"}
{"created":"2024-06-04 19:04:29","title":"3D-HGS: 3D Half-Gaussian Splatting","abstract":"Photo-realistic 3D Reconstruction is a fundamental problem in 3D computer vision. This domain has seen considerable advancements owing to the advent of recent neural rendering techniques. These techniques predominantly aim to focus on learning volumetric representations of 3D scenes and refining these representations via loss functions derived from rendering. Among these, 3D Gaussian Splatting (3D-GS) has emerged as a significant method, surpassing Neural Radiance Fields (NeRFs). 3D-GS uses parameterized 3D Gaussians for modeling both spatial locations and color information, combined with a tile-based fast rendering technique. Despite its superior rendering performance and speed, the use of 3D Gaussian kernels has inherent limitations in accurately representing discontinuous functions, notably at edges and corners for shape discontinuities, and across varying textures for color discontinuities. To address this problem, we propose to employ 3D Half-Gaussian (3D-HGS) kernels, which can be used as a plug-and-play kernel. Our experiments demonstrate their capability to improve the performance of current 3D-GS related methods and achieve state-of-the-art rendering performance on various datasets without compromising rendering speed.","sentences":["Photo-realistic 3D Reconstruction is a fundamental problem in 3D computer vision.","This domain has seen considerable advancements owing to the advent of recent neural rendering techniques.","These techniques predominantly aim to focus on learning volumetric representations of 3D scenes and refining these representations via loss functions derived from rendering.","Among these, 3D Gaussian Splatting (3D-GS) has emerged as a significant method, surpassing Neural Radiance Fields (NeRFs).","3D-GS uses parameterized 3D Gaussians for modeling both spatial locations and color information, combined with a tile-based fast rendering technique.","Despite its superior rendering performance and speed, the use of 3D Gaussian kernels has inherent limitations in accurately representing discontinuous functions, notably at edges and corners for shape discontinuities, and across varying textures for color discontinuities.","To address this problem, we propose to employ 3D Half-Gaussian (3D-HGS) kernels, which can be used as a plug-and-play kernel.","Our experiments demonstrate their capability to improve the performance of current 3D-GS related methods and achieve state-of-the-art rendering performance on various datasets without compromising rendering speed."],"url":"http://arxiv.org/abs/2406.02720v1","category":"cs.CV"}
{"created":"2024-06-04 18:59:42","title":"Optimal Rates for DP-SCO with a Single Epoch and Large Batches","abstract":"The most common algorithms for differentially private (DP) machine learning (ML) are all based on stochastic gradient descent, for example, DP-SGD. These algorithms achieve DP by treating each gradient as an independent private query. However, this independence can cause us to overpay in privacy loss because we don't analyze the entire gradient trajectory. In this work, we propose a new DP algorithm, which we call Accelerated-DP-SRGD (DP stochastic recursive gradient descent), that enables us to break this independence and only pay for privacy in the gradient difference, i.e., in the new information at the current step. Our algorithm achieves the optimal DP-stochastic convex optimization (DP-SCO) error (up to polylog factors) using only a single epoch over the dataset, and converges at the Nesterov's accelerated rate.   Our algorithm can be run in at most $\\sqrt{n}$ batch gradient steps with batch size at least $\\sqrt{n}$, unlike prior work which required $O(n)$ queries with mostly constant batch sizes. To achieve this, our algorithm combines three key ingredients, a variant of stochastic recursive gradients (SRG), accelerated gradient descent, and correlated noise generation from DP continual counting. Finally, we also show that our algorithm improves over existing SoTA on multi-class logistic regression on MNIST and CIFAR-10.","sentences":["The most common algorithms for differentially private (DP) machine learning (ML) are all based on stochastic gradient descent, for example, DP-SGD.","These algorithms achieve DP by treating each gradient as an independent private query.","However, this independence can cause us to overpay in privacy loss because we don't analyze the entire gradient trajectory.","In this work, we propose a new DP algorithm, which we call Accelerated-DP-SRGD (DP stochastic recursive gradient descent), that enables us to break this independence and only pay for privacy in the gradient difference, i.e., in the new information at the current step.","Our algorithm achieves the optimal DP-stochastic convex optimization (DP-SCO) error (up to polylog factors) using only a single epoch over the dataset, and converges at the Nesterov's accelerated rate.   ","Our algorithm can be run in at most $\\sqrt{n}$ batch gradient steps with batch size at least $\\sqrt{n}$, unlike prior work which required $O(n)$ queries with mostly constant batch sizes.","To achieve this, our algorithm combines three key ingredients, a variant of stochastic recursive gradients (SRG), accelerated gradient descent, and correlated noise generation from DP continual counting.","Finally, we also show that our algorithm improves over existing SoTA on multi-class logistic regression on MNIST and CIFAR-10."],"url":"http://arxiv.org/abs/2406.02716v1","category":"cs.LG"}
{"created":"2024-06-04 18:54:10","title":"Self-Trained Model for ECG Complex Delineation","abstract":"Electrocardiogram (ECG) delineation plays a crucial role in assisting cardiologists with accurate diagnoses. Prior research studies have explored various methods, including the application of deep learning techniques, to achieve precise delineation. However, existing approaches face limitations primarily related to dataset size and robustness. In this paper, we introduce a dataset for ECG delineation and propose a novel self-trained method aimed at leveraging a vast amount of unlabeled ECG data. Our approach involves the pseudolabeling of unlabeled data using a neural network trained on our dataset. Subsequently, we train the model on the newly labeled samples to enhance the quality of delineation. We conduct experiments demonstrating that our dataset is a valuable resource for training robust models and that our proposed self-trained method improves the prediction quality of ECG delineation.","sentences":["Electrocardiogram (ECG) delineation plays a crucial role in assisting cardiologists with accurate diagnoses.","Prior research studies have explored various methods, including the application of deep learning techniques, to achieve precise delineation.","However, existing approaches face limitations primarily related to dataset size and robustness.","In this paper, we introduce a dataset for ECG delineation and propose a novel self-trained method aimed at leveraging a vast amount of unlabeled ECG data.","Our approach involves the pseudolabeling of unlabeled data using a neural network trained on our dataset.","Subsequently, we train the model on the newly labeled samples to enhance the quality of delineation.","We conduct experiments demonstrating that our dataset is a valuable resource for training robust models and that our proposed self-trained method improves the prediction quality of ECG delineation."],"url":"http://arxiv.org/abs/2406.02711v1","category":"cs.LG"}
{"created":"2024-06-04 18:38:42","title":"Coupled transport equations with freezing","abstract":"We study a system of two coupled transport equations with freezing. The solutions freeze in time when they are equal. We prove existence and uniqueness of continuous solutions if the initial conditions are continuous. We discuss several qualitative and quantitative properties of the solutions. The equations arise in a model for collisions of a large number of tightly spaced balls.","sentences":["We study a system of two coupled transport equations with freezing.","The solutions freeze in time when they are equal.","We prove existence and uniqueness of continuous solutions if the initial conditions are continuous.","We discuss several qualitative and quantitative properties of the solutions.","The equations arise in a model for collisions of a large number of tightly spaced balls."],"url":"http://arxiv.org/abs/2406.02707v1","category":"math.AP"}
{"created":"2024-06-04 18:00:02","title":"Discrete torsion in gauging non-invertible symmetries","abstract":"In this paper we explain that there exist two complementary generalizations of discrete torsion for non-invertible symmetries in 2d QFT's. Both characterizations are counted by $H^2(G,U(1))$ when one specializes to ordinary finite groups $G$. However, the counting is different for more general fusion categories. Furthermore, only one generalizes the picture of discrete torsion as differences in choices of gauge actions on $B$-fields. We also explain how this same generalization of discrete torsion gives rise to physically-sensible twists on gaugeable algebras and fiber functors.","sentences":["In this paper we explain that there exist two complementary generalizations of discrete torsion for non-invertible symmetries in 2d QFT's.","Both characterizations are counted by $H^2(G,U(1))$ when one specializes to ordinary finite groups $G$. However, the counting is different for more general fusion categories.","Furthermore, only one generalizes the picture of discrete torsion as differences in choices of gauge actions on $B$-fields.","We also explain how this same generalization of discrete torsion gives rise to physically-sensible twists on gaugeable algebras and fiber functors."],"url":"http://arxiv.org/abs/2406.02676v1","category":"hep-th"}
{"created":"2024-06-04 18:00:02","title":"Quantum Transport Theory of Strongly Correlated Matter","abstract":"This report reviews recent progress in computing Kubo formulas for general interacting Hamiltonians. The aim is to calculate electric and thermal magneto-conductivities in strong scattering regimes where Boltzmann equation and Hall conductivity proxies exceed their validity. Three primary approaches are explained.   1. Degeneracy-projected polarization formulas for Hall-type conductivities, which substantially reduce the number of calculated current matrix elements. These expressions generalize the Berry curvature integral formulas to imperfect lattices. 2. Continued fraction representation of dynamical longitudinal conductivities. The calculations produce a set of thermodynamic averages, which can be controllably extrapolated using their mathematical relations to low and high frequency conductivity asymptotics. 3. Hall-type coefficients summation formulas, which are constructed from thermodynamic averages.   The thermodynamic formulas are derived in the operator Hilbert space formalism, which avoids the opacity and high computational cost of the Hamiltonian eigenspectrum. The coefficients can be obtained by well established imaginary-time Monte Carlo sampling, high temperature expansion, traces of operator products, and variational wavefunctions at low temperatures. We demonstrate the power of approaches 1--3 by their application to well known models of lattice electrons and bosons. The calculations clarify the far-reaching influence of strong local interactions on the metallic transport near Mott insulators. Future directions for these approaches are discussed.","sentences":["This report reviews recent progress in computing Kubo formulas for general interacting Hamiltonians.","The aim is to calculate electric and thermal magneto-conductivities in strong scattering regimes where Boltzmann equation and Hall conductivity proxies exceed their validity.","Three primary approaches are explained.   ","1. Degeneracy-projected polarization formulas for Hall-type conductivities, which substantially reduce the number of calculated current matrix elements.","These expressions generalize the Berry curvature integral formulas to imperfect lattices.","2. Continued fraction representation of dynamical longitudinal conductivities.","The calculations produce a set of thermodynamic averages, which can be controllably extrapolated using their mathematical relations to low and high frequency conductivity asymptotics.","3. Hall-type coefficients summation formulas, which are constructed from thermodynamic averages.   ","The thermodynamic formulas are derived in the operator Hilbert space formalism, which avoids the opacity and high computational cost of the Hamiltonian eigenspectrum.","The coefficients can be obtained by well established imaginary-time Monte Carlo sampling, high temperature expansion, traces of operator products, and variational wavefunctions at low temperatures.","We demonstrate the power of approaches 1--3 by their application to well known models of lattice electrons and bosons.","The calculations clarify the far-reaching influence of strong local interactions on the metallic transport near Mott insulators.","Future directions for these approaches are discussed."],"url":"http://arxiv.org/abs/2406.02677v1","category":"cond-mat.str-el"}
{"created":"2024-06-04 18:00:02","title":"Moduli Spaces in CFT: Bootstrap Equation in a Perturbative Example","abstract":"Conformal field theories that exhibit spontaneous breaking of conformal symmetry (a moduli space of vacua) must satisfy a set of bootstrap constraints, involving the usual data (scaling dimensions and OPE coefficients) as well as new data such as the spectrum of asymptotic states in the broken vacuum and form factors. The simplest bootstrap equation arises by expanding a two-point function of local operators in two channels, at short distance using the OPE and at large distance using the EFT in the broken vacuum. We illustrate this equation in what is arguably the simplest perturbative model that exhibits conformal symmetry breaking, namely the real $ABC$ model in $d = 4 -\\epsilon$ dimensions. We investigate the convergence properties of the bootstrap equation and check explicitly many of the non-trivial relations that it imposes on theory data.","sentences":["Conformal field theories that exhibit spontaneous breaking of conformal symmetry (a moduli space of vacua) must satisfy a set of bootstrap constraints, involving the usual data (scaling dimensions and OPE coefficients) as well as new data such as the spectrum of asymptotic states in the broken vacuum and form factors.","The simplest bootstrap equation arises by expanding a two-point function of local operators in two channels, at short distance using the OPE and at large distance using the EFT in the broken vacuum.","We illustrate this equation in what is arguably the simplest perturbative model that exhibits conformal symmetry breaking, namely the real $ABC$ model in $d = 4 -\\epsilon$ dimensions.","We investigate the convergence properties of the bootstrap equation and check explicitly many of the non-trivial relations that it imposes on theory data."],"url":"http://arxiv.org/abs/2406.02679v1","category":"hep-th"}
{"created":"2024-06-04 18:00:00","title":"Symmetric Kernels with Non-Symmetric Data: A Data-Agnostic Learnability Bound","abstract":"Kernel ridge regression (KRR) and Gaussian processes (GPs) are fundamental tools in statistics and machine learning with recent applications to highly over-parameterized deep neural networks. The ability of these tools to learn a target function is directly related to the eigenvalues of their kernel sampled on the input data. Targets having support on higher eigenvalues are more learnable. While kernels are often highly symmetric objects, the data is often not. Thus kernel symmetry seems to have little to no bearing on the above eigenvalues or learnability, making spectral analysis on real-world data challenging. Here, we show that contrary to this common lure, one may use eigenvalues and eigenfunctions associated with highly idealized data-measures to bound learnability on realistic data. As a demonstration, we give a theoretical lower bound on the sample complexity of copying heads for kernels associated with generic transformers acting on natural language.","sentences":["Kernel ridge regression (KRR) and Gaussian processes (GPs) are fundamental tools in statistics and machine learning with recent applications to highly over-parameterized deep neural networks.","The ability of these tools to learn a target function is directly related to the eigenvalues of their kernel sampled on the input data.","Targets having support on higher eigenvalues are more learnable.","While kernels are often highly symmetric objects, the data is often not.","Thus kernel symmetry seems to have little to no bearing on the above eigenvalues or learnability, making spectral analysis on real-world data challenging.","Here, we show that contrary to this common lure, one may use eigenvalues and eigenfunctions associated with highly idealized data-measures to bound learnability on realistic data.","As a demonstration, we give a theoretical lower bound on the sample complexity of copying heads for kernels associated with generic transformers acting on natural language."],"url":"http://arxiv.org/abs/2406.02663v1","category":"stat.ML"}
{"created":"2024-06-04 18:00:00","title":"Higher-order corrections to phase-transition parameters in dimensional reduction","abstract":"The dynamics of phase transitions (PT) in quantum field theories at finite temperature is most accurately described within the framework of dimensional reduction. In this framework, thermodynamic quantities are computed within the 3-dimensional effective field theory (EFT) that results from integrating out the high-temperature Matsubara modes. However, strong-enough PTs, observable in gravitational wave (GW) detectors, occur often nearby the limit of validity of the EFT, where effective operators can no longer be neglected. Here, we perform a quantitative analysis of the impact of these interactions on the determination of PT parameters. We find that they allow for strong PTs in a wider region of parameter space, and that both the peak frequency and the amplitude of the resulting GW power spectrum can change by more than one order of magnitude when they are included. As a byproduct of this work, we derive equations for computing the bounce solution in the presence of higher-derivative terms, consistently with the EFT power counting.","sentences":["The dynamics of phase transitions (PT) in quantum field theories at finite temperature is most accurately described within the framework of dimensional reduction.","In this framework, thermodynamic quantities are computed within the 3-dimensional effective field theory (EFT) that results from integrating out the high-temperature Matsubara modes.","However, strong-enough PTs, observable in gravitational wave (GW) detectors, occur often nearby the limit of validity of the EFT, where effective operators can no longer be neglected.","Here, we perform a quantitative analysis of the impact of these interactions on the determination of PT parameters.","We find that they allow for strong PTs in a wider region of parameter space, and that both the peak frequency and the amplitude of the resulting GW power spectrum can change by more than one order of magnitude when they are included.","As a byproduct of this work, we derive equations for computing the bounce solution in the presence of higher-derivative terms, consistently with the EFT power counting."],"url":"http://arxiv.org/abs/2406.02667v1","category":"hep-ph"}
{"created":"2024-06-04 17:52:14","title":"Maintaining Diversity Provably Helps in Evolutionary Multimodal Optimization","abstract":"In the real world, there exist a class of optimization problems that multiple (local) optimal solutions in the solution space correspond to a single point in the objective space. In this paper, we theoretically show that for such multimodal problems, a simple method that considers the diversity of solutions in the solution space can benefit the search in evolutionary algorithms (EAs). Specifically, we prove that the proposed method, working with crossover, can help enhance the exploration, leading to polynomial or even exponential acceleration on the expected running time. This result is derived by rigorous running time analysis in both single-objective and multi-objective scenarios, including $(\\mu+1)$-GA solving the widely studied single-objective problem, Jump, and NSGA-II and SMS-EMOA (two well-established multi-objective EAs) solving the widely studied bi-objective problem, OneJumpZeroJump. Experiments are also conducted to validate the theoretical results. We hope that our results may encourage the exploration of diversity maintenance in the solution space for multi-objective optimization, where existing EAs usually only consider the diversity in the objective space and can easily be trapped in local optima.","sentences":["In the real world, there exist a class of optimization problems that multiple (local) optimal solutions in the solution space correspond to a single point in the objective space.","In this paper, we theoretically show that for such multimodal problems, a simple method that considers the diversity of solutions in the solution space can benefit the search in evolutionary algorithms (EAs).","Specifically, we prove that the proposed method, working with crossover, can help enhance the exploration, leading to polynomial or even exponential acceleration on the expected running time.","This result is derived by rigorous running time analysis in both single-objective and multi-objective scenarios, including $(\\mu+1)$-GA solving the widely studied single-objective problem, Jump, and NSGA-II and SMS-EMOA (two well-established multi-objective EAs) solving the widely studied bi-objective problem, OneJumpZeroJump.","Experiments are also conducted to validate the theoretical results.","We hope that our results may encourage the exploration of diversity maintenance in the solution space for multi-objective optimization, where existing EAs usually only consider the diversity in the objective space and can easily be trapped in local optima."],"url":"http://arxiv.org/abs/2406.02658v1","category":"cs.NE"}
{"created":"2024-06-04 17:39:31","title":"Pairing susceptibility in the multi-layer Hubbard model","abstract":"We present a systematic study of the interaction, doping, and layer dependence of the $d_{x^2-y^2}$-wave pairing susceptibility of the Hubbard model for a stacked 2D square lattice. We perform a multi-index perturbative expansion up to fourth-order to obtain coefficients in powers of the Hubbard $U$, the inter-layer $V$, and the pair-hopping $J$ interactions. We evaluate the vertex diagrams that contribute to the pairing susceptibility for $\\ell= 2,3, 4$ layered models in the U-V-J interaction space. This provides unprecedented access to the pairing amplitudes, allowing us to identify the processes that enhance or reduce pairing. We distinguish pairing within the diagonal channel, $P^{\\parallel}_{d}$, and off-diagonal channel, $P^{\\perp}_{d}$, and find that, in the absence of $J$, the qualitative behavior of the layered system is equivalent to the single-layer model. In the presence of $J$, we show that pairing is enhanced sublinearly with increasing $\\ell$ and is primarily mediated by the $P^{\\perp}_{d}$ component and find which coefficients and diagram sets are responsible. Finally, we construct a generalized $\\ell$-dependent equation for the d-wave pair susceptibility to speculate pairing beyond $\\ell=4$.","sentences":["We present a systematic study of the interaction, doping, and layer dependence of the $d_{x^2-y^2}$-wave pairing susceptibility of the Hubbard model for a stacked 2D square lattice.","We perform a multi-index perturbative expansion up to fourth-order to obtain coefficients in powers of the Hubbard $U$, the inter-layer $V$, and the pair-hopping $J$ interactions.","We evaluate the vertex diagrams that contribute to the pairing susceptibility for $\\ell= 2,3, 4$ layered models in the U-V-J interaction space.","This provides unprecedented access to the pairing amplitudes, allowing us to identify the processes that enhance or reduce pairing.","We distinguish pairing within the diagonal channel, $P^{\\parallel}_{d}$, and off-diagonal channel, $P^{\\perp}_{d}$, and find that, in the absence of $J$, the qualitative behavior of the layered system is equivalent to the single-layer model.","In the presence of $J$, we show that pairing is enhanced sublinearly with increasing $\\ell$ and is primarily mediated by the $P^{\\perp}_{d}$ component and find which coefficients and diagram sets are responsible.","Finally, we construct a generalized $\\ell$-dependent equation for the d-wave pair susceptibility to speculate pairing beyond $\\ell=4$."],"url":"http://arxiv.org/abs/2406.02656v1","category":"cond-mat.supr-con"}
{"created":"2024-06-05 17:09:51","title":"Polarization Wavefront Lidar: Learning Large Scene Reconstruction from Polarized Wavefronts","abstract":"Lidar has become a cornerstone sensing modality for 3D vision, especially for large outdoor scenarios and autonomous driving. Conventional lidar sensors are capable of providing centimeter-accurate distance information by emitting laser pulses into a scene and measuring the time-of-flight (ToF) of the reflection. However, the polarization of the received light that depends on the surface orientation and material properties is usually not considered. As such, the polarization modality has the potential to improve scene reconstruction beyond distance measurements. In this work, we introduce a novel long-range polarization wavefront lidar sensor (PolLidar) that modulates the polarization of the emitted and received light. Departing from conventional lidar sensors, PolLidar allows access to the raw time-resolved polarimetric wavefronts. We leverage polarimetric wavefronts to estimate normals, distance, and material properties in outdoor scenarios with a novel learned reconstruction method. To train and evaluate the method, we introduce a simulated and real-world long-range dataset with paired raw lidar data, ground truth distance, and normal maps. We find that the proposed method improves normal and distance reconstruction by 53\\% mean angular error and 41\\% mean absolute error compared to existing shape-from-polarization (SfP) and ToF methods. Code and data are open-sourced at https://light.princeton.edu/pollidar.","sentences":["Lidar has become a cornerstone sensing modality for 3D vision, especially for large outdoor scenarios and autonomous driving.","Conventional lidar sensors are capable of providing centimeter-accurate distance information by emitting laser pulses into a scene and measuring the time-of-flight (ToF) of the reflection.","However, the polarization of the received light that depends on the surface orientation and material properties is usually not considered.","As such, the polarization modality has the potential to improve scene reconstruction beyond distance measurements.","In this work, we introduce a novel long-range polarization wavefront lidar sensor (PolLidar) that modulates the polarization of the emitted and received light.","Departing from conventional lidar sensors, PolLidar allows access to the raw time-resolved polarimetric wavefronts.","We leverage polarimetric wavefronts to estimate normals, distance, and material properties in outdoor scenarios with a novel learned reconstruction method.","To train and evaluate the method, we introduce a simulated and real-world long-range dataset with paired raw lidar data, ground truth distance, and normal maps.","We find that the proposed method improves normal and distance reconstruction by 53\\% mean angular error and 41\\% mean absolute error compared to existing shape-from-polarization (SfP) and","ToF methods.","Code and data are open-sourced at https://light.princeton.edu/pollidar."],"url":"http://arxiv.org/abs/2406.03461v1","category":"cs.CV"}
{"created":"2024-06-05 17:07:39","title":"The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement","abstract":"To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics. However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see. The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation. For this, we introduce enhancement models that exploit the widely used PESQ measure. Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment. While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric, an isolated evaluation on the same metric may be misleading. Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening.","sentences":["To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics.","However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see.","The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation.","For this, we introduce enhancement models that exploit the widely used PESQ measure.","Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment.","While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t.","a metric, an isolated evaluation on the same metric may be misleading.","Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening."],"url":"http://arxiv.org/abs/2406.03460v1","category":"eess.AS"}
{"created":"2024-06-05 16:40:53","title":"Pre-trained Large Language Models Use Fourier Features to Compute Addition","abstract":"Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained LLMs add numbers using Fourier features -- dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features. Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy. Introducing pre-trained token embeddings to a randomly initialized model rescues its performance. Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.","sentences":["Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear.","This paper shows that pre-trained LLMs add numbers using Fourier features -- dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain.","Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features.","Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy.","Introducing pre-trained token embeddings to a randomly initialized model rescues its performance.","Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks."],"url":"http://arxiv.org/abs/2406.03445v1","category":"cs.LG"}
{"created":"2024-06-05 16:33:30","title":"Transfer Learning for Latent Variable Network Models","abstract":"We study transfer learning for estimation in latent variable network models. In our setting, the conditional edge probability matrices given the latent variables are represented by $P$ for the source and $Q$ for the target. We wish to estimate $Q$ given two kinds of data: (1) edge data from a subgraph induced by an $o(1)$ fraction of the nodes of $Q$, and (2) edge data from all of $P$. If the source $P$ has no relation to the target $Q$, the estimation error must be $\\Omega(1)$. However, we show that if the latent variables are shared, then vanishing error is possible. We give an efficient algorithm that utilizes the ordering of a suitably defined graph distance. Our algorithm achieves $o(1)$ error and does not assume a parametric form on the source or target networks. Next, for the specific case of Stochastic Block Models we prove a minimax lower bound and show that a simple algorithm achieves this rate. Finally, we empirically demonstrate our algorithm's use on real-world and simulated graph transfer problems.","sentences":["We study transfer learning for estimation in latent variable network models.","In our setting, the conditional edge probability matrices given the latent variables are represented by $P$ for the source and $Q$ for the target.","We wish to estimate $Q$ given two kinds of data: (1) edge data from a subgraph induced by an $o(1)$ fraction of the nodes of $Q$, and (2) edge data from all of $P$. If the source $P$ has no relation to the target $Q$, the estimation error must be $\\Omega(1)$. However, we show that if the latent variables are shared, then vanishing error is possible.","We give an efficient algorithm that utilizes the ordering of a suitably defined graph distance.","Our algorithm achieves $o(1)$ error and does not assume a parametric form on the source or target networks.","Next, for the specific case of Stochastic Block Models we prove a minimax lower bound and show that a simple algorithm achieves this rate.","Finally, we empirically demonstrate our algorithm's use on real-world and simulated graph transfer problems."],"url":"http://arxiv.org/abs/2406.03437v1","category":"cs.LG"}
{"created":"2024-06-05 15:18:08","title":"LLM-based Rewriting of Inappropriate Argumentation using Reinforcement Learning from Machine Feedback","abstract":"Ensuring that online discussions are civil and productive is a major challenge for social media platforms. Such platforms usually rely both on users and on automated detection tools to flag inappropriate arguments of other users, which moderators then review. However, this kind of post-hoc moderation is expensive and time-consuming, and moderators are often overwhelmed by the amount and severity of flagged content. Instead, a promising alternative is to prevent negative behavior during content creation. This paper studies how inappropriate language in arguments can be computationally mitigated. We propose a reinforcement learning-based rewriting approach that balances content preservation and appropriateness based on existing classifiers, prompting an instruction-finetuned large language model (LLM) as our initial policy. Unlike related style transfer tasks, rewriting inappropriate arguments allows deleting and adding content permanently. It is therefore tackled on document level rather than sentence level. We evaluate different weighting schemes for the reward function in both absolute and relative human assessment studies. Systematic experiments on non-parallel data provide evidence that our approach can mitigate the inappropriateness of arguments while largely preserving their content. It significantly outperforms competitive baselines, including few-shot learning, prompting, and humans.","sentences":["Ensuring that online discussions are civil and productive is a major challenge for social media platforms.","Such platforms usually rely both on users and on automated detection tools to flag inappropriate arguments of other users, which moderators then review.","However, this kind of post-hoc moderation is expensive and time-consuming, and moderators are often overwhelmed by the amount and severity of flagged content.","Instead, a promising alternative is to prevent negative behavior during content creation.","This paper studies how inappropriate language in arguments can be computationally mitigated.","We propose a reinforcement learning-based rewriting approach that balances content preservation and appropriateness based on existing classifiers, prompting an instruction-finetuned large language model (LLM) as our initial policy.","Unlike related style transfer tasks, rewriting inappropriate arguments allows deleting and adding content permanently.","It is therefore tackled on document level rather than sentence level.","We evaluate different weighting schemes for the reward function in both absolute and relative human assessment studies.","Systematic experiments on non-parallel data provide evidence that our approach can mitigate the inappropriateness of arguments while largely preserving their content.","It significantly outperforms competitive baselines, including few-shot learning, prompting, and humans."],"url":"http://arxiv.org/abs/2406.03363v1","category":"cs.CL"}
{"created":"2024-06-05 14:37:42","title":"UDQL: Bridging The Gap between MSE Loss and The Optimal Value Function in Offline Reinforcement Learning","abstract":"The Mean Square Error (MSE) is commonly utilized to estimate the solution of the optimal value function in the vast majority of offline reinforcement learning (RL) models and has achieved outstanding performance. However, we find that its principle can lead to overestimation phenomenon for the value function. In this paper, we first theoretically analyze overestimation phenomenon led by MSE and provide the theoretical upper bound of the overestimated error. Furthermore, to address it, we propose a novel Bellman underestimated operator to counteract overestimation phenomenon and then prove its contraction characteristics. At last, we propose the offline RL algorithm based on underestimated operator and diffusion policy model. Extensive experimental results on D4RL tasks show that our method can outperform state-of-the-art offline RL algorithms, which demonstrates that our theoretical analysis and underestimation way are effective for offline RL tasks.","sentences":["The Mean Square Error (MSE) is commonly utilized to estimate the solution of the optimal value function in the vast majority of offline reinforcement learning (RL) models and has achieved outstanding performance.","However, we find that its principle can lead to overestimation phenomenon for the value function.","In this paper, we first theoretically analyze overestimation phenomenon led by MSE and provide the theoretical upper bound of the overestimated error.","Furthermore, to address it, we propose a novel Bellman underestimated operator to counteract overestimation phenomenon and then prove its contraction characteristics.","At last, we propose the offline RL algorithm based on underestimated operator and diffusion policy model.","Extensive experimental results on D4RL tasks show that our method can outperform state-of-the-art offline RL algorithms, which demonstrates that our theoretical analysis and underestimation way are effective for offline RL tasks."],"url":"http://arxiv.org/abs/2406.03324v1","category":"cs.LG"}
{"created":"2024-06-05 14:23:13","title":"Photometric segregation of dwarf and giant FGK stars using the SVO Filter Profile Service and photometric tools","abstract":"This paper is focused on the segregation of FGK dwarf and giant stars through narrow-band photometric data using the Spanish Virtual Observatory (SVO) Filter Profile Service and associated photometric tools. We selected spectra from the MILES, STELIB, and ELODIE stellar libraries, and used SVO photometric tools to derive the synthetic photometry in 15 J-PAS narrow filters, which were especially selected to cover spectral features sensitive to gravity changes. Using machine-learning techniques as the Gaussian mixture model and the support vector machine, we defined several criteria based on J-PAS colours to discriminate between dwarf and giant stars. We selected five colour-colour diagrams that presented the most promising separation between both samples. Our results show an overall accuracy in the studied sample of $\\sim$0.97 for FGK stars, although a dependence on the luminosity type and the stellar effective temperature was found. We also defined a colour-temperature relation for dwarf stars with effective temperatures between 4\\,000 and 7\\,000\\,K, which allows one to estimate the stellar effective temperature from four J-PAS filters ($J0450$, $J0510$, $J0550$, and $J0620$). Additionally, we extended the study to M-type giant and dwarf stars, achieving a similar accuracy to that for FGK stars.","sentences":["This paper is focused on the segregation of FGK dwarf and giant stars through narrow-band photometric data using the Spanish Virtual Observatory (SVO) Filter Profile Service and associated photometric tools.","We selected spectra from the MILES, STELIB, and ELODIE stellar libraries, and used SVO photometric tools to derive the synthetic photometry in 15 J-PAS narrow filters, which were especially selected to cover spectral features sensitive to gravity changes.","Using machine-learning techniques as the Gaussian mixture model and the support vector machine, we defined several criteria based on J-PAS colours to discriminate between dwarf and giant stars.","We selected five colour-colour diagrams that presented the most promising separation between both samples.","Our results show an overall accuracy in the studied sample of $\\sim$0.97 for FGK stars, although a dependence on the luminosity type and the stellar effective temperature was found.","We also defined a colour-temperature relation for dwarf stars with effective temperatures between 4\\,000 and 7\\,000\\,K, which allows one to estimate the stellar effective temperature from four J-PAS filters ($J0450$, $J0510$, $J0550$, and $J0620$).","Additionally, we extended the study to M-type giant and dwarf stars, achieving a similar accuracy to that for FGK stars."],"url":"http://arxiv.org/abs/2406.03310v1","category":"astro-ph.SR"}
{"created":"2024-06-05 13:57:06","title":"Efficient Data-Parallel Continual Learning with Asynchronous Distributed Rehearsal Buffers","abstract":"Deep learning has emerged as a powerful method for extracting valuable information from large volumes of data. However, when new training data arrives continuously (i.e., is not fully available from the beginning), incremental training suffers from catastrophic forgetting (i.e., new patterns are reinforced at the expense of previously acquired knowledge). Training from scratch each time new training data becomes available would result in extremely long training times and massive data accumulation. Rehearsal-based continual learning has shown promise for addressing the catastrophic forgetting challenge, but research to date has not addressed performance and scalability. To fill this gap, we propose an approach based on a distributed rehearsal buffer that efficiently complements data-parallel training on multiple GPUs, allowing us to achieve short runtime and scalability while retaining high accuracy. It leverages a set of buffers (local to each GPU) and uses several asynchronous techniques for updating these local buffers in an embarrassingly parallel fashion, all while handling the communication overheads necessary to augment input mini-batches (groups of training samples fed to the model) using unbiased, global sampling. In this paper we explore the benefits of this approach for classification models. We run extensive experiments on up to 128 GPUs of the ThetaGPU supercomputer to compare our approach with baselines representative of training-from-scratch (the upper bound in terms of accuracy) and incremental training (the lower bound). Results show that rehearsal-based continual learning achieves a top-5 classification accuracy close to the upper bound, while simultaneously exhibiting a runtime close to the lower bound.","sentences":["Deep learning has emerged as a powerful method for extracting valuable information from large volumes of data.","However, when new training data arrives continuously (i.e., is not fully available from the beginning), incremental training suffers from catastrophic forgetting (i.e., new patterns are reinforced at the expense of previously acquired knowledge).","Training from scratch each time new training data becomes available would result in extremely long training times and massive data accumulation.","Rehearsal-based continual learning has shown promise for addressing the catastrophic forgetting challenge, but research to date has not addressed performance and scalability.","To fill this gap, we propose an approach based on a distributed rehearsal buffer that efficiently complements data-parallel training on multiple GPUs, allowing us to achieve short runtime and scalability while retaining high accuracy.","It leverages a set of buffers (local to each GPU) and uses several asynchronous techniques for updating these local buffers in an embarrassingly parallel fashion, all while handling the communication overheads necessary to augment input mini-batches (groups of training samples fed to the model) using unbiased, global sampling.","In this paper we explore the benefits of this approach for classification models.","We run extensive experiments on up to 128 GPUs of the ThetaGPU supercomputer to compare our approach with baselines representative of training-from-scratch (the upper bound in terms of accuracy) and incremental training (the lower bound).","Results show that rehearsal-based continual learning achieves a top-5 classification accuracy close to the upper bound, while simultaneously exhibiting a runtime close to the lower bound."],"url":"http://arxiv.org/abs/2406.03285v1","category":"cs.DC"}
{"created":"2024-06-05 13:06:33","title":"Defending Large Language Models Against Attacks With Residual Stream Activation Analysis","abstract":"The widespread adoption of Large Language Models (LLMs), exemplified by OpenAI's ChatGPT, brings to the forefront the imperative to defend against adversarial threats on these models. These attacks, which manipulate an LLM's output by introducing malicious inputs, undermine the model's integrity and the trust users place in its outputs. In response to this challenge, our paper presents an innovative defensive strategy, given white box access to an LLM, that harnesses residual activation analysis between transformer layers of the LLM. We apply an established methodology for analyzing distinctive activation patterns in the residual streams for a novel result of attack prompt classification. We curate multiple datasets to demonstrate how this method of classification has high accuracy across multiple types of attack scenarios, including our newly-created attack dataset. Furthermore, we enhance the model's resilience by integrating safety fine-tuning techniques for LLMs in order to measure its effect on our capability to detect attacks. The results underscore the effectiveness of our approach in enhancing the detection and mitigation of adversarial inputs, advancing the security framework within which LLMs operate.","sentences":["The widespread adoption of Large Language Models (LLMs), exemplified by OpenAI's ChatGPT, brings to the forefront the imperative to defend against adversarial threats on these models.","These attacks, which manipulate an LLM's output by introducing malicious inputs, undermine the model's integrity and the trust users place in its outputs.","In response to this challenge, our paper presents an innovative defensive strategy, given white box access to an LLM, that harnesses residual activation analysis between transformer layers of the LLM.","We apply an established methodology for analyzing distinctive activation patterns in the residual streams for a novel result of attack prompt classification.","We curate multiple datasets to demonstrate how this method of classification has high accuracy across multiple types of attack scenarios, including our newly-created attack dataset.","Furthermore, we enhance the model's resilience by integrating safety fine-tuning techniques for LLMs in order to measure its effect on our capability to detect attacks.","The results underscore the effectiveness of our approach in enhancing the detection and mitigation of adversarial inputs, advancing the security framework within which LLMs operate."],"url":"http://arxiv.org/abs/2406.03230v1","category":"cs.CR"}
{"created":"2024-06-05 13:03:06","title":"Interactive Image Selection and Training for Brain Tumor Segmentation Network","abstract":"Medical image segmentation is a relevant problem, with deep learning being an exponent. However, the necessity of a high volume of fully annotated images for training massive models can be a problem, especially for applications whose images present a great diversity, such as brain tumors, which can occur in different sizes and shapes. In contrast, a recent methodology, Feature Learning from Image Markers (FLIM), has involved an expert in the learning loop, producing small networks that require few images to train the convolutional layers. In this work, We employ an interactive method for image selection and training based on FLIM, exploring the user's knowledge. The results demonstrated that with our methodology, we could choose a small set of images to train the encoder of a U-shaped network, obtaining performance equal to manual selection and even surpassing the same U-shaped network trained with backpropagation and all training images.","sentences":["Medical image segmentation is a relevant problem, with deep learning being an exponent.","However, the necessity of a high volume of fully annotated images for training massive models can be a problem, especially for applications whose images present a great diversity, such as brain tumors, which can occur in different sizes and shapes.","In contrast, a recent methodology, Feature Learning from Image Markers (FLIM), has involved an expert in the learning loop, producing small networks that require few images to train the convolutional layers.","In this work, We employ an interactive method for image selection and training based on FLIM, exploring the user's knowledge.","The results demonstrated that with our methodology, we could choose a small set of images to train the encoder of a U-shaped network, obtaining performance equal to manual selection and even surpassing the same U-shaped network trained with backpropagation and all training images."],"url":"http://arxiv.org/abs/2406.03225v1","category":"cs.CV"}
{"created":"2024-06-05 11:42:46","title":"Ethical considerations of use of hold-out sets in clinical prediction model management","abstract":"Clinical prediction models are statistical or machine learning models used to quantify the risk of a certain health outcome using patient data. These can then inform potential interventions on patients, causing an effect called performative prediction: predictions inform interventions which influence the outcome they were trying to predict, leading to a potential underestimation of risk in some patients if a model is updated on this data. One suggested resolution to this is the use of hold-out sets, in which a set of patients do not receive model derived risk scores, such that a model can be safely retrained. We present an overview of clinical and research ethics regarding potential implementation of hold-out sets for clinical prediction models in health settings. We focus on the ethical principles of beneficence, non-maleficence, autonomy and justice. We also discuss informed consent, clinical equipoise, and truth-telling. We present illustrative cases of potential hold-out set implementations and discuss statistical issues arising from different hold-out set sampling methods. We also discuss differences between hold-out sets and randomised control trials, in terms of ethics and statistical issues. Finally, we give practical recommendations for researchers interested in the use hold-out sets for clinical prediction models.","sentences":["Clinical prediction models are statistical or machine learning models used to quantify the risk of a certain health outcome using patient data.","These can then inform potential interventions on patients, causing an effect called performative prediction: predictions inform interventions which influence the outcome they were trying to predict, leading to a potential underestimation of risk in some patients if a model is updated on this data.","One suggested resolution to this is the use of hold-out sets, in which a set of patients do not receive model derived risk scores, such that a model can be safely retrained.","We present an overview of clinical and research ethics regarding potential implementation of hold-out sets for clinical prediction models in health settings.","We focus on the ethical principles of beneficence, non-maleficence, autonomy and justice.","We also discuss informed consent, clinical equipoise, and truth-telling.","We present illustrative cases of potential hold-out set implementations and discuss statistical issues arising from different hold-out set sampling methods.","We also discuss differences between hold-out sets and randomised control trials, in terms of ethics and statistical issues.","Finally, we give practical recommendations for researchers interested in the use hold-out sets for clinical prediction models."],"url":"http://arxiv.org/abs/2406.03161v1","category":"cs.LG"}
{"created":"2024-06-05 10:55:11","title":"On the Power of Randomization in Fair Classification and Representation","abstract":"Fair classification and fair representation learning are two important problems in supervised and unsupervised fair machine learning, respectively. Fair classification asks for a classifier that maximizes accuracy on a given data distribution subject to fairness constraints. Fair representation maps a given data distribution over the original feature space to a distribution over a new representation space such that all classifiers over the representation satisfy fairness. In this paper, we examine the power of randomization in both these problems to minimize the loss of accuracy that results when we impose fairness constraints. Previous work on fair classification has characterized the optimal fair classifiers on a given data distribution that maximize accuracy subject to fairness constraints, e.g., Demographic Parity (DP), Equal Opportunity (EO), and Predictive Equality (PE). We refine these characterizations to demonstrate when the optimal randomized fair classifiers can surpass their deterministic counterparts in accuracy. We also show how the optimal randomized fair classifier that we characterize can be obtained as a solution to a convex optimization problem. Recent work has provided techniques to construct fair representations for a given data distribution such that any classifier over this representation satisfies DP. However, the classifiers on these fair representations either come with no or weak accuracy guarantees when compared to the optimal fair classifier on the original data distribution. Extending our ideas for randomized fair classification, we improve on these works, and construct DP-fair, EO-fair, and PE-fair representations that have provably optimal accuracy and suffer no accuracy loss compared to the optimal DP-fair, EO-fair, and PE-fair classifiers respectively on the original data distribution.","sentences":["Fair classification and fair representation learning are two important problems in supervised and unsupervised fair machine learning, respectively.","Fair classification asks for a classifier that maximizes accuracy on a given data distribution subject to fairness constraints.","Fair representation maps a given data distribution over the original feature space to a distribution over a new representation space such that all classifiers over the representation satisfy fairness.","In this paper, we examine the power of randomization in both these problems to minimize the loss of accuracy that results when we impose fairness constraints.","Previous work on fair classification has characterized the optimal fair classifiers on a given data distribution that maximize accuracy subject to fairness constraints, e.g., Demographic Parity (DP), Equal Opportunity (EO), and Predictive Equality (PE).","We refine these characterizations to demonstrate when the optimal randomized fair classifiers can surpass their deterministic counterparts in accuracy.","We also show how the optimal randomized fair classifier that we characterize can be obtained as a solution to a convex optimization problem.","Recent work has provided techniques to construct fair representations for a given data distribution such that any classifier over this representation satisfies DP.","However, the classifiers on these fair representations either come with no or weak accuracy guarantees when compared to the optimal fair classifier on the original data distribution.","Extending our ideas for randomized fair classification, we improve on these works, and construct DP-fair, EO-fair, and PE-fair representations that have provably optimal accuracy and suffer no accuracy loss compared to the optimal DP-fair, EO-fair, and PE-fair classifiers respectively on the original data distribution."],"url":"http://arxiv.org/abs/2406.03142v1","category":"cs.LG"}
{"created":"2024-06-05 10:13:55","title":"RevRIR: Joint Reverberant Speech and Room Impulse Response Embedding using Contrastive Learning with Application to Room Shape Classification","abstract":"This paper focuses on room fingerprinting, a task involving the analysis of an audio recording to determine the specific volume and shape of the room in which it was captured. While it is relatively straightforward to determine the basic room parameters from the Room Impulse Responses (RIR), doing so from a speech signal is a cumbersome task. To address this challenge, we introduce a dual-encoder architecture that facilitates the estimation of room parameters directly from speech utterances. During pre-training, one encoder receives the RIR while the other processes the reverberant speech signal. A contrastive loss function is employed to embed the speech and the acoustic response jointly. In the fine-tuning stage, the specific classification task is trained. In the test phase, only the reverberant utterance is available, and its embedding is used for the task of room shape classification. The proposed scheme is extensively evaluated using simulated acoustic environments.","sentences":["This paper focuses on room fingerprinting, a task involving the analysis of an audio recording to determine the specific volume and shape of the room in which it was captured.","While it is relatively straightforward to determine the basic room parameters from the Room Impulse Responses (RIR), doing so from a speech signal is a cumbersome task.","To address this challenge, we introduce a dual-encoder architecture that facilitates the estimation of room parameters directly from speech utterances.","During pre-training, one encoder receives the RIR while the other processes the reverberant speech signal.","A contrastive loss function is employed to embed the speech and the acoustic response jointly.","In the fine-tuning stage, the specific classification task is trained.","In the test phase, only the reverberant utterance is available, and its embedding is used for the task of room shape classification.","The proposed scheme is extensively evaluated using simulated acoustic environments."],"url":"http://arxiv.org/abs/2406.03120v1","category":"eess.AS"}
{"created":"2024-06-05 07:57:17","title":"From Tarzan to Tolkien: Controlling the Language Proficiency Level of LLMs for Content Generation","abstract":"We study the problem of controlling the difficulty level of text generated by Large Language Models (LLMs) for contexts where end-users are not fully proficient, such as language learners. Using a novel framework, we evaluate the effectiveness of several key approaches for this task, including few-shot prompting, supervised finetuning, and reinforcement learning (RL), utilising both GPT-4 and open source alternatives like LLama2-7B and Mistral-7B.   Our findings reveal a large performance gap between GPT-4 and the open source models when using prompt-based strategies. However, we show how to bridge this gap with a careful combination of finetuning and RL alignment. Our best model, CALM (CEFR-Aligned Language Model), surpasses the performance of GPT-4 and other strategies, at only a fraction of the cost. We further validate the quality of our results through a small-scale human study.","sentences":["We study the problem of controlling the difficulty level of text generated by Large Language Models (LLMs) for contexts where end-users are not fully proficient, such as language learners.","Using a novel framework, we evaluate the effectiveness of several key approaches for this task, including few-shot prompting, supervised finetuning, and reinforcement learning (RL), utilising both GPT-4 and open source alternatives like LLama2-7B and Mistral-7B.   Our findings reveal a large performance gap between GPT-4 and the open source models when using prompt-based strategies.","However, we show how to bridge this gap with a careful combination of finetuning and RL alignment.","Our best model, CALM (CEFR-Aligned Language Model), surpasses the performance of GPT-4 and other strategies, at only a fraction of the cost.","We further validate the quality of our results through a small-scale human study."],"url":"http://arxiv.org/abs/2406.03030v1","category":"cs.CL"}
{"created":"2024-06-05 07:38:55","title":"Is local opposition taking the wind out of the energy transition?","abstract":"Local opposition to the installation of renewable energy sources is a potential threat to the energy transition. Local communities tend to oppose the construction of energy plants due to the associated negative externalities (the so-called 'not in my backyard' or NIMBY phenomenon) according to widespread belief, mostly based on anecdotal evidence. Using administrative data on wind turbine installation and electoral outcomes across municipalities located in the South of Italy during 2000-19, we estimate the impact of wind turbines' installation on incumbent regional governments' electoral support during the next elections. Our main findings, derived by a wind-speed based instrumental variable strategy, point in the direction of a mild and not statistically significant electoral backlash for right-wing regional administrations and of a strong and statistically significant positive reinforcement for left-wing regional administrations. Based on our analysis, the hypothesis of an electoral effect of NIMBY type of behavior in connection with the development of wind turbines appears not to be supported by the data.","sentences":["Local opposition to the installation of renewable energy sources is a potential threat to the energy transition.","Local communities tend to oppose the construction of energy plants due to the associated negative externalities (the so-called 'not in my backyard' or NIMBY phenomenon) according to widespread belief, mostly based on anecdotal evidence.","Using administrative data on wind turbine installation and electoral outcomes across municipalities located in the South of Italy during 2000-19, we estimate the impact of wind turbines' installation on incumbent regional governments' electoral support during the next elections.","Our main findings, derived by a wind-speed based instrumental variable strategy, point in the direction of a mild and not statistically significant electoral backlash for right-wing regional administrations and of a strong and statistically significant positive reinforcement for left-wing regional administrations.","Based on our analysis, the hypothesis of an electoral effect of NIMBY type of behavior in connection with the development of wind turbines appears not to be supported by the data."],"url":"http://arxiv.org/abs/2406.03022v1","category":"econ.EM"}
{"created":"2024-06-05 06:52:29","title":"Quantifying Task Priority for Multi-Task Optimization","abstract":"The goal of multi-task learning is to learn diverse tasks within a single unified network. As each task has its own unique objective function, conflicts emerge during training, resulting in negative transfer among them. Earlier research identified these conflicting gradients in shared parameters between tasks and attempted to realign them in the same direction. However, we prove that such optimization strategies lead to sub-optimal Pareto solutions due to their inability to accurately determine the individual contributions of each parameter across various tasks. In this paper, we propose the concept of task priority to evaluate parameter contributions across different tasks. To learn task priority, we identify the type of connections related to links between parameters influenced by task-specific losses during backpropagation. The strength of connections is gauged by the magnitude of parameters to determine task priority. Based on these, we present a new method named connection strength-based optimization for multi-task learning which consists of two phases. The first phase learns the task priority within the network, while the second phase modifies the gradients while upholding this priority. This ultimately leads to finding new Pareto optimal solutions for multiple tasks. Through extensive experiments, we show that our approach greatly enhances multi-task performance in comparison to earlier gradient manipulation methods.","sentences":["The goal of multi-task learning is to learn diverse tasks within a single unified network.","As each task has its own unique objective function, conflicts emerge during training, resulting in negative transfer among them.","Earlier research identified these conflicting gradients in shared parameters between tasks and attempted to realign them in the same direction.","However, we prove that such optimization strategies lead to sub-optimal Pareto solutions due to their inability to accurately determine the individual contributions of each parameter across various tasks.","In this paper, we propose the concept of task priority to evaluate parameter contributions across different tasks.","To learn task priority, we identify the type of connections related to links between parameters influenced by task-specific losses during backpropagation.","The strength of connections is gauged by the magnitude of parameters to determine task priority.","Based on these, we present a new method named connection strength-based optimization for multi-task learning which consists of two phases.","The first phase learns the task priority within the network, while the second phase modifies the gradients while upholding this priority.","This ultimately leads to finding new Pareto optimal solutions for multiple tasks.","Through extensive experiments, we show that our approach greatly enhances multi-task performance in comparison to earlier gradient manipulation methods."],"url":"http://arxiv.org/abs/2406.02996v1","category":"cs.LG"}
{"created":"2024-06-05 06:42:27","title":"Predicting Genetic Mutation from Whole Slide Images via Biomedical-Linguistic Knowledge Enhanced Multi-label Classification","abstract":"Predicting genetic mutations from whole slide images is indispensable for cancer diagnosis. However, existing work training multiple binary classification models faces two challenges: (a) Training multiple binary classifiers is inefficient and would inevitably lead to a class imbalance problem. (b) The biological relationships among genes are overlooked, which limits the prediction performance. To tackle these challenges, we innovatively design a Biological-knowledge enhanced PathGenomic multi-label Transformer to improve genetic mutation prediction performances. BPGT first establishes a novel gene encoder that constructs gene priors by two carefully designed modules: (a) A gene graph whose node features are the genes' linguistic descriptions and the cancer phenotype, with edges modeled by genes' pathway associations and mutation consistencies. (b) A knowledge association module that fuses linguistic and biomedical knowledge into gene priors by transformer-based graph representation learning, capturing the intrinsic relationships between different genes' mutations. BPGT then designs a label decoder that finally performs genetic mutation prediction by two tailored modules: (a) A modality fusion module that firstly fuses the gene priors with critical regions in WSIs and obtains gene-wise mutation logits. (b) A comparative multi-label loss that emphasizes the inherent comparisons among mutation status to enhance the discrimination capabilities. Sufficient experiments on The Cancer Genome Atlas benchmark demonstrate that BPGT outperforms the state-of-the-art.","sentences":["Predicting genetic mutations from whole slide images is indispensable for cancer diagnosis.","However, existing work training multiple binary classification models faces two challenges: (a) Training multiple binary classifiers is inefficient and would inevitably lead to a class imbalance problem.","(b) The biological relationships among genes are overlooked, which limits the prediction performance.","To tackle these challenges, we innovatively design a Biological-knowledge enhanced PathGenomic multi-label Transformer to improve genetic mutation prediction performances.","BPGT first establishes a novel gene encoder that constructs gene priors by two carefully designed modules: (a) A gene graph whose node features are the genes' linguistic descriptions and the cancer phenotype, with edges modeled by genes' pathway associations and mutation consistencies.","(b) A knowledge association module that fuses linguistic and biomedical knowledge into gene priors by transformer-based graph representation learning, capturing the intrinsic relationships between different genes' mutations.","BPGT then designs a label decoder that finally performs genetic mutation prediction by two tailored modules: (a) A modality fusion module that firstly fuses the gene priors with critical regions in WSIs and obtains gene-wise mutation logits.","(b) A comparative multi-label loss that emphasizes the inherent comparisons among mutation status to enhance the discrimination capabilities.","Sufficient experiments on The Cancer Genome Atlas benchmark demonstrate that BPGT outperforms the state-of-the-art."],"url":"http://arxiv.org/abs/2406.02990v1","category":"cs.CV"}
{"created":"2024-06-05 06:15:48","title":"Readability-guided Idiom-aware Sentence Simplification (RISS) for Chinese","abstract":"Chinese sentence simplification faces challenges due to the lack of large-scale labeled parallel corpora and the prevalence of idioms. To address these challenges, we propose Readability-guided Idiom-aware Sentence Simplification (RISS), a novel framework that combines data augmentation techniques with lexcial simplification. RISS introduces two key components: (1) Readability-guided Paraphrase Selection (RPS), a method for mining high-quality sentence pairs, and (2) Idiom-aware Simplification (IAS), a model that enhances the comprehension and simplification of idiomatic expressions. By integrating RPS and IAS using multi-stage and multi-task learning strategies, RISS outperforms previous state-of-the-art methods on two Chinese sentence simplification datasets. Furthermore, RISS achieves additional improvements when fine-tuned on a small labeled dataset. Our approach demonstrates the potential for more effective and accessible Chinese text simplification.","sentences":["Chinese sentence simplification faces challenges due to the lack of large-scale labeled parallel corpora and the prevalence of idioms.","To address these challenges, we propose Readability-guided Idiom-aware Sentence Simplification (RISS), a novel framework that combines data augmentation techniques with lexcial simplification.","RISS introduces two key components: (1) Readability-guided Paraphrase Selection (RPS), a method for mining high-quality sentence pairs, and (2) Idiom-aware Simplification (IAS), a model that enhances the comprehension and simplification of idiomatic expressions.","By integrating RPS and IAS using multi-stage and multi-task learning strategies, RISS outperforms previous state-of-the-art methods on two Chinese sentence simplification datasets.","Furthermore, RISS achieves additional improvements when fine-tuned on a small labeled dataset.","Our approach demonstrates the potential for more effective and accessible Chinese text simplification."],"url":"http://arxiv.org/abs/2406.02974v1","category":"cs.CL"}
{"created":"2024-06-05 05:54:56","title":"Which exceptional low-dimensional projections of a Gaussian point cloud can be found in polynomial time?","abstract":"Given $d$-dimensional standard Gaussian vectors $\\boldsymbol{x}_1,\\dots, \\boldsymbol{x}_n$, we consider the set of all empirical distributions of its $m$-dimensional projections, for $m$ a fixed constant. Diaconis and Freedman (1984) proved that, if $n/d\\to \\infty$, all such distributions converge to the standard Gaussian distribution. In contrast, we study the proportional asymptotics, whereby $n,d\\to \\infty$ with $n/d\\to \\alpha \\in (0, \\infty)$. In this case, the projection of the data points along a typical random subspace is again Gaussian, but the set $\\mathscr{F}_{m,\\alpha}$ of all probability distributions that are asymptotically feasible as $m$-dimensional projections contains non-Gaussian distributions corresponding to exceptional subspaces.   Non-rigorous methods from statistical physics yield an indirect characterization of $\\mathscr{F}_{m,\\alpha}$ in terms of a generalized Parisi formula. Motivated by the goal of putting this formula on a rigorous basis, and to understand whether these projections can be found efficiently, we study the subset $\\mathscr{F}^{\\rm alg}_{m,\\alpha}\\subseteq \\mathscr{F}_{m,\\alpha}$ of distributions that can be realized by a class of iterative algorithms. We prove that this set is characterized by a certain stochastic optimal control problem, and obtain a dual characterization of this problem in terms of a variational principle that extends Parisi's formula.   As a byproduct, we obtain computationally achievable values for a class of random optimization problems including `generalized spherical perceptron' models.","sentences":["Given $d$-dimensional standard Gaussian vectors $\\boldsymbol{x}_1,\\dots, \\boldsymbol{x}_n$, we consider the set of all empirical distributions of its $m$-dimensional projections, for $m$ a fixed constant.","Diaconis and Freedman (1984) proved that, if $n/d\\to \\infty$, all such distributions converge to the standard Gaussian distribution.","In contrast, we study the proportional asymptotics, whereby $n,d\\to \\infty$ with $n/d\\to \\alpha \\in (0, \\infty)$.","In this case, the projection of the data points along a typical random subspace is again Gaussian, but the set $\\mathscr{F}_{m,\\alpha}$ of all probability distributions that are asymptotically feasible as $m$-dimensional projections contains non-Gaussian distributions corresponding to exceptional subspaces.   ","Non-rigorous methods from statistical physics yield an indirect characterization of $\\mathscr{F}_{m,\\alpha}$ in terms of a generalized Parisi formula.","Motivated by the goal of putting this formula on a rigorous basis, and to understand whether these projections can be found efficiently, we study the subset $\\mathscr{F}^{\\rm alg}_{m,\\alpha}\\subseteq \\mathscr{F}_{m,\\alpha}$ of distributions that can be realized by a class of iterative algorithms.","We prove that this set is characterized by a certain stochastic optimal control problem, and obtain a dual characterization of this problem in terms of a variational principle that extends Parisi's formula.   ","As a byproduct, we obtain computationally achievable values for a class of random optimization problems including `generalized spherical perceptron' models."],"url":"http://arxiv.org/abs/2406.02970v1","category":"math.PR"}
{"created":"2024-06-05 05:38:46","title":"Dataset-Distillation Generative Model for Speech Emotion Recognition","abstract":"Deep learning models for speech rely on large datasets, presenting computational challenges. Yet, performance hinges on training data size. Dataset Distillation (DD) aims to learn a smaller dataset without much performance degradation when training with it. DD has been investigated in computer vision but not yet in speech. This paper presents the first approach for DD to speech targeting Speech Emotion Recognition on IEMOCAP. We employ Generative Adversarial Networks (GANs) not to mimic real data but to distil key discriminative information of IEMOCAP that is useful for downstream training. The GAN then replaces the original dataset and can sample custom synthetic dataset sizes. It performs comparably when following the original class imbalance but improves performance by 0.3% absolute UAR with balanced classes. It also reduces dataset storage and accelerates downstream training by 95% in both cases and reduces speaker information which could help for a privacy application.","sentences":["Deep learning models for speech rely on large datasets, presenting computational challenges.","Yet, performance hinges on training data size.","Dataset Distillation (DD) aims to learn a smaller dataset without much performance degradation when training with it.","DD has been investigated in computer vision but not yet in speech.","This paper presents the first approach for DD to speech targeting Speech Emotion Recognition on IEMOCAP.","We employ Generative Adversarial Networks (GANs) not to mimic real data but to distil key discriminative information of IEMOCAP that is useful for downstream training.","The GAN then replaces the original dataset and can sample custom synthetic dataset sizes.","It performs comparably when following the original class imbalance but improves performance by 0.3% absolute UAR with balanced classes.","It also reduces dataset storage and accelerates downstream training by 95% in both cases and reduces speaker information which could help for a privacy application."],"url":"http://arxiv.org/abs/2406.02963v1","category":"cs.SD"}
{"created":"2024-06-05 05:27:29","title":"Adversarial Moment-Matching Distillation of Large Language Models","abstract":"Knowledge distillation (KD) has been shown to be highly effective in guiding a student model with a larger teacher model and achieving practical benefits in improving the computational and memory efficiency for large language models (LLMs). State-of-the-art KD methods for LLMs mostly rely on minimizing explicit distribution distance between teacher and student probability predictions. Instead of optimizing these mandatory behaviour cloning objectives, we explore an imitation learning strategy for KD of LLMs. In particular, we minimize the imitation gap by matching the action-value moments of the teacher's behavior from both on- and off-policy perspectives. To achieve this action-value moment-matching goal, we propose an adversarial training algorithm to jointly estimate the moment-matching distance and optimize the student policy to minimize it. Results from both task-agnostic instruction-following experiments and task-specific experiments demonstrate the effectiveness of our method and achieve new state-of-the-art performance.","sentences":["Knowledge distillation (KD) has been shown to be highly effective in guiding a student model with a larger teacher model and achieving practical benefits in improving the computational and memory efficiency for large language models (LLMs).","State-of-the-art KD methods for LLMs mostly rely on minimizing explicit distribution distance between teacher and student probability predictions.","Instead of optimizing these mandatory behaviour cloning objectives, we explore an imitation learning strategy for KD of LLMs.","In particular, we minimize the imitation gap by matching the action-value moments of the teacher's behavior from both on- and off-policy perspectives.","To achieve this action-value moment-matching goal, we propose an adversarial training algorithm to jointly estimate the moment-matching distance and optimize the student policy to minimize it.","Results from both task-agnostic instruction-following experiments and task-specific experiments demonstrate the effectiveness of our method and achieve new state-of-the-art performance."],"url":"http://arxiv.org/abs/2406.02959v1","category":"cs.CL"}
{"created":"2024-06-05 05:20:12","title":"AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection","abstract":"With the rapid growth in deepfake video content, we require improved and generalizable methods to detect them. Most existing detection methods either use uni-modal cues or rely on supervised training to capture the dissonance between the audio and visual modalities. While the former disregards the audio-visual correspondences entirely, the latter predominantly focuses on discerning audio-visual cues within the training corpus, thereby potentially overlooking correspondences that can help detect unseen deepfakes. We present Audio-Visual Feature Fusion (AVFF), a two-stage cross-modal learning method that explicitly captures the correspondence between the audio and visual modalities for improved deepfake detection. The first stage pursues representation learning via self-supervision on real videos to capture the intrinsic audio-visual correspondences. To extract rich cross-modal representations, we use contrastive learning and autoencoding objectives, and introduce a novel audio-visual complementary masking and feature fusion strategy. The learned representations are tuned in the second stage, where deepfake classification is pursued via supervised learning on both real and fake videos. Extensive experiments and analysis suggest that our novel representation learning paradigm is highly discriminative in nature. We report 98.6% accuracy and 99.1% AUC on the FakeAVCeleb dataset, outperforming the current audio-visual state-of-the-art by 14.9% and 9.9%, respectively.","sentences":["With the rapid growth in deepfake video content, we require improved and generalizable methods to detect them.","Most existing detection methods either use uni-modal cues or rely on supervised training to capture the dissonance between the audio and visual modalities.","While the former disregards the audio-visual correspondences entirely, the latter predominantly focuses on discerning audio-visual cues within the training corpus, thereby potentially overlooking correspondences that can help detect unseen deepfakes.","We present Audio-Visual Feature Fusion (AVFF), a two-stage cross-modal learning method that explicitly captures the correspondence between the audio and visual modalities for improved deepfake detection.","The first stage pursues representation learning via self-supervision on real videos to capture the intrinsic audio-visual correspondences.","To extract rich cross-modal representations, we use contrastive learning and autoencoding objectives, and introduce a novel audio-visual complementary masking and feature fusion strategy.","The learned representations are tuned in the second stage, where deepfake classification is pursued via supervised learning on both real and fake videos.","Extensive experiments and analysis suggest that our novel representation learning paradigm is highly discriminative in nature.","We report 98.6% accuracy and 99.1% AUC on the FakeAVCeleb dataset, outperforming the current audio-visual state-of-the-art by 14.9% and 9.9%, respectively."],"url":"http://arxiv.org/abs/2406.02951v1","category":"cs.CV"}
{"created":"2024-06-05 04:49:55","title":"Radiomics-guided Multimodal Self-attention Network for Predicting Pathological Complete Response in Breast MRI","abstract":"Breast cancer is the most prevalent cancer among women and predicting pathologic complete response (pCR) after anti-cancer treatment is crucial for patient prognosis and treatment customization. Deep learning has shown promise in medical imaging diagnosis, particularly when utilizing multiple imaging modalities to enhance accuracy. This study presents a model that predicts pCR in breast cancer patients using dynamic contrast-enhanced (DCE) magnetic resonance imaging (MRI) and apparent diffusion coefficient (ADC) maps. Radiomics features are established hand-crafted features of the tumor region and thus could be useful in medical image analysis. Our approach extracts features from both DCE MRI and ADC using an encoder with a self-attention mechanism, leveraging radiomics to guide feature extraction from tumor-related regions. Our experimental results demonstrate the superior performance of our model in predicting pCR compared to other baseline methods.","sentences":["Breast cancer is the most prevalent cancer among women and predicting pathologic complete response (pCR) after anti-cancer treatment is crucial for patient prognosis and treatment customization.","Deep learning has shown promise in medical imaging diagnosis, particularly when utilizing multiple imaging modalities to enhance accuracy.","This study presents a model that predicts pCR in breast cancer patients using dynamic contrast-enhanced (DCE) magnetic resonance imaging (MRI) and apparent diffusion coefficient (ADC) maps.","Radiomics features are established hand-crafted features of the tumor region and thus could be useful in medical image analysis.","Our approach extracts features from both DCE MRI and ADC using an encoder with a self-attention mechanism, leveraging radiomics to guide feature extraction from tumor-related regions.","Our experimental results demonstrate the superior performance of our model in predicting pCR compared to other baseline methods."],"url":"http://arxiv.org/abs/2406.02936v1","category":"eess.IV"}
{"created":"2024-06-05 04:37:06","title":"Exploring Data Efficiency in Zero-Shot Learning with Diffusion Models","abstract":"Zero-Shot Learning (ZSL) aims to enable classifiers to identify unseen classes by enhancing data efficiency at the class level. This is achieved by generating image features from pre-defined semantics of unseen classes. However, most current approaches heavily depend on the number of samples from seen classes, i.e. they do not consider instance-level effectiveness. In this paper, we demonstrate that limited seen examples generally result in deteriorated performance of generative models. To overcome these challenges, we propose ZeroDiff, a Diffusion-based Generative ZSL model. This unified framework incorporates diffusion models to improve data efficiency at both the class and instance levels. Specifically, for instance-level effectiveness, ZeroDiff utilizes a forward diffusion chain to transform limited data into an expanded set of noised data. For class-level effectiveness, we design a two-branch generation structure that consists of a Diffusion-based Feature Generator (DFG) and a Diffusion-based Representation Generator (DRG). DFG focuses on learning and sampling the distribution of cross-entropy-based features, whilst DRG learns the supervised contrastive-based representation to boost the zero-shot capabilities of DFG. Additionally, we employ three discriminators to evaluate generated features from various aspects and introduce a Wasserstein-distance-based mutual learning loss to transfer knowledge among discriminators, thereby enhancing guidance for generation. Demonstrated through extensive experiments on three popular ZSL benchmarks, our ZeroDiff not only achieves significant improvements over existing ZSL methods but also maintains robust performance even with scarce training data. Code will be released upon acceptance.","sentences":["Zero-Shot Learning (ZSL) aims to enable classifiers to identify unseen classes by enhancing data efficiency at the class level.","This is achieved by generating image features from pre-defined semantics of unseen classes.","However, most current approaches heavily depend on the number of samples from seen classes, i.e. they do not consider instance-level effectiveness.","In this paper, we demonstrate that limited seen examples generally result in deteriorated performance of generative models.","To overcome these challenges, we propose ZeroDiff, a Diffusion-based Generative ZSL model.","This unified framework incorporates diffusion models to improve data efficiency at both the class and instance levels.","Specifically, for instance-level effectiveness, ZeroDiff utilizes a forward diffusion chain to transform limited data into an expanded set of noised data.","For class-level effectiveness, we design a two-branch generation structure that consists of a Diffusion-based Feature Generator (DFG) and a Diffusion-based Representation Generator (DRG).","DFG focuses on learning and sampling the distribution of cross-entropy-based features, whilst DRG learns the supervised contrastive-based representation to boost the zero-shot capabilities of DFG.","Additionally, we employ three discriminators to evaluate generated features from various aspects and introduce a Wasserstein-distance-based mutual learning loss to transfer knowledge among discriminators, thereby enhancing guidance for generation.","Demonstrated through extensive experiments on three popular ZSL benchmarks, our ZeroDiff not only achieves significant improvements over existing ZSL methods but also maintains robust performance even with scarce training data.","Code will be released upon acceptance."],"url":"http://arxiv.org/abs/2406.02929v1","category":"cs.CV"}
{"created":"2024-06-05 04:08:41","title":"Visual-Text Cross Alignment: Refining the Similarity Score in Vision-Language Models","abstract":"It has recently been discovered that using a pre-trained vision-language model (VLM), e.g., CLIP, to align a whole query image with several finer text descriptions generated by a large language model can significantly enhance zero-shot performance. However, in this paper, we empirically find that the finer descriptions tend to align more effectively with local areas of the query image rather than the whole image, and then we theoretically validate this finding. Thus, we present a method called weighted visual-text cross alignment (WCA). This method begins with a localized visual prompting technique, designed to identify local visual areas within the query image. The local visual areas are then cross-aligned with the finer descriptions by creating a similarity matrix using the pre-trained VLM. To determine how well a query image aligns with each category, we develop a score function based on the weighted similarities in this matrix. Extensive experiments demonstrate that our method significantly improves zero-shot performance across various datasets, achieving results that are even comparable to few-shot learning methods.","sentences":["It has recently been discovered that using a pre-trained vision-language model (VLM), e.g., CLIP, to align a whole query image with several finer text descriptions generated by a large language model can significantly enhance zero-shot performance.","However, in this paper, we empirically find that the finer descriptions tend to align more effectively with local areas of the query image rather than the whole image, and then we theoretically validate this finding.","Thus, we present a method called weighted visual-text cross alignment (WCA).","This method begins with a localized visual prompting technique, designed to identify local visual areas within the query image.","The local visual areas are then cross-aligned with the finer descriptions by creating a similarity matrix using the pre-trained VLM.","To determine how well a query image aligns with each category, we develop a score function based on the weighted similarities in this matrix.","Extensive experiments demonstrate that our method significantly improves zero-shot performance across various datasets, achieving results that are even comparable to few-shot learning methods."],"url":"http://arxiv.org/abs/2406.02915v1","category":"cs.CV"}
{"created":"2024-06-05 04:04:08","title":"Improving In-Context Learning with Prediction Feedback for Sentiment Analysis","abstract":"Large language models (LLMs) have achieved promising results in sentiment analysis through the in-context learning (ICL) paradigm. However, their ability to distinguish subtle sentiments still remains a challenge. Inspired by the human ability to adjust understanding via feedback, this paper enhances ICL by incorporating prior predictions and feedback, aiming to rectify sentiment misinterpretation of LLMs. Specifically, the proposed framework consists of three steps: (1) acquiring prior predictions of LLMs, (2) devising predictive feedback based on correctness, and (3) leveraging a feedback-driven prompt to refine sentiment understanding. Experimental results across nine sentiment analysis datasets demonstrate the superiority of our framework over conventional ICL methods, with an average F1 improvement of 5.95%.","sentences":["Large language models (LLMs) have achieved promising results in sentiment analysis through the in-context learning (ICL) paradigm.","However, their ability to distinguish subtle sentiments still remains a challenge.","Inspired by the human ability to adjust understanding via feedback, this paper enhances ICL by incorporating prior predictions and feedback, aiming to rectify sentiment misinterpretation of LLMs.","Specifically, the proposed framework consists of three steps: (1) acquiring prior predictions of LLMs, (2) devising predictive feedback based on correctness, and (3) leveraging a feedback-driven prompt to refine sentiment understanding.","Experimental results across nine sentiment analysis datasets demonstrate the superiority of our framework over conventional ICL methods, with an average F1 improvement of 5.95%."],"url":"http://arxiv.org/abs/2406.02911v1","category":"cs.CL"}
{"created":"2024-06-05 03:26:59","title":"Language Model Can Do Knowledge Tracing: Simple but Effective Method to Integrate Language Model and Knowledge Tracing Task","abstract":"Knowledge Tracing (KT) is a critical task in online learning for modeling student knowledge over time. Despite the success of deep learning-based KT models, which rely on sequences of numbers as data, most existing approaches fail to leverage the rich semantic information in the text of questions and concepts. This paper proposes Language model-based Knowledge Tracing (LKT), a novel framework that integrates pre-trained language models (PLMs) with KT methods. By leveraging the power of language models to capture semantic representations, LKT effectively incorporates textual information and significantly outperforms previous KT models on large benchmark datasets. Moreover, we demonstrate that LKT can effectively address the cold-start problem in KT by leveraging the semantic knowledge captured by PLMs. Interpretability of LKT is enhanced compared to traditional KT models due to its use of text-rich data. We conducted the local interpretable model-agnostic explanation technique and analysis of attention scores to interpret the model performance further. Our work highlights the potential of integrating PLMs with KT and paves the way for future research in KT domain.","sentences":["Knowledge Tracing (KT) is a critical task in online learning for modeling student knowledge over time.","Despite the success of deep learning-based KT models, which rely on sequences of numbers as data, most existing approaches fail to leverage the rich semantic information in the text of questions and concepts.","This paper proposes Language model-based Knowledge Tracing (LKT), a novel framework that integrates pre-trained language models (PLMs) with KT methods.","By leveraging the power of language models to capture semantic representations, LKT effectively incorporates textual information and significantly outperforms previous KT models on large benchmark datasets.","Moreover, we demonstrate that LKT can effectively address the cold-start problem in KT by leveraging the semantic knowledge captured by PLMs.","Interpretability of LKT is enhanced compared to traditional KT models due to its use of text-rich data.","We conducted the local interpretable model-agnostic explanation technique and analysis of attention scores to interpret the model performance further.","Our work highlights the potential of integrating PLMs with KT and paves the way for future research in KT domain."],"url":"http://arxiv.org/abs/2406.02893v1","category":"cs.CL"}
{"created":"2024-06-05 15:18:22","title":"Determination of Optimal Chain Coupling made by Embedding in D-Wave Quantum Annealer","abstract":"The qubits in a D-wave quantum annealer (D-wave QA) are designed on a Pegasus graph that is different from structure of a combinatorial optimization problem. This situation requires embedding with the chains connected by ferromagnetic (FM) coupling $J_c$ between the qubits. Weak and strong $J_c$ values induce chain breaking and enforcement of chain energy, which reduce the accuracy of quantum annealing (QA) measurements, respectively. In addition, we confirmed that even though the D-Wave Ocean package provides a default coupling $J_c^{\\text{default}}$, it is not an optimal coupling $J_c^{\\text{optimal}}$ that maximizes the possible correct rate of QA measurements. In this paper, we present an algorithm how $J_c^{\\text{optimal}}$ with the maximum probability $p$ for observing the possible lowest energy is determined. Finally, we confirm that the extracted $J_c^{\\text{optimal}}$ show much better $p$ than $J_c^{\\text{default}}$ in QA measurements of various parameters of frustrated and fully connected combinatorial optimization problems. The open code is available in \\textit{https://github.com/HunpyoLee/OptimizeChainStrength}.","sentences":["The qubits in a D-wave quantum annealer (D-wave QA) are designed on a Pegasus graph that is different from structure of a combinatorial optimization problem.","This situation requires embedding with the chains connected by ferromagnetic (FM) coupling $J_c$ between the qubits.","Weak and strong $J_c$ values induce chain breaking and enforcement of chain energy, which reduce the accuracy of quantum annealing (QA) measurements, respectively.","In addition, we confirmed that even though the D-Wave Ocean package provides a default coupling $J_c^{\\text{default}}$, it is not an optimal coupling $J_c^{\\text{optimal}}$ that maximizes the possible correct rate of QA measurements.","In this paper, we present an algorithm how $J_c^{\\text{optimal}}$ with the maximum probability $p$ for observing the possible lowest energy is determined.","Finally, we confirm that the extracted $J_c^{\\text{optimal}}$ show much better $p$ than $J_c^{\\text{default}}$ in QA measurements of various parameters of frustrated and fully connected combinatorial optimization problems.","The open code is available in \\textit{https://github.com/HunpyoLee/OptimizeChainStrength}."],"url":"http://arxiv.org/abs/2406.03364v1","category":"quant-ph"}
{"created":"2024-06-05 14:49:14","title":"A Flexible Recursive Network for Video Stereo Matching Based on Residual Estimation","abstract":"Due to the high similarity of disparity between consecutive frames in video sequences, the area where disparity changes is defined as the residual map, which can be calculated. Based on this, we propose RecSM, a network based on residual estimation with a flexible recursive structure for video stereo matching. The RecSM network accelerates stereo matching using a Multi-scale Residual Estimation Module (MREM), which employs the temporal context as a reference and rapidly calculates the disparity for the current frame by computing only the residual values between the current and previous frames. To further reduce the error of estimated disparities, we use the Disparity Optimization Module (DOM) and Temporal Attention Module (TAM) to enforce constraints between each module, and together with MREM, form a flexible Stackable Computation Structure (SCS), which allows for the design of different numbers of SCS based on practical scenarios. Experimental results demonstrate that with a stack count of 3, RecSM achieves a 4x speed improvement compared to ACVNet, running at 0.054 seconds based on one NVIDIA RTX 2080TI GPU, with an accuracy decrease of only 0.7%. Code is available at https://github.com/Y0uchenZ/RecSM.","sentences":["Due to the high similarity of disparity between consecutive frames in video sequences, the area where disparity changes is defined as the residual map, which can be calculated.","Based on this, we propose RecSM, a network based on residual estimation with a flexible recursive structure for video stereo matching.","The RecSM network accelerates stereo matching using a Multi-scale Residual Estimation Module (MREM), which employs the temporal context as a reference and rapidly calculates the disparity for the current frame by computing only the residual values between the current and previous frames.","To further reduce the error of estimated disparities, we use the Disparity Optimization Module (DOM) and Temporal Attention Module (TAM) to enforce constraints between each module, and together with MREM, form a flexible Stackable Computation Structure (SCS), which allows for the design of different numbers of SCS based on practical scenarios.","Experimental results demonstrate that with a stack count of 3, RecSM achieves a 4x speed improvement compared to ACVNet, running at 0.054 seconds based on one NVIDIA RTX 2080TI GPU, with an accuracy decrease of only 0.7%.","Code is available at https://github.com/Y0uchenZ/RecSM."],"url":"http://arxiv.org/abs/2406.03333v1","category":"cs.CV"}
{"created":"2024-06-05 14:29:24","title":"Simultaneous Optimized Orthogonal Matching Pursuit with Application to ECG Compression","abstract":"A greedy pursuit strategy which finds a common basis for approximating a set of similar signals is proposed. The strategy extends the Optimized Orthogonal Matching Pursuit approach to selecting the subspace containing the approximation of all the signals in the set. The method, called Simultaneous Optimized Orthogonal Matching Pursuit, is stepwise optimal in the sense of minimizing at each iteration the mean error norm of the joint approximation. When applied to compression of electrocardiograms, significant gains over other transformation based compression techniques are demonstrated on the MIT-BIH Arrhythmia dataset.","sentences":["A greedy pursuit strategy which finds a common basis for approximating a set of similar signals is proposed.","The strategy extends the Optimized Orthogonal Matching Pursuit approach to selecting the subspace containing the approximation of all the signals in the set.","The method, called Simultaneous Optimized Orthogonal Matching Pursuit, is stepwise optimal in the sense of minimizing at each iteration the mean error norm of the joint approximation.","When applied to compression of electrocardiograms, significant gains over other transformation based compression techniques are demonstrated on the MIT-BIH Arrhythmia dataset."],"url":"http://arxiv.org/abs/2406.03316v1","category":"eess.SP"}
{"created":"2024-06-05 13:55:56","title":"Globally and Locally Optimized Pannini Projection for High FoV Rendering of 360-degree Images","abstract":"To render a spherical (360 degree or omnidirectional) image on planar displays, a 2D image -- called as viewport -- must be obtained by projecting a sphere region on a plane, according to the users viewing direction and a predefined field of view (FoV). However, any sphere to plan projection introduces geometric distortions, such as object stretching and/or bending of straight lines, which intensity increases with the considered FoV. In this paper, a fully automatic content-aware projection is proposed, aiming to reduce the geometric distortions when high FoVs are used. This new projection is based on the Pannini projection, whose parameters are firstly globally optimized according to the image content, followed by a local conformality improvement of relevant viewport objects. A crowdsourcing subjective test showed that the proposed projection is the most preferred solution among the considered state-of-the-art sphere to plan projections, producing viewports with a more pleasant visual quality.","sentences":["To render a spherical (360 degree or omnidirectional) image on planar displays, a 2D image -- called as viewport -- must be obtained by projecting a sphere region on a plane, according to the users viewing direction and a predefined field of view (FoV).","However, any sphere to plan projection introduces geometric distortions, such as object stretching and/or bending of straight lines, which intensity increases with the considered FoV. In this paper, a fully automatic content-aware projection is proposed, aiming to reduce the geometric distortions when high FoVs are used.","This new projection is based on the Pannini projection, whose parameters are firstly globally optimized according to the image content, followed by a local conformality improvement of relevant viewport objects.","A crowdsourcing subjective test showed that the proposed projection is the most preferred solution among the considered state-of-the-art sphere to plan projections, producing viewports with a more pleasant visual quality."],"url":"http://arxiv.org/abs/2406.03282v1","category":"cs.MM"}
{"created":"2024-06-05 09:27:42","title":"Particle Filter Optimization: A Bayesian Approach for Global Stochastic Optimization","abstract":"This paper introduces a novel global optimization algorithm called Particle Filter Optimization (PFO), designed for a class of stochastic problems. PFO leverages the Bayesian inference framework of Particle Filters (PF) by integrating the optimization problem into the PF estimation process. In this context, the objective function replaces the measurement, and a customized transitional prior is developed to function as state dynamics. This dynamic replaces classic acquisition function and grants the PF a local optimization capability, facilitating its transformation towards global optimization. In PFO, the particles serve as agents in the optimization problem. Given the noisy nature of measured outputs, the Unscented Transform (UT) is utilized to estimate the true mean, thereby reducing the impact of erroneous information on particle transitions and weight updates. The algorithm is designed to minimize the introduction of unnecessary parameters and adheres to theoretically validated PF procedures, resulting in a robust heuristic algorithm supported by rigorous theoretical foundations.","sentences":["This paper introduces a novel global optimization algorithm called Particle Filter Optimization (PFO), designed for a class of stochastic problems.","PFO leverages the Bayesian inference framework of Particle Filters (PF) by integrating the optimization problem into the PF estimation process.","In this context, the objective function replaces the measurement, and a customized transitional prior is developed to function as state dynamics.","This dynamic replaces classic acquisition function and grants the PF a local optimization capability, facilitating its transformation towards global optimization.","In PFO, the particles serve as agents in the optimization problem.","Given the noisy nature of measured outputs, the Unscented Transform (UT) is utilized to estimate the true mean, thereby reducing the impact of erroneous information on particle transitions and weight updates.","The algorithm is designed to minimize the introduction of unnecessary parameters and adheres to theoretically validated PF procedures, resulting in a robust heuristic algorithm supported by rigorous theoretical foundations."],"url":"http://arxiv.org/abs/2406.03089v1","category":"math.OC"}
{"created":"2024-06-05 08:33:11","title":"Higher order approximation of nonlinear SPDEs with additive space-time white noise","abstract":"We consider strong approximations of $1+1$-dimensional stochastic PDEs driven by additive space-time white noise. It has been long proposed (Davie-Gaines '01, Jentzen-Kloeden '08), as well as observed in simulations, that approximation schemes based on samples from the stochastic convolution, rather than from increments of the underlying Wiener processes, should achieve significantly higher convergence rates with respect to the temporal timestep. The present paper proves this. For a large class of nonlinearities, with possibly superlinear growth, a temporal rate of (almost) $1$ is proved, a major improvement on the rate $1/4$ that is known to be optimal for schemes based on Wiener increments. The spatial rate remains (almost) $1/2$ as it is standard in the literature.","sentences":["We consider strong approximations of $1+1$-dimensional stochastic PDEs driven by additive space-time white noise.","It has been long proposed (Davie-Gaines '01, Jentzen-Kloeden '08), as well as observed in simulations, that approximation schemes based on samples from the stochastic convolution, rather than from increments of the underlying Wiener processes, should achieve significantly higher convergence rates with respect to the temporal timestep.","The present paper proves this.","For a large class of nonlinearities, with possibly superlinear growth, a temporal rate of (almost) $1$ is proved, a major improvement on the rate $1/4$ that is known to be optimal for schemes based on Wiener increments.","The spatial rate remains (almost) $1/2$ as it is standard in the literature."],"url":"http://arxiv.org/abs/2406.03058v1","category":"math.PR"}
{"created":"2024-06-05 07:32:29","title":"DifAttack++: Query-Efficient Black-Box Adversarial Attack via Hierarchical Disentangled Feature Space in Cross Domain","abstract":"This work investigates efficient score-based black-box adversarial attacks with a high Attack Success Rate (ASR) and good generalizability. We design a novel attack method based on a \\textit{Hierarchical} \\textbf{Di}sentangled \\textbf{F}eature space and \\textit{cross domain}, called \\textbf{DifAttack++}, which differs significantly from the existing ones operating over the entire feature space. Specifically, DifAttack++ firstly disentangles an image's latent feature into an \\textit{adversarial feature} (AF) and a \\textit{visual feature} (VF) via an autoencoder equipped with our specially designed \\textbf{H}ierarchical \\textbf{D}ecouple-\\textbf{F}usion (HDF) module, where the AF dominates the adversarial capability of an image, while the VF largely determines its visual appearance. We train such autoencoders for the clean and adversarial image domains respectively, meanwhile realizing feature disentanglement, by using pairs of clean images and their Adversarial Examples (AEs) generated from available surrogate models via white-box attack methods. Eventually, in the black-box attack stage, DifAttack++ iteratively optimizes the AF according to the query feedback from the victim model until a successful AE is generated, while keeping the VF unaltered. Extensive experimental results demonstrate that our method achieves superior ASR and query efficiency than SOTA methods, meanwhile exhibiting much better visual quality of AEs. The code is available at https://github.com/csjunjun/DifAttack.git.","sentences":["This work investigates efficient score-based black-box adversarial attacks with a high Attack Success Rate (ASR) and good generalizability.","We design a novel attack method based on a \\textit{Hierarchical} \\textbf{Di}sentangled \\textbf{F}eature space and \\textit{cross domain}, called \\textbf{DifAttack++}, which differs significantly from the existing ones operating over the entire feature space.","Specifically, DifAttack++ firstly disentangles an image's latent feature into an \\textit{adversarial feature} (AF) and a \\textit{visual feature} (VF) via an autoencoder equipped with our specially designed \\textbf{H}ierarchical \\textbf{D}ecouple-\\textbf{F}usion (HDF) module, where the AF dominates the adversarial capability of an image, while the VF largely determines its visual appearance.","We train such autoencoders for the clean and adversarial image domains respectively, meanwhile realizing feature disentanglement, by using pairs of clean images and their Adversarial Examples (AEs) generated from available surrogate models via white-box attack methods.","Eventually, in the black-box attack stage, DifAttack++ iteratively optimizes the AF according to the query feedback from the victim model until a successful AE is generated, while keeping the VF unaltered.","Extensive experimental results demonstrate that our method achieves superior ASR and query efficiency than SOTA methods, meanwhile exhibiting much better visual quality of AEs.","The code is available at https://github.com/csjunjun/DifAttack.git."],"url":"http://arxiv.org/abs/2406.03017v1","category":"cs.CV"}
{"created":"2024-06-05 07:31:05","title":"Balancing Performance and Efficiency in Zero-shot Robotic Navigation","abstract":"We present an optimization study of the Vision-Language Frontier Maps (VLFM) applied to the Object Goal Navigation task in robotics. Our work evaluates the efficiency and performance of various vision-language models, object detectors, segmentation models, and multi-modal comprehension and Visual Question Answering modules. Using the $\\textit{val-mini}$ and $\\textit{val}$ splits of Habitat-Matterport 3D dataset, we conduct experiments on a desktop with limited VRAM. We propose a solution that achieves a higher success rate (+1.55%) improving over the VLFM BLIP-2 baseline without substantial success-weighted path length loss while requiring $\\textbf{2.3 times}$ less video memory. Our findings provide insights into balancing model performance and computational efficiency, suggesting effective deployment strategies for resource-limited environments.","sentences":["We present an optimization study of the Vision-Language Frontier Maps (VLFM) applied to the Object Goal Navigation task in robotics.","Our work evaluates the efficiency and performance of various vision-language models, object detectors, segmentation models, and multi-modal comprehension and Visual Question Answering modules.","Using the $\\textit{val-mini}$ and $\\textit{val}$ splits of Habitat-Matterport 3D dataset, we conduct experiments on a desktop with limited VRAM.","We propose a solution that achieves a higher success rate (+1.55%) improving over the VLFM BLIP-2 baseline without substantial success-weighted path length loss while requiring $\\textbf{2.3 times}$ less video memory.","Our findings provide insights into balancing model performance and computational efficiency, suggesting effective deployment strategies for resource-limited environments."],"url":"http://arxiv.org/abs/2406.03015v1","category":"cs.RO"}
{"created":"2024-06-05 07:03:56","title":"To Sense or Not To Sense: A Delay Perspective (full version)","abstract":"With the ever-growing demand for low-latency services in machine-to-machine (M2M) communications, the delay performance of random access networks has become a primary concern, which critically depends on the sensing capability of nodes. To understand the effect of sensing on the optimal delay performance, the challenge lies in unifying the delay analysis of sensing-free Aloha and sensing-based Carrier Sense Multiple Access (CSMA) with various design features such as backoff and connection-free or connection-based. In this paper, based on a unified analytical framework, the mean queueing delay of data packets with Aloha and CSMA is characterized and optimized, with which the upper-bound of sensing time for CSMA to outperform Aloha in terms of the minimum mean queueing delay is further obtained. The analysis is also applied to the Random Access-Based Small Data Transmission (RA-SDT) schemes in 5G networks to investigate when and how significant their delay performance can be improved by sensing, which sheds important insights into practical access protocol design.","sentences":["With the ever-growing demand for low-latency services in machine-to-machine (M2M) communications, the delay performance of random access networks has become a primary concern, which critically depends on the sensing capability of nodes.","To understand the effect of sensing on the optimal delay performance, the challenge lies in unifying the delay analysis of sensing-free Aloha and sensing-based Carrier Sense Multiple Access (CSMA) with various design features such as backoff and connection-free or connection-based.","In this paper, based on a unified analytical framework, the mean queueing delay of data packets with Aloha and CSMA is characterized and optimized, with which the upper-bound of sensing time for CSMA to outperform Aloha in terms of the minimum mean queueing delay is further obtained.","The analysis is also applied to the Random Access-Based Small Data Transmission (RA-SDT) schemes in 5G networks to investigate when and how significant their delay performance can be improved by sensing, which sheds important insights into practical access protocol design."],"url":"http://arxiv.org/abs/2406.02999v1","category":"cs.NI"}
{"created":"2024-06-05 01:58:31","title":"Merging bound states in the continuum at third-order $\u0393$ point enabled by controlling Fourier harmonic components in lattice parameters","abstract":"Recent studies have demonstrated that ultrahigh-$Q$ resonances, which are robust to fabrication imperfections, can be realized by merging multiple bound states in the continuum (BICs) in momentum space. The merging of multiple BICs holds significant promise for practical applications, providing a robust means to attain ultrahigh-$Q$ resonances that greatly enhance light-matter interactions. In this study, we introduce a novel approach to achieve the merging of BICs at the edges of the fourth stop band, which opens at the third-order $\\Gamma$ point, in one-dimensional leaky-mode photonic lattices. Photonic band gaps and BICs arise from periodic modulations in lattice parameters. However, near the third-order $\\Gamma$ point, out-of-plane radiation arises by the first and second Fourier harmonic components in the lattice parameters. Accidental BICs can emerge at specific $k$ points where an optimal balance exists between these two Fourier harmonic components. We demonstrate that these accidental BICs are topologically stable, and their positions in momentum space can be precisely controlled by adjusting a specific lattice parameter that influences the strength of the first and second Fourier harmonic components. Furthermore, we show that accidental BICs can be merged at the third-order $\\Gamma$ point, with or without a symmetry-protected BIC, by finely adjusting this specific lattice parameter while keeping other parameters constant.","sentences":["Recent studies have demonstrated that ultrahigh-$Q$ resonances, which are robust to fabrication imperfections, can be realized by merging multiple bound states in the continuum (BICs) in momentum space.","The merging of multiple BICs holds significant promise for practical applications, providing a robust means to attain ultrahigh-$Q$ resonances that greatly enhance light-matter interactions.","In this study, we introduce a novel approach to achieve the merging of BICs at the edges of the fourth stop band, which opens at the third-order $\\Gamma$ point, in one-dimensional leaky-mode photonic lattices.","Photonic band gaps and BICs arise from periodic modulations in lattice parameters.","However, near the third-order $\\Gamma$ point, out-of-plane radiation arises by the first and second Fourier harmonic components in the lattice parameters.","Accidental BICs can emerge at specific $k$ points where an optimal balance exists between these two Fourier harmonic components.","We demonstrate that these accidental BICs are topologically stable, and their positions in momentum space can be precisely controlled by adjusting a specific lattice parameter that influences the strength of the first and second Fourier harmonic components.","Furthermore, we show that accidental BICs can be merged at the third-order $\\Gamma$ point, with or without a symmetry-protected BIC, by finely adjusting this specific lattice parameter while keeping other parameters constant."],"url":"http://arxiv.org/abs/2406.02850v1","category":"physics.optics"}
{"created":"2024-06-05 01:30:58","title":"Statistical inference of convex order by Wasserstein projection","abstract":"Ranking distributions according to a stochastic order has wide applications in diverse areas. Although stochastic dominance has received much attention,convex order, particularly in general dimensions, has yet to be investigated from a statistical point of view. This article addresses this gap by introducing a simple statistical test for convex order based on the Wasserstein projection distance. This projection distance not only encodes whether two distributions are indeed in convex order, but also quantifies the deviation from the desired convex order and produces an optimal convex order approximation. Lipschitz stability of the backward and forward Wasserstein projection distance is proved, which leads to elegant consistency results of the estimator we employ as our test statistic. Combining these with state of the art results regarding the convergence rate of empirical distributions, we also derive upper bounds for the $p$-value and type I error our test statistic, as well as upper bounds on the type II error for an appropriate class of strict alternatives. Lastly, we provide an efficient numerical scheme for our test statistic, by way of an entropic Frank-Wolfe algorithm. Some experiments based on synthetic data sets illuminates the success of our approach empirically.","sentences":["Ranking distributions according to a stochastic order has wide applications in diverse areas.","Although stochastic dominance has received much attention,convex order, particularly in general dimensions, has yet to be investigated from a statistical point of view.","This article addresses this gap by introducing a simple statistical test for convex order based on the Wasserstein projection distance.","This projection distance not only encodes whether two distributions are indeed in convex order, but also quantifies the deviation from the desired convex order and produces an optimal convex order approximation.","Lipschitz stability of the backward and forward Wasserstein projection distance is proved, which leads to elegant consistency results of the estimator we employ as our test statistic.","Combining these with state of the art results regarding the convergence rate of empirical distributions, we also derive upper bounds for the $p$-value and type I error our test statistic, as well as upper bounds on the type II error for an appropriate class of strict alternatives.","Lastly, we provide an efficient numerical scheme for our test statistic, by way of an entropic Frank-Wolfe algorithm.","Some experiments based on synthetic data sets illuminates the success of our approach empirically."],"url":"http://arxiv.org/abs/2406.02840v1","category":"stat.ME"}
{"created":"2024-06-05 01:28:53","title":"You Only Accept Samples Once: Fast, Self-Correcting Stochastic Variational Inference","abstract":"We introduce YOASOVI, an algorithm for performing fast, self-correcting stochastic optimization for Variational Inference (VI) on large Bayesian heirarchical models. To accomplish this, we take advantage of available information on the objective function used for stochastic VI at each iteration and replace regular Monte Carlo sampling with acceptance sampling. Rather than spend computational resources drawing and evaluating over a large sample for the gradient, we draw only one sample and accept it with probability proportional to the expected improvement in the objective. The following paper develops two versions of the algorithm: the first one based on a naive intuition, and another building up the algorithm as a Metropolis-type scheme. Empirical results based on simulations and benchmark datasets for multivariate Gaussian mixture models show that YOASOVI consistently converges faster (in clock time) and within better optimal neighborhoods than both regularized Monte Carlo and Quasi-Monte Carlo VI algorithms.","sentences":["We introduce YOASOVI, an algorithm for performing fast, self-correcting stochastic optimization for Variational Inference (VI) on large Bayesian heirarchical models.","To accomplish this, we take advantage of available information on the objective function used for stochastic VI at each iteration and replace regular Monte Carlo sampling with acceptance sampling.","Rather than spend computational resources drawing and evaluating over a large sample for the gradient, we draw only one sample and accept it with probability proportional to the expected improvement in the objective.","The following paper develops two versions of the algorithm: the first one based on a naive intuition, and another building up the algorithm as a Metropolis-type scheme.","Empirical results based on simulations and benchmark datasets for multivariate Gaussian mixture models show that YOASOVI consistently converges faster (in clock time) and within better optimal neighborhoods than both regularized Monte Carlo and Quasi-Monte Carlo VI algorithms."],"url":"http://arxiv.org/abs/2406.02838v1","category":"stat.ML"}
{"created":"2024-06-04 23:04:30","title":"Secrecy Analysis of CSI Ratio-Based Transmitter Selection with Unreliable Backhaul","abstract":"This paper explores the secrecy performance of a multi-transmitter system with unreliable backhaul links. To improve secrecy, we propose a novel transmitter selection (TS) scheme that selects a transmitter with the maximum ratio of the destination channel power gain to the eavesdropping channel power gain. The backhaul reliability factor is incorporated with the distribution of the channel power gain through the utilization of a mixture distribution. We evaluate the non-zero secrecy rate (NZR) and the secrecy outage probability (SOP) as well as their asymptotes in two scenarios of backhaul activity knowledge, where it is available and where it is unavailable. The results illustrate that because of the unreliable backhaul, the proposed destination-to-eavesdropper channel power gain ratio-based TS scheme is constrained in terms of secrecy performance. However, performance enhancements are observed when the backhaul knowledge activity is utilized. Furthermore, the proposed scheme outperforms all the sub-optimal TS schemes and achieves nearly optimal performance without requiring noise power or the evaluation of the exact secrecy rate measurement.","sentences":["This paper explores the secrecy performance of a multi-transmitter system with unreliable backhaul links.","To improve secrecy, we propose a novel transmitter selection (TS) scheme that selects a transmitter with the maximum ratio of the destination channel power gain to the eavesdropping channel power gain.","The backhaul reliability factor is incorporated with the distribution of the channel power gain through the utilization of a mixture distribution.","We evaluate the non-zero secrecy rate (NZR) and the secrecy outage probability (SOP) as well as their asymptotes in two scenarios of backhaul activity knowledge, where it is available and where it is unavailable.","The results illustrate that because of the unreliable backhaul, the proposed destination-to-eavesdropper channel power gain ratio-based TS scheme is constrained in terms of secrecy performance.","However, performance enhancements are observed when the backhaul knowledge activity is utilized.","Furthermore, the proposed scheme outperforms all the sub-optimal TS schemes and achieves nearly optimal performance without requiring noise power or the evaluation of the exact secrecy rate measurement."],"url":"http://arxiv.org/abs/2406.02812v1","category":"cs.IT"}
{"created":"2024-06-04 22:54:25","title":"Changes in boiling controlled by molar concentration-dependent diffusion of surfactants","abstract":"Boiling is a prevalent phase-change process that plays a vital role in facilitating efficient heat transfer from a heating surface. While this heat transfer mechanism is generally effective, a rapid increase in surface temperature can lead to hydrodynamic instabilities, resulting in a boiling crisis. Previous studies have shown that surfactants often improve boiling performance and change the boiling crisis behavior. Conventional wisdom in this field attributes that these changes in boiling behavior are tied to the critical micelle concentration (CMC) of the particular surfactant. However, our work reveals that these changes in boiling behavior are independent of the CMC for three nonionic surfactants across a wide range of molar concentrations. In addition, visual snapshots of the bubbling behavior indicate changes in bubble formation, such as bubble size and nucleation site density, influenced by the molar concentration-dependent diffusion timescale of surfactants. Hence, these findings offer compelling evidence that boiling behavior, encompassing both boiling performance and boiling crisis, is governed by the dynamic adsorption of surfactants rather than dictated by the CMC. This becomes evident when quantifying the heat transfer coefficient (HTC) and critical heat flux (CHF) using the logarithm of molar concentration, as predicted by theory. Building upon these findings, we propose insights for controlling when CHF modification occurs in specific scenarios involving any surfactants. These insights hold significant potential for optimizing heat transfer processes and leveraging surfactants in energy-related applications to maximize boiling efficiency.","sentences":["Boiling is a prevalent phase-change process that plays a vital role in facilitating efficient heat transfer from a heating surface.","While this heat transfer mechanism is generally effective, a rapid increase in surface temperature can lead to hydrodynamic instabilities, resulting in a boiling crisis.","Previous studies have shown that surfactants often improve boiling performance and change the boiling crisis behavior.","Conventional wisdom in this field attributes that these changes in boiling behavior are tied to the critical micelle concentration (CMC) of the particular surfactant.","However, our work reveals that these changes in boiling behavior are independent of the CMC for three nonionic surfactants across a wide range of molar concentrations.","In addition, visual snapshots of the bubbling behavior indicate changes in bubble formation, such as bubble size and nucleation site density, influenced by the molar concentration-dependent diffusion timescale of surfactants.","Hence, these findings offer compelling evidence that boiling behavior, encompassing both boiling performance and boiling crisis, is governed by the dynamic adsorption of surfactants rather than dictated by the CMC.","This becomes evident when quantifying the heat transfer coefficient (HTC) and critical heat flux (CHF) using the logarithm of molar concentration, as predicted by theory.","Building upon these findings, we propose insights for controlling when CHF modification occurs in specific scenarios involving any surfactants.","These insights hold significant potential for optimizing heat transfer processes and leveraging surfactants in energy-related applications to maximize boiling efficiency."],"url":"http://arxiv.org/abs/2406.02811v1","category":"physics.flu-dyn"}
{"created":"2024-06-04 21:54:59","title":"Immersive Robot Programming Interface for Human-Guided Automation and Randomized Path Planning","abstract":"Researchers are exploring Augmented Reality (AR) interfaces for online robot programming to streamline automation and user interaction in variable manufacturing environments. This study introduces an AR interface for online programming and data visualization that integrates the human in the randomized robot path planning, reducing the inherent randomness of the methods with human intervention. The interface uses holographic items which correspond to physical elements to interact with a redundant manipulator. Utilizing Rapidly Random Tree Star (RRT*) and Spherical Linear Interpolation (SLERP) algorithms, the interface achieves end-effector s progression through collision-free path with smooth rotation. Next, Sequential Quadratic Programming (SQP) achieve robot s configurations for this progression. The platform executes the RRT* algorithm in a loop, with each iteration independently exploring the shortest path through random sampling, leading to variations in the optimized paths produced. These paths are then demonstrated to AR users, who select the most appropriate path based on the environmental context and their intuition. The accuracy and effectiveness of the interface are validated through its implementation and testing with a seven Degree-OF-Freedom (DOF) manipulator, indicating its potential to advance current practices in robot programming. The validation of this paper include two implementations demonstrating the value of human-in-the-loop and context awareness in robotics.","sentences":["Researchers are exploring Augmented Reality (AR) interfaces for online robot programming to streamline automation and user interaction in variable manufacturing environments.","This study introduces an AR interface for online programming and data visualization that integrates the human in the randomized robot path planning, reducing the inherent randomness of the methods with human intervention.","The interface uses holographic items which correspond to physical elements to interact with a redundant manipulator.","Utilizing Rapidly Random Tree Star (RRT*) and Spherical Linear Interpolation (SLERP) algorithms, the interface achieves end-effector s progression through collision-free path with smooth rotation.","Next, Sequential Quadratic Programming (SQP) achieve robot s configurations for this progression.","The platform executes the RRT* algorithm in a loop, with each iteration independently exploring the shortest path through random sampling, leading to variations in the optimized paths produced.","These paths are then demonstrated to AR users, who select the most appropriate path based on the environmental context and their intuition.","The accuracy and effectiveness of the interface are validated through its implementation and testing with a seven Degree-OF-Freedom (DOF) manipulator, indicating its potential to advance current practices in robot programming.","The validation of this paper include two implementations demonstrating the value of human-in-the-loop and context awareness in robotics."],"url":"http://arxiv.org/abs/2406.02799v1","category":"cs.RO"}
{"created":"2024-06-04 20:40:27","title":"Cyclic Sparse Training: Is it Enough?","abstract":"The success of iterative pruning methods in achieving state-of-the-art sparse networks has largely been attributed to improved mask identification and an implicit regularization induced by pruning. We challenge this hypothesis and instead posit that their repeated cyclic training schedules enable improved optimization. To verify this, we show that pruning at initialization is significantly boosted by repeated cyclic training, even outperforming standard iterative pruning methods. The dominant mechanism how this is achieved, as we conjecture, can be attributed to a better exploration of the loss landscape leading to a lower training loss. However, at high sparsity, repeated cyclic training alone is not enough for competitive performance. A strong coupling between learnt parameter initialization and mask seems to be required. Standard methods obtain this coupling via expensive pruning-training iterations, starting from a dense network. To achieve this with sparse training instead, we propose SCULPT-ing, i.e., repeated cyclic training of any sparse mask followed by a single pruning step to couple the parameters and the mask, which is able to match the performance of state-of-the-art iterative pruning methods in the high sparsity regime at reduced computational cost.","sentences":["The success of iterative pruning methods in achieving state-of-the-art sparse networks has largely been attributed to improved mask identification and an implicit regularization induced by pruning.","We challenge this hypothesis and instead posit that their repeated cyclic training schedules enable improved optimization.","To verify this, we show that pruning at initialization is significantly boosted by repeated cyclic training, even outperforming standard iterative pruning methods.","The dominant mechanism how this is achieved, as we conjecture, can be attributed to a better exploration of the loss landscape leading to a lower training loss.","However, at high sparsity, repeated cyclic training alone is not enough for competitive performance.","A strong coupling between learnt parameter initialization and mask seems to be required.","Standard methods obtain this coupling via expensive pruning-training iterations, starting from a dense network.","To achieve this with sparse training instead, we propose SCULPT-ing, i.e., repeated cyclic training of any sparse mask followed by a single pruning step to couple the parameters and the mask, which is able to match the performance of state-of-the-art iterative pruning methods in the high sparsity regime at reduced computational cost."],"url":"http://arxiv.org/abs/2406.02773v1","category":"cs.LG"}
{"created":"2024-06-04 20:36:16","title":"Spatial and social situation-aware transformer-based trajectory prediction of autonomous systems","abstract":"Autonomous transportation systems such as road vehicles or vessels require the consideration of the static and dynamic environment to dislocate without collision. Anticipating the behavior of an agent in a given situation is required to adequately react to it in time. Developing deep learning-based models has become the dominant approach to motion prediction recently. The social environment is often considered through a CNN-LSTM-based sub-module processing a $\\textit{social tensor}$ that includes information of the past trajectory of surrounding agents. For the proposed transformer-based trajectory prediction model, an alternative, computationally more efficient social tensor definition and processing is suggested. It considers the interdependencies between target and surrounding agents at each time step directly instead of relying on information of last hidden LSTM states of individually processed agents. A transformer-based sub-module, the Social Tensor Transformer, is integrated into the overall prediction model. It is responsible for enriching the target agent's dislocation features with social interaction information obtained from the social tensor. For the awareness of spatial limitations, dislocation features are defined in relation to the navigable area. This replaces additional, computationally expensive map processing sub-modules. An ablation study shows, that for longer prediction horizons, the deviation of the predicted trajectory from the ground truth is lower compared to a spatially and socially agnostic model. Even if the performance gain from a spatial-only to a spatial and social context-sensitive model is small in terms of common error measures, by visualizing the results it can be shown that the proposed model in fact is able to predict reactions to surrounding agents and explicitely allows an interpretable behavior.","sentences":["Autonomous transportation systems such as road vehicles or vessels require the consideration of the static and dynamic environment to dislocate without collision.","Anticipating the behavior of an agent in a given situation is required to adequately react to it in time.","Developing deep learning-based models has become the dominant approach to motion prediction recently.","The social environment is often considered through a CNN-LSTM-based sub-module processing a $\\textit{social tensor}$ that includes information of the past trajectory of surrounding agents.","For the proposed transformer-based trajectory prediction model, an alternative, computationally more efficient social tensor definition and processing is suggested.","It considers the interdependencies between target and surrounding agents at each time step directly instead of relying on information of last hidden LSTM states of individually processed agents.","A transformer-based sub-module, the Social Tensor Transformer, is integrated into the overall prediction model.","It is responsible for enriching the target agent's dislocation features with social interaction information obtained from the social tensor.","For the awareness of spatial limitations, dislocation features are defined in relation to the navigable area.","This replaces additional, computationally expensive map processing sub-modules.","An ablation study shows, that for longer prediction horizons, the deviation of the predicted trajectory from the ground truth is lower compared to a spatially and socially agnostic model.","Even if the performance gain from a spatial-only to a spatial and social context-sensitive model is small in terms of common error measures, by visualizing the results it can be shown that the proposed model in fact is able to predict reactions to surrounding agents and explicitely allows an interpretable behavior."],"url":"http://arxiv.org/abs/2406.02767v1","category":"cs.LG"}
{"created":"2024-06-04 20:26:51","title":"Stable MPC with maximal terminal sets and quadratic terminal costs","abstract":"This paper develops a technique for computing a quadratic terminal cost for linear model predictive controllers that is valid for all states in the maximal control invariant set. This maximizes the set of recursively feasible states for the controller, ensures asymptotic stability using standard proofs, and allows for easy tuning of the controller in linear operation.","sentences":["This paper develops a technique for computing a quadratic terminal cost for linear model predictive controllers that is valid for all states in the maximal control invariant set.","This maximizes the set of recursively feasible states for the controller, ensures asymptotic stability using standard proofs, and allows for easy tuning of the controller in linear operation."],"url":"http://arxiv.org/abs/2406.02760v1","category":"math.OC"}
{"created":"2024-06-04 19:37:41","title":"CAMP: Compiler and Allocator-based Heap Memory Protection","abstract":"The heap is a critical and widely used component of many applications. Due to its dynamic nature, combined with the complexity of heap management algorithms, it is also a frequent target for security exploits. To enhance the heap's security, various heap protection techniques have been introduced, but they either introduce significant runtime overhead or have limited protection.   We present CAMP, a new sanitizer for detecting and capturing heap memory corruption. CAMP leverages a compiler and a customized memory allocator. The compiler adds boundary-checking and escape-tracking instructions to the target program, while the memory allocator tracks memory ranges, coordinates with the instrumentation, and neutralizes dangling pointers. With the novel error detection scheme, CAMP enables various compiler optimization strategies and thus eliminates redundant and unnecessary check instrumentation. This design minimizes runtime overhead without sacrificing security guarantees. Our evaluation and comparison of CAMP with existing tools, using both real-world applications and SPEC CPU benchmarks, show that it provides even better heap corruption detection capability with lower runtime overhead.","sentences":["The heap is a critical and widely used component of many applications.","Due to its dynamic nature, combined with the complexity of heap management algorithms, it is also a frequent target for security exploits.","To enhance the heap's security, various heap protection techniques have been introduced, but they either introduce significant runtime overhead or have limited protection.   ","We present CAMP, a new sanitizer for detecting and capturing heap memory corruption.","CAMP leverages a compiler and a customized memory allocator.","The compiler adds boundary-checking and escape-tracking instructions to the target program, while the memory allocator tracks memory ranges, coordinates with the instrumentation, and neutralizes dangling pointers.","With the novel error detection scheme, CAMP enables various compiler optimization strategies and thus eliminates redundant and unnecessary check instrumentation.","This design minimizes runtime overhead without sacrificing security guarantees.","Our evaluation and comparison of CAMP with existing tools, using both real-world applications and SPEC CPU benchmarks, show that it provides even better heap corruption detection capability with lower runtime overhead."],"url":"http://arxiv.org/abs/2406.02737v1","category":"cs.CR"}
{"created":"2024-06-04 19:06:31","title":"Control of Microrobots Using Model Predictive Control and Gaussian Processes for Disturbance Estimation","abstract":"This paper presents a control framework for magnetically actuated micron-scale robots ($\\mu$bots) designed to mitigate disturbances and improve trajectory tracking. To address the challenges posed by unmodeled dynamics and environmental variability, we combine data-driven modeling with model-based control to accurately track desired trajectories using a relatively small amount of data. The system is represented with a simple linear model, and Gaussian Processes (GP) are employed to capture and estimate disturbances. This disturbance-enhanced model is then integrated into a Model Predictive Controller (MPC). Our approach demonstrates promising performance in both simulation and experimental setups, showcasing its potential for precise and reliable microrobot control in complex environments.","sentences":["This paper presents a control framework for magnetically actuated micron-scale robots ($\\mu$bots) designed to mitigate disturbances and improve trajectory tracking.","To address the challenges posed by unmodeled dynamics and environmental variability, we combine data-driven modeling with model-based control to accurately track desired trajectories using a relatively small amount of data.","The system is represented with a simple linear model, and Gaussian Processes (GP) are employed to capture and estimate disturbances.","This disturbance-enhanced model is then integrated into a Model Predictive Controller (MPC).","Our approach demonstrates promising performance in both simulation and experimental setups, showcasing its potential for precise and reliable microrobot control in complex environments."],"url":"http://arxiv.org/abs/2406.02722v1","category":"cs.RO"}
{"created":"2024-06-04 19:02:05","title":"Using Modularized Pin Ridge Filter in Proton FLASH Planning for Liver Stereotactic Ablative Body Radiotherapy","abstract":"We previously developed a FLASH planning framework for streamlined pin-ridge-filter (pin-RF) design, demonstrating its feasibility for single-energy proton FLASH planning. In this study, we refined the pin-RF design for easy assembly using reusable modules, focusing on its application in liver SABR. This framework generates an intermediate IMPT plan and translates it into step widths and thicknesses of pin-RFs for a single-energy FLASH plan. Parameters like energy spacing, monitor unit limit, and spot quantity were adjusted during IMPT planning, resulting in pin-RFs assembled using predefined modules with widths from 1 to 6 mm, each with a WET of 5 mm. This approach was validated on three liver SABR cases. FLASH doses, quantified using the FLASH effectiveness model at 1 to 5 Gy thresholds, were compared to conventional IMPT (IMPT-CONV) doses to assess clinical benefits. The highest demand for 6 mm width modules, moderate for 2-4 mm, and minimal for 1- and 5-mm modules were shown across all cases. At lower dose thresholds, the two-beam case showed significant dose reductions (>23%), while the other two three-beam cases showed moderate reductions (up to 14.7%), indicating the need for higher fractional beam doses for an enhanced FLASH effect. Positive clinical benefits were seen only in the two-beam case at the 5 Gy threshold. At the 1 Gy threshold, the FLASH plan of the two-beam case outperformed its IMPT-CONV plan, reducing dose indicators by up to 28.3%. However, the three-beam cases showed negative clinical benefits at the 1 Gy threshold, with some dose indicators increasing by up to 16% due to lower fractional beam doses and closer beam arrangements. This study evaluated the feasibility of modularizing streamlined pin-RFs in single-energy proton FLASH planning for liver SABR, offering guidance on optimal module composition and strategies to enhance FLASH planning.","sentences":["We previously developed a FLASH planning framework for streamlined pin-ridge-filter (pin-RF) design, demonstrating its feasibility for single-energy proton FLASH planning.","In this study, we refined the pin-RF design for easy assembly using reusable modules, focusing on its application in liver SABR.","This framework generates an intermediate IMPT plan and translates it into step widths and thicknesses of pin-RFs for a single-energy FLASH plan.","Parameters like energy spacing, monitor unit limit, and spot quantity were adjusted during IMPT planning, resulting in pin-RFs assembled using predefined modules with widths from 1 to 6 mm, each with a WET of 5 mm.","This approach was validated on three liver SABR cases.","FLASH doses, quantified using the FLASH effectiveness model at 1 to 5 Gy thresholds, were compared to conventional IMPT (IMPT-CONV) doses to assess clinical benefits.","The highest demand for 6 mm width modules, moderate for 2-4 mm, and minimal for 1- and 5-mm modules were shown across all cases.","At lower dose thresholds, the two-beam case showed significant dose reductions (>23%), while the other two three-beam cases showed moderate reductions (up to 14.7%), indicating the need for higher fractional beam doses for an enhanced FLASH effect.","Positive clinical benefits were seen only in the two-beam case at the 5 Gy threshold.","At the 1 Gy threshold, the FLASH plan of the two-beam case outperformed its IMPT-CONV plan, reducing dose indicators by up to 28.3%.","However, the three-beam cases showed negative clinical benefits at the 1 Gy threshold, with some dose indicators increasing by up to 16% due to lower fractional beam doses and closer beam arrangements.","This study evaluated the feasibility of modularizing streamlined pin-RFs in single-energy proton FLASH planning for liver SABR, offering guidance on optimal module composition and strategies to enhance FLASH planning."],"url":"http://arxiv.org/abs/2406.02718v1","category":"physics.med-ph"}
{"created":"2024-06-04 18:54:49","title":"Efficiency in Pure-Exchange Economies with Risk-Averse Monetary Utilities","abstract":"We study Pareto efficiency in a pure-exchange economy where agents' preferences are represented by risk-averse monetary utilities. These coincide with law-invariant monetary utilities, and they can be shown to correspond to the class of monotone, (quasi-)concave, Schur concave, and translation-invariant utility functionals. This covers a large class of utility functionals, including a variety of law-invariant robust utilities. We show that Pareto optima exist and are comonotone, and we provide a crisp characterization thereof in the case of law-invariant positively homogeneous monetary utilities. This characterization provides an easily implementable algorithm that fully determines the shape of Pareto-optimal allocations. In the special case of law-invariant comonotone-additive monetary utility functionals (concave Yaari-Dual utilities), we provide a closed-form characterization of Pareto optima. As an application, we examine risk-sharing markets where all agents evaluate risk through law-invariant coherent risk measures, a widely popular class of risk measures. In a numerical illustration, we characterize Pareto-optimal risk-sharing for some special types of coherent risk measures.","sentences":["We study Pareto efficiency in a pure-exchange economy where agents' preferences are represented by risk-averse monetary utilities.","These coincide with law-invariant monetary utilities, and they can be shown to correspond to the class of monotone, (quasi-)concave, Schur concave, and translation-invariant utility functionals.","This covers a large class of utility functionals, including a variety of law-invariant robust utilities.","We show that Pareto optima exist and are comonotone, and we provide a crisp characterization thereof in the case of law-invariant positively homogeneous monetary utilities.","This characterization provides an easily implementable algorithm that fully determines the shape of Pareto-optimal allocations.","In the special case of law-invariant comonotone-additive monetary utility functionals (concave Yaari-Dual utilities), we provide a closed-form characterization of Pareto optima.","As an application, we examine risk-sharing markets where all agents evaluate risk through law-invariant coherent risk measures, a widely popular class of risk measures.","In a numerical illustration, we characterize Pareto-optimal risk-sharing for some special types of coherent risk measures."],"url":"http://arxiv.org/abs/2406.02712v1","category":"q-fin.MF"}
{"created":"2024-06-04 18:28:11","title":"MPCR: Multi- and Mixed-Precision Computations Package in R","abstract":"Computational statistics has traditionally utilized double-precision (64-bit) data structures and full-precision operations, resulting in higher-than-necessary accuracy for certain applications. Recently, there has been a growing interest in exploring low-precision options that could reduce computational complexity while still achieving the required level of accuracy. This trend has been amplified by new hardware such as NVIDIA's Tensor Cores in their V100, A100, and H100 GPUs, which are optimized for mixed-precision computations, Intel CPUs with Deep Learning (DL) boost, Google Tensor Processing Units (TPUs), Field Programmable Gate Arrays (FPGAs), ARM CPUs, and others. However, using lower precision may introduce numerical instabilities and accuracy issues. Nevertheless, some applications have shown robustness to low-precision computations, leading to new multi- and mixed-precision algorithms that balance accuracy and computational cost. To address this need, we introduce MPCR, a novel R package that supports three different precision types (16-, 32-, and 64-bit) and their combinations, along with its usage in commonly-used Frequentist/Bayesian statistical examples. The MPCR package is written in C++ and integrated into R through the \\pkg{Rcpp} package, enabling highly optimized operations in various precisions.","sentences":["Computational statistics has traditionally utilized double-precision (64-bit) data structures and full-precision operations, resulting in higher-than-necessary accuracy for certain applications.","Recently, there has been a growing interest in exploring low-precision options that could reduce computational complexity while still achieving the required level of accuracy.","This trend has been amplified by new hardware such as NVIDIA's Tensor Cores in their V100, A100, and H100 GPUs, which are optimized for mixed-precision computations, Intel CPUs with Deep Learning (DL) boost, Google Tensor Processing Units (TPUs), Field Programmable Gate Arrays (FPGAs), ARM CPUs, and others.","However, using lower precision may introduce numerical instabilities and accuracy issues.","Nevertheless, some applications have shown robustness to low-precision computations, leading to new multi- and mixed-precision algorithms that balance accuracy and computational cost.","To address this need, we introduce MPCR, a novel R package that supports three different precision types (16-, 32-, and 64-bit) and their combinations, along with its usage in commonly-used Frequentist/Bayesian statistical examples.","The MPCR package is written in C++ and integrated into R through the \\pkg{Rcpp} package, enabling highly optimized operations in various precisions."],"url":"http://arxiv.org/abs/2406.02701v1","category":"stat.CO"}
{"created":"2024-06-04 18:26:09","title":"Optimization of decoder priors for accurate quantum error correction","abstract":"Accurate decoding of quantum error-correcting codes is a crucial ingredient in protecting quantum information from decoherence. It requires characterizing the error channels corrupting the logical quantum state and providing this information as a prior to the decoder. We introduce a reinforcement learning inspired method for calibrating these priors that aims to minimize the logical error rate. Our method significantly improves the decoding accuracy in repetition and surface code memory experiments executed on Google's Sycamore processor, outperforming the leading decoder-agnostic method by 16% and 3.3% respectively. This calibration approach will serve as an important tool for maximizing the performance of both near-term and future error-corrected quantum devices.","sentences":["Accurate decoding of quantum error-correcting codes is a crucial ingredient in protecting quantum information from decoherence.","It requires characterizing the error channels corrupting the logical quantum state and providing this information as a prior to the decoder.","We introduce a reinforcement learning inspired method for calibrating these priors that aims to minimize the logical error rate.","Our method significantly improves the decoding accuracy in repetition and surface code memory experiments executed on Google's Sycamore processor, outperforming the leading decoder-agnostic method by 16% and 3.3% respectively.","This calibration approach will serve as an important tool for maximizing the performance of both near-term and future error-corrected quantum devices."],"url":"http://arxiv.org/abs/2406.02700v1","category":"quant-ph"}
{"created":"2024-06-04 18:23:54","title":"New Approach to Strongly Coupled N = 4 SYM via Integrability","abstract":"Finding a systematic expansion of the spectrum of free superstrings on AdS5xS5, or equivalently strongly coupled N = 4 SYM in the planar limit, remains an outstanding challenge. No first principle string theory methods are readily available, instead the sole tool at our disposal is the integrability-based Quantum Spectral Curve (QSC). For example, through the QSC the first five orders in the strong coupling expansion of the conformal dimension of an infinite family of short operators have been obtained. However, when using the QSC at strong coupling one must often rely on numerics, and the existing methods for solving the QSC rapidly lose precision as we approach the strong coupling regime.   In this paper, we introduce a new framework that utilises a novel set of QSC variables with a regular strong coupling expansion. We demonstrate how to use this approach to construct a new numerical algorithm that remains stable even at a 't Hooft coupling as large as 106 (or g ~ 100). Employing this approach, we derive new analytic results for some states in the sl(2) sector and beyond. We present a new analytic prediction for a coefficient in the strong coupling expansion of the conformal dimension for the lowest trajectory at a given twist L. For non-lowest trajectories, we uncover a novel feature of mixing with operators outside the sl(2) sector, which manifests as a new type of analytic dependence on the twist.","sentences":["Finding a systematic expansion of the spectrum of free superstrings on AdS5xS5, or equivalently strongly coupled N = 4 SYM in the planar limit, remains an outstanding challenge.","No first principle string theory methods are readily available, instead the sole tool at our disposal is the integrability-based Quantum Spectral Curve (QSC).","For example, through the QSC the first five orders in the strong coupling expansion of the conformal dimension of an infinite family of short operators have been obtained.","However, when using the QSC at strong coupling one must often rely on numerics, and the existing methods for solving the QSC rapidly lose precision as we approach the strong coupling regime.   ","In this paper, we introduce a new framework that utilises a novel set of QSC variables with a regular strong coupling expansion.","We demonstrate how to use this approach to construct a new numerical algorithm that remains stable even at a 't Hooft coupling as large as 106 (or g ~ 100).","Employing this approach, we derive new analytic results for some states in the sl(2) sector and beyond.","We present a new analytic prediction for a coefficient in the strong coupling expansion of the conformal dimension for the lowest trajectory at a given twist L.","For non-lowest trajectories, we uncover a novel feature of mixing with operators outside the sl(2) sector, which manifests as a new type of analytic dependence on the twist."],"url":"http://arxiv.org/abs/2406.02698v1","category":"hep-th"}
{"created":"2024-06-04 14:16:52","title":"Exploring Effects of Hyperdimensional Vectors for Tsetlin Machines","abstract":"Tsetlin machines (TMs) have been successful in several application domains, operating with high efficiency on Boolean representations of the input data. However, Booleanizing complex data structures such as sequences, graphs, images, signal spectra, chemical compounds, and natural language is not trivial. In this paper, we propose a hypervector (HV) based method for expressing arbitrarily large sets of concepts associated with any input data. Using a hyperdimensional space to build vectors drastically expands the capacity and flexibility of the TM. We demonstrate how images, chemical compounds, and natural language text are encoded according to the proposed method, and how the resulting HV-powered TM can achieve significantly higher accuracy and faster learning on well-known benchmarks. Our results open up a new research direction for TMs, namely how to expand and exploit the benefits of operating in hyperspace, including new booleanization strategies, optimization of TM inference and learning, as well as new TM applications.","sentences":["Tsetlin machines (TMs) have been successful in several application domains, operating with high efficiency on Boolean representations of the input data.","However, Booleanizing complex data structures such as sequences, graphs, images, signal spectra, chemical compounds, and natural language is not trivial.","In this paper, we propose a hypervector (HV) based method for expressing arbitrarily large sets of concepts associated with any input data.","Using a hyperdimensional space to build vectors drastically expands the capacity and flexibility of the TM.","We demonstrate how images, chemical compounds, and natural language text are encoded according to the proposed method, and how the resulting HV-powered TM can achieve significantly higher accuracy and faster learning on well-known benchmarks.","Our results open up a new research direction for TMs, namely how to expand and exploit the benefits of operating in hyperspace, including new booleanization strategies, optimization of TM inference and learning, as well as new TM applications."],"url":"http://arxiv.org/abs/2406.02648v1","category":"cs.LG"}
{"created":"2024-06-04 13:11:49","title":"Astral: training physics-informed neural networks with error majorants","abstract":"The primal approach to physics-informed learning is a residual minimization. We argue that residual is, at best, an indirect measure of the error of approximate solution and propose to train with error majorant instead. Since error majorant provides a direct upper bound on error, one can reliably estimate how close PiNN is to the exact solution and stop the optimization process when the desired accuracy is reached. We call loss function associated with error majorant $\\textbf{Astral}$: neur$\\textbf{A}$l a po$\\textbf{ST}$erio$\\textbf{RI}$ function$\\textbf{A}$l Loss. To compare Astral and residual loss functions, we illustrate how error majorants can be derived for various PDEs and conduct experiments with diffusion equations (including anisotropic and in the L-shaped domain), convection-diffusion equation, temporal discretization of Maxwell's equation, and magnetostatics problem. The results indicate that Astral loss is competitive to the residual loss, typically leading to faster convergence and lower error (e.g., for Maxwell's equations, we observe an order of magnitude better relative error and training time). We also report that the error estimate obtained with Astral loss is usually tight enough to be informative, e.g., for a highly anisotropic equation, on average, Astral overestimates error by a factor of $1.5$, and for convection-diffusion by a factor of $1.7$.","sentences":["The primal approach to physics-informed learning is a residual minimization.","We argue that residual is, at best, an indirect measure of the error of approximate solution and propose to train with error majorant instead.","Since error majorant provides a direct upper bound on error, one can reliably estimate how close PiNN is to the exact solution and stop the optimization process when the desired accuracy is reached.","We call loss function associated with error majorant $\\textbf{Astral}$: neur$\\textbf{A}$l a po$\\textbf{ST}$erio$\\textbf{RI}$ function$\\textbf{A}$l Loss.","To compare Astral and residual loss functions, we illustrate how error majorants can be derived for various PDEs and conduct experiments with diffusion equations (including anisotropic and in the L-shaped domain), convection-diffusion equation, temporal discretization of Maxwell's equation, and magnetostatics problem.","The results indicate that Astral loss is competitive to the residual loss, typically leading to faster convergence and lower error (e.g., for Maxwell's equations, we observe an order of magnitude better relative error and training time).","We also report that the error estimate obtained with Astral loss is usually tight enough to be informative, e.g., for a highly anisotropic equation, on average, Astral overestimates error by a factor of $1.5$, and for convection-diffusion by a factor of $1.7$."],"url":"http://arxiv.org/abs/2406.02645v1","category":"physics.comp-ph"}
{"created":"2024-06-05 16:36:57","title":"Investigating the Relationship Between User Specialization and Toxicity on Reddit: A Sentiment Analysis Approach","abstract":"Online platforms host a diverse user base, which can be broadly categorized into \"specialist users\" with focused interests and \"generalist users\" who engage in a wide range of topics. This study explores the behavioral differences between these two user types on the popular platform Reddit, focusing on the level of toxicity in their posts and the associated sentiment scores across 24 emotional categories and a neutral state. By employing community embeddings to represent users in a high-dimensional space, we measure activity diversity using the GS score. We analyze a dataset of 16,291,992 posts from 4,926,237 users spanning the period from 2019 to 2021, assessing the degree of toxicity and sentiment scores for each post. Our findings indicate that specialist users exhibit higher levels of toxic behavior compared to generalist users. Furthermore, specialist users demonstrate elevated scores for annoyance, sadness, and fear, while generalist users show higher scores for curiosity, admiration, and love. These insights contribute to a better understanding of user behavior on online platforms and can inform strategies for fostering healthier online communities.","sentences":["Online platforms host a diverse user base, which can be broadly categorized into \"specialist users\" with focused interests and \"generalist users\" who engage in a wide range of topics.","This study explores the behavioral differences between these two user types on the popular platform Reddit, focusing on the level of toxicity in their posts and the associated sentiment scores across 24 emotional categories and a neutral state.","By employing community embeddings to represent users in a high-dimensional space, we measure activity diversity using the GS score.","We analyze a dataset of 16,291,992 posts from 4,926,237 users spanning the period from 2019 to 2021, assessing the degree of toxicity and sentiment scores for each post.","Our findings indicate that specialist users exhibit higher levels of toxic behavior compared to generalist users.","Furthermore, specialist users demonstrate elevated scores for annoyance, sadness, and fear, while generalist users show higher scores for curiosity, admiration, and love.","These insights contribute to a better understanding of user behavior on online platforms and can inform strategies for fostering healthier online communities."],"url":"http://arxiv.org/abs/2406.03443v1","category":"cs.SI"}
{"created":"2024-06-05 16:34:29","title":"Analysis of experimental data on neutron decay for the possibility of the existence of a right vector boson $W_R$","abstract":"Due to the assumption that sterile neutrinos are right-handed neutrinos, an analysis of the modern experimental situation in neutron decay for right-handed currents was carried out. As a result of the analysis, it was found that there are indications of the existence of a right-handed vector boson $W_R$ with a mass $M_{W_R}\\approx 870_{-140}^{+260} \\text{GeV}$ and a mixing angle with $W_L$: $\\zeta=0.061_{-0.024}^{+0.017}$. This circumstance is the basis for discussing the possibility of expanding the Standard Model with an additional gauge vector boson $W_R$ and right-handed neutrinos.","sentences":["Due to the assumption that sterile neutrinos are right-handed neutrinos, an analysis of the modern experimental situation in neutron decay for right-handed currents was carried out.","As a result of the analysis, it was found that there are indications of the existence of a right-handed vector boson $W_R$ with a mass $M_{W_R}\\approx 870_{-140}^{+260} \\text{GeV}$ and a mixing angle with $W_L$: $\\zeta=0.061_{-0.024}^{+0.017}$. This circumstance is the basis for discussing the possibility of expanding the Standard Model with an additional gauge vector boson $W_R$ and right-handed neutrinos."],"url":"http://arxiv.org/abs/2406.03440v1","category":"hep-ph"}
{"created":"2024-06-05 15:34:21","title":"Controlled fabrication of freestanding monolayer SiC by electron irradiation","abstract":"The design and preparation of novel quantum materials with atomic precision are crucial for exploring new physics and for device applications. Electron irradiation has demonstrated as an effective method for preparing novel quantum materials and quantum structures that could be challenging to obtain otherwise. It features the advantages of precise control over the patterning of such new materials and their integration with other materials with different functionalities. Here, we present a new strategy for fabricating freestanding monolayer SiC within nanopores of a graphene membrane. By regulating the energy of the incident electron beam and the in-situ heating temperature in a scanning transmission electron microscope (STEM), we can effectively control the patterning of nanopores and subsequent growth of monolayer SiC within the graphene lattice. The resultant SiC monolayers seamlessly connect with the graphene lattice, forming a planar structure distinct by a wide direct bandgap. Our in-situ STEM observations further uncover that the growth of monolayer SiC within the graphene nanopore is driven by a combination of bond rotation and atom extrusion, providing new insights into the atom-by-atom self-assembly of freestanding two-dimensional (2D) monolayers.","sentences":["The design and preparation of novel quantum materials with atomic precision are crucial for exploring new physics and for device applications.","Electron irradiation has demonstrated as an effective method for preparing novel quantum materials and quantum structures that could be challenging to obtain otherwise.","It features the advantages of precise control over the patterning of such new materials and their integration with other materials with different functionalities.","Here, we present a new strategy for fabricating freestanding monolayer SiC within nanopores of a graphene membrane.","By regulating the energy of the incident electron beam and the in-situ heating temperature in a scanning transmission electron microscope (STEM), we can effectively control the patterning of nanopores and subsequent growth of monolayer SiC within the graphene lattice.","The resultant SiC monolayers seamlessly connect with the graphene lattice, forming a planar structure distinct by a wide direct bandgap.","Our in-situ STEM observations further uncover that the growth of monolayer SiC within the graphene nanopore is driven by a combination of bond rotation and atom extrusion, providing new insights into the atom-by-atom self-assembly of freestanding two-dimensional (2D) monolayers."],"url":"http://arxiv.org/abs/2406.03383v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-05 15:30:56","title":"A discrete dislocation analysis of size-dependent plasticity in torsion","abstract":"A method for solving three dimensional discrete dislocation plasticity boundary-value problems using a monopole representation of the dislocations is presented. At each time step, the displacement, strain and stress fields in a finite body are obtained by superposition of infinite body dislocation fields and an image field that enforces the boundary conditions. The three dimensional infinite body fields are obtained by representing dislocations as being comprised of points, termed monopoles, that carry dislocation line and Burgers vector information. The image fields are obtained from a three dimensional linear elastic finite element calculation. The implementation of the coupling of the monopole representation with the finite element method, including the interaction of curved dislocations with free surfaces, is presented in some detail because it differs significantly from an implementation with a line based dislocation representation. Numerical convergence and the modeling of dislocation loop nucleation for large scale computations are investigated. The monopole discrete dislocation plasticity framework is used to investigate the effect of size and initial dislocation density on the torsion of wires with diameters varying over three orders of magnitude. Depending on the initial dislocation source density and the wire diameter, three regimes of torsion-twist response are obtained: (i) for wires with a sufficiently small diameter, plastic deformation is nucleation controlled and is strongly size dependent; (ii) for wires with larger diameters dislocation plasticity is dislocation interaction controlled, with the emergence of geometrically necessary dislocations and dislocation pile-ups playing a key role, and is strongly size dependent; and (iii) for wires with sufficiently large diameters plastic deformation becomes less heterogeneous and the dependence on size is greatly diminished.","sentences":["A method for solving three dimensional discrete dislocation plasticity boundary-value problems using a monopole representation of the dislocations is presented.","At each time step, the displacement, strain and stress fields in a finite body are obtained by superposition of infinite body dislocation fields and an image field that enforces the boundary conditions.","The three dimensional infinite body fields are obtained by representing dislocations as being comprised of points, termed monopoles, that carry dislocation line and Burgers vector information.","The image fields are obtained from a three dimensional linear elastic finite element calculation.","The implementation of the coupling of the monopole representation with the finite element method, including the interaction of curved dislocations with free surfaces, is presented in some detail because it differs significantly from an implementation with a line based dislocation representation.","Numerical convergence and the modeling of dislocation loop nucleation for large scale computations are investigated.","The monopole discrete dislocation plasticity framework is used to investigate the effect of size and initial dislocation density on the torsion of wires with diameters varying over three orders of magnitude.","Depending on the initial dislocation source density and the wire diameter, three regimes of torsion-twist response are obtained: (i) for wires with a sufficiently small diameter, plastic deformation is nucleation controlled and is strongly size dependent; (ii) for wires with larger diameters dislocation plasticity is dislocation interaction controlled, with the emergence of geometrically necessary dislocations and dislocation pile-ups playing a key role, and is strongly size dependent; and (iii) for wires with sufficiently large diameters plastic deformation becomes less heterogeneous and the dependence on size is greatly diminished."],"url":"http://arxiv.org/abs/2406.03375v1","category":"cs.CE"}
{"created":"2024-06-05 15:29:11","title":"Aperiodic fragments in periodic solids: Eliminating the need for supercells and background charges in electronic structure calculations of defects","abstract":"To date, computational methods for modeling defects (vacancies, adsorbates, etc.) rely on periodic supercells in which the defect is far enough from its repeated image such that they can be assumed non-interacting. Defects in real solids, however, can be spaced microns or more apart, whereas affordable supercells for density functional theory calculations are no wider than a few nanometers. The relative proximity and periodic repetition of the defect's images may lead to unphysical artifacts, especially if the defect is charged and/or open-shell. Furthermore, to avoid divergence of the periodic electrostatics, a compensating background charge must be introduced if the defect is charged. Even if post-hoc corrections are used, this is a source of unquantifiable error and, in some cases, renders energies useless. In this communication, we introduce an embedding formalism such that a pristine, primitive unit cell may be used for the periodic mean field, after which atoms may be moved or charged within an embedded fragment. This fragment can then be treated with a post-Hartree Fock method to capture important electron correlations pertaining to the defect. By eliminating the need for compensating background charges and periodicity of the defect, we circumvent all associated unphysicalities and numerical issues. Furthermore, the primitive cell calculations drastically reduce computational expense compared to supercell approaches. This method is size-intensive with respect to energy differences and can be routinely applied even to multireference defects, localized excited states, etc. using a variety of fragment solvers. In examining with this approach bond-breaking in a fluorine-substituted graphane monolayer, a difficult testing ground for condensed-phase electronic structure methods, we observe key aspects of the dissociation pathway, specifically a covalent-to-ionic avoided crossing.","sentences":["To date, computational methods for modeling defects (vacancies, adsorbates, etc.) rely on periodic supercells in which the defect is far enough from its repeated image such that they can be assumed non-interacting.","Defects in real solids, however, can be spaced microns or more apart, whereas affordable supercells for density functional theory calculations are no wider than a few nanometers.","The relative proximity and periodic repetition of the defect's images may lead to unphysical artifacts, especially if the defect is charged and/or open-shell.","Furthermore, to avoid divergence of the periodic electrostatics, a compensating background charge must be introduced if the defect is charged.","Even if post-hoc corrections are used, this is a source of unquantifiable error and, in some cases, renders energies useless.","In this communication, we introduce an embedding formalism such that a pristine, primitive unit cell may be used for the periodic mean field, after which atoms may be moved or charged within an embedded fragment.","This fragment can then be treated with a post-Hartree Fock method to capture important electron correlations pertaining to the defect.","By eliminating the need for compensating background charges and periodicity of the defect, we circumvent all associated unphysicalities and numerical issues.","Furthermore, the primitive cell calculations drastically reduce computational expense compared to supercell approaches.","This method is size-intensive with respect to energy differences and can be routinely applied even to multireference defects, localized excited states, etc. using a variety of fragment solvers.","In examining with this approach bond-breaking in a fluorine-substituted graphane monolayer, a difficult testing ground for condensed-phase electronic structure methods, we observe key aspects of the dissociation pathway, specifically a covalent-to-ionic avoided crossing."],"url":"http://arxiv.org/abs/2406.03373v1","category":"physics.chem-ph"}
{"created":"2024-06-05 14:45:00","title":"Minimal U(1) two-Higgs-doublet models for quark and lepton flavour","abstract":"In the context of the 2HDM, and assuming that neutrinos acquire masses via the Weinberg operator, we perform a systematic analysis to determine the minimal quark and lepton flavour patterns, compatible with masses, mixing and CP violation data, realisable by Abelian symmetries. We determine four minimal models for quarks, where the number of independent parameters matches the number of observables. For the lepton sector, three minimal predictive models are identified. Namely, we find scenarios with a preference for the upper/lower octant of the $\\theta_{23}$ atmospheric mixing angle, that exhibit lower bounds on the lightest neutrino masses currently probed by cosmology and testable at future neutrinoless double beta decay experiments, even for a normally-ordered neutrino masses. We investigate the phenomenology of each model taking into account all relevant theoretical, electroweak precision observables, scalar sector constraints, as well as stringent quark flavour processes such as $\\overline{B} \\rightarrow X_s \\gamma$, $B_s \\rightarrow \\mu^- \\mu^+$ and meson oscillations, and the charged lepton flavour-violating decays $e_\\alpha^{-} \\rightarrow e_\\beta^{-} e_\\gamma^{+} e_\\delta^{-}$ and $e_\\alpha \\rightarrow e_\\beta \\gamma$. We show that, in some cases, Abelian flavour symmetries provide a natural framework to suppress flavour-changing neutral couplings and lead to scenarios featuring heavy neutral/charged scalar masses below the TeV scale within the reach of current experiments.","sentences":["In the context of the 2HDM, and assuming that neutrinos acquire masses via the Weinberg operator, we perform a systematic analysis to determine the minimal quark and lepton flavour patterns, compatible with masses, mixing and CP violation data, realisable by Abelian symmetries.","We determine four minimal models for quarks, where the number of independent parameters matches the number of observables.","For the lepton sector, three minimal predictive models are identified.","Namely, we find scenarios with a preference for the upper/lower octant of the $\\theta_{23}$ atmospheric mixing angle, that exhibit lower bounds on the lightest neutrino masses currently probed by cosmology and testable at future neutrinoless double beta decay experiments, even for a normally-ordered neutrino masses.","We investigate the phenomenology of each model taking into account all relevant theoretical, electroweak precision observables, scalar sector constraints, as well as stringent quark flavour processes such as $\\overline{B} \\rightarrow X_s \\gamma$, $B_s \\rightarrow \\mu^- \\mu^+$ and meson oscillations, and the charged lepton flavour-violating decays $e_\\alpha^{-} \\rightarrow e_\\beta^{-} e_\\gamma^{+} e_\\delta^{-}$ and $e_\\alpha \\rightarrow e_\\beta \\gamma$.","We show that, in some cases, Abelian flavour symmetries provide a natural framework to suppress flavour-changing neutral couplings and lead to scenarios featuring heavy neutral/charged scalar masses below the TeV scale within the reach of current experiments."],"url":"http://arxiv.org/abs/2406.03331v1","category":"hep-ph"}
{"created":"2024-06-05 14:15:31","title":"Variational readout through quantum teleportation","abstract":"Sensitivity of gravitational-wave detectors (GWDs) is constrained at low frequencies by quantum radiation-pressure noise, a manifestation of the measurement's back action. One strategy to mitigate this back action involves employing variational readout, which entails cross-correlating the measurement shot noise with radiation-pressure noise. Prior research has demonstrated that variational readout in GWDs can be accomplished through the use of a filter cavity. However, current gravitational-wave detectors necessitate filter cavity lengths on the order of $\\sim 100$ meters, with future detectors anticipated to reach lengths of a few kilometers. This paper introduces a novel approach to variational readout utilizing principle of quantum teleportation, which eliminates the need for a filter cavity. *This document will not be published any journals, since it turned out to be not very useful for the real detectors. The main reason is that one does not need the EPR entanglement to obtain the same result, which does not even shows true variational readout. In other words, both schemes do not exceed the EPR squeezing. Nevertheless this document will be uploaded to stimulate discussions how to realize \"true\" back action evasion, such as variational readout or speed meter, via quantum entanglement.","sentences":["Sensitivity of gravitational-wave detectors (GWDs) is constrained at low frequencies by quantum radiation-pressure noise, a manifestation of the measurement's back action.","One strategy to mitigate this back action involves employing variational readout, which entails cross-correlating the measurement shot noise with radiation-pressure noise.","Prior research has demonstrated that variational readout in GWDs can be accomplished through the use of a filter cavity.","However, current gravitational-wave detectors necessitate filter cavity lengths on the order of $\\sim 100$ meters, with future detectors anticipated to reach lengths of a few kilometers.","This paper introduces a novel approach to variational readout utilizing principle of quantum teleportation, which eliminates the need for a filter cavity.","*This document will not be published any journals, since it turned out to be not very useful for the real detectors.","The main reason is that one does not need the EPR entanglement to obtain the same result, which does not even shows true variational readout.","In other words, both schemes do not exceed the EPR squeezing.","Nevertheless this document will be uploaded to stimulate discussions how to realize \"true\" back action evasion, such as variational readout or speed meter, via quantum entanglement."],"url":"http://arxiv.org/abs/2406.03304v1","category":"quant-ph"}
{"created":"2024-06-05 14:00:11","title":"Sparse Sets in Triangle-free Graphs","abstract":"A set of vertices is $k$-sparse if it induces a graph with a maximum degree of at most $k$. In this missive, we consider the order of the largest $k$-sparse set in a triangle-free graph of fixed order. We show, for example, that every triangle-free graph of order 11 contains a 1-sparse 5-set; every triangle-free graph of order 13 contains a 2-sparse 7-set; and every triangle-free graph of order 8 contains a 3-sparse 6-set. Further, these are all best possible.   For fixed $k$, we consider the growth rate of the largest $k$-sparse set of a triangle-free graph of order $n$. Also, we consider Ramsey numbers of the following type. Given $i$, what is the smallest $n$ having the property that all triangle-free graphs of order $n$ contain a 4-cycle or a $k$-sparse set of order $i$. We use both direct proof techniques and an efficient graph enumeration algorithm to obtain several values for defective Ramsey numbers and a parameter related to largest sparse sets in triangle-free graphs, along with their extremal graphs.","sentences":["A set of vertices is $k$-sparse if it induces a graph with a maximum degree of at most $k$. In this missive, we consider the order of the largest $k$-sparse set in a triangle-free graph of fixed order.","We show, for example, that every triangle-free graph of order 11 contains a 1-sparse 5-set; every triangle-free graph of order 13 contains a 2-sparse 7-set; and every triangle-free graph of order 8 contains a 3-sparse 6-set.","Further, these are all best possible.   ","For fixed $k$, we consider the growth rate of the largest $k$-sparse set of a triangle-free graph of order $n$. Also, we consider Ramsey numbers of the following type.","Given $i$, what is the smallest $n$ having the property that all triangle-free graphs of order $n$ contain a 4-cycle or a $k$-sparse set of order $i$. We use both direct proof techniques and an efficient graph enumeration algorithm to obtain several values for defective Ramsey numbers and a parameter related to largest sparse sets in triangle-free graphs, along with their extremal graphs."],"url":"http://arxiv.org/abs/2406.03290v1","category":"math.CO"}
{"created":"2024-06-05 12:59:18","title":"Holographic drag force with translational symmetry breaking","abstract":"In order to investigate how the drag force is affected by translational symmetry breaking (TSB), we utilize a holographic model in which the background metric remains translational symmetric while a graviton mass or other fields in the theory break this symmetry. We calculate analytically the drag force, considering an asymptotic AdS$_5$ in which parameter $\\alpha$ arises from TSB. This parameter can be intuitively understood as a measure of TSB strength and we anticipate that non-zero values of it will affect the drag force. In this asymptotic AdS$_5$ background, we will demonstrate that a decrease in $\\alpha$ results in a reduction of the drag force. Moreover, we study the diffusion constant, which falls with increasing $\\alpha$. It will eventually be shown that at lower values of $\\alpha$ or $\\mu$ (chemical potential), the transverse diffusion coefficient is larger than the longitudinal one, and the speed of the heavy quark has minimal impact on the ratio.","sentences":["In order to investigate how the drag force is affected by translational symmetry breaking (TSB), we utilize a holographic model in which the background metric remains translational symmetric while a graviton mass or other fields in the theory break this symmetry.","We calculate analytically the drag force, considering an asymptotic AdS$_5$ in which parameter $\\alpha$ arises from TSB.","This parameter can be intuitively understood as a measure of TSB strength and we anticipate that non-zero values of it will affect the drag force.","In this asymptotic AdS$_5$ background, we will demonstrate that a decrease in $\\alpha$ results in a reduction of the drag force.","Moreover, we study the diffusion constant, which falls with increasing $\\alpha$. It will eventually be shown that at lower values of $\\alpha$ or $\\mu$ (chemical potential), the transverse diffusion coefficient is larger than the longitudinal one, and the speed of the heavy quark has minimal impact on the ratio."],"url":"http://arxiv.org/abs/2406.03220v1","category":"hep-th"}
{"created":"2024-06-05 12:58:18","title":"A comprehensive analysis toward the Fermi-LAT source 4FGL J1846.9-0227 -- Jets of a proto-planetary nebula producing gamma-rays?","abstract":"Most of the gamma-ray sources in the Fermi-LAT 14-year Source Catalog are associated with pulsars and blazars. However, unveiling the nature of the still unassociated gamma-ray sources is important for the understanding of high energy emission mechanisms in astrophysical objects. This work presents a comprehensive study toward the region covered by the Fermi source 4FGL J1846.9$-$0227 previously suggested to be a blazar or a massive protostar. Using multiwavelength observations, we analyzed several astrophysical objects in the region as possible counterparts of the Fermi-LAT source. Having discarded most of them after a comprehensive analysis, we suggest that the most likely candidate to be such a counterpart is IRAS 18443$-$0231. We discovered that this source, previously cataloged as a planetary nebula candidate, actually is a proto-planetary nebula. The radio continuum image at 3 GHz associated with such a nebula allowed us to identify a jet-like structure. Additionally, we identified an associated red-shifted CO molecular outflow and a dense molecular clump in which the source is embedded. We obtained a radio spectral index of $-0.47 \\pm 0.08$ for the source, indicating syncrothron emission due to accelerated particles. Thus, we suggest that processes such as proton-proton collisions and relativistic Bremsstrahlung are likely to occur. IRAS 18443$-$0231, lying almost at the center of the Fermi confidence ellipse and related to the hard X-ray source 4XMM J184700.4$-$022752, would be the first association between a proto-planeatry nebula and gamma emission.","sentences":["Most of the gamma-ray sources in the Fermi-LAT 14-year Source Catalog are associated with pulsars and blazars.","However, unveiling the nature of the still unassociated gamma-ray sources is important for the understanding of high energy emission mechanisms in astrophysical objects.","This work presents a comprehensive study toward the region covered by the Fermi source 4FGL J1846.9$-$0227 previously suggested to be a blazar or a massive protostar.","Using multiwavelength observations, we analyzed several astrophysical objects in the region as possible counterparts of the Fermi-LAT source.","Having discarded most of them after a comprehensive analysis, we suggest that the most likely candidate to be such a counterpart is IRAS 18443$-$0231.","We discovered that this source, previously cataloged as a planetary nebula candidate, actually is a proto-planetary nebula.","The radio continuum image at 3 GHz associated with such a nebula allowed us to identify a jet-like structure.","Additionally, we identified an associated red-shifted CO molecular outflow and a dense molecular clump in which the source is embedded.","We obtained a radio spectral index of $-0.47 \\pm 0.08$ for the source, indicating syncrothron emission due to accelerated particles.","Thus, we suggest that processes such as proton-proton collisions and relativistic Bremsstrahlung are likely to occur.","IRAS 18443$-$0231, lying almost at the center of the Fermi confidence ellipse and related to the hard X-ray source 4XMM J184700.4$-$022752, would be the first association between a proto-planeatry nebula and gamma emission."],"url":"http://arxiv.org/abs/2406.03219v1","category":"astro-ph.HE"}
{"created":"2024-06-05 12:06:06","title":"The Amaterasu particle: constraining the superheavy dark matter origin of UHECRs","abstract":"Amaterasu, the second most energetic ($244$ EeV) cosmic ray particle has been recently detected by the Telescope Array (TA) surface detector. The origin of the TA Amaterasu event is puzzling, as its arrival direction points back to a void in the local Universe, lacking conventional astrophysical ultra-high-energy (UHE) cosmic ray (CR) sources. Hence, we explore the possibility if this TA Amaterasu event could have originated from the decay of superheavy dark matter (SHDM) in the Milky Way. Such an origin also opens up multi-messenger detection channels in both UHE gamma-rays and UHE neutrinos. In this present work, using the TA Amaterasu event and the multi-messenger limits/sensitivities from various UHE telescopes, we place stringent constraints on the lifetime and mass of the SHDM. We find that the non-detection of the corresponding gamma-rays at the Pierre Auger Observatory (PAO) and the TA is in severe tension with the SHDM parameter space required to explain the TA Amaterasu event. Additionally, we extend the multi-messenger analysis to the future UHE gamma-ray and UHE neutrino telescopes such as PAO upgrade, GRAND 200k and IceCube-Gen2. We find that the bounds from the future neutrino telescopes will be able to compete with the present UHECR bounds. However, compared to the existing UHE gamma-ray bounds, the future PAO upgrade and the GRAND 200k gamma-ray detectors will improve the bounds on SHDM lifetime by at least one order of magnitude.","sentences":["Amaterasu, the second most energetic ($244$ EeV) cosmic ray particle has been recently detected by the Telescope Array (TA) surface detector.","The origin of the TA Amaterasu event is puzzling, as its arrival direction points back to a void in the local Universe, lacking conventional astrophysical ultra-high-energy (UHE) cosmic ray (CR) sources.","Hence, we explore the possibility if this TA Amaterasu event could have originated from the decay of superheavy dark matter (SHDM) in the Milky Way.","Such an origin also opens up multi-messenger detection channels in both UHE gamma-rays and UHE neutrinos.","In this present work, using the TA Amaterasu event and the multi-messenger limits/sensitivities from various UHE telescopes, we place stringent constraints on the lifetime and mass of the SHDM.","We find that the non-detection of the corresponding gamma-rays at the Pierre Auger Observatory (PAO) and the TA is in severe tension with the SHDM parameter space required to explain the TA Amaterasu event.","Additionally, we extend the multi-messenger analysis to the future UHE gamma-ray and UHE neutrino telescopes such as PAO upgrade, GRAND 200k and IceCube-Gen2.","We find that the bounds from the future neutrino telescopes will be able to compete with the present UHECR bounds.","However, compared to the existing UHE gamma-ray bounds, the future PAO upgrade and the GRAND 200k gamma-ray detectors will improve the bounds on SHDM lifetime by at least one order of magnitude."],"url":"http://arxiv.org/abs/2406.03174v1","category":"hep-ph"}
{"created":"2024-06-05 11:38:15","title":"Cold molecular ions via autoionization below the dissociation limit","abstract":"Several diatomic transition metal oxides, rare-earth metal oxides and fluorides have the unusual property that their bond dissociation energy is larger than their ionization energy. In these molecules, bound levels above the ionization energy can be populated via strong, resonant transitions from the ground state. The only relevant decay channel of these levels is autoionization; predissociation is energetically not possible and radiative decay is many orders of magnitude slower. Starting from translationally cold neutral molecules, translationally cold molecular ions can thus be produced with very high efficiency. By populating bound levels just above the ionization energy, internally cold molecular ions, exclusively occupying the lowest rotational level, are produced. This is experimentally shown here for the dysprosium monoxide molecule, DyO, for which the lowest bond dissociation energy is determined to be 0.0831(6) eV above the ionization energy.","sentences":["Several diatomic transition metal oxides, rare-earth metal oxides and fluorides have the unusual property that their bond dissociation energy is larger than their ionization energy.","In these molecules, bound levels above the ionization energy can be populated via strong, resonant transitions from the ground state.","The only relevant decay channel of these levels is autoionization; predissociation is energetically not possible and radiative decay is many orders of magnitude slower.","Starting from translationally cold neutral molecules, translationally cold molecular ions can thus be produced with very high efficiency.","By populating bound levels just above the ionization energy, internally cold molecular ions, exclusively occupying the lowest rotational level, are produced.","This is experimentally shown here for the dysprosium monoxide molecule, DyO, for which the lowest bond dissociation energy is determined to be 0.0831(6) eV above the ionization energy."],"url":"http://arxiv.org/abs/2406.03160v1","category":"physics.atm-clus"}
{"created":"2024-06-05 11:34:26","title":"Observation of new charmonium(-like) states in $B^+ \\to D^{*\\pm} D^{\\mp} K^+$ decays","abstract":"A study of resonant structures in $B^{+}\\rightarrow{D^{\\ast+}D^{-}K^{+}}$ and $B^{+}\\rightarrow{D^{\\ast-}D^{+}K^{+}}$ decays is performed, using proton-proton collision data at centre-of-mass energies of $\\sqrt{s}=7, 8$, and $13$ TeV recorded by the LHCb experiment, corresponding to an integrated luminosity of 9 fb$^{-1}$. A simultaneous amplitude fit is performed to the two channels with contributions from resonances decaying to $D^{\\ast-}D^{+}$ and $D^{\\ast+}D^{-}$ states linked by $C$ parity. This procedure allows the $C$-parities of resonances in the $D^{\\ast\\pm}D^{\\mp}$ mass spectra to be determined. Four charmonium(-like) states are observed decaying into $D^{\\ast\\pm}D^{\\mp}$: $\\eta_c(3945)$, $h_c(4000)$, $\\chi_{c1}(4010)$ and $h_c(4300)$, with quantum numbers $J^{PC}$ equal to $0^{-+}$, $1^{+-}$, $1^{++}$ and $1^{+-}$, respectively. At least three of these states have not been observed previously. In addition, the existence of the $T_{\\bar{c}\\bar{s}0}^{*}(2870)^{0}$ and $T_{\\bar{c}\\bar{s}1}^{*}(2900)^{0}$ resonances in the $D^-K^+$ mass spectrum, already observed in the $B^+ \\to D^+ D^- K^+$ decay, is confirmed in a different production channel.","sentences":["A study of resonant structures in $B^{+}\\rightarrow{D^{\\ast+}D^{-}K^{+}}$ and $B^{+}\\rightarrow{D^{\\ast-}D^{+}K^{+}}$ decays is performed, using proton-proton collision data at centre-of-mass energies of $\\sqrt{s}=7, 8$, and $13$ TeV recorded by the LHCb experiment, corresponding to an integrated luminosity of 9 fb$^{-1}$. A simultaneous amplitude fit is performed to the two channels with contributions from resonances decaying to $D^{\\ast-}D^{+}$ and $D^{\\ast+}D^{-}$ states linked by $C$ parity.","This procedure allows the $C$-parities of resonances in the $D^{\\ast\\pm}D^{\\mp}$ mass spectra to be determined.","Four charmonium(-like) states are observed decaying into $D^{\\ast\\pm}D^{\\mp}$: $\\eta_c(3945)$, $h_c(4000)$, $\\chi_{c1}(4010)$ and $h_c(4300)$, with quantum numbers $J^{PC}$ equal to $0^{-+}$, $1^{+-}$, $1^{++}$ and $1^{+-}$, respectively.","At least three of these states have not been observed previously.","In addition, the existence of the $T_{\\bar{c}\\bar{s}0}^{*}(2870)^{0}$ and $T_{\\bar{c}\\bar{s}1}^{*}(2900)^{0}$ resonances in the $D^-K^+$ mass spectrum, already observed in the $B^+ \\to D^+ D^- K^+$ decay, is confirmed in a different production channel."],"url":"http://arxiv.org/abs/2406.03156v1","category":"hep-ex"}
{"created":"2024-06-05 11:25:32","title":"Effective short distance interaction in Calogero-Sutherland quantum fluids","abstract":"We consider the effective conformal field theory with symmetry W-infinity x W-infinity that describes the thermodynamic limit of the Calogero-Sutherland model. In the repulsive regime of the free fermion formulation, we identify an attractive interaction between opposite moving particle-hole pairs that dominates the short distance behavior and that is proposed as responsible for the destabilization of the ground state, leading to a new one of bosonic nature. The process is described by a Bogoliubov transformation of the free fermion bilinear operators into bosonic ones, preserving the form of the W-infinity algebra but decoupling the opposite chirality terms in the hamiltonian, as expected in the low energy limit. In coordinate space this interaction has a short range component that arises due to the quantum regularization of the theory. The described dynamical process may be considered as a mechanism of the emergence of the known charge and quantum statistics fractionalization of the low lying excitations of the theory, as predicted in both first and second quantization studies.","sentences":["We consider the effective conformal field theory with symmetry W-infinity x W-infinity that describes the thermodynamic limit of the Calogero-Sutherland model.","In the repulsive regime of the free fermion formulation, we identify an attractive interaction between opposite moving particle-hole pairs that dominates the short distance behavior and that is proposed as responsible for the destabilization of the ground state, leading to a new one of bosonic nature.","The process is described by a Bogoliubov transformation of the free fermion bilinear operators into bosonic ones, preserving the form of the W-infinity algebra but decoupling the opposite chirality terms in the hamiltonian, as expected in the low energy limit.","In coordinate space this interaction has a short range component that arises due to the quantum regularization of the theory.","The described dynamical process may be considered as a mechanism of the emergence of the known charge and quantum statistics fractionalization of the low lying excitations of the theory, as predicted in both first and second quantization studies."],"url":"http://arxiv.org/abs/2406.03153v1","category":"hep-th"}
{"created":"2024-06-05 10:45:56","title":"Strange Dwarfs: a review on the (in)stability","abstract":"White dwarfs are the remnants of stars not massive enough to become supernovae. This review explores the concept of strange dwarfs, a unique class of white dwarfs which contain cores of strange quark matter. Strange dwarfs have different sizes, masses, and evolutionary paths with respect to white dwarfs. They might form through the accumulation of normal matter on strange quark stars or by capture of strangelets. The stability of strange dwarfs has been debated, with initial studies suggesting stability, while later analyses indicated potential instability. This review revisits these discussions, focusing on the critical role of boundary conditions between nuclear and quark matter in determining stability. It also offers insights into their formation, structure, and possible detection in the universe.","sentences":["White dwarfs are the remnants of stars not massive enough to become supernovae.","This review explores the concept of strange dwarfs, a unique class of white dwarfs which contain cores of strange quark matter.","Strange dwarfs have different sizes, masses, and evolutionary paths with respect to white dwarfs.","They might form through the accumulation of normal matter on strange quark stars or by capture of strangelets.","The stability of strange dwarfs has been debated, with initial studies suggesting stability, while later analyses indicated potential instability.","This review revisits these discussions, focusing on the critical role of boundary conditions between nuclear and quark matter in determining stability.","It also offers insights into their formation, structure, and possible detection in the universe."],"url":"http://arxiv.org/abs/2406.03137v1","category":"astro-ph.SR"}
{"created":"2024-06-05 10:22:09","title":"Simulation of Proton and Carbon-12 Ion Beam for Tumor/Cancer Treatment","abstract":"Treating cancer is one of the most challenging task in medical sciences. Only limited types of cancer treatments are available as their study is still ongoing. The earlier therapies like radiotherapy with x-rays, chemotherapy are associated with lot of side-effects. One of the most desirable cancer treatment is using particle beam therapies. These therapies are quite less risky than other types of cancer and tumor treatments. In this paper we present the proton and carbon-12 ion beam simulation that can help in tumor and cancer treatment. We simulated the proton and carbon-12 ion beam in water and soft tissue using geant4 toolkit. The protons are observed to have much better energy deposition in the water and soft tissue than carbon-12 ion and gamma photon beams.","sentences":["Treating cancer is one of the most challenging task in medical sciences.","Only limited types of cancer treatments are available as their study is still ongoing.","The earlier therapies like radiotherapy with x-rays, chemotherapy are associated with lot of side-effects.","One of the most desirable cancer treatment is using particle beam therapies.","These therapies are quite less risky than other types of cancer and tumor treatments.","In this paper we present the proton and carbon-12 ion beam simulation that can help in tumor and cancer treatment.","We simulated the proton and carbon-12 ion beam in water and soft tissue using geant4 toolkit.","The protons are observed to have much better energy deposition in the water and soft tissue than carbon-12 ion and gamma photon beams."],"url":"http://arxiv.org/abs/2406.03126v1","category":"physics.acc-ph"}
{"created":"2024-06-05 09:56:42","title":"Lepton flavor violating decays $Z\\rightarrow l^{\\pm}_{i}l^{\\mp}_{j}$ in the B-L Supersymmetric Standard Model","abstract":"Lepton flavor violation (LFV) represents a clear new physics (NP) signal beyond the standard model (SM). In this paper, we study LFV decays $Z\\rightarrow l^{\\pm}_{i}l^{\\mp}_{j}$ in the B-L Supersymmetric Standard Model(B-LSSM). We calculate these processes separately in the mass eigenstate basis and the electroweak interaction basis, and the latter adopt the mass insertion approximation (MIA) method. The MIA clearly shows the effect of parameters on the LFV decays $Z\\rightarrow l^{\\pm}_{i}l^{\\mp}_{j}$ in the analytic level, which provides a new way for us to analyze the LFV processes. At the same time, the corresponding constraints from the LFV decays $l^{-}_{j} \\rightarrow l^{-}_{i} \\gamma$ and $(g-2)_{\\mu}$ are considered to analyze the numerical results.","sentences":["Lepton flavor violation (LFV) represents a clear new physics (NP) signal beyond the standard model (SM).","In this paper, we study LFV decays $Z\\rightarrow l^{\\pm}_{i}l^{\\mp}_{j}$ in the B-L Supersymmetric Standard Model(B-LSSM).","We calculate these processes separately in the mass eigenstate basis and the electroweak interaction basis, and the latter adopt the mass insertion approximation (MIA) method.","The MIA clearly shows the effect of parameters on the LFV decays $Z\\rightarrow l^{\\pm}_{i}l^{\\mp}_{j}$ in the analytic level, which provides a new way for us to analyze the LFV processes.","At the same time, the corresponding constraints from the LFV decays $l^{-}_{j} \\rightarrow l^{-}_{i} \\gamma$ and $(g-2)_{\\mu}$ are considered to analyze the numerical results."],"url":"http://arxiv.org/abs/2406.03108v1","category":"hep-ph"}
{"created":"2024-06-05 09:43:55","title":"Exploring the Dark Frontier: White Dwarf-Based Constraints on Light Dark Matter","abstract":"In the vast expanse of our galaxy, white dwarfs (WDs) are natural sentinels, capturing the enigmatic dark matter (DM) particles that incessantly traverse their interiors. These celestial bodies provide a unique vantage point for probing interactions between DM particles and their constituents-nuclei or electrons-should such interactions exist. The captured DM particles may accumulate, undergo mutual annihilation, or be evaporated by the WD's own nuclei or electrons, thereby perturbing the standard cooling sequence predicted by stellar evolution theory. This letter reports pioneering constraints on DM-electron interactions derived from an in-depth analysis of four pulsating WDs. By leveraging the period variation rates of their pulsation modes, we delineate the following constraints: for a form factor $F(q) = 1$, in the DM mass range $20 \\mathrm{MeV}/c^{2} \\lesssim m_{\\chi} \\lesssim 80 \\mathrm{MeV}/c^{2}$ with a cross-section limit of $\\sigma_{\\chi,e} \\lesssim 10^{-56} \\mathrm{cm}^{2}$; for a form factor $F(q) = (\\alpha m_{e})^{2}/q^{2}$, in a the DM mass range $20\\mathrm{MeV}/c^{2} \\lesssim m_{\\chi} \\lesssim 70 \\mathrm{MeV}/c^{2}$ with a limit of $\\sigma_{\\chi,e} \\lesssim 10^{-52} \\mathrm{cm}^{2}$. These newly established constraints surpass current direct detection experiments by over fifteen orders of magnitude, forging a path into the uncharted territories of the DM parameter space. This work not only advances our understanding of light dark matter-electron interactions but also exemplifies the potential of WDs as important astrophysical laboratories for probing the elusive nature of DM.","sentences":["In the vast expanse of our galaxy, white dwarfs (WDs) are natural sentinels, capturing the enigmatic dark matter (DM) particles that incessantly traverse their interiors.","These celestial bodies provide a unique vantage point for probing interactions between DM particles and their constituents-nuclei or electrons-should such interactions exist.","The captured DM particles may accumulate, undergo mutual annihilation, or be evaporated by the WD's own nuclei or electrons, thereby perturbing the standard cooling sequence predicted by stellar evolution theory.","This letter reports pioneering constraints on DM-electron interactions derived from an in-depth analysis of four pulsating WDs.","By leveraging the period variation rates of their pulsation modes, we delineate the following constraints: for a form factor $F(q) = 1$, in the DM mass range $20 \\mathrm{MeV}/c^{2} \\lesssim m_{\\chi} \\lesssim 80 \\mathrm{MeV}/c^{2}$ with a cross-section limit of $\\sigma_{\\chi,e} \\lesssim 10^{-56} \\mathrm{cm}^{2}$; for a form factor $F(q) = (\\alpha m_{e})^{2}/q^{2}$, in a the DM mass range $20\\mathrm{MeV}/c^{2} \\lesssim m_{\\chi} \\lesssim 70 \\mathrm{MeV}/c^{2}$ with a limit of $\\sigma_{\\chi,e} \\lesssim 10^{-52} \\mathrm{cm}^{2}$. These newly established constraints surpass current direct detection experiments by over fifteen orders of magnitude, forging a path into the uncharted territories of the DM parameter space.","This work not only advances our understanding of light dark matter-electron interactions but also exemplifies the potential of WDs as important astrophysical laboratories for probing the elusive nature of DM."],"url":"http://arxiv.org/abs/2406.03100v1","category":"hep-ph"}
{"created":"2024-06-05 08:49:51","title":"Statistics for 3-isogeny induced Selmer groups of elliptic curves","abstract":"Given a sixth power free integer $a$, let $E_a$ be the elliptic curve defined by $y^2=x^3+a$. We prove explicit results for the lower density of sixth power free integers $a$ for which the $3$-isogeny induced Selmer group of $E_a$ over $\\mathbb{Q}(\\mu_3)$ has dimension $\\leq 1$. The results are proven by refining the strategy of Davenport--Heilbronn, by relating the statistics for integral binary cubic forms to the statistics for $3$-isogeny induced Selmer groups.","sentences":["Given a sixth power free integer $a$, let $E_a$ be the elliptic curve defined by $y^2=x^3+a$. We prove explicit results for the lower density of sixth power free integers $a$ for which the $3$-isogeny induced Selmer group of $E_a$ over $\\mathbb{Q}(\\mu_3)$ has dimension $\\leq 1$.","The results are proven by refining the strategy of Davenport--Heilbronn, by relating the statistics for integral binary cubic forms to the statistics for $3$-isogeny induced Selmer groups."],"url":"http://arxiv.org/abs/2406.03066v1","category":"math.NT"}
{"created":"2024-06-05 08:27:44","title":"Identification of structural shocks in Bayesian VEC models with two-state Markov-switching heteroskedasticity","abstract":"We develop a Bayesian framework for cointegrated structural VAR models identified by two-state Markovian breaks in conditional covariances. The resulting structural VEC specification with Markov-switching heteroskedasticity (SVEC-MSH) is formulated in the so-called B-parameterization, in which the prior distribution is specified directly for the matrix of the instantaneous reactions of the endogenous variables to structural innovations. We discuss some caveats pertaining to the identification conditions presented earlier in the literature on stationary structural VAR-MSH models, and revise the restrictions to actually ensure the unique global identification through the two-state heteroskedasticity. To enable the posterior inference in the proposed model, we design an MCMC procedure, combining the Gibbs sampler and the Metropolis-Hastings algorithm. The methodology is illustrated both with a simulated as well as real-world data examples.","sentences":["We develop a Bayesian framework for cointegrated structural VAR models identified by two-state Markovian breaks in conditional covariances.","The resulting structural VEC specification with Markov-switching heteroskedasticity (SVEC-MSH) is formulated in the so-called B-parameterization, in which the prior distribution is specified directly for the matrix of the instantaneous reactions of the endogenous variables to structural innovations.","We discuss some caveats pertaining to the identification conditions presented earlier in the literature on stationary structural VAR-MSH models, and revise the restrictions to actually ensure the unique global identification through the two-state heteroskedasticity.","To enable the posterior inference in the proposed model, we design an MCMC procedure, combining the Gibbs sampler and the Metropolis-Hastings algorithm.","The methodology is illustrated both with a simulated as well as real-world data examples."],"url":"http://arxiv.org/abs/2406.03053v1","category":"econ.EM"}
{"created":"2024-06-05 08:13:15","title":"Spying on the quickly variable optical sky with the fast optical photometer SiFAP2","abstract":"The development of detectors with a high time resolution has been pivotal to our comprehension of neutron stars and the accurate measurement of their properties. While high-time resolution astronomy has become a standard in the radio and the high-/very-high-energy bands, progress in the visible band has been comparatively much slower. SiFAP2 is a high-speed optical photometer mounted at the INAF Telescopio Nazionale Galileo. Its potential emerged with the discovery of the first two optical millisecond pulsars: these are among the most efficient particle accelerators and natural laboratories of fundamental physics. Optical millisecond pulsations challenge the standard pulsar paradigm, requiring innovative solutions. Higher photon counting statistics of optical telescopes, compared to high-energy instruments, attain unprecedented sensitivity for weak pulsed signals from bright accreting neutron stars, which are the best candidates for still undetected continuous gravitational waves.","sentences":["The development of detectors with a high time resolution has been pivotal to our comprehension of neutron stars and the accurate measurement of their properties.","While high-time resolution astronomy has become a standard in the radio and the high-/very-high-energy bands, progress in the visible band has been comparatively much slower.","SiFAP2 is a high-speed optical photometer mounted at the INAF Telescopio Nazionale Galileo.","Its potential emerged with the discovery of the first two optical millisecond pulsars: these are among the most efficient particle accelerators and natural laboratories of fundamental physics.","Optical millisecond pulsations challenge the standard pulsar paradigm, requiring innovative solutions.","Higher photon counting statistics of optical telescopes, compared to high-energy instruments, attain unprecedented sensitivity for weak pulsed signals from bright accreting neutron stars, which are the best candidates for still undetected continuous gravitational waves."],"url":"http://arxiv.org/abs/2406.03042v1","category":"astro-ph.HE"}
{"created":"2024-06-05 08:06:13","title":"A closure for Hamilton-connectedness in $\\{K_{1,3},\u0393_3\\}$-free graphs","abstract":"We introduce a closure technique for Hamilton-connectedness of $\\{K_{1,3},\\Gamma_3\\}$-free graphs, where $\\Gamma_3$ is the graph obtained by joining two vertex-disjoint triangles with a path of length $3$. The closure turns a claw-free graph into a line graph of a multigraph while preserving its (non)-Hamilton-connectedness. The most technical parts of the proof are computer-assisted.   The main application of the closure is given in a subsequent paper showing that every $3$-connected $\\{K_{1,3},\\Gamma_3\\}$-free graph is Hamilton-connected, thus resolving one of the two last open cases in the characterization of pairs of connected forbidden subgraphs implying Hamilton-connectedness.","sentences":["We introduce a closure technique for Hamilton-connectedness of $\\{K_{1,3},\\Gamma_3\\}$-free graphs, where $\\Gamma_3$ is the graph obtained by joining two vertex-disjoint triangles with a path of length $3$. The closure turns a claw-free graph into a line graph of a multigraph while preserving its (non)-Hamilton-connectedness.","The most technical parts of the proof are computer-assisted.   ","The main application of the closure is given in a subsequent paper showing that every $3$-connected $\\{K_{1,3},\\Gamma_3\\}$-free graph is Hamilton-connected, thus resolving one of the two last open cases in the characterization of pairs of connected forbidden subgraphs implying Hamilton-connectedness."],"url":"http://arxiv.org/abs/2406.03036v1","category":"math.CO"}
{"created":"2024-06-05 07:56:04","title":"Against Bell's Theorem","abstract":"Bell's theorem supposedly demonstrates an irreconcilable conflict between quantum mechanics and local, realistic hidden variable theories. In this paper we show that all experiments that aim to prove Bell's theorem do not actually achieve this goal. Our conclusions are based on a straightforward statistical analysis of the outcomes of these experiments. The key tool in our study is probability theory and, in particular, the concept of sample space for the dichotomic random variables that quantifies the outcomes of such experiments. We also show that an experimental proof of Bell's theorem is not, in principle, impossible, but it would require a completely different experimental apparatus than those commonly used to allegedly achieve this objective. The main consequence of our work is that we cannot dismiss local realistic hidden variable theories on the basis of the available experimental data.","sentences":["Bell's theorem supposedly demonstrates an irreconcilable conflict between quantum mechanics and local, realistic hidden variable theories.","In this paper we show that all experiments that aim to prove Bell's theorem do not actually achieve this goal.","Our conclusions are based on a straightforward statistical analysis of the outcomes of these experiments.","The key tool in our study is probability theory and, in particular, the concept of sample space for the dichotomic random variables that quantifies the outcomes of such experiments.","We also show that an experimental proof of Bell's theorem is not, in principle, impossible, but it would require a completely different experimental apparatus than those commonly used to allegedly achieve this objective.","The main consequence of our work is that we cannot dismiss local realistic hidden variable theories on the basis of the available experimental data."],"url":"http://arxiv.org/abs/2406.03028v1","category":"quant-ph"}
{"created":"2024-06-05 07:42:19","title":"Quasiperiodic [110] Symmetric Tilt FCC Grain Boundaries","abstract":"In this work, we investigate [110] symmetric tilt FCC grain boundaries (GBs) by a recently developed approach for quasiperiodic interfaces using the Landau-Brazovskii model. On special tilt angles associated with quadratic algebraic numbers, quasiperiodic GBs exhibit generalized Fibonacci substitution rules. The transition mechanism from quasiperiodic GBs to periodic GBs is explored through sphere offsets and spectral coalescence. We also propose an accurate method to calculate the GB energy for arbitrary tilt angle and analyze the factors affecting the GB energy. The revealing GB energy change is continue along with tilt angle except periodic GBs.","sentences":["In this work, we investigate [110] symmetric tilt FCC grain boundaries (GBs) by a recently developed approach for quasiperiodic interfaces using the Landau-Brazovskii model.","On special tilt angles associated with quadratic algebraic numbers, quasiperiodic GBs exhibit generalized Fibonacci substitution rules.","The transition mechanism from quasiperiodic GBs to periodic GBs is explored through sphere offsets and spectral coalescence.","We also propose an accurate method to calculate the GB energy for arbitrary tilt angle and analyze the factors affecting the GB energy.","The revealing GB energy change is continue along with tilt angle except periodic GBs."],"url":"http://arxiv.org/abs/2406.03023v1","category":"physics.atom-ph"}
{"created":"2024-06-05 07:34:24","title":"Parameterization of Stochasticity in Galaxy Clustering and Reconstruction of Tomographic Matter Clustering","abstract":"The stochasticity in galaxy clustering, the mismatch between galaxy and underlying matter distribution, suppresses the matter clustering amplitude reconstructed by the combination of galaxy auto-correlation and galaxy-galaxy lensing cross-correlation. In this work, we solve the stochasticity systematics by parameterizing the cross correlation coefficient $r(k)$ between galaxy and matter. We investigate the performance of 12 kinds of parameterization schemes, against the cosmoDC2 $\\&$ TNG300-1 galaxy samples over a wide range of redshift and flux cut. The 2-parameter fits are found to describe the stochasticity up to $k_{\\rm max}=0.9\\,{\\rm Mpc^{-1}}h$, while the best performing quadratic scheme $r^2_s(k) = 1+c_1 k+c_2 k^2$ reaches better than $1\\%$ accuracy for both the direct ${r}^2_s(k)$ fit and reconstructing matter clustering. Then, we apply the accurate quadratic scheme to forecast the tomographic matter clustering reconstruction by the combination DESI-like LRG $\\times$ CSST-like cosmic shear. Depending on assumption of stochasticity, we find that the neglect of a serious stochasticity would result in significant systematic bias in both the reconstruction and the inferred cosmological parameters, even if we adopt scale cut $k_{\\rm max}=0.1\\,{\\rm Mpc^{-1}}h$. We demonstrate the necessity of including stochasticity in reconstruction, and forecast that the reconstruction alone enables a $S_8$ constraint at about $1.5\\%$ precision, free from galaxy bias and stochasticity. We will validate our method for DESI spectroscopic survey, and the analysis is expected to be complementary to DESI cosmological constraint by BAO and RSD.","sentences":["The stochasticity in galaxy clustering, the mismatch between galaxy and underlying matter distribution, suppresses the matter clustering amplitude reconstructed by the combination of galaxy auto-correlation and galaxy-galaxy lensing cross-correlation.","In this work, we solve the stochasticity systematics by parameterizing the cross correlation coefficient $r(k)$ between galaxy and matter.","We investigate the performance of 12 kinds of parameterization schemes, against the cosmoDC2 $\\&$ TNG300-1 galaxy samples over a wide range of redshift and flux cut.","The 2-parameter fits are found to describe the stochasticity up to $k_{\\rm max}=0.9\\,{\\rm Mpc^{-1}}h$, while the best performing quadratic scheme $r^2_s(k)","= 1+c_1 k+c_2 k^2$ reaches better than $1\\%$ accuracy for both the direct ${r}^2_s(k)$ fit and reconstructing matter clustering.","Then, we apply the accurate quadratic scheme to forecast the tomographic matter clustering reconstruction by the combination DESI-like LRG $\\times$ CSST-like cosmic shear.","Depending on assumption of stochasticity, we find that the neglect of a serious stochasticity would result in significant systematic bias in both the reconstruction and the inferred cosmological parameters, even if we adopt scale cut $k_{\\rm max}=0.1\\,{\\rm Mpc^{-1}}h$. We demonstrate the necessity of including stochasticity in reconstruction, and forecast that the reconstruction alone enables a $S_8$ constraint at about $1.5\\%$ precision, free from galaxy bias and stochasticity.","We will validate our method for DESI spectroscopic survey, and the analysis is expected to be complementary to DESI cosmological constraint by BAO and RSD."],"url":"http://arxiv.org/abs/2406.03018v1","category":"astro-ph.CO"}
{"created":"2024-06-05 06:57:29","title":"Direct observation of the vanishing EELS cross section in graphene","abstract":"In transmission electron energy-loss spectroscopy, the cross section in 2D is quenched by kinematic effects once the momentum transfer becomes smaller than a critical value set by $q_z$, the momentum loss parallel to the beam. Our highly momentum ($\\Delta q = 0.02$~\\r{A}$^{-1}$) and energy ($\\Delta E = 45$~meV) resolved setup is instrumental on delivering the unprecedented experimental verification of quenched 2D EEL spectra on freestanding graphene at momentum transfers $q$ below $0.06$\\r{A}$^{-1}$. We retrieve the intrinsic uniform dielectric response of graphene from measured spectra by quantifying the kinematic suppression.","sentences":["In transmission electron energy-loss spectroscopy, the cross section in 2D is quenched by kinematic effects once the momentum transfer becomes smaller than a critical value set by $q_z$, the momentum loss parallel to the beam.","Our highly momentum ($\\Delta q = 0.02$~\\r{A}$^{-1}$) and energy ($\\Delta E = 45$~meV) resolved setup is instrumental on delivering the unprecedented experimental verification of quenched 2D EEL spectra on freestanding graphene at momentum transfers $q$ below $0.06$\\r{A}$^{-1}$. We retrieve the intrinsic uniform dielectric response of graphene from measured spectra by quantifying the kinematic suppression."],"url":"http://arxiv.org/abs/2406.02998v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-05 06:43:48","title":"A Human-Annotated Video Dataset for Training and Evaluation of 360-Degree Video Summarization Methods","abstract":"In this paper we introduce a new dataset for 360-degree video summarization: the transformation of 360-degree video content to concise 2D-video summaries that can be consumed via traditional devices, such as TV sets and smartphones. The dataset includes ground-truth human-generated summaries, that can be used for training and objectively evaluating 360-degree video summarization methods. Using this dataset, we train and assess two state-of-the-art summarization methods that were originally proposed for 2D-video summarization, to serve as a baseline for future comparisons with summarization methods that are specifically tailored to 360-degree video. Finally, we present an interactive tool that was developed to facilitate the data annotation process and can assist other annotation activities that rely on video fragment selection.","sentences":["In this paper we introduce a new dataset for 360-degree video summarization: the transformation of 360-degree video content to concise 2D-video summaries that can be consumed via traditional devices, such as TV sets and smartphones.","The dataset includes ground-truth human-generated summaries, that can be used for training and objectively evaluating 360-degree video summarization methods.","Using this dataset, we train and assess two state-of-the-art summarization methods that were originally proposed for 2D-video summarization, to serve as a baseline for future comparisons with summarization methods that are specifically tailored to 360-degree video.","Finally, we present an interactive tool that was developed to facilitate the data annotation process and can assist other annotation activities that rely on video fragment selection."],"url":"http://arxiv.org/abs/2406.02991v1","category":"cs.CV"}
{"created":"2024-06-05 06:28:17","title":"Effects of pairing strength on the nuclear structure and double-$\u03b2$ decay predictions within the mapped interacting boson model","abstract":"The low-energy nuclear structure and two-neutrino double-$\\beta$ ($2\\nu\\beta\\beta$) decay are studied within the interacting boson model (IBM) that is based on the nuclear energy density functional (EDF). The IBM Hamiltonian describing the initial and final even-even nuclei, and the interacting boson fermion-fermion Hamiltonian producing the intermediate states of the neighboring odd-odd nuclei, are determined by the microscopic inputs provided by the self-consistent mean-field (SCMF) calculations employing a relativistic EDF and a separable pairing force. Sensitivities of the low-lying structure and $2\\nu\\beta\\beta$-decay properties to the pairing strength are specifically analyzed. It is shown that the SCMF calculations with the decreased and increased pairing strengths lead to the quadrupole-quadrupole interaction strengths in the IBM that are, respectively, significantly enhanced and reduced in magnitude. When the increased pairing is adopted, in particular, the energy levels of the excited $0^+$ states are lowered, and the predicted $2\\nu\\beta\\beta$-decay nuclear matrix elements (NMEs) increase in magnitude systematically. The mapped IBM employing the increased pairing force generates effective NMEs and half-lives that are overall in a reasonable agreement with the experimental data.","sentences":["The low-energy nuclear structure and two-neutrino double-$\\beta$ ($2\\nu\\beta\\beta$) decay are studied within the interacting boson model (IBM) that is based on the nuclear energy density functional (EDF).","The IBM Hamiltonian describing the initial and final even-even nuclei, and the interacting boson fermion-fermion Hamiltonian producing the intermediate states of the neighboring odd-odd nuclei, are determined by the microscopic inputs provided by the self-consistent mean-field (SCMF) calculations employing a relativistic EDF and a separable pairing force.","Sensitivities of the low-lying structure and $2\\nu\\beta\\beta$-decay properties to the pairing strength are specifically analyzed.","It is shown that the SCMF calculations with the decreased and increased pairing strengths lead to the quadrupole-quadrupole interaction strengths in the IBM that are, respectively, significantly enhanced and reduced in magnitude.","When the increased pairing is adopted, in particular, the energy levels of the excited $0^+$ states are lowered, and the predicted $2\\nu\\beta\\beta$-decay nuclear matrix elements (NMEs) increase in magnitude systematically.","The mapped IBM employing the increased pairing force generates effective NMEs and half-lives that are overall in a reasonable agreement with the experimental data."],"url":"http://arxiv.org/abs/2406.02986v1","category":"nucl-th"}
{"created":"2024-06-05 05:29:14","title":"Six-Derivative Yang-Mills Couplings in Heterotic String Theory","abstract":"In this work, we present a comprehensive analysis of the structure of six-derivative bosonic couplings in heterotic string theory. First, we determine the maximal covariant and Yang-Mills gauge invariant basis, which consists of 801 independent coupling constants. By imposing T-duality constraints on the circular reduction of these terms, we obtain 468 relations between the coupling constants at the six-derivative order and the known couplings at lower derivative orders. Through the use of field redefinitions, we are able to eliminate the remaining 333 coupling constants. Remarkably, we find that the Yang-Mills field strength only appears through the trace of two field strengths or their derivatives. Finally, we perform further field redefinition to rewrite the remaining couplings in a canonical form characterized by 85 independent couplings.","sentences":["In this work, we present a comprehensive analysis of the structure of six-derivative bosonic couplings in heterotic string theory.","First, we determine the maximal covariant and Yang-Mills gauge invariant basis, which consists of 801 independent coupling constants.","By imposing T-duality constraints on the circular reduction of these terms, we obtain 468 relations between the coupling constants at the six-derivative order and the known couplings at lower derivative orders.","Through the use of field redefinitions, we are able to eliminate the remaining 333 coupling constants.","Remarkably, we find that the Yang-Mills field strength only appears through the trace of two field strengths or their derivatives.","Finally, we perform further field redefinition to rewrite the remaining couplings in a canonical form characterized by 85 independent couplings."],"url":"http://arxiv.org/abs/2406.02960v1","category":"hep-th"}
{"created":"2024-06-05 05:18:05","title":"Exploring active-sterile neutrino mixings models in MES mechanism using modular $S_3$ symmetry","abstract":"We study the minimal extended seesaw mechanism with one sterile neutrino in a 3+1 framework using modular $S_3$ symmetry. The active-sterile neutrino models are classified based on the assignments of $S_3$ representations and modular weights of the left-handed lepton doublets, triplet right-handed neutrino, and sterile neutrino. No scalar flavons are considered, and the flavor symmetry is broken by the vacuum expectation value (vev) of the modulus $\\tau$. For a particular set of representations of the Leptons and Higgs field, we obtain eleven (11) different models based on different modular weights of charged lepton $(k_L)$ and right-handed neutrino ($k_N$). Out of these, we consider two models, which are discriminated by carrying out the numerical analysis so that the parameter space in each model can fit the latest neutrino oscillation data at 3$\\sigma$. The Planck cosmological bound on the upper limit of the sum of the active neutrino masses $\\sum m_i <0.12$eV is also considered. Finally, the best-fit parameters of the neutrino observables and model predictions are evaluated using the minimum $\\chi^2$ analysis.","sentences":["We study the minimal extended seesaw mechanism with one sterile neutrino in a 3+1 framework using modular $S_3$ symmetry.","The active-sterile neutrino models are classified based on the assignments of $S_3$ representations and modular weights of the left-handed lepton doublets, triplet right-handed neutrino, and sterile neutrino.","No scalar flavons are considered, and the flavor symmetry is broken by the vacuum expectation value (vev) of the modulus $\\tau$. For a particular set of representations of the Leptons and Higgs field, we obtain eleven (11) different models based on different modular weights of charged lepton $(k_L)$ and right-handed neutrino ($k_N$).","Out of these, we consider two models, which are discriminated by carrying out the numerical analysis so that the parameter space in each model can fit the latest neutrino oscillation data at 3$\\sigma$. The Planck cosmological bound on the upper limit of the sum of the active neutrino masses $\\sum m_i","<0.12$eV is also considered.","Finally, the best-fit parameters of the neutrino observables and model predictions are evaluated using the minimum $\\chi^2$ analysis."],"url":"http://arxiv.org/abs/2406.02949v1","category":"hep-ph"}
{"created":"2024-06-05 05:14:28","title":"Shaping the topology of twisted bilayer graphene via time-reversal symmetry breaking","abstract":"Symmetry breaking is an effective tool for tuning the transport and topological properties of 2D layered materials. Among these materials, twisted bilayer graphene (TBG) has emerged as a promising platform for new physics, characterized by a rich interplay between topological features and strongly correlated electronic behavior. In this study, we utilize time-reversal symmetry breaking (TRSB) to manipulate the topological properties of TBG. By varying the strength of TRSB, we discover a topological phase transition between a topological insulating phase, which exhibits a pair of flat bands with opposite Chern numbers, and a novel insulating state where the Chern number, but not the Berry curvature, of the flat bands vanishes. We demonstrate that this topological transition is mediated by a gap closing at the $\\Gamma$ point, and we construct a three-dimensional phase diagram as a function of the twisting angle, the symmetry-breaking parameter, and the mismatch coupling between AA and AB stacking regions. Finally, we show that this novel electronic phase can be identified in the lab by measuring, as a function of the Fermi energy, its non-quantized anomalous Hall conductivity that is induced by the Berry dipole density of the lowest flat bands.","sentences":["Symmetry breaking is an effective tool for tuning the transport and topological properties of 2D layered materials.","Among these materials, twisted bilayer graphene (TBG) has emerged as a promising platform for new physics, characterized by a rich interplay between topological features and strongly correlated electronic behavior.","In this study, we utilize time-reversal symmetry breaking (TRSB) to manipulate the topological properties of TBG.","By varying the strength of TRSB, we discover a topological phase transition between a topological insulating phase, which exhibits a pair of flat bands with opposite Chern numbers, and a novel insulating state where the Chern number, but not the Berry curvature, of the flat bands vanishes.","We demonstrate that this topological transition is mediated by a gap closing at the $\\Gamma$ point, and we construct a three-dimensional phase diagram as a function of the twisting angle, the symmetry-breaking parameter, and the mismatch coupling between AA and AB stacking regions.","Finally, we show that this novel electronic phase can be identified in the lab by measuring, as a function of the Fermi energy, its non-quantized anomalous Hall conductivity that is induced by the Berry dipole density of the lowest flat bands."],"url":"http://arxiv.org/abs/2406.02947v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-05 05:08:57","title":"Kinetic simulations of electron-positron induced streaming instability in the context of gamma-ray halos around pulsars","abstract":"The possibility of slow diffusion regions as the origin for extended TeV emission halos around some pulsars (such as PSR J0633+1746 and PSR B0656+14) challenges the standard scaling of the electron diffusion coefficient in the interstellar medium. Self-generated turbulence by electron-positron pairs streaming out of the pulsar wind nebula was proposed as a possible mechanism to produce the enhanced turbulence required to explain the morphology and brightness of these TeV halos. We perform fully kinetic 1D3V particle-in-cell simulations of this instability, considering the case where streaming electrons and positrons have the same density. This implies purely resonant instability as the beam does not carry any current. We compare the linear phase of the instability with analytical theory and find very reasonable agreement. The non-linear phase of the instability is also studied, which reveals that the intensity of saturated waves is consistent with a momentum exchange criterion between a decelerating beam and growing magnetic waves. With the adopted parameters, the instability-driven wavemodes cover both the Alfv\\'enic (fluid) and kinetic scales. The spectrum of the produced waves is non-symmetric, with left-handed circular polarisation waves being strongly damped when entering the ion-cyclotron branch, while right-handed waves are suppressed at smaller wavelength when entering the Whistler branch. The low-wavenumber part of the spectrum remains symmetric when in the Alfv\\'enic branch. As a result, positrons behave dynamically differently compared to electrons. We also observed a second harmonic plasma emission in the wave spectrum. An MHD-PIC approach is warranted to probe hotter beams and investigate the Alfv\\'en branch physics. This work confirms that the self-confinement scenario develops essentially according to analytical expectations [...](abridged)","sentences":["The possibility of slow diffusion regions as the origin for extended TeV emission halos around some pulsars (such as PSR J0633+1746 and PSR B0656+14) challenges the standard scaling of the electron diffusion coefficient in the interstellar medium.","Self-generated turbulence by electron-positron pairs streaming out of the pulsar wind nebula was proposed as a possible mechanism to produce the enhanced turbulence required to explain the morphology and brightness of these TeV halos.","We perform fully kinetic 1D3V particle-in-cell simulations of this instability, considering the case where streaming electrons and positrons have the same density.","This implies purely resonant instability as the beam does not carry any current.","We compare the linear phase of the instability with analytical theory and find very reasonable agreement.","The non-linear phase of the instability is also studied, which reveals that the intensity of saturated waves is consistent with a momentum exchange criterion between a decelerating beam and growing magnetic waves.","With the adopted parameters, the instability-driven wavemodes cover both the Alfv\\'enic (fluid) and kinetic scales.","The spectrum of the produced waves is non-symmetric, with left-handed circular polarisation waves being strongly damped when entering the ion-cyclotron branch, while right-handed waves are suppressed at smaller wavelength when entering the Whistler branch.","The low-wavenumber part of the spectrum remains symmetric when in the Alfv\\'enic branch.","As a result, positrons behave dynamically differently compared to electrons.","We also observed a second harmonic plasma emission in the wave spectrum.","An MHD-PIC approach is warranted to probe hotter beams and investigate the Alfv\\'en branch physics.","This work confirms that the self-confinement scenario develops essentially according to analytical expectations","[...](abridged)"],"url":"http://arxiv.org/abs/2406.02945v1","category":"astro-ph.HE"}
{"created":"2024-06-05 05:06:07","title":"Texture Zeros in modular Left-Right Symmetric Model","abstract":"Texture zeros play a crucial role in the phenomenological implementation of a particular model in the Beyond Standard Model framework. As such, in this work we study the possibility concerning the origin of texture zero in the neutrino mass matrix in the context of modular Left-Right Symmetric Model. Implementation of texture zeros in the light neutrino mass matrix brings forth the existence of allowed and disallowed classes of the texture zero classification. In the present work, we have studied Neutrinoless Double-Beta Decay, Lepton flavor violation and Resonant Leptogenesis and tried to analyze if the simultaneous study of these three phenomenology are connected with a common parameter space within the model. We have determined the possible combinations of 1-0 and 2-0 textures in the context of resulting light neutrino mass matrix and illustrated the allowed classes for both textures corresponding to the realization of neutrinoless double beta decay, leptogenesis and lepton flavor violation. The study has been carried out for both normal and inverted hierarchy and the results have been illustrated in the present work.","sentences":["Texture zeros play a crucial role in the phenomenological implementation of a particular model in the Beyond Standard Model framework.","As such, in this work we study the possibility concerning the origin of texture zero in the neutrino mass matrix in the context of modular Left-Right Symmetric Model.","Implementation of texture zeros in the light neutrino mass matrix brings forth the existence of allowed and disallowed classes of the texture zero classification.","In the present work, we have studied Neutrinoless Double-Beta Decay, Lepton flavor violation and Resonant Leptogenesis and tried to analyze if the simultaneous study of these three phenomenology are connected with a common parameter space within the model.","We have determined the possible combinations of 1-0 and 2-0 textures in the context of resulting light neutrino mass matrix and illustrated the allowed classes for both textures corresponding to the realization of neutrinoless double beta decay, leptogenesis and lepton flavor violation.","The study has been carried out for both normal and inverted hierarchy and the results have been illustrated in the present work."],"url":"http://arxiv.org/abs/2406.02944v1","category":"hep-ph"}
{"created":"2024-06-05 04:54:49","title":"Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder","abstract":"VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.","sentences":["VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks.","This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers.","It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook.","Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences.","The experimental results demonstrate that PQ-VAE addresses ``index collapse\" effectively, especially for larger codebooks.","The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches.","Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks."],"url":"http://arxiv.org/abs/2406.02940v1","category":"cs.SD"}
{"created":"2024-06-05 04:26:30","title":"SO($N$) singlet-projection model on the pyrochlore lattice","abstract":"We present an extensive quantum Monte Carlo study of a nearest-neighbor, singlet-projection model on the pyrochlore lattice that exhibits SO($N$) symmetry and is sign-problem-free. We find that in contrast to the previously studied two-dimensional variations of this model that harbor critical points between their ground state phases, the non-bipartite pyrochlore lattice in three spatial dimensions appears to exhibit a first-order transition between a magnetically-ordered phase and some, as yet uncharacterized, paramagnetic phase. We also observe that the magnetically-ordered phase survives to a relatively large value of $N=8$, and that it is gone for $N=9$.","sentences":["We present an extensive quantum Monte Carlo study of a nearest-neighbor, singlet-projection model on the pyrochlore lattice that exhibits SO($N$) symmetry and is sign-problem-free.","We find that in contrast to the previously studied two-dimensional variations of this model that harbor critical points between their ground state phases, the non-bipartite pyrochlore lattice in three spatial dimensions appears to exhibit a first-order transition between a magnetically-ordered phase and some, as yet uncharacterized, paramagnetic phase.","We also observe that the magnetically-ordered phase survives to a relatively large value of $N=8$, and that it is gone for $N=9$."],"url":"http://arxiv.org/abs/2406.02926v1","category":"cond-mat.str-el"}
{"created":"2024-06-05 04:00:39","title":"Room-temperature tunable tunneling magnetoresistance in Fe3GaTe2/WSe2/Fe3GaTe2 van der Waals heterostructures","abstract":"The exceptional properties of two-dimensional (2D) magnet materials present a novel approach to fabricate functional magnetic tunnel junctions (MTJ) by constructing full van der Waals (vdW) heterostructures with atomically sharp and clean interfaces. The exploration of vdW MTJ devices with high working temperature and adjustable functionalities holds great potential for advancing the application of 2D materials in magnetic sensing and data storage. Here, we report the observation of highly tunable room-temperature tunneling magnetoresistance through electronic means in a full vdW Fe3GaTe2/WSe2/Fe3GaTe2 MTJ. The spin valve effect of the MTJ can be detected even with the current below 1 nA, both at low and room temperatures, yielding a tunneling magnetoresistance (TMR) of 340% at 2 K and 50% at 300 K, respectively. Importantly, the magnitude and sign of TMR can be modulated by a DC bias current, even at room temperature, a capability that was previously unrealized in full vdW MTJs. This tunable TMR arises from the contribution of energy-dependent localized spin states in the metallic ferromagnet Fe3GaTe2 during tunnel transport when a finite electrical bias is applied. Our work offers a new perspective for designing and exploring room-temperature tunable spintronic devices based on vdW magnet heterostructures.","sentences":["The exceptional properties of two-dimensional (2D) magnet materials present a novel approach to fabricate functional magnetic tunnel junctions (MTJ) by constructing full van der Waals (vdW) heterostructures with atomically sharp and clean interfaces.","The exploration of vdW MTJ devices with high working temperature and adjustable functionalities holds great potential for advancing the application of 2D materials in magnetic sensing and data storage.","Here, we report the observation of highly tunable room-temperature tunneling magnetoresistance through electronic means in a full vdW Fe3GaTe2/WSe2/Fe3GaTe2 MTJ.","The spin valve effect of the MTJ can be detected even with the current below 1 nA, both at low and room temperatures, yielding a tunneling magnetoresistance (TMR) of 340% at 2 K and 50% at 300 K, respectively.","Importantly, the magnitude and sign of TMR can be modulated by a DC bias current, even at room temperature, a capability that was previously unrealized in full vdW MTJs.","This tunable TMR arises from the contribution of energy-dependent localized spin states in the metallic ferromagnet Fe3GaTe2 during tunnel transport when a finite electrical bias is applied.","Our work offers a new perspective for designing and exploring room-temperature tunable spintronic devices based on vdW magnet heterostructures."],"url":"http://arxiv.org/abs/2406.02907v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-05 03:57:44","title":"Staggered fermions","abstract":"These notes are based on a series of lectures on staggered fermions given at the Centre de Physique Th\\'eorique, Luminy, in Marseille, France, January 17-25, 2024.","sentences":["These notes are based on a series of lectures on staggered fermions given at the Centre de Physique Th\\'eorique, Luminy, in Marseille, France, January 17-25, 2024."],"url":"http://arxiv.org/abs/2406.02906v1","category":"hep-lat"}
{"created":"2024-06-05 02:27:39","title":"Dynamically Expanding Capacity of Autonomous Driving with Near-Miss Focused Training Framework","abstract":"The long-tail distribution of real driving data poses challenges for training and testing autonomous vehicles (AV), where rare yet crucial safety-critical scenarios are infrequent. And virtual simulation offers a low-cost and efficient solution. This paper proposes a near-miss focused training framework for AV. Utilizing the driving scenario information provided by sensors in the simulator, we design novel reward functions, which enable background vehicles (BV) to generate near-miss scenarios and ensure gradients exist not only in collision-free scenes but also in collision scenarios. And then leveraging the Robust Adversarial Reinforcement Learning (RARL) framework for simultaneous training of AV and BV to gradually enhance AV and BV capabilities, as well as generating near-miss scenarios tailored to different levels of AV capabilities. Results from three testing strategies indicate that the proposed method generates scenarios closer to near-miss, thus enhancing the capabilities of both AVs and BVs throughout training.","sentences":["The long-tail distribution of real driving data poses challenges for training and testing autonomous vehicles (AV), where rare yet crucial safety-critical scenarios are infrequent.","And virtual simulation offers a low-cost and efficient solution.","This paper proposes a near-miss focused training framework for AV.","Utilizing the driving scenario information provided by sensors in the simulator, we design novel reward functions, which enable background vehicles (BV) to generate near-miss scenarios and ensure gradients exist not only in collision-free scenes but also in collision scenarios.","And then leveraging the Robust Adversarial Reinforcement Learning (RARL) framework for simultaneous training of AV and BV to gradually enhance AV and BV capabilities, as well as generating near-miss scenarios tailored to different levels of AV capabilities.","Results from three testing strategies indicate that the proposed method generates scenarios closer to near-miss, thus enhancing the capabilities of both AVs and BVs throughout training."],"url":"http://arxiv.org/abs/2406.02865v1","category":"cs.RO"}
