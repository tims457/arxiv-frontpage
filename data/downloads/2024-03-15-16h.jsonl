{"created":"2024-03-13 17:59:56","title":"FastMAC: Stochastic Spectral Sampling of Correspondence Graph","abstract":"3D correspondence, i.e., a pair of 3D points, is a fundamental concept in computer vision. A set of 3D correspondences, when equipped with compatibility edges, forms a correspondence graph. This graph is a critical component in several state-of-the-art 3D point cloud registration approaches, e.g., the one based on maximal cliques (MAC). However, its properties have not been well understood. So we present the first study that introduces graph signal processing into the domain of correspondence graph. We exploit the generalized degree signal on correspondence graph and pursue sampling strategies that preserve high-frequency components of this signal. To address time-consuming singular value decomposition in deterministic sampling, we resort to a stochastic approximate sampling strategy. As such, the core of our method is the stochastic spectral sampling of correspondence graph. As an application, we build a complete 3D registration algorithm termed as FastMAC, that reaches real-time speed while leading to little to none performance drop. Through extensive experiments, we validate that FastMAC works for both indoor and outdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while maintaining high registration success rate on KITTI. Codes are publicly available at https://github.com/Forrest-110/FastMAC.","sentences":["3D correspondence, i.e., a pair of 3D points, is a fundamental concept in computer vision.","A set of 3D correspondences, when equipped with compatibility edges, forms a correspondence graph.","This graph is a critical component in several state-of-the-art 3D point cloud registration approaches, e.g., the one based on maximal cliques (MAC).","However, its properties have not been well understood.","So we present the first study that introduces graph signal processing into the domain of correspondence graph.","We exploit the generalized degree signal on correspondence graph and pursue sampling strategies that preserve high-frequency components of this signal.","To address time-consuming singular value decomposition in deterministic sampling, we resort to a stochastic approximate sampling strategy.","As such, the core of our method is the stochastic spectral sampling of correspondence graph.","As an application, we build a complete 3D registration algorithm termed as FastMAC, that reaches real-time speed while leading to little to none performance drop.","Through extensive experiments, we validate that FastMAC works for both indoor and outdoor benchmarks.","For example, FastMAC can accelerate MAC by 80 times while maintaining high registration success rate on KITTI.","Codes are publicly available at https://github.com/Forrest-110/FastMAC."],"url":"http://arxiv.org/abs/2403.08770v1","category":"cs.CV"}
{"created":"2024-03-13 17:58:57","title":"Simple and Scalable Strategies to Continually Pre-train Large Language Models","abstract":"Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\\rightarrow$English) and a stronger distribution shift (English$\\rightarrow$German) at the $405$M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.","sentences":["Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available.","A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training.","However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data.","In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks.","Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\\rightarrow$English) and a stronger distribution shift (English$\\rightarrow$German) at the $405$M parameter model scale with large dataset sizes (hundreds of billions of tokens).","Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM.","Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute.","Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget."],"url":"http://arxiv.org/abs/2403.08763v1","category":"cs.LG"}
{"created":"2024-03-13 17:55:34","title":"Efficient Combinatorial Optimization via Heat Diffusion","abstract":"Combinatorial optimization problems are widespread but inherently challenging due to their discrete nature.The primary limitation of existing methods is that they can only access a small fraction of the solution space at each iteration, resulting in limited efficiency for searching the global optimal. To overcome this challenge, diverging from conventional efforts of expanding the solver's search scope, we focus on enabling information to actively propagate to the solver through heat diffusion. By transforming the target function while preserving its optima, heat diffusion facilitates information flow from distant regions to the solver, providing more efficient navigation. Utilizing heat diffusion, we propose a framework for solving general combinatorial optimization problems. The proposed methodology demonstrates superior performance across a range of the most challenging and widely encountered combinatorial optimizations. Echoing recent advancements in harnessing thermodynamics for generative artificial intelligence, our study further reveals its significant potential in advancing combinatorial optimization.","sentences":["Combinatorial optimization problems are widespread but inherently challenging due to their discrete nature.","The primary limitation of existing methods is that they can only access a small fraction of the solution space at each iteration, resulting in limited efficiency for searching the global optimal.","To overcome this challenge, diverging from conventional efforts of expanding the solver's search scope, we focus on enabling information to actively propagate to the solver through heat diffusion.","By transforming the target function while preserving its optima, heat diffusion facilitates information flow from distant regions to the solver, providing more efficient navigation.","Utilizing heat diffusion, we propose a framework for solving general combinatorial optimization problems.","The proposed methodology demonstrates superior performance across a range of the most challenging and widely encountered combinatorial optimizations.","Echoing recent advancements in harnessing thermodynamics for generative artificial intelligence, our study further reveals its significant potential in advancing combinatorial optimization."],"url":"http://arxiv.org/abs/2403.08757v2","category":"stat.ML"}
{"created":"2024-03-13 17:53:47","title":"DAM: Dynamic Adapter Merging for Continual Video QA Learning","abstract":"We present a parameter-efficient method for continual video question-answering (VidQA) learning. Our method, named DAM, uses the proposed Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable efficient adaptation to continually arriving datasets, (iii) handle inputs from unknown datasets during inference, and (iv) enable knowledge sharing across similar dataset domains. Given a set of continually streaming VidQA datasets, we sequentially train dataset-specific adapters for each dataset while freezing the parameters of a large pretrained video-language backbone. During inference, given a video-question sample from an unknown domain, our method first uses the proposed non-parametric router function to compute a probability for each adapter, reflecting how relevant that adapter is to the current video-question input instance. Subsequently, the proposed dynamic adapter merging scheme aggregates all the adapter weights into a new adapter instance tailored for that particular test sample to compute the final VidQA prediction, mitigating the impact of inaccurate router predictions and facilitating knowledge sharing across domains. Our DAM model outperforms prior state-of-the-art continual learning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA datasets spanning various domains. We further extend DAM to continual image classification and image QA and outperform prior methods by a large margin. The code is publicly available at: https://github.com/klauscc/DAM","sentences":["We present a parameter-efficient method for continual video question-answering (VidQA) learning.","Our method, named DAM, uses the proposed Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable efficient adaptation to continually arriving datasets, (iii) handle inputs from unknown datasets during inference, and (iv) enable knowledge sharing across similar dataset domains.","Given a set of continually streaming VidQA datasets, we sequentially train dataset-specific adapters for each dataset while freezing the parameters of a large pretrained video-language backbone.","During inference, given a video-question sample from an unknown domain, our method first uses the proposed non-parametric router function to compute a probability for each adapter, reflecting how relevant that adapter is to the current video-question input instance.","Subsequently, the proposed dynamic adapter merging scheme aggregates all the adapter weights into a new adapter instance tailored for that particular test sample to compute the final VidQA prediction, mitigating the impact of inaccurate router predictions and facilitating knowledge sharing across domains.","Our DAM model outperforms prior state-of-the-art continual learning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA datasets spanning various domains.","We further extend DAM to continual image classification and image QA and outperform prior methods by a large margin.","The code is publicly available at: https://github.com/klauscc/DAM"],"url":"http://arxiv.org/abs/2403.08755v1","category":"cs.CV"}
{"created":"2024-03-13 17:51:08","title":"Cyclotomic Factors and LRS-Degeneracy","abstract":"We present three new, practical algorithms for polynomials in $\\mathbb{Z}[x]$: one to test if a polynomial is cyclotomic, one to determine which cyclotomic polynomials are factors, and one to determine whether the given polynomial is LRS-degenerate. A polynomial is ``LRS-degenerate'' iff it has two distinct roots $\\alpha, \\beta$ such that $\\beta = \\zeta \\alpha$ for some root of unity $\\zeta$. All three algorithms are based on ``intelligent brute force''. The first two produce the indexes of the cyclotomic polynomials; the third produces a list of degeneracy orders. The algorithms are implemented in CoCoALib.","sentences":["We present three new, practical algorithms for polynomials in $\\mathbb{Z}[x]$: one to test if a polynomial is cyclotomic, one to determine which cyclotomic polynomials are factors, and one to determine whether the given polynomial is LRS-degenerate.","A polynomial is ``LRS-degenerate'' iff it has two distinct roots $\\alpha, \\beta$ such that $\\beta = \\zeta \\alpha$ for some root of unity $\\zeta$. All three algorithms are based on ``intelligent brute force''.","The first two produce the indexes of the cyclotomic polynomials; the third produces a list of degeneracy orders.","The algorithms are implemented in CoCoALib."],"url":"http://arxiv.org/abs/2403.08751v1","category":"math.AC"}
{"created":"2024-03-13 17:48:39","title":"iCONTRA: Toward Thematic Collection Design Via Interactive Concept Transfer","abstract":"Creating thematic collections in industries demands innovative designs and cohesive concepts. Designers may face challenges in maintaining thematic consistency when drawing inspiration from existing objects, landscapes, or artifacts. While AI-powered graphic design tools offer help, they often fail to generate cohesive sets based on specific thematic concepts. In response, we introduce iCONTRA, an interactive CONcept TRAnsfer system. With a user-friendly interface, iCONTRA enables both experienced designers and novices to effortlessly explore creative design concepts and efficiently generate thematic collections. We also propose a zero-shot image editing algorithm, eliminating the need for fine-tuning models, which gradually integrates information from initial objects, ensuring consistency in the generation process without influencing the background. A pilot study suggests iCONTRA's potential to reduce designers' efforts. Experimental results demonstrate its effectiveness in producing consistent and high-quality object concept transfers. iCONTRA stands as a promising tool for innovation and creative exploration in thematic collection design. The source code will be available at: https://github.com/vdkhoi20/iCONTRA.","sentences":["Creating thematic collections in industries demands innovative designs and cohesive concepts.","Designers may face challenges in maintaining thematic consistency when drawing inspiration from existing objects, landscapes, or artifacts.","While AI-powered graphic design tools offer help, they often fail to generate cohesive sets based on specific thematic concepts.","In response, we introduce iCONTRA, an interactive CONcept TRAnsfer system.","With a user-friendly interface, iCONTRA enables both experienced designers and novices to effortlessly explore creative design concepts and efficiently generate thematic collections.","We also propose a zero-shot image editing algorithm, eliminating the need for fine-tuning models, which gradually integrates information from initial objects, ensuring consistency in the generation process without influencing the background.","A pilot study suggests iCONTRA's potential to reduce designers' efforts.","Experimental results demonstrate its effectiveness in producing consistent and high-quality object concept transfers.","iCONTRA stands as a promising tool for innovation and creative exploration in thematic collection design.","The source code will be available at: https://github.com/vdkhoi20/iCONTRA."],"url":"http://arxiv.org/abs/2403.08746v1","category":"cs.CV"}
{"created":"2024-03-13 17:46:28","title":"Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework","abstract":"Large language models (LLMs) can easily generate biased and discriminative responses. As LLMs tap into consequential decision-making (e.g., hiring and healthcare), it is of crucial importance to develop strategies to mitigate these biases. This paper focuses on social bias, tackling the association between demographic information and LLM outputs. We propose a causality-guided debiasing framework that utilizes causal understandings of (1) the data-generating process of the training corpus fed to LLMs, and (2) the internal reasoning process of LLM inference, to guide the design of prompts for debiasing LLM outputs through selection mechanisms. Our framework unifies existing de-biasing prompting approaches such as inhibitive instructions and in-context contrastive examples, and sheds light on new ways of debiasing by encouraging bias-free reasoning. Our strong empirical performance on real-world datasets demonstrates that our framework provides principled guidelines on debiasing LLM outputs even with only the black-box access.","sentences":["Large language models (LLMs) can easily generate biased and discriminative responses.","As LLMs tap into consequential decision-making (e.g., hiring and healthcare), it is of crucial importance to develop strategies to mitigate these biases.","This paper focuses on social bias, tackling the association between demographic information and LLM outputs.","We propose a causality-guided debiasing framework that utilizes causal understandings of (1) the data-generating process of the training corpus fed to LLMs, and (2) the internal reasoning process of LLM inference, to guide the design of prompts for debiasing LLM outputs through selection mechanisms.","Our framework unifies existing de-biasing prompting approaches such as inhibitive instructions and in-context contrastive examples, and sheds light on new ways of debiasing by encouraging bias-free reasoning.","Our strong empirical performance on real-world datasets demonstrates that our framework provides principled guidelines on debiasing LLM outputs even with only the black-box access."],"url":"http://arxiv.org/abs/2403.08743v1","category":"cs.CL"}
{"created":"2024-03-13 17:42:32","title":"The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models","abstract":"A substantial gap persists in understanding the reasons behind the exceptional performance of the Transformer architecture in NLP. A particularly unexplored area involves the mechanistic description of how the distribution of parameters evolves over time during training. In this work we suggest that looking at the time evolution of the statistic distribution of model parameters, and specifically at bifurcation effects, can help understanding the model quality, potentially reducing training costs and evaluation efforts and empirically showing the reasons behind the effectiveness of weights sparsification.","sentences":["A substantial gap persists in understanding the reasons behind the exceptional performance of the Transformer architecture in NLP.","A particularly unexplored area involves the mechanistic description of how the distribution of parameters evolves over time during training.","In this work we suggest that looking at the time evolution of the statistic distribution of model parameters, and specifically at bifurcation effects, can help understanding the model quality, potentially reducing training costs and evaluation efforts and empirically showing the reasons behind the effectiveness of weights sparsification."],"url":"http://arxiv.org/abs/2403.08739v1","category":"cs.CL"}
{"created":"2024-03-13 17:35:50","title":"Light-driven interlayer propagation of collective-mode excitations in layered superconductors","abstract":"Superconductors exhibit a nonlinear interaction with an applied light, which can resonantly excite the collective amplitude (Higgs) mode. Here we study light-induced dynamics of layered superconductors, where each layer is coupled to adjacent layers via the Josephson coupling and the first few layers near the surface are driven by an in-plane-polarized light. We study the system under the assumption that the interlayer Coulomb interactions are sufficiently screened out and that the phase-difference mode becomes available in the low-energy regime. We find that interlayer transport is induced via excitations of the collective amplitude and phase-difference modes, even when the applied electric field is parallel to the planes. We provide analytic calculations as well as numerical simulations of the real-time dynamics, and investigate the influence on the light-induced interlayer Josephson current and intralayer third-harmonic generation.","sentences":["Superconductors exhibit a nonlinear interaction with an applied light, which can resonantly excite the collective amplitude (Higgs) mode.","Here we study light-induced dynamics of layered superconductors, where each layer is coupled to adjacent layers via the Josephson coupling and the first few layers near the surface are driven by an in-plane-polarized light.","We study the system under the assumption that the interlayer Coulomb interactions are sufficiently screened out and that the phase-difference mode becomes available in the low-energy regime.","We find that interlayer transport is induced via excitations of the collective amplitude and phase-difference modes, even when the applied electric field is parallel to the planes.","We provide analytic calculations as well as numerical simulations of the real-time dynamics, and investigate the influence on the light-induced interlayer Josephson current and intralayer third-harmonic generation."],"url":"http://arxiv.org/abs/2403.08734v1","category":"cond-mat.supr-con"}
{"created":"2024-03-13 17:35:28","title":"GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing","abstract":"We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed by the 3D Gaussian Splatting (3DGS).   Our method first renders a collection of images by using the 3DGS and edits them by using a pre-trained 2D diffusion model (ControlNet) based on the input prompt, which is then used to optimise the 3D model.   Our key contribution is multi-view consistent editing, which enables editing all images together instead of iteratively editing one image while updating the 3D model as in previous works.   It leads to faster editing as well as higher visual quality.   This is achieved by the two terms:   (a) depth-conditioned editing that enforces geometric consistency across multi-view images by leveraging naturally consistent depth maps.   (b) attention-based latent code alignment that unifies the appearance of edited images by conditioning their editing to several reference views through self and cross-view attention between images' latent representations.   Experiments demonstrate that our method achieves faster editing and better visual results than previous state-of-the-art methods.","sentences":["We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed by the 3D Gaussian Splatting (3DGS).   ","Our method first renders a collection of images by using the 3DGS and edits them by using a pre-trained 2D diffusion model (ControlNet) based on the input prompt, which is then used to optimise the 3D model.   ","Our key contribution is multi-view consistent editing, which enables editing all images together instead of iteratively editing one image while updating the 3D model as in previous works.   ","It leads to faster editing as well as higher visual quality.   ","This is achieved by the two terms:   (a) depth-conditioned editing that enforces geometric consistency across multi-view images by leveraging naturally consistent depth maps.   ","(b) attention-based latent code alignment that unifies the appearance of edited images by conditioning their editing to several reference views through self and cross-view attention between images' latent representations.   ","Experiments demonstrate that our method achieves faster editing and better visual results than previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.08733v2","category":"cs.CV"}
{"created":"2024-03-13 17:28:20","title":"Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models trained on Corrupted Data","abstract":"We provide a framework for solving inverse problems with diffusion models learned from linearly corrupted data. Our method, Ambient Diffusion Posterior Sampling (A-DPS), leverages a generative model pre-trained on one type of corruption (e.g. image inpainting) to perform posterior sampling conditioned on measurements from a potentially different forward process (e.g. image blurring). We test the efficacy of our approach on standard natural image datasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes outperform models trained on clean data for several image restoration tasks in both speed and performance. We further extend the Ambient Diffusion framework to train MRI models with access only to Fourier subsampled multi-coil MRI measurements at various acceleration factors (R=2, 4, 6, 8). We again observe that models trained on highly subsampled data are better priors for solving inverse problems in the high acceleration regime than models trained on fully sampled data. We open-source our code and the trained Ambient Diffusion MRI models: https://github.com/utcsilab/ambient-diffusion-mri .","sentences":["We provide a framework for solving inverse problems with diffusion models learned from linearly corrupted data.","Our method, Ambient Diffusion Posterior Sampling (A-DPS), leverages a generative model pre-trained on one type of corruption (e.g. image inpainting) to perform posterior sampling conditioned on measurements from a potentially different forward process (e.g. image blurring).","We test the efficacy of our approach on standard natural image datasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes outperform models trained on clean data for several image restoration tasks in both speed and performance.","We further extend the Ambient Diffusion framework to train MRI models with access only to Fourier subsampled multi-coil MRI measurements at various acceleration factors (R=2, 4, 6, 8).","We again observe that models trained on highly subsampled data are better priors for solving inverse problems in the high acceleration regime than models trained on fully sampled data.","We open-source our code and the trained Ambient Diffusion MRI models: https://github.com/utcsilab/ambient-diffusion-mri ."],"url":"http://arxiv.org/abs/2403.08728v1","category":"cs.CV"}
{"created":"2024-03-13 17:25:17","title":"Driving non-trivial quantum phases in conventional semiconductors with intense excitonic fields","abstract":"Inducing novel quantum phases and topologies in materials using intense light fields is a key objective of modern condensed matter physics, but nonetheless faces significant experimental challenges. Alternately, theory predicts that in the dense limit, excitons - collective excitations composed of Coulomb-bound electron-hole pairs - could also drive exotic quantum phenomena. However, the direct observation of these phenomena requires the resolution of electronic structure in momentum space in the presence of excitons, which became possible only recently. Here, using time- and angle-resolved photoemission spectroscopy of an atomically thin semiconductor in the presence of a high-density of resonantly and coherently photoexcited excitons, we observe the Bardeen-Cooper-Schrieffer (BCS) excitonic state - analogous to the Cooper pairs of superconductivity. We see the valence band transform from a conventional paraboloid into a Mexican-hat like Bogoliubov dispersion - a hallmark of the excitonic insulator phase; and we observe the recently predicted giant exciton-driven Floquet effects. Our work realizes the promise that intense bosonic fields, other than photons, can also drive novel quantum phenomena and phases in materials.","sentences":["Inducing novel quantum phases and topologies in materials using intense light fields is a key objective of modern condensed matter physics, but nonetheless faces significant experimental challenges.","Alternately, theory predicts that in the dense limit, excitons - collective excitations composed of Coulomb-bound electron-hole pairs - could also drive exotic quantum phenomena.","However, the direct observation of these phenomena requires the resolution of electronic structure in momentum space in the presence of excitons, which became possible only recently.","Here, using time- and angle-resolved photoemission spectroscopy of an atomically thin semiconductor in the presence of a high-density of resonantly and coherently photoexcited excitons, we observe the Bardeen-Cooper-Schrieffer (BCS) excitonic state - analogous to the Cooper pairs of superconductivity.","We see the valence band transform from a conventional paraboloid into a Mexican-hat like Bogoliubov dispersion - a hallmark of the excitonic insulator phase; and we observe the recently predicted giant exciton-driven Floquet effects.","Our work realizes the promise that intense bosonic fields, other than photons, can also drive novel quantum phenomena and phases in materials."],"url":"http://arxiv.org/abs/2403.08725v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-13 17:17:48","title":"SOTOPIA-$\u03c0$: Interactive Learning of Socially Intelligent Language Agents","abstract":"Humans learn social skills through both imitation and social interaction. This social learning process is largely understudied by existing research on building language agents. Motivated by this gap, we propose an interactive learning method, SOTOPIA-$\\pi$, improving the social intelligence of language agents. This method leverages behavior cloning and self-reinforcement training on filtered social interaction data according to large language model (LLM) ratings. We show that our training method allows a 7B LLM to reach the social goal completion ability of an expert model (GPT-4-based agent), while improving the safety of language agents and maintaining general QA ability on the MMLU benchmark. We also find that this training paradigm uncovers some difficulties in LLM-based evaluation of social intelligence: LLM-based evaluators overestimate the abilities of the language agents trained specifically for social interaction.","sentences":["Humans learn social skills through both imitation and social interaction.","This social learning process is largely understudied by existing research on building language agents.","Motivated by this gap, we propose an interactive learning method, SOTOPIA-$\\pi$, improving the social intelligence of language agents.","This method leverages behavior cloning and self-reinforcement training on filtered social interaction data according to large language model (LLM) ratings.","We show that our training method allows a 7B LLM to reach the social goal completion ability of an expert model (GPT-4-based agent), while improving the safety of language agents and maintaining general QA ability on the MMLU benchmark.","We also find that this training paradigm uncovers some difficulties in LLM-based evaluation of social intelligence: LLM-based evaluators overestimate the abilities of the language agents trained specifically for social interaction."],"url":"http://arxiv.org/abs/2403.08715v2","category":"cs.CL"}
{"created":"2024-03-13 17:15:06","title":"Boundary geometry controls topological defect transitions that determine lumen nucleation in embryonic development","abstract":"Topological defects determine the collective properties of anisotropic materials. How their configurations are controlled is not well understood however, especially in 3D, where bulk-surface coupling can render the geometry of confining boundaries relevant. This is particularly important in living matter, where 2D topological defects have been linked to essential biological functions, whereas the role of 3D defects is unclear. Motivated by multicellular systems interacting with extracellular boundaries, we consider a polar fluid confined within curved boundaries imposing weak surface anchoring. We report a novel charge-preserving transition between different defect configurations, controlled by the boundary shape, and invariant to changes in the material parameters. We test if this geometry-driven transition occurs in confined multicellular systems and investigate the biological role of 3D polar defects in the mouse epiblast -- an embryonic tissue consisting of apico-basally polarised cells. We find that fluid-filled lumina -- structures essential for subsequent embryonic development -- tend to form near defect positions of polar fluids in embryo-like confinement geometries. Moreover, by experimentally perturbing embryo shape beyond the transition point, we trigger the formation of additional lumen nucleation sites at the predicted position. Thus, our work reveals how boundary geometry controls polar defects, and how embryos use this mechanism for shape-dependent lumen formation. Because this defect control principle is independent of specific material properties, we expect it to apply universally to systems with orientational order.","sentences":["Topological defects determine the collective properties of anisotropic materials.","How their configurations are controlled is not well understood however, especially in 3D, where bulk-surface coupling can render the geometry of confining boundaries relevant.","This is particularly important in living matter, where 2D topological defects have been linked to essential biological functions, whereas the role of 3D defects is unclear.","Motivated by multicellular systems interacting with extracellular boundaries, we consider a polar fluid confined within curved boundaries imposing weak surface anchoring.","We report a novel charge-preserving transition between different defect configurations, controlled by the boundary shape, and invariant to changes in the material parameters.","We test if this geometry-driven transition occurs in confined multicellular systems and investigate the biological role of 3D polar defects in the mouse epiblast -- an embryonic tissue consisting of apico-basally polarised cells.","We find that fluid-filled lumina -- structures essential for subsequent embryonic development -- tend to form near defect positions of polar fluids in embryo-like confinement geometries.","Moreover, by experimentally perturbing embryo shape beyond the transition point, we trigger the formation of additional lumen nucleation sites at the predicted position.","Thus, our work reveals how boundary geometry controls polar defects, and how embryos use this mechanism for shape-dependent lumen formation.","Because this defect control principle is independent of specific material properties, we expect it to apply universally to systems with orientational order."],"url":"http://arxiv.org/abs/2403.08710v1","category":"cond-mat.soft"}
{"created":"2024-03-13 17:05:05","title":"Review of Generative AI Methods in Cybersecurity","abstract":"Large language models (LLMs) and generative artificial intelligence (GenAI) constitute paradigm shifts in cybersecurity that present hitherto unseen challenges as well as opportunities. In examining the state-of-the-art application of GenAI in cybersecurity, this work highlights how models like Google's Gemini and ChatGPT-4 potentially enhance security protocols, vulnerability assessment, and threat identification. Our research highlights the significance of a novel approach that employs LLMs to identify and eliminate sophisticated cyber threats. This paper presents a thorough assessment of LLMs' ability to produce important security insights, hence broadening the potential applications of AI-driven cybersecurity solutions. Our findings demonstrate the significance of GenAI in improving digital security. It offers recommendations for further investigations into the intricate relationship between cybersecurity requirements and artificial intelligence's potential.","sentences":["Large language models (LLMs) and generative artificial intelligence (GenAI) constitute paradigm shifts in cybersecurity that present hitherto unseen challenges as well as opportunities.","In examining the state-of-the-art application of GenAI in cybersecurity, this work highlights how models like Google's Gemini and ChatGPT-4 potentially enhance security protocols, vulnerability assessment, and threat identification.","Our research highlights the significance of a novel approach that employs LLMs to identify and eliminate sophisticated cyber threats.","This paper presents a thorough assessment of LLMs' ability to produce important security insights, hence broadening the potential applications of AI-driven cybersecurity solutions.","Our findings demonstrate the significance of GenAI in improving digital security.","It offers recommendations for further investigations into the intricate relationship between cybersecurity requirements and artificial intelligence's potential."],"url":"http://arxiv.org/abs/2403.08701v1","category":"cs.CR"}
{"created":"2024-03-13 17:02:27","title":"Implicit Regularization of Gradient Flow on One-Layer Softmax Attention","abstract":"We study gradient flow on the exponential loss for a classification problem with a one-layer softmax attention model, where the key and query weight matrices are trained separately. Under a separability assumption on the data, we show that when gradient flow achieves the minimal loss value, it further implicitly minimizes the nuclear norm of the product of the key and query weight matrices. Such implicit regularization can be described by a Support Vector Machine (SVM) problem with respect to the attention weights. This finding contrasts with prior results showing that the gradient descent induces an implicit regularization on the Frobenius norm on the product weight matrix when the key and query matrices are combined into a single weight matrix for training. For diagonal key and query matrices, our analysis builds upon the reparameterization technique and exploits approximate KKT conditions of the SVM associated with the classification data. Moreover, the results are extended to general weights configurations given proper alignment of the weight matrices' singular spaces with the data features at initialization.","sentences":["We study gradient flow on the exponential loss for a classification problem with a one-layer softmax attention model, where the key and query weight matrices are trained separately.","Under a separability assumption on the data, we show that when gradient flow achieves the minimal loss value, it further implicitly minimizes the nuclear norm of the product of the key and query weight matrices.","Such implicit regularization can be described by a Support Vector Machine (SVM) problem with respect to the attention weights.","This finding contrasts with prior results showing that the gradient descent induces an implicit regularization on the Frobenius norm on the product weight matrix when the key and query matrices are combined into a single weight matrix for training.","For diagonal key and query matrices, our analysis builds upon the reparameterization technique and exploits approximate KKT conditions of the SVM associated with the classification data.","Moreover, the results are extended to general weights configurations given proper alignment of the weight matrices' singular spaces with the data features at initialization."],"url":"http://arxiv.org/abs/2403.08699v1","category":"cs.LG"}
{"created":"2024-03-13 16:44:49","title":"Exploiting Structural Consistency of Chest Anatomy for Unsupervised Anomaly Detection in Radiography Images","abstract":"Radiography imaging protocols focus on particular body regions, therefore producing images of great similarity and yielding recurrent anatomical structures across patients. Exploiting this structured information could potentially ease the detection of anomalies from radiography images. To this end, we propose a Simple Space-Aware Memory Matrix for In-painting and Detecting anomalies from radiography images (abbreviated as SimSID). We formulate anomaly detection as an image reconstruction task, consisting of a space-aware memory matrix and an in-painting block in the feature space. During the training, SimSID can taxonomize the ingrained anatomical structures into recurrent visual patterns, and in the inference, it can identify anomalies (unseen/modified visual patterns) from the test image. Our SimSID surpasses the state of the arts in unsupervised anomaly detection by +8.0%, +5.0%, and +9.9% AUC scores on ZhangLab, COVIDx, and CheXpert benchmark datasets, respectively. Code: https://github.com/MrGiovanni/SimSID","sentences":["Radiography imaging protocols focus on particular body regions, therefore producing images of great similarity and yielding recurrent anatomical structures across patients.","Exploiting this structured information could potentially ease the detection of anomalies from radiography images.","To this end, we propose a Simple Space-Aware Memory Matrix for In-painting and Detecting anomalies from radiography images (abbreviated as SimSID).","We formulate anomaly detection as an image reconstruction task, consisting of a space-aware memory matrix and an in-painting block in the feature space.","During the training, SimSID can taxonomize the ingrained anatomical structures into recurrent visual patterns, and in the inference, it can identify anomalies (unseen/modified visual patterns) from the test image.","Our SimSID surpasses the state of the arts in unsupervised anomaly detection by +8.0%, +5.0%, and +9.9% AUC scores on ZhangLab, COVIDx, and CheXpert benchmark datasets, respectively.","Code: https://github.com/MrGiovanni/SimSID"],"url":"http://arxiv.org/abs/2403.08689v1","category":"eess.IV"}
{"created":"2024-03-13 16:44:39","title":"Token Alignment via Character Matching for Subword Completion","abstract":"Generative models, widely utilized in various applications, can often struggle with prompts corresponding to partial tokens. This struggle stems from tokenization, where partial tokens fall out of distribution during inference, leading to incorrect or nonsensical outputs. This paper examines a technique to alleviate the tokenization artifact on text completion in generative models, maintaining performance even in regular non-subword cases. The method, termed token alignment, involves backtracking to the last complete tokens and ensuring the model's generation aligns with the prompt. This approach showcases marked improvement across many partial token scenarios, including nuanced cases like space-prefix and partial indentation, with only a minor time increase. The technique and analysis detailed in this paper contribute to the continuous advancement of generative models in handling partial inputs, bearing relevance for applications like code completion and text autocompletion.","sentences":["Generative models, widely utilized in various applications, can often struggle with prompts corresponding to partial tokens.","This struggle stems from tokenization, where partial tokens fall out of distribution during inference, leading to incorrect or nonsensical outputs.","This paper examines a technique to alleviate the tokenization artifact on text completion in generative models, maintaining performance even in regular non-subword cases.","The method, termed token alignment, involves backtracking to the last complete tokens and ensuring the model's generation aligns with the prompt.","This approach showcases marked improvement across many partial token scenarios, including nuanced cases like space-prefix and partial indentation, with only a minor time increase.","The technique and analysis detailed in this paper contribute to the continuous advancement of generative models in handling partial inputs, bearing relevance for applications like code completion and text autocompletion."],"url":"http://arxiv.org/abs/2403.08688v1","category":"cs.CL"}
{"created":"2024-03-13 16:11:25","title":"A Distributed Adaptive Algorithm for Non-Smooth Spatial Filtering Problems in Wireless Sensor Networks","abstract":"A wireless sensor network often relies on a fusion center to process the data collected by each of its sensing nodes. Such an approach relies on the continuous transmission of raw data to the fusion center, which typically has a major impact on the sensors' battery life. To address this issue in the particular context of spatial filtering and signal fusion problems, we recently proposed the Distributed Adaptive Signal Fusion (DASF) algorithm, which distributively computes a spatial filter expressed as the solution of a smooth optimization problem involving the network-wide sensor signal statistics. In this work, we show that the DASF algorithm can be extended to compute the filters associated with a certain class of non-smooth optimization problems. This extension makes the addition of sparsity-inducing norms to the problem's cost function possible, allowing sensor selection to be performed in a distributed fashion, alongside the filtering task of interest, thereby further reducing the network's energy consumption. We provide a description of the algorithm, prove its convergence, and validate its performance and solution tracking capabilities with numerical experiments.","sentences":["A wireless sensor network often relies on a fusion center to process the data collected by each of its sensing nodes.","Such an approach relies on the continuous transmission of raw data to the fusion center, which typically has a major impact on the sensors' battery life.","To address this issue in the particular context of spatial filtering and signal fusion problems, we recently proposed the Distributed Adaptive Signal Fusion (DASF) algorithm, which distributively computes a spatial filter expressed as the solution of a smooth optimization problem involving the network-wide sensor signal statistics.","In this work, we show that the DASF algorithm can be extended to compute the filters associated with a certain class of non-smooth optimization problems.","This extension makes the addition of sparsity-inducing norms to the problem's cost function possible, allowing sensor selection to be performed in a distributed fashion, alongside the filtering task of interest, thereby further reducing the network's energy consumption.","We provide a description of the algorithm, prove its convergence, and validate its performance and solution tracking capabilities with numerical experiments."],"url":"http://arxiv.org/abs/2403.08658v1","category":"eess.SP"}
{"created":"2024-03-13 16:10:04","title":"Physical Memory Attacks and a Memory Safe Management System for Memory Defense","abstract":"Programming errors, defective hardware components (such as hard disk spindle defects), and environmental hazards can lead to invalid memory operations. In addition, less predictable forms of environmental stress, such as radiation, thermal influence, and energy fluctuations, can induce hardware faults. Sometimes, a soft error can occur instead of a complete failure, such as a bit-flip. The 'natural' factors that can cause bit-flips are replicable through targeted attacks that result in significant compromises, including full privileged system access. Existing physical defense solutions have consistently been circumvented shortly after deployment. We will explore the concept of a novel software-based low-level layer that can protect vulnerable memory targeted by physical attack vectors related to bit-flip vulnerabilities.","sentences":["Programming errors, defective hardware components (such as hard disk spindle defects), and environmental hazards can lead to invalid memory operations.","In addition, less predictable forms of environmental stress, such as radiation, thermal influence, and energy fluctuations, can induce hardware faults.","Sometimes, a soft error can occur instead of a complete failure, such as a bit-flip.","The 'natural' factors that can cause bit-flips are replicable through targeted attacks that result in significant compromises, including full privileged system access.","Existing physical defense solutions have consistently been circumvented shortly after deployment.","We will explore the concept of a novel software-based low-level layer that can protect vulnerable memory targeted by physical attack vectors related to bit-flip vulnerabilities."],"url":"http://arxiv.org/abs/2403.08656v1","category":"cs.CR"}
{"created":"2024-03-13 16:06:07","title":"HAIFIT: Human-Centered AI for Fashion Image Translation","abstract":"In the realm of fashion design, sketches serve as the canvas for expressing an artist's distinctive drawing style and creative vision, capturing intricate details like stroke variations and texture nuances. The advent of sketch-to-image cross-modal translation technology has notably aided designers. However, existing methods often compromise these sketch details during image generation, resulting in images that deviate from the designer's intended concept. This limitation hampers the ability to offer designers a precise preview of the final output. To overcome this challenge, we introduce HAIFIT, a novel approach that transforms sketches into high-fidelity, lifelike clothing images by integrating multi-scale features and capturing extensive feature map dependencies from diverse perspectives. Through extensive qualitative and quantitative evaluations conducted on our self-collected dataset, our method demonstrates superior performance compared to existing methods in generating photorealistic clothing images. Our method excels in preserving the distinctive style and intricate details essential for fashion design applications.","sentences":["In the realm of fashion design, sketches serve as the canvas for expressing an artist's distinctive drawing style and creative vision, capturing intricate details like stroke variations and texture nuances.","The advent of sketch-to-image cross-modal translation technology has notably aided designers.","However, existing methods often compromise these sketch details during image generation, resulting in images that deviate from the designer's intended concept.","This limitation hampers the ability to offer designers a precise preview of the final output.","To overcome this challenge, we introduce HAIFIT, a novel approach that transforms sketches into high-fidelity, lifelike clothing images by integrating multi-scale features and capturing extensive feature map dependencies from diverse perspectives.","Through extensive qualitative and quantitative evaluations conducted on our self-collected dataset, our method demonstrates superior performance compared to existing methods in generating photorealistic clothing images.","Our method excels in preserving the distinctive style and intricate details essential for fashion design applications."],"url":"http://arxiv.org/abs/2403.08651v1","category":"cs.CV"}
{"created":"2024-03-13 16:02:18","title":"Meta Reinforcement Learning for Resource Allocation in Aerial Active-RIS-assisted Networks with Rate-Splitting Multiple Access","abstract":"Mounting a reconfigurable intelligent surface (RIS) on an unmanned aerial vehicle (UAV) holds promise for improving traditional terrestrial network performance. Unlike conventional methods deploying passive RIS on UAVs, this study delves into the efficacy of an aerial active RIS (AARIS). Specifically, the downlink transmission of an AARIS network is investigated, where the base station (BS) leverages rate-splitting multiple access (RSMA) for effective interference management and benefits from the support of an AARIS for jointly amplifying and reflecting the BS's transmit signals. Considering both the non-trivial energy consumption of the active RIS and the limited energy storage of the UAV, we propose an innovative element selection strategy for optimizing the on/off status of RIS elements, which adaptively and remarkably manages the system's power consumption. To this end, a resource management problem is formulated, aiming to maximize the system energy efficiency (EE) by jointly optimizing the transmit beamforming at the BS, the element activation, the phase shift and the amplification factor at the RIS, the RSMA common data rate at users, as well as the UAV's trajectory. Due to the dynamicity nature of UAV and user mobility, a deep reinforcement learning (DRL) algorithm is designed for resource allocation, utilizing meta-learning to adaptively handle fast time-varying system dynamics. Simulations indicate that incorporating an active RIS at the UAV leads to substantial EE gain, compared to passive RIS-aided UAV. We observe the superiority of the RSMA-based AARIS system in terms of EE, compared to existing approaches adopting non-orthogonal multiple access (NOMA).","sentences":["Mounting a reconfigurable intelligent surface (RIS) on an unmanned aerial vehicle (UAV) holds promise for improving traditional terrestrial network performance.","Unlike conventional methods deploying passive RIS on UAVs, this study delves into the efficacy of an aerial active RIS (AARIS).","Specifically, the downlink transmission of an AARIS network is investigated, where the base station (BS) leverages rate-splitting multiple access (RSMA) for effective interference management and benefits from the support of an AARIS for jointly amplifying and reflecting the BS's transmit signals.","Considering both the non-trivial energy consumption of the active RIS and the limited energy storage of the UAV, we propose an innovative element selection strategy for optimizing the on/off status of RIS elements, which adaptively and remarkably manages the system's power consumption.","To this end, a resource management problem is formulated, aiming to maximize the system energy efficiency (EE) by jointly optimizing the transmit beamforming at the BS, the element activation, the phase shift and the amplification factor at the RIS, the RSMA common data rate at users, as well as the UAV's trajectory.","Due to the dynamicity nature of UAV and user mobility, a deep reinforcement learning (DRL) algorithm is designed for resource allocation, utilizing meta-learning to adaptively handle fast time-varying system dynamics.","Simulations indicate that incorporating an active RIS at the UAV leads to substantial EE gain, compared to passive RIS-aided UAV.","We observe the superiority of the RSMA-based AARIS system in terms of EE, compared to existing approaches adopting non-orthogonal multiple access (NOMA)."],"url":"http://arxiv.org/abs/2403.08648v1","category":"cs.IT"}
{"created":"2024-03-13 15:52:20","title":"Refractive COLMAP: Refractive Structure-from-Motion Revisited","abstract":"In this paper, we present a complete refractive Structure-from-Motion (RSfM) framework for underwater 3D reconstruction using refractive camera setups (for both, flat- and dome-port underwater housings). Despite notable achievements in refractive multi-view geometry over the past decade, a robust, complete and publicly available solution for such tasks is not available at present, and often practical applications have to resort to approximating refraction effects by the intrinsic (distortion) parameters of a pinhole camera model. To fill this gap, we have integrated refraction considerations throughout the entire SfM process within the state-of-the-art, open-source SfM framework COLMAP. Numerical simulations and reconstruction results on synthetically generated but photo-realistic images with ground truth validate that enabling refraction does not compromise accuracy or robustness as compared to in-air reconstructions. Finally, we demonstrate the capability of our approach for large-scale refractive scenarios using a dataset consisting of nearly 6000 images. The implementation is released as open-source at: https://cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.","sentences":["In this paper, we present a complete refractive Structure-from-Motion (RSfM) framework for underwater 3D reconstruction using refractive camera setups (for both, flat- and dome-port underwater housings).","Despite notable achievements in refractive multi-view geometry over the past decade, a robust, complete and publicly available solution for such tasks is not available at present, and often practical applications have to resort to approximating refraction effects by the intrinsic (distortion) parameters of a pinhole camera model.","To fill this gap, we have integrated refraction considerations throughout the entire SfM process within the state-of-the-art, open-source SfM framework COLMAP.","Numerical simulations and reconstruction results on synthetically generated but photo-realistic images with ground truth validate that enabling refraction does not compromise accuracy or robustness as compared to in-air reconstructions.","Finally, we demonstrate the capability of our approach for large-scale refractive scenarios using a dataset consisting of nearly 6000 images.","The implementation is released as open-source at: https://cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater."],"url":"http://arxiv.org/abs/2403.08640v1","category":"cs.CV"}
{"created":"2024-03-13 15:47:26","title":"Human Alignment of Large Language Models through Online Preference Optimisation","abstract":"Ensuring alignment of language models' outputs with human preferences is critical to guarantee a useful, safe, and pleasant user experience. Thus, human alignment has been extensively studied recently and several methods such as Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation (DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper, our contribution is two-fold. First, we show the equivalence between two recent alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD, that leverages the regularised sampling approach proposed by Nash-MD.   This equivalence may seem surprising at first sight, since IPO is an offline method whereas Nash-MD is an online method using a preference model. However, this equivalence can be proven when we consider the online version of IPO, that is when both generations are sampled by the online policy and annotated by a trained preference model. Optimising the IPO loss with such a stream of data becomes then equivalent to finding the Nash equilibrium of the preference model through self-play. Building on this equivalence, we introduce the IPO-MD algorithm that generates data with a mixture policy (between the online and reference policy) similarly as the general Nash-MD algorithm. We compare online-IPO and IPO-MD to different online versions of existing losses on preference data such as DPO and SLiC on a summarisation task.","sentences":["Ensuring alignment of language models' outputs with human preferences is critical to guarantee a useful, safe, and pleasant user experience.","Thus, human alignment has been extensively studied recently and several methods such as Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation (DPO) and Sequence Likelihood Calibration (SLiC) have emerged.","In this paper, our contribution is two-fold.","First, we show the equivalence between two recent alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror Descent (Nash-MD).","Second, we introduce a generalisation of IPO, named IPO-MD, that leverages the regularised sampling approach proposed by Nash-MD.   ","This equivalence may seem surprising at first sight, since IPO is an offline method whereas Nash-MD is an online method using a preference model.","However, this equivalence can be proven when we consider the online version of IPO, that is when both generations are sampled by the online policy and annotated by a trained preference model.","Optimising the IPO loss with such a stream of data becomes then equivalent to finding the Nash equilibrium of the preference model through self-play.","Building on this equivalence, we introduce the IPO-MD algorithm that generates data with a mixture policy (between the online and reference policy) similarly as the general Nash-MD algorithm.","We compare online-IPO and IPO-MD to different online versions of existing losses on preference data such as DPO and SLiC on a summarisation task."],"url":"http://arxiv.org/abs/2403.08635v1","category":"cs.LG"}
{"created":"2024-03-13 15:39:57","title":"Towards a Privacy and Security-Aware Framework for Ethical AI: Guiding the Development and Assessment of AI Systems","abstract":"As artificial intelligence continues its unprecedented global expansion, accompanied by a proliferation of benefits, an increasing apprehension about the privacy and security implications of AI-enabled systems emerges. The pivotal question of effectively controlling AI development at both jurisdictional and organizational levels has become a prominent theme in contemporary discourse. While the European Parliament and Council have taken a decisive step by reaching a political agreement on the EU AI Act, the first comprehensive AI law, organizations still find it challenging to adapt to the fast-evolving AI landscape, lacking a universal tool for evaluating the privacy and security dimensions of their AI models and systems. In response to this critical challenge, this study conducts a systematic literature review spanning the years 2020 to 2023, with a primary focus on establishing a unified definition of key concepts in AI Ethics, particularly emphasizing the domains of privacy and security. Through the synthesis of knowledge extracted from the SLR, this study presents a conceptual framework tailored for privacy- and security-aware AI systems. This framework is designed to assist diverse stakeholders, including organizations, academic institutions, and governmental bodies, in both the development and critical assessment of AI systems. Essentially, the proposed framework serves as a guide for ethical decision-making, fostering an environment wherein AI is developed and utilized with a strong commitment to ethical principles. In addition, the study unravels the key issues and challenges surrounding the privacy and security dimensions, delineating promising avenues for future research, thereby contributing to the ongoing dialogue on the globalization and democratization of AI ethics.","sentences":["As artificial intelligence continues its unprecedented global expansion, accompanied by a proliferation of benefits, an increasing apprehension about the privacy and security implications of AI-enabled systems emerges.","The pivotal question of effectively controlling AI development at both jurisdictional and organizational levels has become a prominent theme in contemporary discourse.","While the European Parliament and Council have taken a decisive step by reaching a political agreement on the EU AI Act, the first comprehensive AI law, organizations still find it challenging to adapt to the fast-evolving AI landscape, lacking a universal tool for evaluating the privacy and security dimensions of their AI models and systems.","In response to this critical challenge, this study conducts a systematic literature review spanning the years 2020 to 2023, with a primary focus on establishing a unified definition of key concepts in AI Ethics, particularly emphasizing the domains of privacy and security.","Through the synthesis of knowledge extracted from the SLR, this study presents a conceptual framework tailored for privacy- and security-aware AI systems.","This framework is designed to assist diverse stakeholders, including organizations, academic institutions, and governmental bodies, in both the development and critical assessment of AI systems.","Essentially, the proposed framework serves as a guide for ethical decision-making, fostering an environment wherein AI is developed and utilized with a strong commitment to ethical principles.","In addition, the study unravels the key issues and challenges surrounding the privacy and security dimensions, delineating promising avenues for future research, thereby contributing to the ongoing dialogue on the globalization and democratization of AI ethics."],"url":"http://arxiv.org/abs/2403.08624v1","category":"cs.CY"}
{"created":"2024-03-13 15:32:26","title":"Measurements of the charge ratio and polarization of cosmic-ray muons with the Super-Kamiokande detector","abstract":"We present the results of the charge ratio ($R$) and polarization ($P^{\\mu}_{0}$) measurements using the decay electron events collected from 2008 September to 2022 June by the Super-Kamiokande detector. Because of its underground location and long operation, we performed high precision measurements by accumulating cosmic-ray muons. We measured the muon charge ratio to be $R=1.32 \\pm 0.02$ $(\\mathrm{stat.}{+}\\mathrm{syst.})$ at $E_{\\mu}\\cos \\theta_{\\mathrm{Zenith}}=0.7^{+0.3}_{-0.2}$ $\\mathrm{TeV}$, where $E_{\\mu}$ is the muon energy and $\\theta_{\\mathrm{Zenith}}$ is the zenith angle of incoming cosmic-ray muons. This result is consistent with the Honda flux model while this suggests a tension with the $\\pi K$ model of $1.9\\sigma$. We also measured the muon polarization at the production location to be $P^{\\mu}_{0}=0.52 \\pm 0.02$ $(\\mathrm{stat.}{+}\\mathrm{syst.})$ at the muon momentum of $0.9^{+0.6}_{-0.1}$ $\\mathrm{TeV}/c$ at the surface of the mountain; this also suggests a tension with the Honda flux model of $1.5\\sigma$. This is the most precise measurement ever to experimentally determine the cosmic-ray muon polarization near $1~\\mathrm{TeV}/c$. These measurement results are useful to improve the atmospheric neutrino simulations.","sentences":["We present the results of the charge ratio ($R$) and polarization ($P^{\\mu}_{0}$) measurements using the decay electron events collected from 2008 September to 2022 June by the Super-Kamiokande detector.","Because of its underground location and long operation, we performed high precision measurements by accumulating cosmic-ray muons.","We measured the muon charge ratio to be $R=1.32 \\pm 0.02$ $(\\mathrm{stat.}{+}\\mathrm{syst.})$ at $E_{\\mu}\\cos \\theta_{\\mathrm{Zenith}}=0.7^{+0.3}_{-0.2}$ $\\mathrm{TeV}$, where $E_{\\mu}$ is the muon energy and $\\theta_{\\mathrm{Zenith}}$ is the zenith angle of incoming cosmic-ray muons.","This result is consistent with the Honda flux model while this suggests a tension with the $\\pi K$ model of $1.9\\sigma$. We also measured the muon polarization at the production location to be $P^{\\mu}_{0}=0.52 \\pm 0.02$ $(\\mathrm{stat.}{+}\\mathrm{syst.})$ at the muon momentum of $0.9^{+0.6}_{-0.1}$ $\\mathrm{TeV}/c$ at the surface of the mountain; this also suggests a tension with the Honda flux model of $1.5\\sigma$. This is the most precise measurement ever to experimentally determine the cosmic-ray muon polarization near $1~\\mathrm{TeV}/c$. These measurement results are useful to improve the atmospheric neutrino simulations."],"url":"http://arxiv.org/abs/2403.08619v1","category":"hep-ex"}
{"created":"2024-03-13 15:32:08","title":"Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples","abstract":"Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models. This corruption often arises from non-expert labeling or adversarial attacks. Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive. To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining. We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update. Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data. We demonstrate Verifix's effectiveness on both synthetic and real-world label noise. Experiments on the CIFAR dataset with 25% synthetic corruption show 7.36% generalization improvements on average. Additionally, we observe generalization improvements of up to 2.63% on naturally corrupted datasets like WebVision1.0 and Clothing1M.","sentences":["Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models.","This corruption often arises from non-expert labeling or adversarial attacks.","Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive.","To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining.","We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update.","Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data.","We demonstrate Verifix's effectiveness on both synthetic and real-world label noise.","Experiments on the CIFAR dataset with 25% synthetic corruption show 7.36% generalization improvements on average.","Additionally, we observe generalization improvements of up to 2.63% on naturally corrupted datasets like WebVision1.0 and Clothing1M."],"url":"http://arxiv.org/abs/2403.08618v1","category":"cs.LG"}
{"created":"2024-03-13 15:23:55","title":"Link Prediction for Social Networks using Representation Learning and Heuristic-based Features","abstract":"The exponential growth in scale and relevance of social networks enable them to provide expansive insights. Predicting missing links in social networks efficiently can help in various modern-day business applications ranging from generating recommendations to influence analysis. Several categories of solutions exist for the same. Here, we explore various feature extraction techniques to generate representations of nodes and edges in a social network that allow us to predict missing links. We compare the results of using ten feature extraction techniques categorized across Structural embeddings, Neighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics, followed by modeling with ensemble classifiers and custom Neural Networks. Further, we propose combining heuristic-based features and learned representations that demonstrate improved performance for the link prediction task on social network datasets. Using this method to generate accurate recommendations for many applications is a matter of further study that appears very promising. The code for all the experiments has been made public.","sentences":["The exponential growth in scale and relevance of social networks enable them to provide expansive insights.","Predicting missing links in social networks efficiently can help in various modern-day business applications ranging from generating recommendations to influence analysis.","Several categories of solutions exist for the same.","Here, we explore various feature extraction techniques to generate representations of nodes and edges in a social network that allow us to predict missing links.","We compare the results of using ten feature extraction techniques categorized across Structural embeddings, Neighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics, followed by modeling with ensemble classifiers and custom Neural Networks.","Further, we propose combining heuristic-based features and learned representations that demonstrate improved performance for the link prediction task on social network datasets.","Using this method to generate accurate recommendations for many applications is a matter of further study that appears very promising.","The code for all the experiments has been made public."],"url":"http://arxiv.org/abs/2403.08613v1","category":"cs.SI"}
{"created":"2024-03-13 15:20:30","title":"MedInsight: A Multi-Source Context Augmentation Framework for Generating Patient-Centric Medical Responses using Large Language Models","abstract":"Large Language Models (LLMs) have shown impressive capabilities in generating human-like responses. However, their lack of domain-specific knowledge limits their applicability in healthcare settings, where contextual and comprehensive responses are vital. To address this challenge and enable the generation of patient-centric responses that are contextually relevant and comprehensive, we propose MedInsight:a novel retrieval augmented framework that augments LLM inputs (prompts) with relevant background information from multiple sources. MedInsight extracts pertinent details from the patient's medical record or consultation transcript. It then integrates information from authoritative medical textbooks and curated web resources based on the patient's health history and condition. By constructing an augmented context combining the patient's record with relevant medical knowledge, MedInsight generates enriched, patient-specific responses tailored for healthcare applications such as diagnosis, treatment recommendations, or patient education. Experiments on the MTSamples dataset validate MedInsight's effectiveness in generating contextually appropriate medical responses. Quantitative evaluation using the Ragas metric and TruLens for answer similarity and answer correctness demonstrates the model's efficacy. Furthermore, human evaluation studies involving Subject Matter Expert (SMEs) confirm MedInsight's utility, with moderate inter-rater agreement on the relevance and correctness of the generated responses.","sentences":["Large Language Models (LLMs) have shown impressive capabilities in generating human-like responses.","However, their lack of domain-specific knowledge limits their applicability in healthcare settings, where contextual and comprehensive responses are vital.","To address this challenge and enable the generation of patient-centric responses that are contextually relevant and comprehensive, we propose MedInsight:a novel retrieval augmented framework that augments LLM inputs (prompts) with relevant background information from multiple sources.","MedInsight extracts pertinent details from the patient's medical record or consultation transcript.","It then integrates information from authoritative medical textbooks and curated web resources based on the patient's health history and condition.","By constructing an augmented context combining the patient's record with relevant medical knowledge, MedInsight generates enriched, patient-specific responses tailored for healthcare applications such as diagnosis, treatment recommendations, or patient education.","Experiments on the MTSamples dataset validate MedInsight's effectiveness in generating contextually appropriate medical responses.","Quantitative evaluation using the Ragas metric and TruLens for answer similarity and answer correctness demonstrates the model's efficacy.","Furthermore, human evaluation studies involving Subject Matter Expert (SMEs) confirm MedInsight's utility, with moderate inter-rater agreement on the relevance and correctness of the generated responses."],"url":"http://arxiv.org/abs/2403.08607v1","category":"cs.CL"}
{"created":"2024-03-13 15:13:44","title":"DevBench: A Comprehensive Benchmark for Software Development","abstract":"Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. To this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing. DevBench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts. Our findings offer actionable insights for the future development of LLMs toward real-world programming applications. Our benchmark is available at https://github.com/open-compass/DevBench","sentences":["Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities.","However, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities.","To this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing.","DevBench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task.","Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench.","Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts.","Our findings offer actionable insights for the future development of LLMs toward real-world programming applications.","Our benchmark is available at https://github.com/open-compass/DevBench"],"url":"http://arxiv.org/abs/2403.08604v1","category":"cs.CL"}
{"created":"2024-03-13 15:10:06","title":"Exploring global symmetry-breaking superradiant phase via phase competition","abstract":"Superradiant phase transitions play a fundamental role in understanding the mechanism of collective light-matter interaction at the quantum level. Here we investigate multiple superradiant phases and phase transitions with different symmetry-breaking patterns in a two-mode V-type Dicke model. Interestingly, we show that there exists a quadruple point where one normal phase, one global symmetry-breaking superradiant phase and two local symmetry-breaking superradiant phases meet. Such a global phase results from the phase competition between two local superradiant phases and can not occur in the standard $\\Lambda$- and $\\Xi$-type three-level configurations in quantum optics. Moreover, we exhibit a sequential first-order quantum phase transition from one local to the global again to the other local superradiant phase. Our study opens up a perspective of exploring multi-level quantum critical phenomena with global symmetry breaking.","sentences":["Superradiant phase transitions play a fundamental role in understanding the mechanism of collective light-matter interaction at the quantum level.","Here we investigate multiple superradiant phases and phase transitions with different symmetry-breaking patterns in a two-mode V-type Dicke model.","Interestingly, we show that there exists a quadruple point where one normal phase, one global symmetry-breaking superradiant phase and two local symmetry-breaking superradiant phases meet.","Such a global phase results from the phase competition between two local superradiant phases and can not occur in the standard $\\Lambda$- and $\\Xi$-type three-level configurations in quantum optics.","Moreover, we exhibit a sequential first-order quantum phase transition from one local to the global again to the other local superradiant phase.","Our study opens up a perspective of exploring multi-level quantum critical phenomena with global symmetry breaking."],"url":"http://arxiv.org/abs/2403.08602v1","category":"quant-ph"}
{"created":"2024-03-13 14:59:07","title":"Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments","abstract":"Large Language Models (LLMs) have shown potential in reasoning over structured environments, e.g., knowledge graph and table. Such tasks typically require multi-hop reasoning, i.e., match natural language utterance with instances in the environment. Previous methods leverage LLMs to incrementally build a reasoning path, where the LLMs either invoke tools or pick up schemas by step-by-step interacting with the environment. We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments. In Readi, LLMs initially generate a reasoning path given a query, and edit the path only when necessary. We instantiate the path on structured environments and provide feedback to edit the path if anything goes wrong. Experimental results on three KGQA datasets and two TableQA datasets show the effectiveness of Readi, significantly surpassing all LLM-based methods (by 9.1% on WebQSP, 12.4% on MQA-3H and 10.9% on WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and 74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ). Our code will be available upon publication.","sentences":["Large Language Models (LLMs) have shown potential in reasoning over structured environments, e.g., knowledge graph and table.","Such tasks typically require multi-hop reasoning, i.e., match natural language utterance with instances in the environment.","Previous methods leverage LLMs to incrementally build a reasoning path, where the LLMs either invoke tools or pick up schemas by step-by-step interacting with the environment.","We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments.","In Readi, LLMs initially generate a reasoning path given a query, and edit the path only when necessary.","We instantiate the path on structured environments and provide feedback to edit the path if anything goes wrong.","Experimental results on three KGQA datasets and two TableQA datasets show the effectiveness of Readi, significantly surpassing all LLM-based methods (by 9.1% on WebQSP, 12.4% on MQA-3H and 10.9% on WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and 74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ).","Our code will be available upon publication."],"url":"http://arxiv.org/abs/2403.08593v1","category":"cs.CL"}
{"created":"2024-03-13 14:52:09","title":"Long-term monitoring of large-scale magnetic fields across optical and near-infrared domains with ESPaDOnS, Narval and SPIRou. The cases of EV Lac, DS Leo, and CN Leo","abstract":"Dynamo models of stellar magnetic fields for partly and fully convective stars are guided by observational constraints. Zeeman-Doppler imaging has revealed a variety of magnetic field geometries and, for fully convective stars in particular, a dichotomy: either strong, mostly axisymmetric, and dipole-dominated or weak, non-axisymmetric, and multipole-dominated. This dichotomy is explained by dynamo bistability or by long-term magnetic cycles, but there is no definite conclusion on the matter. We analysed optical spectropolarimetric data sets collected with ESPaDOnS and Narval between 2005 and 2016, and near-infrared SPIRou data obtained between 2019 and 2022 for three active M dwarfs with masses between 0.1 and 0.6 MSun: EV Lac, DS Leo, and CN Leo. We looked for changes in time series of longitudinal magnetic field, width of unpolarised mean-line profiles, and large-scale field topology as retrieved with principal component analysis and Zeeman-Doppler imaging. We retrieved pulsating (EV Lac), stable (DS Leo), and sine-like (CN Leo) long-term trends in longitudinal field. The width of near-infrared mean-line profiles exhibits rotational modulation only for DS Leo, whereas in the optical it is evident for both EV Lac and DS Leo. The line width variations are not necessarily correlated to those of the longitudinal field, suggesting complex relations between small- and large-scale field. We also recorded topological changes: a reduced axisymmetry for EV Lac and a transition from toroidal- to poloidal-dominated regime for DS Leo. For CN Leo, the topology remained dipolar and axisymmetric, with only an oscillation in field strength. Our results show a peculiar evolution of the magnetic field for each M dwarf, confirming that M dwarfs with distinct masses and rotation periods can undergo magnetic long-term variations, and suggesting a variety of cyclic behaviours of their magnetic fields.","sentences":["Dynamo models of stellar magnetic fields for partly and fully convective stars are guided by observational constraints.","Zeeman-Doppler imaging has revealed a variety of magnetic field geometries and, for fully convective stars in particular, a dichotomy: either strong, mostly axisymmetric, and dipole-dominated or weak, non-axisymmetric, and multipole-dominated.","This dichotomy is explained by dynamo bistability or by long-term magnetic cycles, but there is no definite conclusion on the matter.","We analysed optical spectropolarimetric data sets collected with ESPaDOnS and Narval between 2005 and 2016, and near-infrared SPIRou data obtained between 2019 and 2022 for three active M dwarfs with masses between 0.1 and 0.6 MSun:","EV Lac, DS Leo, and CN Leo.","We looked for changes in time series of longitudinal magnetic field, width of unpolarised mean-line profiles, and large-scale field topology as retrieved with principal component analysis and Zeeman-Doppler imaging.","We retrieved pulsating (EV Lac), stable (DS Leo), and sine-like (CN Leo) long-term trends in longitudinal field.","The width of near-infrared mean-line profiles exhibits rotational modulation only for DS Leo, whereas in the optical it is evident for both EV Lac and DS Leo.","The line width variations are not necessarily correlated to those of the longitudinal field, suggesting complex relations between small- and large-scale field.","We also recorded topological changes: a reduced axisymmetry for EV Lac and a transition from toroidal- to poloidal-dominated regime for DS Leo.","For CN Leo, the topology remained dipolar and axisymmetric, with only an oscillation in field strength.","Our results show a peculiar evolution of the magnetic field for each M dwarf, confirming that M dwarfs with distinct masses and rotation periods can undergo magnetic long-term variations, and suggesting a variety of cyclic behaviours of their magnetic fields."],"url":"http://arxiv.org/abs/2403.08590v1","category":"astro-ph.SR"}
{"created":"2024-03-13 14:42:06","title":"Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems","abstract":"Stochastic gradient descent (SGD) exhibits strong algorithmic regularization effects in practice and plays an important role in the generalization of modern machine learning. However, prior research has revealed instances where the generalization performance of SGD is worse than ridge regression due to uneven optimization along different dimensions. Preconditioning offers a natural solution to this issue by rebalancing optimization across different directions. Yet, the extent to which preconditioning can enhance the generalization performance of SGD and whether it can bridge the existing gap with ridge regression remains uncertain. In this paper, we study the generalization performance of SGD with preconditioning for the least squared problem. We make a comprehensive comparison between preconditioned SGD and (standard \\& preconditioned) ridge regression. Our study makes several key contributions toward understanding and improving SGD with preconditioning. First, we establish excess risk bounds (generalization performance) for preconditioned SGD and ridge regression under an arbitrary preconditions matrix. Second, leveraging the excessive risk characterization of preconditioned SGD and ridge regression, we show that (through construction) there exists a simple preconditioned matrix that can outperform (standard \\& preconditioned) ridge regression. Finally, we show that our proposed preconditioning matrix is straightforward enough to allow robust estimation from finite samples while maintaining a theoretical advantage over ridge regression. Our empirical results align with our theoretical findings, collectively showcasing the enhanced regularization effect of preconditioned SGD.","sentences":["Stochastic gradient descent (SGD) exhibits strong algorithmic regularization effects in practice and plays an important role in the generalization of modern machine learning.","However, prior research has revealed instances where the generalization performance of SGD is worse than ridge regression due to uneven optimization along different dimensions.","Preconditioning offers a natural solution to this issue by rebalancing optimization across different directions.","Yet, the extent to which preconditioning can enhance the generalization performance of SGD and whether it can bridge the existing gap with ridge regression remains uncertain.","In this paper, we study the generalization performance of SGD with preconditioning for the least squared problem.","We make a comprehensive comparison between preconditioned SGD and (standard \\& preconditioned) ridge regression.","Our study makes several key contributions toward understanding and improving SGD with preconditioning.","First, we establish excess risk bounds (generalization performance) for preconditioned SGD and ridge regression under an arbitrary preconditions matrix.","Second, leveraging the excessive risk characterization of preconditioned SGD and ridge regression, we show that (through construction) there exists a simple preconditioned matrix that can outperform (standard \\& preconditioned) ridge regression.","Finally, we show that our proposed preconditioning matrix is straightforward enough to allow robust estimation from finite samples while maintaining a theoretical advantage over ridge regression.","Our empirical results align with our theoretical findings, collectively showcasing the enhanced regularization effect of preconditioned SGD."],"url":"http://arxiv.org/abs/2403.08585v1","category":"cs.LG"}
{"created":"2024-03-13 14:19:08","title":"Non-discrimination Criteria for Generative Language Models","abstract":"Within recent years, generative AI, such as large language models, has undergone rapid development. As these models become increasingly available to the public, concerns arise about perpetuating and amplifying harmful biases in applications. Gender stereotypes can be harmful and limiting for the individuals they target, whether they consist of misrepresentation or discrimination. Recognizing gender bias as a pervasive societal construct, this paper studies how to uncover and quantify the presence of gender biases in generative language models. In particular, we derive generative AI analogues of three well-known non-discrimination criteria from classification, namely independence, separation and sufficiency. To demonstrate these criteria in action, we design prompts for each of the criteria with a focus on occupational gender stereotype, specifically utilizing the medical test to introduce the ground truth in the generative AI context. Our results address the presence of occupational gender bias within such conversational language models.","sentences":["Within recent years, generative AI, such as large language models, has undergone rapid development.","As these models become increasingly available to the public, concerns arise about perpetuating and amplifying harmful biases in applications.","Gender stereotypes can be harmful and limiting for the individuals they target, whether they consist of misrepresentation or discrimination.","Recognizing gender bias as a pervasive societal construct, this paper studies how to uncover and quantify the presence of gender biases in generative language models.","In particular, we derive generative AI analogues of three well-known non-discrimination criteria from classification, namely independence, separation and sufficiency.","To demonstrate these criteria in action, we design prompts for each of the criteria with a focus on occupational gender stereotype, specifically utilizing the medical test to introduce the ground truth in the generative AI context.","Our results address the presence of occupational gender bias within such conversational language models."],"url":"http://arxiv.org/abs/2403.08564v1","category":"cs.CL"}
{"created":"2024-03-13 14:17:34","title":"Distributed Deep Learning for Modulation Classification in 6G Cell-Free Wireless Networks","abstract":"In the evolution of 6th Generation (6G) technology, the emergence of cell-free networking presents a paradigm shift, revolutionizing user experiences within densely deployed networks where distributed access points collaborate. However, the integration of intelligent mechanisms is crucial for optimizing the efficiency, scalability, and adaptability of these 6G cell-free networks. One application aiming to optimize spectrum usage is Automatic Modulation Classification (AMC), a vital component for classifying and dynamically adjusting modulation schemes. This paper explores different distributed solutions for AMC in cell-free networks, addressing the training, computational complexity, and accuracy of two practical approaches. The first approach addresses scenarios where signal sharing is not feasible due to privacy concerns or fronthaul limitations. Our findings reveal that maintaining comparable accuracy is remarkably achievable, yet it comes with an increase in computational demand. The second approach considers a central model and multiple distributed models collaboratively classifying the modulation. This hybrid model leverages diversity gain through signal combining and requires synchronization and signal sharing. The hybrid model demonstrates superior performance, achieving a 2.5% improvement in accuracy with equivalent total computational load. Notably, the hybrid model distributes the computational load across multiple devices, resulting in a lower individual computational load.","sentences":["In the evolution of 6th Generation (6G) technology, the emergence of cell-free networking presents a paradigm shift, revolutionizing user experiences within densely deployed networks where distributed access points collaborate.","However, the integration of intelligent mechanisms is crucial for optimizing the efficiency, scalability, and adaptability of these 6G cell-free networks.","One application aiming to optimize spectrum usage is Automatic Modulation Classification (AMC), a vital component for classifying and dynamically adjusting modulation schemes.","This paper explores different distributed solutions for AMC in cell-free networks, addressing the training, computational complexity, and accuracy of two practical approaches.","The first approach addresses scenarios where signal sharing is not feasible due to privacy concerns or fronthaul limitations.","Our findings reveal that maintaining comparable accuracy is remarkably achievable, yet it comes with an increase in computational demand.","The second approach considers a central model and multiple distributed models collaboratively classifying the modulation.","This hybrid model leverages diversity gain through signal combining and requires synchronization and signal sharing.","The hybrid model demonstrates superior performance, achieving a 2.5% improvement in accuracy with equivalent total computational load.","Notably, the hybrid model distributes the computational load across multiple devices, resulting in a lower individual computational load."],"url":"http://arxiv.org/abs/2403.08563v1","category":"eess.SP"}
{"created":"2024-03-13 14:14:47","title":"Structural perspective on constraint-based learning of Markov networks","abstract":"Markov networks are probabilistic graphical models that employ undirected graphs to depict conditional independence relationships among variables. Our focus lies in constraint-based structure learning, which entails learning the undirected graph from data through the execution of conditional independence tests. We establish theoretical limits concerning two critical aspects of constraint-based learning of Markov networks: the number of tests and the sizes of the conditioning sets. These bounds uncover an exciting interplay between the structural properties of the graph and the amount of tests required to learn a Markov network. The starting point of our work is that the graph parameter maximum pairwise connectivity, $\\kappa$, that is, the maximum number of vertex-disjoint paths connecting a pair of vertices in the graph, is responsible for the sizes of independence tests required to learn the graph. On one hand, we show that at least one test with the size of the conditioning set at least $\\kappa$ is always necessary. On the other hand, we prove that any graph can be learned by performing tests of size at most $\\kappa$. This completely resolves the question of the minimum size of conditioning sets required to learn the graph. When it comes to the number of tests, our upper bound on the sizes of conditioning sets implies that every $n$-vertex graph can be learned by at most $n^{\\kappa}$ tests with conditioning sets of sizes at most $\\kappa$. We show that for any upper bound $q$ on the sizes of the conditioning sets, there exist graphs with $O(n q)$ vertices that require at least $n^{\\Omega(\\kappa)}$ tests to learn. This lower bound holds even when the treewidth and the maximum degree of the graph are at most $\\kappa+2$. On the positive side, we prove that every graph of bounded treewidth can be learned by a polynomial number of tests with conditioning sets of sizes at most $2\\kappa$.","sentences":["Markov networks are probabilistic graphical models that employ undirected graphs to depict conditional independence relationships among variables.","Our focus lies in constraint-based structure learning, which entails learning the undirected graph from data through the execution of conditional independence tests.","We establish theoretical limits concerning two critical aspects of constraint-based learning of Markov networks: the number of tests and the sizes of the conditioning sets.","These bounds uncover an exciting interplay between the structural properties of the graph and the amount of tests required to learn a Markov network.","The starting point of our work is that the graph parameter maximum pairwise connectivity, $\\kappa$, that is, the maximum number of vertex-disjoint paths connecting a pair of vertices in the graph, is responsible for the sizes of independence tests required to learn the graph.","On one hand, we show that at least one test with the size of the conditioning set at least $\\kappa$ is always necessary.","On the other hand, we prove that any graph can be learned by performing tests of size at most $\\kappa$. This completely resolves the question of the minimum size of conditioning sets required to learn the graph.","When it comes to the number of tests, our upper bound on the sizes of conditioning sets implies that every $n$-vertex graph can be learned by at most $n^{\\kappa}$ tests with conditioning sets of sizes at most $\\kappa$. We show that for any upper bound $q$ on the sizes of the conditioning sets, there exist graphs with $O(n q)$ vertices that require at least $n^{\\Omega(\\kappa)}$ tests to learn.","This lower bound holds even when the treewidth and the maximum degree of the graph are at most $\\kappa+2$. On the positive side, we prove that every graph of bounded treewidth can be learned by a polynomial number of tests with conditioning sets of sizes at most $2\\kappa$."],"url":"http://arxiv.org/abs/2403.08562v1","category":"cs.LG"}
{"created":"2024-03-13 14:10:10","title":"End-to-End Amp Modeling: From Data to Controllable Guitar Amplifier Models","abstract":"This paper describes a data-driven approach to creating real-time neural network models of guitar amplifiers, recreating the amplifiers' sonic response to arbitrary inputs at the full range of controls present on the physical device. While the focus on the paper is on the data collection pipeline, we demonstrate the effectiveness of this conditioned black-box approach by training an LSTM model to the task, and comparing its performance to an offline white-box SPICE circuit simulation. Our listening test results demonstrate that the neural amplifier modeling approach can match the subjective performance of a high-quality SPICE model, all while using an automated, non-intrusive data collection process, and an end-to-end trainable, real-time feasible neural network model.","sentences":["This paper describes a data-driven approach to creating real-time neural network models of guitar amplifiers, recreating the amplifiers' sonic response to arbitrary inputs at the full range of controls present on the physical device.","While the focus on the paper is on the data collection pipeline, we demonstrate the effectiveness of this conditioned black-box approach by training an LSTM model to the task, and comparing its performance to an offline white-box SPICE circuit simulation.","Our listening test results demonstrate that the neural amplifier modeling approach can match the subjective performance of a high-quality SPICE model, all while using an automated, non-intrusive data collection process, and an end-to-end trainable, real-time feasible neural network model."],"url":"http://arxiv.org/abs/2403.08559v1","category":"cs.SD"}
{"created":"2024-03-13 14:08:25","title":"SM4Depth: Seamless Monocular Metric Depth Estimation across Multiple Cameras and Scenes by One Model","abstract":"The generalization of monocular metric depth estimation (MMDE) has been a longstanding challenge. Recent methods made progress by combining relative and metric depth or aligning input image focal length. However, they are still beset by challenges in camera, scene, and data levels: (1) Sensitivity to different cameras; (2) Inconsistent accuracy across scenes; (3) Reliance on massive training data. This paper proposes SM4Depth, a seamless MMDE method, to address all the issues above within a single network. First, we reveal that a consistent field of view (FOV) is the key to resolve ``metric ambiguity'' across cameras, which guides us to propose a more straightforward preprocessing unit. Second, to achieve consistently high accuracy across scenes, we explicitly model the metric scale determination as discretizing the depth interval into bins and propose variation-based unnormalized depth bins. This method bridges the depth gap of diverse scenes by reducing the ambiguity of the conventional metric bin. Third, to reduce the reliance on massive training data, we propose a ``divide and conquer\" solution. Instead of estimating directly from the vast solution space, the correct metric bins are estimated from multiple solution sub-spaces for complexity reduction. Finally, with just 150K RGB-D pairs and a consumer-grade GPU for training, SM4Depth achieves state-of-the-art performance on most previously unseen datasets, especially surpassing ZoeDepth and Metric3D on mRI$_\\theta$. The code can be found at https://github.com/1hao-Liu/SM4Depth.","sentences":["The generalization of monocular metric depth estimation (MMDE) has been a longstanding challenge.","Recent methods made progress by combining relative and metric depth or aligning input image focal length.","However, they are still beset by challenges in camera, scene, and data levels: (1) Sensitivity to different cameras; (2) Inconsistent accuracy across scenes; (3) Reliance on massive training data.","This paper proposes SM4Depth, a seamless MMDE method, to address all the issues above within a single network.","First, we reveal that a consistent field of view (FOV) is the key to resolve ``metric ambiguity'' across cameras, which guides us to propose a more straightforward preprocessing unit.","Second, to achieve consistently high accuracy across scenes, we explicitly model the metric scale determination as discretizing the depth interval into bins and propose variation-based unnormalized depth bins.","This method bridges the depth gap of diverse scenes by reducing the ambiguity of the conventional metric bin.","Third, to reduce the reliance on massive training data, we propose a ``divide and conquer\" solution.","Instead of estimating directly from the vast solution space, the correct metric bins are estimated from multiple solution sub-spaces for complexity reduction.","Finally, with just 150K RGB-D pairs and a consumer-grade GPU for training, SM4Depth achieves state-of-the-art performance on most previously unseen datasets, especially surpassing ZoeDepth and Metric3D on mRI$_\\theta$. The code can be found at https://github.com/1hao-Liu/SM4Depth."],"url":"http://arxiv.org/abs/2403.08556v1","category":"cs.CV"}
{"created":"2024-03-13 14:06:51","title":"Federated Knowledge Graph Unlearning via Diffusion Model","abstract":"Federated learning (FL) promotes the development and application of artificial intelligence technologies by enabling model sharing and collaboration while safeguarding data privacy. Knowledge graph (KG) embedding representation provides a foundation for knowledge reasoning and applications by mapping entities and relations into vector space. Federated KG embedding enables the utilization of knowledge from diverse client sources while safeguarding the privacy of local data. However, due to demands such as privacy protection and the need to adapt to dynamic data changes, investigations into machine unlearning (MU) have been sparked. However, it is challenging to maintain the performance of KG embedding models while forgetting the influence of specific forgotten data on the model. In this paper, we propose FedDM, a novel framework tailored for machine unlearning in federated knowledge graphs. Leveraging diffusion models, we generate noisy data to sensibly mitigate the influence of specific knowledge on FL models while preserving the overall performance concerning the remaining data. We conduct experimental evaluations on benchmark datasets to assess the efficacy of the proposed model. Extensive experiments demonstrate that FedDM yields promising results in knowledge forgetting.","sentences":["Federated learning (FL) promotes the development and application of artificial intelligence technologies by enabling model sharing and collaboration while safeguarding data privacy.","Knowledge graph (KG) embedding representation provides a foundation for knowledge reasoning and applications by mapping entities and relations into vector space.","Federated KG embedding enables the utilization of knowledge from diverse client sources while safeguarding the privacy of local data.","However, due to demands such as privacy protection and the need to adapt to dynamic data changes, investigations into machine unlearning (MU) have been sparked.","However, it is challenging to maintain the performance of KG embedding models while forgetting the influence of specific forgotten data on the model.","In this paper, we propose FedDM, a novel framework tailored for machine unlearning in federated knowledge graphs.","Leveraging diffusion models, we generate noisy data to sensibly mitigate the influence of specific knowledge on FL models while preserving the overall performance concerning the remaining data.","We conduct experimental evaluations on benchmark datasets to assess the efficacy of the proposed model.","Extensive experiments demonstrate that FedDM yields promising results in knowledge forgetting."],"url":"http://arxiv.org/abs/2403.08554v1","category":"cs.LG"}
{"created":"2024-03-13 14:02:54","title":"GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting","abstract":"Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. Furthermore, we integrate existing vector quantization technique to build an image codec. Experimental results demonstrate that our codec attains rate-distortion performance comparable to compression-based INRs such as COIN and COIN++, while facilitating decoding speeds of approximately 1000 FPS. Additionally, preliminary proof of concept shows that our codec surpasses COIN and COIN++ in performance when using partial bits-back coding.","sentences":["Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available.","However, this requirement often hinders their use on low-end devices with limited memory.","In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage.","We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color.","Subsequently, we unveil a novel rendering algorithm based on accumulated summation.","Remarkably, our method with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size.","Furthermore, we integrate existing vector quantization technique to build an image codec.","Experimental results demonstrate that our codec attains rate-distortion performance comparable to compression-based INRs such as COIN and COIN++, while facilitating decoding speeds of approximately 1000 FPS.","Additionally, preliminary proof of concept shows that our codec surpasses COIN and COIN++ in performance when using partial bits-back coding."],"url":"http://arxiv.org/abs/2403.08551v2","category":"eess.IV"}
{"created":"2024-03-13 14:00:00","title":"Search for low-mass resonances decaying into two jets and produced in association with a photon or a jet at $\\sqrt{s}=13$ TeV with the ATLAS detector","abstract":"A search is performed for localized excesses in the low-mass dijet invariant mass distribution, targeting a hypothetical new particle decaying into two jets and produced in association with either a high transverse momentum photon or a jet. The search uses the full Run 2 data sample from LHC proton-proton collisions collected by the ATLAS experiment at a center-of-mass energy of 13 TeV during 2015-2018. Two variants of the search are presented for each type of initial-state radiation: one that makes no jet flavor requirements and one that requires both of the jets to have been identified as containing $b$-hadrons. No excess is observed relative to the Standard Model prediction, and the data are used to set upper limits on the production cross-section for a benchmark $Z'$ model and, separately, for generic, beyond the Standard Model scenarios which might produce a Gaussian-shaped contribution to dijet invariant mass distributions. The results extend the current constraints on dijet resonances to the mass range between 200 and 650 GeV.","sentences":["A search is performed for localized excesses in the low-mass dijet invariant mass distribution, targeting a hypothetical new particle decaying into two jets and produced in association with either a high transverse momentum photon or a jet.","The search uses the full Run 2 data sample from LHC proton-proton collisions collected by the ATLAS experiment at a center-of-mass energy of 13 TeV during 2015-2018.","Two variants of the search are presented for each type of initial-state radiation: one that makes no jet flavor requirements and one that requires both of the jets to have been identified as containing $b$-hadrons.","No excess is observed relative to the Standard Model prediction, and the data are used to set upper limits on the production cross-section for a benchmark $Z'$ model and, separately, for generic, beyond the Standard Model scenarios which might produce a Gaussian-shaped contribution to dijet invariant mass distributions.","The results extend the current constraints on dijet resonances to the mass range between 200 and 650 GeV."],"url":"http://arxiv.org/abs/2403.08547v1","category":"hep-ex"}
{"created":"2024-03-13 13:56:34","title":"AIGCs Confuse AI Too: Investigating and Explaining Synthetic Image-induced Hallucinations in Large Vision-Language Models","abstract":"The evolution of Artificial Intelligence Generated Contents (AIGCs) is advancing towards higher quality. The growing interactions with AIGCs present a new challenge to the data-driven AI community: While AI-generated contents have played a crucial role in a wide range of AI models, the potential hidden risks they introduce have not been thoroughly examined. Beyond human-oriented forgery detection, AI-generated content poses potential issues for AI models originally designed to process natural data. In this study, we underscore the exacerbated hallucination phenomena in Large Vision-Language Models (LVLMs) caused by AI-synthetic images. Remarkably, our findings shed light on a consistent AIGC \\textbf{hallucination bias}: the object hallucinations induced by synthetic images are characterized by a greater quantity and a more uniform position distribution, even these synthetic images do not manifest unrealistic or additional relevant visual features compared to natural images. Moreover, our investigations on Q-former and Linear projector reveal that synthetic images may present token deviations after visual projection, thereby amplifying the hallucination bias.","sentences":["The evolution of Artificial Intelligence Generated Contents (AIGCs) is advancing towards higher quality.","The growing interactions with AIGCs present a new challenge to the data-driven AI community: While AI-generated contents have played a crucial role in a wide range of AI models, the potential hidden risks they introduce have not been thoroughly examined.","Beyond human-oriented forgery detection, AI-generated content poses potential issues for AI models originally designed to process natural data.","In this study, we underscore the exacerbated hallucination phenomena in Large Vision-Language Models (LVLMs) caused by AI-synthetic images.","Remarkably, our findings shed light on a consistent AIGC \\textbf{hallucination bias}: the object hallucinations induced by synthetic images are characterized by a greater quantity and a more uniform position distribution, even these synthetic images do not manifest unrealistic or additional relevant visual features compared to natural images.","Moreover, our investigations on Q-former and Linear projector reveal that synthetic images may present token deviations after visual projection, thereby amplifying the hallucination bias."],"url":"http://arxiv.org/abs/2403.08542v1","category":"cs.CV"}
{"created":"2024-03-13 13:51:02","title":"HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional Image Classifiers","abstract":"Convolutional Neural Networks (CNNs) are nowadays the model of choice in Computer Vision, thanks to their ability to automatize the feature extraction process in visual tasks. However, the knowledge acquired during training is fully subsymbolic, and hence difficult to understand and explain to end users. In this paper, we propose a new technique called HOLMES (HOLonym-MEronym based Semantic inspection) that decomposes a label into a set of related concepts, and provides component-level explanations for an image classification model. Specifically, HOLMES leverages ontologies, web scraping and transfer learning to automatically construct meronym (parts)-based detectors for a given holonym (class). Then, it produces heatmaps at the meronym level and finally, by probing the holonym CNN with occluded images, it highlights the importance of each part on the classification output. Compared to state-of-the-art saliency methods, HOLMES takes a step further and provides information about both where and what the holonym CNN is looking at, without relying on densely annotated datasets and without forcing concepts to be associated to single computational units. Extensive experimental evaluation on different categories of objects (animals, tools and vehicles) shows the feasibility of our approach. On average, HOLMES explanations include at least two meronyms, and the ablation of a single meronym roughly halves the holonym model confidence. The resulting heatmaps were quantitatively evaluated using the deletion/insertion/preservation curves. All metrics were comparable to those achieved by GradCAM, while offering the advantage of further decomposing the heatmap in human-understandable concepts, thus highlighting both the relevance of meronyms to object classification, as well as HOLMES ability to capture it. The code is available at https://github.com/FrancesC0de/HOLMES.","sentences":["Convolutional Neural Networks (CNNs) are nowadays the model of choice in Computer Vision, thanks to their ability to automatize the feature extraction process in visual tasks.","However, the knowledge acquired during training is fully subsymbolic, and hence difficult to understand and explain to end users.","In this paper, we propose a new technique called HOLMES (HOLonym-MEronym based Semantic inspection) that decomposes a label into a set of related concepts, and provides component-level explanations for an image classification model.","Specifically, HOLMES leverages ontologies, web scraping and transfer learning to automatically construct meronym (parts)-based detectors for a given holonym (class).","Then, it produces heatmaps at the meronym level and finally, by probing the holonym CNN with occluded images, it highlights the importance of each part on the classification output.","Compared to state-of-the-art saliency methods, HOLMES takes a step further and provides information about both where and what the holonym CNN is looking at, without relying on densely annotated datasets and without forcing concepts to be associated to single computational units.","Extensive experimental evaluation on different categories of objects (animals, tools and vehicles) shows the feasibility of our approach.","On average, HOLMES explanations include at least two meronyms, and the ablation of a single meronym roughly halves the holonym model confidence.","The resulting heatmaps were quantitatively evaluated using the deletion/insertion/preservation curves.","All metrics were comparable to those achieved by GradCAM, while offering the advantage of further decomposing the heatmap in human-understandable concepts, thus highlighting both the relevance of meronyms to object classification, as well as HOLMES ability to capture it.","The code is available at https://github.com/FrancesC0de/HOLMES."],"url":"http://arxiv.org/abs/2403.08536v1","category":"cs.CV"}
{"created":"2024-03-13 13:38:58","title":"Pig aggression classification using CNN, Transformers and Recurrent Networks","abstract":"The development of techniques that can be used to analyze and detect animal behavior is a crucial activity for the livestock sector, as it is possible to monitor the stress and animal welfare and contributes to decision making in the farm. Thus, the development of applications can assist breeders in making decisions to improve production performance and reduce costs, once the animal behavior is analyzed by humans and this can lead to susceptible errors and time consumption. Aggressiveness in pigs is an example of behavior that is studied to reduce its impact through animal classification and identification. However, this process is laborious and susceptible to errors, which can be reduced through automation by visually classifying videos captured in controlled environment. The captured videos can be used for training and, as a result, for classification through computer vision and artificial intelligence, employing neural network techniques. The main techniques utilized in this study are variants of transformers: STAM, TimeSformer, and ViViT, as well as techniques using convolutions, such as ResNet3D2, Resnet(2+1)D, and CnnLstm. These techniques were employed for pig video classification with the objective of identifying aggressive and non-aggressive behaviors. In this work, various techniques were compared to analyze the contribution of using transformers, in addition to the effectiveness of the convolution technique in video classification. The performance was evaluated using accuracy, precision, and recall. The TimerSformer technique showed the best results in video classification, with median accuracy of 0.729.","sentences":["The development of techniques that can be used to analyze and detect animal behavior is a crucial activity for the livestock sector, as it is possible to monitor the stress and animal welfare and contributes to decision making in the farm.","Thus, the development of applications can assist breeders in making decisions to improve production performance and reduce costs, once the animal behavior is analyzed by humans and this can lead to susceptible errors and time consumption.","Aggressiveness in pigs is an example of behavior that is studied to reduce its impact through animal classification and identification.","However, this process is laborious and susceptible to errors, which can be reduced through automation by visually classifying videos captured in controlled environment.","The captured videos can be used for training and, as a result, for classification through computer vision and artificial intelligence, employing neural network techniques.","The main techniques utilized in this study are variants of transformers: STAM, TimeSformer, and ViViT, as well as techniques using convolutions, such as ResNet3D2, Resnet(2+1)D, and CnnLstm.","These techniques were employed for pig video classification with the objective of identifying aggressive and non-aggressive behaviors.","In this work, various techniques were compared to analyze the contribution of using transformers, in addition to the effectiveness of the convolution technique in video classification.","The performance was evaluated using accuracy, precision, and recall.","The TimerSformer technique showed the best results in video classification, with median accuracy of 0.729."],"url":"http://arxiv.org/abs/2403.08528v1","category":"cs.CV"}
{"created":"2024-03-13 13:12:57","title":"Content-aware Masked Image Modeling Transformer for Stereo Image Compression","abstract":"Existing learning-based stereo image codec adopt sophisticated transformation with simple entropy models derived from single image codecs to encode latent representations. However, those entropy models struggle to effectively capture the spatial-disparity characteristics inherent in stereo images, which leads to suboptimal rate-distortion results. In this paper, we propose a stereo image compression framework, named CAMSIC. CAMSIC independently transforms each image to latent representation and employs a powerful decoder-free Transformer entropy model to capture both spatial and disparity dependencies, by introducing a novel content-aware masked image modeling (MIM) technique. Our content-aware MIM facilitates efficient bidirectional interaction between prior information and estimated tokens, which naturally obviates the need for an extra Transformer decoder. Experiments show that our stereo image codec achieves state-of-the-art rate-distortion performance on two stereo image datasets Cityscapes and InStereo2K with fast encoding and decoding speed.","sentences":["Existing learning-based stereo image codec adopt sophisticated transformation with simple entropy models derived from single image codecs to encode latent representations.","However, those entropy models struggle to effectively capture the spatial-disparity characteristics inherent in stereo images, which leads to suboptimal rate-distortion results.","In this paper, we propose a stereo image compression framework, named CAMSIC.","CAMSIC independently transforms each image to latent representation and employs a powerful decoder-free Transformer entropy model to capture both spatial and disparity dependencies, by introducing a novel content-aware masked image modeling (MIM) technique.","Our content-aware MIM facilitates efficient bidirectional interaction between prior information and estimated tokens, which naturally obviates the need for an extra Transformer decoder.","Experiments show that our stereo image codec achieves state-of-the-art rate-distortion performance on two stereo image datasets Cityscapes and InStereo2K with fast encoding and decoding speed."],"url":"http://arxiv.org/abs/2403.08505v1","category":"eess.IV"}
{"created":"2024-03-13 13:10:20","title":"Masked Generative Story Transformer with Character Guidance and Caption Augmentation","abstract":"Story Visualization (SV) is a challenging generative vision task, that requires both visual quality and consistency between different frames in generated image sequences. Previous approaches either employ some kind of memory mechanism to maintain context throughout an auto-regressive generation of the image sequence, or model the generation of the characters and their background separately, to improve the rendering of characters. On the contrary, we embrace a completely parallel transformer-based approach, exclusively relying on Cross-Attention with past and future captions to achieve consistency. Additionally, we propose a Character Guidance technique to focus on the generation of characters in an implicit manner, by forming a combination of text-conditional and character-conditional logits in the logit space. We also employ a caption-augmentation technique, carried out by a Large Language Model (LLM), to enhance the robustness of our approach. The combination of these methods culminates into state-of-the-art (SOTA) results over various metrics in the most prominent SV benchmark (Pororo-SV), attained with constraint resources while achieving superior computational complexity compared to previous arts. The validity of our quantitative results is supported by a human survey.","sentences":["Story Visualization (SV) is a challenging generative vision task, that requires both visual quality and consistency between different frames in generated image sequences.","Previous approaches either employ some kind of memory mechanism to maintain context throughout an auto-regressive generation of the image sequence, or model the generation of the characters and their background separately, to improve the rendering of characters.","On the contrary, we embrace a completely parallel transformer-based approach, exclusively relying on Cross-Attention with past and future captions to achieve consistency.","Additionally, we propose a Character Guidance technique to focus on the generation of characters in an implicit manner, by forming a combination of text-conditional and character-conditional logits in the logit space.","We also employ a caption-augmentation technique, carried out by a Large Language Model (LLM), to enhance the robustness of our approach.","The combination of these methods culminates into state-of-the-art (SOTA) results over various metrics in the most prominent SV benchmark (Pororo-SV), attained with constraint resources while achieving superior computational complexity compared to previous arts.","The validity of our quantitative results is supported by a human survey."],"url":"http://arxiv.org/abs/2403.08502v1","category":"cs.CV"}
{"created":"2024-03-13 13:06:31","title":"Gaussian Splatting in Style","abstract":"Scene stylization extends the work of neural style transfer to three spatial dimensions. A vital challenge in this problem is to maintain the uniformity of the stylized appearance across a multi-view setting. A vast majority of the previous works achieve this by optimizing the scene with a specific style image. In contrast, we propose a novel architecture trained on a collection of style images, that at test time produces high quality stylized novel views. Our work builds up on the framework of 3D Gaussian splatting. For a given scene, we take the pretrained Gaussians and process them using a multi resolution hash grid and a tiny MLP to obtain the conditional stylised views. The explicit nature of 3D Gaussians give us inherent advantages over NeRF-based methods including geometric consistency, along with having a fast training and rendering regime. This enables our method to be useful for vast practical use cases such as in augmented or virtual reality applications. Through our experiments, we show our methods achieve state-of-the-art performance with superior visual quality on various indoor and outdoor real-world data.","sentences":["Scene stylization extends the work of neural style transfer to three spatial dimensions.","A vital challenge in this problem is to maintain the uniformity of the stylized appearance across a multi-view setting.","A vast majority of the previous works achieve this by optimizing the scene with a specific style image.","In contrast, we propose a novel architecture trained on a collection of style images, that at test time produces high quality stylized novel views.","Our work builds up on the framework of 3D Gaussian splatting.","For a given scene, we take the pretrained Gaussians and process them using a multi resolution hash grid and a tiny MLP to obtain the conditional stylised views.","The explicit nature of 3D Gaussians give us inherent advantages over NeRF-based methods including geometric consistency, along with having a fast training and rendering regime.","This enables our method to be useful for vast practical use cases such as in augmented or virtual reality applications.","Through our experiments, we show our methods achieve state-of-the-art performance with superior visual quality on various indoor and outdoor real-world data."],"url":"http://arxiv.org/abs/2403.08498v1","category":"cs.CV"}
{"created":"2024-03-14 17:32:31","title":"cosmocnc: A fast, flexible, and accurate galaxy cluster number count likelihood code for cosmology","abstract":"We introduce cosmocnc, a Python package for computing the number count likelihood of galaxy cluster catalogues in a fast, flexible and accurate way. cosmocnc offers three types of likelihoods: an unbinned, a binned, and an extreme value likelihood. It also supports the addition of stacked cluster data, which is modelled consistently with the cluster catalogue. The unbinned likelihood, which is the main focus of the code, can take an arbitrary number of mass observables as input and deal with several complexities in the data, such as variations in the properties of the cluster observable across the survey footprint, the possibility of different clusters having measurements for different combinations of mass observables, redshift measurement uncertainties, and the presence on unconfirmed detections in the catalogue. If there are more than one mass observables, the unbinned likelihood is computed with the backward convolutional approach, a novel approach that is first implemented in cosmocnc. After developing the likelihood formalism and describing its implementation, we validate the code with synthetic Simons-Observatory-like catalogues, finding excellent agreement between their properties and cosmocnc's predictions and obtaining constraints on cosmological and scaling relation parameters featuring negligible biases. cosmocnc is publicly available at github.com/inigozubeldia/cosmocnc.","sentences":["We introduce cosmocnc, a Python package for computing the number count likelihood of galaxy cluster catalogues in a fast, flexible and accurate way.","cosmocnc offers three types of likelihoods: an unbinned, a binned, and an extreme value likelihood.","It also supports the addition of stacked cluster data, which is modelled consistently with the cluster catalogue.","The unbinned likelihood, which is the main focus of the code, can take an arbitrary number of mass observables as input and deal with several complexities in the data, such as variations in the properties of the cluster observable across the survey footprint, the possibility of different clusters having measurements for different combinations of mass observables, redshift measurement uncertainties, and the presence on unconfirmed detections in the catalogue.","If there are more than one mass observables, the unbinned likelihood is computed with the backward convolutional approach, a novel approach that is first implemented in cosmocnc.","After developing the likelihood formalism and describing its implementation, we validate the code with synthetic Simons-Observatory-like catalogues, finding excellent agreement between their properties and cosmocnc's predictions and obtaining constraints on cosmological and scaling relation parameters featuring negligible biases.","cosmocnc is publicly available at github.com/inigozubeldia/cosmocnc."],"url":"http://arxiv.org/abs/2403.09589v1","category":"astro-ph.CO"}
{"created":"2024-03-13 17:59:54","title":"The thermalization of $\u03b3$-rays in radioactive expanding ejecta: A simple model and its application for Kilonovae and Ia SNe","abstract":"A semi-analytic approximation is derived for the time-dependent fraction $f_\\gamma(t)$ of the energy deposited by radioactive decay $\\gamma$-rays in a homologously expanding plasma of general structure. An analytic approximation is given for spherically symmetric plasma distributions. Applied to Kilonovae (KNe) associated with neutron stars mergers and Type Ia supernovae, our semi-analytic and analytic approximations reproduce, with a few percent and 10% accuracy, respectively, the energy deposition rates, $\\dot{Q}_\\text{dep}$, obtained in numeric Monte Carlo calculations. The time $t_\\gamma$ beyond which $\\gamma$-ray deposition is inefficient is determined by an effective frequency-independent $\\gamma$-ray opacity $\\kappa_{\\gamma,\\text{eff}}$, $t_\\gamma = \\sqrt{\\kappa_{\\gamma,\\text{eff}}\\langle\\Sigma\\rangle t^2}$, where $\\langle\\Sigma\\rangle\\propto t^{-2}$ is the average plasma column density. For $\\beta$-decay dominated energy release, $\\kappa_{\\gamma,\\text{eff}}$ is typically close to the effective Compton scattering opacity, $\\kappa_{\\gamma,\\text{eff}} \\approx 0.025~{\\rm {cm}^{2}\\,g^{-1}}$ with a weak dependence on composition. For KNe, $\\kappa_{\\gamma,\\text{eff}}$ depends mainly on the initial electron fraction $Y_e$, $\\kappa_{\\gamma,\\text{eff}} \\approx 0.03(0.05)~{\\rm {cm}^{2}\\,g^{-1}}$ for $Y_e \\gtrsim (\\lesssim) 0.25$ (in contrast with earlier work that found $\\kappa_{\\gamma,\\text{eff}}$ larger by 1-2 orders of magnitude for low $Y_e$), and is insensitive to the (large) nuclear physics uncertainties. Determining $t_\\gamma$ from observations will therefore measure the ejecta $\\langle\\Sigma\\rangle t^2$, providing a stringent test of models. For $\\langle\\Sigma\\rangle t^2=2\\times10^{11}~{\\rm g\\,{cm}^{-2}\\,s^2}$, a typical value expected for KNe, $t_\\gamma\\approx1$ d.","sentences":["A semi-analytic approximation is derived for the time-dependent fraction $f_\\gamma(t)$ of the energy deposited by radioactive decay $\\gamma$-rays in a homologously expanding plasma of general structure.","An analytic approximation is given for spherically symmetric plasma distributions.","Applied to Kilonovae (KNe) associated with neutron stars mergers and Type Ia supernovae, our semi-analytic and analytic approximations reproduce, with a few percent and 10% accuracy, respectively, the energy deposition rates, $\\dot{Q}_\\text{dep}$, obtained in numeric Monte Carlo calculations.","The time $t_\\gamma$ beyond which $\\gamma$-ray deposition is inefficient is determined by an effective frequency-independent $\\gamma$-ray opacity $\\kappa_{\\gamma,\\text{eff}}$, $t_\\gamma = \\sqrt{\\kappa_{\\gamma,\\text{eff}}\\langle\\Sigma\\rangle t^2}$, where $\\langle\\Sigma\\rangle\\propto t^{-2}$ is the average plasma column density.","For $\\beta$-decay dominated energy release, $\\kappa_{\\gamma,\\text{eff}}$ is typically close to the effective Compton scattering opacity, $\\kappa_{\\gamma,\\text{eff}} \\approx 0.025~{\\rm {cm}^{2}\\,g^{-1}}$ with a weak dependence on composition.","For KNe, $\\kappa_{\\gamma,\\text{eff}}$ depends mainly on the initial electron fraction $Y_e$, $\\kappa_{\\gamma,\\text{eff}} \\approx 0.03(0.05)~{\\rm {cm}^{2}\\,g^{-1}}$ for $Y_e \\gtrsim (\\lesssim) 0.25$ (in contrast with earlier work that found $\\kappa_{\\gamma,\\text{eff}}$ larger by 1-2 orders of magnitude for low $Y_e$), and is insensitive to the (large) nuclear physics uncertainties.","Determining $t_\\gamma$ from observations will therefore measure the ejecta $\\langle\\Sigma\\rangle t^2$, providing a stringent test of models.","For $\\langle\\Sigma\\rangle t^2=2\\times10^{11}~{\\rm g\\,{cm}^{-2}\\,s^2}$, a typical value expected for KNe, $t_\\gamma\\approx1$ d."],"url":"http://arxiv.org/abs/2403.08769v1","category":"astro-ph.HE"}
{"created":"2024-03-13 17:59:03","title":"An Analytic Description of Electron Thermalization in Kilonovae Ejecta","abstract":"A simple analytic description is provided of the rate of energy deposition by $\\beta$-decay electrons in the homologously expanding radioactive plasma ejected in neutron star mergers, valid for a wide range of ejecta parameters -- initial entropy, electron fraction $\\{s_0,Y_e\\}$ and density $\\rho t^3$. The formulae are derived using detailed numerical calculations following the time-dependent composition and $\\beta$-decay emission spectra (including the effect of delayed deposition). The deposition efficiency depends mainly on $\\rho t^3$ and only weakly on $\\{s_0,Y_e\\}$. The time $t_e$ at which the ratio between the rates of electron energy deposition and energy production drops to $1-e^{-1}$, is given by $t_e=t_{0e}\\Big(\\frac{\\rho t^3}{0.5(\\rho t^3)_0}\\Big)^a$, where $(\\rho t^3)_0=\\frac{0.05M_{\\odot}}{4\\pi(0.2c)^3}$, $t_{0e}(s_0,Y_e)\\approx17$ days and $0.4\\le a(s_0,Y_e)\\le0.5$. The fractional uncertainty in $t_e$ due to nuclear physics uncertainties is $\\approx10\\%$. The result $a\\le0.5$ reflects the fact that the characteristic $\\beta$-decay electron energies do not decrease with time (largely due to \"inverted decay chains\" in which a slowly-decaying isotope decays to a rapidly-decaying isotope with higher end-point energy). We provide an analytic approximation for the time-dependent electron energy deposition rate, reproducing the numerical results to better than $50\\%$ (typically $<30\\%$, well within the energy production rate uncertainty due to nuclear physics uncertainties) over a 3-4 orders-of-magnitude deposition rate decrease with time. Our results may be easily incorporated in calculations of kilonovae light curves (with general density and composition structures), eliminating the need to numerically follow the time-dependent electron spectra. Identifying $t_e$, e.g. in the bolometric light curve, will constrain the (properly averaged) ejecta $\\rho t^3$.","sentences":["A simple analytic description is provided of the rate of energy deposition by $\\beta$-decay electrons in the homologously expanding radioactive plasma ejected in neutron star mergers, valid for a wide range of ejecta parameters -- initial entropy, electron fraction $\\{s_0,Y_e\\}$ and density $\\rho t^3$. The formulae are derived using detailed numerical calculations following the time-dependent composition and $\\beta$-decay emission spectra (including the effect of delayed deposition).","The deposition efficiency depends mainly on $\\rho t^3$ and only weakly on $\\{s_0,Y_e\\}$. The time $t_e$ at which the ratio between the rates of electron energy deposition and energy production drops to $1-e^{-1}$, is given by $t_e=t_{0e}\\Big(\\frac{\\rho t^3}{0.5(\\rho t^3)_0}\\Big)^a$, where $(\\rho t^3)_0=\\frac{0.05M_{\\odot}}{4\\pi(0.2c)^3}$, $t_{0e}(s_0,Y_e)\\approx17$ days and $0.4\\le a(s_0,Y_e)\\le0.5$.","The fractional uncertainty in $t_e$ due to nuclear physics uncertainties is $\\approx10\\%$. The result $a\\le0.5$ reflects the fact that the characteristic $\\beta$-decay electron energies do not decrease with time (largely due to \"inverted decay chains\" in which a slowly-decaying isotope decays to a rapidly-decaying isotope with higher end-point energy).","We provide an analytic approximation for the time-dependent electron energy deposition rate, reproducing the numerical results to better than $50\\%$ (typically $<30\\%$, well within the energy production rate uncertainty due to nuclear physics uncertainties) over a 3-4 orders-of-magnitude deposition rate decrease with time.","Our results may be easily incorporated in calculations of kilonovae light curves (with general density and composition structures), eliminating the need to numerically follow the time-dependent electron spectra.","Identifying $t_e$, e.g. in the bolometric light curve, will constrain the (properly averaged) ejecta","$\\rho t^3$."],"url":"http://arxiv.org/abs/2403.08765v1","category":"astro-ph.HE"}
{"created":"2024-03-13 17:56:12","title":"Spatiotemporal Diffusion Model with Paired Sampling for Accelerated Cardiac Cine MRI","abstract":"Current deep learning reconstruction for accelerated cardiac cine MRI suffers from spatial and temporal blurring. We aim to improve image sharpness and motion delineation for cine MRI under high undersampling rates. A spatiotemporal diffusion enhancement model conditional on an existing deep learning reconstruction along with a novel paired sampling strategy was developed. The diffusion model provided sharper tissue boundaries and clearer motion than the original reconstruction in experts evaluation on clinical data. The innovative paired sampling strategy substantially reduced artificial noises in the generative results.","sentences":["Current deep learning reconstruction for accelerated cardiac cine MRI suffers from spatial and temporal blurring.","We aim to improve image sharpness and motion delineation for cine MRI under high undersampling rates.","A spatiotemporal diffusion enhancement model conditional on an existing deep learning reconstruction along with a novel paired sampling strategy was developed.","The diffusion model provided sharper tissue boundaries and clearer motion than the original reconstruction in experts evaluation on clinical data.","The innovative paired sampling strategy substantially reduced artificial noises in the generative results."],"url":"http://arxiv.org/abs/2403.08758v1","category":"eess.IV"}
{"created":"2024-03-13 17:51:47","title":"Invalid proxies and volatility changes","abstract":"When in proxy-SVARs the covariance matrix of VAR disturbances is subject to exogenous, permanent, nonrecurring breaks that generate target impulse response functions (IRFs) that change across volatility regimes, even strong, exogenous external instruments can result in inconsistent estimates of the dynamic causal effects of interest if the breaks are not properly accounted for. In such cases, it is essential to explicitly incorporate the shifts in unconditional volatility in order to point-identify the target structural shocks and possibly restore consistency. We demonstrate that, under a necessary and sufficient rank condition that leverages moments implied by changes in volatility, the target IRFs can be point-identified and consistently estimated. Importantly, standard asymptotic inference remains valid in this context despite (i) the covariance between the proxies and the instrumented structural shocks being local-to-zero, as in Staiger and Stock (1997), and (ii) the potential failure of instrument exogeneity. We introduce a novel identification strategy that appropriately combines external instruments with \"informative\" changes in volatility, thus obviating the need to assume proxy relevance and exogeneity in estimation. We illustrate the effectiveness of the suggested method by revisiting a fiscal proxy-SVAR previously estimated in the literature, complementing the fiscal instruments with information derived from the massive reduction in volatility observed in the transition from the Great Inflation to the Great Moderation regimes.","sentences":["When in proxy-SVARs the covariance matrix of VAR disturbances is subject to exogenous, permanent, nonrecurring breaks that generate target impulse response functions (IRFs) that change across volatility regimes, even strong, exogenous external instruments can result in inconsistent estimates of the dynamic causal effects of interest if the breaks are not properly accounted for.","In such cases, it is essential to explicitly incorporate the shifts in unconditional volatility in order to point-identify the target structural shocks and possibly restore consistency.","We demonstrate that, under a necessary and sufficient rank condition that leverages moments implied by changes in volatility, the target IRFs can be point-identified and consistently estimated.","Importantly, standard asymptotic inference remains valid in this context despite (i) the covariance between the proxies and the instrumented structural shocks being local-to-zero, as in Staiger and Stock (1997), and (ii) the potential failure of instrument exogeneity.","We introduce a novel identification strategy that appropriately combines external instruments with \"informative\" changes in volatility, thus obviating the need to assume proxy relevance and exogeneity in estimation.","We illustrate the effectiveness of the suggested method by revisiting a fiscal proxy-SVAR previously estimated in the literature, complementing the fiscal instruments with information derived from the massive reduction in volatility observed in the transition from the Great Inflation to the Great Moderation regimes."],"url":"http://arxiv.org/abs/2403.08753v1","category":"econ.EM"}
{"created":"2024-03-13 17:30:06","title":"Fault Localization in a Microfabricated Surface Ion Trap using Diamond Nitrogen-Vacancy Center Magnetometry","abstract":"As quantum computing hardware becomes more complex with ongoing design innovations and growing capabilities, the quantum computing community needs increasingly powerful techniques for fabrication failure root-cause analysis. This is especially true for trapped-ion quantum computing. As trapped-ion quantum computing aims to scale to thousands of ions, the electrode numbers are growing to several hundred with likely integrated-photonic components also adding to the electrical and fabrication complexity, making faults even harder to locate. In this work, we used a high-resolution quantum magnetic imaging technique, based on nitrogen-vacancy (NV) centers in diamond, to investigate short-circuit faults in an ion trap chip. We imaged currents from these short-circuit faults to ground and compared to intentionally-created faults, finding that the root-cause of the faults was failures in the on-chip trench capacitors. This work, where we exploited the performance advantages of a quantum magnetic sensing technique to troubleshoot a piece of quantum computing hardware, is a unique example of the evolving synergy between emerging quantum technologies to achieve capabilities that were previously inaccessible.","sentences":["As quantum computing hardware becomes more complex with ongoing design innovations and growing capabilities, the quantum computing community needs increasingly powerful techniques for fabrication failure root-cause analysis.","This is especially true for trapped-ion quantum computing.","As trapped-ion quantum computing aims to scale to thousands of ions, the electrode numbers are growing to several hundred with likely integrated-photonic components also adding to the electrical and fabrication complexity, making faults even harder to locate.","In this work, we used a high-resolution quantum magnetic imaging technique, based on nitrogen-vacancy (NV) centers in diamond, to investigate short-circuit faults in an ion trap chip.","We imaged currents from these short-circuit faults to ground and compared to intentionally-created faults, finding that the root-cause of the faults was failures in the on-chip trench capacitors.","This work, where we exploited the performance advantages of a quantum magnetic sensing technique to troubleshoot a piece of quantum computing hardware, is a unique example of the evolving synergy between emerging quantum technologies to achieve capabilities that were previously inaccessible."],"url":"http://arxiv.org/abs/2403.08731v1","category":"physics.ins-det"}
{"created":"2024-03-13 17:27:31","title":"Euclid: Testing photometric selection of emission-line galaxy targets","abstract":"Multi-object spectroscopic galaxy surveys typically make use of photometric and colour criteria to select targets. Conversely, the Euclid NISP slitless spectrograph will record spectra for every source over its field of view. Slitless spectroscopy has the advantage of avoiding defining a priori a galaxy sample, but at the price of making the selection function harder to quantify. The Euclid Wide Survey aims at building robust statistical samples of emission-line galaxies with fluxes in the Halpha-NII complex brighter than 2e-16 erg/s/cm^2 and within 0.9<z<1.8. At faint fluxes, we expect significant contamination by wrongly measured redshifts, either due to emission-line misidentification or noise fluctuations, with the consequence of reducing the purity of the final samples. This can be significantly improved by exploiting Euclid photometric information to identify emission-line galaxies over the redshifts of interest. To this goal, we compare and quantify the performance of six machine-learning classification algorithms. We consider the case when only Euclid photometric and morphological measurements are used and when these are supplemented by ground-based photometric data. We train and test the classifiers on two mock galaxy samples, the EL-COSMOS and Euclid Flagship2 catalogues. Dense neural networks and support vector classifiers obtain the best performance, with comparable results in terms of the adopted metrics. When training on Euclid photometry alone, these can remove 87% of the sources that are fainter than the nominal flux limit or lie outside the range 0.9<z<1.8, a figure that increases to 97% when ground-based photometry is included. These results show how by using the photometric information available to Euclid it will be possible to efficiently identify and discard spurious interlopers, allowing us to build robust spectroscopic samples for cosmological investigations.","sentences":["Multi-object spectroscopic galaxy surveys typically make use of photometric and colour criteria to select targets.","Conversely, the Euclid NISP slitless spectrograph will record spectra for every source over its field of view.","Slitless spectroscopy has the advantage of avoiding defining a priori a galaxy sample, but at the price of making the selection function harder to quantify.","The Euclid Wide Survey aims at building robust statistical samples of emission-line galaxies with fluxes in the Halpha-NII complex brighter than 2e-16 erg/s/cm^2 and within 0.9<z<1.8.","At faint fluxes, we expect significant contamination by wrongly measured redshifts, either due to emission-line misidentification or noise fluctuations, with the consequence of reducing the purity of the final samples.","This can be significantly improved by exploiting Euclid photometric information to identify emission-line galaxies over the redshifts of interest.","To this goal, we compare and quantify the performance of six machine-learning classification algorithms.","We consider the case when only Euclid photometric and morphological measurements are used and when these are supplemented by ground-based photometric data.","We train and test the classifiers on two mock galaxy samples, the EL-COSMOS and Euclid Flagship2 catalogues.","Dense neural networks and support vector classifiers obtain the best performance, with comparable results in terms of the adopted metrics.","When training on Euclid photometry alone, these can remove 87% of the sources that are fainter than the nominal flux limit or lie outside the range 0.9<z<1.8, a figure that increases to 97% when ground-based photometry is included.","These results show how by using the photometric information available to Euclid it will be possible to efficiently identify and discard spurious interlopers, allowing us to build robust spectroscopic samples for cosmological investigations."],"url":"http://arxiv.org/abs/2403.08726v1","category":"astro-ph.CO"}
{"created":"2024-03-13 17:19:05","title":"Improved Trade-offs Between Amortization and Download Bandwidth for Linear HSS","abstract":"A Homomorphic Secret Sharing (HSS) scheme is a secret-sharing scheme that shares a secret $x$ among $s$ servers, and additionally allows an output client to reconstruct some function $f(x)$ using information that can be locally computed by each server. A key parameter in HSS schemes is download rate, which quantifies how much information the output client needs to download from the servers. Often, download rate is improved by amortizing over $\\ell$ instances of the problem, making $\\ell$ also a key parameter of interest.   Recent work (Fosli, Ishai, Kolobov, and Wootters 2022) established a limit on the download rate of linear HSS schemes for computing low-degree polynomials and constructed schemes that achieve this optimal download rate; their schemes required amortization over $\\ell = \\Omega(s \\log(s))$ instances of the problem. Subsequent work (Blackwell and Wootters, 2023) completely characterized linear HSS schemes that achieve optimal download rate in terms of a coding-theoretic notion termed optimal labelweight codes. A consequence of this characterization was that $\\ell = \\Omega(s \\log(s))$ is in fact necessary to achieve optimal download rate.   In this paper, we characterize all linear HSS schemes, showing that schemes of any download rate are equivalent to a generalization of optimal labelweight codes. This equivalence is constructive and provides a way to obtain an explicit linear HSS scheme from any linear code. Using this characterization, we present explicit linear HSS schemes with slightly sub-optimal rate but with much improved amortization $\\ell = O(s)$. Our constructions are based on algebraic geometry codes (specifically Hermitian codes and Goppa codes).","sentences":["A Homomorphic Secret Sharing (HSS) scheme is a secret-sharing scheme that shares a secret $x$ among $s$ servers, and additionally allows an output client to reconstruct some function $f(x)$ using information that can be locally computed by each server.","A key parameter in HSS schemes is download rate, which quantifies how much information the output client needs to download from the servers.","Often, download rate is improved by amortizing over $\\ell$ instances of the problem, making $\\ell$ also a key parameter of interest.   ","Recent work (Fosli, Ishai, Kolobov, and Wootters 2022) established a limit on the download rate of linear HSS schemes for computing low-degree polynomials and constructed schemes that achieve this optimal download rate; their schemes required amortization over $\\ell = \\Omega(s \\log(s))$ instances of the problem.","Subsequent work (Blackwell and Wootters, 2023) completely characterized linear HSS schemes that achieve optimal download rate in terms of a coding-theoretic notion termed optimal labelweight codes.","A consequence of this characterization was that $\\ell = \\Omega(s \\log(s))$ is in fact necessary to achieve optimal download rate.   ","In this paper, we characterize all linear HSS schemes, showing that schemes of any download rate are equivalent to a generalization of optimal labelweight codes.","This equivalence is constructive and provides a way to obtain an explicit linear HSS scheme from any linear code.","Using this characterization, we present explicit linear HSS schemes with slightly sub-optimal rate but with much improved amortization $\\ell = O(s)$.","Our constructions are based on algebraic geometry codes (specifically Hermitian codes and Goppa codes)."],"url":"http://arxiv.org/abs/2403.08719v1","category":"cs.IT"}
{"created":"2024-03-13 17:18:19","title":"DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation","abstract":"We introduce DIFFTACTILE, a physics-based differentiable tactile simulation system designed to enhance robotic manipulation with dense and physically accurate tactile feedback. In contrast to prior tactile simulators which primarily focus on manipulating rigid bodies and often rely on simplified approximations to model stress and deformations of materials in contact, DIFFTACTILE emphasizes physics-based contact modeling with high fidelity, supporting simulations of diverse contact modes and interactions with objects possessing a wide range of material properties. Our system incorporates several key components, including a Finite Element Method (FEM)-based soft body model for simulating the sensing elastomer, a multi-material simulator for modeling diverse object types (such as elastic, elastoplastic, cables) under manipulation, a penalty-based contact model for handling contact dynamics. The differentiable nature of our system facilitates gradient-based optimization for both 1) refining physical properties in simulation using real-world data, hence narrowing the sim-to-real gap and 2) efficient learning of tactile-assisted grasping and contact-rich manipulation skills. Additionally, we introduce a method to infer the optical response of our tactile sensor to contact using an efficient pixel-based neural module. We anticipate that DIFFTACTILE will serve as a useful platform for studying contact-rich manipulations, leveraging the benefits of dense tactile feedback and differentiable physics. Code and supplementary materials are available at the project website https://difftactile.github.io/.","sentences":["We introduce DIFFTACTILE, a physics-based differentiable tactile simulation system designed to enhance robotic manipulation with dense and physically accurate tactile feedback.","In contrast to prior tactile simulators which primarily focus on manipulating rigid bodies and often rely on simplified approximations to model stress and deformations of materials in contact, DIFFTACTILE emphasizes physics-based contact modeling with high fidelity, supporting simulations of diverse contact modes and interactions with objects possessing a wide range of material properties.","Our system incorporates several key components, including a Finite Element Method (FEM)-based soft body model for simulating the sensing elastomer, a multi-material simulator for modeling diverse object types (such as elastic, elastoplastic, cables) under manipulation, a penalty-based contact model for handling contact dynamics.","The differentiable nature of our system facilitates gradient-based optimization for both 1) refining physical properties in simulation using real-world data, hence narrowing the sim-to-real gap and 2) efficient learning of tactile-assisted grasping and contact-rich manipulation skills.","Additionally, we introduce a method to infer the optical response of our tactile sensor to contact using an efficient pixel-based neural module.","We anticipate that DIFFTACTILE will serve as a useful platform for studying contact-rich manipulations, leveraging the benefits of dense tactile feedback and differentiable physics.","Code and supplementary materials are available at the project website https://difftactile.github.io/."],"url":"http://arxiv.org/abs/2403.08716v1","category":"cs.RO"}
{"created":"2024-03-13 17:17:36","title":"Dynamic computerized tomography using inexact models and motion estimation","abstract":"Reconstructing a dynamic object with affine motion in computerized tomography (CT) leads to motion artifacts if the motion is not taken into account. In most cases, the actual motion is neither known nor can be determined easily. As a consequence, the respective model that describes CT is incomplete. The iterative RESESOP-Kaczmarz method can - under certain conditions and by exploiting the modeling error - reconstruct dynamic objects at different time points even if the exact motion is unknown. However, the method is very time-consuming. To speed the reconstruction process up and obtain better results, we combine the following three steps: 1. RESESOP-Kacmarz with only a few iterations is implemented to reconstruct the object at different time points. 2. The motion is estimated via landmark detection, e.g. using deep learning. 3. The estimated motion is integrated into the reconstruction process, allowing the use of dynamic filtered backprojection. We give a short review of all methods involved and present numerical results as a proof of principle.","sentences":["Reconstructing a dynamic object with affine motion in computerized tomography (CT) leads to motion artifacts if the motion is not taken into account.","In most cases, the actual motion is neither known nor can be determined easily.","As a consequence, the respective model that describes CT is incomplete.","The iterative RESESOP-Kaczmarz method can - under certain conditions and by exploiting the modeling error - reconstruct dynamic objects at different time points even if the exact motion is unknown.","However, the method is very time-consuming.","To speed the reconstruction process up and obtain better results, we combine the following three steps: 1. RESESOP-Kacmarz with only a few iterations is implemented to reconstruct the object at different time points.","2.","The motion is estimated via landmark detection, e.g. using deep learning.","3. The estimated motion is integrated into the reconstruction process, allowing the use of dynamic filtered backprojection.","We give a short review of all methods involved and present numerical results as a proof of principle."],"url":"http://arxiv.org/abs/2403.08714v1","category":"math.NA"}
{"created":"2024-03-13 17:16:21","title":"Controlled-Joint Remote Implementation of Operators and its Possible Generalization","abstract":"The existing notion of the shared entangled state-assisted remote preparation of unitary operator (equivalently the existing notion of quantum remote control) using local operation and classical communication is generalized to a scenario where under the control of a supervisor two users can jointly implement arbitrary unitaries (one unknown unitary operation by each or equivalently a single unitary decomposed into two unitaries of the same dimension and given to two users) on an unknown quantum state available with a geographically separated user. It is explicitly shown that the task can be performed using a four-qubit hyperentangled state, which is entangled simultaneously in both spatial and polarization degrees of freedom of photons. The proposed protocol which can be viewed as primitive for distributed photonic quantum computing is further generalized to the case that drops the restrictions on the number of controllers and the number of parties performing unitaries and allows both the numbers to be arbitrary. It is also shown that all the existing variants of quantum remote control schemes can be obtained as special cases of the present scheme.","sentences":["The existing notion of the shared entangled state-assisted remote preparation of unitary operator (equivalently the existing notion of quantum remote control) using local operation and classical communication is generalized to a scenario where under the control of a supervisor two users can jointly implement arbitrary unitaries (one unknown unitary operation by each or equivalently a single unitary decomposed into two unitaries of the same dimension and given to two users) on an unknown quantum state available with a geographically separated user.","It is explicitly shown that the task can be performed using a four-qubit hyperentangled state, which is entangled simultaneously in both spatial and polarization degrees of freedom of photons.","The proposed protocol which can be viewed as primitive for distributed photonic quantum computing is further generalized to the case that drops the restrictions on the number of controllers and the number of parties performing unitaries and allows both the numbers to be arbitrary.","It is also shown that all the existing variants of quantum remote control schemes can be obtained as special cases of the present scheme."],"url":"http://arxiv.org/abs/2403.08712v1","category":"quant-ph"}
{"created":"2024-03-13 17:12:33","title":"Optimal adaptation of surface-code decoders to local noise","abstract":"Information obtained from noise characterization of a quantum device can be used in classical decoding algorithms to improve the performance of quantum error-correcting codes. Focusing on the surface code under local (i.e. single-qubit) noise, we present a simple method to determine the maximum extent to which adapting a surface-code decoder to a noise feature can lead to a performance improvement. Our method is based on a tensor-network decoding algorithm, which uses the syndrome information as well as a process matrix description of the noise to compute a near-optimal correction. By selectively mischaracterizing the noise model input to the decoder and measuring the resulting loss in fidelity of the logical qubit, we can determine the relative importance of individual noise parameters for decoding. We apply this method to several physically relevant uncorrelated noise models with features such as coherence, spatial inhomogeneity and bias. While noise generally requires many parameters to describe completely, we find that to achieve near optimal decoding it appears only necessary adapt the decoder to a small number of critical parameters.","sentences":["Information obtained from noise characterization of a quantum device can be used in classical decoding algorithms to improve the performance of quantum error-correcting codes.","Focusing on the surface code under local (i.e. single-qubit) noise, we present a simple method to determine the maximum extent to which adapting a surface-code decoder to a noise feature can lead to a performance improvement.","Our method is based on a tensor-network decoding algorithm, which uses the syndrome information as well as a process matrix description of the noise to compute a near-optimal correction.","By selectively mischaracterizing the noise model input to the decoder and measuring the resulting loss in fidelity of the logical qubit, we can determine the relative importance of individual noise parameters for decoding.","We apply this method to several physically relevant uncorrelated noise models with features such as coherence, spatial inhomogeneity and bias.","While noise generally requires many parameters to describe completely, we find that to achieve near optimal decoding it appears only necessary adapt the decoder to a small number of critical parameters."],"url":"http://arxiv.org/abs/2403.08706v1","category":"quant-ph"}
{"created":"2024-03-13 17:06:52","title":"On the Stochasticity of Aerosol-Cloud Interactions within a Data-driven Framework","abstract":"Aerosol-cloud interactions (ACI) pose the largest uncertainty for climate projections. Among many challenges of understanding ACI, the question of whether ACI is deterministic or stochastic has not been explicitly formulated and asked. Here we attempt to answer this question by predicting cloud droplet number concentration Nc from aerosol number concentration Na and ambient conditions. We use aerosol properties, vertical velocity fluctuation w', and meteorological states (temperature T and water vapor mixing ratio q_v) from the ACTIVATE field observations (2020 to 2022) as predictor variables to estimate Nc. We show that the climatological Nc can be successfully predicted using a machine learning model despite the strongly nonlinear and multi-scale nature of ACI. However, the observation-trained machine learning model fails to predict Nc in individual cases while it successfully predicts Nc of randomly selected data points that cover a broad spatiotemporal scale, suggesting the stochastic nature of ACI at fine spatiotemporal scales.","sentences":["Aerosol-cloud interactions (ACI) pose the largest uncertainty for climate projections.","Among many challenges of understanding ACI, the question of whether ACI is deterministic or stochastic has not been explicitly formulated and asked.","Here we attempt to answer this question by predicting cloud droplet number concentration Nc from aerosol number concentration Na and ambient conditions.","We use aerosol properties, vertical velocity fluctuation w', and meteorological states (temperature T and water vapor mixing ratio q_v) from the ACTIVATE field observations (2020 to 2022) as predictor variables to estimate Nc.","We show that the climatological Nc can be successfully predicted using a machine learning model despite the strongly nonlinear and multi-scale nature of ACI.","However, the observation-trained machine learning model fails to predict Nc in individual cases while it successfully predicts Nc of randomly selected data points that cover a broad spatiotemporal scale, suggesting the stochastic nature of ACI at fine spatiotemporal scales."],"url":"http://arxiv.org/abs/2403.08702v1","category":"physics.ao-ph"}
{"created":"2024-03-13 17:51:02","title":"Neural reproducing kernel Banach spaces and representer theorems for deep networks","abstract":"Studying the function spaces defined by neural networks helps to understand the corresponding learning models and their inductive bias. While in some limits neural networks correspond to function spaces that are reproducing kernel Hilbert spaces, these regimes do not capture the properties of the networks used in practice. In contrast, in this paper we show that deep neural networks define suitable reproducing kernel Banach spaces.   These spaces are equipped with norms that enforce a form of sparsity, enabling them to adapt to potential latent structures within the input data and their representations. In particular, leveraging the theory of reproducing kernel Banach spaces, combined with variational results, we derive representer theorems that justify the finite architectures commonly employed in applications. Our study extends analogous results for shallow networks and can be seen as a step towards considering more practically plausible neural architectures.","sentences":["Studying the function spaces defined by neural networks helps to understand the corresponding learning models and their inductive bias.","While in some limits neural networks correspond to function spaces that are reproducing kernel Hilbert spaces, these regimes do not capture the properties of the networks used in practice.","In contrast, in this paper we show that deep neural networks define suitable reproducing kernel Banach spaces.   ","These spaces are equipped with norms that enforce a form of sparsity, enabling them to adapt to potential latent structures within the input data and their representations.","In particular, leveraging the theory of reproducing kernel Banach spaces, combined with variational results, we derive representer theorems that justify the finite architectures commonly employed in applications.","Our study extends analogous results for shallow networks and can be seen as a step towards considering more practically plausible neural architectures."],"url":"http://arxiv.org/abs/2403.08750v1","category":"stat.ML"}
{"created":"2024-03-13 17:20:25","title":"Historical Astronomical Diagrams Decomposition in Geometric Primitives","abstract":"Automatically extracting the geometric content from the hundreds of thousands of diagrams drawn in historical manuscripts would enable historians to study the diffusion of astronomical knowledge on a global scale. However, state-of-the-art vectorization methods, often designed to tackle modern data, are not adapted to the complexity and diversity of historical astronomical diagrams. Our contribution is thus twofold. First, we introduce a unique dataset of 303 astronomical diagrams from diverse traditions, ranging from the XIIth to the XVIIIth century, annotated with more than 3000 line segments, circles and arcs. Second, we develop a model that builds on DINO-DETR to enable the prediction of multiple geometric primitives. We show that it can be trained solely on synthetic data and accurately predict primitives on our challenging dataset. Our approach widely improves over the LETR baseline, which is restricted to lines, by introducing a meaningful parametrization for multiple primitives, jointly training for detection and parameter refinement, using deformable attention and training on rich synthetic data. Our dataset and code are available on our webpage.","sentences":["Automatically extracting the geometric content from the hundreds of thousands of diagrams drawn in historical manuscripts would enable historians to study the diffusion of astronomical knowledge on a global scale.","However, state-of-the-art vectorization methods, often designed to tackle modern data, are not adapted to the complexity and diversity of historical astronomical diagrams.","Our contribution is thus twofold.","First, we introduce a unique dataset of 303 astronomical diagrams from diverse traditions, ranging from the XIIth to the XVIIIth century, annotated with more than 3000 line segments, circles and arcs.","Second, we develop a model that builds on DINO-DETR to enable the prediction of multiple geometric primitives.","We show that it can be trained solely on synthetic data and accurately predict primitives on our challenging dataset.","Our approach widely improves over the LETR baseline, which is restricted to lines, by introducing a meaningful parametrization for multiple primitives, jointly training for detection and parameter refinement, using deformable attention and training on rich synthetic data.","Our dataset and code are available on our webpage."],"url":"http://arxiv.org/abs/2403.08721v1","category":"cs.CV"}
{"created":"2024-03-13 17:18:39","title":"Probabilistic Metaplasticity for Continual Learning with Memristors","abstract":"Crossbar architectures utilizing memristor devices hold promise to address continual learning challenges in resource-constrained edge devices. However, these nanoscale devices often exhibit low precision and high variability in conductance modulation, rendering them unsuitable for continual learning solutions that consolidate weights through precise modulation. This issue can be circumvented by accumulating weight gradients in auxiliary high-precision memory and updating memristor weights when gradients are equivalent to memristor weight resolution. However, it leads to frequent memory access, high memory overhead, and energy dissipation. In this research, we propose probabilistic metaplasticity, which consolidates weights by modulating their update probability rather than magnitude. The proposed mechanism eliminates high-precision modification to weight magnitude and consequently, high-precision memory for gradient accumulation. We demonstrate the efficacy of the proposed mechanism by integrating probabilistic metaplasticity into a spiking network trained on an error threshold with low-precision memristor weights. Evaluations of two continual learning benchmarks show that probabilistic metaplasticity consumes ~67% lower memory for additional parameters and up to two orders of magnitude lower energy during parameter updates compared to an auxiliary memory-based solution while achieving state-of-the-art performance. The proposed model shows potential for energy-efficient continual learning with low-precision emerging devices.","sentences":["Crossbar architectures utilizing memristor devices hold promise to address continual learning challenges in resource-constrained edge devices.","However, these nanoscale devices often exhibit low precision and high variability in conductance modulation, rendering them unsuitable for continual learning solutions that consolidate weights through precise modulation.","This issue can be circumvented by accumulating weight gradients in auxiliary high-precision memory and updating memristor weights when gradients are equivalent to memristor weight resolution.","However, it leads to frequent memory access, high memory overhead, and energy dissipation.","In this research, we propose probabilistic metaplasticity, which consolidates weights by modulating their update probability rather than magnitude.","The proposed mechanism eliminates high-precision modification to weight magnitude and consequently, high-precision memory for gradient accumulation.","We demonstrate the efficacy of the proposed mechanism by integrating probabilistic metaplasticity into a spiking network trained on an error threshold with low-precision memristor weights.","Evaluations of two continual learning benchmarks show that probabilistic metaplasticity consumes ~67% lower memory for additional parameters and up to two orders of magnitude lower energy during parameter updates compared to an auxiliary memory-based solution while achieving state-of-the-art performance.","The proposed model shows potential for energy-efficient continual learning with low-precision emerging devices."],"url":"http://arxiv.org/abs/2403.08718v1","category":"eess.SY"}
{"created":"2024-03-13 16:44:36","title":"Digital Twin-assisted Reinforcement Learning for Resource-aware Microservice Offloading in Edge Computing","abstract":"Collaborative edge computing (CEC) has emerged as a promising paradigm, enabling edge nodes to collaborate and execute microservices from end devices. Microservice offloading, a fundamentally important problem, decides when and where microservices are executed upon the arrival of services. However, the dynamic nature of the real-world CEC environment often leads to inefficient microservice offloading strategies, resulting in underutilized resources and network congestion. To address this challenge, we formulate an online joint microservice offloading and bandwidth allocation problem, JMOBA, to minimize the average completion time of services. In this paper, we introduce a novel microservice offloading algorithm, DTDRLMO, which leverages deep reinforcement learning (DRL) and digital twin technology. Specifically, we employ digital twin techniques to predict and adapt to changing edge node loads and network conditions of CEC in real-time. Furthermore, this approach enables the generation of an efficient offloading plan, selecting the most suitable edge node for each microservice. Simulation results on real-world and synthetic datasets demonstrate that DTDRLMO outperforms heuristic and learning-based methods in average service completion time.","sentences":["Collaborative edge computing (CEC) has emerged as a promising paradigm, enabling edge nodes to collaborate and execute microservices from end devices.","Microservice offloading, a fundamentally important problem, decides when and where microservices are executed upon the arrival of services.","However, the dynamic nature of the real-world CEC environment often leads to inefficient microservice offloading strategies, resulting in underutilized resources and network congestion.","To address this challenge, we formulate an online joint microservice offloading and bandwidth allocation problem, JMOBA, to minimize the average completion time of services.","In this paper, we introduce a novel microservice offloading algorithm, DTDRLMO, which leverages deep reinforcement learning (DRL) and digital twin technology.","Specifically, we employ digital twin techniques to predict and adapt to changing edge node loads and network conditions of CEC in real-time.","Furthermore, this approach enables the generation of an efficient offloading plan, selecting the most suitable edge node for each microservice.","Simulation results on real-world and synthetic datasets demonstrate that DTDRLMO outperforms heuristic and learning-based methods in average service completion time."],"url":"http://arxiv.org/abs/2403.08687v1","category":"cs.NI"}
{"created":"2024-03-13 16:39:32","title":"Single file motion of robot swarms","abstract":"We present experimental results on the single file motion of a group of robots interacting with each other through position sensors. We successfully replicate the fundamental diagram typical of these systems, with a transition from free flow to congested traffic as the density of the system increases. In the latter scenario we also observe the characteristic stop-and-go waves. The unique advantages of this novel system, such as experimental stability and repeatability, allow for extended experimental runs, facilitating a comprehensive statistical analysis of the global dynamics. Above a certain density, we observe a divergence of the average jam duration and the average number of robots involved in it. This discovery enables us to precisely identify another transition: from congested intermittent flow (for intermediate densities) to a totally congested scenario for high densities. Beyond this finding, the present work demonstrates the suitability of robot swarms to model complex behaviors in many particle systems.","sentences":["We present experimental results on the single file motion of a group of robots interacting with each other through position sensors.","We successfully replicate the fundamental diagram typical of these systems, with a transition from free flow to congested traffic as the density of the system increases.","In the latter scenario we also observe the characteristic stop-and-go waves.","The unique advantages of this novel system, such as experimental stability and repeatability, allow for extended experimental runs, facilitating a comprehensive statistical analysis of the global dynamics.","Above a certain density, we observe a divergence of the average jam duration and the average number of robots involved in it.","This discovery enables us to precisely identify another transition: from congested intermittent flow (for intermediate densities) to a totally congested scenario for high densities.","Beyond this finding, the present work demonstrates the suitability of robot swarms to model complex behaviors in many particle systems."],"url":"http://arxiv.org/abs/2403.08683v1","category":"nlin.AO"}
{"created":"2024-03-13 16:20:30","title":"BHAC-QGP: three-dimensional MHD simulations of relativistic heavy-ion collisions, II. Application to Au-Au collisions","abstract":"We present BHAC-QGP, a new numerical code to simulate the evolution of matter created in heavy-ion collisions. BHAC-QGP is based on the Black Hole Accretion Code (BHAC), which has been designed to model astrophysical processes through the solution of the equations of general-relativistic magnetohydrodynamics. Like the mother code, BHAC-QGP uses Adaptive Mesh Refinement (AMR), which allows for a dynamic adjustment of the resolution in regions of the computational domain where a particularly high accuracy is needed. We here discuss a number of applications of BHAC-QGP to Au-Au collisions at Relativistic Heavy-Ion Collider (RHIC) energies and show that the code is able to reproduce results of other simulations of these scenarios, but with much higher accuracy.","sentences":["We present BHAC-QGP, a new numerical code to simulate the evolution of matter created in heavy-ion collisions.","BHAC-QGP is based on the Black Hole Accretion Code (BHAC), which has been designed to model astrophysical processes through the solution of the equations of general-relativistic magnetohydrodynamics.","Like the mother code, BHAC-QGP uses Adaptive Mesh Refinement (AMR), which allows for a dynamic adjustment of the resolution in regions of the computational domain where a particularly high accuracy is needed.","We here discuss a number of applications of BHAC-QGP to Au-Au collisions at Relativistic Heavy-Ion Collider (RHIC) energies and show that the code is able to reproduce results of other simulations of these scenarios, but with much higher accuracy."],"url":"http://arxiv.org/abs/2403.08669v1","category":"hep-ph"}
{"created":"2024-03-13 16:20:27","title":"BHAC-QGP: three-dimensional MHD simulations of relativistic heavy-ion collisions, I. Methods and tests","abstract":"We present BHAC-QGP, a new numerical code to simulate the evolution of matter created in heavy-ion collisions in the presence of electromagnetic fields. It is derived from the Black Hole Accretion Code (BHAC), which has been designed to model astrophysical processes in a general-relativistic magnetohydrodynamical description. As the original Black Hole Accretion Code, BHAC-QGP benefits from the use of Adaptive Mesh Refinement (AMR), which allows us to dynamically adjust the resolution where necessary, and makes use of time-dependent Milne coordinates and the ultrarelativistic equation of state, $P = e/3$. We demonstrate that BHAC-QGP accurately passes a number of systematic and rigorous tests.","sentences":["We present BHAC-QGP, a new numerical code to simulate the evolution of matter created in heavy-ion collisions in the presence of electromagnetic fields.","It is derived from the Black Hole Accretion Code (BHAC), which has been designed to model astrophysical processes in a general-relativistic magnetohydrodynamical description.","As the original Black Hole Accretion Code, BHAC-QGP benefits from the use of Adaptive Mesh Refinement (AMR), which allows us to dynamically adjust the resolution where necessary, and makes use of time-dependent Milne coordinates and the ultrarelativistic equation of state, $P = e/3$. We demonstrate that BHAC-QGP accurately passes a number of systematic and rigorous tests."],"url":"http://arxiv.org/abs/2403.08668v1","category":"hep-ph"}
{"created":"2024-03-13 16:16:20","title":"Self-Supervised Learning for Covariance Estimation","abstract":"We consider the use of deep learning for covariance estimation. We propose to globally learn a neural network that will then be applied locally at inference time. Leveraging recent advancements in self-supervised foundational models, we train the network without any labeling by simply masking different samples and learning to predict their covariance given their surrounding neighbors. The architecture is based on the popular attention mechanism. Its main advantage over classical methods is the automatic exploitation of global characteristics without any distributional assumptions or regularization. It can be pre-trained as a foundation model and then be repurposed for various downstream tasks, e.g., adaptive target detection in radar or hyperspectral imagery.","sentences":["We consider the use of deep learning for covariance estimation.","We propose to globally learn a neural network that will then be applied locally at inference time.","Leveraging recent advancements in self-supervised foundational models, we train the network without any labeling by simply masking different samples and learning to predict their covariance given their surrounding neighbors.","The architecture is based on the popular attention mechanism.","Its main advantage over classical methods is the automatic exploitation of global characteristics without any distributional assumptions or regularization.","It can be pre-trained as a foundation model and then be repurposed for various downstream tasks, e.g., adaptive target detection in radar or hyperspectral imagery."],"url":"http://arxiv.org/abs/2403.08662v1","category":"eess.SP"}
{"created":"2024-03-13 15:36:53","title":"Environment-Induced Information Scrambling Transition with Charge Conservations","abstract":"In generic closed quantum systems, the complexity of operators increases under time evolution governed by the Heisenberg equation, reflecting the scrambling of local quantum information. However, when systems interact with an external environment, the system-environment coupling allows operators to escape from the system, inducing a dynamical transition between the scrambling phase and the dissipative phase. This transition is known as the environment-induced information scrambling transition, originally proposed in Majorana fermion systems. In this work, we advance this dicovery by investigating the transition in charge-conserved systems with space-time randomness. We construct solvable Brownian Sachdev-Ye-Kitaev models of complex fermions coupled to an environment, enabling the analytical computation of operator growth. We determine the critical dissipation strength, which is proportional to $n(1-n)$ with $n$ being the density of the complex fermions, arising from the suppression in the quantum Lyapunov exponent due to the Pauli blockade in the scattering process. We further analyze the density dependence of maximally scrambled operators at late time. Our results shed light on the intriguing interplay between information scrambling, dissipation, and conservation laws.","sentences":["In generic closed quantum systems, the complexity of operators increases under time evolution governed by the Heisenberg equation, reflecting the scrambling of local quantum information.","However, when systems interact with an external environment, the system-environment coupling allows operators to escape from the system, inducing a dynamical transition between the scrambling phase and the dissipative phase.","This transition is known as the environment-induced information scrambling transition, originally proposed in Majorana fermion systems.","In this work, we advance this dicovery by investigating the transition in charge-conserved systems with space-time randomness.","We construct solvable Brownian Sachdev-Ye-Kitaev models of complex fermions coupled to an environment, enabling the analytical computation of operator growth.","We determine the critical dissipation strength, which is proportional to $n(1-n)$ with $n$ being the density of the complex fermions, arising from the suppression in the quantum Lyapunov exponent due to the Pauli blockade in the scattering process.","We further analyze the density dependence of maximally scrambled operators at late time.","Our results shed light on the intriguing interplay between information scrambling, dissipation, and conservation laws."],"url":"http://arxiv.org/abs/2403.08622v1","category":"quant-ph"}
{"created":"2024-03-13 15:21:14","title":"On the Convergence of Locally Adaptive and Scalable Diffusion-Based Sampling Methods for Deep Bayesian Neural Network Posteriors","abstract":"Achieving robust uncertainty quantification for deep neural networks represents an important requirement in many real-world applications of deep learning such as medical imaging where it is necessary to assess the reliability of a neural network's prediction. Bayesian neural networks are a promising approach for modeling uncertainties in deep neural networks. Unfortunately, generating samples from the posterior distribution of neural networks is a major challenge. One significant advance in that direction would be the incorporation of adaptive step sizes, similar to modern neural network optimizers, into Monte Carlo Markov chain sampling algorithms without significantly increasing computational demand. Over the past years, several papers have introduced sampling algorithms with claims that they achieve this property. However, do they indeed converge to the correct distribution? In this paper, we demonstrate that these methods can have a substantial bias in the distribution they sample, even in the limit of vanishing step sizes and at full batch size.","sentences":["Achieving robust uncertainty quantification for deep neural networks represents an important requirement in many real-world applications of deep learning such as medical imaging where it is necessary to assess the reliability of a neural network's prediction.","Bayesian neural networks are a promising approach for modeling uncertainties in deep neural networks.","Unfortunately, generating samples from the posterior distribution of neural networks is a major challenge.","One significant advance in that direction would be the incorporation of adaptive step sizes, similar to modern neural network optimizers, into Monte Carlo Markov chain sampling algorithms without significantly increasing computational demand.","Over the past years, several papers have introduced sampling algorithms with claims that they achieve this property.","However, do they indeed converge to the correct distribution?","In this paper, we demonstrate that these methods can have a substantial bias in the distribution they sample, even in the limit of vanishing step sizes and at full batch size."],"url":"http://arxiv.org/abs/2403.08609v2","category":"cs.LG"}
{"created":"2024-03-13 15:07:23","title":"Toward mapping turbulence in the intracluster medium III. Constraints on the turbulent power spectrum with Athena/X-IFU","abstract":"Context. Future X-ray observatories with high spectral resolution and imaging capabilities will enable measurements and mappings of emission line shifts in the intracluster medium (ICM). Such direct measurements can serve as unique probes of turbulent motions in the ICM. Determining the level and scales of turbulence will improve our understanding of the galaxy cluster dynamical evolution and assembly, together with a more precise evaluation of the non thermal support pressure budget. This will allow for more accurate constraints to be placed on the masses of galaxy clusters, among other potential benfits. Aims. In this view, we implemented the methods presented in the previous instalments of our work to characterize the turbulence in the ICM in a feasibility study with the X-IFU on board the future European X-ray observatory, Athena. Methods. From idealized mock observations of a toy model cluster, we reconstructed the second-order structure function built with the observed velocity field to constrain the turbulence. We carefully accounted for the various sources of errors to derive the most realistic and comprehensive error budget within the limits of our approach. With prior assumptions on the dissipation scale and power spectrum slope, we constrained the parameters of the turbulent power spectrum model through the use of MCMC sampling. Results. With favourable assumptions, we were able to retrieve the injection scale, velocity dispersion, and power spectrum slope, with 1sigma uncertainties better than ~15% of the input values. We demonstrated the efficiency of our carefully set framework to constrain the turbulence in the ICM from high-resolution X-ray spectroscopic observations, paving the way for more in-depth investigation of the optimal required observing strategy within a more restrictive observational setup with the future X-IFU instrument.","sentences":["Context.","Future X-ray observatories with high spectral resolution and imaging capabilities will enable measurements and mappings of emission line shifts in the intracluster medium (ICM).","Such direct measurements can serve as unique probes of turbulent motions in the ICM.","Determining the level and scales of turbulence will improve our understanding of the galaxy cluster dynamical evolution and assembly, together with a more precise evaluation of the non thermal support pressure budget.","This will allow for more accurate constraints to be placed on the masses of galaxy clusters, among other potential benfits.","Aims.","In this view, we implemented the methods presented in the previous instalments of our work to characterize the turbulence in the ICM in a feasibility study with the X-IFU on board the future European X-ray observatory, Athena. Methods.","From idealized mock observations of a toy model cluster, we reconstructed the second-order structure function built with the observed velocity field to constrain the turbulence.","We carefully accounted for the various sources of errors to derive the most realistic and comprehensive error budget within the limits of our approach.","With prior assumptions on the dissipation scale and power spectrum slope, we constrained the parameters of the turbulent power spectrum model through the use of MCMC sampling.","Results.","With favourable assumptions, we were able to retrieve the injection scale, velocity dispersion, and power spectrum slope, with 1sigma uncertainties better than ~15% of the input values.","We demonstrated the efficiency of our carefully set framework to constrain the turbulence in the ICM from high-resolution X-ray spectroscopic observations, paving the way for more in-depth investigation of the optimal required observing strategy within a more restrictive observational setup with the future X-IFU instrument."],"url":"http://arxiv.org/abs/2403.08601v1","category":"astro-ph.CO"}
{"created":"2024-03-13 15:03:50","title":"Adaptive morphing of wing and tail for stable, resilient, and energy-efficient flight of avian-informed drones","abstract":"Avian-informed drones feature morphing wing and tail surfaces, enhancing agility and adaptability in flight. Despite their large potential, realising their full capabilities remains challenging due to the lack of generalized control strategies accommodating their large degrees of freedom and cross-coupling effects between their control surfaces. Here we propose a new body-rate controller for avian-informed drones that uses all available actuators to control the motion of the drone. The method exhibits robustness against physical perturbations, turbulent airflow, and even loss of certain actuators mid-flight. Furthermore, wing and tail morphing is leveraged to enhance energy efficiency at 8m/s, 10m/s and 12m/s using in-flight Bayesian optimization. The resulting morphing configurations yield significant gains across all three speeds of up to 11.5% compared to non-morphing configurations and display a strong resemblance to avian flight at different speeds. This research lays the groundwork for the development of autonomous avian-informed drones that operate under diverse wind conditions, emphasizing the role of morphing in improving energy efficiency.","sentences":["Avian-informed drones feature morphing wing and tail surfaces, enhancing agility and adaptability in flight.","Despite their large potential, realising their full capabilities remains challenging due to the lack of generalized control strategies accommodating their large degrees of freedom and cross-coupling effects between their control surfaces.","Here we propose a new body-rate controller for avian-informed drones that uses all available actuators to control the motion of the drone.","The method exhibits robustness against physical perturbations, turbulent airflow, and even loss of certain actuators mid-flight.","Furthermore, wing and tail morphing is leveraged to enhance energy efficiency at 8m/s, 10m/s and 12m/s using in-flight Bayesian optimization.","The resulting morphing configurations yield significant gains across all three speeds of up to 11.5% compared to non-morphing configurations and display a strong resemblance to avian flight at different speeds.","This research lays the groundwork for the development of autonomous avian-informed drones that operate under diverse wind conditions, emphasizing the role of morphing in improving energy efficiency."],"url":"http://arxiv.org/abs/2403.08598v1","category":"cs.RO"}
{"created":"2024-03-13 14:29:09","title":"System-bath correlations and finite-time operation enhance the efficiency of a dissipative quantum battery","abstract":"The reduced state of a small system strongly coupled to a thermal bath may be athermal and used as a small battery once disconnected. If the disconnecting process is too slow, the coupling between the battery and the bath weakens, and at some point, the battery will be in a thermal state that can not be used as a battery. Thus, the unitarily extractable energy (a.k.a ergotropy) decreases with the disconnection time. The work required to disconnect the battery also depends on the disconnection time. We study the efficiency of this battery, defined as the ratio between the ergotropy to the work cost of disconnecting and connecting the battery back to the bath to close the cycle, as a function of the disconnecting time in the Caldeira-Leggett model of a quantum battery. We consider two scenarios. In the first scenario, we assume that the discharged battery is uncorrelated to the bath at the connecting time and find that the efficiency peaks at an optimal disconnecting time. In the second scenario, the discharged battery is correlated to the bath, and find that the optimal efficiency corresponds to an instantaneous disconnection. On top of these results, we analyze various thermodynamic quantities for these Caldeira-Leggett quantum batteries that allow us to express the first and second laws of thermodynamics in the mentioned cycles in simple form despite the system-bath initial correlations and strong coupling regime of the working device.","sentences":["The reduced state of a small system strongly coupled to a thermal bath may be athermal and used as a small battery once disconnected.","If the disconnecting process is too slow, the coupling between the battery and the bath weakens, and at some point, the battery will be in a thermal state that can not be used as a battery.","Thus, the unitarily extractable energy (a.k.a ergotropy) decreases with the disconnection time.","The work required to disconnect the battery also depends on the disconnection time.","We study the efficiency of this battery, defined as the ratio between the ergotropy to the work cost of disconnecting and connecting the battery back to the bath to close the cycle, as a function of the disconnecting time in the Caldeira-Leggett model of a quantum battery.","We consider two scenarios.","In the first scenario, we assume that the discharged battery is uncorrelated to the bath at the connecting time and find that the efficiency peaks at an optimal disconnecting time.","In the second scenario, the discharged battery is correlated to the bath, and find that the optimal efficiency corresponds to an instantaneous disconnection.","On top of these results, we analyze various thermodynamic quantities for these Caldeira-Leggett quantum batteries that allow us to express the first and second laws of thermodynamics in the mentioned cycles in simple form despite the system-bath initial correlations and strong coupling regime of the working device."],"url":"http://arxiv.org/abs/2403.08573v1","category":"quant-ph"}
{"created":"2024-03-13 14:24:09","title":"Consistent Prompting for Rehearsal-Free Continual Learning","abstract":"Continual learning empowers models to adapt autonomously to the ever-changing environment or data streams without forgetting old knowledge. Prompt-based approaches are built on frozen pre-trained models to learn the task-specific prompts and classifiers efficiently. Existing prompt-based methods are inconsistent between training and testing, limiting their effectiveness. Two types of inconsistency are revealed. Test predictions are made from all classifiers while training only focuses on the current task classifier without holistic alignment, leading to Classifier inconsistency. Prompt inconsistency indicates that the prompt selected during testing may not correspond to the one associated with this task during training. In this paper, we propose a novel prompt-based method, Consistent Prompting (CPrompt), for more aligned training and testing. Specifically, all existing classifiers are exposed to prompt training, resulting in classifier consistency learning. In addition, prompt consistency learning is proposed to enhance prediction robustness and boost prompt selection accuracy. Our Consistent Prompting surpasses its prompt-based counterparts and achieves state-of-the-art performance on multiple continual learning benchmarks. Detailed analysis shows that improvements come from more consistent training and testing.","sentences":["Continual learning empowers models to adapt autonomously to the ever-changing environment or data streams without forgetting old knowledge.","Prompt-based approaches are built on frozen pre-trained models to learn the task-specific prompts and classifiers efficiently.","Existing prompt-based methods are inconsistent between training and testing, limiting their effectiveness.","Two types of inconsistency are revealed.","Test predictions are made from all classifiers while training only focuses on the current task classifier without holistic alignment, leading to Classifier inconsistency.","Prompt inconsistency indicates that the prompt selected during testing may not correspond to the one associated with this task during training.","In this paper, we propose a novel prompt-based method, Consistent Prompting (CPrompt), for more aligned training and testing.","Specifically, all existing classifiers are exposed to prompt training, resulting in classifier consistency learning.","In addition, prompt consistency learning is proposed to enhance prediction robustness and boost prompt selection accuracy.","Our Consistent Prompting surpasses its prompt-based counterparts and achieves state-of-the-art performance on multiple continual learning benchmarks.","Detailed analysis shows that improvements come from more consistent training and testing."],"url":"http://arxiv.org/abs/2403.08568v2","category":"cs.CV"}
{"created":"2024-03-13 14:00:18","title":"Wet TinyML: Chemical Neural Network Using Gene Regulation and Cell Plasticity","abstract":"In our earlier work, we introduced the concept of Gene Regulatory Neural Network (GRNN), which utilizes natural neural network-like structures inherent in biological cells to perform computing tasks using chemical inputs. We define this form of chemical-based neural network as Wet TinyML. The GRNN structures are based on the gene regulatory network and have weights associated with each link based on the estimated interactions between the genes. The GRNNs can be used for conventional computing by employing an application-based search process similar to the Network Architecture Search. This study advances this concept by incorporating cell plasticity, to further exploit natural cell's adaptability, in order to diversify the GRNN search that can match larger spectrum as well as dynamic computing tasks. As an example application, we show that through the directed cell plasticity, we can extract the mathematical regression evolution enabling it to match to dynamic system applications. We also conduct energy analysis by comparing the chemical energy of the GRNN to its silicon counterpart, where this analysis includes both artificial neural network algorithms executed on von Neumann architecture as well as neuromorphic processors. The concept of Wet TinyML can pave the way for the new emergence of chemical-based, energy-efficient and miniature Biological AI.","sentences":["In our earlier work, we introduced the concept of Gene Regulatory Neural Network (GRNN), which utilizes natural neural network-like structures inherent in biological cells to perform computing tasks using chemical inputs.","We define this form of chemical-based neural network as Wet TinyML.","The GRNN structures are based on the gene regulatory network and have weights associated with each link based on the estimated interactions between the genes.","The GRNNs can be used for conventional computing by employing an application-based search process similar to the Network Architecture Search.","This study advances this concept by incorporating cell plasticity, to further exploit natural cell's adaptability, in order to diversify the GRNN search that can match larger spectrum as well as dynamic computing tasks.","As an example application, we show that through the directed cell plasticity, we can extract the mathematical regression evolution enabling it to match to dynamic system applications.","We also conduct energy analysis by comparing the chemical energy of the GRNN to its silicon counterpart, where this analysis includes both artificial neural network algorithms executed on von Neumann architecture as well as neuromorphic processors.","The concept of Wet TinyML can pave the way for the new emergence of chemical-based, energy-efficient and miniature Biological AI."],"url":"http://arxiv.org/abs/2403.08549v1","category":"cs.NE"}
{"created":"2024-03-13 13:33:35","title":"From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning","abstract":"In this work we propose an audio recording segmentation method based on an adaptive change point detection (A-CPD) for machine guided weak label annotation of audio recording segments. The goal is to maximize the amount of information gained about the temporal activation's of the target sounds. For each unlabeled audio recording, we use a prediction model to derive a probability curve used to guide annotation. The prediction model is initially pre-trained on available annotated sound event data with classes that are disjoint from the classes in the unlabeled dataset. The prediction model then gradually adapts to the annotations provided by the annotator in an active learning loop. The queries used to guide the weak label annotator towards strong labels are derived using change point detection on these probabilities. We show that it is possible to derive strong labels of high quality even with a limited annotation budget, and show favorable results for A-CPD when compared to two baseline query strategies.","sentences":["In this work we propose an audio recording segmentation method based on an adaptive change point detection (A-CPD) for machine guided weak label annotation of audio recording segments.","The goal is to maximize the amount of information gained about the temporal activation's of the target sounds.","For each unlabeled audio recording, we use a prediction model to derive a probability curve used to guide annotation.","The prediction model is initially pre-trained on available annotated sound event data with classes that are disjoint from the classes in the unlabeled dataset.","The prediction model then gradually adapts to the annotations provided by the annotator in an active learning loop.","The queries used to guide the weak label annotator towards strong labels are derived using change point detection on these probabilities.","We show that it is possible to derive strong labels of high quality even with a limited annotation budget, and show favorable results for A-CPD when compared to two baseline query strategies."],"url":"http://arxiv.org/abs/2403.08525v1","category":"cs.SD"}
{"created":"2024-03-13 13:28:11","title":"Colloidal Homogenisation for the Hydrodynamics of Nematic Liquid Crystals","abstract":"This paper analytically explores a simplified model for the hydrodynamics of nematic liquid crystal colloids. We integrate a Stokes equation for the velocity field with a Ginzburg-Landau transported heat flow for the director field. The study focuses on a bounded spatial domain containing periodically distributed colloidal particles, which impose no-anchoring conditions on the nematic liquid crystal. By progressively reducing the particle size to zero and simultaneously increasing the number of particles, we delve into the associated homogenisation problem. Our analysis uncovers a form of decoupling where the velocity field asymptotically satisfies a Darcy equation, independent of the director, while the director follows a gradient flow, unaffected by the velocity field. One of the most intricate aspects of the homogenisation process is the absence of an extension operator for the director field that preserves the uniform estimates related to the system's energy. We address this challenge with a novel variation of the Aubin-Lions lemma, specifically adapted for homogenisation problems.","sentences":["This paper analytically explores a simplified model for the hydrodynamics of nematic liquid crystal colloids.","We integrate a Stokes equation for the velocity field with a Ginzburg-Landau transported heat flow for the director field.","The study focuses on a bounded spatial domain containing periodically distributed colloidal particles, which impose no-anchoring conditions on the nematic liquid crystal.","By progressively reducing the particle size to zero and simultaneously increasing the number of particles, we delve into the associated homogenisation problem.","Our analysis uncovers a form of decoupling where the velocity field asymptotically satisfies a Darcy equation, independent of the director, while the director follows a gradient flow, unaffected by the velocity field.","One of the most intricate aspects of the homogenisation process is the absence of an extension operator for the director field that preserves the uniform estimates related to the system's energy.","We address this challenge with a novel variation of the Aubin-Lions lemma, specifically adapted for homogenisation problems."],"url":"http://arxiv.org/abs/2403.08520v1","category":"math.AP"}
{"created":"2024-03-13 13:24:24","title":"Plotinus: A Satellite Internet Digital Twin System","abstract":"The development of integrated space-air-ground network (SAGIN) requires sophisticated satellite Internet emulation tools that can handle complex, dynamic topologies and offer in-depth analysis. Existing emulation platforms struggle with challenges like the need for detailed implementation across all network layers, real-time response times, and the ability to scale. Plotinus, a new digital twin system based on microservices for satellite Internet emulation, aims to solve these problems. It features a modular design, allowing for easy replacement of the physical layer to emulate different aerial vehicles and analyze channel interference. It also enables the replacement of path computation methods to simplify testing and deploying algorithms. In particular, Plotinus allows for real-time emulation with live network traffic, enhancing the realism of network models. Evaluation result shows that Plotinus's effective emulation of dynamic satellite networks with real-world devices. Its adaptability for various communication models and algorithm testing highlights Plotinus's role as a vital tool for developing and analyzing SAGIN systems, offering a scalable, real-time response, and flexible digital twin system.","sentences":["The development of integrated space-air-ground network (SAGIN) requires sophisticated satellite Internet emulation tools that can handle complex, dynamic topologies and offer in-depth analysis.","Existing emulation platforms struggle with challenges like the need for detailed implementation across all network layers, real-time response times, and the ability to scale.","Plotinus, a new digital twin system based on microservices for satellite Internet emulation, aims to solve these problems.","It features a modular design, allowing for easy replacement of the physical layer to emulate different aerial vehicles and analyze channel interference.","It also enables the replacement of path computation methods to simplify testing and deploying algorithms.","In particular, Plotinus allows for real-time emulation with live network traffic, enhancing the realism of network models.","Evaluation result shows that Plotinus's effective emulation of dynamic satellite networks with real-world devices.","Its adaptability for various communication models and algorithm testing highlights Plotinus's role as a vital tool for developing and analyzing SAGIN systems, offering a scalable, real-time response, and flexible digital twin system."],"url":"http://arxiv.org/abs/2403.08515v1","category":"cs.NI"}
{"created":"2024-03-13 13:23:37","title":"3D Spectrum Mapping and Reconstruction under Multi-Radiation Source Scenarios","abstract":"Spectrum map construction, which is crucial in cognitive radio (CR) system, visualizes the invisible space of the electromagnetic spectrum for spectrum-resource management and allocation. Traditional reconstruction methods are generally for two-dimensional (2D) spectrum map and driven by abundant sampling data. In this paper, we propose a data-model-knowledge-driven reconstruction scheme to construct the three-dimensional (3D) spectrum map under multi-radiation source scenarios. We firstly design a maximum and minimum path loss difference (MMPLD) clustering algorithm to detect the number of radiation sources in a 3D space. Then, we develop a joint location-power estimation method based on the heuristic population evolutionary optimization algorithm. Considering the variation of electromagnetic environment, we self-learn the path loss (PL) model based on the sampling data. Finally, the 3D spectrum is reconstructed according to the self-learned PL model and the extracted knowledge of radiation sources. Simulations show that the proposed 3D spectrum map reconstruction scheme not only has splendid adaptability to the environment, but also achieves high spectrum construction accuracy even when the sampling rate is very low.","sentences":["Spectrum map construction, which is crucial in cognitive radio (CR) system, visualizes the invisible space of the electromagnetic spectrum for spectrum-resource management and allocation.","Traditional reconstruction methods are generally for two-dimensional (2D) spectrum map and driven by abundant sampling data.","In this paper, we propose a data-model-knowledge-driven reconstruction scheme to construct the three-dimensional (3D) spectrum map under multi-radiation source scenarios.","We firstly design a maximum and minimum path loss difference (MMPLD) clustering algorithm to detect the number of radiation sources in a 3D space.","Then, we develop a joint location-power estimation method based on the heuristic population evolutionary optimization algorithm.","Considering the variation of electromagnetic environment, we self-learn the path loss (PL) model based on the sampling data.","Finally, the 3D spectrum is reconstructed according to the self-learned PL model and the extracted knowledge of radiation sources.","Simulations show that the proposed 3D spectrum map reconstruction scheme not only has splendid adaptability to the environment, but also achieves high spectrum construction accuracy even when the sampling rate is very low."],"url":"http://arxiv.org/abs/2403.08513v1","category":"eess.SP"}
{"created":"2024-03-13 12:46:03","title":"Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts","abstract":"Conventional wisdom suggests parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-domain (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient finetuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OOD tasks in both zero-shot and gradient-based adaptation settings. In addition, we provide a thorough analysis of the superiority of learned over hand-designed sparsity patterns for sparse expert methods and the pivotal importance of the sparsity level in balancing between in-domain and out-of-domain generalization. Our code is publicly available.","sentences":["Conventional wisdom suggests parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning.","In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-domain (OOD) tasks.","In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task.","SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient finetuning.","We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OOD tasks in both zero-shot and gradient-based adaptation settings.","In addition, we provide a thorough analysis of the superiority of learned over hand-designed sparsity patterns for sparse expert methods and the pivotal importance of the sparsity level in balancing between in-domain and out-of-domain generalization.","Our code is publicly available."],"url":"http://arxiv.org/abs/2403.08477v1","category":"cs.CV"}
{"created":"2024-03-13 12:40:32","title":"Emergent Continuous Time Crystal in Dissipative Quantum Spin System without Driving","abstract":"Time crystal, a nonequilibrium phenomenon extending spontaneous symmetry breaking into the temporal dimension, holds fundamental significance in understanding quantum many-body physics. In this work, we explore the nonequilibrium phase diagram of a two-dimensional dissipative Heisenberg spin system in the absence of explicit driving. We numerically identify the emergence of novel nonstationary oscillatory states by analyzing the spin dynamics. These states are categorized as limit cycle and chaos based on the Lyapunov exponent. Remarkably, the observed limit cycle behavior represents a continuous time crystal (CTC), spontaneously breaking the continuous time translation symmetry of the system. We further confirm those oscillatory behaviors by studying the stability against local perturbations applied to the system. Finally, we investigate the robustness of the emergent CTC by introducing isotropic Gaussian-type white noise into the interactions. This study provides many insights into the intricate interplay between dissipation-induced decay processes and interaction-induced spin precession, deepening our understanding of dissipative quantum many-body systems.","sentences":["Time crystal, a nonequilibrium phenomenon extending spontaneous symmetry breaking into the temporal dimension, holds fundamental significance in understanding quantum many-body physics.","In this work, we explore the nonequilibrium phase diagram of a two-dimensional dissipative Heisenberg spin system in the absence of explicit driving.","We numerically identify the emergence of novel nonstationary oscillatory states by analyzing the spin dynamics.","These states are categorized as limit cycle and chaos based on the Lyapunov exponent.","Remarkably, the observed limit cycle behavior represents a continuous time crystal (CTC), spontaneously breaking the continuous time translation symmetry of the system.","We further confirm those oscillatory behaviors by studying the stability against local perturbations applied to the system.","Finally, we investigate the robustness of the emergent CTC by introducing isotropic Gaussian-type white noise into the interactions.","This study provides many insights into the intricate interplay between dissipation-induced decay processes and interaction-induced spin precession, deepening our understanding of dissipative quantum many-body systems."],"url":"http://arxiv.org/abs/2403.08476v1","category":"quant-ph"}
{"created":"2024-03-13 12:39:02","title":"Negative Wigner function by decaying interaction from equilibrium","abstract":"Bosonic systems with negative Wigner function superposition states are fundamentally witnessing nonlinear quantum dynamics beyond linearized systems and, recently, have become essential resources of quantum technology with many applications. Typically, they appear due to sophisticated combination of external drives, nonlinear control, measurements or strong nonlinear dissipation of subsystems to an environment. Here, we propose a conceptually different and more autonomous way to obtain such states, avoiding these ingredients, using purely sudden interaction decay in the paradigmatic interacting qubit-oscillator system weakly coupled to bath at thermal equilibrium in a low-temperature limit. We demonstrate simultaneously detectable unconditional negative Wigner function and quantum coherence and their qualitative enhancement employing more qubits.","sentences":["Bosonic systems with negative Wigner function superposition states are fundamentally witnessing nonlinear quantum dynamics beyond linearized systems and, recently, have become essential resources of quantum technology with many applications.","Typically, they appear due to sophisticated combination of external drives, nonlinear control, measurements or strong nonlinear dissipation of subsystems to an environment.","Here, we propose a conceptually different and more autonomous way to obtain such states, avoiding these ingredients, using purely sudden interaction decay in the paradigmatic interacting qubit-oscillator system weakly coupled to bath at thermal equilibrium in a low-temperature limit.","We demonstrate simultaneously detectable unconditional negative Wigner function and quantum coherence and their qualitative enhancement employing more qubits."],"url":"http://arxiv.org/abs/2403.08474v1","category":"quant-ph"}
{"created":"2024-03-13 12:38:43","title":"Mechanism Design Optimization through CAD-Based Bayesian Optimization and Quantified Constraints","abstract":"This research delves into optimizing mechanism design, with an emphasis on the energy efficiency and the expansive design possibilities of reciprocating mechanisms. It investigates how to efficiently integrate Computer-Aided Design (CAD) simulations with Bayesian Optimization (BO) and a constrained design space, aiming to enhance the design optimization process beyond the confines of traditional kinematic and dynamic analysis. The study sets out to create a novel optimization framework that merges CAD simulations with a BO strategy. Initially, the feasibility of a mechanism design is assessed through CAD-motion simulations, which gauge its practicality. Upon deeming a design feasible, an evaluation via CAD-motion simulations is conducted to ascertain the objective value. This research proposes utilizing non-parametric Gaussian processes for crafting a surrogate model of the objective function, considering the design space's static and dynamic constraints. The findings reveal that the introduced CAD-based Bayesian Optimization framework adeptly identifies optimal design parameters that minimize root mean square (RMS) torque while complying with predetermined constraints. This method markedly diminishes the complexity seen in analytical approaches, rendering it adaptable to intricate mechanisms and practicable for machine builders. The framework evidences the utility of integrating constraints in the optimization process, showing promise for attaining globally optimal designs efficiently. A case study on an emergency ventilator, with three design parameters, demonstrates a 71% RMS torque reduction after 255 CAD-based evaluations, underscoring the approach's effectiveness and its potential for refining mechanism design optimization.","sentences":["This research delves into optimizing mechanism design, with an emphasis on the energy efficiency and the expansive design possibilities of reciprocating mechanisms.","It investigates how to efficiently integrate Computer-Aided Design (CAD) simulations with Bayesian Optimization (BO) and a constrained design space, aiming to enhance the design optimization process beyond the confines of traditional kinematic and dynamic analysis.","The study sets out to create a novel optimization framework that merges CAD simulations with a BO strategy.","Initially, the feasibility of a mechanism design is assessed through CAD-motion simulations, which gauge its practicality.","Upon deeming a design feasible, an evaluation via CAD-motion simulations is conducted to ascertain the objective value.","This research proposes utilizing non-parametric Gaussian processes for crafting a surrogate model of the objective function, considering the design space's static and dynamic constraints.","The findings reveal that the introduced CAD-based Bayesian Optimization framework adeptly identifies optimal design parameters that minimize root mean square (RMS) torque while complying with predetermined constraints.","This method markedly diminishes the complexity seen in analytical approaches, rendering it adaptable to intricate mechanisms and practicable for machine builders.","The framework evidences the utility of integrating constraints in the optimization process, showing promise for attaining globally optimal designs efficiently.","A case study on an emergency ventilator, with three design parameters, demonstrates a 71% RMS torque reduction after 255 CAD-based evaluations, underscoring the approach's effectiveness and its potential for refining mechanism design optimization."],"url":"http://arxiv.org/abs/2403.08473v1","category":"eess.SY"}
{"created":"2024-03-13 12:07:14","title":"Better Fit: Accommodate Variations in Clothing Types for Virtual Try-on","abstract":"Image-based virtual try-on aims to transfer target in-shop clothing to a dressed model image, the objectives of which are totally taking off original clothing while preserving the contents outside of the try-on area, naturally wearing target clothing and correctly inpainting the gap between target clothing and original clothing. Tremendous efforts have been made to facilitate this popular research area, but cannot keep the type of target clothing with the try-on area affected by original clothing. In this paper, we focus on the unpaired virtual try-on situation where target clothing and original clothing on the model are different, i.e., the practical scenario. To break the correlation between the try-on area and the original clothing and make the model learn the correct information to inpaint, we propose an adaptive mask training paradigm that dynamically adjusts training masks. It not only improves the alignment and fit of clothing but also significantly enhances the fidelity of virtual try-on experience. Furthermore, we for the first time propose two metrics for unpaired try-on evaluation, the Semantic-Densepose-Ratio (SDR) and Skeleton-LPIPS (S-LPIPS), to evaluate the correctness of clothing type and the accuracy of clothing texture. For unpaired try-on validation, we construct a comprehensive cross-try-on benchmark (Cross-27) with distinctive clothing items and model physiques, covering a broad try-on scenarios. Experiments demonstrate the effectiveness of the proposed methods, contributing to the advancement of virtual try-on technology and offering new insights and tools for future research in the field. The code, model and benchmark will be publicly released.","sentences":["Image-based virtual try-on aims to transfer target in-shop clothing to a dressed model image, the objectives of which are totally taking off original clothing while preserving the contents outside of the try-on area, naturally wearing target clothing and correctly inpainting the gap between target clothing and original clothing.","Tremendous efforts have been made to facilitate this popular research area, but cannot keep the type of target clothing with the try-on area affected by original clothing.","In this paper, we focus on the unpaired virtual try-on situation where target clothing and original clothing on the model are different, i.e., the practical scenario.","To break the correlation between the try-on area and the original clothing and make the model learn the correct information to inpaint, we propose an adaptive mask training paradigm that dynamically adjusts training masks.","It not only improves the alignment and fit of clothing but also significantly enhances the fidelity of virtual try-on experience.","Furthermore, we for the first time propose two metrics for unpaired try-on evaluation, the Semantic-Densepose-Ratio (SDR) and Skeleton-LPIPS (S-LPIPS), to evaluate the correctness of clothing type and the accuracy of clothing texture.","For unpaired try-on validation, we construct a comprehensive cross-try-on benchmark (Cross-27) with distinctive clothing items and model physiques, covering a broad try-on scenarios.","Experiments demonstrate the effectiveness of the proposed methods, contributing to the advancement of virtual try-on technology and offering new insights and tools for future research in the field.","The code, model and benchmark will be publicly released."],"url":"http://arxiv.org/abs/2403.08453v1","category":"cs.CV"}
{"created":"2024-03-13 12:00:48","title":"Generating Synthetic Computed Tomography for Radiotherapy: SynthRAD2023 Challenge Report","abstract":"Radiation therapy plays a crucial role in cancer treatment, necessitating precise delivery of radiation to tumors while sparing healthy tissues over multiple days. Computed tomography (CT) is integral for treatment planning, offering electron density data crucial for accurate dose calculations. However, accurately representing patient anatomy is challenging, especially in adaptive radiotherapy, where CT is not acquired daily. Magnetic resonance imaging (MRI) provides superior soft-tissue contrast. Still, it lacks electron density information while cone beam CT (CBCT) lacks direct electron density calibration and is mainly used for patient positioning. Adopting MRI-only or CBCT-based adaptive radiotherapy eliminates the need for CT planning but presents challenges. Synthetic CT (sCT) generation techniques aim to address these challenges by using image synthesis to bridge the gap between MRI, CBCT, and CT. The SynthRAD2023 challenge was organized to compare synthetic CT generation methods using multi-center ground truth data from 1080 patients, divided into two tasks: 1) MRI-to-CT and 2) CBCT-to-CT. The evaluation included image similarity and dose-based metrics from proton and photon plans. The challenge attracted significant participation, with 617 registrations and 22/17 valid submissions for tasks 1/2. Top-performing teams achieved high structural similarity indices (>0.87/0.90) and gamma pass rates for photon (>98.1%/99.0%) and proton (>99.0%/97.3%) plans. However, no significant correlation was found between image similarity metrics and dose accuracy, emphasizing the need for dose evaluation when assessing the clinical applicability of sCT. SynthRAD2023 facilitated the investigation and benchmarking of sCT generation techniques, providing insights for developing MRI-only and CBCT-based adaptive radiotherapy.","sentences":["Radiation therapy plays a crucial role in cancer treatment, necessitating precise delivery of radiation to tumors while sparing healthy tissues over multiple days.","Computed tomography (CT) is integral for treatment planning, offering electron density data crucial for accurate dose calculations.","However, accurately representing patient anatomy is challenging, especially in adaptive radiotherapy, where CT is not acquired daily.","Magnetic resonance imaging (MRI) provides superior soft-tissue contrast.","Still, it lacks electron density information while cone beam CT (CBCT) lacks direct electron density calibration and is mainly used for patient positioning.","Adopting MRI-only or CBCT-based adaptive radiotherapy eliminates the need for CT planning but presents challenges.","Synthetic CT (sCT) generation techniques aim to address these challenges by using image synthesis to bridge the gap between MRI, CBCT, and CT.","The SynthRAD2023 challenge was organized to compare synthetic CT generation methods using multi-center ground truth data from 1080 patients, divided into two tasks: 1) MRI-to-CT and 2) CBCT-to-CT.","The evaluation included image similarity and dose-based metrics from proton and photon plans.","The challenge attracted significant participation, with 617 registrations and 22/17 valid submissions for tasks 1/2.","Top-performing teams achieved high structural similarity indices (>0.87/0.90) and gamma pass rates for photon (>98.1%/99.0%) and proton (>99.0%/97.3%) plans.","However, no significant correlation was found between image similarity metrics and dose accuracy, emphasizing the need for dose evaluation when assessing the clinical applicability of sCT.","SynthRAD2023 facilitated the investigation and benchmarking of sCT generation techniques, providing insights for developing MRI-only and CBCT-based adaptive radiotherapy."],"url":"http://arxiv.org/abs/2403.08447v1","category":"physics.med-ph"}
{"created":"2024-03-13 11:54:25","title":"Stabilizer ground states: theory, algorithms and applications","abstract":"Stabilizer states have been commonly utilized in quantum information, quantum error correction, and quantum circuit simulation due to their simple mathematical structure. In this work, we apply stabilizer states to tackle quantum many-body problems and introduce the concept of stabilizer ground states. We present a simplified equivalent formalism for identifying stabilizer ground states of general Pauli Hamiltonians. Moreover, we also develop an exact and linear-scaled algorithm to obtain stabilizer ground states of 1D local Hamiltonians and thus free from discrete optimization. This proposed theoretical formalism and linear-scaled algorithm are not only applicable to finite-size systems, but also adaptable to infinite periodic systems. The scalability and efficiency of the algorithms are numerically benchmarked on different Hamiltonians. Finally, we demonstrate that stabilizer ground states can find various promising applications including better design on variational quantum algorithms, qualitative understanding of phase transitions, and cornerstones for more advanced ground state ansatzes.","sentences":["Stabilizer states have been commonly utilized in quantum information, quantum error correction, and quantum circuit simulation due to their simple mathematical structure.","In this work, we apply stabilizer states to tackle quantum many-body problems and introduce the concept of stabilizer ground states.","We present a simplified equivalent formalism for identifying stabilizer ground states of general Pauli Hamiltonians.","Moreover, we also develop an exact and linear-scaled algorithm to obtain stabilizer ground states of 1D local Hamiltonians and thus free from discrete optimization.","This proposed theoretical formalism and linear-scaled algorithm are not only applicable to finite-size systems, but also adaptable to infinite periodic systems.","The scalability and efficiency of the algorithms are numerically benchmarked on different Hamiltonians.","Finally, we demonstrate that stabilizer ground states can find various promising applications including better design on variational quantum algorithms, qualitative understanding of phase transitions, and cornerstones for more advanced ground state ansatzes."],"url":"http://arxiv.org/abs/2403.08441v1","category":"quant-ph"}
{"created":"2024-03-13 11:26:43","title":"DeepCSHAP: Utilizing Shapley Values to Explain Deep Complex-Valued Neural Networks","abstract":"Deep Neural Networks are widely used in academy as well as corporate and public applications, including safety critical applications such as health care and autonomous driving. The ability to explain their output is critical for safety reasons as well as acceptance among applicants. A multitude of methods have been proposed to explain real-valued neural networks. Recently, complex-valued neural networks have emerged as a new class of neural networks dealing with complex-valued input data without the necessity of projecting them onto $\\mathbb{R}^2$. This brings up the need to develop explanation algorithms for this kind of neural networks. In this paper we provide these developments. While we focus on adapting the widely used DeepSHAP algorithm to the complex domain, we also present versions of four gradient based explanation methods suitable for use in complex-valued neural networks. We evaluate the explanation quality of all presented algorithms and provide all of them as an open source library adaptable to most recent complex-valued neural network architectures.","sentences":["Deep Neural Networks are widely used in academy as well as corporate and public applications, including safety critical applications such as health care and autonomous driving.","The ability to explain their output is critical for safety reasons as well as acceptance among applicants.","A multitude of methods have been proposed to explain real-valued neural networks.","Recently, complex-valued neural networks have emerged as a new class of neural networks dealing with complex-valued input data without the necessity of projecting them onto $\\mathbb{R}^2$. This brings up the need to develop explanation algorithms for this kind of neural networks.","In this paper we provide these developments.","While we focus on adapting the widely used DeepSHAP algorithm to the complex domain, we also present versions of four gradient based explanation methods suitable for use in complex-valued neural networks.","We evaluate the explanation quality of all presented algorithms and provide all of them as an open source library adaptable to most recent complex-valued neural network architectures."],"url":"http://arxiv.org/abs/2403.08428v1","category":"cs.LG"}
{"created":"2024-03-13 10:58:55","title":"Causal Graph Neural Networks for Wildfire Danger Prediction","abstract":"Wildfire forecasting is notoriously hard due to the complex interplay of different factors such as weather conditions, vegetation types and human activities. Deep learning models show promise in dealing with this complexity by learning directly from data. However, to inform critical decision making, we argue that we need models that are right for the right reasons; that is, the implicit rules learned should be grounded by the underlying processes driving wildfires. In that direction, we propose integrating causality with Graph Neural Networks (GNNs) that explicitly model the causal mechanism among complex variables via graph learning. The causal adjacency matrix considers the synergistic effect among variables and removes the spurious links from highly correlated impacts. Our methodology's effectiveness is demonstrated through superior performance forecasting wildfire patterns in the European boreal and mediterranean biome. The gain is especially prominent in a highly imbalanced dataset, showcasing an enhanced robustness of the model to adapt to regime shifts in functional relationships. Furthermore, SHAP values from our trained model further enhance our understanding of the model's inner workings.","sentences":["Wildfire forecasting is notoriously hard due to the complex interplay of different factors such as weather conditions, vegetation types and human activities.","Deep learning models show promise in dealing with this complexity by learning directly from data.","However, to inform critical decision making, we argue that we need models that are right for the right reasons; that is, the implicit rules learned should be grounded by the underlying processes driving wildfires.","In that direction, we propose integrating causality with Graph Neural Networks (GNNs) that explicitly model the causal mechanism among complex variables via graph learning.","The causal adjacency matrix considers the synergistic effect among variables and removes the spurious links from highly correlated impacts.","Our methodology's effectiveness is demonstrated through superior performance forecasting wildfire patterns in the European boreal and mediterranean biome.","The gain is especially prominent in a highly imbalanced dataset, showcasing an enhanced robustness of the model to adapt to regime shifts in functional relationships.","Furthermore, SHAP values from our trained model further enhance our understanding of the model's inner workings."],"url":"http://arxiv.org/abs/2403.08414v1","category":"cs.LG"}
{"created":"2024-03-13 10:51:18","title":"Iterative Online Image Synthesis via Diffusion Model for Imbalanced Classification","abstract":"Accurate and robust classification of diseases is important for proper diagnosis and treatment. However, medical datasets often face challenges related to limited sample sizes and inherent imbalanced distributions, due to difficulties in data collection and variations in disease prevalence across different types. In this paper, we introduce an Iterative Online Image Synthesis (IOIS) framework to address the class imbalance problem in medical image classification. Our framework incorporates two key modules, namely Online Image Synthesis (OIS) and Accuracy Adaptive Sampling (AAS), which collectively target the imbalance classification issue at both the instance level and the class level. The OIS module alleviates the data insufficiency problem by generating representative samples tailored for online training of the classifier. On the other hand, the AAS module dynamically balances the synthesized samples among various classes, targeting those with low training accuracy. To evaluate the effectiveness of our proposed method in addressing imbalanced classification, we conduct experiments on the HAM10000 and APTOS datasets. The results obtained demonstrate the superiority of our approach over state-of-the-art methods as well as the effectiveness of each component. The source code will be released upon acceptance.","sentences":["Accurate and robust classification of diseases is important for proper diagnosis and treatment.","However, medical datasets often face challenges related to limited sample sizes and inherent imbalanced distributions, due to difficulties in data collection and variations in disease prevalence across different types.","In this paper, we introduce an Iterative Online Image Synthesis (IOIS) framework to address the class imbalance problem in medical image classification.","Our framework incorporates two key modules, namely Online Image Synthesis (OIS) and Accuracy Adaptive Sampling (AAS), which collectively target the imbalance classification issue at both the instance level and the class level.","The OIS module alleviates the data insufficiency problem by generating representative samples tailored for online training of the classifier.","On the other hand, the AAS module dynamically balances the synthesized samples among various classes, targeting those with low training accuracy.","To evaluate the effectiveness of our proposed method in addressing imbalanced classification, we conduct experiments on the HAM10000 and APTOS datasets.","The results obtained demonstrate the superiority of our approach over state-of-the-art methods as well as the effectiveness of each component.","The source code will be released upon acceptance."],"url":"http://arxiv.org/abs/2403.08407v1","category":"cs.CV"}
{"created":"2024-03-13 09:43:14","title":"A Generalized Framework with Adaptive Weighted Soft-Margin for Imbalanced SVM Classification","abstract":"Category imbalance is one of the most popular and important issues in the domain of classification. In this paper, we present a new generalized framework with Adaptive Weight function for soft-margin Weighted SVM (AW-WSVM), which aims to enhance the issue of imbalance and outlier sensitivity in standard support vector machine (SVM) for classifying two-class data. The weight coefficient is introduced into the unconstrained soft-margin support vector machines, and the sample weights are updated before each training. The Adaptive Weight function (AW function) is constructed from the distance between the samples and the decision hyperplane, assigning different weights to each sample. A weight update method is proposed, taking into account the proximity of the support vectors to the decision hyperplane. Before training, the weights of the corresponding samples are initialized according to different categories. Subsequently, the samples close to the decision hyperplane are identified and assigned more weights. At the same time, lower weights are assigned to samples that are far from the decision hyperplane. Furthermore, we also put forward an effective way to eliminate noise. To evaluate the strength of the proposed generalized framework, we conducted experiments on standard datasets and emotion classification datasets with different imbalanced ratios (IR). The experimental results prove that the proposed generalized framework outperforms in terms of accuracy, recall metrics and G-mean, validating the effectiveness of the weighted strategy provided in this paper in enhancing support vector machines.","sentences":["Category imbalance is one of the most popular and important issues in the domain of classification.","In this paper, we present a new generalized framework with Adaptive Weight function for soft-margin Weighted SVM (AW-WSVM), which aims to enhance the issue of imbalance and outlier sensitivity in standard support vector machine (SVM) for classifying two-class data.","The weight coefficient is introduced into the unconstrained soft-margin support vector machines, and the sample weights are updated before each training.","The Adaptive Weight function (AW function) is constructed from the distance between the samples and the decision hyperplane, assigning different weights to each sample.","A weight update method is proposed, taking into account the proximity of the support vectors to the decision hyperplane.","Before training, the weights of the corresponding samples are initialized according to different categories.","Subsequently, the samples close to the decision hyperplane are identified and assigned more weights.","At the same time, lower weights are assigned to samples that are far from the decision hyperplane.","Furthermore, we also put forward an effective way to eliminate noise.","To evaluate the strength of the proposed generalized framework, we conducted experiments on standard datasets and emotion classification datasets with different imbalanced ratios (IR).","The experimental results prove that the proposed generalized framework outperforms in terms of accuracy, recall metrics and G-mean, validating the effectiveness of the weighted strategy provided in this paper in enhancing support vector machines."],"url":"http://arxiv.org/abs/2403.08378v1","category":"cs.CV"}
{"created":"2024-03-13 09:30:31","title":"Inhomogeneous Floquet thermalization","abstract":"How a closed system thermalizes, especially in the absence of global conservation laws but in the presence of disorder and interactions, is one of the central questions in non-equilibrium statistical mechanics. We explore this for a disordered, periodically driven Ising chain. Our numerical results reveal inhomogeneous thermalization leading to a distribution of thermalization timescales within a single disordered sample, which we encode via a distribution of effective local temperatures. Using this, we find an excellent collapse $\\textit{without}$ $\\textit{any}$ $\\textit{fitting}$ $\\textit{parameters}$ of the local relaxation dynamics for the entire range of disorder values in the ergodic regime when adapting the disorder-averaged diagonal entanglement entropy as internal `time' of the system. This approach evidences a remarkably uniform parametrization of the dynamical many-body evolution of local temperature within the otherwise highly heterogeneous ergodic regime, independent of the strength of the disorder.","sentences":["How a closed system thermalizes, especially in the absence of global conservation laws but in the presence of disorder and interactions, is one of the central questions in non-equilibrium statistical mechanics.","We explore this for a disordered, periodically driven Ising chain.","Our numerical results reveal inhomogeneous thermalization leading to a distribution of thermalization timescales within a single disordered sample, which we encode via a distribution of effective local temperatures.","Using this, we find an excellent collapse $\\textit{without}$ $\\textit{any}$ $\\textit{fitting}$ $\\textit{parameters}$ of the local relaxation dynamics for the entire range of disorder values in the ergodic regime when adapting the disorder-averaged diagonal entanglement entropy as internal `time' of the system.","This approach evidences a remarkably uniform parametrization of the dynamical many-body evolution of local temperature within the otherwise highly heterogeneous ergodic regime, independent of the strength of the disorder."],"url":"http://arxiv.org/abs/2403.08369v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-13 09:24:59","title":"Decoupled Federated Learning on Long-Tailed and Non-IID data with Feature Statistics","abstract":"Federated learning is designed to enhance data security and privacy, but faces challenges when dealing with heterogeneous data in long-tailed and non-IID distributions. This paper explores an overlooked scenario where tail classes are sparsely distributed over a few clients, causing the models trained with these classes to have a lower probability of being selected during client aggregation, leading to slower convergence rates and poorer model performance. To address this issue, we propose a two-stage Decoupled Federated learning framework using Feature Statistics (DFL-FS). In the first stage, the server estimates the client's class coverage distributions through masked local feature statistics clustering to select models for aggregation to accelerate convergence and enhance feature learning without privacy leakage. In the second stage, DFL-FS employs federated feature regeneration based on global feature statistics and utilizes resampling and weighted covariance to calibrate the global classifier to enhance the model's adaptability to long-tailed data distributions. We conducted experiments on CIFAR10-LT and CIFAR100-LT datasets with various long-tailed rates. The results demonstrate that our method outperforms state-of-the-art methods in both accuracy and convergence rate.","sentences":["Federated learning is designed to enhance data security and privacy, but faces challenges when dealing with heterogeneous data in long-tailed and non-IID distributions.","This paper explores an overlooked scenario where tail classes are sparsely distributed over a few clients, causing the models trained with these classes to have a lower probability of being selected during client aggregation, leading to slower convergence rates and poorer model performance.","To address this issue, we propose a two-stage Decoupled Federated learning framework using Feature Statistics (DFL-FS).","In the first stage, the server estimates the client's class coverage distributions through masked local feature statistics clustering to select models for aggregation to accelerate convergence and enhance feature learning without privacy leakage.","In the second stage, DFL-FS employs federated feature regeneration based on global feature statistics and utilizes resampling and weighted covariance to calibrate the global classifier to enhance the model's adaptability to long-tailed data distributions.","We conducted experiments on CIFAR10-LT and CIFAR100-LT datasets with various long-tailed rates.","The results demonstrate that our method outperforms state-of-the-art methods in both accuracy and convergence rate."],"url":"http://arxiv.org/abs/2403.08364v1","category":"cs.LG"}
{"created":"2024-03-13 08:54:31","title":"CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model","abstract":"Instruction tuning represents a prevalent strategy employed by Multimodal Large Language Models (MLLMs) to align with human instructions and adapt to new tasks. Nevertheless, MLLMs encounter the challenge of adapting to users' evolving knowledge and demands. Therefore, how to retain existing skills while acquiring new knowledge needs to be investigated. In this paper, we present a comprehensive benchmark, namely Continual Instruction tuNing (CoIN), to assess existing MLLMs in the sequential instruction tuning paradigm. CoIN comprises 10 commonly used datasets spanning 8 task categories, ensuring a diverse range of instructions and tasks. Besides, the trained model is evaluated from two aspects: Instruction Following and General Knowledge, which assess the alignment with human intention and knowledge preserved for reasoning, respectively. Experiments on CoIN demonstrate that current powerful MLLMs still suffer catastrophic forgetting, and the failure in intention alignment assumes the main responsibility, instead of the knowledge forgetting. To this end, we introduce MoELoRA to MLLMs which is effective to retain the previous instruction alignment. Experimental results consistently illustrate the forgetting decreased from this method on CoIN.","sentences":["Instruction tuning represents a prevalent strategy employed by Multimodal Large Language Models (MLLMs) to align with human instructions and adapt to new tasks.","Nevertheless, MLLMs encounter the challenge of adapting to users' evolving knowledge and demands.","Therefore, how to retain existing skills while acquiring new knowledge needs to be investigated.","In this paper, we present a comprehensive benchmark, namely Continual Instruction tuNing (CoIN), to assess existing MLLMs in the sequential instruction tuning paradigm.","CoIN comprises 10 commonly used datasets spanning 8 task categories, ensuring a diverse range of instructions and tasks.","Besides, the trained model is evaluated from two aspects: Instruction Following and General Knowledge, which assess the alignment with human intention and knowledge preserved for reasoning, respectively.","Experiments on CoIN demonstrate that current powerful MLLMs still suffer catastrophic forgetting, and the failure in intention alignment assumes the main responsibility, instead of the knowledge forgetting.","To this end, we introduce MoELoRA to MLLMs which is effective to retain the previous instruction alignment.","Experimental results consistently illustrate the forgetting decreased from this method on CoIN."],"url":"http://arxiv.org/abs/2403.08350v1","category":"cs.CV"}
{"created":"2024-03-13 08:44:43","title":"Schr{\u00f6}dinger eigenfunctions sharing the same modulus and applications to the control of quantum systems","abstract":"In this paper we investigate when linearly independent eigenfunctions of the Schr\\''odinger operator may have the same modulus. General properties are established and the one-dimensional case is treated in full generality. The study is motivated by its application to the bilinear control of the Schr{\\\"o}dinger equation. By assuming that the potentials of interaction satisfy a saturation property and by adapting a strategy recently proposed by Duca and Nersesyan, we discuss when the system can be steered arbitrarily fast between energy levels. Extensions of the previous results to quantum graphs are finally presented.","sentences":["In this paper we investigate when linearly independent eigenfunctions of the Schr\\''odinger operator may have the same modulus.","General properties are established and the one-dimensional case is treated in full generality.","The study is motivated by its application to the bilinear control of the Schr{\\\"o}dinger equation.","By assuming that the potentials of interaction satisfy a saturation property and by adapting a strategy recently proposed by Duca and Nersesyan, we discuss when the system can be steered arbitrarily fast between energy levels.","Extensions of the previous results to quantum graphs are finally presented."],"url":"http://arxiv.org/abs/2403.08341v1","category":"math.OC"}
{"created":"2024-03-13 08:43:06","title":"MorphoGear: An UAV with Multi-Limb Morphogenetic Gear for Rough-Terrain Locomotion","abstract":"Robots able to run, fly, and grasp have a high potential to solve a wide scope of tasks and navigate in complex environments. Several mechatronic designs of such robots with adaptive morphologies are emerging. However, the task of landing on an uneven surface, traversing rough terrain, and manipulating objects still presents high challenges.   This paper introduces the design of a novel rotor UAV MorphoGear with morphogenetic gear and includes a description of the robot's mechanics, electronics, and control architecture, as well as walking behavior and an analysis of experimental results. MorphoGear is able to fly, walk on surfaces with several gaits, and grasp objects with four compatible robotic limbs. Robotic limbs with three degrees of freedom (DoFs) are used by this UAV as pedipulators when walking or flying and as manipulators when performing actions in the environment. We performed a locomotion analysis of the landing gear of the robot. Three types of robot gaits have been developed.   The experimental results revealed low crosstrack error of the most accurate gait (mean of 1.9 cm and max of 5.5 cm) and the ability of the drone to move with a 210 mm step length. Another type of robot gait also showed low crosstrack error (mean of 2.3 cm and max of 6.9 cm). The proposed MorphoGear system can potentially achieve a high scope of tasks in environmental surveying, delivery, and high-altitude operations.","sentences":["Robots able to run, fly, and grasp have a high potential to solve a wide scope of tasks and navigate in complex environments.","Several mechatronic designs of such robots with adaptive morphologies are emerging.","However, the task of landing on an uneven surface, traversing rough terrain, and manipulating objects still presents high challenges.   ","This paper introduces the design of a novel rotor UAV MorphoGear with morphogenetic gear and includes a description of the robot's mechanics, electronics, and control architecture, as well as walking behavior and an analysis of experimental results.","MorphoGear is able to fly, walk on surfaces with several gaits, and grasp objects with four compatible robotic limbs.","Robotic limbs with three degrees of freedom (DoFs) are used by this UAV as pedipulators when walking or flying and as manipulators when performing actions in the environment.","We performed a locomotion analysis of the landing gear of the robot.","Three types of robot gaits have been developed.   ","The experimental results revealed low crosstrack error of the most accurate gait (mean of 1.9 cm and max of 5.5 cm) and the ability of the drone to move with a 210 mm step length.","Another type of robot gait also showed low crosstrack error (mean of 2.3 cm and max of 6.9 cm).","The proposed MorphoGear system can potentially achieve a high scope of tasks in environmental surveying, delivery, and high-altitude operations."],"url":"http://arxiv.org/abs/2403.08340v1","category":"cs.RO"}
{"created":"2024-03-13 08:41:55","title":"LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments","abstract":"Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information. This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods. Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework. The findings from our simulations attest to the system's adeptness in adjusting to a multiplicity of traffic environments without the need for additional training. Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management. The related code is available at \\href{https://github.com/Traffic-Alpha/LLM-Assisted-Light}{https://github.com/Traffic-Alpha/LLM-Assisted-Light}.","sentences":["Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications.","Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor.","Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios.","In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties.","Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information.","This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods.","Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework.","The findings from our simulations attest to the system's adeptness in adjusting to a multiplicity of traffic environments without the need for additional training.","Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management.","The related code is available at \\href{https://github.com/Traffic-Alpha/LLM-Assisted-Light}{https://github.com/Traffic-Alpha/LLM-Assisted-Light}."],"url":"http://arxiv.org/abs/2403.08337v1","category":"eess.SY"}
{"created":"2024-03-13 08:00:07","title":"DrFER: Learning Disentangled Representations for 3D Facial Expression Recognition","abstract":"Facial Expression Recognition (FER) has consistently been a focal point in the field of facial analysis. In the context of existing methodologies for 3D FER or 2D+3D FER, the extraction of expression features often gets entangled with identity information, compromising the distinctiveness of these features. To tackle this challenge, we introduce the innovative DrFER method, which brings the concept of disentangled representation learning to the field of 3D FER. DrFER employs a dual-branch framework to effectively disentangle expression information from identity information. Diverging from prior disentanglement endeavors in the 3D facial domain, we have carefully reconfigured both the loss functions and network structure to make the overall framework adaptable to point cloud data. This adaptation enhances the capability of the framework in recognizing facial expressions, even in cases involving varying head poses. Extensive evaluations conducted on the BU-3DFE and Bosphorus datasets substantiate that DrFER surpasses the performance of other 3D FER methods.","sentences":["Facial Expression Recognition (FER) has consistently been a focal point in the field of facial analysis.","In the context of existing methodologies for 3D FER or 2D+3D FER, the extraction of expression features often gets entangled with identity information, compromising the distinctiveness of these features.","To tackle this challenge, we introduce the innovative DrFER method, which brings the concept of disentangled representation learning to the field of 3D FER.","DrFER employs a dual-branch framework to effectively disentangle expression information from identity information.","Diverging from prior disentanglement endeavors in the 3D facial domain, we have carefully reconfigured both the loss functions and network structure to make the overall framework adaptable to point cloud data.","This adaptation enhances the capability of the framework in recognizing facial expressions, even in cases involving varying head poses.","Extensive evaluations conducted on the BU-3DFE and Bosphorus datasets substantiate that DrFER surpasses the performance of other 3D FER methods."],"url":"http://arxiv.org/abs/2403.08318v1","category":"cs.CV"}
{"created":"2024-03-13 06:16:03","title":"On the conservation laws and the structure of the nonlinearity for SQG and its generalizations","abstract":"Using a new definition for the nonlinear term, we prove that all weak solutions to the SQG equation (and mSQG) conserve the angular momentum. This result is new for the weak solutions of [Resnick, '95] and rules out the possibility of anomalous dissipation of angular momentum. We also prove conservation of the Hamiltonian under conjecturally optimal assumptions, sharpening a well-known criterion of [Cheskidov-Constantin-Friedlander-Shvydkoy, '08]. Moreover, we show that our new estimate for the nonlinearity is optimal and that it characterizes the mSQG nonlinearity uniquely among active scalar nonlinearities with a scaling symmetry.","sentences":["Using a new definition for the nonlinear term, we prove that all weak solutions to the SQG equation (and mSQG) conserve the angular momentum.","This result is new for the weak solutions of [Resnick, '95] and rules out the possibility of anomalous dissipation of angular momentum.","We also prove conservation of the Hamiltonian under conjecturally optimal assumptions, sharpening a well-known criterion of [Cheskidov-Constantin-Friedlander-Shvydkoy, '08].","Moreover, we show that our new estimate for the nonlinearity is optimal and that it characterizes the mSQG nonlinearity uniquely among active scalar nonlinearities with a scaling symmetry."],"url":"http://arxiv.org/abs/2403.08279v1","category":"math.AP"}
{"created":"2024-03-13 06:14:17","title":"Point-to-set Principle and Constructive Dimension Faithfulness","abstract":"We introduce a constructive analogue of $\\Phi$-dimension, a notion of Hausdorff dimension developed using a restricted class of coverings of a set. A class of coverings $\\Phi$ is said to be \"faithful\" to Hausdorff dimension if the $\\Phi$-dimension and Hausdorff dimension coincide for every set.   We prove a Point-to-Set Principle for $\\Phi$-dimension, through which we get Point-to-Set Principles for Hausdorff Dimension, continued-fraction dimension and dimension of Cantor Coverings as special cases. Using the Point-to-Set Principle for Cantor coverings and a new technique for the construction of sequences satisfying a certain Kolmogorov complexity condition, we show that the notions of faithfulness of Cantor coverings at the Hausdorff and constructive levels are equivalent.   We adapt the result by Albeverio, Ivanenko, Lebid, and Torbin to derive the necessary and sufficient conditions for the constructive dimension faithfulness of the coverings generated by the Cantor series expansion. This condition yields two general classes of representations of reals, one whose constructive dimensions that are equivalent to the constructive Hausdorff dimensions, and another, whose effective dimensions are different from the effective Hausdorff dimensions, completely classifying Cantor series expansions of reals.","sentences":["We introduce a constructive analogue of $\\Phi$-dimension, a notion of Hausdorff dimension developed using a restricted class of coverings of a set.","A class of coverings $\\Phi$ is said to be \"faithful\" to Hausdorff dimension if the $\\Phi$-dimension and Hausdorff dimension coincide for every set.   ","We prove a Point-to-Set Principle for $\\Phi$-dimension, through which we get Point-to-Set Principles for Hausdorff Dimension, continued-fraction dimension and dimension of Cantor Coverings as special cases.","Using the Point-to-Set Principle for Cantor coverings and a new technique for the construction of sequences satisfying a certain Kolmogorov complexity condition, we show that the notions of faithfulness of Cantor coverings at the Hausdorff and constructive levels are equivalent.   ","We adapt the result by Albeverio, Ivanenko, Lebid, and Torbin to derive the necessary and sufficient conditions for the constructive dimension faithfulness of the coverings generated by the Cantor series expansion.","This condition yields two general classes of representations of reals, one whose constructive dimensions that are equivalent to the constructive Hausdorff dimensions, and another, whose effective dimensions are different from the effective Hausdorff dimensions, completely classifying Cantor series expansions of reals."],"url":"http://arxiv.org/abs/2403.08278v1","category":"cs.IT"}
{"created":"2024-03-13 05:46:23","title":"A posteriori error estimates for the Generalized Burgers-Huxley equation with weakly singular kernels","abstract":"This paper explores the residual based a posteriori error estimations for the generalized Burgers-Huxley equation (GBHE) featuring weakly singular kernels. Initially, we present a reliable and efficient error estimator for both the stationary GBHE and the semi-discrete GBHE with memory, utilizing the discontinuous Galerkin finite element method (DGFEM) in spatial dimensions. Additionally, employing backward Euler and Crank Nicolson discretization in the temporal domain and DGFEM in spatial dimensions, we introduce an estimator for the fully discrete GBHE, taking into account the influence of past history. The paper also establishes optimal $L^2$ error estimates for both the stationary GBHE and GBHE. Ultimately, we validate the effectiveness of the proposed error estimator through numerical results, demonstrating its efficacy in an adaptive refinement strategy.","sentences":["This paper explores the residual based a posteriori error estimations for the generalized Burgers-Huxley equation (GBHE) featuring weakly singular kernels.","Initially, we present a reliable and efficient error estimator for both the stationary GBHE and the semi-discrete GBHE with memory, utilizing the discontinuous Galerkin finite element method (DGFEM) in spatial dimensions.","Additionally, employing backward Euler and Crank Nicolson discretization in the temporal domain and DGFEM in spatial dimensions, we introduce an estimator for the fully discrete GBHE, taking into account the influence of past history.","The paper also establishes optimal $L^2$ error estimates for both the stationary GBHE and GBHE.","Ultimately, we validate the effectiveness of the proposed error estimator through numerical results, demonstrating its efficacy in an adaptive refinement strategy."],"url":"http://arxiv.org/abs/2403.08269v1","category":"math.NA"}
{"created":"2024-03-13 05:30:30","title":"GPT, Ontology, and CAABAC: A Tripartite Personalized Access Control Model Anchored by Compliance, Context and Attribute","abstract":"As digital healthcare evolves, the security of electronic health records (EHR) becomes increasingly crucial. This study presents the GPT-Onto-CAABAC framework, integrating Generative Pretrained Transformer (GPT), medical-legal ontologies and Context-Aware Attribute-Based Access Control (CAABAC) to enhance EHR access security. Unlike traditional models, GPT-Onto-CAABAC dynamically interprets policies and adapts to changing healthcare and legal environments, offering customized access control solutions. Through empirical evaluation, this framework is shown to be effective in improving EHR security by accurately aligning access decisions with complex regulatory and situational requirements. The findings suggest its broader applicability in sectors where access control must meet stringent compliance and adaptability standards.","sentences":["As digital healthcare evolves, the security of electronic health records (EHR) becomes increasingly crucial.","This study presents the GPT-Onto-CAABAC framework, integrating Generative Pretrained Transformer (GPT), medical-legal ontologies and Context-Aware Attribute-Based Access Control (CAABAC) to enhance EHR access security.","Unlike traditional models, GPT-Onto-CAABAC dynamically interprets policies and adapts to changing healthcare and legal environments, offering customized access control solutions.","Through empirical evaluation, this framework is shown to be effective in improving EHR security by accurately aligning access decisions with complex regulatory and situational requirements.","The findings suggest its broader applicability in sectors where access control must meet stringent compliance and adaptability standards."],"url":"http://arxiv.org/abs/2403.08264v1","category":"cs.CY"}
{"created":"2024-03-13 05:08:47","title":"PNeSM: Arbitrary 3D Scene Stylization via Prompt-Based Neural Style Mapping","abstract":"3D scene stylization refers to transform the appearance of a 3D scene to match a given style image, ensuring that images rendered from different viewpoints exhibit the same style as the given style image, while maintaining the 3D consistency of the stylized scene. Several existing methods have obtained impressive results in stylizing 3D scenes. However, the models proposed by these methods need to be re-trained when applied to a new scene. In other words, their models are coupled with a specific scene and cannot adapt to arbitrary other scenes. To address this issue, we propose a novel 3D scene stylization framework to transfer an arbitrary style to an arbitrary scene, without any style-related or scene-related re-training. Concretely, we first map the appearance of the 3D scene into a 2D style pattern space, which realizes complete disentanglement of the geometry and appearance of the 3D scene and makes our model be generalized to arbitrary 3D scenes. Then we stylize the appearance of the 3D scene in the 2D style pattern space via a prompt-based 2D stylization algorithm. Experimental results demonstrate that our proposed framework is superior to SOTA methods in both visual quality and generalization.","sentences":["3D scene stylization refers to transform the appearance of a 3D scene to match a given style image, ensuring that images rendered from different viewpoints exhibit the same style as the given style image, while maintaining the 3D consistency of the stylized scene.","Several existing methods have obtained impressive results in stylizing 3D scenes.","However, the models proposed by these methods need to be re-trained when applied to a new scene.","In other words, their models are coupled with a specific scene and cannot adapt to arbitrary other scenes.","To address this issue, we propose a novel 3D scene stylization framework to transfer an arbitrary style to an arbitrary scene, without any style-related or scene-related re-training.","Concretely, we first map the appearance of the 3D scene into a 2D style pattern space, which realizes complete disentanglement of the geometry and appearance of the 3D scene and makes our model be generalized to arbitrary 3D scenes.","Then we stylize the appearance of the 3D scene in the 2D style pattern space via a prompt-based 2D stylization algorithm.","Experimental results demonstrate that our proposed framework is superior to SOTA methods in both visual quality and generalization."],"url":"http://arxiv.org/abs/2403.08252v1","category":"cs.CV"}
{"created":"2024-03-13 04:58:58","title":"Evaluating the Efficiency and Cost-effectiveness of RPB-based CO2 Capture: A Comprehensive Approach to Simultaneous Design and Operating Condition Optimization","abstract":"Despite ongoing global initiatives to reduce CO2 emissions, implementing large-scale CO2 capture using amine solvents is fraught with economic uncertainties and technical hurdles. The Rotating Packed Bed (RPB) presents a promising alternative to traditional packed towers, offering compact design and adaptability. Nonetheless, scaling RPB processes to an industrial level is challenging due to the nascent nature of its application. The complexity of designing RPB units, setting operating conditions, and evaluating process performance adds layers of difficulty to the adoption of RPB-based systems in industries. This study introduces an optimization-driven design and evaluation for CO2 capture processes utilizing RPB columns. By employing detailed process simulation, we aim to concurrently optimize unit design and operating parameters, underscoring its advantage over conventional sequential approaches. Our process design method integrates heuristic design recommendations as constraints, resulting in 9.4% to 12.7% cost savings compared to conventional sequential design methods. Furthermore, our comprehensive process-level analysis reveals that using concentrated MEA solvent can yield total cost savings of 13.4% to 25.0% compared to the standard 30wt% MEA solvent. Additionally, the RPB unit can deliver an 8.5 to 23.6 times reduction in packing volume. While the commercial-scale feasibility of RPB technology has been established, the advancement of this field hinges on acquiring a broader and more robust dataset from commercial-scale implementations. Employing strategic methods like modularization could significantly reduce the entry barriers for CO2 capture projects, facilitating their broader adoption and implementation.","sentences":["Despite ongoing global initiatives to reduce CO2 emissions, implementing large-scale CO2 capture using amine solvents is fraught with economic uncertainties and technical hurdles.","The Rotating Packed Bed (RPB) presents a promising alternative to traditional packed towers, offering compact design and adaptability.","Nonetheless, scaling RPB processes to an industrial level is challenging due to the nascent nature of its application.","The complexity of designing RPB units, setting operating conditions, and evaluating process performance adds layers of difficulty to the adoption of RPB-based systems in industries.","This study introduces an optimization-driven design and evaluation for CO2 capture processes utilizing RPB columns.","By employing detailed process simulation, we aim to concurrently optimize unit design and operating parameters, underscoring its advantage over conventional sequential approaches.","Our process design method integrates heuristic design recommendations as constraints, resulting in 9.4% to 12.7% cost savings compared to conventional sequential design methods.","Furthermore, our comprehensive process-level analysis reveals that using concentrated MEA solvent can yield total cost savings of 13.4% to 25.0% compared to the standard 30wt% MEA solvent.","Additionally, the RPB unit can deliver an 8.5 to 23.6 times reduction in packing volume.","While the commercial-scale feasibility of RPB technology has been established, the advancement of this field hinges on acquiring a broader and more robust dataset from commercial-scale implementations.","Employing strategic methods like modularization could significantly reduce the entry barriers for CO2 capture projects, facilitating their broader adoption and implementation."],"url":"http://arxiv.org/abs/2403.08244v1","category":"cs.CE"}
{"created":"2024-03-13 04:43:10","title":"A Novel Feature Learning-based Bio-inspired Neural Network for Real-time Collision-free Rescue of Multi-Robot Systems","abstract":"Natural disasters and urban accidents drive the demand for rescue robots to provide safer, faster, and more efficient rescue trajectories. In this paper, a feature learning-based bio-inspired neural network (FLBBINN) is proposed to quickly generate a heuristic rescue path in complex and dynamic environments, as traditional approaches usually cannot provide a satisfactory solution to real-time responses to sudden environmental changes. The neurodynamic model is incorporated into the feature learning method that can use environmental information to improve path planning strategies. Task assignment and collision-free rescue trajectory are generated through robot poses and the dynamic landscape of neural activity. A dual-channel scale filter, a neural activity channel, and a secondary distance fusion are employed to extract and filter feature neurons. After completion of the feature learning process, a neurodynamics-based feature matrix is established to quickly generate the new heuristic rescue paths with parameter-driven topological adaptability. The proposed FLBBINN aims to reduce the computational complexity of the neural network-based approach and enable the feature learning method to achieve real-time responses to environmental changes. Several simulations and experiments have been conducted to evaluate the performance of the proposed FLBBINN. The results show that the proposed FLBBINN would significantly improve the speed, efficiency, and optimality for rescue operations.","sentences":["Natural disasters and urban accidents drive the demand for rescue robots to provide safer, faster, and more efficient rescue trajectories.","In this paper, a feature learning-based bio-inspired neural network (FLBBINN) is proposed to quickly generate a heuristic rescue path in complex and dynamic environments, as traditional approaches usually cannot provide a satisfactory solution to real-time responses to sudden environmental changes.","The neurodynamic model is incorporated into the feature learning method that can use environmental information to improve path planning strategies.","Task assignment and collision-free rescue trajectory are generated through robot poses and the dynamic landscape of neural activity.","A dual-channel scale filter, a neural activity channel, and a secondary distance fusion are employed to extract and filter feature neurons.","After completion of the feature learning process, a neurodynamics-based feature matrix is established to quickly generate the new heuristic rescue paths with parameter-driven topological adaptability.","The proposed FLBBINN aims to reduce the computational complexity of the neural network-based approach and enable the feature learning method to achieve real-time responses to environmental changes.","Several simulations and experiments have been conducted to evaluate the performance of the proposed FLBBINN.","The results show that the proposed FLBBINN would significantly improve the speed, efficiency, and optimality for rescue operations."],"url":"http://arxiv.org/abs/2403.08238v1","category":"cs.RO"}
{"created":"2024-03-13 04:11:41","title":"Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs","abstract":"Recently, Large Language Models (LLMs) have demonstrated great potential in robotic applications by providing essential general knowledge for situations that can not be pre-programmed beforehand. Generally speaking, mobile robots need to understand maps to execute tasks such as localization or navigation. In this letter, we address the problem of enabling LLMs to comprehend Area Graph, a text-based map representation, in order to enhance their applicability in the field of mobile robotics. Area Graph is a hierarchical, topometric semantic map representation utilizing polygons to demark areas such as rooms, corridors or buildings. In contrast to commonly used map representations, such as occupancy grid maps or point clouds, osmAG (Area Graph in OpensStreetMap format) is stored in a XML textual format naturally readable by LLMs. Furthermore, conventional robotic algorithms such as localization and path planning are compatible with osmAG, facilitating this map representation comprehensible by LLMs, traditional robotic algorithms and humans. Our experiments show that with a proper map representation, LLMs possess the capability to understand maps and answer queries based on that understanding. Following simple fine-tuning of LLaMA2 models, it surpassed ChatGPT-3.5 in tasks involving topology and hierarchy understanding. Our dataset, dataset generation code, fine-tuned LoRA adapters can be accessed at https://github.com/xiefujing/LLM-osmAG-Comprehension.","sentences":["Recently, Large Language Models (LLMs) have demonstrated great potential in robotic applications by providing essential general knowledge for situations that can not be pre-programmed beforehand.","Generally speaking, mobile robots need to understand maps to execute tasks such as localization or navigation.","In this letter, we address the problem of enabling LLMs to comprehend Area Graph, a text-based map representation, in order to enhance their applicability in the field of mobile robotics.","Area Graph is a hierarchical, topometric semantic map representation utilizing polygons to demark areas such as rooms, corridors or buildings.","In contrast to commonly used map representations, such as occupancy grid maps or point clouds, osmAG (Area Graph in OpensStreetMap format) is stored in a XML textual format naturally readable by LLMs.","Furthermore, conventional robotic algorithms such as localization and path planning are compatible with osmAG, facilitating this map representation comprehensible by LLMs, traditional robotic algorithms and humans.","Our experiments show that with a proper map representation, LLMs possess the capability to understand maps and answer queries based on that understanding.","Following simple fine-tuning of LLaMA2 models, it surpassed ChatGPT-3.5 in tasks involving topology and hierarchy understanding.","Our dataset, dataset generation code, fine-tuned LoRA adapters can be accessed at https://github.com/xiefujing/LLM-osmAG-Comprehension."],"url":"http://arxiv.org/abs/2403.08228v1","category":"cs.RO"}
{"created":"2024-03-13 03:45:14","title":"Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators","abstract":"We propose an operator learning approach to accelerate geometric Markov chain Monte Carlo (MCMC) for solving infinite-dimensional nonlinear Bayesian inverse problems. While geometric MCMC employs high-quality proposals that adapt to posterior local geometry, it requires computing local gradient and Hessian information of the log-likelihood, incurring a high cost when the parameter-to-observable (PtO) map is defined through expensive model simulations. We consider a delayed-acceptance geometric MCMC method driven by a neural operator surrogate of the PtO map, where the proposal is designed to exploit fast surrogate approximations of the log-likelihood and, simultaneously, its gradient and Hessian. To achieve a substantial speedup, the surrogate needs to be accurate in predicting both the observable and its parametric derivative (the derivative of the observable with respect to the parameter). Training such a surrogate via conventional operator learning using input--output samples often demands a prohibitively large number of model simulations. In this work, we present an extension of derivative-informed operator learning [O'Leary-Roseberry et al., J. Comput. Phys., 496 (2024)] using input--output--derivative training samples. Such a learning method leads to derivative-informed neural operator (DINO) surrogates that accurately predict the observable and its parametric derivative at a significantly lower training cost than the conventional method. Cost and error analysis for reduced basis DINO surrogates are provided. Numerical studies on PDE-constrained Bayesian inversion demonstrate that DINO-driven MCMC generates effective posterior samples 3--9 times faster than geometric MCMC and 60--97 times faster than prior geometry-based MCMC. Furthermore, the training cost of DINO surrogates breaks even after collecting merely 10--25 effective posterior samples compared to geometric MCMC.","sentences":["We propose an operator learning approach to accelerate geometric Markov chain Monte Carlo (MCMC) for solving infinite-dimensional nonlinear Bayesian inverse problems.","While geometric MCMC employs high-quality proposals that adapt to posterior local geometry, it requires computing local gradient and Hessian information of the log-likelihood, incurring a high cost when the parameter-to-observable (PtO) map is defined through expensive model simulations.","We consider a delayed-acceptance geometric MCMC method driven by a neural operator surrogate of the PtO map, where the proposal is designed to exploit fast surrogate approximations of the log-likelihood and, simultaneously, its gradient and Hessian.","To achieve a substantial speedup, the surrogate needs to be accurate in predicting both the observable and its parametric derivative (the derivative of the observable with respect to the parameter).","Training such a surrogate via conventional operator learning using input--output samples often demands a prohibitively large number of model simulations.","In this work, we present an extension of derivative-informed operator learning [O'Leary-Roseberry et al., J. Comput.","Phys., 496 (2024)] using input--output--derivative training samples.","Such a learning method leads to derivative-informed neural operator (DINO) surrogates that accurately predict the observable and its parametric derivative at a significantly lower training cost than the conventional method.","Cost and error analysis for reduced basis DINO surrogates are provided.","Numerical studies on PDE-constrained Bayesian inversion demonstrate that DINO-driven MCMC generates effective posterior samples 3--9 times faster than geometric MCMC and 60--97 times faster than prior geometry-based MCMC.","Furthermore, the training cost of DINO surrogates breaks even after collecting merely 10--25 effective posterior samples compared to geometric MCMC."],"url":"http://arxiv.org/abs/2403.08220v1","category":"math.NA"}
{"created":"2024-03-13 03:24:36","title":"LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving","abstract":"Despite the impressive performance achieved by data-fusion networks with duplex encoders for visual semantic segmentation, they become ineffective when spatial geometric data are not available. Implicitly infusing the spatial geometric prior knowledge acquired by a duplex-encoder teacher model into a single-encoder student model is a practical, albeit less explored research avenue. This paper delves into this topic and resorts to knowledge distillation approaches to address this problem. We introduce the Learning to Infuse \"X\" (LIX) framework, with novel contributions in both logit distillation and feature distillation aspects. We present a mathematical proof that underscores the limitation of using a single fixed weight in decoupled knowledge distillation and introduce a logit-wise dynamic weight controller as a solution to this issue. Furthermore, we develop an adaptively-recalibrated feature distillation algorithm, including two technical novelties: feature recalibration via kernel regression and in-depth feature consistency quantification via centered kernel alignment. Extensive experiments conducted with intermediate-fusion and late-fusion networks across various public datasets provide both quantitative and qualitative evaluations, demonstrating the superior performance of our LIX framework when compared to other state-of-the-art approaches.","sentences":["Despite the impressive performance achieved by data-fusion networks with duplex encoders for visual semantic segmentation, they become ineffective when spatial geometric data are not available.","Implicitly infusing the spatial geometric prior knowledge acquired by a duplex-encoder teacher model into a single-encoder student model is a practical, albeit less explored research avenue.","This paper delves into this topic and resorts to knowledge distillation approaches to address this problem.","We introduce the Learning to Infuse \"X\" (LIX) framework, with novel contributions in both logit distillation and feature distillation aspects.","We present a mathematical proof that underscores the limitation of using a single fixed weight in decoupled knowledge distillation and introduce a logit-wise dynamic weight controller as a solution to this issue.","Furthermore, we develop an adaptively-recalibrated feature distillation algorithm, including two technical novelties: feature recalibration via kernel regression and in-depth feature consistency quantification via centered kernel alignment.","Extensive experiments conducted with intermediate-fusion and late-fusion networks across various public datasets provide both quantitative and qualitative evaluations, demonstrating the superior performance of our LIX framework when compared to other state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.08215v1","category":"cs.CV"}
{"created":"2024-03-13 03:10:11","title":"Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks","abstract":"In the rapidly evolving landscape of communication and network security, the increasing reliance on deep neural networks (DNNs) and cloud services for data processing presents a significant vulnerability: the potential for backdoors that can be exploited by malicious actors. Our approach leverages advanced tensor decomposition algorithms Independent Vector Analysis (IVA), Multiset Canonical Correlation Analysis (MCCA), and Parallel Factor Analysis (PARAFAC2) to meticulously analyze the weights of pre-trained DNNs and distinguish between backdoored and clean models effectively. The key strengths of our method lie in its domain independence, adaptability to various network architectures, and ability to operate without access to the training data of the scrutinized models. This not only ensures versatility across different application scenarios but also addresses the challenge of identifying backdoors without prior knowledge of the specific triggers employed to alter network behavior. We have applied our detection pipeline to three distinct computer vision datasets, encompassing both image classification and object detection tasks. The results demonstrate a marked improvement in both accuracy and efficiency over existing backdoor detection methods. This advancement enhances the security of deep learning and AI in networked systems, providing essential cybersecurity against evolving threats in emerging technologies.","sentences":["In the rapidly evolving landscape of communication and network security, the increasing reliance on deep neural networks (DNNs) and cloud services for data processing presents a significant vulnerability: the potential for backdoors that can be exploited by malicious actors.","Our approach leverages advanced tensor decomposition algorithms Independent Vector Analysis (IVA), Multiset Canonical Correlation Analysis (MCCA), and Parallel Factor Analysis (PARAFAC2) to meticulously analyze the weights of pre-trained DNNs and distinguish between backdoored and clean models effectively.","The key strengths of our method lie in its domain independence, adaptability to various network architectures, and ability to operate without access to the training data of the scrutinized models.","This not only ensures versatility across different application scenarios but also addresses the challenge of identifying backdoors without prior knowledge of the specific triggers employed to alter network behavior.","We have applied our detection pipeline to three distinct computer vision datasets, encompassing both image classification and object detection tasks.","The results demonstrate a marked improvement in both accuracy and efficiency over existing backdoor detection methods.","This advancement enhances the security of deep learning and AI in networked systems, providing essential cybersecurity against evolving threats in emerging technologies."],"url":"http://arxiv.org/abs/2403.08208v1","category":"cs.CR"}
{"created":"2024-03-13 02:55:27","title":"Learnable Community-Aware Transformer for Brain Connectome Analysis with Token Clustering","abstract":"Neuroscientific research has revealed that the complex brain network can be organized into distinct functional communities, each characterized by a cohesive group of regions of interest (ROIs) with strong interconnections. These communities play a crucial role in comprehending the functional organization of the brain and its implications for neurological conditions, including Autism Spectrum Disorder (ASD) and biological differences, such as in gender. Traditional models have been constrained by the necessity of predefined community clusters, limiting their flexibility and adaptability in deciphering the brain's functional organization. Furthermore, these models were restricted by a fixed number of communities, hindering their ability to accurately represent the brain's dynamic nature. In this study, we present a token clustering brain transformer-based model ($\\texttt{TC-BrainTF}$) for joint community clustering and classification. Our approach proposes a novel token clustering (TC) module based on the transformer architecture, which utilizes learnable prompt tokens with orthogonal loss where each ROI embedding is projected onto the prompt embedding space, effectively clustering ROIs into communities and reducing the dimensions of the node representation via merging with communities. Our results demonstrate that our learnable community-aware model $\\texttt{TC-BrainTF}$ offers improved accuracy in identifying ASD and classifying genders through rigorous testing on ABIDE and HCP datasets. Additionally, the qualitative analysis on $\\texttt{TC-BrainTF}$ has demonstrated the effectiveness of the designed TC module and its relevance to neuroscience interpretations.","sentences":["Neuroscientific research has revealed that the complex brain network can be organized into distinct functional communities, each characterized by a cohesive group of regions of interest (ROIs) with strong interconnections.","These communities play a crucial role in comprehending the functional organization of the brain and its implications for neurological conditions, including Autism Spectrum Disorder (ASD) and biological differences, such as in gender.","Traditional models have been constrained by the necessity of predefined community clusters, limiting their flexibility and adaptability in deciphering the brain's functional organization.","Furthermore, these models were restricted by a fixed number of communities, hindering their ability to accurately represent the brain's dynamic nature.","In this study, we present a token clustering brain transformer-based model ($\\texttt{TC-BrainTF}$) for joint community clustering and classification.","Our approach proposes a novel token clustering (TC) module based on the transformer architecture, which utilizes learnable prompt tokens with orthogonal loss where each ROI embedding is projected onto the prompt embedding space, effectively clustering ROIs into communities and reducing the dimensions of the node representation via merging with communities.","Our results demonstrate that our learnable community-aware model $\\texttt{TC-BrainTF}$ offers improved accuracy in identifying ASD and classifying genders through rigorous testing on ABIDE and HCP datasets.","Additionally, the qualitative analysis on $\\texttt{TC-BrainTF}$ has demonstrated the effectiveness of the designed TC module and its relevance to neuroscience interpretations."],"url":"http://arxiv.org/abs/2403.08203v1","category":"q-bio.NC"}
{"created":"2024-03-13 02:44:33","title":"PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay for Smart Healthcare","abstract":"We propose PAGE, a domain-incremental adaptation strategy with past-agnostic generative replay for smart healthcare. PAGE enables generative replay without the aid of any preserved data or information from prior domains. When adapting to a new domain, it exploits real data from the new distribution and the current model to generate synthetic data that retain the learned knowledge of previous domains. By replaying the synthetic data with the new real data during training, PAGE achieves a good balance between domain adaptation and knowledge retention. In addition, we incorporate an extended inductive conformal prediction (EICP) method into PAGE to produce a confidence score and a credibility value for each detection result. This makes the predictions interpretable and provides statistical guarantees for disease detection in smart healthcare applications. We demonstrate PAGE's effectiveness in domain-incremental disease detection with three distinct disease datasets collected from commercially available WMSs. PAGE achieves highly competitive performance against state-of-the-art with superior scalability, data privacy, and feasibility. Furthermore, PAGE can enable up to 75% reduction in clinical workload with the help of EICP.","sentences":["We propose PAGE, a domain-incremental adaptation strategy with past-agnostic generative replay for smart healthcare.","PAGE enables generative replay without the aid of any preserved data or information from prior domains.","When adapting to a new domain, it exploits real data from the new distribution and the current model to generate synthetic data that retain the learned knowledge of previous domains.","By replaying the synthetic data with the new real data during training, PAGE achieves a good balance between domain adaptation and knowledge retention.","In addition, we incorporate an extended inductive conformal prediction (EICP) method into PAGE to produce a confidence score and a credibility value for each detection result.","This makes the predictions interpretable and provides statistical guarantees for disease detection in smart healthcare applications.","We demonstrate PAGE's effectiveness in domain-incremental disease detection with three distinct disease datasets collected from commercially available WMSs.","PAGE achieves highly competitive performance against state-of-the-art with superior scalability, data privacy, and feasibility.","Furthermore, PAGE can enable up to 75% reduction in clinical workload with the help of EICP."],"url":"http://arxiv.org/abs/2403.08197v1","category":"cs.LG"}
{"created":"2024-03-13 02:41:53","title":"SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech Recognition Evaluation","abstract":"In the wake of the surging tide of deep learning over the past decade, Automatic Speech Recognition (ASR) has garnered substantial attention, leading to the emergence of numerous publicly accessible ASR systems that are actively being integrated into our daily lives. Nonetheless, the impartial and replicable evaluation of these ASR systems encounters challenges due to various crucial subtleties. In this paper we introduce the SpeechColab Leaderboard, a general-purpose, open-source platform designed for ASR evaluation. With this platform: (i) We report a comprehensive benchmark, unveiling the current state-of-the-art panorama for ASR systems, covering both open-source models and industrial commercial services. (ii) We quantize how distinct nuances in the scoring pipeline influence the final benchmark outcomes. These include nuances related to capitalization, punctuation, interjection, contraction, synonym usage, compound words, etc. These issues have gained prominence in the context of the transition towards an End-to-End future. (iii) We propose a practical modification to the conventional Token-Error-Rate (TER) evaluation metric, with inspirations from Kolmogorov complexity and Normalized Information Distance (NID). This adaptation, called modified-TER (mTER), achieves proper normalization and symmetrical treatment of reference and hypothesis. By leveraging this platform as a large-scale testing ground, this study demonstrates the robustness and backward compatibility of mTER when compared to TER. The SpeechColab Leaderboard is accessible at https://github.com/SpeechColab/Leaderboard","sentences":["In the wake of the surging tide of deep learning over the past decade, Automatic Speech Recognition (ASR) has garnered substantial attention, leading to the emergence of numerous publicly accessible ASR systems that are actively being integrated into our daily lives.","Nonetheless, the impartial and replicable evaluation of these ASR systems encounters challenges due to various crucial subtleties.","In this paper we introduce the SpeechColab Leaderboard, a general-purpose, open-source platform designed for ASR evaluation.","With this platform: (i) We report a comprehensive benchmark, unveiling the current state-of-the-art panorama for ASR systems, covering both open-source models and industrial commercial services.","(ii) We quantize how distinct nuances in the scoring pipeline influence the final benchmark outcomes.","These include nuances related to capitalization, punctuation, interjection, contraction, synonym usage, compound words, etc.","These issues have gained prominence in the context of the transition towards an End-to-End future.","(iii) We propose a practical modification to the conventional Token-Error-Rate (TER) evaluation metric, with inspirations from Kolmogorov complexity and Normalized Information Distance (NID).","This adaptation, called modified-TER (mTER), achieves proper normalization and symmetrical treatment of reference and hypothesis.","By leveraging this platform as a large-scale testing ground, this study demonstrates the robustness and backward compatibility of mTER when compared to TER.","The SpeechColab Leaderboard is accessible at https://github.com/SpeechColab/Leaderboard"],"url":"http://arxiv.org/abs/2403.08196v1","category":"cs.CL"}
{"created":"2024-03-13 02:33:28","title":"Learning-driven Physically-aware Large-scale Circuit Gate Sizing","abstract":"Gate sizing plays an important role in timing optimization after physical design. Existing machine learning-based gate sizing works cannot optimize timing on multiple timing paths simultaneously and neglect the physical constraint on layouts. They cause sub-optimal sizing solutions and low-efficiency issues when compared with commercial gate sizing tools. In this work, we propose a learning-driven physically-aware gate sizing framework to optimize timing performance on large-scale circuits efficiently. In our gradient descent optimization-based work, for obtaining accurate gradients, a multi-modal gate sizing-aware timing model is achieved via learning timing information on multiple timing paths and physical information on multiple-scaled layouts jointly. Then, gradient generation based on the sizing-oriented estimator and adaptive back-propagation are developed to update gate sizes. Our results demonstrate that our work achieves higher timing performance improvements in a faster way compared with the commercial gate sizing tool.","sentences":["Gate sizing plays an important role in timing optimization after physical design.","Existing machine learning-based gate sizing works cannot optimize timing on multiple timing paths simultaneously and neglect the physical constraint on layouts.","They cause sub-optimal sizing solutions and low-efficiency issues when compared with commercial gate sizing tools.","In this work, we propose a learning-driven physically-aware gate sizing framework to optimize timing performance on large-scale circuits efficiently.","In our gradient descent optimization-based work, for obtaining accurate gradients, a multi-modal gate sizing-aware timing model is achieved via learning timing information on multiple timing paths and physical information on multiple-scaled layouts jointly.","Then, gradient generation based on the sizing-oriented estimator and adaptive back-propagation are developed to update gate sizes.","Our results demonstrate that our work achieves higher timing performance improvements in a faster way compared with the commercial gate sizing tool."],"url":"http://arxiv.org/abs/2403.08193v1","category":"cs.LG"}
{"created":"2024-03-13 02:05:21","title":"Effects of wave damping and finite perpendicular scale on three-dimensional Alfv\u00e9n wave parametric decay in low-beta plasmas","abstract":"Shear Alfven wave parametric decay instability (PDI) provides a potential path toward significant wave dissipation and plasma heating. However, fundamental questions regarding how PDI is excited in a realistic three-dimensional (3D) open system and how critically the finite perpendicular wave scale -- as found in both the laboratory and space plasmas -- affects the excitation remain poorly understood. Here, we present the first 3D, open-boundary, hybrid kinetic-fluid simulations of kinetic Alfven wave PDI in low-beta plasmas. Key findings are that the PDI excitation is strongly limited by the wave damping present, including electron-ion collisional damping (represented by a constant resistivity) and geometrical attenuation associated with the finite-scale Alfven wave, and ion Landau damping of the child acoustic wave. The perpendicular wave scale alone, however, plays no discernible role, with different wave scales exhibiting similar instability growth. These findings are corroborated by theoretical analysis and estimates. The new understanding of 3D kinetic Alfven wave PDI physics is essential for laboratory study of the basic plasma process and may also help evaluate the relevance/role of PDI in low-beta space plasmas.","sentences":["Shear Alfven wave parametric decay instability (PDI) provides a potential path toward significant wave dissipation and plasma heating.","However, fundamental questions regarding how PDI is excited in a realistic three-dimensional (3D) open system and how critically the finite perpendicular wave scale -- as found in both the laboratory and space plasmas -- affects the excitation remain poorly understood.","Here, we present the first 3D, open-boundary, hybrid kinetic-fluid simulations of kinetic Alfven wave PDI in low-beta plasmas.","Key findings are that the PDI excitation is strongly limited by the wave damping present, including electron-ion collisional damping (represented by a constant resistivity) and geometrical attenuation associated with the finite-scale Alfven wave, and ion Landau damping of the child acoustic wave.","The perpendicular wave scale alone, however, plays no discernible role, with different wave scales exhibiting similar instability growth.","These findings are corroborated by theoretical analysis and estimates.","The new understanding of 3D kinetic Alfven wave PDI physics is essential for laboratory study of the basic plasma process and may also help evaluate the relevance/role of PDI in low-beta space plasmas."],"url":"http://arxiv.org/abs/2403.08179v1","category":"physics.plasm-ph"}
{"created":"2024-03-13 01:48:01","title":"Versatile Defense Against Adversarial Attacks on Image Recognition","abstract":"Adversarial attacks present a significant security risk to image recognition tasks. Defending against these attacks in a real-life setting can be compared to the way antivirus software works, with a key consideration being how well the defense can adapt to new and evolving attacks. Another important factor is the resources involved in terms of time and cost for training defense models and updating the model database. Training many models that are specific to each type of attack can be time-consuming and expensive. Ideally, we should be able to train one single model that can handle a wide range of attacks. It appears that a defense method based on image-to-image translation may be capable of this. The proposed versatile defense approach in this paper only requires training one model to effectively resist various unknown adversarial attacks. The trained model has successfully improved the classification accuracy from nearly zero to an average of 86%, performing better than other defense methods proposed in prior studies. When facing the PGD attack and the MI-FGSM attack, versatile defense model even outperforms the attack-specific models trained based on these two attacks. The robustness check also shows that our versatile defense model performs stably regardless with the attack strength.","sentences":["Adversarial attacks present a significant security risk to image recognition tasks.","Defending against these attacks in a real-life setting can be compared to the way antivirus software works, with a key consideration being how well the defense can adapt to new and evolving attacks.","Another important factor is the resources involved in terms of time and cost for training defense models and updating the model database.","Training many models that are specific to each type of attack can be time-consuming and expensive.","Ideally, we should be able to train one single model that can handle a wide range of attacks.","It appears that a defense method based on image-to-image translation may be capable of this.","The proposed versatile defense approach in this paper only requires training one model to effectively resist various unknown adversarial attacks.","The trained model has successfully improved the classification accuracy from nearly zero to an average of 86%, performing better than other defense methods proposed in prior studies.","When facing the PGD attack and the MI-FGSM attack, versatile defense model even outperforms the attack-specific models trained based on these two attacks.","The robustness check also shows that our versatile defense model performs stably regardless with the attack strength."],"url":"http://arxiv.org/abs/2403.08170v1","category":"cs.CV"}
{"created":"2024-03-13 01:18:55","title":"Iterative Learning for Joint Image Denoising and Motion Artifact Correction of 3D Brain MRI","abstract":"Image noise and motion artifacts greatly affect the quality of brain MRI and negatively influence downstream medical image analysis. Previous studies often focus on 2D methods that process each volumetric MR image slice-by-slice, thus losing important 3D anatomical information. Additionally, these studies generally treat image denoising and artifact correction as two standalone tasks, without considering their potential relationship, especially on low-quality images where severe noise and motion artifacts occur simultaneously. To address these issues, we propose a Joint image Denoising and motion Artifact Correction (JDAC) framework via iterative learning to handle noisy MRIs with motion artifacts, consisting of an adaptive denoising model and an anti-artifact model. In the adaptive denoising model, we first design a novel noise level estimation strategy, and then adaptively reduce the noise through a U-Net backbone with feature normalization conditioning on the estimated noise variance. The anti-artifact model employs another U-Net for eliminating motion artifacts, incorporating a novel gradient-based loss function designed to maintain the integrity of brain anatomy during the motion correction process. These two models are iteratively employed for joint image denoising and artifact correction through an iterative learning framework. An early stopping strategy depending on noise level estimation is applied to accelerate the iteration process. The denoising model is trained with 9,544 T1-weighted MRIs with manually added Gaussian noise as supervision. The anti-artifact model is trained on 552 T1-weighted MRIs with motion artifacts and paired motion-free images. Experimental results on a public dataset and a clinical study suggest the effectiveness of JDAC in both tasks of denoising and motion artifact correction, compared with several state-of-the-art methods.","sentences":["Image noise and motion artifacts greatly affect the quality of brain MRI and negatively influence downstream medical image analysis.","Previous studies often focus on 2D methods that process each volumetric MR image slice-by-slice, thus losing important 3D anatomical information.","Additionally, these studies generally treat image denoising and artifact correction as two standalone tasks, without considering their potential relationship, especially on low-quality images where severe noise and motion artifacts occur simultaneously.","To address these issues, we propose a Joint image Denoising and motion Artifact Correction (JDAC) framework via iterative learning to handle noisy MRIs with motion artifacts, consisting of an adaptive denoising model and an anti-artifact model.","In the adaptive denoising model, we first design a novel noise level estimation strategy, and then adaptively reduce the noise through a U-Net backbone with feature normalization conditioning on the estimated noise variance.","The anti-artifact model employs another U-Net for eliminating motion artifacts, incorporating a novel gradient-based loss function designed to maintain the integrity of brain anatomy during the motion correction process.","These two models are iteratively employed for joint image denoising and artifact correction through an iterative learning framework.","An early stopping strategy depending on noise level estimation is applied to accelerate the iteration process.","The denoising model is trained with 9,544 T1-weighted MRIs with manually added Gaussian noise as supervision.","The anti-artifact model is trained on 552 T1-weighted MRIs with motion artifacts and paired motion-free images.","Experimental results on a public dataset and a clinical study suggest the effectiveness of JDAC in both tasks of denoising and motion artifact correction, compared with several state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.08162v1","category":"eess.IV"}
{"created":"2024-03-13 01:07:55","title":"LAFS: Landmark-based Facial Self-supervised Learning for Face Recognition","abstract":"In this work we focus on learning facial representations that can be adapted to train effective face recognition models, particularly in the absence of labels. Firstly, compared with existing labelled face datasets, a vastly larger magnitude of unlabeled faces exists in the real world. We explore the learning strategy of these unlabeled facial images through self-supervised pretraining to transfer generalized face recognition performance. Moreover, motivated by one recent finding, that is, the face saliency area is critical for face recognition, in contrast to utilizing random cropped blocks of images for constructing augmentations in pretraining, we utilize patches localized by extracted facial landmarks. This enables our method - namely LAndmark-based Facial Self-supervised learning LAFS), to learn key representation that is more critical for face recognition. We also incorporate two landmark-specific augmentations which introduce more diversity of landmark information to further regularize the learning. With learned landmark-based facial representations, we further adapt the representation for face recognition with regularization mitigating variations in landmark positions. Our method achieves significant improvement over the state-of-the-art on multiple face recognition benchmarks, especially on more challenging few-shot scenarios.","sentences":["In this work we focus on learning facial representations that can be adapted to train effective face recognition models, particularly in the absence of labels.","Firstly, compared with existing labelled face datasets, a vastly larger magnitude of unlabeled faces exists in the real world.","We explore the learning strategy of these unlabeled facial images through self-supervised pretraining to transfer generalized face recognition performance.","Moreover, motivated by one recent finding, that is, the face saliency area is critical for face recognition, in contrast to utilizing random cropped blocks of images for constructing augmentations in pretraining, we utilize patches localized by extracted facial landmarks.","This enables our method - namely LAndmark-based Facial Self-supervised learning LAFS), to learn key representation that is more critical for face recognition.","We also incorporate two landmark-specific augmentations which introduce more diversity of landmark information to further regularize the learning.","With learned landmark-based facial representations, we further adapt the representation for face recognition with regularization mitigating variations in landmark positions.","Our method achieves significant improvement over the state-of-the-art on multiple face recognition benchmarks, especially on more challenging few-shot scenarios."],"url":"http://arxiv.org/abs/2403.08161v1","category":"cs.CV"}
{"created":"2024-03-13 00:43:10","title":"NeRF-Supervised Feature Point Detection and Description","abstract":"Feature point detection and description is the backbone for various computer vision applications, such as Structure-from-Motion, visual SLAM, and visual place recognition. While learning-based methods have surpassed traditional handcrafted techniques, their training often relies on simplistic homography-based simulations of multi-view perspectives, limiting model generalisability. This paper introduces a novel approach leveraging neural radiance fields (NeRFs) for realistic multi-view training data generation. We create a diverse multi-view dataset using NeRFs, consisting of indoor and outdoor scenes. Our proposed methodology adapts state-of-the-art feature detectors and descriptors to train on NeRF-synthesised views supervised by perspective projective geometry. Our experiments demonstrate that the proposed methods achieve competitive or superior performance on standard benchmarks for relative pose estimation, point cloud registration, and homography estimation while requiring significantly less training data compared to existing approaches.","sentences":["Feature point detection and description is the backbone for various computer vision applications, such as Structure-from-Motion, visual SLAM, and visual place recognition.","While learning-based methods have surpassed traditional handcrafted techniques, their training often relies on simplistic homography-based simulations of multi-view perspectives, limiting model generalisability.","This paper introduces a novel approach leveraging neural radiance fields (NeRFs) for realistic multi-view training data generation.","We create a diverse multi-view dataset using NeRFs, consisting of indoor and outdoor scenes.","Our proposed methodology adapts state-of-the-art feature detectors and descriptors to train on NeRF-synthesised views supervised by perspective projective geometry.","Our experiments demonstrate that the proposed methods achieve competitive or superior performance on standard benchmarks for relative pose estimation, point cloud registration, and homography estimation while requiring significantly less training data compared to existing approaches."],"url":"http://arxiv.org/abs/2403.08156v1","category":"cs.CV"}
{"created":"2024-03-12 23:59:15","title":"BAGEL: Bootstrapping Agents by Guiding Exploration with Language","abstract":"Following natural language instructions by executing actions in digital environments (e.g. web-browsers and REST APIs) is a challenging task for language model (LM) agents. Unfortunately, LM agents often fail to generalize to new environments without human demonstrations. This work presents BAGEL, a method for bootstrapping LM agents without human supervision. BAGEL converts a seed set of randomly explored trajectories or synthetic instructions, into demonstrations, via round-trips between two noisy LM components: an LM labeler which converts a trajectory into a synthetic instruction, and a zero-shot LM agent which maps the synthetic instruction into a refined trajectory. By performing these round-trips iteratively, BAGEL quickly converts the initial distribution of trajectories towards those that are well-described by natural language. We use BAGEL demonstrations to adapt a zero shot LM agent at test time via in-context learning over retrieved demonstrations, and find improvements of over 2-13% absolute on ToolQA and MiniWob++, with up to 13x reduction in execution failures.","sentences":["Following natural language instructions by executing actions in digital environments (e.g. web-browsers and REST APIs) is a challenging task for language model (LM) agents.","Unfortunately, LM agents often fail to generalize to new environments without human demonstrations.","This work presents BAGEL, a method for bootstrapping LM agents without human supervision.","BAGEL converts a seed set of randomly explored trajectories or synthetic instructions, into demonstrations, via round-trips between two noisy LM components: an LM labeler which converts a trajectory into a synthetic instruction, and a zero-shot LM agent which maps the synthetic instruction into a refined trajectory.","By performing these round-trips iteratively, BAGEL quickly converts the initial distribution of trajectories towards those that are well-described by natural language.","We use BAGEL demonstrations to adapt a zero shot LM agent at test time via in-context learning over retrieved demonstrations, and find improvements of over 2-13% absolute on ToolQA and MiniWob++, with up to 13x reduction in execution failures."],"url":"http://arxiv.org/abs/2403.08140v1","category":"cs.CL"}
{"created":"2024-03-12 23:39:43","title":"Cost-Effective Methodology for Complex Tuning Searches in HPC: Navigating Interdependencies and Dimensionality","abstract":"Tuning searches are pivotal in High-Performance Computing (HPC), addressing complex optimization challenges in computational applications. The complexity arises not only from finely tuning parameters within routines but also potential interdependencies among them, rendering traditional optimization methods inefficient. Instead of scrutinizing interdependencies among parameters and routines, practitioners often face the dilemma of conducting independent tuning searches for each routine, thereby overlooking interdependence, or pursuing a more resource-intensive joint search for all routines. This decision is driven by the consideration that some interdependence analysis and high-dimensional decomposition techniques in literature may be prohibitively expensive in HPC tuning searches. Our methodology adapts and refines these methods to ensure computational feasibility while maximizing performance gains in real-world scenarios. Our methodology leverages a cost-effective interdependence analysis to decide whether to merge several tuning searches into a joint search or conduct orthogonal searches. Tested on synthetic functions with varying levels of parameter interdependence, our methodology efficiently explores the search space. In comparison to Bayesian-optimization-based full independent or fully joint searches, our methodology suggested an optimized breakdown of independent and merged searches that led to final configurations up to 8% more accurate, reducing the search time by up to 95%. When applied to GPU-offloaded Real-Time Time-Dependent Density Functional Theory (RT-TDDFT), an application in computational materials science that challenges modern HPC autotuners, our methodology achieved an effective tuning search. Its adaptability and efficiency extend beyond RT-TDDFT, making it valuable for related applications in HPC.","sentences":["Tuning searches are pivotal in High-Performance Computing (HPC), addressing complex optimization challenges in computational applications.","The complexity arises not only from finely tuning parameters within routines but also potential interdependencies among them, rendering traditional optimization methods inefficient.","Instead of scrutinizing interdependencies among parameters and routines, practitioners often face the dilemma of conducting independent tuning searches for each routine, thereby overlooking interdependence, or pursuing a more resource-intensive joint search for all routines.","This decision is driven by the consideration that some interdependence analysis and high-dimensional decomposition techniques in literature may be prohibitively expensive in HPC tuning searches.","Our methodology adapts and refines these methods to ensure computational feasibility while maximizing performance gains in real-world scenarios.","Our methodology leverages a cost-effective interdependence analysis to decide whether to merge several tuning searches into a joint search or conduct orthogonal searches.","Tested on synthetic functions with varying levels of parameter interdependence, our methodology efficiently explores the search space.","In comparison to Bayesian-optimization-based full independent or fully joint searches, our methodology suggested an optimized breakdown of independent and merged searches that led to final configurations up to 8% more accurate, reducing the search time by up to 95%.","When applied to GPU-offloaded Real-Time Time-Dependent Density Functional Theory (RT-TDDFT), an application in computational materials science that challenges modern HPC autotuners, our methodology achieved an effective tuning search.","Its adaptability and efficiency extend beyond RT-TDDFT, making it valuable for related applications in HPC."],"url":"http://arxiv.org/abs/2403.08131v1","category":"cs.DC"}
{"created":"2024-03-12 23:21:09","title":"Towards Independence Criterion in Machine Unlearning of Features and Labels","abstract":"This work delves into the complexities of machine unlearning in the face of distributional shifts, particularly focusing on the challenges posed by non-uniform feature and label removal. With the advent of regulations like the GDPR emphasizing data privacy and the right to be forgotten, machine learning models face the daunting task of unlearning sensitive information without compromising their integrity or performance. Our research introduces a novel approach that leverages influence functions and principles of distributional independence to address these challenges. By proposing a comprehensive framework for machine unlearning, we aim to ensure privacy protection while maintaining model performance and adaptability across varying distributions. Our method not only facilitates efficient data removal but also dynamically adjusts the model to preserve its generalization capabilities. Through extensive experimentation, we demonstrate the efficacy of our approach in scenarios characterized by significant distributional shifts, making substantial contributions to the field of machine unlearning. This research paves the way for developing more resilient and adaptable unlearning techniques, ensuring models remain robust and accurate in the dynamic landscape of data privacy and machine learning.","sentences":["This work delves into the complexities of machine unlearning in the face of distributional shifts, particularly focusing on the challenges posed by non-uniform feature and label removal.","With the advent of regulations like the GDPR emphasizing data privacy and the right to be forgotten, machine learning models face the daunting task of unlearning sensitive information without compromising their integrity or performance.","Our research introduces a novel approach that leverages influence functions and principles of distributional independence to address these challenges.","By proposing a comprehensive framework for machine unlearning, we aim to ensure privacy protection while maintaining model performance and adaptability across varying distributions.","Our method not only facilitates efficient data removal but also dynamically adjusts the model to preserve its generalization capabilities.","Through extensive experimentation, we demonstrate the efficacy of our approach in scenarios characterized by significant distributional shifts, making substantial contributions to the field of machine unlearning.","This research paves the way for developing more resilient and adaptable unlearning techniques, ensuring models remain robust and accurate in the dynamic landscape of data privacy and machine learning."],"url":"http://arxiv.org/abs/2403.08124v1","category":"cs.LG"}
{"created":"2024-03-12 23:20:28","title":"6D Movable Antenna Based on User Distribution: Modeling and Optimization","abstract":"In this paper, we propose a new six-dimensional (6D) movable antenna (6DMA) system for future wireless networks to improve the communication performance. Unlike the traditional fixed-position antenna (FPA) and existing fluid antenna/two-dimensional (2D) movable antenna (FA/2DMA) systems that adjust the positions of antennas only, the proposed 6DMA system consists of distributed antenna surfaces with independently adjustable three-dimensional (3D) positions as well as 3D rotations within a given space. In particular, this paper applies the 6DMA to the base station (BS) in wireless networks to provide full degrees of freedom (DoFs) for the BS to adapt to the dynamic user spatial distribution in the network. However, a challenging new problem arises on how to optimally control the 6D positions and rotations of all 6DMA surfaces at the BS to maximize the network capacity based on the user spatial distribution, subject to the practical constraints on 6D antennas' movement. To tackle this problem, we first model the 6DMA-enabled BS and the user channels with the BS in terms of 6D positions and rotations of all 6DMA surfaces. Next, we propose an efficient alternating optimization algorithm to search for the best 6D positions and rotations of all 6DMA surfaces by leveraging the Monte Carlo simulation technique. Specifically, we sequentially optimize the 3D position/3D rotation of each 6DMA surface with those of the other surfaces fixed in an iterative manner. Numerical results show that our proposed 6DMA-BS can significantly improve the network capacity as compared to the benchmark BS architectures with FPAs or MAs with limited/partial movability, especially when the user distribution is more spatially non-uniform.","sentences":["In this paper, we propose a new six-dimensional (6D) movable antenna (6DMA) system for future wireless networks to improve the communication performance.","Unlike the traditional fixed-position antenna (FPA) and existing fluid antenna/two-dimensional (2D) movable antenna (FA/2DMA) systems that adjust the positions of antennas only, the proposed 6DMA system consists of distributed antenna surfaces with independently adjustable three-dimensional (3D) positions as well as 3D rotations within a given space.","In particular, this paper applies the 6DMA to the base station (BS) in wireless networks to provide full degrees of freedom (DoFs) for the BS to adapt to the dynamic user spatial distribution in the network.","However, a challenging new problem arises on how to optimally control the 6D positions and rotations of all 6DMA surfaces at the BS to maximize the network capacity based on the user spatial distribution, subject to the practical constraints on 6D antennas' movement.","To tackle this problem, we first model the 6DMA-enabled BS and the user channels with the BS in terms of 6D positions and rotations of all 6DMA surfaces.","Next, we propose an efficient alternating optimization algorithm to search for the best 6D positions and rotations of all 6DMA surfaces by leveraging the Monte Carlo simulation technique.","Specifically, we sequentially optimize the 3D position/3D rotation of each 6DMA surface with those of the other surfaces fixed in an iterative manner.","Numerical results show that our proposed 6DMA-BS can significantly improve the network capacity as compared to the benchmark BS architectures with FPAs or MAs with limited/partial movability, especially when the user distribution is more spatially non-uniform."],"url":"http://arxiv.org/abs/2403.08123v1","category":"cs.IT"}
{"created":"2024-03-12 23:19:28","title":"High energy dissipation rates from the impingement of free paper-thin sheets of liquids: Determination of the volume of the energy dissipation zone","abstract":"The micromixing time of impinging thin liquid sheets depends upon the energy dissipation rate. The kinetic energy released by the impingement has been previously studied and was found to be a function of the coefficient of restitution of the collision. In this work, the volume within which the released kinetic energy is dissipated was investigated. The volume of energy dissipation was determined by measuring the time required for the velocity of the single sheet (prior to the collision) to be reduced to the velocity in the mixed sheet (after the collision). Although different, the velocity in single sheets and the velocity in mixed sheets have been previously shown to be constant. High-speed video was used to measure the velocity of features, generated in the front single sheet, as they passed through the impingement zone and into the mixed sheet. The experimental results showed that the time required for the velocity change was approximately equal to the residence time of liquid in the impingement zone. A new equation for the energy dissipation rate was developed and compared with the energy dissipation rate derived from turbulence energy-cascade theory. This comparison showed that the large-eddy turnover time was approximately equal to the residence time in the impingement zone; a result that is in accordance with the notion from turbulence energy-cascade theory that large, energy-containing eddies lose their energy within approximately one large-eddy turnover time. Within the impingement zone, the large-eddy kinetic energy was found to decay exponentially with time.","sentences":["The micromixing time of impinging thin liquid sheets depends upon the energy dissipation rate.","The kinetic energy released by the impingement has been previously studied and was found to be a function of the coefficient of restitution of the collision.","In this work, the volume within which the released kinetic energy is dissipated was investigated.","The volume of energy dissipation was determined by measuring the time required for the velocity of the single sheet (prior to the collision) to be reduced to the velocity in the mixed sheet (after the collision).","Although different, the velocity in single sheets and the velocity in mixed sheets have been previously shown to be constant.","High-speed video was used to measure the velocity of features, generated in the front single sheet, as they passed through the impingement zone and into the mixed sheet.","The experimental results showed that the time required for the velocity change was approximately equal to the residence time of liquid in the impingement zone.","A new equation for the energy dissipation rate was developed and compared with the energy dissipation rate derived from turbulence energy-cascade theory.","This comparison showed that the large-eddy turnover time was approximately equal to the residence time in the impingement zone; a result that is in accordance with the notion from turbulence energy-cascade theory that large, energy-containing eddies lose their energy within approximately one large-eddy turnover time.","Within the impingement zone, the large-eddy kinetic energy was found to decay exponentially with time."],"url":"http://arxiv.org/abs/2403.08122v1","category":"physics.flu-dyn"}
{"created":"2024-03-12 22:21:48","title":"Efficient Language Model Architectures for Differentially Private Federated Learning","abstract":"Cross-device federated learning (FL) is a technique that trains a model on data distributed across typically millions of edge devices without data leaving the devices. SGD is the standard client optimizer for on device training in cross-device FL, favored for its memory and computational efficiency. However, in centralized training of neural language models, adaptive optimizers are preferred as they offer improved stability and performance. In light of this, we ask if language models can be modified such that they can be efficiently trained with SGD client optimizers and answer this affirmatively.   We propose a scale-invariant Coupled Input Forget Gate (SI CIFG) recurrent network by modifying the sigmoid and tanh activations in the recurrent cell and show that this new model converges faster and achieves better utility than the standard CIFG recurrent model in cross-device FL in large scale experiments. We further show that the proposed scale invariant modification also helps in federated learning of larger transformer models. Finally, we demonstrate the scale invariant modification is also compatible with other non-adaptive algorithms. Particularly, our results suggest an improved privacy utility trade-off in federated learning with differential privacy.","sentences":["Cross-device federated learning (FL) is a technique that trains a model on data distributed across typically millions of edge devices without data leaving the devices.","SGD is the standard client optimizer for on device training in cross-device FL, favored for its memory and computational efficiency.","However, in centralized training of neural language models, adaptive optimizers are preferred as they offer improved stability and performance.","In light of this, we ask if language models can be modified such that they can be efficiently trained with SGD client optimizers and answer this affirmatively.   ","We propose a scale-invariant Coupled Input Forget Gate (SI CIFG) recurrent network by modifying the sigmoid and tanh activations in the recurrent cell and show that this new model converges faster and achieves better utility than the standard CIFG recurrent model in cross-device FL in large scale experiments.","We further show that the proposed scale invariant modification also helps in federated learning of larger transformer models.","Finally, we demonstrate the scale invariant modification is also compatible with other non-adaptive algorithms.","Particularly, our results suggest an improved privacy utility trade-off in federated learning with differential privacy."],"url":"http://arxiv.org/abs/2403.08100v1","category":"cs.LG"}
{"created":"2024-03-12 22:19:58","title":"Application of Distributed Arithmetic to Adaptive Filtering Algorithms: Trends, Challenges and Future","abstract":"The utilization of distributed arithmetic (DA) in AF algorithms has gained significant attention in recent years due to its potential to enhance computational efficiency and reduce resource requirements. This paper presents an exploration of the application of DA to adaptive filtering (AF) algorithms, analyzing trends, discussing challenges, and outlining future prospects. It begins by providing an overview of both DA and AF algorithms, highlighting their individual merits and established applications. Subsequently, the integration of DA into AF algorithms is explored, showcasing its ability to optimize multiply-accumulate operations and mitigate the computational burden associated with AF algorithms. Throughout the paper, the critical trends observed in the field are discussed, including advancements in DA-based hardware architectures. Moreover, the challenges encountered in implementing DA-based AF is also discussed. The continued evolution of DA techniques to cater to the demands of modern AF applications, including real-time processing, resource-constrained environments, and high-dimensional data streams is anticipated. In conclusion, this paper consolidates the current state of applying DA to AF algorithms, offering insights into prevailing trends, discussing challenges, and presenting future research and development in the field. The fusion of these two domains holds promise for achieving improved computational efficiency, reduced hardware complexity, and enhanced performance in various signal processing applications.","sentences":["The utilization of distributed arithmetic (DA) in AF algorithms has gained significant attention in recent years due to its potential to enhance computational efficiency and reduce resource requirements.","This paper presents an exploration of the application of DA to adaptive filtering (AF) algorithms, analyzing trends, discussing challenges, and outlining future prospects.","It begins by providing an overview of both DA and AF algorithms, highlighting their individual merits and established applications.","Subsequently, the integration of DA into AF algorithms is explored, showcasing its ability to optimize multiply-accumulate operations and mitigate the computational burden associated with AF algorithms.","Throughout the paper, the critical trends observed in the field are discussed, including advancements in DA-based hardware architectures.","Moreover, the challenges encountered in implementing DA-based AF is also discussed.","The continued evolution of DA techniques to cater to the demands of modern AF applications, including real-time processing, resource-constrained environments, and high-dimensional data streams is anticipated.","In conclusion, this paper consolidates the current state of applying DA to AF algorithms, offering insights into prevailing trends, discussing challenges, and presenting future research and development in the field.","The fusion of these two domains holds promise for achieving improved computational efficiency, reduced hardware complexity, and enhanced performance in various signal processing applications."],"url":"http://arxiv.org/abs/2403.08099v1","category":"eess.SY"}
{"created":"2024-03-12 19:36:43","title":"Neural, Muscular, and Perceptual responses with shoulder exoskeleton use over Days","abstract":"Passive shoulder exoskeletons have been widely introduced in the industry to aid upper extremity movements during repetitive overhead work. As an ergonomic intervention, it is important to understand how users adapt to these devices over time and if these induce external stress while working. The study evaluated the use of an exoskeleton over a period of 3 days by assessing the neural, physiological, and perceptual responses of twenty-four participants by comparing a physical task against the same task with an additional cognitive workload. Over days adaptation to task irrespective of task and group were identified. Electromyography (EMG) analysis of shoulder and back muscles reveals lower muscle activity in the exoskeleton group irrespective of task. Functional connectivity analysis using functional near infrared spectroscopy (fNIRS) reveals that exoskeletons benefit users by reducing task demands in the motor planning and execution regions. Sex-based differences were also identified in these neuromuscular assessments.","sentences":["Passive shoulder exoskeletons have been widely introduced in the industry to aid upper extremity movements during repetitive overhead work.","As an ergonomic intervention, it is important to understand how users adapt to these devices over time and if these induce external stress while working.","The study evaluated the use of an exoskeleton over a period of 3 days by assessing the neural, physiological, and perceptual responses of twenty-four participants by comparing a physical task against the same task with an additional cognitive workload.","Over days adaptation to task irrespective of task and group were identified.","Electromyography (EMG) analysis of shoulder and back muscles reveals lower muscle activity in the exoskeleton group irrespective of task.","Functional connectivity analysis using functional near infrared spectroscopy (fNIRS) reveals that exoskeletons benefit users by reducing task demands in the motor planning and execution regions.","Sex-based differences were also identified in these neuromuscular assessments."],"url":"http://arxiv.org/abs/2403.08044v1","category":"q-bio.QM"}
{"created":"2024-03-12 19:34:50","title":"CT evaluation of 2D and 3D holistic deep learning methods for the volumetric segmentation of airway lesions","abstract":"This research embarked on a comparative exploration of the holistic segmentation capabilities of Convolutional Neural Networks (CNNs) in both 2D and 3D formats, focusing on cystic fibrosis (CF) lesions. The study utilized data from two CF reference centers, covering five major CF structural changes. Initially, it compared the 2D and 3D models, highlighting the 3D model's superior capability in capturing complex features like mucus plugs and consolidations. To improve the 2D model's performance, a loss adapted to fine structures segmentation was implemented and evaluated, significantly enhancing its accuracy, though not surpassing the 3D model's performance. The models underwent further validation through external evaluation against pulmonary function tests (PFTs), confirming the robustness of the findings. Moreover, this study went beyond comparing metrics; it also included comprehensive assessments of the models' interpretability and reliability, providing valuable insights for their clinical application.","sentences":["This research embarked on a comparative exploration of the holistic segmentation capabilities of Convolutional Neural Networks (CNNs) in both 2D and 3D formats, focusing on cystic fibrosis (CF) lesions.","The study utilized data from two CF reference centers, covering five major CF structural changes.","Initially, it compared the 2D and 3D models, highlighting the 3D model's superior capability in capturing complex features like mucus plugs and consolidations.","To improve the 2D model's performance, a loss adapted to fine structures segmentation was implemented and evaluated, significantly enhancing its accuracy, though not surpassing the 3D model's performance.","The models underwent further validation through external evaluation against pulmonary function tests (PFTs), confirming the robustness of the findings.","Moreover, this study went beyond comparing metrics; it also included comprehensive assessments of the models' interpretability and reliability, providing valuable insights for their clinical application."],"url":"http://arxiv.org/abs/2403.08042v1","category":"eess.IV"}
{"created":"2024-03-12 19:23:13","title":"MicroT: Low-Energy and Adaptive Models for MCUs","abstract":"We propose MicroT, a low-energy, multi-task adaptive model framework for resource-constrained MCUs. We divide the original model into a feature extractor and a classifier. The feature extractor is obtained through self-supervised knowledge distillation and further optimized into part and full models through model splitting and joint training. These models are then deployed on MCUs, with classifiers added and trained on local tasks, ultimately performing stage-decision for joint inference. In this process, the part model initially processes the sample, and if the confidence score falls below the set threshold, the full model will resume and continue the inference. We evaluate MicroT on two models, three datasets, and two MCU boards. Our experimental evaluation shows that MicroT effectively improves model performance and reduces energy consumption when dealing with multiple local tasks. Compared to the unoptimized feature extractor, MicroT can improve accuracy by up to 9.87%. On MCUs, compared to the standard full model inference, MicroT can save up to about 29.13% in energy consumption. MicroT also allows users to adaptively adjust the stage-decision ratio as needed, better balancing model performance and energy consumption. Under the standard stage-decision ratio configuration, MicroT can increase accuracy by 5.91% and save about 14.47% of energy consumption.","sentences":["We propose MicroT, a low-energy, multi-task adaptive model framework for resource-constrained MCUs.","We divide the original model into a feature extractor and a classifier.","The feature extractor is obtained through self-supervised knowledge distillation and further optimized into part and full models through model splitting and joint training.","These models are then deployed on MCUs, with classifiers added and trained on local tasks, ultimately performing stage-decision for joint inference.","In this process, the part model initially processes the sample, and if the confidence score falls below the set threshold, the full model will resume and continue the inference.","We evaluate MicroT on two models, three datasets, and two MCU boards.","Our experimental evaluation shows that MicroT effectively improves model performance and reduces energy consumption when dealing with multiple local tasks.","Compared to the unoptimized feature extractor, MicroT can improve accuracy by up to 9.87%.","On MCUs, compared to the standard full model inference, MicroT can save up to about 29.13% in energy consumption.","MicroT also allows users to adaptively adjust the stage-decision ratio as needed, better balancing model performance and energy consumption.","Under the standard stage-decision ratio configuration, MicroT can increase accuracy by 5.91% and save about 14.47% of energy consumption."],"url":"http://arxiv.org/abs/2403.08040v1","category":"cs.LG"}
{"created":"2024-03-12 18:27:15","title":"The intermittently-resonant coevolution of migrating planets and their pulsating stars","abstract":"Hot Jupiters are expected to form far from their host star and move toward close-in, circular orbits via a smooth, monotonic decay due to mild and constant tidal dissipation. Yet, three systems have recently been found exhibiting planet-induced stellar pulsations suggesting unexpectedly strong tidal interactions. Here we combine stellar evolution and tide models to show that dynamical tides raised by eccentric gas giants can give rise to chains of resonance locks with multiple modes, enriching the dynamics seen in single-mode resonance locking of circularized systems. These series of resonance locks yield orders-of-magnitude larger changes in eccentricity and harmonic pulsations relative to those expected from a single episode of resonance locking or nonresonant tidal interactions. Resonances become more frequent as a star evolves off the main sequence providing an alternative explanation to the origin of some stellar pulsators and yielding the concept of \"dormant migrating giants\". Evolution trajectories are characterized by competing episodes of inward/outward migration and spin-up/-down of the star which are sensitive to the system parameters, revealing a new challenge in modeling migration paths and in contextualizing the observed populations of giant exoplanets and stellar binaries. This sensitivity however offers a new window to constrain the stellar properties of planetary hosts via tidal asteroseismology.","sentences":["Hot Jupiters are expected to form far from their host star and move toward close-in, circular orbits via a smooth, monotonic decay due to mild and constant tidal dissipation.","Yet, three systems have recently been found exhibiting planet-induced stellar pulsations suggesting unexpectedly strong tidal interactions.","Here we combine stellar evolution and tide models to show that dynamical tides raised by eccentric gas giants can give rise to chains of resonance locks with multiple modes, enriching the dynamics seen in single-mode resonance locking of circularized systems.","These series of resonance locks yield orders-of-magnitude larger changes in eccentricity and harmonic pulsations relative to those expected from a single episode of resonance locking or nonresonant tidal interactions.","Resonances become more frequent as a star evolves off the main sequence providing an alternative explanation to the origin of some stellar pulsators and yielding the concept of \"dormant migrating giants\".","Evolution trajectories are characterized by competing episodes of inward/outward migration and spin-up/-down of the star which are sensitive to the system parameters, revealing a new challenge in modeling migration paths and in contextualizing the observed populations of giant exoplanets and stellar binaries.","This sensitivity however offers a new window to constrain the stellar properties of planetary hosts via tidal asteroseismology."],"url":"http://arxiv.org/abs/2403.08014v1","category":"astro-ph.EP"}
{"created":"2024-03-12 18:12:02","title":"Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging","abstract":"The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such large models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world applications. Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data. In this paper, we explore training open-source small multimodal models (SMMs) to bridge biomedical competency gaps for unmet clinical needs. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on training a lightweight adapter to ground each modality to the text embedding space. We conduct a comprehensive study of this approach on radiology imaging. For training, we assemble a large dataset with over 1 million image-text pairs. For evaluation, we propose a clinically driven novel approach using GPT-4 and demonstrate its parity with expert evaluation. We also study grounding qualitatively using attention. For best practice, we conduct a systematic ablation study on various choices in data engineering and multimodal training. The resulting LLaVA-Rad (7B) model attains state-of-the-art results on radiology tasks such as report generation and cross-modal retrieval, even outperforming much larger models such as GPT-4V and Med-PaLM M (84B). LLaVA-Rad is fast and can be run on a single V100 GPU in private settings, offering a promising state-of-the-art tool for real-world clinical applications.","sentences":["The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such large models in biomedicine.","However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world applications.","Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications.","Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data.","In this paper, we explore training open-source small multimodal models (SMMs) to bridge biomedical competency gaps for unmet clinical needs.","To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on training a lightweight adapter to ground each modality to the text embedding space.","We conduct a comprehensive study of this approach on radiology imaging.","For training, we assemble a large dataset with over 1 million image-text pairs.","For evaluation, we propose a clinically driven novel approach using GPT-4 and demonstrate its parity with expert evaluation.","We also study grounding qualitatively using attention.","For best practice, we conduct a systematic ablation study on various choices in data engineering and multimodal training.","The resulting LLaVA-Rad (7B) model attains state-of-the-art results on radiology tasks such as report generation and cross-modal retrieval, even outperforming much larger models such as GPT-4V and Med-PaLM M (84B).","LLaVA-Rad is fast and can be run on a single V100 GPU in private settings, offering a promising state-of-the-art tool for real-world clinical applications."],"url":"http://arxiv.org/abs/2403.08002v1","category":"cs.CL"}
{"created":"2024-03-12 18:01:10","title":"Dynamic Field of View Reduction Related to Subjective Sickness Measures in an HMD-based Data Analysis Task","abstract":"Various factors influence the degree of cybersickness a user can suffer in an immersive virtual environment, some of which can be controlled without adapting the virtual environment itself. When using HMDs, one example is the size of the field of view. However, the degree to which factors like this can be manipulated without affecting the user negatively in other ways is limited. Another prominent characteristic of cybersickness is that it affects individuals very differently. Therefore, to account for both the possible disruptive nature of alleviating factors and the high interpersonal variance, a promising approach may be to intervene only in cases where users experience discomfort symptoms, and only as much as necessary. Thus, we conducted a first experiment, where the field of view was decreased when people feel uncomfortable, to evaluate the possible positive impact on sickness and negative influence on presence. While we found no significant evidence for any of these possible effects, interesting further results and observations were made.","sentences":["Various factors influence the degree of cybersickness a user can suffer in an immersive virtual environment, some of which can be controlled without adapting the virtual environment itself.","When using HMDs, one example is the size of the field of view.","However, the degree to which factors like this can be manipulated without affecting the user negatively in other ways is limited.","Another prominent characteristic of cybersickness is that it affects individuals very differently.","Therefore, to account for both the possible disruptive nature of alleviating factors and the high interpersonal variance, a promising approach may be to intervene only in cases where users experience discomfort symptoms, and only as much as necessary.","Thus, we conducted a first experiment, where the field of view was decreased when people feel uncomfortable, to evaluate the possible positive impact on sickness and negative influence on presence.","While we found no significant evidence for any of these possible effects, interesting further results and observations were made."],"url":"http://arxiv.org/abs/2403.07992v1","category":"cs.HC"}
{"created":"2024-03-12 18:00:58","title":"Dissipative frequency converter: from Lindblad dynamics to non-Hermitian topology","abstract":"A topological frequency converter represents a dynamical counterpart of the integer quantum Hall effect, where a two-level system enacts a quantized time-averaged power transfer between two driving modes of incommensurate frequency. Here, we investigate as to what extent temporal coherence in the quantum dynamics of the two-level system is important for the topological quantization of the converter. To this end, we consider dissipative channels corresponding to spontaneous decay and dephasing in the instantaneous eigenbasis of the Hamiltonian as well as spontaneous decay in a fixed basis. The dissipation is modelled using both a full Lindblad and an effective non-Hermitian (NH) Hamiltonian description. For all three dissipation channels we find a transition from the unperturbed dynamics to a quantum watchdog effect, which destroys any power transfer in the strong coupling limit. This is striking because the watchdog effect leads to perfectly adiabatic dynamics in the instantaneous eigenbasis, at first glance similar to the unperturbed case. Furthermore, it is found that dephasing immediately leads to an exponential decay of the power transfer in time due to loss of polarisation in the mixed quantum state. Finally, we discuss the appearance in the effective NH trajectory description of non-adiabatic processes, which are suppressed in the full Lindblad dynamics.","sentences":["A topological frequency converter represents a dynamical counterpart of the integer quantum Hall effect, where a two-level system enacts a quantized time-averaged power transfer between two driving modes of incommensurate frequency.","Here, we investigate as to what extent temporal coherence in the quantum dynamics of the two-level system is important for the topological quantization of the converter.","To this end, we consider dissipative channels corresponding to spontaneous decay and dephasing in the instantaneous eigenbasis of the Hamiltonian as well as spontaneous decay in a fixed basis.","The dissipation is modelled using both a full Lindblad and an effective non-Hermitian (NH) Hamiltonian description.","For all three dissipation channels we find a transition from the unperturbed dynamics to a quantum watchdog effect, which destroys any power transfer in the strong coupling limit.","This is striking because the watchdog effect leads to perfectly adiabatic dynamics in the instantaneous eigenbasis, at first glance similar to the unperturbed case.","Furthermore, it is found that dephasing immediately leads to an exponential decay of the power transfer in time due to loss of polarisation in the mixed quantum state.","Finally, we discuss the appearance in the effective NH trajectory description of non-adiabatic processes, which are suppressed in the full Lindblad dynamics."],"url":"http://arxiv.org/abs/2403.07991v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-12 18:00:00","title":"Superphot+: Realtime Fitting and Classification of Supernova Light Curves","abstract":"Photometric classifications of supernova (SN) light curves have become necessary to utilize the full potential of large samples of observations obtained from wide-field photometric surveys, such as the Zwicky Transient Facility (ZTF) and the Vera C. Rubin Observatory. Here, we present a photometric classifier for SN light curves that does not rely on redshift information and still maintains comparable accuracy to redshift-dependent classifiers. Our new package, Superphot+, uses a parametric model to extract meaningful features from multiband SN light curves. We train a gradient-boosted machine with fit parameters from 6,061 ZTF SNe that pass data quality cuts and are spectroscopically classified as one of five classes: SN Ia, SN II, SN Ib/c, SN IIn, and SLSN-I. Without redshift information, our classifier yields a class-averaged F1-score of 0.61 +/- 0.02 and a total accuracy of 0.83 +/- 0.01. Including redshift information improves these metrics to 0.71 +/- 0.02 and 0.88 +/- 0.01, respectively. We assign new class probabilities to 3,558 ZTF transients that show SN-like characteristics (based on the ALeRCE Broker light curve and stamp classifiers), but lack spectroscopic classifications. Finally, we compare our predicted SN labels with those generated by the ALeRCE light curve classifier, finding that the two classifiers agree on photometric labels for 82 +/- 2% of light curves with spectroscopic labels and 72% of light curves without spectroscopic labels. Superphot+ is currently classifying ZTF SNe in real time via the ANTARES Broker, and is designed for simple adaptation to six-band Rubin light curves in the future.","sentences":["Photometric classifications of supernova (SN) light curves have become necessary to utilize the full potential of large samples of observations obtained from wide-field photometric surveys, such as the Zwicky Transient Facility (ZTF) and the Vera C. Rubin Observatory.","Here, we present a photometric classifier for SN light curves that does not rely on redshift information and still maintains comparable accuracy to redshift-dependent classifiers.","Our new package, Superphot+, uses a parametric model to extract meaningful features from multiband SN light curves.","We train a gradient-boosted machine with fit parameters from 6,061 ZTF SNe that pass data quality cuts and are spectroscopically classified as one of five classes: SN Ia, SN II, SN Ib/c, SN IIn, and SLSN-I. Without redshift information, our classifier yields a class-averaged F1-score of 0.61 +/- 0.02 and a total accuracy of 0.83 +/- 0.01.","Including redshift information improves these metrics to 0.71 +/- 0.02 and 0.88 +/- 0.01, respectively.","We assign new class probabilities to 3,558 ZTF transients that show SN-like characteristics (based on the ALeRCE Broker light curve and stamp classifiers), but lack spectroscopic classifications.","Finally, we compare our predicted SN labels with those generated by the ALeRCE light curve classifier, finding that the two classifiers agree on photometric labels for 82 +/- 2% of light curves with spectroscopic labels and 72% of light curves without spectroscopic labels.","Superphot+ is currently classifying ZTF SNe in real time via the ANTARES Broker, and is designed for simple adaptation to six-band Rubin light curves in the future."],"url":"http://arxiv.org/abs/2403.07975v1","category":"astro-ph.HE"}
{"created":"2024-03-13 17:58:00","title":"MIM4D: Masked Modeling with Multi-View Video for Autonomous Driving Representation Learning","abstract":"Learning robust and scalable visual representations from massive multi-view video data remains a challenge in computer vision and autonomous driving. Existing pre-training methods either rely on expensive supervised learning with 3D annotations, limiting the scalability, or focus on single-frame or monocular inputs, neglecting the temporal information. We propose MIM4D, a novel pre-training paradigm based on dual masked image modeling (MIM). MIM4D leverages both spatial and temporal relations by training on masked multi-view video inputs. It constructs pseudo-3D features using continuous scene flow and projects them onto 2D plane for supervision. To address the lack of dense 3D supervision, MIM4D reconstruct pixels by employing 3D volumetric differentiable rendering to learn geometric representations. We demonstrate that MIM4D achieves state-of-the-art performance on the nuScenes dataset for visual representation learning in autonomous driving. It significantly improves existing methods on multiple downstream tasks, including BEV segmentation (8.7% IoU), 3D object detection (3.5% mAP), and HD map construction (1.4% mAP). Our work offers a new choice for learning representation at scale in autonomous driving. Code and models are released at https://github.com/hustvl/MIM4D","sentences":["Learning robust and scalable visual representations from massive multi-view video data remains a challenge in computer vision and autonomous driving.","Existing pre-training methods either rely on expensive supervised learning with 3D annotations, limiting the scalability, or focus on single-frame or monocular inputs, neglecting the temporal information.","We propose MIM4D, a novel pre-training paradigm based on dual masked image modeling (MIM).","MIM4D leverages both spatial and temporal relations by training on masked multi-view video inputs.","It constructs pseudo-3D features using continuous scene flow and projects them onto 2D plane for supervision.","To address the lack of dense 3D supervision, MIM4D reconstruct pixels by employing 3D volumetric differentiable rendering to learn geometric representations.","We demonstrate that MIM4D achieves state-of-the-art performance on the nuScenes dataset for visual representation learning in autonomous driving.","It significantly improves existing methods on multiple downstream tasks, including BEV segmentation (8.7% IoU), 3D object detection (3.5% mAP), and HD map construction (1.4% mAP).","Our work offers a new choice for learning representation at scale in autonomous driving.","Code and models are released at https://github.com/hustvl/MIM4D"],"url":"http://arxiv.org/abs/2403.08760v1","category":"cs.CV"}
{"created":"2024-03-13 17:57:26","title":"Observational tests in scale invariance I: galaxy clusters and rotation of galaxies","abstract":"Galaxy velocities in clusters, rotation curves of galaxies, and \"vertical\" oscillations in the Milky Way currently show too high velocities with respect to the masses thought to be involved. While these velocity excesses are currently interpreted as the consequence of dark matter, it can also be naturally explained as a consequence of scale invariant theory, which rests on a very simple first principle: the addition of a new fundamental symmetry. In the present work, the case of scale invariance, present in General Relativity and Maxwell equations for the empty space without charge and current, is considered. Cosmological models predict a rapid decrease of these effects with increasing mean density up to the critical density, where they totally disappear. Starting from the scale invariant geodesic equation by Dirac (1973), for which a demonstration by an action principle is presented, a modified Newton equation is derived. The solutions of this equation are applied to clusters of galaxies, galactic rotation at different redshifts and \"vertical\" motions in the Milky Way. In this new framework, the convergence of theoretical predictions and observations, in different gravitational systems, epochs, mass range and spatial scales, opens interesting perspectives that deserve to be explored further.","sentences":["Galaxy velocities in clusters, rotation curves of galaxies, and \"vertical\" oscillations in the Milky Way currently show too high velocities with respect to the masses thought to be involved.","While these velocity excesses are currently interpreted as the consequence of dark matter, it can also be naturally explained as a consequence of scale invariant theory, which rests on a very simple first principle: the addition of a new fundamental symmetry.","In the present work, the case of scale invariance, present in General Relativity and Maxwell equations for the empty space without charge and current, is considered.","Cosmological models predict a rapid decrease of these effects with increasing mean density up to the critical density, where they totally disappear.","Starting from the scale invariant geodesic equation by Dirac (1973), for which a demonstration by an action principle is presented, a modified Newton equation is derived.","The solutions of this equation are applied to clusters of galaxies, galactic rotation at different redshifts and \"vertical\" motions in the Milky Way.","In this new framework, the convergence of theoretical predictions and observations, in different gravitational systems, epochs, mass range and spatial scales, opens interesting perspectives that deserve to be explored further."],"url":"http://arxiv.org/abs/2403.08759v1","category":"astro-ph.GA"}
{"created":"2024-03-13 17:51:33","title":"A local model for the optical energy and momentum transfer in dielectric media and the microscopic origin of Abraham's force density","abstract":"We report on the continuity equations for linear momentum and energy associated to a recently introduced electromagnetic formulation based on classical dipolar sources [Eur. Phys. J. Plus 138, 1034 (2023)]. When connected to the mass-polariton quasi-particle dynamics, these equations provide a consistent microscopic description of the local optical energy and momentum transfer inside dielectric media, called microscopic mass-polariton formulation. This procedure also unveils the true microscopic origin of the long-known Abraham optical force density as an interplay between induced dipoles and mechanical stresses generated within the material.","sentences":["We report on the continuity equations for linear momentum and energy associated to a recently introduced electromagnetic formulation based on classical dipolar sources [Eur. Phys.","J. Plus 138, 1034 (2023)].","When connected to the mass-polariton quasi-particle dynamics, these equations provide a consistent microscopic description of the local optical energy and momentum transfer inside dielectric media, called microscopic mass-polariton formulation.","This procedure also unveils the true microscopic origin of the long-known Abraham optical force density as an interplay between induced dipoles and mechanical stresses generated within the material."],"url":"http://arxiv.org/abs/2403.08752v1","category":"physics.optics"}
{"created":"2024-03-13 17:47:02","title":"Boundary controllability for a fourth order degenerate parabolic equation with a singular potential","abstract":"In this paper, we prove the null controllability of a one-dimensional fourth-order degenerate parabolic equation with a singular potential. Here, we analyze cases where boundary control conditions are applied at the left endpoint. We utilize a spectral decomposition involving Bessel functions and their zeros in a convenient weighted Sobolev space for a degenerate parabolic operator with specific boundary conditions. We establish the well-posedness of the system using semigroup operator theory. Subsequently, we employ the moment method by Fattorini and Russell to obtain an upper estimate of the cost of controllability. Additionally, we derive a lower estimate of the cost of controllability using a representation theorem for analytic functions of exponential type.","sentences":["In this paper, we prove the null controllability of a one-dimensional fourth-order degenerate parabolic equation with a singular potential.","Here, we analyze cases where boundary control conditions are applied at the left endpoint.","We utilize a spectral decomposition involving Bessel functions and their zeros in a convenient weighted Sobolev space for a degenerate parabolic operator with specific boundary conditions.","We establish the well-posedness of the system using semigroup operator theory.","Subsequently, we employ the moment method by Fattorini and Russell to obtain an upper estimate of the cost of controllability.","Additionally, we derive a lower estimate of the cost of controllability using a representation theorem for analytic functions of exponential type."],"url":"http://arxiv.org/abs/2403.08745v1","category":"math.AP"}
{"created":"2024-03-13 17:38:05","title":"ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation","abstract":"Existing Machine Learning approaches for local citation recommendation directly map or translate a query, which is typically a claim or an entity mention, to citation-worthy research papers. Within such a formulation, it is challenging to pinpoint why one should cite a specific research paper for a particular query, leading to limited recommendation interpretability. To alleviate this, we introduce the evidence-grounded local citation recommendation task, where the target latent space comprises evidence spans for recommending specific papers. Using a distantly-supervised evidence retrieval and multi-step re-ranking framework, our proposed system, ILCiteR, recommends papers to cite for a query grounded on similar evidence spans extracted from the existing research literature. Unlike past formulations that simply output recommendations, ILCiteR retrieves ranked lists of evidence span and recommended paper pairs. Secondly, previously proposed neural models for citation recommendation require expensive training on massive labeled data, ideally after every significant update to the pool of candidate papers. In contrast, ILCiteR relies solely on distant supervision from a dynamic evidence database and pre-trained Transformer-based Language Models without any model training. We contribute a novel dataset for the evidence-grounded local citation recommendation task and demonstrate the efficacy of our proposed conditional neural rank-ensembling approach for re-ranking evidence spans.","sentences":["Existing Machine Learning approaches for local citation recommendation directly map or translate a query, which is typically a claim or an entity mention, to citation-worthy research papers.","Within such a formulation, it is challenging to pinpoint why one should cite a specific research paper for a particular query, leading to limited recommendation interpretability.","To alleviate this, we introduce the evidence-grounded local citation recommendation task, where the target latent space comprises evidence spans for recommending specific papers.","Using a distantly-supervised evidence retrieval and multi-step re-ranking framework, our proposed system, ILCiteR, recommends papers to cite for a query grounded on similar evidence spans extracted from the existing research literature.","Unlike past formulations that simply output recommendations, ILCiteR retrieves ranked lists of evidence span and recommended paper pairs.","Secondly, previously proposed neural models for citation recommendation require expensive training on massive labeled data, ideally after every significant update to the pool of candidate papers.","In contrast, ILCiteR relies solely on distant supervision from a dynamic evidence database and pre-trained Transformer-based Language Models without any model training.","We contribute a novel dataset for the evidence-grounded local citation recommendation task and demonstrate the efficacy of our proposed conditional neural rank-ensembling approach for re-ranking evidence spans."],"url":"http://arxiv.org/abs/2403.08737v1","category":"cs.IR"}
{"created":"2024-03-13 17:21:07","title":"Isotope effects in supercooled H$_2$O and D$_2$O and a corresponding-states-like rescaling of the temperature and pressure","abstract":"Water shows anomalous properties that are enhanced upon supercooling. The unusual behavior is observed in both H$_2$O and D$_2$O, however with different temperature dependences for the two isotopes. It is often noted that comparing the properties of the isotopes at two different temperatures (i.e., a temperature shift) approximately accounts for many of the observations with a temperature shift of 7.2 K in the temperature of maximum density being the most well-known example. However, the physical justification for such a shift is unclear. Motivated by recent work demonstrating a corresponding-states-like rescaling for water properties in three classical water models that all exhibit a liquid-liquid transition and critical point (B. Uralcan, et al., J. Chem. Phys. 150, 064503 (2019)), the applicability of this approach for reconciling the differences in temperature- and pressure-dependent thermodynamic properties of H$_2$O and D$_2$O is investigated here. Utilizing previously published data and equations-of-state for H$_2$O and D$_2$O, we show that the available data and models for these isotopes are consistent with such a low temperature correspondence. These observations provide support for the hypothesis that a liquid-liquid critical point, which is predicted to occur at low temperatures and high pressures, is the origin of many of water's anomalies.","sentences":["Water shows anomalous properties that are enhanced upon supercooling.","The unusual behavior is observed in both H$_2$O and D$_2$O, however with different temperature dependences for the two isotopes.","It is often noted that comparing the properties of the isotopes at two different temperatures (i.e., a temperature shift) approximately accounts for many of the observations with a temperature shift of 7.2 K in the temperature of maximum density being the most well-known example.","However, the physical justification for such a shift is unclear.","Motivated by recent work demonstrating a corresponding-states-like rescaling for water properties in three classical water models that all exhibit a liquid-liquid transition and critical point (B. Uralcan, et al., J. Chem.","Phys. 150, 064503 (2019)), the applicability of this approach for reconciling the differences in temperature- and pressure-dependent thermodynamic properties of H$_2$O and D$_2$O is investigated here.","Utilizing previously published data and equations-of-state for H$_2$O and D$_2$O, we show that the available data and models for these isotopes are consistent with such a low temperature correspondence.","These observations provide support for the hypothesis that a liquid-liquid critical point, which is predicted to occur at low temperatures and high pressures, is the origin of many of water's anomalies."],"url":"http://arxiv.org/abs/2403.08722v1","category":"physics.chem-ph"}
{"created":"2024-03-13 17:15:38","title":"On the geometric phase and its role in the design of elastic topological materials","abstract":"The geometric phase provides important mathematical insights to understand the occurrence and evolution of the dynamic response in a diverse spectrum of systems ranging from quantum to classical mechanics. While the concept of geometric phase, which is an additional phase factor occurring in dynamical systems, holds the same meaning across different fields of application, its use and interpretation can acquire important nuances specific to the system of interest. In recent years, the development of the concept of quantum topological materials and its extension to classical mechanical systems have renewed the interest in the study of the geometric phase. This study reviews the concept of geometric phase and discusses, by means of either established or original results, its role in the design of elastic materials. Concepts of differential geometry and topology are put forward to provide a theoretical understanding of the geometric phase and its connection to the physical properties of the system. Then, the concept of geometric phase is applied to different types of elastic waveguides to explain how either topologically trivial or non-trivial behavior can emerge based on a proper geometric design of the waveguide.","sentences":["The geometric phase provides important mathematical insights to understand the occurrence and evolution of the dynamic response in a diverse spectrum of systems ranging from quantum to classical mechanics.","While the concept of geometric phase, which is an additional phase factor occurring in dynamical systems, holds the same meaning across different fields of application, its use and interpretation can acquire important nuances specific to the system of interest.","In recent years, the development of the concept of quantum topological materials and its extension to classical mechanical systems have renewed the interest in the study of the geometric phase.","This study reviews the concept of geometric phase and discusses, by means of either established or original results, its role in the design of elastic materials.","Concepts of differential geometry and topology are put forward to provide a theoretical understanding of the geometric phase and its connection to the physical properties of the system.","Then, the concept of geometric phase is applied to different types of elastic waveguides to explain how either topologically trivial or non-trivial behavior can emerge based on a proper geometric design of the waveguide."],"url":"http://arxiv.org/abs/2403.08711v1","category":"physics.app-ph"}
{"created":"2024-03-13 17:15:05","title":"On the Microlocal Regularity of the Gevrey Vectors for second order partial differential operators with non negative characteristic form of first kind","abstract":"We study the microlocal regularity of the analytic/Gevrey vectors for the following class of second order partial differential equations \\begin{align*}   P(x,D) = \\sum_{\\ell,j=1}^{n} a_{\\ell,j}(x) D_{\\ell} D_{j} + \\sum_{\\ell=1}^{n} i b_{\\ell}(x) D_{\\ell} +c(x), \\end{align*} where $a_{\\ell,j}(x) = a_{j,\\ell}(x)$, $b_{\\ell}(x)$, $\\ell,j \\in \\lbrace 1,\\dots,\\, n\\rbrace$, are real valued real Gevrey functions of order $s$ and $c(x)$ is a Gevrey function of order $s$, $s \\geq 1$, on $\\Omega$ open neighborhood of the origin in $\\mathbb{R}^{n}$. Thus providing a microlocal version of a result due to M. Derridj in \"Gevrey regularity of Gevrey vectors of second order partial differential operators with non negative characteristic form\", Complex Anal. Synerg. $\\mathbf{6}$, 10 (2020), https://doi.org/10.1007/s40627-020-00047-8.","sentences":["We study the microlocal regularity of the analytic/Gevrey vectors for the following class of second order partial differential equations \\begin{align*}   P(x,D) = \\sum_{\\ell,j=1}^{n} a_{\\ell,j}(x) D_{\\ell} D_{j} + \\sum_{\\ell=1}^{n} i b_{\\ell}(x) D_{\\ell} +c(x), \\end{align*} where $a_{\\ell,j}(x)","= a_{j,\\ell}(x)$, $b_{\\ell}(x)$, $\\ell,j \\in \\lbrace 1,\\dots,\\, n\\rbrace$, are real valued real Gevrey functions of order $s$ and $c(x)$ is a Gevrey function of order $s$, $s \\geq 1$, on $\\Omega$ open neighborhood of the origin in $\\mathbb{R}^{n}$. Thus providing a microlocal version of a result due to M. Derridj in \"Gevrey regularity of Gevrey vectors of second order partial differential operators with non negative characteristic form\", Complex Anal.","Synerg.","$\\mathbf{6}$, 10 (2020), https://doi.org/10.1007/s40627-020-00047-8."],"url":"http://arxiv.org/abs/2403.08709v1","category":"math.AP"}
{"created":"2024-03-13 17:12:36","title":"Improved Randomized Approximation of Hard Universality and Emptiness Problems","abstract":"We build on recent research on polynomial randomized approximation (PRAX) algorithms for the hard problems of NFA universality and NFA equivalence. Loosely speaking, PRAX algorithms use sampling of infinite domains within any desired accuracy $\\delta$. In the spirit of experimental mathematics, we extend the concept of PRAX algorithms to be applicable to the emptiness and universality problems in any domain whose instances admit a tractable distribution as defined in this paper. A technical result here is that a linear (w.r.t. $1/\\delta$) number of samples is sufficient, as opposed to the quadratic number of samples in previous papers. We show how the improved and generalized PRAX algorithms apply to universality and emptiness problems in various domains: ordinary automata, tautology testing of propositions, 2D automata, and to solution sets of certain Diophantine equations.","sentences":["We build on recent research on polynomial randomized approximation (PRAX) algorithms for the hard problems of NFA universality and NFA equivalence.","Loosely speaking, PRAX algorithms use sampling of infinite domains within any desired accuracy $\\delta$.","In the spirit of experimental mathematics, we extend the concept of PRAX algorithms to be applicable to the emptiness and universality problems in any domain whose instances admit a tractable distribution as defined in this paper.","A technical result here is that a linear (w.r.t. $1/\\delta$) number of samples is sufficient, as opposed to the quadratic number of samples in previous papers.","We show how the improved and generalized PRAX algorithms apply to universality and emptiness problems in various domains: ordinary automata, tautology testing of propositions, 2D automata, and to solution sets of certain Diophantine equations."],"url":"http://arxiv.org/abs/2403.08707v1","category":"cs.DS"}
{"created":"2024-03-13 17:09:32","title":"Scalarization of isolated black holes in scalar Gauss-Bonnet theory in the fixing-the-equations approach","abstract":"One of the most promising avenues to perform numerical evolutions in theories beyond General Relativity is the fixing-the-equations approach, a proposal in which new ``driver'' equations are added to the evolution equations in a way that allows for stable numerical evolutions. In this direction, we extend the numerical relativity code SpECTRE to evolve a ``fixed'' version of scalar Gauss-Bonnet theory in the decoupling limit, a phenomenologically interesting theory that allows for hairy black hole solutions in vacuum. We focus on isolated black hole systems both with and without linear and angular momentum, and propose a new driver equation to improve the recovery of such stationary solutions. We demonstrate the effectiveness of the latter by numerically evolving black holes that undergo spontaneous scalarization using different driver equations. Finally, we evaluate the accuracy of the obtained solutions by comparing with the original unaltered theory.","sentences":["One of the most promising avenues to perform numerical evolutions in theories beyond General Relativity is the fixing-the-equations approach, a proposal in which new ``driver'' equations are added to the evolution equations in a way that allows for stable numerical evolutions.","In this direction, we extend the numerical relativity code SpECTRE to evolve a ``fixed'' version of scalar Gauss-Bonnet theory in the decoupling limit, a phenomenologically interesting theory that allows for hairy black hole solutions in vacuum.","We focus on isolated black hole systems both with and without linear and angular momentum, and propose a new driver equation to improve the recovery of such stationary solutions.","We demonstrate the effectiveness of the latter by numerically evolving black holes that undergo spontaneous scalarization using different driver equations.","Finally, we evaluate the accuracy of the obtained solutions by comparing with the original unaltered theory."],"url":"http://arxiv.org/abs/2403.08705v1","category":"gr-qc"}
{"created":"2024-03-13 17:08:36","title":"Improved Dynamics for the Maximum Common Subgraph Problem","abstract":"The Maximum Common Subgraph (MCS) problem plays a crucial role across various domains, bridging theoretical exploration and practical applications in fields like bioinformatics and social network analysis. Despite its wide applicability, MCS is notoriously challenging and is classified as an NP-Complete (NPC) problem. This study introduces new heuristics aimed at mitigating these challenges through the reformulation of the MCS problem as the Maximum Clique and its complement, the Maximum Independent Set. Our first heuristic leverages the Motzkin-Straus theorem to reformulate the Maximum Clique Problem as a constrained optimization problem, continuing the work of Pelillo in Replicator Equations, Maximal Cliques, and Graph Isomorphism (1999) with replicator dynamics and introducing annealed imitation heuristics as in Dominant Sets and Hierarchical Clustering (Pavan and Pelillo, 2003) to improve chances of convergence to better local optima. The second technique applies heuristics drawn upon strategies for the Maximum Independent Set problem to efficiently reduce graph sizes as used by Akiwa and Iwata in 2014. This enables faster computation and, in many instances, yields near-optimal solutions. Furthermore we look at the implementation of both techniques in a single algorithm and find that it is a promising approach. Our techniques were tested on randomly generated Erd\\H{o}s-R\\'enyi graph pairs. Results indicate the potential for application and substantial impact on future research directions.","sentences":["The Maximum Common Subgraph (MCS) problem plays a crucial role across various domains, bridging theoretical exploration and practical applications in fields like bioinformatics and social network analysis.","Despite its wide applicability, MCS is notoriously challenging and is classified as an NP-Complete (NPC) problem.","This study introduces new heuristics aimed at mitigating these challenges through the reformulation of the MCS problem as the Maximum Clique and its complement, the Maximum Independent Set.","Our first heuristic leverages the Motzkin-Straus theorem to reformulate the Maximum Clique Problem as a constrained optimization problem, continuing the work of Pelillo in Replicator Equations, Maximal Cliques, and Graph Isomorphism (1999) with replicator dynamics and introducing annealed imitation heuristics as in Dominant Sets and Hierarchical Clustering (Pavan and Pelillo, 2003) to improve chances of convergence to better local optima.","The second technique applies heuristics drawn upon strategies for the Maximum Independent Set problem to efficiently reduce graph sizes as used by Akiwa and Iwata in 2014.","This enables faster computation and, in many instances, yields near-optimal solutions.","Furthermore we look at the implementation of both techniques in a single algorithm and find that it is a promising approach.","Our techniques were tested on randomly generated Erd\\H{o}s-R\\'enyi graph pairs.","Results indicate the potential for application and substantial impact on future research directions."],"url":"http://arxiv.org/abs/2403.08703v1","category":"cs.DM"}
{"created":"2024-03-13 16:58:37","title":"Deep Learning for In-Orbit Cloud Segmentation and Classification in Hyperspectral Satellite Data","abstract":"This article explores the latest Convolutional Neural Networks (CNNs) for cloud detection aboard hyperspectral satellites. The performance of the latest 1D CNN (1D-Justo-LiuNet) and two recent 2D CNNs (nnU-net and 2D-Justo-UNet-Simple) for cloud segmentation and classification is assessed. Evaluation criteria include precision and computational efficiency for in-orbit deployment. Experiments utilize NASA's EO-1 Hyperion data, with varying spectral channel numbers after Principal Component Analysis. Results indicate that 1D-Justo-LiuNet achieves the highest accuracy, outperforming 2D CNNs, while maintaining compactness with larger spectral channel sets, albeit with increased inference times. However, the performance of 1D CNN degrades with significant channel reduction. In this context, the 2D-Justo-UNet-Simple offers the best balance for in-orbit deployment, considering precision, memory, and time costs. While nnU-net is suitable for on-ground processing, deployment of lightweight 1D-Justo-LiuNet is recommended for high-precision applications. Alternatively, lightweight 2D-Justo-UNet-Simple is recommended for balanced costs between timing and precision in orbit.","sentences":["This article explores the latest Convolutional Neural Networks (CNNs) for cloud detection aboard hyperspectral satellites.","The performance of the latest 1D CNN (1D-Justo-LiuNet) and two recent 2D CNNs (nnU-net and 2D-Justo-UNet-Simple) for cloud segmentation and classification is assessed.","Evaluation criteria include precision and computational efficiency for in-orbit deployment.","Experiments utilize NASA's EO-1 Hyperion data, with varying spectral channel numbers after Principal Component Analysis.","Results indicate that 1D-Justo-LiuNet achieves the highest accuracy, outperforming 2D CNNs, while maintaining compactness with larger spectral channel sets, albeit with increased inference times.","However, the performance of 1D CNN degrades with significant channel reduction.","In this context, the 2D-Justo-UNet-Simple offers the best balance for in-orbit deployment, considering precision, memory, and time costs.","While nnU-net is suitable for on-ground processing, deployment of lightweight 1D-Justo-LiuNet is recommended for high-precision applications.","Alternatively, lightweight 2D-Justo-UNet-Simple is recommended for balanced costs between timing and precision in orbit."],"url":"http://arxiv.org/abs/2403.08695v1","category":"cs.CV"}
{"created":"2024-03-13 16:49:36","title":"Controllability of continuous networks and a kernel-based learning approximation","abstract":"Residual deep neural networks are formulated as interacting particle systems leading to a description through neural differential equations, and, in the case of large input data, through mean-field neural networks. The mean-field description allows also the recast of the training processes as a controllability problem for the solution to the mean-field dynamics. We show theoretical results on the controllability of the linear microscopic and mean-field dynamics through the Hilbert Uniqueness Method and propose a computational approach based on kernel learning methods to solve numerically, and efficiently, the training problem. Further aspects of the structural properties of the mean-field equation will be reviewed.","sentences":["Residual deep neural networks are formulated as interacting particle systems leading to a description through neural differential equations, and, in the case of large input data, through mean-field neural networks.","The mean-field description allows also the recast of the training processes as a controllability problem for the solution to the mean-field dynamics.","We show theoretical results on the controllability of the linear microscopic and mean-field dynamics through the Hilbert Uniqueness Method and propose a computational approach based on kernel learning methods to solve numerically, and efficiently, the training problem.","Further aspects of the structural properties of the mean-field equation will be reviewed."],"url":"http://arxiv.org/abs/2403.08690v1","category":"math.OC"}
{"created":"2024-03-13 16:32:54","title":"Path-dependency of capital return in periodic growth processes","abstract":"Periodic growth processes are investigated. The expected value of the profit rate, on accrual basis, does not directly depend on divestments, neither on the capitalization path. The expected value of capitalization is path dependent. Because of the path-dependent capitalization, the return rate on capital is path-dependent, and the time-average return rate on capital differs from the expected-value return rate on capital for the growth cycle. In the absence of intermediate divestments, the internal rate of return is path-independent, thereby differing from the expected value of the rate of return on capital. It is shown that the rotation cycle length maximizing the return rate on equity is independent of market interest rate. Leveraging effect enters the microeconomics of the growth processes through a separate leveraging equation, where the leverage coefficient may reach positive or negative values. Correspondingly, from the viewpoint of wealth accumulation, the often-suggested dependency of suitable rotation length on discount rate appears to be a modeling artifact. In other words, the net present value computation is based on maximization of consumption utility, instead of a capital growth objective; borrowing is obligatory, but the leverage effect is absent.","sentences":["Periodic growth processes are investigated.","The expected value of the profit rate, on accrual basis, does not directly depend on divestments, neither on the capitalization path.","The expected value of capitalization is path dependent.","Because of the path-dependent capitalization, the return rate on capital is path-dependent, and the time-average return rate on capital differs from the expected-value return rate on capital for the growth cycle.","In the absence of intermediate divestments, the internal rate of return is path-independent, thereby differing from the expected value of the rate of return on capital.","It is shown that the rotation cycle length maximizing the return rate on equity is independent of market interest rate.","Leveraging effect enters the microeconomics of the growth processes through a separate leveraging equation, where the leverage coefficient may reach positive or negative values.","Correspondingly, from the viewpoint of wealth accumulation, the often-suggested dependency of suitable rotation length on discount rate appears to be a modeling artifact.","In other words, the net present value computation is based on maximization of consumption utility, instead of a capital growth objective; borrowing is obligatory, but the leverage effect is absent."],"url":"http://arxiv.org/abs/2403.08678v1","category":"econ.GN"}
{"created":"2024-03-13 16:25:55","title":"When can we Approximate Wide Contrastive Models with Neural Tangent Kernels and Principal Component Analysis?","abstract":"Contrastive learning is a paradigm for learning representations from unlabelled data that has been highly successful for image and text data. Several recent works have examined contrastive losses to claim that contrastive models effectively learn spectral embeddings, while few works show relations between (wide) contrastive models and kernel principal component analysis (PCA). However, it is not known if trained contrastive models indeed correspond to kernel methods or PCA. In this work, we analyze the training dynamics of two-layer contrastive models, with non-linear activation, and answer when these models are close to PCA or kernel methods. It is well known in the supervised setting that neural networks are equivalent to neural tangent kernel (NTK) machines, and that the NTK of infinitely wide networks remains constant during training. We provide the first convergence results of NTK for contrastive losses, and present a nuanced picture: NTK of wide networks remains almost constant for cosine similarity based contrastive losses, but not for losses based on dot product similarity. We further study the training dynamics of contrastive models with orthogonality constraints on output layer, which is implicitly assumed in works relating contrastive learning to spectral embedding. Our deviation bounds suggest that representations learned by contrastive models are close to the principal components of a certain matrix computed from random features. We empirically show that our theoretical results possibly hold beyond two-layer networks.","sentences":["Contrastive learning is a paradigm for learning representations from unlabelled data that has been highly successful for image and text data.","Several recent works have examined contrastive losses to claim that contrastive models effectively learn spectral embeddings, while few works show relations between (wide) contrastive models and kernel principal component analysis (PCA).","However, it is not known if trained contrastive models indeed correspond to kernel methods or PCA.","In this work, we analyze the training dynamics of two-layer contrastive models, with non-linear activation, and answer when these models are close to PCA or kernel methods.","It is well known in the supervised setting that neural networks are equivalent to neural tangent kernel (NTK) machines, and that the NTK of infinitely wide networks remains constant during training.","We provide the first convergence results of NTK for contrastive losses, and present a nuanced picture: NTK of wide networks remains almost constant for cosine similarity based contrastive losses, but not for losses based on dot product similarity.","We further study the training dynamics of contrastive models with orthogonality constraints on output layer, which is implicitly assumed in works relating contrastive learning to spectral embedding.","Our deviation bounds suggest that representations learned by contrastive models are close to the principal components of a certain matrix computed from random features.","We empirically show that our theoretical results possibly hold beyond two-layer networks."],"url":"http://arxiv.org/abs/2403.08673v1","category":"cs.LG"}
{"created":"2024-03-13 16:22:30","title":"Non-linear collision-induced breakage equation: approximate solution and error estimation","abstract":"This article aims to provide approximate solutions for the non-linear collision-induced breakage equation using two different semi-analytical schemes, i.e., variational iteration method (VIM) and optimized decomposition method (ODM). The study also includes the detailed convergence analysis and error estimation for ODM in the case of product collisional ($K(\\epsilon,\\rho)=\\epsilon\\rho$) and breakage ($b(\\epsilon,\\rho,\\sigma)=\\frac{2}{\\rho}$) kernels with an exponential decay initial condition. By contrasting estimated concentration function and moments with exact solutions, the novelty of the suggested approaches is presented considering three numerical examples. Interestingly, in one case, VIM provides a closed-form solution, however, finite term series solutions obtained via both schemes supply a great approximation for the concentration function and moments.","sentences":["This article aims to provide approximate solutions for the non-linear collision-induced breakage equation using two different semi-analytical schemes, i.e., variational iteration method (VIM) and optimized decomposition method (ODM).","The study also includes the detailed convergence analysis and error estimation for ODM in the case of product collisional ($K(\\epsilon,\\rho)=\\epsilon\\rho$) and breakage ($b(\\epsilon,\\rho,\\sigma)=\\frac{2}{\\rho}$) kernels with an exponential decay initial condition.","By contrasting estimated concentration function and moments with exact solutions, the novelty of the suggested approaches is presented considering three numerical examples.","Interestingly, in one case, VIM provides a closed-form solution, however, finite term series solutions obtained via both schemes supply a great approximation for the concentration function and moments."],"url":"http://arxiv.org/abs/2403.08672v1","category":"math.NA"}
{"created":"2024-03-13 16:15:20","title":"Negative pressure as a quantum effect in free-streaming in the cosmological background","abstract":"We present a study of energy density and pressure of a free real scalar quantum field after its decoupling from a thermal bath in the spatially flat Friedman-Lemaitre-Robertson-Walker space-time by solving the Klein-Gordon equation both analytically and numerically for different predetermined scale factor functions $a(t)$. The energy density and pressure, defined by subtracting the vacuum expectation values at the decoupling time, feature corrections with respect to the classical free-streaming solution of the relativistic Boltzmann equation. We show that if the expansion rate is comparable or larger than $mc^2/\\hbar$ or $KT_0/\\hbar$ where $m$ is the mass and $T_0$ the decoupling temperature, both energy density and pressure gets strong quantum corrections which substantially modify their classical dependence on the scale factor $a(t)$ and drive pressure to large negative values. For a minimally coupled field with a very low mass in an expanding de Sitter universe quantum corrections are dominant driving pressure and energy density to become asymptotically constant with an equation of state $p/\\varepsilon \\simeq -1$, thereby mimicking a cosmological constant. For a minimally coupled massless field, quantum corrections are asymptotically dominant for any accelerated expansion.","sentences":["We present a study of energy density and pressure of a free real scalar quantum field after its decoupling from a thermal bath in the spatially flat Friedman-Lemaitre-Robertson-Walker space-time by solving the Klein-Gordon equation both analytically and numerically for different predetermined scale factor functions $a(t)$. The energy density and pressure, defined by subtracting the vacuum expectation values at the decoupling time, feature corrections with respect to the classical free-streaming solution of the relativistic Boltzmann equation.","We show that if the expansion rate is comparable or larger than $mc^2/\\hbar$ or $KT_0/\\hbar$ where $m$ is the mass and $T_0$ the decoupling temperature, both energy density and pressure gets strong quantum corrections which substantially modify their classical dependence on the scale factor $a(t)$ and drive pressure to large negative values.","For a minimally coupled field with a very low mass in an expanding de Sitter universe quantum corrections are dominant driving pressure and energy density to become asymptotically constant with an equation of state $p/\\varepsilon \\simeq -1$, thereby mimicking a cosmological constant.","For a minimally coupled massless field, quantum corrections are asymptotically dominant for any accelerated expansion."],"url":"http://arxiv.org/abs/2403.08661v1","category":"gr-qc"}
{"created":"2024-03-13 16:06:27","title":"Physics-Guided Inverse Regression for Crop Quality Assessment","abstract":"We present an innovative approach leveraging Physics-Guided Neural Networks (PGNNs) for enhancing agricultural quality assessments. Central to our methodology is the application of physics-guided inverse regression, a technique that significantly improves the model's ability to precisely predict quality metrics of crops. This approach directly addresses the challenges of scalability, speed, and practicality that traditional assessment methods face. By integrating physical principles, notably Fick`s second law of diffusion, into neural network architectures, our developed PGNN model achieves a notable advancement in enhancing both the interpretability and accuracy of assessments. Empirical validation conducted on cucumbers and mushrooms demonstrates the superior capability of our model in outperforming conventional computer vision techniques in postharvest quality evaluation. This underscores our contribution as a scalable and efficient solution to the pressing demands of global food supply challenges.","sentences":["We present an innovative approach leveraging Physics-Guided Neural Networks (PGNNs) for enhancing agricultural quality assessments.","Central to our methodology is the application of physics-guided inverse regression, a technique that significantly improves the model's ability to precisely predict quality metrics of crops.","This approach directly addresses the challenges of scalability, speed, and practicality that traditional assessment methods face.","By integrating physical principles, notably Fick`s second law of diffusion, into neural network architectures, our developed PGNN model achieves a notable advancement in enhancing both the interpretability and accuracy of assessments.","Empirical validation conducted on cucumbers and mushrooms demonstrates the superior capability of our model in outperforming conventional computer vision techniques in postharvest quality evaluation.","This underscores our contribution as a scalable and efficient solution to the pressing demands of global food supply challenges."],"url":"http://arxiv.org/abs/2403.08653v1","category":"stat.ME"}
{"created":"2024-03-13 16:06:26","title":"Extracting Explanations, Justification, and Uncertainty from Black-Box Deep Neural Networks","abstract":"Deep Neural Networks (DNNs) do not inherently compute or exhibit empirically-justified task confidence. In mission critical applications, it is important to both understand associated DNN reasoning and its supporting evidence. In this paper, we propose a novel Bayesian approach to extract explanations, justifications, and uncertainty estimates from DNNs. Our approach is efficient both in terms of memory and computation, and can be applied to any black box DNN without any retraining, including applications to anomaly detection and out-of-distribution detection tasks. We validate our approach on the CIFAR-10 dataset, and show that it can significantly improve the interpretability and reliability of DNNs.","sentences":["Deep Neural Networks (DNNs) do not inherently compute or exhibit empirically-justified task confidence.","In mission critical applications, it is important to both understand associated DNN reasoning and its supporting evidence.","In this paper, we propose a novel Bayesian approach to extract explanations, justifications, and uncertainty estimates from DNNs.","Our approach is efficient both in terms of memory and computation, and can be applied to any black box DNN without any retraining, including applications to anomaly detection and out-of-distribution detection tasks.","We validate our approach on the CIFAR-10 dataset, and show that it can significantly improve the interpretability and reliability of DNNs."],"url":"http://arxiv.org/abs/2403.08652v1","category":"cs.LG"}
{"created":"2024-03-13 16:01:09","title":"Rigidity of Einstein manifolds with positive Yamabe invariant","abstract":"We provide optimal pinching results on closed Einstein manifolds with positive Yamabe invariant in any dimension, extending the optimal bound for the scalar curvature due to Gursky and LeBrun in dimension four. We also improve the known bounds of the Yamabe invariant \\emph{via} the $L^{\\frac{n}{2}}$-norm of the Weyl tensor for low-dimensional Einstein manifolds. Finally, we discuss some advances on an algebraic inequality involving the Weyl tensor for dimensions $5$ and $6$.","sentences":["We provide optimal pinching results on closed Einstein manifolds with positive Yamabe invariant in any dimension, extending the optimal bound for the scalar curvature due to Gursky and LeBrun in dimension four.","We also improve the known bounds of the Yamabe invariant \\emph{via} the $L^{\\frac{n}{2}}$-norm of the Weyl tensor for low-dimensional Einstein manifolds.","Finally, we discuss some advances on an algebraic inequality involving the Weyl tensor for dimensions $5$ and $6$."],"url":"http://arxiv.org/abs/2403.08647v1","category":"math.DG"}
{"created":"2024-03-13 15:46:37","title":"A Decade's Battle on Dataset Bias: Are We There Yet?","abstract":"We revisit the \"dataset classification\" experiment suggested by Torralba and Efros a decade ago, in the new era with large-scale, diverse, and hopefully less biased datasets as well as more capable neural network architectures. Surprisingly, we observe that modern neural networks can achieve excellent accuracy in classifying which dataset an image is from: e.g., we report 84.7% accuracy on held-out validation data for the three-way classification problem consisting of the YFCC, CC, and DataComp datasets. Our further experiments show that such a dataset classifier could learn semantic features that are generalizable and transferable, which cannot be simply explained by memorization. We hope our discovery will inspire the community to rethink the issue involving dataset bias and model capabilities.","sentences":["We revisit the \"dataset classification\" experiment suggested by Torralba and Efros a decade ago, in the new era with large-scale, diverse, and hopefully less biased datasets as well as more capable neural network architectures.","Surprisingly, we observe that modern neural networks can achieve excellent accuracy in classifying which dataset an image is from: e.g., we report 84.7% accuracy on held-out validation data for the three-way classification problem consisting of the YFCC, CC, and DataComp datasets.","Our further experiments show that such a dataset classifier could learn semantic features that are generalizable and transferable, which cannot be simply explained by memorization.","We hope our discovery will inspire the community to rethink the issue involving dataset bias and model capabilities."],"url":"http://arxiv.org/abs/2403.08632v1","category":"cs.CV"}
{"created":"2024-03-13 15:45:29","title":"Leveraging Non-Decimated Wavelet Packet Features and Transformer Models for Time Series Forecasting","abstract":"This article combines wavelet analysis techniques with machine learning methods for univariate time series forecasting, focusing on three main contributions. Firstly, we consider the use of Daubechies wavelets with different numbers of vanishing moments as input features to both non-temporal and temporal forecasting methods, by selecting these numbers during the cross-validation phase. Secondly, we compare the use of both the non-decimated wavelet transform and the non-decimated wavelet packet transform for computing these features, the latter providing a much larger set of potentially useful coefficient vectors. The wavelet coefficients are computed using a shifted version of the typical pyramidal algorithm to ensure no leakage of future information into these inputs. Thirdly, we evaluate the use of these wavelet features on a significantly wider set of forecasting methods than previous studies, including both temporal and non-temporal models, and both statistical and deep learning-based methods. The latter include state-of-the-art transformer-based neural network architectures. Our experiments suggest significant benefit in replacing higher-order lagged features with wavelet features across all examined non-temporal methods for one-step-forward forecasting, and modest benefit when used as inputs for temporal deep learning-based models for long-horizon forecasting.","sentences":["This article combines wavelet analysis techniques with machine learning methods for univariate time series forecasting, focusing on three main contributions.","Firstly, we consider the use of Daubechies wavelets with different numbers of vanishing moments as input features to both non-temporal and temporal forecasting methods, by selecting these numbers during the cross-validation phase.","Secondly, we compare the use of both the non-decimated wavelet transform and the non-decimated wavelet packet transform for computing these features, the latter providing a much larger set of potentially useful coefficient vectors.","The wavelet coefficients are computed using a shifted version of the typical pyramidal algorithm to ensure no leakage of future information into these inputs.","Thirdly, we evaluate the use of these wavelet features on a significantly wider set of forecasting methods than previous studies, including both temporal and non-temporal models, and both statistical and deep learning-based methods.","The latter include state-of-the-art transformer-based neural network architectures.","Our experiments suggest significant benefit in replacing higher-order lagged features with wavelet features across all examined non-temporal methods for one-step-forward forecasting, and modest benefit when used as inputs for temporal deep learning-based models for long-horizon forecasting."],"url":"http://arxiv.org/abs/2403.08630v1","category":"stat.ME"}
{"created":"2024-03-13 15:41:20","title":"Optimal sub-Gaussian variance proxy for truncated Gaussian and exponential random variables","abstract":"This paper establishes the optimal sub-Gaussian variance proxy for truncated Gaussian and truncated exponential random variables. The proofs rely on first characterizing the optimal variance proxy as the unique solution to a set of two equations and then observing that for these two truncated distributions, one may find explicit solutions to this set of equations. Moreover, we establish the conditions under which the optimal variance proxy coincides with the variance, thereby characterizing the strict sub-Gaussianity of the truncated random variables. Specifically, we demonstrate that truncated Gaussian variables exhibit strict sub-Gaussian behavior if and only if they are symmetric, meaning their truncation is symmetric with respect to the mean. Conversely, truncated exponential variables are shown to never exhibit strict sub-Gaussian properties. These findings contribute to the understanding of these prevalent probability distributions in statistics and machine learning, providing a valuable foundation for improved and optimal modeling and decision-making processes.","sentences":["This paper establishes the optimal sub-Gaussian variance proxy for truncated Gaussian and truncated exponential random variables.","The proofs rely on first characterizing the optimal variance proxy as the unique solution to a set of two equations and then observing that for these two truncated distributions, one may find explicit solutions to this set of equations.","Moreover, we establish the conditions under which the optimal variance proxy coincides with the variance, thereby characterizing the strict sub-Gaussianity of the truncated random variables.","Specifically, we demonstrate that truncated Gaussian variables exhibit strict sub-Gaussian behavior if and only if they are symmetric, meaning their truncation is symmetric with respect to the mean.","Conversely, truncated exponential variables are shown to never exhibit strict sub-Gaussian properties.","These findings contribute to the understanding of these prevalent probability distributions in statistics and machine learning, providing a valuable foundation for improved and optimal modeling and decision-making processes."],"url":"http://arxiv.org/abs/2403.08628v1","category":"math.ST"}
{"created":"2024-03-13 15:35:45","title":"Spin-resolved counting statistics as a sensitive probe of spin correlation in transport through a quantum dot spin valve","abstract":"We investigate the noise in spin transport through a single quantum dot (QD) tunnel coupled to ferromagnetic electrodes with noncollinear magnetizations. Based on a spin-resolved quantum master equation, auto- and cross-correlations of spin-resolved currents are analyzed to reveal the underlying spin transport dynamics and characteristics for various polarizations. We find the currents of majority and minority spins could be strongly autocorrelated despite uncorrelated charge transfer. The interplay between tunnel coupling and the Coulomb interaction gives rise to an exchange magnetic field, leading to the precession of the accumulated spin in the QD. It strongly suppresses the bunching of spin tunneling events and results in a unique double-peak structure in the noise of the net spin current. The spin autocorrelation is found to be susceptible to magnetization alignments, which may serve as a sensitive tool to measure the magnetization directions between the ferromagnetic electrodes.","sentences":["We investigate the noise in spin transport through a single quantum dot (QD) tunnel coupled to ferromagnetic electrodes with noncollinear magnetizations.","Based on a spin-resolved quantum master equation, auto- and cross-correlations of spin-resolved currents are analyzed to reveal the underlying spin transport dynamics and characteristics for various polarizations.","We find the currents of majority and minority spins could be strongly autocorrelated despite uncorrelated charge transfer.","The interplay between tunnel coupling and the Coulomb interaction gives rise to an exchange magnetic field, leading to the precession of the accumulated spin in the QD.","It strongly suppresses the bunching of spin tunneling events and results in a unique double-peak structure in the noise of the net spin current.","The spin autocorrelation is found to be susceptible to magnetization alignments, which may serve as a sensitive tool to measure the magnetization directions between the ferromagnetic electrodes."],"url":"http://arxiv.org/abs/2403.08621v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-13 15:15:51","title":"Effect of Earth's Oblateness on Black Hole Imaging Through Earth-Space and Space-Space VLBI","abstract":"Earth-based Very Long Baseline Interferometry (VLBI) has made rapid advances in imaging black holes. However, due to the limitations imposed on terrestrial VLBI by the Earth's finite size and turbulent atmosphere, it is imperative to have a space-based component in future VLBI missions. Herein, this paper investigates the effect of Earth's oblateness, also known as the $J_{2}$ effect, on orbiters in Earth-Space and Space-Space VLBI. The paper provides an extensive discussion on how the $J_{2}$ effect can directly impact orbit selection for black hole observations and how through informed choices of orbital parameters, the effect can be used to the mission's advantage, a fact that has not been addressed in existing space-VLBI investigations. We provide a comprehensive study of how the orbital parameters of several current space VLBI proposals will vary specifically due to the $J_{2}$ effect. For black hole accretion flow targets of interest, we have demonstrated how the $J_{2}$ effect leads to modest increase in shorter baseline coverage, filling gaps in the $(u,v)$ plane. Subsequently, we construct a simple analytical formalism that allows isolation of the impact of the $J_{2}$ effect on the $(u,v)$ plane without requiring computationally intensive orbit propagation simulations. By directly constructing $(u,v)$ coverage using the $J_{2}$ affected and invariant equations of motion, we obtain distinct coverage patterns for M87* and SgrA* that show extremely dense coverage on short baselines as well as long term orbital stability on longer baselines.","sentences":["Earth-based Very Long Baseline Interferometry (VLBI) has made rapid advances in imaging black holes.","However, due to the limitations imposed on terrestrial VLBI by the Earth's finite size and turbulent atmosphere, it is imperative to have a space-based component in future VLBI missions.","Herein, this paper investigates the effect of Earth's oblateness, also known as the $J_{2}$ effect, on orbiters in Earth-Space and Space-Space VLBI.","The paper provides an extensive discussion on how the $J_{2}$ effect can directly impact orbit selection for black hole observations and how through informed choices of orbital parameters, the effect can be used to the mission's advantage, a fact that has not been addressed in existing space-VLBI investigations.","We provide a comprehensive study of how the orbital parameters of several current space VLBI proposals will vary specifically due to the $J_{2}$ effect.","For black hole accretion flow targets of interest, we have demonstrated how the $J_{2}$ effect leads to modest increase in shorter baseline coverage, filling gaps in the $(u,v)$ plane.","Subsequently, we construct a simple analytical formalism that allows isolation of the impact of the $J_{2}$ effect on the $(u,v)$ plane without requiring computationally intensive orbit propagation simulations.","By directly constructing $(u,v)$ coverage using the $J_{2}$ affected and invariant equations of motion, we obtain distinct coverage patterns for M87* and SgrA* that show extremely dense coverage on short baselines as well as long term orbital stability on longer baselines."],"url":"http://arxiv.org/abs/2403.08606v1","category":"astro-ph.HE"}
{"created":"2024-03-13 15:10:52","title":"Hyperbolic Anderson equations with general time-independent Gaussian noise: Stratonovich regime","abstract":"In this paper, we investigate the   hyperbolic Anderson equation   generated by a time-independent Gaussian noise   with two objectives: The solvability and intermittency.   First, we prove that Dalang's condition is necessary and sufficient   for existence of the solution. Second, we establish the   precise long time and high moment asymptotics for the solution under   the usual homogeneity assumption of the covariance of the Gaussian noise.   Our approach is fundamentally different from the ones existing in   literature.   The main contributions in our approach include the representation of Stratonovich   moment under Laplace transform via the moments of the Brownian motions in   Gaussian potentials   and some large deviation skills developed   in dealing effectively with the Stratonovich chaos expansion.","sentences":["In this paper, we investigate the   hyperbolic Anderson equation   generated by a time-independent Gaussian noise   with two objectives: The solvability and intermittency.   ","First, we prove that Dalang's condition is necessary and sufficient   for existence of the solution.","Second, we establish the   precise long time and high moment asymptotics for the solution under   the usual homogeneity assumption of the covariance of the Gaussian noise.   ","Our approach is fundamentally different from the ones existing in   literature.   ","The main contributions in our approach include the representation of Stratonovich   moment under Laplace transform via the moments of the Brownian motions in   Gaussian potentials   and some large deviation skills developed   in dealing effectively with the Stratonovich chaos expansion."],"url":"http://arxiv.org/abs/2403.08603v1","category":"math.PR"}
{"created":"2024-03-13 15:03:30","title":"Hamiltonian Boundary Value Methods (HBVMs) for functional differential equations with piecewise continuous arguments","abstract":"In this paper, a class of high-order methods to numerically solve Functional Differential Equations with Piecewise Continuous Arguments (FDEPCAs) is discussed. The framework stems from the expansion of the vector field associated with the reference differential equation along the shifted and scaled Legendre polynomial orthonormal basis, working on a suitable extension of Hamiltonian Boundary Value Methods. Within the design of the methods, a proper generalization of the perturbation results coming from the field of ordinary differential equations is considered, with the aim of handling the case of FDEPCAs. The error analysis of the devised family of methods is performed, while a few numerical tests on Hamiltonian FDEPCAs are provided, to give evidence to the theoretical findings and show the effectiveness of the obtained resolution strategy.","sentences":["In this paper, a class of high-order methods to numerically solve Functional Differential Equations with Piecewise Continuous Arguments (FDEPCAs) is discussed.","The framework stems from the expansion of the vector field associated with the reference differential equation along the shifted and scaled Legendre polynomial orthonormal basis, working on a suitable extension of Hamiltonian Boundary Value Methods.","Within the design of the methods, a proper generalization of the perturbation results coming from the field of ordinary differential equations is considered, with the aim of handling the case of FDEPCAs.","The error analysis of the devised family of methods is performed, while a few numerical tests on Hamiltonian FDEPCAs are provided, to give evidence to the theoretical findings and show the effectiveness of the obtained resolution strategy."],"url":"http://arxiv.org/abs/2403.08597v1","category":"math.NA"}
{"created":"2024-03-13 15:03:13","title":"Patching-based Deep Learning model for the Inpainting of Bragg Coherent Diffraction patterns affected by detectors' gaps","abstract":"We propose a deep learning algorithm for the inpainting of Bragg Coherent Diffraction Imaging (BCDI) patterns affected by detector gaps. These regions of missing intensity can compromise the accuracy of reconstruction algorithms, inducing artifacts in the final result. It is thus desirable to restore the intensity in these regions in order to ensure more reliable reconstructions. The key aspect of our method lies in the choice of training the neural network with cropped sections of both experimental diffraction data and simulated data and subsequently patching the predictions generated by the model along the gap, thus completing the full diffraction peak. This provides us with more experimental training data and allows for a faster model training due to the limited size, while the neural network can be applied to arbitrarily larger BCDI datasets. Moreover, our method not only broadens the scope of application but also ensures the preservation of data integrity and reliability in the face of challenging experimental conditions.","sentences":["We propose a deep learning algorithm for the inpainting of Bragg Coherent Diffraction Imaging (BCDI) patterns affected by detector gaps.","These regions of missing intensity can compromise the accuracy of reconstruction algorithms, inducing artifacts in the final result.","It is thus desirable to restore the intensity in these regions in order to ensure more reliable reconstructions.","The key aspect of our method lies in the choice of training the neural network with cropped sections of both experimental diffraction data and simulated data and subsequently patching the predictions generated by the model along the gap, thus completing the full diffraction peak.","This provides us with more experimental training data and allows for a faster model training due to the limited size, while the neural network can be applied to arbitrarily larger BCDI datasets.","Moreover, our method not only broadens the scope of application but also ensures the preservation of data integrity and reliability in the face of challenging experimental conditions."],"url":"http://arxiv.org/abs/2403.08596v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-13 14:57:10","title":"Data-Efficient Sleep Staging with Synthetic Time Series Pretraining","abstract":"Analyzing electroencephalographic (EEG) time series can be challenging, especially with deep neural networks, due to the large variability among human subjects and often small datasets. To address these challenges, various strategies, such as self-supervised learning, have been suggested, but they typically rely on extensive empirical datasets. Inspired by recent advances in computer vision, we propose a pretraining task termed \"frequency pretraining\" to pretrain a neural network for sleep staging by predicting the frequency content of randomly generated synthetic time series. Our experiments demonstrate that our method surpasses fully supervised learning in scenarios with limited data and few subjects, and matches its performance in regimes with many subjects. Furthermore, our results underline the relevance of frequency information for sleep stage scoring, while also demonstrating that deep neural networks utilize information beyond frequencies to enhance sleep staging performance, which is consistent with previous research. We anticipate that our approach will be advantageous across a broad spectrum of applications where EEG data is limited or derived from a small number of subjects, including the domain of brain-computer interfaces.","sentences":["Analyzing electroencephalographic (EEG) time series can be challenging, especially with deep neural networks, due to the large variability among human subjects and often small datasets.","To address these challenges, various strategies, such as self-supervised learning, have been suggested, but they typically rely on extensive empirical datasets.","Inspired by recent advances in computer vision, we propose a pretraining task termed \"frequency pretraining\" to pretrain a neural network for sleep staging by predicting the frequency content of randomly generated synthetic time series.","Our experiments demonstrate that our method surpasses fully supervised learning in scenarios with limited data and few subjects, and matches its performance in regimes with many subjects.","Furthermore, our results underline the relevance of frequency information for sleep stage scoring, while also demonstrating that deep neural networks utilize information beyond frequencies to enhance sleep staging performance, which is consistent with previous research.","We anticipate that our approach will be advantageous across a broad spectrum of applications where EEG data is limited or derived from a small number of subjects, including the domain of brain-computer interfaces."],"url":"http://arxiv.org/abs/2403.08592v1","category":"cs.LG"}
{"created":"2024-03-13 14:51:16","title":"Can physical information aid the generalization ability of Neural Networks for hydraulic modeling?","abstract":"Application of Neural Networks to river hydraulics is fledgling, despite the field suffering from data scarcity, a challenge for machine learning techniques. Consequently, many purely data-driven Neural Networks proved to lack predictive capabilities. In this work, we propose to mitigate such problem by introducing physical information into the training phase. The idea is borrowed from Physics-Informed Neural Networks which have been recently proposed in other contexts. Physics-Informed Neural Networks embed physical information in the form of the residual of the Partial Differential Equations (PDEs) governing the phenomenon and, as such, are conceived as neural solvers, i.e. an alternative to traditional numerical solvers. Such approach is seldom suitable for environmental hydraulics, where epistemic uncertainties are large, and computing residuals of PDEs exhibits difficulties similar to those faced by classical numerical methods. Instead, we envisaged the employment of Neural Networks as neural operators, featuring physical constraints formulated without resorting to PDEs. The proposed novel methodology shares similarities with data augmentation and regularization. We show that incorporating such soft physical information can improve predictive capabilities.","sentences":["Application of Neural Networks to river hydraulics is fledgling, despite the field suffering from data scarcity, a challenge for machine learning techniques.","Consequently, many purely data-driven Neural Networks proved to lack predictive capabilities.","In this work, we propose to mitigate such problem by introducing physical information into the training phase.","The idea is borrowed from Physics-Informed Neural Networks which have been recently proposed in other contexts.","Physics-Informed Neural Networks embed physical information in the form of the residual of the Partial Differential Equations (PDEs) governing the phenomenon and, as such, are conceived as neural solvers, i.e. an alternative to traditional numerical solvers.","Such approach is seldom suitable for environmental hydraulics, where epistemic uncertainties are large, and computing residuals of PDEs exhibits difficulties similar to those faced by classical numerical methods.","Instead, we envisaged the employment of Neural Networks as neural operators, featuring physical constraints formulated without resorting to PDEs.","The proposed novel methodology shares similarities with data augmentation and regularization.","We show that incorporating such soft physical information can improve predictive capabilities."],"url":"http://arxiv.org/abs/2403.08589v1","category":"cs.LG"}
{"created":"2024-03-13 14:42:55","title":"PRAGO: Differentiable Multi-View Pose Optimization From Objectness Detections","abstract":"Robustly estimating camera poses from a set of images is a fundamental task which remains challenging for differentiable methods, especially in the case of small and sparse camera pose graphs. To overcome this challenge, we propose Pose-refined Rotation Averaging Graph Optimization (PRAGO). From a set of objectness detections on unordered images, our method reconstructs the rotational pose, and in turn, the absolute pose, in a differentiable manner benefiting from the optimization of a sequence of geometrical tasks. We show how our objectness pose-refinement module in PRAGO is able to refine the inherent ambiguities in pairwise relative pose estimation without removing edges and avoiding making early decisions on the viability of graph edges. PRAGO then refines the absolute rotations through iterative graph construction, reweighting the graph edges to compute the final rotational pose, which can be converted into absolute poses using translation averaging. We show that PRAGO is able to outperform non-differentiable solvers on small and sparse scenes extracted from 7-Scenes achieving a relative improvement of 21% for rotations while achieving similar translation estimates.","sentences":["Robustly estimating camera poses from a set of images is a fundamental task which remains challenging for differentiable methods, especially in the case of small and sparse camera pose graphs.","To overcome this challenge, we propose Pose-refined Rotation Averaging Graph Optimization (PRAGO).","From a set of objectness detections on unordered images, our method reconstructs the rotational pose, and in turn, the absolute pose, in a differentiable manner benefiting from the optimization of a sequence of geometrical tasks.","We show how our objectness pose-refinement module in PRAGO is able to refine the inherent ambiguities in pairwise relative pose estimation without removing edges and avoiding making early decisions on the viability of graph edges.","PRAGO then refines the absolute rotations through iterative graph construction, reweighting the graph edges to compute the final rotational pose, which can be converted into absolute poses using translation averaging.","We show that PRAGO is able to outperform non-differentiable solvers on small and sparse scenes extracted from 7-Scenes achieving a relative improvement of 21% for rotations while achieving similar translation estimates."],"url":"http://arxiv.org/abs/2403.08586v1","category":"cs.CV"}
{"created":"2024-03-13 14:36:09","title":"Electroweak Evolution Equations and Isospin Conservation","abstract":"In processes taking place at energies much higher than the weak scale, electroweak corrections can be taken into account by using electroweak evolution equations, that are analogous to the DGLAP equations in QCD. We show that weak isospin conservation in these equations imposes to modify the expressions of the splitting functions commonly used in the literature. These modifications have a profound impact on the parton distribution functions.","sentences":["In processes taking place at energies much higher than the weak scale, electroweak corrections can be taken into account by using electroweak evolution equations, that are analogous to the DGLAP equations in QCD.","We show that weak isospin conservation in these equations imposes to modify the expressions of the splitting functions commonly used in the literature.","These modifications have a profound impact on the parton distribution functions."],"url":"http://arxiv.org/abs/2403.08583v1","category":"hep-ph"}
{"created":"2024-03-13 14:34:34","title":"Machine Learning Optimized Orthogonal Basis Piecewise Polynomial Approximation","abstract":"Piecewise Polynomials (PPs) are utilized in several engineering disciplines, like trajectory planning, to approximate position profiles given in the form of a set of points. While the approximation target along with domain-specific requirements, like Ck -continuity, can be formulated as a system of equations and a result can be computed directly, such closed-form solutions posses limited flexibility with respect to polynomial degrees, polynomial bases or adding further domain-specific requirements. Sufficiently complex optimization goals soon call for the use of numerical methods, like gradient descent. Since gradient descent lies at the heart of training Artificial Neural Networks (ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set of gradient-based optimizers potentially suitable for a wide range of optimization problems beyond the training task for ANNs. Our approach is to utilize the versatility of PP models and combine it with the potential of modern ML optimizers for the use in function approximation in 1D trajectory planning in the context of electronic cam design. We utilize available optimizers of the ML framework TensorFlow directly, outside of the scope of ANNs, to optimize model parameters of our PP model. In this paper, we show how an orthogonal polynomial basis contributes to improving approximation and continuity optimization performance. Utilizing Chebyshev polynomials of the first kind, we develop a novel regularization approach enabling clearly improved convergence behavior. We show that, using this regularization approach, Chebyshev basis performs better than power basis for all relevant optimizers in the combined approximation and continuity optimization setting and demonstrate usability of the presented approach within the electronic cam domain.","sentences":["Piecewise Polynomials (PPs) are utilized in several engineering disciplines, like trajectory planning, to approximate position profiles given in the form of a set of points.","While the approximation target along with domain-specific requirements, like Ck -continuity, can be formulated as a system of equations and a result can be computed directly, such closed-form solutions posses limited flexibility with respect to polynomial degrees, polynomial bases or adding further domain-specific requirements.","Sufficiently complex optimization goals soon call for the use of numerical methods, like gradient descent.","Since gradient descent lies at the heart of training Artificial Neural Networks (ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set of gradient-based optimizers potentially suitable for a wide range of optimization problems beyond the training task for ANNs.","Our approach is to utilize the versatility of PP models and combine it with the potential of modern ML optimizers for the use in function approximation in 1D trajectory planning in the context of electronic cam design.","We utilize available optimizers of the ML framework TensorFlow directly, outside of the scope of ANNs, to optimize model parameters of our PP model.","In this paper, we show how an orthogonal polynomial basis contributes to improving approximation and continuity optimization performance.","Utilizing Chebyshev polynomials of the first kind, we develop a novel regularization approach enabling clearly improved convergence behavior.","We show that, using this regularization approach, Chebyshev basis performs better than power basis for all relevant optimizers in the combined approximation and continuity optimization setting and demonstrate usability of the presented approach within the electronic cam domain."],"url":"http://arxiv.org/abs/2403.08579v1","category":"cs.LG"}
{"created":"2024-03-13 14:29:53","title":"Global solutions of the one-dimensional compressible Euler equations with nonlocal interactions via the inviscid limit","abstract":"We are concerned with the global existence of finite-energy entropy solutions of the one-dimensional compressible Euler equations with (possibly) damping, alignment forces, and nonlocal interactions: Newtonian repulsion and quadratic confinement. Both the polytropic gas law and the general gas law are analyzed. This is achieved by constructing a sequence of solutions of the one-dimensional compressible Navier-Stokes-type equations with density-dependent viscosity under the stress-free boundary condition and then taking the vanishing viscosity limit. The main difficulties in this paper arise from the appearance of the nonlocal terms. In particular, some uniform higher moment estimates for the compressible Navier-Stokes equations on expanding intervals with stress-free boundary conditions are obtained by careful design of the approximate initial data.","sentences":["We are concerned with the global existence of finite-energy entropy solutions of the one-dimensional compressible Euler equations with (possibly) damping, alignment forces, and nonlocal interactions: Newtonian repulsion and quadratic confinement.","Both the polytropic gas law and the general gas law are analyzed.","This is achieved by constructing a sequence of solutions of the one-dimensional compressible Navier-Stokes-type equations with density-dependent viscosity under the stress-free boundary condition and then taking the vanishing viscosity limit.","The main difficulties in this paper arise from the appearance of the nonlocal terms.","In particular, some uniform higher moment estimates for the compressible Navier-Stokes equations on expanding intervals with stress-free boundary conditions are obtained by careful design of the approximate initial data."],"url":"http://arxiv.org/abs/2403.08576v1","category":"math.AP"}
{"created":"2024-03-13 14:29:11","title":"Higher order Schauder estimates for parabolic equations with degenerate or singular weights","abstract":"In this paper, we complete the analysis initiated in [AFV24] establishing some higher order $C^{k+2,\\alpha}$ Schauder estimates ($k \\in \\mathbb{N}$) for a a class of parabolic equations with weights that are degenerate/singular on a characteristic hyperplane. The $C^{2,\\alpha}$-estimates are obtained through a blow-up argument and a Liouville theorem, while the higher order estimates are obtained by a fine iteration procedure. As a byproduct, we present two applications. First, we prove similar Schauder estimates when the degeneracy/singularity of the weight occurs on a regular hypersurface of cylindrical type. Second, we provide an alternative proof of the higher order boundary Harnack principles established in [BG16,Kuk22].","sentences":["In this paper, we complete the analysis initiated in [AFV24] establishing some higher order $C^{k+2,\\alpha}$ Schauder estimates ($k \\in \\mathbb{N}$) for a a class of parabolic equations with weights that are degenerate/singular on a characteristic hyperplane.","The $C^{2,\\alpha}$-estimates are obtained through a blow-up argument and a Liouville theorem, while the higher order estimates are obtained by a fine iteration procedure.","As a byproduct, we present two applications.","First, we prove similar Schauder estimates when the degeneracy/singularity of the weight occurs on a regular hypersurface of cylindrical type.","Second, we provide an alternative proof of the higher order boundary Harnack principles established in [BG16,Kuk22]."],"url":"http://arxiv.org/abs/2403.08575v1","category":"math.AP"}
{"created":"2024-03-13 14:25:15","title":"A Physics-driven GraphSAGE Method for Physical Process Simulations Described by Partial Differential Equations","abstract":"Physics-informed neural networks (PINNs) have successfully addressed various computational physics problems based on partial differential equations (PDEs). However, while tackling issues related to irregularities like singularities and oscillations, trained solutions usually suffer low accuracy. In addition, most current works only offer the trained solution for predetermined input parameters. If any change occurs in input parameters, transfer learning or retraining is required, and traditional numerical techniques also need an independent simulation. In this work, a physics-driven GraphSAGE approach (PD-GraphSAGE) based on the Galerkin method and piecewise polynomial nodal basis functions is presented to solve computational problems governed by irregular PDEs and to develop parametric PDE surrogate models. This approach employs graph representations of physical domains, thereby reducing the demands for evaluated points due to local refinement. A distance-related edge feature and a feature mapping strategy are devised to help training and convergence for singularity and oscillation situations, respectively. The merits of the proposed method are demonstrated through a couple of cases. Moreover, the robust PDE surrogate model for heat conduction problems parameterized by the Gaussian random field source is successfully established, which not only provides the solution accurately but is several times faster than the finite element method in our experiments.","sentences":["Physics-informed neural networks (PINNs) have successfully addressed various computational physics problems based on partial differential equations (PDEs).","However, while tackling issues related to irregularities like singularities and oscillations, trained solutions usually suffer low accuracy.","In addition, most current works only offer the trained solution for predetermined input parameters.","If any change occurs in input parameters, transfer learning or retraining is required, and traditional numerical techniques also need an independent simulation.","In this work, a physics-driven GraphSAGE approach (PD-GraphSAGE) based on the Galerkin method and piecewise polynomial nodal basis functions is presented to solve computational problems governed by irregular PDEs and to develop parametric PDE surrogate models.","This approach employs graph representations of physical domains, thereby reducing the demands for evaluated points due to local refinement.","A distance-related edge feature and a feature mapping strategy are devised to help training and convergence for singularity and oscillation situations, respectively.","The merits of the proposed method are demonstrated through a couple of cases.","Moreover, the robust PDE surrogate model for heat conduction problems parameterized by the Gaussian random field source is successfully established, which not only provides the solution accurately but is several times faster than the finite element method in our experiments."],"url":"http://arxiv.org/abs/2403.08569v1","category":"cs.LG"}
{"created":"2024-03-13 14:22:13","title":"A Novel Implicit Neural Representation for Volume Data","abstract":"The storage of medical images is one of the challenges in the medical imaging field. There are variable works that use implicit neural representation (INR) to compress volumetric medical images. However, there is room to improve the compression rate for volumetric medical images. Most of the INR techniques need a huge amount of GPU memory and a long training time for high-quality medical volume rendering. In this paper, we present a novel implicit neural representation to compress volume data using our proposed architecture, that is, the Lanczos downsampling scheme, SIREN deep network, and SRDenseNet high-resolution scheme. Our architecture can effectively reduce training time, and gain a high compression rate while retaining the final rendering quality. Moreover, it can save GPU memory in comparison with the existing works. The experiments show that the quality of reconstructed images and training speed using our architecture is higher than current works which use the SIREN only. Besides, the GPU memory cost is evidently decreased","sentences":["The storage of medical images is one of the challenges in the medical imaging field.","There are variable works that use implicit neural representation (INR) to compress volumetric medical images.","However, there is room to improve the compression rate for volumetric medical images.","Most of the INR techniques need a huge amount of GPU memory and a long training time for high-quality medical volume rendering.","In this paper, we present a novel implicit neural representation to compress volume data using our proposed architecture, that is, the Lanczos downsampling scheme, SIREN deep network, and SRDenseNet high-resolution scheme.","Our architecture can effectively reduce training time, and gain a high compression rate while retaining the final rendering quality.","Moreover, it can save GPU memory in comparison with the existing works.","The experiments show that the quality of reconstructed images and training speed using our architecture is higher than current works which use the SIREN only.","Besides, the GPU memory cost is evidently decreased"],"url":"http://arxiv.org/abs/2403.08566v1","category":"eess.IV"}
{"created":"2024-03-13 14:06:53","title":"Heterogeneous Nucleation and Growth of Sessile Chemically Active Droplets","abstract":"Droplets are essential for spatially controlling biomolecules in cells. To work properly, cells need to control the emergence and morphology of droplets. On the one hand, driven chemical reactions can affect droplets profoundly. For instance, reactions can control how droplets nucleate and how large they grow. On the other hand, droplets coexist with various organelles and other structures inside cells, which could affect their nucleation and morphology. To understand the interplay of these two aspects, we study a continuous field theory of active phase separation. Our numerical simulations reveal that reactions suppress nucleation while attractive walls enhance it. Intriguingly, these two effects are coupled, leading to shapes that deviate substantially from the spherical caps predicted for passive systems. These distortions result from anisotropic fluxes responding to the boundary conditions dictated by the Young-Dupr\\'e equation. Interestingly, an electrostatic analogy of chemical reactions confirms these effects. We thus demonstrate how driven chemical reactions affect the emergence and morphology of droplets, which could be crucial for understanding biological cells and improving technical applications, e.g., in chemical engineering.","sentences":["Droplets are essential for spatially controlling biomolecules in cells.","To work properly, cells need to control the emergence and morphology of droplets.","On the one hand, driven chemical reactions can affect droplets profoundly.","For instance, reactions can control how droplets nucleate and how large they grow.","On the other hand, droplets coexist with various organelles and other structures inside cells, which could affect their nucleation and morphology.","To understand the interplay of these two aspects, we study a continuous field theory of active phase separation.","Our numerical simulations reveal that reactions suppress nucleation while attractive walls enhance it.","Intriguingly, these two effects are coupled, leading to shapes that deviate substantially from the spherical caps predicted for passive systems.","These distortions result from anisotropic fluxes responding to the boundary conditions dictated by the Young-Dupr\\'e equation.","Interestingly, an electrostatic analogy of chemical reactions confirms these effects.","We thus demonstrate how driven chemical reactions affect the emergence and morphology of droplets, which could be crucial for understanding biological cells and improving technical applications, e.g., in chemical engineering."],"url":"http://arxiv.org/abs/2403.08555v1","category":"cond-mat.soft"}
{"created":"2024-03-13 17:59:04","title":"MonoOcc: Digging into Monocular Semantic Occupancy Prediction","abstract":"Monocular Semantic Occupancy Prediction aims to infer the complete 3D geometry and semantic information of scenes from only 2D images. It has garnered significant attention, particularly due to its potential to enhance the 3D perception of autonomous vehicles. However, existing methods rely on a complex cascaded framework with relatively limited information to restore 3D scenes, including a dependency on supervision solely on the whole network's output, single-frame input, and the utilization of a small backbone. These challenges, in turn, hinder the optimization of the framework and yield inferior prediction results, particularly concerning smaller and long-tailed objects. To address these issues, we propose MonoOcc. In particular, we (i) improve the monocular occupancy prediction framework by proposing an auxiliary semantic loss as supervision to the shallow layers of the framework and an image-conditioned cross-attention module to refine voxel features with visual clues, and (ii) employ a distillation module that transfers temporal information and richer knowledge from a larger image backbone to the monocular semantic occupancy prediction framework with low cost of hardware. With these advantages, our method yields state-of-the-art performance on the camera-based SemanticKITTI Scene Completion benchmark. Codes and models can be accessed at https://github.com/ucaszyp/MonoOcc","sentences":["Monocular Semantic Occupancy Prediction aims to infer the complete 3D geometry and semantic information of scenes from only 2D images.","It has garnered significant attention, particularly due to its potential to enhance the 3D perception of autonomous vehicles.","However, existing methods rely on a complex cascaded framework with relatively limited information to restore 3D scenes, including a dependency on supervision solely on the whole network's output, single-frame input, and the utilization of a small backbone.","These challenges, in turn, hinder the optimization of the framework and yield inferior prediction results, particularly concerning smaller and long-tailed objects.","To address these issues, we propose MonoOcc.","In particular, we (i) improve the monocular occupancy prediction framework by proposing an auxiliary semantic loss as supervision to the shallow layers of the framework and an image-conditioned cross-attention module to refine voxel features with visual clues, and (ii) employ a distillation module that transfers temporal information and richer knowledge from a larger image backbone to the monocular semantic occupancy prediction framework with low cost of hardware.","With these advantages, our method yields state-of-the-art performance on the camera-based SemanticKITTI Scene Completion benchmark.","Codes and models can be accessed at https://github.com/ucaszyp/MonoOcc"],"url":"http://arxiv.org/abs/2403.08766v1","category":"cs.CV"}
{"created":"2024-03-13 17:58:34","title":"Segmentation of Knee Bones for Osteoarthritis Assessment: A Comparative Analysis of Supervised, Few-Shot, and Zero-Shot Learning Approaches","abstract":"Knee osteoarthritis is a degenerative joint disease that induces chronic pain and disability. Bone morphological analysis is a promising tool to understand the mechanical aspect of this disorder. This study proposes a 2D bone morphological analysis using manually segmented bones to explore morphological features related to distinct pain conditions. Furthermore, six semantic segmentation algorithms are assessed for extracting femur and tibia bones from X-ray images. Our analysis reveals that the morphology of the femur undergoes significant changes in instances where pain worsens. Conversely, improvements in pain may not manifest pronounced alterations in bone shape. The few-shot-learning-based algorithm, UniverSeg, demonstrated superior segmentation results with Dice scores of 99.69% for femur and 99.60% for tibia. Regarding pain condition classification, the zero-shot-learning-based algorithm, CP-SAM, achieved the highest accuracy at 66% among all models. UniverSeg is recommended for automatic knee bone segmentation, while SAM models show potential with prompt encoder modifications for optimized outcomes. These findings highlight the effectiveness of few-shot learning for semantic segmentation and the potential of zero-shot learning in enhancing classification models for knee osteoarthritis diagnosis.","sentences":["Knee osteoarthritis is a degenerative joint disease that induces chronic pain and disability.","Bone morphological analysis is a promising tool to understand the mechanical aspect of this disorder.","This study proposes a 2D bone morphological analysis using manually segmented bones to explore morphological features related to distinct pain conditions.","Furthermore, six semantic segmentation algorithms are assessed for extracting femur and tibia bones from X-ray images.","Our analysis reveals that the morphology of the femur undergoes significant changes in instances where pain worsens.","Conversely, improvements in pain may not manifest pronounced alterations in bone shape.","The few-shot-learning-based algorithm, UniverSeg, demonstrated superior segmentation results with Dice scores of 99.69% for femur and 99.60% for tibia.","Regarding pain condition classification, the zero-shot-learning-based algorithm, CP-SAM, achieved the highest accuracy at 66% among all models.","UniverSeg is recommended for automatic knee bone segmentation, while SAM models show potential with prompt encoder modifications for optimized outcomes.","These findings highlight the effectiveness of few-shot learning for semantic segmentation and the potential of zero-shot learning in enhancing classification models for knee osteoarthritis diagnosis."],"url":"http://arxiv.org/abs/2403.08761v1","category":"eess.IV"}
{"created":"2024-03-13 17:44:16","title":"Learning How to Strategically Disclose Information","abstract":"Strategic information disclosure, in its simplest form, considers a game between an information provider (sender) who has access to some private information that an information receiver is interested in. While the receiver takes an action that affects the utilities of both players, the sender can design information (or modify beliefs) of the receiver through signal commitment, hence posing a Stackelberg game. However, obtaining a Stackelberg equilibrium for this game traditionally requires the sender to have access to the receiver's objective. In this work, we consider an online version of information design where a sender interacts with a receiver of an unknown type who is adversarially chosen at each round. Restricting attention to Gaussian prior and quadratic costs for the sender and the receiver, we show that $\\mathcal{O}(\\sqrt{T})$ regret is achievable with full information feedback, where $T$ is the total number of interactions between the sender and the receiver. Further, we propose a novel parametrization that allows the sender to achieve $\\mathcal{O}(\\sqrt{T})$ regret for a general convex utility function. We then consider the Bayesian Persuasion problem with an additional cost term in the objective function, which penalizes signaling policies that are more informative and obtain $\\mathcal{O}(\\log(T))$ regret. Finally, we establish a sublinear regret bound for the partial information feedback setting and provide simulations to support our theoretical results.","sentences":["Strategic information disclosure, in its simplest form, considers a game between an information provider (sender) who has access to some private information that an information receiver is interested in.","While the receiver takes an action that affects the utilities of both players, the sender can design information (or modify beliefs) of the receiver through signal commitment, hence posing a Stackelberg game.","However, obtaining a Stackelberg equilibrium for this game traditionally requires the sender to have access to the receiver's objective.","In this work, we consider an online version of information design where a sender interacts with a receiver of an unknown type who is adversarially chosen at each round.","Restricting attention to Gaussian prior and quadratic costs for the sender and the receiver, we show that $\\mathcal{O}(\\sqrt{T})$ regret is achievable with full information feedback, where $T$ is the total number of interactions between the sender and the receiver.","Further, we propose a novel parametrization that allows the sender to achieve $\\mathcal{O}(\\sqrt{T})$ regret for a general convex utility function.","We then consider the Bayesian Persuasion problem with an additional cost term in the objective function, which penalizes signaling policies that are more informative and obtain $\\mathcal{O}(\\log(T))$ regret.","Finally, we establish a sublinear regret bound for the partial information feedback setting and provide simulations to support our theoretical results."],"url":"http://arxiv.org/abs/2403.08741v1","category":"cs.GT"}
{"created":"2024-03-13 17:29:45","title":"Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization","abstract":"Multimodal Large Language Models (MLLMs) excel in generating responses based on visual inputs. However, they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information. We treat this bias as a \"preference\" for pretraining statistics, which hinders the model's grounding in visual input. To mitigate this issue, we propose Bootstrapped Preference Optimization (BPO), which conducts preference learning with datasets containing negative responses bootstrapped from the model itself. Specifically, we propose the following two strategies: 1) using distorted image inputs to the MLLM for eliciting responses that contain signified pretraining bias; 2) leveraging text-based LLM to explicitly inject erroneous but common elements into the original response. Those undesirable responses are paired with original annotated responses from the datasets to construct the preference dataset, which is subsequently utilized to perform preference learning. Our approach effectively suppresses pretrained LLM bias, enabling enhanced grounding in visual inputs. Extensive experimentation demonstrates significant performance improvements across multiple benchmarks, advancing the state-of-the-art in multimodal conversational systems.","sentences":["Multimodal Large Language Models (MLLMs) excel in generating responses based on visual inputs.","However, they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information.","We treat this bias as a \"preference\" for pretraining statistics, which hinders the model's grounding in visual input.","To mitigate this issue, we propose Bootstrapped Preference Optimization (BPO), which conducts preference learning with datasets containing negative responses bootstrapped from the model itself.","Specifically, we propose the following two strategies: 1) using distorted image inputs to the MLLM for eliciting responses that contain signified pretraining bias; 2) leveraging text-based LLM to explicitly inject erroneous but common elements into the original response.","Those undesirable responses are paired with original annotated responses from the datasets to construct the preference dataset, which is subsequently utilized to perform preference learning.","Our approach effectively suppresses pretrained LLM bias, enabling enhanced grounding in visual inputs.","Extensive experimentation demonstrates significant performance improvements across multiple benchmarks, advancing the state-of-the-art in multimodal conversational systems."],"url":"http://arxiv.org/abs/2403.08730v1","category":"cs.CL"}
{"created":"2024-03-13 16:38:26","title":"OneVOS: Unifying Video Object Segmentation with All-in-One Transformer Framework","abstract":"Contemporary Video Object Segmentation (VOS) approaches typically consist stages of feature extraction, matching, memory management, and multiple objects aggregation. Recent advanced models either employ a discrete modeling for these components in a sequential manner, or optimize a combined pipeline through substructure aggregation. However, these existing explicit staged approaches prevent the VOS framework from being optimized as a unified whole, leading to the limited capacity and suboptimal performance in tackling complex videos. In this paper, we propose OneVOS, a novel framework that unifies the core components of VOS with All-in-One Transformer. Specifically, to unify all aforementioned modules into a vision transformer, we model all the features of frames, masks and memory for multiple objects as transformer tokens, and integrally accomplish feature extraction, matching and memory management of multiple objects through the flexible attention mechanism. Furthermore, a Unidirectional Hybrid Attention is proposed through a double decoupling of the original attention operation, to rectify semantic errors and ambiguities of stored tokens in OneVOS framework. Finally, to alleviate the storage burden and expedite inference, we propose the Dynamic Token Selector, which unveils the working mechanism of OneVOS and naturally leads to a more efficient version of OneVOS. Extensive experiments demonstrate the superiority of OneVOS, achieving state-of-the-art performance across 7 datasets, particularly excelling in complex LVOS and MOSE datasets with 70.1% and 66.4% $J \\& F$ scores, surpassing previous state-of-the-art methods by 4.2% and 7.0%, respectively. And our code will be available for reproducibility and further research.","sentences":["Contemporary Video Object Segmentation (VOS) approaches typically consist stages of feature extraction, matching, memory management, and multiple objects aggregation.","Recent advanced models either employ a discrete modeling for these components in a sequential manner, or optimize a combined pipeline through substructure aggregation.","However, these existing explicit staged approaches prevent the VOS framework from being optimized as a unified whole, leading to the limited capacity and suboptimal performance in tackling complex videos.","In this paper, we propose OneVOS, a novel framework that unifies the core components of VOS with All-in-One Transformer.","Specifically, to unify all aforementioned modules into a vision transformer, we model all the features of frames, masks and memory for multiple objects as transformer tokens, and integrally accomplish feature extraction, matching and memory management of multiple objects through the flexible attention mechanism.","Furthermore, a Unidirectional Hybrid Attention is proposed through a double decoupling of the original attention operation, to rectify semantic errors and ambiguities of stored tokens in OneVOS framework.","Finally, to alleviate the storage burden and expedite inference, we propose the Dynamic Token Selector, which unveils the working mechanism of OneVOS and naturally leads to a more efficient version of OneVOS.","Extensive experiments demonstrate the superiority of OneVOS, achieving state-of-the-art performance across 7 datasets, particularly excelling in complex LVOS and MOSE datasets with 70.1% and 66.4% $J \\& F$ scores, surpassing previous state-of-the-art methods by 4.2% and 7.0%, respectively.","And our code will be available for reproducibility and further research."],"url":"http://arxiv.org/abs/2403.08682v1","category":"cs.CV"}
{"created":"2024-03-13 16:00:41","title":"Covariance Fitting Interferometric Phase Linking: Modular Framework and Optimization Algorithms","abstract":"Interferometric phase linking (IPL) has become a prominent technique for processing images of areas containing distributed scaterrers in SAR interferometry. Traditionally, IPL consists in estimating consistent phase differences between all pairs of SAR images in a time series from the sample covariance matrix of pixel patches on a sliding window. This paper reformulates this task as a covariance fitting problem: in this setup, IPL appears as a form of projection of an input covariance matrix so that it satisfies the phase closure property. Given this modular formulation, we propose an overview of covariance matrix estimates, regularization options, and matrix distances, that can be of interest when processing multi-temporal SAR data. In particular, we will observe that most of the existing IPL algorithms appear as special instances of this framework. We then present tools to efficiently solve related optimization problems on the torus of phase-only complex vectors: majorization-minimization and Riemannian optimization. We conclude by illustrating the merits of different options on a real-world case study.","sentences":["Interferometric phase linking (IPL) has become a prominent technique for processing images of areas containing distributed scaterrers in SAR interferometry.","Traditionally, IPL consists in estimating consistent phase differences between all pairs of SAR images in a time series from the sample covariance matrix of pixel patches on a sliding window.","This paper reformulates this task as a covariance fitting problem: in this setup, IPL appears as a form of projection of an input covariance matrix so that it satisfies the phase closure property.","Given this modular formulation, we propose an overview of covariance matrix estimates, regularization options, and matrix distances, that can be of interest when processing multi-temporal SAR data.","In particular, we will observe that most of the existing IPL algorithms appear as special instances of this framework.","We then present tools to efficiently solve related optimization problems on the torus of phase-only complex vectors: majorization-minimization and Riemannian optimization.","We conclude by illustrating the merits of different options on a real-world case study."],"url":"http://arxiv.org/abs/2403.08646v1","category":"stat.AP"}
{"created":"2024-03-13 15:22:56","title":"Tangential Fixpoint Iterations for Gromov-Wasserstein Barycenters","abstract":"The Gromov-Wasserstein (GW) transport problem is a relaxation of classic optimal transport, which seeks a transport between two measures while preserving their internal geometry. Due to meeting this theoretical underpinning, it is a valuable tool for the analysis of objects that do not possess a natural embedding or should be studied independently of it. Prime applications can thus be found in e.g. shape matching, classification and interpolation tasks. To tackle the latter, one theoretically justified approach is the employment of multi-marginal GW transport and GW barycenters, which are Fr\\'echet means with respect to the GW distance. However, because the computation of GW itself already poses a quadratic and non-convex optimization problem, the determination of GW barycenters is a hard task and algorithms for their computation are scarce. In this paper, we revisit a known procedure for the determination of Fr\\'echet means in Riemannian manifolds via tangential approximations in the context of GW. We provide a characterization of barycenters in the GW tangent space, which ultimately gives rise to a fixpoint iteration for approximating GW barycenters using multi-marginal plans. We propose a relaxation of this fixpoint iteration and show that it monotonously decreases the barycenter loss. In certain cases our proposed method naturally provides us with barycentric embeddings. The resulting algorithm is capable of producing qualitative shape interpolations between multiple 3d shapes with support sizes of over thousands of points in reasonable time. In addition, we verify our method on shape classification and multi-graph matching tasks.","sentences":["The Gromov-Wasserstein (GW) transport problem is a relaxation of classic optimal transport, which seeks a transport between two measures while preserving their internal geometry.","Due to meeting this theoretical underpinning, it is a valuable tool for the analysis of objects that do not possess a natural embedding or should be studied independently of it.","Prime applications can thus be found in e.g. shape matching, classification and interpolation tasks.","To tackle the latter, one theoretically justified approach is the employment of multi-marginal GW transport and GW barycenters, which are Fr\\'echet means with respect to the GW distance.","However, because the computation of GW itself already poses a quadratic and non-convex optimization problem, the determination of GW barycenters is a hard task and algorithms for their computation are scarce.","In this paper, we revisit a known procedure for the determination of Fr\\'echet means in Riemannian manifolds via tangential approximations in the context of GW.","We provide a characterization of barycenters in the GW tangent space, which ultimately gives rise to a fixpoint iteration for approximating GW barycenters using multi-marginal plans.","We propose a relaxation of this fixpoint iteration and show that it monotonously decreases the barycenter loss.","In certain cases our proposed method naturally provides us with barycentric embeddings.","The resulting algorithm is capable of producing qualitative shape interpolations between multiple 3d shapes with support sizes of over thousands of points in reasonable time.","In addition, we verify our method on shape classification and multi-graph matching tasks."],"url":"http://arxiv.org/abs/2403.08612v1","category":"math.NA"}
{"created":"2024-03-13 14:46:38","title":"Quantum plasmonics model of refractive index sensing using photon correlations","abstract":"The interaction between the electric dipole moments of a quantum emitter and a metal nanoparticle gives rise to unique optical properties, such as interference-induced photon correlations, that could be useful for enhanced intensity-based sensing. Using the quantum theory of photodetection, we propose a nanosensor system comprising a quantum emitter and a metal nanoparticle that explores the possibility of utilizing higher-order photon correlations for refractive index sensing. Both the refractive index sensitivity and resolution of the nanosensor, whose scattering spectrum lies within the visible region, are predicted. The sensor is supported by a substrate and driven weakly by a coherent field. By calculating the mean photocount and its second factorial moment resulting from the scattered field of the system, the sensing performance of the intensity and intensity-intensity correlation, are compared at optimal driving wavelengths. The mean photocount was found to be inherently low, inhibiting the role of interference-induced photon antibunching in minimizing the sensor's intensity shot noise. However, a regime in which the noise could be reduced below the shot noise limit is identified, leading to a quantum enhancement in the sensing performance.","sentences":["The interaction between the electric dipole moments of a quantum emitter and a metal nanoparticle gives rise to unique optical properties, such as interference-induced photon correlations, that could be useful for enhanced intensity-based sensing.","Using the quantum theory of photodetection, we propose a nanosensor system comprising a quantum emitter and a metal nanoparticle that explores the possibility of utilizing higher-order photon correlations for refractive index sensing.","Both the refractive index sensitivity and resolution of the nanosensor, whose scattering spectrum lies within the visible region, are predicted.","The sensor is supported by a substrate and driven weakly by a coherent field.","By calculating the mean photocount and its second factorial moment resulting from the scattered field of the system, the sensing performance of the intensity and intensity-intensity correlation, are compared at optimal driving wavelengths.","The mean photocount was found to be inherently low, inhibiting the role of interference-induced photon antibunching in minimizing the sensor's intensity shot noise.","However, a regime in which the noise could be reduced below the shot noise limit is identified, leading to a quantum enhancement in the sensing performance."],"url":"http://arxiv.org/abs/2403.08588v1","category":"quant-ph"}
{"created":"2024-03-13 14:43:36","title":"An improved particle swarm optimization algorithm and its application to search for new magnetic ground states in the Hubbard model","abstract":"An improved particle swarm optimization algorithm is proposed and its superiority over standard particle swarm optimization algorithm is tested on two typical benchmark functions. By employing this algorithm to search for the magnetic ground states of the Hubbard model on the real-space square lattice with finite size based on the mean-field approximation, two new magnetic states, namely the double striped-type antiferromagnetic state and the triple antiferromagnetic state, are found. We further perform mean-field calculations in the thermodynamical limit to confirm that these two new magnetic states are not a result of a finite-size effect, where the properties of the double striped-type antiferromagnetic state are also presented.","sentences":["An improved particle swarm optimization algorithm is proposed and its superiority over standard particle swarm optimization algorithm is tested on two typical benchmark functions.","By employing this algorithm to search for the magnetic ground states of the Hubbard model on the real-space square lattice with finite size based on the mean-field approximation, two new magnetic states, namely the double striped-type antiferromagnetic state and the triple antiferromagnetic state, are found.","We further perform mean-field calculations in the thermodynamical limit to confirm that these two new magnetic states are not a result of a finite-size effect, where the properties of the double striped-type antiferromagnetic state are also presented."],"url":"http://arxiv.org/abs/2403.08587v1","category":"cond-mat.str-el"}
{"created":"2024-03-13 14:35:17","title":"Impact of spin-entropy on the thermoelectric properties of a 2D magnet","abstract":"Heat-to-charge conversion efficiency of thermoelectric materials is closely linked to the entropy per charge carrier. Thus, magnetic materials are promising building blocks for highly efficient energy harvesters, as their carrier entropy is boosted by a spin degree of freedom. In this work, we investigate how this spin entropy impacts heat-to-charge conversion in A-type antiferromagnet CrSBr. We perform simultaneous measurements of electrical conductance and thermocurrent while changing magnetic order using temperature and magnetic field as tuning parameters. We find a strong enhancement of the thermoelectric power factor around the N\\'eel temperature. We further reveal that the power factor at low temperature can be increased by up to 600% upon applying a magnetic field. Our results demonstrate that the thermoelectric properties of 2D magnets can be optimized by exploiting the sizeable impact of spin entropy and confirm thermoelectric measurements as a sensitive tool to investigate subtle magnetic phase transitions in low-dimensional magnets.","sentences":["Heat-to-charge conversion efficiency of thermoelectric materials is closely linked to the entropy per charge carrier.","Thus, magnetic materials are promising building blocks for highly efficient energy harvesters, as their carrier entropy is boosted by a spin degree of freedom.","In this work, we investigate how this spin entropy impacts heat-to-charge conversion in A-type antiferromagnet CrSBr.","We perform simultaneous measurements of electrical conductance and thermocurrent while changing magnetic order using temperature and magnetic field as tuning parameters.","We find a strong enhancement of the thermoelectric power factor around the N\\'eel temperature.","We further reveal that the power factor at low temperature can be increased by up to 600% upon applying a magnetic field.","Our results demonstrate that the thermoelectric properties of 2D magnets can be optimized by exploiting the sizeable impact of spin entropy and confirm thermoelectric measurements as a sensitive tool to investigate subtle magnetic phase transitions in low-dimensional magnets."],"url":"http://arxiv.org/abs/2403.08581v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-13 14:29:10","title":"Optimizing Conical Intersections Without Explicit Use of Non-Adiabatic Couplings","abstract":"We present two alternative methods for optimizing minimum energy conical intersection (MECI) molecular geometries without knowledge of the derivative coupling (DC). These methods are based on the utilization of Lagrange multipliers: i) one method uses an approximate calculation of the DC, while the other ii) do not require the DC. Both methods use the fact that information of the DC is contained in the Hessian of the squared energy difference. Tests done on a set of small molecular systems, in comparison with other methods, show the ability of the proposed methods to optimize MECIs. Finally, we apply the methods to the furimamide molecule, to optimize and characterize its S$_1$ /S$_2$ MECI, and to optimizing the S$_0$ /S$_1$ MECI of the silver trimer.","sentences":["We present two alternative methods for optimizing minimum energy conical intersection (MECI) molecular geometries without knowledge of the derivative coupling (DC).","These methods are based on the utilization of Lagrange multipliers: i) one method uses an approximate calculation of the DC, while the other ii) do not require the DC.","Both methods use the fact that information of the DC is contained in the Hessian of the squared energy difference.","Tests done on a set of small molecular systems, in comparison with other methods, show the ability of the proposed methods to optimize MECIs.","Finally, we apply the methods to the furimamide molecule, to optimize and characterize its S$_1$ /S$_2$ MECI, and to optimizing the S$_0$ /S$_1$ MECI of the silver trimer."],"url":"http://arxiv.org/abs/2403.08574v1","category":"physics.chem-ph"}
{"created":"2024-03-13 14:06:18","title":"Regret Analysis of Policy Optimization over Submanifolds for Linearly Constrained Online LQG","abstract":"Recent advancement in online optimization and control has provided novel tools to study online linear quadratic regulator (LQR) problems, where cost matrices are varying adversarially over time. However, the controller parameterization of existing works may not satisfy practical conditions like sparsity due to physical connections. In this work, we study online linear quadratic Gaussian problems with a given linear constraint imposed on the controller. Inspired by the recent work of [1] which proposed, for a linearly constrained policy optimization of an offline LQR, a second order method equipped with a Riemannian metric that emerges naturally in the context of optimal control problems, we propose online optimistic Newton on manifold (OONM) which provides an online controller based on the prediction on the first and second order information of the function sequence. To quantify the proposed algorithm, we leverage the notion of regret defined as the sub-optimality of its cumulative cost to that of a (locally) minimizing controller sequence and provide the regret bound in terms of the path-length of the minimizer sequence. Simulation results are also provided to verify the property of OONM.","sentences":["Recent advancement in online optimization and control has provided novel tools to study online linear quadratic regulator (LQR) problems, where cost matrices are varying adversarially over time.","However, the controller parameterization of existing works may not satisfy practical conditions like sparsity due to physical connections.","In this work, we study online linear quadratic Gaussian problems with a given linear constraint imposed on the controller.","Inspired by the recent work of [1] which proposed, for a linearly constrained policy optimization of an offline LQR, a second order method equipped with a Riemannian metric that emerges naturally in the context of optimal control problems, we propose online optimistic Newton on manifold (OONM) which provides an online controller based on the prediction on the first and second order information of the function sequence.","To quantify the proposed algorithm, we leverage the notion of regret defined as the sub-optimality of its cumulative cost to that of a (locally) minimizing controller sequence and provide the regret bound in terms of the path-length of the minimizer sequence.","Simulation results are also provided to verify the property of OONM."],"url":"http://arxiv.org/abs/2403.08553v1","category":"math.OC"}
{"created":"2024-03-13 14:02:42","title":"CINA: Conditional Implicit Neural Atlas for Spatio-Temporal Representation of Fetal Brains","abstract":"We introduce a conditional implicit neural atlas (CINA) for spatio-temporal atlas generation from Magnetic Resonance Images (MRI) of the neurotypical and pathological fetal brain, that is fully independent of affine or non-rigid registration. During training, CINA learns a general representation of the fetal brain and encodes subject specific information into latent code. After training, CINA can construct a faithful atlas with tissue probability maps of the fetal brain for any gestational age (GA) and anatomical variation covered within the training domain. Thus, CINA is competent to represent both, neurotypical and pathological brains. Furthermore, a trained CINA model can be fit to brain MRI of unseen subjects via test-time optimization of the latent code. CINA can then produce probabilistic tissue maps tailored to a particular subject. We evaluate our method on a total of 198 T2 weighted MRI of normal and abnormal fetal brains from the dHCP and FeTA datasets. We demonstrate CINA's capability to represent a fetal brain atlas that can be flexibly conditioned on GA and on anatomical variations like ventricular volume or degree of cortical folding, making it a suitable tool for modeling both neurotypical and pathological brains. We quantify the fidelity of our atlas by means of tissue segmentation and age prediction and compare it to an established baseline. CINA demonstrates superior accuracy for neurotypical brains and pathological brains with ventriculomegaly. Moreover, CINA scores a mean absolute error of 0.23 weeks in fetal brain age prediction, further confirming an accurate representation of fetal brain development.","sentences":["We introduce a conditional implicit neural atlas (CINA) for spatio-temporal atlas generation from Magnetic Resonance Images (MRI) of the neurotypical and pathological fetal brain, that is fully independent of affine or non-rigid registration.","During training, CINA learns a general representation of the fetal brain and encodes subject specific information into latent code.","After training, CINA can construct a faithful atlas with tissue probability maps of the fetal brain for any gestational age (GA) and anatomical variation covered within the training domain.","Thus, CINA is competent to represent both, neurotypical and pathological brains.","Furthermore, a trained CINA model can be fit to brain MRI of unseen subjects via test-time optimization of the latent code.","CINA can then produce probabilistic tissue maps tailored to a particular subject.","We evaluate our method on a total of 198 T2 weighted MRI of normal and abnormal fetal brains from the dHCP and FeTA datasets.","We demonstrate CINA's capability to represent a fetal brain atlas that can be flexibly conditioned on GA and on anatomical variations like ventricular volume or degree of cortical folding, making it a suitable tool for modeling both neurotypical and pathological brains.","We quantify the fidelity of our atlas by means of tissue segmentation and age prediction and compare it to an established baseline.","CINA demonstrates superior accuracy for neurotypical brains and pathological brains with ventriculomegaly.","Moreover, CINA scores a mean absolute error of 0.23 weeks in fetal brain age prediction, further confirming an accurate representation of fetal brain development."],"url":"http://arxiv.org/abs/2403.08550v1","category":"cs.LG"}
{"created":"2024-03-13 13:54:00","title":"Language models scale reliably with over-training and on downstream tasks","abstract":"Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e., \"Chinchilla optimal\" regime); however, in practice, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but ultimately models are compared based on downstream task performance. In this paper, we address both shortcomings. To do so, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we investigate scaling in the over-trained regime. We fit scaling laws that extrapolate in both the number of model parameters and the ratio of training tokens to parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute. Second, we relate the perplexity of a language model to its downstream task performance via a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models using experiments that take 20$\\times$ less compute. Our experiments are available at https://github.com/mlfoundations/scaling.","sentences":["Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated.","For instance, scaling is usually studied in the compute-optimal training regime (i.e., \"Chinchilla optimal\" regime); however, in practice, models are often over-trained to reduce inference costs.","Moreover, scaling laws mostly predict loss on next-token prediction, but ultimately models are compared based on downstream task performance.","In this paper, we address both shortcomings.","To do so, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions.","First, we investigate scaling in the over-trained regime.","We fit scaling laws that extrapolate in both the number of model parameters and the ratio of training tokens to parameters.","This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute.","Second, we relate the perplexity of a language model to its downstream task performance via a power law.","We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models using experiments that take 20$\\times$ less compute.","Our experiments are available at https://github.com/mlfoundations/scaling."],"url":"http://arxiv.org/abs/2403.08540v1","category":"cs.CL"}
{"created":"2024-03-13 13:49:12","title":"Ensuring connectedness for the Maximum Quasi-clique and Densest $k$-subgraph problems","abstract":"Given an undirected graph $G$, a quasi-clique is a subgraph of $G$ whose density is at least $\\gamma$ $(0 < \\gamma \\leq 1)$. Two optimization problems can be defined for quasi-cliques: the Maximum Quasi-Clique (MQC) Problem, which finds a quasi-clique with maximum vertex cardinality, and the Densest $k$-Subgraph (DKS) Problem, which finds the densest subgraph given a fixed cardinality constraint. Most existing approaches to solve both problems often disregard the requirement of connectedness, which may lead to solutions containing isolated components that are meaningless for many real-life applications. To address this issue, we propose two flow-based connectedness constraints to be integrated into known Mixed-Integer Linear Programming (MILP) formulations for either MQC or DKS problems. We compare the performance of MILP formulations enhanced with our connectedness constraints in terms of both running time and number of solved instances against existing approaches that ensure quasi-clique connectedness. Experimental results demonstrate that our constraints are quite competitive, making them valuable for practical applications requiring connectedness.","sentences":["Given an undirected graph $G$, a quasi-clique is a subgraph of $G$ whose density is at least $\\gamma$ $(0 <","\\gamma \\leq 1)$.","Two optimization problems can be defined for quasi-cliques: the Maximum Quasi-Clique (MQC) Problem, which finds a quasi-clique with maximum vertex cardinality, and the Densest $k$-Subgraph (DKS) Problem, which finds the densest subgraph given a fixed cardinality constraint.","Most existing approaches to solve both problems often disregard the requirement of connectedness, which may lead to solutions containing isolated components that are meaningless for many real-life applications.","To address this issue, we propose two flow-based connectedness constraints to be integrated into known Mixed-Integer Linear Programming (MILP) formulations for either MQC or DKS problems.","We compare the performance of MILP formulations enhanced with our connectedness constraints in terms of both running time and number of solved instances against existing approaches that ensure quasi-clique connectedness.","Experimental results demonstrate that our constraints are quite competitive, making them valuable for practical applications requiring connectedness."],"url":"http://arxiv.org/abs/2403.08534v1","category":"cs.DM"}
{"created":"2024-03-13 13:30:45","title":"Elliptic billiard with harmonic potential: Classical description","abstract":"The classical dynamics of the isotropic two-dimensional harmonic oscillator confined by an elliptic hard wall is discussed. The interplay between the harmonic potential with circular symmetry and the boundary with elliptical symmetry does not spoil the separability in elliptic coordinates; however, it generates non-trivial energy and momentum dependencies in the billiard. We analyze the equi-momentum surfaces in the parameters space and classify the kinds of motion the particle can have in the billiard. The winding numbers and periods of the rotational and librational trajectories are analytically calculated and numerically verified. A remarkable finding is the possibility of having degenerate rotational trajectories with the same energy but different second constant of motion and different caustics and periods. The conditions to get these degenerate trajectories are analyzed. Similarly, we show that obtaining two different rotational trajectories with the same period and second constant of motion but different energy is possible.","sentences":["The classical dynamics of the isotropic two-dimensional harmonic oscillator confined by an elliptic hard wall is discussed.","The interplay between the harmonic potential with circular symmetry and the boundary with elliptical symmetry does not spoil the separability in elliptic coordinates; however, it generates non-trivial energy and momentum dependencies in the billiard.","We analyze the equi-momentum surfaces in the parameters space and classify the kinds of motion the particle can have in the billiard.","The winding numbers and periods of the rotational and librational trajectories are analytically calculated and numerically verified.","A remarkable finding is the possibility of having degenerate rotational trajectories with the same energy but different second constant of motion and different caustics and periods.","The conditions to get these degenerate trajectories are analyzed.","Similarly, we show that obtaining two different rotational trajectories with the same period and second constant of motion but different energy is possible."],"url":"http://arxiv.org/abs/2403.08523v1","category":"nlin.CD"}
{"created":"2024-03-13 13:27:40","title":"Projective Quantum Eigensolver via Adiabatically Decoupled Subsystem Evolution: a Resource Efficient Approach to Molecular Energetics in Noisy Quantum Computers","abstract":"Quantum computers hold immense potential in the field of chemistry, ushering new frontiers to solve complex many body problems that are beyond the reach of classical computers. However, noise in the current quantum hardware limits their applicability to large chemical systems. This work encompasses the development of a projective formalism that aims to compute ground-state energies of molecular systems accurately using Noisy Intermediate Scale Quantum (NISQ) hardware in a resource efficient manner. Our approach is reliant upon the formulation of a bipartitely decoupled parameterized ansatz within the disentangled unitary coupled cluster (dUCC) framework based on the principles of synergetics. Such decoupling emulates the total parameter optimization in a lower dimensional manifold, while a mutual synergistic relationship among the parameters is exploited to ensure characteristic accuracy. Without any pre-circuit measurements, our method leads to a highly compact fixed-depth ansatz with shallower circuits and fewer expectation value evaluations. Through analytical and numerical demonstrations, we demonstrate the method's superior performance under noise while concurrently ensuring requisite accuracy in future fault-tolerant systems. This approach enables rapid exploration of emerging chemical spaces by efficient utilization of near-term quantum hardware resources.","sentences":["Quantum computers hold immense potential in the field of chemistry, ushering new frontiers to solve complex many body problems that are beyond the reach of classical computers.","However, noise in the current quantum hardware limits their applicability to large chemical systems.","This work encompasses the development of a projective formalism that aims to compute ground-state energies of molecular systems accurately using Noisy Intermediate Scale Quantum (NISQ) hardware in a resource efficient manner.","Our approach is reliant upon the formulation of a bipartitely decoupled parameterized ansatz within the disentangled unitary coupled cluster (dUCC) framework based on the principles of synergetics.","Such decoupling emulates the total parameter optimization in a lower dimensional manifold, while a mutual synergistic relationship among the parameters is exploited to ensure characteristic accuracy.","Without any pre-circuit measurements, our method leads to a highly compact fixed-depth ansatz with shallower circuits and fewer expectation value evaluations.","Through analytical and numerical demonstrations, we demonstrate the method's superior performance under noise while concurrently ensuring requisite accuracy in future fault-tolerant systems.","This approach enables rapid exploration of emerging chemical spaces by efficient utilization of near-term quantum hardware resources."],"url":"http://arxiv.org/abs/2403.08519v1","category":"quant-ph"}
{"created":"2024-03-13 12:54:09","title":"Compliant Hierarchical Control for Arbitrary Equality and Inequality Tasks with Strict and Soft Priorities","abstract":"When a robotic system is redundant with respect to a given task, the remaining degrees of freedom can be used to satisfy additional objectives. With current robotic systems having more and more degrees of freedom, this can lead to an entire hierarchy of tasks that need to be solved according to given priorities. In this paper, the first compliant control strategy is presented that allows to consider an arbitrary number of equality and inequality tasks, while still preserving the natural inertia of the robot. The approach is therefore a generalization of a passivity-based controller to the case of an arbitrary number of equality and inequality tasks. The key idea of the method is to use a Weighted Hierarchical Quadratic Problem to extract the set of active tasks and use the latter to perform a coordinate transformation that inertially decouples the tasks. Thereby unifying the line of research focusing on optimization-based and passivity-based multi-task controllers. The method is validated in simulation.","sentences":["When a robotic system is redundant with respect to a given task, the remaining degrees of freedom can be used to satisfy additional objectives.","With current robotic systems having more and more degrees of freedom, this can lead to an entire hierarchy of tasks that need to be solved according to given priorities.","In this paper, the first compliant control strategy is presented that allows to consider an arbitrary number of equality and inequality tasks, while still preserving the natural inertia of the robot.","The approach is therefore a generalization of a passivity-based controller to the case of an arbitrary number of equality and inequality tasks.","The key idea of the method is to use a Weighted Hierarchical Quadratic Problem to extract the set of active tasks and use the latter to perform a coordinate transformation that inertially decouples the tasks.","Thereby unifying the line of research focusing on optimization-based and passivity-based multi-task controllers.","The method is validated in simulation."],"url":"http://arxiv.org/abs/2403.08491v1","category":"cs.RO"}
{"created":"2024-03-13 12:52:33","title":"Protocol Optimization for Functional Cardiac CT Imaging Using Noise Emulation in the Raw Data Domain","abstract":"Four-dimensional (4D) wide coverage computed tomography (CT) is an effective imaging modality for measuring the mechanical function of the myocardium. However, repeated CT measurement across several heartbeats is still a concern. A projection-domain noise emulation method is presented to generate accurate low-dose (mA modulated) 4D cardiac CT scans from high-dose scans, enabling protocol optimization to deliver sufficient image quality for functional cardiac analysis while using a dose level that is as low as reasonably achievable. Given a targeted low-dose mA modulation curve, the proposed noise emulation method injects both quantum and electronic noise of proper magnitude and correlation to the high-dose data in projection domain. A spatially varying detector gain term as well as its calibration method were proposed to further improve the noise emulation accuracy. To determine the low dose threshold, a projection domain image quality (IQ) metric was proposed that is based on the number of projection rays that do not fall under the non-linear region of the detector. Experiments were performed to validate the noise emulation method with both phantom and clinical data. For both phantom and clinical data, the low-dose emulated images exhibited similar noise magnitude, artifacts, and texture to that of the real low-dose images. The proposed channel-dependent detector gain term resulted in additional increase in emulation accuracy. Using the proposed IQ metric, recommended kVp and mA settings were calculated for low dose 4D Cardiac CT acquisitions for patients of different sizes. In conclusion, a detailed method to estimate system-dependent parameters for a raw-data based low dose emulation framework was described. The proposed low-dose emulation method can be used to prospectively select patient-specific minimal-dose protocols for functional cardiac CT.","sentences":["Four-dimensional (4D) wide coverage computed tomography (CT) is an effective imaging modality for measuring the mechanical function of the myocardium.","However, repeated CT measurement across several heartbeats is still a concern.","A projection-domain noise emulation method is presented to generate accurate low-dose (mA modulated) 4D cardiac CT scans from high-dose scans, enabling protocol optimization to deliver sufficient image quality for functional cardiac analysis while using a dose level that is as low as reasonably achievable.","Given a targeted low-dose mA modulation curve, the proposed noise emulation method injects both quantum and electronic noise of proper magnitude and correlation to the high-dose data in projection domain.","A spatially varying detector gain term as well as its calibration method were proposed to further improve the noise emulation accuracy.","To determine the low dose threshold, a projection domain image quality (IQ) metric was proposed that is based on the number of projection rays that do not fall under the non-linear region of the detector.","Experiments were performed to validate the noise emulation method with both phantom and clinical data.","For both phantom and clinical data, the low-dose emulated images exhibited similar noise magnitude, artifacts, and texture to that of the real low-dose images.","The proposed channel-dependent detector gain term resulted in additional increase in emulation accuracy.","Using the proposed IQ metric, recommended kVp and mA settings were calculated for low dose 4D Cardiac CT acquisitions for patients of different sizes.","In conclusion, a detailed method to estimate system-dependent parameters for a raw-data based low dose emulation framework was described.","The proposed low-dose emulation method can be used to prospectively select patient-specific minimal-dose protocols for functional cardiac CT."],"url":"http://arxiv.org/abs/2403.08486v1","category":"physics.med-ph"}
{"created":"2024-03-13 12:50:23","title":"Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH Mask based Efficient Fine-tuning","abstract":"In view of the huge number of parameters of Large language models (LLMs) , tuning all parameters is very costly, and accordingly fine-tuning specific parameters is more sensible. Most of parameter efficient fine-tuning (PEFT) concentrate on parameter selection strategies, such as additive method, selective method and reparametrization-based method. However, there are few methods that consider the impact of data samples on parameter selecting, such as Fish Mask based method. Fish Mask randomly choose a part of data samples and treat them equally during parameter selection, which is unable to dynamically select optimal parameters for inconstant data distributions. In this work, we adopt a data-oriented perspective, then proposing an IRD ($\\mathrm{\\underline I}$terative sample-parameter $\\mathrm{\\underline R}$ange $\\mathrm{\\underline D}$ecreasing) algorithm to search the best setting of sample-parameter pair for FISH Mask. In each iteration, by searching the set of samples and parameters with larger Fish information, IRD can find better sample-parameter pair in most scale. We demonstrate the effectiveness and rationality of proposed strategy by conducting experiments on GLUE benchmark. Experimental results show our strategy optimizes the parameter selection and achieves preferable performance.","sentences":["In view of the huge number of parameters of Large language models (LLMs) , tuning all parameters is very costly, and accordingly fine-tuning specific parameters is more sensible.","Most of parameter efficient fine-tuning (PEFT) concentrate on parameter selection strategies, such as additive method, selective method and reparametrization-based method.","However, there are few methods that consider the impact of data samples on parameter selecting, such as Fish Mask based method.","Fish Mask randomly choose a part of data samples and treat them equally during parameter selection, which is unable to dynamically select optimal parameters for inconstant data distributions.","In this work, we adopt a data-oriented perspective, then proposing an IRD ($\\mathrm{\\underline I}$terative sample-parameter $\\mathrm{\\underline R}$ange $\\mathrm{\\underline D}$ecreasing) algorithm to search the best setting of sample-parameter pair for FISH Mask.","In each iteration, by searching the set of samples and parameters with larger Fish information, IRD can find better sample-parameter pair in most scale.","We demonstrate the effectiveness and rationality of proposed strategy by conducting experiments on GLUE benchmark.","Experimental results show our strategy optimizes the parameter selection and achieves preferable performance."],"url":"http://arxiv.org/abs/2403.08484v1","category":"cs.CL"}
{"created":"2024-03-13 12:49:49","title":"Study of Physical Characteristics of the New Half-Heusler Alloy BaHgSn by DFT Analysis","abstract":"To investigate the physical characteristics of the half-Heusler BaHgSn molecule, we used theoretical calculations within the Density Functional Theory (DFT) framework utilizing the LSDA+mBJ technique in this study. Using the optimal lattice parameters, we discover that half-Heusler BaHgSn exhibits a Dirac semimetal behavior with a band gap of 0.1 eV. Thomas Charpin's numerical first-principles calculation approach was applied to determine the elastic constants of hexagonal BaHgSn alloys. The material's optical characteristics verified its prospective use in infrared-visible devices. According to a thermo-electric properties analysis, at 20x10^18 {\\Omega}-1.m-1.s-1, the electrical conductivity reaches its maximum after increasing gradually up to 500 K. Compared to other compounds, these results indicate that BaHgSn has potential for use in opto-electronic and thermo-electric devices.","sentences":["To investigate the physical characteristics of the half-Heusler BaHgSn molecule, we used theoretical calculations within the Density Functional Theory (DFT) framework utilizing the LSDA+mBJ technique in this study.","Using the optimal lattice parameters, we discover that half-Heusler BaHgSn exhibits a Dirac semimetal behavior with a band gap of 0.1 eV. Thomas Charpin's numerical first-principles calculation approach was applied to determine the elastic constants of hexagonal BaHgSn alloys.","The material's optical characteristics verified its prospective use in infrared-visible devices.","According to a thermo-electric properties analysis, at 20x10^18 {\\Omega}-1.m-1.s-1, the electrical conductivity reaches its maximum after increasing gradually up to 500 K. Compared to other compounds, these results indicate that BaHgSn has potential for use in opto-electronic and thermo-electric devices."],"url":"http://arxiv.org/abs/2403.08483v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-13 12:34:39","title":"Convergence of ADAM for Lipschitz Objective Functions","abstract":"The aim of this paper is to prove the exponential convergence, local   and global, of Adam algorithm under precise conditions on the   parameters, when the objective function lacks   differentiability. More precisely, we require Lipschitz continuity,   and control on the gradient whenever it exists. We provide also   examples of interesting functions that satisfies the required   restrictions.","sentences":["The aim of this paper is to prove the exponential convergence, local   and global, of Adam algorithm under precise conditions on the   parameters, when the objective function lacks   differentiability.","More precisely, we require Lipschitz continuity,   and control on the gradient whenever it exists.","We provide also   examples of interesting functions that satisfies the required   restrictions."],"url":"http://arxiv.org/abs/2403.08470v1","category":"math.OC"}
{"created":"2024-03-13 12:26:55","title":"Diffusion Models with Implicit Guidance for Medical Anomaly Detection","abstract":"Diffusion models have advanced unsupervised anomaly detection by improving the transformation of pathological images into pseudo-healthy equivalents. Nonetheless, standard approaches may compromise critical information during pathology removal, leading to restorations that do not align with unaffected regions in the original scans. Such discrepancies can inadvertently increase false positive rates and reduce specificity, complicating radiological evaluations. This paper introduces Temporal Harmonization for Optimal Restoration (THOR), which refines the de-noising process by integrating implicit guidance through temporal anomaly maps. THOR aims to preserve the integrity of healthy tissue in areas unaffected by pathology. Comparative evaluations show that THOR surpasses existing diffusion-based methods in detecting and segmenting anomalies in brain MRIs and wrist X-rays. Code: https://github.com/ci-ber/THOR_DDPM.","sentences":["Diffusion models have advanced unsupervised anomaly detection by improving the transformation of pathological images into pseudo-healthy equivalents.","Nonetheless, standard approaches may compromise critical information during pathology removal, leading to restorations that do not align with unaffected regions in the original scans.","Such discrepancies can inadvertently increase false positive rates and reduce specificity, complicating radiological evaluations.","This paper introduces Temporal Harmonization for Optimal Restoration (THOR), which refines the de-noising process by integrating implicit guidance through temporal anomaly maps.","THOR aims to preserve the integrity of healthy tissue in areas unaffected by pathology.","Comparative evaluations show that THOR surpasses existing diffusion-based methods in detecting and segmenting anomalies in brain MRIs and wrist X-rays.","Code: https://github.com/ci-ber/THOR_DDPM."],"url":"http://arxiv.org/abs/2403.08464v1","category":"eess.IV"}
{"created":"2024-03-13 12:25:34","title":"The Heisenberg-RIXS instrument at the European XFEL","abstract":"Resonant Inelastic X-ray Scattering (RIXS) is an ideal X-ray spectroscopy method to push the combination of energy and time resolutions to the Fourier transform ultimate limit, because it is unaffected by the core-hole lifetime energy broadening. And in pump-probe experiments the interaction time is made very short by the same core-hole lifetime. RIXS is very photon hungry so it takes great advantage from high repetition rate pulsed X-ray sources like the European XFEL. The hRIXS instrument is designed for RIXS experiments in the soft X-ray range with energy resolution approaching the Fourier and the Heisenberg limits. It is based on a spherical grating with variable line spacing (VLS) and a position-sensitive 2D detector. Initially, two gratings are installed to adequately cover the whole photon energy range. With optimized spot size on the sample and small pixel detector the energy resolution can be better than 40 meV at any photon energy below 1000 eV. At the SCS instrument of the European XFEL the spectrometer can be easily positioned thanks to air-pads on a high-quality floor, allowing the scattering angle to be continuously adjusted over the 65-145 deg range. It can be coupled to two different sample interaction chamber, one for liquid jets and one for solids, each equipped at the state-of-the-art and compatible for optical laser pumping in collinear geometry. The measured performances, in terms of energy resolution and count rate on the detector, closely match design expectations. hRIXS is open to public users since the summer of 2022.","sentences":["Resonant Inelastic X-ray Scattering (RIXS) is an ideal X-ray spectroscopy method to push the combination of energy and time resolutions to the Fourier transform ultimate limit, because it is unaffected by the core-hole lifetime energy broadening.","And in pump-probe experiments the interaction time is made very short by the same core-hole lifetime.","RIXS is very photon hungry so it takes great advantage from high repetition rate pulsed X-ray sources like the European XFEL.","The hRIXS instrument is designed for RIXS experiments in the soft X-ray range with energy resolution approaching the Fourier and the Heisenberg limits.","It is based on a spherical grating with variable line spacing (VLS) and a position-sensitive 2D detector.","Initially, two gratings are installed to adequately cover the whole photon energy range.","With optimized spot size on the sample and small pixel detector the energy resolution can be better than 40 meV at any photon energy below 1000 eV. At the SCS instrument of the European XFEL the spectrometer can be easily positioned thanks to air-pads on a high-quality floor, allowing the scattering angle to be continuously adjusted over the 65-145 deg range.","It can be coupled to two different sample interaction chamber, one for liquid jets and one for solids, each equipped at the state-of-the-art and compatible for optical laser pumping in collinear geometry.","The measured performances, in terms of energy resolution and count rate on the detector, closely match design expectations.","hRIXS is open to public users since the summer of 2022."],"url":"http://arxiv.org/abs/2403.08461v1","category":"cond-mat.str-el"}
{"created":"2024-03-13 12:09:44","title":"IAMCV Multi-Scenario Vehicle Interaction Dataset","abstract":"The acquisition and analysis of high-quality sensor data constitute an essential requirement in shaping the development of fully autonomous driving systems. This process is indispensable for enhancing road safety and ensuring the effectiveness of the technological advancements in the automotive industry. This study introduces the Interaction of Autonomous and Manually-Controlled Vehicles (IAMCV) dataset, a novel and extensive dataset focused on inter-vehicle interactions. The dataset, enriched with a sophisticated array of sensors such as Light Detection and Ranging, cameras, Inertial Measurement Unit/Global Positioning System, and vehicle bus data acquisition, provides a comprehensive representation of real-world driving scenarios that include roundabouts, intersections, country roads, and highways, recorded across diverse locations in Germany. Furthermore, the study shows the versatility of the IAMCV dataset through several proof-of-concept use cases. Firstly, an unsupervised trajectory clustering algorithm illustrates the dataset's capability in categorizing vehicle movements without the need for labeled training data. Secondly, we compare an online camera calibration method with the Robot Operating System-based standard, using images captured in the dataset. Finally, a preliminary test employing the YOLOv8 object-detection model is conducted, augmented by reflections on the transferability of object detection across various LIDAR resolutions. These use cases underscore the practical utility of the collected dataset, emphasizing its potential to advance research and innovation in the area of intelligent vehicles.","sentences":["The acquisition and analysis of high-quality sensor data constitute an essential requirement in shaping the development of fully autonomous driving systems.","This process is indispensable for enhancing road safety and ensuring the effectiveness of the technological advancements in the automotive industry.","This study introduces the Interaction of Autonomous and Manually-Controlled Vehicles (IAMCV) dataset, a novel and extensive dataset focused on inter-vehicle interactions.","The dataset, enriched with a sophisticated array of sensors such as Light Detection and Ranging, cameras, Inertial Measurement Unit/Global Positioning System, and vehicle bus data acquisition, provides a comprehensive representation of real-world driving scenarios that include roundabouts, intersections, country roads, and highways, recorded across diverse locations in Germany.","Furthermore, the study shows the versatility of the IAMCV dataset through several proof-of-concept use cases.","Firstly, an unsupervised trajectory clustering algorithm illustrates the dataset's capability in categorizing vehicle movements without the need for labeled training data.","Secondly, we compare an online camera calibration method with the Robot Operating System-based standard, using images captured in the dataset.","Finally, a preliminary test employing the YOLOv8 object-detection model is conducted, augmented by reflections on the transferability of object detection across various LIDAR resolutions.","These use cases underscore the practical utility of the collected dataset, emphasizing its potential to advance research and innovation in the area of intelligent vehicles."],"url":"http://arxiv.org/abs/2403.08455v1","category":"cs.RO"}
{"created":"2024-03-13 11:56:10","title":"COSTREAM: Learned Cost Models for Operator Placement in Edge-Cloud Environments","abstract":"In this work, we present COSTREAM, a novel learned cost model for Distributed Stream Processing Systems that provides accurate predictions of the execution costs of a streaming query in an edge-cloud environment. The cost model can be used to find an initial placement of operators across heterogeneous hardware, which is particularly important in these environments. In our evaluation, we demonstrate that COSTREAM can produce highly accurate cost estimates for the initial operator placement and even generalize to unseen placements, queries, and hardware. When using COSTREAM to optimize the placements of streaming operators, a median speed-up of around 21x can be achieved compared to baselines.","sentences":["In this work, we present COSTREAM, a novel learned cost model for Distributed Stream Processing Systems that provides accurate predictions of the execution costs of a streaming query in an edge-cloud environment.","The cost model can be used to find an initial placement of operators across heterogeneous hardware, which is particularly important in these environments.","In our evaluation, we demonstrate that COSTREAM can produce highly accurate cost estimates for the initial operator placement and even generalize to unseen placements, queries, and hardware.","When using COSTREAM to optimize the placements of streaming operators, a median speed-up of around 21x can be achieved compared to baselines."],"url":"http://arxiv.org/abs/2403.08444v1","category":"cs.DC"}
{"created":"2024-03-13 11:35:02","title":"GRF-based Predictive Flocking Control with Dynamic Pattern Formation","abstract":"It is promising but challenging to design flocking control for a robot swarm to autonomously follow changing patterns or shapes in a optimal distributed manner. The optimal flocking control with dynamic pattern formation is, therefore, investigated in this paper. A predictive flocking control algorithm is proposed based on a Gibbs random field (GRF), where bio-inspired potential energies are used to charaterize ``robot-robot'' and ``robot-environment'' interactions. Specialized performance-related energies, e.g., motion smoothness, are introduced in the proposed design to improve the flocking behaviors. The optimal control is obtained by maximizing a posterior distribution of a GRF. A region-based shape control is accomplished for pattern formation in light of a mean shift technique. The proposed algorithm is evaluated via the comparison with two state-of-the-art flocking control methods in an environment with obstacles. Both numerical simulations and real-world experiments are conducted to demonstrate the efficiency of the proposed design.","sentences":["It is promising but challenging to design flocking control for a robot swarm to autonomously follow changing patterns or shapes in a optimal distributed manner.","The optimal flocking control with dynamic pattern formation is, therefore, investigated in this paper.","A predictive flocking control algorithm is proposed based on a Gibbs random field (GRF), where bio-inspired potential energies are used to charaterize ``robot-robot'' and ``robot-environment'' interactions.","Specialized performance-related energies, e.g., motion smoothness, are introduced in the proposed design to improve the flocking behaviors.","The optimal control is obtained by maximizing a posterior distribution of a GRF.","A region-based shape control is accomplished for pattern formation in light of a mean shift technique.","The proposed algorithm is evaluated via the comparison with two state-of-the-art flocking control methods in an environment with obstacles.","Both numerical simulations and real-world experiments are conducted to demonstrate the efficiency of the proposed design."],"url":"http://arxiv.org/abs/2403.08434v1","category":"cs.RO"}
{"created":"2024-03-13 11:20:34","title":"Specification Overfitting in Artificial Intelligence","abstract":"Machine learning (ML) and artificial intelligence (AI) approaches are often criticized for their inherent bias and for their lack of control, accountability, and transparency. Consequently, regulatory bodies struggle with containing this technology's potential negative side effects. High-level requirements such as fairness and robustness need to be formalized into concrete specification metrics, imperfect proxies that capture isolated aspects of the underlying requirements. Given possible trade-offs between different metrics and their vulnerability to over-optimization, integrating specification metrics in system development processes is not trivial. This paper defines specification overfitting, a scenario where systems focus excessively on specified metrics to the detriment of high-level requirements and task performance. We present an extensive literature survey to categorize how researchers propose, measure, and optimize specification metrics in several AI fields (e.g., natural language processing, computer vision, reinforcement learning). Using a keyword-based search on papers from major AI conferences and journals between 2018 and mid-2023, we identify and analyze 74 papers that propose or optimize specification metrics. We find that although most papers implicitly address specification overfitting (e.g., by reporting more than one specification metric), they rarely discuss which role specification metrics should play in system development or explicitly define the scope and assumptions behind metric formulations.","sentences":["Machine learning (ML) and artificial intelligence (AI) approaches are often criticized for their inherent bias and for their lack of control, accountability, and transparency.","Consequently, regulatory bodies struggle with containing this technology's potential negative side effects.","High-level requirements such as fairness and robustness need to be formalized into concrete specification metrics, imperfect proxies that capture isolated aspects of the underlying requirements.","Given possible trade-offs between different metrics and their vulnerability to over-optimization, integrating specification metrics in system development processes is not trivial.","This paper defines specification overfitting, a scenario where systems focus excessively on specified metrics to the detriment of high-level requirements and task performance.","We present an extensive literature survey to categorize how researchers propose, measure, and optimize specification metrics in several AI fields (e.g., natural language processing, computer vision, reinforcement learning).","Using a keyword-based search on papers from major AI conferences and journals between 2018 and mid-2023, we identify and analyze 74 papers that propose or optimize specification metrics.","We find that although most papers implicitly address specification overfitting (e.g., by reporting more than one specification metric), they rarely discuss which role specification metrics should play in system development or explicitly define the scope and assumptions behind metric formulations."],"url":"http://arxiv.org/abs/2403.08425v1","category":"cs.AI"}
{"created":"2024-03-13 11:16:43","title":"Tastle: Distract Large Language Models for Automatic Jailbreak Attack","abstract":"Large language models (LLMs) have achieved significant advances in recent days. Extensive efforts have been made before the public release of LLMs to align their behaviors with human values. The primary goal of alignment is to ensure their helpfulness, honesty and harmlessness. However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors. The jailbreak is to intentionally develop a malicious prompt that escapes from the LLM security restrictions to produce uncensored detrimental contents. Previous works explore different jailbreak methods for red teaming LLMs, yet they encounter challenges regarding to effectiveness and scalability. In this work, we propose Tastle, a novel black-box jailbreak framework for automated red teaming of LLMs. We designed malicious content concealing and memory reframing with an iterative optimization algorithm to jailbreak LLMs, motivated by the research about the distractibility and over-confidence phenomenon of LLMs. Extensive experiments of jailbreaking both open-source and proprietary LLMs demonstrate the superiority of our framework in terms of effectiveness, scalability and transferability. We also evaluate the effectiveness of existing jailbreak defense methods against our attack and highlight the crucial need to develop more effective and practical defense strategies.","sentences":["Large language models (LLMs) have achieved significant advances in recent days.","Extensive efforts have been made before the public release of LLMs to align their behaviors with human values.","The primary goal of alignment is to ensure their helpfulness, honesty and harmlessness.","However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors.","The jailbreak is to intentionally develop a malicious prompt that escapes from the LLM security restrictions to produce uncensored detrimental contents.","Previous works explore different jailbreak methods for red teaming LLMs, yet they encounter challenges regarding to effectiveness and scalability.","In this work, we propose Tastle, a novel black-box jailbreak framework for automated red teaming of LLMs.","We designed malicious content concealing and memory reframing with an iterative optimization algorithm to jailbreak LLMs, motivated by the research about the distractibility and over-confidence phenomenon of LLMs.","Extensive experiments of jailbreaking both open-source and proprietary LLMs demonstrate the superiority of our framework in terms of effectiveness, scalability and transferability.","We also evaluate the effectiveness of existing jailbreak defense methods against our attack and highlight the crucial need to develop more effective and practical defense strategies."],"url":"http://arxiv.org/abs/2403.08424v1","category":"cs.CR"}
{"created":"2024-03-13 11:14:10","title":"Stochastic action for the entanglement of a noisy monitored two-qubit system","abstract":"We study the effect of local unitary noise on the entanglement evolution of a two-qubit system subject to local monitoring and inter-qubit coupling. We construct a stochastic Hamiltonian by incorporating the noise into the Chantasri-Dressel-Jordan path integral and use it to identify the optimal entanglement dynamics and to develop a diagrammatic method for a closed-form approximation of the average entanglement dynamics with an analytical dependence on the noise and measurement intensity. We find that both the optimal trajectory and diagrammatic expansion capture the oscillations of entanglement at short times. Numerical investigation of long-time steady-state entanglement reveals a non-monotonic relationship between concurrence and noise strength.","sentences":["We study the effect of local unitary noise on the entanglement evolution of a two-qubit system subject to local monitoring and inter-qubit coupling.","We construct a stochastic Hamiltonian by incorporating the noise into the Chantasri-Dressel-Jordan path integral and use it to identify the optimal entanglement dynamics and to develop a diagrammatic method for a closed-form approximation of the average entanglement dynamics with an analytical dependence on the noise and measurement intensity.","We find that both the optimal trajectory and diagrammatic expansion capture the oscillations of entanglement at short times.","Numerical investigation of long-time steady-state entanglement reveals a non-monotonic relationship between concurrence and noise strength."],"url":"http://arxiv.org/abs/2403.08422v1","category":"quant-ph"}
{"created":"2024-03-13 11:11:59","title":"Low-Cost and Real-Time Industrial Human Action Recognitions Based on Large-Scale Foundation Models","abstract":"Industrial managements, including quality control, cost and safety optimization, etc., heavily rely on high quality industrial human action recognitions (IHARs) which were hard to be implemented in large-scale industrial scenes due to their high costs and poor real-time performance. In this paper, we proposed a large-scale foundation model(LSFM)-based IHAR method, wherein various LSFMs and lightweight methods were jointly used, for the first time, to fulfill low-cost dataset establishment and real-time IHARs. Comprehensive tests on in-situ large-scale industrial manufacturing lines elucidated that the proposed method realized great reduction on employment costs, superior real-time performance, and satisfactory accuracy and generalization capabilities, indicating its great potential as a backbone IHAR method, especially for large-scale industrial applications.","sentences":["Industrial managements, including quality control, cost and safety optimization, etc., heavily rely on high quality industrial human action recognitions (IHARs) which were hard to be implemented in large-scale industrial scenes due to their high costs and poor real-time performance.","In this paper, we proposed a large-scale foundation model(LSFM)-based IHAR method, wherein various LSFMs and lightweight methods were jointly used, for the first time, to fulfill low-cost dataset establishment and real-time IHARs.","Comprehensive tests on in-situ large-scale industrial manufacturing lines elucidated that the proposed method realized great reduction on employment costs, superior real-time performance, and satisfactory accuracy and generalization capabilities, indicating its great potential as a backbone IHAR method, especially for large-scale industrial applications."],"url":"http://arxiv.org/abs/2403.08420v1","category":"cs.CV"}
{"created":"2024-03-13 11:10:09","title":"Boundary and distributed optimal control for a population dynamics PDE model with discontinuous in time Galerkin FEM schemes","abstract":"We consider fully discrete finite element approximations for a semilinear optimal control system of partial differential equations in two cases: for distributed and Robin boundary control. The ecological predator-prey optimal control model is approximated by conforming finite element methods mimicking the spatial part, while a discontinuous Galerkin method is used for the time discretization. We investigate the sensitivity of the solution distance from the target function, in cases with smooth and rough initial data. We employ low, and higher-order polynomials in time and space whenever proper regularity is present. The approximation schemes considered are with and without control constraints, driving efficiently the system to desired states realized using non-linear gradient methods.","sentences":["We consider fully discrete finite element approximations for a semilinear optimal control system of partial differential equations in two cases: for distributed and Robin boundary control.","The ecological predator-prey optimal control model is approximated by conforming finite element methods mimicking the spatial part, while a discontinuous Galerkin method is used for the time discretization.","We investigate the sensitivity of the solution distance from the target function, in cases with smooth and rough initial data.","We employ low, and higher-order polynomials in time and space whenever proper regularity is present.","The approximation schemes considered are with and without control constraints, driving efficiently the system to desired states realized using non-linear gradient methods."],"url":"http://arxiv.org/abs/2403.08419v1","category":"math.NA"}
{"created":"2024-03-13 10:52:07","title":"On the universal properties of stochastic processes under optimally tuned Poisson restart","abstract":"Poisson restart assumes that a stochastic process is interrupted and starts again at random time moments. A number of studies have demonstrated that this strategy may minimize the expected completion time in some classes of random search tasks. What is more, it turned out that under optimally tuned restart rate, any stochastic process, regardless of its nature and statistical details, satisfies a number of universal relations for the statistical moments of completion time. In this paper, we describe several new universal properties of optimally restarted processes. Also we obtain a universal inequality for the quadratic statistical moments of completion time in the optimization problem where stochastic process has several possible completion scenarios.","sentences":["Poisson restart assumes that a stochastic process is interrupted and starts again at random time moments.","A number of studies have demonstrated that this strategy may minimize the expected completion time in some classes of random search tasks.","What is more, it turned out that under optimally tuned restart rate, any stochastic process, regardless of its nature and statistical details, satisfies a number of universal relations for the statistical moments of completion time.","In this paper, we describe several new universal properties of optimally restarted processes.","Also we obtain a universal inequality for the quadratic statistical moments of completion time in the optimization problem where stochastic process has several possible completion scenarios."],"url":"http://arxiv.org/abs/2403.08409v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-13 10:39:37","title":"Stealthy and hyperuniform isotropic photonic bandgap structure in 3D","abstract":"In photonic crystals the propagation of light is governed by their photonic band structure, an ensemble of propagating states grouped into bands, separated by photonic band gaps. Due to discrete symmetries in spatially strictly periodic dielectric structures their photonic band structure is intrinsically anisotropic. However, for many applications, such as manufacturing artificial structural color materials or developing photonic computing devices, but also for the fundamental understanding of light-matter interactions, it is of major interest to seek materials with long range non-periodic dielectric structures which allow the formation of {\\it isotropic} photonic band gaps. Here, we report the first ever 3D isotropic photonic band gap for an optimized disordered stealthy hyperuniform structure for microwaves. The transmission spectra are directly compared to a diamond pattern and an amorphous structure with similar node density. The band structure is measured experimentally for all three microwave structures, manufactured by 3D-Laser-printing for meta-materials with refractive index up to $n=2.1$. Results agree well with finite-difference-time-domain numerical investigations and a priori calculations of the band-gap for the hyperuniform structure: the diamond structure shows gaps but being anisotropic as expected, the stealthy hyperuniform pattern shows an isotropic gap of very similar magnitude, while the amorphous structure does not show a gap at all. The centimeter scaled microwave structures may serve as prototypes for micrometer scaled structures with bandgaps in the technologically very interesting region of infrared (IR).","sentences":["In photonic crystals the propagation of light is governed by their photonic band structure, an ensemble of propagating states grouped into bands, separated by photonic band gaps.","Due to discrete symmetries in spatially strictly periodic dielectric structures their photonic band structure is intrinsically anisotropic.","However, for many applications, such as manufacturing artificial structural color materials or developing photonic computing devices, but also for the fundamental understanding of light-matter interactions, it is of major interest to seek materials with long range non-periodic dielectric structures which allow the formation of {\\it isotropic} photonic band gaps.","Here, we report the first ever 3D isotropic photonic band gap for an optimized disordered stealthy hyperuniform structure for microwaves.","The transmission spectra are directly compared to a diamond pattern and an amorphous structure with similar node density.","The band structure is measured experimentally for all three microwave structures, manufactured by 3D-Laser-printing for meta-materials with refractive index up to $n=2.1$. Results agree well with finite-difference-time-domain numerical investigations and a priori calculations of the band-gap for the hyperuniform structure: the diamond structure shows gaps but being anisotropic as expected, the stealthy hyperuniform pattern shows an isotropic gap of very similar magnitude, while the amorphous structure does not show a gap at all.","The centimeter scaled microwave structures may serve as prototypes for micrometer scaled structures with bandgaps in the technologically very interesting region of infrared (IR)."],"url":"http://arxiv.org/abs/2403.08404v1","category":"physics.app-ph"}
{"created":"2024-03-13 09:49:26","title":"Optimizing Risk-averse Human-AI Hybrid Teams","abstract":"We anticipate increased instances of humans and AI systems working together in what we refer to as a hybrid team. The increase in collaboration is expected as AI systems gain proficiency and their adoption becomes more widespread. However, their behavior is not error-free, making hybrid teams a very suitable solution. As such, we consider methods for improving performance for these teams of humans and AI systems. For hybrid teams, we will refer to both the humans and AI systems as agents. To improve team performance over that seen for agents operating individually, we propose a manager which learns, through a standard Reinforcement Learning scheme, how to best delegate, over time, the responsibility of taking a decision to any of the agents. We further guide the manager's learning so they also minimize how many changes in delegation are made resulting from undesirable team behavior. We demonstrate the optimality of our manager's performance in several grid environments which include failure states which terminate an episode and should be avoided. We perform our experiments with teams of agents with varying degrees of acceptable risk, in the form of proximity to a failure state, and measure the manager's ability to make effective delegation decisions with respect to its own risk-based constraints, then compare these to the optimal decisions. Our results show our manager can successfully learn desirable delegations which result in team paths near/exactly optimal with respect to path length and number of delegations.","sentences":["We anticipate increased instances of humans and AI systems working together in what we refer to as a hybrid team.","The increase in collaboration is expected as AI systems gain proficiency and their adoption becomes more widespread.","However, their behavior is not error-free, making hybrid teams a very suitable solution.","As such, we consider methods for improving performance for these teams of humans and AI systems.","For hybrid teams, we will refer to both the humans and AI systems as agents.","To improve team performance over that seen for agents operating individually, we propose a manager which learns, through a standard Reinforcement Learning scheme, how to best delegate, over time, the responsibility of taking a decision to any of the agents.","We further guide the manager's learning so they also minimize how many changes in delegation are made resulting from undesirable team behavior.","We demonstrate the optimality of our manager's performance in several grid environments which include failure states which terminate an episode and should be avoided.","We perform our experiments with teams of agents with varying degrees of acceptable risk, in the form of proximity to a failure state, and measure the manager's ability to make effective delegation decisions with respect to its own risk-based constraints, then compare these to the optimal decisions.","Our results show our manager can successfully learn desirable delegations which result in team paths near/exactly optimal with respect to path length and number of delegations."],"url":"http://arxiv.org/abs/2403.08386v1","category":"cs.AI"}
{"created":"2024-03-13 09:38:22","title":"Error-Free Near-Optimal Validated Agreement","abstract":"Byzantine agreement enables n processes to agree on a common L-bit value, despite t > 0 arbitrary failures. A long line of work has been dedicated to improving the worst-case bit complexity of Byzantine agreement in synchrony. This has culminated in COOL, an error-free (deterministically secure against a computationally unbounded adversary) algorithm that achieves a near-optimal bit complexity of O(nL + n^2 log n). COOL satisfies strong validity: if all correct processes propose the same value, only that value can be decided. Thus, whenever correct processes do not a priori agree, COOL might decide on \"bottom\", thus limiting its application in today's state machine replication (SMR) and blockchain protocols. In this work, we focus on the aforementioned limitation. Can we design an error-free near-optimal Byzantine agreement algorithm applicable in today's SMR and blockchain protocols? Can we design an error-free near-optimal agreement algorithm with external validity (a.k.a. validated agreement) stipulating that only values valid according to a predetermined predicate can be decided?   This paper answers the question affirmatively. Namely, we present EXT, an error-free synchronous Byzantine agreement algorithm that satisfies external (along with strong) validity while exchanging O(n log n L + n^2 log n) bits in the worst case. Importantly, EXT is optimally resilient (tolerates t < n / 3 failures) and terminates in optimal O(n) rounds. Perhaps surprisingly, we construct EXT by exploiting existing concepts: (1) the recursive framework proposed by Berman, Garay and Perry and Coan and Welch and recently restated by Momose and Ren, (2) the aforementioned COOL algorithm introduced by Chen, and (3) the data dissemination primitive introduced by Das, Xiang and Ren.","sentences":["Byzantine agreement enables n processes to agree on a common L-bit value, despite t > 0 arbitrary failures.","A long line of work has been dedicated to improving the worst-case bit complexity of Byzantine agreement in synchrony.","This has culminated in COOL, an error-free (deterministically secure against a computationally unbounded adversary) algorithm that achieves a near-optimal bit complexity of O(nL + n^2 log n).","COOL satisfies strong validity: if all correct processes propose the same value, only that value can be decided.","Thus, whenever correct processes do not a priori agree, COOL might decide on \"bottom\", thus limiting its application in today's state machine replication (SMR) and blockchain protocols.","In this work, we focus on the aforementioned limitation.","Can we design an error-free near-optimal Byzantine agreement algorithm applicable in today's SMR and blockchain protocols?","Can we design an error-free near-optimal agreement algorithm with external validity (a.k.a. validated agreement) stipulating that only values valid according to a predetermined predicate can be decided?   ","This paper answers the question affirmatively.","Namely, we present EXT, an error-free synchronous Byzantine agreement algorithm that satisfies external (along with strong) validity while exchanging O(n log n L + n^2 log n) bits in the worst case.","Importantly, EXT is optimally resilient (tolerates t < n / 3 failures) and terminates in optimal O(n) rounds.","Perhaps surprisingly, we construct EXT by exploiting existing concepts: (1) the recursive framework proposed by Berman, Garay and Perry and Coan and Welch and recently restated by Momose and Ren, (2) the aforementioned COOL algorithm introduced by Chen, and (3) the data dissemination primitive introduced by Das, Xiang and Ren."],"url":"http://arxiv.org/abs/2403.08374v1","category":"cs.DC"}
{"created":"2024-03-13 09:32:03","title":"User-Centric Beam Selection and Precoding Design for Coordinated Multiple-Satellite Systems","abstract":"This paper introduces a joint optimization framework for user-centric beam selection and linear precoding (LP) design in a coordinated multiple-satellite (CoMSat) system, employing a Digital-Fourier-Transform-based (DFT) beamforming (BF) technique. Regarding serving users at their target SINRs and minimizing the total transmit power, the scheme aims to efficiently determine satellites for users to associate with and activate the best cluster of beams together with optimizing LP for every satellite-to-user transmission. These technical objectives are first framed as a complex mixed-integer programming (MIP) challenge. To tackle this, we reformulate it into a joint cluster association and LP design problem. Then, by theoretically analyzing the duality relationship between downlink and uplink transmissions, we develop an efficient iterative method to identify the optimal solution. Additionally, a simpler duality approach for rapid beam selection and LP design is presented for comparison purposes. Simulation results underscore the effectiveness of our proposed schemes across various settings.","sentences":["This paper introduces a joint optimization framework for user-centric beam selection and linear precoding (LP) design in a coordinated multiple-satellite (CoMSat) system, employing a Digital-Fourier-Transform-based (DFT) beamforming (BF) technique.","Regarding serving users at their target SINRs and minimizing the total transmit power, the scheme aims to efficiently determine satellites for users to associate with and activate the best cluster of beams together with optimizing LP for every satellite-to-user transmission.","These technical objectives are first framed as a complex mixed-integer programming (MIP) challenge.","To tackle this, we reformulate it into a joint cluster association and LP design problem.","Then, by theoretically analyzing the duality relationship between downlink and uplink transmissions, we develop an efficient iterative method to identify the optimal solution.","Additionally, a simpler duality approach for rapid beam selection and LP design is presented for comparison purposes.","Simulation results underscore the effectiveness of our proposed schemes across various settings."],"url":"http://arxiv.org/abs/2403.08371v1","category":"eess.SP"}
{"created":"2024-03-13 09:25:44","title":"APACE: Agile and Perception-Aware Trajectory Generation for Quadrotor Flights","abstract":"Various perception-aware planning approaches have attempted to enhance the state estimation accuracy during maneuvers, while the feature matchability among frames, a crucial factor influencing estimation accuracy, has often been overlooked. In this paper, we present APACE, an Agile and Perception-Aware trajeCtory gEneration framework for quadrotors aggressive flight, that takes into account feature matchability during trajectory planning. We seek to generate a perception-aware trajectory that reduces the error of visual-based estimator while satisfying the constraints on smoothness, safety, agility and the quadrotor dynamics. The perception objective is achieved by maximizing the number of covisible features while ensuring small enough parallax angles. Additionally, we propose a differentiable and accurate visibility model that allows decomposition of the trajectory planning problem for efficient optimization resolution. Through validations conducted in both a photorealistic simulator and real-world experiments, we demonstrate that the trajectories generated by our method significantly improve state estimation accuracy, with root mean square error (RMSE) reduced by up to an order of magnitude. The source code will be released to benefit the community.","sentences":["Various perception-aware planning approaches have attempted to enhance the state estimation accuracy during maneuvers, while the feature matchability among frames, a crucial factor influencing estimation accuracy, has often been overlooked.","In this paper, we present APACE, an Agile and Perception-Aware trajeCtory gEneration framework for quadrotors aggressive flight, that takes into account feature matchability during trajectory planning.","We seek to generate a perception-aware trajectory that reduces the error of visual-based estimator while satisfying the constraints on smoothness, safety, agility and the quadrotor dynamics.","The perception objective is achieved by maximizing the number of covisible features while ensuring small enough parallax angles.","Additionally, we propose a differentiable and accurate visibility model that allows decomposition of the trajectory planning problem for efficient optimization resolution.","Through validations conducted in both a photorealistic simulator and real-world experiments, we demonstrate that the trajectories generated by our method significantly improve state estimation accuracy, with root mean square error (RMSE) reduced by up to an order of magnitude.","The source code will be released to benefit the community."],"url":"http://arxiv.org/abs/2403.08365v1","category":"cs.RO"}
{"created":"2024-03-13 09:00:38","title":"Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods","abstract":"Data augmentation is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models. It primarily involves the application of appropriate data transformation operations to create new data samples with desired properties. Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually. Automated data augmentation methods aim to automate the process. State-of-the-art approaches typically rely on automated machine learning (AutoML) principles. This work presents a comprehensive survey of AutoML-based data augmentation techniques. We discuss various approaches for accomplishing data augmentation with AutoML, including data manipulation, data integration and data synthesis techniques. We present extensive discussion of techniques for realizing each of the major subtasks of the data augmentation process: search space design, hyperparameter optimization and model evaluation. Finally, we carried out an extensive comparison and analysis of the performance of automated data augmentation techniques and state-of-the-art methods based on classical augmentation approaches. The results show that AutoML methods for data augmentation currently outperform state-of-the-art techniques based on conventional approaches.","sentences":["Data augmentation is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models.","It primarily involves the application of appropriate data transformation operations to create new data samples with desired properties.","Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually.","Automated data augmentation methods aim to automate the process.","State-of-the-art approaches typically rely on automated machine learning (AutoML) principles.","This work presents a comprehensive survey of AutoML-based data augmentation techniques.","We discuss various approaches for accomplishing data augmentation with AutoML, including data manipulation, data integration and data synthesis techniques.","We present extensive discussion of techniques for realizing each of the major subtasks of the data augmentation process: search space design, hyperparameter optimization and model evaluation.","Finally, we carried out an extensive comparison and analysis of the performance of automated data augmentation techniques and state-of-the-art methods based on classical augmentation approaches.","The results show that AutoML methods for data augmentation currently outperform state-of-the-art techniques based on conventional approaches."],"url":"http://arxiv.org/abs/2403.08352v1","category":"cs.LG"}
{"created":"2024-03-13 08:51:10","title":"Discretization of Total Variation in Optimization with Integrality Constraint","abstract":"We introduce discretizations of infinite-dimensional optimization problems with total variation regularization and integrality constraints on the optimization variables. We advance the discretization of the dual formulation of the total variation term with Raviart--Thomas functions which is known from literature for certain convex problems. Since we have an integrality constraint, the previous analysis from Caillaud and Chambolle [10] does not hold anymore. Even weaker $\\Gamma$-convergence results do not hold anymore because the recovery sequences generally need to attain non-integer values to recover the total variation of the limit function. We solve this issue by introducing a discretization of the input functions on an embedded, finer mesh. A superlinear coupling of the mesh sizes implies an averaging on the coarser mesh of the Raviart--Thomas ansatz, which enables to recover the total variation of integer-valued limit functions with integer-valued discretized input functions. Moreover, we are able to estimate the discretized total variation of the recovery sequence by the total variation of its limit and an error depending on the mesh size ratio. For the discretized optimization problems, we additionally add a constraint that vanishes in the limit and enforces compactness of the sequence of minimizers, which yields their convergence to a minimizer of the original problem. This constraint contains a degree of freedom whose admissible range is determined. Its choice may have a strong impact on the solutions in practice as we demonstrate with an example from imaging.","sentences":["We introduce discretizations of infinite-dimensional optimization problems with total variation regularization and integrality constraints on the optimization variables.","We advance the discretization of the dual formulation of the total variation term with Raviart--Thomas functions which is known from literature for certain convex problems.","Since we have an integrality constraint, the previous analysis from Caillaud and Chambolle","[10] does not hold anymore.","Even weaker $\\Gamma$-convergence results do not hold anymore because the recovery sequences generally need to attain non-integer values to recover the total variation of the limit function.","We solve this issue by introducing a discretization of the input functions on an embedded, finer mesh.","A superlinear coupling of the mesh sizes implies an averaging on the coarser mesh of the Raviart--Thomas ansatz, which enables to recover the total variation of integer-valued limit functions with integer-valued discretized input functions.","Moreover, we are able to estimate the discretized total variation of the recovery sequence by the total variation of its limit and an error depending on the mesh size ratio.","For the discretized optimization problems, we additionally add a constraint that vanishes in the limit and enforces compactness of the sequence of minimizers, which yields their convergence to a minimizer of the original problem.","This constraint contains a degree of freedom whose admissible range is determined.","Its choice may have a strong impact on the solutions in practice as we demonstrate with an example from imaging."],"url":"http://arxiv.org/abs/2403.08346v1","category":"math.NA"}
