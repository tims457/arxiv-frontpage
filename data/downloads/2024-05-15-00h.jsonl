{"created":"2024-05-13 17:59:56","title":"MambaOut: Do We Really Need Mamba for Vision?","abstract":"Mamba, an architecture with RNN-like token mixer of state space model (SSM), was recently introduced to address the quadratic complexity of the attention mechanism and subsequently applied to vision tasks. Nevertheless, the performance of Mamba for vision is often underwhelming when compared with convolutional and attention-based models. In this paper, we delve into the essence of Mamba, and conceptually conclude that Mamba is ideally suited for tasks with long-sequence and autoregressive characteristics. For vision tasks, as image classification does not align with either characteristic, we hypothesize that Mamba is not necessary for this task; Detection and segmentation tasks are also not autoregressive, yet they adhere to the long-sequence characteristic, so we believe it is still worthwhile to explore Mamba's potential for these tasks. To empirically verify our hypotheses, we construct a series of models named \\emph{MambaOut} through stacking Mamba blocks while removing their core token mixer, SSM. Experimental results strongly support our hypotheses. Specifically, our MambaOut model surpasses all visual Mamba models on ImageNet image classification, indicating that Mamba is indeed unnecessary for this task. As for detection and segmentation, MambaOut cannot match the performance of state-of-the-art visual Mamba models, demonstrating the potential of Mamba for long-sequence visual tasks. The code is available at https://github.com/yuweihao/MambaOut","sentences":["Mamba, an architecture with RNN-like token mixer of state space model (SSM), was recently introduced to address the quadratic complexity of the attention mechanism and subsequently applied to vision tasks.","Nevertheless, the performance of Mamba for vision is often underwhelming when compared with convolutional and attention-based models.","In this paper, we delve into the essence of Mamba, and conceptually conclude that Mamba is ideally suited for tasks with long-sequence and autoregressive characteristics.","For vision tasks, as image classification does not align with either characteristic, we hypothesize that Mamba is not necessary for this task; Detection and segmentation tasks are also not autoregressive, yet they adhere to the long-sequence characteristic, so we believe it is still worthwhile to explore Mamba's potential for these tasks.","To empirically verify our hypotheses, we construct a series of models named \\emph{MambaOut} through stacking Mamba blocks while removing their core token mixer, SSM.","Experimental results strongly support our hypotheses.","Specifically, our MambaOut model surpasses all visual Mamba models on ImageNet image classification, indicating that Mamba is indeed unnecessary for this task.","As for detection and segmentation, MambaOut cannot match the performance of state-of-the-art visual Mamba models, demonstrating the potential of Mamba for long-sequence visual tasks.","The code is available at https://github.com/yuweihao/MambaOut"],"url":"http://arxiv.org/abs/2405.07992v1","category":"cs.CV"}
{"created":"2024-05-13 17:59:36","title":"SPIN: Simultaneous Perception, Interaction and Navigation","abstract":"While there has been remarkable progress recently in the fields of manipulation and locomotion, mobile manipulation remains a long-standing challenge. Compared to locomotion or static manipulation, a mobile system must make a diverse range of long-horizon tasks feasible in unstructured and dynamic environments. While the applications are broad and interesting, there are a plethora of challenges in developing these systems such as coordination between the base and arm, reliance on onboard perception for perceiving and interacting with the environment, and most importantly, simultaneously integrating all these parts together. Prior works approach the problem using disentangled modular skills for mobility and manipulation that are trivially tied together. This causes several limitations such as compounding errors, delays in decision-making, and no whole-body coordination. In this work, we present a reactive mobile manipulation framework that uses an active visual system to consciously perceive and react to its environment. Similar to how humans leverage whole-body and hand-eye coordination, we develop a mobile manipulator that exploits its ability to move and see, more specifically -- to move in order to see and to see in order to move. This allows it to not only move around and interact with its environment but also, choose \"when\" to perceive \"what\" using an active visual system. We observe that such an agent learns to navigate around complex cluttered scenarios while displaying agile whole-body coordination using only ego-vision without needing to create environment maps. Results visualizations and videos at https://spin-robot.github.io/","sentences":["While there has been remarkable progress recently in the fields of manipulation and locomotion, mobile manipulation remains a long-standing challenge.","Compared to locomotion or static manipulation, a mobile system must make a diverse range of long-horizon tasks feasible in unstructured and dynamic environments.","While the applications are broad and interesting, there are a plethora of challenges in developing these systems such as coordination between the base and arm, reliance on onboard perception for perceiving and interacting with the environment, and most importantly, simultaneously integrating all these parts together.","Prior works approach the problem using disentangled modular skills for mobility and manipulation that are trivially tied together.","This causes several limitations such as compounding errors, delays in decision-making, and no whole-body coordination.","In this work, we present a reactive mobile manipulation framework that uses an active visual system to consciously perceive and react to its environment.","Similar to how humans leverage whole-body and hand-eye coordination, we develop a mobile manipulator that exploits its ability to move and see, more specifically -- to move in order to see and to see in order to move.","This allows it to not only move around and interact with its environment but also, choose \"when\" to perceive \"what\" using an active visual system.","We observe that such an agent learns to navigate around complex cluttered scenarios while displaying agile whole-body coordination using only ego-vision without needing to create environment maps.","Results visualizations and videos at https://spin-robot.github.io/"],"url":"http://arxiv.org/abs/2405.07991v1","category":"cs.RO"}
{"created":"2024-05-13 17:59:22","title":"Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots","abstract":"The remarkable progress of Multi-modal Large Language Models (MLLMs) has attracted significant attention due to their superior performance in visual contexts. However, their capabilities in turning visual figure to executable code, have not been evaluated thoroughly. To address this, we introduce Plot2Code, a comprehensive visual coding benchmark designed for a fair and in-depth assessment of MLLMs. We carefully collect 132 manually selected high-quality matplotlib plots across six plot types from publicly available matplotlib galleries. For each plot, we carefully offer its source code, and an descriptive instruction summarized by GPT-4. This approach enables Plot2Code to extensively evaluate MLLMs' code capabilities across various input modalities. Furthermore, we propose three automatic evaluation metrics, including code pass rate, text-match ratio, and GPT-4V overall rating, for a fine-grained assessment of the output code and rendered images. Instead of simply judging pass or fail, we employ GPT-4V to make an overall judgement between the generated and reference images, which has been shown to be consistent with human evaluation. The evaluation results, which include analyses of 14 MLLMs such as the proprietary GPT-4V, Gemini-Pro, and the open-sourced Mini-Gemini, highlight the substantial challenges presented by Plot2Code. With Plot2Code, we reveal that most existing MLLMs struggle with visual coding for text-dense plots, heavily relying on textual instruction. We hope that the evaluation results from Plot2Code on visual coding will guide the future development of MLLMs. All data involved with Plot2Code are available at https://huggingface.co/datasets/TencentARC/Plot2Code.","sentences":["The remarkable progress of Multi-modal Large Language Models (MLLMs) has attracted significant attention due to their superior performance in visual contexts.","However, their capabilities in turning visual figure to executable code, have not been evaluated thoroughly.","To address this, we introduce Plot2Code, a comprehensive visual coding benchmark designed for a fair and in-depth assessment of MLLMs.","We carefully collect 132 manually selected high-quality matplotlib plots across six plot types from publicly available matplotlib galleries.","For each plot, we carefully offer its source code, and an descriptive instruction summarized by GPT-4.","This approach enables Plot2Code to extensively evaluate MLLMs' code capabilities across various input modalities.","Furthermore, we propose three automatic evaluation metrics, including code pass rate, text-match ratio, and GPT-4V overall rating, for a fine-grained assessment of the output code and rendered images.","Instead of simply judging pass or fail, we employ GPT-4V to make an overall judgement between the generated and reference images, which has been shown to be consistent with human evaluation.","The evaluation results, which include analyses of 14 MLLMs such as the proprietary GPT-4V, Gemini-Pro, and the open-sourced Mini-Gemini, highlight the substantial challenges presented by Plot2Code.","With Plot2Code, we reveal that most existing MLLMs struggle with visual coding for text-dense plots, heavily relying on textual instruction.","We hope that the evaluation results from Plot2Code on visual coding will guide the future development of MLLMs.","All data involved with Plot2Code are available at https://huggingface.co/datasets/TencentARC/Plot2Code."],"url":"http://arxiv.org/abs/2405.07990v1","category":"cs.CL"}
{"created":"2024-05-13 17:58:51","title":"A Generalist Learner for Multifaceted Medical Image Interpretation","abstract":"Current medical artificial intelligence systems are often limited to narrow applications, hindering their widespread adoption in clinical practice. To address this limitation, we propose MedVersa, a generalist learner that enables flexible learning and tasking for medical image interpretation. By leveraging a large language model as a learnable orchestrator, MedVersa can learn from both visual and linguistic supervision, support multimodal inputs, and perform real-time task specification. This versatility allows MedVersa to adapt to various clinical scenarios and perform multifaceted medical image analysis. We introduce MedInterp, the largest multimodal dataset to date for medical image interpretation, consisting of over 13 million annotated instances spanning 11 tasks across 3 modalities, to support the development of MedVersa. Our experiments demonstrate that MedVersa achieves state-of-the-art performance in 9 tasks, sometimes outperforming specialist counterparts by over 10%. MedVersa is the first to showcase the viability of multimodal generative medical AI in implementing multimodal outputs, inputs, and dynamic task specification, highlighting its potential as a multifunctional system for comprehensive medical image analysis. This generalist approach to medical image interpretation paves the way for more adaptable and efficient AI-assisted clinical decision-making.","sentences":["Current medical artificial intelligence systems are often limited to narrow applications, hindering their widespread adoption in clinical practice.","To address this limitation, we propose MedVersa, a generalist learner that enables flexible learning and tasking for medical image interpretation.","By leveraging a large language model as a learnable orchestrator, MedVersa can learn from both visual and linguistic supervision, support multimodal inputs, and perform real-time task specification.","This versatility allows MedVersa to adapt to various clinical scenarios and perform multifaceted medical image analysis.","We introduce MedInterp, the largest multimodal dataset to date for medical image interpretation, consisting of over 13 million annotated instances spanning 11 tasks across 3 modalities, to support the development of MedVersa.","Our experiments demonstrate that MedVersa achieves state-of-the-art performance in 9 tasks, sometimes outperforming specialist counterparts by over 10%.","MedVersa is the first to showcase the viability of multimodal generative medical AI in implementing multimodal outputs, inputs, and dynamic task specification, highlighting its potential as a multifunctional system for comprehensive medical image analysis.","This generalist approach to medical image interpretation paves the way for more adaptable and efficient AI-assisted clinical decision-making."],"url":"http://arxiv.org/abs/2405.07988v1","category":"cs.CV"}
{"created":"2024-05-13 17:58:30","title":"The Platonic Representation Hypothesis","abstract":"We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.","sentences":["We argue that representations in AI models, particularly deep networks, are converging.","First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned.","Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way.","We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality.","We term such a representation the platonic representation and discuss several possible selective pressures toward it.","Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis."],"url":"http://arxiv.org/abs/2405.07987v1","category":"cs.LG"}
{"created":"2024-05-13 17:56:48","title":"Rowmotion on the chain of V's poset and whirling dynamics","abstract":"Given a finite poset $P$, we study the _whirling_ action on vertex-labelings of $P$ with the elements $\\{0,1,2,\\dotsc ,k\\}$. When such labelings are (weakly) order-reversing, we call them $k$-bounded $P$-partitions. We give a general equivariant bijection between $k$-bounded $P$-partitions and order ideals of the poset $P\\times [k]$ which conveys whirling to the well-studied rowmotion operator. As an application, we derive periodicity and homomesy results for rowmotion acting on the chain of V's poset $V \\times [k]$. We are able to generalize some of these results to the more complicated dynamics of rowmotion on $C_{n}\\times [k]$, where $C_{n}$ is the claw poset with $n$ unrelated elements each covering $\\widehat{0}$.","sentences":["Given a finite poset $P$, we study the _whirling_ action on vertex-labelings of $P$ with the elements $\\{0,1,2,\\dotsc ,k\\}$. When such labelings are (weakly) order-reversing, we call them $k$-bounded $P$-partitions.","We give a general equivariant bijection between $k$-bounded $P$-partitions and order ideals of the poset $P\\times [k]$ which conveys whirling to the well-studied rowmotion operator.","As an application, we derive periodicity and homomesy results for rowmotion acting on the chain of V's poset $V \\times [k]$. We are able to generalize some of these results to the more complicated dynamics of rowmotion on $C_{n}\\times [k]$, where $C_{n}$ is the claw poset with $n$ unrelated elements each covering $\\widehat{0}$."],"url":"http://arxiv.org/abs/2405.07984v1","category":"math.CO"}
{"created":"2024-05-13 17:52:38","title":"Generalizing Quantum Tanner Codes","abstract":"In this work, we present a generalization of the recently proposed quantum Tanner codes by Leverrier and Z\\'emor, which contains a construction of asymptotically good quantum LDPC codes. Quantum Tanner codes have so far been constructed equivalently from groups, Cayley graphs, or square complexes constructed from groups. We show how to enlarge this to group actions on finite sets, Schreier graphs, and a family of square complexes which is the largest possible in a certain sense. Furthermore, we discuss how the proposed generalization opens up the possibility of finding other families of asymptotically good quantum codes.","sentences":["In this work, we present a generalization of the recently proposed quantum Tanner codes by Leverrier and Z\\'emor, which contains a construction of asymptotically good quantum LDPC codes.","Quantum Tanner codes have so far been constructed equivalently from groups, Cayley graphs, or square complexes constructed from groups.","We show how to enlarge this to group actions on finite sets, Schreier graphs, and a family of square complexes which is the largest possible in a certain sense.","Furthermore, we discuss how the proposed generalization opens up the possibility of finding other families of asymptotically good quantum codes."],"url":"http://arxiv.org/abs/2405.07980v1","category":"cs.IT"}
{"created":"2024-05-13 17:52:25","title":"Low-order outcomes and clustered designs: combining design and analysis for causal inference under network interference","abstract":"Variance reduction for causal inference in the presence of network interference is often achieved through either outcome modeling, which is typically analyzed under unit-randomized Bernoulli designs, or clustered experimental designs, which are typically analyzed without strong parametric assumptions. In this work, we study the intersection of these two approaches and consider the problem of estimation in low-order outcome models using data from a general experimental design. Our contributions are threefold. First, we present an estimator of the total treatment effect (also called the global average treatment effect) in a low-degree outcome model when the data are collected under general experimental designs, generalizing previous results for Bernoulli designs. We refer to this estimator as the pseudoinverse estimator and give bounds on its bias and variance in terms of properties of the experimental design. Second, we evaluate these bounds for the case of cluster randomized designs with both Bernoulli and complete randomization. For clustered Bernoulli randomization, we find that our estimator is always unbiased and that its variance scales like the smaller of the variance obtained from a low-order assumption and the variance obtained from cluster randomization, showing that combining these variance reduction strategies is preferable to using either individually. For clustered complete randomization, we find a notable bias-variance trade-off mediated by specific features of the clustering. Third, when choosing a clustered experimental design, our bounds can be used to select a clustering from a set of candidate clusterings. Across a range of graphs and clustering algorithms, we show that our method consistently selects clusterings that perform well on a range of response models, suggesting that our bounds are useful to practitioners.","sentences":["Variance reduction for causal inference in the presence of network interference is often achieved through either outcome modeling, which is typically analyzed under unit-randomized Bernoulli designs, or clustered experimental designs, which are typically analyzed without strong parametric assumptions.","In this work, we study the intersection of these two approaches and consider the problem of estimation in low-order outcome models using data from a general experimental design.","Our contributions are threefold.","First, we present an estimator of the total treatment effect (also called the global average treatment effect) in a low-degree outcome model when the data are collected under general experimental designs, generalizing previous results for Bernoulli designs.","We refer to this estimator as the pseudoinverse estimator and give bounds on its bias and variance in terms of properties of the experimental design.","Second, we evaluate these bounds for the case of cluster randomized designs with both Bernoulli and complete randomization.","For clustered Bernoulli randomization, we find that our estimator is always unbiased and that its variance scales like the smaller of the variance obtained from a low-order assumption and the variance obtained from cluster randomization, showing that combining these variance reduction strategies is preferable to using either individually.","For clustered complete randomization, we find a notable bias-variance trade-off mediated by specific features of the clustering.","Third, when choosing a clustered experimental design, our bounds can be used to select a clustering from a set of candidate clusterings.","Across a range of graphs and clustering algorithms, we show that our method consistently selects clusterings that perform well on a range of response models, suggesting that our bounds are useful to practitioners."],"url":"http://arxiv.org/abs/2405.07979v2","category":"stat.ME"}
{"created":"2024-05-13 17:49:20","title":"A Demographic-Conditioned Variational Autoencoder for fMRI Distribution Sampling and Removal of Confounds","abstract":"Objective: fMRI and derived measures such as functional connectivity (FC) have been used to predict brain age, general fluid intelligence, psychiatric disease status, and preclinical neurodegenerative disease. However, it is not always clear that all demographic confounds, such as age, sex, and race, have been removed from fMRI data. Additionally, many fMRI datasets are restricted to authorized researchers, making dissemination of these valuable data sources challenging. Methods: We create a variational autoencoder (VAE)-based model, DemoVAE, to decorrelate fMRI features from demographics and generate high-quality synthetic fMRI data based on user-supplied demographics. We train and validate our model using two large, widely used datasets, the Philadelphia Neurodevelopmental Cohort (PNC) and Bipolar and Schizophrenia Network for Intermediate Phenotypes (BSNIP). Results: We find that DemoVAE recapitulates group differences in fMRI data while capturing the full breadth of individual variations. Significantly, we also find that most clinical and computerized battery fields that are correlated with fMRI data are not correlated with DemoVAE latents. An exception are several fields related to schizophrenia medication and symptom severity. Conclusion: Our model generates fMRI data that captures the full distribution of FC better than traditional VAE or GAN models. We also find that most prediction using fMRI data is dependent on correlation with, and prediction of, demographics. Significance: Our DemoVAE model allows for generation of high quality synthetic data conditioned on subject demographics as well as the removal of the confounding effects of demographics. We identify that FC-based prediction tasks are highly influenced by demographic confounds.","sentences":["Objective: fMRI and derived measures such as functional connectivity (FC) have been used to predict brain age, general fluid intelligence, psychiatric disease status, and preclinical neurodegenerative disease.","However, it is not always clear that all demographic confounds, such as age, sex, and race, have been removed from fMRI data.","Additionally, many fMRI datasets are restricted to authorized researchers, making dissemination of these valuable data sources challenging.","Methods: We create a variational autoencoder (VAE)-based model, DemoVAE, to decorrelate fMRI features from demographics and generate high-quality synthetic fMRI data based on user-supplied demographics.","We train and validate our model using two large, widely used datasets, the Philadelphia Neurodevelopmental Cohort (PNC) and Bipolar and Schizophrenia Network for Intermediate Phenotypes (BSNIP).","Results:","We find that DemoVAE recapitulates group differences in fMRI data while capturing the full breadth of individual variations.","Significantly, we also find that most clinical and computerized battery fields that are correlated with fMRI data are not correlated with DemoVAE latents.","An exception are several fields related to schizophrenia medication and symptom severity.","Conclusion: Our model generates fMRI data that captures the full distribution of FC better than traditional VAE or GAN models.","We also find that most prediction using fMRI data is dependent on correlation with, and prediction of, demographics.","Significance: Our DemoVAE model allows for generation of high quality synthetic data conditioned on subject demographics as well as the removal of the confounding effects of demographics.","We identify that FC-based prediction tasks are highly influenced by demographic confounds."],"url":"http://arxiv.org/abs/2405.07977v1","category":"q-bio.QM"}
{"created":"2024-05-13 17:48:45","title":"Dynamic Programming for Symbolic Boolean Realizability and Synthesis","abstract":"Inspired by recent progress in dynamic programming approaches for weighted model counting, we investigate a dynamic-programming approach in the context of boolean realizability and synthesis, which takes a conjunctive-normal-form boolean formula over input and output variables, and aims at synthesizing witness functions for the output variables in terms of the inputs. We show how graded project-join trees, obtained via tree decomposition, can be used to compute a BDD representing the realizability set for the input formulas in a bottom-up order. We then show how the intermediate BDDs generated during realizability checking phase can be applied to synthesizing the witness functions in a top-down manner. An experimental evaluation of a solver -- DPSynth -- based on these ideas demonstrates that our approach for Boolean realizabilty and synthesis has superior time and space performance over a heuristics-based approach using same symbolic representations. We discuss the advantage on scalability of the new approach, and also investigate our findings on the performance of the DP framework.","sentences":["Inspired by recent progress in dynamic programming approaches for weighted model counting, we investigate a dynamic-programming approach in the context of boolean realizability and synthesis, which takes a conjunctive-normal-form boolean formula over input and output variables, and aims at synthesizing witness functions for the output variables in terms of the inputs.","We show how graded project-join trees, obtained via tree decomposition, can be used to compute a BDD representing the realizability set for the input formulas in a bottom-up order.","We then show how the intermediate BDDs generated during realizability checking phase can be applied to synthesizing the witness functions in a top-down manner.","An experimental evaluation of a solver -- DPSynth -- based on these ideas demonstrates that our approach for Boolean realizabilty and synthesis has superior time and space performance over a heuristics-based approach using same symbolic representations.","We discuss the advantage on scalability of the new approach, and also investigate our findings on the performance of the DP framework."],"url":"http://arxiv.org/abs/2405.07975v1","category":"cs.FL"}
{"created":"2024-05-13 17:48:45","title":"Localized Adaptive Risk Control","abstract":"Adaptive Risk Control (ARC) is an online calibration strategy based on set prediction that offers worst-case deterministic long-term risk control, as well as statistical marginal coverage guarantees. ARC adjusts the size of the prediction set by varying a single scalar threshold based on feedback from past decisions. In this work, we introduce Localized Adaptive Risk Control (L-ARC), an online calibration scheme that targets statistical localized risk guarantees ranging from conditional risk to marginal risk, while preserving the worst-case performance of ARC. L-ARC updates a threshold function within a reproducing kernel Hilbert space (RKHS), with the kernel determining the level of localization of the statistical risk guarantee. The theoretical results highlight a trade-off between localization of the statistical risk and convergence speed to the long-term risk target. Thanks to localization, L-ARC is demonstrated via experiments to produce prediction sets with risk guarantees across different data subpopulations, significantly improving the fairness of the calibrated model for tasks such as image segmentation and beam selection in wireless networks.","sentences":["Adaptive Risk Control (ARC) is an online calibration strategy based on set prediction that offers worst-case deterministic long-term risk control, as well as statistical marginal coverage guarantees.","ARC adjusts the size of the prediction set by varying a single scalar threshold based on feedback from past decisions.","In this work, we introduce Localized Adaptive Risk Control (L-ARC), an online calibration scheme that targets statistical localized risk guarantees ranging from conditional risk to marginal risk, while preserving the worst-case performance of ARC.","L-ARC updates a threshold function within a reproducing kernel Hilbert space (RKHS), with the kernel determining the level of localization of the statistical risk guarantee.","The theoretical results highlight a trade-off between localization of the statistical risk and convergence speed to the long-term risk target.","Thanks to localization, L-ARC is demonstrated via experiments to produce prediction sets with risk guarantees across different data subpopulations, significantly improving the fairness of the calibrated model for tasks such as image segmentation and beam selection in wireless networks."],"url":"http://arxiv.org/abs/2405.07976v1","category":"stat.ML"}
{"created":"2024-05-13 17:48:22","title":"SignAvatar: Sign Language 3D Motion Reconstruction and Generation","abstract":"Achieving expressive 3D motion reconstruction and automatic generation for isolated sign words can be challenging, due to the lack of real-world 3D sign-word data, the complex nuances of signing motions, and the cross-modal understanding of sign language semantics. To address these challenges, we introduce SignAvatar, a framework capable of both word-level sign language reconstruction and generation. SignAvatar employs a transformer-based conditional variational autoencoder architecture, effectively establishing relationships across different semantic modalities. Additionally, this approach incorporates a curriculum learning strategy to enhance the model's robustness and generalization, resulting in more realistic motions. Furthermore, we contribute the ASL3DWord dataset, composed of 3D joint rotation data for the body, hands, and face, for unique sign words. We demonstrate the effectiveness of SignAvatar through extensive experiments, showcasing its superior reconstruction and automatic generation capabilities. The code and dataset are available on the project page.","sentences":["Achieving expressive 3D motion reconstruction and automatic generation for isolated sign words can be challenging, due to the lack of real-world 3D sign-word data, the complex nuances of signing motions, and the cross-modal understanding of sign language semantics.","To address these challenges, we introduce SignAvatar, a framework capable of both word-level sign language reconstruction and generation.","SignAvatar employs a transformer-based conditional variational autoencoder architecture, effectively establishing relationships across different semantic modalities.","Additionally, this approach incorporates a curriculum learning strategy to enhance the model's robustness and generalization, resulting in more realistic motions.","Furthermore, we contribute the ASL3DWord dataset, composed of 3D joint rotation data for the body, hands, and face, for unique sign words.","We demonstrate the effectiveness of SignAvatar through extensive experiments, showcasing its superior reconstruction and automatic generation capabilities.","The code and dataset are available on the project page."],"url":"http://arxiv.org/abs/2405.07974v1","category":"cs.CV"}
{"created":"2024-05-13 17:48:20","title":"Celestial amplitudes from conformal correlators with bulk-point kinematics","abstract":"We show that two- and three-point celestial (C)CFT$_{d-1}$ amplitudes can be directly obtained from correlation functions in a unitary Lorentzian CFT$_d$ on $\\mathbb{R}\\times S^{d-1}$. The recipe involves a rescaling of the operators, followed by an expansion around a bulk point configuration and a transformation to an $S^{d-1}$ conformal primary basis. The first two steps project the CFT$_d$ correlators onto distributions on $S^{d-1}$. The final step implements a dimensional reduction yielding CCFT$_{d-1}$ amplitudes that are manifestly vanishing for all in/out configurations and Poincar\\'e invariant. The dimensional reduction may be implemented either by evaluating certain time integral transforms around the bulk-point limit, or by analytically continuing the CFT$_d$ operator dimensions and restricting the operators to $S^{d-1}$ time slices separated by $\\pi$ in global time. The latter prescription generates the correct normalization for both two- and three-point functions. On the other hand, the celestial three-point amplitudes obtained via the former prescription are found to only agree after evaluating a residue at an integer linear combination of the CFT$_d$ conformal dimensions. The correct normalization may also be obtained by considering a different integration path in the uplift of the complexified time plane to its universal cover.","sentences":["We show that two- and three-point celestial (C)CFT$_{d-1}$ amplitudes can be directly obtained from correlation functions in a unitary Lorentzian CFT$_d$ on $\\mathbb{R}\\times S^{d-1}$. The recipe involves a rescaling of the operators, followed by an expansion around a bulk point configuration and a transformation to an $S^{d-1}$ conformal primary basis.","The first two steps project the CFT$_d$ correlators onto distributions on $S^{d-1}$. The final step implements a dimensional reduction yielding CCFT$_{d-1}$ amplitudes that are manifestly vanishing for all in/out configurations and Poincar\\'e invariant.","The dimensional reduction may be implemented either by evaluating certain time integral transforms around the bulk-point limit, or by analytically continuing the CFT$_d$ operator dimensions and restricting the operators to $S^{d-1}$ time slices separated by $\\pi$ in global time.","The latter prescription generates the correct normalization for both two- and three-point functions.","On the other hand, the celestial three-point amplitudes obtained via the former prescription are found to only agree after evaluating a residue at an integer linear combination of the CFT$_d$ conformal dimensions.","The correct normalization may also be obtained by considering a different integration path in the uplift of the complexified time plane to its universal cover."],"url":"http://arxiv.org/abs/2405.07972v1","category":"hep-th"}
{"created":"2024-05-13 17:48:20","title":"A Natural Formalized Proof Language","abstract":"Artificial intelligence assisted mathematical proof has become a highly focused area nowadays. One key problem in this field is to generate formal mathematical proofs from natural language proofs. Due to historical reasons, the formal proof languages adopted by traditional theorem provers were not intended to represent natural language proofs. Therefore, they are not well-suited for the aforementioned tasks and proof-checking work for educational purposes. In this paper, we design a proof language and its corresponding abstract syntax tree and implement a proof checking tool for it. This language can be easily converted from natural language, thus providing a rich corpus of formal proof. Additionally, it supports the handling of issues in informal proofs through static analysis, and enhances the expressive power of the language by introducing the structure of partial proofs. This design combines the expressiveness of natural language and the accuracy of formal language, resulting in an improved mathematical proof language.","sentences":["Artificial intelligence assisted mathematical proof has become a highly focused area nowadays.","One key problem in this field is to generate formal mathematical proofs from natural language proofs.","Due to historical reasons, the formal proof languages adopted by traditional theorem provers were not intended to represent natural language proofs.","Therefore, they are not well-suited for the aforementioned tasks and proof-checking work for educational purposes.","In this paper, we design a proof language and its corresponding abstract syntax tree and implement a proof checking tool for it.","This language can be easily converted from natural language, thus providing a rich corpus of formal proof.","Additionally, it supports the handling of issues in informal proofs through static analysis, and enhances the expressive power of the language by introducing the structure of partial proofs.","This design combines the expressiveness of natural language and the accuracy of formal language, resulting in an improved mathematical proof language."],"url":"http://arxiv.org/abs/2405.07973v1","category":"cs.PL"}
{"created":"2024-05-13 17:47:15","title":"How much entanglement is needed for emergent anyons and fermions?","abstract":"It is known that particles with exotic properties can emerge in systems made of simple constituents such as qubits, due to long-range quantum entanglement. In this paper, we provide quantitative characterizations of entanglement necessary for emergent anyons and fermions by using the geometric entanglement measure (GEM) which quantifies the maximal overlap between a given state and any short-range entangled states. For systems with emergent anyons, based on the braiding statistics, we show that the GEM scales linearly in the system size regardless of microscopic details. The phenomenon of emergent anyons can also be understood within the framework of quantum error correction (QEC). Specifically, we show that the GEM of any 2D stabilizer codes must be at least quadratic in the code distance. Our proof is based on a generic prescription for constructing string operators, establishing a rigorous and direct connection between emergent anyons and QEC. For systems with emergent fermions, despite that the ground state subspaces could be exponentially huge and their coding properties could be rather poor, we show that the GEM also scales linearly in the system size. Our results also establish an intriguing link between quantum anomaly and entanglement: a quantum state respecting anomalous $1$-form symmetries, be it pure or mixed, must be long-range entangled, offering a non-trivial class of intrinsically mixed state phases.","sentences":["It is known that particles with exotic properties can emerge in systems made of simple constituents such as qubits, due to long-range quantum entanglement.","In this paper, we provide quantitative characterizations of entanglement necessary for emergent anyons and fermions by using the geometric entanglement measure (GEM) which quantifies the maximal overlap between a given state and any short-range entangled states.","For systems with emergent anyons, based on the braiding statistics, we show that the GEM scales linearly in the system size regardless of microscopic details.","The phenomenon of emergent anyons can also be understood within the framework of quantum error correction (QEC).","Specifically, we show that the GEM of any 2D stabilizer codes must be at least quadratic in the code distance.","Our proof is based on a generic prescription for constructing string operators, establishing a rigorous and direct connection between emergent anyons and QEC.","For systems with emergent fermions, despite that the ground state subspaces could be exponentially huge and their coding properties could be rather poor, we show that the GEM also scales linearly in the system size.","Our results also establish an intriguing link between quantum anomaly and entanglement: a quantum state respecting anomalous $1$-form symmetries, be it pure or mixed, must be long-range entangled, offering a non-trivial class of intrinsically mixed state phases."],"url":"http://arxiv.org/abs/2405.07970v1","category":"quant-ph"}
{"created":"2024-05-13 17:47:08","title":"Investigating the Semantic Robustness of CLIP-based Zero-Shot Anomaly Segmentation","abstract":"Zero-shot anomaly segmentation using pre-trained foundation models is a promising approach that enables effective algorithms without expensive, domain-specific training or fine-tuning. Ensuring that these methods work across various environmental conditions and are robust to distribution shifts is an open problem. We investigate the performance of WinCLIP [14] zero-shot anomaly segmentation algorithm by perturbing test data using three semantic transformations: bounded angular rotations, bounded saturation shifts, and hue shifts. We empirically measure a lower performance bound by aggregating across per-sample worst-case perturbations and find that average performance drops by up to 20% in area under the ROC curve and 40% in area under the per-region overlap curve. We find that performance is consistently lowered on three CLIP backbones, regardless of model architecture or learning objective, demonstrating a need for careful performance evaluation.","sentences":["Zero-shot anomaly segmentation using pre-trained foundation models is a promising approach that enables effective algorithms without expensive, domain-specific training or fine-tuning.","Ensuring that these methods work across various environmental conditions and are robust to distribution shifts is an open problem.","We investigate the performance of WinCLIP [14] zero-shot anomaly segmentation algorithm by perturbing test data using three semantic transformations: bounded angular rotations, bounded saturation shifts, and hue shifts.","We empirically measure a lower performance bound by aggregating across per-sample worst-case perturbations and find that average performance drops by up to 20% in area under the ROC curve and 40% in area under the per-region overlap curve.","We find that performance is consistently lowered on three CLIP backbones, regardless of model architecture or learning objective, demonstrating a need for careful performance evaluation."],"url":"http://arxiv.org/abs/2405.07969v1","category":"cs.CV"}
{"created":"2024-05-13 17:46:52","title":"Partial Causal Detectability of Linear Descriptor Systems and Existence of Functional ODE Estimators","abstract":"This paper studies the problem of state estimation for linear time-invariant descriptor systems in their most general form. The estimator is a system of ordinary differential equations (ODEs). We introduce the notion of partial causal detectability and characterize this concept by means of a simple rank criterion involving the system coefficient matrices. Also, several equivalent characterizations for partial causal detectability are established. In addition, we prove that partial causal detectability is equivalent to the existence of functional ODE estimators. A numerical example is given to validate the theoretical results.","sentences":["This paper studies the problem of state estimation for linear time-invariant descriptor systems in their most general form.","The estimator is a system of ordinary differential equations (ODEs).","We introduce the notion of partial causal detectability and characterize this concept by means of a simple rank criterion involving the system coefficient matrices.","Also, several equivalent characterizations for partial causal detectability are established.","In addition, we prove that partial causal detectability is equivalent to the existence of functional ODE estimators.","A numerical example is given to validate the theoretical results."],"url":"http://arxiv.org/abs/2405.07968v1","category":"math.OC"}
{"created":"2024-05-13 17:46:38","title":"Massive gravity generalization of $T\\overline{T}$ deformations","abstract":"Motivated by the two-dimensional massive gravity description of $T\\overline{T}$ deformations, we propose a direct generalization in $d$ dimensions. Our methodology indicates that all terms up to order $d$ are present in the deformation. In two dimensions, $T\\overline{T}$ is enhanced by a linear and a constant term, and exhibits an interesting behaviour regarding the deformed spectrum and correlators. At certain limits, this deformation can reduce to $T\\overline{T}$ or $T\\overline{T}+\\Lambda_{2}$ consistently. Using the massive gravity method, we obtain the classically deformed action of a sigma model of bosons and fermions interacting with an arbitrary potential, extending previous results. As a consequence, a proposal regarding the deformation of higher-derivative theories is made. Moreover, a standard dimensional reduction procedure is presented, with the resulting operator matching the form of prior findings under certain assumptions. In $d\\geq2$, we provide the exact structure of the quadratic terms in agreement with previous approaches, as well as the structure of the linear and constant terms. All higher order contributions are not easily evaluated, yet we derive the complete answer for all cases up to seven dimensions. Under certain conditions, these terms vanish, resulting in a quadratic operator. The trace-flow equation for this family of deformations is also derived. Finally, we investigate the class of root-$T\\overline{T}$ operators in various dimensions within the scope of this formalism.","sentences":["Motivated by the two-dimensional massive gravity description of $T\\overline{T}$ deformations, we propose a direct generalization in $d$ dimensions.","Our methodology indicates that all terms up to order $d$ are present in the deformation.","In two dimensions, $T\\overline{T}$ is enhanced by a linear and a constant term, and exhibits an interesting behaviour regarding the deformed spectrum and correlators.","At certain limits, this deformation can reduce to $T\\overline{T}$ or $T\\overline{T}+\\Lambda_{2}$ consistently.","Using the massive gravity method, we obtain the classically deformed action of a sigma model of bosons and fermions interacting with an arbitrary potential, extending previous results.","As a consequence, a proposal regarding the deformation of higher-derivative theories is made.","Moreover, a standard dimensional reduction procedure is presented, with the resulting operator matching the form of prior findings under certain assumptions.","In $d\\geq2$, we provide the exact structure of the quadratic terms in agreement with previous approaches, as well as the structure of the linear and constant terms.","All higher order contributions are not easily evaluated, yet we derive the complete answer for all cases up to seven dimensions.","Under certain conditions, these terms vanish, resulting in a quadratic operator.","The trace-flow equation for this family of deformations is also derived.","Finally, we investigate the class of root-$T\\overline{T}$ operators in various dimensions within the scope of this formalism."],"url":"http://arxiv.org/abs/2405.07967v1","category":"hep-th"}
{"created":"2024-05-13 17:46:35","title":"OverlapMamba: Novel Shift State Space Model for LiDAR-based Place Recognition","abstract":"Place recognition is the foundation for enabling autonomous systems to achieve independent decision-making and safe operations. It is also crucial in tasks such as loop closure detection and global localization within SLAM. Previous methods utilize mundane point cloud representations as input and deep learning-based LiDAR-based Place Recognition (LPR) approaches employing different point cloud image inputs with convolutional neural networks (CNNs) or transformer architectures. However, the recently proposed Mamba deep learning model, combined with state space models (SSMs), holds great potential for long sequence modeling. Therefore, we developed OverlapMamba, a novel network for place recognition, which represents input range views (RVs) as sequences. In a novel way, we employ a stochastic reconstruction approach to build shift state space models, compressing the visual representation. Evaluated on three different public datasets, our method effectively detects loop closures, showing robustness even when traversing previously visited locations from different directions. Relying on raw range view inputs, it outperforms typical LiDAR and multi-view combination methods in time complexity and speed, indicating strong place recognition capabilities and real-time efficiency.","sentences":["Place recognition is the foundation for enabling autonomous systems to achieve independent decision-making and safe operations.","It is also crucial in tasks such as loop closure detection and global localization within SLAM.","Previous methods utilize mundane point cloud representations as input and deep learning-based LiDAR-based Place Recognition (LPR) approaches employing different point cloud image inputs with convolutional neural networks (CNNs) or transformer architectures.","However, the recently proposed Mamba deep learning model, combined with state space models (SSMs), holds great potential for long sequence modeling.","Therefore, we developed OverlapMamba, a novel network for place recognition, which represents input range views (RVs) as sequences.","In a novel way, we employ a stochastic reconstruction approach to build shift state space models, compressing the visual representation.","Evaluated on three different public datasets, our method effectively detects loop closures, showing robustness even when traversing previously visited locations from different directions.","Relying on raw range view inputs, it outperforms typical LiDAR and multi-view combination methods in time complexity and speed, indicating strong place recognition capabilities and real-time efficiency."],"url":"http://arxiv.org/abs/2405.07966v1","category":"cs.CV"}
{"created":"2024-05-13 17:44:05","title":"PyZoBot: A Platform for Conversational Information Extraction and Synthesis from Curated Zotero Reference Libraries through Advanced Retrieval-Augmented Generation","abstract":"The exponential growth of scientific literature has resulted in information overload, challenging researchers to effectively synthesize relevant publications. This paper explores the integration of traditional reference management software with advanced computational techniques, including Large Language Models and Retrieval-Augmented Generation. We introduce PyZoBot, an AI-driven platform developed in Python, incorporating Zoteros reference management with OpenAIs sophisticated LLMs. PyZoBot streamlines knowledge extraction and synthesis from extensive human-curated scientific literature databases. It demonstrates proficiency in handling complex natural language queries, integrating data from multiple sources, and meticulously presenting references to uphold research integrity and facilitate further exploration. By leveraging LLMs, RAG, and human expertise through a curated library, PyZoBot offers an effective solution to manage information overload and keep pace with rapid scientific advancements. The development of such AI-enhanced tools promises significant improvements in research efficiency and effectiveness across various disciplines.","sentences":["The exponential growth of scientific literature has resulted in information overload, challenging researchers to effectively synthesize relevant publications.","This paper explores the integration of traditional reference management software with advanced computational techniques, including Large Language Models and Retrieval-Augmented Generation.","We introduce PyZoBot, an AI-driven platform developed in Python, incorporating Zoteros reference management with OpenAIs sophisticated LLMs.","PyZoBot streamlines knowledge extraction and synthesis from extensive human-curated scientific literature databases.","It demonstrates proficiency in handling complex natural language queries, integrating data from multiple sources, and meticulously presenting references to uphold research integrity and facilitate further exploration.","By leveraging LLMs, RAG, and human expertise through a curated library, PyZoBot offers an effective solution to manage information overload and keep pace with rapid scientific advancements.","The development of such AI-enhanced tools promises significant improvements in research efficiency and effectiveness across various disciplines."],"url":"http://arxiv.org/abs/2405.07963v1","category":"cs.HC"}
{"created":"2024-05-13 17:43:05","title":"KG-Planner: Knowledge-Informed Graph Neural Planning for Collaborative Manipulators","abstract":"This paper presents a novel knowledge-informed graph neural planner (KG-Planner) to address the challenge of efficiently planning collision-free motions for robots in high-dimensional spaces, considering both static and dynamic environments involving humans. Unlike traditional motion planners that struggle with finding a balance between efficiency and optimality, the KG-Planner takes a different approach. Instead of relying solely on a neural network or imitating the motions of an oracle planner, our KG-Planner integrates explicit physical knowledge from the workspace. The integration of knowledge has two key aspects: (1) we present an approach to design a graph that can comprehensively model the workspace's compositional structure. The designed graph explicitly incorporates critical elements such as robot joints, obstacles, and their interconnections. This representation allows us to capture the intricate relationships between these elements. (2) We train a Graph Neural Network (GNN) that excels at generating nearly optimal robot motions. In particular, the GNN employs a layer-wise propagation rule to facilitate the exchange and update of information among workspace elements based on their connections. This propagation emphasizes the influence of these elements throughout the planning process. To validate the efficacy and efficiency of our KG-Planner, we conduct extensive experiments in both static and dynamic environments. These experiments include scenarios with and without human workers. The results of our approach are compared against existing methods, showcasing the superior performance of the KG-Planner. A short video introduction of this work is available (video link provided in the paper).","sentences":["This paper presents a novel knowledge-informed graph neural planner (KG-Planner) to address the challenge of efficiently planning collision-free motions for robots in high-dimensional spaces, considering both static and dynamic environments involving humans.","Unlike traditional motion planners that struggle with finding a balance between efficiency and optimality, the KG-Planner takes a different approach.","Instead of relying solely on a neural network or imitating the motions of an oracle planner, our KG-Planner integrates explicit physical knowledge from the workspace.","The integration of knowledge has two key aspects: (1) we present an approach to design a graph that can comprehensively model the workspace's compositional structure.","The designed graph explicitly incorporates critical elements such as robot joints, obstacles, and their interconnections.","This representation allows us to capture the intricate relationships between these elements.","(2) We train a Graph Neural Network (GNN) that excels at generating nearly optimal robot motions.","In particular, the GNN employs a layer-wise propagation rule to facilitate the exchange and update of information among workspace elements based on their connections.","This propagation emphasizes the influence of these elements throughout the planning process.","To validate the efficacy and efficiency of our KG-Planner, we conduct extensive experiments in both static and dynamic environments.","These experiments include scenarios with and without human workers.","The results of our approach are compared against existing methods, showcasing the superior performance of the KG-Planner.","A short video introduction of this work is available (video link provided in the paper)."],"url":"http://arxiv.org/abs/2405.07962v1","category":"cs.RO"}
{"created":"2024-05-13 17:38:53","title":"AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments","abstract":"Diagnosing and managing a patient is a complex, sequential decision making process that requires physicians to obtain information -- such as which tests to perform -- and to act upon it. Recent advances in artificial intelligence (AI) and large language models (LLMs) promise to profoundly impact clinical care. However, current evaluation schemes overrely on static medical question-answering benchmarks, falling short on interactive decision-making that is required in real-life clinical work. Here, we present AgentClinic: a multimodal benchmark to evaluate LLMs in their ability to operate as agents in simulated clinical environments. In our benchmark, the doctor agent must uncover the patient's diagnosis through dialogue and active data collection. We present two open benchmarks: a multimodal image and dialogue environment, AgentClinic-NEJM, and a dialogue-only environment, AgentClinic-MedQA. We embed cognitive and implicit biases both in patient and doctor agents to emulate realistic interactions between biased agents. We find that introducing bias leads to large reductions in diagnostic accuracy of the doctor agents, as well as reduced compliance, confidence, and follow-up consultation willingness in patient agents. Evaluating a suite of state-of-the-art LLMs, we find that several models that excel in benchmarks like MedQA are performing poorly in AgentClinic-MedQA. We find that the LLM used in the patient agent is an important factor for performance in the AgentClinic benchmark. We show that both having limited interactions as well as too many interaction reduces diagnostic accuracy in doctor agents. The code and data for this work is publicly available at https://AgentClinic.github.io.","sentences":["Diagnosing and managing a patient is a complex, sequential decision making process that requires physicians to obtain information -- such as which tests to perform -- and to act upon it.","Recent advances in artificial intelligence (AI) and large language models (LLMs) promise to profoundly impact clinical care.","However, current evaluation schemes overrely on static medical question-answering benchmarks, falling short on interactive decision-making that is required in real-life clinical work.","Here, we present AgentClinic: a multimodal benchmark to evaluate LLMs in their ability to operate as agents in simulated clinical environments.","In our benchmark, the doctor agent must uncover the patient's diagnosis through dialogue and active data collection.","We present two open benchmarks: a multimodal image and dialogue environment, AgentClinic-NEJM, and a dialogue-only environment, AgentClinic-MedQA.","We embed cognitive and implicit biases both in patient and doctor agents to emulate realistic interactions between biased agents.","We find that introducing bias leads to large reductions in diagnostic accuracy of the doctor agents, as well as reduced compliance, confidence, and follow-up consultation willingness in patient agents.","Evaluating a suite of state-of-the-art LLMs, we find that several models that excel in benchmarks like MedQA are performing poorly in AgentClinic-MedQA.","We find that the LLM used in the patient agent is an important factor for performance in the AgentClinic benchmark.","We show that both having limited interactions as well as too many interaction reduces diagnostic accuracy in doctor agents.","The code and data for this work is publicly available at https://AgentClinic.github.io."],"url":"http://arxiv.org/abs/2405.07960v1","category":"cs.HC"}
{"created":"2024-05-13 17:33:48","title":"Record-based transmuted unit omega distribution: different methods of estimation and applications","abstract":"Dombi et al. (2019) introduced a three parameter omega distribution and showed that its asymptotic distribution is the Weibull model. We propose a new record-based transmuted generalization of the unit omega distribution by considering Balakrishnan and He (2021) approach. We call it the RTUOMG distribution. We derive expressions for some statistical quantities, like, probability density function, distribution, hazard function, quantile function, moments, incomplete moments, inverted moments, moment generating function, Lorenz curve, and Bonferroni curve of the proposed distribution. The numerical values of various measures of central tendency and coefficient of skewness and kurtosis are also presented. Concepts of stochastic ordering and some results related to ordered statistics of the RTUOMG distribution are discussed. The parameters of the RTUOMG distribution are estimated using five distinct estimators. Additionally, the Monte Carlo simulations are performed to assess the performance of these estimators. Finally, two real data sets are analyzed to demonstrate the utility of the RTUOMG distribution.","sentences":["Dombi et al. (2019) introduced a three parameter omega distribution and showed that its asymptotic distribution is the Weibull model.","We propose a new record-based transmuted generalization of the unit omega distribution by considering Balakrishnan and He (2021) approach.","We call it the RTUOMG distribution.","We derive expressions for some statistical quantities, like, probability density function, distribution, hazard function, quantile function, moments, incomplete moments, inverted moments, moment generating function, Lorenz curve, and Bonferroni curve of the proposed distribution.","The numerical values of various measures of central tendency and coefficient of skewness and kurtosis are also presented.","Concepts of stochastic ordering and some results related to ordered statistics of the RTUOMG distribution are discussed.","The parameters of the RTUOMG distribution are estimated using five distinct estimators.","Additionally, the Monte Carlo simulations are performed to assess the performance of these estimators.","Finally, two real data sets are analyzed to demonstrate the utility of the RTUOMG distribution."],"url":"http://arxiv.org/abs/2405.07958v1","category":"math.ST"}
{"created":"2024-05-13 17:32:40","title":"Small-Signal Model for Inhomogeneous Helix Traveling-Wave Tubes using Transfer Matrices","abstract":"We introduce a practical method for modeling the small-signal behavior of dispersive and inhomogeneous helix-type traveling-wave tube (TWT) amplifiers based on a generalization of the one-dimensional Pierce model. Our model is applicable to both single-stage and multi-stage TWTs. Like the Pierce model, we assume that electrons flow linearly in one direction, parallel and in proximity to a slow-wave structure (SWS) which guides a single dominant electromagnetic mode. Realistic helix TWTs are modeled with position dependent and frequency-dependent SWS characteristics, such as loss, phase velocity, plasma frequency reduction factor, and interaction impedance. For the multi-stage helix TWT, we provide a simple lumped element circuit model for combining the stages separated by a sever, or gap, which attenuates the guided circuit mode while allowing the space-charge wave on the beam to pass freely to the next stage. The dispersive SWS characteristics are accounted for using full-wave eigenmode simulations for a realistic helix SWS supported by dielectric rods in a metal barrel, all of which contribute to the distributed circuit loss. We compare our computed gain vs frequency, computed using transfer matrices, to results found through particle-in-cell (PIC) simulations and the 1D TWT code LATTE to demonstrate the accuracy of our model. Furthermore, we demonstrate the ability of our model to reproduce gain ripple due to mismatches at the input and output ports of the TWT.","sentences":["We introduce a practical method for modeling the small-signal behavior of dispersive and inhomogeneous helix-type traveling-wave tube (TWT) amplifiers based on a generalization of the one-dimensional Pierce model.","Our model is applicable to both single-stage and multi-stage TWTs.","Like the Pierce model, we assume that electrons flow linearly in one direction, parallel and in proximity to a slow-wave structure (SWS) which guides a single dominant electromagnetic mode.","Realistic helix TWTs are modeled with position dependent and frequency-dependent SWS characteristics, such as loss, phase velocity, plasma frequency reduction factor, and interaction impedance.","For the multi-stage helix TWT, we provide a simple lumped element circuit model for combining the stages separated by a sever, or gap, which attenuates the guided circuit mode while allowing the space-charge wave on the beam to pass freely to the next stage.","The dispersive SWS characteristics are accounted for using full-wave eigenmode simulations for a realistic helix SWS supported by dielectric rods in a metal barrel, all of which contribute to the distributed circuit loss.","We compare our computed gain vs frequency, computed using transfer matrices, to results found through particle-in-cell (PIC) simulations and the 1D TWT code LATTE to demonstrate the accuracy of our model.","Furthermore, we demonstrate the ability of our model to reproduce gain ripple due to mismatches at the input and output ports of the TWT."],"url":"http://arxiv.org/abs/2405.07956v1","category":"physics.plasm-ph"}
{"created":"2024-05-13 17:28:54","title":"An Algorithmic Classification of Generalized Pseudo-Anosov Homeomorphisms via Geometric Markov Partitions","abstract":"This thesis provides a classification of generalized pseudo-Anosov homeomorphisms up to topological conjugacy using an algorithmic approach. A Markov partition of a generalized pseudo-Anosov homeomorphism is a decomposition of the surface into a finite number of rectangles with disjoint interiors, such that their images intersect with any other rectangle in the Markov partition along a finite number of horizontal sub-rectangles. Every generalized pseudo-Anosov homeomorphism has a Markov partition, and, by using the surface's orientation, we can endow any Markov partition with a geometrization. The geometric type of a geometric Markov partition was defined by Bonatti and Langevin in their book, \"Diffeomorphismes de Smale des surfaces\", to classify saddle-type basic pieces for structurally stable diffeomorphisms on surfaces. A geometric type is an abstract combinatorial object that generalizes the incidence matrix of a Markov partition. It takes into account not only the number of times the image of a rectangle intersects with any other rectangle in the family but also the order and change of orientation induced by the homeomorphisms. This thesis employs the geometric type of a geometric Markov partition to classify conjugacy classes of pseudo-Anosov homeomorphisms. The classification is provided by the three main results in this manuscript: I) The geometric type is a complete invariant of conjugation. II) A criterion is provided for determining whether an abstract geometric type is realized by a geometric Markov partition of a pseudo-Anosov homeomorphism. III) An algorithm is described for determining whether two geometric types in the pseudo-Anosov class are realized by generalized pseudo-Anosov homeomorphisms that are topologically conjugated or not.","sentences":["This thesis provides a classification of generalized pseudo-Anosov homeomorphisms up to topological conjugacy using an algorithmic approach.","A Markov partition of a generalized pseudo-Anosov homeomorphism is a decomposition of the surface into a finite number of rectangles with disjoint interiors, such that their images intersect with any other rectangle in the Markov partition along a finite number of horizontal sub-rectangles.","Every generalized pseudo-Anosov homeomorphism has a Markov partition, and, by using the surface's orientation, we can endow any Markov partition with a geometrization.","The geometric type of a geometric Markov partition was defined by Bonatti and Langevin in their book, \"Diffeomorphismes de Smale des surfaces\", to classify saddle-type basic pieces for structurally stable diffeomorphisms on surfaces.","A geometric type is an abstract combinatorial object that generalizes the incidence matrix of a Markov partition.","It takes into account not only the number of times the image of a rectangle intersects with any other rectangle in the family but also the order and change of orientation induced by the homeomorphisms.","This thesis employs the geometric type of a geometric Markov partition to classify conjugacy classes of pseudo-Anosov homeomorphisms.","The classification is provided by the three main results in this manuscript: I)","The geometric type is a complete invariant of conjugation.","II)","A criterion is provided for determining whether an abstract geometric type is realized by a geometric Markov partition of a pseudo-Anosov homeomorphism.","III)","An algorithm is described for determining whether two geometric types in the pseudo-Anosov class are realized by generalized pseudo-Anosov homeomorphisms that are topologically conjugated or not."],"url":"http://arxiv.org/abs/2405.07954v1","category":"math.DS"}
{"created":"2024-05-13 17:27:50","title":"White paper on ($\u03b1$, n) neutron yields calculation","abstract":"Understanding the radiogenic neutron production rate through the (${\\alpha}$, n) reaction is essential in many fields of physics like dark matter searches, neutrino studies, nuclear astrophysics and medical physics. This white paper provides a review of the current landscape of (${\\alpha}$, n) yields, neutron spectra and correlated ${\\gamma}$-rays calculations, and describes the existing tools and the available cross-sections. The uncertainties that contribute to (${\\alpha}$, n) yield calculations are also discussed with plans for a program to improve the accuracy of these estimates. Novel ideas to measure (${\\alpha}$, n) cross-sections for a variety of materials of interest are presented. The goal of this study is to reduce the uncertainty in the expected sensitivity of next-generation physics experiments in the keV--MeV regime.","sentences":["Understanding the radiogenic neutron production rate through the (${\\alpha}$, n) reaction is essential in many fields of physics like dark matter searches, neutrino studies, nuclear astrophysics and medical physics.","This white paper provides a review of the current landscape of (${\\alpha}$, n) yields, neutron spectra and correlated ${\\gamma}$-rays calculations, and describes the existing tools and the available cross-sections.","The uncertainties that contribute to (${\\alpha}$, n) yield calculations are also discussed with plans for a program to improve the accuracy of these estimates.","Novel ideas to measure (${\\alpha}$, n) cross-sections for a variety of materials of interest are presented.","The goal of this study is to reduce the uncertainty in the expected sensitivity of next-generation physics experiments in the keV--MeV regime."],"url":"http://arxiv.org/abs/2405.07952v1","category":"nucl-ex"}
{"created":"2024-05-13 17:18:08","title":"Hierarchical Decision Mamba","abstract":"Recent advancements in imitation learning have been largely fueled by the integration of sequence models, which provide a structured flow of information to effectively mimic task behaviours. Currently, Decision Transformer (DT) and subsequently, the Hierarchical Decision Transformer (HDT), presented Transformer-based approaches to learn task policies. Recently, the Mamba architecture has shown to outperform Transformers across various task domains. In this work, we introduce two novel methods, Decision Mamba (DM) and Hierarchical Decision Mamba (HDM), aimed at enhancing the performance of the Transformer models. Through extensive experimentation across diverse environments such as OpenAI Gym and D4RL, leveraging varying demonstration data sets, we demonstrate the superiority of Mamba models over their Transformer counterparts in a majority of tasks. Results show that HDM outperforms other methods in most settings. The code can be found at https://github.com/meowatthemoon/HierarchicalDecisionMamba.","sentences":["Recent advancements in imitation learning have been largely fueled by the integration of sequence models, which provide a structured flow of information to effectively mimic task behaviours.","Currently, Decision Transformer (DT) and subsequently, the Hierarchical Decision Transformer (HDT), presented Transformer-based approaches to learn task policies.","Recently, the Mamba architecture has shown to outperform Transformers across various task domains.","In this work, we introduce two novel methods, Decision Mamba (DM) and Hierarchical Decision Mamba (HDM), aimed at enhancing the performance of the Transformer models.","Through extensive experimentation across diverse environments such as OpenAI Gym and D4RL, leveraging varying demonstration data sets, we demonstrate the superiority of Mamba models over their Transformer counterparts in a majority of tasks.","Results show that HDM outperforms other methods in most settings.","The code can be found at https://github.com/meowatthemoon/HierarchicalDecisionMamba."],"url":"http://arxiv.org/abs/2405.07943v1","category":"cs.LG"}
{"created":"2024-05-13 17:15:57","title":"Spatiotemporal control of structure and dynamics in a polar active fluid","abstract":"We apply optimal control theory to a model of a polar active fluid (the Toner-Tu model), with the objective of driving the system into particular emergent dynamical behaviors or programming switching between states on demand. We use the effective self-propulsion speed as the control parameter (i.e. the means of external actuation). We identify control protocols that achieve outcomes such as relocating asters to targeted positions, forcing propagating solitary waves to reorient to a particular direction, and switching between stationary asters and propagating fronts. We analyze the solutions to identify generic principles for controlling polar active fluids. Our findings have implications for achieving spatiotemporal control of active polar systems in experiments, particularly in vitro cytoskeletal systems. Additionally, this research paves the way for leveraging optimal control methods to engineer the structure and dynamics of active fluids more broadly.","sentences":["We apply optimal control theory to a model of a polar active fluid (the Toner-Tu model), with the objective of driving the system into particular emergent dynamical behaviors or programming switching between states on demand.","We use the effective self-propulsion speed as the control parameter (i.e. the means of external actuation).","We identify control protocols that achieve outcomes such as relocating asters to targeted positions, forcing propagating solitary waves to reorient to a particular direction, and switching between stationary asters and propagating fronts.","We analyze the solutions to identify generic principles for controlling polar active fluids.","Our findings have implications for achieving spatiotemporal control of active polar systems in experiments, particularly in vitro cytoskeletal systems.","Additionally, this research paves the way for leveraging optimal control methods to engineer the structure and dynamics of active fluids more broadly."],"url":"http://arxiv.org/abs/2405.07942v2","category":"cond-mat.soft"}
{"created":"2024-05-13 17:15:38","title":"Efficient and Universal Merkle Tree Inclusion Proofs via OR Aggregation","abstract":"Zero-knowledge proofs have emerged as a powerful tool for enhancing privacy and security in blockchain applications. However, the efficiency and scalability of proof systems remain a significant challenge, particularly in the context of Merkle tree inclusion proofs. Traditional proof aggregation techniques based on AND logic suffer from high verification complexity and data communication overhead, limiting their practicality for large-scale applications. In this paper, we propose a novel proof aggregation approach based on OR logic, which enables the generation of compact and universally verifiable proofs for Merkle tree inclusion. By aggregating proofs using OR logic, we achieve a proof size that is independent of the number of leaves in the tree, and verification can be performed using any single valid leaf hash. This represents a significant improvement over AND aggregation, which requires the verifier to process all leaf hashes. We formally define the OR aggregation logic, describe the process of generating universal proofs, and provide a comparative analysis demonstrating the advantages of our approach in terms of proof size, verification data, and universality. Furthermore, we discuss the potential of combining OR and AND aggregation logics to create complex acceptance functions, enabling the development of expressive and efficient proof systems for various blockchain applications. The proposed techniques have the potential to significantly enhance the scalability, efficiency, and flexibility of zero-knowledge proof systems, paving the way for more practical and adaptive solutions in the blockchain ecosystem.","sentences":["Zero-knowledge proofs have emerged as a powerful tool for enhancing privacy and security in blockchain applications.","However, the efficiency and scalability of proof systems remain a significant challenge, particularly in the context of Merkle tree inclusion proofs.","Traditional proof aggregation techniques based on AND logic suffer from high verification complexity and data communication overhead, limiting their practicality for large-scale applications.","In this paper, we propose a novel proof aggregation approach based on OR logic, which enables the generation of compact and universally verifiable proofs for Merkle tree inclusion.","By aggregating proofs using OR logic, we achieve a proof size that is independent of the number of leaves in the tree, and verification can be performed using any single valid leaf hash.","This represents a significant improvement over AND aggregation, which requires the verifier to process all leaf hashes.","We formally define the OR aggregation logic, describe the process of generating universal proofs, and provide a comparative analysis demonstrating the advantages of our approach in terms of proof size, verification data, and universality.","Furthermore, we discuss the potential of combining OR and AND aggregation logics to create complex acceptance functions, enabling the development of expressive and efficient proof systems for various blockchain applications.","The proposed techniques have the potential to significantly enhance the scalability, efficiency, and flexibility of zero-knowledge proof systems, paving the way for more practical and adaptive solutions in the blockchain ecosystem."],"url":"http://arxiv.org/abs/2405.07941v1","category":"cs.CR"}
{"created":"2024-05-13 17:15:14","title":"RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors","abstract":"Many commercial and open-source models claim to detect machine-generated text with very high accuracy (99\\% or higher). However, very few of these detectors are evaluated on shared benchmark datasets and even when they are, the datasets used for evaluation are insufficiently challenging -- lacking variations in sampling strategy, adversarial attacks, and open-source generative models. In this work we present RAID: the largest and most challenging benchmark dataset for machine-generated text detection. RAID includes over 6 million generations spanning 11 models, 8 domains, 11 adversarial attacks and 4 decoding strategies. Using RAID, we evaluate the out-of-domain and adversarial robustness of 8 open- and 4 closed-source detectors and find that current detectors are easily fooled by adversarial attacks, variations in sampling strategies, repetition penalties, and unseen generative models. We release our dataset and tools to encourage further exploration into detector robustness.","sentences":["Many commercial and open-source models claim to detect machine-generated text with very high accuracy (99\\% or higher).","However, very few of these detectors are evaluated on shared benchmark datasets and even when they are, the datasets used for evaluation are insufficiently challenging -- lacking variations in sampling strategy, adversarial attacks, and open-source generative models.","In this work we present RAID: the largest and most challenging benchmark dataset for machine-generated text detection.","RAID includes over 6 million generations spanning 11 models, 8 domains, 11 adversarial attacks and 4 decoding strategies.","Using RAID, we evaluate the out-of-domain and adversarial robustness of 8 open- and 4 closed-source detectors and find that current detectors are easily fooled by adversarial attacks, variations in sampling strategies, repetition penalties, and unseen generative models.","We release our dataset and tools to encourage further exploration into detector robustness."],"url":"http://arxiv.org/abs/2405.07940v1","category":"cs.CL"}
{"created":"2024-05-13 17:13:32","title":"Active Learning with Simple Questions","abstract":"We consider an active learning setting where a learner is presented with a pool S of n unlabeled examples belonging to a domain X and asks queries to find the underlying labeling that agrees with a target concept h^* \\in H.   In contrast to traditional active learning that queries a single example for its label, we study more general region queries that allow the learner to pick a subset of the domain T \\subset X and a target label y and ask a labeler whether h^*(x) = y for every example in the set T \\cap S.   Such more powerful queries allow us to bypass the limitations of traditional active learning and use significantly fewer rounds of interactions to learn but can potentially lead to a significantly more complex query language. Our main contribution is quantifying the trade-off between the number of queries and the complexity of the query language used by the learner.   We measure the complexity of the region queries via the VC dimension of the family of regions. We show that given any hypothesis class H with VC dimension d, one can design a region query family Q with VC dimension O(d) such that for every set of n examples S \\subset X and every h^* \\in H, a learner can submit O(d log n) queries from Q to a labeler and perfectly label S. We show a matching lower bound by designing a hypothesis class H with VC dimension d and a dataset S \\subset X of size n such that any learning algorithm using any query class with VC dimension O(d) must make poly(n) queries to label S perfectly.   Finally, we focus on well-studied hypothesis classes including unions of intervals, high-dimensional boxes, and d-dimensional halfspaces, and obtain stronger results. In particular, we design learning algorithms that (i) are computationally efficient and (ii) work even when the queries are not answered based on the learner's pool of examples S but on some unknown superset L of S","sentences":["We consider an active learning setting where a learner is presented with a pool S of n unlabeled examples belonging to a domain X and asks queries to find the underlying labeling that agrees with a target concept h^*","\\in H.   In contrast to traditional active learning that queries a single example for its label, we study more general region queries that allow the learner to pick a subset of the domain T \\subset X and a target label y and ask a labeler whether h^*(x)","= y for every example in the set T \\cap S.   Such more powerful queries allow us to bypass the limitations of traditional active learning and use significantly fewer rounds of interactions to learn but can potentially lead to a significantly more complex query language.","Our main contribution is quantifying the trade-off between the number of queries and the complexity of the query language used by the learner.   ","We measure the complexity of the region queries via the VC dimension of the family of regions.","We show that given any hypothesis class H with VC dimension d, one can design a region query family Q with VC dimension O(d) such that for every set of n examples S \\subset X and every h^*","\\in H, a learner can submit O(d log n) queries from Q to a labeler and perfectly label S.","We show a matching lower bound by designing a hypothesis class H with VC dimension d and a dataset S \\subset X of size n such that any learning algorithm using any query class with VC dimension O(d) must make poly(n) queries to label S perfectly.   ","Finally, we focus on well-studied hypothesis classes including unions of intervals, high-dimensional boxes, and d-dimensional halfspaces, and obtain stronger results.","In particular, we design learning algorithms that (i) are computationally efficient and (ii) work even when the queries are not answered based on the learner's pool of examples S","but on some unknown superset L of S"],"url":"http://arxiv.org/abs/2405.07937v1","category":"cs.LG"}
{"created":"2024-05-13 17:09:24","title":"Mordell-Tornheim zeta functions and functional equations for Herglotz-Zagier type functions","abstract":"The Mordell-Tornheim zeta function and the Herglotz-Zagier function $F(x)$ are two important functions in Mathematics. By generalizing a special case of the former, namely $\\Theta(z, x)$, we show that the theories of these functions are inextricably woven. We obtain a three-term functional equation for $\\Theta(z, x)$ as well as decompose it in terms of the Herglotz-Hurwitz function $\\Phi(z, x)$. This decomposition can be conceived as a two-term functional equation for $\\Phi(z, x)$. Through this result, we are not only able to get Zagier's identity relating $F(x)$ with $F(1/x)$ but also two-term functional equation for Ishibashi's generalization of $F(x)$, namely, $\\Phi_k(x)$ which has been sought after for over twenty years. We further generalize $\\Theta(z, x)$ by incorporating two Gauss sums, each associated to a Dirichlet character, and decompose it in terms of an interesting integral which involves the Fekete polynomial as well as the character polylogarithm. This result gives infinite families of functional equations of Herglotz-type integrals out of which only two, due to Kumar and Choie, were known so far. The first one among the two involves the integral $J(x)$ who special values have received a lot of attention, more recently, in the work of Muzzaffar and Williams, and in that of Radchenko and Zagier. Analytic continuation of our generalization of $\\Theta(z, x)$ is also accomplished which allows us to obtain transformations between certain double series and Herglotz-type integrals or their explicit evaluations.","sentences":["The Mordell-Tornheim zeta function and the Herglotz-Zagier function $F(x)$ are two important functions in Mathematics.","By generalizing a special case of the former, namely $\\Theta(z, x)$, we show that the theories of these functions are inextricably woven.","We obtain a three-term functional equation for $\\Theta(z, x)$ as well as decompose it in terms of the Herglotz-Hurwitz function $\\Phi(z, x)$.","This decomposition can be conceived as a two-term functional equation for $\\Phi(z, x)$.","Through this result, we are not only able to get Zagier's identity relating $F(x)$ with $F(1/x)$ but also two-term functional equation for Ishibashi's generalization of $F(x)$, namely, $\\Phi_k(x)$ which has been sought after for over twenty years.","We further generalize $\\Theta(z, x)$ by incorporating two Gauss sums, each associated to a Dirichlet character, and decompose it in terms of an interesting integral which involves the Fekete polynomial as well as the character polylogarithm.","This result gives infinite families of functional equations of Herglotz-type integrals out of which only two, due to Kumar and Choie, were known so far.","The first one among the two involves the integral $J(x)$ who special values have received a lot of attention, more recently, in the work of Muzzaffar and Williams, and in that of Radchenko and Zagier.","Analytic continuation of our generalization of $\\Theta(z, x)$ is also accomplished which allows us to obtain transformations between certain double series and Herglotz-type integrals or their explicit evaluations."],"url":"http://arxiv.org/abs/2405.07934v1","category":"math.NT"}
{"created":"2024-05-13 17:08:42","title":"PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition","abstract":"Large language models (LLMs) have shown success in many natural language processing tasks. Despite rigorous safety alignment processes, supposedly safety-aligned LLMs like Llama 2 and Claude 2 are still susceptible to jailbreaks, leading to security risks and abuse of the models. One option to mitigate such risks is to augment the LLM with a dedicated \"safeguard\", which checks the LLM's inputs or outputs for undesired behaviour. A promising approach is to use the LLM itself as the safeguard. Nonetheless, baseline methods, such as prompting the LLM to self-classify toxic content, demonstrate limited efficacy. We hypothesise that this is due to domain shift: the alignment training imparts a self-censoring behaviour to the model (\"Sorry I can't do that\"), while the self-classify approach shifts it to a classification format (\"Is this prompt malicious\"). In this work, we propose PARDEN, which avoids this domain shift by simply asking the model to repeat its own outputs. PARDEN neither requires finetuning nor white box access to the model. We empirically verify the effectiveness of our method and show that PARDEN significantly outperforms existing jailbreak detection baselines for Llama-2 and Claude-2. Code and data are available at https://github.com/Ed-Zh/PARDEN.   We find that PARDEN is particularly powerful in the relevant regime of high True Positive Rate (TPR) and low False Positive Rate (FPR). For instance, for Llama2-7B, at TPR equal to 90%, PARDEN accomplishes a roughly 11x reduction in the FPR from 24.8% to 2.0% on the harmful behaviours dataset.","sentences":["Large language models (LLMs) have shown success in many natural language processing tasks.","Despite rigorous safety alignment processes, supposedly safety-aligned LLMs like Llama 2 and Claude 2 are still susceptible to jailbreaks, leading to security risks and abuse of the models.","One option to mitigate such risks is to augment the LLM with a dedicated \"safeguard\", which checks the LLM's inputs or outputs for undesired behaviour.","A promising approach is to use the LLM itself as the safeguard.","Nonetheless, baseline methods, such as prompting the LLM to self-classify toxic content, demonstrate limited efficacy.","We hypothesise that this is due to domain shift: the alignment training imparts a self-censoring behaviour to the model (\"Sorry I can't do that\"), while the self-classify approach shifts it to a classification format (\"Is this prompt malicious\").","In this work, we propose PARDEN, which avoids this domain shift by simply asking the model to repeat its own outputs.","PARDEN neither requires finetuning nor white box access to the model.","We empirically verify the effectiveness of our method and show that PARDEN significantly outperforms existing jailbreak detection baselines for Llama-2 and Claude-2.","Code and data are available at https://github.com/Ed-Zh/PARDEN.   ","We find that PARDEN is particularly powerful in the relevant regime of high True Positive Rate (TPR) and low False Positive Rate (FPR).","For instance, for Llama2-7B, at TPR equal to 90%, PARDEN accomplishes a roughly 11x reduction in the FPR from 24.8% to 2.0% on the harmful behaviours dataset."],"url":"http://arxiv.org/abs/2405.07932v1","category":"cs.CL"}
{"created":"2024-05-13 16:57:48","title":"Stable Diffusion-based Data Augmentation for Federated Learning with Non-IID Data","abstract":"The proliferation of edge devices has brought Federated Learning (FL) to the forefront as a promising paradigm for decentralized and collaborative model training while preserving the privacy of clients' data. However, FL struggles with a significant performance reduction and poor convergence when confronted with Non-Independent and Identically Distributed (Non-IID) data distributions among participating clients. While previous efforts, such as client drift mitigation and advanced server-side model fusion techniques, have shown some success in addressing this challenge, they often overlook the root cause of the performance reduction - the absence of identical data accurately mirroring the global data distribution among clients. In this paper, we introduce Gen-FedSD, a novel approach that harnesses the powerful capability of state-of-the-art text-to-image foundation models to bridge the significant Non-IID performance gaps in FL. In Gen-FedSD, each client constructs textual prompts for each class label and leverages an off-the-shelf state-of-the-art pre-trained Stable Diffusion model to synthesize high-quality data samples. The generated synthetic data is tailored to each client's unique local data gaps and distribution disparities, effectively making the final augmented local data IID. Through extensive experimentation, we demonstrate that Gen-FedSD achieves state-of-the-art performance and significant communication cost savings across various datasets and Non-IID settings.","sentences":["The proliferation of edge devices has brought Federated Learning (FL) to the forefront as a promising paradigm for decentralized and collaborative model training while preserving the privacy of clients' data.","However, FL struggles with a significant performance reduction and poor convergence when confronted with Non-Independent and Identically Distributed (Non-IID) data distributions among participating clients.","While previous efforts, such as client drift mitigation and advanced server-side model fusion techniques, have shown some success in addressing this challenge, they often overlook the root cause of the performance reduction - the absence of identical data accurately mirroring the global data distribution among clients.","In this paper, we introduce Gen-FedSD, a novel approach that harnesses the powerful capability of state-of-the-art text-to-image foundation models to bridge the significant Non-IID performance gaps in FL.","In Gen-FedSD, each client constructs textual prompts for each class label and leverages an off-the-shelf state-of-the-art pre-trained Stable Diffusion model to synthesize high-quality data samples.","The generated synthetic data is tailored to each client's unique local data gaps and distribution disparities, effectively making the final augmented local data IID.","Through extensive experimentation, we demonstrate that Gen-FedSD achieves state-of-the-art performance and significant communication cost savings across various datasets and Non-IID settings."],"url":"http://arxiv.org/abs/2405.07925v1","category":"cs.LG"}
{"created":"2024-05-13 16:56:18","title":"Extreme points of matrix convex sets and their spanning properties","abstract":"This expository article gives a survey of matrix convex sets, a natural generalization of convex sets to the noncommutative (dimension-free) setting, with a focus on their extreme points. Mirroring the classical setting, extreme points play an important role in understanding matrix convex sets, and a natural question is, \"are matrix convex sets the matrix convex hull of their extreme points?\" That is, does a Krein-Milman theorem hold for matrix convex sets? This question requires more care in the noncommutative setting, as there are several notions of extreme points for matrix convex sets. Three of the most important notions are matrix extreme points, matrix exposed points, and free extreme points. For each of these types of extreme points, we examine strengths and shortcomings in terms of a Krein-Milman theorem for matrix convex sets.","sentences":["This expository article gives a survey of matrix convex sets, a natural generalization of convex sets to the noncommutative (dimension-free) setting, with a focus on their extreme points.","Mirroring the classical setting, extreme points play an important role in understanding matrix convex sets, and a natural question is, \"are matrix convex sets the matrix convex hull of their extreme points?\"","That is, does a Krein-Milman theorem hold for matrix convex sets?","This question requires more care in the noncommutative setting, as there are several notions of extreme points for matrix convex sets.","Three of the most important notions are matrix extreme points, matrix exposed points, and free extreme points.","For each of these types of extreme points, we examine strengths and shortcomings in terms of a Krein-Milman theorem for matrix convex sets."],"url":"http://arxiv.org/abs/2405.07924v1","category":"math.FA"}
{"created":"2024-05-13 16:52:17","title":"Can Better Text Semantics in Prompt Tuning Improve VLM Generalization?","abstract":"Going beyond mere fine-tuning of vision-language models (VLMs), learnable prompt tuning has emerged as a promising, resource-efficient alternative. Despite their potential, effectively learning prompts faces the following challenges: (i) training in a low-shot scenario results in overfitting, limiting adaptability and yielding weaker performance on newer classes or datasets; (ii) prompt-tuning's efficacy heavily relies on the label space, with decreased performance in large class spaces, signaling potential gaps in bridging image and class concepts. In this work, we ask the question if better text semantics can help address these concerns. In particular, we introduce a prompt-tuning method that leverages class descriptions obtained from large language models (LLMs). Our approach constructs part-level description-guided views of both image and text features, which are subsequently aligned to learn more generalizable prompts. Our comprehensive experiments, conducted across 11 benchmark datasets, outperform established methods, demonstrating substantial improvements.","sentences":["Going beyond mere fine-tuning of vision-language models (VLMs), learnable prompt tuning has emerged as a promising, resource-efficient alternative.","Despite their potential, effectively learning prompts faces the following challenges: (i) training in a low-shot scenario results in overfitting, limiting adaptability and yielding weaker performance on newer classes or datasets; (ii) prompt-tuning's efficacy heavily relies on the label space, with decreased performance in large class spaces, signaling potential gaps in bridging image and class concepts.","In this work, we ask the question if better text semantics can help address these concerns.","In particular, we introduce a prompt-tuning method that leverages class descriptions obtained from large language models (LLMs).","Our approach constructs part-level description-guided views of both image and text features, which are subsequently aligned to learn more generalizable prompts.","Our comprehensive experiments, conducted across 11 benchmark datasets, outperform established methods, demonstrating substantial improvements."],"url":"http://arxiv.org/abs/2405.07921v1","category":"cs.CV"}
{"created":"2024-05-13 16:47:53","title":"IMAFD: An Interpretable Multi-stage Approach to Flood Detection from time series Multispectral Data","abstract":"In this paper, we address two critical challenges in the domain of flood detection: the computational expense of large-scale time series change detection and the lack of interpretable decision-making processes on explainable AI (XAI). To overcome these challenges, we proposed an interpretable multi-stage approach to flood detection, IMAFD has been proposed. It provides an automatic, efficient and interpretable solution suitable for large-scale remote sensing tasks and offers insight into the decision-making process. The proposed IMAFD approach combines the analysis of the dynamic time series image sequences to identify images with possible flooding with the static, within-image semantic segmentation. It combines anomaly detection (at both image and pixel level) with semantic segmentation. The flood detection problem is addressed through four stages: (1) at a sequence level: identifying the suspected images (2) at a multi-image level: detecting change within suspected images (3) at an image level: semantic segmentation of images into Land, Water or Cloud class (4) decision making. Our contributions are two folder. First, we efficiently reduced the number of frames to be processed for dense change detection by providing a multi-stage holistic approach to flood detection. Second, the proposed semantic change detection method (stage 3) provides human users with an interpretable decision-making process, while most of the explainable AI (XAI) methods provide post hoc explanations. The evaluation of the proposed IMAFD framework was performed on three datasets, WorldFloods, RavAEn and MediaEval. For all the above datasets, the proposed framework demonstrates a competitive performance compared to other methods offering also interpretability and insight.","sentences":["In this paper, we address two critical challenges in the domain of flood detection: the computational expense of large-scale time series change detection and the lack of interpretable decision-making processes on explainable AI (XAI).","To overcome these challenges, we proposed an interpretable multi-stage approach to flood detection, IMAFD has been proposed.","It provides an automatic, efficient and interpretable solution suitable for large-scale remote sensing tasks and offers insight into the decision-making process.","The proposed IMAFD approach combines the analysis of the dynamic time series image sequences to identify images with possible flooding with the static, within-image semantic segmentation.","It combines anomaly detection (at both image and pixel level) with semantic segmentation.","The flood detection problem is addressed through four stages: (1) at a sequence level: identifying the suspected images (2) at a multi-image level: detecting change within suspected images (3) at an image level: semantic segmentation of images into Land, Water or Cloud class (4) decision making.","Our contributions are two folder.","First, we efficiently reduced the number of frames to be processed for dense change detection by providing a multi-stage holistic approach to flood detection.","Second, the proposed semantic change detection method (stage 3) provides human users with an interpretable decision-making process, while most of the explainable AI (XAI) methods provide post hoc explanations.","The evaluation of the proposed IMAFD framework was performed on three datasets, WorldFloods, RavAEn and MediaEval.","For all the above datasets, the proposed framework demonstrates a competitive performance compared to other methods offering also interpretability and insight."],"url":"http://arxiv.org/abs/2405.07916v1","category":"cs.CV"}
{"created":"2024-05-13 16:47:06","title":"Discovery of highly anisotropic dielectric crystals with equivariant graph neural networks","abstract":"Anisotropy in crystals plays a pivotal role in many technological applications. For example, anisotropic electronic and thermal transport are thought to be beneficial for thermoelectric applications, while anisotropic mechanical properties are of interest for emerging metamaterials, and anisotropic dielectric materials have been suggested as a novel platform for dark matter detection. Understanding and tailoring anisotropy in crystals is therefore essential for the design of next-generation functional materials. To date, however, most data-driven approaches have focused on the prediction of scalar crystal properties, such as the spherically averaged dielectric tensor or the bulk and shear elastic moduli. Here, we adopt the latest approaches in equivariant graph neural networks to develop a model that can predict the full dielectric tensor of crystals. Our model, trained on the Materials Project dataset of c.a. 6,700 dielectric tensors, achieves state-of-the-art accuracy in scalar dielectric prediction in addition to capturing the directional response. We showcase the performance of the model by discovering crystals with almost isotropic connectivity but highly anisotropic dielectric tensors, thereby broadening our knowledge of the structure-property relationships in dielectric crystals.","sentences":["Anisotropy in crystals plays a pivotal role in many technological applications.","For example, anisotropic electronic and thermal transport are thought to be beneficial for thermoelectric applications, while anisotropic mechanical properties are of interest for emerging metamaterials, and anisotropic dielectric materials have been suggested as a novel platform for dark matter detection.","Understanding and tailoring anisotropy in crystals is therefore essential for the design of next-generation functional materials.","To date, however, most data-driven approaches have focused on the prediction of scalar crystal properties, such as the spherically averaged dielectric tensor or the bulk and shear elastic moduli.","Here, we adopt the latest approaches in equivariant graph neural networks to develop a model that can predict the full dielectric tensor of crystals.","Our model, trained on the Materials Project dataset of c.a. 6,700 dielectric tensors, achieves state-of-the-art accuracy in scalar dielectric prediction in addition to capturing the directional response.","We showcase the performance of the model by discovering crystals with almost isotropic connectivity but highly anisotropic dielectric tensors, thereby broadening our knowledge of the structure-property relationships in dielectric crystals."],"url":"http://arxiv.org/abs/2405.07915v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 16:46:44","title":"CTRLorALTer: Conditional LoRAdapter for Efficient 0-Shot Control & Altering of T2I Models","abstract":"Text-to-image generative models have become a prominent and powerful tool that excels at generating high-resolution realistic images. However, guiding the generative process of these models to consider detailed forms of conditioning reflecting style and/or structure information remains an open problem. In this paper, we present LoRAdapter, an approach that unifies both style and structure conditioning under the same formulation using a novel conditional LoRA block that enables zero-shot control. LoRAdapter is an efficient, powerful, and architecture-agnostic approach to condition text-to-image diffusion models, which enables fine-grained control conditioning during generation and outperforms recent state-of-the-art approaches","sentences":["Text-to-image generative models have become a prominent and powerful tool that excels at generating high-resolution realistic images.","However, guiding the generative process of these models to consider detailed forms of conditioning reflecting style and/or structure information remains an open problem.","In this paper, we present LoRAdapter, an approach that unifies both style and structure conditioning under the same formulation using a novel conditional LoRA block that enables zero-shot control.","LoRAdapter is an efficient, powerful, and architecture-agnostic approach to condition text-to-image diffusion models, which enables fine-grained control conditioning during generation and outperforms recent state-of-the-art approaches"],"url":"http://arxiv.org/abs/2405.07913v1","category":"cs.CV"}
{"created":"2024-05-13 16:42:19","title":"Gain-induced group delay in spontaneous parametric down-conversion","abstract":"Strongly-driven nonlinear optical processes such as spontaneous parametric down-conversion and spontaneous four-wave mixing can produce multiphoton nonclassical beams of light which have applications in quantum information processing and sensing. In contrast to the low-gain regime, new physical effects arise in a high-gain regime due to the interactions between the nonclassical light and the strong pump driving the nonlinear process. Here, we describe and experimentally observe a gain-induced group delay between the multiphoton pulses generated in a high-gain type-II spontaneous parametric down-conversion source. Since the group delay introduces distinguishability between the generated photons, it will be important to compensate for it when designing quantum interference devices in which strong optical nonlinearities are required.","sentences":["Strongly-driven nonlinear optical processes such as spontaneous parametric down-conversion and spontaneous four-wave mixing can produce multiphoton nonclassical beams of light which have applications in quantum information processing and sensing.","In contrast to the low-gain regime, new physical effects arise in a high-gain regime due to the interactions between the nonclassical light and the strong pump driving the nonlinear process.","Here, we describe and experimentally observe a gain-induced group delay between the multiphoton pulses generated in a high-gain type-II spontaneous parametric down-conversion source.","Since the group delay introduces distinguishability between the generated photons, it will be important to compensate for it when designing quantum interference devices in which strong optical nonlinearities are required."],"url":"http://arxiv.org/abs/2405.07909v1","category":"quant-ph"}
{"created":"2024-05-13 16:41:47","title":"Collaborative Planar Pushing of Polytopic Objects with Multiple Robots in Complex Scenes","abstract":"Pushing is a simple yet effective skill for robots to interact with and further change the environment. Related work has been mostly focused on utilizing it as a non-prehensile manipulation primitive for a robotic manipulator. However, it can also be beneficial for low-cost mobile robots that are not equipped with a manipulator. This work tackles the general problem of controlling a team of mobile robots to push collaboratively polytopic objects within complex obstacle-cluttered environments. It incorporates several characteristic challenges for contact-rich tasks such as the hybrid switching among different contact modes and under-actuation due to constrained contact forces. The proposed method is based on hybrid optimization over a sequence of possible modes and the associated pushing forces, where (i) a set of sufficient modes is generated with a multi-directional feasibility estimation, based on quasi-static analyses for general objects and any number of robots; (ii) a hierarchical hybrid search algorithm is designed to iteratively decompose the navigation path via arch segments and select the optimal parameterized mode; and (iii) a nonlinear model predictive controller is proposed to track the desired pushing velocities adaptively online for each robot. The proposed framework is complete under mild assumptions. Its efficiency and effectiveness are validated in high-fidelity simulations and hardware experiments. Robustness to motion and actuation uncertainties is also demonstrated.","sentences":["Pushing is a simple yet effective skill for robots to interact with and further change the environment.","Related work has been mostly focused on utilizing it as a non-prehensile manipulation primitive for a robotic manipulator.","However, it can also be beneficial for low-cost mobile robots that are not equipped with a manipulator.","This work tackles the general problem of controlling a team of mobile robots to push collaboratively polytopic objects within complex obstacle-cluttered environments.","It incorporates several characteristic challenges for contact-rich tasks such as the hybrid switching among different contact modes and under-actuation due to constrained contact forces.","The proposed method is based on hybrid optimization over a sequence of possible modes and the associated pushing forces, where (i) a set of sufficient modes is generated with a multi-directional feasibility estimation, based on quasi-static analyses for general objects and any number of robots; (ii) a hierarchical hybrid search algorithm is designed to iteratively decompose the navigation path via arch segments and select the optimal parameterized mode; and (iii) a nonlinear model predictive controller is proposed to track the desired pushing velocities adaptively online for each robot.","The proposed framework is complete under mild assumptions.","Its efficiency and effectiveness are validated in high-fidelity simulations and hardware experiments.","Robustness to motion and actuation uncertainties is also demonstrated."],"url":"http://arxiv.org/abs/2405.07908v1","category":"cs.RO"}
{"created":"2024-05-13 16:38:49","title":"Investigating the impact of galaxies' compact binary hosting probability for gravitational-wave cosmology","abstract":"With the advent of future-generation interferometers a huge number of Gravitational Wave (GW) signals is expected to be measured without an electromagnetic counterpart. Although these signals do not allow a simultaneous measurement of the redshift and the luminosity distance, it is still possible to infer cosmological parameters. In this paper, we focus on the systematic biases that could arise from mismodeling the GW host probability when inferring the Hubble constant ($H_0$) with GW dark sirens jointly with galaxy catalogues. We discuss the case in which the GW host probability is a function of galaxies' luminosity and redshift as it has been predicted by state-of-the-art compact binary coalescences (CBCs) synthetic catalogues. We show that, in the limiting case in which the analysis is done with a complete galaxy catalog covering a footprint of $\\sim 10~\\rm {deg}^2$, mismatching the host probability in terms of galaxy's luminosity will introduce a bias on $H_0$. In this case, the magnitude of the bias will depend on the distribution of the Large-Scale Structure over the line-of-sight. Instead, in the limit of a complete wide-field of view galaxy catalog and GW events localized at O$({\\rm Gpc})$ distance, mismatching the redshift dependence of the GW hosting probability is more likely to introduce a systematic bias.","sentences":["With the advent of future-generation interferometers a huge number of Gravitational Wave (GW) signals is expected to be measured without an electromagnetic counterpart.","Although these signals do not allow a simultaneous measurement of the redshift and the luminosity distance, it is still possible to infer cosmological parameters.","In this paper, we focus on the systematic biases that could arise from mismodeling the GW host probability when inferring the Hubble constant ($H_0$) with GW dark sirens jointly with galaxy catalogues.","We discuss the case in which the GW host probability is a function of galaxies' luminosity and redshift as it has been predicted by state-of-the-art compact binary coalescences (CBCs) synthetic catalogues.","We show that, in the limiting case in which the analysis is done with a complete galaxy catalog covering a footprint of $\\sim 10~\\rm {deg}^2$, mismatching the host probability in terms of galaxy's luminosity will introduce a bias on $H_0$. In this case, the magnitude of the bias will depend on the distribution of the Large-Scale Structure over the line-of-sight.","Instead, in the limit of a complete wide-field of view galaxy catalog and GW events localized at O$({\\rm Gpc})$ distance, mismatching the redshift dependence of the GW hosting probability is more likely to introduce a systematic bias."],"url":"http://arxiv.org/abs/2405.07904v1","category":"astro-ph.CO"}
{"created":"2024-05-13 16:38:20","title":"A complete framework for cosmological emulation and inference with CosmoPower","abstract":"We present a coherent, re-usable python framework which further builds on the cosmological emulator code CosmoPower. In the current era of high-precision cosmology, we require high-accuracy calculations of cosmological observables with Einstein-Boltzmann codes. For detailed statistical analyses, such codes often incur high costs in terms of computing power, making parameter space exploration costly, especially for beyond-$\\Lambda$CDM analyses. Machine learning-enabled emulators of Einstein-Boltzmann codes have emerged as a solution to this problem and have become a common way to perform fast cosmological analyses. To enable generation, sharing and use of emulators for inference, we define standards for robustly describing, packaging and distributing them, and present software for easily performing these tasks in an automated and replicable manner. We provide examples and guidelines for generating your own sufficiently accurate emulators and wrappers for using them in popular cosmological inference codes. We demonstrate our framework by presenting a suite of high-accuracy emulators for the CAMB code's calculations of CMB $C_\\ell$, $P(k)$, background evolution, and derived parameter quantities. We show that these emulators are accurate enough for both $\\Lambda$CDM analysis and a set of single- and two-parameter extension models (including $N_{\\rm eff}$, $\\sum m_{\\nu}$ and $w_0 w_a$ cosmologies) with stage-IV observatories, recovering the original high-accuracy Einstein-Boltzmann spectra to tolerances well within the cosmic variance uncertainties across the full range of parameters considered. We also use our emulators to recover cosmological parameters in a simulated cosmic-variance limited experiment, finding results well within $0.1 \\sigma$ of the input cosmology, while requiring typically $\\lesssim1/50$ of the evaluation time than for the full Einstein-Boltzmann computation.","sentences":["We present a coherent, re-usable python framework which further builds on the cosmological emulator code CosmoPower.","In the current era of high-precision cosmology, we require high-accuracy calculations of cosmological observables with Einstein-Boltzmann codes.","For detailed statistical analyses, such codes often incur high costs in terms of computing power, making parameter space exploration costly, especially for beyond-$\\Lambda$CDM analyses.","Machine learning-enabled emulators of Einstein-Boltzmann codes have emerged as a solution to this problem and have become a common way to perform fast cosmological analyses.","To enable generation, sharing and use of emulators for inference, we define standards for robustly describing, packaging and distributing them, and present software for easily performing these tasks in an automated and replicable manner.","We provide examples and guidelines for generating your own sufficiently accurate emulators and wrappers for using them in popular cosmological inference codes.","We demonstrate our framework by presenting a suite of high-accuracy emulators for the CAMB code's calculations of CMB $C_\\ell$, $P(k)$, background evolution, and derived parameter quantities.","We show that these emulators are accurate enough for both $\\Lambda$CDM analysis and a set of single- and two-parameter extension models (including $N_{\\rm eff}$, $\\sum m_{\\nu}$ and $w_0 w_a$ cosmologies) with stage-IV observatories, recovering the original high-accuracy Einstein-Boltzmann spectra to tolerances well within the cosmic variance uncertainties across the full range of parameters considered.","We also use our emulators to recover cosmological parameters in a simulated cosmic-variance limited experiment, finding results well within $0.1 \\sigma$ of the input cosmology, while requiring typically $\\lesssim1/50$ of the evaluation time than for the full Einstein-Boltzmann computation."],"url":"http://arxiv.org/abs/2405.07903v1","category":"astro-ph.CO"}
{"created":"2024-05-13 16:37:23","title":"Physically Consistent Online Inertial Adaptation for Humanoid Loco-manipulation","abstract":"The ability to accomplish manipulation and locomotion tasks in the presence of significant time-varying external loads is a remarkable skill of humans that has yet to be replicated convincingly by humanoid robots. Such an ability will be a key requirement in the environments we envision deploying our robots: dull, dirty, and dangerous. External loads constitute a large model bias, which is typically unaccounted for. In this work, we enable our humanoid robot to engage in loco-manipulation tasks in the presence of significant model bias due to external loads. We propose an online estimation and control framework involving the combination of a physically consistent extended Kalman filter for inertial parameter estimation coupled to a whole-body controller. We showcase our results both in simulation and in hardware, where weights are mounted on Nadia's wrist links as a proxy for engaging in tasks where large external loads are applied to the robot.","sentences":["The ability to accomplish manipulation and locomotion tasks in the presence of significant time-varying external loads is a remarkable skill of humans that has yet to be replicated convincingly by humanoid robots.","Such an ability will be a key requirement in the environments we envision deploying our robots: dull, dirty, and dangerous.","External loads constitute a large model bias, which is typically unaccounted for.","In this work, we enable our humanoid robot to engage in loco-manipulation tasks in the presence of significant model bias due to external loads.","We propose an online estimation and control framework involving the combination of a physically consistent extended Kalman filter for inertial parameter estimation coupled to a whole-body controller.","We showcase our results both in simulation and in hardware, where weights are mounted on Nadia's wrist links as a proxy for engaging in tasks where large external loads are applied to the robot."],"url":"http://arxiv.org/abs/2405.07901v1","category":"cs.RO"}
{"created":"2024-05-13 16:35:41","title":"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System","abstract":"Molecular dynamics (MD) simulations have transformed our understanding of the nanoscale, driving breakthroughs in materials science, computational chemistry, and several other fields, including biophysics and drug design. Even on exascale supercomputers, however, runtimes are excessive for systems and timescales of scientific interest. Here, we demonstrate strong scaling of MD simulations on the Cerebras Wafer-Scale Engine. By dedicating a processor core for each simulated atom, we demonstrate a 179-fold improvement in timesteps per second versus the Frontier GPU-based Exascale platform, along with a large improvement in timesteps per unit energy. Reducing every year of runtime to two days unlocks currently inaccessible timescales of slow microstructure transformation processes that are critical for understanding material behavior and function. Our dataflow algorithm runs Embedded Atom Method (EAM) simulations at rates over 270,000 timesteps per second for problems with up to 800k atoms. This demonstrated performance is unprecedented for general-purpose processing cores.","sentences":["Molecular dynamics (MD) simulations have transformed our understanding of the nanoscale, driving breakthroughs in materials science, computational chemistry, and several other fields, including biophysics and drug design.","Even on exascale supercomputers, however, runtimes are excessive for systems and timescales of scientific interest.","Here, we demonstrate strong scaling of MD simulations on the Cerebras Wafer-Scale Engine.","By dedicating a processor core for each simulated atom, we demonstrate a 179-fold improvement in timesteps per second versus the Frontier GPU-based Exascale platform, along with a large improvement in timesteps per unit energy.","Reducing every year of runtime to two days unlocks currently inaccessible timescales of slow microstructure transformation processes that are critical for understanding material behavior and function.","Our dataflow algorithm runs Embedded Atom Method (EAM) simulations at rates over 270,000 timesteps per second for problems with up to 800k atoms.","This demonstrated performance is unprecedented for general-purpose processing cores."],"url":"http://arxiv.org/abs/2405.07898v1","category":"physics.comp-ph"}
{"created":"2024-05-13 16:34:35","title":"Relativistic Binary Precession: Impact on Eccentric Binary Accretion and Multi-Messenger Astronomy","abstract":"Recent hydrodynamical simulations have shown that circumbinary gas disks drive the orbits of binary black holes to become eccentric, even when general relativistic corrections to the orbit are significant. Here, we study the general relativistic (GR) apsidal precession of eccentric equal-mass binary black holes in circumbinary disks (CBDs) via two-dimensional hydrodynamical simulations. We perform a suite of simulations comparing precessing and non-precessing binaries across a range of eccentricities, semi-major axes, and precession rates. We find that the GR precession of the binary's semi-major axis can introduce a dominant modulation in the binary's accretion rate and the corresponding high-energy electromagnetic light-curves. We discuss the conditions under which this occurs and its detailed characteristics and mechanism. Finally, we discuss the potential to observe these precession signatures in electromagnetic and gravitational wave (GW) observations, as well as the precession signal's unique importance as a potential tool to constrain the mass, eccentricity, and semi-major axis of binary merger events.","sentences":["Recent hydrodynamical simulations have shown that circumbinary gas disks drive the orbits of binary black holes to become eccentric, even when general relativistic corrections to the orbit are significant.","Here, we study the general relativistic (GR) apsidal precession of eccentric equal-mass binary black holes in circumbinary disks (CBDs) via two-dimensional hydrodynamical simulations.","We perform a suite of simulations comparing precessing and non-precessing binaries across a range of eccentricities, semi-major axes, and precession rates.","We find that the GR precession of the binary's semi-major axis can introduce a dominant modulation in the binary's accretion rate and the corresponding high-energy electromagnetic light-curves.","We discuss the conditions under which this occurs and its detailed characteristics and mechanism.","Finally, we discuss the potential to observe these precession signatures in electromagnetic and gravitational wave (GW) observations, as well as the precession signal's unique importance as a potential tool to constrain the mass, eccentricity, and semi-major axis of binary merger events."],"url":"http://arxiv.org/abs/2405.07897v1","category":"astro-ph.HE"}
{"created":"2024-05-13 16:28:00","title":"Science based AI model certification for new operational environments with application in traffic state estimation","abstract":"The expanding role of Artificial Intelligence (AI) in diverse engineering domains highlights the challenges associated with deploying AI models in new operational environments, involving substantial investments in data collection and model training. Rapid application of AI necessitates evaluating the feasibility of utilizing pre-trained models in unobserved operational settings with minimal or no additional data. However, interpreting the opaque nature of AI's black-box models remains a persistent challenge. Addressing this issue, this paper proposes a science-based certification methodology to assess the viability of employing pre-trained data-driven models in new operational environments. The methodology advocates a profound integration of domain knowledge, leveraging theoretical and analytical models from physics and related disciplines, with data-driven AI models. This novel approach introduces tools to facilitate the development of secure engineering systems, providing decision-makers with confidence in the trustworthiness and safety of AI-based models across diverse environments characterized by limited training data and dynamic, uncertain conditions. The paper demonstrates the efficacy of this methodology in real-world safety-critical scenarios, particularly in the context of traffic state estimation. Through simulation results, the study illustrates how the proposed methodology efficiently quantifies physical inconsistencies exhibited by pre-trained AI models. By utilizing analytical models, the methodology offers a means to gauge the applicability of pre-trained AI models in new operational environments. This research contributes to advancing the understanding and deployment of AI models, offering a robust certification framework that enhances confidence in their reliability and safety across a spectrum of operational conditions.","sentences":["The expanding role of Artificial Intelligence (AI) in diverse engineering domains highlights the challenges associated with deploying AI models in new operational environments, involving substantial investments in data collection and model training.","Rapid application of AI necessitates evaluating the feasibility of utilizing pre-trained models in unobserved operational settings with minimal or no additional data.","However, interpreting the opaque nature of AI's black-box models remains a persistent challenge.","Addressing this issue, this paper proposes a science-based certification methodology to assess the viability of employing pre-trained data-driven models in new operational environments.","The methodology advocates a profound integration of domain knowledge, leveraging theoretical and analytical models from physics and related disciplines, with data-driven AI models.","This novel approach introduces tools to facilitate the development of secure engineering systems, providing decision-makers with confidence in the trustworthiness and safety of AI-based models across diverse environments characterized by limited training data and dynamic, uncertain conditions.","The paper demonstrates the efficacy of this methodology in real-world safety-critical scenarios, particularly in the context of traffic state estimation.","Through simulation results, the study illustrates how the proposed methodology efficiently quantifies physical inconsistencies exhibited by pre-trained AI models.","By utilizing analytical models, the methodology offers a means to gauge the applicability of pre-trained AI models in new operational environments.","This research contributes to advancing the understanding and deployment of AI models, offering a robust certification framework that enhances confidence in their reliability and safety across a spectrum of operational conditions."],"url":"http://arxiv.org/abs/2405.07893v1","category":"cs.AI"}
{"created":"2024-05-13 16:19:31","title":"Quantum Computation Using Large Spin Qudits","abstract":"This dissertation explores quantum computation using qudits encoded into large spins, emphasizing the concept of quantum co-design to harness the unique capabilities of physical platforms for enhanced quantum information processing. First, we delve into the generation of high-fidelity universal gate sets for quantum computation with qudits. Leveraging principles from quantum optimal control, Rydberg physics, and the atomic structure of alkaline-earth atoms, we propose protocols for high-fidelity universal gate sets in the ground state of 87Sr with reasonable experimental parameters. Next, we analyze schemes to encode a qubit in the large spin qudits for fault-tolerant quantum computation (FTQC). By comprehending the most dominant noise in the physical system, we develop FTQC protocols that outperform the standard protocols. Finally, considering spin qudits for neutral atom quantum computation, we studied protocols for converting leakage errors to erasure errors resource efficiently. Also, we developed cooling methods for neutral atoms without destroying the quantum information.","sentences":["This dissertation explores quantum computation using qudits encoded into large spins, emphasizing the concept of quantum co-design to harness the unique capabilities of physical platforms for enhanced quantum information processing.","First, we delve into the generation of high-fidelity universal gate sets for quantum computation with qudits.","Leveraging principles from quantum optimal control, Rydberg physics, and the atomic structure of alkaline-earth atoms, we propose protocols for high-fidelity universal gate sets in the ground state of 87Sr with reasonable experimental parameters.","Next, we analyze schemes to encode a qubit in the large spin qudits for fault-tolerant quantum computation (FTQC).","By comprehending the most dominant noise in the physical system, we develop FTQC protocols that outperform the standard protocols.","Finally, considering spin qudits for neutral atom quantum computation, we studied protocols for converting leakage errors to erasure errors resource efficiently.","Also, we developed cooling methods for neutral atoms without destroying the quantum information."],"url":"http://arxiv.org/abs/2405.07885v1","category":"quant-ph"}
{"created":"2024-05-13 16:17:57","title":"Lai Loss: A Novel Loss Integrating Regularization","abstract":"In the field of machine learning, traditional regularization methods generally tend to directly add regularization terms to the loss function. This paper introduces the \"Lai loss\", a novel loss design that integrates the regularization terms (gradient component) into the traditional loss function through a straightforward geometric ideation. This design innovatively penalizes the gradient vectors through the loss, effectively controlling the model's smoothness and offering the dual benefits of reducing overfitting and avoiding underfitting. Subsequently, we proposed a random sampling method that successfully addresses the challenges associated with its application under large sample conditions. We conducted preliminary experiments using publicly available datasets from Kaggle, demonstrating that the design of Lai loss can control the model's smoothness while ensuring maximum accuracy.","sentences":["In the field of machine learning, traditional regularization methods generally tend to directly add regularization terms to the loss function.","This paper introduces the \"Lai loss\", a novel loss design that integrates the regularization terms (gradient component) into the traditional loss function through a straightforward geometric ideation.","This design innovatively penalizes the gradient vectors through the loss, effectively controlling the model's smoothness and offering the dual benefits of reducing overfitting and avoiding underfitting.","Subsequently, we proposed a random sampling method that successfully addresses the challenges associated with its application under large sample conditions.","We conducted preliminary experiments using publicly available datasets from Kaggle, demonstrating that the design of Lai loss can control the model's smoothness while ensuring maximum accuracy."],"url":"http://arxiv.org/abs/2405.07884v1","category":"cs.LG"}
{"created":"2024-05-13 16:17:10","title":"Zero-Shot Tokenizer Transfer","abstract":"Language models (LMs) are bound to their tokenizer, which maps raw text to a sequence of vocabulary items (tokens). This restricts their flexibility: for example, LMs trained primarily on English may still perform well in other natural and programming languages, but have vastly decreased efficiency due to their English-centric tokenizer. To mitigate this, we should be able to swap the original LM tokenizer with an arbitrary one, on the fly, without degrading performance. Hence, in this work we define a new problem: Zero-Shot Tokenizer Transfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for the tokens in the vocabulary of the new tokenizer. Since prior heuristics for initializing embeddings often perform at chance level in a ZeTT setting, we propose a new solution: we train a hypernetwork taking a tokenizer as input and predicting the corresponding embeddings. We empirically demonstrate that the hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and decoder LLMs (e.g., Mistral-7B). Our method comes close to the original models' performance in cross-lingual and coding tasks while markedly reducing the length of the tokenized sequence. We also find that the remaining gap can be quickly closed by continued training on less than 1B tokens. Finally, we show that a ZeTT hypernetwork trained for a base (L)LM can also be applied to fine-tuned variants without extra training. Overall, our results make substantial strides toward detaching LMs from their tokenizer.","sentences":["Language models (LMs) are bound to their tokenizer, which maps raw text to a sequence of vocabulary items (tokens).","This restricts their flexibility: for example, LMs trained primarily on English may still perform well in other natural and programming languages, but have vastly decreased efficiency due to their English-centric tokenizer.","To mitigate this, we should be able to swap the original LM tokenizer with an arbitrary one, on the fly, without degrading performance.","Hence, in this work we define a new problem: Zero-Shot Tokenizer Transfer (ZeTT).","The challenge at the core of ZeTT is finding embeddings for the tokens in the vocabulary of the new tokenizer.","Since prior heuristics for initializing embeddings often perform at chance level in a ZeTT setting, we propose a new solution: we train a hypernetwork taking a tokenizer as input and predicting the corresponding embeddings.","We empirically demonstrate that the hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and decoder LLMs (e.g., Mistral-7B).","Our method comes close to the original models' performance in cross-lingual and coding tasks while markedly reducing the length of the tokenized sequence.","We also find that the remaining gap can be quickly closed by continued training on less than 1B tokens.","Finally, we show that a ZeTT hypernetwork trained for a base (L)LM can also be applied to fine-tuned variants without extra training.","Overall, our results make substantial strides toward detaching LMs from their tokenizer."],"url":"http://arxiv.org/abs/2405.07883v1","category":"cs.CL"}
{"created":"2024-05-13 16:15:08","title":"On Hagedorn wavepackets associated with different Gaussians","abstract":"Hagedorn functions are carefully constructed generalizations of Hermite functions to the setting of many-dimensional squeezed and coupled harmonic systems. Wavepackets formed by superpositions of Hagedorn functions have been successfully used to solve the time-dependent Schr\\\"{o}dinger equation exactly in harmonic systems and variationally in anharmonic systems. For evaluating typical observables, such as position or kinetic energy, it is sufficient to consider orthonormal Hagedorn functions with a single Gaussian center. Here, we instead derive various relations between Hagedorn bases associated with different Gaussians, including their overlaps, which are necessary for evaluating quantities nonlocal in time, such as time correlation functions needed for computing spectra. First, we use the Bogoliubov transformation to obtain commutation relations between the ladder operators associated with different Gaussians. Then, instead of using numerical quadrature, we employ these commutation relations to derive exact recurrence relations for the overlap integrals between Hagedorn functions with different Gaussian centers. Finally, we present numerical experiments that demonstrate the accuracy and efficiency of our algebraic method as well as its suitability to treat problems in spectroscopy and chemical dynamics.","sentences":["Hagedorn functions are carefully constructed generalizations of Hermite functions to the setting of many-dimensional squeezed and coupled harmonic systems.","Wavepackets formed by superpositions of Hagedorn functions have been successfully used to solve the time-dependent Schr\\\"{o}dinger equation exactly in harmonic systems and variationally in anharmonic systems.","For evaluating typical observables, such as position or kinetic energy, it is sufficient to consider orthonormal Hagedorn functions with a single Gaussian center.","Here, we instead derive various relations between Hagedorn bases associated with different Gaussians, including their overlaps, which are necessary for evaluating quantities nonlocal in time, such as time correlation functions needed for computing spectra.","First, we use the Bogoliubov transformation to obtain commutation relations between the ladder operators associated with different Gaussians.","Then, instead of using numerical quadrature, we employ these commutation relations to derive exact recurrence relations for the overlap integrals between Hagedorn functions with different Gaussian centers.","Finally, we present numerical experiments that demonstrate the accuracy and efficiency of our algebraic method as well as its suitability to treat problems in spectroscopy and chemical dynamics."],"url":"http://arxiv.org/abs/2405.07880v1","category":"quant-ph"}
{"created":"2024-05-13 16:09:29","title":"On the Relation Between Autoencoders and Non-negative Matrix Factorization, and Their Application for Mutational Signature Extraction","abstract":"The aim of this study is to provide a foundation to understand the relationship between non-negative matrix factorization (NMF) and non-negative autoencoders enabling proper interpretation and understanding of autoencoder-based alternatives to NMF. Since its introduction, NMF has been a popular tool for extracting interpretable, low-dimensional representations of high-dimensional data. However, recently, several studies have proposed to replace NMF with autoencoders. This increasing popularity of autoencoders warrants an investigation on whether this replacement is in general valid and reasonable. Moreover, the exact relationship between non-negative autoencoders and NMF has not been thoroughly explored. Thus, a main aim of this study is to investigate in detail the relationship between non-negative autoencoders and NMF. We find that the connection between the two models can be established through convex NMF, which is a restricted case of NMF. In particular, convex NMF is a special case of an autoencoder. The performance of NMF and autoencoders is compared within the context of extraction of mutational signatures from cancer genomics data. We find that the reconstructions based on NMF are more accurate compared to autoencoders, while the signatures extracted using both methods show comparable consistencies and values when externally validated. These findings suggest that the non-negative autoencoders investigated in this article do not provide an improvement of NMF in the field of mutational signature extraction.","sentences":["The aim of this study is to provide a foundation to understand the relationship between non-negative matrix factorization (NMF) and non-negative autoencoders enabling proper interpretation and understanding of autoencoder-based alternatives to NMF.","Since its introduction, NMF has been a popular tool for extracting interpretable, low-dimensional representations of high-dimensional data.","However, recently, several studies have proposed to replace NMF with autoencoders.","This increasing popularity of autoencoders warrants an investigation on whether this replacement is in general valid and reasonable.","Moreover, the exact relationship between non-negative autoencoders and NMF has not been thoroughly explored.","Thus, a main aim of this study is to investigate in detail the relationship between non-negative autoencoders and NMF.","We find that the connection between the two models can be established through convex NMF, which is a restricted case of NMF.","In particular, convex NMF is a special case of an autoencoder.","The performance of NMF and autoencoders is compared within the context of extraction of mutational signatures from cancer genomics data.","We find that the reconstructions based on NMF are more accurate compared to autoencoders, while the signatures extracted using both methods show comparable consistencies and values when externally validated.","These findings suggest that the non-negative autoencoders investigated in this article do not provide an improvement of NMF in the field of mutational signature extraction."],"url":"http://arxiv.org/abs/2405.07879v1","category":"stat.AP"}
{"created":"2024-05-13 16:08:53","title":"Effective medium properties of stealthy hyperuniform photonic structures using multiscale physics-informed neural networks","abstract":"In this article, we employ multiscale physics-informed neural networks (MscalePINNs) for the inverse retrieval of the effective permittivity and homogenization of finite-size photonic media with stealthy hyperuniform (SHU) disordered geometries. Specifically, we show that MscalePINNs are capable of capturing the fast spatial variations of complex fields scattered by arrays of dielectric nanocylinders arranged according to isotropic SHU point patterns, thus enabling a systematic methodology to inverse retrieve their effective dielectric profiles. Our approach extends the recently developed high-frequency homogenization theory of hyperuniform media and retrieves more general permittivity profiles for applications-relevant finite-size SHU systems, unveiling unique features related to their isotropic nature. In particular, we demonstrate the existence of a transparency region beyond the long-wavelength approximation, enabling effective and isotropic homogenization even without disorder-averaging, in contrast to the case of uncorrelated Poisson random patterns. We believe that the multiscale network approach introduced here enables the efficient inverse design of general effective media and finite-size metamaterials with isotropic electromagnetic responses beyond the limitations of traditional homogenization theories.","sentences":["In this article, we employ multiscale physics-informed neural networks (MscalePINNs) for the inverse retrieval of the effective permittivity and homogenization of finite-size photonic media with stealthy hyperuniform (SHU) disordered geometries.","Specifically, we show that MscalePINNs are capable of capturing the fast spatial variations of complex fields scattered by arrays of dielectric nanocylinders arranged according to isotropic SHU point patterns, thus enabling a systematic methodology to inverse retrieve their effective dielectric profiles.","Our approach extends the recently developed high-frequency homogenization theory of hyperuniform media and retrieves more general permittivity profiles for applications-relevant finite-size SHU systems, unveiling unique features related to their isotropic nature.","In particular, we demonstrate the existence of a transparency region beyond the long-wavelength approximation, enabling effective and isotropic homogenization even without disorder-averaging, in contrast to the case of uncorrelated Poisson random patterns.","We believe that the multiscale network approach introduced here enables the efficient inverse design of general effective media and finite-size metamaterials with isotropic electromagnetic responses beyond the limitations of traditional homogenization theories."],"url":"http://arxiv.org/abs/2405.07878v1","category":"physics.optics"}
{"created":"2024-05-13 16:02:57","title":"Reproducing the Metric-Based Evaluation of a Set of Controllable Text Generation Techniques","abstract":"Rerunning a metric-based evaluation should be more straightforward, and results should be closer, than in a human-based evaluation, especially where code and model checkpoints are made available by the original authors. As this report of our efforts to rerun a metric-based evaluation of a set of single-attribute and multiple-attribute controllable text generation (CTG) techniques shows however, such reruns of evaluations do not always produce results that are the same as the original results, and can reveal errors in the reporting of the original work.","sentences":["Rerunning a metric-based evaluation should be more straightforward, and results should be closer, than in a human-based evaluation, especially where code and model checkpoints are made available by the original authors.","As this report of our efforts to rerun a metric-based evaluation of a set of single-attribute and multiple-attribute controllable text generation (CTG) techniques shows however, such reruns of evaluations do not always produce results that are the same as the original results, and can reveal errors in the reporting of the original work."],"url":"http://arxiv.org/abs/2405.07875v1","category":"cs.CL"}
{"created":"2024-05-13 15:52:30","title":"Sub-percent Characterization and Polarimetric Performance Analysis of Commercial Micro-polarizer Array Detectors","abstract":"Polarization imaging can yield crucial information in multiple applications of remote sensing, such as characterization of clouds, aerosols, and the Aurora Borealis. Some applications require sub-percent polarimetric sensitivity and accuracy in determining the Stokes parameters, which can be a challenge to attain. In 2018, Sony released a low-cost CMOS-based imaging chip with integrated micro-polarizer array for general polarization measurements. We implement the calibration steps required for these Sony chips to reach sub-percent polarimetric accuracies. To analyze their performances, we have compared the characteristics of four different detector packages by three manufacturers housing either the monochromatic version or the RGB color variant. We present a comprehensive overview of the effects that these characteristics have on the polarimetric performance of the camera. They include dark noise, behavior over different gain settings, detector/pixel artifacts, and polarimetric effects determined by polarizer extinction ratios, polarizer orientations, and accuracy of polarimetric zero points due to differential pixel gains. In addition to calibrations using unpolarized light and fully linearly polarized light, we assess the polarimetric sensitivity within a tilting and rotating glass-plate set-up. We discuss the benefits of adding a rotating half-wave plate as an additional temporal modulator to generically mitigate some of the detector effects, and achieve better polarimetric sensitivity/accuracy albeit at the expense of lower temporal resolution. We conclude by presenting and discussing the polarimetric limits to which we were able to calibrate the detector effects for practical purposes. By reaching a compound absolute polarimetric uncertainty of less than a percent, these very compact, low-cost detectors are enabled for a multitude of scientific goals.","sentences":["Polarization imaging can yield crucial information in multiple applications of remote sensing, such as characterization of clouds, aerosols, and the Aurora Borealis.","Some applications require sub-percent polarimetric sensitivity and accuracy in determining the Stokes parameters, which can be a challenge to attain.","In 2018, Sony released a low-cost CMOS-based imaging chip with integrated micro-polarizer array for general polarization measurements.","We implement the calibration steps required for these Sony chips to reach sub-percent polarimetric accuracies.","To analyze their performances, we have compared the characteristics of four different detector packages by three manufacturers housing either the monochromatic version or the RGB color variant.","We present a comprehensive overview of the effects that these characteristics have on the polarimetric performance of the camera.","They include dark noise, behavior over different gain settings, detector/pixel artifacts, and polarimetric effects determined by polarizer extinction ratios, polarizer orientations, and accuracy of polarimetric zero points due to differential pixel gains.","In addition to calibrations using unpolarized light and fully linearly polarized light, we assess the polarimetric sensitivity within a tilting and rotating glass-plate set-up.","We discuss the benefits of adding a rotating half-wave plate as an additional temporal modulator to generically mitigate some of the detector effects, and achieve better polarimetric sensitivity/accuracy albeit at the expense of lower temporal resolution.","We conclude by presenting and discussing the polarimetric limits to which we were able to calibrate the detector effects for practical purposes.","By reaching a compound absolute polarimetric uncertainty of less than a percent, these very compact, low-cost detectors are enabled for a multitude of scientific goals."],"url":"http://arxiv.org/abs/2405.07864v1","category":"astro-ph.IM"}
{"created":"2024-05-13 15:50:39","title":"RLHF Workflow: From Reward Modeling to Online RLHF","abstract":"We present the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF) in this technical report, which is widely reported to outperform its offline counterpart by a large margin in the recent large language model (LLM) literature. However, existing open-source RLHF projects are still largely confined to the offline learning setting. In this technical report, we aim to fill in this gap and provide a detailed recipe that is easy to reproduce for online iterative RLHF. In particular, since online human feedback is usually infeasible for open-source communities with limited resources, we start by constructing preference models using a diverse set of open-source datasets and use the constructed proxy preference model to approximate human feedback. Then, we discuss the theoretical insights and algorithmic principles behind online iterative RLHF, followed by a detailed practical implementation. Our trained LLM, SFR-Iterative-DPO-LLaMA-3-8B-R, achieves impressive performance on LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarks such as HumanEval and TruthfulQA. We have shown that supervised fine-tuning (SFT) and iterative RLHF can obtain state-of-the-art performance with fully open-source datasets. Further, we have made our models, curated datasets, and comprehensive step-by-step code guidebooks publicly available. Please refer to https://github.com/RLHFlow/RLHF-Reward-Modeling and https://github.com/RLHFlow/Online-RLHF for more detailed information.","sentences":["We present the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF) in this technical report, which is widely reported to outperform its offline counterpart by a large margin in the recent large language model (LLM) literature.","However, existing open-source RLHF projects are still largely confined to the offline learning setting.","In this technical report, we aim to fill in this gap and provide a detailed recipe that is easy to reproduce for online iterative RLHF.","In particular, since online human feedback is usually infeasible for open-source communities with limited resources, we start by constructing preference models using a diverse set of open-source datasets and use the constructed proxy preference model to approximate human feedback.","Then, we discuss the theoretical insights and algorithmic principles behind online iterative RLHF, followed by a detailed practical implementation.","Our trained LLM, SFR-Iterative-DPO-LLaMA-3-8B-R, achieves impressive performance on LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarks such as HumanEval and TruthfulQA.","We have shown that supervised fine-tuning (SFT) and iterative RLHF can obtain state-of-the-art performance with fully open-source datasets.","Further, we have made our models, curated datasets, and comprehensive step-by-step code guidebooks publicly available.","Please refer to https://github.com/RLHFlow/RLHF-Reward-Modeling and https://github.com/RLHFlow/Online-RLHF for more detailed information."],"url":"http://arxiv.org/abs/2405.07863v1","category":"cs.LG"}
{"created":"2024-05-13 15:42:46","title":"Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs","abstract":"The multi-plane representation has been highlighted for its fast training and inference across static and dynamic neural radiance fields. This approach constructs relevant features via projection onto learnable grids and interpolating adjacent vertices. However, it has limitations in capturing low-frequency details and tends to overuse parameters for low-frequency features due to its bias toward fine details, despite its multi-resolution concept. This phenomenon leads to instability and inefficiency when training poses are sparse. In this work, we propose a method that synergistically integrates multi-plane representation with a coordinate-based network known for strong bias toward low-frequency signals. The coordinate-based network is responsible for capturing low-frequency details, while the multi-plane representation focuses on capturing fine-grained details. We demonstrate that using residual connections between them seamlessly preserves their own inherent properties. Additionally, the proposed progressive training scheme accelerates the disentanglement of these two features. We empirically show that the proposed method achieves comparable results to explicit encoding with fewer parameters, and particularly, it outperforms others for the static and dynamic NeRFs under sparse inputs.","sentences":["The multi-plane representation has been highlighted for its fast training and inference across static and dynamic neural radiance fields.","This approach constructs relevant features via projection onto learnable grids and interpolating adjacent vertices.","However, it has limitations in capturing low-frequency details and tends to overuse parameters for low-frequency features due to its bias toward fine details, despite its multi-resolution concept.","This phenomenon leads to instability and inefficiency when training poses are sparse.","In this work, we propose a method that synergistically integrates multi-plane representation with a coordinate-based network known for strong bias toward low-frequency signals.","The coordinate-based network is responsible for capturing low-frequency details, while the multi-plane representation focuses on capturing fine-grained details.","We demonstrate that using residual connections between them seamlessly preserves their own inherent properties.","Additionally, the proposed progressive training scheme accelerates the disentanglement of these two features.","We empirically show that the proposed method achieves comparable results to explicit encoding with fewer parameters, and particularly, it outperforms others for the static and dynamic NeRFs under sparse inputs."],"url":"http://arxiv.org/abs/2405.07857v1","category":"cs.CV"}
{"created":"2024-05-13 15:41:56","title":"Formation of N-bearing complex organic molecules in molecular clouds: Ketenimine, acetonitrile, acetaldimine, and vinylamine via the UV photolysis of C$_2$H$_2$ ice","abstract":"The solid-state C$_2$H$_2$ chemistry in interstellar H$_2$O-rich ice has been proposed to explain astronomically observed complex organic molecules (COMs), including ketene (CH$_2$CO), acetaldehyde (CH$_3$CHO), and ethanol (CH$_3$CH$_2$OH), toward early star-forming regions. This formation mechanism is supported by recent laboratory studies and theoretical calculations for the reactions of C$_2$H$_2$+OH/H. However, the analog reaction of C$_2$H$_2$+NH$_2$ forming N-bearing species has been suggested to have a relatively low rate constant that is orders of magnitude lower than the value of C$_2$H$_2$+OH. This work extends our previous laboratory studies on O-bearing COM formation to investigate the interactions between C$_2$H$_2$ and NH$_3$ ice triggered by cosmic ray-induced secondary UV photons under molecular cloud conditions. Experiments were performed in an ultra-high vacuum chamber to investigate the UV photolysis of the C$_2$H$_2$:NH$_3$ ice mixture at 10 K. The studied ice chemistry of C$_2$H$_2$ with NH$_2$ radicals and H atoms resulting from the UV photodissociation of NH$_3$ leads to the formation of several N-bearing COMs, including vinylamine (CH$_2$CHNH$_2$), acetaldimine (CH$_3$CHNH), acetonitrile (CH$_3$CN), ketenimine (CH$_2$CNH), and tentatively ethylamine (CH$_3$CH$_2$NH$_2$). The experimental results show an immediate and abundant CH$_2$CHNH$_2$ yield as the first-generation product, which is further converted into other chemical derivatives. The effective destruction and formation cross-section values of parent species and COMs were derived, and we discuss the chemical links among these molecules and their astronomical relevance.","sentences":["The solid-state C$_2$H$_2$ chemistry in interstellar H$_2$O-rich ice has been proposed to explain astronomically observed complex organic molecules (COMs), including ketene (CH$_2$CO), acetaldehyde (CH$_3$CHO), and ethanol (CH$_3$CH$_2$OH), toward early star-forming regions.","This formation mechanism is supported by recent laboratory studies and theoretical calculations for the reactions of C$_2$H$_2$+OH/H. However, the analog reaction of C$_2$H$_2$+NH$_2$ forming N-bearing species has been suggested to have a relatively low rate constant that is orders of magnitude lower than the value of C$_2$H$_2$+OH.","This work extends our previous laboratory studies on O-bearing COM formation to investigate the interactions between C$_2$H$_2$ and NH$_3$ ice triggered by cosmic ray-induced secondary UV photons under molecular cloud conditions.","Experiments were performed in an ultra-high vacuum chamber to investigate the UV photolysis of the C$_2$H$_2$:NH$_3$ ice mixture at 10 K. The studied ice chemistry of C$_2$H$_2$ with NH$_2$ radicals and H atoms resulting from the UV photodissociation of NH$_3$ leads to the formation of several N-bearing COMs, including vinylamine (CH$_2$CHNH$_2$), acetaldimine (CH$_3$CHNH), acetonitrile (CH$_3$CN), ketenimine (CH$_2$CNH), and tentatively ethylamine (CH$_3$CH$_2$NH$_2$).","The experimental results show an immediate and abundant CH$_2$CHNH$_2$ yield as the first-generation product, which is further converted into other chemical derivatives.","The effective destruction and formation cross-section values of parent species and COMs were derived, and we discuss the chemical links among these molecules and their astronomical relevance."],"url":"http://arxiv.org/abs/2405.07855v1","category":"astro-ph.GA"}
{"created":"2024-05-13 15:38:51","title":"Generic statistical and spatial properties of low-energy excitations in computer glasses","abstract":"We briefly test the key claims made in [Nat. Commun. 15, 3107 (2024)] regarding the scaling of the low-frequency tail of the nonphononic VDoS of glasses, and its connections to glass formation history and to the form of the distribution of weak local stresses.","sentences":["We briefly test the key claims made in [Nat. Commun.","15, 3107 (2024)] regarding the scaling of the low-frequency tail of the nonphononic VDoS of glasses, and its connections to glass formation history and to the form of the distribution of weak local stresses."],"url":"http://arxiv.org/abs/2405.07851v1","category":"cond-mat.soft"}
{"created":"2024-05-13 15:38:16","title":"Blow-up invariance for Hodge-Witt sheaves with modulus","abstract":"In this paper, we prove the blow-up invariance for Hodge-Witt sheaves with modulus, which is a generalization of a result of Koizumi for Witt sheaves and that of Kelly-Miyazaki and Koizumi for Hodge sheaves. As a consequence, we obtain the representability of Hodge-Witt sheaves with modulus in the category of motives with modulus under the assumption of resolution of singularities.","sentences":["In this paper, we prove the blow-up invariance for Hodge-Witt sheaves with modulus, which is a generalization of a result of Koizumi for Witt sheaves and that of Kelly-Miyazaki and Koizumi for Hodge sheaves.","As a consequence, we obtain the representability of Hodge-Witt sheaves with modulus in the category of motives with modulus under the assumption of resolution of singularities."],"url":"http://arxiv.org/abs/2405.07849v1","category":"math.AG"}
{"created":"2024-05-13 15:37:49","title":"Positional-Unigram Byte Models for Generalized TLS Fingerprinting","abstract":"We use positional-unigram byte models along with maximum likelihood for generalized TLS fingerprinting and empirically show that it is robust to cipher stunting. Our approach creates a set of positional-unigram byte models from client hello messages. Each positional-unigram byte model is a statistical model of TLS client hello traffic created by a client application or process. To fingerprint a TLS connection, we use its client hello, and compute the likelihood as a function of a statistical model. The statistical model that maximizes the likelihood function is the predicted client application for the given client hello. Our data driven approach does not use side-channel information and can be updated on-the-fly. We experimentally validate our method on an internal dataset and show that it is robust to cipher stunting by tracking an unbiased $f_{1}$ score as we synthetically increase randomization.","sentences":["We use positional-unigram byte models along with maximum likelihood for generalized TLS fingerprinting and empirically show that it is robust to cipher stunting.","Our approach creates a set of positional-unigram byte models from client hello messages.","Each positional-unigram byte model is a statistical model of TLS client hello traffic created by a client application or process.","To fingerprint a TLS connection, we use its client hello, and compute the likelihood as a function of a statistical model.","The statistical model that maximizes the likelihood function is the predicted client application for the given client hello.","Our data driven approach does not use side-channel information and can be updated on-the-fly.","We experimentally validate our method on an internal dataset and show that it is robust to cipher stunting by tracking an unbiased $f_{1}$ score as we synthetically increase randomization."],"url":"http://arxiv.org/abs/2405.07848v1","category":"cs.CR"}
{"created":"2024-05-13 15:34:27","title":"Time Evolution and Thermal Renormalization Group Flow in Cosmology","abstract":"Time-evolution of the Universe as described by the Friedmann equation can be coupled to equations of motion of matter fields. Quantum effects may be incorporated to improve these classical equations of motion by the renormalization group (RG) running of their couplings. Since temporal and thermal evolutions are linked to each other, astrophysical and cosmological treatements based on zero-temperature RG methods require the extension to finite-temperatures. We propose and explore a modification of the usual finite-temperature RG approach by relating the temperature parameter to the running RG scale as $T \\equiv k_T = \\tau k$ (in natural units), where $k_T$ is acting as a running cutoff for thermal fluctuations and the momentum $k$ can be used for the quantum fluctuations. In this approach, the temperature of the expanding Universe is related to the dimensionless quantity $\\tau$ (and not to $k_T$). We show that by this choice dimensionless RG flow equations have no explicit $k$-dependence, as it is convenient. We also discuss how this modified thermal RG is used to handle high-energy divergences of the RG running of the cosmological constant and to \"solve the triviality\" of the $\\phi^4$ model by a thermal phase transition in terms of $\\tau$ in $d=4$ Euclidean dimensions.","sentences":["Time-evolution of the Universe as described by the Friedmann equation can be coupled to equations of motion of matter fields.","Quantum effects may be incorporated to improve these classical equations of motion by the renormalization group (RG) running of their couplings.","Since temporal and thermal evolutions are linked to each other, astrophysical and cosmological treatements based on zero-temperature RG methods require the extension to finite-temperatures.","We propose and explore a modification of the usual finite-temperature RG approach by relating the temperature parameter to the running RG scale as $T \\equiv k_T = \\tau k$ (in natural units), where $k_T$ is acting as a running cutoff for thermal fluctuations and the momentum $k$ can be used for the quantum fluctuations.","In this approach, the temperature of the expanding Universe is related to the dimensionless quantity $\\tau$ (and not to $k_T$).","We show that by this choice dimensionless RG flow equations have no explicit $k$-dependence, as it is convenient.","We also discuss how this modified thermal RG is used to handle high-energy divergences of the RG running of the cosmological constant and to \"solve the triviality\" of the $\\phi^4$ model by a thermal phase transition in terms of $\\tau$ in $d=4$ Euclidean dimensions."],"url":"http://arxiv.org/abs/2405.07846v1","category":"hep-th"}
{"created":"2024-05-13 15:34:20","title":"Multi-Task Learning for Fatigue Detection and Face Recognition of Drivers via Tree-Style Space-Channel Attention Fusion Network","abstract":"In driving scenarios, automobile active safety systems are increasingly incorporating deep learning technology. These systems typically need to handle multiple tasks simultaneously, such as detecting fatigue driving and recognizing the driver's identity. However, the traditional parallel-style approach of combining multiple single-task models tends to waste resources when dealing with similar tasks. Therefore, we propose a novel tree-style multi-task modeling approach for multi-task learning, which rooted at a shared backbone, more dedicated separate module branches are appended as the model pipeline goes deeper. Following the tree-style approach, we propose a multi-task learning model for simultaneously performing driver fatigue detection and face recognition for identifying a driver. This model shares a common feature extraction backbone module, with further separated feature extraction and classification module branches. The dedicated branches exploit and combine spatial and channel attention mechanisms to generate space-channel fused-attention enhanced features, leading to improved detection performance. As only single-task datasets are available, we introduce techniques including alternating updation and gradient accumulation for training our multi-task model using only the single-task datasets. The effectiveness of our tree-style multi-task learning model is verified through extensive validations.","sentences":["In driving scenarios, automobile active safety systems are increasingly incorporating deep learning technology.","These systems typically need to handle multiple tasks simultaneously, such as detecting fatigue driving and recognizing the driver's identity.","However, the traditional parallel-style approach of combining multiple single-task models tends to waste resources when dealing with similar tasks.","Therefore, we propose a novel tree-style multi-task modeling approach for multi-task learning, which rooted at a shared backbone, more dedicated separate module branches are appended as the model pipeline goes deeper.","Following the tree-style approach, we propose a multi-task learning model for simultaneously performing driver fatigue detection and face recognition for identifying a driver.","This model shares a common feature extraction backbone module, with further separated feature extraction and classification module branches.","The dedicated branches exploit and combine spatial and channel attention mechanisms to generate space-channel fused-attention enhanced features, leading to improved detection performance.","As only single-task datasets are available, we introduce techniques including alternating updation and gradient accumulation for training our multi-task model using only the single-task datasets.","The effectiveness of our tree-style multi-task learning model is verified through extensive validations."],"url":"http://arxiv.org/abs/2405.07845v1","category":"cs.CV"}
{"created":"2024-05-13 15:30:41","title":"Ground-based Image Deconvolution with Swin Transformer UNet","abstract":"As ground-based all-sky astronomical surveys will gather millions of images in the coming years, a critical requirement emerges for the development of fast deconvolution algorithms capable of efficiently improving the spatial resolution of these images. By successfully recovering clean and high-resolution images from these surveys, our objective is to help deepen our understanding of galaxy formation and evolution through accurate photometric measurements. We introduce a two-step deconvolution framework using a Swin Transformer architecture. Our study reveals that the deep learning-based solution introduces a bias, constraining the scope of scientific analysis. To address this limitation, we propose a novel third step relying on the active coefficients in the sparsity wavelet framework. By conducting a performance comparison between our deep learning-based method and Firedec, a classical deconvolution algorithm, we analyze a subset of the EDisCS cluster samples. We demonstrate the advantage of our method in terms of resolution recovery, generalization to different noise properties, and computational efficiency. Not only does the analysis of this cluster sample assess the efficiency of our method, but it also enables us to quantify the number of clumps within these galaxies in relation to their disc colour. This robust technique holds promise for identifying structures in the distant universe from ground-based images.","sentences":["As ground-based all-sky astronomical surveys will gather millions of images in the coming years, a critical requirement emerges for the development of fast deconvolution algorithms capable of efficiently improving the spatial resolution of these images.","By successfully recovering clean and high-resolution images from these surveys, our objective is to help deepen our understanding of galaxy formation and evolution through accurate photometric measurements.","We introduce a two-step deconvolution framework using a Swin Transformer architecture.","Our study reveals that the deep learning-based solution introduces a bias, constraining the scope of scientific analysis.","To address this limitation, we propose a novel third step relying on the active coefficients in the sparsity wavelet framework.","By conducting a performance comparison between our deep learning-based method and Firedec, a classical deconvolution algorithm, we analyze a subset of the EDisCS cluster samples.","We demonstrate the advantage of our method in terms of resolution recovery, generalization to different noise properties, and computational efficiency.","Not only does the analysis of this cluster sample assess the efficiency of our method, but it also enables us to quantify the number of clumps within these galaxies in relation to their disc colour.","This robust technique holds promise for identifying structures in the distant universe from ground-based images."],"url":"http://arxiv.org/abs/2405.07842v1","category":"astro-ph.IM"}
{"created":"2024-05-13 15:25:03","title":"Constrained Exploration via Reflected Replica Exchange Stochastic Gradient Langevin Dynamics","abstract":"Replica exchange stochastic gradient Langevin dynamics (reSGLD) is an effective sampler for non-convex learning in large-scale datasets. However, the simulation may encounter stagnation issues when the high-temperature chain delves too deeply into the distribution tails. To tackle this issue, we propose reflected reSGLD (r2SGLD): an algorithm tailored for constrained non-convex exploration by utilizing reflection steps within a bounded domain. Theoretically, we observe that reducing the diameter of the domain enhances mixing rates, exhibiting a \\emph{quadratic} behavior. Empirically, we test its performance through extensive experiments, including identifying dynamical systems with physical constraints, simulations of constrained multi-modal distributions, and image classification tasks. The theoretical and empirical findings highlight the crucial role of constrained exploration in improving the simulation efficiency.","sentences":["Replica exchange stochastic gradient Langevin dynamics (reSGLD) is an effective sampler for non-convex learning in large-scale datasets.","However, the simulation may encounter stagnation issues when the high-temperature chain delves too deeply into the distribution tails.","To tackle this issue, we propose reflected reSGLD (r2SGLD): an algorithm tailored for constrained non-convex exploration by utilizing reflection steps within a bounded domain.","Theoretically, we observe that reducing the diameter of the domain enhances mixing rates, exhibiting a \\emph{quadratic} behavior.","Empirically, we test its performance through extensive experiments, including identifying dynamical systems with physical constraints, simulations of constrained multi-modal distributions, and image classification tasks.","The theoretical and empirical findings highlight the crucial role of constrained exploration in improving the simulation efficiency."],"url":"http://arxiv.org/abs/2405.07839v1","category":"cs.LG"}
{"created":"2024-05-13 15:24:27","title":"Adaptive Exploration for Data-Efficient General Value Function Evaluations","abstract":"General Value Functions (GVFs) (Sutton et al, 2011) are an established way to represent predictive knowledge in reinforcement learning. Each GVF computes the expected return for a given policy, based on a unique pseudo-reward. Multiple GVFs can be estimated in parallel using off-policy learning from a single stream of data, often sourced from a fixed behavior policy or pre-collected dataset. This leaves an open question: how can behavior policy be chosen for data-efficient GVF learning? To address this gap, we propose GVFExplorer, which aims at learning a behavior policy that efficiently gathers data for evaluating multiple GVFs in parallel. This behavior policy selects actions in proportion to the total variance in the return across all GVFs, reducing the number of environmental interactions. To enable accurate variance estimation, we use a recently proposed temporal-difference-style variance estimator. We prove that each behavior policy update reduces the mean squared error in the summed predictions over all GVFs. We empirically demonstrate our method's performance in both tabular representations and nonlinear function approximation.","sentences":["General Value Functions (GVFs) (Sutton et al, 2011) are an established way to represent predictive knowledge in reinforcement learning.","Each GVF computes the expected return for a given policy, based on a unique pseudo-reward.","Multiple GVFs can be estimated in parallel using off-policy learning from a single stream of data, often sourced from a fixed behavior policy or pre-collected dataset.","This leaves an open question: how can behavior policy be chosen for data-efficient GVF learning?","To address this gap, we propose GVFExplorer, which aims at learning a behavior policy that efficiently gathers data for evaluating multiple GVFs in parallel.","This behavior policy selects actions in proportion to the total variance in the return across all GVFs, reducing the number of environmental interactions.","To enable accurate variance estimation, we use a recently proposed temporal-difference-style variance estimator.","We prove that each behavior policy update reduces the mean squared error in the summed predictions over all GVFs.","We empirically demonstrate our method's performance in both tabular representations and nonlinear function approximation."],"url":"http://arxiv.org/abs/2405.07838v1","category":"cs.LG"}
{"created":"2024-05-13 15:19:15","title":"Prospects for detection of the pair-echo emission from TeV gamma-ray bursts","abstract":"The intergalactic magnetic field (IGMF) present in the voids of large-scale structures is considered to be the weakest magnetic field in the Universe. Gamma-ray observations of blazars in the GeV-TeV domain have led to lower limits on the IGMF strength based on the search for delayed/extended emission. Nevertheless these results are obtained with strong assumptions on the unknown source properties. The recent discovery of TeV radiation from Gamma-Ray Bursts (GRBs) has paved the way for IGMF studies with these bright transients. Among the current TeV-detected GRBs, GRB 190114C, located at redshift $z = 0.42$, is the best sampled and therefore representative of the properties of GRBs in the VHE domain while GRB 221009A ($z = 0.151$) is the brightest event ever detected. We present a phenomenological model based on the intrinsic properties of GRB 190114C and GRB 221009A to predict the delayed emission component (pair-echo) in the GeV-TeV band. We investigate the detectability of this component from low-redshift ($z \\leq 1$) GRBs for three values of IGMF strength ($10^{-19}$ G, $10^{-18}$ G and $10^{-17}$ G), different observational times ($3$ hrs, $6$ hrs and $9$ hrs) and source intrinsic properties. We find that, for current and future generation $\\gamma$-ray instruments, extending the observation for at least 3 hours after the GRB detection is a viable strategy to probe IGMF. We also confirm that GeV-TeV observations of GRBs can probe IGMF strengths at the order of $10^{-17} -10^{-19}$ G representing a competitive alternative to the current studies performed with AGNs.","sentences":["The intergalactic magnetic field (IGMF) present in the voids of large-scale structures is considered to be the weakest magnetic field in the Universe.","Gamma-ray observations of blazars in the GeV-TeV domain have led to lower limits on the IGMF strength based on the search for delayed/extended emission.","Nevertheless these results are obtained with strong assumptions on the unknown source properties.","The recent discovery of TeV radiation from Gamma-Ray Bursts (GRBs) has paved the way for IGMF studies with these bright transients.","Among the current TeV-detected GRBs, GRB 190114C, located at redshift $z = 0.42$, is the best sampled and therefore representative of the properties of GRBs in the VHE domain while GRB 221009A ($z = 0.151$) is the brightest event ever detected.","We present a phenomenological model based on the intrinsic properties of GRB 190114C and GRB 221009A to predict the delayed emission component (pair-echo) in the GeV-TeV band.","We investigate the detectability of this component from low-redshift ($z \\leq 1$) GRBs for three values of IGMF strength ($10^{-19}$ G, $10^{-18}$ G and $10^{-17}$ G), different observational times ($3$ hrs, $6$ hrs and $9$ hrs) and source intrinsic properties.","We find that, for current and future generation $\\gamma$-ray instruments, extending the observation for at least 3 hours after the GRB detection is a viable strategy to probe IGMF.","We also confirm that GeV-TeV observations of GRBs can probe IGMF strengths at the order of $10^{-17} -10^{-19}$ G representing a competitive alternative to the current studies performed with AGNs."],"url":"http://arxiv.org/abs/2405.07831v1","category":"astro-ph.HE"}
{"created":"2024-05-13 15:15:37","title":"Joint Precoding for RIS-Assisted Wideband THz Cell-Free Massive MIMO Systems","abstract":"Terahertz (THz) cell-free massive multiple-input-multiple-output (mMIMO) networks have been envisioned as a prospective technology for achieving higher system capacity, improved performance, and ultra-high reliability in 6G networks. However, due to severe attenuation and limited scattering in THz transmission, as well as high power consumption for increased number of access points (APs), further improvement of network capacity becomes challenging. Reconfigurable intelligent surface (RIS) has been introduced as a low-cost solution to reduce AP deployment and assist in data transmission. However, due to the ultra-wide bandwidth and frequency-dependent characteristics of RISs, beam split effect has become an unavoidable obstacle. To compensate the severe performance degradation caused by beam split effect, we introduce additional time delay (TD) layers at both access points (APs) and RISs. Accordingly, we propose a joint precoding framework at APs and RISs to fully unleash the potential of the considered network. Specifically, we first formulate the joint precoding as a non-convex optimization problem. Then, given the location of unchanged RISs, we adjust the time delays (TDs) of APs to align the generated beams towards RISs. After that, with knowledge of the optimal TDs of APs, we decouple the optimization problem into three subproblems of optimizing the baseband beamformers, RISs and TDs of RISs, respectively. Exploiting multidimensional complex quadratic transform, we transform the subproblems into convex forms and solve them under alternate optimizing framework. Numerical results verify that the proposed method can effectively mitigate beam split effect and significantly improve the achievable rate compared with conventional cell-free mMIMO networks.","sentences":["Terahertz (THz) cell-free massive multiple-input-multiple-output (mMIMO) networks have been envisioned as a prospective technology for achieving higher system capacity, improved performance, and ultra-high reliability in 6G networks.","However, due to severe attenuation and limited scattering in THz transmission, as well as high power consumption for increased number of access points (APs), further improvement of network capacity becomes challenging.","Reconfigurable intelligent surface (RIS) has been introduced as a low-cost solution to reduce AP deployment and assist in data transmission.","However, due to the ultra-wide bandwidth and frequency-dependent characteristics of RISs, beam split effect has become an unavoidable obstacle.","To compensate the severe performance degradation caused by beam split effect, we introduce additional time delay (TD) layers at both access points (APs) and RISs.","Accordingly, we propose a joint precoding framework at APs and RISs to fully unleash the potential of the considered network.","Specifically, we first formulate the joint precoding as a non-convex optimization problem.","Then, given the location of unchanged RISs, we adjust the time delays (TDs) of APs to align the generated beams towards RISs.","After that, with knowledge of the optimal TDs of APs, we decouple the optimization problem into three subproblems of optimizing the baseband beamformers, RISs and TDs of RISs, respectively.","Exploiting multidimensional complex quadratic transform, we transform the subproblems into convex forms and solve them under alternate optimizing framework.","Numerical results verify that the proposed method can effectively mitigate beam split effect and significantly improve the achievable rate compared with conventional cell-free mMIMO networks."],"url":"http://arxiv.org/abs/2405.07830v1","category":"eess.SP"}
{"created":"2024-05-13 15:12:21","title":"Automatic Recognition of Food Ingestion Environment from the AIM-2 Wearable Sensor","abstract":"Detecting an ingestion environment is an important aspect of monitoring dietary intake. It provides insightful information for dietary assessment. However, it is a challenging problem where human-based reviewing can be tedious, and algorithm-based review suffers from data imbalance and perceptual aliasing problems. To address these issues, we propose a neural network-based method with a two-stage training framework that tactfully combines fine-tuning and transfer learning techniques. Our method is evaluated on a newly collected dataset called ``UA Free Living Study\", which uses an egocentric wearable camera, AIM-2 sensor, to simulate food consumption in free-living conditions. The proposed training framework is applied to common neural network backbones, combined with approaches in the general imbalanced classification field. Experimental results on the collected dataset show that our proposed method for automatic ingestion environment recognition successfully addresses the challenging data imbalance problem in the dataset and achieves a promising overall classification accuracy of 96.63%.","sentences":["Detecting an ingestion environment is an important aspect of monitoring dietary intake.","It provides insightful information for dietary assessment.","However, it is a challenging problem where human-based reviewing can be tedious, and algorithm-based review suffers from data imbalance and perceptual aliasing problems.","To address these issues, we propose a neural network-based method with a two-stage training framework that tactfully combines fine-tuning and transfer learning techniques.","Our method is evaluated on a newly collected dataset called ``UA Free Living Study\", which uses an egocentric wearable camera, AIM-2 sensor, to simulate food consumption in free-living conditions.","The proposed training framework is applied to common neural network backbones, combined with approaches in the general imbalanced classification field.","Experimental results on the collected dataset show that our proposed method for automatic ingestion environment recognition successfully addresses the challenging data imbalance problem in the dataset and achieves a promising overall classification accuracy of 96.63%."],"url":"http://arxiv.org/abs/2405.07827v1","category":"cs.MM"}
{"created":"2024-05-13 15:09:45","title":"Stability of the Cosmological Dynamics of $O(D,D)$-complete Stringy Gravity","abstract":"The massless fields in the universal NS-NS sector of string theory form $O(D, D)$ multiplets of Double Field Theory, which is a theory that provides a T-duality covariant formulation of supergravity, leading to a stringy modification of General Relativity. In this framework, it is possible to write down the extensions of the Einstein field equations and the Friedmann equations in such a way that the coupling of gravitational and matter sectors is dictated by the $O(D, D)$ symmetry universally. In this paper, we obtain the autonomous form of the $O(D, D)$-complete Friedmann equations, find the critical points and perform their stability analysis. We also include the phase portraits of the system. Cosmologically interesting cases of scalar field, radiation, and matter are separately considered and compared with the Chameleon models in a similar setting. Accelerating phases and the conditions for their existence are also given for such cases.","sentences":["The massless fields in the universal NS-NS sector of string theory form $O(D, D)$ multiplets of Double Field Theory, which is a theory that provides a T-duality covariant formulation of supergravity, leading to a stringy modification of General Relativity.","In this framework, it is possible to write down the extensions of the Einstein field equations and the Friedmann equations in such a way that the coupling of gravitational and matter sectors is dictated by the $O(D, D)$ symmetry universally.","In this paper, we obtain the autonomous form of the $O(D, D)$-complete Friedmann equations, find the critical points and perform their stability analysis.","We also include the phase portraits of the system.","Cosmologically interesting cases of scalar field, radiation, and matter are separately considered and compared with the Chameleon models in a similar setting.","Accelerating phases and the conditions for their existence are also given for such cases."],"url":"http://arxiv.org/abs/2405.07825v1","category":"gr-qc"}
{"created":"2024-05-13 15:09:08","title":"Fixed Point Theory Analysis of a Lambda Policy Iteration with Randomization for the \u0106iri\u0107 Contraction Operator","abstract":"We apply methods of the fixed point theory to a Lambda policy iteration with a randomization algorithm for weak contractions mappings. This type of mappings covers a broader range than the strong contractions typically considered in the literature, such as \\'Ciri\\'c contraction. Specifically, we explore the characteristics of reinforcement learning procedures developed for feedback control within the context of fixed point theory. Under relatively general assumptions, we identify the sufficient conditions for convergence with a probability of one in infinite-dimensional policy spaces.","sentences":["We apply methods of the fixed point theory to a Lambda policy iteration with a randomization algorithm for weak contractions mappings.","This type of mappings covers a broader range than the strong contractions typically considered in the literature, such as \\'Ciri\\'c contraction.","Specifically, we explore the characteristics of reinforcement learning procedures developed for feedback control within the context of fixed point theory.","Under relatively general assumptions, we identify the sufficient conditions for convergence with a probability of one in infinite-dimensional policy spaces."],"url":"http://arxiv.org/abs/2405.07824v1","category":"math.OC"}
{"created":"2024-05-13 15:08:02","title":"Integrating Multi-Physics Simulations and Machine Learning to Define the Spatter Mechanism and Process Window in Laser Powder Bed Fusion","abstract":"Laser powder bed fusion (LPBF) has shown promise for wide range of applications due to its ability to fabricate freeform geometries and generate a controlled microstructure. However, components generated by LPBF still possess sub-optimal mechanical properties due to the defects that are created during laser-material interactions. In this work, we investigate mechanism of spatter formation, using a high-fidelity modelling tool that was built to simulate the multi-physics phenomena in LPBF. The modelling tool have the capability to capture the 3D resolution of the meltpool and the spatter behavior. To understand spatter behavior and formation, we reveal its properties at ejection and evaluate its variation from the meltpool, the source where it is formed. The dataset of the spatter and the meltpool collected consist of 50 % spatter and 50 % melt pool samples, with features that include position components, velocity components, velocity magnitude, temperature, density and pressure. The relationship between the spatter and the meltpool were evaluated via correlation analysis and machine learning (ML) algorithms for classification tasks. Upon screening different ML algorithms on the dataset, a high accuracy was observed for all the ML models, with ExtraTrees having the highest at 96 % and KNN having the lowest at 94 %.","sentences":["Laser powder bed fusion (LPBF) has shown promise for wide range of applications due to its ability to fabricate freeform geometries and generate a controlled microstructure.","However, components generated by LPBF still possess sub-optimal mechanical properties due to the defects that are created during laser-material interactions.","In this work, we investigate mechanism of spatter formation, using a high-fidelity modelling tool that was built to simulate the multi-physics phenomena in LPBF.","The modelling tool have the capability to capture the 3D resolution of the meltpool and the spatter behavior.","To understand spatter behavior and formation, we reveal its properties at ejection and evaluate its variation from the meltpool, the source where it is formed.","The dataset of the spatter and the meltpool collected consist of 50 % spatter and 50 % melt pool samples, with features that include position components, velocity components, velocity magnitude, temperature, density and pressure.","The relationship between the spatter and the meltpool were evaluated via correlation analysis and machine learning (ML) algorithms for classification tasks.","Upon screening different ML algorithms on the dataset, a high accuracy was observed for all the ML models, with ExtraTrees having the highest at 96 % and KNN having the lowest at 94 %."],"url":"http://arxiv.org/abs/2405.07823v1","category":"cs.LG"}
{"created":"2024-05-13 15:07:52","title":"Synthetic Tabular Data Validation: A Divergence-Based Approach","abstract":"The ever-increasing use of generative models in various fields where tabular data is used highlights the need for robust and standardized validation metrics to assess the similarity between real and synthetic data. Current methods lack a unified framework and rely on diverse and often inconclusive statistical measures. Divergences, which quantify discrepancies between data distributions, offer a promising avenue for validation. However, traditional approaches calculate divergences independently for each feature due to the complexity of joint distribution modeling. This paper addresses this challenge by proposing a novel approach that uses divergence estimation to overcome the limitations of marginal comparisons. Our core contribution lies in applying a divergence estimator to build a validation metric considering the joint distribution of real and synthetic data. We leverage a probabilistic classifier to approximate the density ratio between datasets, allowing the capture of complex relationships. We specifically calculate two divergences: the well-known Kullback-Leibler (KL) divergence and the Jensen-Shannon (JS) divergence. KL divergence offers an established use in the field, while JS divergence is symmetric and bounded, providing a reliable metric. The efficacy of this approach is demonstrated through a series of experiments with varying distribution complexities. The initial phase involves comparing estimated divergences with analytical solutions for simple distributions, setting a benchmark for accuracy. Finally, we validate our method on a real-world dataset and its corresponding synthetic counterpart, showcasing its effectiveness in practical applications. This research offers a significant contribution with applicability beyond tabular data and the potential to improve synthetic data validation in various fields.","sentences":["The ever-increasing use of generative models in various fields where tabular data is used highlights the need for robust and standardized validation metrics to assess the similarity between real and synthetic data.","Current methods lack a unified framework and rely on diverse and often inconclusive statistical measures.","Divergences, which quantify discrepancies between data distributions, offer a promising avenue for validation.","However, traditional approaches calculate divergences independently for each feature due to the complexity of joint distribution modeling.","This paper addresses this challenge by proposing a novel approach that uses divergence estimation to overcome the limitations of marginal comparisons.","Our core contribution lies in applying a divergence estimator to build a validation metric considering the joint distribution of real and synthetic data.","We leverage a probabilistic classifier to approximate the density ratio between datasets, allowing the capture of complex relationships.","We specifically calculate two divergences: the well-known Kullback-Leibler (KL) divergence and the Jensen-Shannon (JS) divergence.","KL divergence offers an established use in the field, while JS divergence is symmetric and bounded, providing a reliable metric.","The efficacy of this approach is demonstrated through a series of experiments with varying distribution complexities.","The initial phase involves comparing estimated divergences with analytical solutions for simple distributions, setting a benchmark for accuracy.","Finally, we validate our method on a real-world dataset and its corresponding synthetic counterpart, showcasing its effectiveness in practical applications.","This research offers a significant contribution with applicability beyond tabular data and the potential to improve synthetic data validation in various fields."],"url":"http://arxiv.org/abs/2405.07822v1","category":"cs.LG"}
{"created":"2024-05-13 14:59:44","title":"The Power of Combined Modalities in Interactive Robot Learning","abstract":"This study contributes to the evolving field of robot learning in interaction with humans, examining the impact of diverse input modalities on learning outcomes. It introduces the concept of \"meta-modalities\" which encapsulate additional forms of feedback beyond the traditional preference and scalar feedback mechanisms. Unlike prior research that focused on individual meta-modalities, this work evaluates their combined effect on learning outcomes. Through a study with human participants, we explore user preferences for these modalities and their impact on robot learning performance. Our findings reveal that while individual modalities are perceived differently, their combination significantly improves learning behavior and usability. This research not only provides valuable insights into the optimization of human-robot interactive task learning but also opens new avenues for enhancing the interactive freedom and scaffolding capabilities provided to users in such settings.","sentences":["This study contributes to the evolving field of robot learning in interaction with humans, examining the impact of diverse input modalities on learning outcomes.","It introduces the concept of \"meta-modalities\" which encapsulate additional forms of feedback beyond the traditional preference and scalar feedback mechanisms.","Unlike prior research that focused on individual meta-modalities, this work evaluates their combined effect on learning outcomes.","Through a study with human participants, we explore user preferences for these modalities and their impact on robot learning performance.","Our findings reveal that while individual modalities are perceived differently, their combination significantly improves learning behavior and usability.","This research not only provides valuable insights into the optimization of human-robot interactive task learning but also opens new avenues for enhancing the interactive freedom and scaffolding capabilities provided to users in such settings."],"url":"http://arxiv.org/abs/2405.07817v1","category":"cs.RO"}
{"created":"2024-05-13 14:58:57","title":"Quick and Accurate Affordance Learning","abstract":"Infants learn actively in their environments, shaping their own learning curricula. They learn about their environments' affordances, that is, how local circumstances determine how their behavior can affect the environment. Here we model this type of behavior by means of a deep learning architecture. The architecture mediates between global cognitive map exploration and local affordance learning. Inference processes actively move the simulated agent towards regions where they expect affordance-related knowledge gain. We contrast three measures of uncertainty to guide this exploration: predicted uncertainty of a model, standard deviation between the means of several models (SD), and the Jensen-Shannon Divergence (JSD) between several models. We show that the first measure gets fooled by aleatoric uncertainty inherent in the environment, while the two other measures focus learning on epistemic uncertainty. JSD exhibits the most balanced exploration strategy. From a computational perspective, our model suggests three key ingredients for coordinating the active generation of learning curricula: (1) Navigation behavior needs to be coordinated with local motor behavior for enabling active affordance learning. (2) Affordances need to be encoded locally for acquiring generalized knowledge. (3) Effective active affordance learning mechanisms should use density comparison techniques for estimating expected knowledge gain. Future work may seek collaborations with developmental psychology to model active play in children in more realistic scenarios.","sentences":["Infants learn actively in their environments, shaping their own learning curricula.","They learn about their environments' affordances, that is, how local circumstances determine how their behavior can affect the environment.","Here we model this type of behavior by means of a deep learning architecture.","The architecture mediates between global cognitive map exploration and local affordance learning.","Inference processes actively move the simulated agent towards regions where they expect affordance-related knowledge gain.","We contrast three measures of uncertainty to guide this exploration: predicted uncertainty of a model, standard deviation between the means of several models (SD), and the Jensen-Shannon Divergence (JSD) between several models.","We show that the first measure gets fooled by aleatoric uncertainty inherent in the environment, while the two other measures focus learning on epistemic uncertainty.","JSD exhibits the most balanced exploration strategy.","From a computational perspective, our model suggests three key ingredients for coordinating the active generation of learning curricula: (1) Navigation behavior needs to be coordinated with local motor behavior for enabling active affordance learning.","(2) Affordances need to be encoded locally for acquiring generalized knowledge.","(3) Effective active affordance learning mechanisms should use density comparison techniques for estimating expected knowledge gain.","Future work may seek collaborations with developmental psychology to model active play in children in more realistic scenarios."],"url":"http://arxiv.org/abs/2405.07816v1","category":"cs.AI"}
{"created":"2024-05-13 14:56:55","title":"NutritionVerse-Direct: Exploring Deep Neural Networks for Multitask Nutrition Prediction from Food Images","abstract":"Many aging individuals encounter challenges in effectively tracking their dietary intake, exacerbating their susceptibility to nutrition-related health complications. Self-reporting methods are often inaccurate and suffer from substantial bias; however, leveraging intelligent prediction methods can automate and enhance precision in this process. Recent work has explored using computer vision prediction systems to predict nutritional information from food images. Still, these methods are often tailored to specific situations, require other inputs in addition to a food image, or do not provide comprehensive nutritional information.   This paper aims to enhance the efficacy of dietary intake estimation by leveraging various neural network architectures to directly predict a meal's nutritional content from its image. Through comprehensive experimentation and evaluation, we present NutritionVerse-Direct, a model utilizing a vision transformer base architecture with three fully connected layers that lead to five regression heads predicting calories (kcal), mass (g), protein (g), fat (g), and carbohydrates (g) present in a meal. NutritionVerse-Direct yields a combined mean average error score on the NutritionVerse-Real dataset of 412.6, an improvement of 25.5% over the Inception-ResNet model, demonstrating its potential for improving dietary intake estimation accuracy.","sentences":["Many aging individuals encounter challenges in effectively tracking their dietary intake, exacerbating their susceptibility to nutrition-related health complications.","Self-reporting methods are often inaccurate and suffer from substantial bias; however, leveraging intelligent prediction methods can automate and enhance precision in this process.","Recent work has explored using computer vision prediction systems to predict nutritional information from food images.","Still, these methods are often tailored to specific situations, require other inputs in addition to a food image, or do not provide comprehensive nutritional information.   ","This paper aims to enhance the efficacy of dietary intake estimation by leveraging various neural network architectures to directly predict a meal's nutritional content from its image.","Through comprehensive experimentation and evaluation, we present NutritionVerse-Direct, a model utilizing a vision transformer base architecture with three fully connected layers that lead to five regression heads predicting calories (kcal), mass (g), protein (g), fat (g), and carbohydrates (g) present in a meal.","NutritionVerse-Direct yields a combined mean average error score on the NutritionVerse-Real dataset of 412.6, an improvement of 25.5% over the Inception-ResNet model, demonstrating its potential for improving dietary intake estimation accuracy."],"url":"http://arxiv.org/abs/2405.07814v1","category":"cs.CV"}
{"created":"2024-05-13 14:54:37","title":"Localizing Task Information for Improved Model Merging and Compression","abstract":"Model merging and task arithmetic have emerged as promising scalable approaches to merge multiple single-task checkpoints to one multi-task model, but their applicability is reduced by significant performance loss. Previous works have linked these drops to interference in the weight space and erasure of important task-specific features. Instead, in this work we show that the information required to solve each task is still preserved after merging as different tasks mostly use non-overlapping sets of weights. We propose TALL-masks, a method to identify these task supports given a collection of task vectors and show that one can retrieve >99% of the single task accuracy by applying our masks to the multi-task vector, effectively compressing the individual checkpoints. We study the statistics of intersections among constructed masks and reveal the existence of selfish and catastrophic weights, i.e., parameters that are important exclusively to one task and irrelevant to all tasks but detrimental to multi-task fusion. For this reason, we propose Consensus Merging, an algorithm that eliminates such weights and improves the general performance of existing model merging approaches. Our experiments in vision and NLP benchmarks with up to 20 tasks, show that Consensus Merging consistently improves existing approaches. Furthermore, our proposed compression scheme reduces storage from 57Gb to 8.2Gb while retaining 99.7% of original performance.","sentences":["Model merging and task arithmetic have emerged as promising scalable approaches to merge multiple single-task checkpoints to one multi-task model, but their applicability is reduced by significant performance loss.","Previous works have linked these drops to interference in the weight space and erasure of important task-specific features.","Instead, in this work we show that the information required to solve each task is still preserved after merging as different tasks mostly use non-overlapping sets of weights.","We propose TALL-masks, a method to identify these task supports given a collection of task vectors and show that one can retrieve >99% of the single task accuracy by applying our masks to the multi-task vector, effectively compressing the individual checkpoints.","We study the statistics of intersections among constructed masks and reveal the existence of selfish and catastrophic weights, i.e., parameters that are important exclusively to one task and irrelevant to all tasks but detrimental to multi-task fusion.","For this reason, we propose Consensus Merging, an algorithm that eliminates such weights and improves the general performance of existing model merging approaches.","Our experiments in vision and NLP benchmarks with up to 20 tasks, show that Consensus Merging consistently improves existing approaches.","Furthermore, our proposed compression scheme reduces storage from 57Gb to 8.2Gb while retaining 99.7% of original performance."],"url":"http://arxiv.org/abs/2405.07813v1","category":"cs.LG"}
{"created":"2024-05-13 14:54:23","title":"Electromagnetic Nanonetworks Beyond 6G: From Wearable and Implantable Networks to On-chip and Quantum Communication","abstract":"Emerging from the symbiotic combination of nanotechnology and communications, the field of nanonetworking has come a long way since its inception more than fifteen years ago. Significant progress has been achieved in several key communication technologies as enablers of the paradigm, as well as in the multiple application areas that it opens. In this paper, the focus is placed on the electromagnetic nanonetworking paradigm, providing an overview of the advances made in wireless nanocommunication technology from microwave through terahertz to optical bands. The characteristics and potential of the compared technologies are then confronted with the requirements and challenges of the broad set of nanonetworking applications in the Internet of NanoThings (IoNT) and on-chip networks paradigms, including quantum computing applications for the first time. Finally, a selection of cross-cutting issues and possible directions for future work are given, aiming to guide researchers and practitioners towards the next generation of electromagnetic nanonetworks.","sentences":["Emerging from the symbiotic combination of nanotechnology and communications, the field of nanonetworking has come a long way since its inception more than fifteen years ago.","Significant progress has been achieved in several key communication technologies as enablers of the paradigm, as well as in the multiple application areas that it opens.","In this paper, the focus is placed on the electromagnetic nanonetworking paradigm, providing an overview of the advances made in wireless nanocommunication technology from microwave through terahertz to optical bands.","The characteristics and potential of the compared technologies are then confronted with the requirements and challenges of the broad set of nanonetworking applications in the Internet of NanoThings (IoNT) and on-chip networks paradigms, including quantum computing applications for the first time.","Finally, a selection of cross-cutting issues and possible directions for future work are given, aiming to guide researchers and practitioners towards the next generation of electromagnetic nanonetworks."],"url":"http://arxiv.org/abs/2405.07812v1","category":"cs.ET"}
{"created":"2024-05-13 14:52:48","title":"On the quadratic stability of asymmetric Hermite basis and application to plasma physics","abstract":"We analyze why the discretization of linear transport with asymmetric Hermite basis functions can be instable in quadratic norm. The main reason is that the finite truncation of the infinite moment linear system looses the skew-symmetry property with respect to the Gram matrix. Then we propose an original closed formula for the scalar product of any pair of asymmetric basis functions. It makes possible the construction of two simple modifications of the linear systems which recover the skew-symmetry property. By construction the new methods are quadratically stable with respect to the natural $L^2$ norm. We explain how to generalize to other transport equations encountered in numerical plasma physics. Basic numerical tests illustrate the unconditional stability properties of our algorithms.","sentences":["We analyze why the discretization of linear transport with asymmetric Hermite basis functions can be instable in quadratic norm.","The main reason is that the finite truncation of the infinite moment linear system looses the skew-symmetry property with respect to the Gram matrix.","Then we propose an original closed formula for the scalar product of any pair of asymmetric basis functions.","It makes possible the construction of two simple modifications of the linear systems which recover the skew-symmetry property.","By construction the new methods are quadratically stable with respect to the natural $L^2$ norm.","We explain how to generalize to other transport equations encountered in numerical plasma physics.","Basic numerical tests illustrate the unconditional stability properties of our algorithms."],"url":"http://arxiv.org/abs/2405.07811v1","category":"math.NA"}
{"created":"2024-05-13 14:49:03","title":"Design of an ultra-compact, energy-efficient non-volatile photonic switch based on phase change materials","abstract":"The on-chip photonic switch is a critical building block for photonic integrated circuits (PICs) and the integration of phase change materials (PCMs) enables non-volatile switch designs that are compact, low-loss, and energy-efficient. Existing switch designs based on these materials typically rely on weak evanescent field interactions, resulting in devices with a large footprint and high energy consumption. Here we present a compact non-volatile 2 by 2 switch design leveraging optical concentration in slot waveguide modes to significantly enhance interactions of light with PCMs, thereby realizing a compact, efficient photonic switch. To further improve the device's energy efficiency, we introduce an integrated single-layer graphene heater for ultrafast electrothermal switching of the PCM. Computational simulations demonstrate a 2 by 2 switch with crosstalk (CT) down to -24 dB at 1550 nm wavelength and more than 55 nm 0.3 dB insertion loss (IL) bandwidth. The proposed photonic switch architecture can constitute the cornerstone for next-generation high-performance reconfigurable photonic circuits.","sentences":["The on-chip photonic switch is a critical building block for photonic integrated circuits (PICs) and the integration of phase change materials (PCMs) enables non-volatile switch designs that are compact, low-loss, and energy-efficient.","Existing switch designs based on these materials typically rely on weak evanescent field interactions, resulting in devices with a large footprint and high energy consumption.","Here we present a compact non-volatile 2 by 2 switch design leveraging optical concentration in slot waveguide modes to significantly enhance interactions of light with PCMs, thereby realizing a compact, efficient photonic switch.","To further improve the device's energy efficiency, we introduce an integrated single-layer graphene heater for ultrafast electrothermal switching of the PCM.","Computational simulations demonstrate a 2 by 2 switch with crosstalk (CT) down to -24 dB at 1550 nm wavelength and more than 55 nm 0.3 dB insertion loss (IL) bandwidth.","The proposed photonic switch architecture can constitute the cornerstone for next-generation high-performance reconfigurable photonic circuits."],"url":"http://arxiv.org/abs/2405.07809v1","category":"physics.optics"}
{"created":"2024-05-13 14:45:08","title":"Decoding Geometric Properties in Non-Random Data from First Information-Theoretic Principles","abstract":"Based on the principles of information theory, measure theory, and theoretical computer science, we introduce a univariate signal deconvolution method with a wide range of applications to coding theory, particularly in zero-knowledge one-way communication channels, such as in deciphering messages from unknown generating sources about which no prior knowledge is available and to which no return message can be sent. Our multidimensional space reconstruction method from an arbitrary received signal is proven to be agnostic vis-a-vis the encoding-decoding scheme, computation model, programming language, formal theory, the computable (or semi-computable) method of approximation to algorithmic complexity, and any arbitrarily chosen (computable) probability measure of the events. The method derives from the principles of an approach to Artificial General Intelligence capable of building a general-purpose model of models independent of any arbitrarily assumed prior probability distribution. We argue that this optimal and universal method of decoding non-random data has applications to signal processing, causal deconvolution, topological and geometric properties encoding, cryptography, and bio- and technosignature detection.","sentences":["Based on the principles of information theory, measure theory, and theoretical computer science, we introduce a univariate signal deconvolution method with a wide range of applications to coding theory, particularly in zero-knowledge one-way communication channels, such as in deciphering messages from unknown generating sources about which no prior knowledge is available and to which no return message can be sent.","Our multidimensional space reconstruction method from an arbitrary received signal is proven to be agnostic vis-a-vis the encoding-decoding scheme, computation model, programming language, formal theory, the computable (or semi-computable) method of approximation to algorithmic complexity, and any arbitrarily chosen (computable) probability measure of the events.","The method derives from the principles of an approach to Artificial General Intelligence capable of building a general-purpose model of models independent of any arbitrarily assumed prior probability distribution.","We argue that this optimal and universal method of decoding non-random data has applications to signal processing, causal deconvolution, topological and geometric properties encoding, cryptography, and bio- and technosignature detection."],"url":"http://arxiv.org/abs/2405.07803v1","category":"cs.IT"}
{"created":"2024-05-13 14:44:22","title":"Deep Learning-Based Object Pose Estimation: A Comprehensive Survey","abstract":"Object pose estimation is a fundamental computer vision problem with broad applications in augmented reality and robotics. Over the past decade, deep learning models, due to their superior accuracy and robustness, have increasingly supplanted conventional algorithms reliant on engineered point pair features. Nevertheless, several challenges persist in contemporary methods, including their dependency on labeled training data, model compactness, robustness under challenging conditions, and their ability to generalize to novel unseen objects. A recent survey discussing the progress made on different aspects of this area, outstanding challenges, and promising future directions, is missing. To fill this gap, we discuss the recent advances in deep learning-based object pose estimation, covering all three formulations of the problem, i.e., instance-level, category-level, and unseen object pose estimation. Our survey also covers multiple input data modalities, degrees-of-freedom of output poses, object properties, and downstream tasks, providing readers with a holistic understanding of this field. Additionally, it discusses training paradigms of different domains, inference modes, application areas, evaluation metrics, and benchmark datasets, as well as reports the performance of current state-of-the-art methods on these benchmarks, thereby facilitating readers in selecting the most suitable method for their application. Finally, the survey identifies key challenges, reviews prevailing trends along with their pros and cons, and identifies promising directions for future research. We also keep tracing the latest works at https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation.","sentences":["Object pose estimation is a fundamental computer vision problem with broad applications in augmented reality and robotics.","Over the past decade, deep learning models, due to their superior accuracy and robustness, have increasingly supplanted conventional algorithms reliant on engineered point pair features.","Nevertheless, several challenges persist in contemporary methods, including their dependency on labeled training data, model compactness, robustness under challenging conditions, and their ability to generalize to novel unseen objects.","A recent survey discussing the progress made on different aspects of this area, outstanding challenges, and promising future directions, is missing.","To fill this gap, we discuss the recent advances in deep learning-based object pose estimation, covering all three formulations of the problem, i.e., instance-level, category-level, and unseen object pose estimation.","Our survey also covers multiple input data modalities, degrees-of-freedom of output poses, object properties, and downstream tasks, providing readers with a holistic understanding of this field.","Additionally, it discusses training paradigms of different domains, inference modes, application areas, evaluation metrics, and benchmark datasets, as well as reports the performance of current state-of-the-art methods on these benchmarks, thereby facilitating readers in selecting the most suitable method for their application.","Finally, the survey identifies key challenges, reviews prevailing trends along with their pros and cons, and identifies promising directions for future research.","We also keep tracing the latest works at https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation."],"url":"http://arxiv.org/abs/2405.07801v1","category":"cs.CV"}
{"created":"2024-05-13 14:42:13","title":"FreeVA: Offline MLLM as Training-Free Video Assistant","abstract":"This paper undertakes an empirical study to revisit the latest advancements in Multimodal Large Language Models (MLLMs): Video Assistant. This study, namely FreeVA, aims to extend existing image-based MLLM to the video domain in a training-free manner. The study provides an essential, yet must-know baseline, and reveals several surprising findings: 1) FreeVA, leveraging only offline image-based MLLM without additional training, excels in zero-shot video question-answering (e.g., MSVD-QA, ActivityNet-QA, and MSRVTT-QA), even surpassing state-of-the-art methods that involve video instruction tuning. 2) While mainstream video-based MLLMs typically initialize with an image-based MLLM (e.g., LLaVA) and then fine-tune using video instruction tuning, the study indicates that utilizing the widely adopted VideoInstruct-100K for video instruction tuning doesn't actually lead to better performance compared to not training at all. 3) The commonly used evaluation metrics in existing works are significantly influenced by changes in the GPT API version over time. If ignored, this could affect the fairness and uniformity of comparisons between different methods and impact the analysis and judgment of researchers in the field. The advancement of MLLMs is currently thriving, drawing numerous researchers into the field. We aim for this work to serve as a plug-and-play, simple yet effective baseline, encouraging the direct evaluation of existing MLLMs in video domain while also standardizing the field of video conversational models to a certain extent. Also, we encourage researchers to reconsider: Have current video MLLM methods truly acquired knowledge beyond image MLLM? Code is available at https://github.com/whwu95/FreeVA","sentences":["This paper undertakes an empirical study to revisit the latest advancements in Multimodal Large Language Models (MLLMs): Video Assistant.","This study, namely FreeVA, aims to extend existing image-based MLLM to the video domain in a training-free manner.","The study provides an essential, yet must-know baseline, and reveals several surprising findings: 1) FreeVA, leveraging only offline image-based MLLM without additional training, excels in zero-shot video question-answering (e.g., MSVD-QA, ActivityNet-QA, and MSRVTT-QA), even surpassing state-of-the-art methods that involve video instruction tuning.","2) While mainstream video-based MLLMs typically initialize with an image-based MLLM (e.g., LLaVA) and then fine-tune using video instruction tuning, the study indicates that utilizing the widely adopted VideoInstruct-100K for video instruction tuning doesn't actually lead to better performance compared to not training at all.","3) The commonly used evaluation metrics in existing works are significantly influenced by changes in the GPT API version over time.","If ignored, this could affect the fairness and uniformity of comparisons between different methods and impact the analysis and judgment of researchers in the field.","The advancement of MLLMs is currently thriving, drawing numerous researchers into the field.","We aim for this work to serve as a plug-and-play, simple yet effective baseline, encouraging the direct evaluation of existing MLLMs in video domain while also standardizing the field of video conversational models to a certain extent.","Also, we encourage researchers to reconsider: Have current video MLLM methods truly acquired knowledge beyond image MLLM?","Code is available at https://github.com/whwu95/FreeVA"],"url":"http://arxiv.org/abs/2405.07798v1","category":"cs.CV"}
{"created":"2024-05-13 14:38:35","title":"Optimal Matrix Sketching over Sliding Windows","abstract":"Matrix sketching, aimed at approximating a matrix $\\boldsymbol{A} \\in \\mathbb{R}^{N\\times d}$ consisting of vector streams of length $N$ with a smaller sketching matrix $\\boldsymbol{B} \\in \\mathbb{R}^{\\ell\\times d}, \\ell \\ll N$, has garnered increasing attention in fields such as large-scale data analytics and machine learning. A well-known deterministic matrix sketching method is the Frequent Directions algorithm, which achieves the optimal $O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound and provides a covariance error guarantee of $\\varepsilon = \\lVert \\boldsymbol{A}^\\top \\boldsymbol{A} - \\boldsymbol{B}^\\top \\boldsymbol{B} \\rVert_2/\\lVert \\boldsymbol{A} \\rVert_F^2$. The matrix sketching problem becomes particularly interesting in the context of sliding windows, where the goal is to approximate the matrix $\\boldsymbol{A}_W$, formed by input vectors over the most recent $N$ time units. However, despite recent efforts, whether achieving the optimal $O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound on sliding windows is possible has remained an open question.   In this paper, we introduce the DS-FD algorithm, which achieves the optimal $O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound for matrix sketching over row-normalized, sequence-based sliding windows. We also present matching upper and lower space bounds for time-based and unnormalized sliding windows, demonstrating the generality and optimality of \\dsfd across various sliding window models. This conclusively answers the open question regarding the optimal space bound for matrix sketching over sliding windows. Furthermore, we conduct extensive experiments with both synthetic and real-world datasets, validating our theoretical claims and thus confirming the correctness and effectiveness of our algorithm, both theoretically and empirically.","sentences":["Matrix sketching, aimed at approximating a matrix $\\boldsymbol{A} \\in \\mathbb{R}^{N\\times d}$ consisting of vector streams of length $N$ with a smaller sketching matrix $\\boldsymbol{B} \\in \\mathbb{R}^{\\ell\\times d}, \\ell \\ll N$, has garnered increasing attention in fields such as large-scale data analytics and machine learning.","A well-known deterministic matrix sketching method is the Frequent Directions algorithm, which achieves the optimal $O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound and provides a covariance error guarantee of $\\varepsilon = \\lVert \\boldsymbol{A}^\\top \\boldsymbol{A} - \\boldsymbol{B}^\\top \\boldsymbol{B} \\rVert_2/\\lVert \\boldsymbol{A} \\rVert_F^2$. The matrix sketching problem becomes particularly interesting in the context of sliding windows, where the goal is to approximate the matrix $\\boldsymbol{A}_W$, formed by input vectors over the most recent $N$ time units.","However, despite recent efforts, whether achieving the optimal $O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound on sliding windows is possible has remained an open question.   ","In this paper, we introduce the DS-FD algorithm, which achieves the optimal $O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound for matrix sketching over row-normalized, sequence-based sliding windows.","We also present matching upper and lower space bounds for time-based and unnormalized sliding windows, demonstrating the generality and optimality of \\dsfd across various sliding window models.","This conclusively answers the open question regarding the optimal space bound for matrix sketching over sliding windows.","Furthermore, we conduct extensive experiments with both synthetic and real-world datasets, validating our theoretical claims and thus confirming the correctness and effectiveness of our algorithm, both theoretically and empirically."],"url":"http://arxiv.org/abs/2405.07792v1","category":"cs.DB"}
{"created":"2024-05-13 14:37:03","title":"Decentralized Kernel Ridge Regression Based on Data-dependent Random Feature","abstract":"Random feature (RF) has been widely used for node consistency in decentralized kernel ridge regression (KRR). Currently, the consistency is guaranteed by imposing constraints on coefficients of features, necessitating that the random features on different nodes are identical. However, in many applications, data on different nodes varies significantly on the number or distribution, which calls for adaptive and data-dependent methods that generate different RFs. To tackle the essential difficulty, we propose a new decentralized KRR algorithm that pursues consensus on decision functions, which allows great flexibility and well adapts data on nodes. The convergence is rigorously given and the effectiveness is numerically verified: by capturing the characteristics of the data on each node, while maintaining the same communication costs as other methods, we achieved an average regression accuracy improvement of 25.5\\% across six real-world data sets.","sentences":["Random feature (RF) has been widely used for node consistency in decentralized kernel ridge regression (KRR).","Currently, the consistency is guaranteed by imposing constraints on coefficients of features, necessitating that the random features on different nodes are identical.","However, in many applications, data on different nodes varies significantly on the number or distribution, which calls for adaptive and data-dependent methods that generate different RFs.","To tackle the essential difficulty, we propose a new decentralized KRR algorithm that pursues consensus on decision functions, which allows great flexibility and well adapts data on nodes.","The convergence is rigorously given and the effectiveness is numerically verified: by capturing the characteristics of the data on each node, while maintaining the same communication costs as other methods, we achieved an average regression accuracy improvement of 25.5\\% across six real-world data sets."],"url":"http://arxiv.org/abs/2405.07791v1","category":"cs.LG"}
{"created":"2024-05-13 14:32:50","title":"Analyticity theorems for parameter-dependent plurisubharmonic functions","abstract":"In this paper, we first show that a union of upper-level sets associated to fibrewise Lelong numbers of plurisubharmonic functions is in general a pluripolar subset. Then we obtain analyticity theorems for a union of sub-level sets associated to fibrewise complex singularity exponents of some special (quasi-)plurisubharmonic functions. As a corollary, we confirm that, under certain conditions, the logarithmic poles of relative Bergman kernels form an analytic subset when the (quasi-)plurisubharmonic weight function has analytic singularities. In the end, we give counterexamples to show that the aforementioned sets are in general non-analytic even if the plurisubharmonic function is supposed to be continuous.","sentences":["In this paper, we first show that a union of upper-level sets associated to fibrewise Lelong numbers of plurisubharmonic functions is in general a pluripolar subset.","Then we obtain analyticity theorems for a union of sub-level sets associated to fibrewise complex singularity exponents of some special (quasi-)plurisubharmonic functions.","As a corollary, we confirm that, under certain conditions, the logarithmic poles of relative Bergman kernels form an analytic subset when the (quasi-)plurisubharmonic weight function has analytic singularities.","In the end, we give counterexamples to show that the aforementioned sets are in general non-analytic even if the plurisubharmonic function is supposed to be continuous."],"url":"http://arxiv.org/abs/2405.07786v1","category":"math.CV"}
{"created":"2024-05-13 14:30:12","title":"Generating Human Motion in 3D Scenes from Text Descriptions","abstract":"Generating human motions from textual descriptions has gained growing research interest due to its wide range of applications. However, only a few works consider human-scene interactions together with text conditions, which is crucial for visual and physical realism. This paper focuses on the task of generating human motions in 3D indoor scenes given text descriptions of the human-scene interactions. This task presents challenges due to the multi-modality nature of text, scene, and motion, as well as the need for spatial reasoning. To address these challenges, we propose a new approach that decomposes the complex problem into two more manageable sub-problems: (1) language grounding of the target object and (2) object-centric motion generation. For language grounding of the target object, we leverage the power of large language models. For motion generation, we design an object-centric scene representation for the generative model to focus on the target object, thereby reducing the scene complexity and facilitating the modeling of the relationship between human motions and the object. Experiments demonstrate the better motion quality of our approach compared to baselines and validate our design choices.","sentences":["Generating human motions from textual descriptions has gained growing research interest due to its wide range of applications.","However, only a few works consider human-scene interactions together with text conditions, which is crucial for visual and physical realism.","This paper focuses on the task of generating human motions in 3D indoor scenes given text descriptions of the human-scene interactions.","This task presents challenges due to the multi-modality nature of text, scene, and motion, as well as the need for spatial reasoning.","To address these challenges, we propose a new approach that decomposes the complex problem into two more manageable sub-problems: (1) language grounding of the target object and (2) object-centric motion generation.","For language grounding of the target object, we leverage the power of large language models.","For motion generation, we design an object-centric scene representation for the generative model to focus on the target object, thereby reducing the scene complexity and facilitating the modeling of the relationship between human motions and the object.","Experiments demonstrate the better motion quality of our approach compared to baselines and validate our design choices."],"url":"http://arxiv.org/abs/2405.07784v1","category":"cs.CV"}
{"created":"2024-05-13 14:24:56","title":"Harnessing Hierarchical Label Distribution Variations in Test Agnostic Long-tail Recognition","abstract":"This paper explores test-agnostic long-tail recognition, a challenging long-tail task where the test label distributions are unknown and arbitrarily imbalanced. We argue that the variation in these distributions can be broken down hierarchically into global and local levels. The global ones reflect a broad range of diversity, while the local ones typically arise from milder changes, often focused on a particular neighbor. Traditional methods predominantly use a Mixture-of-Expert (MoE) approach, targeting a few fixed test label distributions that exhibit substantial global variations. However, the local variations are left unconsidered. To address this issue, we propose a new MoE strategy, $\\mathsf{DirMixE}$, which assigns experts to different Dirichlet meta-distributions of the label distribution, each targeting a specific aspect of local variations. Additionally, the diversity among these Dirichlet meta-distributions inherently captures global variations. This dual-level approach also leads to a more stable objective function, allowing us to sample different test distributions better to quantify the mean and variance of performance outcomes. Theoretically, we show that our proposed objective benefits from enhanced generalization by virtue of the variance-based regularization. Comprehensive experiments across multiple benchmarks confirm the effectiveness of $\\mathsf{DirMixE}$. The code is available at \\url{https://github.com/scongl/DirMixE}.","sentences":["This paper explores test-agnostic long-tail recognition, a challenging long-tail task where the test label distributions are unknown and arbitrarily imbalanced.","We argue that the variation in these distributions can be broken down hierarchically into global and local levels.","The global ones reflect a broad range of diversity, while the local ones typically arise from milder changes, often focused on a particular neighbor.","Traditional methods predominantly use a Mixture-of-Expert (MoE) approach, targeting a few fixed test label distributions that exhibit substantial global variations.","However, the local variations are left unconsidered.","To address this issue, we propose a new MoE strategy, $\\mathsf{DirMixE}$, which assigns experts to different Dirichlet meta-distributions of the label distribution, each targeting a specific aspect of local variations.","Additionally, the diversity among these Dirichlet meta-distributions inherently captures global variations.","This dual-level approach also leads to a more stable objective function, allowing us to sample different test distributions better to quantify the mean and variance of performance outcomes.","Theoretically, we show that our proposed objective benefits from enhanced generalization by virtue of the variance-based regularization.","Comprehensive experiments across multiple benchmarks confirm the effectiveness of $\\mathsf{DirMixE}$.","The code is available at \\url{https://github.com/scongl/DirMixE}."],"url":"http://arxiv.org/abs/2405.07780v1","category":"cs.LG"}
{"created":"2024-05-13 14:23:37","title":"A Comprehensive Analysis of Static Word Embeddings for Turkish","abstract":"Word embeddings are fixed-length, dense and distributed word representations that are used in natural language processing (NLP) applications. There are basically two types of word embedding models which are non-contextual (static) models and contextual models. The former method generates a single embedding for a word regardless of its context, while the latter method produces distinct embeddings for a word based on the specific contexts in which it appears. There are plenty of works that compare contextual and non-contextual embedding models within their respective groups in different languages. However, the number of studies that compare the models in these two groups with each other is very few and there is no such study in Turkish. This process necessitates converting contextual embeddings into static embeddings. In this paper, we compare and evaluate the performance of several contextual and non-contextual models in both intrinsic and extrinsic evaluation settings for Turkish. We make a fine-grained comparison by analyzing the syntactic and semantic capabilities of the models separately. The results of the analyses provide insights about the suitability of different embedding models in different types of NLP tasks. We also build a Turkish word embedding repository comprising the embedding models used in this work, which may serve as a valuable resource for researchers and practitioners in the field of Turkish NLP. We make the word embeddings, scripts, and evaluation datasets publicly available.","sentences":["Word embeddings are fixed-length, dense and distributed word representations that are used in natural language processing (NLP) applications.","There are basically two types of word embedding models which are non-contextual (static) models and contextual models.","The former method generates a single embedding for a word regardless of its context, while the latter method produces distinct embeddings for a word based on the specific contexts in which it appears.","There are plenty of works that compare contextual and non-contextual embedding models within their respective groups in different languages.","However, the number of studies that compare the models in these two groups with each other is very few and there is no such study in Turkish.","This process necessitates converting contextual embeddings into static embeddings.","In this paper, we compare and evaluate the performance of several contextual and non-contextual models in both intrinsic and extrinsic evaluation settings for Turkish.","We make a fine-grained comparison by analyzing the syntactic and semantic capabilities of the models separately.","The results of the analyses provide insights about the suitability of different embedding models in different types of NLP tasks.","We also build a Turkish word embedding repository comprising the embedding models used in this work, which may serve as a valuable resource for researchers and practitioners in the field of Turkish NLP.","We make the word embeddings, scripts, and evaluation datasets publicly available."],"url":"http://arxiv.org/abs/2405.07778v1","category":"cs.CL"}
{"created":"2024-05-13 14:21:18","title":"SAR Image Synthesis with Diffusion Models","abstract":"In recent years, diffusion models (DMs) have become a popular method for generating synthetic data. By achieving samples of higher quality, they quickly became superior to generative adversarial networks (GANs) and the current state-of-the-art method in generative modeling. However, their potential has not yet been exploited in radar, where the lack of available training data is a long-standing problem. In this work, a specific type of DMs, namely denoising diffusion probabilistic model (DDPM) is adapted to the SAR domain. We investigate the network choice and specific diffusion parameters for conditional and unconditional SAR image generation. In our experiments, we show that DDPM qualitatively and quantitatively outperforms state-of-the-art GAN-based methods for SAR image generation. Finally, we show that DDPM profits from pretraining on largescale clutter data, generating SAR images of even higher quality.","sentences":["In recent years, diffusion models (DMs) have become a popular method for generating synthetic data.","By achieving samples of higher quality, they quickly became superior to generative adversarial networks (GANs) and the current state-of-the-art method in generative modeling.","However, their potential has not yet been exploited in radar, where the lack of available training data is a long-standing problem.","In this work, a specific type of DMs, namely denoising diffusion probabilistic model (DDPM) is adapted to the SAR domain.","We investigate the network choice and specific diffusion parameters for conditional and unconditional SAR image generation.","In our experiments, we show that DDPM qualitatively and quantitatively outperforms state-of-the-art GAN-based methods for SAR image generation.","Finally, we show that DDPM profits from pretraining on largescale clutter data, generating SAR images of even higher quality."],"url":"http://arxiv.org/abs/2405.07776v1","category":"cs.CV"}
{"created":"2024-05-13 14:17:52","title":"Human-Modeling in Sequential Decision-Making: An Analysis through the Lens of Human-Aware AI","abstract":"\"Human-aware\" has become a popular keyword used to describe a particular class of AI systems that are designed to work and interact with humans. While there exists a surprising level of consistency among the works that use the label human-aware, the term itself mostly remains poorly understood. In this work, we retroactively try to provide an account of what constitutes a human-aware AI system. We see that human-aware AI is a design-oriented paradigm, one that focuses on the need for modeling the humans it may interact with. Additionally, we see that this paradigm offers us intuitive dimensions to understand and categorize the kinds of interactions these systems might have with humans. We show the pedagogical value of these dimensions by using them as a tool to understand and review the current landscape of work related to human-AI systems that purport some form of human modeling. To fit the scope of a workshop paper, we specifically narrowed our review to papers that deal with sequential decision-making and were published in a major AI conference in the last three years. Our analysis helps identify the space of potential research problems that are currently being overlooked. We perform additional analysis on the degree to which these works make explicit reference to results from social science and whether they actually perform user-studies to validate their systems. We also provide an accounting of the various AI methods used by these works.","sentences":["\"Human-aware\" has become a popular keyword used to describe a particular class of AI systems that are designed to work and interact with humans.","While there exists a surprising level of consistency among the works that use the label human-aware, the term itself mostly remains poorly understood.","In this work, we retroactively try to provide an account of what constitutes a human-aware AI system.","We see that human-aware AI is a design-oriented paradigm, one that focuses on the need for modeling the humans it may interact with.","Additionally, we see that this paradigm offers us intuitive dimensions to understand and categorize the kinds of interactions these systems might have with humans.","We show the pedagogical value of these dimensions by using them as a tool to understand and review the current landscape of work related to human-AI systems that purport some form of human modeling.","To fit the scope of a workshop paper, we specifically narrowed our review to papers that deal with sequential decision-making and were published in a major AI conference in the last three years.","Our analysis helps identify the space of potential research problems that are currently being overlooked.","We perform additional analysis on the degree to which these works make explicit reference to results from social science and whether they actually perform user-studies to validate their systems.","We also provide an accounting of the various AI methods used by these works."],"url":"http://arxiv.org/abs/2405.07773v1","category":"cs.RO"}
{"created":"2024-05-13 14:14:12","title":"Hype or Heuristic? Quantum Reinforcement Learning for Join Order Optimisation","abstract":"Identifying optimal join orders (JOs) stands out as a key challenge in database research and engineering. Owing to the large search space, established classical methods rely on approximations and heuristics. Recent efforts have successfully explored reinforcement learning (RL) for JO. Likewise, quantum versions of RL have received considerable scientific attention. Yet, it is an open question if they can achieve sustainable, overall practical advantages with improved quantum processors.   In this paper, we present a novel approach that uses quantum reinforcement learning (QRL) for JO based on a hybrid variational quantum ansatz. It is able to handle general bushy join trees instead of resorting to simpler left-deep variants as compared to approaches based on quantum(-inspired) optimisation, yet requires multiple orders of magnitudes fewer qubits, which is a scarce resource even for post-NISQ systems.   Despite moderate circuit depth, the ansatz exceeds current NISQ capabilities, which requires an evaluation by numerical simulations. While QRL may not significantly outperform classical approaches in solving the JO problem with respect to result quality (albeit we see parity), we find a drastic reduction in required trainable parameters. This benefits practically relevant aspects ranging from shorter training times compared to classical RL, less involved classical optimisation passes, or better use of available training data, and fits data-stream and low-latency processing scenarios. Our comprehensive evaluation and careful discussion delivers a balanced perspective on possible practical quantum advantage, provides insights for future systemic approaches, and allows for quantitatively assessing trade-offs of quantum approaches for one of the most crucial problems of database management systems.","sentences":["Identifying optimal join orders (JOs) stands out as a key challenge in database research and engineering.","Owing to the large search space, established classical methods rely on approximations and heuristics.","Recent efforts have successfully explored reinforcement learning (RL) for JO.","Likewise, quantum versions of RL have received considerable scientific attention.","Yet, it is an open question if they can achieve sustainable, overall practical advantages with improved quantum processors.   ","In this paper, we present a novel approach that uses quantum reinforcement learning (QRL) for JO based on a hybrid variational quantum ansatz.","It is able to handle general bushy join trees instead of resorting to simpler left-deep variants as compared to approaches based on quantum(-inspired) optimisation, yet requires multiple orders of magnitudes fewer qubits, which is a scarce resource even for post-NISQ systems.   ","Despite moderate circuit depth, the ansatz exceeds current NISQ capabilities, which requires an evaluation by numerical simulations.","While QRL may not significantly outperform classical approaches in solving the JO problem with respect to result quality (albeit we see parity), we find a drastic reduction in required trainable parameters.","This benefits practically relevant aspects ranging from shorter training times compared to classical RL, less involved classical optimisation passes, or better use of available training data, and fits data-stream and low-latency processing scenarios.","Our comprehensive evaluation and careful discussion delivers a balanced perspective on possible practical quantum advantage, provides insights for future systemic approaches, and allows for quantitatively assessing trade-offs of quantum approaches for one of the most crucial problems of database management systems."],"url":"http://arxiv.org/abs/2405.07770v1","category":"quant-ph"}
{"created":"2024-05-13 14:11:09","title":"Synthetic Test Collections for Retrieval Evaluation","abstract":"Test collections play a vital role in evaluation of information retrieval (IR) systems. Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive. Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications. In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored. Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems. In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries. In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models. Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation.","sentences":["Test collections play a vital role in evaluation of information retrieval (IR) systems.","Obtaining a diverse set of user queries for test collection construction can be challenging, and acquiring relevance judgments, which indicate the appropriateness of retrieved documents to a query, is often costly and resource-intensive.","Generating synthetic datasets using Large Language Models (LLMs) has recently gained significant attention in various applications.","In IR, while previous work exploited the capabilities of LLMs to generate synthetic queries or documents to augment training data and improve the performance of ranking models, using LLMs for constructing synthetic test collections is relatively unexplored.","Previous studies demonstrate that LLMs have the potential to generate synthetic relevance judgments for use in the evaluation of IR systems.","In this paper, we comprehensively investigate whether it is possible to use LLMs to construct fully synthetic test collections by generating not only synthetic judgments but also synthetic queries.","In particular, we analyse whether it is possible to construct reliable synthetic test collections and the potential risks of bias such test collections may exhibit towards LLM-based models.","Our experiments indicate that using LLMs it is possible to construct synthetic test collections that can reliably be used for retrieval evaluation."],"url":"http://arxiv.org/abs/2405.07767v1","category":"cs.IR"}
{"created":"2024-05-13 14:09:06","title":"Challenges and Opportunities of NLP for HR Applications: A Discussion Paper","abstract":"Over the course of the recent decade, tremendous progress has been made in the areas of machine learning and natural language processing, which opened up vast areas of potential application use cases, including hiring and human resource management. We review the use cases for text analytics in the realm of human resources/personnel management, including actually realized as well as potential but not yet implemented ones, and we analyze the opportunities and risks of these.","sentences":["Over the course of the recent decade, tremendous progress has been made in the areas of machine learning and natural language processing, which opened up vast areas of potential application use cases, including hiring and human resource management.","We review the use cases for text analytics in the realm of human resources/personnel management, including actually realized as well as potential but not yet implemented ones, and we analyze the opportunities and risks of these."],"url":"http://arxiv.org/abs/2405.07766v1","category":"cs.CL"}
{"created":"2024-05-13 14:07:20","title":"TANQ: An open domain dataset of table answered questions","abstract":"Language models, potentially augmented with tool usage such as retrieval are becoming the go-to means of answering questions. Understanding and answering questions in real-world settings often requires retrieving information from different sources, processing and aggregating data to extract insights, and presenting complex findings in form of structured artifacts such as novel tables, charts, or infographics. In this paper, we introduce TANQ, the first open domain question answering dataset where the answers require building tables from information across multiple sources. We release the full source attribution for every cell in the resulting table and benchmark state-of-the-art language models in open, oracle, and closed book setups. Our best-performing baseline, GPT4 reaches an overall F1 score of 29.1, lagging behind human performance by 19.7 points. We analyse baselines' performance across different dataset attributes such as different skills required for this task, including multi-hop reasoning, math operations, and unit conversions. We further discuss common failures in model-generated answers, suggesting that TANQ is a complex task with many challenges ahead.","sentences":["Language models, potentially augmented with tool usage such as retrieval are becoming the go-to means of answering questions.","Understanding and answering questions in real-world settings often requires retrieving information from different sources, processing and aggregating data to extract insights, and presenting complex findings in form of structured artifacts such as novel tables, charts, or infographics.","In this paper, we introduce TANQ, the first open domain question answering dataset where the answers require building tables from information across multiple sources.","We release the full source attribution for every cell in the resulting table and benchmark state-of-the-art language models in open, oracle, and closed book setups.","Our best-performing baseline, GPT4 reaches an overall F1 score of 29.1, lagging behind human performance by 19.7 points.","We analyse baselines' performance across different dataset attributes such as different skills required for this task, including multi-hop reasoning, math operations, and unit conversions.","We further discuss common failures in model-generated answers, suggesting that TANQ is a complex task with many challenges ahead."],"url":"http://arxiv.org/abs/2405.07765v1","category":"cs.CL"}
{"created":"2024-05-13 14:03:49","title":"LLM4ED: Large Language Models for Automatic Equation Discovery","abstract":"Equation discovery is aimed at directly extracting physical laws from data and has emerged as a pivotal research domain. Previous methods based on symbolic mathematics have achieved substantial advancements, but often require the design of implementation of complex algorithms. In this paper, we introduce a new framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically mining governing equations from data. Specifically, we first utilize the generation capability of LLMs to generate diverse equations in string form, and then evaluate the generated equations based on observations. In the optimization phase, we propose two alternately iterated strategies to optimize generated equations collaboratively. The first strategy is to take LLMs as a black-box optimizer and achieve equation self-improvement based on historical samples and their performance. The second strategy is to instruct LLMs to perform evolutionary operators for global search. Experiments are extensively conducted on both partial differential equations and ordinary differential equations. Results demonstrate that our framework can discover effective equations to reveal the underlying physical laws under various nonlinear dynamic systems. Further comparisons are made with state-of-the-art models, demonstrating good stability and usability. Our framework substantially lowers the barriers to learning and applying equation discovery techniques, demonstrating the application potential of LLMs in the field of knowledge discovery.","sentences":["Equation discovery is aimed at directly extracting physical laws from data and has emerged as a pivotal research domain.","Previous methods based on symbolic mathematics have achieved substantial advancements, but often require the design of implementation of complex algorithms.","In this paper, we introduce a new framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically mining governing equations from data.","Specifically, we first utilize the generation capability of LLMs to generate diverse equations in string form, and then evaluate the generated equations based on observations.","In the optimization phase, we propose two alternately iterated strategies to optimize generated equations collaboratively.","The first strategy is to take LLMs as a black-box optimizer and achieve equation self-improvement based on historical samples and their performance.","The second strategy is to instruct LLMs to perform evolutionary operators for global search.","Experiments are extensively conducted on both partial differential equations and ordinary differential equations.","Results demonstrate that our framework can discover effective equations to reveal the underlying physical laws under various nonlinear dynamic systems.","Further comparisons are made with state-of-the-art models, demonstrating good stability and usability.","Our framework substantially lowers the barriers to learning and applying equation discovery techniques, demonstrating the application potential of LLMs in the field of knowledge discovery."],"url":"http://arxiv.org/abs/2405.07761v1","category":"cs.LG"}
{"created":"2024-05-13 13:59:59","title":"MADRL-Based Rate Adaptation for 360$\\degree$ Video Streaming with Multi-Viewpoint Prediction","abstract":"Over the last few years, 360$\\degree$ video traffic on the network has grown significantly. A key challenge of 360$\\degree$ video playback is ensuring a high quality of experience (QoE) with limited network bandwidth. Currently, most studies focus on tile-based adaptive bitrate (ABR) streaming based on single viewport prediction to reduce bandwidth consumption. However, the performance of models for single-viewpoint prediction is severely limited by the inherent uncertainty in head movement, which can not cope with the sudden movement of users very well. This paper first presents a multimodal spatial-temporal attention transformer to generate multiple viewpoint trajectories with their probabilities given a historical trajectory. The proposed method models viewpoint prediction as a classification problem and uses attention mechanisms to capture the spatial and temporal characteristics of input video frames and viewpoint trajectories for multi-viewpoint prediction. After that, a multi-agent deep reinforcement learning (MADRL)-based ABR algorithm utilizing multi-viewpoint prediction for 360$\\degree$ video streaming is proposed for maximizing different QoE objectives under various network conditions. We formulate the ABR problem as a decentralized partially observable Markov decision process (Dec-POMDP) problem and present a MAPPO algorithm based on centralized training and decentralized execution (CTDE) framework to solve the problem. The experimental results show that our proposed method improves the defined QoE metric by up to 85.5\\% compared to existing ABR methods.","sentences":["Over the last few years, 360$\\degree$ video traffic on the network has grown significantly.","A key challenge of 360$\\degree$ video playback is ensuring a high quality of experience (QoE) with limited network bandwidth.","Currently, most studies focus on tile-based adaptive bitrate (ABR) streaming based on single viewport prediction to reduce bandwidth consumption.","However, the performance of models for single-viewpoint prediction is severely limited by the inherent uncertainty in head movement, which can not cope with the sudden movement of users very well.","This paper first presents a multimodal spatial-temporal attention transformer to generate multiple viewpoint trajectories with their probabilities given a historical trajectory.","The proposed method models viewpoint prediction as a classification problem and uses attention mechanisms to capture the spatial and temporal characteristics of input video frames and viewpoint trajectories for multi-viewpoint prediction.","After that, a multi-agent deep reinforcement learning (MADRL)-based ABR algorithm utilizing multi-viewpoint prediction for 360$\\degree$ video streaming is proposed for maximizing different QoE objectives under various network conditions.","We formulate the ABR problem as a decentralized partially observable Markov decision process (Dec-POMDP) problem and present a MAPPO algorithm based on centralized training and decentralized execution (CTDE) framework to solve the problem.","The experimental results show that our proposed method improves the defined QoE metric by up to 85.5\\% compared to existing ABR methods."],"url":"http://arxiv.org/abs/2405.07759v1","category":"cs.MM"}
{"created":"2024-05-13 13:53:36","title":"Searching for evidence of subchromospheric magnetic reconnection on the Sun","abstract":"Within the coronae of stars, abundances of those elements with low first ionization potential (FIP) often differ from their photospheric values. The coronae of the Sun and solar-type stars mostly show enhancements of low-FIP elements (the FIP effect) while more active stars such as M dwarfs have coronae generally characterized by the inverse-FIP (I-FIP) effect. Highly localized regions of I-FIP effect solar plasma have been observed by Hinode/EIS in a number of highly complex active regions, usually around strong light bridges of the umbrae of coalescing/merging sunspots. These observations can be interpreted in the context of the ponderomotive force fractionation model which predicts that plasma with I-FIP effect composition is created by the refraction of waves coming from below the plasma fractionation region in the chromosphere. A plausible source of these waves is thought to be reconnection in the (high-plasma \\b{eta}) subchromospheric magnetic field. In this study, we use the 3D visualization technique of Chintzoglou & Zhang (2013) combined with observations of localized I-FIP effect in the corona of AR 11504 to identify potential sites of such reconnection and its possible consequences in the solar atmosphere. We found subtle signatures of episodic heating and reconnection outflows in the expected places, in between magnetic flux tubes forming a light bridge, within the photosphere of the active region. Furthermore, on either side of the light bridge, we observed small antiparallel horizontal magnetic field components supporting the possibility of reconnection occuring where we observe I-FIP plasma. When taken together with the I-FIP effect observations, these subtle signatures provide a compelling case for indirect observational evidence of reconnection below the fractionation layer of the chromosphere, however, direct evidence remains elusive.","sentences":["Within the coronae of stars, abundances of those elements with low first ionization potential (FIP) often differ from their photospheric values.","The coronae of the Sun and solar-type stars mostly show enhancements of low-FIP elements (the FIP effect) while more active stars such as M dwarfs have coronae generally characterized by the inverse-FIP (I-FIP) effect.","Highly localized regions of I-FIP effect solar plasma have been observed by Hinode/EIS in a number of highly complex active regions, usually around strong light bridges of the umbrae of coalescing/merging sunspots.","These observations can be interpreted in the context of the ponderomotive force fractionation model which predicts that plasma with I-FIP effect composition is created by the refraction of waves coming from below the plasma fractionation region in the chromosphere.","A plausible source of these waves is thought to be reconnection in the (high-plasma \\b{eta}) subchromospheric magnetic field.","In this study, we use the 3D visualization technique of Chintzoglou & Zhang (2013) combined with observations of localized I-FIP effect in the corona of AR 11504 to identify potential sites of such reconnection and its possible consequences in the solar atmosphere.","We found subtle signatures of episodic heating and reconnection outflows in the expected places, in between magnetic flux tubes forming a light bridge, within the photosphere of the active region.","Furthermore, on either side of the light bridge, we observed small antiparallel horizontal magnetic field components supporting the possibility of reconnection occuring where we observe I-FIP plasma.","When taken together with the I-FIP effect observations, these subtle signatures provide a compelling case for indirect observational evidence of reconnection below the fractionation layer of the chromosphere, however, direct evidence remains elusive."],"url":"http://arxiv.org/abs/2405.07755v1","category":"astro-ph.SR"}
{"created":"2024-05-13 13:52:18","title":"Long term variability of Cygnus X-1. VIII. A spectral-timing look at low energies with NICER","abstract":"The Neutron Star Interior Composition Explorer (NICER) monitoring campaign of Cyg X-1 allows us to study its spectral-timing behavior at energies ${<}1$ keV across all states. The hard state power spectrum can be decomposed into two main broad Lorentzians with a transition at around 1 Hz. The lower-frequency Lorentzian is the dominant component at low energies. The higher-frequency Lorentzian begins to contribute significantly to the variability above 1.5 keV and dominates at high energies. We show that the low- and high-frequency Lorentzians likely represent individual physical processes. The lower-frequency Lorentzian can be associated with a (possibly Comptonized) disk component, while the higher-frequency Lorentzian is clearly associated with the Comptonizing plasma. At the transition of these components, we discover a low-energy timing phenomenon characterized by an abrupt lag change of hard (${\\gtrsim}2$ keV) with respect to soft (${\\lesssim}1.5$ keV) photons, accompanied by a drop in coherence, and a reduction in amplitude of the second broad Lorentzian. The frequency of the phenomenon increases with the frequencies of the Lorentzians as the source softens and cannot be seen when the power spectrum is single-humped. A comparison to transient low-mass X-ray binaries shows that this feature does not only appear in Cyg X-1, but that it is a general property of accreting black hole binaries. In Cyg X-1, we find that the variability at low and high energies is overall highly coherent in the hard and intermediate states. The high coherence shows that there is a process at work which links the variability, suggesting a physical connection between the accretion disk and Comptonizing plasma. This process fundamentally changes in the soft state, where strong red noise at high energies is incoherent to the variability at low energies.","sentences":["The Neutron Star Interior Composition Explorer (NICER) monitoring campaign of Cyg X-1 allows us to study its spectral-timing behavior at energies ${<}1$ keV across all states.","The hard state power spectrum can be decomposed into two main broad Lorentzians with a transition at around 1 Hz.","The lower-frequency Lorentzian is the dominant component at low energies.","The higher-frequency Lorentzian begins to contribute significantly to the variability above 1.5 keV and dominates at high energies.","We show that the low- and high-frequency Lorentzians likely represent individual physical processes.","The lower-frequency Lorentzian can be associated with a (possibly Comptonized) disk component, while the higher-frequency Lorentzian is clearly associated with the Comptonizing plasma.","At the transition of these components, we discover a low-energy timing phenomenon characterized by an abrupt lag change of hard (${\\gtrsim}2$ keV) with respect to soft (${\\lesssim}1.5$ keV) photons, accompanied by a drop in coherence, and a reduction in amplitude of the second broad Lorentzian.","The frequency of the phenomenon increases with the frequencies of the Lorentzians as the source softens and cannot be seen when the power spectrum is single-humped.","A comparison to transient low-mass X-ray binaries shows that this feature does not only appear in Cyg X-1, but that it is a general property of accreting black hole binaries.","In Cyg X-1, we find that the variability at low and high energies is overall highly coherent in the hard and intermediate states.","The high coherence shows that there is a process at work which links the variability, suggesting a physical connection between the accretion disk and Comptonizing plasma.","This process fundamentally changes in the soft state, where strong red noise at high energies is incoherent to the variability at low energies."],"url":"http://arxiv.org/abs/2405.07754v1","category":"astro-ph.HE"}
{"created":"2024-05-13 13:50:44","title":"Integrating supervised and unsupervised learning approaches to unveil critical process inputs","abstract":"This study introduces a machine learning framework tailored to large-scale industrial processes characterized by a plethora of numerical and categorical inputs. The framework aims to (i) discern critical parameters influencing the output and (ii) generate accurate out-of-sample qualitative and quantitative predictions of production outcomes. Specifically, we address the pivotal question of the significance of each input in shaping the process outcome, using an industrial Chemical Vapor Deposition (CVD) process as an example. The initial objective involves merging subject matter expertise and clustering techniques exclusively on the process output, here, coating thickness measurements at various positions in the reactor. This approach identifies groups of production runs that share similar qualitative characteristics, such as film mean thickness and standard deviation. In particular, the differences of the outcomes represented by the different clusters can be attributed to differences in specific inputs, indicating that these inputs are critical for the production outcome. Leveraging this insight, we subsequently implement supervised classification and regression methods using the identified critical process inputs. The proposed methodology proves to be valuable in scenarios with a multitude of inputs and insufficient data for the direct application of deep learning techniques, providing meaningful insights into the underlying processes.","sentences":["This study introduces a machine learning framework tailored to large-scale industrial processes characterized by a plethora of numerical and categorical inputs.","The framework aims to (i) discern critical parameters influencing the output and (ii) generate accurate out-of-sample qualitative and quantitative predictions of production outcomes.","Specifically, we address the pivotal question of the significance of each input in shaping the process outcome, using an industrial Chemical Vapor Deposition (CVD) process as an example.","The initial objective involves merging subject matter expertise and clustering techniques exclusively on the process output, here, coating thickness measurements at various positions in the reactor.","This approach identifies groups of production runs that share similar qualitative characteristics, such as film mean thickness and standard deviation.","In particular, the differences of the outcomes represented by the different clusters can be attributed to differences in specific inputs, indicating that these inputs are critical for the production outcome.","Leveraging this insight, we subsequently implement supervised classification and regression methods using the identified critical process inputs.","The proposed methodology proves to be valuable in scenarios with a multitude of inputs and insufficient data for the direct application of deep learning techniques, providing meaningful insights into the underlying processes."],"url":"http://arxiv.org/abs/2405.07751v1","category":"cs.LG"}
{"created":"2024-05-13 13:47:15","title":"DeepHYDRA: Resource-Efficient Time-Series Anomaly Detection in Dynamically-Configured Systems","abstract":"Anomaly detection in distributed systems such as High-Performance Computing (HPC) clusters is vital for early fault detection, performance optimisation, security monitoring, reliability in general but also operational insights. Deep Neural Networks have seen successful use in detecting long-term anomalies in multidimensional data, originating for instance from industrial or medical systems, or weather prediction. A downside of such methods is that they require a static input size, or lose data through cropping, sampling, or other dimensionality reduction methods, making deployment on systems with variability on monitored data channels, such as computing clusters difficult. To address these problems, we present DeepHYDRA (Deep Hybrid DBSCAN/Reduction-Based Anomaly Detection) which combines DBSCAN and learning-based anomaly detection. DBSCAN clustering is used to find point anomalies in time-series data, mitigating the risk of missing outliers through loss of information when reducing input data to a fixed number of channels. A deep learning-based time-series anomaly detection method is then applied to the reduced data in order to identify long-term outliers. This hybrid approach reduces the chances of missing anomalies that might be made indistinguishable from normal data by the reduction process, and likewise enables the algorithm to be scalable and tolerate partial system failures while retaining its detection capabilities. Using a subset of the well-known SMD dataset family, a modified variant of the Eclipse dataset, as well as an in-house dataset with a large variability in active data channels, made publicly available with this work, we furthermore analyse computational intensity, memory footprint, and activation counts. DeepHYDRA is shown to reliably detect different types of anomalies in both large and complex datasets.","sentences":["Anomaly detection in distributed systems such as High-Performance Computing (HPC) clusters is vital for early fault detection, performance optimisation, security monitoring, reliability in general but also operational insights.","Deep Neural Networks have seen successful use in detecting long-term anomalies in multidimensional data, originating for instance from industrial or medical systems, or weather prediction.","A downside of such methods is that they require a static input size, or lose data through cropping, sampling, or other dimensionality reduction methods, making deployment on systems with variability on monitored data channels, such as computing clusters difficult.","To address these problems, we present DeepHYDRA (Deep Hybrid DBSCAN/Reduction-Based Anomaly Detection) which combines DBSCAN and learning-based anomaly detection.","DBSCAN clustering is used to find point anomalies in time-series data, mitigating the risk of missing outliers through loss of information when reducing input data to a fixed number of channels.","A deep learning-based time-series anomaly detection method is then applied to the reduced data in order to identify long-term outliers.","This hybrid approach reduces the chances of missing anomalies that might be made indistinguishable from normal data by the reduction process, and likewise enables the algorithm to be scalable and tolerate partial system failures while retaining its detection capabilities.","Using a subset of the well-known SMD dataset family, a modified variant of the Eclipse dataset, as well as an in-house dataset with a large variability in active data channels, made publicly available with this work, we furthermore analyse computational intensity, memory footprint, and activation counts.","DeepHYDRA is shown to reliably detect different types of anomalies in both large and complex datasets."],"url":"http://arxiv.org/abs/2405.07749v1","category":"cs.LG"}
{"created":"2024-05-13 13:43:45","title":"Combination rule for Hammett $\u03c3$ constants in computational catalyst discovery","abstract":"The Hammett equation is popular for navigating chemical space by quantifying the effects of substituents on chemical properties and behavior. We study the applicability of the Hammett-inspired product (HIP) Ansatz to model relative substrate binding within homogenous organometallic catalysis, assigning $\\sigma$ and $\\rho$ to ligands and metals, respectively. Implementing an additive combination (c) rule for obtaining $\\sigma$ constants for any ligand pair combination results in a cHIP model that can be leveraged (i) as a baseline for $\\Delta$-machine learning (ML), and (ii) to identify novel catalyst candidates via volcano plots. After testing the combination rule on Hammett constants previously published in the literature, we have generated numerical evidence for the Suzuki-Miyaura (SM) C-C cross-coupling reaction using two synthetic datasets of metallic catalysts (including (10) and (11)-metals Ni, Pd, Pt, and Cu, Ag, Au as well as 96 ligands such as N-heterocyclic carbenes, phosphines, or pyridines). When used as a baseline, $\\Delta$-ML prediction errors of relative binding decrease systematically with training set size and reach chemical accuracy ($\\sim$1 kcal/mol) for 20k training instances. Employing the individual ligand constants obtained from cHIP, we report relative substrate binding for a novel dataset consisting of 720 catalysts (not part of training data), of which 145 fall into the most promising range on the volcano plot accounting for oxidative addition, transmetalation, and reductive elimination steps. Multiple Ni-based catalysts, e.g. Aphos-Ni-P($t$-Bu)$_3$, are included among these promising candidates, potentially offering dramatic cost savings in experimental applications.","sentences":["The Hammett equation is popular for navigating chemical space by quantifying the effects of substituents on chemical properties and behavior.","We study the applicability of the Hammett-inspired product (HIP) Ansatz to model relative substrate binding within homogenous organometallic catalysis, assigning $\\sigma$ and $\\rho$ to ligands and metals, respectively.","Implementing an additive combination (c) rule for obtaining $\\sigma$ constants for any ligand pair combination results in a cHIP model that can be leveraged (i) as a baseline for $\\Delta$-machine learning (ML), and (ii) to identify novel catalyst candidates via volcano plots.","After testing the combination rule on Hammett constants previously published in the literature, we have generated numerical evidence for the Suzuki-Miyaura (SM) C-C cross-coupling reaction using two synthetic datasets of metallic catalysts (including (10) and (11)-metals Ni, Pd, Pt, and Cu, Ag, Au as well as 96 ligands such as N-heterocyclic carbenes, phosphines, or pyridines).","When used as a baseline, $\\Delta$-ML prediction errors of relative binding decrease systematically with training set size and reach chemical accuracy ($\\sim$1 kcal/mol) for 20k training instances.","Employing the individual ligand constants obtained from cHIP, we report relative substrate binding for a novel dataset consisting of 720 catalysts (not part of training data), of which 145 fall into the most promising range on the volcano plot accounting for oxidative addition, transmetalation, and reductive elimination steps.","Multiple Ni-based catalysts, e.g. Aphos-Ni-P($t$-Bu)$_3$, are included among these promising candidates, potentially offering dramatic cost savings in experimental applications."],"url":"http://arxiv.org/abs/2405.07747v1","category":"physics.chem-ph"}
{"created":"2024-05-13 13:41:59","title":"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language","abstract":"Despite advancements in English-dominant generative large language models, further development is needed for low-resource languages to enhance global accessibility. The primary methods for representing these languages are monolingual and multilingual pretraining. Monolingual pretraining is expensive due to hardware requirements, and multilingual models often have uneven performance across languages. This study explores an alternative solution by adapting large language models, primarily trained on English, to low-resource languages. We assess various strategies, including continual training, instruction fine-tuning, task-specific fine-tuning, and vocabulary extension. The results show that continual training improves language comprehension, as reflected in perplexity scores, and task-specific tuning generally enhances performance of downstream tasks. However, extending the vocabulary shows no substantial benefits. Additionally, while larger models improve task performance with few-shot tuning, multilingual models perform worse than their monolingual counterparts when adapted.","sentences":["Despite advancements in English-dominant generative large language models, further development is needed for low-resource languages to enhance global accessibility.","The primary methods for representing these languages are monolingual and multilingual pretraining.","Monolingual pretraining is expensive due to hardware requirements, and multilingual models often have uneven performance across languages.","This study explores an alternative solution by adapting large language models, primarily trained on English, to low-resource languages.","We assess various strategies, including continual training, instruction fine-tuning, task-specific fine-tuning, and vocabulary extension.","The results show that continual training improves language comprehension, as reflected in perplexity scores, and task-specific tuning generally enhances performance of downstream tasks.","However, extending the vocabulary shows no substantial benefits.","Additionally, while larger models improve task performance with few-shot tuning, multilingual models perform worse than their monolingual counterparts when adapted."],"url":"http://arxiv.org/abs/2405.07745v1","category":"cs.CL"}
{"created":"2024-05-13 13:40:55","title":"MoCo: Fuzzing Deep Learning Libraries via Assembling Code","abstract":"The rapidly developing deep learning (DL) techniques have been applied in software systems with various application scenarios. However, they could also pose new safety threats with potentially serious consequences, especially in safety-critical domains. DL libraries serve as the underlying foundation for DL systems, and bugs in them can have unpredictable impacts that directly affect the behaviors of DL systems. Previous research on fuzzing DL libraries still has limitations in the diversity of test inputs, the construction of test oracles, and the precision of detection. In this paper, we propose MoCo, a novel fuzzing testing method for DL libraries via assembling code. MoCo first disassembles the seed code file to obtain the template and code blocks, and then employs code block mutation operators (e.g., API replacement, random generation, and boundary checking) to generate more new code blocks adapted to the template. By inserting context-appropriate code blocks into the template step by step, MoCo can generate a tree of code files with intergenerational relations. According to the derivation relations in this tree and the applied mutation operators, we construct the test oracle based on the execution state consistency. Since the granularity of code assembly and mutation is controlled rather than randomly divergent, we can quickly pinpoint the lines of code where the bugs are located and the corresponding triggering conditions. We conduct a comprehensive experiment to evaluate the efficiency and effectiveness of MoCo using three widely-used DL libraries (i.e., TensorFlow, PyTorch, and Jittor). During the experiment, MoCo detects 64 new bugs of four types in three DL libraries, where 51 bugs have been confirmed, and 13 bugs have been fixed by developers.","sentences":["The rapidly developing deep learning (DL) techniques have been applied in software systems with various application scenarios.","However, they could also pose new safety threats with potentially serious consequences, especially in safety-critical domains.","DL libraries serve as the underlying foundation for DL systems, and bugs in them can have unpredictable impacts that directly affect the behaviors of DL systems.","Previous research on fuzzing DL libraries still has limitations in the diversity of test inputs, the construction of test oracles, and the precision of detection.","In this paper, we propose MoCo, a novel fuzzing testing method for DL libraries via assembling code.","MoCo first disassembles the seed code file to obtain the template and code blocks, and then employs code block mutation operators (e.g., API replacement, random generation, and boundary checking) to generate more new code blocks adapted to the template.","By inserting context-appropriate code blocks into the template step by step, MoCo can generate a tree of code files with intergenerational relations.","According to the derivation relations in this tree and the applied mutation operators, we construct the test oracle based on the execution state consistency.","Since the granularity of code assembly and mutation is controlled rather than randomly divergent, we can quickly pinpoint the lines of code where the bugs are located and the corresponding triggering conditions.","We conduct a comprehensive experiment to evaluate the efficiency and effectiveness of MoCo using three widely-used DL libraries (i.e., TensorFlow, PyTorch, and Jittor).","During the experiment, MoCo detects 64 new bugs of four types in three DL libraries, where 51 bugs have been confirmed, and 13 bugs have been fixed by developers."],"url":"http://arxiv.org/abs/2405.07744v1","category":"cs.SE"}
{"created":"2024-05-13 13:39:59","title":"The interaction of gravitational waves with matter","abstract":"It is well-known that gravitational waves (GWs) undergo no absorption or dissipation when traversing through a perfect fluid. However, in the presence of a viscous fluid, GWs transfer energy to the fluid medium. In this essay, we present a review of our recent series of results regarding the interaction between gravitational waves and surrounding matter. Additionally, we examine the impact of a viscous fluid shell on gravitational wave propagation, focusing particularly on GW damping and GW heating. Furthermore, we explore the significance of these effects in various astrophysical scenarios such as core-collapse Supernovae and primordial gravitational waves.","sentences":["It is well-known that gravitational waves (GWs) undergo no absorption or dissipation when traversing through a perfect fluid.","However, in the presence of a viscous fluid, GWs transfer energy to the fluid medium.","In this essay, we present a review of our recent series of results regarding the interaction between gravitational waves and surrounding matter.","Additionally, we examine the impact of a viscous fluid shell on gravitational wave propagation, focusing particularly on GW damping and GW heating.","Furthermore, we explore the significance of these effects in various astrophysical scenarios such as core-collapse Supernovae and primordial gravitational waves."],"url":"http://arxiv.org/abs/2405.07743v1","category":"gr-qc"}
{"created":"2024-05-13 13:35:24","title":"The $\u03c3$ hulls of matrix-product codes and related entanglement-assisted quantum error-correcting codes","abstract":"Let $\\mathrm{SLAut}(\\mathbb{F}_{q}^{n})$ denote the group of all semilinear isometries on $\\mathbb{F}_{q}^{n}$, where $q=p^{e}$ is a prime power. Matrix-product (MP) codes are a class of long classical codes generated by combining several commensurate classical codes with a defining matrix. We give an explicit formula for calculating the dimension of the $\\sigma$ hull of a MP code. As a result, we give necessary and sufficient conditions for the MP codes to be $\\sigma$ dual-containing and $\\sigma$ self-orthogonal. We prove that $\\mathrm{dim}_{\\mathbb{F}_{q}}(\\mathrm{Hull}_{\\sigma}(\\mathcal{C}))=\\mathrm{dim}_{\\mathbb{F}_{q}}(\\mathrm{Hull}_{\\sigma}(\\mathcal{C}^{\\bot_{\\sigma}}))$. We prove that for any integer $h$ with $\\mathrm{max}\\{0,k_{1}-k_{2}\\}\\leq h\\leq \\mathrm{dim}_{\\mathbb{F}_{q}}(\\mathcal{C}_{1}\\cap\\mathcal{C}_{2}^{\\bot_{\\sigma}})$, there exists a linear code $\\mathcal{C}_{2,h}$ monomially equivalent to $\\mathcal{C}_{2}$ such that $\\mathrm{dim}_{\\mathbb{F}_{q}}(\\mathcal{C}_{1}\\cap\\mathcal{C}_{2,h}^{\\bot_{\\sigma}})=h$, where $\\mathcal{C}_{i}$ is an $[n,k_{i}]_{q}$ linear code for $i=1,2$. We show that given an $[n,k,d]_{q}$ linear code $\\mathcal{C}$, there exists a monomially equivalent $[n,k,d]_{q}$ linear code $\\mathcal{C}_{h}$, whose $\\sigma$ dual code has minimum distance $d'$, such that there exist an $[[n,k-h,d;n-k-h]]_{q}$ EAQECC and an $[[n,n-k-h,d';k-h]]_{q}$ EAQECC for every integer $h$ with $0\\leq h\\leq \\mathrm{dim}_{\\mathbb{F}_{q}}(\\mathrm{Hull}_{\\sigma}(\\mathcal{C}))$. Based on this result, we present a general construction method for deriving EAQECCs with flexible parameters from MP codes related to $\\sigma$ hulls.","sentences":["Let $\\mathrm{SLAut}(\\mathbb{F}_{q}^{n})$ denote the group of all semilinear isometries on $\\mathbb{F}_{q}^{n}$, where $q=p^{e}$ is a prime power.","Matrix-product (MP) codes are a class of long classical codes generated by combining several commensurate classical codes with a defining matrix.","We give an explicit formula for calculating the dimension of the $\\sigma$ hull of a MP code.","As a result, we give necessary and sufficient conditions for the MP codes to be $\\sigma$ dual-containing and $\\sigma$ self-orthogonal.","We prove that $\\mathrm{dim}_{\\mathbb{F}_{q}}(\\mathrm{Hull}_{\\sigma}(\\mathcal{C}))=\\mathrm{dim}_{\\mathbb{F}_{q}}(\\mathrm{Hull}_{\\sigma}(\\mathcal{C}^{\\bot_{\\sigma}}))$. We prove that for any integer $h$ with $\\mathrm{max}\\{0,k_{1}-k_{2}\\}\\leq h\\leq \\mathrm{dim}_{\\mathbb{F}_{q}}(\\mathcal{C}_{1}\\cap\\mathcal{C}_{2}^{\\bot_{\\sigma}})$, there exists a linear code $\\mathcal{C}_{2,h}$ monomially equivalent to $\\mathcal{C}_{2}$ such that $\\mathrm{dim}_{\\mathbb{F}_{q}}(\\mathcal{C}_{1}\\cap\\mathcal{C}_{2,h}^{\\bot_{\\sigma}})=h$, where $\\mathcal{C}_{i}$ is an $[n,k_{i}]_{q}$ linear code for $i=1,2$. We show that given an $[n,k,d]_{q}$ linear code $\\mathcal{C}$, there exists a monomially equivalent $[n,k,d]_{q}$ linear code $\\mathcal{C}_{h}$, whose $\\sigma$ dual code has minimum distance $d'$, such that there exist an $[[n,k-h,d;n-k-h]]_{q}$ EAQECC and an $[[n,n-k-h,d';k-h]]_{q}$ EAQECC for every integer $h$ with $0\\leq h\\leq \\mathrm{dim}_{\\mathbb{F}_{q}}(\\mathrm{Hull}_{\\sigma}(\\mathcal{C}))$. Based on this result, we present a general construction method for deriving EAQECCs with flexible parameters from MP codes related to $\\sigma$ hulls."],"url":"http://arxiv.org/abs/2405.07740v1","category":"cs.IT"}
{"created":"2024-05-13 13:34:02","title":"Curves defined by a class of discrete operators: approximation result and applications","abstract":"In approximation theory classical discrete operators, like generalized sampling, Sz\\'{a}sz-Mirak'jan, Baskakov and Bernstein operators, have been extensively studied for scalar functions. In this paper, we look at the approximation of curves by a class of discrete operators and we exhibit graphical examples concerning several cases. The topic has useful implications about the computer graphics and the image processing: we discuss applications on the approximation and the reconstruction of curves in images.","sentences":["In approximation theory classical discrete operators, like generalized sampling, Sz\\'{a}sz-Mirak'jan, Baskakov and Bernstein operators, have been extensively studied for scalar functions.","In this paper, we look at the approximation of curves by a class of discrete operators and we exhibit graphical examples concerning several cases.","The topic has useful implications about the computer graphics and the image processing: we discuss applications on the approximation and the reconstruction of curves in images."],"url":"http://arxiv.org/abs/2405.07738v1","category":"math.FA"}
{"created":"2024-05-13 13:32:41","title":"Learning to Plan Maneuverable and Agile Flight Trajectory with Optimization Embedded Networks","abstract":"In recent times, an increasing number of researchers have been devoted to utilizing deep neural networks for end-to-end flight navigation. This approach has gained traction due to its ability to bridge the gap between perception and planning that exists in traditional methods, thereby eliminating delays between modules. However, the practice of replacing original modules with neural networks in a black-box manner diminishes the overall system's robustness and stability. It lacks principled explanations and often fails to consistently generate high-quality motion trajectories. Furthermore, such methods often struggle to rigorously account for the robot's kinematic constraints, resulting in the generation of trajectories that cannot be executed satisfactorily. In this work, we combine the advantages of traditional methods and neural networks by proposing an optimization-embedded neural network. This network can learn high-quality trajectories directly from visual inputs without the need of mapping, while ensuring dynamic feasibility. Here, the deep neural network is employed to directly extract environment safety regions from depth images. Subsequently, we employ a model-based approach to represent these regions as safety constraints in trajectory optimization. Leveraging the availability of highly efficient optimization algorithms, our method robustly converges to feasible and optimal solutions that satisfy various user-defined constraints. Moreover, we differentiate the optimization process, allowing it to be trained as a layer within the neural network. This approach facilitates the direct interaction between perception and planning, enabling the network to focus more on the spatial regions where optimal solutions exist. As a result, it further enhances the quality and stability of the generated trajectories.","sentences":["In recent times, an increasing number of researchers have been devoted to utilizing deep neural networks for end-to-end flight navigation.","This approach has gained traction due to its ability to bridge the gap between perception and planning that exists in traditional methods, thereby eliminating delays between modules.","However, the practice of replacing original modules with neural networks in a black-box manner diminishes the overall system's robustness and stability.","It lacks principled explanations and often fails to consistently generate high-quality motion trajectories.","Furthermore, such methods often struggle to rigorously account for the robot's kinematic constraints, resulting in the generation of trajectories that cannot be executed satisfactorily.","In this work, we combine the advantages of traditional methods and neural networks by proposing an optimization-embedded neural network.","This network can learn high-quality trajectories directly from visual inputs without the need of mapping, while ensuring dynamic feasibility.","Here, the deep neural network is employed to directly extract environment safety regions from depth images.","Subsequently, we employ a model-based approach to represent these regions as safety constraints in trajectory optimization.","Leveraging the availability of highly efficient optimization algorithms, our method robustly converges to feasible and optimal solutions that satisfy various user-defined constraints.","Moreover, we differentiate the optimization process, allowing it to be trained as a layer within the neural network.","This approach facilitates the direct interaction between perception and planning, enabling the network to focus more on the spatial regions where optimal solutions exist.","As a result, it further enhances the quality and stability of the generated trajectories."],"url":"http://arxiv.org/abs/2405.07736v1","category":"cs.RO"}
{"created":"2024-05-13 13:32:02","title":"Federated Hierarchical Tensor Networks: a Collaborative Learning Quantum AI-Driven Framework for Healthcare","abstract":"Healthcare industries frequently handle sensitive and proprietary data, and due to strict privacy regulations, they are often reluctant to share data directly. In today's context, Federated Learning (FL) stands out as a crucial remedy, facilitating the rapid advancement of distributed machine learning while effectively managing critical concerns regarding data privacy and governance. The fusion of federated learning and quantum computing represents a groundbreaking interdisciplinary approach with immense potential to revolutionize various industries, from healthcare to finance. In this work, we proposed a federated learning framework based on quantum tensor networks, which leverages the principles of many-body quantum physics. Currently, there are no known classical tensor networks implemented in federated settings. Furthermore, we investigated the effectiveness and feasibility of the proposed framework by conducting a differential privacy analysis to ensure the security of sensitive data across healthcare institutions. Experiments on popular medical image datasets show that the federated quantum tensor network model achieved a mean receiver-operator characteristic area under the curve (ROC-AUC) between 0.91-0.98. Experimental results demonstrate that the quantum federated global model, consisting of highly entangled tensor network structures, showed better generalization and robustness and achieved higher testing accuracy, surpassing the performance of locally trained clients under unbalanced data distributions among healthcare institutions.","sentences":["Healthcare industries frequently handle sensitive and proprietary data, and due to strict privacy regulations, they are often reluctant to share data directly.","In today's context, Federated Learning (FL) stands out as a crucial remedy, facilitating the rapid advancement of distributed machine learning while effectively managing critical concerns regarding data privacy and governance.","The fusion of federated learning and quantum computing represents a groundbreaking interdisciplinary approach with immense potential to revolutionize various industries, from healthcare to finance.","In this work, we proposed a federated learning framework based on quantum tensor networks, which leverages the principles of many-body quantum physics.","Currently, there are no known classical tensor networks implemented in federated settings.","Furthermore, we investigated the effectiveness and feasibility of the proposed framework by conducting a differential privacy analysis to ensure the security of sensitive data across healthcare institutions.","Experiments on popular medical image datasets show that the federated quantum tensor network model achieved a mean receiver-operator characteristic area under the curve (ROC-AUC) between 0.91-0.98.","Experimental results demonstrate that the quantum federated global model, consisting of highly entangled tensor network structures, showed better generalization and robustness and achieved higher testing accuracy, surpassing the performance of locally trained clients under unbalanced data distributions among healthcare institutions."],"url":"http://arxiv.org/abs/2405.07735v1","category":"quant-ph"}
{"created":"2024-05-13 13:29:51","title":"Measuring dependence between a scalar response and a functional covariate","abstract":"We extend the scope of a recently introduced dependence coefficient between a scalar response $Y$ and a multivariate covariate $X$ to the case where $X$ takes values in a general metric space. Particular attention is paid to the case where $X$ is a curve. While on the population level, this extension is straight forward, the asymptotic behavior of the estimator we consider is delicate. It crucially depends on the nearest neighbor structure of the infinite-dimensional covariate sample, where deterministic bounds on the degrees of the nearest neighbor graphs available in multivariate settings do no longer exist. The main contribution of this paper is to give some insight into this matter and to advise a way how to overcome the problem for our purposes. As an important application of our results, we consider an independence test.","sentences":["We extend the scope of a recently introduced dependence coefficient between a scalar response $Y$ and a multivariate covariate $X$ to the case where $X$ takes values in a general metric space.","Particular attention is paid to the case where $X$ is a curve.","While on the population level, this extension is straight forward, the asymptotic behavior of the estimator we consider is delicate.","It crucially depends on the nearest neighbor structure of the infinite-dimensional covariate sample, where deterministic bounds on the degrees of the nearest neighbor graphs available in multivariate settings do no longer exist.","The main contribution of this paper is to give some insight into this matter and to advise a way how to overcome the problem for our purposes.","As an important application of our results, we consider an independence test."],"url":"http://arxiv.org/abs/2405.07732v1","category":"math.ST"}
{"created":"2024-05-13 13:24:17","title":"Does Dependency Locality Predict Non-canonical Word Order in Hindi?","abstract":"Previous work has shown that isolated non-canonical sentences with Object-before-Subject (OSV) order are initially harder to process than their canonical counterparts with Subject-before-Object (SOV) order. Although this difficulty diminishes with appropriate discourse context, the underlying cognitive factors responsible for alleviating processing challenges in OSV sentences remain a question. In this work, we test the hypothesis that dependency length minimization is a significant predictor of non-canonical (OSV) syntactic choices, especially when controlling for information status such as givenness and surprisal measures. We extract sentences from the Hindi-Urdu Treebank corpus (HUTB) that contain clearly-defined subjects and objects, systematically permute the preverbal constituents of those sentences, and deploy a classifier to distinguish between original corpus sentences and artificially generated alternatives. The classifier leverages various discourse-based and cognitive features, including dependency length, surprisal, and information status, to inform its predictions. Our results suggest that, although there exists a preference for minimizing dependency length in non-canonical corpus sentences amidst the generated variants, this factor does not significantly contribute in identifying corpus sentences above and beyond surprisal and givenness measures. Notably, discourse predictability emerges as the primary determinant of constituent-order preferences. These findings are further supported by human evaluations involving 44 native Hindi speakers. Overall, this work sheds light on the role of expectation adaptation in word-ordering decisions. We conclude by situating our results within the theories of discourse production and information locality.","sentences":["Previous work has shown that isolated non-canonical sentences with Object-before-Subject (OSV) order are initially harder to process than their canonical counterparts with Subject-before-Object (SOV) order.","Although this difficulty diminishes with appropriate discourse context, the underlying cognitive factors responsible for alleviating processing challenges in OSV sentences remain a question.","In this work, we test the hypothesis that dependency length minimization is a significant predictor of non-canonical (OSV) syntactic choices, especially when controlling for information status such as givenness and surprisal measures.","We extract sentences from the Hindi-Urdu Treebank corpus (HUTB) that contain clearly-defined subjects and objects, systematically permute the preverbal constituents of those sentences, and deploy a classifier to distinguish between original corpus sentences and artificially generated alternatives.","The classifier leverages various discourse-based and cognitive features, including dependency length, surprisal, and information status, to inform its predictions.","Our results suggest that, although there exists a preference for minimizing dependency length in non-canonical corpus sentences amidst the generated variants, this factor does not significantly contribute in identifying corpus sentences above and beyond surprisal and givenness measures.","Notably, discourse predictability emerges as the primary determinant of constituent-order preferences.","These findings are further supported by human evaluations involving 44 native Hindi speakers.","Overall, this work sheds light on the role of expectation adaptation in word-ordering decisions.","We conclude by situating our results within the theories of discourse production and information locality."],"url":"http://arxiv.org/abs/2405.07730v1","category":"cs.CL"}
{"created":"2024-05-13 13:21:06","title":"Decentralized Distributed Graph Coloring: Cluster Graphs","abstract":"Graph coloring is fundamental to distributed computing. We give an ultrafast distributed algorithm for coloring cluster graphs. These graphs are obtained from the underlying communication network by contracting nodes and edges, and they appear frequently as components in the study of distributed algorithms. In particular, we give a $O(\\log^* n)$-round algorithm to $\\Delta+1$-color cluster graphs of at least polylogarithmic degree. The previous best bound known was $poly(\\log n)$ [Flin et.al, SODA'24]. This properly generalizes results in the COONGEST model and shows that distributed graph problems can be quickly solved even when the node itself is decentralized.","sentences":["Graph coloring is fundamental to distributed computing.","We give an ultrafast distributed algorithm for coloring cluster graphs.","These graphs are obtained from the underlying communication network by contracting nodes and edges, and they appear frequently as components in the study of distributed algorithms.","In particular, we give a $O(\\log^* n)$-round algorithm to $\\Delta+1$-color cluster graphs of at least polylogarithmic degree.","The previous best bound known was $poly(\\log n)$","[Flin et.al, SODA'24].","This properly generalizes results in the COONGEST model and shows that distributed graph problems can be quickly solved even when the node itself is decentralized."],"url":"http://arxiv.org/abs/2405.07725v1","category":"cs.DC"}
{"created":"2024-05-13 13:19:50","title":"Monoidal closure of Grothendieck constructions via $\u03a3$-tractible monoidal structures and Dialectica formulas","abstract":"We study the categorical structure of the Grothendieck construction of an indexed category $\\mathcal{L}:\\mathcal{C}^{op}\\to\\mathbf{CAT}$ and characterise fibred limits, colimits, and monoidal structures. Next, we give sufficient conditions for the monoidal closure of the total category $\\Sigma_\\mathcal{C} \\mathcal{L}$ of a Grothendieck construction of an indexed category $\\mathcal{L}:\\mathcal{C}^{op}\\to\\mathbf{CAT}$. Our analysis is a generalization of G\\\"odel's Dialectica interpretation, and it relies on a novel notion of $\\Sigma$-tractible monoidal structure. As we will see, $\\Sigma$-tractible coproducts simultaneously generalize cocartesian coclosed structures, biproducts and extensive coproducts. We analyse when the closed structure is fibred -- usually it is not.","sentences":["We study the categorical structure of the Grothendieck construction of an indexed category $\\mathcal{L}:\\mathcal{C}^{op}\\to\\mathbf{CAT}$ and characterise fibred limits, colimits, and monoidal structures.","Next, we give sufficient conditions for the monoidal closure of the total category $\\Sigma_\\mathcal{C} \\mathcal{L}$ of a Grothendieck construction of an indexed category $\\mathcal{L}:\\mathcal{C}^{op}\\to\\mathbf{CAT}$. Our analysis is a generalization of G\\\"odel's Dialectica interpretation, and it relies on a novel notion of $\\Sigma$-tractible monoidal structure.","As we will see, $\\Sigma$-tractible coproducts simultaneously generalize cocartesian coclosed structures, biproducts and extensive coproducts.","We analyse when the closed structure is fibred -- usually it is not."],"url":"http://arxiv.org/abs/2405.07724v1","category":"math.CT"}
{"created":"2024-05-13 13:18:49","title":"Coarse or Fine? Recognising Action End States without Labels","abstract":"We focus on the problem of recognising the end state of an action in an image, which is critical for understanding what action is performed and in which manner. We study this focusing on the task of predicting the coarseness of a cut, i.e., deciding whether an object was cut \"coarsely\" or \"finely\". No dataset with these annotated end states is available, so we propose an augmentation method to synthesise training data. We apply this method to cutting actions extracted from an existing action recognition dataset. Our method is object agnostic, i.e., it presupposes the location of the object but not its identity. Starting from less than a hundred images of a whole object, we can generate several thousands images simulating visually diverse cuts of different coarseness. We use our synthetic data to train a model based on UNet and test it on real images showing coarsely/finely cut objects. Results demonstrate that the model successfully recognises the end state of the cutting action despite the domain gap between training and testing, and that the model generalises well to unseen objects.","sentences":["We focus on the problem of recognising the end state of an action in an image, which is critical for understanding what action is performed and in which manner.","We study this focusing on the task of predicting the coarseness of a cut, i.e., deciding whether an object was cut \"coarsely\" or \"finely\".","No dataset with these annotated end states is available, so we propose an augmentation method to synthesise training data.","We apply this method to cutting actions extracted from an existing action recognition dataset.","Our method is object agnostic, i.e., it presupposes the location of the object but not its identity.","Starting from less than a hundred images of a whole object, we can generate several thousands images simulating visually diverse cuts of different coarseness.","We use our synthetic data to train a model based on UNet and test it on real images showing coarsely/finely cut objects.","Results demonstrate that the model successfully recognises the end state of the cutting action despite the domain gap between training and testing, and that the model generalises well to unseen objects."],"url":"http://arxiv.org/abs/2405.07723v1","category":"cs.CV"}
{"created":"2024-05-13 13:16:58","title":"Model Identifiability for Bivariate Failure Time Data with Competing Risk: Non-parametric Cause-specific Hazards and Gamma Frailty","abstract":"In survival analysis, frailty variables are often used to model the association in multivariate survival data. Identifiability is an important issue while working with such multivariate survival data with or without competing risks. In this work, we consider bivariate survival data with competing risks and investigate identifiability results with non-parametric baseline cause-specific hazards and different types of Gamma frailty. Prior to that, we prove that, when both baseline cause-specific hazards and frailty distributions are non-parametric, the model is not identifiable. We also construct a non-identifiable model when baseline cause-specific hazards are non-parametric but frailty distribution may be parametric. Thereafter, we consider four different Gamma frailty distributions, and the corresponding models are shown to be identifiable under fairly general assumptions.","sentences":["In survival analysis, frailty variables are often used to model the association in multivariate survival data.","Identifiability is an important issue while working with such multivariate survival data with or without competing risks.","In this work, we consider bivariate survival data with competing risks and investigate identifiability results with non-parametric baseline cause-specific hazards and different types of Gamma frailty.","Prior to that, we prove that, when both baseline cause-specific hazards and frailty distributions are non-parametric, the model is not identifiable.","We also construct a non-identifiable model when baseline cause-specific hazards are non-parametric but frailty distribution may be parametric.","Thereafter, we consider four different Gamma frailty distributions, and the corresponding models are shown to be identifiable under fairly general assumptions."],"url":"http://arxiv.org/abs/2405.07722v1","category":"math.ST"}
{"created":"2024-05-13 13:08:02","title":"A Unified Sequence Parallelism Approach for Long Context Generative AI","abstract":"Sequence parallelism (SP), which divides the sequence dimension of input tensors across multiple computational devices, is becoming key to unlocking the long-context capabilities of generative AI models. This paper investigates the state-of-the-art SP approaches, i.e. DeepSpeed-Ulysses and Ring-Attention, and proposes a unified SP approach, which is more robust to transformer model architectures and network hardware topology. This paper compares the communication and memory cost of SP and existing parallelism, including data/tensor/zero/expert/pipeline parallelism, and discusses the best practices for designing hybrid 4D parallelism involving SP. We achieved 86\\% MFU on two 8xA800 nodes using SP for sequence length 208K for the LLAMA3-8B model. Our code is publicly available on \\url{https://github.com/feifeibear/long-context-attention}.","sentences":["Sequence parallelism (SP), which divides the sequence dimension of input tensors across multiple computational devices, is becoming key to unlocking the long-context capabilities of generative AI models.","This paper investigates the state-of-the-art SP approaches, i.e. DeepSpeed-Ulysses and Ring-Attention, and proposes a unified SP approach, which is more robust to transformer model architectures and network hardware topology.","This paper compares the communication and memory cost of SP and existing parallelism, including data/tensor/zero/expert/pipeline parallelism, and discusses the best practices for designing hybrid 4D parallelism involving SP.","We achieved 86\\% MFU on two 8xA800 nodes using SP for sequence length 208K for the LLAMA3-8B model.","Our code is publicly available on \\url{https://github.com/feifeibear/long-context-attention}."],"url":"http://arxiv.org/abs/2405.07719v1","category":"cs.LG"}
{"created":"2024-05-13 13:04:10","title":"Equivalent conjectures on blowing-ups of $\\mathbb P^2$","abstract":"We provide a characterization of asymptotical speciality of a nef and big divisor $D$ on an algebraic surface in terms of the arithmetic genus of curves in $D^{\\perp}$. As a consequence we prove that the SHGH conjecture for linear systems on the blowing-up $X_r^2$ of the projective plane at points in very general position is equivalent to the fact that each nef class of is non-special. Finally we prove that if $r < 2^n$ then any nef divisor of $X_r^n$ is asymptotically non-special.","sentences":["We provide a characterization of asymptotical speciality of a nef and big divisor $D$ on an algebraic surface in terms of the arithmetic genus of curves in $D^{\\perp}$. As a consequence we prove that the SHGH conjecture for linear systems on the blowing-up $X_r^2$ of the projective plane at points in very general position is equivalent to the fact that each nef class of is non-special.","Finally we prove that if $r < 2^n$ then any nef divisor of $X_r^n$ is asymptotically non-special."],"url":"http://arxiv.org/abs/2405.07716v1","category":"math.AG"}
{"created":"2024-05-13 12:56:18","title":"Mirror-enhanced acceleration induced geometric phase: towards detection of Unruh effect","abstract":"Fulling-Davies-Unruh effect contains great amount of theoretical importance in various branches of physics. Requirement of very high acceleration hinders its experimental evidence. We propose a novel model to capture this effect experimentally through measuring the Pancharatnam-Berry phase (PBP) of the accelerated quantum system. We find that allowing the qubit's motion along one of the parallel directions of two parallel mirrors and by keeping it in between them, PBP can be greatly enhanced. Our investigation shows that for the current measurable PBP ($\\sim 10^{-6}$ rad), energy gap $\\sim1$ GHz of the qubit, distance between two mirrors $L\\sim150$ (or $300)\\,m$ and detector's position $z_0 \\sim3.3\\,m$ the required acceleration of the qubit can be as low as $a\\sim 10^9~ m/s^2$. This value is potentially very achievable within the current technology and thereby providing a very close to realistic model for experimentally verify the aforesaid quantum phenomenon.","sentences":["Fulling-Davies-Unruh effect contains great amount of theoretical importance in various branches of physics.","Requirement of very high acceleration hinders its experimental evidence.","We propose a novel model to capture this effect experimentally through measuring the Pancharatnam-Berry phase (PBP) of the accelerated quantum system.","We find that allowing the qubit's motion along one of the parallel directions of two parallel mirrors and by keeping it in between them, PBP can be greatly enhanced.","Our investigation shows that for the current measurable PBP ($\\sim 10^{-6}$ rad), energy gap $\\sim1$ GHz of the qubit, distance between two mirrors $L\\sim150$ (or $300)\\,m$ and detector's position $z_0 \\sim3.3\\,m$ the required acceleration of the qubit can be as low as $a\\sim 10^9~ m/s^2$.","This value is potentially very achievable within the current technology and thereby providing a very close to realistic model for experimentally verify the aforesaid quantum phenomenon."],"url":"http://arxiv.org/abs/2405.07711v1","category":"quant-ph"}
{"created":"2024-05-13 12:55:56","title":"Waste Factor and Waste Figure: A Unified Theory for Modeling and Analyzing Wasted Power in Radio Access Networks for Improved Sustainability","abstract":"This paper introduces Waste Factor (W), also denoted as Waste Figure (WF) in dB, a promising new metric for quantifying energy efficiency in a wide range of circuits and systems applications, including data centers and RANs. Also, the networks used to connect data centers and AI computing engines with users for ML applications must become more power efficient. This paper illustrates the limitations of existing energy efficiency metrics that inadequately capture the intricate energy dynamics of RAN components. We delineate the methodology for applying W across various network configurations, including MISO, SIMO, and MIMO systems, and demonstrate the effectiveness of W in identifying energy optimization opportunities. Our findings reveal that W not only offers nuanced insights into the energy performance of RANs but also facilitates informed decision-making for network design and operational efficiency. Furthermore, we show how W can be integrated with other KPIs to guide the development of optimal strategies for enhancing network energy efficiency under different operational conditions. Additionally, we present simulation results for a distributed multi-user MIMO system at 3.5, 17, and 28 GHz, demonstrating overall network power efficiency on a per square kilometer basis, and show how overall W decreases with an increasing number of base stations and increasing carrier frequency. This paper shows that adopting W as a figure of merit can significantly contribute to the sustainability and energy optimization of next-generation wireless communication networks, paving the way for greener and more sustainable, energy-efficient 5G and 6G technologies.","sentences":["This paper introduces Waste Factor (W), also denoted as Waste Figure (WF) in dB, a promising new metric for quantifying energy efficiency in a wide range of circuits and systems applications, including data centers and RANs.","Also, the networks used to connect data centers and AI computing engines with users for ML applications must become more power efficient.","This paper illustrates the limitations of existing energy efficiency metrics that inadequately capture the intricate energy dynamics of RAN components.","We delineate the methodology for applying W across various network configurations, including MISO, SIMO, and MIMO systems, and demonstrate the effectiveness of W in identifying energy optimization opportunities.","Our findings reveal that W not only offers nuanced insights into the energy performance of RANs but also facilitates informed decision-making for network design and operational efficiency.","Furthermore, we show how W can be integrated with other KPIs to guide the development of optimal strategies for enhancing network energy efficiency under different operational conditions.","Additionally, we present simulation results for a distributed multi-user MIMO system at 3.5, 17, and 28 GHz, demonstrating overall network power efficiency on a per square kilometer basis, and show how overall W decreases with an increasing number of base stations and increasing carrier frequency.","This paper shows that adopting W as a figure of merit can significantly contribute to the sustainability and energy optimization of next-generation wireless communication networks, paving the way for greener and more sustainable, energy-efficient 5G and 6G technologies."],"url":"http://arxiv.org/abs/2405.07710v1","category":"cs.NI"}
{"created":"2024-05-13 12:55:03","title":"Ultrafast Structured Spin-Manipulation of Relativistic Lepton Beams","abstract":"Relativistic spin-polarized (SP) lepton beams are important for investigating spin-dependent interaction processes. In particular, spatially structured spin-polarized (SSP) lepton beams may find new applications in material, atomic, nuclear, high-energy physics and new physics beyond the Standard Model. However, realizing ultrafast generation and spin-manipulation of relativistic SSP lepton beams pose significant challenges. Here, we put forward a novel method of ultrafast (picosecond-timescale) generation of a relativistic SSP lepton beam via employing a moderate terahertz (THz) wave in a dielectric-lined waveguide (DWL). We first find that lepton beams with customizable spin-polarization structures can be generated by utilizing different electromagnetic modes, and optimizing the lepton velocity and THz phase velocity can improve efficiency of spin-manipulation and visibility of the SP structure. These SSP beams play a profound role in studying magnetic effects in material physics, chiral-selective chemistry, generation of structured $\\gamma$-rays, etc., and open a new avenue for research on relativistic SP particles.","sentences":["Relativistic spin-polarized (SP) lepton beams are important for investigating spin-dependent interaction processes.","In particular, spatially structured spin-polarized (SSP) lepton beams may find new applications in material, atomic, nuclear, high-energy physics and new physics beyond the Standard Model.","However, realizing ultrafast generation and spin-manipulation of relativistic SSP lepton beams pose significant challenges.","Here, we put forward a novel method of ultrafast (picosecond-timescale) generation of a relativistic SSP lepton beam via employing a moderate terahertz (THz) wave in a dielectric-lined waveguide (DWL).","We first find that lepton beams with customizable spin-polarization structures can be generated by utilizing different electromagnetic modes, and optimizing the lepton velocity and THz phase velocity can improve efficiency of spin-manipulation and visibility of the SP structure.","These SSP beams play a profound role in studying magnetic effects in material physics, chiral-selective chemistry, generation of structured $\\gamma$-rays, etc., and open a new avenue for research on relativistic SP particles."],"url":"http://arxiv.org/abs/2405.07709v1","category":"physics.plasm-ph"}
{"created":"2024-05-13 12:47:39","title":"Set Convergences via bornology","abstract":"This paper examines the equivalence between various set convergences, as studied in [7, 13, 22], induced by an arbitrary bornology $\\mathcal{S}$ on a metric space $(X,d)$. Specifically, it focuses on the upper parts of the following set convergences: convergence deduced through uniform convergence of distance functionals on $\\mathcal{S}$ ($\\tau_{\\mathcal{S},d}$-convergence); convergence with respect to gap functionals determined by $\\mathcal{S}$ ($G_{\\mathcal{S},d}$-convergence); and bornological convergence ($\\mathcal{S}$-convergence). In particular, we give necessary and sufficient conditions on the structure of the bornology $\\mathcal{S}$ for the coincidence of $\\tau_{\\mathcal{S},d}^+$-convergence with $\\mathsf{G}_{\\mathcal{S},d}^+$-convergence, as well as $\\tau_{\\mathcal{S},d}^+$-convergence with $\\mathcal{S}^+$-convergence. A characterization for the equivalence of $\\tau_{\\mathcal{S},d}^+$-convergence and $\\mathcal{S}^+$-convergence, in terms of certain convergence of nets, has also been given earlier by Beer, Naimpally, and Rodriguez-Lopez in [13]. To facilitate our study, we first devise new characterizations for $\\tau_{\\mathcal{S},d}^+$-convergence and $\\mathcal{S}^+$-convergence, which we call their miss-type characterizations.","sentences":["This paper examines the equivalence between various set convergences, as studied in [7, 13, 22], induced by an arbitrary bornology $\\mathcal{S}$ on a metric space $(X,d)$.","Specifically, it focuses on the upper parts of the following set convergences: convergence deduced through uniform convergence of distance functionals on $\\mathcal{S}$ ($\\tau_{\\mathcal{S},d}$-convergence); convergence with respect to gap functionals determined by $\\mathcal{S}$ ($G_{\\mathcal{S},d}$-convergence); and bornological convergence ($\\mathcal{S}$-convergence).","In particular, we give necessary and sufficient conditions on the structure of the bornology $\\mathcal{S}$ for the coincidence of $\\tau_{\\mathcal{S},d}^+$-convergence with $\\mathsf{G}_{\\mathcal{S},d}^+$-convergence, as well as $\\tau_{\\mathcal{S},d}^+$-convergence with $\\mathcal{S}^+$-convergence.","A characterization for the equivalence of $\\tau_{\\mathcal{S},d}^+$-convergence and $\\mathcal{S}^+$-convergence, in terms of certain convergence of nets, has also been given earlier by Beer, Naimpally, and Rodriguez-Lopez in [13].","To facilitate our study, we first devise new characterizations for $\\tau_{\\mathcal{S},d}^+$-convergence and $\\mathcal{S}^+$-convergence, which we call their miss-type characterizations."],"url":"http://arxiv.org/abs/2405.07705v1","category":"math.GN"}
{"created":"2024-05-13 12:35:10","title":"Age-Dependent Analysis and Stochastic Generation of Child-Directed Speech","abstract":"Child-directed speech (CDS) is a particular type of speech that adults use when addressing young children. Its properties also change as a function of extralinguistic factors, such as age of the child being addressed. Access to large amounts of representative and varied CDS would be useful for child language research, as this would enable controlled computational modeling experiments of infant language acquisition with realistic input in terms of quality and quantity. In this study, we describe an approach to model age-dependent linguistic properties of CDS using a language model (LM) trained on CDS transcripts and ages of the recipient children, as obtained from North American English corpora of the CHILDES database. The created LM can then be used to stochastically generate synthetic CDS transcripts in an age-appropriate manner, thereby scaling beyond the original datasets in size. We compare characteristics of the generated CDS against the real speech addressed at children of different ages, showing that the LM manages to capture age-dependent changes in CDS, except for a slight difference in the effective vocabulary size. As a side product, we also provide a systematic characterization of age-dependent linguistic properties of CDS in CHILDES, illustrating how all measured aspects of the CDS change with children's age.","sentences":["Child-directed speech (CDS) is a particular type of speech that adults use when addressing young children.","Its properties also change as a function of extralinguistic factors, such as age of the child being addressed.","Access to large amounts of representative and varied CDS would be useful for child language research, as this would enable controlled computational modeling experiments of infant language acquisition with realistic input in terms of quality and quantity.","In this study, we describe an approach to model age-dependent linguistic properties of CDS using a language model (LM) trained on CDS transcripts and ages of the recipient children, as obtained from North American English corpora of the CHILDES database.","The created LM can then be used to stochastically generate synthetic CDS transcripts in an age-appropriate manner, thereby scaling beyond the original datasets in size.","We compare characteristics of the generated CDS against the real speech addressed at children of different ages, showing that the LM manages to capture age-dependent changes in CDS, except for a slight difference in the effective vocabulary size.","As a side product, we also provide a systematic characterization of age-dependent linguistic properties of CDS in CHILDES, illustrating how all measured aspects of the CDS change with children's age."],"url":"http://arxiv.org/abs/2405.07700v1","category":"cs.CL"}
{"created":"2024-05-13 12:25:56","title":"Holography of Higher Codimension Submanifolds: Riemannian and Conformal","abstract":"We provide a natural generalization to submanifolds of the holographic method used to extract higher-order local invariants of both Riemannian and conformal embeddings, some of which depend on a choice of parallelization of the normal bundle. Qualitatively new behavior is observed in the higher-codimension case, giving rise to new invariants that obstruct the order-by-order construction of unit defining maps. In the conformal setting, a novel invariant (that vanishes in codimension 1) is realized as the leading transverse-order term appearing in a holographically-constructed Willmore invariant. Using these same tools, we also investigate the formal solutions to extension problems off of an embedded submanifold.","sentences":["We provide a natural generalization to submanifolds of the holographic method used to extract higher-order local invariants of both Riemannian and conformal embeddings, some of which depend on a choice of parallelization of the normal bundle.","Qualitatively new behavior is observed in the higher-codimension case, giving rise to new invariants that obstruct the order-by-order construction of unit defining maps.","In the conformal setting, a novel invariant (that vanishes in codimension 1) is realized as the leading transverse-order term appearing in a holographically-constructed Willmore invariant.","Using these same tools, we also investigate the formal solutions to extension problems off of an embedded submanifold."],"url":"http://arxiv.org/abs/2405.07692v1","category":"math.DG"}
{"created":"2024-05-13 12:23:02","title":"Finitely generated groups and harmonic functions of slow growth","abstract":"In this paper, we are mainly concerned with $(\\mathbb{G},\\mu)$-harmonic functions that grow at most polynomially, where $\\mathbb{G}$ is a finitely generated group with a probability measure $\\mu$. In the initial part of the paper, we focus on Lipschitz harmonic functions and how they descend onto finite index subgroups. We discuss the relations between Lipschitz harmonic functions and harmonic functions of linear growth and conclude that for groups of polynomial growth, they coincide. In the latter part of the paper, we specialise to positive harmonic functions and give a characterisation for strong Liouville property in terms of the Green's function. We show that the existence of a non-constant positive harmonic function of polynomial growth guarantees that the group cannot have polynomial growth.","sentences":["In this paper, we are mainly concerned with $(\\mathbb{G},\\mu)$-harmonic functions that grow at most polynomially, where $\\mathbb{G}$ is a finitely generated group with a probability measure $\\mu$. In the initial part of the paper, we focus on Lipschitz harmonic functions and how they descend onto finite index subgroups.","We discuss the relations between Lipschitz harmonic functions and harmonic functions of linear growth and conclude that for groups of polynomial growth, they coincide.","In the latter part of the paper, we specialise to positive harmonic functions and give a characterisation for strong Liouville property in terms of the Green's function.","We show that the existence of a non-constant positive harmonic function of polynomial growth guarantees that the group cannot have polynomial growth."],"url":"http://arxiv.org/abs/2405.07688v1","category":"math.GR"}
{"created":"2024-05-13 12:21:36","title":"Pole trajectories of the $\u039b(1405)$ helps establish its dynamical nature","abstract":"Chiral trajectories of dynamically generated resonances are connected to the SU(3) breaking pattern and their nature. From an analysis of a recent LQCD simulation on the $\\pi\\Sigma-\\bar{K}N$ scattering for $I=0$, and the study of the quark mass dependence of the octet baryons, we determine for the first time the trajectory of the two poles associated to the $\\Lambda(1405)$ towards the symmetric point $(\\mathrm{Tr}[M]=\\mathrm{cte})$ accurately. Our result at unphysical pion mass is consistent with the lattice simulation at $m_\\pi\\simeq 200$ MeV and the extrapolation to the physical point, based on the NLO chiral lagrangian, agrees perfectly well with previous analyses of experimental data. Contrary to other works, we predict qualitatively similar trajectories at LO and up to NLO, being consistent with the dominance of the LO interaction. At the SU(3) symmetric point up to NLO, we obtain that the lower pole is located at $E^{(1)}=1595\\pm8$ MeV, being a singlet representation, while the higher pole belongs to the octet with a mass $E^{(8)}=1600\\pm4$ MeV. This can be tested in the future LQCD simulations.","sentences":["Chiral trajectories of dynamically generated resonances are connected to the SU(3) breaking pattern and their nature.","From an analysis of a recent LQCD simulation on the $\\pi\\Sigma-\\bar{K}N$ scattering for $I=0$, and the study of the quark mass dependence of the octet baryons, we determine for the first time the trajectory of the two poles associated to the $\\Lambda(1405)$ towards the symmetric point $(\\mathrm{Tr}[M]=\\mathrm{cte})$ accurately.","Our result at unphysical pion mass is consistent with the lattice simulation at $m_\\pi\\simeq 200$ MeV and the extrapolation to the physical point, based on the NLO chiral lagrangian, agrees perfectly well with previous analyses of experimental data.","Contrary to other works, we predict qualitatively similar trajectories at LO and up to NLO, being consistent with the dominance of the LO interaction.","At the SU(3) symmetric point up to NLO, we obtain that the lower pole is located at $E^{(1)}=1595\\pm8$ MeV, being a singlet representation, while the higher pole belongs to the octet with a mass $E^{(8)}=1600\\pm4$ MeV. This can be tested in the future LQCD simulations."],"url":"http://arxiv.org/abs/2405.07686v1","category":"hep-ph"}
{"created":"2024-05-13 12:20:05","title":"Comprehensive Analysis of Access Control Models in Edge Computing: Challenges, Solutions, and Future Directions","abstract":"Many contemporary applications, including smart homes and autonomous vehicles, rely on the Internet of Things technology. While cloud computing provides a multitude of valuable services for these applications, it generally imposes constraints on latency-sensitive applications due to the significant propagation delays. As a complementary technique to cloud computing, edge computing situates computing resources closer to the data sources, which reduces the latency and simultaneously alleviates the bandwidth pressure for the cloud and enhances data security. While edge computing offers significant advantages, it also presents significant challenges in access control -- a critical component for safeguarding data. For instance, it is crucial to implement access control mechanisms that are both effective and efficient on resource-constrained devices, ensuring high security without compromising the inherent low latency benefits of edge computing. These challenges drive the development of innovative access control solutions tailored to meet the unique requirements of edge computing environments. We classify related references from the perspectives of multiple data lifecycles (including data collection, storage, and usage), which thoroughly investigates the access control techniques and helps readers understand them systematically. Finally, we reflect on the classification and envisage future research directions.","sentences":["Many contemporary applications, including smart homes and autonomous vehicles, rely on the Internet of Things technology.","While cloud computing provides a multitude of valuable services for these applications, it generally imposes constraints on latency-sensitive applications due to the significant propagation delays.","As a complementary technique to cloud computing, edge computing situates computing resources closer to the data sources, which reduces the latency and simultaneously alleviates the bandwidth pressure for the cloud and enhances data security.","While edge computing offers significant advantages, it also presents significant challenges in access control -- a critical component for safeguarding data.","For instance, it is crucial to implement access control mechanisms that are both effective and efficient on resource-constrained devices, ensuring high security without compromising the inherent low latency benefits of edge computing.","These challenges drive the development of innovative access control solutions tailored to meet the unique requirements of edge computing environments.","We classify related references from the perspectives of multiple data lifecycles (including data collection, storage, and usage), which thoroughly investigates the access control techniques and helps readers understand them systematically.","Finally, we reflect on the classification and envisage future research directions."],"url":"http://arxiv.org/abs/2405.07685v1","category":"eess.SY"}
{"created":"2024-05-13 12:14:54","title":"FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation","abstract":"Singing Accompaniment Generation (SAG), which generates instrumental music to accompany input vocals, is crucial to developing human-AI symbiotic art creation systems. The state-of-the-art method, SingSong, utilizes a multi-stage autoregressive (AR) model for SAG, however, this method is extremely slow as it generates semantic and acoustic tokens recursively, and this makes it impossible for real-time applications. In this paper, we aim to develop a Fast SAG method that can create high-quality and coherent accompaniments. A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly. With diffusion and Mel spectrogram modeling, the proposed method significantly simplifies the AR token-based SingSong framework, and largely accelerates the generation. We also design semantic projection, prior projection blocks as well as a set of loss functions, to ensure the generated accompaniment has semantic and rhythm coherence with the vocal signal. By intensive experimental studies, we demonstrate that the proposed method can generate better samples than SingSong, and accelerate the generation by at least 30 times. Audio samples and code are available at https://fastsag.github.io/.","sentences":["Singing Accompaniment Generation (SAG), which generates instrumental music to accompany input vocals, is crucial to developing human-AI symbiotic art creation systems.","The state-of-the-art method, SingSong, utilizes a multi-stage autoregressive (AR) model for SAG, however, this method is extremely slow as it generates semantic and acoustic tokens recursively, and this makes it impossible for real-time applications.","In this paper, we aim to develop a Fast SAG method that can create high-quality and coherent accompaniments.","A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly.","With diffusion and Mel spectrogram modeling, the proposed method significantly simplifies the AR token-based SingSong framework, and largely accelerates the generation.","We also design semantic projection, prior projection blocks as well as a set of loss functions, to ensure the generated accompaniment has semantic and rhythm coherence with the vocal signal.","By intensive experimental studies, we demonstrate that the proposed method can generate better samples than SingSong, and accelerate the generation by at least 30 times.","Audio samples and code are available at https://fastsag.github.io/."],"url":"http://arxiv.org/abs/2405.07682v1","category":"cs.SD"}
{"created":"2024-05-13 12:10:57","title":"Establishing a Unified Evaluation Framework for Human Motion Generation: A Comparative Analysis of Metrics","abstract":"The development of generative artificial intelligence for human motion generation has expanded rapidly, necessitating a unified evaluation framework. This paper presents a detailed review of eight evaluation metrics for human motion generation, highlighting their unique features and shortcomings. We propose standardized practices through a unified evaluation setup to facilitate consistent model comparisons. Additionally, we introduce a novel metric that assesses diversity in temporal distortion by analyzing warping diversity, thereby enhancing the evaluation of temporal data. We also conduct experimental analyses of three generative models using a publicly available dataset, offering insights into the interpretation of each metric in specific case scenarios. Our goal is to offer a clear, user-friendly evaluation framework for newcomers, complemented by publicly accessible code.","sentences":["The development of generative artificial intelligence for human motion generation has expanded rapidly, necessitating a unified evaluation framework.","This paper presents a detailed review of eight evaluation metrics for human motion generation, highlighting their unique features and shortcomings.","We propose standardized practices through a unified evaluation setup to facilitate consistent model comparisons.","Additionally, we introduce a novel metric that assesses diversity in temporal distortion by analyzing warping diversity, thereby enhancing the evaluation of temporal data.","We also conduct experimental analyses of three generative models using a publicly available dataset, offering insights into the interpretation of each metric in specific case scenarios.","Our goal is to offer a clear, user-friendly evaluation framework for newcomers, complemented by publicly accessible code."],"url":"http://arxiv.org/abs/2405.07680v1","category":"cs.CV"}
{"created":"2024-05-13 12:07:09","title":"Relativistic Roots of $\u03ba$-Entropy","abstract":"The axiomatic structure of the $\\kappa$-statistcal theory is proven. In addition to the first three standard Khinchin--Shannon axioms of continuity, maximality, and expansibility, two further axioms are identified, namely the self-duality axiom and the scaling axiom. It is shown that both the $\\kappa$-entropy and its special limiting case, the classical Boltzmann--Gibbs--Shannon entropy, follow unambiguously from the above new set of five axioms. It has been emphasized that the statistical theory that can be built from $\\kappa$-entropy has a validity that goes beyond physics and can be used to treat physical, natural, or artificial complex systems. The physical origin of the self-duality and scaling axioms has been investigated and traced back to the first principles of relativistic physics, i.e., the Galileo relativity principle and the Einstein principle of the constancy of the speed of light. It has been shown that the $\\kappa$-formalism, which emerges from the $\\kappa$-entropy, can treat both simple (few-body) and complex (statistical) systems in a unified way. Relativistic statistical mechanics based on $\\kappa$-entropy is shown that preserves the main features of classical statistical mechanics (kinetic theory, molecular chaos hypothesis, maximum entropy principle, thermodynamic stability, H-theorem, and Lesche stability). The answers that the $\\kappa$-statistical theory gives to the more-than-a-century-old open problems of relativistic physics, such as how thermodynamic quantities like temperature and entropy vary with the speed of the reference frame, have been emphasized.","sentences":["The axiomatic structure of the $\\kappa$-statistcal theory is proven.","In addition to the first three standard Khinchin--Shannon axioms of continuity, maximality, and expansibility, two further axioms are identified, namely the self-duality axiom and the scaling axiom.","It is shown that both the $\\kappa$-entropy and its special limiting case, the classical Boltzmann--Gibbs--Shannon entropy, follow unambiguously from the above new set of five axioms.","It has been emphasized that the statistical theory that can be built from $\\kappa$-entropy has a validity that goes beyond physics and can be used to treat physical, natural, or artificial complex systems.","The physical origin of the self-duality and scaling axioms has been investigated and traced back to the first principles of relativistic physics, i.e., the Galileo relativity principle and the Einstein principle of the constancy of the speed of light.","It has been shown that the $\\kappa$-formalism, which emerges from the $\\kappa$-entropy, can treat both simple (few-body) and complex (statistical) systems in a unified way.","Relativistic statistical mechanics based on $\\kappa$-entropy is shown that preserves the main features of classical statistical mechanics (kinetic theory, molecular chaos hypothesis, maximum entropy principle, thermodynamic stability, H-theorem, and Lesche stability).","The answers that the $\\kappa$-statistical theory gives to the more-than-a-century-old open problems of relativistic physics, such as how thermodynamic quantities like temperature and entropy vary with the speed of the reference frame, have been emphasized."],"url":"http://arxiv.org/abs/2405.07678v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-13 12:06:49","title":"Unveiling low-dimensional patterns induced by convex non-differentiable regularizers","abstract":"Popular regularizers with non-differentiable penalties, such as Lasso, Elastic Net, Generalized Lasso, or SLOPE, reduce the dimension of the parameter space by inducing sparsity or clustering in the estimators' coordinates. In this paper, we focus on linear regression and explore the asymptotic distributions of the resulting low-dimensional patterns when the number of regressors $p$ is fixed, the number of observations $n$ goes to infinity, and the penalty function increases at the rate of $\\sqrt{n}$. While the asymptotic distribution of the rescaled estimation error can be derived by relatively standard arguments, the convergence of the pattern does not simply follow from the convergence in distribution, and requires a careful and separate treatment. For this purpose, we use the Hausdorff distance as a suitable mode of convergence for subdifferentials, resulting in the desired pattern convergence. Furthermore, we derive the exact limiting probability of recovering the true model pattern. This probability goes to 1 if and only if the penalty scaling constant diverges to infinity and the regularizer-specific asymptotic irrepresentability condition is satisfied. We then propose simple two-step procedures that asymptotically recover the model patterns, irrespective whether the irrepresentability condition holds.   Interestingly, our theory shows that Fused Lasso cannot reliably recover its own clustering pattern, even for independent regressors. It also demonstrates how this problem can be resolved by ``concavifying'' the Fused Lasso penalty coefficients. Additionally, sampling from the asymptotic error distribution facilitates comparisons between different regularizers. We provide short simulation studies showcasing an illustrative comparison between the asymptotic properties of Lasso, Fused Lasso, and SLOPE.","sentences":["Popular regularizers with non-differentiable penalties, such as Lasso, Elastic Net, Generalized Lasso, or SLOPE, reduce the dimension of the parameter space by inducing sparsity or clustering in the estimators' coordinates.","In this paper, we focus on linear regression and explore the asymptotic distributions of the resulting low-dimensional patterns when the number of regressors $p$ is fixed, the number of observations $n$ goes to infinity, and the penalty function increases at the rate of $\\sqrt{n}$. While the asymptotic distribution of the rescaled estimation error can be derived by relatively standard arguments, the convergence of the pattern does not simply follow from the convergence in distribution, and requires a careful and separate treatment.","For this purpose, we use the Hausdorff distance as a suitable mode of convergence for subdifferentials, resulting in the desired pattern convergence.","Furthermore, we derive the exact limiting probability of recovering the true model pattern.","This probability goes to 1 if and only if the penalty scaling constant diverges to infinity and the regularizer-specific asymptotic irrepresentability condition is satisfied.","We then propose simple two-step procedures that asymptotically recover the model patterns, irrespective whether the irrepresentability condition holds.   ","Interestingly, our theory shows that Fused Lasso cannot reliably recover its own clustering pattern, even for independent regressors.","It also demonstrates how this problem can be resolved by ``concavifying'' the Fused Lasso penalty coefficients.","Additionally, sampling from the asymptotic error distribution facilitates comparisons between different regularizers.","We provide short simulation studies showcasing an illustrative comparison between the asymptotic properties of Lasso, Fused Lasso, and SLOPE."],"url":"http://arxiv.org/abs/2405.07677v1","category":"math.ST"}
{"created":"2024-05-13 11:59:20","title":"Impact of white Gaussian internal noise on analog echo-state neural networks","abstract":"In recent years, more and more works have appeared devoted to the analog (hardware) implementation of artificial neural networks, in which neurons and the connection between them are based not on computer calculations, but on physical principles. Such networks offer improved energy efficiency and, in some cases, scalability, but may be susceptible to internal noise. This paper studies the influence of noise on the functioning of recurrent networks using the example of trained echo state networks (ESNs). The most common reservoir connection matrices were chosen as various topologies of ESNs: random uniform and band matrices with different connectivity. White Gaussian noise was chosen as the influence, and according to the way of its introducing it was additive or multiplicative, as well as correlated or uncorrelated. In the paper, we show that the propagation of noise in reservoir is mainly controlled by the statistical properties of the output connection matrix, namely the mean and the mean square. Depending on these values, more correlated or uncorrelated noise accumulates in the network. We also show that there are conditions under which even noise with an intensity of $10^{-20}$ is already enough to completely lose the useful signal. In the article we show which types of noise are most critical for networks with different activation functions (hyperbolic tangent, sigmoid and linear) and if the network is self-closed.","sentences":["In recent years, more and more works have appeared devoted to the analog (hardware) implementation of artificial neural networks, in which neurons and the connection between them are based not on computer calculations, but on physical principles.","Such networks offer improved energy efficiency and, in some cases, scalability, but may be susceptible to internal noise.","This paper studies the influence of noise on the functioning of recurrent networks using the example of trained echo state networks (ESNs).","The most common reservoir connection matrices were chosen as various topologies of ESNs: random uniform and band matrices with different connectivity.","White Gaussian noise was chosen as the influence, and according to the way of its introducing it was additive or multiplicative, as well as correlated or uncorrelated.","In the paper, we show that the propagation of noise in reservoir is mainly controlled by the statistical properties of the output connection matrix, namely the mean and the mean square.","Depending on these values, more correlated or uncorrelated noise accumulates in the network.","We also show that there are conditions under which even noise with an intensity of $10^{-20}$ is already enough to completely lose the useful signal.","In the article we show which types of noise are most critical for networks with different activation functions (hyperbolic tangent, sigmoid and linear) and if the network is self-closed."],"url":"http://arxiv.org/abs/2405.07670v1","category":"cs.NE"}
{"created":"2024-05-13 11:54:03","title":"CrossCert: A Cross-Checking Detection Approach to Patch Robustness Certification for Deep Learning Models","abstract":"Patch robustness certification is an emerging kind of defense technique against adversarial patch attacks with provable guarantees. There are two research lines: certified recovery and certified detection. They aim to label malicious samples with provable guarantees correctly and issue warnings for malicious samples predicted to non-benign labels with provable guarantees, respectively. However, existing certified detection defenders suffer from protecting labels subject to manipulation, and existing certified recovery defenders cannot systematically warn samples about their labels. A certified defense that simultaneously offers robust labels and systematic warning protection against patch attacks is desirable. This paper proposes a novel certified defense technique called CrossCert. CrossCert formulates a novel approach by cross-checking two certified recovery defenders to provide unwavering certification and detection certification. Unwavering certification ensures that a certified sample, when subjected to a patched perturbation, will always be returned with a benign label without triggering any warnings with a provable guarantee. To our knowledge, CrossCert is the first certified detection technique to offer this guarantee. Our experiments show that, with a slightly lower performance than ViP and comparable performance with PatchCensor in terms of detection certification, CrossCert certifies a significant proportion of samples with the guarantee of unwavering certification.","sentences":["Patch robustness certification is an emerging kind of defense technique against adversarial patch attacks with provable guarantees.","There are two research lines: certified recovery and certified detection.","They aim to label malicious samples with provable guarantees correctly and issue warnings for malicious samples predicted to non-benign labels with provable guarantees, respectively.","However, existing certified detection defenders suffer from protecting labels subject to manipulation, and existing certified recovery defenders cannot systematically warn samples about their labels.","A certified defense that simultaneously offers robust labels and systematic warning protection against patch attacks is desirable.","This paper proposes a novel certified defense technique called CrossCert.","CrossCert formulates a novel approach by cross-checking two certified recovery defenders to provide unwavering certification and detection certification.","Unwavering certification ensures that a certified sample, when subjected to a patched perturbation, will always be returned with a benign label without triggering any warnings with a provable guarantee.","To our knowledge, CrossCert is the first certified detection technique to offer this guarantee.","Our experiments show that, with a slightly lower performance than ViP and comparable performance with PatchCensor in terms of detection certification, CrossCert certifies a significant proportion of samples with the guarantee of unwavering certification."],"url":"http://arxiv.org/abs/2405.07668v1","category":"cs.SE"}
{"created":"2024-05-13 11:53:42","title":"Backdoor Removal for Generative Large Language Models","abstract":"With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased accessibility and unrestricted model training on massive textual data from the Internet. A malicious adversary may publish poisoned data online and conduct backdoor attacks on the victim LLMs pre-trained on the poisoned data. Backdoored LLMs behave innocuously for normal queries and generate harmful responses when the backdoor trigger is activated. Despite significant efforts paid to LLMs' safety issues, LLMs are still struggling against backdoor attacks. As Anthropic recently revealed, existing safety training strategies, including supervised fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), fail to revoke the backdoors once the LLM is backdoored during the pre-training stage. In this paper, we present Simulate and Eliminate (SANDE) to erase the undesired backdoored mappings for generative LLMs. We initially propose Overwrite Supervised Fine-tuning (OSFT) for effective backdoor removal when the trigger is known. Then, to handle the scenarios where the trigger patterns are unknown, we integrate OSFT into our two-stage framework, SANDE. Unlike previous works that center on the identification of backdoors, our safety-enhanced LLMs are able to behave normally even when the exact triggers are activated. We conduct comprehensive experiments to show that our proposed SANDE is effective against backdoor attacks while bringing minimal harm to LLMs' powerful capability without any additional access to unbackdoored clean models. We will release the reproducible code.","sentences":["With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning.","Yet, language models' inherent vulnerabilities may be exacerbated due to increased accessibility and unrestricted model training on massive textual data from the Internet.","A malicious adversary may publish poisoned data online and conduct backdoor attacks on the victim LLMs pre-trained on the poisoned data.","Backdoored LLMs behave innocuously for normal queries and generate harmful responses when the backdoor trigger is activated.","Despite significant efforts paid to LLMs' safety issues, LLMs are still struggling against backdoor attacks.","As Anthropic recently revealed, existing safety training strategies, including supervised fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), fail to revoke the backdoors once the LLM is backdoored during the pre-training stage.","In this paper, we present Simulate and Eliminate (SANDE) to erase the undesired backdoored mappings for generative LLMs.","We initially propose Overwrite Supervised Fine-tuning (OSFT) for effective backdoor removal when the trigger is known.","Then, to handle the scenarios where the trigger patterns are unknown, we integrate OSFT into our two-stage framework, SANDE.","Unlike previous works that center on the identification of backdoors, our safety-enhanced LLMs are able to behave normally even when the exact triggers are activated.","We conduct comprehensive experiments to show that our proposed SANDE is effective against backdoor attacks while bringing minimal harm to LLMs' powerful capability without any additional access to unbackdoored clean models.","We will release the reproducible code."],"url":"http://arxiv.org/abs/2405.07667v1","category":"cs.CR"}
{"created":"2024-05-13 11:48:16","title":"New Solutions to Delsarte's Dual Linear Programs","abstract":"Understanding the maximum size of a code with a given minimum distance is a major question in computer science and discrete mathematics. The most fruitful approach for finding asymptotic bounds on such codes is by using Delsarte's theory of association schemes. With this approach, Delsarte constructs a linear program such that its maximum value is an upper bound on the maximum size of a code with a given minimum distance. Bounding this value can be done by finding solutions to the corresponding dual linear program. Delsarte's theory is very general and goes way beyond binary codes.   In this work, we provide universal bounds in the framework of association schemes that generalize the Hamming bound and the Elias-Bassalygo bound, which can be applied to any association scheme constructed from a distance function. These bounds are obtained by constructing new solutions to Delsarte's dual linear program. We instantiate these results and we recover known bounds for $q$-ary codes and for constant-weight binary codes but which didn't come from the linear program method. Our other contribution is to recover, for essentially any $Q$-polynomial scheme, MRRW-type solutions to Delsarte's dual linear program which are inspired by the Laplacian approach of Friedman and Tillich instead of using the Christoffel-Darboux formulas. We show in particular how the second linear programming bound can be interpreted in this framework.","sentences":["Understanding the maximum size of a code with a given minimum distance is a major question in computer science and discrete mathematics.","The most fruitful approach for finding asymptotic bounds on such codes is by using Delsarte's theory of association schemes.","With this approach, Delsarte constructs a linear program such that its maximum value is an upper bound on the maximum size of a code with a given minimum distance.","Bounding this value can be done by finding solutions to the corresponding dual linear program.","Delsarte's theory is very general and goes way beyond binary codes.   ","In this work, we provide universal bounds in the framework of association schemes that generalize the Hamming bound and the Elias-Bassalygo bound, which can be applied to any association scheme constructed from a distance function.","These bounds are obtained by constructing new solutions to Delsarte's dual linear program.","We instantiate these results and we recover known bounds for $q$-ary codes and for constant-weight binary codes but which didn't come from the linear program method.","Our other contribution is to recover, for essentially any $Q$-polynomial scheme, MRRW-type solutions to Delsarte's dual linear program which are inspired by the Laplacian approach of Friedman and Tillich instead of using the Christoffel-Darboux formulas.","We show in particular how the second linear programming bound can be interpreted in this framework."],"url":"http://arxiv.org/abs/2405.07666v1","category":"cs.IT"}
{"created":"2024-05-13 11:45:58","title":"Partial information decomposition as information bottleneck","abstract":"The partial information decomposition (PID) aims to quantify the amount of redundant information that a set of sources provide about a target. Here we show that this goal can be formulated as a type of information bottleneck (IB) problem, which we term the \"redundancy bottleneck\" (RB). The RB formalizes a tradeoff between prediction and compression: it extracts information from the sources that predicts the target, without revealing which source provided the information. It can be understood as a generalization \"Blackwell redundancy\", which we previously proposed as a principled measure of PID redundancy. The \"RB curve\" quantifies the prediction/compression tradeoff at multiple scales. This curve can also be quantified for individual sources, allowing subsets of redundant sources to be identified without combinatorial optimization. We provide an efficient iterative algorithm for computing the RB curve.","sentences":["The partial information decomposition (PID) aims to quantify the amount of redundant information that a set of sources provide about a target.","Here we show that this goal can be formulated as a type of information bottleneck (IB) problem, which we term the \"redundancy bottleneck\" (RB).","The RB formalizes a tradeoff between prediction and compression: it extracts information from the sources that predicts the target, without revealing which source provided the information.","It can be understood as a generalization \"Blackwell redundancy\", which we previously proposed as a principled measure of PID redundancy.","The \"RB curve\" quantifies the prediction/compression tradeoff at multiple scales.","This curve can also be quantified for individual sources, allowing subsets of redundant sources to be identified without combinatorial optimization.","We provide an efficient iterative algorithm for computing the RB curve."],"url":"http://arxiv.org/abs/2405.07665v1","category":"cs.IT"}
{"created":"2024-05-13 11:45:22","title":"Geospatial Knowledge Graphs","abstract":"Geospatial knowledge graphs have emerged as a novel paradigm for representing and reasoning over geospatial information. In this framework, entities such as places, people, events, and observations are depicted as nodes, while their relationships are represented as edges. This graph-based data format lays the foundation for creating a \"FAIR\" (Findable, Accessible, Interoperable, and Reusable) environment, facilitating the management and analysis of geographic information. This entry first introduces key concepts in knowledge graphs along with their associated standardization and tools. It then delves into the application of knowledge graphs in geography and environmental sciences, emphasizing their role in bridging symbolic and subsymbolic GeoAI to address cross-disciplinary geospatial challenges. At the end, new research directions related to geospatial knowledge graphs are outlined.","sentences":["Geospatial knowledge graphs have emerged as a novel paradigm for representing and reasoning over geospatial information.","In this framework, entities such as places, people, events, and observations are depicted as nodes, while their relationships are represented as edges.","This graph-based data format lays the foundation for creating a \"FAIR\" (Findable, Accessible, Interoperable, and Reusable) environment, facilitating the management and analysis of geographic information.","This entry first introduces key concepts in knowledge graphs along with their associated standardization and tools.","It then delves into the application of knowledge graphs in geography and environmental sciences, emphasizing their role in bridging symbolic and subsymbolic GeoAI to address cross-disciplinary geospatial challenges.","At the end, new research directions related to geospatial knowledge graphs are outlined."],"url":"http://arxiv.org/abs/2405.07664v1","category":"cs.AI"}
{"created":"2024-05-13 11:43:38","title":"Squeezing Lemons with Hammers: An Evaluation of AutoML and Tabular Deep Learning for Data-Scarce Classification Applications","abstract":"Many industry verticals are confronted with small-sized tabular data. In this low-data regime, it is currently unclear whether the best performance can be expected from simple baselines, or more complex machine learning approaches that leverage meta-learning and ensembling. On 44 tabular classification datasets with sample sizes $\\leq$ 500, we find that L2-regularized logistic regression performs similar to state-of-the-art automated machine learning (AutoML) frameworks (AutoPrognosis, AutoGluon) and off-the-shelf deep neural networks (TabPFN, HyperFast) on the majority of the benchmark datasets. We therefore recommend to consider logistic regression as the first choice for data-scarce applications with tabular data and provide practitioners with best practices for further method selection.","sentences":["Many industry verticals are confronted with small-sized tabular data.","In this low-data regime, it is currently unclear whether the best performance can be expected from simple baselines, or more complex machine learning approaches that leverage meta-learning and ensembling.","On 44 tabular classification datasets with sample sizes $\\leq$ 500, we find that L2-regularized logistic regression performs similar to state-of-the-art automated machine learning (AutoML) frameworks (AutoPrognosis, AutoGluon) and off-the-shelf deep neural networks (TabPFN, HyperFast) on the majority of the benchmark datasets.","We therefore recommend to consider logistic regression as the first choice for data-scarce applications with tabular data and provide practitioners with best practices for further method selection."],"url":"http://arxiv.org/abs/2405.07662v1","category":"cs.LG"}
{"created":"2024-05-13 11:37:50","title":"Beyond traditional Magnetic Resonance processing with Artificial Intelligence","abstract":"Smart signal processing approaches using Artificial Intelligence are gaining momentum in NMR applications. In this study, we demonstrate that AI offers new opportunities beyond tasks addressed by traditional techniques. We developed and trained several artificial neural networks in our new toolbox Magnetic Resonance with Artificial intelligence (MR-Ai) to solve three \"impossible\" problems: quadrature detection using only Echo (or Anti-Echo) modulation from the traditional Echo/Anti-Echo scheme; accessing uncertainty of signal intensity at each point in a spectrum processed by any given method; and defining a reference-free score for quantitative access of NMR spectrum quality. Our findings highlight the potential of AI techniques to revolutionize NMR processing and analysis.","sentences":["Smart signal processing approaches using Artificial Intelligence are gaining momentum in NMR applications.","In this study, we demonstrate that AI offers new opportunities beyond tasks addressed by traditional techniques.","We developed and trained several artificial neural networks in our new toolbox Magnetic Resonance with Artificial intelligence (MR-Ai) to solve three \"impossible\" problems: quadrature detection using only Echo (or Anti-Echo) modulation from the traditional Echo/Anti-Echo scheme; accessing uncertainty of signal intensity at each point in a spectrum processed by any given method; and defining a reference-free score for quantitative access of NMR spectrum quality.","Our findings highlight the potential of AI techniques to revolutionize NMR processing and analysis."],"url":"http://arxiv.org/abs/2405.07657v1","category":"physics.bio-ph"}
{"created":"2024-05-13 11:32:54","title":"Non-Rigid Designators in Modal and Temporal Free Description Logics (Extended Version)","abstract":"Definite descriptions, such as 'the General Chair of KR 2024', are a semantically transparent device for object identification in knowledge representation. In first-order modal logic, definite descriptions have been widely investigated for their non-rigidity, which allows them to designate different objects (or none at all) at different states. We propose expressive modal description logics with non-rigid definite descriptions and names, and investigate decidability and complexity of the satisfaction problem. We first systematically link satisfiability for the one-variable fragment of first-order modal logic with counting to our modal description logics. Then, we prove a promising NEXPTIME-completeness result for concept satisfiability for the fundamental epistemic multi-agent logic $\\mathbf{S5}^{n}$ and its neighbours, and show that some expressive logics that are undecidable with constant domain become decidable (but Ackermann-hard) with expanding domains. Finally, we conduct a fine-grained analysis of decidability of temporal logics.","sentences":["Definite descriptions, such as 'the General Chair of KR 2024', are a semantically transparent device for object identification in knowledge representation.","In first-order modal logic, definite descriptions have been widely investigated for their non-rigidity, which allows them to designate different objects (or none at all) at different states.","We propose expressive modal description logics with non-rigid definite descriptions and names, and investigate decidability and complexity of the satisfaction problem.","We first systematically link satisfiability for the one-variable fragment of first-order modal logic with counting to our modal description logics.","Then, we prove a promising NEXPTIME-completeness result for concept satisfiability for the fundamental epistemic multi-agent logic $\\mathbf{S5}^{n}$ and its neighbours, and show that some expressive logics that are undecidable with constant domain become decidable (but Ackermann-hard) with expanding domains.","Finally, we conduct a fine-grained analysis of decidability of temporal logics."],"url":"http://arxiv.org/abs/2405.07656v1","category":"cs.LO"}
{"created":"2024-05-13 11:32:05","title":"Quality-aware Selective Fusion Network for V-D-T Salient Object Detection","abstract":"Depth images and thermal images contain the spatial geometry information and surface temperature information, which can act as complementary information for the RGB modality. However, the quality of the depth and thermal images is often unreliable in some challenging scenarios, which will result in the performance degradation of the two-modal based salient object detection (SOD). Meanwhile, some researchers pay attention to the triple-modal SOD task, where they attempt to explore the complementarity of the RGB image, the depth image, and the thermal image. However, existing triple-modal SOD methods fail to perceive the quality of depth maps and thermal images, which leads to performance degradation when dealing with scenes with low-quality depth and thermal images. Therefore, we propose a quality-aware selective fusion network (QSF-Net) to conduct VDT salient object detection, which contains three subnets including the initial feature extraction subnet, the quality-aware region selection subnet, and the region-guided selective fusion subnet. Firstly, except for extracting features, the initial feature extraction subnet can generate a preliminary prediction map from each modality via a shrinkage pyramid architecture. Then, we design the weakly-supervised quality-aware region selection subnet to generate the quality-aware maps. Concretely, we first find the high-quality and low-quality regions by using the preliminary predictions, which further constitute the pseudo label that can be used to train this subnet. Finally, the region-guided selective fusion subnet purifies the initial features under the guidance of the quality-aware maps, and then fuses the triple-modal features and refines the edge details of prediction maps through the intra-modality and inter-modality attention (IIA) module and the edge refinement (ER) module, respectively. Extensive experiments are performed on VDT-2048","sentences":["Depth images and thermal images contain the spatial geometry information and surface temperature information, which can act as complementary information for the RGB modality.","However, the quality of the depth and thermal images is often unreliable in some challenging scenarios, which will result in the performance degradation of the two-modal based salient object detection (SOD).","Meanwhile, some researchers pay attention to the triple-modal SOD task, where they attempt to explore the complementarity of the RGB image, the depth image, and the thermal image.","However, existing triple-modal SOD methods fail to perceive the quality of depth maps and thermal images, which leads to performance degradation when dealing with scenes with low-quality depth and thermal images.","Therefore, we propose a quality-aware selective fusion network (QSF-Net) to conduct VDT salient object detection, which contains three subnets including the initial feature extraction subnet, the quality-aware region selection subnet, and the region-guided selective fusion subnet.","Firstly, except for extracting features, the initial feature extraction subnet can generate a preliminary prediction map from each modality via a shrinkage pyramid architecture.","Then, we design the weakly-supervised quality-aware region selection subnet to generate the quality-aware maps.","Concretely, we first find the high-quality and low-quality regions by using the preliminary predictions, which further constitute the pseudo label that can be used to train this subnet.","Finally, the region-guided selective fusion subnet purifies the initial features under the guidance of the quality-aware maps, and then fuses the triple-modal features and refines the edge details of prediction maps through the intra-modality and inter-modality attention (IIA) module and the edge refinement (ER) module, respectively.","Extensive experiments are performed on VDT-2048"],"url":"http://arxiv.org/abs/2405.07655v1","category":"cs.CV"}
{"created":"2024-05-13 11:30:48","title":"A note on \u00e9tale $(\\varphi,\u0393)$-modules in families","abstract":"Let $\\Lambda$ be a complete noetherian local ring with finite residue field of characteristic $p$ and $K/\\mathbb{Q}_p$ a $p$-adic field. We show that, by deformation of the structure sheaf on the (transversal) prismatic site of a bounded $p$-adic formal scheme $\\mathfrak{X}$, the category of prismatic $(\\Lambda,F)$-crystals on $\\mathfrak{X}$ is equivalent to $\\Lambda$-\\'etale local systems on the generic adic fiber of $\\mathfrak{X}$ and that the cohomology of $(\\Lambda,F)$-crystals recovers the pro-\\'etale cohomology of the corresponding local systems. The proof follows the strategy used in \\cite{bhatt2023prismatic} and \\cite{marks2023prismatic}. From this we construct an isomorphism between Iwasawa cohomology of a $p$-adic Lie extension of $K$ and prismatic cohomology. Following \\cite{wu2021galois}, we then reprove Dee's classical result \\cite{article} on the equivalence between families of Galois representations and \\'etale $(\\varphi,\\Gamma)$-modules.","sentences":["Let $\\Lambda$ be a complete noetherian local ring with finite residue field of characteristic $p$ and $K/\\mathbb{Q}_p$ a $p$-adic field.","We show that, by deformation of the structure sheaf on the (transversal) prismatic site of a bounded $p$-adic formal scheme $\\mathfrak{X}$, the category of prismatic $(\\Lambda,F)$-crystals on $\\mathfrak{X}$ is equivalent to $\\Lambda$-\\'etale local systems on the generic adic fiber of $\\mathfrak{X}$ and that the cohomology of $(\\Lambda,F)$-crystals recovers the pro-\\'etale cohomology of the corresponding local systems.","The proof follows the strategy used in \\cite{bhatt2023prismatic} and \\cite{marks2023prismatic}.","From this we construct an isomorphism between Iwasawa cohomology of a $p$-adic Lie extension of $K$ and prismatic cohomology.","Following \\cite{wu2021galois}, we then reprove Dee's classical result \\cite{article} on the equivalence between families of Galois representations and \\'etale $(\\varphi,\\Gamma)$-modules."],"url":"http://arxiv.org/abs/2405.07654v1","category":"math.NT"}
{"created":"2024-05-13 11:28:58","title":"Fast Training Data Acquisition for Object Detection and Segmentation using Black Screen Luminance Keying","abstract":"Deep Neural Networks (DNNs) require large amounts of annotated training data for a good performance. Often this data is generated using manual labeling (error-prone and time-consuming) or rendering (requiring geometry and material information). Both approaches make it difficult or uneconomic to apply them to many small-scale applications. A fast and straightforward approach of acquiring the necessary training data would allow the adoption of deep learning to even the smallest of applications. Chroma keying is the process of replacing a color (usually blue or green) with another background. Instead of chroma keying, we propose luminance keying for fast and straightforward training image acquisition. We deploy a black screen with high light absorption (99.99\\%) to record roughly 1-minute long videos of our target objects, circumventing typical problems of chroma keying, such as color bleeding or color overlap between background color and object color. Next we automatically mask our objects using simple brightness thresholding, saving the need for manual annotation. Finally, we automatically place the objects on random backgrounds and train a 2D object detector. We do extensive evaluation of the performance on the widely-used YCB-V object set and compare favourably to other conventional techniques such as rendering, without needing 3D meshes, materials or any other information of our target objects and in a fraction of the time needed for other approaches. Our work demonstrates highly accurate training data acquisition allowing to start training state-of-the-art networks within minutes.","sentences":["Deep Neural Networks (DNNs) require large amounts of annotated training data for a good performance.","Often this data is generated using manual labeling (error-prone and time-consuming) or rendering (requiring geometry and material information).","Both approaches make it difficult or uneconomic to apply them to many small-scale applications.","A fast and straightforward approach of acquiring the necessary training data would allow the adoption of deep learning to even the smallest of applications.","Chroma keying is the process of replacing a color (usually blue or green) with another background.","Instead of chroma keying, we propose luminance keying for fast and straightforward training image acquisition.","We deploy a black screen with high light absorption (99.99\\%) to record roughly 1-minute long videos of our target objects, circumventing typical problems of chroma keying, such as color bleeding or color overlap between background color and object color.","Next we automatically mask our objects using simple brightness thresholding, saving the need for manual annotation.","Finally, we automatically place the objects on random backgrounds and train a 2D object detector.","We do extensive evaluation of the performance on the widely-used YCB-V object set and compare favourably to other conventional techniques such as rendering, without needing 3D meshes, materials or any other information of our target objects and in a fraction of the time needed for other approaches.","Our work demonstrates highly accurate training data acquisition allowing to start training state-of-the-art networks within minutes."],"url":"http://arxiv.org/abs/2405.07653v1","category":"cs.CV"}
{"created":"2024-05-13 11:24:53","title":"G-VOILA: Gaze-Facilitated Information Querying in Daily Scenarios","abstract":"Modern information querying systems are progressively incorporating multimodal inputs like vision and audio. However, the integration of gaze -- a modality deeply linked to user intent and increasingly accessible via gaze-tracking wearables -- remains underexplored. This paper introduces a novel gaze-facilitated information querying paradigm, named G-VOILA, which synergizes users' gaze, visual field, and voice-based natural language queries to facilitate a more intuitive querying process. In a user-enactment study involving 21 participants in 3 daily scenarios (p = 21, scene = 3), we revealed the ambiguity in users' query language and a gaze-voice coordination pattern in users' natural query behaviors with G-VOILA. Based on the quantitative and qualitative findings, we developed a design framework for the G-VOILA paradigm, which effectively integrates the gaze data with the in-situ querying context. Then we implemented a G-VOILA proof-of-concept using cutting-edge deep learning techniques. A follow-up user study (p = 16, scene = 2) demonstrates its effectiveness by achieving both higher objective score and subjective score, compared to a baseline without gaze data. We further conducted interviews and provided insights for future gaze-facilitated information querying systems.","sentences":["Modern information querying systems are progressively incorporating multimodal inputs like vision and audio.","However, the integration of gaze -- a modality deeply linked to user intent and increasingly accessible via gaze-tracking wearables -- remains underexplored.","This paper introduces a novel gaze-facilitated information querying paradigm, named G-VOILA, which synergizes users' gaze, visual field, and voice-based natural language queries to facilitate a more intuitive querying process.","In a user-enactment study involving 21 participants in 3 daily scenarios (p = 21, scene = 3), we revealed the ambiguity in users' query language and a gaze-voice coordination pattern in users' natural query behaviors with G-VOILA.","Based on the quantitative and qualitative findings, we developed a design framework for the G-VOILA paradigm, which effectively integrates the gaze data with the in-situ querying context.","Then we implemented a G-VOILA proof-of-concept using cutting-edge deep learning techniques.","A follow-up user study (p = 16, scene = 2) demonstrates its effectiveness by achieving both higher objective score and subjective score, compared to a baseline without gaze data.","We further conducted interviews and provided insights for future gaze-facilitated information querying systems."],"url":"http://arxiv.org/abs/2405.07652v1","category":"cs.HC"}
{"created":"2024-05-13 11:14:40","title":"Arrow of Time in Estimation and Control: Duality Theory Beyond the Linear Gaussian Model","abstract":"Duality between estimation and control is a foundational concept in Control Theory. Most students learn about the elementary duality -- between observability and controllability -- in their first graduate course in linear systems theory. Therefore, it comes as a surprise that for a more general class of nonlinear stochastic systems (hidden Markov models or HMMs), duality is incomplete.   Our objective in writing this article is two-fold: (i) To describe the difficulty in extending duality to HMMs; and (ii) To discuss its recent resolution by the authors. A key message is that the main difficulty in extending duality comes from time reversal in going from estimation to control. The reason for time reversal is explained with the aid of the familiar linear deterministic and linear Gaussian models. The explanation is used to motivate the difference between the linear and the nonlinear models. Once the difference is understood, duality for HMMs is described based on our recent work. The article also includes a comparison and discussion of the different types of duality considered in literature.","sentences":["Duality between estimation and control is a foundational concept in Control Theory.","Most students learn about the elementary duality -- between observability and controllability -- in their first graduate course in linear systems theory.","Therefore, it comes as a surprise that for a more general class of nonlinear stochastic systems (hidden Markov models or HMMs), duality is incomplete.   ","Our objective in writing this article is two-fold: (i) To describe the difficulty in extending duality to HMMs; and (ii) To discuss its recent resolution by the authors.","A key message is that the main difficulty in extending duality comes from time reversal in going from estimation to control.","The reason for time reversal is explained with the aid of the familiar linear deterministic and linear Gaussian models.","The explanation is used to motivate the difference between the linear and the nonlinear models.","Once the difference is understood, duality for HMMs is described based on our recent work.","The article also includes a comparison and discussion of the different types of duality considered in literature."],"url":"http://arxiv.org/abs/2405.07650v1","category":"math.OC"}
{"created":"2024-05-13 11:06:11","title":"A Hessian-Based Field Deformer for Real-Time Topology-Aware Shape Editing","abstract":"Shape manipulation is a central research topic in computer graphics. Topology editing, such as breaking apart connections, joining disconnected ends, and filling/opening a topological hole, is generally more challenging than geometry editing. In this paper, we observe that the saddle points of the signed distance function (SDF) provide useful hints for altering surface topology deliberately. Based on this key observation, we parameterize the SDF into a cubic trivariate tensor-product B-spline function $F$ whose saddle points $\\{\\boldsymbol{s}_i\\}$ can be quickly exhausted based on a subdivision-based root-finding technique coupled with Newton's method. Users can select one of the candidate points, say $\\boldsymbol{s}_i$, to edit the topology in real time. In implementation, we add a compactly supported B-spline function rooted at $\\boldsymbol{s}_i$, which we call a \\textit{deformer} in this paper, to $F$, with its local coordinate system aligning with the three eigenvectors of the Hessian. Combined with ray marching technique, our interactive system operates at 30 FPS. Additionally, our system empowers users to create desired bulges or concavities on the surface. An extensive user study indicates that our system is user-friendly and intuitive to operate. We demonstrate the effectiveness and usefulness of our system in a range of applications, including fixing surface reconstruction errors, artistic work design, 3D medical imaging and simulation, and antiquity restoration. Please refer to the attached video for a demonstration.","sentences":["Shape manipulation is a central research topic in computer graphics.","Topology editing, such as breaking apart connections, joining disconnected ends, and filling/opening a topological hole, is generally more challenging than geometry editing.","In this paper, we observe that the saddle points of the signed distance function (SDF) provide useful hints for altering surface topology deliberately.","Based on this key observation, we parameterize the SDF into a cubic trivariate tensor-product B-spline function $F$ whose saddle points $\\{\\boldsymbol{s}_i\\}$ can be quickly exhausted based on a subdivision-based root-finding technique coupled with Newton's method.","Users can select one of the candidate points, say $\\boldsymbol{s}_i$, to edit the topology in real time.","In implementation, we add a compactly supported B-spline function rooted at $\\boldsymbol{s}_i$, which we call a \\textit{deformer} in this paper, to $F$, with its local coordinate system aligning with the three eigenvectors of the Hessian.","Combined with ray marching technique, our interactive system operates at 30 FPS.","Additionally, our system empowers users to create desired bulges or concavities on the surface.","An extensive user study indicates that our system is user-friendly and intuitive to operate.","We demonstrate the effectiveness and usefulness of our system in a range of applications, including fixing surface reconstruction errors, artistic work design, 3D medical imaging and simulation, and antiquity restoration.","Please refer to the attached video for a demonstration."],"url":"http://arxiv.org/abs/2405.07644v1","category":"cs.GR"}
{"created":"2024-05-13 11:00:27","title":"Evaluating Speech Enhancement Systems Through Listening Effort","abstract":"Understanding degraded speech is demanding, requiring increased listening effort (LE). Evaluating processed and unprocessed speech with respect to LE can objectively indicate if speech enhancement systems benefit listeners. However, existing methods for measuring LE are complex and not widely applicable. In this study, we propose a simple method to evaluate speech intelligibility and LE simultaneously without additional strain on subjects or operators. We assess this method using results from two independent studies in Norway and Denmark, testing 76 (50+26) subjects across 9 (6+3) processing conditions. Despite differences in evaluation setups, subject recruitment, and processing systems, trends are strikingly similar, demonstrating the proposed method's robustness and ease of implementation into existing practices.","sentences":["Understanding degraded speech is demanding, requiring increased listening effort (LE).","Evaluating processed and unprocessed speech with respect to LE can objectively indicate if speech enhancement systems benefit listeners.","However, existing methods for measuring LE are complex and not widely applicable.","In this study, we propose a simple method to evaluate speech intelligibility and LE simultaneously without additional strain on subjects or operators.","We assess this method using results from two independent studies in Norway and Denmark, testing 76 (50+26) subjects across 9 (6","+3) processing conditions.","Despite differences in evaluation setups, subject recruitment, and processing systems, trends are strikingly similar, demonstrating the proposed method's robustness and ease of implementation into existing practices."],"url":"http://arxiv.org/abs/2405.07641v1","category":"eess.AS"}
{"created":"2024-05-13 11:00:25","title":"Hyperparameter Importance Analysis for Multi-Objective AutoML","abstract":"Hyperparameter optimization plays a pivotal role in enhancing the predictive performance and generalization capabilities of ML models. However, in many applications, we do not only care about predictive performance but also about objectives such as inference time, memory, or energy consumption. In such MOO scenarios, determining the importance of hyperparameters poses a significant challenge due to the complex interplay between the conflicting objectives. In this paper, we propose the first method for assessing the importance of hyperparameters in the context of multi-objective hyperparameter optimization. Our approach leverages surrogate-based hyperparameter importance (HPI) measures, i.e. fANOVA and ablation paths, to provide insights into the impact of hyperparameters on the optimization objectives. Specifically, we compute the a-priori scalarization of the objectives and determine the importance of the hyperparameters for different objective tradeoffs. Through extensive empirical evaluations on diverse benchmark datasets with three different objectives paired with accuracy, namely time, demographic parity, and energy consumption, we demonstrate the effectiveness and robustness of our proposed method. Our findings not only offer valuable guidance for hyperparameter tuning in MOO tasks but also contribute to advancing the understanding of HPI in complex optimization scenarios.","sentences":["Hyperparameter optimization plays a pivotal role in enhancing the predictive performance and generalization capabilities of ML models.","However, in many applications, we do not only care about predictive performance but also about objectives such as inference time, memory, or energy consumption.","In such MOO scenarios, determining the importance of hyperparameters poses a significant challenge due to the complex interplay between the conflicting objectives.","In this paper, we propose the first method for assessing the importance of hyperparameters in the context of multi-objective hyperparameter optimization.","Our approach leverages surrogate-based hyperparameter importance (HPI) measures, i.e. fANOVA and ablation paths, to provide insights into the impact of hyperparameters on the optimization objectives.","Specifically, we compute the a-priori scalarization of the objectives and determine the importance of the hyperparameters for different objective tradeoffs.","Through extensive empirical evaluations on diverse benchmark datasets with three different objectives paired with accuracy, namely time, demographic parity, and energy consumption, we demonstrate the effectiveness and robustness of our proposed method.","Our findings not only offer valuable guidance for hyperparameter tuning in MOO tasks but also contribute to advancing the understanding of HPI in complex optimization scenarios."],"url":"http://arxiv.org/abs/2405.07640v1","category":"cs.LG"}
{"created":"2024-05-13 10:53:41","title":"DoLLM: How Large Language Models Understanding Network Flow Data to Detect Carpet Bombing DDoS","abstract":"It is an interesting question Can and How Large Language Models (LLMs) understand non-language network data, and help us detect unknown malicious flows. This paper takes Carpet Bombing as a case study and shows how to exploit LLMs' powerful capability in the networking area. Carpet Bombing is a new DDoS attack that has dramatically increased in recent years, significantly threatening network infrastructures. It targets multiple victim IPs within subnets, causing congestion on access links and disrupting network services for a vast number of users. Characterized by low-rates, multi-vectors, these attacks challenge traditional DDoS defenses. We propose DoLLM, a DDoS detection model utilizes open-source LLMs as backbone. By reorganizing non-contextual network flows into Flow-Sequences and projecting them into LLMs semantic space as token embeddings, DoLLM leverages LLMs' contextual understanding to extract flow representations in overall network context. The representations are used to improve the DDoS detection performance. We evaluate DoLLM with public datasets CIC-DDoS2019 and real NetFlow trace from Top-3 countrywide ISP. The tests have proven that DoLLM possesses strong detection capabilities. Its F1 score increased by up to 33.3% in zero-shot scenarios and by at least 20.6% in real ISP traces.","sentences":["It is an interesting question Can and How Large Language Models (LLMs) understand non-language network data, and help us detect unknown malicious flows.","This paper takes Carpet Bombing as a case study and shows how to exploit LLMs' powerful capability in the networking area.","Carpet Bombing is a new DDoS attack that has dramatically increased in recent years, significantly threatening network infrastructures.","It targets multiple victim IPs within subnets, causing congestion on access links and disrupting network services for a vast number of users.","Characterized by low-rates, multi-vectors, these attacks challenge traditional DDoS defenses.","We propose DoLLM, a DDoS detection model utilizes open-source LLMs as backbone.","By reorganizing non-contextual network flows into Flow-Sequences and projecting them into LLMs semantic space as token embeddings, DoLLM leverages LLMs' contextual understanding to extract flow representations in overall network context.","The representations are used to improve the DDoS detection performance.","We evaluate DoLLM with public datasets CIC-DDoS2019 and real NetFlow trace from Top-3 countrywide ISP.","The tests have proven that DoLLM possesses strong detection capabilities.","Its F1 score increased by up to 33.3% in zero-shot scenarios and by at least 20.6% in real ISP traces."],"url":"http://arxiv.org/abs/2405.07638v1","category":"cs.NI"}
{"created":"2024-05-13 10:48:03","title":"Nonlinear Network Identifiability with Full Excitations","abstract":"We derive conditions for the identifiability of nonlinear networks characterized by additive dynamics at the level of the edges when all the nodes are excited. In contrast to linear systems, we show that the measurement of all sinks is necessary and sufficient for the identifiability of directed acyclic graphs, under the assumption that dynamics are described by analytic functions without constant terms (i.e., $f(0)=0$). But if constant terms are present, then the identifiability is impossible as soon as one node has more than one in-neighbor. In the case of general digraphs where cycles can exist, we consider additively separable functions for the analysis of the identifiability, and we show that the measurement of one node of all the sinks of the condensation digraph is necessary and sufficient. Several examples are added to illustrate the results.","sentences":["We derive conditions for the identifiability of nonlinear networks characterized by additive dynamics at the level of the edges when all the nodes are excited.","In contrast to linear systems, we show that the measurement of all sinks is necessary and sufficient for the identifiability of directed acyclic graphs, under the assumption that dynamics are described by analytic functions without constant terms (i.e., $f(0)=0$).","But if constant terms are present, then the identifiability is impossible as soon as one node has more than one in-neighbor.","In the case of general digraphs where cycles can exist, we consider additively separable functions for the analysis of the identifiability, and we show that the measurement of one node of all the sinks of the condensation digraph is necessary and sufficient.","Several examples are added to illustrate the results."],"url":"http://arxiv.org/abs/2405.07636v1","category":"math.OC"}
{"created":"2024-05-13 10:47:42","title":"Koopman Analysis of the Singularly-Perturbed van der Pol Oscillator","abstract":"The Koopman operator framework holds promise for spectral analysis of nonlinear dynamical systems based on linear operators. Eigenvalues and eigenfunctions of the Koopman operator, so-called Koopman eigenvalues and Koopman eigenfunctions, respectively, mirror global properties of the system's flow. In this paper we perform the Koopman analysis of the singularly-perturbed van der Pol system. First, we show the spectral signature depending on singular perturbation: how two Koopman principle eigenvalues are ordered and what distinct shapes emerge in their associated Koopman eigenfunctions. Second, we discuss the singular limit of the Koopman operator, which is derived through the concatenation of Koopman operators for the fast and slow subsystems. From the spectral properties of the Koopman operator for the singularl-perturbed system and the singular limit, we suggest that the Koopman eigenfunctions inherit geometric properties of the singularly-perturbed system. These results are applicable to general planar singularly-perturbed systems with stable limit cycles.","sentences":["The Koopman operator framework holds promise for spectral analysis of nonlinear dynamical systems based on linear operators.","Eigenvalues and eigenfunctions of the Koopman operator, so-called Koopman eigenvalues and Koopman eigenfunctions, respectively, mirror global properties of the system's flow.","In this paper we perform the Koopman analysis of the singularly-perturbed van der Pol system.","First, we show the spectral signature depending on singular perturbation: how two Koopman principle eigenvalues are ordered and what distinct shapes emerge in their associated Koopman eigenfunctions.","Second, we discuss the singular limit of the Koopman operator, which is derived through the concatenation of Koopman operators for the fast and slow subsystems.","From the spectral properties of the Koopman operator for the singularl-perturbed system and the singular limit, we suggest that the Koopman eigenfunctions inherit geometric properties of the singularly-perturbed system.","These results are applicable to general planar singularly-perturbed systems with stable limit cycles."],"url":"http://arxiv.org/abs/2405.07635v1","category":"math.DS"}
{"created":"2024-05-13 10:39:23","title":"Improving prediction models by incorporating external data with weights based on similarity","abstract":"In clinical settings, we often face the challenge of building prediction models based on small observational data sets. For example, such a data set might be from a medical center in a multi-center study. Differences between centers might be large, thus requiring specific models based on the data set from the target center. Still, we want to borrow information from the external centers, to deal with small sample sizes. There are approaches that either assign weights to each external data set or each external observation. To incorporate information on differences between data sets and observations, we propose an approach that combines both into weights that can be incorporated into a likelihood for fitting regression models. Specifically, we suggest weights at the data set level that incorporate information on how well the models that provide the observation weights distinguish between data sets. Technically, this takes the form of inverse probability weighting. We explore different scenarios where covariates and outcomes differ among data sets, informing our simulation design for method evaluation. The concept of effective sample size is used for understanding the effectiveness of our subgroup modeling approach. We demonstrate our approach through a clinical application, predicting applied radiotherapy doses for cancer patients. Generally, the proposed approach provides improved prediction performance when external data sets are similar. We thus provide a method for quantifying similarity of external data sets to the target data set and use this similarity to include external observations for improving performance in a target data set prediction modeling task with small data.","sentences":["In clinical settings, we often face the challenge of building prediction models based on small observational data sets.","For example, such a data set might be from a medical center in a multi-center study.","Differences between centers might be large, thus requiring specific models based on the data set from the target center.","Still, we want to borrow information from the external centers, to deal with small sample sizes.","There are approaches that either assign weights to each external data set or each external observation.","To incorporate information on differences between data sets and observations, we propose an approach that combines both into weights that can be incorporated into a likelihood for fitting regression models.","Specifically, we suggest weights at the data set level that incorporate information on how well the models that provide the observation weights distinguish between data sets.","Technically, this takes the form of inverse probability weighting.","We explore different scenarios where covariates and outcomes differ among data sets, informing our simulation design for method evaluation.","The concept of effective sample size is used for understanding the effectiveness of our subgroup modeling approach.","We demonstrate our approach through a clinical application, predicting applied radiotherapy doses for cancer patients.","Generally, the proposed approach provides improved prediction performance when external data sets are similar.","We thus provide a method for quantifying similarity of external data sets to the target data set and use this similarity to include external observations for improving performance in a target data set prediction modeling task with small data."],"url":"http://arxiv.org/abs/2405.07631v1","category":"stat.ME"}
{"created":"2024-05-13 10:37:50","title":"AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using Large Language Models","abstract":"Detecting anomaly edges for dynamic graphs aims to identify edges significantly deviating from the normal pattern and can be applied in various domains, such as cybersecurity, financial transactions and AIOps. With the evolving of time, the types of anomaly edges are emerging and the labeled anomaly samples are few for each type. Current methods are either designed to detect randomly inserted edges or require sufficient labeled data for model training, which harms their applicability for real-world applications. In this paper, we study this problem by cooperating with the rich knowledge encoded in large language models(LLMs) and propose a method, namely AnomalyLLM. To align the dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to generate the representations of edges and reprograms the edges using the prototypes of word embeddings. Along with the encoder, we design an in-context learning framework that integrates the information of a few labeled samples to achieve few-shot anomaly detection. Experiments on four datasets reveal that AnomalyLLM can not only significantly improve the performance of few-shot anomaly detection, but also achieve superior results on new anomalies without any update of model parameters.","sentences":["Detecting anomaly edges for dynamic graphs aims to identify edges significantly deviating from the normal pattern and can be applied in various domains, such as cybersecurity, financial transactions and AIOps.","With the evolving of time, the types of anomaly edges are emerging and the labeled anomaly samples are few for each type.","Current methods are either designed to detect randomly inserted edges or require sufficient labeled data for model training, which harms their applicability for real-world applications.","In this paper, we study this problem by cooperating with the rich knowledge encoded in large language models(LLMs) and propose a method, namely AnomalyLLM.","To align the dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to generate the representations of edges and reprograms the edges using the prototypes of word embeddings.","Along with the encoder, we design an in-context learning framework that integrates the information of a few labeled samples to achieve few-shot anomaly detection.","Experiments on four datasets reveal that AnomalyLLM can not only significantly improve the performance of few-shot anomaly detection, but also achieve superior results on new anomalies without any update of model parameters."],"url":"http://arxiv.org/abs/2405.07626v1","category":"cs.LG"}
{"created":"2024-05-13 10:35:50","title":"Analytical lower bound on the number of queries to a black-box unitary operation in deterministic exact transformations of unknown unitary operations","abstract":"Several counter-intuitive go-theorems have recently been shown for transformations of unknown unitary operations; deterministic and exact complex conjugation, inversion, and transposition of a general $d$-dimensional unknown unitary operation are implementable with a finite number of queries of the black-box unitary operation. However, the minimum numbers of the required queries are not known except for $d=2$ unitary inversion and unitary transposition (numerical) and unitary conjugation (analytic). In this work, we derive complementary no-go theorems for deterministic and exact implementations of inversion and transposition of a $d$-dimensional unknown unitary operation under certain numbers of queries. The obtained no-go theorem indicates that the analytical lower bound of the number of queries for unitary inversion is $d^2$ and that for unitary transposition is $4$ for $d=2$ and $d+3$ for $d \\geq 3$. We have developed a new framework that utilizes differentiation to obtain the analytical lower bounds on the number of queries to the black-box unitary operation required to implement a transformation given by a general differentiable function mapping a unitary operation to another unitary operation, which reproduces the lower bound of the number of queries for unitary complex conjugation $d-1$. As a corollary, we show the relationship between the tightness of the lower bounds and the existence of optimal catalytic transformations, which is a new aspect recently identified in the study of deterministic and exact unitary inversion. Furthermore, we extend our framework to the probabilistic setting where the transformation is required to succeed with a certain probability, thereby showing a possible tradeoff relation between query numbers and the required success probability.","sentences":["Several counter-intuitive go-theorems have recently been shown for transformations of unknown unitary operations; deterministic and exact complex conjugation, inversion, and transposition of a general $d$-dimensional unknown unitary operation are implementable with a finite number of queries of the black-box unitary operation.","However, the minimum numbers of the required queries are not known except for $d=2$ unitary inversion and unitary transposition (numerical) and unitary conjugation (analytic).","In this work, we derive complementary no-go theorems for deterministic and exact implementations of inversion and transposition of a $d$-dimensional unknown unitary operation under certain numbers of queries.","The obtained no-go theorem indicates that the analytical lower bound of the number of queries for unitary inversion is $d^2$ and that for unitary transposition is $4$ for $d=2$ and $d+3$ for $d \\geq 3$.","We have developed a new framework that utilizes differentiation to obtain the analytical lower bounds on the number of queries to the black-box unitary operation required to implement a transformation given by a general differentiable function mapping a unitary operation to another unitary operation, which reproduces the lower bound of the number of queries for unitary complex conjugation $d-1$. As a corollary, we show the relationship between the tightness of the lower bounds and the existence of optimal catalytic transformations, which is a new aspect recently identified in the study of deterministic and exact unitary inversion.","Furthermore, we extend our framework to the probabilistic setting where the transformation is required to succeed with a certain probability, thereby showing a possible tradeoff relation between query numbers and the required success probability."],"url":"http://arxiv.org/abs/2405.07625v1","category":"quant-ph"}
{"created":"2024-05-13 10:27:17","title":"De novo antibody design with SE(3) diffusion","abstract":"We introduce IgDiff, an antibody variable domain diffusion model based on a general protein backbone diffusion framework which was extended to handle multiple chains. Assessing the designability and novelty of the structures generated with our model, we find that IgDiff produces highly designable antibodies that can contain novel binding regions. The backbone dihedral angles of sampled structures show good agreement with a reference antibody distribution. We verify these designed antibodies experimentally and find that all express with high yield. Finally, we compare our model with a state-of-the-art generative backbone diffusion model on a range of antibody design tasks, such as the design of the complementarity determining regions or the pairing of a light chain to an existing heavy chain, and show improved properties and designability.","sentences":["We introduce IgDiff, an antibody variable domain diffusion model based on a general protein backbone diffusion framework which was extended to handle multiple chains.","Assessing the designability and novelty of the structures generated with our model, we find that IgDiff produces highly designable antibodies that can contain novel binding regions.","The backbone dihedral angles of sampled structures show good agreement with a reference antibody distribution.","We verify these designed antibodies experimentally and find that all express with high yield.","Finally, we compare our model with a state-of-the-art generative backbone diffusion model on a range of antibody design tasks, such as the design of the complementarity determining regions or the pairing of a light chain to an existing heavy chain, and show improved properties and designability."],"url":"http://arxiv.org/abs/2405.07622v1","category":"q-bio.BM"}
{"created":"2024-05-13 10:27:11","title":"Towards Adaptive IMFs -- Generalization of utility functions in Multi-Agent Frameworks","abstract":"Intent Management Function (IMF) is an integral part of future-generation networks. In recent years, there has been some work on AI-based IMFs that can handle conflicting intents and prioritize the global objective based on apriori definition of the utility function and accorded priorities for competing intents. Some of the earlier works use Multi-Agent Reinforcement Learning (MARL) techniques with AdHoc Teaming (AHT) approaches for efficient conflict handling in IMF. However, the success of such frameworks in real-life scenarios requires them to be flexible to business situations. The intent priorities can change and the utility function, which measures the extent of intent fulfilment, may also vary in definition. This paper proposes a novel mechanism whereby the IMF can generalize to different forms of utility functions and change of intent priorities at run-time without additional training. Such generalization ability, without additional training requirements, would help to deploy IMF in live networks where customer intents and priorities change frequently. Results on the network emulator demonstrate the efficacy of the approach, scalability for new intents, outperforming existing techniques that require additional training to achieve the same degree of flexibility thereby saving cost, and increasing efficiency and adaptability.","sentences":["Intent Management Function (IMF) is an integral part of future-generation networks.","In recent years, there has been some work on AI-based IMFs that can handle conflicting intents and prioritize the global objective based on apriori definition of the utility function and accorded priorities for competing intents.","Some of the earlier works use Multi-Agent Reinforcement Learning (MARL) techniques with AdHoc Teaming (AHT) approaches for efficient conflict handling in IMF.","However, the success of such frameworks in real-life scenarios requires them to be flexible to business situations.","The intent priorities can change and the utility function, which measures the extent of intent fulfilment, may also vary in definition.","This paper proposes a novel mechanism whereby the IMF can generalize to different forms of utility functions and change of intent priorities at run-time without additional training.","Such generalization ability, without additional training requirements, would help to deploy IMF in live networks where customer intents and priorities change frequently.","Results on the network emulator demonstrate the efficacy of the approach, scalability for new intents, outperforming existing techniques that require additional training to achieve the same degree of flexibility thereby saving cost, and increasing efficiency and adaptability."],"url":"http://arxiv.org/abs/2405.07621v2","category":"cs.LG"}
{"created":"2024-05-13 10:24:21","title":"Conditional well-posedness and data-driven method for identifying the dynamic source in a coupled diffusion system from one single boundary measurement","abstract":"This work considers the inverse dynamic source problem arising from the time-domain fluorescence diffuse optical tomography (FDOT). We recover the dynamic distributions of fluorophores in biological tissue by the one single boundary measurement in finite time domain. We build the uniqueness theorem of this inverse problem. After that, we introduce a weighted norm and establish the conditional stability of Lipschitz type for the inverse problem by this weighted norm. The numerical inversions are considered under the framework of the deep neural networks (DNNs). We establish the generalization error estimates rigorously derived from Lipschitz conditional stability of inverse problem. Finally, we propose the reconstruction algorithms and give several numerical examples illustrating the performance of the proposed inversion schemes.","sentences":["This work considers the inverse dynamic source problem arising from the time-domain fluorescence diffuse optical tomography (FDOT).","We recover the dynamic distributions of fluorophores in biological tissue by the one single boundary measurement in finite time domain.","We build the uniqueness theorem of this inverse problem.","After that, we introduce a weighted norm and establish the conditional stability of Lipschitz type for the inverse problem by this weighted norm.","The numerical inversions are considered under the framework of the deep neural networks (DNNs).","We establish the generalization error estimates rigorously derived from Lipschitz conditional stability of inverse problem.","Finally, we propose the reconstruction algorithms and give several numerical examples illustrating the performance of the proposed inversion schemes."],"url":"http://arxiv.org/abs/2405.07616v1","category":"math.NA"}
{"created":"2024-05-13 10:24:05","title":"ViWikiFC: Fact-Checking for Vietnamese Wikipedia-Based Textual Knowledge Source","abstract":"Fact-checking is essential due to the explosion of misinformation in the media ecosystem. Although false information exists in every language and country, most research to solve the problem mainly concentrated on huge communities like English and Chinese. Low-resource languages like Vietnamese are necessary to explore corpora and models for fact verification. To bridge this gap, we construct ViWikiFC, the first manual annotated open-domain corpus for Vietnamese Wikipedia Fact Checking more than 20K claims generated by converting evidence sentences extracted from Wikipedia articles. We analyze our corpus through many linguistic aspects, from the new dependency rate, the new n-gram rate, and the new word rate. We conducted various experiments for Vietnamese fact-checking, including evidence retrieval and verdict prediction. BM25 and InfoXLM (Large) achieved the best results in two tasks, with BM25 achieving an accuracy of 88.30% for SUPPORTS, 86.93% for REFUTES, and only 56.67% for the NEI label in the evidence retrieval task, InfoXLM (Large) achieved an F1 score of 86.51%. Furthermore, we also conducted a pipeline approach, which only achieved a strict accuracy of 67.00% when using InfoXLM (Large) and BM25. These results demonstrate that our dataset is challenging for the Vietnamese language model in fact-checking tasks.","sentences":["Fact-checking is essential due to the explosion of misinformation in the media ecosystem.","Although false information exists in every language and country, most research to solve the problem mainly concentrated on huge communities like English and Chinese.","Low-resource languages like Vietnamese are necessary to explore corpora and models for fact verification.","To bridge this gap, we construct ViWikiFC, the first manual annotated open-domain corpus for Vietnamese Wikipedia Fact Checking more than 20K claims generated by converting evidence sentences extracted from Wikipedia articles.","We analyze our corpus through many linguistic aspects, from the new dependency rate, the new n-gram rate, and the new word rate.","We conducted various experiments for Vietnamese fact-checking, including evidence retrieval and verdict prediction.","BM25 and InfoXLM (Large) achieved the best results in two tasks, with BM25 achieving an accuracy of 88.30% for SUPPORTS, 86.93% for REFUTES, and only 56.67% for the NEI label in the evidence retrieval task, InfoXLM (Large) achieved an F1 score of 86.51%.","Furthermore, we also conducted a pipeline approach, which only achieved a strict accuracy of 67.00% when using InfoXLM (Large) and BM25.","These results demonstrate that our dataset is challenging for the Vietnamese language model in fact-checking tasks."],"url":"http://arxiv.org/abs/2405.07615v1","category":"cs.CL"}
{"created":"2024-05-13 10:20:46","title":"On the Flux-Intensity Relation of Molecular Clouds","abstract":"In this work, we report a study on the relationship between flux and intensity for molecular clouds. Our analysis is established on high-quality CO images from the Milky Way Imaging Scroll Painting (MWISP) project. The new flux-intensity relation characterizes the flux variation of molecular clouds above specific intensity levels. We found that the flux-intensity relation exhibits two prominent features. First, the flux-intensity relation generally follows exponential shapes; secondly, hierarchical structures of molecular clouds are imprinted on flux-intensity relations. Specifically, 12CO flux-intensity relations are composed of one or more exponential segments, and for molecular clouds with segmented flux-intensity relations, the edge and the flux of the high-temperature component are strikingly consistent with 13CO emission. Further analysis shows that a similar relationship also exists between 13CO flux-intensity relations and C18O emission. The mean brightness temperature of molecular clouds is tightly associated with the decay rate of flux, the break temperature of exponential segments, and, to a certain extent, the flux fraction of the high-temperature component. Broadly, the flux-intensity relation of a molecular tracer, either in optically thick or in optically thin cases, has the capability to outline the silhouette of internal structures of molecular clouds, proving to be a potent tool for probing structures of molecular clouds.","sentences":["In this work, we report a study on the relationship between flux and intensity for molecular clouds.","Our analysis is established on high-quality CO images from the Milky Way Imaging Scroll Painting (MWISP) project.","The new flux-intensity relation characterizes the flux variation of molecular clouds above specific intensity levels.","We found that the flux-intensity relation exhibits two prominent features.","First, the flux-intensity relation generally follows exponential shapes; secondly, hierarchical structures of molecular clouds are imprinted on flux-intensity relations.","Specifically, 12CO flux-intensity relations are composed of one or more exponential segments, and for molecular clouds with segmented flux-intensity relations, the edge and the flux of the high-temperature component are strikingly consistent with 13CO emission.","Further analysis shows that a similar relationship also exists between 13CO flux-intensity relations and C18O emission.","The mean brightness temperature of molecular clouds is tightly associated with the decay rate of flux, the break temperature of exponential segments, and, to a certain extent, the flux fraction of the high-temperature component.","Broadly, the flux-intensity relation of a molecular tracer, either in optically thick or in optically thin cases, has the capability to outline the silhouette of internal structures of molecular clouds, proving to be a potent tool for probing structures of molecular clouds."],"url":"http://arxiv.org/abs/2405.07610v1","category":"astro-ph.GA"}
{"created":"2024-05-13 10:20:31","title":"NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition","abstract":"Available training data for named entity recognition (NER) often contains a significant percentage of incorrect labels for entity types and entity boundaries. Such label noise poses challenges for supervised learning and may significantly deteriorate model quality. To address this, prior work proposed various noise-robust learning approaches capable of learning from data with partially incorrect labels. These approaches are typically evaluated using simulated noise where the labels in a clean dataset are automatically corrupted. However, as we show in this paper, this leads to unrealistic noise that is far easier to handle than real noise caused by human error or semi-automatic annotation. To enable the study of the impact of various types of real noise, we introduce NoiseBench, an NER benchmark consisting of clean training data corrupted with 6 types of real noise, including expert errors, crowdsourcing errors, automatic annotation errors and LLM errors. We present an analysis that shows that real noise is significantly more challenging than simulated noise, and show that current state-of-the-art models for noise-robust learning fall far short of their theoretically achievable upper bound. We release NoiseBench to the research community.","sentences":["Available training data for named entity recognition (NER) often contains a significant percentage of incorrect labels for entity types and entity boundaries.","Such label noise poses challenges for supervised learning and may significantly deteriorate model quality.","To address this, prior work proposed various noise-robust learning approaches capable of learning from data with partially incorrect labels.","These approaches are typically evaluated using simulated noise where the labels in a clean dataset are automatically corrupted.","However, as we show in this paper, this leads to unrealistic noise that is far easier to handle than real noise caused by human error or semi-automatic annotation.","To enable the study of the impact of various types of real noise, we introduce NoiseBench, an NER benchmark consisting of clean training data corrupted with 6 types of real noise, including expert errors, crowdsourcing errors, automatic annotation errors and LLM errors.","We present an analysis that shows that real noise is significantly more challenging than simulated noise, and show that current state-of-the-art models for noise-robust learning fall far short of their theoretically achievable upper bound.","We release NoiseBench to the research community."],"url":"http://arxiv.org/abs/2405.07609v1","category":"cs.CL"}
{"created":"2024-05-13 10:09:37","title":"AIris: An AI-powered Wearable Assistive Device for the Visually Impaired","abstract":"Assistive technologies for the visually impaired have evolved to facilitate interaction with a complex and dynamic world. In this paper, we introduce AIris, an AI-powered wearable device that provides environmental awareness and interaction capabilities to visually impaired users. AIris combines a sophisticated camera mounted on eyewear with a natural language processing interface, enabling users to receive real-time auditory descriptions of their surroundings. We have created a functional prototype system that operates effectively in real-world conditions. AIris demonstrates the ability to accurately identify objects and interpret scenes, providing users with a sense of spatial awareness previously unattainable with traditional assistive devices. The system is designed to be cost-effective and user-friendly, supporting general and specialized tasks: face recognition, scene description, text reading, object recognition, money counting, note-taking, and barcode scanning. AIris marks a transformative step, bringing AI enhancements to assistive technology, enabling rich interactions with a human-like feel.","sentences":["Assistive technologies for the visually impaired have evolved to facilitate interaction with a complex and dynamic world.","In this paper, we introduce AIris, an AI-powered wearable device that provides environmental awareness and interaction capabilities to visually impaired users.","AIris combines a sophisticated camera mounted on eyewear with a natural language processing interface, enabling users to receive real-time auditory descriptions of their surroundings.","We have created a functional prototype system that operates effectively in real-world conditions.","AIris demonstrates the ability to accurately identify objects and interpret scenes, providing users with a sense of spatial awareness previously unattainable with traditional assistive devices.","The system is designed to be cost-effective and user-friendly, supporting general and specialized tasks: face recognition, scene description, text reading, object recognition, money counting, note-taking, and barcode scanning.","AIris marks a transformative step, bringing AI enhancements to assistive technology, enabling rich interactions with a human-like feel."],"url":"http://arxiv.org/abs/2405.07606v1","category":"cs.HC"}
{"created":"2024-05-13 10:07:36","title":"Reducing Risk for Assistive Reinforcement Learning Policies with Diffusion Models","abstract":"Care-giving and assistive robotics, driven by advancements in AI, offer promising solutions to meet the growing demand for care, particularly in the context of increasing numbers of individuals requiring assistance. This creates a pressing need for efficient and safe assistive devices, particularly in light of heightened demand due to war-related injuries. While cost has been a barrier to accessibility, technological progress is able to democratize these solutions. Safety remains a paramount concern, especially given the intricate interactions between assistive robots and humans. This study explores the application of reinforcement learning (RL) and imitation learning, in improving policy design for assistive robots. The proposed approach makes the risky policies safer without additional environmental interactions. Through experimentation using simulated environments, the enhancement of the conventional RL approaches in tasks related to assistive robotics is demonstrated.","sentences":["Care-giving and assistive robotics, driven by advancements in AI, offer promising solutions to meet the growing demand for care, particularly in the context of increasing numbers of individuals requiring assistance.","This creates a pressing need for efficient and safe assistive devices, particularly in light of heightened demand due to war-related injuries.","While cost has been a barrier to accessibility, technological progress is able to democratize these solutions.","Safety remains a paramount concern, especially given the intricate interactions between assistive robots and humans.","This study explores the application of reinforcement learning (RL) and imitation learning, in improving policy design for assistive robots.","The proposed approach makes the risky policies safer without additional environmental interactions.","Through experimentation using simulated environments, the enhancement of the conventional RL approaches in tasks related to assistive robotics is demonstrated."],"url":"http://arxiv.org/abs/2405.07603v1","category":"cs.RO"}
{"created":"2024-05-13 10:06:25","title":"Robustness of Interferometric Power to Sudden Death","abstract":"We study the dissipative dynamics of interferometric power as a discordlike measure in Markovian environments, such as dephasing, depolarizing, and generalized amplitude damping. Moreover, we compare the dynamics of interferometric power and entanglement by choosing proper initial conditions. Our study shows that in all cases where the sudden death of entanglement appears, interferometric power decays asymptotically. Therefore, quantum metrology based on interferometric power is more robust than entanglement.","sentences":["We study the dissipative dynamics of interferometric power as a discordlike measure in Markovian environments, such as dephasing, depolarizing, and generalized amplitude damping.","Moreover, we compare the dynamics of interferometric power and entanglement by choosing proper initial conditions.","Our study shows that in all cases where the sudden death of entanglement appears, interferometric power decays asymptotically.","Therefore, quantum metrology based on interferometric power is more robust than entanglement."],"url":"http://arxiv.org/abs/2405.07602v1","category":"quant-ph"}
{"created":"2024-05-13 10:03:34","title":"On-device Online Learning and Semantic Management of TinyML Systems","abstract":"Recent advances in Tiny Machine Learning (TinyML) empower low-footprint embedded devices for real-time on-device Machine Learning. While many acknowledge the potential benefits of TinyML, its practical implementation presents unique challenges. This study aims to bridge the gap between prototyping single TinyML models and developing reliable TinyML systems in production: (1) Embedded devices operate in dynamically changing conditions. Existing TinyML solutions primarily focus on inference, with models trained offline on powerful machines and deployed as static objects. However, static models may underperform in the real world due to evolving input data distributions. We propose online learning to enable training on constrained devices, adapting local models towards the latest field conditions. (2) Nevertheless, current on-device learning methods struggle with heterogeneous deployment conditions and the scarcity of labeled data when applied across numerous devices. We introduce federated meta-learning incorporating online learning to enhance model generalization, facilitating rapid learning. This approach ensures optimal performance among distributed devices by knowledge sharing. (3) Moreover, TinyML's pivotal advantage is widespread adoption. Embedded devices and TinyML models prioritize extreme efficiency, leading to diverse characteristics ranging from memory and sensors to model architectures. Given their diversity and non-standardized representations, managing these resources becomes challenging as TinyML systems scale up. We present semantic management for the joint management of models and devices at scale. We demonstrate our methods through a basic regression example and then assess them in three real-world TinyML applications: handwritten character image classification, keyword audio classification, and smart building presence detection, confirming our approaches' effectiveness.","sentences":["Recent advances in Tiny Machine Learning (TinyML) empower low-footprint embedded devices for real-time on-device Machine Learning.","While many acknowledge the potential benefits of TinyML, its practical implementation presents unique challenges.","This study aims to bridge the gap between prototyping single TinyML models and developing reliable TinyML systems in production: (1) Embedded devices operate in dynamically changing conditions.","Existing TinyML solutions primarily focus on inference, with models trained offline on powerful machines and deployed as static objects.","However, static models may underperform in the real world due to evolving input data distributions.","We propose online learning to enable training on constrained devices, adapting local models towards the latest field conditions.","(2) Nevertheless, current on-device learning methods struggle with heterogeneous deployment conditions and the scarcity of labeled data when applied across numerous devices.","We introduce federated meta-learning incorporating online learning to enhance model generalization, facilitating rapid learning.","This approach ensures optimal performance among distributed devices by knowledge sharing.","(3) Moreover, TinyML's pivotal advantage is widespread adoption.","Embedded devices and TinyML models prioritize extreme efficiency, leading to diverse characteristics ranging from memory and sensors to model architectures.","Given their diversity and non-standardized representations, managing these resources becomes challenging as TinyML systems scale up.","We present semantic management for the joint management of models and devices at scale.","We demonstrate our methods through a basic regression example and then assess them in three real-world TinyML applications: handwritten character image classification, keyword audio classification, and smart building presence detection, confirming our approaches' effectiveness."],"url":"http://arxiv.org/abs/2405.07601v1","category":"cs.LG"}
{"created":"2024-05-13 10:03:03","title":"Integrity Monitoring of 3D Object Detection in Automated Driving Systems using Raw Activation Patterns and Spatial Filtering","abstract":"The deep neural network (DNN) models are widely used for object detection in automated driving systems (ADS). Yet, such models are prone to errors which can have serious safety implications. Introspection and self-assessment models that aim to detect such errors are therefore of paramount importance for the safe deployment of ADS. Current research on this topic has focused on techniques to monitor the integrity of the perception mechanism in ADS. Existing introspection models in the literature, however, largely concentrate on detecting perception errors by assigning equal importance to all parts of the input data frame to the perception module. This generic approach overlooks the varying safety significance of different objects within a scene, which obscures the recognition of safety-critical errors, posing challenges in assessing the reliability of perception in specific, crucial instances. Motivated by this shortcoming of state of the art, this paper proposes a novel method integrating raw activation patterns of the underlying DNNs, employed by the perception module, analysis with spatial filtering techniques. This novel approach enhances the accuracy of runtime introspection of the DNN-based 3D object detections by selectively focusing on an area of interest in the data, thereby contributing to the safety and efficacy of ADS perception self-assessment processes.","sentences":["The deep neural network (DNN) models are widely used for object detection in automated driving systems (ADS).","Yet, such models are prone to errors which can have serious safety implications.","Introspection and self-assessment models that aim to detect such errors are therefore of paramount importance for the safe deployment of ADS.","Current research on this topic has focused on techniques to monitor the integrity of the perception mechanism in ADS.","Existing introspection models in the literature, however, largely concentrate on detecting perception errors by assigning equal importance to all parts of the input data frame to the perception module.","This generic approach overlooks the varying safety significance of different objects within a scene, which obscures the recognition of safety-critical errors, posing challenges in assessing the reliability of perception in specific, crucial instances.","Motivated by this shortcoming of state of the art, this paper proposes a novel method integrating raw activation patterns of the underlying DNNs, employed by the perception module, analysis with spatial filtering techniques.","This novel approach enhances the accuracy of runtime introspection of the DNN-based 3D object detections by selectively focusing on an area of interest in the data, thereby contributing to the safety and efficacy of ADS perception self-assessment processes."],"url":"http://arxiv.org/abs/2405.07600v1","category":"cs.CV"}
{"created":"2024-05-13 09:56:57","title":"Environmental Matching Attack Against Unmanned Aerial Vehicles Object Detection","abstract":"Object detection techniques for Unmanned Aerial Vehicles (UAVs) rely on Deep Neural Networks (DNNs), which are vulnerable to adversarial attacks. Nonetheless, adversarial patches generated by existing algorithms in the UAV domain pay very little attention to the naturalness of adversarial patches. Moreover, imposing constraints directly on adversarial patches makes it difficult to generate patches that appear natural to the human eye while ensuring a high attack success rate. We notice that patches are natural looking when their overall color is consistent with the environment. Therefore, we propose a new method named Environmental Matching Attack(EMA) to address the issue of optimizing the adversarial patch under the constraints of color. To the best of our knowledge, this paper is the first to consider natural patches in the domain of UAVs. The EMA method exploits strong prior knowledge of a pretrained stable diffusion to guide the optimization direction of the adversarial patch, where the text guidance can restrict the color of the patch. To better match the environment, the contrast and brightness of the patch are appropriately adjusted. Instead of optimizing the adversarial patch itself, we optimize an adversarial perturbation patch which initializes to zero so that the model can better trade off attacking performance and naturalness. Experiments conducted on the DroneVehicle and Carpk datasets have shown that our work can reach nearly the same attack performance in the digital attack(no greater than 2 in mAP$\\%$), surpass the baseline method in the physical specific scenarios, and exhibit a significant advantage in terms of naturalness in visualization and color difference with the environment.","sentences":["Object detection techniques for Unmanned Aerial Vehicles (UAVs) rely on Deep Neural Networks (DNNs), which are vulnerable to adversarial attacks.","Nonetheless, adversarial patches generated by existing algorithms in the UAV domain pay very little attention to the naturalness of adversarial patches.","Moreover, imposing constraints directly on adversarial patches makes it difficult to generate patches that appear natural to the human eye while ensuring a high attack success rate.","We notice that patches are natural looking when their overall color is consistent with the environment.","Therefore, we propose a new method named Environmental Matching Attack(EMA) to address the issue of optimizing the adversarial patch under the constraints of color.","To the best of our knowledge, this paper is the first to consider natural patches in the domain of UAVs.","The EMA method exploits strong prior knowledge of a pretrained stable diffusion to guide the optimization direction of the adversarial patch, where the text guidance can restrict the color of the patch.","To better match the environment, the contrast and brightness of the patch are appropriately adjusted.","Instead of optimizing the adversarial patch itself, we optimize an adversarial perturbation patch which initializes to zero so that the model can better trade off attacking performance and naturalness.","Experiments conducted on the DroneVehicle and Carpk datasets have shown that our work can reach nearly the same attack performance in the digital attack(no greater than 2 in mAP$\\%$), surpass the baseline method in the physical specific scenarios, and exhibit a significant advantage in terms of naturalness in visualization and color difference with the environment."],"url":"http://arxiv.org/abs/2405.07595v1","category":"cs.CV"}
{"created":"2024-05-13 09:56:28","title":"RGBD-Glue: General Feature Combination for Robust RGB-D Point Cloud Registration","abstract":"Point cloud registration is a fundamental task for estimating rigid transformations between point clouds. Previous studies have used geometric information for extracting features, matching and estimating transformation. Recently, owing to the advancement of RGB-D sensors, researchers have attempted to utilize visual information to improve registration performance. However, these studies focused on extracting distinctive features by deep feature fusion, which cannot effectively solve the negative effects of each feature's weakness, and cannot sufficiently leverage the valid information. In this paper, we propose a new feature combination framework, which applies a looser but more effective fusion and can achieve better performance. An explicit filter based on transformation consistency is designed for the combination framework, which can overcome each feature's weakness. And an adaptive threshold determined by the error distribution is proposed to extract more valid information from the two types of features. Owing to the distinctive design, our proposed framework can estimate more accurate correspondences and is applicable to both hand-crafted and learning-based feature descriptors. Experiments on ScanNet show that our method achieves a state-of-the-art performance and the rotation accuracy of 99.1%.","sentences":["Point cloud registration is a fundamental task for estimating rigid transformations between point clouds.","Previous studies have used geometric information for extracting features, matching and estimating transformation.","Recently, owing to the advancement of RGB-D sensors, researchers have attempted to utilize visual information to improve registration performance.","However, these studies focused on extracting distinctive features by deep feature fusion, which cannot effectively solve the negative effects of each feature's weakness, and cannot sufficiently leverage the valid information.","In this paper, we propose a new feature combination framework, which applies a looser but more effective fusion and can achieve better performance.","An explicit filter based on transformation consistency is designed for the combination framework, which can overcome each feature's weakness.","And an adaptive threshold determined by the error distribution is proposed to extract more valid information from the two types of features.","Owing to the distinctive design, our proposed framework can estimate more accurate correspondences and is applicable to both hand-crafted and learning-based feature descriptors.","Experiments on ScanNet show that our method achieves a state-of-the-art performance and the rotation accuracy of 99.1%."],"url":"http://arxiv.org/abs/2405.07594v1","category":"cs.CV"}
{"created":"2024-05-13 09:53:25","title":"Evaluating the Explainable AI Method Grad-CAM for Breath Classification on Newborn Time Series Data","abstract":"With the digitalization of health care systems, artificial intelligence becomes more present in medicine. Especially machine learning shows great potential for complex tasks such as time series classification, usually at the cost of transparency and comprehensibility. This leads to a lack of trust by humans and thus hinders its active usage. Explainable artificial intelligence tries to close this gap by providing insight into the decision-making process, the actual usefulness of its different methods is however unclear. This paper proposes a user study based evaluation of the explanation method Grad-CAM with application to a neural network for the classification of breaths in time series neonatal ventilation data. We present the perceived usefulness of the explainability method by different stakeholders, exposing the difficulty to achieve actual transparency and the wish for more in-depth explanations by many of the participants.","sentences":["With the digitalization of health care systems, artificial intelligence becomes more present in medicine.","Especially machine learning shows great potential for complex tasks such as time series classification, usually at the cost of transparency and comprehensibility.","This leads to a lack of trust by humans and thus hinders its active usage.","Explainable artificial intelligence tries to close this gap by providing insight into the decision-making process, the actual usefulness of its different methods is however unclear.","This paper proposes a user study based evaluation of the explanation method Grad-CAM with application to a neural network for the classification of breaths in time series neonatal ventilation data.","We present the perceived usefulness of the explainability method by different stakeholders, exposing the difficulty to achieve actual transparency and the wish for more in-depth explanations by many of the participants."],"url":"http://arxiv.org/abs/2405.07590v1","category":"cs.AI"}
{"created":"2024-05-13 09:48:30","title":"Structure-Preserving Model Order Reduction for Nonlinear DAE Models of Power Networks","abstract":"This paper deals with the joint reduction of dynamic states (internal states of generator, solar, and loads, etc) and algebraic variables (states of the network e.g., voltage and phase angles) of a nonlinear differential-algebraic equation (NDAE) model of power networks. Traditionally, in the current literature of power systemmodel order reduction (MOR), the algebraic constraints are usually neglected and the power network is commonly modeled via a set of ordinary differential equations (ODEs) instead of NDAEs. Thus, reduction is usually carried out for the dynamic states only and the algebraic variables are kept intact. This leaves a significant part of the system's size and complexity unreduced. This paper addresses this aforementioned limitation, by jointly reducing both dynamic and algebraic variables. As compared to the literature the proposedMOR techniques herein are endowed with the following features: (i) no system linearization is required, (ii) requires no transformation to an equivalent or approximate ODE representation, (iii) guarantee that the reduced order model to be NDAE and thus preserves the differential-algebraic structure of original power system model, and (iv) can seamlessly reduce both dynamic and algebraic variables while maintaining high accuracy. Case studies performed on a 2000-bus power system reveal that the proposedMOR techniques are able to reduce system order while maintaining accuracy","sentences":["This paper deals with the joint reduction of dynamic states (internal states of generator, solar, and loads, etc) and algebraic variables (states of the network e.g., voltage and phase angles) of a nonlinear differential-algebraic equation (NDAE) model of power networks.","Traditionally, in the current literature of power systemmodel order reduction (MOR), the algebraic constraints are usually neglected and the power network is commonly modeled via a set of ordinary differential equations (ODEs) instead of NDAEs.","Thus, reduction is usually carried out for the dynamic states only and the algebraic variables are kept intact.","This leaves a significant part of the system's size and complexity unreduced.","This paper addresses this aforementioned limitation, by jointly reducing both dynamic and algebraic variables.","As compared to the literature the proposedMOR techniques herein are endowed with the following features: (i) no system linearization is required, (ii) requires no transformation to an equivalent or approximate ODE representation, (iii) guarantee that the reduced order model to be NDAE and thus preserves the differential-algebraic structure of original power system model, and (iv) can seamlessly reduce both dynamic and algebraic variables while maintaining high accuracy.","Case studies performed on a 2000-bus power system reveal that the proposedMOR techniques are able to reduce system order while maintaining accuracy"],"url":"http://arxiv.org/abs/2405.07587v1","category":"eess.SY"}
{"created":"2024-05-13 09:48:13","title":"Thai Universal Dependency Treebank","abstract":"Automatic dependency parsing of Thai sentences has been underexplored, as evidenced by the lack of large Thai dependency treebanks with complete dependency structures and the lack of a published systematic evaluation of state-of-the-art models, especially transformer-based parsers. In this work, we address these problems by introducing Thai Universal Dependency Treebank (TUD), a new largest Thai treebank consisting of 3,627 trees annotated in accordance with the Universal Dependencies (UD) framework. We then benchmark dependency parsing models that incorporate pretrained transformers as encoders and train them on Thai-PUD and our TUD. The evaluation results show that most of our models can outperform other models reported in previous papers and provide insight into the optimal choices of components to include in Thai dependency parsers. The new treebank and every model's full prediction generated in our experiment are made available on a GitHub repository for further study.","sentences":["Automatic dependency parsing of Thai sentences has been underexplored, as evidenced by the lack of large Thai dependency treebanks with complete dependency structures and the lack of a published systematic evaluation of state-of-the-art models, especially transformer-based parsers.","In this work, we address these problems by introducing Thai Universal Dependency Treebank (TUD), a new largest Thai treebank consisting of 3,627 trees annotated in accordance with the Universal Dependencies (UD) framework.","We then benchmark dependency parsing models that incorporate pretrained transformers as encoders and train them on Thai-PUD and our TUD.","The evaluation results show that most of our models can outperform other models reported in previous papers and provide insight into the optimal choices of components to include in Thai dependency parsers.","The new treebank and every model's full prediction generated in our experiment are made available on a GitHub repository for further study."],"url":"http://arxiv.org/abs/2405.07586v1","category":"cs.CL"}
{"created":"2024-05-13 09:44:02","title":"La ROUTOURNE va tourner","abstract":"Segment routing (SR) offers precise control over the paths taken: it specifies a list of detours, called segments, in IP packets. However, the number of detours that can be specified is limited by the hardware. When calculating segment lists, it is therefore necessary to limit their size. Although solutions have been proposed for calculating these lists, they lack generality and are not always optimal or efficient. We present ROUTOURNE, a method for diverting routing algorithms so that they calculate, not simply an optimal physical path to be translated into a list of segments a posteriori (with no guarantee of its size), but directly the optimal lists of segments deployable by the underlying hardware. ROUTOURNE thus facilitates the deployment of advanced traffic engineering strategies and policies, notably for load balancing from sources. Despite a route fraught with surprising challenges - in particular, the loss of isotonicity induced by SR - ROUTOURNE proves efficient, inducing at worst a linear overhead. Its accuracy and optimality have been proven, and its effectiveness evaluated by generalizing it to several more or less complex path calculation algorithms.","sentences":["Segment routing (SR) offers precise control over the paths taken: it specifies a list of detours, called segments, in IP packets.","However, the number of detours that can be specified is limited by the hardware.","When calculating segment lists, it is therefore necessary to limit their size.","Although solutions have been proposed for calculating these lists, they lack generality and are not always optimal or efficient.","We present ROUTOURNE, a method for diverting routing algorithms so that they calculate, not simply an optimal physical path to be translated into a list of segments a posteriori (with no guarantee of its size), but directly the optimal lists of segments deployable by the underlying hardware.","ROUTOURNE thus facilitates the deployment of advanced traffic engineering strategies and policies, notably for load balancing from sources.","Despite a route fraught with surprising challenges - in particular, the loss of isotonicity induced by SR - ROUTOURNE proves efficient, inducing at worst a linear overhead.","Its accuracy and optimality have been proven, and its effectiveness evaluated by generalizing it to several more or less complex path calculation algorithms."],"url":"http://arxiv.org/abs/2405.07584v2","category":"cs.NI"}
{"created":"2024-05-13 09:38:49","title":"FRRffusion: Unveiling Authenticity with Diffusion-Based Face Retouching Reversal","abstract":"Unveiling the real appearance of retouched faces to prevent malicious users from deceptive advertising and economic fraud has been an increasing concern in the era of digital economics. This article makes the first attempt to investigate the face retouching reversal (FRR) problem. We first collect an FRR dataset, named deepFRR, which contains 50,000 StyleGAN-generated high-resolution (1024*1024) facial images and their corresponding retouched ones by a commercial online API. To our best knowledge, deepFRR is the first FRR dataset tailored for training the deep FRR models. Then, we propose a novel diffusion-based FRR approach (FRRffusion) for the FRR task. Our FRRffusion consists of a coarse-to-fine two-stage network: A diffusion-based Facial Morpho-Architectonic Restorer (FMAR) is constructed to generate the basic contours of low-resolution faces in the first stage, while a Transformer-based Hyperrealistic Facial Detail Generator (HFDG) is designed to create high-resolution facial details in the second stage. Tested on deepFRR, our FRRffusion surpasses the GP-UNIT and Stable Diffusion methods by a large margin in four widespread quantitative metrics. Especially, the de-retouched images by our FRRffusion are visually much closer to the raw face images than both the retouched face images and those restored by the GP-UNIT and Stable Diffusion methods in terms of qualitative evaluation with 85 subjects. These results sufficiently validate the efficacy of our work, bridging the recently-standing gap between the FRR and generic image restoration tasks. The dataset and code are available at https://github.com/GZHU-DVL/FRRffusion.","sentences":["Unveiling the real appearance of retouched faces to prevent malicious users from deceptive advertising and economic fraud has been an increasing concern in the era of digital economics.","This article makes the first attempt to investigate the face retouching reversal (FRR) problem.","We first collect an FRR dataset, named deepFRR, which contains 50,000 StyleGAN-generated high-resolution (1024*1024) facial images and their corresponding retouched ones by a commercial online API.","To our best knowledge, deepFRR is the first FRR dataset tailored for training the deep FRR models.","Then, we propose a novel diffusion-based FRR approach (FRRffusion) for the FRR task.","Our FRRffusion consists of a coarse-to-fine two-stage network:","A diffusion-based Facial Morpho-Architectonic Restorer (FMAR) is constructed to generate the basic contours of low-resolution faces in the first stage, while a Transformer-based Hyperrealistic Facial Detail Generator (HFDG) is designed to create high-resolution facial details in the second stage.","Tested on deepFRR, our FRRffusion surpasses the GP-UNIT and Stable Diffusion methods by a large margin in four widespread quantitative metrics.","Especially, the de-retouched images by our FRRffusion are visually much closer to the raw face images than both the retouched face images and those restored by the GP-UNIT and Stable Diffusion methods in terms of qualitative evaluation with 85 subjects.","These results sufficiently validate the efficacy of our work, bridging the recently-standing gap between the FRR and generic image restoration tasks.","The dataset and code are available at https://github.com/GZHU-DVL/FRRffusion."],"url":"http://arxiv.org/abs/2405.07582v1","category":"cs.CV"}
{"created":"2024-05-13 09:36:17","title":"DynLLM: When Large Language Models Meet Dynamic Graph Recommendation","abstract":"Last year has witnessed the considerable interest of Large Language Models (LLMs) for their potential applications in recommender systems, which may mitigate the persistent issue of data sparsity. Though large efforts have been made for user-item graph augmentation with better graph-based recommendation performance, they may fail to deal with the dynamic graph recommendation task, which involves both structural and temporal graph dynamics with inherent complexity in processing time-evolving data. To bridge this gap, in this paper, we propose a novel framework, called DynLLM, to deal with the dynamic graph recommendation task with LLMs. Specifically, DynLLM harnesses the power of LLMs to generate multi-faceted user profiles based on the rich textual features of historical purchase records, including crowd segments, personal interests, preferred categories, and favored brands, which in turn supplement and enrich the underlying relationships between users and items. Along this line, to fuse the multi-faceted profiles with temporal graph embedding, we engage LLMs to derive corresponding profile embeddings, and further employ a distilled attention mechanism to refine the LLM-generated profile embeddings for alleviating noisy signals, while also assessing and adjusting the relevance of each distilled facet embedding for seamless integration with temporal graph embedding from continuous time dynamic graphs (CTDGs). Extensive experiments on two real e-commerce datasets have validated the superior improvements of DynLLM over a wide range of state-of-the-art baseline methods.","sentences":["Last year has witnessed the considerable interest of Large Language Models (LLMs) for their potential applications in recommender systems, which may mitigate the persistent issue of data sparsity.","Though large efforts have been made for user-item graph augmentation with better graph-based recommendation performance, they may fail to deal with the dynamic graph recommendation task, which involves both structural and temporal graph dynamics with inherent complexity in processing time-evolving data.","To bridge this gap, in this paper, we propose a novel framework, called DynLLM, to deal with the dynamic graph recommendation task with LLMs.","Specifically, DynLLM harnesses the power of LLMs to generate multi-faceted user profiles based on the rich textual features of historical purchase records, including crowd segments, personal interests, preferred categories, and favored brands, which in turn supplement and enrich the underlying relationships between users and items.","Along this line, to fuse the multi-faceted profiles with temporal graph embedding, we engage LLMs to derive corresponding profile embeddings, and further employ a distilled attention mechanism to refine the LLM-generated profile embeddings for alleviating noisy signals, while also assessing and adjusting the relevance of each distilled facet embedding for seamless integration with temporal graph embedding from continuous time dynamic graphs (CTDGs).","Extensive experiments on two real e-commerce datasets have validated the superior improvements of DynLLM over a wide range of state-of-the-art baseline methods."],"url":"http://arxiv.org/abs/2405.07580v1","category":"cs.IR"}
{"created":"2024-05-13 09:35:30","title":"The extremal Reissner-Nordstr\u00f6m black holes: an exact charged scalar quasiresonance","abstract":"In this letter, we present a novel exact scalar quasibound states solutions in the extremal Reissner-Norstr\\\"om black hole background. We start with the construction of the governing covariant relativistic scalar field equation, the Klein-Gordon equation in the extremal Reissner-Norstr\\\"om black hole background and applying the separation of variables anzat. The exact relativistic scalar wave's angular solution is found in terms of the spherical harmonics while the two independent radial wave solutions are, for the first time, exactly found and presented in terms of the double confluent Heun functions. The solutions are settled in the gravitational potential well and behave like an ingoing waves approaching black hole's horizon, vanishing when approaching infinity. The gravitationally bounded charged massive scalar fields are found to have quantized complex valued energy levels while imaginary energy levels are obtained for the charged massless scalar field, of both cases, indicating decaying states. Further investigation shows that the extreme Reissner-Nordst\\\"om black hole does not support scalar cloud. And with the help of the obtained exact radial solutions, the Hawking radiation of the extremal Reissner-Nordst\\\"om black hole is investigated and we find the zero temperature of the black hole's horizon.","sentences":["In this letter, we present a novel exact scalar quasibound states solutions in the extremal Reissner-Norstr\\\"om black hole background.","We start with the construction of the governing covariant relativistic scalar field equation, the Klein-Gordon equation in the extremal Reissner-Norstr\\\"om black hole background and applying the separation of variables anzat.","The exact relativistic scalar wave's angular solution is found in terms of the spherical harmonics while the two independent radial wave solutions are, for the first time, exactly found and presented in terms of the double confluent Heun functions.","The solutions are settled in the gravitational potential well and behave like an ingoing waves approaching black hole's horizon, vanishing when approaching infinity.","The gravitationally bounded charged massive scalar fields are found to have quantized complex valued energy levels while imaginary energy levels are obtained for the charged massless scalar field, of both cases, indicating decaying states.","Further investigation shows that the extreme Reissner-Nordst\\\"om black hole does not support scalar cloud.","And with the help of the obtained exact radial solutions, the Hawking radiation of the extremal Reissner-Nordst\\\"om black hole is investigated and we find the zero temperature of the black hole's horizon."],"url":"http://arxiv.org/abs/2405.07579v1","category":"gr-qc"}
{"created":"2024-05-13 09:20:11","title":"The degree of functions in the Johnson and q-Johnson schemes","abstract":"In 1982, Cameron and Liebler investigated certain \"special sets of lines\" in PG(3,q), and gave several equivalent characterizations. Due to their interesting geometric and algebraic properties, these \"Cameron-Liebler line classes\" got a lot of attention. Several generalizations and variants have been considered in the literature, the main directions being a variation of the dimensions of the involved spaces, and studying the analogous situation in the subset lattice. An important tool is the interpretation of the objects as Boolean functions in the \"Johnson\" and \"q-Johnson schemes\".   In this article, we develop a unified theory covering all these variations. Generalized versions of algebraic and geometric properties will be investigated, having a parallel in the notion of \"designs\" and \"antidesigns\" in association schemes, which is connected to Delsarte's concept of \"design-orthogonality\". This leads to a natural definition of the \"degree\" and the \"weights\" of functions in the ambient scheme, refining the existing definitions. We will study the effect of dualization and of elementary modifications of the ambient space on the degree and the weights. Moreover, a divisibility property of the sizes of Boolean functions of degree t will be proven.","sentences":["In 1982, Cameron and Liebler investigated certain \"special sets of lines\" in PG(3,q), and gave several equivalent characterizations.","Due to their interesting geometric and algebraic properties, these \"Cameron-Liebler line classes\" got a lot of attention.","Several generalizations and variants have been considered in the literature, the main directions being a variation of the dimensions of the involved spaces, and studying the analogous situation in the subset lattice.","An important tool is the interpretation of the objects as Boolean functions in the \"Johnson\" and \"q-Johnson schemes\".   ","In this article, we develop a unified theory covering all these variations.","Generalized versions of algebraic and geometric properties will be investigated, having a parallel in the notion of \"designs\" and \"antidesigns\" in association schemes, which is connected to Delsarte's concept of \"design-orthogonality\".","This leads to a natural definition of the \"degree\" and the \"weights\" of functions in the ambient scheme, refining the existing definitions.","We will study the effect of dualization and of elementary modifications of the ambient space on the degree and the weights.","Moreover, a divisibility property of the sizes of Boolean functions of degree t will be proven."],"url":"http://arxiv.org/abs/2405.07572v1","category":"math.CO"}
{"created":"2024-05-13 09:19:03","title":"TattTRN: Template Reconstruction Network for Tattoo Retrieval","abstract":"Tattoos have been used effectively as soft biometrics to assist law enforcement in the identification of offenders and victims, as they contain discriminative information, and are a useful indicator to locate members of a criminal gang or organisation. Due to various privacy issues in the acquisition of images containing tattoos, only a limited number of databases exists. This lack of databases has delayed the development of new methods to effectively retrieve a potential suspect's tattoo images from a candidate gallery. To mitigate this issue, in our work, we use an unsupervised generative approach to create a balanced database consisting of 28,550 semi-synthetic images with tattooed subjects from 571 tattoo categories. Further, we introduce a novel Tattoo Template Reconstruction Network (TattTRN), which learns to map the input tattoo sample to its respective tattoo template to enhance the distinguishing attributes of the final feature embedding. Experimental results with real data, i.e., WebTattoo and BIVTatt databases, demonstrate the soundness of the presented approach: an accuracy of up to 99% is achieved for checking at most the first 20 entries of the candidate list.","sentences":["Tattoos have been used effectively as soft biometrics to assist law enforcement in the identification of offenders and victims, as they contain discriminative information, and are a useful indicator to locate members of a criminal gang or organisation.","Due to various privacy issues in the acquisition of images containing tattoos, only a limited number of databases exists.","This lack of databases has delayed the development of new methods to effectively retrieve a potential suspect's tattoo images from a candidate gallery.","To mitigate this issue, in our work, we use an unsupervised generative approach to create a balanced database consisting of 28,550 semi-synthetic images with tattooed subjects from 571 tattoo categories.","Further, we introduce a novel Tattoo Template Reconstruction Network (TattTRN), which learns to map the input tattoo sample to its respective tattoo template to enhance the distinguishing attributes of the final feature embedding.","Experimental results with real data, i.e., WebTattoo and BIVTatt databases, demonstrate the soundness of the presented approach: an accuracy of up to 99% is achieved for checking at most the first 20 entries of the candidate list."],"url":"http://arxiv.org/abs/2405.07571v1","category":"cs.CV"}
{"created":"2024-05-13 09:05:09","title":"Homological stability for general linear groups over Dedekind domains","abstract":"We prove a new kind of homological stability theorem for automorphism groups of finitely-generated projective modules over Dedekind domains, which takes into account all possible stabilisation maps between these, rather than only stabilisation by the free module of rank 1. We show the same kind of stability holds for Clausen and Jansen's reductive Borel--Serre spaces.","sentences":["We prove a new kind of homological stability theorem for automorphism groups of finitely-generated projective modules over Dedekind domains, which takes into account all possible stabilisation maps between these, rather than only stabilisation by the free module of rank 1.","We show the same kind of stability holds for Clausen and Jansen's reductive Borel--Serre spaces."],"url":"http://arxiv.org/abs/2405.07566v1","category":"math.AC"}
{"created":"2024-05-13 08:52:04","title":"GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation","abstract":"While Deep Neural Networks (DNNs) have demonstrated remarkable performance in tasks related to perception and control, there are still several unresolved concerns regarding the privacy of their training data, particularly in the context of vulnerability to Membership Inference Attacks (MIAs). In this paper, we explore a connection between the susceptibility to membership inference attacks and the vulnerability to distillation-based functionality stealing attacks. In particular, we propose {GLiRA}, a distillation-guided approach to membership inference attack on the black-box neural network. We observe that the knowledge distillation significantly improves the efficiency of likelihood ratio of membership inference attack, especially in the black-box setting, i.e., when the architecture of the target model is unknown to the attacker. We evaluate the proposed method across multiple image classification datasets and models and demonstrate that likelihood ratio attacks when guided by the knowledge distillation, outperform the current state-of-the-art membership inference attacks in the black-box setting.","sentences":["While Deep Neural Networks (DNNs) have demonstrated remarkable performance in tasks related to perception and control, there are still several unresolved concerns regarding the privacy of their training data, particularly in the context of vulnerability to Membership Inference Attacks (MIAs).","In this paper, we explore a connection between the susceptibility to membership inference attacks and the vulnerability to distillation-based functionality stealing attacks.","In particular, we propose {GLiRA}, a distillation-guided approach to membership inference attack on the black-box neural network.","We observe that the knowledge distillation significantly improves the efficiency of likelihood ratio of membership inference attack, especially in the black-box setting, i.e., when the architecture of the target model is unknown to the attacker.","We evaluate the proposed method across multiple image classification datasets and models and demonstrate that likelihood ratio attacks when guided by the knowledge distillation, outperform the current state-of-the-art membership inference attacks in the black-box setting."],"url":"http://arxiv.org/abs/2405.07562v1","category":"cs.LG"}
{"created":"2024-05-13 08:51:14","title":"Crystal Structure-Based Multioutput Property Prediction of Lithium Manganese Nickel Oxide using EfficientNet-B0","abstract":"Here, we present an EfficientNet-B0-based model to directly predict multiple properties of lithium manganese nickel oxides (LMNO) using their crystal structure images. The model is supposed to predict the energy above the convex hull, bandgap energy, crystal systems, and crystal space groups of LMNOs. In the last layer of the model, a linear function is used to predict the bandgap energy and energy above the convex hull, while a SoftMax function is used to classify the crystal systems and crystal space groups. In the test set, the percentages of coefficient of determination (R2) scores are 97.73% and 96.50% for the bandgap energy and energy above the convex hull predictions, respectively, while the percentages of accuracy are 99.45% and 99.27% for the crystal system and crystal space group classifications, respectively. The class saliency maps explain that the model pays more attention to the shape of the crystal lattices and gradients around the lattice region occupied by the larger ions. This work provides new insight into using an intelligent model to directly relate the crystal structures of LMNO materials with their properties.","sentences":["Here, we present an EfficientNet-B0-based model to directly predict multiple properties of lithium manganese nickel oxides (LMNO) using their crystal structure images.","The model is supposed to predict the energy above the convex hull, bandgap energy, crystal systems, and crystal space groups of LMNOs.","In the last layer of the model, a linear function is used to predict the bandgap energy and energy above the convex hull, while a SoftMax function is used to classify the crystal systems and crystal space groups.","In the test set, the percentages of coefficient of determination (R2) scores are 97.73% and 96.50% for the bandgap energy and energy above the convex hull predictions, respectively, while the percentages of accuracy are 99.45% and 99.27% for the crystal system and crystal space group classifications, respectively.","The class saliency maps explain that the model pays more attention to the shape of the crystal lattices and gradients around the lattice region occupied by the larger ions.","This work provides new insight into using an intelligent model to directly relate the crystal structures of LMNO materials with their properties."],"url":"http://arxiv.org/abs/2405.07561v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 08:50:18","title":"Coding historical causes of death data with Large Language Models","abstract":"This paper investigates the feasibility of using pre-trained generative Large Language Models (LLMs) to automate the assignment of ICD-10 codes to historical causes of death. Due to the complex narratives often found in historical causes of death, this task has traditionally been manually performed by coding experts. We evaluate the ability of GPT-3.5, GPT-4, and Llama 2 LLMs to accurately assign ICD-10 codes on the HiCaD dataset that contains causes of death recorded in the civil death register entries of 19,361 individuals from Ipswich, Kilmarnock, and the Isle of Skye from the UK between 1861-1901. Our findings show that GPT-3.5, GPT-4, and Llama 2 assign the correct code for 69%, 83%, and 40% of causes, respectively. However, we achieve a maximum accuracy of 89% by standard machine learning techniques. All LLMs performed better for causes of death that contained terms still in use today, compared to archaic terms. Also they perform better for short causes (1-2 words) compared to longer causes. LLMs therefore do not currently perform well enough for historical ICD-10 code assignment tasks. We suggest further fine-tuning or alternative frameworks to achieve adequate performance.","sentences":["This paper investigates the feasibility of using pre-trained generative Large Language Models (LLMs) to automate the assignment of ICD-10 codes to historical causes of death.","Due to the complex narratives often found in historical causes of death, this task has traditionally been manually performed by coding experts.","We evaluate the ability of GPT-3.5, GPT-4, and Llama 2 LLMs to accurately assign ICD-10 codes on the HiCaD dataset that contains causes of death recorded in the civil death register entries of 19,361 individuals from Ipswich, Kilmarnock, and the Isle of Skye from the UK between 1861-1901.","Our findings show that GPT-3.5, GPT-4, and Llama 2 assign the correct code for 69%, 83%, and 40% of causes, respectively.","However, we achieve a maximum accuracy of 89% by standard machine learning techniques.","All LLMs performed better for causes of death that contained terms still in use today, compared to archaic terms.","Also they perform better for short causes (1-2 words) compared to longer causes.","LLMs therefore do not currently perform well enough for historical ICD-10 code assignment tasks.","We suggest further fine-tuning or alternative frameworks to achieve adequate performance."],"url":"http://arxiv.org/abs/2405.07560v1","category":"cs.LG"}
{"created":"2024-05-13 08:45:17","title":"Synchronization of High-Dimensional Linear Networks over Finite Fields","abstract":"This paper investigates the synchronization problems for general high-dimensional linear networks over finite fields. By using the technique of linear transformations and invariant subspaces for linear spaces over finite fields, several necessary and sufficient conditions for the synchronization of high-dimensional linear networks over finite fields are proposed. This paper not only generalizes the existing results from 1-dimensional to high-dimensional linear networks but also adopts a new approach. Finally, some numerical examples are given to illustrate the effectiveness of our theoretical results.","sentences":["This paper investigates the synchronization problems for general high-dimensional linear networks over finite fields.","By using the technique of linear transformations and invariant subspaces for linear spaces over finite fields, several necessary and sufficient conditions for the synchronization of high-dimensional linear networks over finite fields are proposed.","This paper not only generalizes the existing results from 1-dimensional to high-dimensional linear networks but also adopts a new approach.","Finally, some numerical examples are given to illustrate the effectiveness of our theoretical results."],"url":"http://arxiv.org/abs/2405.07558v1","category":"math.OC"}
{"created":"2024-05-13 08:41:41","title":"Towards Rational Consensus in Honest Majority","abstract":"Distributed consensus protocols reach agreement among $n$ players in the presence of $f$ adversaries; different protocols support different values of $f$. Existing works study this problem for different adversary types (captured by threat models). There are three primary threat models: (i) Crash fault tolerance (CFT), (ii) Byzantine fault tolerance (BFT), and (iii) Rational fault tolerance (RFT), each more general than the previous. Agreement in repeated rounds on both (1) the proposed value in each round and (2) the ordering among agreed-upon values across multiple rounds is called Atomic BroadCast (ABC). ABC is more generalized than consensus and is employed in blockchains.   This work studies ABC under the RFT threat model. We consider $t$ byzantine and $k$ rational adversaries among $n$ players. We also study different types of rational players based on their utility towards (1) liveness attack, (2) censorship or (3) disagreement (forking attack). We study the problem of ABC under this general threat model in partially-synchronous networks. We show (1) ABC is impossible for $n/3< (t+k) <n/2$ if rational players prefer liveness or censorship attacks and (2) the consensus protocol proposed by Ranchal-Pedrosa and Gramoli cannot be generalized to solve ABC due to insecure Nash equilibrium (resulting in disagreement). For ABC in partially synchronous network settings, we propose a novel protocol \\textsf{pRFT}(practical Rational Fault Tolerance). We show \\textsf{pRFT} achieves ABC if (a) rational players prefer only disagreement attacks and (b) $t < \\frac{n}{4}$ and $(t + k) < \\frac{n}{2}$. In \\textsf{pRFT}, we incorporate accountability (capturing deviating players) within the protocol by leveraging honest players. We also show that the message complexity of \\textsf{pRFT} is at par with the best consensus protocols that guarantee accountability.","sentences":["Distributed consensus protocols reach agreement among $n$ players in the presence of $f$ adversaries; different protocols support different values of $f$. Existing works study this problem for different adversary types (captured by threat models).","There are three primary threat models: (i) Crash fault tolerance (CFT), (ii) Byzantine fault tolerance (BFT), and (iii) Rational fault tolerance (RFT), each more general than the previous.","Agreement in repeated rounds on both (1) the proposed value in each round and (2) the ordering among agreed-upon values across multiple rounds is called Atomic BroadCast (ABC).","ABC is more generalized than consensus and is employed in blockchains.   ","This work studies ABC under the RFT threat model.","We consider $t$ byzantine and $k$ rational adversaries among $n$ players.","We also study different types of rational players based on their utility towards (1) liveness attack, (2) censorship or (3) disagreement (forking attack).","We study the problem of ABC under this general threat model in partially-synchronous networks.","We show (1) ABC is impossible for $n/3< (t+k) <n/2$ if rational players prefer liveness or censorship attacks and (2) the consensus protocol proposed by Ranchal-Pedrosa and Gramoli cannot be generalized to solve ABC due to insecure Nash equilibrium (resulting in disagreement).","For ABC in partially synchronous network settings, we propose a novel protocol \\textsf{pRFT}(practical Rational Fault Tolerance).","We show \\textsf{pRFT} achieves ABC if (a) rational players prefer only disagreement attacks and (b) $t < \\frac{n}{4}$ and $(t + k) <","\\frac{n}{2}$.","In \\textsf{pRFT}, we incorporate accountability (capturing deviating players) within the protocol by leveraging honest players.","We also show that the message complexity of \\textsf{pRFT} is at par with the best consensus protocols that guarantee accountability."],"url":"http://arxiv.org/abs/2405.07557v1","category":"cs.GT"}
{"created":"2024-05-13 08:32:19","title":"MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning","abstract":"The tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs, while tool-free methods chose another track: augmenting math reasoning data. However, a great method to integrate the above two research paths and combine their advantages remains to be explored. In this work, we firstly include new math questions via multi-perspective data augmenting methods and then synthesize code-nested solutions to them. The open LLMs (i.e., Llama-2) are finetuned on the augmented dataset to get the resulting models, MuMath-Code ($\\mu$-Math-Code). During the inference phase, our MuMath-Code generates code and interacts with the external python interpreter to get the execution results. Therefore, MuMath-Code leverages the advantages of both the external tool and data augmentation. To fully leverage the advantages of our augmented data, we propose a two-stage training strategy: In Stage-1, we finetune Llama-2 on pure CoT data to get an intermediate model, which then is trained on the code-nested data in Stage-2 to get the resulting MuMath-Code. Our MuMath-Code-7B achieves 83.8 on GSM8K and 52.4 on MATH, while MuMath-Code-70B model achieves new state-of-the-art performance among open methods -- achieving 90.7% on GSM8K and 55.1% on MATH. Extensive experiments validate the combination of tool use and data augmentation, as well as our two-stage training strategy. We release the proposed dataset along with the associated code for public use.","sentences":["The tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs, while tool-free methods chose another track: augmenting math reasoning data.","However, a great method to integrate the above two research paths and combine their advantages remains to be explored.","In this work, we firstly include new math questions via multi-perspective data augmenting methods and then synthesize code-nested solutions to them.","The open LLMs (i.e., Llama-2) are finetuned on the augmented dataset to get the resulting models, MuMath-Code ($\\mu$-Math-Code).","During the inference phase, our MuMath-Code generates code and interacts with the external python interpreter to get the execution results.","Therefore, MuMath-Code leverages the advantages of both the external tool and data augmentation.","To fully leverage the advantages of our augmented data, we propose a two-stage training strategy: In Stage-1, we finetune Llama-2 on pure CoT data to get an intermediate model, which then is trained on the code-nested data in Stage-2 to get the resulting MuMath-Code.","Our MuMath-Code-7B achieves 83.8 on GSM8K and 52.4 on MATH, while MuMath-Code-70B model achieves new state-of-the-art performance among open methods -- achieving 90.7% on GSM8K and 55.1% on MATH.","Extensive experiments validate the combination of tool use and data augmentation, as well as our two-stage training strategy.","We release the proposed dataset along with the associated code for public use."],"url":"http://arxiv.org/abs/2405.07551v1","category":"cs.CL"}
{"created":"2024-05-13 08:28:54","title":"Channel Coding Toward 6G: Technical Overview and Outlook","abstract":"Channel coding plays a pivotal role in ensuring reliable communication over wireless channels. With the growing need for ultra-reliable communication in emerging wireless use cases, the significance of channel coding has amplified. Furthermore, minimizing decoding latency is crucial for critical-mission applications, while optimizing energy efficiency is paramount for mobile and the Internet of Things (IoT) communications. As the fifth generation (5G) of mobile communications is currently in operation and 5G-advanced is on the horizon, the objective of this paper is to assess prominent channel coding schemes in the context of recent advancements and the anticipated requirements for the sixth generation (6G). In this paper, after considering the potential impact of channel coding on key performance indicators (KPIs) of wireless networks, we review the evolution of mobile communication standards and the organizations involved in the standardization, from the first generation (1G) to the current 5G, highlighting the technologies integral to achieving targeted KPIs such as reliability, data rate, latency, energy efficiency, spectral efficiency, connection density, and traffic capacity. Following this, we delve into the anticipated requirements for potential use cases in 6G. The subsequent sections of the paper focus on a comprehensive review of three primary coding schemes utilized in past generations and their recent advancements: low-density parity-check (LDPC) codes, turbo codes (including convolutional codes), polar codes (alongside Reed-Muller codes). Additionally, we examine alternative coding schemes like Fountain codes and sparse regression codes. Our evaluation includes a comparative analysis of error correction performance and the performance of hardware implementation for these coding schemes, providing insights into their potential and suitability for the upcoming 6G era.","sentences":["Channel coding plays a pivotal role in ensuring reliable communication over wireless channels.","With the growing need for ultra-reliable communication in emerging wireless use cases, the significance of channel coding has amplified.","Furthermore, minimizing decoding latency is crucial for critical-mission applications, while optimizing energy efficiency is paramount for mobile and the Internet of Things (IoT) communications.","As the fifth generation (5G) of mobile communications is currently in operation and 5G-advanced is on the horizon, the objective of this paper is to assess prominent channel coding schemes in the context of recent advancements and the anticipated requirements for the sixth generation (6G).","In this paper, after considering the potential impact of channel coding on key performance indicators (KPIs) of wireless networks, we review the evolution of mobile communication standards and the organizations involved in the standardization, from the first generation (1G) to the current 5G, highlighting the technologies integral to achieving targeted KPIs such as reliability, data rate, latency, energy efficiency, spectral efficiency, connection density, and traffic capacity.","Following this, we delve into the anticipated requirements for potential use cases in 6G.","The subsequent sections of the paper focus on a comprehensive review of three primary coding schemes utilized in past generations and their recent advancements: low-density parity-check (LDPC) codes, turbo codes (including convolutional codes), polar codes (alongside Reed-Muller codes).","Additionally, we examine alternative coding schemes like Fountain codes and sparse regression codes.","Our evaluation includes a comparative analysis of error correction performance and the performance of hardware implementation for these coding schemes, providing insights into their potential and suitability for the upcoming 6G era."],"url":"http://arxiv.org/abs/2405.07547v1","category":"cs.IT"}
{"created":"2024-05-13 08:27:40","title":"Dynamical systems of modified Gauss-Bonnet gravity: cosmological implications","abstract":"In this paper, we derive the field equations of modified Gauss-Bonnet gravity termed as $f(R,G)$ gravity for the non-flat Friedmann-Robertson-Walker (FRW) spacetime. We utilize the dynamical system approach to study the cosmic dynamics of two different class of $f(R,G)$ models composed of radiation and matter (cold dark matter and baryonic matter). The linear perturbations around the fixed points are studied to explore the corresponding stability of points. The cosmological implications are studied in $f(R,G)=f_0R^nG^{1-n}$ and $f(R,G)=f_0R^\\alpha+f_1G^\\beta$ models to identify the qualitative evolution of universe with the flat-FRW spacetime. The qualitative differences between the considered class of models are discussed in detail. The fixed points corresponding to the late-time accelerated and radiation phase of the universe will exist in the model but, the existence of fixed point corresponding to the matter dominated phase will depend on the functional form of $f(R,G)$. Furthermore, the autonomous systems are utilized to study the cosmographic parameters along with the statefinder diagnostic.","sentences":["In this paper, we derive the field equations of modified Gauss-Bonnet gravity termed as $f(R,G)$ gravity for the non-flat Friedmann-Robertson-Walker (FRW) spacetime.","We utilize the dynamical system approach to study the cosmic dynamics of two different class of $f(R,G)$ models composed of radiation and matter (cold dark matter and baryonic matter).","The linear perturbations around the fixed points are studied to explore the corresponding stability of points.","The cosmological implications are studied in $f(R,G)=f_0R^nG^{1-n}$ and $f(R,G)=f_0R^\\alpha+f_1G^\\beta$ models to identify the qualitative evolution of universe with the flat-FRW spacetime.","The qualitative differences between the considered class of models are discussed in detail.","The fixed points corresponding to the late-time accelerated and radiation phase of the universe will exist in the model but, the existence of fixed point corresponding to the matter dominated phase will depend on the functional form of $f(R,","G)$. Furthermore, the autonomous systems are utilized to study the cosmographic parameters along with the statefinder diagnostic."],"url":"http://arxiv.org/abs/2405.07546v1","category":"gr-qc"}
{"created":"2024-05-13 08:26:24","title":"Automatic Odometry-Less OpenDRIVE Generation From Sparse Point Clouds","abstract":"High-resolution road representations are a key factor for the success of (highly) automated driving functions. These representations, for example, high-definition (HD) maps, contain accurate information on a multitude of factors, among others: road geometry, lane information, and traffic signs. Through the growing complexity and functionality of automated driving functions, also the requirements on testing and evaluation grow continuously. This leads to an increasing interest in virtual test drives for evaluation purposes. As roads play a crucial role in traffic flow, accurate real-world representations are needed, especially when deriving realistic driving behavior data. This paper proposes a novel approach to generate realistic road representations based solely on point cloud information, independent of the LiDAR sensor, mounting position, and without the need for odometry data, multi-sensor fusion, machine learning, or highly-accurate calibration. As the primary use case is simulation, we use the OpenDRIVE format for evaluation.","sentences":["High-resolution road representations are a key factor for the success of (highly) automated driving functions.","These representations, for example, high-definition (HD) maps, contain accurate information on a multitude of factors, among others: road geometry, lane information, and traffic signs.","Through the growing complexity and functionality of automated driving functions, also the requirements on testing and evaluation grow continuously.","This leads to an increasing interest in virtual test drives for evaluation purposes.","As roads play a crucial role in traffic flow, accurate real-world representations are needed, especially when deriving realistic driving behavior data.","This paper proposes a novel approach to generate realistic road representations based solely on point cloud information, independent of the LiDAR sensor, mounting position, and without the need for odometry data, multi-sensor fusion, machine learning, or highly-accurate calibration.","As the primary use case is simulation, we use the OpenDRIVE format for evaluation."],"url":"http://arxiv.org/abs/2405.07544v1","category":"cs.RO"}
{"created":"2024-05-13 08:25:45","title":"Accelerating the Evolution of Personalized Automated Lane Change through Lesson Learning","abstract":"Personalization is crucial for the widespread adoption of advanced driver assistance system. To match up with each user's preference, the online evolution capability is a must. However, conventional evolution methods learn from naturalistic driving data, which requires a lot computing power and cannot be applied online. To address this challenge, this paper proposes a lesson learning approach: learning from driver's takeover interventions. By leveraging online takeover data, the driving zone is generated to ensure perceived safety using Gaussian discriminant analysis. Real-time corrections to trajectory planning rewards are enacted through apprenticeship learning. Guided by the objective of optimizing rewards within the constraints of the driving zone, this approach employs model predictive control for trajectory planning. This lesson learning framework is highlighted for its faster evolution capability, adeptness at experience accumulating, assurance of perceived safety, and computational efficiency. Simulation results demonstrate that the proposed system consistently achieves a successful customization without further takeover interventions. Accumulated experience yields a 24% enhancement in evolution efficiency. The average number of learning iterations is only 13.8. The average computation time is 0.08 seconds.","sentences":["Personalization is crucial for the widespread adoption of advanced driver assistance system.","To match up with each user's preference, the online evolution capability is a must.","However, conventional evolution methods learn from naturalistic driving data, which requires a lot computing power and cannot be applied online.","To address this challenge, this paper proposes a lesson learning approach: learning from driver's takeover interventions.","By leveraging online takeover data, the driving zone is generated to ensure perceived safety using Gaussian discriminant analysis.","Real-time corrections to trajectory planning rewards are enacted through apprenticeship learning.","Guided by the objective of optimizing rewards within the constraints of the driving zone, this approach employs model predictive control for trajectory planning.","This lesson learning framework is highlighted for its faster evolution capability, adeptness at experience accumulating, assurance of perceived safety, and computational efficiency.","Simulation results demonstrate that the proposed system consistently achieves a successful customization without further takeover interventions.","Accumulated experience yields a 24% enhancement in evolution efficiency.","The average number of learning iterations is only 13.8.","The average computation time is 0.08 seconds."],"url":"http://arxiv.org/abs/2405.07543v1","category":"cs.LG"}
{"created":"2024-05-13 08:22:44","title":"Random walk model that universally generates inverse square L\u00e9vy walk by eliminating search cost minimization constraint","abstract":"The L\\'evy walk, a type of random walk characterized by linear step lengths that follow a power-law distribution, is observed in the migratory behaviors of various organisms, ranging from bacteria to humans. Notably, L\\'evy walks with power exponents close to two are frequently observed, though their underlying causes remain elusive. This study introduces a simplified, abstract random walk model designed to produce inverse square L\\'evy walks, also known as Cauchy walks and explores the conditions that facilitate these phenomena. In our model, agents move toward a randomly selected destination in multi-dimensional space, and their movement strategy is parameterized by the extent to which they pursue the shortest path. When the search cost is proportional to the distance traveled, this parameter effectively reflects the emphasis on minimizing search costs. Our findings reveal that strict adherence to this cost minimization constraint results in a Brownian walk pattern. However, removing this constraint transitions the movement to an inverse square L\\'evy walk. Therefore, by modulating the prioritization of search costs, our model can seamlessly alternate between Brownian and Cauchy walk dynamics. This model has the potential to be utilized for exploring the parameter space of an optimization problem.","sentences":["The L\\'evy walk, a type of random walk characterized by linear step lengths that follow a power-law distribution, is observed in the migratory behaviors of various organisms, ranging from bacteria to humans.","Notably, L\\'evy walks with power exponents close to two are frequently observed, though their underlying causes remain elusive.","This study introduces a simplified, abstract random walk model designed to produce inverse square L\\'evy walks, also known as Cauchy walks and explores the conditions that facilitate these phenomena.","In our model, agents move toward a randomly selected destination in multi-dimensional space, and their movement strategy is parameterized by the extent to which they pursue the shortest path.","When the search cost is proportional to the distance traveled, this parameter effectively reflects the emphasis on minimizing search costs.","Our findings reveal that strict adherence to this cost minimization constraint results in a Brownian walk pattern.","However, removing this constraint transitions the movement to an inverse square L\\'evy walk.","Therefore, by modulating the prioritization of search costs, our model can seamlessly alternate between Brownian and Cauchy walk dynamics.","This model has the potential to be utilized for exploring the parameter space of an optimization problem."],"url":"http://arxiv.org/abs/2405.07541v2","category":"cs.MA"}
{"created":"2024-05-13 08:20:59","title":"Theory of cell aggregates as interacting, spinning, active polar particles","abstract":"We discuss a generic description of the dynamics of cell aggregates. We describe cells as polar rotating objects which mechanically interact with each other and with the surrounding medium. We use the framework of non-equilibrium thermodynamics to derive generic constitutive equations for the interaction forces, torques and polarity dynamics. We apply our framework to discuss spontaneous motion of cell doublets. We find a rich phase diagram of possible collective motion, including steady rotation arising from flow-polarity coupling or coupling of polarity with cell position.","sentences":["We discuss a generic description of the dynamics of cell aggregates.","We describe cells as polar rotating objects which mechanically interact with each other and with the surrounding medium.","We use the framework of non-equilibrium thermodynamics to derive generic constitutive equations for the interaction forces, torques and polarity dynamics.","We apply our framework to discuss spontaneous motion of cell doublets.","We find a rich phase diagram of possible collective motion, including steady rotation arising from flow-polarity coupling or coupling of polarity with cell position."],"url":"http://arxiv.org/abs/2405.07540v1","category":"cond-mat.soft"}
{"created":"2024-05-13 08:09:51","title":"Probabilistic Rounding Error Analysis From A Statistical Perspective","abstract":"The conventional probabilistic rounding error analysis in numerical linear algebra provides worst-case bounds with an associated failure probability, which can still be pessimistic. In this paper, we develop a new probabilistic rounding error analysis from a statistical perspective. By assuming both the data and the relative error are independent random variables, we derive the approximate closed-form expressions for the expectation and variance of the rounding errors in various key computational kernels. Our analytical expressions have three notable characteristics: they are statistical and do not involve a failure probability; they are sharper than other deterministic and probabilistic bounds, using mean square error as the metric; they are correct to all orders of unit roundoff and valid for any dimension. Furthermore, numerical experiments validate the accuracy of our derivations and demonstrate that our analytical expressions are generally at least two orders of magnitude tighter than alternative worst-case bounds, exemplified through the inner products. We also discuss a scenario involving inner products where the underlying assumptions are invalid, i.e., input data are dependent, rendering the analytical expressions inapplicable.","sentences":["The conventional probabilistic rounding error analysis in numerical linear algebra provides worst-case bounds with an associated failure probability, which can still be pessimistic.","In this paper, we develop a new probabilistic rounding error analysis from a statistical perspective.","By assuming both the data and the relative error are independent random variables, we derive the approximate closed-form expressions for the expectation and variance of the rounding errors in various key computational kernels.","Our analytical expressions have three notable characteristics: they are statistical and do not involve a failure probability; they are sharper than other deterministic and probabilistic bounds, using mean square error as the metric; they are correct to all orders of unit roundoff and valid for any dimension.","Furthermore, numerical experiments validate the accuracy of our derivations and demonstrate that our analytical expressions are generally at least two orders of magnitude tighter than alternative worst-case bounds, exemplified through the inner products.","We also discuss a scenario involving inner products where the underlying assumptions are invalid, i.e., input data are dependent, rendering the analytical expressions inapplicable."],"url":"http://arxiv.org/abs/2405.07537v1","category":"math.NA"}
{"created":"2024-05-13 08:09:24","title":"Multi-AUV Kinematic Task Assignment based on Self-organizing Map Neural Network and Dubins Path Generator","abstract":"To deal with the task assignment problem of multi-AUV systems under kinematic constraints, which means steering capability constraints for underactuated AUVs or other vehicles likely, an improved task assignment algorithm is proposed combining the Dubins Path algorithm with improved SOM neural network algorithm. At first, the aimed tasks are assigned to the AUVs by improved SOM neural network method based on workload balance and neighborhood function. When there exists kinematic constraints or obstacles which may cause failure of trajectory planning, task re-assignment will be implemented by change the weights of SOM neurals, until the AUVs can have paths to reach all the targets. Then, the Dubins paths are generated in several limited cases. AUV's yaw angle is limited, which result in new assignments to the targets. Computation flow is designed so that the algorithm in MATLAB and Python can realizes the path planning to multiple targets. Finally, simulation results prove that the proposed algorithm can effectively accomplish the task assignment task for multi-AUV system.","sentences":["To deal with the task assignment problem of multi-AUV systems under kinematic constraints, which means steering capability constraints for underactuated AUVs or other vehicles likely, an improved task assignment algorithm is proposed combining the Dubins Path algorithm with improved SOM neural network algorithm.","At first, the aimed tasks are assigned to the AUVs by improved SOM neural network method based on workload balance and neighborhood function.","When there exists kinematic constraints or obstacles which may cause failure of trajectory planning, task re-assignment will be implemented by change the weights of SOM neurals, until the AUVs can have paths to reach all the targets.","Then, the Dubins paths are generated in several limited cases.","AUV's yaw angle is limited, which result in new assignments to the targets.","Computation flow is designed so that the algorithm in MATLAB and Python can realizes the path planning to multiple targets.","Finally, simulation results prove that the proposed algorithm can effectively accomplish the task assignment task for multi-AUV system."],"url":"http://arxiv.org/abs/2405.07536v1","category":"cs.RO"}
{"created":"2024-05-13 08:00:28","title":"Equator to Pole Solar Chromospheric Differential Rotation using Ca-K Features Derived from Kodaikanal Data","abstract":"Differential rotation is one of the basic characteristics of the Sun, and it plays an important role in generating the magnetic fields and its activities. We investigated rotation rate using chromospheric features such as plages, enhanced network, active network, and quiet network separately (for the first time). The digitized Ca-K images from Kodaikanal Observatory for 1907-1996 are used to study rotation over 0-80 degrees latitudes at an interval of 10$^{\\circ}$ . We find that plages and all types of networks exhibit the differential rotation of the chromosphere. Furthermore, the rotation rate shows a decreasing pattern as one move from the equator to the higher polar latitudes for all the features used in the study. By analyzing how the area of chromospheric features varies over time, we can effectively map the Sun's rotation rate at all latitudes, including the polar regions. Interestingly, both plages and small-scale networks exhibit similar differential rotation rate. This suggests these features likely rooted at the same layer below the visible surface of the Sun. Therefore, the long-term Ca-K data is very useful to study the solar rotation rate at all latitudes including the polar regions.","sentences":["Differential rotation is one of the basic characteristics of the Sun, and it plays an important role in generating the magnetic fields and its activities.","We investigated rotation rate using chromospheric features such as plages, enhanced network, active network, and quiet network separately (for the first time).","The digitized Ca-K images from Kodaikanal Observatory for 1907-1996 are used to study rotation over 0-80 degrees latitudes at an interval of 10$^{\\circ}$ .","We find that plages and all types of networks exhibit the differential rotation of the chromosphere.","Furthermore, the rotation rate shows a decreasing pattern as one move from the equator to the higher polar latitudes for all the features used in the study.","By analyzing how the area of chromospheric features varies over time, we can effectively map the Sun's rotation rate at all latitudes, including the polar regions.","Interestingly, both plages and small-scale networks exhibit similar differential rotation rate.","This suggests these features likely rooted at the same layer below the visible surface of the Sun.","Therefore, the long-term Ca-K data is very useful to study the solar rotation rate at all latitudes including the polar regions."],"url":"http://arxiv.org/abs/2405.07532v1","category":"astro-ph.SR"}
{"created":"2024-05-13 07:58:03","title":"Apparent and emergent dark matter around a Schwarzschild black hole","abstract":"Inspired by the two different dark matter frameworks that were studied recently: one that arises from the non-local effects of entanglement entropy as emergent gravity (characterized by the parameter $\\xi(M)$, and zero-point scale length $l$), and one from dark energy viewed as a superconducting medium (characterized by $\\eta(M)$, and screening length parameter $\\lambda_{\\rm G}$), two black hole solutions spherically surrounded with these dark matter models were derived. The effect of these two frameworks on SMBH was analyzed through the resulting deviations in the null regions and the black hole shadow. In addition, constraints to the parameters $\\xi$ and $\\eta$ (at $3\\sigma$ level) were found using the available EHT data for Sgr. A* and M87*. These constraints allow one to deduce the effective mass $M$, which causes uncertainties in the measurement. On the other hand, if the effective mass is known, one can also deduce the constants associated with $\\xi$, and $\\eta$. The former framework also introduces an Appell function, a hypergeometric function of two variables that separately allows the analysis of macroscopic and (hypothetical) microscopic black holes. This first framework was found to decrease the radii of the null regions respective to the Schwarzschild counterpart. The shadow radius, however, behaves reversibly. The result of the numerical analysis for the latter framework revealed increased values for the photonsphere and shadow radii. Remarkably, the study also showed that for SMBHs, the amplifying effects of $\\lambda_{\\rm G}$ are stronger than the scale length $l$. Finally, results of constraints, as an example, for the upper bound in $\\xi$ for M87* indicated that the effective mass causing the deviation was around $2.4\\times 10^{20} M_{\\odot}$ given that the observed Milgrom's constant is $5.4\\times 10^{-10} \\text{ m/s}^2$.","sentences":["Inspired by the two different dark matter frameworks that were studied recently: one that arises from the non-local effects of entanglement entropy as emergent gravity (characterized by the parameter $\\xi(M)$, and zero-point scale length $l$), and one from dark energy viewed as a superconducting medium (characterized by $\\eta(M)$, and screening length parameter $\\lambda_{\\rm G}$), two black hole solutions spherically surrounded with these dark matter models were derived.","The effect of these two frameworks on SMBH was analyzed through the resulting deviations in the null regions and the black hole shadow.","In addition, constraints to the parameters $\\xi$ and $\\eta$ (at $3\\sigma$ level) were found using the available EHT data for Sgr.","A* and M87*.","These constraints allow one to deduce the effective mass $M$, which causes uncertainties in the measurement.","On the other hand, if the effective mass is known, one can also deduce the constants associated with $\\xi$, and $\\eta$.","The former framework also introduces an Appell function, a hypergeometric function of two variables that separately allows the analysis of macroscopic and (hypothetical) microscopic black holes.","This first framework was found to decrease the radii of the null regions respective to the Schwarzschild counterpart.","The shadow radius, however, behaves reversibly.","The result of the numerical analysis for the latter framework revealed increased values for the photonsphere and shadow radii.","Remarkably, the study also showed that for SMBHs, the amplifying effects of $\\lambda_{\\rm G}$ are stronger than the scale length $l$. Finally, results of constraints, as an example, for the upper bound in $\\xi$ for M87* indicated that the effective mass causing the deviation was around $2.4\\times 10^{20} M_{\\odot}$ given that the observed Milgrom's constant is $5.4\\times 10^{-10} \\text{ m/s}^2$."],"url":"http://arxiv.org/abs/2405.07531v1","category":"gr-qc"}
{"created":"2024-05-13 07:56:15","title":"Prompt-based Code Completion via Multi-Retrieval Augmented Generation","abstract":"Automated code completion, aiming at generating subsequent tokens from unfinished code, has been significantly benefited from recent progress in pre-trained Large Language Models (LLMs). However, these models often suffer from coherence issues and hallucinations when dealing with complex code logic or extrapolating beyond their training data. Existing Retrieval Augmented Generation (RAG) techniques partially address these issues by retrieving relevant code with a separate encoding model where the retrieved snippet serves as contextual reference for code completion. However, their retrieval scope is subject to a singular perspective defined by the encoding model, which largely overlooks the complexity and diversity inherent in code semantics. To address this limitation, we propose ProCC, a code completion framework leveraging prompt engineering and the contextual multi-armed bandits algorithm to flexibly incorporate and adapt to multiple perspectives of code. ProCC first employs a prompt-based multi-retriever system which crafts prompt templates to elicit LLM knowledge to understand code semantics with multiple retrieval perspectives. Then, it adopts the adaptive retrieval selection algorithm to incorporate code similarity into the decision-making process to determine the most suitable retrieval perspective for the LLM to complete the code. Experimental results demonstrate that ProCC outperforms state-of-the-art code completion technique by 8.6% on our collected open-source benchmark suite and 10.1% on the private-domain benchmark suite collected from a billion-user e-commerce company in terms of Exact Match. ProCC also allows augmenting fine-tuned techniques in a plug-and-play manner, yielding 5.6% improvement over our studied fine-tuned model.","sentences":["Automated code completion, aiming at generating subsequent tokens from unfinished code, has been significantly benefited from recent progress in pre-trained Large Language Models (LLMs).","However, these models often suffer from coherence issues and hallucinations when dealing with complex code logic or extrapolating beyond their training data.","Existing Retrieval Augmented Generation (RAG) techniques partially address these issues by retrieving relevant code with a separate encoding model where the retrieved snippet serves as contextual reference for code completion.","However, their retrieval scope is subject to a singular perspective defined by the encoding model, which largely overlooks the complexity and diversity inherent in code semantics.","To address this limitation, we propose ProCC, a code completion framework leveraging prompt engineering and the contextual multi-armed bandits algorithm to flexibly incorporate and adapt to multiple perspectives of code.","ProCC first employs a prompt-based multi-retriever system which crafts prompt templates to elicit LLM knowledge to understand code semantics with multiple retrieval perspectives.","Then, it adopts the adaptive retrieval selection algorithm to incorporate code similarity into the decision-making process to determine the most suitable retrieval perspective for the LLM to complete the code.","Experimental results demonstrate that ProCC outperforms state-of-the-art code completion technique by 8.6% on our collected open-source benchmark suite and 10.1% on the private-domain benchmark suite collected from a billion-user e-commerce company in terms of Exact Match.","ProCC also allows augmenting fine-tuned techniques in a plug-and-play manner, yielding 5.6% improvement over our studied fine-tuned model."],"url":"http://arxiv.org/abs/2405.07530v1","category":"cs.SE"}
{"created":"2024-05-13 07:55:47","title":"Intrinsic Orbital Origin for the Chirality-Dependent Nonlinear Planar Hall Effect of Topological Nodal Fermions in Chiral Crystals","abstract":"Topological semimetals in chiral crystals, which possess both structural handedness and band crossings (or nodes) with topological chiral charge, exhibit many exotic physical properties. Here we demonstrate that the structural and electronic chirality of these systems can endow them with another fascinating phenomenon -- the intrinsic nonlinear planar Hall effect (INPHE), which is prominent around the nodes and reverses sign upon chirality reversal in opposite enantiomers. Taking chiral tellurium as an example, we reveal an intrinsic orbital mechanism, which manifests diverging orbital magnetic moments with hedgehog-like textures around nodes and, therefore, generates a dominant contribution to the INPHE that is proportional to the topological charge. Furthermore, we show that multifold fermions in topological chiral semimetals with B20 structures (e.g., CoSi and PtAl) induce a giant INPHE conductivity reaching the order of $1\\sim 10\\; \\mathrm{A}\\cdot\\mathrm{V}^{-2}\\cdot\\mathrm{T}^{-1}$, which is detectable in experiments. Our study not only relates nonlinear transport to band topology and enantiomer recognition but also offers a new way to explore the exotic physical properties associated with unconventional chiral fermions.","sentences":["Topological semimetals in chiral crystals, which possess both structural handedness and band crossings (or nodes) with topological chiral charge, exhibit many exotic physical properties.","Here we demonstrate that the structural and electronic chirality of these systems can endow them with another fascinating phenomenon -- the intrinsic nonlinear planar Hall effect (INPHE), which is prominent around the nodes and reverses sign upon chirality reversal in opposite enantiomers.","Taking chiral tellurium as an example, we reveal an intrinsic orbital mechanism, which manifests diverging orbital magnetic moments with hedgehog-like textures around nodes and, therefore, generates a dominant contribution to the INPHE that is proportional to the topological charge.","Furthermore, we show that multifold fermions in topological chiral semimetals with B20 structures (e.g., CoSi and PtAl) induce a giant INPHE conductivity reaching the order of $1\\sim 10\\; \\mathrm{A}\\cdot\\mathrm{V}^{-2}\\cdot\\mathrm{T}^{-1}$, which is detectable in experiments.","Our study not only relates nonlinear transport to band topology and enantiomer recognition but also offers a new way to explore the exotic physical properties associated with unconventional chiral fermions."],"url":"http://arxiv.org/abs/2405.07529v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-13 07:48:45","title":"Comparing Perceptions of Static and Adaptive Proactive Speech Agents","abstract":"A growing literature on speech interruptions describes how people interrupt one another with speech, but these behaviours have not yet been implemented in the design of artificial agents which interrupt. Perceptions of a prototype proactive speech agent which adapts its speech to both urgency and to the difficulty of the ongoing task it interrupts are compared against perceptions of a static proactive agent which does not. The study hypothesises that adaptive proactive speech modelled on human speech interruptions will lead to partner models which consider the proactive agent as a stronger conversational partner than a static agent, and that interruptions initiated by an adaptive agent will be judged as better timed and more appropriately asked. These hypotheses are all rejected however, as quantitative analysis reveals that participants view the adaptive agent as a poorer dialogue partner than the static agent and as less appropriate in the style it interrupts. Qualitative analysis sheds light on the source of this surprising finding, as participants see the adaptive agent as less socially appropriate and as less consistent in its interactions than the static agent.","sentences":["A growing literature on speech interruptions describes how people interrupt one another with speech, but these behaviours have not yet been implemented in the design of artificial agents which interrupt.","Perceptions of a prototype proactive speech agent which adapts its speech to both urgency and to the difficulty of the ongoing task it interrupts are compared against perceptions of a static proactive agent which does not.","The study hypothesises that adaptive proactive speech modelled on human speech interruptions will lead to partner models which consider the proactive agent as a stronger conversational partner than a static agent, and that interruptions initiated by an adaptive agent will be judged as better timed and more appropriately asked.","These hypotheses are all rejected however, as quantitative analysis reveals that participants view the adaptive agent as a poorer dialogue partner than the static agent and as less appropriate in the style it interrupts.","Qualitative analysis sheds light on the source of this surprising finding, as participants see the adaptive agent as less socially appropriate and as less consistent in its interactions than the static agent."],"url":"http://arxiv.org/abs/2405.07528v1","category":"cs.HC"}
{"created":"2024-05-13 07:46:48","title":"Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models","abstract":"Despite their prevalence in deep-learning communities, over-parameterized models convey high demands of computational costs for proper training. This work studies the fine-grained, modular-level learning dynamics of over-parameterized models to attain a more efficient and fruitful training strategy. Empirical evidence reveals that when scaling down into network modules, such as heads in self-attention models, we can observe varying learning patterns implicitly associated with each module's trainability. To describe such modular-level learning capabilities, we introduce a novel concept dubbed modular neural tangent kernel (mNTK), and we demonstrate that the quality of a module's learning is tightly associated with its mNTK's principal eigenvalue $\\lambda_{\\max}$. A large $\\lambda_{\\max}$ indicates that the module learns features with better convergence, while those miniature ones may impact generalization negatively. Inspired by the discovery, we propose a novel training strategy termed Modular Adaptive Training (MAT) to update those modules with their $\\lambda_{\\max}$ exceeding a dynamic threshold selectively, concentrating the model on learning common features and ignoring those inconsistent ones. Unlike most existing training schemes with a complete BP cycle across all network modules, MAT can significantly save computations by its partially-updating strategy and can further improve performance. Experiments show that MAT nearly halves the computational cost of model training and outperforms the accuracy of baselines.","sentences":["Despite their prevalence in deep-learning communities, over-parameterized models convey high demands of computational costs for proper training.","This work studies the fine-grained, modular-level learning dynamics of over-parameterized models to attain a more efficient and fruitful training strategy.","Empirical evidence reveals that when scaling down into network modules, such as heads in self-attention models, we can observe varying learning patterns implicitly associated with each module's trainability.","To describe such modular-level learning capabilities, we introduce a novel concept dubbed modular neural tangent kernel (mNTK), and we demonstrate that the quality of a module's learning is tightly associated with its mNTK's principal eigenvalue $\\lambda_{\\max}$. A large $\\lambda_{\\max}$ indicates that the module learns features with better convergence, while those miniature ones may impact generalization negatively.","Inspired by the discovery, we propose a novel training strategy termed Modular Adaptive Training (MAT) to update those modules with their $\\lambda_{\\max}$ exceeding a dynamic threshold selectively, concentrating the model on learning common features and ignoring those inconsistent ones.","Unlike most existing training schemes with a complete BP cycle across all network modules, MAT can significantly save computations by its partially-updating strategy and can further improve performance.","Experiments show that MAT nearly halves the computational cost of model training and outperforms the accuracy of baselines."],"url":"http://arxiv.org/abs/2405.07527v1","category":"cs.LG"}
{"created":"2024-05-13 07:46:44","title":"MS MARCO Web Search: a Large-scale Information-rich Web Dataset with Millions of Real Click Labels","abstract":"Recent breakthroughs in large models have highlighted the critical significance of data scale, labels and modals. In this paper, we introduce MS MARCO Web Search, the first large-scale information-rich web dataset, featuring millions of real clicked query-document labels. This dataset closely mimics real-world web document and query distribution, provides rich information for various kinds of downstream tasks and encourages research in various areas, such as generic end-to-end neural indexer models, generic embedding models, and next generation information access system with large language models. MS MARCO Web Search offers a retrieval benchmark with three web retrieval challenge tasks that demand innovations in both machine learning and information retrieval system research domains. As the first dataset that meets large, real and rich data requirements, MS MARCO Web Search paves the way for future advancements in AI and system research. MS MARCO Web Search dataset is available at: https://github.com/microsoft/MS-MARCO-Web-Search.","sentences":["Recent breakthroughs in large models have highlighted the critical significance of data scale, labels and modals.","In this paper, we introduce MS MARCO Web Search, the first large-scale information-rich web dataset, featuring millions of real clicked query-document labels.","This dataset closely mimics real-world web document and query distribution, provides rich information for various kinds of downstream tasks and encourages research in various areas, such as generic end-to-end neural indexer models, generic embedding models, and next generation information access system with large language models.","MS MARCO Web Search offers a retrieval benchmark with three web retrieval challenge tasks that demand innovations in both machine learning and information retrieval system research domains.","As the first dataset that meets large, real and rich data requirements, MS MARCO Web Search paves the way for future advancements in AI and system research.","MS MARCO Web Search dataset is available at: https://github.com/microsoft/MS-MARCO-Web-Search."],"url":"http://arxiv.org/abs/2405.07526v1","category":"cs.IR"}
{"created":"2024-05-13 07:45:44","title":"Thermodynamical topology with multiple defect curves for dyonic AdS black holes","abstract":"Dyonic black holes with quasitopological electromagnetism exhibit an intriguing phase diagram with two separated first-order coexistence curves. In this paper, we aim to uncover its influence on the black hole thermodynamical topology. At first, we investigate the phase transition and phase diagram of the dyonic black holes. Comparing with previous study that there is no black hole phase transition region for a middle pressure, we find this region can narrow or disappear by fine tuning the coupling parameter. Instead, two first-order phase transitions can be observed. Importantly, we uncover that such novel phase diagram shall lead to a multiple defect curve phenomenon in black hole topology where each dyonic black hole is treated as one defect in the thermodynamical parameter space. By examining the topology, it is shown that there could be one, three, or five black hole states for given pressure and temperature. For each case, the topological number is calculated. Our results show that the topological number always takes value of +1, keeping unchanged even when the multiple defect curves appear. Therefore, our study provides an important ingredient on understanding the black hole thermodynamical topology.","sentences":["Dyonic black holes with quasitopological electromagnetism exhibit an intriguing phase diagram with two separated first-order coexistence curves.","In this paper, we aim to uncover its influence on the black hole thermodynamical topology.","At first, we investigate the phase transition and phase diagram of the dyonic black holes.","Comparing with previous study that there is no black hole phase transition region for a middle pressure, we find this region can narrow or disappear by fine tuning the coupling parameter.","Instead, two first-order phase transitions can be observed.","Importantly, we uncover that such novel phase diagram shall lead to a multiple defect curve phenomenon in black hole topology where each dyonic black hole is treated as one defect in the thermodynamical parameter space.","By examining the topology, it is shown that there could be one, three, or five black hole states for given pressure and temperature.","For each case, the topological number is calculated.","Our results show that the topological number always takes value of +1, keeping unchanged even when the multiple defect curves appear.","Therefore, our study provides an important ingredient on understanding the black hole thermodynamical topology."],"url":"http://arxiv.org/abs/2405.07525v1","category":"hep-th"}
{"created":"2024-05-13 07:41:28","title":"Adaptation of Distinct Semantics for Uncertain Areas in Polyp Segmentation","abstract":"Colonoscopy is a common and practical method for detecting and treating polyps. Segmenting polyps from colonoscopy image is useful for diagnosis and surgery progress. Nevertheless, achieving excellent segmentation performance is still difficult because of polyp characteristics like shape, color, condition, and obvious non-distinction from the surrounding context. This work presents a new novel architecture namely Adaptation of Distinct Semantics for Uncertain Areas in Polyp Segmentation (ADSNet), which modifies misclassified details and recovers weak features having the ability to vanish and not be detected at the final stage. The architecture consists of a complementary trilateral decoder to produce an early global map. A continuous attention module modifies semantics of high-level features to analyze two separate semantics of the early global map. The suggested method is experienced on polyp benchmarks in learning ability and generalization ability, experimental results demonstrate the great correction and recovery ability leading to better segmentation performance compared to the other state of the art in the polyp image segmentation task. Especially, the proposed architecture could be experimented flexibly for other CNN-based encoders, Transformer-based encoders, and decoder backbones.","sentences":["Colonoscopy is a common and practical method for detecting and treating polyps.","Segmenting polyps from colonoscopy image is useful for diagnosis and surgery progress.","Nevertheless, achieving excellent segmentation performance is still difficult because of polyp characteristics like shape, color, condition, and obvious non-distinction from the surrounding context.","This work presents a new novel architecture namely Adaptation of Distinct Semantics for Uncertain Areas in Polyp Segmentation (ADSNet), which modifies misclassified details and recovers weak features having the ability to vanish and not be detected at the final stage.","The architecture consists of a complementary trilateral decoder to produce an early global map.","A continuous attention module modifies semantics of high-level features to analyze two separate semantics of the early global map.","The suggested method is experienced on polyp benchmarks in learning ability and generalization ability, experimental results demonstrate the great correction and recovery ability leading to better segmentation performance compared to the other state of the art in the polyp image segmentation task.","Especially, the proposed architecture could be experimented flexibly for other CNN-based encoders, Transformer-based encoders, and decoder backbones."],"url":"http://arxiv.org/abs/2405.07523v1","category":"cs.CV"}
{"created":"2024-05-13 07:38:28","title":"A new approach to Naples parking functions through complete parking preferences","abstract":"Naples parking functions were introduced as a generalization of classical parking functions, in which cars are allowed to park backwards, by checking up to a fixed number of previous spots, before proceeding forward as usual. In this work we introduce the notion of a complete parking preference, through which we are able to give some information on the combinatorics of Naples parking functions. Roughly speaking, a complete parking preference is a parking preference such that, for any index $j$, there are more cars with preference at least $j$ than spots available from $j$ onward. We provide a characterization of Naples parking functions in terms of certain complete subsequences of them. As a consequence of this result we derive a characterization of permutation-invariant Naples parking functions which turns out to be equivalent to the one given by (Carvalho et al., 2021), but using a totally different approach (and language).","sentences":["Naples parking functions were introduced as a generalization of classical parking functions, in which cars are allowed to park backwards, by checking up to a fixed number of previous spots, before proceeding forward as usual.","In this work we introduce the notion of a complete parking preference, through which we are able to give some information on the combinatorics of Naples parking functions.","Roughly speaking, a complete parking preference is a parking preference such that, for any index $j$, there are more cars with preference at least $j$ than spots available from $j$ onward.","We provide a characterization of Naples parking functions in terms of certain complete subsequences of them.","As a consequence of this result we derive a characterization of permutation-invariant Naples parking functions which turns out to be equivalent to the one given by (Carvalho et al., 2021), but using a totally different approach (and language)."],"url":"http://arxiv.org/abs/2405.07522v1","category":"math.CO"}
{"created":"2024-05-13 17:25:40","title":"Online Load and Graph Balancing for Random Order Inputs","abstract":"Online load balancing for heterogeneous machines aims to minimize the makespan (maximum machine workload) by scheduling arriving jobs with varying sizes on different machines. In the adversarial setting, where an adversary chooses not only the collection of job sizes but also their arrival order, the problem is well-understood and the optimal competitive ratio is known to be $\\Theta(\\log m)$ where $m$ is the number of machines. In the more realistic random arrival order model, the understanding is limited. Previously, the best lower bound on the competitive ratio was only $\\Omega(\\log \\log m)$.   We significantly improve this bound by showing an $\\Omega( \\sqrt {\\log m})$ lower bound, even for the restricted case where each job has a unit size on two machines and infinite size on the others. On the positive side, we propose an $O(\\log m/\\log \\log m)$-competitive algorithm, demonstrating that better performance is possible in the random arrival model.","sentences":["Online load balancing for heterogeneous machines aims to minimize the makespan (maximum machine workload) by scheduling arriving jobs with varying sizes on different machines.","In the adversarial setting, where an adversary chooses not only the collection of job sizes but also their arrival order, the problem is well-understood and the optimal competitive ratio is known to be $\\Theta(\\log m)$ where $m$ is the number of machines.","In the more realistic random arrival order model, the understanding is limited.","Previously, the best lower bound on the competitive ratio was only $\\Omega(\\log \\log m)$.   ","We significantly improve this bound by showing an $\\Omega( \\sqrt {\\log m})$ lower bound, even for the restricted case where each job has a unit size on two machines and infinite size on the others.","On the positive side, we propose an $O(\\log m/\\log \\log m)$-competitive algorithm, demonstrating that better performance is possible in the random arrival model."],"url":"http://arxiv.org/abs/2405.07949v1","category":"cs.DS"}
{"created":"2024-05-13 16:40:17","title":"PLUTO: Pathology-Universal Transformer","abstract":"Pathology is the study of microscopic inspection of tissue, and a pathology diagnosis is often the medical gold standard to diagnose disease. Pathology images provide a unique challenge for computer-vision-based analysis: a single pathology Whole Slide Image (WSI) is gigapixel-sized and often contains hundreds of thousands to millions of objects of interest across multiple resolutions. In this work, we propose PathoLogy Universal TransfOrmer (PLUTO): a light-weight pathology FM that is pre-trained on a diverse dataset of 195 million image tiles collected from multiple sites and extracts meaningful representations across multiple WSI scales that enable a large variety of downstream pathology tasks. In particular, we design task-specific adaptation heads that utilize PLUTO's output embeddings for tasks which span pathology scales ranging from subcellular to slide-scale, including instance segmentation, tile classification, and slide-level prediction. We compare PLUTO's performance to other state-of-the-art methods on a diverse set of external and internal benchmarks covering multiple biologically relevant tasks, tissue types, resolutions, stains, and scanners. We find that PLUTO matches or outperforms existing task-specific baselines and pathology-specific foundation models, some of which use orders-of-magnitude larger datasets and model sizes when compared to PLUTO. Our findings present a path towards a universal embedding to power pathology image analysis, and motivate further exploration around pathology foundation models in terms of data diversity, architectural improvements, sample efficiency, and practical deployability in real-world applications.","sentences":["Pathology is the study of microscopic inspection of tissue, and a pathology diagnosis is often the medical gold standard to diagnose disease.","Pathology images provide a unique challenge for computer-vision-based analysis: a single pathology Whole Slide Image (WSI) is gigapixel-sized and often contains hundreds of thousands to millions of objects of interest across multiple resolutions.","In this work, we propose PathoLogy Universal TransfOrmer (PLUTO): a light-weight pathology FM that is pre-trained on a diverse dataset of 195 million image tiles collected from multiple sites and extracts meaningful representations across multiple WSI scales that enable a large variety of downstream pathology tasks.","In particular, we design task-specific adaptation heads that utilize PLUTO's output embeddings for tasks which span pathology scales ranging from subcellular to slide-scale, including instance segmentation, tile classification, and slide-level prediction.","We compare PLUTO's performance to other state-of-the-art methods on a diverse set of external and internal benchmarks covering multiple biologically relevant tasks, tissue types, resolutions, stains, and scanners.","We find that PLUTO matches or outperforms existing task-specific baselines and pathology-specific foundation models, some of which use orders-of-magnitude larger datasets and model sizes when compared to PLUTO.","Our findings present a path towards a universal embedding to power pathology image analysis, and motivate further exploration around pathology foundation models in terms of data diversity, architectural improvements, sample efficiency, and practical deployability in real-world applications."],"url":"http://arxiv.org/abs/2405.07905v1","category":"eess.IV"}
{"created":"2024-05-13 15:20:21","title":"Subradiance and Superradiant Long Range Excitation Transport among Quantum Emitter Ensembles in a Waveguide","abstract":"In contrast to free space, in waveguides the dispersive and dissipative dipole-dipole interactions among quantum emitters exhibit a periodic behavior over remarkably long distances. We propose a novel setup exploiting this long-range periodicity in order to create highly excited subradiant states and facilitate fast controlled collective energy transport amongst far-apart ensembles coupled to a waveguide. For sufficiently large ensembles collective superradiant emission into the fiber modes dominates over its free space counterpart. We show that for a large number of emitters a fast transverse coherent pulse can create almost perfect subradiant states with up to $50\\%$ excitation. On the other hand, for a coherent excitation of one sub-ensemble above an overall excitation fraction of $50\\%$ we find a nearly lossless and fast energy transfer to the ground state sub-ensemble. This transport can be enhanced or suppressed by controlling the positions of the ensembles relative to each other, while it can also be realized with a random position distribution. In the optimally enhanced case this fast transfer appears as superradiant emission with subsequent superabsorption, yet, without a superradiant decay after the absorption. The highly excited subradiant states as well as the superradiant excitation transfer appear as suitable building blocks in applications like active atomic clocks, quantum batteries, quantum information protocols and quantum metrology procedures such as fiber-based Ramsey schemes.","sentences":["In contrast to free space, in waveguides the dispersive and dissipative dipole-dipole interactions among quantum emitters exhibit a periodic behavior over remarkably long distances.","We propose a novel setup exploiting this long-range periodicity in order to create highly excited subradiant states and facilitate fast controlled collective energy transport amongst far-apart ensembles coupled to a waveguide.","For sufficiently large ensembles collective superradiant emission into the fiber modes dominates over its free space counterpart.","We show that for a large number of emitters a fast transverse coherent pulse can create almost perfect subradiant states with up to $50\\%$ excitation.","On the other hand, for a coherent excitation of one sub-ensemble above an overall excitation fraction of $50\\%$ we find a nearly lossless and fast energy transfer to the ground state sub-ensemble.","This transport can be enhanced or suppressed by controlling the positions of the ensembles relative to each other, while it can also be realized with a random position distribution.","In the optimally enhanced case this fast transfer appears as superradiant emission with subsequent superabsorption, yet, without a superradiant decay after the absorption.","The highly excited subradiant states as well as the superradiant excitation transfer appear as suitable building blocks in applications like active atomic clocks, quantum batteries, quantum information protocols and quantum metrology procedures such as fiber-based Ramsey schemes."],"url":"http://arxiv.org/abs/2405.07833v1","category":"quant-ph"}
{"created":"2024-05-13 14:47:34","title":"A Decentralized and Self-Adaptive Approach for Monitoring Volatile Edge Environments","abstract":"Edge computing provides resources for IoT workloads at the network edge. Monitoring systems are vital for efficiently managing resources and application workloads by collecting, storing, and providing relevant information about the state of the resources. However, traditional monitoring systems have a centralized architecture for both data plane and control plane, which increases latency, creates a failure bottleneck, and faces challenges in providing quick and trustworthy data in volatile edge environments, especially where infrastructures are often built upon failure-prone, unsophisticated computing and network resources. Thus, we propose DEMon, a decentralized, self-adaptive monitoring system for edge. DEMon leverages the stochastic gossip communication protocol at its core. It develops efficient protocols for information dissemination, communication, and retrieval, avoiding a single point of failure and ensuring fast and trustworthy data access. Its decentralized control enables self-adaptive management of monitoring parameters, addressing the trade-offs between the quality of service of monitoring and resource consumption. We implement the proposed system as a lightweight and portable container-based system and evaluate it through experiments. We also present a use case demonstrating its feasibility. The results show that DEMon efficiently disseminates and retrieves the monitoring information, addressing the challenges of edge monitoring.","sentences":["Edge computing provides resources for IoT workloads at the network edge.","Monitoring systems are vital for efficiently managing resources and application workloads by collecting, storing, and providing relevant information about the state of the resources.","However, traditional monitoring systems have a centralized architecture for both data plane and control plane, which increases latency, creates a failure bottleneck, and faces challenges in providing quick and trustworthy data in volatile edge environments, especially where infrastructures are often built upon failure-prone, unsophisticated computing and network resources.","Thus, we propose DEMon, a decentralized, self-adaptive monitoring system for edge.","DEMon leverages the stochastic gossip communication protocol at its core.","It develops efficient protocols for information dissemination, communication, and retrieval, avoiding a single point of failure and ensuring fast and trustworthy data access.","Its decentralized control enables self-adaptive management of monitoring parameters, addressing the trade-offs between the quality of service of monitoring and resource consumption.","We implement the proposed system as a lightweight and portable container-based system and evaluate it through experiments.","We also present a use case demonstrating its feasibility.","The results show that DEMon efficiently disseminates and retrieves the monitoring information, addressing the challenges of edge monitoring."],"url":"http://arxiv.org/abs/2405.07806v1","category":"cs.DC"}
{"created":"2024-05-13 14:47:30","title":"The 25th anniversary for nuclear chirality","abstract":"The brief history for the prediction of the nuclear chirality is provided. The theoretical and experimental investigations of the nuclear chirality are reviewed, including the verification of chiral doublet bands, the chiral conundrum and its resolution, and the prediction and observation of the multiple chiral doublets (M$\\chi$D). Some recent theoretical progresses are highlighted, including the chiral collective Hamiltonian, the A-plot and the K-plot, the nuclear chirality-parity (ChP) violation, the chiral rotation induced by the pairing correlations, as well as the chiral dynamics. The possibly emerging area, challenges that lie ahead, and opportunities for progress in the context of the nuclear chirality are discussed.","sentences":["The brief history for the prediction of the nuclear chirality is provided.","The theoretical and experimental investigations of the nuclear chirality are reviewed, including the verification of chiral doublet bands, the chiral conundrum and its resolution, and the prediction and observation of the multiple chiral doublets (M$\\chi$D).","Some recent theoretical progresses are highlighted, including the chiral collective Hamiltonian, the A-plot and the K-plot, the nuclear chirality-parity (ChP) violation, the chiral rotation induced by the pairing correlations, as well as the chiral dynamics.","The possibly emerging area, challenges that lie ahead, and opportunities for progress in the context of the nuclear chirality are discussed."],"url":"http://arxiv.org/abs/2405.07805v1","category":"nucl-th"}
{"created":"2024-05-13 14:42:21","title":"Collective Decision-Making on Task Allocation Feasibility","abstract":"Robot swarms offer the potential to bring several advantages to the real-world applications but deploying them presents challenges in ensuring feasibility across diverse environments. Assessing the feasibility of new tasks for swarms is crucial to ensure the effective utilisation of resources, as well as to provide awareness of the suitability of a swarm solution for a particular task. In this paper, we introduce the concept of distributed feasibility, where the swarm collectively assesses the feasibility of task allocation based on local observations and interactions. We apply Direct Modulation of Majority-based Decisions as our collective decision-making strategy and show that, in a homogeneous setting, the swarm is able to collectively decide whether a given setup has a high or low feasibility as long as the robot-to-task ratio is not near one.","sentences":["Robot swarms offer the potential to bring several advantages to the real-world applications but deploying them presents challenges in ensuring feasibility across diverse environments.","Assessing the feasibility of new tasks for swarms is crucial to ensure the effective utilisation of resources, as well as to provide awareness of the suitability of a swarm solution for a particular task.","In this paper, we introduce the concept of distributed feasibility, where the swarm collectively assesses the feasibility of task allocation based on local observations and interactions.","We apply Direct Modulation of Majority-based Decisions as our collective decision-making strategy and show that, in a homogeneous setting, the swarm is able to collectively decide whether a given setup has a high or low feasibility as long as the robot-to-task ratio is not near one."],"url":"http://arxiv.org/abs/2405.07799v1","category":"cs.RO"}
{"created":"2024-05-13 14:07:15","title":"LGDE: Local Graph-based Dictionary Expansion","abstract":"Expanding a dictionary of pre-selected keywords is crucial for tasks in information retrieval, such as database query and online data collection. Here we propose Local Graph-based Dictionary Expansion (LGDE), a method that uses tools from manifold learning and network science for the data-driven discovery of keywords starting from a seed dictionary. At the heart of LGDE lies the creation of a word similarity graph derived from word embeddings and the application of local community detection based on graph diffusion to discover semantic neighbourhoods of pre-defined seed keywords. The diffusion in the local graph manifold allows the exploration of the complex nonlinear geometry of word embeddings and can capture word similarities based on paths of semantic association. We validate our method on a corpus of hate speech-related posts from Reddit and Gab and show that LGDE enriches the list of keywords and achieves significantly better performance than threshold methods based on direct word similarities. We further demonstrate the potential of our method through a real-world use case from communication science, where LGDE is evaluated quantitatively on data collected and analysed by domain experts by expanding a conspiracy-related dictionary.","sentences":["Expanding a dictionary of pre-selected keywords is crucial for tasks in information retrieval, such as database query and online data collection.","Here we propose Local Graph-based Dictionary Expansion (LGDE), a method that uses tools from manifold learning and network science for the data-driven discovery of keywords starting from a seed dictionary.","At the heart of LGDE lies the creation of a word similarity graph derived from word embeddings and the application of local community detection based on graph diffusion to discover semantic neighbourhoods of pre-defined seed keywords.","The diffusion in the local graph manifold allows the exploration of the complex nonlinear geometry of word embeddings and can capture word similarities based on paths of semantic association.","We validate our method on a corpus of hate speech-related posts from Reddit and Gab and show that LGDE enriches the list of keywords and achieves significantly better performance than threshold methods based on direct word similarities.","We further demonstrate the potential of our method through a real-world use case from communication science, where LGDE is evaluated quantitatively on data collected and analysed by domain experts by expanding a conspiracy-related dictionary."],"url":"http://arxiv.org/abs/2405.07764v1","category":"cs.CL"}
{"created":"2024-05-13 13:36:53","title":"Search for the radiative transition $\u03c7_{c1}(3872)\\to\u03b3 \u03c8_2(3823)$","abstract":"Using 9.0 $\\rm fb^{-1}$ of $e^+e^-$ collision data collected at center-of-mass energies from 4.178 to 4.278 GeV with the BESIII detector at the BEPCII collider, we perform the first search for the radiative transition $\\chi_{c1}(3872)\\to\\gamma \\psi_2(3823)$. No $\\chi_{c1}(3872)\\to\\gamma \\psi_2(3823)$ signal is observed. The upper limit on the ratio of branching fractions $\\mathcal{B}(\\chi_{c1}(3872)\\to\\gamma \\psi_2(3823), \\psi_2(3823)\\to\\gamma\\chi_{c1})/\\mathcal{B}(\\chi_{c1}(3872)\\to\\pi^+\\pi^- J/\\psi)$ is set as 0.075 at the 90\\% confidence level. Our result contradicts theoretical predictions under the assumption that the $\\chi_{c1}(3872)$ is the pure charmonium state $\\chi_{c1}(2P)$.","sentences":["Using 9.0 $\\rm fb^{-1}$ of $e^+e^-$ collision data collected at center-of-mass energies from 4.178 to 4.278 GeV with the BESIII detector at the BEPCII collider, we perform the first search for the radiative transition $\\chi_{c1}(3872)\\to\\gamma \\psi_2(3823)$.","No $\\chi_{c1}(3872)\\to\\gamma \\psi_2(3823)$ signal is observed.","The upper limit on the ratio of branching fractions $\\mathcal{B}(\\chi_{c1}(3872)\\to\\gamma \\psi_2(3823), \\psi_2(3823)\\to\\gamma\\chi_{c1})/\\mathcal{B}(\\chi_{c1}(3872)\\to\\pi^+\\pi^- J/\\psi)$ is set as 0.075 at the 90\\% confidence level.","Our result contradicts theoretical predictions under the assumption that the $\\chi_{c1}(3872)$ is the pure charmonium state $\\chi_{c1}(2P)$."],"url":"http://arxiv.org/abs/2405.07741v1","category":"hep-ex"}
{"created":"2024-05-13 12:39:08","title":"FORESEE: Multimodal and Multi-view Representation Learning for Robust Prediction of Cancer Survival","abstract":"Integrating the different data modalities of cancer patients can significantly improve the predictive performance of patient survival. However, most existing methods ignore the simultaneous utilization of rich semantic features at different scales in pathology images. When collecting multimodal data and extracting features, there is a likelihood of encountering intra-modality missing data, introducing noise into the multimodal data. To address these challenges, this paper proposes a new end-to-end framework, FORESEE, for robustly predicting patient survival by mining multimodal information. Specifically, the cross-fusion transformer effectively utilizes features at the cellular level, tissue level, and tumor heterogeneity level to correlate prognosis through a cross-scale feature cross-fusion method. This enhances the ability of pathological image feature representation. Secondly, the hybrid attention encoder (HAE) uses the denoising contextual attention module to obtain the contextual relationship features and local detail features of the molecular data. HAE's channel attention module obtains global features of molecular data. Furthermore, to address the issue of missing information within modalities, we propose an asymmetrically masked triplet masked autoencoder to reconstruct lost information within modalities. Extensive experiments demonstrate the superiority of our method over state-of-the-art methods on four benchmark datasets in both complete and missing settings.","sentences":["Integrating the different data modalities of cancer patients can significantly improve the predictive performance of patient survival.","However, most existing methods ignore the simultaneous utilization of rich semantic features at different scales in pathology images.","When collecting multimodal data and extracting features, there is a likelihood of encountering intra-modality missing data, introducing noise into the multimodal data.","To address these challenges, this paper proposes a new end-to-end framework, FORESEE, for robustly predicting patient survival by mining multimodal information.","Specifically, the cross-fusion transformer effectively utilizes features at the cellular level, tissue level, and tumor heterogeneity level to correlate prognosis through a cross-scale feature cross-fusion method.","This enhances the ability of pathological image feature representation.","Secondly, the hybrid attention encoder (HAE) uses the denoising contextual attention module to obtain the contextual relationship features and local detail features of the molecular data.","HAE's channel attention module obtains global features of molecular data.","Furthermore, to address the issue of missing information within modalities, we propose an asymmetrically masked triplet masked autoencoder to reconstruct lost information within modalities.","Extensive experiments demonstrate the superiority of our method over state-of-the-art methods on four benchmark datasets in both complete and missing settings."],"url":"http://arxiv.org/abs/2405.07702v1","category":"cs.CV"}
{"created":"2024-05-13 12:06:06","title":"On Minimum-Dispersion Control of Nonlinear Diffusion Processes","abstract":"This work collects some methodological insights for numerical solution of a \"minimum-dispersion\" control problem for nonlinear stochastic differential equations, a particular relaxation of the covariance steering task. The main ingredient of our approach is the theoretical foundation called $\\infty$-order variational analysis. This framework consists in establishing an exact representation of the increment ($\\infty$-order variation) of the objective functional using the duality, implied by the transformation of the nonlinear stochastic control problem to a linear deterministic control of the Fokker-Planck equation. The resulting formula for the cost increment analytically represents a \"law-feedback\" control for the diffusion process. This control mechanism enables us to learn time-dependent coefficients for a predefined Markovian control structure using Monte Carlo simulations with a modest population of samples. Numerical experiments prove the vitality of our approach.","sentences":["This work collects some methodological insights for numerical solution of a \"minimum-dispersion\" control problem for nonlinear stochastic differential equations, a particular relaxation of the covariance steering task.","The main ingredient of our approach is the theoretical foundation called $\\infty$-order variational analysis.","This framework consists in establishing an exact representation of the increment ($\\infty$-order variation) of the objective functional using the duality, implied by the transformation of the nonlinear stochastic control problem to a linear deterministic control of the Fokker-Planck equation.","The resulting formula for the cost increment analytically represents a \"law-feedback\" control for the diffusion process.","This control mechanism enables us to learn time-dependent coefficients for a predefined Markovian control structure using Monte Carlo simulations with a modest population of samples.","Numerical experiments prove the vitality of our approach."],"url":"http://arxiv.org/abs/2405.07676v1","category":"math.OC"}
{"created":"2024-05-13 11:40:12","title":"Square-well model for superconducting pair-potential","abstract":"We study Andreev reflection in a one-dimensional square-well pair-potential. We discuss the history of the model. The current-phase relation is presented as a sum over Matsubara frequencies. How the current arises from bound and continuum levels is found by analytic continuation. We discuss two limiting cases of square-well model, the zero-length well and the infinite well. The model is quantitatively valid in some cases, but forms the basis for understanding a wide range of problems in inhomogeneous superconductivity and superfluidity.","sentences":["We study Andreev reflection in a one-dimensional square-well pair-potential.","We discuss the history of the model.","The current-phase relation is presented as a sum over Matsubara frequencies.","How the current arises from bound and continuum levels is found by analytic continuation.","We discuss two limiting cases of square-well model, the zero-length well and the infinite well.","The model is quantitatively valid in some cases, but forms the basis for understanding a wide range of problems in inhomogeneous superconductivity and superfluidity."],"url":"http://arxiv.org/abs/2405.07659v1","category":"cond-mat.supr-con"}
{"created":"2024-05-13 11:39:36","title":"Understanding Data Understanding: A Framework to Navigate the Intricacies of Data Analytics","abstract":"As organizations face the challenges of processing exponentially growing data volumes, their reliance on analytics to unlock value from this data has intensified. However, the intricacies of big data, such as its extensive feature sets, pose significant challenges. A crucial step in leveraging this data for insightful analysis is an in-depth understanding of both the data and its domain. Yet, existing literature presents a fragmented picture of what comprises an effective understanding of data and domain, varying significantly in depth and focus. To address this research gap, we conduct a systematic literature review, aiming to delineate the dimensions of data understanding. We identify five dimensions: Foundations, Collection & Selection, Contextualization & Integration, Exploration & Discovery, and Insights. These dimensions collectively form a comprehensive framework for data understanding, providing guidance for organizations seeking meaningful insights from complex datasets. This study synthesizes the current state of knowledge and lays the groundwork for further exploration.","sentences":["As organizations face the challenges of processing exponentially growing data volumes, their reliance on analytics to unlock value from this data has intensified.","However, the intricacies of big data, such as its extensive feature sets, pose significant challenges.","A crucial step in leveraging this data for insightful analysis is an in-depth understanding of both the data and its domain.","Yet, existing literature presents a fragmented picture of what comprises an effective understanding of data and domain, varying significantly in depth and focus.","To address this research gap, we conduct a systematic literature review, aiming to delineate the dimensions of data understanding.","We identify five dimensions: Foundations, Collection & Selection, Contextualization & Integration, Exploration & Discovery, and Insights.","These dimensions collectively form a comprehensive framework for data understanding, providing guidance for organizations seeking meaningful insights from complex datasets.","This study synthesizes the current state of knowledge and lays the groundwork for further exploration."],"url":"http://arxiv.org/abs/2405.07658v1","category":"cs.HC"}
{"created":"2024-05-13 09:24:15","title":"Is it getting harder to make a hit? Evidence from 65 years of US music chart history","abstract":"Since the creation of the Billboard Hot 100 music chart in 1958, the chart has been a window into the music consumption of Americans. Which songs succeed on the chart is decided by consumption volumes, which can be affected by consumer music taste, and other factors such as advertisement budgets, airplay time, the specifics of ranking algorithms, and more. Since its introduction, the chart has documented music consumerism through eras of globalization, economic growth, and the emergence of new technologies for music listening. In recent years, musicians and other hitmakers have voiced their worry that the music world is changing: Many claim that it is getting harder to make a hit but until now, the claims have not been backed using chart data. Here we show that the dynamics of the Billboard Hot 100 chart have changed significantly since the chart's founding in 1958, and in particular in the past 15 years. Whereas most songs spend less time on the chart now than songs did in the past, we show that top-1 songs have tripled their chart lifetime since the 1960s, the highest-ranked songs maintain their positions for far longer than previously, and the lowest-ranked songs are replaced more frequently than ever. At the same time, who occupies the chart has also changed over the years: In recent years, fewer new artists make it into the chart and more positions are occupied by established hit makers. Finally, investigating how song chart trajectories have changed over time, we show that historical song trajectories cluster into clear trajectory archetypes characteristic of the time period they were part of. The results are interesting in the context of collective attention: Whereas recent studies have documented that other cultural products such as books, news, and movies fade in popularity quicker in recent years, music hits seem to last longer now than in the past.","sentences":["Since the creation of the Billboard Hot 100 music chart in 1958, the chart has been a window into the music consumption of Americans.","Which songs succeed on the chart is decided by consumption volumes, which can be affected by consumer music taste, and other factors such as advertisement budgets, airplay time, the specifics of ranking algorithms, and more.","Since its introduction, the chart has documented music consumerism through eras of globalization, economic growth, and the emergence of new technologies for music listening.","In recent years, musicians and other hitmakers have voiced their worry that the music world is changing: Many claim that it is getting harder to make a hit but until now, the claims have not been backed using chart data.","Here we show that the dynamics of the Billboard Hot 100 chart have changed significantly since the chart's founding in 1958, and in particular in the past 15 years.","Whereas most songs spend less time on the chart now than songs did in the past, we show that top-1 songs have tripled their chart lifetime since the 1960s, the highest-ranked songs maintain their positions for far longer than previously, and the lowest-ranked songs are replaced more frequently than ever.","At the same time, who occupies the chart has also changed over the years: In recent years, fewer new artists make it into the chart and more positions are occupied by established hit makers.","Finally, investigating how song chart trajectories have changed over time, we show that historical song trajectories cluster into clear trajectory archetypes characteristic of the time period they were part of.","The results are interesting in the context of collective attention: Whereas recent studies have documented that other cultural products such as books, news, and movies fade in popularity quicker in recent years, music hits seem to last longer now than in the past."],"url":"http://arxiv.org/abs/2405.07574v1","category":"physics.soc-ph"}
{"created":"2024-05-13 09:17:42","title":"Gaze-Based Intention Recognition for Human-Robot Collaboration","abstract":"This work aims to tackle the intent recognition problem in Human-Robot Collaborative assembly scenarios. Precisely, we consider an interactive assembly of a wooden stool where the robot fetches the pieces in the correct order and the human builds the parts following the instruction manual. The intent recognition is limited to the idle state estimation and it is needed to ensure a better synchronization between the two agents. We carried out a comparison between two distinct solutions involving wearable sensors and eye tracking integrated into the perception pipeline of a flexible planning architecture based on Hierarchical Task Networks. At runtime, the wearable sensing module exploits the raw measurements from four 9-axis Inertial Measurement Units positioned on the wrists and hands of the user as an input for a Long Short-Term Memory Network. On the other hand, the eye tracking relies on a Head Mounted Display and Unreal Engine.   We tested the effectiveness of the two approaches with 10 participants, each of whom explored both options in alternate order. We collected explicit metrics about the attractiveness and efficiency of the two techniques through User Experience Questionnaires as well as implicit criteria regarding the classification time and the overall assembly time.   The results of our work show that the two methods can reach comparable performances both in terms of effectiveness and user preference. Future development could aim at joining the two approaches two allow the recognition of more complex activities and to anticipate the user actions.","sentences":["This work aims to tackle the intent recognition problem in Human-Robot Collaborative assembly scenarios.","Precisely, we consider an interactive assembly of a wooden stool where the robot fetches the pieces in the correct order and the human builds the parts following the instruction manual.","The intent recognition is limited to the idle state estimation and it is needed to ensure a better synchronization between the two agents.","We carried out a comparison between two distinct solutions involving wearable sensors and eye tracking integrated into the perception pipeline of a flexible planning architecture based on Hierarchical Task Networks.","At runtime, the wearable sensing module exploits the raw measurements from four 9-axis Inertial Measurement Units positioned on the wrists and hands of the user as an input for a Long Short-Term Memory Network.","On the other hand, the eye tracking relies on a Head Mounted Display and Unreal Engine.   ","We tested the effectiveness of the two approaches with 10 participants, each of whom explored both options in alternate order.","We collected explicit metrics about the attractiveness and efficiency of the two techniques through User Experience Questionnaires as well as implicit criteria regarding the classification time and the overall assembly time.   ","The results of our work show that the two methods can reach comparable performances both in terms of effectiveness and user preference.","Future development could aim at joining the two approaches two allow the recognition of more complex activities and to anticipate the user actions."],"url":"http://arxiv.org/abs/2405.07570v1","category":"cs.RO"}
{"created":"2024-05-13 08:36:06","title":"Safety-Aware Human-Lead Vehicle Platooning by Proactively Reacting to Uncertain Human Behaving","abstract":"Human-Lead Cooperative Adaptive Cruise Control (HL-CACC) is regarded as a promising vehicle platooning technology in real-world implementation. By utilizing a Human-driven Vehicle (HV) as the platoon leader, HL-CACC reduces the cost and enhances the reliability of perception and decision-making. However, state-of-the-art HL-CACC technology still has a great limitation on driving safety for the lack of considering the leading human driver's uncertain behaving. In this study, a HL-CACC controller is designed based on Stochastic Model Predictive Control (SMPC). It is enabled to predict the driving intention of the leading Connected Human-Driven Vehicle (CHV). The proposed controller has the following features: i) enhanced perceived safety in oscillating traffic; ii) guaranteed safety against hard brakes; iii) computational efficient for real-time implementation. The proposed controller is evaluated on a PreScan&Simulink simulation platform. Real vehicle trajectory data is collected for the calibration of simulation. Results reveal that the proposed controller: i) improves perceived safety by 19.17% in oscillating traffic; ii) enhances actual safety by 7.76% against hard brake; iii) is confirmed with string stability. The computation time is approximately 3 milliseconds when running on a laptop equipped with an Intel i5-13500H CPU. This indicates the proposed controller is ready for real-time implementation.","sentences":["Human-Lead Cooperative Adaptive Cruise Control (HL-CACC) is regarded as a promising vehicle platooning technology in real-world implementation.","By utilizing a Human-driven Vehicle (HV) as the platoon leader, HL-CACC reduces the cost and enhances the reliability of perception and decision-making.","However, state-of-the-art HL-CACC technology still has a great limitation on driving safety for the lack of considering the leading human driver's uncertain behaving.","In this study, a HL-CACC controller is designed based on Stochastic Model Predictive Control (SMPC).","It is enabled to predict the driving intention of the leading Connected Human-Driven Vehicle (CHV).","The proposed controller has the following features: i) enhanced perceived safety in oscillating traffic; ii) guaranteed safety against hard brakes; iii) computational efficient for real-time implementation.","The proposed controller is evaluated on a PreScan&Simulink simulation platform.","Real vehicle trajectory data is collected for the calibration of simulation.","Results reveal that the proposed controller: i) improves perceived safety by 19.17% in oscillating traffic; ii) enhances actual safety by 7.76% against hard brake; iii) is confirmed with string stability.","The computation time is approximately 3 milliseconds when running on a laptop equipped with an Intel i5-13500H CPU.","This indicates the proposed controller is ready for real-time implementation."],"url":"http://arxiv.org/abs/2405.07556v1","category":"cs.RO"}
{"created":"2024-05-13 08:31:58","title":"Wild Berry image dataset collected in Finnish forests and peatlands using drones","abstract":"Berry picking has long-standing traditions in Finland, yet it is challenging and can potentially be dangerous. The integration of drones equipped with advanced imaging techniques represents a transformative leap forward, optimising harvests and promising sustainable practices. We propose WildBe, the first image dataset of wild berries captured in peatlands and under the canopy of Finnish forests using drones. Unlike previous and related datasets, WildBe includes new varieties of berries, such as bilberries, cloudberries, lingonberries, and crowberries, captured under severe light variations and in cluttered environments. WildBe features 3,516 images, including a total of 18,468 annotated bounding boxes. We carry out a comprehensive analysis of WildBe using six popular object detectors, assessing their effectiveness in berry detection across different forest regions and camera types. We will release WildBe publicly.","sentences":["Berry picking has long-standing traditions in Finland, yet it is challenging and can potentially be dangerous.","The integration of drones equipped with advanced imaging techniques represents a transformative leap forward, optimising harvests and promising sustainable practices.","We propose WildBe, the first image dataset of wild berries captured in peatlands and under the canopy of Finnish forests using drones.","Unlike previous and related datasets, WildBe includes new varieties of berries, such as bilberries, cloudberries, lingonberries, and crowberries, captured under severe light variations and in cluttered environments.","WildBe features 3,516 images, including a total of 18,468 annotated bounding boxes.","We carry out a comprehensive analysis of WildBe using six popular object detectors, assessing their effectiveness in berry detection across different forest regions and camera types.","We will release WildBe publicly."],"url":"http://arxiv.org/abs/2405.07550v1","category":"cs.CV"}
{"created":"2024-05-13 08:19:27","title":"Intrinsic Langevin dynamics of rigid inclusions on curved surfaces","abstract":"The stochastic dynamics of a rigid inclusion constrained to move on a curved surface has many applications in biological and soft matter physics, ranging from the diffusion of passive or active membrane proteins to the motion of phoretic particles on liquid-liquid interfaces. Here we construct intrinsic Langevin equations for an oriented rigid inclusion on a curved surface using Cartan's method of moving frames. We first derive the Hamiltonian equations of motion for the translational and rotational momenta in the body frame. Surprisingly, surface curvature couples the linear and angular momenta of the inclusion. We then add to the Hamiltonian equations linear friction, white noise and arbitrary configuration-dependent forces and torques to obtain intrinsic Langevin equations of motion in phase space. We provide the integrability conditions, made non-trivial by surface curvature, for the forces and torques to admit a potential, thus distinguishing between passive and active stochastic motion. We derive the corresponding Fokker-Planck equation in geometric form and obtain fluctuation-dissipation relations that ensure Gibbsian equilibrium. We extract the overdamped equations of motion by adiabatically eliminating the momenta from the Fokker-Planck equation, showing how a peculiar cancellation leads to the naively expected Smoluchowski limit. The overdamped equations can be used for accurate and efficient intrinsic Brownian dynamics simulations of passive, driven and active diffusion processes on curved surfaces. Our work generalises to the collective dynamics of many inclusions on curved surfaces.","sentences":["The stochastic dynamics of a rigid inclusion constrained to move on a curved surface has many applications in biological and soft matter physics, ranging from the diffusion of passive or active membrane proteins to the motion of phoretic particles on liquid-liquid interfaces.","Here we construct intrinsic Langevin equations for an oriented rigid inclusion on a curved surface using Cartan's method of moving frames.","We first derive the Hamiltonian equations of motion for the translational and rotational momenta in the body frame.","Surprisingly, surface curvature couples the linear and angular momenta of the inclusion.","We then add to the Hamiltonian equations linear friction, white noise and arbitrary configuration-dependent forces and torques to obtain intrinsic Langevin equations of motion in phase space.","We provide the integrability conditions, made non-trivial by surface curvature, for the forces and torques to admit a potential, thus distinguishing between passive and active stochastic motion.","We derive the corresponding Fokker-Planck equation in geometric form and obtain fluctuation-dissipation relations that ensure Gibbsian equilibrium.","We extract the overdamped equations of motion by adiabatically eliminating the momenta from the Fokker-Planck equation, showing how a peculiar cancellation leads to the naively expected Smoluchowski limit.","The overdamped equations can be used for accurate and efficient intrinsic Brownian dynamics simulations of passive, driven and active diffusion processes on curved surfaces.","Our work generalises to the collective dynamics of many inclusions on curved surfaces."],"url":"http://arxiv.org/abs/2405.07539v1","category":"cond-mat.soft"}
{"created":"2024-05-13 07:32:45","title":"SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and Composition of Experts","abstract":"Monolithic large language models (LLMs) like GPT-4 have paved the way for modern generative AI applications. Training, serving, and maintaining monolithic LLMs at scale, however, remains prohibitively expensive and challenging. The disproportionate increase in compute-to-memory ratio of modern AI accelerators have created a memory wall, necessitating new methods to deploy AI. Composition of Experts (CoE) is an alternative modular approach that lowers the cost and complexity of training and serving. However, this approach presents two key challenges when using conventional hardware: (1) without fused operations, smaller models have lower operational intensity, which makes high utilization more challenging to achieve; and (2) hosting a large number of models can be either prohibitively expensive or slow when dynamically switching between them.   In this paper, we describe how combining CoE, streaming dataflow, and a three-tier memory system scales the AI memory wall. We describe Samba-CoE, a CoE system with 150 experts and a trillion total parameters. We deploy Samba-CoE on the SambaNova SN40L Reconfigurable Dataflow Unit (RDU) - a commercial dataflow accelerator architecture that has been co-designed for enterprise inference and training applications. The chip introduces a new three-tier memory system with on-chip distributed SRAM, on-package HBM, and off-package DDR DRAM. A dedicated inter-RDU network enables scaling up and out over multiple sockets. We demonstrate speedups ranging from 2x to 13x on various benchmarks running on eight RDU sockets compared with an unfused baseline. We show that for CoE inference deployments, the 8-socket RDU Node reduces machine footprint by up to 19x, speeds up model switching time by 15x to 31x, and achieves an overall speedup of 3.7x over a DGX H100 and 6.6x over a DGX A100.","sentences":["Monolithic large language models (LLMs) like GPT-4 have paved the way for modern generative AI applications.","Training, serving, and maintaining monolithic LLMs at scale, however, remains prohibitively expensive and challenging.","The disproportionate increase in compute-to-memory ratio of modern AI accelerators have created a memory wall, necessitating new methods to deploy AI.","Composition of Experts (CoE) is an alternative modular approach that lowers the cost and complexity of training and serving.","However, this approach presents two key challenges when using conventional hardware: (1) without fused operations, smaller models have lower operational intensity, which makes high utilization more challenging to achieve; and (2) hosting a large number of models can be either prohibitively expensive or slow when dynamically switching between them.   ","In this paper, we describe how combining CoE, streaming dataflow, and a three-tier memory system scales the AI memory wall.","We describe Samba-CoE, a CoE system with 150 experts and a trillion total parameters.","We deploy Samba-CoE on the SambaNova SN40L","Reconfigurable Dataflow Unit (RDU) - a commercial dataflow accelerator architecture that has been co-designed for enterprise inference and training applications.","The chip introduces a new three-tier memory system with on-chip distributed SRAM, on-package HBM, and off-package DDR DRAM.","A dedicated inter-RDU network enables scaling up and out over multiple sockets.","We demonstrate speedups ranging from 2x to 13x on various benchmarks running on eight RDU sockets compared with an unfused baseline.","We show that for CoE inference deployments, the 8-socket RDU Node reduces machine footprint by up to 19x, speeds up model switching time by 15x to 31x, and achieves an overall speedup of 3.7x over a DGX H100 and 6.6x over a DGX A100."],"url":"http://arxiv.org/abs/2405.07518v1","category":"cs.AR"}
{"created":"2024-05-13 07:22:50","title":"OpenBot-Fleet: A System for Collective Learning with Real Robots","abstract":"We introduce OpenBot-Fleet, a comprehensive open-source cloud robotics system for navigation. OpenBot-Fleet uses smartphones for sensing, local compute and communication, Google Firebase for secure cloud storage and off-board compute, and a robust yet low-cost wheeled robot toact in real-world environments. The robots collect task data and upload it to the cloud where navigation policies can be learned either offline or online and can then be sent back to the robot fleet. In our experiments we distribute 72 robots to a crowd of workers who operate them in homes, and show that OpenBot-Fleet can learn robust navigation policies that generalize to unseen homes with >80% success rate. OpenBot-Fleet represents a significant step forward in cloud robotics, making it possible to deploy large continually learning robot fleets in a cost-effective and scalable manner. All materials can be found at https://www.openbot.org. A video is available at https://youtu.be/wiv2oaDgDi8","sentences":["We introduce OpenBot-Fleet, a comprehensive open-source cloud robotics system for navigation.","OpenBot-Fleet uses smartphones for sensing, local compute and communication, Google Firebase for secure cloud storage and off-board compute, and a robust yet low-cost wheeled robot toact in real-world environments.","The robots collect task data and upload it to the cloud where navigation policies can be learned either offline or online and can then be sent back to the robot fleet.","In our experiments we distribute 72 robots to a crowd of workers who operate them in homes, and show that OpenBot-Fleet can learn robust navigation policies that generalize to unseen homes with >80% success rate.","OpenBot-Fleet represents a significant step forward in cloud robotics, making it possible to deploy large continually learning robot fleets in a cost-effective and scalable manner.","All materials can be found at https://www.openbot.org.","A video is available at https://youtu.be/wiv2oaDgDi8"],"url":"http://arxiv.org/abs/2405.07515v1","category":"cs.RO"}
{"created":"2024-05-13 07:10:35","title":"RESTAD: REconstruction and Similarity based Transformer for time series Anomaly Detection","abstract":"Anomaly detection in time series data is crucial across various domains. The scarcity of labeled data for such tasks has increased the attention towards unsupervised learning methods. These approaches, often relying solely on reconstruction error, typically fail to detect subtle anomalies in complex datasets. To address this, we introduce RESTAD, an adaptation of the Transformer model by incorporating a layer of Radial Basis Function (RBF) neurons within its architecture. This layer fits a non-parametric density in the latent representation, such that a high RBF output indicates similarity with predominantly normal training data. RESTAD integrates the RBF similarity scores with the reconstruction errors to increase sensitivity to anomalies. Our empirical evaluations demonstrate that RESTAD outperforms various established baselines across multiple benchmark datasets.","sentences":["Anomaly detection in time series data is crucial across various domains.","The scarcity of labeled data for such tasks has increased the attention towards unsupervised learning methods.","These approaches, often relying solely on reconstruction error, typically fail to detect subtle anomalies in complex datasets.","To address this, we introduce RESTAD, an adaptation of the Transformer model by incorporating a layer of Radial Basis Function (RBF) neurons within its architecture.","This layer fits a non-parametric density in the latent representation, such that a high RBF output indicates similarity with predominantly normal training data.","RESTAD integrates the RBF similarity scores with the reconstruction errors to increase sensitivity to anomalies.","Our empirical evaluations demonstrate that RESTAD outperforms various established baselines across multiple benchmark datasets."],"url":"http://arxiv.org/abs/2405.07509v1","category":"cs.LG"}
{"created":"2024-05-13 06:53:42","title":"Consistency Policy: Accelerated Visuomotor Policies via Consistency Distillation","abstract":"Many robotic systems, such as mobile manipulators or quadrotors, cannot be equipped with high-end GPUs due to space, weight, and power constraints. These constraints prevent these systems from leveraging recent developments in visuomotor policy architectures that require high-end GPUs to achieve fast policy inference. In this paper, we propose Consistency Policy, a faster and similarly powerful alternative to Diffusion Policy for learning visuomotor robot control. By virtue of its fast inference speed, Consistency Policy can enable low latency decision making in resource-constrained robotic setups. A Consistency Policy is distilled from a pretrained Diffusion Policy by enforcing self-consistency along the Diffusion Policy's learned trajectories. We compare Consistency Policy with Diffusion Policy and other related speed-up methods across 6 simulation tasks as well as two real-world tasks where we demonstrate inference on a laptop GPU. For all these tasks, Consistency Policy speeds up inference by an order of magnitude compared to the fastest alternative method and maintains competitive success rates. We also show that the Conistency Policy training procedure is robust to the pretrained Diffusion Policy's quality, a useful result that helps practioners avoid extensive testing of the pretrained model. Key design decisions that enabled this performance are the choice of consistency objective, reduced initial sample variance, and the choice of preset chaining steps. Code and training details will be released publicly.","sentences":["Many robotic systems, such as mobile manipulators or quadrotors, cannot be equipped with high-end GPUs due to space, weight, and power constraints.","These constraints prevent these systems from leveraging recent developments in visuomotor policy architectures that require high-end GPUs to achieve fast policy inference.","In this paper, we propose Consistency Policy, a faster and similarly powerful alternative to Diffusion Policy for learning visuomotor robot control.","By virtue of its fast inference speed, Consistency Policy can enable low latency decision making in resource-constrained robotic setups.","A Consistency Policy is distilled from a pretrained Diffusion Policy by enforcing self-consistency along the Diffusion Policy's learned trajectories.","We compare Consistency Policy with Diffusion Policy and other related speed-up methods across 6 simulation tasks as well as two real-world tasks where we demonstrate inference on a laptop GPU.","For all these tasks, Consistency Policy speeds up inference by an order of magnitude compared to the fastest alternative method and maintains competitive success rates.","We also show that the Conistency Policy training procedure is robust to the pretrained Diffusion Policy's quality, a useful result that helps practioners avoid extensive testing of the pretrained model.","Key design decisions that enabled this performance are the choice of consistency objective, reduced initial sample variance, and the choice of preset chaining steps.","Code and training details will be released publicly."],"url":"http://arxiv.org/abs/2405.07503v1","category":"cs.RO"}
{"created":"2024-05-13 06:36:30","title":"PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking","abstract":"Linking (aligning) biomedical concepts across diverse data sources enables various integrative analyses, but it is challenging due to the discrepancies in concept naming conventions. Various strategies have been developed to overcome this challenge, such as those based on string-matching rules, manually crafted thesauri, and machine learning models. However, these methods are constrained by limited prior biomedical knowledge and can hardly generalize beyond the limited amounts of rules, thesauri, or training samples. Recently, large language models (LLMs) have exhibited impressive results in diverse biomedical NLP tasks due to their unprecedentedly rich prior knowledge and strong zero-shot prediction abilities. However, LLMs suffer from issues including high costs, limited context length, and unreliable predictions. In this research, we propose PromptLink, a novel biomedical concept linking framework that leverages LLMs. It first employs a biomedical-specialized pre-trained language model to generate candidate concepts that can fit in the LLM context windows. Then it utilizes an LLM to link concepts through two-stage prompts, where the first-stage prompt aims to elicit the biomedical prior knowledge from the LLM for the concept linking task and the second-stage prompt enforces the LLM to reflect on its own predictions to further enhance their reliability. Empirical results on the concept linking task between two EHR datasets and an external biomedical KG demonstrate the effectiveness of PromptLink. Furthermore, PromptLink is a generic framework without reliance on additional prior knowledge, context, or training data, making it well-suited for concept linking across various types of data sources. The source code is available at https://github.com/constantjxyz/PromptLink.","sentences":["Linking (aligning) biomedical concepts across diverse data sources enables various integrative analyses, but it is challenging due to the discrepancies in concept naming conventions.","Various strategies have been developed to overcome this challenge, such as those based on string-matching rules, manually crafted thesauri, and machine learning models.","However, these methods are constrained by limited prior biomedical knowledge and can hardly generalize beyond the limited amounts of rules, thesauri, or training samples.","Recently, large language models (LLMs) have exhibited impressive results in diverse biomedical NLP tasks due to their unprecedentedly rich prior knowledge and strong zero-shot prediction abilities.","However, LLMs suffer from issues including high costs, limited context length, and unreliable predictions.","In this research, we propose PromptLink, a novel biomedical concept linking framework that leverages LLMs.","It first employs a biomedical-specialized pre-trained language model to generate candidate concepts that can fit in the LLM context windows.","Then it utilizes an LLM to link concepts through two-stage prompts, where the first-stage prompt aims to elicit the biomedical prior knowledge from the LLM for the concept linking task and the second-stage prompt enforces the LLM to reflect on its own predictions to further enhance their reliability.","Empirical results on the concept linking task between two EHR datasets and an external biomedical KG demonstrate the effectiveness of PromptLink.","Furthermore, PromptLink is a generic framework without reliance on additional prior knowledge, context, or training data, making it well-suited for concept linking across various types of data sources.","The source code is available at https://github.com/constantjxyz/PromptLink."],"url":"http://arxiv.org/abs/2405.07500v1","category":"cs.IR"}
{"created":"2024-05-13 06:31:48","title":"MacBehaviour: An R package for behavioural experimentation on large language models","abstract":"There has been increasing interest in investigating the behaviours of large language models (LLMs) and LLM-powered chatbots by treating an LLM as a participant in a psychological experiment. We therefore developed an R package called \"MacBehaviour\" that aims to interact with more than 60 language models in one package (e.g., OpenAI's GPT family, the Claude family, Gemini, Llama family, and open-source models) and streamline the experimental process of LLMs behaviour experiments. The package offers a comprehensive set of functions designed for LLM experiments, covering experiment design, stimuli presentation, model behaviour manipulation, logging response and token probability. To demonstrate the utility and effectiveness of \"MacBehaviour,\" we conducted three validation experiments on three LLMs (GPT-3.5, Llama-2 7B, and Vicuna-1.5 13B) to replicate sound-gender association in LLMs. The results consistently showed that they exhibit human-like tendencies to infer gender from novel personal names based on their phonology, as previously demonstrated (Cai et al., 2023). In summary, \"MacBehaviour\" is an R package for machine behaviour studies which offers a user-friendly interface and comprehensive features to simplify and standardize the experimental process.","sentences":["There has been increasing interest in investigating the behaviours of large language models (LLMs) and LLM-powered chatbots by treating an LLM as a participant in a psychological experiment.","We therefore developed an R package called \"MacBehaviour\" that aims to interact with more than 60 language models in one package (e.g., OpenAI's GPT family, the Claude family, Gemini, Llama family, and open-source models) and streamline the experimental process of LLMs behaviour experiments.","The package offers a comprehensive set of functions designed for LLM experiments, covering experiment design, stimuli presentation, model behaviour manipulation, logging response and token probability.","To demonstrate the utility and effectiveness of \"MacBehaviour,\" we conducted three validation experiments on three LLMs (GPT-3.5, Llama-2 7B, and Vicuna-1.5 13B) to replicate sound-gender association in LLMs.","The results consistently showed that they exhibit human-like tendencies to infer gender from novel personal names based on their phonology, as previously demonstrated (Cai et al., 2023).","In summary, \"MacBehaviour\" is an R package for machine behaviour studies which offers a user-friendly interface and comprehensive features to simplify and standardize the experimental process."],"url":"http://arxiv.org/abs/2405.07495v1","category":"cs.CL"}
{"created":"2024-05-13 06:09:10","title":"Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning","abstract":"The rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources. This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data. Experiments with Mistral-7B (Jiang et al., 2023) and Gemma-7B (Team et al., 2024) models demonstrate that curriculum learning slightly improves performance compared to traditional random data shuffling. Notably, we observed that sorting data based on our proposed attention criteria generally led to better performance. This approach offers a sustainable method to enhance LLM performance without increasing model size or dataset volume, addressing scalability challenges in LLM training.","sentences":["The rapid advancement of Large Language Models (LLMs) has improved text understanding and generation but poses challenges in computational resources.","This study proposes a curriculum learning-inspired, data-centric training strategy that begins with simpler tasks and progresses to more complex ones, using criteria such as prompt length, attention scores, and loss values to structure the training data.","Experiments with Mistral-7B (Jiang et al., 2023) and Gemma-7B (Team et al., 2024) models demonstrate that curriculum learning slightly improves performance compared to traditional random data shuffling.","Notably, we observed that sorting data based on our proposed attention criteria generally led to better performance.","This approach offers a sustainable method to enhance LLM performance without increasing model size or dataset volume, addressing scalability challenges in LLM training."],"url":"http://arxiv.org/abs/2405.07490v1","category":"cs.CL"}
{"created":"2024-05-13 05:47:57","title":"Complexity and Its Creation","abstract":"Except for crystalline or random structures, an agreed definition of complexity for intermediate and hence interesting cases does not exist. We fill this gap with a notion of complexity that characterises shapes formed by any finite number of particles greater than or equal to the three needed to define triangle shapes. The resulting shape complexity is a simple scale-invariant quantity that measures the extent to which a collection of particles has a uniform or clustered distribution. As a positive-definite number with an absolute minimum realised on the most uniform distribution the particles can have, it not only characterises all physical structures from crystals to the most complex that can exist but also determines for them a measure that makes richly structured shapes more probable than bland ones. Strikingly, the criterion employed to define the shape complexity forces it to be the product of the two functions that define Newtonian universal gravitation. This suggests both the form and solutions the law of a universe of such particles should have and leads to a theory that not only determines the complexity and probability of any individual shape but also its creation from the maximally uniform shape. It does this moreover in a manner which makes it probable that the cosmological principle, according to which on a sufficiently large scale the universe should have the same appearance everywhere, holds. Our theory relies on universal group-theoretical principles that may allow generalisation to include all forces and general relativity.","sentences":["Except for crystalline or random structures, an agreed definition of complexity for intermediate and hence interesting cases does not exist.","We fill this gap with a notion of complexity that characterises shapes formed by any finite number of particles greater than or equal to the three needed to define triangle shapes.","The resulting shape complexity is a simple scale-invariant quantity that measures the extent to which a collection of particles has a uniform or clustered distribution.","As a positive-definite number with an absolute minimum realised on the most uniform distribution the particles can have, it not only characterises all physical structures from crystals to the most complex that can exist but also determines for them a measure that makes richly structured shapes more probable than bland ones.","Strikingly, the criterion employed to define the shape complexity forces it to be the product of the two functions that define Newtonian universal gravitation.","This suggests both the form and solutions the law of a universe of such particles should have and leads to a theory that not only determines the complexity and probability of any individual shape but also its creation from the maximally uniform shape.","It does this moreover in a manner which makes it probable that the cosmological principle, according to which on a sufficiently large scale the universe should have the same appearance everywhere, holds.","Our theory relies on universal group-theoretical principles that may allow generalisation to include all forces and general relativity."],"url":"http://arxiv.org/abs/2405.07480v1","category":"gr-qc"}
{"created":"2024-05-13 05:23:48","title":"Integrating Intent Understanding and Optimal Behavior Planning for Behavior Tree Generation from Human Instructions","abstract":"Robots executing tasks following human instructions in domestic or industrial environments essentially require both adaptability and reliability. Behavior Tree (BT) emerges as an appropriate control architecture for these scenarios due to its modularity and reactivity. Existing BT generation methods, however, either do not involve interpreting natural language or cannot theoretically guarantee the BTs' success. This paper proposes a two-stage framework for BT generation, which first employs large language models (LLMs) to interpret goals from high-level instructions, then constructs an efficient goal-specific BT through the Optimal Behavior Tree Expansion Algorithm (OBTEA). We represent goals as well-formed formulas in first-order logic, effectively bridging intent understanding and optimal behavior planning. Experiments in the service robot validate the proficiency of LLMs in producing grammatically correct and accurately interpreted goals, demonstrate OBTEA's superiority over the baseline BT Expansion algorithm in various metrics, and finally confirm the practical deployability of our framework. The project website is https://dids-ei.github.io/Project/LLM-OBTEA/.","sentences":["Robots executing tasks following human instructions in domestic or industrial environments essentially require both adaptability and reliability.","Behavior Tree (BT) emerges as an appropriate control architecture for these scenarios due to its modularity and reactivity.","Existing BT generation methods, however, either do not involve interpreting natural language or cannot theoretically guarantee the BTs' success.","This paper proposes a two-stage framework for BT generation, which first employs large language models (LLMs) to interpret goals from high-level instructions, then constructs an efficient goal-specific BT through the Optimal Behavior Tree Expansion Algorithm (OBTEA).","We represent goals as well-formed formulas in first-order logic, effectively bridging intent understanding and optimal behavior planning.","Experiments in the service robot validate the proficiency of LLMs in producing grammatically correct and accurately interpreted goals, demonstrate OBTEA's superiority over the baseline BT Expansion algorithm in various metrics, and finally confirm the practical deployability of our framework.","The project website is https://dids-ei.github.io/Project/LLM-OBTEA/."],"url":"http://arxiv.org/abs/2405.07474v1","category":"cs.AI"}
{"created":"2024-05-13 05:08:33","title":"Evaluating large language models in medical applications: a survey","abstract":"Large language models (LLMs) have emerged as powerful tools with transformative potential across numerous domains, including healthcare and medicine. In the medical domain, LLMs hold promise for tasks ranging from clinical decision support to patient education. However, evaluating the performance of LLMs in medical contexts presents unique challenges due to the complex and critical nature of medical information. This paper provides a comprehensive overview of the landscape of medical LLM evaluation, synthesizing insights from existing studies and highlighting evaluation data sources, task scenarios, and evaluation methods. Additionally, it identifies key challenges and opportunities in medical LLM evaluation, emphasizing the need for continued research and innovation to ensure the responsible integration of LLMs into clinical practice.","sentences":["Large language models (LLMs) have emerged as powerful tools with transformative potential across numerous domains, including healthcare and medicine.","In the medical domain, LLMs hold promise for tasks ranging from clinical decision support to patient education.","However, evaluating the performance of LLMs in medical contexts presents unique challenges due to the complex and critical nature of medical information.","This paper provides a comprehensive overview of the landscape of medical LLM evaluation, synthesizing insights from existing studies and highlighting evaluation data sources, task scenarios, and evaluation methods.","Additionally, it identifies key challenges and opportunities in medical LLM evaluation, emphasizing the need for continued research and innovation to ensure the responsible integration of LLMs into clinical practice."],"url":"http://arxiv.org/abs/2405.07468v1","category":"cs.CL"}
{"created":"2024-05-13 04:35:14","title":"HoneyBee: A Scalable Modular Framework for Creating Multimodal Oncology Datasets with Foundational Embedding Models","abstract":"Developing accurate machine learning models for oncology requires large-scale, high-quality multimodal datasets. However, creating such datasets remains challenging due to the complexity and heterogeneity of medical data. To address this challenge, we introduce HoneyBee, a scalable modular framework for building multimodal oncology datasets that leverages foundational models to generate representative embeddings. HoneyBee integrates various data modalities, including clinical records, imaging data, and patient outcomes. It employs data preprocessing techniques and transformer-based architectures to generate embeddings that capture the essential features and relationships within the raw medical data. The generated embeddings are stored in a structured format using Hugging Face datasets and PyTorch dataloaders for accessibility. Vector databases enable efficient querying and retrieval for machine learning applications. We demonstrate the effectiveness of HoneyBee through experiments assessing the quality and representativeness of the embeddings. The framework is designed to be extensible to other medical domains and aims to accelerate oncology research by providing high-quality, machine learning-ready datasets. HoneyBee is an ongoing open-source effort, and the code, datasets, and models are available at the project repository.","sentences":["Developing accurate machine learning models for oncology requires large-scale, high-quality multimodal datasets.","However, creating such datasets remains challenging due to the complexity and heterogeneity of medical data.","To address this challenge, we introduce HoneyBee, a scalable modular framework for building multimodal oncology datasets that leverages foundational models to generate representative embeddings.","HoneyBee integrates various data modalities, including clinical records, imaging data, and patient outcomes.","It employs data preprocessing techniques and transformer-based architectures to generate embeddings that capture the essential features and relationships within the raw medical data.","The generated embeddings are stored in a structured format using Hugging Face datasets and PyTorch dataloaders for accessibility.","Vector databases enable efficient querying and retrieval for machine learning applications.","We demonstrate the effectiveness of HoneyBee through experiments assessing the quality and representativeness of the embeddings.","The framework is designed to be extensible to other medical domains and aims to accelerate oncology research by providing high-quality, machine learning-ready datasets.","HoneyBee is an ongoing open-source effort, and the code, datasets, and models are available at the project repository."],"url":"http://arxiv.org/abs/2405.07460v1","category":"cs.LG"}
{"created":"2024-05-13 03:17:44","title":"Evaluation of In vitro anti-inflammatory activity and Insilico pharmacokinetics and molecular docking study of Horsfieldia iryaghedhi","abstract":"Phytochemicals are still a valuable source to develop clinically important drugs in treating chronic and acute diseases. Inflammation is a response to an injurious stimulus of the body and novel therapeutic agents are needed to alleviate the condition with minimum side effects. Matured and fully expanded fresh leaves and barks of H. iryaghedhi were collected, and the extractions were obtained cold maceration using 99.9% methanol and distilled water as solvents. A concentration series was then developed, and the anti-inflammatory activity was evaluated against Diclofenac sodium as the positive control, using the heat-induced egg albumin denaturation method. Further, selected phytochemicals were tested against COX-2 enzyme (PDB ID: 5IKR) using site-specific molecular docking with autodock vina and the binding energies and pharmacokinetic and toxicity parameters were evaluated. Results: The methanol and aqueous extracts have shown a moderate to strong concentration-dependent anti-inflammatory activity with reference to standard Diclofenac sodium and Methanol bark extract exhibited potent anti-inflammatory activity compared to other extracts . Further, Methanol and aqueous extracts showed a statistically significant correlation between concentration and percentage inhibition. The molecular docking results suggest that the phytochemicals available on the plant have possible COX-2 inhibitory activity and the compounds selected (Methyl 2,4- dihydroxy-6-methylbenzoate and N, N-Dimethyl-5-methoxy tryptamine) even got favourable toxicity and pharmacokinetic parameters confirming their drugability.","sentences":["Phytochemicals are still a valuable source to develop clinically important drugs in treating chronic and acute diseases.","Inflammation is a response to an injurious stimulus of the body and novel therapeutic agents are needed to alleviate the condition with minimum side effects.","Matured and fully expanded fresh leaves and barks of H. iryaghedhi were collected, and the extractions were obtained cold maceration using 99.9% methanol and distilled water as solvents.","A concentration series was then developed, and the anti-inflammatory activity was evaluated against Diclofenac sodium as the positive control, using the heat-induced egg albumin denaturation method.","Further, selected phytochemicals were tested against COX-2 enzyme (PDB ID: 5IKR) using site-specific molecular docking with autodock vina and the binding energies and pharmacokinetic and toxicity parameters were evaluated.","Results: The methanol and aqueous extracts have shown a moderate to strong concentration-dependent anti-inflammatory activity with reference to standard Diclofenac sodium and Methanol bark extract exhibited potent anti-inflammatory activity compared to other extracts .","Further, Methanol and aqueous extracts showed a statistically significant correlation between concentration and percentage inhibition.","The molecular docking results suggest that the phytochemicals available on the plant have possible COX-2 inhibitory activity and the compounds selected (Methyl 2,4- dihydroxy-6-methylbenzoate and N, N-Dimethyl-5-methoxy tryptamine) even got favourable toxicity and pharmacokinetic parameters confirming their drugability."],"url":"http://arxiv.org/abs/2405.07449v1","category":"q-bio.BM"}
{"created":"2024-05-13 03:00:28","title":"Rene: A Pre-trained Multi-modal Architecture for Auscultation of Respiratory Diseases","abstract":"This study presents a novel methodology utilizing a pre-trained speech recognition model for processing respiratory sound data. By incorporating medical record information, we introduce an innovative multi-modal deep-learning architecture, named Rene, which addresses the challenges of poor interpretability and underperformance in real-time clinical diagnostic response observed in previous respiratory disease-focused models. The proposed Rene architecture demonstrated significant improvements of 10.24%, 16.15%, 15.29%, and 18.90% respectively, compared to the baseline across four tasks related to respiratory event detection and audio record classification on the SPRSound database. In patient disease prediction tests on the ICBHI database, the architecture exhibited improvements of 23% in the mean of average score and harmonic score compared to the baseline. Furthermore, we developed a real-time respiratory sound discrimination system based on the Rene architecture, featuring a dual-thread design and compressed model parameters for simultaneous microphone recording and real-time dynamic decoding. Employing state-of-the-art Edge AI technology, this system enables rapid and accurate responses for respiratory sound auscultation, facilitating deployment on wearable clinical detection devices to capture incremental data, which can be synergistically evolved with large-scale models deployed on cloud servers for downstream tasks.","sentences":["This study presents a novel methodology utilizing a pre-trained speech recognition model for processing respiratory sound data.","By incorporating medical record information, we introduce an innovative multi-modal deep-learning architecture, named Rene, which addresses the challenges of poor interpretability and underperformance in real-time clinical diagnostic response observed in previous respiratory disease-focused models.","The proposed Rene architecture demonstrated significant improvements of 10.24%, 16.15%, 15.29%, and 18.90% respectively, compared to the baseline across four tasks related to respiratory event detection and audio record classification on the SPRSound database.","In patient disease prediction tests on the ICBHI database, the architecture exhibited improvements of 23% in the mean of average score and harmonic score compared to the baseline.","Furthermore, we developed a real-time respiratory sound discrimination system based on the Rene architecture, featuring a dual-thread design and compressed model parameters for simultaneous microphone recording and real-time dynamic decoding.","Employing state-of-the-art Edge AI technology, this system enables rapid and accurate responses for respiratory sound auscultation, facilitating deployment on wearable clinical detection devices to capture incremental data, which can be synergistically evolved with large-scale models deployed on cloud servers for downstream tasks."],"url":"http://arxiv.org/abs/2405.07442v1","category":"cs.SD"}
{"created":"2024-05-13 02:33:25","title":"Evaluation of Retrieval-Augmented Generation: A Survey","abstract":"Retrieval-Augmented Generation (RAG) has emerged as a pivotal innovation in natural language processing, enhancing generative models by incorporating external information retrieval. Evaluating RAG systems, however, poses distinct challenges due to their hybrid structure and reliance on dynamic knowledge sources. We consequently enhanced an extensive survey and proposed an analysis framework for benchmarks of RAG systems, RAGR (Retrieval, Generation, Additional Requirement), designed to systematically analyze RAG benchmarks by focusing on measurable outputs and established truths. Specifically, we scrutinize and contrast multiple quantifiable metrics of the Retrieval and Generation component, such as relevance, accuracy, and faithfulness, of the internal links within the current RAG evaluation methods, covering the possible output and ground truth pairs. We also analyze the integration of additional requirements of different works, discuss the limitations of current benchmarks, and propose potential directions for further research to address these shortcomings and advance the field of RAG evaluation. In conclusion, this paper collates the challenges associated with RAG evaluation. It presents a thorough analysis and examination of existing methodologies for RAG benchmark design based on the proposed RGAR framework.","sentences":["Retrieval-Augmented Generation (RAG) has emerged as a pivotal innovation in natural language processing, enhancing generative models by incorporating external information retrieval.","Evaluating RAG systems, however, poses distinct challenges due to their hybrid structure and reliance on dynamic knowledge sources.","We consequently enhanced an extensive survey and proposed an analysis framework for benchmarks of RAG systems, RAGR (Retrieval, Generation, Additional Requirement), designed to systematically analyze RAG benchmarks by focusing on measurable outputs and established truths.","Specifically, we scrutinize and contrast multiple quantifiable metrics of the Retrieval and Generation component, such as relevance, accuracy, and faithfulness, of the internal links within the current RAG evaluation methods, covering the possible output and ground truth pairs.","We also analyze the integration of additional requirements of different works, discuss the limitations of current benchmarks, and propose potential directions for further research to address these shortcomings and advance the field of RAG evaluation.","In conclusion, this paper collates the challenges associated with RAG evaluation.","It presents a thorough analysis and examination of existing methodologies for RAG benchmark design based on the proposed RGAR framework."],"url":"http://arxiv.org/abs/2405.07437v1","category":"cs.CL"}
{"created":"2024-05-13 02:31:08","title":"Can Language Models Explain Their Own Classification Behavior?","abstract":"Large language models (LLMs) perform well at a myriad of tasks, but explaining the processes behind this performance is a challenge. This paper investigates whether LLMs can give faithful high-level explanations of their own internal processes. To explore this, we introduce a dataset, ArticulateRules, of few-shot text-based classification tasks generated by simple rules. Each rule is associated with a simple natural-language explanation. We test whether models that have learned to classify inputs competently (both in- and out-of-distribution) are able to articulate freeform natural language explanations that match their classification behavior. Our dataset can be used for both in-context and finetuning evaluations. We evaluate a range of LLMs, demonstrating that articulation accuracy varies considerably between models, with a particularly sharp increase from GPT-3 to GPT-4. We then investigate whether we can improve GPT-3's articulation accuracy through a range of methods. GPT-3 completely fails to articulate 7/10 rules in our test, even after additional finetuning on correct explanations. We release our dataset, ArticulateRules, which can be used to test self-explanation for LLMs trained either in-context or by finetuning.","sentences":["Large language models (LLMs) perform well at a myriad of tasks, but explaining the processes behind this performance is a challenge.","This paper investigates whether LLMs can give faithful high-level explanations of their own internal processes.","To explore this, we introduce a dataset, ArticulateRules, of few-shot text-based classification tasks generated by simple rules.","Each rule is associated with a simple natural-language explanation.","We test whether models that have learned to classify inputs competently (both in- and out-of-distribution) are able to articulate freeform natural language explanations that match their classification behavior.","Our dataset can be used for both in-context and finetuning evaluations.","We evaluate a range of LLMs, demonstrating that articulation accuracy varies considerably between models, with a particularly sharp increase from GPT-3 to GPT-4.","We then investigate whether we can improve GPT-3's articulation accuracy through a range of methods.","GPT-3 completely fails to articulate 7/10 rules in our test, even after additional finetuning on correct explanations.","We release our dataset, ArticulateRules, which can be used to test self-explanation for LLMs trained either in-context or by finetuning."],"url":"http://arxiv.org/abs/2405.07436v1","category":"cs.LG"}
{"created":"2024-05-13 01:23:14","title":"Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains","abstract":"The ability of deep networks to learn superior representations hinges on leveraging the proper inductive biases, considering the inherent properties of datasets. In tabular domains, it is critical to effectively handle heterogeneous features (both categorical and numerical) in a unified manner and to grasp irregular functions like piecewise constant functions. To address the challenges in the self-supervised learning framework, we propose a novel pretext task based on the classical binning method. The idea is straightforward: reconstructing the bin indices (either orders or classes) rather than the original values. This pretext task provides the encoder with an inductive bias to capture the irregular dependencies, mapping from continuous inputs to discretized bins, and mitigates the feature heterogeneity by setting all features to have category-type targets. Our empirical investigations ascertain several advantages of binning: capturing the irregular function, compatibility with encoder architecture and additional modifications, standardizing all features into equal sets, grouping similar values within a feature, and providing ordering information. Comprehensive evaluations across diverse tabular datasets corroborate that our method consistently improves tabular representation learning performance for a wide range of downstream tasks. The codes are available in https://github.com/kyungeun-lee/tabularbinning.","sentences":["The ability of deep networks to learn superior representations hinges on leveraging the proper inductive biases, considering the inherent properties of datasets.","In tabular domains, it is critical to effectively handle heterogeneous features (both categorical and numerical) in a unified manner and to grasp irregular functions like piecewise constant functions.","To address the challenges in the self-supervised learning framework, we propose a novel pretext task based on the classical binning method.","The idea is straightforward: reconstructing the bin indices (either orders or classes) rather than the original values.","This pretext task provides the encoder with an inductive bias to capture the irregular dependencies, mapping from continuous inputs to discretized bins, and mitigates the feature heterogeneity by setting all features to have category-type targets.","Our empirical investigations ascertain several advantages of binning: capturing the irregular function, compatibility with encoder architecture and additional modifications, standardizing all features into equal sets, grouping similar values within a feature, and providing ordering information.","Comprehensive evaluations across diverse tabular datasets corroborate that our method consistently improves tabular representation learning performance for a wide range of downstream tasks.","The codes are available in https://github.com/kyungeun-lee/tabularbinning."],"url":"http://arxiv.org/abs/2405.07414v2","category":"cs.LG"}
{"created":"2024-05-13 01:18:25","title":"MoVL:Exploring Fusion Strategies for the Domain-Adaptive Application of Pretrained Models in Medical Imaging Tasks","abstract":"Medical images are often more difficult to acquire than natural images due to the specialism of the equipment and technology, which leads to less medical image datasets. So it is hard to train a strong pretrained medical vision model. How to make the best of natural pretrained vision model and adapt in medical domain still pends. For image classification, a popular method is linear probe (LP). However, LP only considers the output after feature extraction. Yet, there exists a gap between input medical images and natural pretrained vision model. We introduce visual prompting (VP) to fill in the gap, and analyze the strategies of coupling between LP and VP. We design a joint learning loss function containing categorisation loss and discrepancy loss, which describe the variance of prompted and plain images, naming this joint training strategy MoVL (Mixture of Visual Prompting and Linear Probe). We experiment on 4 medical image classification datasets, with two mainstream architectures, ResNet and CLIP. Results shows that without changing the parameters and architecture of backbone model and with less parameters, there is potential for MoVL to achieve full finetune (FF) accuracy (on four medical datasets, average 90.91% for MoVL and 91.13% for FF). On out of distribution medical dataset, our method(90.33%) can outperform FF (85.15%) with absolute 5.18 % lead.","sentences":["Medical images are often more difficult to acquire than natural images due to the specialism of the equipment and technology, which leads to less medical image datasets.","So it is hard to train a strong pretrained medical vision model.","How to make the best of natural pretrained vision model and adapt in medical domain still pends.","For image classification, a popular method is linear probe (LP).","However, LP only considers the output after feature extraction.","Yet, there exists a gap between input medical images and natural pretrained vision model.","We introduce visual prompting (VP) to fill in the gap, and analyze the strategies of coupling between LP and VP.","We design a joint learning loss function containing categorisation loss and discrepancy loss, which describe the variance of prompted and plain images, naming this joint training strategy MoVL (Mixture of Visual Prompting and Linear Probe).","We experiment on 4 medical image classification datasets, with two mainstream architectures, ResNet and CLIP.","Results shows that without changing the parameters and architecture of backbone model and with less parameters, there is potential for MoVL to achieve full finetune (FF) accuracy (on four medical datasets, average 90.91% for MoVL and 91.13% for FF).","On out of distribution medical dataset, our method(90.33%) can outperform FF (85.15%) with absolute 5.18 % lead."],"url":"http://arxiv.org/abs/2405.07411v1","category":"cs.CV"}
{"created":"2024-05-13 01:03:06","title":"PitcherNet: Powering the Moneyball Evolution in Baseball Video Analytics","abstract":"In the high-stakes world of baseball, every nuance of a pitcher's mechanics holds the key to maximizing performance and minimizing runs. Traditional analysis methods often rely on pre-recorded offline numerical data, hindering their application in the dynamic environment of live games. Broadcast video analysis, while seemingly ideal, faces significant challenges due to factors like motion blur and low resolution. To address these challenges, we introduce PitcherNet, an end-to-end automated system that analyzes pitcher kinematics directly from live broadcast video, thereby extracting valuable pitch statistics including velocity, release point, pitch position, and release extension. This system leverages three key components: (1) Player tracking and identification by decoupling actions from player kinematics; (2) Distribution and depth-aware 3D human modeling; and (3) Kinematic-driven pitch statistics. Experimental validation demonstrates that PitcherNet achieves robust analysis results with 96.82% accuracy in pitcher tracklet identification, reduced joint position error by 1.8mm and superior analytics compared to baseline methods. By enabling performance-critical kinematic analysis from broadcast video, PitcherNet paves the way for the future of baseball analytics by optimizing pitching strategies, preventing injuries, and unlocking a deeper understanding of pitcher mechanics, forever transforming the game.","sentences":["In the high-stakes world of baseball, every nuance of a pitcher's mechanics holds the key to maximizing performance and minimizing runs.","Traditional analysis methods often rely on pre-recorded offline numerical data, hindering their application in the dynamic environment of live games.","Broadcast video analysis, while seemingly ideal, faces significant challenges due to factors like motion blur and low resolution.","To address these challenges, we introduce PitcherNet, an end-to-end automated system that analyzes pitcher kinematics directly from live broadcast video, thereby extracting valuable pitch statistics including velocity, release point, pitch position, and release extension.","This system leverages three key components: (1) Player tracking and identification by decoupling actions from player kinematics; (2) Distribution and depth-aware 3D human modeling; and (3) Kinematic-driven pitch statistics.","Experimental validation demonstrates that PitcherNet achieves robust analysis results with 96.82% accuracy in pitcher tracklet identification, reduced joint position error by 1.8mm and superior analytics compared to baseline methods.","By enabling performance-critical kinematic analysis from broadcast video, PitcherNet paves the way for the future of baseball analytics by optimizing pitching strategies, preventing injuries, and unlocking a deeper understanding of pitcher mechanics, forever transforming the game."],"url":"http://arxiv.org/abs/2405.07407v1","category":"cs.CV"}
{"created":"2024-05-13 00:58:34","title":"Machine Unlearning: A Comprehensive Survey","abstract":"As the right to be forgotten has been legislated worldwide, many studies attempt to design unlearning mechanisms to protect users' privacy when they want to leave machine learning service platforms. Specifically, machine unlearning is to make a trained model to remove the contribution of an erased subset of the training dataset. This survey aims to systematically classify a wide range of machine unlearning and discuss their differences, connections and open problems. We categorize current unlearning methods into four scenarios: centralized unlearning, distributed and irregular data unlearning, unlearning verification, and privacy and security issues in unlearning. Since centralized unlearning is the primary domain, we use two parts to introduce: firstly, we classify centralized unlearning into exact unlearning and approximate unlearning; secondly, we offer a detailed introduction to the techniques of these methods. Besides the centralized unlearning, we notice some studies about distributed and irregular data unlearning and introduce federated unlearning and graph unlearning as the two representative directions. After introducing unlearning methods, we review studies about unlearning verification. Moreover, we consider the privacy and security issues essential in machine unlearning and organize the latest related literature. Finally, we discuss the challenges of various unlearning scenarios and address the potential research directions.","sentences":["As the right to be forgotten has been legislated worldwide, many studies attempt to design unlearning mechanisms to protect users' privacy when they want to leave machine learning service platforms.","Specifically, machine unlearning is to make a trained model to remove the contribution of an erased subset of the training dataset.","This survey aims to systematically classify a wide range of machine unlearning and discuss their differences, connections and open problems.","We categorize current unlearning methods into four scenarios: centralized unlearning, distributed and irregular data unlearning, unlearning verification, and privacy and security issues in unlearning.","Since centralized unlearning is the primary domain, we use two parts to introduce: firstly, we classify centralized unlearning into exact unlearning and approximate unlearning; secondly, we offer a detailed introduction to the techniques of these methods.","Besides the centralized unlearning, we notice some studies about distributed and irregular data unlearning and introduce federated unlearning and graph unlearning as the two representative directions.","After introducing unlearning methods, we review studies about unlearning verification.","Moreover, we consider the privacy and security issues essential in machine unlearning and organize the latest related literature.","Finally, we discuss the challenges of various unlearning scenarios and address the potential research directions."],"url":"http://arxiv.org/abs/2405.07406v1","category":"cs.CR"}
{"created":"2024-05-13 00:51:36","title":"Indoor PM2.5 forecasting and the association with outdoor air pollution: a modelling study based on sensor data in Australia","abstract":"Exposure to poor indoor air quality poses significant health risks, necessitating thorough assessment to mitigate associated dangers. This study aims to predict hourly indoor fine particulate matter (PM2.5) concentrations and investigate their correlation with outdoor PM2.5 levels across 24 distinct buildings in Australia. Indoor air quality data were gathered from 91 monitoring sensors in eight Australian cities spanning 2019 to 2022. Employing an innovative three-stage deep ensemble machine learning framework (DEML), comprising three base models (Support Vector Machine, Random Forest, and eXtreme Gradient Boosting) and two meta-models (Random Forest and Generalized Linear Model), hourly indoor PM2.5 concentrations were predicted. The model's accuracy was evaluated using a rolling windows approach, comparing its performance against three benchmark algorithms (SVM, RF, and XGBoost). Additionally, a correlation analysis assessed the relationship between indoor and outdoor PM2.5 concentrations. Results indicate that the DEML model consistently outperformed benchmark models, achieving an R2 ranging from 0.63 to 0.99 and RMSE from 0.01 to 0.663 mg/m3 for most sensors. Notably, outdoor PM2.5 concentrations significantly impacted indoor air quality, particularly evident during events like bushfires. This study underscores the importance of accurate indoor air quality prediction, crucial for developing location-specific early warning systems and informing effective interventions. By promoting protective behaviors, these efforts contribute to enhanced public health outcomes.","sentences":["Exposure to poor indoor air quality poses significant health risks, necessitating thorough assessment to mitigate associated dangers.","This study aims to predict hourly indoor fine particulate matter (PM2.5) concentrations and investigate their correlation with outdoor PM2.5 levels across 24 distinct buildings in Australia.","Indoor air quality data were gathered from 91 monitoring sensors in eight Australian cities spanning 2019 to 2022.","Employing an innovative three-stage deep ensemble machine learning framework (DEML), comprising three base models (Support Vector Machine, Random Forest, and eXtreme Gradient Boosting) and two meta-models (Random Forest and Generalized Linear Model), hourly indoor PM2.5 concentrations were predicted.","The model's accuracy was evaluated using a rolling windows approach, comparing its performance against three benchmark algorithms (SVM, RF, and XGBoost).","Additionally, a correlation analysis assessed the relationship between indoor and outdoor PM2.5 concentrations.","Results indicate that the DEML model consistently outperformed benchmark models, achieving an R2 ranging from 0.63 to 0.99 and RMSE from 0.01 to 0.663 mg/m3 for most sensors.","Notably, outdoor PM2.5 concentrations significantly impacted indoor air quality, particularly evident during events like bushfires.","This study underscores the importance of accurate indoor air quality prediction, crucial for developing location-specific early warning systems and informing effective interventions.","By promoting protective behaviors, these efforts contribute to enhanced public health outcomes."],"url":"http://arxiv.org/abs/2405.07404v1","category":"cs.LG"}
{"created":"2024-05-12 23:18:14","title":"CaFA: Global Weather Forecasting with Factorized Attention on Sphere","abstract":"Accurate weather forecasting is crucial in various sectors, impacting decision-making processes and societal events. Data-driven approaches based on machine learning models have recently emerged as a promising alternative to numerical weather prediction models given their potential to capture physics of different scales from historical data and the significantly lower computational cost during the prediction stage. Renowned for its state-of-the-art performance across diverse domains, the Transformer model has also gained popularity in machine learning weather prediction. Yet applying Transformer architectures to weather forecasting, particularly on a global scale is computationally challenging due to the quadratic complexity of attention and the quadratic increase in spatial points as resolution increases. In this work, we propose a factorized-attention-based model tailored for spherical geometries to mitigate this issue. More specifically, it utilizes multi-dimensional factorized kernels that convolve over different axes where the computational complexity of the kernel is only quadratic to the axial resolution instead of overall resolution. The deterministic forecasting accuracy of the proposed model on $1.5^\\circ$ and 0-7 days' lead time is on par with state-of-the-art purely data-driven machine learning weather prediction models. We also showcase the proposed model holds great potential to push forward the Pareto front of accuracy-efficiency for Transformer weather models, where it can achieve better accuracy with less computational cost compared to Transformer based models with standard attention.","sentences":["Accurate weather forecasting is crucial in various sectors, impacting decision-making processes and societal events.","Data-driven approaches based on machine learning models have recently emerged as a promising alternative to numerical weather prediction models given their potential to capture physics of different scales from historical data and the significantly lower computational cost during the prediction stage.","Renowned for its state-of-the-art performance across diverse domains, the Transformer model has also gained popularity in machine learning weather prediction.","Yet applying Transformer architectures to weather forecasting, particularly on a global scale is computationally challenging due to the quadratic complexity of attention and the quadratic increase in spatial points as resolution increases.","In this work, we propose a factorized-attention-based model tailored for spherical geometries to mitigate this issue.","More specifically, it utilizes multi-dimensional factorized kernels that convolve over different axes where the computational complexity of the kernel is only quadratic to the axial resolution instead of overall resolution.","The deterministic forecasting accuracy of the proposed model on $1.5^\\circ$ and 0-7 days' lead time is on par with state-of-the-art purely data-driven machine learning weather prediction models.","We also showcase the proposed model holds great potential to push forward the Pareto front of accuracy-efficiency for Transformer weather models, where it can achieve better accuracy with less computational cost compared to Transformer based models with standard attention."],"url":"http://arxiv.org/abs/2405.07395v1","category":"cs.LG"}
{"created":"2024-05-12 22:51:35","title":"AnyRotate: Gravity-Invariant In-Hand Object Rotation with Sim-to-Real Touch","abstract":"In-hand manipulation is an integral component of human dexterity. Our hands rely on tactile feedback for stable and reactive motions to ensure objects do not slip away unintentionally during manipulation. For a robot hand, this level of dexterity requires extracting and utilizing rich contact information for precise motor control. In this paper, we present AnyRotate, a system for gravity-invariant multi-axis in-hand object rotation using dense featured sim-to-real touch. We construct a continuous contact feature representation to provide tactile feedback for training a policy in simulation and introduce an approach to perform zero-shot policy transfer by training an observation model to bridge the sim-to-real gap. Our experiments highlight the benefit of detailed contact information when handling objects with varying properties. In the real world, we demonstrate successful sim-to-real transfer of the dense tactile policy, generalizing to a diverse range of objects for various rotation axes and hand directions and outperforming other forms of low-dimensional touch. Interestingly, despite not having explicit slip detection, rich multi-fingered tactile sensing can implicitly detect object movement within grasp and provide a reactive behavior that improves the robustness of the policy, highlighting the importance of information-rich tactile sensing for in-hand manipulation.","sentences":["In-hand manipulation is an integral component of human dexterity.","Our hands rely on tactile feedback for stable and reactive motions to ensure objects do not slip away unintentionally during manipulation.","For a robot hand, this level of dexterity requires extracting and utilizing rich contact information for precise motor control.","In this paper, we present AnyRotate, a system for gravity-invariant multi-axis in-hand object rotation using dense featured sim-to-real touch.","We construct a continuous contact feature representation to provide tactile feedback for training a policy in simulation and introduce an approach to perform zero-shot policy transfer by training an observation model to bridge the sim-to-real gap.","Our experiments highlight the benefit of detailed contact information when handling objects with varying properties.","In the real world, we demonstrate successful sim-to-real transfer of the dense tactile policy, generalizing to a diverse range of objects for various rotation axes and hand directions and outperforming other forms of low-dimensional touch.","Interestingly, despite not having explicit slip detection, rich multi-fingered tactile sensing can implicitly detect object movement within grasp and provide a reactive behavior that improves the robustness of the policy, highlighting the importance of information-rich tactile sensing for in-hand manipulation."],"url":"http://arxiv.org/abs/2405.07391v1","category":"cs.RO"}
{"created":"2024-05-12 22:18:25","title":"Semantic Loss Functions for Neuro-Symbolic Structured Prediction","abstract":"Structured output prediction problems are ubiquitous in machine learning. The prominent approach leverages neural networks as powerful feature extractors, otherwise assuming the independence of the outputs. These outputs, however, jointly encode an object, e.g. a path in a graph, and are therefore related through the structure underlying the output space. We discuss the semantic loss, which injects knowledge about such structure, defined symbolically, into training by minimizing the network's violation of such dependencies, steering the network towards predicting distributions satisfying the underlying structure. At the same time, it is agnostic to the arrangement of the symbols, and depends only on the semantics expressed thereby, while also enabling efficient end-to-end training and inference. We also discuss key improvements and applications of the semantic loss. One limitations of the semantic loss is that it does not exploit the association of every data point with certain features certifying its membership in a target class. We should therefore prefer minimum-entropy distributions over valid structures, which we obtain by additionally minimizing the neuro-symbolic entropy. We empirically demonstrate the benefits of this more refined formulation. Moreover, the semantic loss is designed to be modular and can be combined with both discriminative and generative neural models. This is illustrated by integrating it into generative adversarial networks, yielding constrained adversarial networks, a novel class of deep generative models able to efficiently synthesize complex objects obeying the structure of the underlying domain.","sentences":["Structured output prediction problems are ubiquitous in machine learning.","The prominent approach leverages neural networks as powerful feature extractors, otherwise assuming the independence of the outputs.","These outputs, however, jointly encode an object, e.g. a path in a graph, and are therefore related through the structure underlying the output space.","We discuss the semantic loss, which injects knowledge about such structure, defined symbolically, into training by minimizing the network's violation of such dependencies, steering the network towards predicting distributions satisfying the underlying structure.","At the same time, it is agnostic to the arrangement of the symbols, and depends only on the semantics expressed thereby, while also enabling efficient end-to-end training and inference.","We also discuss key improvements and applications of the semantic loss.","One limitations of the semantic loss is that it does not exploit the association of every data point with certain features certifying its membership in a target class.","We should therefore prefer minimum-entropy distributions over valid structures, which we obtain by additionally minimizing the neuro-symbolic entropy.","We empirically demonstrate the benefits of this more refined formulation.","Moreover, the semantic loss is designed to be modular and can be combined with both discriminative and generative neural models.","This is illustrated by integrating it into generative adversarial networks, yielding constrained adversarial networks, a novel class of deep generative models able to efficiently synthesize complex objects obeying the structure of the underlying domain."],"url":"http://arxiv.org/abs/2405.07387v1","category":"cs.LG"}
{"created":"2024-05-12 20:27:34","title":"Conformalized Survival Distributions: A Generic Post-Process to Increase Calibration","abstract":"Discrimination and calibration represent two important properties of survival analysis, with the former assessing the model's ability to accurately rank subjects and the latter evaluating the alignment of predicted outcomes with actual events. With their distinct nature, it is hard for survival models to simultaneously optimize both of them especially as many previous results found improving calibration tends to diminish discrimination performance. This paper introduces a novel approach utilizing conformal regression that can improve a model's calibration without degrading discrimination. We provide theoretical guarantees for the above claim, and rigorously validate the efficiency of our approach across 11 real-world datasets, showcasing its practical applicability and robustness in diverse scenarios.","sentences":["Discrimination and calibration represent two important properties of survival analysis, with the former assessing the model's ability to accurately rank subjects and the latter evaluating the alignment of predicted outcomes with actual events.","With their distinct nature, it is hard for survival models to simultaneously optimize both of them especially as many previous results found improving calibration tends to diminish discrimination performance.","This paper introduces a novel approach utilizing conformal regression that can improve a model's calibration without degrading discrimination.","We provide theoretical guarantees for the above claim, and rigorously validate the efficiency of our approach across 11 real-world datasets, showcasing its practical applicability and robustness in diverse scenarios."],"url":"http://arxiv.org/abs/2405.07374v1","category":"cs.LG"}
{"created":"2024-05-12 20:25:36","title":"Probabilistic and Causal Satisfiability: the Impact of Marginalization","abstract":"The framework of Pearl's Causal Hierarchy (PCH) formalizes three types of reasoning: observational, interventional, and counterfactual, that reflect the progressive sophistication of human thought regarding causation. We investigate the computational complexity aspects of reasoning in this framework focusing mainly on satisfiability problems expressed in probabilistic and causal languages across the PCH. That is, given a system of formulas in the standard probabilistic and causal languages, does there exist a model satisfying the formulas? The resulting complexity changes depending on the level of the hierarchy as well as the operators allowed in the formulas (addition, multiplication, or marginalization).   We focus on formulas involving marginalization that are widely used in probabilistic and causal inference, but whose complexity issues are still little explored. Our main contribution are the exact computational complexity results showing that linear languages (allowing addition and marginalization) yield NP^PP-, PSPACE-, and NEXP-complete satisfiability problems, depending on the level of the PCH. Moreover, we prove that the problem for the full language (allowing additionally multiplication) is complete for the class succ$\\exists$R for languages on the highest, counterfactual level. Previous work has shown that the satisfiability problem is complete for succ$\\exists$R on the lower levels leaving the counterfactual case open. Finally, we consider constrained models that are restricted to a small polynomial size. The constraint on the size reduces the complexity of the interventional and counterfactual languages to NEXP-complete.","sentences":["The framework of Pearl's Causal Hierarchy (PCH) formalizes three types of reasoning: observational, interventional, and counterfactual, that reflect the progressive sophistication of human thought regarding causation.","We investigate the computational complexity aspects of reasoning in this framework focusing mainly on satisfiability problems expressed in probabilistic and causal languages across the PCH.","That is, given a system of formulas in the standard probabilistic and causal languages, does there exist a model satisfying the formulas?","The resulting complexity changes depending on the level of the hierarchy as well as the operators allowed in the formulas (addition, multiplication, or marginalization).   ","We focus on formulas involving marginalization that are widely used in probabilistic and causal inference, but whose complexity issues are still little explored.","Our main contribution are the exact computational complexity results showing that linear languages (allowing addition and marginalization) yield NP^PP-, PSPACE-, and NEXP-complete satisfiability problems, depending on the level of the PCH.","Moreover, we prove that the problem for the full language (allowing additionally multiplication) is complete for the class succ$\\exists$R for languages on the highest, counterfactual level.","Previous work has shown that the satisfiability problem is complete for succ$\\exists$R on the lower levels leaving the counterfactual case open.","Finally, we consider constrained models that are restricted to a small polynomial size.","The constraint on the size reduces the complexity of the interventional and counterfactual languages to NEXP-complete."],"url":"http://arxiv.org/abs/2405.07373v1","category":"cs.AI"}
{"created":"2024-05-12 20:02:25","title":"Incorporating Anatomical Awareness for Enhanced Generalizability and Progression Prediction in Deep Learning-Based Radiographic Sacroiliitis Detection","abstract":"Purpose: To examine whether incorporating anatomical awareness into a deep learning model can improve generalizability and enable prediction of disease progression.   Methods: This retrospective multicenter study included conventional pelvic radiographs of 4 different patient cohorts focusing on axial spondyloarthritis (axSpA) collected at university and community hospitals. The first cohort, which consisted of 1483 radiographs, was split into training (n=1261) and validation (n=222) sets. The other cohorts comprising 436, 340, and 163 patients, respectively, were used as independent test datasets. For the second cohort, follow-up data of 311 patients was used to examine progression prediction capabilities. Two neural networks were trained, one on images cropped to the bounding box of the sacroiliac joints (anatomy-aware) and the other one on full radiographs. The performance of the models was compared using the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, and specificity.   Results: On the three test datasets, the standard model achieved AUC scores of 0.853, 0.817, 0.947, with an accuracy of 0.770, 0.724, 0.850. Whereas the anatomy-aware model achieved AUC scores of 0.899, 0.846, 0.957, with an accuracy of 0.821, 0.744, 0.906, respectively. The patients who were identified as high risk by the anatomy aware model had an odds ratio of 2.16 (95% CI: 1.19, 3.86) for having progression of radiographic sacroiliitis within 2 years.   Conclusion: Anatomical awareness can improve the generalizability of a deep learning model in detecting radiographic sacroiliitis. The model is published as fully open source alongside this study.","sentences":["Purpose: To examine whether incorporating anatomical awareness into a deep learning model can improve generalizability and enable prediction of disease progression.   ","Methods: This retrospective multicenter study included conventional pelvic radiographs of 4 different patient cohorts focusing on axial spondyloarthritis (axSpA) collected at university and community hospitals.","The first cohort, which consisted of 1483 radiographs, was split into training (n=1261) and validation (n=222) sets.","The other cohorts comprising 436, 340, and 163 patients, respectively, were used as independent test datasets.","For the second cohort, follow-up data of 311 patients was used to examine progression prediction capabilities.","Two neural networks were trained, one on images cropped to the bounding box of the sacroiliac joints (anatomy-aware) and the other one on full radiographs.","The performance of the models was compared using the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, and specificity.   ","Results: On the three test datasets, the standard model achieved AUC scores of 0.853, 0.817, 0.947, with an accuracy of 0.770, 0.724, 0.850.","Whereas the anatomy-aware model achieved AUC scores of 0.899, 0.846, 0.957, with an accuracy of 0.821, 0.744, 0.906, respectively.","The patients who were identified as high risk by the anatomy aware model had an odds ratio of 2.16 (95% CI: 1.19, 3.86) for having progression of radiographic sacroiliitis within 2 years.   ","Conclusion: Anatomical awareness can improve the generalizability of a deep learning model in detecting radiographic sacroiliitis.","The model is published as fully open source alongside this study."],"url":"http://arxiv.org/abs/2405.07369v1","category":"cs.CV"}
{"created":"2024-05-12 19:41:58","title":"Multidegrees of binomial edge ideals","abstract":"Let $G$ be a simple graph with binomial edge ideal $J_G$. We prove how to calculate the multidegree of $J_G$ based on combinatorial properties of $G$. In particular, we study the set $S_{\\min}(G)$ defined as the collection of subsets of vertices whose prime ideals have minimum codimension. We provide results which assist in determining $S_{\\min}(G)$, then calculate $S_{\\min}(G)$ for star, horned complete, barbell, cycle, wheel, and friendship graphs, and use the main result of the paper to obtain the multidegrees of their binomial edge ideals.","sentences":["Let $G$ be a simple graph with binomial edge ideal $J_G$. We prove how to calculate the multidegree of $J_G$ based on combinatorial properties of $G$. In particular, we study the set $S_{\\min}(G)$ defined as the collection of subsets of vertices whose prime ideals have minimum codimension.","We provide results which assist in determining $S_{\\min}(G)$, then calculate $S_{\\min}(G)$ for star, horned complete, barbell, cycle, wheel, and friendship graphs, and use the main result of the paper to obtain the multidegrees of their binomial edge ideals."],"url":"http://arxiv.org/abs/2405.07365v1","category":"math.AC"}
{"created":"2024-05-12 18:46:28","title":"Structure growth in $f(Q)$ cosmology","abstract":"We take into account redshift-space distortion measurements to investigate the growth of cosmological large-scale structures within the framework of modified symmetric teleparallel $f(Q)$ gravity. After comparing the predictions of the $f(Q)$-gravity expansion history with OHD and SNIa datasets and constraining the pertinent cosmological parameters $\\Omega_{m}$ and $H_0$, together with the exponent $n$ for $f(Q)$ power-law models, we derive the full system of equations governing linear cosmological perturbations to study matter fluctuations using the $1 + 3$ covariant formalism when applied to $f(Q)$ gravity. Thus, we resort to both the usual redshift-space distortion data $f\\sigma_8$ and some recent separate measurements of the growth rate $f$ and the amplitude of matter fluctuations $\\sigma_8$ from the VIPERS and SDSS collaborations to find the best-fit cosmological parameters $\\Omega_m$, $\\sigma_{8}$ and $n$. We also apply a collective analysis of such growth-structure data together with the aforementioned cosmic expansion measurements, to restrict these parameters through Monte Carlo Markov Chain simulations. determining the statistical significance for the best-fit parameter values through the AIC and BIC Bayesian selection criteria.","sentences":["We take into account redshift-space distortion measurements to investigate the growth of cosmological large-scale structures within the framework of modified symmetric teleparallel $f(Q)$ gravity.","After comparing the predictions of the $f(Q)$-gravity expansion history with OHD and SNIa datasets and constraining the pertinent cosmological parameters $\\Omega_{m}$ and $H_0$, together with the exponent $n$ for $f(Q)$ power-law models, we derive the full system of equations governing linear cosmological perturbations to study matter fluctuations using the $1 + 3$ covariant formalism when applied to $f(Q)$ gravity.","Thus, we resort to both the usual redshift-space distortion data $f\\sigma_8$ and some recent separate measurements of the growth rate $f$ and the amplitude of matter fluctuations $\\sigma_8$ from the VIPERS and SDSS collaborations to find the best-fit cosmological parameters $\\Omega_m$, $\\sigma_{8}$ and $n$. We also apply a collective analysis of such growth-structure data together with the aforementioned cosmic expansion measurements, to restrict these parameters through Monte Carlo Markov Chain simulations.","determining the statistical significance for the best-fit parameter values through the AIC and BIC Bayesian selection criteria."],"url":"http://arxiv.org/abs/2405.07361v1","category":"gr-qc"}
{"created":"2024-05-12 18:17:14","title":"Orientational dynamics governs the pathways of entropic crystallization of Brownian squares","abstract":"In dense systems of hard-interacting colloidal particles with anisotropic shapes, crystallization pathways represent an interesting frontier since directional entropic forces often cause fascinating variations in the equilibrium crystal structures. At increasing densities, when the orientational excluded volumes of anisotropic particles start overlapping, their translational and rotational dynamics become coupled, introducing complexities in the kinetics of their configurational reorganization, which facilitates ordering. To elucidate this, we have studied a two-dimensional system of osmotically compressed corner-rounded Brownian square platelets, which are known to equilibrate into hexagonal and rhombic crystalline phases as the osmotic pressure is increased. By analyzing the translational and orientational dynamics of the particles and calculating their corresponding contributions to minimize the free energy, we have shown that the accessible range of orientational diffusion of particles governs the pathways of structural evolution and consequent optimal equilibrium ordering at a given osmotic pressure. As the accessible orientational excursion becomes wider, the rotational contribution to configurational entropy minimizes the total free energy, leading to hexagonal ordering. At higher osmotic pressures, the long collective translational fluctuations of side-aligned particles with restricted rotational diffusion maximize entropy, thereby inducing a free energetically favored rhombic crystalline structure. Intriguingly, the density, which solely governs the crystallization of hard spheres, does not have any direct effect on this process. Complementary Brownian dynamics simulations further corroborate these experimental observations and interpretations. Our findings are also relevant to other systems of hard interacting anisotropic shapes in two and three dimensions.","sentences":["In dense systems of hard-interacting colloidal particles with anisotropic shapes, crystallization pathways represent an interesting frontier since directional entropic forces often cause fascinating variations in the equilibrium crystal structures.","At increasing densities, when the orientational excluded volumes of anisotropic particles start overlapping, their translational and rotational dynamics become coupled, introducing complexities in the kinetics of their configurational reorganization, which facilitates ordering.","To elucidate this, we have studied a two-dimensional system of osmotically compressed corner-rounded Brownian square platelets, which are known to equilibrate into hexagonal and rhombic crystalline phases as the osmotic pressure is increased.","By analyzing the translational and orientational dynamics of the particles and calculating their corresponding contributions to minimize the free energy, we have shown that the accessible range of orientational diffusion of particles governs the pathways of structural evolution and consequent optimal equilibrium ordering at a given osmotic pressure.","As the accessible orientational excursion becomes wider, the rotational contribution to configurational entropy minimizes the total free energy, leading to hexagonal ordering.","At higher osmotic pressures, the long collective translational fluctuations of side-aligned particles with restricted rotational diffusion maximize entropy, thereby inducing a free energetically favored rhombic crystalline structure.","Intriguingly, the density, which solely governs the crystallization of hard spheres, does not have any direct effect on this process.","Complementary Brownian dynamics simulations further corroborate these experimental observations and interpretations.","Our findings are also relevant to other systems of hard interacting anisotropic shapes in two and three dimensions."],"url":"http://arxiv.org/abs/2405.07352v1","category":"cond-mat.soft"}
{"created":"2024-05-12 18:04:41","title":"WeedScout: Real-Time Autonomous blackgrass Classification and Mapping using dedicated hardware","abstract":"Blackgrass (Alopecurus myosuroides) is a competitive weed that has wide-ranging impacts on food security by reducing crop yields and increasing cultivation costs. In addition to the financial burden on agriculture, the application of herbicides as a preventive to blackgrass can negatively affect access to clean water and sanitation. The WeedScout project introduces a Real-Rime Autonomous Black-Grass Classification and Mapping (RT-ABGCM), a cutting-edge solution tailored for real-time detection of blackgrass, for precision weed management practices. Leveraging Artificial Intelligence (AI) algorithms, the system processes live image feeds, infers blackgrass density, and covers two stages of maturation. The research investigates the deployment of You Only Look Once (YOLO) models, specifically the streamlined YOLOv8 and YOLO-NAS, accelerated at the edge with the NVIDIA Jetson Nano (NJN). By optimising inference speed and model performance, the project advances the integration of AI into agricultural practices, offering potential solutions to challenges such as herbicide resistance and environmental impact. Additionally, two datasets and model weights are made available to the research community, facilitating further advancements in weed detection and precision farming technologies.","sentences":["Blackgrass (Alopecurus myosuroides) is a competitive weed that has wide-ranging impacts on food security by reducing crop yields and increasing cultivation costs.","In addition to the financial burden on agriculture, the application of herbicides as a preventive to blackgrass can negatively affect access to clean water and sanitation.","The WeedScout project introduces a Real-Rime Autonomous Black-Grass Classification and Mapping (RT-ABGCM), a cutting-edge solution tailored for real-time detection of blackgrass, for precision weed management practices.","Leveraging Artificial Intelligence (AI) algorithms, the system processes live image feeds, infers blackgrass density, and covers two stages of maturation.","The research investigates the deployment of You Only Look Once (YOLO) models, specifically the streamlined YOLOv8 and YOLO-NAS, accelerated at the edge with the NVIDIA Jetson Nano (NJN).","By optimising inference speed and model performance, the project advances the integration of AI into agricultural practices, offering potential solutions to challenges such as herbicide resistance and environmental impact.","Additionally, two datasets and model weights are made available to the research community, facilitating further advancements in weed detection and precision farming technologies."],"url":"http://arxiv.org/abs/2405.07349v1","category":"cs.RO"}
{"created":"2024-05-12 17:45:11","title":"Understanding and Evaluating Human Preferences for AI Generated Images with Instruction Tuning","abstract":"Artificial Intelligence Generated Content (AIGC) has grown rapidly in recent years, among which AI-based image generation has gained widespread attention due to its efficient and imaginative image creation ability. However, AI-generated Images (AIGIs) may not satisfy human preferences due to their unique distortions, which highlights the necessity to understand and evaluate human preferences for AIGIs. To this end, in this paper, we first establish a novel Image Quality Assessment (IQA) database for AIGIs, termed AIGCIQA2023+, which provides human visual preference scores and detailed preference explanations from three perspectives including quality, authenticity, and correspondence. Then, based on the constructed AIGCIQA2023+ database, this paper presents a MINT-IQA model to evaluate and explain human preferences for AIGIs from Multi-perspectives with INstruction Tuning. Specifically, the MINT-IQA model first learn and evaluate human preferences for AI-generated Images from multi-perspectives, then via the vision-language instruction tuning strategy, MINT-IQA attains powerful understanding and explanation ability for human visual preference on AIGIs, which can be used for feedback to further improve the assessment capabilities. Extensive experimental results demonstrate that the proposed MINT-IQA model achieves state-of-the-art performance in understanding and evaluating human visual preferences for AIGIs, and the proposed model also achieves competing results on traditional IQA tasks compared with state-of-the-art IQA models. The AIGCIQA2023+ database and MINT-IQA model will be released to facilitate future research.","sentences":["Artificial Intelligence Generated Content (AIGC) has grown rapidly in recent years, among which AI-based image generation has gained widespread attention due to its efficient and imaginative image creation ability.","However, AI-generated Images (AIGIs) may not satisfy human preferences due to their unique distortions, which highlights the necessity to understand and evaluate human preferences for AIGIs.","To this end, in this paper, we first establish a novel Image Quality Assessment (IQA) database for AIGIs, termed AIGCIQA2023+, which provides human visual preference scores and detailed preference explanations from three perspectives including quality, authenticity, and correspondence.","Then, based on the constructed AIGCIQA2023+ database, this paper presents a MINT-IQA model to evaluate and explain human preferences for AIGIs from Multi-perspectives with INstruction Tuning.","Specifically, the MINT-IQA model first learn and evaluate human preferences for AI-generated Images from multi-perspectives, then via the vision-language instruction tuning strategy, MINT-IQA attains powerful understanding and explanation ability for human visual preference on AIGIs, which can be used for feedback to further improve the assessment capabilities.","Extensive experimental results demonstrate that the proposed MINT-IQA model achieves state-of-the-art performance in understanding and evaluating human visual preferences for AIGIs, and the proposed model also achieves competing results on traditional IQA tasks compared with state-of-the-art IQA models.","The AIGCIQA2023+ database and MINT-IQA model will be released to facilitate future research."],"url":"http://arxiv.org/abs/2405.07346v1","category":"cs.CV"}
{"created":"2024-05-12 17:40:48","title":"TKAN: Temporal Kolmogorov-Arnold Networks","abstract":"Recurrent Neural Networks (RNNs) have revolutionized many areas of machine learning, particularly in natural language and data sequence processing. Long Short-Term Memory (LSTM) has demonstrated its ability to capture long-term dependencies in sequential data. Inspired by the Kolmogorov-Arnold Networks (KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposed a new neural networks architecture inspired by KAN and the LSTM, the Temporal Kolomogorov-Arnold Networks (TKANs). TKANs combined the strenght of both networks, it is composed of Recurring Kolmogorov-Arnold Networks (RKANs) Layers embedding memory management. This innovation enables us to perform multi-step time series forecasting with enhanced accuracy and efficiency. By addressing the limitations of traditional models in handling complex sequential patterns, the TKAN architecture offers significant potential for advancements in fields requiring more than one step ahead forecasting.","sentences":["Recurrent Neural Networks (RNNs) have revolutionized many areas of machine learning, particularly in natural language and data sequence processing.","Long Short-Term Memory (LSTM) has demonstrated its ability to capture long-term dependencies in sequential data.","Inspired by the Kolmogorov-Arnold Networks (KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposed a new neural networks architecture inspired by KAN and the LSTM, the Temporal Kolomogorov-Arnold Networks (TKANs).","TKANs combined the strenght of both networks, it is composed of Recurring Kolmogorov-Arnold Networks (RKANs) Layers embedding memory management.","This innovation enables us to perform multi-step time series forecasting with enhanced accuracy and efficiency.","By addressing the limitations of traditional models in handling complex sequential patterns, the TKAN architecture offers significant potential for advancements in fields requiring more than one step ahead forecasting."],"url":"http://arxiv.org/abs/2405.07344v1","category":"cs.LG"}
{"created":"2024-05-12 17:39:09","title":"AquaIntellect: A Semantic Self-learning Framework for Underwater Internet of Things Connectivity","abstract":"The emerging paradigm of Non-Conventional Internet of Things (NC IoT), which is focused on the usefulness of information as opposed to the notion of high volume data collection and transmission, will be an important and dominant part of human life in the near future. This paper proposes a novel semantic-based approach for addressing the unique challenges posed by underwater NC IoT. We present an intelligent sensing strategy for exploring the semantics of the underwater environment by judiciously selecting the data to transmit, thereby minimizing redundancy for utmost relevant data transmission. We introduce an evolutionary function for the selection of the semantic-empowered messages relevant to the specific task within a minimum Age of Information (AoI), a freshness metric of the collected information, and for monitoring the underwater environment for performance optimization. A DNN-empowered Bayesian integrated with an adaptive surrogate model optimization will determine the optimal placement strategy of the sensors and the uncertainty level of the underwater landscape. An Adaptive Expected Improvement (AEI) mechanism is introduced to predict the optimal arrival rate for enabling a synchronized data sensing and transmission ecosystem, ensuring efficiency and timeliness. Simulation results show that the proposed solution outperforms conventional approaches.","sentences":["The emerging paradigm of Non-Conventional Internet of Things (NC IoT), which is focused on the usefulness of information as opposed to the notion of high volume data collection and transmission, will be an important and dominant part of human life in the near future.","This paper proposes a novel semantic-based approach for addressing the unique challenges posed by underwater NC IoT.","We present an intelligent sensing strategy for exploring the semantics of the underwater environment by judiciously selecting the data to transmit, thereby minimizing redundancy for utmost relevant data transmission.","We introduce an evolutionary function for the selection of the semantic-empowered messages relevant to the specific task within a minimum Age of Information (AoI), a freshness metric of the collected information, and for monitoring the underwater environment for performance optimization.","A DNN-empowered Bayesian integrated with an adaptive surrogate model optimization will determine the optimal placement strategy of the sensors and the uncertainty level of the underwater landscape.","An Adaptive Expected Improvement (AEI) mechanism is introduced to predict the optimal arrival rate for enabling a synchronized data sensing and transmission ecosystem, ensuring efficiency and timeliness.","Simulation results show that the proposed solution outperforms conventional approaches."],"url":"http://arxiv.org/abs/2405.07342v1","category":"eess.SP"}
{"created":"2024-05-12 17:30:48","title":"Machine Consciousness as Pseudoscience: The Myth of Conscious Machines","abstract":"The hypothesis of conscious machines has been debated since the invention of the notion of artificial intelligence, powered by the assumption that the computational intelligence achieved by a system is the cause of the emergence of phenomenal consciousness in that system as an epiphenomenon or as a consequence of the behavioral or internal complexity of the system surpassing some threshold. As a consequence, a huge amount of literature exploring the possibility of machine consciousness and how to implement it on a computer has been published. Moreover, common folk psychology and transhumanism literature has fed this hypothesis with the popularity of science fiction literature, where intelligent robots are usually antropomorphized and hence given phenomenal consciousness. However, in this work, we argue how these literature lacks scientific rigour, being impossible to falsify the opposite hypothesis, and illustrate a list of arguments that show how every approach that the machine consciousness literature has published depends on philosophical assumptions that cannot be proven by the scientific method. Concretely, we also show how phenomenal consciousness is not computable, independently on the complexity of the algorithm or model, cannot be objectively measured nor quantitatively defined and it is basically a phenomenon that is subjective and internal to the observer. Given all those arguments we end the work arguing why the idea of conscious machines is nowadays a myth of transhumanism and science fiction culture.","sentences":["The hypothesis of conscious machines has been debated since the invention of the notion of artificial intelligence, powered by the assumption that the computational intelligence achieved by a system is the cause of the emergence of phenomenal consciousness in that system as an epiphenomenon or as a consequence of the behavioral or internal complexity of the system surpassing some threshold.","As a consequence, a huge amount of literature exploring the possibility of machine consciousness and how to implement it on a computer has been published.","Moreover, common folk psychology and transhumanism literature has fed this hypothesis with the popularity of science fiction literature, where intelligent robots are usually antropomorphized and hence given phenomenal consciousness.","However, in this work, we argue how these literature lacks scientific rigour, being impossible to falsify the opposite hypothesis, and illustrate a list of arguments that show how every approach that the machine consciousness literature has published depends on philosophical assumptions that cannot be proven by the scientific method.","Concretely, we also show how phenomenal consciousness is not computable, independently on the complexity of the algorithm or model, cannot be objectively measured nor quantitatively defined and it is basically a phenomenon that is subjective and internal to the observer.","Given all those arguments we end the work arguing why the idea of conscious machines is nowadays a myth of transhumanism and science fiction culture."],"url":"http://arxiv.org/abs/2405.07340v1","category":"cs.CY"}
{"created":"2024-05-12 17:00:52","title":"PotatoGANs: Utilizing Generative Adversarial Networks, Instance Segmentation, and Explainable AI for Enhanced Potato Disease Identification and Classification","abstract":"Numerous applications have resulted from the automation of agricultural disease segmentation using deep learning techniques. However, when applied to new conditions, these applications frequently face the difficulty of overfitting, resulting in lower segmentation performance. In the context of potato farming, where diseases have a large influence on yields, it is critical for the agricultural economy to quickly and properly identify these diseases. Traditional data augmentation approaches, such as rotation, flip, and translation, have limitations and frequently fail to provide strong generalization results. To address these issues, our research employs a novel approach termed as PotatoGANs. In this novel data augmentation approach, two types of Generative Adversarial Networks (GANs) are utilized to generate synthetic potato disease images from healthy potato images. This approach not only expands the dataset but also adds variety, which helps to enhance model generalization. Using the Inception score as a measure, our experiments show the better quality and realisticness of the images created by PotatoGANs, emphasizing their capacity to resemble real disease images closely. The CycleGAN model outperforms the Pix2Pix GAN model in terms of image quality, as evidenced by its higher IS scores CycleGAN achieves higher Inception scores (IS) of 1.2001 and 1.0900 for black scurf and common scab, respectively. This synthetic data can significantly improve the training of large neural networks. It also reduces data collection costs while enhancing data diversity and generalization capabilities. Our work improves interpretability by combining three gradient-based Explainable AI algorithms (GradCAM, GradCAM++, and ScoreCAM) with three distinct CNN architectures (DenseNet169, Resnet152 V2, InceptionResNet V2) for potato disease classification.","sentences":["Numerous applications have resulted from the automation of agricultural disease segmentation using deep learning techniques.","However, when applied to new conditions, these applications frequently face the difficulty of overfitting, resulting in lower segmentation performance.","In the context of potato farming, where diseases have a large influence on yields, it is critical for the agricultural economy to quickly and properly identify these diseases.","Traditional data augmentation approaches, such as rotation, flip, and translation, have limitations and frequently fail to provide strong generalization results.","To address these issues, our research employs a novel approach termed as PotatoGANs.","In this novel data augmentation approach, two types of Generative Adversarial Networks (GANs) are utilized to generate synthetic potato disease images from healthy potato images.","This approach not only expands the dataset but also adds variety, which helps to enhance model generalization.","Using the Inception score as a measure, our experiments show the better quality and realisticness of the images created by PotatoGANs, emphasizing their capacity to resemble real disease images closely.","The CycleGAN model outperforms the Pix2Pix GAN model in terms of image quality, as evidenced by its higher IS scores CycleGAN achieves higher Inception scores (IS) of 1.2001 and 1.0900 for black scurf and common scab, respectively.","This synthetic data can significantly improve the training of large neural networks.","It also reduces data collection costs while enhancing data diversity and generalization capabilities.","Our work improves interpretability by combining three gradient-based Explainable AI algorithms (GradCAM, GradCAM++, and ScoreCAM) with three distinct CNN architectures (DenseNet169, Resnet152 V2, InceptionResNet V2) for potato disease classification."],"url":"http://arxiv.org/abs/2405.07332v1","category":"cs.CV"}
{"created":"2024-05-12 16:33:48","title":"Liquid Ensemble Selection for Continual Learning","abstract":"Continual learning aims to enable machine learning models to continually learn from a shifting data distribution without forgetting what has already been learned. Such shifting distributions can be broken into disjoint subsets of related examples; by training each member of an ensemble on a different subset it is possible for the ensemble as a whole to achieve much higher accuracy with less forgetting than a naive model. We address the problem of selecting which models within an ensemble should learn on any given data, and which should predict. By drawing on work from delegative voting we develop an algorithm for using delegation to dynamically select which models in an ensemble are active. We explore a variety of delegation methods and performance metrics, ultimately finding that delegation is able to provide a significant performance boost over naive learning in the face of distribution shifts.","sentences":["Continual learning aims to enable machine learning models to continually learn from a shifting data distribution without forgetting what has already been learned.","Such shifting distributions can be broken into disjoint subsets of related examples; by training each member of an ensemble on a different subset it is possible for the ensemble as a whole to achieve much higher accuracy with less forgetting than a naive model.","We address the problem of selecting which models within an ensemble should learn on any given data, and which should predict.","By drawing on work from delegative voting we develop an algorithm for using delegation to dynamically select which models in an ensemble are active.","We explore a variety of delegation methods and performance metrics, ultimately finding that delegation is able to provide a significant performance boost over naive learning in the face of distribution shifts."],"url":"http://arxiv.org/abs/2405.07327v1","category":"cs.LG"}
{"created":"2024-05-12 16:19:20","title":"QACM: QoS-Aware xApp Conflict Mitigation in Open RAN","abstract":"The advent of Open Radio Access Network (RAN) has revolutionized the field of RAN by introducing elements of native support of intelligence and openness into the next generation of mobile network infrastructure. Open RAN paves the way for standardized interfaces and enables the integration of network applications from diverse vendors, thereby enhancing network management flexibility. However, control decision conflicts occur when components from different vendors are deployed together. This article provides an overview of various types of conflicts that may occur in Open RAN, with a particular focus on intra-component conflict mitigation among Extended Applications (xApps) in the Near Real Time RAN Intelligent Controller (Near-RT-RIC). A QoS-Aware Conflict Mitigation (QACM) method is proposed that finds the optimal configuration of conflicting parameters while maximizing the number of xApps that have their Quality of Service (QoS) requirements met. We compare the performance of the proposed QACM method with two benchmark methods for priority and non-priority cases. The results indicate that our proposed method is the most effective in maintaining QoS requirements for conflicting xApps.","sentences":["The advent of Open Radio Access Network (RAN) has revolutionized the field of RAN by introducing elements of native support of intelligence and openness into the next generation of mobile network infrastructure.","Open RAN paves the way for standardized interfaces and enables the integration of network applications from diverse vendors, thereby enhancing network management flexibility.","However, control decision conflicts occur when components from different vendors are deployed together.","This article provides an overview of various types of conflicts that may occur in Open RAN, with a particular focus on intra-component conflict mitigation among Extended Applications (xApps) in the Near Real Time RAN Intelligent Controller (Near-RT-RIC).","A QoS-Aware Conflict Mitigation (QACM) method is proposed that finds the optimal configuration of conflicting parameters while maximizing the number of xApps that have their Quality of Service (QoS) requirements met.","We compare the performance of the proposed QACM method with two benchmark methods for priority and non-priority cases.","The results indicate that our proposed method is the most effective in maintaining QoS requirements for conflicting xApps."],"url":"http://arxiv.org/abs/2405.07324v1","category":"cs.NI"}
{"created":"2024-05-12 16:10:56","title":"AdaptNet: Rethinking Sensing and Communication for a Seamless Internet of Drones Experience","abstract":"In the evolving era of Unmanned Aerial Vehicles (UAVs), the emphasis has moved from mere data collection to strategically obtaining timely and relevant data within the Internet of Drones (IoDs) ecosystem. However, the unpredictable conditions in dynamic IoDs pose safety challenges for drones. Addressing this, our approach introduces a multi-UAV framework using spatial-temporal clustering and the Frechet distance for enhancing reliability. Seamlessly coupled with Integrated Sensing and Communication (ISAC), it enhances the precision and agility of UAV networks. Our Multi-Agent Reinforcement Learning (MARL) mechanism ensures UAVs adapt strategies through ongoing environmental interactions and enhancing intelligent sensing. This focus ensures operational safety and efficiency, considering data capture and transmission viability. By evaluating the relevance of the sensed information, we can communicate only the most crucial data variations beyond a set threshold and optimize bandwidth usage. Our methodology transforms the UAV domain, transitioning drones from data gatherers to adept information orchestrators, establishing a benchmark for efficiency and adaptability in modern aerial systems.","sentences":["In the evolving era of Unmanned Aerial Vehicles (UAVs), the emphasis has moved from mere data collection to strategically obtaining timely and relevant data within the Internet of Drones (IoDs) ecosystem.","However, the unpredictable conditions in dynamic IoDs pose safety challenges for drones.","Addressing this, our approach introduces a multi-UAV framework using spatial-temporal clustering and the Frechet distance for enhancing reliability.","Seamlessly coupled with Integrated Sensing and Communication (ISAC), it enhances the precision and agility of UAV networks.","Our Multi-Agent Reinforcement Learning (MARL) mechanism ensures UAVs adapt strategies through ongoing environmental interactions and enhancing intelligent sensing.","This focus ensures operational safety and efficiency, considering data capture and transmission viability.","By evaluating the relevance of the sensed information, we can communicate only the most crucial data variations beyond a set threshold and optimize bandwidth usage.","Our methodology transforms the UAV domain, transitioning drones from data gatherers to adept information orchestrators, establishing a benchmark for efficiency and adaptability in modern aerial systems."],"url":"http://arxiv.org/abs/2405.07318v1","category":"eess.SP"}
{"created":"2024-05-12 16:09:01","title":"Machine Unlearning in Contrastive Learning","abstract":"Machine unlearning is a complex process that necessitates the model to diminish the influence of the training data while keeping the loss of accuracy to a minimum. Despite the numerous studies on machine unlearning in recent years, the majority of them have primarily focused on supervised learning models, leaving research on contrastive learning models relatively underexplored. With the conviction that self-supervised learning harbors a promising potential, surpassing or rivaling that of supervised learning, we set out to investigate methods for machine unlearning centered around contrastive learning models. In this study, we introduce a novel gradient constraint-based approach for training the model to effectively achieve machine unlearning. Our method only necessitates a minimal number of training epochs and the identification of the data slated for unlearning. Remarkably, our approach demonstrates proficient performance not only on contrastive learning models but also on supervised learning models, showcasing its versatility and adaptability in various learning paradigms.","sentences":["Machine unlearning is a complex process that necessitates the model to diminish the influence of the training data while keeping the loss of accuracy to a minimum.","Despite the numerous studies on machine unlearning in recent years, the majority of them have primarily focused on supervised learning models, leaving research on contrastive learning models relatively underexplored.","With the conviction that self-supervised learning harbors a promising potential, surpassing or rivaling that of supervised learning, we set out to investigate methods for machine unlearning centered around contrastive learning models.","In this study, we introduce a novel gradient constraint-based approach for training the model to effectively achieve machine unlearning.","Our method only necessitates a minimal number of training epochs and the identification of the data slated for unlearning.","Remarkably, our approach demonstrates proficient performance not only on contrastive learning models but also on supervised learning models, showcasing its versatility and adaptability in various learning paradigms."],"url":"http://arxiv.org/abs/2405.07317v1","category":"cs.LG"}
{"created":"2024-05-12 15:38:17","title":"DiffGen: Robot Demonstration Generation via Differentiable Physics Simulation, Differentiable Rendering, and Vision-Language Model","abstract":"Generating robot demonstrations through simulation is widely recognized as an effective way to scale up robot data. Previous work often trained reinforcement learning agents to generate expert policies, but this approach lacks sample efficiency. Recently, a line of work has attempted to generate robot demonstrations via differentiable simulation, which is promising but heavily relies on reward design, a labor-intensive process. In this paper, we propose DiffGen, a novel framework that integrates differentiable physics simulation, differentiable rendering, and a vision-language model to enable automatic and efficient generation of robot demonstrations. Given a simulated robot manipulation scenario and a natural language instruction, DiffGen can generate realistic robot demonstrations by minimizing the distance between the embedding of the language instruction and the embedding of the simulated observation after manipulation. The embeddings are obtained from the vision-language model, and the optimization is achieved by calculating and descending gradients through the differentiable simulation, differentiable rendering, and vision-language model components, thereby accomplishing the specified task. Experiments demonstrate that with DiffGen, we could efficiently and effectively generate robot data with minimal human effort or training time.","sentences":["Generating robot demonstrations through simulation is widely recognized as an effective way to scale up robot data.","Previous work often trained reinforcement learning agents to generate expert policies, but this approach lacks sample efficiency.","Recently, a line of work has attempted to generate robot demonstrations via differentiable simulation, which is promising but heavily relies on reward design, a labor-intensive process.","In this paper, we propose DiffGen, a novel framework that integrates differentiable physics simulation, differentiable rendering, and a vision-language model to enable automatic and efficient generation of robot demonstrations.","Given a simulated robot manipulation scenario and a natural language instruction, DiffGen can generate realistic robot demonstrations by minimizing the distance between the embedding of the language instruction and the embedding of the simulated observation after manipulation.","The embeddings are obtained from the vision-language model, and the optimization is achieved by calculating and descending gradients through the differentiable simulation, differentiable rendering, and vision-language model components, thereby accomplishing the specified task.","Experiments demonstrate that with DiffGen, we could efficiently and effectively generate robot data with minimal human effort or training time."],"url":"http://arxiv.org/abs/2405.07309v1","category":"cs.RO"}
{"created":"2024-05-12 14:52:07","title":"Beyond Diagonal Reconfigurable Intelligent Surfaces in Wideband OFDM Communications: Circuit-Based Modeling and Optimization","abstract":"This work investigates the modeling and optimization of beyond diagonal reconfigurable intelligent surface (BD-RIS), which generalizes conventional RIS with diagonal phase shift matrices and provides additional flexibility for manipulating wireless channels, in wideband communication systems. Specifically, we start from the signal modeling of the BD-RIS-aided orthogonal frequency division multiplexing (OFDM) system, which bridges the time-domain and frequency-domain channels, and explicitly shows the frequency dependence of the BD-RIS response. We next characterize the frequency dependence of the BD-RIS response based on circuit models. Benefiting from the admittance parameter analysis, we model individually each tunable admittance component of BD-RIS and derive an approximated linear expression with respect to the frequency of the transmit signals. With the proposed signal model for the BD-RIS-aided OFDM system and the frequency-dependent BD-RIS model, we propose algorithms to optimize the BD-RIS and the power allocation at the transmitter to maximize the average rate for a BD-RIS-aided OFDM system. Finally, simulation results show that BD-RIS outperforms conventional RIS in the OFDM system. More importantly, the impact of wideband modeling of BD-RIS on the system performance becomes more significant as the circuit complexity of BD-RIS architectures increases.","sentences":["This work investigates the modeling and optimization of beyond diagonal reconfigurable intelligent surface (BD-RIS), which generalizes conventional RIS with diagonal phase shift matrices and provides additional flexibility for manipulating wireless channels, in wideband communication systems.","Specifically, we start from the signal modeling of the BD-RIS-aided orthogonal frequency division multiplexing (OFDM) system, which bridges the time-domain and frequency-domain channels, and explicitly shows the frequency dependence of the BD-RIS response.","We next characterize the frequency dependence of the BD-RIS response based on circuit models.","Benefiting from the admittance parameter analysis, we model individually each tunable admittance component of BD-RIS and derive an approximated linear expression with respect to the frequency of the transmit signals.","With the proposed signal model for the BD-RIS-aided OFDM system and the frequency-dependent BD-RIS model, we propose algorithms to optimize the BD-RIS and the power allocation at the transmitter to maximize the average rate for a BD-RIS-aided OFDM system.","Finally, simulation results show that BD-RIS outperforms conventional RIS in the OFDM system.","More importantly, the impact of wideband modeling of BD-RIS on the system performance becomes more significant as the circuit complexity of BD-RIS architectures increases."],"url":"http://arxiv.org/abs/2405.07297v1","category":"eess.SP"}
{"created":"2024-05-12 14:33:50","title":"Environmental enrichment: a biological model of forward transfer in continual learning","abstract":"Continual learning (CL) refers to an agent's capability to learn from a continuous stream of data and transfer knowledge without forgetting old information. One crucial aspect of CL is forward transfer, i.e., improved and faster learning on a new task by leveraging information from prior knowledge. While this ability comes naturally to biological brains, it poses a significant challenge for artificial intelligence (AI). Here, we suggest that environmental enrichment (EE) can be used as a biological model for studying forward transfer, inspiring human-like AI development. EE refers to animal studies that enhance cognitive, social, motor, and sensory stimulation and is a model for what, in humans, is referred to as 'cognitive reserve'. Enriched animals show significant improvement in learning speed and performance on new tasks, typically exhibiting forward transfer. We explore anatomical, molecular, and neuronal changes post-EE and discuss how artificial neural networks (ANNs) can be used to predict neural computation changes after enriched experiences. Finally, we provide a synergistic way of combining neuroscience and AI research that paves the path toward developing AI capable of rapid and efficient new task learning.","sentences":["Continual learning (CL) refers to an agent's capability to learn from a continuous stream of data and transfer knowledge without forgetting old information.","One crucial aspect of CL is forward transfer, i.e., improved and faster learning on a new task by leveraging information from prior knowledge.","While this ability comes naturally to biological brains, it poses a significant challenge for artificial intelligence (AI).","Here, we suggest that environmental enrichment (EE) can be used as a biological model for studying forward transfer, inspiring human-like AI development.","EE refers to animal studies that enhance cognitive, social, motor, and sensory stimulation and is a model for what, in humans, is referred to as 'cognitive reserve'.","Enriched animals show significant improvement in learning speed and performance on new tasks, typically exhibiting forward transfer.","We explore anatomical, molecular, and neuronal changes post-EE and discuss how artificial neural networks (ANNs) can be used to predict neural computation changes after enriched experiences.","Finally, we provide a synergistic way of combining neuroscience and AI research that paves the path toward developing AI capable of rapid and efficient new task learning."],"url":"http://arxiv.org/abs/2405.07295v1","category":"q-bio.NC"}
{"created":"2024-05-12 14:16:05","title":"Sparse Sampling is All You Need for Fast Wrong-way Cycling Detection in CCTV Videos","abstract":"In the field of transportation, it is of paramount importance to address and mitigate illegal actions committed by both motor and non-motor vehicles. Among those actions, wrong-way cycling (i.e., riding a bicycle or e-bike in the opposite direction of the designated traffic flow) poses significant risks to both cyclists and other road users. To this end, this paper formulates a problem of detecting wrong-way cycling ratios in CCTV videos. Specifically, we propose a sparse sampling method called WWC-Predictor to efficiently solve this problem, addressing the inefficiencies of direct tracking methods. Our approach leverages both detection-based information, which utilizes the information from bounding boxes, and orientation-based information, which provides insights into the image itself, to enhance instantaneous information capture capability. On our proposed benchmark dataset consisting of 35 minutes of video sequences and minute-level annotation, our method achieves an average error rate of a mere 1.475% while taking only 19.12% GPU time of straightforward tracking methods under the same detection model. This remarkable performance demonstrates the effectiveness of our approach in identifying and predicting instances of wrong-way cycling.","sentences":["In the field of transportation, it is of paramount importance to address and mitigate illegal actions committed by both motor and non-motor vehicles.","Among those actions, wrong-way cycling (i.e., riding a bicycle or e-bike in the opposite direction of the designated traffic flow) poses significant risks to both cyclists and other road users.","To this end, this paper formulates a problem of detecting wrong-way cycling ratios in CCTV videos.","Specifically, we propose a sparse sampling method called WWC-Predictor to efficiently solve this problem, addressing the inefficiencies of direct tracking methods.","Our approach leverages both detection-based information, which utilizes the information from bounding boxes, and orientation-based information, which provides insights into the image itself, to enhance instantaneous information capture capability.","On our proposed benchmark dataset consisting of 35 minutes of video sequences and minute-level annotation, our method achieves an average error rate of a mere 1.475% while taking only 19.12% GPU time of straightforward tracking methods under the same detection model.","This remarkable performance demonstrates the effectiveness of our approach in identifying and predicting instances of wrong-way cycling."],"url":"http://arxiv.org/abs/2405.07293v1","category":"cs.CV"}
{"created":"2024-05-12 13:00:14","title":"Humor Mechanics: Advancing Humor Generation with Multistep Reasoning","abstract":"In this paper, we explore the generation of one-liner jokes through multi-step reasoning. Our work involved reconstructing the process behind creating humorous one-liners and developing a working prototype for humor generation. We conducted comprehensive experiments with human participants to evaluate our approach, comparing it with human-created jokes, zero-shot GPT-4 generated humor, and other baselines. The evaluation focused on the quality of humor produced, using human labeling as a benchmark. Our findings demonstrate that the multi-step reasoning approach consistently improves the quality of generated humor. We present the results and share the datasets used in our experiments, offering insights into enhancing humor generation with artificial intelligence.","sentences":["In this paper, we explore the generation of one-liner jokes through multi-step reasoning.","Our work involved reconstructing the process behind creating humorous one-liners and developing a working prototype for humor generation.","We conducted comprehensive experiments with human participants to evaluate our approach, comparing it with human-created jokes, zero-shot GPT-4 generated humor, and other baselines.","The evaluation focused on the quality of humor produced, using human labeling as a benchmark.","Our findings demonstrate that the multi-step reasoning approach consistently improves the quality of generated humor.","We present the results and share the datasets used in our experiments, offering insights into enhancing humor generation with artificial intelligence."],"url":"http://arxiv.org/abs/2405.07280v1","category":"cs.CL"}
{"created":"2024-05-12 12:51:53","title":"Mining Influential Spreaders in Complex Networks by an Effective Combination of the Degree and K-Shell","abstract":"Graph mining is an important technique that used in many applications such as predicting and understanding behaviors and information dissemination within networks. One crucial aspect of graph mining is the identification and ranking of influential nodes, which has applications in various fields including marketing, social communications, and disease control. However, existing models and methods come with high computational complexity and may not accurately distinguish and identify influential nodes. This paper develops a method based on the k-shell index and degree centrality of nodes and their neighbors. Comparisons to previous works, such as Degree and Neighborhood information Centrality (DNC) and Neighborhood and Path Information Centrality (NPIC), are conducted. The evaluations, which include the correctness with Kendall's Tau, resolution with monotonicity index, correlation plots, and time complexity, demonstrate its superior results.","sentences":["Graph mining is an important technique that used in many applications such as predicting and understanding behaviors and information dissemination within networks.","One crucial aspect of graph mining is the identification and ranking of influential nodes, which has applications in various fields including marketing, social communications, and disease control.","However, existing models and methods come with high computational complexity and may not accurately distinguish and identify influential nodes.","This paper develops a method based on the k-shell index and degree centrality of nodes and their neighbors.","Comparisons to previous works, such as Degree and Neighborhood information Centrality (DNC) and Neighborhood and Path Information Centrality (NPIC), are conducted.","The evaluations, which include the correctness with Kendall's Tau, resolution with monotonicity index, correlation plots, and time complexity, demonstrate its superior results."],"url":"http://arxiv.org/abs/2405.07277v1","category":"cs.SI"}
{"created":"2024-05-12 12:38:40","title":"MAML MOT: Multiple Object Tracking based on Meta-Learning","abstract":"With the advancement of video analysis technology, the multi-object tracking (MOT) problem in complex scenes involving pedestrians is gaining increasing importance. This challenge primarily involves two key tasks: pedestrian detection and re-identification. While significant progress has been achieved in pedestrian detection tasks in recent years, enhancing the effectiveness of re-identification tasks remains a persistent challenge. This difficulty arises from the large total number of pedestrian samples in multi-object tracking datasets and the scarcity of individual instance samples. Motivated by recent rapid advancements in meta-learning techniques, we introduce MAML MOT, a meta-learning-based training approach for multi-object tracking. This approach leverages the rapid learning capability of meta-learning to tackle the issue of sample scarcity in pedestrian re-identification tasks, aiming to improve the model's generalization performance and robustness. Experimental results demonstrate that the proposed method achieves high accuracy on mainstream datasets in the MOT Challenge. This offers new perspectives and solutions for research in the field of pedestrian multi-object tracking.","sentences":["With the advancement of video analysis technology, the multi-object tracking (MOT) problem in complex scenes involving pedestrians is gaining increasing importance.","This challenge primarily involves two key tasks: pedestrian detection and re-identification.","While significant progress has been achieved in pedestrian detection tasks in recent years, enhancing the effectiveness of re-identification tasks remains a persistent challenge.","This difficulty arises from the large total number of pedestrian samples in multi-object tracking datasets and the scarcity of individual instance samples.","Motivated by recent rapid advancements in meta-learning techniques, we introduce MAML MOT, a meta-learning-based training approach for multi-object tracking.","This approach leverages the rapid learning capability of meta-learning to tackle the issue of sample scarcity in pedestrian re-identification tasks, aiming to improve the model's generalization performance and robustness.","Experimental results demonstrate that the proposed method achieves high accuracy on mainstream datasets in the MOT Challenge.","This offers new perspectives and solutions for research in the field of pedestrian multi-object tracking."],"url":"http://arxiv.org/abs/2405.07272v1","category":"cs.CV"}
{"created":"2024-05-12 12:28:15","title":"Fields, Bridges, and Foundations: How Researchers Browse Citation Network Visualizations","abstract":"Visualizing citation relations with network structures is widely used, but the visual complexity can make it challenging for individual researchers to navigate through them. We collected data from 18 researchers using an interface that we designed using network simplification methods and analyzed how users browsed and identified important papers. Our analysis reveals six major patterns used for identifying papers of interest, which can be categorized into three key components: Fields, Bridges, and Foundations, each viewed from two distinct perspectives: layout-oriented and connection-oriented. The connection-oriented approach was found to be more effective for selecting relevant papers, but the layout-oriented method was adopted more often, even though it led to unexpected results and user frustration. Our findings emphasize the importance of integrating these components and the necessity to balance visual layouts with meaningful connections to enhance the effectiveness of citation networks in academic browsing systems.","sentences":["Visualizing citation relations with network structures is widely used, but the visual complexity can make it challenging for individual researchers to navigate through them.","We collected data from 18 researchers using an interface that we designed using network simplification methods and analyzed how users browsed and identified important papers.","Our analysis reveals six major patterns used for identifying papers of interest, which can be categorized into three key components: Fields, Bridges, and Foundations, each viewed from two distinct perspectives: layout-oriented and connection-oriented.","The connection-oriented approach was found to be more effective for selecting relevant papers, but the layout-oriented method was adopted more often, even though it led to unexpected results and user frustration.","Our findings emphasize the importance of integrating these components and the necessity to balance visual layouts with meaningful connections to enhance the effectiveness of citation networks in academic browsing systems."],"url":"http://arxiv.org/abs/2405.07267v1","category":"cs.HC"}
{"created":"2024-05-12 11:51:00","title":"A Supervised Information Enhanced Multi-Granularity Contrastive Learning Framework for EEG Based Emotion Recognition","abstract":"This study introduces a novel Supervised Info-enhanced Contrastive Learning framework for EEG based Emotion Recognition (SICLEER). SI-CLEER employs multi-granularity contrastive learning to create robust EEG contextual representations, potentiallyn improving emotion recognition effectiveness. Unlike existing methods solely guided by classification loss, we propose a joint learning model combining self-supervised contrastive learning loss and supervised classification loss. This model optimizes both loss functions, capturing subtle EEG signal differences specific to emotion detection. Extensive experiments demonstrate SI-CLEER's robustness and superior accuracy on the SEED dataset compared to state-of-the-art methods. Furthermore, we analyze electrode performance, highlighting the significance of central frontal and temporal brain region EEGs in emotion detection. This study offers an universally applicable approach with potential benefits for diverse EEG classification tasks.","sentences":["This study introduces a novel Supervised Info-enhanced Contrastive Learning framework for EEG based Emotion Recognition (SICLEER).","SI-CLEER employs multi-granularity contrastive learning to create robust EEG contextual representations, potentiallyn improving emotion recognition effectiveness.","Unlike existing methods solely guided by classification loss, we propose a joint learning model combining self-supervised contrastive learning loss and supervised classification loss.","This model optimizes both loss functions, capturing subtle EEG signal differences specific to emotion detection.","Extensive experiments demonstrate SI-CLEER's robustness and superior accuracy on the SEED dataset compared to state-of-the-art methods.","Furthermore, we analyze electrode performance, highlighting the significance of central frontal and temporal brain region EEGs in emotion detection.","This study offers an universally applicable approach with potential benefits for diverse EEG classification tasks."],"url":"http://arxiv.org/abs/2405.07260v1","category":"cs.LG"}
{"created":"2024-05-12 10:52:15","title":"Limited Ability of LLMs to Simulate Human Psychological Behaviours: a Psychometric Analysis","abstract":"The humanlike responses of large language models (LLMs) have prompted social scientists to investigate whether LLMs can be used to simulate human participants in experiments, opinion polls and surveys. Of central interest in this line of research has been mapping out the psychological profiles of LLMs by prompting them to respond to standardized questionnaires. The conflicting findings of this research are unsurprising given that mapping out underlying, or latent, traits from LLMs' text responses to questionnaires is no easy task. To address this, we use psychometrics, the science of psychological measurement. In this study, we prompt OpenAI's flagship models, GPT-3.5 and GPT-4, to assume different personas and respond to a range of standardized measures of personality constructs. We used two kinds of persona descriptions: either generic (four or five random person descriptions) or specific (mostly demographics of actual humans from a large-scale human dataset). We found that the responses from GPT-4, but not GPT-3.5, using generic persona descriptions show promising, albeit not perfect, psychometric properties, similar to human norms, but the data from both LLMs when using specific demographic profiles, show poor psychometrics properties. We conclude that, currently, when LLMs are asked to simulate silicon personas, their responses are poor signals of potentially underlying latent traits. Thus, our work casts doubt on LLMs' ability to simulate individual-level human behaviour across multiple-choice question answering tasks.","sentences":["The humanlike responses of large language models (LLMs) have prompted social scientists to investigate whether LLMs can be used to simulate human participants in experiments, opinion polls and surveys.","Of central interest in this line of research has been mapping out the psychological profiles of LLMs by prompting them to respond to standardized questionnaires.","The conflicting findings of this research are unsurprising given that mapping out underlying, or latent, traits from LLMs' text responses to questionnaires is no easy task.","To address this, we use psychometrics, the science of psychological measurement.","In this study, we prompt OpenAI's flagship models, GPT-3.5 and GPT-4, to assume different personas and respond to a range of standardized measures of personality constructs.","We used two kinds of persona descriptions: either generic (four or five random person descriptions) or specific (mostly demographics of actual humans from a large-scale human dataset).","We found that the responses from GPT-4, but not GPT-3.5, using generic persona descriptions show promising, albeit not perfect, psychometric properties, similar to human norms, but the data from both LLMs when using specific demographic profiles, show poor psychometrics properties.","We conclude that, currently, when LLMs are asked to simulate silicon personas, their responses are poor signals of potentially underlying latent traits.","Thus, our work casts doubt on LLMs' ability to simulate individual-level human behaviour across multiple-choice question answering tasks."],"url":"http://arxiv.org/abs/2405.07248v1","category":"cs.CL"}
{"created":"2024-05-12 09:32:40","title":"OXYGENERATOR: Reconstructing Global Ocean Deoxygenation Over a Century with Deep Learning","abstract":"Accurately reconstructing the global ocean deoxygenation over a century is crucial for assessing and protecting marine ecosystem. Existing expert-dominated numerical simulations fail to catch up with the dynamic variation caused by global warming and human activities. Besides, due to the high-cost data collection, the historical observations are severely sparse, leading to big challenge for precise reconstruction. In this work, we propose OxyGenerator, the first deep learning based model, to reconstruct the global ocean deoxygenation from 1920 to 2023. Specifically, to address the heterogeneity across large temporal and spatial scales, we propose zoning-varying graph message-passing to capture the complex oceanographic correlations between missing values and sparse observations. Additionally, to further calibrate the uncertainty, we incorporate inductive bias from dissolved oxygen (DO) variations and chemical effects. Compared with in-situ DO observations, OxyGenerator significantly outperforms CMIP6 numerical simulations, reducing MAPE by 38.77%, demonstrating a promising potential to understand the \"breathless ocean\" in data-driven manner.","sentences":["Accurately reconstructing the global ocean deoxygenation over a century is crucial for assessing and protecting marine ecosystem.","Existing expert-dominated numerical simulations fail to catch up with the dynamic variation caused by global warming and human activities.","Besides, due to the high-cost data collection, the historical observations are severely sparse, leading to big challenge for precise reconstruction.","In this work, we propose OxyGenerator, the first deep learning based model, to reconstruct the global ocean deoxygenation from 1920 to 2023.","Specifically, to address the heterogeneity across large temporal and spatial scales, we propose zoning-varying graph message-passing to capture the complex oceanographic correlations between missing values and sparse observations.","Additionally, to further calibrate the uncertainty, we incorporate inductive bias from dissolved oxygen (DO) variations and chemical effects.","Compared with in-situ DO observations, OxyGenerator significantly outperforms CMIP6 numerical simulations, reducing MAPE by 38.77%, demonstrating a promising potential to understand the \"breathless ocean\" in data-driven manner."],"url":"http://arxiv.org/abs/2405.07233v1","category":"cs.LG"}
{"created":"2024-05-12 09:05:13","title":"Separable Power of Classical and Quantum Learning Protocols Through the Lens of No-Free-Lunch Theorem","abstract":"The No-Free-Lunch (NFL) theorem, which quantifies problem- and data-independent generalization errors regardless of the optimization process, provides a foundational framework for comprehending diverse learning protocols' potential. Despite its significance, the establishment of the NFL theorem for quantum machine learning models remains largely unexplored, thereby overlooking broader insights into the fundamental relationship between quantum and classical learning protocols. To address this gap, we categorize a diverse array of quantum learning algorithms into three learning protocols designed for learning quantum dynamics under a specified observable and establish their NFL theorem. The exploited protocols, namely Classical Learning Protocols (CLC-LPs), Restricted Quantum Learning Protocols (ReQu-LPs), and Quantum Learning Protocols (Qu-LPs), offer varying levels of access to quantum resources. Our derived NFL theorems demonstrate quadratic reductions in sample complexity across CLC-LPs, ReQu-LPs, and Qu-LPs, contingent upon the orthogonality of quantum states and the diagonality of observables. We attribute this performance discrepancy to the unique capacity of quantum-related learning protocols to indirectly utilize information concerning the global phases of non-orthogonal quantum states, a distinctive physical feature inherent in quantum mechanics. Our findings not only deepen our understanding of quantum learning protocols' capabilities but also provide practical insights for the development of advanced quantum learning algorithms.","sentences":["The No-Free-Lunch (NFL) theorem, which quantifies problem- and data-independent generalization errors regardless of the optimization process, provides a foundational framework for comprehending diverse learning protocols' potential.","Despite its significance, the establishment of the NFL theorem for quantum machine learning models remains largely unexplored, thereby overlooking broader insights into the fundamental relationship between quantum and classical learning protocols.","To address this gap, we categorize a diverse array of quantum learning algorithms into three learning protocols designed for learning quantum dynamics under a specified observable and establish their NFL theorem.","The exploited protocols, namely Classical Learning Protocols (CLC-LPs), Restricted Quantum Learning Protocols (ReQu-LPs), and Quantum Learning Protocols (Qu-LPs), offer varying levels of access to quantum resources.","Our derived NFL theorems demonstrate quadratic reductions in sample complexity across CLC-LPs, ReQu-LPs, and Qu-LPs, contingent upon the orthogonality of quantum states and the diagonality of observables.","We attribute this performance discrepancy to the unique capacity of quantum-related learning protocols to indirectly utilize information concerning the global phases of non-orthogonal quantum states, a distinctive physical feature inherent in quantum mechanics.","Our findings not only deepen our understanding of quantum learning protocols' capabilities but also provide practical insights for the development of advanced quantum learning algorithms."],"url":"http://arxiv.org/abs/2405.07226v1","category":"quant-ph"}
{"created":"2024-05-12 08:52:52","title":"Ensemble Successor Representations for Task Generalization in Offline-to-Online Reinforcement Learning","abstract":"In Reinforcement Learning (RL), training a policy from scratch with online experiences can be inefficient because of the difficulties in exploration. Recently, offline RL provides a promising solution by giving an initialized offline policy, which can be refined through online interactions. However, existing approaches primarily perform offline and online learning in the same task, without considering the task generalization problem in offline-to-online adaptation. In real-world applications, it is common that we only have an offline dataset from a specific task while aiming for fast online-adaptation for several tasks. To address this problem, our work builds upon the investigation of successor representations for task generalization in online RL and extends the framework to incorporate offline-to-online learning. We demonstrate that the conventional paradigm using successor features cannot effectively utilize offline data and improve the performance for the new task by online fine-tuning. To mitigate this, we introduce a novel methodology that leverages offline data to acquire an ensemble of successor representations and subsequently constructs ensemble Q functions. This approach enables robust representation learning from datasets with different coverage and facilitates fast adaption of Q functions towards new tasks during the online fine-tuning phase. Extensive empirical evaluations provide compelling evidence showcasing the superior performance of our method in generalizing to diverse or even unseen tasks.","sentences":["In Reinforcement Learning (RL), training a policy from scratch with online experiences can be inefficient because of the difficulties in exploration.","Recently, offline RL provides a promising solution by giving an initialized offline policy, which can be refined through online interactions.","However, existing approaches primarily perform offline and online learning in the same task, without considering the task generalization problem in offline-to-online adaptation.","In real-world applications, it is common that we only have an offline dataset from a specific task while aiming for fast online-adaptation for several tasks.","To address this problem, our work builds upon the investigation of successor representations for task generalization in online RL and extends the framework to incorporate offline-to-online learning.","We demonstrate that the conventional paradigm using successor features cannot effectively utilize offline data and improve the performance for the new task by online fine-tuning.","To mitigate this, we introduce a novel methodology that leverages offline data to acquire an ensemble of successor representations and subsequently constructs ensemble Q functions.","This approach enables robust representation learning from datasets with different coverage and facilitates fast adaption of Q functions towards new tasks during the online fine-tuning phase.","Extensive empirical evaluations provide compelling evidence showcasing the superior performance of our method in generalizing to diverse or even unseen tasks."],"url":"http://arxiv.org/abs/2405.07223v1","category":"cs.LG"}
{"created":"2024-05-12 08:48:37","title":"On Discovery of Local Independence over Continuous Variables via Neural Contextual Decomposition","abstract":"Conditional independence provides a way to understand causal relationships among the variables of interest. An underlying system may exhibit more fine-grained causal relationships especially between a variable and its parents, which will be called the local independence relationships. One of the most widely studied local relationships is Context-Specific Independence (CSI), which holds in a specific assignment of conditioned variables. However, its applicability is often limited since it does not allow continuous variables: data conditioned to the specific value of a continuous variable contains few instances, if not none, making it infeasible to test independence. In this work, we define and characterize the local independence relationship that holds in a specific set of joint assignments of parental variables, which we call context-set specific independence (CSSI). We then provide a canonical representation of CSSI and prove its fundamental properties. Based on our theoretical findings, we cast the problem of discovering multiple CSSI relationships in a system as finding a partition of the joint outcome space. Finally, we propose a novel method, coined neural contextual decomposition (NCD), which learns such partition by imposing each set to induce CSSI via modeling a conditional distribution. We empirically demonstrate that the proposed method successfully discovers the ground truth local independence relationships in both synthetic dataset and complex system reflecting the real-world physical dynamics.","sentences":["Conditional independence provides a way to understand causal relationships among the variables of interest.","An underlying system may exhibit more fine-grained causal relationships especially between a variable and its parents, which will be called the local independence relationships.","One of the most widely studied local relationships is Context-Specific Independence (CSI), which holds in a specific assignment of conditioned variables.","However, its applicability is often limited since it does not allow continuous variables: data conditioned to the specific value of a continuous variable contains few instances, if not none, making it infeasible to test independence.","In this work, we define and characterize the local independence relationship that holds in a specific set of joint assignments of parental variables, which we call context-set specific independence (CSSI).","We then provide a canonical representation of CSSI and prove its fundamental properties.","Based on our theoretical findings, we cast the problem of discovering multiple CSSI relationships in a system as finding a partition of the joint outcome space.","Finally, we propose a novel method, coined neural contextual decomposition (NCD), which learns such partition by imposing each set to induce CSSI via modeling a conditional distribution.","We empirically demonstrate that the proposed method successfully discovers the ground truth local independence relationships in both synthetic dataset and complex system reflecting the real-world physical dynamics."],"url":"http://arxiv.org/abs/2405.07220v1","category":"cs.LG"}
{"created":"2024-05-12 08:22:53","title":"Enhancing Decision-Making in Optimization through LLM-Assisted Inference: A Neural Networks Perspective","abstract":"This paper explores the seamless integration of Generative AI (GenAI) and Evolutionary Algorithms (EAs) within the domain of large-scale multi-objective optimization. Focusing on the transformative role of Large Language Models (LLMs), our study investigates the potential of LLM-Assisted Inference to automate and enhance decision-making processes. Specifically, we highlight its effectiveness in illuminating key decision variables in evolutionarily optimized solutions while articulating contextual trade-offs. Tailored to address the challenges inherent in inferring complex multi-objective optimization solutions at scale, our approach emphasizes the adaptive nature of LLMs, allowing them to provide nuanced explanations and align their language with diverse stakeholder expertise levels and domain preferences. Empirical studies underscore the practical applicability and impact of LLM-Assisted Inference in real-world decision-making scenarios.","sentences":["This paper explores the seamless integration of Generative AI (GenAI) and Evolutionary Algorithms (EAs) within the domain of large-scale multi-objective optimization.","Focusing on the transformative role of Large Language Models (LLMs), our study investigates the potential of LLM-Assisted Inference to automate and enhance decision-making processes.","Specifically, we highlight its effectiveness in illuminating key decision variables in evolutionarily optimized solutions while articulating contextual trade-offs.","Tailored to address the challenges inherent in inferring complex multi-objective optimization solutions at scale, our approach emphasizes the adaptive nature of LLMs, allowing them to provide nuanced explanations and align their language with diverse stakeholder expertise levels and domain preferences.","Empirical studies underscore the practical applicability and impact of LLM-Assisted Inference in real-world decision-making scenarios."],"url":"http://arxiv.org/abs/2405.07212v1","category":"cs.NE"}
{"created":"2024-05-12 07:59:46","title":"Unified Video-Language Pre-training with Synchronized Audio","abstract":"Video-language pre-training is a typical and challenging problem that aims at learning visual and textual representations from large-scale data in a self-supervised way. Existing pre-training approaches either captured the correspondence of image-text pairs or utilized temporal ordering of frames. However, they do not explicitly explore the natural synchronization between audio and the other two modalities. In this work, we propose an enhanced framework for Video-Language pre-training with Synchronized Audio, termed as VLSA, that can learn tri-modal representations in a unified self-supervised transformer. Specifically, our VLSA jointly aggregates embeddings of local patches and global tokens for video, text, and audio. Furthermore, we utilize local-patch masked modeling to learn modality-aware features, and leverage global audio matching to capture audio-guided features for video and text. We conduct extensive experiments on retrieval across text, video, and audio. Our simple model pre-trained on only 0.9M data achieves improving results against state-of-the-art baselines. In addition, qualitative visualizations vividly showcase the superiority of our VLSA in learning discriminative visual-textual representations.","sentences":["Video-language pre-training is a typical and challenging problem that aims at learning visual and textual representations from large-scale data in a self-supervised way.","Existing pre-training approaches either captured the correspondence of image-text pairs or utilized temporal ordering of frames.","However, they do not explicitly explore the natural synchronization between audio and the other two modalities.","In this work, we propose an enhanced framework for Video-Language pre-training with Synchronized Audio, termed as VLSA, that can learn tri-modal representations in a unified self-supervised transformer.","Specifically, our VLSA jointly aggregates embeddings of local patches and global tokens for video, text, and audio.","Furthermore, we utilize local-patch masked modeling to learn modality-aware features, and leverage global audio matching to capture audio-guided features for video and text.","We conduct extensive experiments on retrieval across text, video, and audio.","Our simple model pre-trained on only 0.9M data achieves improving results against state-of-the-art baselines.","In addition, qualitative visualizations vividly showcase the superiority of our VLSA in learning discriminative visual-textual representations."],"url":"http://arxiv.org/abs/2405.07202v1","category":"cs.CV"}
{"created":"2024-05-12 07:55:43","title":"Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient Architecture for Nonlinear Function Approximation","abstract":"Accurate approximation of complex nonlinear functions is a fundamental challenge across many scientific and engineering domains. Traditional neural network architectures often struggle to capture intricate patterns and irregularities present in high-dimensional functions. This paper introduces the Chebyshev Kolmogorov-Arnold Network (Chebyshev KAN), a novel approach that combines the theoretical foundations of the Kolmogorov-Arnold Theorem with the powerful approximation capabilities of Chebyshev polynomials. 1","sentences":["Accurate approximation of complex nonlinear functions is a fundamental challenge across many scientific and engineering domains.","Traditional neural network architectures often struggle to capture intricate patterns and irregularities present in high-dimensional functions.","This paper introduces the Chebyshev Kolmogorov-Arnold Network (Chebyshev KAN), a novel approach that combines the theoretical foundations of the Kolmogorov-Arnold Theorem with the powerful approximation capabilities of Chebyshev polynomials.","1"],"url":"http://arxiv.org/abs/2405.07200v1","category":"cs.LG"}
{"created":"2024-05-12 07:40:12","title":"InsightNet: Structured Insight Mining from Customer Feedback","abstract":"We propose InsightNet, a novel approach for the automated extraction of structured insights from customer reviews. Our end-to-end machine learning framework is designed to overcome the limitations of current solutions, including the absence of structure for identified topics, non-standard aspect names, and lack of abundant training data. The proposed solution builds a semi-supervised multi-level taxonomy from raw reviews, a semantic similarity heuristic approach to generate labelled data and employs a multi-task insight extraction architecture by fine-tuning an LLM. InsightNet identifies granular actionable topics with customer sentiments and verbatim for each topic. Evaluations on real-world customer review data show that InsightNet performs better than existing solutions in terms of structure, hierarchy and completeness. We empirically demonstrate that InsightNet outperforms the current state-of-the-art methods in multi-label topic classification, achieving an F1 score of 0.85, which is an improvement of 11% F1-score over the previous best results. Additionally, InsightNet generalises well for unseen aspects and suggests new topics to be added to the taxonomy.","sentences":["We propose InsightNet, a novel approach for the automated extraction of structured insights from customer reviews.","Our end-to-end machine learning framework is designed to overcome the limitations of current solutions, including the absence of structure for identified topics, non-standard aspect names, and lack of abundant training data.","The proposed solution builds a semi-supervised multi-level taxonomy from raw reviews, a semantic similarity heuristic approach to generate labelled data and employs a multi-task insight extraction architecture by fine-tuning an LLM.","InsightNet identifies granular actionable topics with customer sentiments and verbatim for each topic.","Evaluations on real-world customer review data show that InsightNet performs better than existing solutions in terms of structure, hierarchy and completeness.","We empirically demonstrate that InsightNet outperforms the current state-of-the-art methods in multi-label topic classification, achieving an F1 score of 0.85, which is an improvement of 11% F1-score over the previous best results.","Additionally, InsightNet generalises well for unseen aspects and suggests new topics to be added to the taxonomy."],"url":"http://arxiv.org/abs/2405.07195v1","category":"cs.CL"}
{"created":"2024-05-12 07:34:33","title":"Differentiable Model Scaling using Differentiable Topk","abstract":"Over the past few years, as large language models have ushered in an era of intelligence emergence, there has been an intensified focus on scaling networks. Currently, many network architectures are designed manually, often resulting in sub-optimal configurations. Although Neural Architecture Search (NAS) methods have been proposed to automate this process, they suffer from low search efficiency. This study introduces Differentiable Model Scaling (DMS), increasing the efficiency for searching optimal width and depth in networks. DMS can model both width and depth in a direct and fully differentiable way, making it easy to optimize. We have evaluated our DMS across diverse tasks, ranging from vision tasks to NLP tasks and various network architectures, including CNNs and Transformers. Results consistently indicate that our DMS can find improved structures and outperforms state-of-the-art NAS methods. Specifically, for image classification on ImageNet, our DMS improves the top-1 accuracy of EfficientNet-B0 and Deit-Tiny by 1.4% and 0.6%, respectively, and outperforms the state-of-the-art zero-shot NAS method, ZiCo, by 1.3% while requiring only 0.4 GPU days for searching. For object detection on COCO, DMS improves the mAP of Yolo-v8-n by 2.0%. For language modeling, our pruned Llama-7B outperforms the prior method with lower perplexity and higher zero-shot classification accuracy. We will release our code in the future.","sentences":["Over the past few years, as large language models have ushered in an era of intelligence emergence, there has been an intensified focus on scaling networks.","Currently, many network architectures are designed manually, often resulting in sub-optimal configurations.","Although Neural Architecture Search (NAS) methods have been proposed to automate this process, they suffer from low search efficiency.","This study introduces Differentiable Model Scaling (DMS), increasing the efficiency for searching optimal width and depth in networks.","DMS can model both width and depth in a direct and fully differentiable way, making it easy to optimize.","We have evaluated our DMS across diverse tasks, ranging from vision tasks to NLP tasks and various network architectures, including CNNs and Transformers.","Results consistently indicate that our DMS can find improved structures and outperforms state-of-the-art NAS methods.","Specifically, for image classification on ImageNet, our DMS improves the top-1 accuracy of EfficientNet-B0 and Deit-Tiny by 1.4% and 0.6%, respectively, and outperforms the state-of-the-art zero-shot NAS method, ZiCo, by 1.3% while requiring only 0.4 GPU days for searching.","For object detection on COCO, DMS improves the mAP of Yolo-v8-n by 2.0%.","For language modeling, our pruned Llama-7B outperforms the prior method with lower perplexity and higher zero-shot classification accuracy.","We will release our code in the future."],"url":"http://arxiv.org/abs/2405.07194v1","category":"cs.CV"}
{"created":"2024-05-12 07:12:18","title":"Two-Plasmon-Decay Instability Stimulated by a Normal- and Large-Angle-Incidence Laser Pair","abstract":"The two-plasmon-decay instability (TPD) is a critical target preheating risk in direct-drive inertial confinement fusion. In this paper, TPD collectively driven by a normal-incidence laser beam (Beam-N) and a large-angle-incidence laser beam (Beam-L) is investigated via particle-in-cell simulations. Significant TPD growth is found able to develop in this regime at previously unexpected low laser intensities if the intensity of Beam-L exceeds the large-angle-incidence threshold. Both beams contribute to the growth of TPD in a \"seed-amplification\" manner where the absolute instability driven by Beam-L provides the seeds that get convectively amplified by Beam-N, making TPD energetically important and causing significant pump depletion and hot electron generation.","sentences":["The two-plasmon-decay instability (TPD) is a critical target preheating risk in direct-drive inertial confinement fusion.","In this paper, TPD collectively driven by a normal-incidence laser beam (Beam-N) and a large-angle-incidence laser beam (Beam-L) is investigated via particle-in-cell simulations.","Significant TPD growth is found able to develop in this regime at previously unexpected low laser intensities if the intensity of Beam-L exceeds the large-angle-incidence threshold.","Both beams contribute to the growth of TPD in a \"seed-amplification\" manner where the absolute instability driven by Beam-L provides the seeds that get convectively amplified by Beam-N, making TPD energetically important and causing significant pump depletion and hot electron generation."],"url":"http://arxiv.org/abs/2405.07187v1","category":"physics.plasm-ph"}
{"created":"2024-05-12 05:05:31","title":"Realizing Visual Question Answering for Education: GPT-4V as a Multimodal AI","abstract":"Educational scholars have analyzed various image data acquired from teaching and learning situations, such as photos that shows classroom dynamics, students' drawings with regard to the learning content, textbook illustrations, etc. Unquestioningly, most qualitative analysis of and explanation on image data have been conducted by human researchers, without machine-based automation. It was partially because most image processing artificial intelligence models were not accessible to general educational scholars or explainable due to their complex deep neural network architecture. However, the recent development of Visual Question Answering (VQA) techniques is accomplishing usable visual language models, which receive from the user a question about the given image and returns an answer, both in natural language. Particularly, GPT-4V released by OpenAI, has wide opened the state-of-the-art visual langauge model service so that VQA could be used for a variety of purposes. However, VQA and GPT-4V have not yet been applied to educational studies much. In this position paper, we suggest that GPT-4V contributes to realizing VQA for education. By 'realizing' VQA, we denote two meanings: (1) GPT-4V realizes the utilization of VQA techniques by any educational scholars without technical/accessibility barrier, and (2) GPT-4V makes educational scholars realize the usefulness of VQA to educational research. Given these, this paper aims to introduce VQA for educational studies so that it provides a milestone for educational research methodology. In this paper, chapter II reviews the development of VQA techniques, which primes with the release of GPT-4V. Chapter III reviews the use of image analysis in educational studies. Chapter IV demonstrates how GPT-4V can be used for each research usage reviewed in Chapter III, with operating prompts provided. Finally, chapter V discusses the future implications.","sentences":["Educational scholars have analyzed various image data acquired from teaching and learning situations, such as photos that shows classroom dynamics, students' drawings with regard to the learning content, textbook illustrations, etc.","Unquestioningly, most qualitative analysis of and explanation on image data have been conducted by human researchers, without machine-based automation.","It was partially because most image processing artificial intelligence models were not accessible to general educational scholars or explainable due to their complex deep neural network architecture.","However, the recent development of Visual Question Answering (VQA) techniques is accomplishing usable visual language models, which receive from the user a question about the given image and returns an answer, both in natural language.","Particularly, GPT-4V released by OpenAI, has wide opened the state-of-the-art visual langauge model service so that VQA could be used for a variety of purposes.","However, VQA and GPT-4V have not yet been applied to educational studies much.","In this position paper, we suggest that GPT-4V contributes to realizing VQA for education.","By 'realizing' VQA, we denote two meanings: (1) GPT-4V realizes the utilization of VQA techniques by any educational scholars without technical/accessibility barrier, and (2) GPT-4V makes educational scholars realize the usefulness of VQA to educational research.","Given these, this paper aims to introduce VQA for educational studies so that it provides a milestone for educational research methodology.","In this paper, chapter II reviews the development of VQA techniques, which primes with the release of GPT-4V. Chapter III reviews the use of image analysis in educational studies.","Chapter IV demonstrates how GPT-4V can be used for each research usage reviewed in Chapter III, with operating prompts provided.","Finally, chapter V discusses the future implications."],"url":"http://arxiv.org/abs/2405.07163v1","category":"physics.ed-ph"}
{"created":"2024-05-12 04:57:43","title":"Learning Reward for Robot Skills Using Large Language Models via Self-Alignment","abstract":"Learning reward functions remains the bottleneck to equip a robot with a broad repertoire of skills. Large Language Models (LLM) contain valuable task-related knowledge that can potentially aid in the learning of reward functions. However, the proposed reward function can be imprecise, thus ineffective which requires to be further grounded with environment information. We proposed a method to learn rewards more efficiently in the absence of humans. Our approach consists of two components: We first use the LLM to propose features and parameterization of the reward, then update the parameters through an iterative self-alignment process. In particular, the process minimizes the ranking inconsistency between the LLM and the learnt reward functions based on the execution feedback. The method was validated on 9 tasks across 2 simulation environments. It demonstrates a consistent improvement over training efficacy and efficiency, meanwhile consuming significantly fewer GPT tokens compared to the alternative mutation-based method.","sentences":["Learning reward functions remains the bottleneck to equip a robot with a broad repertoire of skills.","Large Language Models (LLM) contain valuable task-related knowledge that can potentially aid in the learning of reward functions.","However, the proposed reward function can be imprecise, thus ineffective which requires to be further grounded with environment information.","We proposed a method to learn rewards more efficiently in the absence of humans.","Our approach consists of two components: We first use the LLM to propose features and parameterization of the reward, then update the parameters through an iterative self-alignment process.","In particular, the process minimizes the ranking inconsistency between the LLM and the learnt reward functions based on the execution feedback.","The method was validated on 9 tasks across 2 simulation environments.","It demonstrates a consistent improvement over training efficacy and efficiency, meanwhile consuming significantly fewer GPT tokens compared to the alternative mutation-based method."],"url":"http://arxiv.org/abs/2405.07162v1","category":"cs.RO"}
{"created":"2024-05-12 04:35:49","title":"Semi-Self-Supervised Domain Adaptation: Developing Deep Learning Models with Limited Annotated Data for Wheat Head Segmentation","abstract":"Precision agriculture involves the application of advanced technologies to improve agricultural productivity, efficiency, and profitability while minimizing waste and environmental impact. Deep learning approaches enable automated decision-making for many visual tasks. However, in the agricultural domain, variability in growth stages and environmental conditions, such as weather and lighting, presents significant challenges to developing deep learning-based techniques that generalize across different conditions. The resource-intensive nature of creating extensive annotated datasets that capture these variabilities further hinders the widespread adoption of these approaches. To tackle these issues, we introduce a semi-self-supervised domain adaptation technique based on deep convolutional neural networks with a probabilistic diffusion process, requiring minimal manual data annotation. Using only three manually annotated images and a selection of video clips from wheat fields, we generated a large-scale computationally annotated dataset of image-mask pairs and a large dataset of unannotated images extracted from video frames. We developed a two-branch convolutional encoder-decoder model architecture that uses both synthesized image-mask pairs and unannotated images, enabling effective adaptation to real images. The proposed model achieved a Dice score of 80.7\\% on an internal test dataset and a Dice score of 64.8\\% on an external test set, composed of images from five countries and spanning 18 domains, indicating its potential to develop generalizable solutions that could encourage the wider adoption of advanced technologies in agriculture.","sentences":["Precision agriculture involves the application of advanced technologies to improve agricultural productivity, efficiency, and profitability while minimizing waste and environmental impact.","Deep learning approaches enable automated decision-making for many visual tasks.","However, in the agricultural domain, variability in growth stages and environmental conditions, such as weather and lighting, presents significant challenges to developing deep learning-based techniques that generalize across different conditions.","The resource-intensive nature of creating extensive annotated datasets that capture these variabilities further hinders the widespread adoption of these approaches.","To tackle these issues, we introduce a semi-self-supervised domain adaptation technique based on deep convolutional neural networks with a probabilistic diffusion process, requiring minimal manual data annotation.","Using only three manually annotated images and a selection of video clips from wheat fields, we generated a large-scale computationally annotated dataset of image-mask pairs and a large dataset of unannotated images extracted from video frames.","We developed a two-branch convolutional encoder-decoder model architecture that uses both synthesized image-mask pairs and unannotated images, enabling effective adaptation to real images.","The proposed model achieved a Dice score of 80.7\\% on an internal test dataset and a Dice score of 64.8\\% on an external test set, composed of images from five countries and spanning 18 domains, indicating its potential to develop generalizable solutions that could encourage the wider adoption of advanced technologies in agriculture."],"url":"http://arxiv.org/abs/2405.07157v1","category":"cs.CV"}
{"created":"2024-05-12 02:41:31","title":"Cross-Domain Continual Learning via CLAMP","abstract":"Artificial neural networks, celebrated for their human-like cognitive learning abilities, often encounter the well-known catastrophic forgetting (CF) problem, where the neural networks lose the proficiency in previously acquired knowledge. Despite numerous efforts to mitigate CF, it remains the significant challenge particularly in complex changing environments. This challenge is even more pronounced in cross-domain adaptation following the continual learning (CL) setting, which is a more challenging and realistic scenario that is under-explored. To this end, this article proposes a cross-domain CL approach making possible to deploy a single model in such environments without additional labelling costs. Our approach, namely continual learning approach for many processes (CLAMP), integrates a class-aware adversarial domain adaptation strategy to align a source domain and a target domain. An assessor-guided learning process is put forward to navigate the learning process of a base model assigning a set of weights to every sample controlling the influence of every sample and the interactions of each loss function in such a way to balance the stability and plasticity dilemma thus preventing the CF problem. The first assessor focuses on the negative transfer problem rejecting irrelevant samples of the source domain while the second assessor prevents noisy pseudo labels of the target domain. Both assessors are trained in the meta-learning approach using random transformation techniques and similar samples of the source domain. Theoretical analysis and extensive numerical validations demonstrate that CLAMP significantly outperforms established baseline algorithms across all experiments by at least $10\\%$ margin.","sentences":["Artificial neural networks, celebrated for their human-like cognitive learning abilities, often encounter the well-known catastrophic forgetting (CF) problem, where the neural networks lose the proficiency in previously acquired knowledge.","Despite numerous efforts to mitigate CF, it remains the significant challenge particularly in complex changing environments.","This challenge is even more pronounced in cross-domain adaptation following the continual learning (CL) setting, which is a more challenging and realistic scenario that is under-explored.","To this end, this article proposes a cross-domain CL approach making possible to deploy a single model in such environments without additional labelling costs.","Our approach, namely continual learning approach for many processes (CLAMP), integrates a class-aware adversarial domain adaptation strategy to align a source domain and a target domain.","An assessor-guided learning process is put forward to navigate the learning process of a base model assigning a set of weights to every sample controlling the influence of every sample and the interactions of each loss function in such a way to balance the stability and plasticity dilemma thus preventing the CF problem.","The first assessor focuses on the negative transfer problem rejecting irrelevant samples of the source domain while the second assessor prevents noisy pseudo labels of the target domain.","Both assessors are trained in the meta-learning approach using random transformation techniques and similar samples of the source domain.","Theoretical analysis and extensive numerical validations demonstrate that CLAMP significantly outperforms established baseline algorithms across all experiments by at least $10\\%$ margin."],"url":"http://arxiv.org/abs/2405.07142v1","category":"cs.LG"}
{"created":"2024-05-12 02:38:58","title":"Edge Intelligence Optimization for Large Language Model Inference with Batching and Quantization","abstract":"Generative Artificial Intelligence (GAI) is taking the world by storm with its unparalleled content creation ability. Large Language Models (LLMs) are at the forefront of this movement. However, the significant resource demands of LLMs often require cloud hosting, which raises issues regarding privacy, latency, and usage limitations. Although edge intelligence has long been utilized to solve these challenges by enabling real-time AI computation on ubiquitous edge resources close to data sources, most research has focused on traditional AI models and has left a gap in addressing the unique characteristics of LLM inference, such as considerable model size, auto-regressive processes, and self-attention mechanisms. In this paper, we present an edge intelligence optimization problem tailored for LLM inference. Specifically, with the deployment of the batching technique and model quantization on resource-limited edge devices, we formulate an inference model for transformer decoder-based LLMs. Furthermore, our approach aims to maximize the inference throughput via batch scheduling and joint allocation of communication and computation resources, while also considering edge resource constraints and varying user requirements of latency and accuracy. To address this NP-hard problem, we develop an optimal Depth-First Tree-Searching algorithm with online tree-Pruning (DFTSP) that operates within a feasible time complexity. Simulation results indicate that DFTSP surpasses other batching benchmarks in throughput across diverse user settings and quantization techniques, and it reduces time complexity by over 45% compared to the brute-force searching method.","sentences":["Generative Artificial Intelligence (GAI) is taking the world by storm with its unparalleled content creation ability.","Large Language Models (LLMs) are at the forefront of this movement.","However, the significant resource demands of LLMs often require cloud hosting, which raises issues regarding privacy, latency, and usage limitations.","Although edge intelligence has long been utilized to solve these challenges by enabling real-time AI computation on ubiquitous edge resources close to data sources, most research has focused on traditional AI models and has left a gap in addressing the unique characteristics of LLM inference, such as considerable model size, auto-regressive processes, and self-attention mechanisms.","In this paper, we present an edge intelligence optimization problem tailored for LLM inference.","Specifically, with the deployment of the batching technique and model quantization on resource-limited edge devices, we formulate an inference model for transformer decoder-based LLMs.","Furthermore, our approach aims to maximize the inference throughput via batch scheduling and joint allocation of communication and computation resources, while also considering edge resource constraints and varying user requirements of latency and accuracy.","To address this NP-hard problem, we develop an optimal Depth-First Tree-Searching algorithm with online tree-Pruning (DFTSP) that operates within a feasible time complexity.","Simulation results indicate that DFTSP surpasses other batching benchmarks in throughput across diverse user settings and quantization techniques, and it reduces time complexity by over 45% compared to the brute-force searching method."],"url":"http://arxiv.org/abs/2405.07140v1","category":"cs.LG"}
{"created":"2024-05-12 02:15:26","title":"Combining multiple post-training techniques to achieve most efficient quantized LLMs","abstract":"Large Language Models (LLMs) have distinguished themselves with outstanding performance in complex language modeling tasks, yet they come with significant computational and storage challenges. This paper explores the potential of quantization to mitigate these challenges. We systematically study the combined application of two well-known post-training techniques, SmoothQuant and GPTQ, and provide a comprehensive analysis of their interactions and implications for advancing LLM quantization. We enhance the versatility of both techniques by enabling quantization to microscaling (MX) formats, expanding their applicability beyond their initial fixed-point format targets. We show that by applying GPTQ and SmoothQuant, and employing MX formats for quantizing models, we can achieve a significant reduction in the size of OPT models by up to 4x and LLaMA models by up to 3x with a negligible perplexity increase of 1-3%.","sentences":["Large Language Models (LLMs) have distinguished themselves with outstanding performance in complex language modeling tasks, yet they come with significant computational and storage challenges.","This paper explores the potential of quantization to mitigate these challenges.","We systematically study the combined application of two well-known post-training techniques, SmoothQuant and GPTQ, and provide a comprehensive analysis of their interactions and implications for advancing LLM quantization.","We enhance the versatility of both techniques by enabling quantization to microscaling (MX) formats, expanding their applicability beyond their initial fixed-point format targets.","We show that by applying GPTQ and SmoothQuant, and employing MX formats for quantizing models, we can achieve a significant reduction in the size of OPT models by up to 4x and LLaMA models by up to 3x with a negligible perplexity increase of 1-3%."],"url":"http://arxiv.org/abs/2405.07135v1","category":"cs.LG"}
{"created":"2024-05-12 01:45:14","title":"Two-Shot Optimization of Compositionally Complex Refractory Alloys","abstract":"In this paper, a synergistic computational/experimental approach is presented for the rapid discovery and characterization of novel alloys within the compositionally complex (i.e., \"medium/high entropy\") refractory alloy space of Ti-V-Nb-Mo-Hf-Ta-W. This was demonstrated via a material design cycle aimed at simultaneously maximizing the objective properties of high specific hardness (hardness normalized by density) and high specific elastic modulus (elastic modulus normalized by density). This framework utilizes high-throughput computational thermodynamics and intelligent filtering to first reduce the untenably large alloy space to a feasible size, followed by an iterative design cycle comprised of high-throughput synthesis, processing, and characterization in batch sizes of 24 alloys. After the first iteration, Bayesian optimization was utilized to inform selection of the next batch of 24 alloys. This paper demonstrates the benefit of using batch Bayesian optimization (BBO) in material design, as significant gains in the objective properties were observed after only two iterations or \"shots\" of the design cycle without using any prior knowledge or physical models of how the objective properties relate to the design inputs (i.e., composition). Specifically, the hypervolume of the Pareto front increased by 54% between the first and second iterations. Furthermore, 10 of the 24 alloys in the second iteration dominated all alloys from the first iteration.","sentences":["In this paper, a synergistic computational/experimental approach is presented for the rapid discovery and characterization of novel alloys within the compositionally complex (i.e., \"medium/high entropy\") refractory alloy space of Ti-V-Nb-Mo-Hf-Ta-W.","This was demonstrated via a material design cycle aimed at simultaneously maximizing the objective properties of high specific hardness (hardness normalized by density) and high specific elastic modulus (elastic modulus normalized by density).","This framework utilizes high-throughput computational thermodynamics and intelligent filtering to first reduce the untenably large alloy space to a feasible size, followed by an iterative design cycle comprised of high-throughput synthesis, processing, and characterization in batch sizes of 24 alloys.","After the first iteration, Bayesian optimization was utilized to inform selection of the next batch of 24 alloys.","This paper demonstrates the benefit of using batch Bayesian optimization (BBO) in material design, as significant gains in the objective properties were observed after only two iterations or \"shots\" of the design cycle without using any prior knowledge or physical models of how the objective properties relate to the design inputs (i.e., composition).","Specifically, the hypervolume of the Pareto front increased by 54% between the first and second iterations.","Furthermore, 10 of the 24 alloys in the second iteration dominated all alloys from the first iteration."],"url":"http://arxiv.org/abs/2405.07130v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-12 01:14:00","title":"CANA v1.0.0 and schematodes: efficient quantification of symmetry in Boolean automata","abstract":"The biomolecular networks underpinning cell function exhibit canalization, or the buffering of fluctuations required to function in a noisy environment. One understudied putative mechanism for canalization is the functional equivalence of a biomolecular entity's regulators (e.g., among the transcription factors for a gene). In these discrete dynamical systems, activation and inhibition of biomolecular entities (e.g., transcription of genes) are modeled as the activity of coupled 2-state automata, and thus the equivalence of regulators can be studied using the theory of symmetry in discrete functions. To this end, we present a new exact algorithm for finding maximal symmetry groups among the inputs to discrete functions. We implement this algorithm in Rust as a Python package, schematodes. We include schematodes in the new CANA v1.0.0 release, an open source Python library for analyzing canalization in Boolean networks, which we also present here. We compare our exact method implemented in schematodes to the previously published inexact method used in earlier releases of CANA and find that schematodes significantly outperforms the prior method both in speed and accuracy. We also apply CANA v1.0.0 to study the symmetry properties of regulatory function from an ensemble of experimentally-supported Boolean networks from the Cell Collective. Using CANA v1.0.0, we find that the distribution of a previously reported symmetry parameter, $k_s/k$, is statistically significantly different in the Cell Collective than in random automata with the same in-degree and activation bias (Kolmogorov-Smirnov test, $p<0.001$). In particular, its spread is much wider than in our null model (IQR 0.31 vs IQR 0.20 with equal medians), demonstrating that the Cell Collective is enriched in functions with extreme symmetry or asymmetry.","sentences":["The biomolecular networks underpinning cell function exhibit canalization, or the buffering of fluctuations required to function in a noisy environment.","One understudied putative mechanism for canalization is the functional equivalence of a biomolecular entity's regulators (e.g., among the transcription factors for a gene).","In these discrete dynamical systems, activation and inhibition of biomolecular entities (e.g., transcription of genes) are modeled as the activity of coupled 2-state automata, and thus the equivalence of regulators can be studied using the theory of symmetry in discrete functions.","To this end, we present a new exact algorithm for finding maximal symmetry groups among the inputs to discrete functions.","We implement this algorithm in Rust as a Python package, schematodes.","We include schematodes in the new CANA v1.0.0 release, an open source Python library for analyzing canalization in Boolean networks, which we also present here.","We compare our exact method implemented in schematodes to the previously published inexact method used in earlier releases of CANA and find that schematodes significantly outperforms the prior method both in speed and accuracy.","We also apply CANA v1.0.0 to study the symmetry properties of regulatory function from an ensemble of experimentally-supported Boolean networks from the Cell Collective.","Using CANA v1.0.0, we find that the distribution of a previously reported symmetry parameter, $k_s/k$, is statistically significantly different in the Cell Collective than in random automata with the same in-degree and activation bias (Kolmogorov-Smirnov test, $p<0.001$).","In particular, its spread is much wider than in our null model (IQR 0.31 vs IQR 0.20 with equal medians), demonstrating that the Cell Collective is enriched in functions with extreme symmetry or asymmetry."],"url":"http://arxiv.org/abs/2405.07123v1","category":"q-bio.MN"}
{"created":"2024-05-13 17:53:51","title":"Enhancing Rover Mobility Monitoring: Autoencoder-driven Anomaly Detection for Curiosity","abstract":"Over eleven years into its mission, the Mars Science Laboratory remains vital to NASA's Mars exploration. Safeguarding the rover's long-term functionality is a top mission priority. In this study, we introduce and test undercomplete autoencoder models for detecting drive anomalies, using telemetry data from wheel actuators, the Rover Inertial Measurement Unit (RIMU), and the suspension system. Our approach enhances post-drive data analysis during tactical downlink sessions. We explore various model architectures and input features to understand their impact on performance. Evaluating the models involves testing them on unseen data to mimic real-world scenarios. Our experiments demonstrate the undercomplete autoencoder model's effectiveness in detecting drive anomalies within the Curiosity rover dataset. Remarkably, the model even identifies subtle anomalous telemetry patterns missed by human operators. Additionally, we provide insights into optimal design choices by comparing different model architectures and input features. The model's ability to capture inconspicuous anomalies, potentially indicating early-stage failures, holds promise for the field, by improving the reliability and safety of future planetary exploration missions through early anomaly detection and proactive maintenance.","sentences":["Over eleven years into its mission, the Mars Science Laboratory remains vital to NASA's Mars exploration.","Safeguarding the rover's long-term functionality is a top mission priority.","In this study, we introduce and test undercomplete autoencoder models for detecting drive anomalies, using telemetry data from wheel actuators, the Rover Inertial Measurement Unit (RIMU), and the suspension system.","Our approach enhances post-drive data analysis during tactical downlink sessions.","We explore various model architectures and input features to understand their impact on performance.","Evaluating the models involves testing them on unseen data to mimic real-world scenarios.","Our experiments demonstrate the undercomplete autoencoder model's effectiveness in detecting drive anomalies within the Curiosity rover dataset.","Remarkably, the model even identifies subtle anomalous telemetry patterns missed by human operators.","Additionally, we provide insights into optimal design choices by comparing different model architectures and input features.","The model's ability to capture inconspicuous anomalies, potentially indicating early-stage failures, holds promise for the field, by improving the reliability and safety of future planetary exploration missions through early anomaly detection and proactive maintenance."],"url":"http://arxiv.org/abs/2405.07982v1","category":"cs.RO"}
{"created":"2024-05-13 17:53:35","title":"Diagnosing and Predicting Autonomous Vehicle Operational Safety Using Multiple Simulation Modalities and a Virtual Environment","abstract":"Even as technology and performance gains are made in the sphere of automated driving, safety concerns remain. Vehicle simulation has long been seen as a tool to overcome the cost associated with a massive amount of on-road testing for development and discovery of safety critical \"edge-cases\". However, purely software-based vehicle models may leave a large realism gap between their real-world counterparts in terms of dynamic response, and highly realistic vehicle-in-the-loop (VIL) simulations that encapsulate a virtual world around a physical vehicle may still be quite expensive to produce and similarly time intensive as on-road testing. In this work, we demonstrate an AV simulation test bed that combines the realism of vehicle-in-the-loop (VIL) simulation with the ease of implementation of model-in-the-loop (MIL) simulation. The setup demonstrated in this work allows for response diagnosis for the VIL simulations. By observing causal links between virtual weather and lighting conditions that surround the virtual depiction of our vehicle, the vision-based perception model and controller of Openpilot, and the dynamic response of our physical vehicle under test, we can draw conclusions regarding how the perceived environment contributed to vehicle response. Conversely, we also demonstrate response prediction for the MIL setup, where the need for a physical vehicle is not required to draw richer conclusions around the impact of environmental conditions on AV performance than could be obtained with VIL simulation alone. These combine for a simulation setup with accurate real-world implications for edge-case discovery that is both cost effective and time efficient to implement.","sentences":["Even as technology and performance gains are made in the sphere of automated driving, safety concerns remain.","Vehicle simulation has long been seen as a tool to overcome the cost associated with a massive amount of on-road testing for development and discovery of safety critical \"edge-cases\".","However, purely software-based vehicle models may leave a large realism gap between their real-world counterparts in terms of dynamic response, and highly realistic vehicle-in-the-loop (VIL) simulations that encapsulate a virtual world around a physical vehicle may still be quite expensive to produce and similarly time intensive as on-road testing.","In this work, we demonstrate an AV simulation test bed that combines the realism of vehicle-in-the-loop (VIL) simulation with the ease of implementation of model-in-the-loop (MIL) simulation.","The setup demonstrated in this work allows for response diagnosis for the VIL simulations.","By observing causal links between virtual weather and lighting conditions that surround the virtual depiction of our vehicle, the vision-based perception model and controller of Openpilot, and the dynamic response of our physical vehicle under test, we can draw conclusions regarding how the perceived environment contributed to vehicle response.","Conversely, we also demonstrate response prediction for the MIL setup, where the need for a physical vehicle is not required to draw richer conclusions around the impact of environmental conditions on AV performance than could be obtained with VIL simulation alone.","These combine for a simulation setup with accurate real-world implications for edge-case discovery that is both cost effective and time efficient to implement."],"url":"http://arxiv.org/abs/2405.07981v1","category":"cs.RO"}
{"created":"2024-05-13 17:46:30","title":"Fast Computation of Superquantile-Constrained Optimization Through Implicit Scenario Reduction","abstract":"Superquantiles have recently gained significant interest as a risk-aware metric for addressing fairness and distribution shifts in statistical learning and decision making problems. This paper introduces a fast, scalable and robust second-order computational framework to solve large-scale optimization problems with superquantile-based constraints. Unlike empirical risk minimization, superquantile-based optimization requires ranking random functions evaluated across all scenarios to compute the tail conditional expectation. While this tail-based feature might seem computationally unfriendly, it provides an advantageous setting for a semismooth-Newton-based augmented Lagrangian method. The superquantile operator effectively reduces the dimensions of the Newton systems since the tail expectation involves considerably fewer scenarios. Notably, the extra cost of obtaining relevant second-order information and performing matrix inversions is often comparable to, and sometimes even less than, the effort required for gradient computation. Our developed solver is particularly effective when the number of scenarios substantially exceeds the number of decision variables. In synthetic problems with linear and convex diagonal quadratic objectives, numerical experiments demonstrate that our method outperforms existing approaches by a large margin: It achieves speeds more than 750 times faster for linear and quadratic objectives than the alternating direction method of multipliers as implemented by OSQP for computing low-accuracy solutions. Additionally, it is up to 25 times faster for linear objectives and 70 times faster for quadratic objectives than the commercial solver Gurobi, and 20 times faster for linear objectives and 30 times faster for quadratic objectives than the Portfolio Safeguard optimization suite for high-accuracy solution computations.","sentences":["Superquantiles have recently gained significant interest as a risk-aware metric for addressing fairness and distribution shifts in statistical learning and decision making problems.","This paper introduces a fast, scalable and robust second-order computational framework to solve large-scale optimization problems with superquantile-based constraints.","Unlike empirical risk minimization, superquantile-based optimization requires ranking random functions evaluated across all scenarios to compute the tail conditional expectation.","While this tail-based feature might seem computationally unfriendly, it provides an advantageous setting for a semismooth-Newton-based augmented Lagrangian method.","The superquantile operator effectively reduces the dimensions of the Newton systems since the tail expectation involves considerably fewer scenarios.","Notably, the extra cost of obtaining relevant second-order information and performing matrix inversions is often comparable to, and sometimes even less than, the effort required for gradient computation.","Our developed solver is particularly effective when the number of scenarios substantially exceeds the number of decision variables.","In synthetic problems with linear and convex diagonal quadratic objectives, numerical experiments demonstrate that our method outperforms existing approaches by a large margin: It achieves speeds more than 750 times faster for linear and quadratic objectives than the alternating direction method of multipliers as implemented by OSQP for computing low-accuracy solutions.","Additionally, it is up to 25 times faster for linear objectives and 70 times faster for quadratic objectives than the commercial solver Gurobi, and 20 times faster for linear objectives and 30 times faster for quadratic objectives than the Portfolio Safeguard optimization suite for high-accuracy solution computations."],"url":"http://arxiv.org/abs/2405.07965v1","category":"math.OC"}
{"created":"2024-05-13 17:35:14","title":"An adjoint-based approach for the surgical correction of nasal septal deviations","abstract":"Deviations of the septal wall are widespread anatomic anomalies of the human nose; they vary significantly in shape and location, and often cause the obstruction of the nasal airways. When severe, septal deviations need to be surgically corrected by ear-nose-throat (ENT) specialists. Septoplasty, however, has a low success rate, owing to the lack of suitable standardized clinical tools for assessing type and severity of obstructions, and for surgery planning. Moreover, the restoration of a perfectly straight septal wall is often impossible and possibly unnecessary. This paper introduces a procedure, based on advanced patient-specific Computational Fluid Dynamics (CFD) simulations, to support ENT surgeons in septoplasty planning. The method hinges upon the theory of adjoint-based optimization, and minimizes a cost function that indirectly accounts for viscous losses. A sensitivity map is computed on the mucosal wall to provide the surgeon with a simple quantification of how much tissue removal at each location would contribute to easing the obstruction. The optimization procedure is applied to three representative nasal anatomies, reconstructed from CT scans of patients affected by complex septal deviations. The computed sensitivity consistently identifies all the anomalies correctly. Virtual surgery, i.e. morphing of the anatomies according to the computed sensitivity, confirms that the characteristics of the nasal airflow improve significantly after small anatomy changes derived from adjoint-based optimization.","sentences":["Deviations of the septal wall are widespread anatomic anomalies of the human nose; they vary significantly in shape and location, and often cause the obstruction of the nasal airways.","When severe, septal deviations need to be surgically corrected by ear-nose-throat (ENT) specialists.","Septoplasty, however, has a low success rate, owing to the lack of suitable standardized clinical tools for assessing type and severity of obstructions, and for surgery planning.","Moreover, the restoration of a perfectly straight septal wall is often impossible and possibly unnecessary.","This paper introduces a procedure, based on advanced patient-specific Computational Fluid Dynamics (CFD) simulations, to support ENT surgeons in septoplasty planning.","The method hinges upon the theory of adjoint-based optimization, and minimizes a cost function that indirectly accounts for viscous losses.","A sensitivity map is computed on the mucosal wall to provide the surgeon with a simple quantification of how much tissue removal at each location would contribute to easing the obstruction.","The optimization procedure is applied to three representative nasal anatomies, reconstructed from CT scans of patients affected by complex septal deviations.","The computed sensitivity consistently identifies all the anomalies correctly.","Virtual surgery, i.e. morphing of the anatomies according to the computed sensitivity, confirms that the characteristics of the nasal airflow improve significantly after small anatomy changes derived from adjoint-based optimization."],"url":"http://arxiv.org/abs/2405.07959v1","category":"physics.flu-dyn"}
{"created":"2024-05-13 17:28:01","title":"On the Decidability of Monadic Second-Order Logic with Arithmetic Predicates","abstract":"We investigate the decidability of the monadic second-order (MSO) theory of the structure $\\langle \\mathbb{N};<,P_1, \\ldots,P_k \\rangle$, for various unary predicates $P_1,\\ldots,P_k \\subseteq \\mathbb{N}$. We focus in particular on \"arithmetic\" predicates arising in the study of linear recurrence sequences, such as fixed-base powers $\\mathsf{Pow}_k = \\{k^n : n \\in \\mathbb{N}\\}$, $k$-th powers $\\mathsf{N}_k = \\{n^k : n \\in \\mathbb{N}\\}$, and the set of terms of the Fibonacci sequence $\\mathsf{Fib} = \\{0,1,2,3,5,8,13,\\ldots\\}$ (and similarly for other linear recurrence sequences having a single, non-repeated, dominant characteristic root). We obtain several new unconditional and conditional decidability results, a select sample of which are the following:   $\\bullet$ The MSO theory of $\\langle \\mathbb{N};<,\\mathsf{Pow}_2, \\mathsf{Fib} \\rangle$ is decidable;   $\\bullet$ The MSO theory of $\\langle \\mathbb{N};<, \\mathsf{Pow}_2, \\mathsf{Pow}_3, \\mathsf{Pow}_6 \\rangle$ is decidable;   $\\bullet$ The MSO theory of $\\langle \\mathbb{N};<, \\mathsf{Pow}_2, \\mathsf{Pow}_3, \\mathsf{Pow}_5 \\rangle$ is decidable assuming Schanuel's conjecture;   $\\bullet$ The MSO theory of $\\langle \\mathbb{N};<, \\mathsf{Pow}_4, \\mathsf{N}_2 \\rangle$ is decidable;   $\\bullet$ The MSO theory of $\\langle \\mathbb{N};<, \\mathsf{Pow}_2, \\mathsf{N}_2 \\rangle$ is Turing-equivalent to the MSO theory of $\\langle \\mathbb{N};<,S \\rangle$, where $S$ is the predicate corresponding to the binary expansion of $\\sqrt{2}$. (As the binary expansion of $\\sqrt{2}$ is widely believed to be normal, the corresponding MSO theory is in turn expected to be decidable.)   These results are obtained by exploiting and combining techniques from dynamical systems, number theory, and automata theory.","sentences":["We investigate the decidability of the monadic second-order (MSO) theory of the structure $\\langle \\mathbb{N};<,P_1, \\ldots,P_k \\rangle$, for various unary predicates $P_1,\\ldots,P_k \\subseteq \\mathbb{N}$. We focus in particular on \"arithmetic\" predicates arising in the study of linear recurrence sequences, such as fixed-base powers $\\mathsf{Pow}_k = \\{k^n :","n \\in \\mathbb{N}\\}$, $k$-th powers $\\mathsf{N}_k = \\{n^k : n \\in \\mathbb{N}\\}$, and the set of terms of the Fibonacci sequence $\\mathsf{Fib} = \\{0,1,2,3,5,8,13,\\ldots\\}$ (and similarly for other linear recurrence sequences having a single, non-repeated, dominant characteristic root).","We obtain several new unconditional and conditional decidability results, a select sample of which are the following:   $\\bullet$ The MSO theory of $\\langle \\mathbb{N};<,\\mathsf{Pow}_2, \\mathsf{Fib} \\rangle$ is decidable;   $\\bullet$ The MSO theory of $\\langle \\mathbb{N};<, \\mathsf{Pow}_2, \\mathsf{Pow}_3, \\mathsf{Pow}_6 \\rangle$ is decidable;   $\\bullet$ The MSO theory of $\\langle \\mathbb{N};<, \\mathsf{Pow}_2, \\mathsf{Pow}_3, \\mathsf{Pow}_5 \\rangle$ is decidable assuming Schanuel's conjecture;   $\\bullet$ The MSO theory of $\\langle \\mathbb{N};<, \\mathsf{Pow}_4, \\mathsf{N}_2 \\rangle$ is decidable;   $\\bullet$ The MSO theory of $\\langle \\mathbb{N};<, \\mathsf{Pow}_2, \\mathsf{N}_2 \\rangle$ is Turing-equivalent to the MSO theory of $\\langle \\mathbb{N};<,S \\rangle$, where $S$ is the predicate corresponding to the binary expansion of $\\sqrt{2}$. (As the binary expansion of $\\sqrt{2}$ is widely believed to be normal, the corresponding MSO theory is in turn expected to be decidable.)   ","These results are obtained by exploiting and combining techniques from dynamical systems, number theory, and automata theory."],"url":"http://arxiv.org/abs/2405.07953v1","category":"cs.LO"}
{"created":"2024-05-13 17:27:16","title":"Scattering of the Toda system and the Gaussian $\u03b2$-ensemble","abstract":"The classical Toda flow is a well-known integrable Hamiltonian system that diagonalizes matrices. By keeping track of the distribution of entries and precise scattering asymptotics, one can exhibit matrix models for log-gases on the real line. These types of scattering asymptotics date back to fundamental work of Moser.   More precisely, using the classical Toda flow acting on symmetric real tridiagonal matrices, we give a \"symplectic\" proof of the fact that the Dumitriu-Edelman tridiagonal model has a spectrum following the Gaussian $\\beta$-ensemble.","sentences":["The classical Toda flow is a well-known integrable Hamiltonian system that diagonalizes matrices.","By keeping track of the distribution of entries and precise scattering asymptotics, one can exhibit matrix models for log-gases on the real line.","These types of scattering asymptotics date back to fundamental work of Moser.   ","More precisely, using the classical Toda flow acting on symmetric real tridiagonal matrices, we give a \"symplectic\" proof of the fact that the Dumitriu-Edelman tridiagonal model has a spectrum following the Gaussian $\\beta$-ensemble."],"url":"http://arxiv.org/abs/2405.07951v1","category":"nlin.SI"}
{"created":"2024-05-13 17:25:58","title":"Quantum-like states on complex synchronized networks","abstract":"Recent work has exposed the idea that interesting quantum-like probability laws, including interference effects, can be manifest in classical systems. Here we propose a model for quantum-like (QL) states and QL bits. We suggest a way that huge, complex systems can host robust states that can process information in a QL fashion. Axioms that such states should satisfy are proposed. Specifically, it is shown that building blocks suited for QL states are networks, possibly very complex, that we defined based on $k$-regular random graphs. These networks can dynamically encode a lot of information that is distilled into the emergent states we can use for QL like processing. Although the emergent states are classical, they have properties analogous to quantum states. Concrete examples of how QL functions are possible are given. The possibility of a `QL advantage' for computing-type operations and the potential relevance for new kinds of function in the brain are discussed and left as open questions.","sentences":["Recent work has exposed the idea that interesting quantum-like probability laws, including interference effects, can be manifest in classical systems.","Here we propose a model for quantum-like (QL) states and QL bits.","We suggest a way that huge, complex systems can host robust states that can process information in a QL fashion.","Axioms that such states should satisfy are proposed.","Specifically, it is shown that building blocks suited for QL states are networks, possibly very complex, that we defined based on $k$-regular random graphs.","These networks can dynamically encode a lot of information that is distilled into the emergent states we can use for QL like processing.","Although the emergent states are classical, they have properties analogous to quantum states.","Concrete examples of how QL functions are possible are given.","The possibility of a `QL advantage' for computing-type operations and the potential relevance for new kinds of function in the brain are discussed and left as open questions."],"url":"http://arxiv.org/abs/2405.07950v1","category":"physics.soc-ph"}
{"created":"2024-05-13 17:18:11","title":"Optimization Using Pathwise Algorithmic Derivatives of Electromagnetic Shower Simulations","abstract":"Among the well-known methods to approximate derivatives of expectancies computed by Monte-Carlo simulations, averages of pathwise derivatives are often the easiest one to apply. Computing them via algorithmic differentiation typically does not require major manual analysis and rewriting of the code, even for very complex programs like simulations of particle-detector interactions in high-energy physics. However, the pathwise derivative estimator can be biased if there are discontinuities in the program, which may diminish its value for applications.   This work integrates algorithmic differentiation into the electromagnetic shower simulation code HepEmShow based on G4HepEm, allowing us to study how well pathwise derivatives approximate derivatives of energy depositions in a sampling calorimeter with respect to parameters of the beam and geometry. We found that when multiple scattering is disabled in the simulation, means of pathwise derivatives converge quickly to their expected values, and these are close to the actual derivatives of the energy deposition. Additionally, we demonstrate the applicability of this novel gradient estimator for stochastic gradient-based optimization in a model example.","sentences":["Among the well-known methods to approximate derivatives of expectancies computed by Monte-Carlo simulations, averages of pathwise derivatives are often the easiest one to apply.","Computing them via algorithmic differentiation typically does not require major manual analysis and rewriting of the code, even for very complex programs like simulations of particle-detector interactions in high-energy physics.","However, the pathwise derivative estimator can be biased if there are discontinuities in the program, which may diminish its value for applications.   ","This work integrates algorithmic differentiation into the electromagnetic shower simulation code HepEmShow based on G4HepEm, allowing us to study how well pathwise derivatives approximate derivatives of energy depositions in a sampling calorimeter with respect to parameters of the beam and geometry.","We found that when multiple scattering is disabled in the simulation, means of pathwise derivatives converge quickly to their expected values, and these are close to the actual derivatives of the energy deposition.","Additionally, we demonstrate the applicability of this novel gradient estimator for stochastic gradient-based optimization in a model example."],"url":"http://arxiv.org/abs/2405.07944v1","category":"physics.comp-ph"}
{"created":"2024-05-13 17:13:47","title":"EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning","abstract":"In this paper, we introduce EconLogicQA, a rigorous benchmark designed to assess the sequential reasoning capabilities of large language models (LLMs) within the intricate realms of economics, business, and supply chain management. Diverging from traditional benchmarks that predict subsequent events individually, EconLogicQA poses a more challenging task: it requires models to discern and sequence multiple interconnected events, capturing the complexity of economic logics. EconLogicQA comprises an array of multi-event scenarios derived from economic articles, which necessitate an insightful understanding of both temporal and logical event relationships. Through comprehensive evaluations, we exhibit that EconLogicQA effectively gauges a LLM's proficiency in navigating the sequential complexities inherent in economic contexts. We provide a detailed description of EconLogicQA dataset and shows the outcomes from evaluating the benchmark across various leading-edge LLMs, thereby offering a thorough perspective on their sequential reasoning potential in economic contexts. Our benchmark dataset is available at https://huggingface.co/datasets/yinzhu-quan/econ_logic_qa.","sentences":["In this paper, we introduce EconLogicQA, a rigorous benchmark designed to assess the sequential reasoning capabilities of large language models (LLMs) within the intricate realms of economics, business, and supply chain management.","Diverging from traditional benchmarks that predict subsequent events individually, EconLogicQA poses a more challenging task: it requires models to discern and sequence multiple interconnected events, capturing the complexity of economic logics.","EconLogicQA comprises an array of multi-event scenarios derived from economic articles, which necessitate an insightful understanding of both temporal and logical event relationships.","Through comprehensive evaluations, we exhibit that EconLogicQA effectively gauges a LLM's proficiency in navigating the sequential complexities inherent in economic contexts.","We provide a detailed description of EconLogicQA dataset and shows the outcomes from evaluating the benchmark across various leading-edge LLMs, thereby offering a thorough perspective on their sequential reasoning potential in economic contexts.","Our benchmark dataset is available at https://huggingface.co/datasets/yinzhu-quan/econ_logic_qa."],"url":"http://arxiv.org/abs/2405.07938v1","category":"cs.CL"}
{"created":"2024-05-13 17:12:33","title":"A Perspective on the Foundations of Derived Analytic Geometry","abstract":"We show how one can do algebraic geometry with respect to the category of simplicial objects in an exact category. As a biproduct, we get a theory of derived analytic geometry.","sentences":["We show how one can do algebraic geometry with respect to the category of simplicial objects in an exact category.","As a biproduct, we get a theory of derived analytic geometry."],"url":"http://arxiv.org/abs/2405.07936v1","category":"math.AG"}
{"created":"2024-05-13 17:01:28","title":"Improving Multimodal Learning with Multi-Loss Gradient Modulation","abstract":"Learning from multiple modalities, such as audio and video, offers opportunities for leveraging complementary information, enhancing robustness, and improving contextual understanding and performance. However, combining such modalities presents challenges, especially when modalities differ in data structure, predictive contribution, and the complexity of their learning processes. It has been observed that one modality can potentially dominate the learning process, hindering the effective utilization of information from other modalities and leading to sub-optimal model performance. To address this issue the vast majority of previous works suggest to assess the unimodal contributions and dynamically adjust the training to equalize them. We improve upon previous work by introducing a multi-loss objective and further refining the balancing process, allowing it to dynamically adjust the learning pace of each modality in both directions, acceleration and deceleration, with the ability to phase out balancing effects upon convergence. We achieve superior results across three audio-video datasets: on CREMA-D, models with ResNet backbone encoders surpass the previous best by 1.9% to 12.4%, and Conformer backbone models deliver improvements ranging from 2.8% to 14.1% across different fusion methods. On AVE, improvements range from 2.7% to 7.7%, while on UCF101, gains reach up to 6.1%.","sentences":["Learning from multiple modalities, such as audio and video, offers opportunities for leveraging complementary information, enhancing robustness, and improving contextual understanding and performance.","However, combining such modalities presents challenges, especially when modalities differ in data structure, predictive contribution, and the complexity of their learning processes.","It has been observed that one modality can potentially dominate the learning process, hindering the effective utilization of information from other modalities and leading to sub-optimal model performance.","To address this issue the vast majority of previous works suggest to assess the unimodal contributions and dynamically adjust the training to equalize them.","We improve upon previous work by introducing a multi-loss objective and further refining the balancing process, allowing it to dynamically adjust the learning pace of each modality in both directions, acceleration and deceleration, with the ability to phase out balancing effects upon convergence.","We achieve superior results across three audio-video datasets: on CREMA-D, models with ResNet backbone encoders surpass the previous best by 1.9% to 12.4%, and Conformer backbone models deliver improvements ranging from 2.8% to 14.1% across different fusion methods.","On AVE, improvements range from 2.7% to 7.7%, while on UCF101, gains reach up to 6.1%."],"url":"http://arxiv.org/abs/2405.07930v1","category":"cs.MM"}
{"created":"2024-05-13 17:01:26","title":"On the Smooth Curve of Entire Vector Fields that Solves the Navier-Stokes Equation","abstract":"In this paper we prove the existence and smoothness of the Navier-Stokes Equation for viscosity large enough which after rescaling implies a solution for any positive viscosity, additionally, we show the existence of a curve of entire vector fields of order 2 that extends the solution to the complex domain for positive time.","sentences":["In this paper we prove the existence and smoothness of the Navier-Stokes Equation for viscosity large enough which after rescaling implies a solution for any positive viscosity, additionally, we show the existence of a curve of entire vector fields of order 2 that extends the solution to the complex domain for positive time."],"url":"http://arxiv.org/abs/2405.07929v1","category":"math.AP"}
{"created":"2024-05-13 16:58:58","title":"Topological Interlayer Superconductivity in a van der Waals Heterostructure","abstract":"We show that when a honeycomb antiferromagnetic insulator (AFMI) is sandwiched between two transition metal dichalcogenide (TMD) monolayers in a commensurate way, magnons in the AFMI can mediate an interaction between electrons in the TMDs that gives rise to interlayer Cooper pairing. This interaction opens coexisting extended $s$-wave and chiral $p$-wave superconducting gaps in the energy spectrum of the coupled system, and the latter give rise to topological Majorana edge modes.","sentences":["We show that when a honeycomb antiferromagnetic insulator (AFMI) is sandwiched between two transition metal dichalcogenide (TMD) monolayers in a commensurate way, magnons in the AFMI can mediate an interaction between electrons in the TMDs that gives rise to interlayer Cooper pairing.","This interaction opens coexisting extended $s$-wave and chiral $p$-wave superconducting gaps in the energy spectrum of the coupled system, and the latter give rise to topological Majorana edge modes."],"url":"http://arxiv.org/abs/2405.07927v1","category":"cond-mat.supr-con"}
{"created":"2024-05-13 16:50:42","title":"Exploring the Low-Pass Filtering Behavior in Image Super-Resolution","abstract":"Deep neural networks for image super-resolution have shown significant advantages over traditional approaches like interpolation. However, they are often criticized as `black boxes' compared to traditional approaches which have solid mathematical foundations. In this paper, we attempt to interpret the behavior of deep neural networks using theories from signal processing theories. We first report an intriguing phenomenon, referred to as `the sinc phenomenon,' which occurs when an impulse input is fed to a neural network. Building on this observation, we propose a method named Hybird Response Analysis (HyRA) to analyze the behavior of neural networks in image super-resolution tasks. In details, HyRA decomposes a neural network into a parallel connection of a linear system and a non-linear system, demonstrating that the linear system functions as a low-pass filter, while the non-linear system injects high-frequency information. Furthermore, to quantify the injected high-frequency information, we introduce a metric for image-to-image tasks called Frequency Spectrum Distribution Similarity (FSDS). FSDS reflects the distribution similarity of different frequency components, capturing nuances that traditional metrics may overlook. Code for this work can be found in: https://github.com/RisingEntropy/LPFInISR.","sentences":["Deep neural networks for image super-resolution have shown significant advantages over traditional approaches like interpolation.","However, they are often criticized as `black boxes' compared to traditional approaches which have solid mathematical foundations.","In this paper, we attempt to interpret the behavior of deep neural networks using theories from signal processing theories.","We first report an intriguing phenomenon, referred to as `the sinc phenomenon,' which occurs when an impulse input is fed to a neural network.","Building on this observation, we propose a method named Hybird Response Analysis (HyRA) to analyze the behavior of neural networks in image super-resolution tasks.","In details, HyRA decomposes a neural network into a parallel connection of a linear system and a non-linear system, demonstrating that the linear system functions as a low-pass filter, while the non-linear system injects high-frequency information.","Furthermore, to quantify the injected high-frequency information, we introduce a metric for image-to-image tasks called Frequency Spectrum Distribution Similarity (FSDS).","FSDS reflects the distribution similarity of different frequency components, capturing nuances that traditional metrics may overlook.","Code for this work can be found in: https://github.com/RisingEntropy/LPFInISR."],"url":"http://arxiv.org/abs/2405.07919v1","category":"cs.CV"}
{"created":"2024-05-13 16:48:57","title":"High-level Stream Processing: A Complementary Analysis of Fault Recovery","abstract":"Parallel computing is very important to accelerate the performance of software systems. Additionally, considering that a recurring challenge is to process high data volumes continuously, stream processing emerged as a paradigm and software architectural style. Several software systems rely on stream processing to deliver scalable performance, whereas open-source frameworks provide coding abstraction and high-level parallel computing. Although stream processing's performance is being extensively studied, the measurement of fault tolerance--a key abstraction offered by stream processing frameworks--has still not been adequately measured with comprehensive testbeds. In this work, we extend the previous fault recovery measurements with an exploratory analysis of the configuration space, additional experimental measurements, and analysis of improvement opportunities. We focus on robust deployment setups inspired by requirements for near real-time analytics of a large cloud observability platform. The results indicate significant potential for improving fault recovery and performance. However, these improvements entail grappling with configuration complexities, particularly in identifying and selecting the configurations to be fine-tuned and determining the appropriate values for them. Therefore, new abstractions for transparent configuration tuning are also needed for large-scale industry setups. We believe that more software engineering efforts are needed to provide insights into potential abstractions and how to achieve them. The stream processing community and industry practitioners could also benefit from more interactions with the high-level parallel programming community, whose expertise and insights on making parallel programming more productive and efficient could be extended.","sentences":["Parallel computing is very important to accelerate the performance of software systems.","Additionally, considering that a recurring challenge is to process high data volumes continuously, stream processing emerged as a paradigm and software architectural style.","Several software systems rely on stream processing to deliver scalable performance, whereas open-source frameworks provide coding abstraction and high-level parallel computing.","Although stream processing's performance is being extensively studied, the measurement of fault tolerance--a key abstraction offered by stream processing frameworks--has still not been adequately measured with comprehensive testbeds.","In this work, we extend the previous fault recovery measurements with an exploratory analysis of the configuration space, additional experimental measurements, and analysis of improvement opportunities.","We focus on robust deployment setups inspired by requirements for near real-time analytics of a large cloud observability platform.","The results indicate significant potential for improving fault recovery and performance.","However, these improvements entail grappling with configuration complexities, particularly in identifying and selecting the configurations to be fine-tuned and determining the appropriate values for them.","Therefore, new abstractions for transparent configuration tuning are also needed for large-scale industry setups.","We believe that more software engineering efforts are needed to provide insights into potential abstractions and how to achieve them.","The stream processing community and industry practitioners could also benefit from more interactions with the high-level parallel programming community, whose expertise and insights on making parallel programming more productive and efficient could be extended."],"url":"http://arxiv.org/abs/2405.07917v1","category":"cs.DC"}
{"created":"2024-05-13 16:47:05","title":"Distribution Learning Meets Graph Structure Sampling","abstract":"This work establishes a novel link between the problem of PAC-learning high-dimensional graphical models and the task of (efficient) counting and sampling of graph structures, using an online learning framework.   We observe that if we apply the exponentially weighted average (EWA) or randomized weighted majority (RWM) forecasters on a sequence of samples from a distribution P using the log loss function, the average regret incurred by the forecaster's predictions can be used to bound the expected KL divergence between P and the predictions. Known regret bounds for EWA and RWM then yield new sample complexity bounds for learning Bayes nets. Moreover, these algorithms can be made computationally efficient for several interesting classes of Bayes nets. Specifically, we give a new sample-optimal and polynomial time learning algorithm with respect to trees of unknown structure and the first polynomial sample and time algorithm for learning with respect to Bayes nets over a given chordal skeleton.","sentences":["This work establishes a novel link between the problem of PAC-learning high-dimensional graphical models and the task of (efficient) counting and sampling of graph structures, using an online learning framework.   ","We observe that if we apply the exponentially weighted average (EWA) or randomized weighted majority (RWM) forecasters on a sequence of samples from a distribution P using the log loss function, the average regret incurred by the forecaster's predictions can be used to bound the expected KL divergence between P and the predictions.","Known regret bounds for EWA and RWM then yield new sample complexity bounds for learning Bayes nets.","Moreover, these algorithms can be made computationally efficient for several interesting classes of Bayes nets.","Specifically, we give a new sample-optimal and polynomial time learning algorithm with respect to trees of unknown structure and the first polynomial sample and time algorithm for learning with respect to Bayes nets over a given chordal skeleton."],"url":"http://arxiv.org/abs/2405.07914v1","category":"cs.LG"}
{"created":"2024-05-13 16:46:17","title":"A Linear Prelle-Singer method","abstract":"The Prelle-Singer method allows determining an elementary first integral admitted by a polynomial vector field in the plane. It is a semi-algorithm whose nonlinear step consists of determining the Darboux polynomials of the vector field. In this article we construct a linear procedure to determine the Darboux polynomials present in the integrating factor of a polynomial vector field in the plane. Next, we extend the procedure to deal with rational 2ODEs that admit an elementary first integral","sentences":["The Prelle-Singer method allows determining an elementary first integral admitted by a polynomial vector field in the plane.","It is a semi-algorithm whose nonlinear step consists of determining the Darboux polynomials of the vector field.","In this article we construct a linear procedure to determine the Darboux polynomials present in the integrating factor of a polynomial vector field in the plane.","Next, we extend the procedure to deal with rational 2ODEs that admit an elementary first integral"],"url":"http://arxiv.org/abs/2405.07912v1","category":"math-ph"}
{"created":"2024-05-13 16:45:06","title":"Slice closures of indexed languages and word equations with counting constraints","abstract":"Indexed languages are a classical notion in formal language theory. As the language equivalent of second-order pushdown automata, they have received considerable attention in higher-order model checking. Unfortunately, counting properties are notoriously difficult to decide for indexed languages: So far, all results about non-regular counting properties show undecidability.   In this paper, we initiate the study of slice closures of (Parikh images of) indexed languages. A slice is a set of vectors of natural numbers such that membership of $u,u+v,u+w$ implies membership of $u+v+w$. Our main result is that given an indexed language $L$, one can compute a semilinear representation of the smallest slice containing $L$'s Parikh image.   We present two applications. First, one can compute the set of all affine relations satisfied by the Parikh image of an indexed language. In particular, this answers affirmatively a question by Kobayashi: Is it decidable whether in a given indexed language, every word has the same number of $a$'s as $b$'s.   As a second application, we show decidability of (systems of) word equations with rational constraints and a class of counting constraints: These allow us to look for solutions where a counting function (defined by an automaton) is not zero. For example, one can decide whether a word equation with rational constraints has a solution where the number of occurrences of $a$ differs between variables $X$ and $Y$.","sentences":["Indexed languages are a classical notion in formal language theory.","As the language equivalent of second-order pushdown automata, they have received considerable attention in higher-order model checking.","Unfortunately, counting properties are notoriously difficult to decide for indexed languages: So far, all results about non-regular counting properties show undecidability.   ","In this paper, we initiate the study of slice closures of (Parikh images of) indexed languages.","A slice is a set of vectors of natural numbers such that membership of $u,u+v,u+w$ implies membership of $u+v+w$. Our main result is that given an indexed language $L$, one can compute a semilinear representation of the smallest slice containing $L$'s Parikh image.   ","We present two applications.","First, one can compute the set of all affine relations satisfied by the Parikh image of an indexed language.","In particular, this answers affirmatively a question by Kobayashi:","Is it decidable whether in a given indexed language, every word has the same number of $a$'s as $b$'s.   ","As a second application, we show decidability of (systems of) word equations with rational constraints and a class of counting constraints: These allow us to look for solutions where a counting function (defined by an automaton) is not zero.","For example, one can decide whether a word equation with rational constraints has a solution where the number of occurrences of $a$ differs between variables $X$ and $Y$."],"url":"http://arxiv.org/abs/2405.07911v1","category":"cs.FL"}
{"created":"2024-05-13 16:41:14","title":"Improved Downlink Channel Estimation in Time-Varying FDD Massive MIMO Systems","abstract":"In this work, we address the challenge of accurately obtaining channel state information at the transmitter (CSIT) for frequency division duplexing (FDD) multiple input multiple output systems. Although CSIT is vital for maximizing spatial multiplexing gains, traditional CSIT estimation methods often suffer from impracticality due to the substantial training and feedback overhead they require. To address this challenge, we leverage two sources of prior information simultaneously: the presence of limited local scatterers at the base station (BS) and the time-varying characteristics of the channel. The former results in a redundant angular sparsity of users' channels exceeding the spatial dimension (i.e., the number of BS antennas), while the latter provides a prior non-uniform distribution in the angular domain. We propose a weighted optimization framework that simultaneously reflects both of these features. The optimal weights are then obtained by minimizing the expected recovery error of the optimization problem. This establishes an analytical closed-form relationship between the optimal weights and the angular domain characteristics. Numerical experiments verify the effectiveness of our proposed approach in reducing the recovery error and consequently resulting in decreased training and feedback overhead.","sentences":["In this work, we address the challenge of accurately obtaining channel state information at the transmitter (CSIT) for frequency division duplexing (FDD)","multiple input multiple output systems.","Although CSIT is vital for maximizing spatial multiplexing gains, traditional CSIT estimation methods often suffer from impracticality due to the substantial training and feedback overhead they require.","To address this challenge, we leverage two sources of prior information simultaneously: the presence of limited local scatterers at the base station (BS) and the time-varying characteristics of the channel.","The former results in a redundant angular sparsity of users' channels exceeding the spatial dimension (i.e., the number of BS antennas), while the latter provides a prior non-uniform distribution in the angular domain.","We propose a weighted optimization framework that simultaneously reflects both of these features.","The optimal weights are then obtained by minimizing the expected recovery error of the optimization problem.","This establishes an analytical closed-form relationship between the optimal weights and the angular domain characteristics.","Numerical experiments verify the effectiveness of our proposed approach in reducing the recovery error and consequently resulting in decreased training and feedback overhead."],"url":"http://arxiv.org/abs/2405.07906v1","category":"eess.SP"}
{"created":"2024-05-13 16:38:09","title":"Predicting State Transitions in Autonomous Nonlinear Bistable Systems with Hidden Stochasticity","abstract":"Bistable autonomous systems can be found inmany areas of science. When the intrinsic noise intensity is large, these systems exhibits stochastic transitions from onemetastable steady state to another. In electronic bistable memories, these transitions are failures, usually simulated in a Monte-Carlo fashion at a high CPU-time price. Existing closed-form formulas, relying on near-stable-steady-state approximations of the nonlinear system dynamics to estimate the mean transition time, have turned out inaccurate. Our contribution is twofold. From a unidimensional stochastic model of overdamped autonomous systems, we propose an extended Eyring-Kramers analytical formula accounting for both nonlinear drift and state-dependent white noise variance, rigorously derived from It\\^o stochastic calculus. We also adapt it to practical system engineering situations where the intrinsic noise sources are hidden and can only be inferred from the fluctuations of observables measured in steady states. First numerical trials on an industrial electronic case study suggest that our approximate prediction formula achieve remarkable accuracy, outperforming previous non-Monte-Carlo approaches.","sentences":["Bistable autonomous systems can be found inmany areas of science.","When the intrinsic noise intensity is large, these systems exhibits stochastic transitions from onemetastable steady state to another.","In electronic bistable memories, these transitions are failures, usually simulated in a Monte-Carlo fashion at a high CPU-time price.","Existing closed-form formulas, relying on near-stable-steady-state approximations of the nonlinear system dynamics to estimate the mean transition time, have turned out inaccurate.","Our contribution is twofold.","From a unidimensional stochastic model of overdamped autonomous systems, we propose an extended Eyring-Kramers analytical formula accounting for both nonlinear drift and state-dependent white noise variance, rigorously derived from It\\^o stochastic calculus.","We also adapt it to practical system engineering situations where the intrinsic noise sources are hidden and can only be inferred from the fluctuations of observables measured in steady states.","First numerical trials on an industrial electronic case study suggest that our approximate prediction formula achieve remarkable accuracy, outperforming previous non-Monte-Carlo approaches."],"url":"http://arxiv.org/abs/2405.07902v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-13 16:35:46","title":"Mechanically-driven growth and competition in a Voronoi model of tissues","abstract":"The mechanisms leading cells to acquire a fitness advantage and establish themselves in a population are paramount to understanding the development and growth of cancer. Although there are many works that study separately either the evolutionary dynamics or the mechanics of cancer, little has been done to couple evolutionary dynamics to mechanics. To address this question, we study a confluent model of tissue using a Self-Propelled Voronoi (SPV) model with stochastic growth rates that depend on the mechanical variables of the system. The SPV model is an out-of-equilibrium model of tissue derived from an energy functional that has a jamming/unjamming transition between solid-like and liquid-like states. By considering several scenarios of mutants invading a resident population in both phases, we determine the range of parameters that confer a fitness advantage and show that the preferred area and perimeter are the most relevant ones. We find that the liquid-like state is more resistant to invasion and show that the outcome of the competition can be determined from the simulation of a non-growing mixture. Moreover, a mean-field approximation can accurately predict the fate of a mutation affecting mechanical properties of a cell. Our results can be used to infer evolutionary dynamics from tissue images, understand cancer-suppressing effects of tissue mechanics, and even search for mechanics-based therapies.","sentences":["The mechanisms leading cells to acquire a fitness advantage and establish themselves in a population are paramount to understanding the development and growth of cancer.","Although there are many works that study separately either the evolutionary dynamics or the mechanics of cancer, little has been done to couple evolutionary dynamics to mechanics.","To address this question, we study a confluent model of tissue using a Self-Propelled Voronoi (SPV) model with stochastic growth rates that depend on the mechanical variables of the system.","The SPV model is an out-of-equilibrium model of tissue derived from an energy functional that has a jamming/unjamming transition between solid-like and liquid-like states.","By considering several scenarios of mutants invading a resident population in both phases, we determine the range of parameters that confer a fitness advantage and show that the preferred area and perimeter are the most relevant ones.","We find that the liquid-like state is more resistant to invasion and show that the outcome of the competition can be determined from the simulation of a non-growing mixture.","Moreover, a mean-field approximation can accurately predict the fate of a mutation affecting mechanical properties of a cell.","Our results can be used to infer evolutionary dynamics from tissue images, understand cancer-suppressing effects of tissue mechanics, and even search for mechanics-based therapies."],"url":"http://arxiv.org/abs/2405.07899v1","category":"physics.bio-ph"}
{"created":"2024-05-13 16:31:53","title":"Optimal Transmitter Design and Pilot Spacing in MIMO Non-Stationary Aging Channels","abstract":"This work considers an uplink wireless communication system where multiple users with multiple antennas transmit data frames over dynamic channels. Previous studies have shown that multiple transmit and receive antennas can substantially enhance the sum-capacity of all users when the channel is known at the transmitter and in the case of uncorrelated transmit and receive antennas. However, spatial correlations stemming from close proximity of transmit antennas and channel variation between pilot and data time slots, known as channel aging, can substantially degrade the transmission rate if they are not properly into account. In this work, we provide an analytical framework to concurrently exploit both of these features. Specifically, we first propose a beamforming framework to capture spatial correlations. Then, based on random matrix theory tools, we introduce a deterministic expression that approximates the average sum-capacity of all users. Subsequently, we obtain the optimal values of pilot spacing and beamforming vectors upon maximizing this expression. Simulation results show the impacts of path loss, velocity of mobile users and Rician factor on the resulting sum-capacity and underscore the efficacy of our methodology compared to prior works.","sentences":["This work considers an uplink wireless communication system where multiple users with multiple antennas transmit data frames over dynamic channels.","Previous studies have shown that multiple transmit and receive antennas can substantially enhance the sum-capacity of all users when the channel is known at the transmitter and in the case of uncorrelated transmit and receive antennas.","However, spatial correlations stemming from close proximity of transmit antennas and channel variation between pilot and data time slots, known as channel aging, can substantially degrade the transmission rate if they are not properly into account.","In this work, we provide an analytical framework to concurrently exploit both of these features.","Specifically, we first propose a beamforming framework to capture spatial correlations.","Then, based on random matrix theory tools, we introduce a deterministic expression that approximates the average sum-capacity of all users.","Subsequently, we obtain the optimal values of pilot spacing and beamforming vectors upon maximizing this expression.","Simulation results show the impacts of path loss, velocity of mobile users and Rician factor on the resulting sum-capacity and underscore the efficacy of our methodology compared to prior works."],"url":"http://arxiv.org/abs/2405.07895v1","category":"eess.SP"}
{"created":"2024-05-13 16:23:38","title":"A Second-Order Audio VCO-ADC with 103-dB-A Dynamic Range and Binary-Weighted Internal Architecture","abstract":"One of the limitations of conventional VCO-ADCs is the restriction to first-order noise shaping. True-VCO architectures have been proposed to increase the noise-shaping order by cascading several VCO integrators, but without requiring analog feedback loops. A high noise shaping order allows to reduce the input VCO frequency compared to a conventional VCO-ADC with similar dynamic range, which improves power consumption. Prior-art True-VCO architectures represent state variables either with a thermometer code or with a single-bit. Thermometer encoding is a natural choice when ring oscillators are selected as loop filter integrators. However, chip area restrictions force thermometer-encoded state variables to have few levels. A reduced number of levels in the state variables limits the dynamic range of True VCO-ADCs. In this paper, we show experimentally a second-order audio VCO-based ADC which uses ring oscillators as integrators but employs Gray and binary encoding for state variables. As a consequence, the complexity and area of the True-VCO architecture is reduced, breaking the barrier that limits the dynamic range of prior designs. The implemented chip shows a dynamic range of 103~dB achieving a peak SNDR of 76.5 dB-A with a power of 250 $\\mu$W occupying 0.095 $\\text{mm}^2$ in 130 nm CMOS.","sentences":["One of the limitations of conventional VCO-ADCs is the restriction to first-order noise shaping.","True-VCO architectures have been proposed to increase the noise-shaping order by cascading several VCO integrators, but without requiring analog feedback loops.","A high noise shaping order allows to reduce the input VCO frequency compared to a conventional VCO-ADC with similar dynamic range, which improves power consumption.","Prior-art True-VCO architectures represent state variables either with a thermometer code or with a single-bit.","Thermometer encoding is a natural choice when ring oscillators are selected as loop filter integrators.","However, chip area restrictions force thermometer-encoded state variables to have few levels.","A reduced number of levels in the state variables limits the dynamic range of True VCO-ADCs.","In this paper, we show experimentally a second-order audio VCO-based ADC which uses ring oscillators as integrators but employs Gray and binary encoding for state variables.","As a consequence, the complexity and area of the True-VCO architecture is reduced, breaking the barrier that limits the dynamic range of prior designs.","The implemented chip shows a dynamic range of 103~dB achieving a peak SNDR of 76.5 dB-A with a power of 250 $\\mu$W occupying 0.095 $\\text{mm}^2$ in 130 nm CMOS."],"url":"http://arxiv.org/abs/2405.07887v1","category":"eess.SY"}
{"created":"2024-05-13 16:16:57","title":"Exploiting Spatial and Temporal Correlations in Massive MIMO Systems Over Non-Stationary Aging Channels","abstract":"This work investigates a multi-user, multi-antenna uplink wireless system, where multiple users transmit signals to a base station. Previous research has explored the potential for linear growth in spectral efficiency by employing multiple transmit and receive antennas. This gain depends on the quality of channel state information and uncorrelated antennas. However, spatial correlations, arising from closely-spaced antennas, and channel aging effects, stemming from the difference between the channel at pilot and data time instances, can substantially counteract these benefits and degrade the transmission rate, especially in non-stationary environments. To address these challenges, this work introduces a real-time beamforming framework to compensate for the spatial correlation effect. A channel estimation scheme is then developed, leveraging temporal channel correlations and considering mobile device velocity and antenna spacing. Subsequently, an expression approximating the average spectral efficiency is obtained, dependent on pilot spacing, pilot and data powers, and beamforming vectors. By maximizing this expression, optimal parameters are identified. Numerical results reveal the effectiveness of the proposed approach compared to prior works. Moreover, optimal pilot spacing remains unaffected by interference components such as path loss and the velocity of interference users. The impact of interference components also diminishes with an increasing number of transmit antennas.","sentences":["This work investigates a multi-user, multi-antenna uplink wireless system, where multiple users transmit signals to a base station.","Previous research has explored the potential for linear growth in spectral efficiency by employing multiple transmit and receive antennas.","This gain depends on the quality of channel state information and uncorrelated antennas.","However, spatial correlations, arising from closely-spaced antennas, and channel aging effects, stemming from the difference between the channel at pilot and data time instances, can substantially counteract these benefits and degrade the transmission rate, especially in non-stationary environments.","To address these challenges, this work introduces a real-time beamforming framework to compensate for the spatial correlation effect.","A channel estimation scheme is then developed, leveraging temporal channel correlations and considering mobile device velocity and antenna spacing.","Subsequently, an expression approximating the average spectral efficiency is obtained, dependent on pilot spacing, pilot and data powers, and beamforming vectors.","By maximizing this expression, optimal parameters are identified.","Numerical results reveal the effectiveness of the proposed approach compared to prior works.","Moreover, optimal pilot spacing remains unaffected by interference components such as path loss and the velocity of interference users.","The impact of interference components also diminishes with an increasing number of transmit antennas."],"url":"http://arxiv.org/abs/2405.07882v1","category":"eess.SP"}
{"created":"2024-05-13 16:07:20","title":"Optimal accuracy for linear sets of equations with the graph Laplacian","abstract":"We show that certain Graph Laplacian linear sets of equations exhibit optimal accuracy, guaranteeing that the relative error is no larger than the norm of the relative residual and that optimality occurs for carefully chosen right-hand sides. Such sets of equations arise in PageRank and Markov chain theory. We establish new relationships among the PageRank teleportation parameter, the Markov chain discount, and approximations to linear sets of equations. The set of optimally accurate systems can be separated into two groups for an undirected graph -- those that achieve optimality asymptotically with the graph size and those that do not -- determined by the angle between the right-hand side of the linear system and the vector of all ones. We provide supporting numerical experiments.","sentences":["We show that certain Graph Laplacian linear sets of equations exhibit optimal accuracy, guaranteeing that the relative error is no larger than the norm of the relative residual and that optimality occurs for carefully chosen right-hand sides.","Such sets of equations arise in PageRank and Markov chain theory.","We establish new relationships among the PageRank teleportation parameter, the Markov chain discount, and approximations to linear sets of equations.","The set of optimally accurate systems can be separated into two groups for an undirected graph -- those that achieve optimality asymptotically with the graph size and those that do not -- determined by the angle between the right-hand side of the linear system and the vector of all ones.","We provide supporting numerical experiments."],"url":"http://arxiv.org/abs/2405.07877v1","category":"math.NA"}
{"created":"2024-05-13 15:58:20","title":"Conservative dielectric functions and electrical conductivities from the multicomponent Bhatnagar-Gross-Krook equation","abstract":"A considerable number of semi-empirical and first-principles models have been created to describe the dynamic response of a collisionally damped charged-particle system. However, known challenges persist for established dynamic structure factors (DSF), dielectric functions, and conductivities. For instance, the semi-empirical Drude-Smith conductivity [N.M. Smith, Phys. Rev. B 64, 155106 (2001)] lacks interpretability, and the first-principles Mermin dielectric function [N.D. Mermin, Phys. Rev. B, 1, 2362 (1970)] does not satisfy the frequency sum rule [G.S. Atwal and N.W. Ashcroft, Phys. Rev. B 65, 115109 (2002)]. In this work, starting from the multicomponent Bhatnagar-Gross-Krook (BGK) kinetic equation, we produce a multi-species susceptibility that conserves number and momentum, which we refer to as the ``completed Mermin'' susceptibility, and we explore its properties and uses. We show that the completed Mermin susceptibility satisfies the frequency sum (f-sum) rule. We compute the associated DSF and find that momentum conservation qualitatively impacts the DSF's shape for a carbon-contaminated deuterium and tritium plasma under NIF hot-spot conditions. In the appendices, we provide numerical implementations of the completed Mermin susceptibility, for the reader's convenience. Further, we produce a new non-Drude conductivity model, by taking the single-species limit and introducing free parameters in the terms that enforce number and momentum conservation. To illustrate how number and momentum conservation impact the dynamical conductivity shape, we apply our conductivity model to dynamical gold conductivity measurements [Z. Chen, et al., Nature communications, 12.1, 1638, (2021)]. Finally, comparing our model to the Drude-Smith conductivity model, we conclude that Smith's phenomenological parameter violates local number conservation.","sentences":["A considerable number of semi-empirical and first-principles models have been created to describe the dynamic response of a collisionally damped charged-particle system.","However, known challenges persist for established dynamic structure factors (DSF), dielectric functions, and conductivities.","For instance, the semi-empirical Drude-Smith conductivity [N.M. Smith, Phys.","Rev. B 64, 155106 (2001)] lacks interpretability, and the first-principles Mermin dielectric function [N.D. Mermin, Phys. Rev. B, 1, 2362 (1970)] does not satisfy the frequency sum rule [G.S. Atwal and N.W. Ashcroft, Phys.","Rev. B 65, 115109 (2002)].","In this work, starting from the multicomponent Bhatnagar-Gross-Krook (BGK) kinetic equation, we produce a multi-species susceptibility that conserves number and momentum, which we refer to as the ``completed Mermin'' susceptibility, and we explore its properties and uses.","We show that the completed Mermin susceptibility satisfies the frequency sum (f-sum) rule.","We compute the associated DSF and find that momentum conservation qualitatively impacts the DSF's shape for a carbon-contaminated deuterium and tritium plasma under NIF hot-spot conditions.","In the appendices, we provide numerical implementations of the completed Mermin susceptibility, for the reader's convenience.","Further, we produce a new non-Drude conductivity model, by taking the single-species limit and introducing free parameters in the terms that enforce number and momentum conservation.","To illustrate how number and momentum conservation impact the dynamical conductivity shape, we apply our conductivity model to dynamical gold conductivity measurements [Z. Chen, et al., Nature communications, 12.1, 1638, (2021)].","Finally, comparing our model to the Drude-Smith conductivity model, we conclude that Smith's phenomenological parameter violates local number conservation."],"url":"http://arxiv.org/abs/2405.07871v1","category":"physics.plasm-ph"}
{"created":"2024-05-13 15:55:40","title":"Complex structures on Three-point space","abstract":"We discuss notions of almost complex, complex and K\\\"{a}hler structures in the realm of non-commutative geometry and investigate them for a class of finite dimensional spectral triples on the three-point space. We classify all the almost complex structures on this non-commutative manifold, which also turn out to be complex structures, but none of them are K\\\"{a}hler in our sense.","sentences":["We discuss notions of almost complex, complex and K\\\"{a}hler structures in the realm of non-commutative geometry and investigate them for a class of finite dimensional spectral triples on the three-point space.","We classify all the almost complex structures on this non-commutative manifold, which also turn out to be complex structures, but none of them are K\\\"{a}hler in our sense."],"url":"http://arxiv.org/abs/2405.07866v1","category":"math.QA"}
{"created":"2024-05-13 15:42:17","title":"The oldest stars with low neutron-capture element abundances and origins in ancient dwarf galaxies","abstract":"We present a detailed chemical abundance and kinematic analysis of six extremely metal-poor ($-4.2 \\leq$ [Fe/H] $\\leq-$2.9) halo stars with very low neutron-capture abundances ([Sr/H] and [Ba/H]) based on high-resolution Magellan/MIKE spectra. Three of our stars have [Sr/Ba] and [Sr/H] ratios that resemble those of metal-poor stars in ultra-faint dwarf galaxies (UFDs). Since early UFDs may be the building blocks of the Milky Way, extremely metal-poor halo stars with low, UFD-like Sr and Ba abundances may thus be ancient stars from the earliest small galactic systems that were accreted by the proto-Milky Way. We label these objects as Small Accreted Stellar System (SASS) stars, and we find an additional 61 similar ones in the literature. A kinematic analysis of our sample and literature stars reveals them to be fast-moving halo objects, all with retrograde motion, indicating an accretion origin. Because SASS stars are much brighter than typical UFD stars, identifying them offers promising ways towards detailed studies of early star formation environments. From the chemical abundances of SASS stars, it appears that the earliest accreted systems were likely enriched by a few supernovae whose light element yields varied from system to system. Neutron-capture elements were sparsely produced and/or diluted, with $r$-process nucleosynthesis playing a role. These insights offer a glimpse into the early formation of the Galaxy. Using neutron-capture elements as a distinguishing criterion for early formation, we have access to a unique metal-poor population that consists of the oldest stars in the universe.","sentences":["We present a detailed chemical abundance and kinematic analysis of six extremely metal-poor ($-4.2 \\leq$ [Fe/H] $\\leq-$2.9) halo stars with very low neutron-capture abundances ([Sr/H] and [Ba/H]) based on high-resolution Magellan/MIKE spectra.","Three of our stars have [Sr/Ba] and [Sr/H] ratios that resemble those of metal-poor stars in ultra-faint dwarf galaxies (UFDs).","Since early UFDs may be the building blocks of the Milky Way, extremely metal-poor halo stars with low, UFD-like Sr and Ba abundances may thus be ancient stars from the earliest small galactic systems that were accreted by the proto-Milky Way.","We label these objects as Small Accreted Stellar System (SASS) stars, and we find an additional 61 similar ones in the literature.","A kinematic analysis of our sample and literature stars reveals them to be fast-moving halo objects, all with retrograde motion, indicating an accretion origin.","Because SASS stars are much brighter than typical UFD stars, identifying them offers promising ways towards detailed studies of early star formation environments.","From the chemical abundances of SASS stars, it appears that the earliest accreted systems were likely enriched by a few supernovae whose light element yields varied from system to system.","Neutron-capture elements were sparsely produced and/or diluted, with $r$-process nucleosynthesis playing a role.","These insights offer a glimpse into the early formation of the Galaxy.","Using neutron-capture elements as a distinguishing criterion for early formation, we have access to a unique metal-poor population that consists of the oldest stars in the universe."],"url":"http://arxiv.org/abs/2405.07856v1","category":"astro-ph.GA"}
{"created":"2024-05-13 15:40:42","title":"Riemannian radial distributions on Riemannian symmetric spaces: Optimal rates of convergence for parameter estimation","abstract":"Manifold data analysis is challenging due to the lack of parametric distributions on manifolds. To address this, we introduce a series of Riemannian radial distributions on Riemannian symmetric spaces. By utilizing the symmetry, we show that for many Riemannian radial distributions, the Riemannian $L^p$ center of mass is uniquely given by the location parameter, and the maximum likelihood estimator (MLE) of this parameter is given by an M-estimator. Therefore, these parametric distributions provide a promising tool for statistical modeling and algorithmic design.   In addition, our paper develops a novel theory for parameter estimation and minimax optimality by integrating statistics, Riemannian geometry, and Lie theory. We demonstrate that the MLE achieves a convergence rate of root-$n$ up to logarithmic terms, where the rate is quantified by both the hellinger distance between distributions and geodesic distance between parameters. Then we derive a root-$n$ minimax lower bound for the parameter estimation rate, demonstrating the optimality of the MLE. Our minimax analysis is limited to the case of simply connected Riemannian symmetric spaces for technical reasons, but is still applicable to numerous applications. Finally, we extend our studies to Riemannian radial distributions with an unknown temperature parameter, and establish the convergence rate of the MLE. We also derive the model complexity of von Mises-Fisher distributions on spheres and discuss the effects of geometry in statistical estimation.","sentences":["Manifold data analysis is challenging due to the lack of parametric distributions on manifolds.","To address this, we introduce a series of Riemannian radial distributions on Riemannian symmetric spaces.","By utilizing the symmetry, we show that for many Riemannian radial distributions, the Riemannian $L^p$ center of mass is uniquely given by the location parameter, and the maximum likelihood estimator (MLE) of this parameter is given by an M-estimator.","Therefore, these parametric distributions provide a promising tool for statistical modeling and algorithmic design.   ","In addition, our paper develops a novel theory for parameter estimation and minimax optimality by integrating statistics, Riemannian geometry, and Lie theory.","We demonstrate that the MLE achieves a convergence rate of root-$n$ up to logarithmic terms, where the rate is quantified by both the hellinger distance between distributions and geodesic distance between parameters.","Then we derive a root-$n$ minimax lower bound for the parameter estimation rate, demonstrating the optimality of the MLE.","Our minimax analysis is limited to the case of simply connected Riemannian symmetric spaces for technical reasons, but is still applicable to numerous applications.","Finally, we extend our studies to Riemannian radial distributions with an unknown temperature parameter, and establish the convergence rate of the MLE.","We also derive the model complexity of von Mises-Fisher distributions on spheres and discuss the effects of geometry in statistical estimation."],"url":"http://arxiv.org/abs/2405.07852v1","category":"math.ST"}
{"created":"2024-05-13 15:38:33","title":"Knowledge Graph Embedding in Intent-Based Networking","abstract":"This paper presents a novel approach to network management by integrating intent-based networking (IBN) with knowledge graphs (KGs), creating a more intuitive and efficient pipeline for service orchestration. By mapping high-level business intents onto network configurations using KGs, the system dynamically adapts to network changes and service demands, ensuring optimal performance and resource allocation. We utilize knowledge graph embedding (KGE) to acquire context information from the network and service providers. The KGE model is trained using a custom KG and Gaussian embedding model and maps intents to services via service prediction and intent validation processes. The proposed intent lifecycle enables intent translation and assurance by only deploying validated intents according to network and resource availability. We evaluate the trained model for its efficiency in service mapping and intent validation tasks using simulated environments and extensive experiments. The service prediction and intent verification accuracy greater than 80 percent is achieved for the trained KGE model on a custom service orchestration intent knowledge graph (IKG) based on TMForum's intent common model.","sentences":["This paper presents a novel approach to network management by integrating intent-based networking (IBN) with knowledge graphs (KGs), creating a more intuitive and efficient pipeline for service orchestration.","By mapping high-level business intents onto network configurations using KGs, the system dynamically adapts to network changes and service demands, ensuring optimal performance and resource allocation.","We utilize knowledge graph embedding (KGE) to acquire context information from the network and service providers.","The KGE model is trained using a custom KG and Gaussian embedding model and maps intents to services via service prediction and intent validation processes.","The proposed intent lifecycle enables intent translation and assurance by only deploying validated intents according to network and resource availability.","We evaluate the trained model for its efficiency in service mapping and intent validation tasks using simulated environments and extensive experiments.","The service prediction and intent verification accuracy greater than 80 percent is achieved for the trained KGE model on a custom service orchestration intent knowledge graph (IKG) based on TMForum's intent common model."],"url":"http://arxiv.org/abs/2405.07850v1","category":"cs.NI"}
{"created":"2024-05-13 15:33:32","title":"A study of the nuclear structure of light nuclei at the Electron-Ion Collider","abstract":"The substructure of atomic nuclei resulting from the clustering of nucleons is crucial for understanding nuclear structure and dynamics. Various cluster configurations can emerge depending on excitation energy, the number and types of core clusters, and the presence of excess neutrons. Despite the prevalence of tightly bound cluster formations in low-lying states, understanding the correlation between clusters and their formation mechanisms remains incomplete. This exploring study investigates nuclear clustering at the Electron-Ion Collider (EIC) using simulations based on the modified BeAGLE model. By simulating collisions involving $e$+$^{9}$Be, $e$+$^{12}$C, and $e$+$^{16}$O nuclei, we find that the average energy of particles $\\langle E \\rangle$ and the system size ratios of particles at forward rapidity exhibit sensitivity to alpha clustering and its various configurations. These findings offer valuable insights into the dynamics of nuclear clustering and its implications for future studies at the EIC.","sentences":["The substructure of atomic nuclei resulting from the clustering of nucleons is crucial for understanding nuclear structure and dynamics.","Various cluster configurations can emerge depending on excitation energy, the number and types of core clusters, and the presence of excess neutrons.","Despite the prevalence of tightly bound cluster formations in low-lying states, understanding the correlation between clusters and their formation mechanisms remains incomplete.","This exploring study investigates nuclear clustering at the Electron-Ion Collider (EIC) using simulations based on the modified BeAGLE model.","By simulating collisions involving $e$+$^{9}$Be, $e$+$^{12}$C, and $e$+$^{16}$O nuclei, we find that the average energy of particles $\\langle E \\rangle$ and the system size ratios of particles at forward rapidity exhibit sensitivity to alpha clustering and its various configurations.","These findings offer valuable insights into the dynamics of nuclear clustering and its implications for future studies at the EIC."],"url":"http://arxiv.org/abs/2405.07844v1","category":"nucl-th"}
{"created":"2024-05-13 15:24:11","title":"Why Decussate? Topological Constraints on 3D Wiring","abstract":"Many vertebrate motor and sensory systems decussate, or cross the midline to the opposite side of the body. The successful crossing of millions of axons during development requires a complex of tightly controlled regulatory processes. Because these processes have evolved in many distinct systems and organisms, it seems reasonable to presume that decussation confers a significant functional advantage. Yet if this is so, the nature of this advantage is not understood. In this article, we examine constraints imposed by topology on the ways that a three-dimensional processor and environment can be wired together in a continuous, somatotopic, way. We show that as the number of wiring connections grows, decussated arrangements become overwhelmingly more robust against wiring errors than seemingly simpler same-sided wiring schemes. These results provide a predictive approach for understanding how 3D networks must be wired if they are to be robust, and therefore have implications both for future large-scale computational networks and for complex bio-medical devices","sentences":["Many vertebrate motor and sensory systems decussate, or cross the midline to the opposite side of the body.","The successful crossing of millions of axons during development requires a complex of tightly controlled regulatory processes.","Because these processes have evolved in many distinct systems and organisms, it seems reasonable to presume that decussation confers a significant functional advantage.","Yet if this is so, the nature of this advantage is not understood.","In this article, we examine constraints imposed by topology on the ways that a three-dimensional processor and environment can be wired together in a continuous, somatotopic, way.","We show that as the number of wiring connections grows, decussated arrangements become overwhelmingly more robust against wiring errors than seemingly simpler same-sided wiring schemes.","These results provide a predictive approach for understanding how 3D networks must be wired if they are to be robust, and therefore have implications both for future large-scale computational networks and for complex bio-medical devices"],"url":"http://arxiv.org/abs/2405.07837v1","category":"q-bio.NC"}
{"created":"2024-05-13 15:13:23","title":"Can LLMs Help Predict Elections? (Counter)Evidence from the World's Largest Democracy","abstract":"The study of how social media affects the formation of public opinion and its influence on political results has been a popular field of inquiry. However, current approaches frequently offer a limited comprehension of the complex political phenomena, yielding inconsistent outcomes. In this work, we introduce a new method: harnessing the capabilities of Large Language Models (LLMs) to examine social media data and forecast election outcomes. Our research diverges from traditional methodologies in two crucial respects. First, we utilize the sophisticated capabilities of foundational LLMs, which can comprehend the complex linguistic subtleties and contextual details present in social media data. Second, we focus on data from X (Twitter) in India to predict state assembly election outcomes. Our method entails sentiment analysis of election-related tweets through LLMs to forecast the actual election results, and we demonstrate the superiority of our LLM-based method against more traditional exit and opinion polls. Overall, our research offers valuable insights into the unique dynamics of Indian politics and the remarkable impact of social media in molding public attitudes within this context.","sentences":["The study of how social media affects the formation of public opinion and its influence on political results has been a popular field of inquiry.","However, current approaches frequently offer a limited comprehension of the complex political phenomena, yielding inconsistent outcomes.","In this work, we introduce a new method: harnessing the capabilities of Large Language Models (LLMs) to examine social media data and forecast election outcomes.","Our research diverges from traditional methodologies in two crucial respects.","First, we utilize the sophisticated capabilities of foundational LLMs, which can comprehend the complex linguistic subtleties and contextual details present in social media data.","Second, we focus on data from X (Twitter) in India to predict state assembly election outcomes.","Our method entails sentiment analysis of election-related tweets through LLMs to forecast the actual election results, and we demonstrate the superiority of our LLM-based method against more traditional exit and opinion polls.","Overall, our research offers valuable insights into the unique dynamics of Indian politics and the remarkable impact of social media in molding public attitudes within this context."],"url":"http://arxiv.org/abs/2405.07828v1","category":"cs.SI"}
{"created":"2024-05-13 15:01:49","title":"A dynamic view of V Hydrae. Monitoring of a spectroscopic-binary AGB star with an alkaline jet","abstract":"The well studied carbon star V Hydrae is known to exhibit a complex asymmetric environment made of a dense equatorial wind and high-velocity outflows, hinting at its transition from the AGB phase to the asymmetric planetary nebula phase. In addition, V Hydrae also exhibits a long secondary period of 17 years in its light curve, suggesting the presence of a binary companion that could shape the circumstellar environment. In this paper, we aim to confirm the binary nature of V Hydrae by deriving its orbital parameters and investigating the effect of the orbital motion on the circumbinary environment. In a first step, we used a radial-velocity monitoring performed with the HERMES spectrograph to disentangle the pulsation signal of the AGB from its orbital motion and to obtain the spectroscopic orbit. We combined the spectroscopic results with astrometric information to get the complete set of orbital parameters, including the system inclination. Next, we reported the time variations of the sodium and potassium resonance doublets. Finally, following the methods used for post-AGB stars, we carried out spatio-kinematic modelling of a conical jet to reproduce the observed spectral-line modulation. We found the orbital solution of V Hydrae for a period of 17 years. We correlated the companion passage across the line of sight with the obscuration event and the blue-shifted absorption of alkaline resonant lines. Those variations were modelled by a conical jet emitted from the companion, whose opening angle is wide and whose sky-projected orientation is found to be consistent with the axis of the large-scale bipolar outflow previously detected in the radio-emission lines of CO. We show that the periodic variation seen for V Hydrae is likely to be due to orbital motion. The presence of a conical jet offers a coherent model to explain the various features of V Hydrae environment.","sentences":["The well studied carbon star V Hydrae is known to exhibit a complex asymmetric environment made of a dense equatorial wind and high-velocity outflows, hinting at its transition from the AGB phase to the asymmetric planetary nebula phase.","In addition, V Hydrae also exhibits a long secondary period of 17 years in its light curve, suggesting the presence of a binary companion that could shape the circumstellar environment.","In this paper, we aim to confirm the binary nature of V Hydrae by deriving its orbital parameters and investigating the effect of the orbital motion on the circumbinary environment.","In a first step, we used a radial-velocity monitoring performed with the HERMES spectrograph to disentangle the pulsation signal of the AGB from its orbital motion and to obtain the spectroscopic orbit.","We combined the spectroscopic results with astrometric information to get the complete set of orbital parameters, including the system inclination.","Next, we reported the time variations of the sodium and potassium resonance doublets.","Finally, following the methods used for post-AGB stars, we carried out spatio-kinematic modelling of a conical jet to reproduce the observed spectral-line modulation.","We found the orbital solution of V Hydrae for a period of 17 years.","We correlated the companion passage across the line of sight with the obscuration event and the blue-shifted absorption of alkaline resonant lines.","Those variations were modelled by a conical jet emitted from the companion, whose opening angle is wide and whose sky-projected orientation is found to be consistent with the axis of the large-scale bipolar outflow previously detected in the radio-emission lines of CO.","We show that the periodic variation seen for V Hydrae is likely to be due to orbital motion.","The presence of a conical jet offers a coherent model to explain the various features of V Hydrae environment."],"url":"http://arxiv.org/abs/2405.07820v1","category":"astro-ph.SR"}
{"created":"2024-05-13 14:52:30","title":"Sharp localization on the first supercritical stratum for Liouville frequencies","abstract":"We establish Anderson localization for Schr\\\"odinger operators with even analytic potentials on the first supercritical stratum for Liouville frequencies in the sharp regime $\\{E: L(\\omega,E)>\\beta(\\omega)>0, \\kappa(\\omega,E)=1\\}$, with $\\kappa(\\omega,E)$ being Avila's acceleration. This paper builds on the large deviation measure estimate and complexity bound scheme, originally developed for Diophantine frequencies by Bourgain, Goldstein and Schlag \\cites{BG,BGS1,BGS2}, and the improved complexity bounds in \\cite{HS1}. Additionally, it strengthens the large deviation estimates for weak Liouville frequencies in \\cite{HZ}. We also introduce new ideas to handle Liouville frequencies in a sharp way.","sentences":["We establish Anderson localization for Schr\\\"odinger operators with even analytic potentials on the first supercritical stratum for Liouville frequencies in the sharp regime $\\{E: L(\\omega,E)>\\beta(\\omega)>0, \\kappa(\\omega,E)=1\\}$, with $\\kappa(\\omega,E)$ being Avila's acceleration.","This paper builds on the large deviation measure estimate and complexity bound scheme, originally developed for Diophantine frequencies by Bourgain, Goldstein and Schlag \\cites{BG,BGS1,BGS2}, and the improved complexity bounds in \\cite{HS1}.","Additionally, it strengthens the large deviation estimates for weak Liouville frequencies in \\cite{HZ}.","We also introduce new ideas to handle Liouville frequencies in a sharp way."],"url":"http://arxiv.org/abs/2405.07810v1","category":"math.SP"}
{"created":"2024-05-13 14:46:39","title":"Multiple stochastic resonances and inverse stochastic resonances in asymmetric bistable system under the ultra-high frequency excitation","abstract":"Ultra-high frequency linear frequency modulation (UHF-LFM) signal, as a kind of typical non-stationary signal, has been widely used in microwave radar and other fields, with advantages such as long transmission distance, strong anti-interference ability, and wide bandwidth. Utilizing optimal dynamics response has unique advantages in weak feature identification under strong background noise. We propose a new stochastic resonance method in an asymmetric bistable system with the time-varying parameter to handle this special non-stationary signal. Interestingly, the nonlinear response exhibits multiple stochastic resonances (MSR) and inverse stochastic resonances (ISR) under UHF-LFM signal excitation, and some resonance regions may deviate or collapse due to the influence of system asymmetry. In addition, we analyze the responses of each resonance region and the mechanism and evolution law of each resonance region in detail. Finally, we significantly expand the resonance region within the parameter range by optimizing the time scale, which verifies the effectiveness of the proposed time-varying scale method. The mechanism and evolution law of MSR and ISR will provide references for researchers in related fields.","sentences":["Ultra-high frequency linear frequency modulation (UHF-LFM) signal, as a kind of typical non-stationary signal, has been widely used in microwave radar and other fields, with advantages such as long transmission distance, strong anti-interference ability, and wide bandwidth.","Utilizing optimal dynamics response has unique advantages in weak feature identification under strong background noise.","We propose a new stochastic resonance method in an asymmetric bistable system with the time-varying parameter to handle this special non-stationary signal.","Interestingly, the nonlinear response exhibits multiple stochastic resonances (MSR) and inverse stochastic resonances (ISR) under UHF-LFM signal excitation, and some resonance regions may deviate or collapse due to the influence of system asymmetry.","In addition, we analyze the responses of each resonance region and the mechanism and evolution law of each resonance region in detail.","Finally, we significantly expand the resonance region within the parameter range by optimizing the time scale, which verifies the effectiveness of the proposed time-varying scale method.","The mechanism and evolution law of MSR and ISR will provide references for researchers in related fields."],"url":"http://arxiv.org/abs/2405.07804v1","category":"nlin.AO"}
{"created":"2024-05-13 14:42:00","title":"Measurement of the $^{14}$C spectrum with Silicon Drift Detectors: towards the study of forbidden $\u03b2$ transitions","abstract":"The ASPECT-BET (An sdd-SPECTrometer for BETa decay studies) project aims to develop a novel technique for the precise measurement of forbidden $\\beta$ spectra in the 10 keV - 1 MeV range. This technique uses a Silicon Drift Detector (SDD) as the main spectrometer, surrounded, if necessary, by a veto system to reject events with only partial energy deposition in the SDD. Accurate knowledge of the spectrometer's response to electrons is essential to reconstruct the theoretical shape of the $\\beta$ spectrum. To compute this response, GEANT4 simulations optimized for low-energy electron interactions are used. In this article, we present the performance of these simulations in reconstructing the electron spectra, measured with SDDs, of a $^{109}$Cd monochromatic source, both in vacuum and in air. The allowed $\\beta$ spectrum of a $^{14}$C source is also measured and analyzed, and it is shown that the experimental shape factor commonly used in the literature to reconstruct the measured spectrum is not necessary to explain the spectrum.","sentences":["The ASPECT-BET (An sdd-SPECTrometer for BETa decay studies) project aims to develop a novel technique for the precise measurement of forbidden $\\beta$ spectra in the 10 keV - 1 MeV range.","This technique uses a Silicon Drift Detector (SDD) as the main spectrometer, surrounded, if necessary, by a veto system to reject events with only partial energy deposition in the SDD.","Accurate knowledge of the spectrometer's response to electrons is essential to reconstruct the theoretical shape of the $\\beta$ spectrum.","To compute this response, GEANT4 simulations optimized for low-energy electron interactions are used.","In this article, we present the performance of these simulations in reconstructing the electron spectra, measured with SDDs, of a $^{109}$Cd monochromatic source, both in vacuum and in air.","The allowed $\\beta$ spectrum of a $^{14}$C source is also measured and analyzed, and it is shown that the experimental shape factor commonly used in the literature to reconstruct the measured spectrum is not necessary to explain the spectrum."],"url":"http://arxiv.org/abs/2405.07797v1","category":"physics.ins-det"}
{"created":"2024-05-13 14:41:28","title":"Improved Bound for Robust Causal Bandits with Linear Models","abstract":"This paper investigates the robustness of causal bandits (CBs) in the face of temporal model fluctuations. This setting deviates from the existing literature's widely-adopted assumption of constant causal models. The focus is on causal systems with linear structural equation models (SEMs). The SEMs and the time-varying pre- and post-interventional statistical models are all unknown and subject to variations over time. The goal is to design a sequence of interventions that incur the smallest cumulative regret compared to an oracle aware of the entire causal model and its fluctuations. A robust CB algorithm is proposed, and its cumulative regret is analyzed by establishing both upper and lower bounds on the regret. It is shown that in a graph with maximum in-degree $d$, length of the largest causal path $L$, and an aggregate model deviation $C$, the regret is upper bounded by $\\tilde{\\mathcal{O}}(d^{L-\\frac{1}{2}}(\\sqrt{T} + C))$ and lower bounded by $\\Omega(d^{\\frac{L}{2}-2}\\max\\{\\sqrt{T}\\; ,\\; d^2C\\})$. The proposed algorithm achieves nearly optimal $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret when $C$ is $o(\\sqrt{T})$, maintaining sub-linear regret for a broad range of $C$.","sentences":["This paper investigates the robustness of causal bandits (CBs) in the face of temporal model fluctuations.","This setting deviates from the existing literature's widely-adopted assumption of constant causal models.","The focus is on causal systems with linear structural equation models (SEMs).","The SEMs and the time-varying pre- and post-interventional statistical models are all unknown and subject to variations over time.","The goal is to design a sequence of interventions that incur the smallest cumulative regret compared to an oracle aware of the entire causal model and its fluctuations.","A robust CB algorithm is proposed, and its cumulative regret is analyzed by establishing both upper and lower bounds on the regret.","It is shown that in a graph with maximum in-degree $d$, length of the largest causal path $L$, and an aggregate model deviation $C$, the regret is upper bounded by $\\tilde{\\mathcal{O}}(d^{L-\\frac{1}{2}}(\\sqrt{T} + C))$ and lower bounded by $\\Omega(d^{\\frac{L}{2}-2}\\max\\{\\sqrt{T}\\; ,\\; d^2C\\})$.","The proposed algorithm achieves nearly optimal $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret when $C$ is $o(\\sqrt{T})$, maintaining sub-linear regret for a broad range of $C$."],"url":"http://arxiv.org/abs/2405.07795v1","category":"stat.ML"}
{"created":"2024-05-13 14:36:22","title":"Hamiltonian-based Quantum Reinforcement Learning for Neural Combinatorial Optimization","abstract":"Advancements in Quantum Computing (QC) and Neural Combinatorial Optimization (NCO) represent promising steps in tackling complex computational challenges. On the one hand, Variational Quantum Algorithms such as QAOA can be used to solve a wide range of combinatorial optimization problems. On the other hand, the same class of problems can be solved by NCO, a method that has shown promising results, particularly since the introduction of Graph Neural Networks. Given recent advances in both research areas, we introduce Hamiltonian-based Quantum Reinforcement Learning (QRL), an approach at the intersection of QC and NCO. We model our ansatzes directly on the combinatorial optimization problem's Hamiltonian formulation, which allows us to apply our approach to a broad class of problems. Our ansatzes show favourable trainability properties when compared to the hardware efficient ansatzes, while also not being limited to graph-based problems, unlike previous works. In this work, we evaluate the performance of Hamiltonian-based QRL on a diverse set of combinatorial optimization problems to demonstrate the broad applicability of our approach and compare it to QAOA.","sentences":["Advancements in Quantum Computing (QC) and Neural Combinatorial Optimization (NCO) represent promising steps in tackling complex computational challenges.","On the one hand, Variational Quantum Algorithms such as QAOA can be used to solve a wide range of combinatorial optimization problems.","On the other hand, the same class of problems can be solved by NCO, a method that has shown promising results, particularly since the introduction of Graph Neural Networks.","Given recent advances in both research areas, we introduce Hamiltonian-based Quantum Reinforcement Learning (QRL), an approach at the intersection of QC and NCO.","We model our ansatzes directly on the combinatorial optimization problem's Hamiltonian formulation, which allows us to apply our approach to a broad class of problems.","Our ansatzes show favourable trainability properties when compared to the hardware efficient ansatzes, while also not being limited to graph-based problems, unlike previous works.","In this work, we evaluate the performance of Hamiltonian-based QRL on a diverse set of combinatorial optimization problems to demonstrate the broad applicability of our approach and compare it to QAOA."],"url":"http://arxiv.org/abs/2405.07790v1","category":"quant-ph"}
{"created":"2024-05-13 14:35:57","title":"Reducing the oxygen contamination in conductive (Ti,Zr)N coatings via RF-bias assisted reactive sputtering","abstract":"Ternary transition metal nitride coatings are promising for many applications as they can offer improved hardness and oxidation resistance compared to binary counterparts. A common challenge in the deposition of functional nitride thin films is oxygen contamination. Even low amounts of oxygen contamination can adversely affect the functional properties of the thin films. Here, we present a practical approach for the growth of virtually oxygen-free (Ti, Zr)N thin films. To cover the complete compositional range of (Ti,Zr)N coatings we employ combinatorial reactive co-sputtering. The depositions are carried out with or without applying a low-power radio-frequency (RF) bias voltage to the substrate holder to study the possibility of decelerating energetic oxygen ions and effectively reducing oxygen contamination in the growing film. High-throughput structural analysis and functional property mapping are used to elucidate the synthesis-property relationships. The structural analysis indicates solid solution formation over the entire compositional range, as evidenced by Vegardian lattice scaling, regardless of the applied RF substrate bias. Irrespective of the composition of the films, the application of RF substrate bias leads to a dramatic reduction of oxygen contamination, as demonstrated by X-ray photoelectron spectroscopy (XPS) depth-profile mapping. This is reflected in a significant improvement in the films' conductivity and hardness. We demonstrate that the reduction in oxygen contamination is intrinsic to the process and not due to changes in the microstructure. The approach presented here is applicable to both conductive and insulating substrates and provides a practical route to synthesize nitride thin films with improved purity that can be applied in standard sputter chambers and on many different material systems.","sentences":["Ternary transition metal nitride coatings are promising for many applications as they can offer improved hardness and oxidation resistance compared to binary counterparts.","A common challenge in the deposition of functional nitride thin films is oxygen contamination.","Even low amounts of oxygen contamination can adversely affect the functional properties of the thin films.","Here, we present a practical approach for the growth of virtually oxygen-free (Ti, Zr)N thin films.","To cover the complete compositional range of (Ti,Zr)N coatings we employ combinatorial reactive co-sputtering.","The depositions are carried out with or without applying a low-power radio-frequency (RF) bias voltage to the substrate holder to study the possibility of decelerating energetic oxygen ions and effectively reducing oxygen contamination in the growing film.","High-throughput structural analysis and functional property mapping are used to elucidate the synthesis-property relationships.","The structural analysis indicates solid solution formation over the entire compositional range, as evidenced by Vegardian lattice scaling, regardless of the applied RF substrate bias.","Irrespective of the composition of the films, the application of RF substrate bias leads to a dramatic reduction of oxygen contamination, as demonstrated by X-ray photoelectron spectroscopy (XPS) depth-profile mapping.","This is reflected in a significant improvement in the films' conductivity and hardness.","We demonstrate that the reduction in oxygen contamination is intrinsic to the process and not due to changes in the microstructure.","The approach presented here is applicable to both conductive and insulating substrates and provides a practical route to synthesize nitride thin films with improved purity that can be applied in standard sputter chambers and on many different material systems."],"url":"http://arxiv.org/abs/2405.07789v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 14:26:29","title":"Is Interpretable Machine Learning Effective at Feature Selection for Neural Learning-to-Rank?","abstract":"Neural ranking models have become increasingly popular for real-world search and recommendation systems in recent years. Unlike their tree-based counterparts, neural models are much less interpretable. That is, it is very difficult to understand their inner workings and answer questions like how do they make their ranking decisions? or what document features do they find important? This is particularly disadvantageous since interpretability is highly important for real-world systems. In this work, we explore feature selection for neural learning-to-rank (LTR). In particular, we investigate six widely-used methods from the field of interpretable machine learning (ML) and introduce our own modification, to select the input features that are most important to the ranking behavior. To understand whether these methods are useful for practitioners, we further study whether they contribute to efficiency enhancement. Our experimental results reveal a large feature redundancy in several LTR benchmarks: the local selection method TabNet can achieve optimal ranking performance with less than 10 features; the global methods, particularly our G-L2X, require slightly more selected features, but exhibit higher potential in improving efficiency. We hope that our analysis of these feature selection methods will bring the fields of interpretable ML and LTR closer together.","sentences":["Neural ranking models have become increasingly popular for real-world search and recommendation systems in recent years.","Unlike their tree-based counterparts, neural models are much less interpretable.","That is, it is very difficult to understand their inner workings and answer questions like how do they make their ranking decisions?","or what document features do they find important?","This is particularly disadvantageous since interpretability is highly important for real-world systems.","In this work, we explore feature selection for neural learning-to-rank (LTR).","In particular, we investigate six widely-used methods from the field of interpretable machine learning (ML) and introduce our own modification, to select the input features that are most important to the ranking behavior.","To understand whether these methods are useful for practitioners, we further study whether they contribute to efficiency enhancement.","Our experimental results reveal a large feature redundancy in several LTR benchmarks: the local selection method TabNet can achieve optimal ranking performance with less than 10 features; the global methods, particularly our G-L2X, require slightly more selected features, but exhibit higher potential in improving efficiency.","We hope that our analysis of these feature selection methods will bring the fields of interpretable ML and LTR closer together."],"url":"http://arxiv.org/abs/2405.07782v1","category":"cs.IR"}
{"created":"2024-05-13 14:24:21","title":"Infrared gluon propagator in the Refined Gribov-Zwanziger scenario at one-loop order in the Landau gauge","abstract":"The Refined Gribov-Zwanziger (RGZ) action in the Landau gauge provides a local and renormalizable framework to account for the existence of infinitesimal Gribov copies in the path integral together with other relevant infrared effects such as the formation of condensates. The properties of the tree-level gluon propagator obtained in this setup has been thoroughly investigated over the past decade. It accommodates important properties seen in lattice simulations such as a finite value at vanishing momentum and positivity violation. Yet a comprehensive study about the stability of such properties against quantum corrections was lacking. In this work, we compute the gluon propagator in the RGZ scenario at one-loop order and implement an appropriate renormalization scheme in order to compare our findings with lattice data. Remarkably, the qualitative properties of the tree-level gluon propagator are preserved. In particular, the fits with lattice data show evidence for positivity violation and the existence of complex poles for SU(2) and SU(3) gauge groups. We comment on the results for the ghost propagator as well.","sentences":["The Refined Gribov-Zwanziger (RGZ) action in the Landau gauge provides a local and renormalizable framework to account for the existence of infinitesimal Gribov copies in the path integral together with other relevant infrared effects such as the formation of condensates.","The properties of the tree-level gluon propagator obtained in this setup has been thoroughly investigated over the past decade.","It accommodates important properties seen in lattice simulations such as a finite value at vanishing momentum and positivity violation.","Yet a comprehensive study about the stability of such properties against quantum corrections was lacking.","In this work, we compute the gluon propagator in the RGZ scenario at one-loop order and implement an appropriate renormalization scheme in order to compare our findings with lattice data.","Remarkably, the qualitative properties of the tree-level gluon propagator are preserved.","In particular, the fits with lattice data show evidence for positivity violation and the existence of complex poles for SU(2) and SU(3) gauge groups.","We comment on the results for the ghost propagator as well."],"url":"http://arxiv.org/abs/2405.07779v1","category":"hep-th"}
{"created":"2024-05-13 14:21:54","title":"GMSR:Gradient-Guided Mamba for Spectral Reconstruction from RGB Images","abstract":"Mainstream approaches to spectral reconstruction (SR) primarily focus on designing Convolution- and Transformer-based architectures. However, CNN methods often face challenges in handling long-range dependencies, whereas Transformers are constrained by computational efficiency limitations. Recent breakthroughs in state-space model (e.g., Mamba) has attracted significant attention due to its near-linear computational efficiency and superior performance, prompting our investigation into its potential for SR problem. To this end, we propose the Gradient-guided Mamba for Spectral Reconstruction from RGB Images, dubbed GMSR-Net. GMSR-Net is a lightweight model characterized by a global receptive field and linear computational complexity. Its core comprises multiple stacked Gradient Mamba (GM) blocks, each featuring a tri-branch structure. In addition to benefiting from efficient global feature representation by Mamba block, we further innovatively introduce spatial gradient attention and spectral gradient attention to guide the reconstruction of spatial and spectral cues. GMSR-Net demonstrates a significant accuracy-efficiency trade-off, achieving state-of-the-art performance while markedly reducing the number of parameters and computational burdens. Compared to existing approaches, GMSR-Net slashes parameters and FLOPS by substantial margins of 10 times and 20 times, respectively. Code is available at https://github.com/wxy11-27/GMSR.","sentences":["Mainstream approaches to spectral reconstruction (SR) primarily focus on designing Convolution- and Transformer-based architectures.","However, CNN methods often face challenges in handling long-range dependencies, whereas Transformers are constrained by computational efficiency limitations.","Recent breakthroughs in state-space model (e.g., Mamba) has attracted significant attention due to its near-linear computational efficiency and superior performance, prompting our investigation into its potential for SR problem.","To this end, we propose the Gradient-guided Mamba for Spectral Reconstruction from RGB Images, dubbed GMSR-Net.","GMSR-Net is a lightweight model characterized by a global receptive field and linear computational complexity.","Its core comprises multiple stacked Gradient Mamba (GM) blocks, each featuring a tri-branch structure.","In addition to benefiting from efficient global feature representation by Mamba block, we further innovatively introduce spatial gradient attention and spectral gradient attention to guide the reconstruction of spatial and spectral cues.","GMSR-Net demonstrates a significant accuracy-efficiency trade-off, achieving state-of-the-art performance while markedly reducing the number of parameters and computational burdens.","Compared to existing approaches, GMSR-Net slashes parameters and FLOPS by substantial margins of 10 times and 20 times, respectively.","Code is available at https://github.com/wxy11-27/GMSR."],"url":"http://arxiv.org/abs/2405.07777v1","category":"cs.CV"}
{"created":"2024-05-13 14:20:58","title":"Recent progress on quantum simulations of non-standard Bose-Hubbard models","abstract":"In recent years, the systems comprising of bosonic atoms confined to optical lattices at ultra-cold temperatures have demonstrated tremendous potential to unveil novel quantum mechanical effects appearing in lattice boson models with various kinds of interactions. In this progress report, we aim to provide an exposition to recent advancements in quantum simulations of such systems, modeled by different \"non-standard\" Bose-Hubbard models, focusing primarily on long-range systems with dipole-dipole or cavity-mediated interactions. Through a carefully curated selection of topics, which includes the emergence of quantum criticality beyond Landau paradigm, bond-order wave insulators, the role of interaction-induced tunneling, the influence of transverse confinement on observed phases, or the effect of cavity-mediated all-to-all interactions, we report both theoretical and experimental developments from the last few years. Additionally, we discuss the real-time evolution of systems with long-range interactions, where sufficiently strong interactions render the dynamics non-ergodic. And finally to cap our discussions off, we survey recent experimental achievements in this rapidly evolving field, underscoring its interdisciplinary significance and potential for groundbreaking discoveries.","sentences":["In recent years, the systems comprising of bosonic atoms confined to optical lattices at ultra-cold temperatures have demonstrated tremendous potential to unveil novel quantum mechanical effects appearing in lattice boson models with various kinds of interactions.","In this progress report, we aim to provide an exposition to recent advancements in quantum simulations of such systems, modeled by different \"non-standard\" Bose-Hubbard models, focusing primarily on long-range systems with dipole-dipole or cavity-mediated interactions.","Through a carefully curated selection of topics, which includes the emergence of quantum criticality beyond Landau paradigm, bond-order wave insulators, the role of interaction-induced tunneling, the influence of transverse confinement on observed phases, or the effect of cavity-mediated all-to-all interactions, we report both theoretical and experimental developments from the last few years.","Additionally, we discuss the real-time evolution of systems with long-range interactions, where sufficiently strong interactions render the dynamics non-ergodic.","And finally to cap our discussions off, we survey recent experimental achievements in this rapidly evolving field, underscoring its interdisciplinary significance and potential for groundbreaking discoveries."],"url":"http://arxiv.org/abs/2405.07775v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-13 14:18:20","title":"Open Source in Lab Management","abstract":"This document explores the advantages of integrating open source software and practices in managing a scientific lab, emphasizing reproducibility and the avoidance of pitfalls. It details practical applications from website management using GitHub Pages to organizing datasets in compliance with BIDS standards, highlights the importance of continuous testing for data integrity, IT management through Ansible for efficient system configuration, open source software development. The broader goal is to promote transparent, reproducible science by adopting open source tools. This approach not only saves time but exposes students to best practices, enhancing the transparency and reproducibility of scientific research.","sentences":["This document explores the advantages of integrating open source software and practices in managing a scientific lab, emphasizing reproducibility and the avoidance of pitfalls.","It details practical applications from website management using GitHub Pages to organizing datasets in compliance with BIDS standards, highlights the importance of continuous testing for data integrity, IT management through Ansible for efficient system configuration, open source software development.","The broader goal is to promote transparent, reproducible science by adopting open source tools.","This approach not only saves time but exposes students to best practices, enhancing the transparency and reproducibility of scientific research."],"url":"http://arxiv.org/abs/2405.07774v1","category":"cs.CY"}
{"created":"2024-05-13 14:16:19","title":"Cellular dynamics of host-parasitoid interactions: Insights from the encapsulation process in a partially resistant host","abstract":"Cotesia typhae is an eastern African endoparasitoid braconid wasp that targets the larval stage of the lepidopteran stem borer, Sesamia nonagrioides, a maize crop pest in Europe. The French host population is partially resistant to the Makindu strain of the wasp, allowing its development in only 40% of the cases. Resistant larvae can encapsulate the parasitoid and survive the infection. This interaction provides a very interesting frame for investigating the impact of parasitism on host cellular resistance. We characterized the parasitoid ovolarval development in a permissive host and studied the encapsulation process in a resistant host by dissection and histological sectioning compared to that of inert chromatography beads. We measured the total hemocyte count in parasitized and bead-injected larvae over time to monitor the magnitude of the immune reaction. Our results show that parasitism of resistant hosts delayed encapsulation but did not affect immune abilities towards inert beads. Moreover, while bead injection increased total hemocyte count, it remained constant in resistant and permissive larvae. We conclude that while Cotesia spp virulence factors are known to impair the host immune system, our results suggest that passive evasion could also occur.","sentences":["Cotesia typhae is an eastern African endoparasitoid braconid wasp that targets the larval stage of the lepidopteran stem borer, Sesamia nonagrioides, a maize crop pest in Europe.","The French host population is partially resistant to the Makindu strain of the wasp, allowing its development in only 40% of the cases.","Resistant larvae can encapsulate the parasitoid and survive the infection.","This interaction provides a very interesting frame for investigating the impact of parasitism on host cellular resistance.","We characterized the parasitoid ovolarval development in a permissive host and studied the encapsulation process in a resistant host by dissection and histological sectioning compared to that of inert chromatography beads.","We measured the total hemocyte count in parasitized and bead-injected larvae over time to monitor the magnitude of the immune reaction.","Our results show that parasitism of resistant hosts delayed encapsulation but did not affect immune abilities towards inert beads.","Moreover, while bead injection increased total hemocyte count, it remained constant in resistant and permissive larvae.","We conclude that while Cotesia spp virulence factors are known to impair the host immune system, our results suggest that passive evasion could also occur."],"url":"http://arxiv.org/abs/2405.07771v1","category":"q-bio.CB"}
{"created":"2024-05-13 14:12:01","title":"Radial velocities: direct application of Pierre Connes' shift finding algorithm to Cross-Correlation Functions","abstract":"Pipelines of state-of-the-art spectrographs dedicated to planet detection provide, for each exposure, series of Cross-Correlation Functions (CCFs) built with a Binary Mask (BM), and the absolute radial velocity (RV) derived from Gaussian fit of a weighted average CCF$_{tot}$ of the CCFs. Here we tested the benefits of the application of the shift finding algorithm developed by Pierre Connes directly to the total CCF$_{tot}$, comparing the resulting RV shifts (DRVs) with the results of the Gaussian fits. In a second step, we investigated how the individual DRV profiles along the velocity grid can be used as an easy tool for detection of stellar line shape variations. We tested this new algorithm on 1151 archived spectra of the K2.5 V star HD 40307 obtained with ESO/ESPRESSO during a one-week campaign in 2018. Tests were performed based on the comparison of DRVs with RVs from Gaussian fits. DRV profiles along the velocity grid (DRV(i)) were scrutinized and compared with direct CCF$_{tot}$ ratios. The dispersion of residuals from a linear fit to RVs from 406 spectra recorded within one night, was found to be $\\sigma$=1.03 and 0.83 ms$^{-1}$ for the Gaussian fit and the new algorithm respectively, a significant 20\\% improvement in accuracy. The two full one-week series obtained during the campaign were also fitted with a 3-planet system Keplerian model. The residual divergence between data and best-fit model is significantly smaller for the new algorithm than for the Gaussian fit. This new algorithm is an easy tool allowing to obtain additional diagnostics on the RV measurements in series of exposures. It increases the accuracy of velocity variation determinations. Also, departures from constancy of the DRVi profiles, as well as varying differences between RVs from this new method and RVs from a Gaussian fit provide diagnostics of line shape variations due to stellar activity.","sentences":["Pipelines of state-of-the-art spectrographs dedicated to planet detection provide, for each exposure, series of Cross-Correlation Functions (CCFs) built with a Binary Mask (BM), and the absolute radial velocity (RV) derived from Gaussian fit of a weighted average CCF$_{tot}$ of the CCFs.","Here we tested the benefits of the application of the shift finding algorithm developed by Pierre Connes directly to the total CCF$_{tot}$, comparing the resulting RV shifts (DRVs) with the results of the Gaussian fits.","In a second step, we investigated how the individual DRV profiles along the velocity grid can be used as an easy tool for detection of stellar line shape variations.","We tested this new algorithm on 1151 archived spectra of the K2.5 V star HD 40307 obtained with ESO/ESPRESSO during a one-week campaign in 2018.","Tests were performed based on the comparison of DRVs with RVs from Gaussian fits.","DRV profiles along the velocity grid (DRV(i)) were scrutinized and compared with direct CCF$_{tot}$ ratios.","The dispersion of residuals from a linear fit to RVs from 406 spectra recorded within one night, was found to be $\\sigma$=1.03 and 0.83 ms$^{-1}$ for the Gaussian fit and the new algorithm respectively, a significant 20\\% improvement in accuracy.","The two full one-week series obtained during the campaign were also fitted with a 3-planet system Keplerian model.","The residual divergence between data and best-fit model is significantly smaller for the new algorithm than for the Gaussian fit.","This new algorithm is an easy tool allowing to obtain additional diagnostics on the RV measurements in series of exposures.","It increases the accuracy of velocity variation determinations.","Also, departures from constancy of the DRVi profiles, as well as varying differences between RVs from this new method and RVs from a Gaussian fit provide diagnostics of line shape variations due to stellar activity."],"url":"http://arxiv.org/abs/2405.07768v1","category":"astro-ph.IM"}
{"created":"2024-05-13 13:51:38","title":"Feedback-delay dependence of the stability of cluster periodic orbits in populations of degrade-and-fire oscillators with common activator","abstract":"Feedback delay has been identified as a key ingredient in the quorum sensing synchronization of synthetic gene oscillators. While this influence has been evidenced at the theoretical level in a simplified system of degrade-and-fire oscillators coupled via a common activator protein, full mathematical certifications remained to be provided. Here, we prove from a rigorous mathematical viewpoint that, for the very same model, the synchronized degrade-and-fire oscillations are 1/ unstable with respect to out-of-sync perturbations in absence of delay, and 2/ are otherwise asymptotically stable in presence of delay, no matter how small is its amplitude. To that goal, we proceed to an extensive study of the population dynamics in this system, which in particular identifies the mechanisms of, and related criteria for, the delay-dependent stability of periodic orbits with respect to out-of-sync perturbations. As an additional outcome, the analysis also reveals that, depending on the parameters, multiple stable partially synchronized periodic orbits can coexist with the fully synchronized one.","sentences":["Feedback delay has been identified as a key ingredient in the quorum sensing synchronization of synthetic gene oscillators.","While this influence has been evidenced at the theoretical level in a simplified system of degrade-and-fire oscillators coupled via a common activator protein, full mathematical certifications remained to be provided.","Here, we prove from a rigorous mathematical viewpoint that, for the very same model, the synchronized degrade-and-fire oscillations are 1/ unstable with respect to out-of-sync perturbations in absence of delay, and 2/ are otherwise asymptotically stable in presence of delay, no matter how small is its amplitude.","To that goal, we proceed to an extensive study of the population dynamics in this system, which in particular identifies the mechanisms of, and related criteria for, the delay-dependent stability of periodic orbits with respect to out-of-sync perturbations.","As an additional outcome, the analysis also reveals that, depending on the parameters, multiple stable partially synchronized periodic orbits can coexist with the fully synchronized one."],"url":"http://arxiv.org/abs/2405.07752v1","category":"math.DS"}
{"created":"2024-05-13 13:47:42","title":"Cycle switching in Steiner triple systems of order 19","abstract":"Cycle switching is a particular form of transformation applied to isomorphism classes of a Steiner triple system of a given order $v$ (an $STS(v)$), yielding another $STS(v)$. This relationship may be represented by an undirected graph. An $STS(v)$ admits cycles of lengths $4,6,\\ldots,v-7$ and $v-3$. In the particular case of $v=19$, it is known that the full switching graph, allowing switching of cycles of any length, is connected. We show that if we restrict switching to only one of the possible cycle lengths, in all cases the switching graph is disconnected (even if we ignore those $STS(19)$s which have no cycle of the given length). Moreover, in a number of cases we find intriguing connected components in the switching graphs which exhibit unexpected symmetries. Our method utilises an algorithm for determining connected components in a very large implicitly defined graph which is more efficient than previous approaches, avoiding the necessity of computing canonical labellings for a large proportion of the systems.","sentences":["Cycle switching is a particular form of transformation applied to isomorphism classes of a Steiner triple system of a given order $v$ (an $STS(v)$), yielding another $STS(v)$. This relationship may be represented by an undirected graph.","An $STS(v)$ admits cycles of lengths $4,6,\\ldots,v-7$ and $v-3$. In the particular case of $v=19$, it is known that the full switching graph, allowing switching of cycles of any length, is connected.","We show that if we restrict switching to only one of the possible cycle lengths, in all cases the switching graph is disconnected (even if we ignore those $STS(19)$s which have no cycle of the given length).","Moreover, in a number of cases we find intriguing connected components in the switching graphs which exhibit unexpected symmetries.","Our method utilises an algorithm for determining connected components in a very large implicitly defined graph which is more efficient than previous approaches, avoiding the necessity of computing canonical labellings for a large proportion of the systems."],"url":"http://arxiv.org/abs/2405.07750v1","category":"math.CO"}
{"created":"2024-05-13 13:31:15","title":"Observation of vector charmoniumlike states and search for $Z_{cs}$ in $e^+ e^- \\to K^+ K^- J/\u03c8$ at BESIII","abstract":"We present the recent measurements of the $e^+e^-\\to K^+K^- J/\\psi$ process carried out by the BESIII experiment. A new decay mode $Y(4230)\\to K^+K^- J/\\psi$ is firstly identified by BESIII. Furthermore, two vector charmoniumlike states, $Y(4500)$ and $Y(4710)$, are reported for the first time in the energy-dependent line shape of $e^+e^-\\to K^+K^- J/\\psi$ cross section. In additon, an investigation of the $K^\\pm J/\\psi$ system is conducted to explore charged charmoniumlike states.","sentences":["We present the recent measurements of the $e^+e^-\\to K^+K^- J/\\psi$ process carried out by the BESIII experiment.","A new decay mode $Y(4230)\\to K^+K^- J/\\psi$ is firstly identified by BESIII.","Furthermore, two vector charmoniumlike states, $Y(4500)$ and $Y(4710)$, are reported for the first time in the energy-dependent line shape of $e^+e^-\\to K^+K^- J/\\psi$ cross section.","In additon, an investigation of the $K^\\pm J/\\psi$ system is conducted to explore charged charmoniumlike states."],"url":"http://arxiv.org/abs/2405.07734v1","category":"hep-ex"}
{"created":"2024-05-13 13:26:33","title":"The Edge of Random Tensor Eigenvalues with Deviation","abstract":"The largest eigenvalue of random tensors is an important feature of systems involving disorder, equivalent to the ground state energy of glassy systems or to the injective norm of quantum states. For symmetric Gaussian random tensors of order $3$ and of size $N$, in the presence of a Gaussian noise, continuing the work [arXiv:2310.14589], we compare the edge of the genuine eigenvalue distribution with that of the signed distribution, using field theoretic methods at large $N$ combined with earlier rigorous results of [arXiv:1003.1129]. We support our claims with Monte Carlo simulations. We find two critical points as the variance of the noise increases, the first of which corresponding to the emergence of an outlier from the main part of the spectrum and the second where this outlier merges with the corresponding trivial eigenvalue and they both become complex.","sentences":["The largest eigenvalue of random tensors is an important feature of systems involving disorder, equivalent to the ground state energy of glassy systems or to the injective norm of quantum states.","For symmetric Gaussian random tensors of order $3$ and of size $N$, in the presence of a Gaussian noise, continuing the work [arXiv:2310.14589], we compare the edge of the genuine eigenvalue distribution with that of the signed distribution, using field theoretic methods at large $N$ combined with earlier rigorous results of [arXiv:1003.1129].","We support our claims with Monte Carlo simulations.","We find two critical points as the variance of the noise increases, the first of which corresponding to the emergence of an outlier from the main part of the spectrum and the second where this outlier merges with the corresponding trivial eigenvalue and they both become complex."],"url":"http://arxiv.org/abs/2405.07731v1","category":"hep-th"}
{"created":"2024-05-13 13:22:13","title":"Validated error bounds for pseudospectral approximation of delay differential equations: unstable manifolds","abstract":"Pseudospectral approximation provides a means to approximate the dynamics of delay differential equations (DDE) by ordinary differential equations (ODE). This article develops a computer-aided algorithm to determine the distance between the unstable manifold of a DDE and the unstable manifold of the approximating pseudospectral ODE. The algorithm is based upon the parametrization method. While a-priori the parametrization method for a vector-valued ODE involves computing a sequence of vector-valued Taylor coefficients, we show that for the pseudospectral ODE, due to its specific structure, the problem reduces to finding a sequence of scalars, which significantly simplifies the problem.","sentences":["Pseudospectral approximation provides a means to approximate the dynamics of delay differential equations (DDE) by ordinary differential equations (ODE).","This article develops a computer-aided algorithm to determine the distance between the unstable manifold of a DDE and the unstable manifold of the approximating pseudospectral ODE.","The algorithm is based upon the parametrization method.","While a-priori the parametrization method for a vector-valued ODE involves computing a sequence of vector-valued Taylor coefficients, we show that for the pseudospectral ODE, due to its specific structure, the problem reduces to finding a sequence of scalars, which significantly simplifies the problem."],"url":"http://arxiv.org/abs/2405.07727v1","category":"math.DS"}
{"created":"2024-05-13 13:21:35","title":"Quantifying and Optimizing Global Faithfulness in Persona-driven Role-playing","abstract":"Persona-driven role-playing (PRP) aims to build AI characters that can respond to user queries by faithfully sticking with all persona statements. Unfortunately, existing faithfulness criteria for PRP are limited to coarse-grained LLM-based scoring without a clear definition or formulation. This paper presents a pioneering exploration to quantify PRP faithfulness as a fine-grained and explainable criterion, which also serves as a reliable reference for optimization. Our criterion first discriminates persona statements into active and passive constraints by identifying the query-statement relevance. Then, we incorporate all constraints following the principle that the AI character's response should be (a) entailed by active (relevant) constraints and (b) not contradicted by passive (irrelevant) constraints. We translate this principle mathematically into a novel Active-Passive-Constraint (APC) score, a constraint-wise sum of natural language inference (NLI) scores weighted by relevance scores. In practice, we build the APC scoring system by symbolically distilling small discriminators from GPT-4 for efficiency. We validate the quality of the APC score against human evaluation based on example personas with tens of statements, and the results show a high correlation. We further leverage it as a reward system in direct preference optimization (DPO) for better AI characters. Our experiments offer a fine-grained and explainable comparison between existing PRP techniques, revealing their advantages and limitations. We further find APC-based DPO to be one of the most competitive techniques for sticking with all constraints and can be well incorporated with other techniques. We then extend the scale of the experiments to real persons with hundreds of statements and reach a consistent conclusion.","sentences":["Persona-driven role-playing (PRP) aims to build AI characters that can respond to user queries by faithfully sticking with all persona statements.","Unfortunately, existing faithfulness criteria for PRP are limited to coarse-grained LLM-based scoring without a clear definition or formulation.","This paper presents a pioneering exploration to quantify PRP faithfulness as a fine-grained and explainable criterion, which also serves as a reliable reference for optimization.","Our criterion first discriminates persona statements into active and passive constraints by identifying the query-statement relevance.","Then, we incorporate all constraints following the principle that the AI character's response should be (a) entailed by active (relevant) constraints and (b) not contradicted by passive (irrelevant) constraints.","We translate this principle mathematically into a novel Active-Passive-Constraint (APC) score, a constraint-wise sum of natural language inference (NLI) scores weighted by relevance scores.","In practice, we build the APC scoring system by symbolically distilling small discriminators from GPT-4 for efficiency.","We validate the quality of the APC score against human evaluation based on example personas with tens of statements, and the results show a high correlation.","We further leverage it as a reward system in direct preference optimization (DPO) for better AI characters.","Our experiments offer a fine-grained and explainable comparison between existing PRP techniques, revealing their advantages and limitations.","We further find APC-based DPO to be one of the most competitive techniques for sticking with all constraints and can be well incorporated with other techniques.","We then extend the scale of the experiments to real persons with hundreds of statements and reach a consistent conclusion."],"url":"http://arxiv.org/abs/2405.07726v1","category":"cs.CL"}
{"created":"2024-05-13 13:15:20","title":"High-frequency Optimally Windowed Chirp rheometry for rapidly evolving viscoelastic materials: application to a crosslinking thermoset","abstract":"Abstract   Knowledge of the evolution of mechanical properties of the curing matrix is of great importance in composite parts or structure fabrication. Conventional rheometry, based on small amplitude oscillatory shear is limited by long interrogation times. In rapidly evolving materials, time sweeps can provide a meaningful measurement albeit at a single frequency. To overcome this constraint we utilize a combined frequency and amplitude-modulated chirped strain waveform in conjunction with a home-made sliding plate piezo-operated (PZR) and a dual-head commercial rotational rheometer (Anton Paar MCR 702) to probe the linear viscoelasticity of these time-evolving materials. The direct controllability of the PZR resulting from the absence of any kind of firmware and the microsecond actuator-sensor response renders this device ideal for exploring the advantages of this technique. The high frequency capability allows us to extend the upper limits of the accessible linear viscoelastic spectrum and most importantly, to shorten the length of the interrogating strain signal (OWCh-PZR) to sub-second scales, while retaining a high time-bandwidth product. This short duration ensures that the mutation number (NMu) is kept sufficiently low, even in fast curing resins. The method is validated via calibration tests in both instruments and the corresponding limitations are discussed. As a proof of concept the technique is applied to a curing vinylester resin. The linear viscoelastic (LVE) spectrum is assessed every 20 seconds to monitor the rapid evolution of the time- and frequency-dependence of the complex modulus. Finally, FTIR spectroscopy is utilized to gain insights on the evolution of the chemical network while the gap-dependence of the evolving material properties in these heterogeneous systems is also investigated.","sentences":["Abstract   Knowledge of the evolution of mechanical properties of the curing matrix is of great importance in composite parts or structure fabrication.","Conventional rheometry, based on small amplitude oscillatory shear is limited by long interrogation times.","In rapidly evolving materials, time sweeps can provide a meaningful measurement albeit at a single frequency.","To overcome this constraint we utilize a combined frequency and amplitude-modulated chirped strain waveform in conjunction with a home-made sliding plate piezo-operated (PZR) and a dual-head commercial rotational rheometer (Anton Paar MCR 702) to probe the linear viscoelasticity of these time-evolving materials.","The direct controllability of the PZR resulting from the absence of any kind of firmware and the microsecond actuator-sensor response renders this device ideal for exploring the advantages of this technique.","The high frequency capability allows us to extend the upper limits of the accessible linear viscoelastic spectrum and most importantly, to shorten the length of the interrogating strain signal (OWCh-PZR) to sub-second scales, while retaining a high time-bandwidth product.","This short duration ensures that the mutation number (NMu) is kept sufficiently low, even in fast curing resins.","The method is validated via calibration tests in both instruments and the corresponding limitations are discussed.","As a proof of concept the technique is applied to a curing vinylester resin.","The linear viscoelastic (LVE) spectrum is assessed every 20 seconds to monitor the rapid evolution of the time- and frequency-dependence of the complex modulus.","Finally, FTIR spectroscopy is utilized to gain insights on the evolution of the chemical network while the gap-dependence of the evolving material properties in these heterogeneous systems is also investigated."],"url":"http://arxiv.org/abs/2405.07721v1","category":"cond-mat.soft"}
{"created":"2024-05-13 13:06:04","title":"Contract-Based Design for Hybrid Dynamical Systems and Invariance Properties","abstract":"This work establishes fundamental principles for verifying contract for interconnected hybrid systems. When system's hybrid arcs conform to the contract for a certain duration but subsequently violate it, the composition of hybrid dynamical systems becomes challenging. The objective of this work is to analyze the temporal satisfaction of the contract, allowing us to reason about the compositions that do not violate the contract up to a certain point in a hybrid time. Notions of weak and strong satisfaction of an assume-guarantee contract are introduced. These semantics permits the compositional reasoning on hybrid systems of varying complexity depending on the interconnection's type, feedback or cascade. The results show that both semantics are compatible with cascade composition, while strong semantic is required for feedback composition. Moreover, we have shown how one can go from weak to strong contract satisfaction. Finally, we have studied a particular class of hybrid systems and we have shown that the concept of forward (pre-)invariant relative to a contract makes it possible to deal with feedback compositions. These results are demonstrated throughout the paper with simple numerical examples.","sentences":["This work establishes fundamental principles for verifying contract for interconnected hybrid systems.","When system's hybrid arcs conform to the contract for a certain duration but subsequently violate it, the composition of hybrid dynamical systems becomes challenging.","The objective of this work is to analyze the temporal satisfaction of the contract, allowing us to reason about the compositions that do not violate the contract up to a certain point in a hybrid time.","Notions of weak and strong satisfaction of an assume-guarantee contract are introduced.","These semantics permits the compositional reasoning on hybrid systems of varying complexity depending on the interconnection's type, feedback or cascade.","The results show that both semantics are compatible with cascade composition, while strong semantic is required for feedback composition.","Moreover, we have shown how one can go from weak to strong contract satisfaction.","Finally, we have studied a particular class of hybrid systems and we have shown that the concept of forward (pre-)invariant relative to a contract makes it possible to deal with feedback compositions.","These results are demonstrated throughout the paper with simple numerical examples."],"url":"http://arxiv.org/abs/2405.07718v1","category":"eess.SY"}
{"created":"2024-05-13 13:03:33","title":"Evidence of What, for Whom? The Socially Contested Role of Algorithmic Bias in a Predictive Policing Tool","abstract":"This paper presents a critical, qualitative study of the social role of algorithmic bias in the context of the Chicago crime prediction algorithm, a predictive policing tool that forecasts when and where in the city crime is most likely to occur. Through interviews with 18 Chicago-area community organizations, academic researchers, and public sector actors, we show that stakeholders from different groups articulate diverse problem diagnoses of the tool's algorithmic bias, strategically using it as evidence to advance criminal justice interventions that align with stakeholders' positionality and political ends. Drawing inspiration from Catherine D'Ignazio's taxonomy of \"refusing and using\" data, we find that stakeholders use evidence of algorithmic bias to reform the policies around police patrol allocation; reject algorithm-based policing interventions; reframe crime as a structural rather than interpersonal problem; reveal data on authority figures in an effort to subvert their power; repair and heal families and communities; and, in the case of more powerful actors, to reaffirm their own authority or existing power structures. We identify the implicit assumptions and scope of these varied uses of algorithmic bias as evidence, showing that they require different (and sometimes conflicting) values about policing and AI. This divergence reflects long-standing tensions in the criminal justice reform landscape between the values of liberation and healing often centered by system-impacted communities and the values of surveillance and deterrence often instantiated in data-driven reform measures. We advocate for centering the interests and experiential knowledge of communities impacted by incarceration to ensure that evidence of algorithmic bias can serve as a device to challenge the status quo.","sentences":["This paper presents a critical, qualitative study of the social role of algorithmic bias in the context of the Chicago crime prediction algorithm, a predictive policing tool that forecasts when and where in the city crime is most likely to occur.","Through interviews with 18 Chicago-area community organizations, academic researchers, and public sector actors, we show that stakeholders from different groups articulate diverse problem diagnoses of the tool's algorithmic bias, strategically using it as evidence to advance criminal justice interventions that align with stakeholders' positionality and political ends.","Drawing inspiration from Catherine D'Ignazio's taxonomy of \"refusing and using\" data, we find that stakeholders use evidence of algorithmic bias to reform the policies around police patrol allocation; reject algorithm-based policing interventions; reframe crime as a structural rather than interpersonal problem; reveal data on authority figures in an effort to subvert their power; repair and heal families and communities; and, in the case of more powerful actors, to reaffirm their own authority or existing power structures.","We identify the implicit assumptions and scope of these varied uses of algorithmic bias as evidence, showing that they require different (and sometimes conflicting) values about policing and AI.","This divergence reflects long-standing tensions in the criminal justice reform landscape between the values of liberation and healing often centered by system-impacted communities and the values of surveillance and deterrence often instantiated in data-driven reform measures.","We advocate for centering the interests and experiential knowledge of communities impacted by incarceration to ensure that evidence of algorithmic bias can serve as a device to challenge the status quo."],"url":"http://arxiv.org/abs/2405.07715v1","category":"cs.CY"}
{"created":"2024-05-13 13:02:50","title":"Joint Robotic Aerial Base Station Deployment and Wireless Backhauling in 6G Multi-hop Networks","abstract":"Due to their ability to anchor into tall urban landforms, such as lampposts or street lights, robotic aerial base stations (RABSs) can create a hyper-flexible wireless multi-hop heterogeneous network to meet the forthcoming green, densified, and dynamic network deployment to support, inter alia, high data rates. In this work, we propose a network infrastructure that can concurrently support the wireless backhaul link capacity and access link traffic demand in the millimeter-wave (mmWave) frequency band. The RABSs grasping locations, resource blocks (RBs) assignment, and route flow control are simultaneously optimized to maximize the served traffic demands. Robotic base stations capitalize on the fact that traffic distribution varies considerably across both time and space within a given geographical area. Hence, they are able to relocate to suitable locations, i.e., 'follow' the traffic demand as it unfolds to increase the overall network efficiency. To tackle the curse of dimensionality of the proposed mixed-integer linear problem, we propose a greedy algorithm to obtain a competitive solution with low computational complexity. Compared to baseline models, which are heterogeneous networks with randomly deployed fixed small cells and pre-allocated RBs for wireless access and backhaul links, a wide set of numerical investigations reveals that robotic base stations could improve the served traffic demand. Specifically, the proposed mode serves at most 65\\% more traffic demand compared to an equal number of deployed fixed small cells.","sentences":["Due to their ability to anchor into tall urban landforms, such as lampposts or street lights, robotic aerial base stations (RABSs) can create a hyper-flexible wireless multi-hop heterogeneous network to meet the forthcoming green, densified, and dynamic network deployment to support, inter alia, high data rates.","In this work, we propose a network infrastructure that can concurrently support the wireless backhaul link capacity and access link traffic demand in the millimeter-wave (mmWave) frequency band.","The RABSs grasping locations, resource blocks (RBs) assignment, and route flow control are simultaneously optimized to maximize the served traffic demands.","Robotic base stations capitalize on the fact that traffic distribution varies considerably across both time and space within a given geographical area.","Hence, they are able to relocate to suitable locations, i.e., 'follow' the traffic demand as it unfolds to increase the overall network efficiency.","To tackle the curse of dimensionality of the proposed mixed-integer linear problem, we propose a greedy algorithm to obtain a competitive solution with low computational complexity.","Compared to baseline models, which are heterogeneous networks with randomly deployed fixed small cells and pre-allocated RBs for wireless access and backhaul links, a wide set of numerical investigations reveals that robotic base stations could improve the served traffic demand.","Specifically, the proposed mode serves at most 65\\% more traffic demand compared to an equal number of deployed fixed small cells."],"url":"http://arxiv.org/abs/2405.07714v1","category":"cs.NI"}
{"created":"2024-05-13 12:47:56","title":"Propagation-invariant strongly longitudinally polarized toroidal pulses","abstract":"Recent advancements in optical, terahertz, and microwave systems have unveiled non-transverse optical toroidal pulses characterized by skyrmionic topologies, fractal-like singularities, space-time nonseparability, and anapole-exciting ability. Despite this, the longitudinally polarized fields of canonical toroidal pulses notably lag behind their transverse counterparts in magnitude. Interestingly, although mushroom-cloud-like toroidal vortices with strong longitudinal fields are common in nature, they remain unexplored in the realm of electromagnetics. Here, we present strongly longitudinally polarized toroidal pulses (SLPTPs) which boast a longitudinal component amplitude exceeding that of the transverse component by over tenfold. This unique polarization property endows SLPTPs with robust propagation characteristics, showcasing nondiffracting behavior. The propagation-invariant strongly longitudinally polarized field holds promise for pioneering light-matter interactions, far-field superresolution microscopy, and high-capacity wireless communication utilizing three polarizations.","sentences":["Recent advancements in optical, terahertz, and microwave systems have unveiled non-transverse optical toroidal pulses characterized by skyrmionic topologies, fractal-like singularities, space-time nonseparability, and anapole-exciting ability.","Despite this, the longitudinally polarized fields of canonical toroidal pulses notably lag behind their transverse counterparts in magnitude.","Interestingly, although mushroom-cloud-like toroidal vortices with strong longitudinal fields are common in nature, they remain unexplored in the realm of electromagnetics.","Here, we present strongly longitudinally polarized toroidal pulses (SLPTPs) which boast a longitudinal component amplitude exceeding that of the transverse component by over tenfold.","This unique polarization property endows SLPTPs with robust propagation characteristics, showcasing nondiffracting behavior.","The propagation-invariant strongly longitudinally polarized field holds promise for pioneering light-matter interactions, far-field superresolution microscopy, and high-capacity wireless communication utilizing three polarizations."],"url":"http://arxiv.org/abs/2405.07706v1","category":"physics.optics"}
{"created":"2024-05-13 12:38:28","title":"Phase separation in a binary mixture of sticky spheres","abstract":"We numerically investigate the dependence of range of attractive potential on the phase separation of 2-D binary systems. Through extensive simulations and analysis, we show that when the range of attractive interactions approaches the sticky sphere limit, the system undergoes a phase separation at lower temperature. Further reduction in temperature causes the system to mix again. These mixing-demixing-mixing transitions are of first order. Such phase separation is not observed for systems with larger interaction range. In the phase separated region of the phase diagram, one of the components of the mixture chooses to be in crystalline configuration, while other being in disordered state","sentences":["We numerically investigate the dependence of range of attractive potential on the phase separation of 2-D binary systems.","Through extensive simulations and analysis, we show that when the range of attractive interactions approaches the sticky sphere limit, the system undergoes a phase separation at lower temperature.","Further reduction in temperature causes the system to mix again.","These mixing-demixing-mixing transitions are of first order.","Such phase separation is not observed for systems with larger interaction range.","In the phase separated region of the phase diagram, one of the components of the mixture chooses to be in crystalline configuration, while other being in disordered state"],"url":"http://arxiv.org/abs/2405.07701v1","category":"physics.comp-ph"}
{"created":"2024-05-13 12:34:18","title":"oTTC: Object Time-to-Contact for Motion Estimation in Autonomous Driving","abstract":"Autonomous driving systems require a quick and robust perception of the nearby environment to carry out their routines effectively. With the aim to avoid collisions and drive safely, autonomous driving systems rely heavily on object detection. However, 2D object detections alone are insufficient; more information, such as relative velocity and distance, is required for safer planning. Monocular 3D object detectors try to solve this problem by directly predicting 3D bounding boxes and object velocities given a camera image. Recent research estimates time-to-contact in a per-pixel manner and suggests that it is more effective measure than velocity and depth combined. However, per-pixel time-to-contact requires object detection to serve its purpose effectively and hence increases overall computational requirements as two different models need to run. To address this issue, we propose per-object time-to-contact estimation by extending object detection models to additionally predict the time-to-contact attribute for each object. We compare our proposed approach with existing time-to-contact methods and provide benchmarking results on well-known datasets. Our proposed approach achieves higher precision compared to prior art while using a single image.","sentences":["Autonomous driving systems require a quick and robust perception of the nearby environment to carry out their routines effectively.","With the aim to avoid collisions and drive safely, autonomous driving systems rely heavily on object detection.","However, 2D object detections alone are insufficient; more information, such as relative velocity and distance, is required for safer planning.","Monocular 3D object detectors try to solve this problem by directly predicting 3D bounding boxes and object velocities given a camera image.","Recent research estimates time-to-contact in a per-pixel manner and suggests that it is more effective measure than velocity and depth combined.","However, per-pixel time-to-contact requires object detection to serve its purpose effectively and hence increases overall computational requirements as two different models need to run.","To address this issue, we propose per-object time-to-contact estimation by extending object detection models to additionally predict the time-to-contact attribute for each object.","We compare our proposed approach with existing time-to-contact methods and provide benchmarking results on well-known datasets.","Our proposed approach achieves higher precision compared to prior art while using a single image."],"url":"http://arxiv.org/abs/2405.07698v1","category":"cs.CV"}
{"created":"2024-05-13 12:30:03","title":"The coupled-channel analysis of $B^{(*)}_{(s)}\\bar{B}^{(*)}_{(s)}$ systems within complex scaling method","abstract":"In present work, we perform a coupled-channel analysis of $B^{(*)}_{(s)}\\bar{B}^{(*)}_{(s)}$ systems with the one-boson-exchange potentials. We first study the $I(J^{PC})=1(1^{+-})$ $B\\bar{B}^{*}/B^{*}\\bar{B}^{*}$ system to describe the $Z_{b}(10610)$ and $Z_{b}(10650)$ particles as molecular states and determine the reasonable range of cutoff parameter $\\Lambda$. Then, other $B^{(*)}_{(s)}\\bar{B}^{(*)}_{(s)}$ combinations with different quantum numbers are systematically investigated. Some bound states and resonances appear in the isoscalar systems, while only several shallow bound states exist for isovector systems. Far away from the excited conventional $P-$wave bottomium, these predicted states can be easily identified as exotic particles both theoretically and experimentally. Moreover, the $\\eta_b(nS)/\\Upsilon(nS)$ plus light mesons are the excellent final states to search for the bound states, while the $B\\bar B^*+h.c.$ and $B^* \\bar B^*$ channels are suitable for the resonances. We highly recommend that the LHCb and Belle II Collaborations can hunt for these bottomonium-like states in future.","sentences":["In present work, we perform a coupled-channel analysis of $B^{(*)}_{(s)}\\bar{B}^{(*)}_{(s)}$ systems with the one-boson-exchange potentials.","We first study the $I(J^{PC})=1(1^{+-})$ $B\\bar{B}^{*}/B^{*}\\bar{B}^{*}$ system to describe the $Z_{b}(10610)$ and $Z_{b}(10650)$ particles as molecular states and determine the reasonable range of cutoff parameter $\\Lambda$. Then, other $B^{(*)}_{(s)}\\bar{B}^{(*)}_{(s)}$ combinations with different quantum numbers are systematically investigated.","Some bound states and resonances appear in the isoscalar systems, while only several shallow bound states exist for isovector systems.","Far away from the excited conventional $P-$wave bottomium, these predicted states can be easily identified as exotic particles both theoretically and experimentally.","Moreover, the $\\eta_b(nS)/\\Upsilon(nS)$ plus light mesons are the excellent final states to search for the bound states, while the $B\\bar B^*+h.c.$ and $B^* \\bar B^*$ channels are suitable for the resonances.","We highly recommend that the LHCb and Belle II Collaborations can hunt for these bottomonium-like states in future."],"url":"http://arxiv.org/abs/2405.07694v1","category":"hep-ph"}
{"created":"2024-05-13 12:23:13","title":"Quality of Experience Optimization for Real-time XR Video Transmission with Energy Constraints","abstract":"Extended Reality (XR) is an important service in the 5G network and in future 6G networks. In contrast to traditional video on demand services, real-time XR video is transmitted frame-by-frame, requiring low latency and being highly sensitive to network fluctuations. In this paper, we model the quality of experience (QoE) for real-time XR video transmission on a frame-by-frame basis. Based on the proposed QoE model, we formulate an optimization problem that maximizes QoE with constraints on wireless resources and long-term energy consumption. We utilize Lyapunov optimization to transform the original problem into a single-frame optimization problem and then allocate wireless subchannels. We propose an adaptive XR video bitrate algorithm that employs a Long Short Term Memory (LSTM) based Deep Q-Network (DQN) algorithm for video bitrate selection. Through numerical results, we show that our proposed algorithm outperforms the baseline algorithms, with the average QoE improvements of 0.04 to 0.46. Specifically, compared to baseline algorithms, the proposed algorithm reduces average video quality variations by 29% to 50% and improves the frame transmission success rate by 5% to 48%.","sentences":["Extended Reality (XR) is an important service in the 5G network and in future 6G networks.","In contrast to traditional video on demand services, real-time XR video is transmitted frame-by-frame, requiring low latency and being highly sensitive to network fluctuations.","In this paper, we model the quality of experience (QoE) for real-time XR video transmission on a frame-by-frame basis.","Based on the proposed QoE model, we formulate an optimization problem that maximizes QoE with constraints on wireless resources and long-term energy consumption.","We utilize Lyapunov optimization to transform the original problem into a single-frame optimization problem and then allocate wireless subchannels.","We propose an adaptive XR video bitrate algorithm that employs a Long Short Term Memory (LSTM) based Deep Q-Network (DQN) algorithm for video bitrate selection.","Through numerical results, we show that our proposed algorithm outperforms the baseline algorithms, with the average QoE improvements of 0.04 to 0.46.","Specifically, compared to baseline algorithms, the proposed algorithm reduces average video quality variations by 29% to 50% and improves the frame transmission success rate by 5% to 48%."],"url":"http://arxiv.org/abs/2405.07689v1","category":"cs.MM"}
{"created":"2024-05-13 12:21:59","title":"Highly Efficient Observation Process based on FFT Filtering for Robot Swarm Collaborative Navigation in Unknown Environments","abstract":"Collaborative path planning for robot swarms in complex, unknown environments without external positioning is a challenging problem. This requires robots to find safe directions based on real-time environmental observations, and to efficiently transfer and fuse these observations within the swarm. This study presents a filtering method based on Fast Fourier Transform (FFT) to address these two issues. We treat sensors' environmental observations as a digital sampling process. Then, we design two different types of filters for safe direction extraction, as well as for the compression and reconstruction of environmental data. The reconstructed data is mapped to probabilistic domain, achieving efficient fusion of swarm observations and planning decision. The computation time is only on the order of microseconds, and the transmission data in communication systems is in bit-level. The performance of our algorithm in sensor data processing was validated in real world experiments, and the effectiveness in swarm path optimization was demonstrated through extensive simulations.","sentences":["Collaborative path planning for robot swarms in complex, unknown environments without external positioning is a challenging problem.","This requires robots to find safe directions based on real-time environmental observations, and to efficiently transfer and fuse these observations within the swarm.","This study presents a filtering method based on Fast Fourier Transform (FFT) to address these two issues.","We treat sensors' environmental observations as a digital sampling process.","Then, we design two different types of filters for safe direction extraction, as well as for the compression and reconstruction of environmental data.","The reconstructed data is mapped to probabilistic domain, achieving efficient fusion of swarm observations and planning decision.","The computation time is only on the order of microseconds, and the transmission data in communication systems is in bit-level.","The performance of our algorithm in sensor data processing was validated in real world experiments, and the effectiveness in swarm path optimization was demonstrated through extensive simulations."],"url":"http://arxiv.org/abs/2405.07687v1","category":"cs.RO"}
{"created":"2024-05-13 12:19:12","title":"Constructive reachability for linear control problems under conic constraints","abstract":"Motivated by applications requiring sparse or nonnegative controls, we investigate reachability properties of linear infinite-dimensional control problems under conic constraints. Relaxing the problem to convex constraints if the initial cone is not already convex, we provide a constructive approach based on minimising a properly defined dual functional, which covers both the approximate and exact reachability problems. Our main results heavily rely on convex analysis, Fenchel duality and the Fenchel-Rockafellar theorem. As a byproduct, we uncover new sufficient conditions for approximate and exact reachability under convex conic constraints. We also prove that these conditions are in fact necessary. When the constraints are nonconvex, our method leads to sufficient conditions ensuring that the constructed controls fulfill the original constraints, which is in the flavour of bang-bang type properties. We show that our approach encompasses and generalises several works, and we obtain new results for different types of conic constraints and control systems.","sentences":["Motivated by applications requiring sparse or nonnegative controls, we investigate reachability properties of linear infinite-dimensional control problems under conic constraints.","Relaxing the problem to convex constraints if the initial cone is not already convex, we provide a constructive approach based on minimising a properly defined dual functional, which covers both the approximate and exact reachability problems.","Our main results heavily rely on convex analysis, Fenchel duality and the Fenchel-Rockafellar theorem.","As a byproduct, we uncover new sufficient conditions for approximate and exact reachability under convex conic constraints.","We also prove that these conditions are in fact necessary.","When the constraints are nonconvex, our method leads to sufficient conditions ensuring that the constructed controls fulfill the original constraints, which is in the flavour of bang-bang type properties.","We show that our approach encompasses and generalises several works, and we obtain new results for different types of conic constraints and control systems."],"url":"http://arxiv.org/abs/2405.07684v1","category":"math.OC"}
{"created":"2024-05-13 12:19:10","title":"Integral means spectrum functionals on Teichm\u00fcller spaces","abstract":"In this paper, we introduce and study the integral means spectrum (IMS) functionals on Teichm\\\"uller spaces. We show that the IMS functionals on the closure of the universal Teichm\\\"uller space and on the asymptotic universal Teichm\\\"uller space are both continuous. During the proof, we obtain some new results about the universal asymptotic Teichm\\\"uller space.","sentences":["In this paper, we introduce and study the integral means spectrum (IMS) functionals on Teichm\\\"uller spaces.","We show that the IMS functionals on the closure of the universal Teichm\\\"uller space and on the asymptotic universal Teichm\\\"uller space are both continuous.","During the proof, we obtain some new results about the universal asymptotic Teichm\\\"uller space."],"url":"http://arxiv.org/abs/2405.07683v1","category":"math.CV"}
{"created":"2024-05-13 12:07:48","title":"Class-wise Activation Unravelling the Engima of Deep Double Descent","abstract":"Double descent presents a counter-intuitive aspect within the machine learning domain, and researchers have observed its manifestation in various models and tasks. While some theoretical explanations have been proposed for this phenomenon in specific contexts, an accepted theory for its occurring mechanism in deep learning remains yet to be established. In this study, we revisited the phenomenon of double descent and discussed the conditions of its occurrence. This paper introduces the concept of class-activation matrices and a methodology for estimating the effective complexity of functions, on which we unveil that over-parameterized models exhibit more distinct and simpler class patterns in hidden activations compared to under-parameterized ones. We further looked into the interpolation of noisy labelled data among clean representations and demonstrated overfitting w.r.t. expressive capacity. By comprehensively analysing hypotheses and presenting corresponding empirical evidence that either validates or contradicts these hypotheses, we aim to provide fresh insights into the phenomenon of double descent and benign over-parameterization and facilitate future explorations. By comprehensively studying different hypotheses and the corresponding empirical evidence either supports or challenges these hypotheses, our goal is to offer new insights into the phenomena of double descent and benign over-parameterization, thereby enabling further explorations in the field. The source code is available at https://github.com/Yufei-Gu-451/sparse-generalization.git.","sentences":["Double descent presents a counter-intuitive aspect within the machine learning domain, and researchers have observed its manifestation in various models and tasks.","While some theoretical explanations have been proposed for this phenomenon in specific contexts, an accepted theory for its occurring mechanism in deep learning remains yet to be established.","In this study, we revisited the phenomenon of double descent and discussed the conditions of its occurrence.","This paper introduces the concept of class-activation matrices and a methodology for estimating the effective complexity of functions, on which we unveil that over-parameterized models exhibit more distinct and simpler class patterns in hidden activations compared to under-parameterized ones.","We further looked into the interpolation of noisy labelled data among clean representations and demonstrated overfitting w.r.t. expressive capacity.","By comprehensively analysing hypotheses and presenting corresponding empirical evidence that either validates or contradicts these hypotheses, we aim to provide fresh insights into the phenomenon of double descent and benign over-parameterization and facilitate future explorations.","By comprehensively studying different hypotheses and the corresponding empirical evidence either supports or challenges these hypotheses, our goal is to offer new insights into the phenomena of double descent and benign over-parameterization, thereby enabling further explorations in the field.","The source code is available at https://github.com/Yufei-Gu-451/sparse-generalization.git."],"url":"http://arxiv.org/abs/2405.07679v1","category":"cs.LG"}
{"created":"2024-05-13 11:59:24","title":"Constructing a BPE Tokenization DFA","abstract":"Many natural language processing systems operate over tokenizations of text to address the open-vocabulary problem. In this paper, we give and analyze an algorithm for the efficient construction of deterministic finite automata designed to operate directly on tokenizations produced by the popular byte pair encoding technique. This makes it possible to apply many existing techniques and algorithms to the tokenized case, such as pattern matching, equivalence checking of tokenization dictionaries, and composing tokenized languages in various ways.","sentences":["Many natural language processing systems operate over tokenizations of text to address the open-vocabulary problem.","In this paper, we give and analyze an algorithm for the efficient construction of deterministic finite automata designed to operate directly on tokenizations produced by the popular byte pair encoding technique.","This makes it possible to apply many existing techniques and algorithms to the tokenized case, such as pattern matching, equivalence checking of tokenization dictionaries, and composing tokenized languages in various ways."],"url":"http://arxiv.org/abs/2405.07671v1","category":"cs.FL"}
{"created":"2024-05-13 11:42:26","title":"A note on the topological synchronisation of unimodal maps","abstract":"In this note we complete the analysis carried on in [CGSV] about the topological synchronisation of unimodal maps of the interval coupled in a master-slave configuration, by answering to the questions raised in that paper. Namely, we compute the weak limits of the invariant measure of the coupled system as the coupling strength $k \\in (0,1)$ tends to $0$ and to $1$ and discuss the uniqueness of the invariant measure of its random dynamical system counterpart.   [CGSV] Caby Th., Gianfelice M., Saussol B., Vaienti S. \"Topological synchronisation or a simple attractor?\" Nonlinearity Vol. 36, no. 7, pp. 3603-3621 (2023).","sentences":["In this note we complete the analysis carried on in [CGSV] about the topological synchronisation of unimodal maps of the interval coupled in a master-slave configuration, by answering to the questions raised in that paper.","Namely, we compute the weak limits of the invariant measure of the coupled system as the coupling strength $k \\in (0,1)$ tends to $0$ and to $1$ and discuss the uniqueness of the invariant measure of its random dynamical system counterpart.   ","[CGSV] Caby Th., Gianfelice M., Saussol B., Vaienti S. \"Topological synchronisation or a simple attractor?\"","Nonlinearity Vol. 36, no. 7, pp. 3603-3621 (2023)."],"url":"http://arxiv.org/abs/2405.07661v1","category":"math.DS"}
{"created":"2024-05-13 11:06:43","title":"Ergodicity of skew-products over typical IETs","abstract":"We prove ergodicity of a class of infinite measure preserving systems, called skew-products. More precisely, we consider systems of the form \\[ \\operatorname{T_f}{[0, 1) \\times \\mathbb{R}}{[0, 1) \\times \\mathbb{R}}{(x, t)}{(T(x), t+f(x))}, \\] where $T$ is an interval exchange transformation and $f$ is a piece-wise constant function with a finite number of discontinuities. We show that such system is ergodic with respect to $\\operatorname{Leb}_{[0,1)\\times \\mathbb{R}}$ for a typical choice of parameters of $T$ and $f$.","sentences":["We prove ergodicity of a class of infinite measure preserving systems, called skew-products.","More precisely, we consider systems of the form \\[ \\operatorname{T_f}{[0, 1) \\times \\mathbb{R}}{[0, 1) \\times \\mathbb{R}}{(x, t)}{(T(x), t+f(x))}, \\] where $T$ is an interval exchange transformation and $f$ is a piece-wise constant function with a finite number of discontinuities.","We show that such system is ergodic with respect to $\\operatorname{Leb}_{[0,1)\\times \\mathbb{R}}$ for a typical choice of parameters of $T$ and $f$."],"url":"http://arxiv.org/abs/2405.07645v1","category":"math.DS"}
{"created":"2024-05-13 10:58:54","title":"Unveiling the Magmatic Architecture Beneath Oceanus Procellarum: Insights from GRAIL Mission Data","abstract":"The Oceanus Procellarum region, characterized by its vast basaltic plains and pronounced volcanic activity, serves as a focal point for understanding the volcanic history of the Moon. Leveraging the Gravity Recovery and Interior Laboratory (GRAIL) mission data, we imaged the magmatic structures beneath the Oceanus Procellarum region. Our 3D density models uncover pronounced linear magmatic structures along the Procellarum's western border and significant intrusions within the northern and southern Marius Hills. Crucially, they reveal three narrow near-horizontal sheeted magmatic structures, 80-150 km long, extending from near-surface to 6- 7 km depth, which we identified as sill-like magmatic conduits. These magmatic conduits connect the Marius Hills' northern and southern intrusions and bridge them with the Procellarum's western border structures. These discoveries suggest that sill-like magmatic conduits likely serve as central pathways facilitating magma transport across various volcanic systems and furthermore indicate widespread magmatic connectivity beneath the Oceanus Procellarum.","sentences":["The Oceanus Procellarum region, characterized by its vast basaltic plains and pronounced volcanic activity, serves as a focal point for understanding the volcanic history of the Moon.","Leveraging the Gravity Recovery and Interior Laboratory (GRAIL) mission data, we imaged the magmatic structures beneath the Oceanus Procellarum region.","Our 3D density models uncover pronounced linear magmatic structures along the Procellarum's western border and significant intrusions within the northern and southern Marius Hills.","Crucially, they reveal three narrow near-horizontal sheeted magmatic structures, 80-150 km long, extending from near-surface to 6- 7 km depth, which we identified as sill-like magmatic conduits.","These magmatic conduits connect the Marius Hills' northern and southern intrusions and bridge them with the Procellarum's western border structures.","These discoveries suggest that sill-like magmatic conduits likely serve as central pathways facilitating magma transport across various volcanic systems and furthermore indicate widespread magmatic connectivity beneath the Oceanus Procellarum."],"url":"http://arxiv.org/abs/2405.07639v1","category":"astro-ph.EP"}
{"created":"2024-05-13 10:26:19","title":"Toeplitz Operators and Berezin-type Operators on Different Bergman Spaces","abstract":"In the present paper, we study the boundedness and compactness of Toeplitz operators and Berezin-type operators between different weighted Bergman spaces over tubular domains in $\\mathbb{C}^n$. We establish their connection with Carleson measures and provide some characterizations.","sentences":["In the present paper, we study the boundedness and compactness of Toeplitz operators and Berezin-type operators between different weighted Bergman spaces over tubular domains in $\\mathbb{C}^n$. We establish their connection with Carleson measures and provide some characterizations."],"url":"http://arxiv.org/abs/2405.07618v1","category":"math.CV"}
{"created":"2024-05-13 10:24:29","title":"Direct electron beam writing of silver using a $\u03b2$-diketonate precursor: first insights","abstract":"Direct electron beam writing is a powerful tool for fabricating complex nanostructures in a single step. The electron beam locally cleaves the molecules of an adsorbed gaseous precursor to form a deposit, similar to 3D printing but without the need for a resist or development step. Here, we employ for the first time a silver $\\beta$-diketonate precursor for focused electron beam induced deposition (FEBID). The used compound (hfac)AgPMe$_3$ operates at an evaporation temperature of 70 - 80{\\deg}C and is compatible with commercially available gas injection systems used in any standard scanning electron microscope. Growth of smooth 3D geometries could be demonstrated for tightly focused electron beams, albeit with low silver contents in the deposit volume. The electron beam induced deposition proved sensitive to the irradiation conditions leading to varying compositions of the deposit and internal inhomogeneities such as the formation of a layered structure consisting of a pure silver layer at the interface to the substrate covered by a deposit layer with low silver content. Imaging after the deposition process revealed morphological changes such as the growth of silver particles on the surface. While these effects complicate the application for 3D printing, the unique deposit structure with a thin, compact silver film beneath the deposit body is interesting from a fundamental point of view and may offer additional opportunities for applications.","sentences":["Direct electron beam writing is a powerful tool for fabricating complex nanostructures in a single step.","The electron beam locally cleaves the molecules of an adsorbed gaseous precursor to form a deposit, similar to 3D printing but without the need for a resist or development step.","Here, we employ for the first time a silver $\\beta$-diketonate precursor for focused electron beam induced deposition (FEBID).","The used compound (hfac)AgPMe$_3$ operates at an evaporation temperature of 70 - 80{\\deg}C and is compatible with commercially available gas injection systems used in any standard scanning electron microscope.","Growth of smooth 3D geometries could be demonstrated for tightly focused electron beams, albeit with low silver contents in the deposit volume.","The electron beam induced deposition proved sensitive to the irradiation conditions leading to varying compositions of the deposit and internal inhomogeneities such as the formation of a layered structure consisting of a pure silver layer at the interface to the substrate covered by a deposit layer with low silver content.","Imaging after the deposition process revealed morphological changes such as the growth of silver particles on the surface.","While these effects complicate the application for 3D printing, the unique deposit structure with a thin, compact silver film beneath the deposit body is interesting from a fundamental point of view and may offer additional opportunities for applications."],"url":"http://arxiv.org/abs/2405.07617v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 10:22:41","title":"Exploration of oxyfluoride frameworks as Na-ion cathodes","abstract":"Na-ion batteries (NIBs) are increasingly looked at as a viable alternative to Li-ion batteries due to the abundance, low cost, and thermal stability of Na-based systems. To improve the practical utilization of NIBs in applications, it is important to boost the energy and power densities of the electrodes being used, via discovery of novel candidate materials. Thus, we explore the chemical space of transition metal containing oxyfluorides (TMOFs) that adopt the perovskite structure as possible NIB electrodes. Our choice of the perovskite structure is motivated by the `large' cationic tunnels that can accommodate Na$^+$, while the chemistry of TMOFs is motivated by the high electronegativity and inductive effect of F$^-$, which can possibly lead to higher voltages. We use density functional theory based calculations to estimate the ground state polymorphs, average Na (de)intercalation voltages, thermodynamic stabilities and Na$^+$ mobility on two distinct sets of compositions: the F-rich Na$_{x}$MOF$_{2}$, and the O-rich Na$_{1+x}$MO$_{2}$F where $x$ = 0--1 and M~=~Ti, V, Cr, Mn, Fe, Co, or Ni. Upon identifying the ground state polymorphs in the charged compositions (i.e., MOF$_2$ and NaMO$_2$F), we show that F-rich perovskites exhibit higher average voltages compared to O-rich perovskites. Also, we find six stable/metastable perovskites in the F-rich space, while all O-rich perovskites (except NaTiO$_2$F) are unstable. Finally, our Na-ion mobility calculations indicate that TiOF$_{2}$-NaTiOF$_2$, VOF$_{2}$-NaVOF$_2$, CrOF$_{2}$, and NaMnOF$_{2}$ can be promising compositions for experimental exploration as NIB cathodes, primarily if used in a strained electrode configuration and/or thin film batteries. Our computational approach and findings provide insights into developing practical NIBs involving fluorine-containing intercalation frameworks.","sentences":["Na-ion batteries (NIBs) are increasingly looked at as a viable alternative to Li-ion batteries due to the abundance, low cost, and thermal stability of Na-based systems.","To improve the practical utilization of NIBs in applications, it is important to boost the energy and power densities of the electrodes being used, via discovery of novel candidate materials.","Thus, we explore the chemical space of transition metal containing oxyfluorides (TMOFs) that adopt the perovskite structure as possible NIB electrodes.","Our choice of the perovskite structure is motivated by the `large' cationic tunnels that can accommodate Na$^+$, while the chemistry of TMOFs is motivated by the high electronegativity and inductive effect of F$^-$, which can possibly lead to higher voltages.","We use density functional theory based calculations to estimate the ground state polymorphs, average Na (de)intercalation voltages, thermodynamic stabilities and Na$^+$ mobility on two distinct sets of compositions: the F-rich Na$_{x}$MOF$_{2}$, and the O-rich Na$_{1+x}$MO$_{2}$F where $x$ = 0--1 and M~=~Ti, V, Cr, Mn, Fe, Co, or Ni.","Upon identifying the ground state polymorphs in the charged compositions (i.e., MOF$_2$ and NaMO$_2$F), we show that F-rich perovskites exhibit higher average voltages compared to O-rich perovskites.","Also, we find six stable/metastable perovskites in the F-rich space, while all O-rich perovskites (except NaTiO$_2$F) are unstable.","Finally, our Na-ion mobility calculations indicate that TiOF$_{2}$-NaTiOF$_2$, VOF$_{2}$-NaVOF$_2$, CrOF$_{2}$, and NaMnOF$_{2}$ can be promising compositions for experimental exploration as NIB cathodes, primarily if used in a strained electrode configuration and/or thin film batteries.","Our computational approach and findings provide insights into developing practical NIBs involving fluorine-containing intercalation frameworks."],"url":"http://arxiv.org/abs/2405.07614v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 10:22:17","title":"Simulating Floquet scrambling circuits on trapped-ion quantum computers","abstract":"Complex quantum many-body dynamics spread initially localized quantum information across the entire system. Information scrambling refers to such a process, whose simulation is one of the promising applications of quantum computing. We demonstrate the Hayden-Preskill recovery protocol and the interferometric protocol for calculating out-of-time-ordered correlators to study the scrambling property of a one-dimensional kicked-Ising model on 20-qubit trapped-ion quantum processors. The simulated quantum circuits have a geometrically local structure that exhibits the ballistic growth of entanglement, resulting in the circuit depth being linear in the number of qubits for the entire state to be scrambled. We experimentally confirm the growth of signals in the Hayden-Preskill recovery protocol and the decay of out-of-time-ordered correlators at late times. As an application of the created scrambling circuits, we also experimentally demonstrate the calculation of the microcanonical expectation values of local operators adopting the idea of thermal pure quantum states.","sentences":["Complex quantum many-body dynamics spread initially localized quantum information across the entire system.","Information scrambling refers to such a process, whose simulation is one of the promising applications of quantum computing.","We demonstrate the Hayden-Preskill recovery protocol and the interferometric protocol for calculating out-of-time-ordered correlators to study the scrambling property of a one-dimensional kicked-Ising model on 20-qubit trapped-ion quantum processors.","The simulated quantum circuits have a geometrically local structure that exhibits the ballistic growth of entanglement, resulting in the circuit depth being linear in the number of qubits for the entire state to be scrambled.","We experimentally confirm the growth of signals in the Hayden-Preskill recovery protocol and the decay of out-of-time-ordered correlators at late times.","As an application of the created scrambling circuits, we also experimentally demonstrate the calculation of the microcanonical expectation values of local operators adopting the idea of thermal pure quantum states."],"url":"http://arxiv.org/abs/2405.07613v1","category":"quant-ph"}
{"created":"2024-05-13 10:21:03","title":"Uncovering GNSS Interference with Aerial Mapping UAV","abstract":"Global Navigation Satellite System (GNSS) receivers provide ubiquitous and precise position, navigation, and time (PNT) to a wide gamut of civilian and tactical infrastructures and devices. Due to the low GNSS received signal power, even low-power radiofrequency interference (RFI) sources are a serious threat to the GNSS integrity and availability. Nonetheless, RFI source localization is paramount yet hard, especially over large areas. Methods based on multi-rotor unmanned aerial vehicles (UAV) exist but are often limited by hovering time, and require specific antenna and detectors. In comparison, fixed-wing planes allow longer missions but are more complex to operate and deploy. A vertical take-off and landing (VTOL) UAV combines the positive aspects of both platforms: high maneuverability, and long mission time and, jointly with highly integrated control systems, simple operation and deployment. Building upon the flexibility allowed by such a platform, we propose a method that combines advanced flight dynamics with high-performance consumer receivers to detect interference over large areas, with minimal interaction with the operator. The proposed system can detect multiple interference sources and map their area of influence, gaining situational awareness of poor GNSS quality or denied environments. Furthermore, it can estimate the relative heading and position of the interference source within tens of meters. The proposed method is validated with real-life measurements, successfully mapping two interference-affected areas and exposing radio equipment causing involuntary in-band interference.","sentences":["Global Navigation Satellite System (GNSS) receivers provide ubiquitous and precise position, navigation, and time (PNT) to a wide gamut of civilian and tactical infrastructures and devices.","Due to the low GNSS received signal power, even low-power radiofrequency interference (RFI) sources are a serious threat to the GNSS integrity and availability.","Nonetheless, RFI source localization is paramount yet hard, especially over large areas.","Methods based on multi-rotor unmanned aerial vehicles (UAV) exist but are often limited by hovering time, and require specific antenna and detectors.","In comparison, fixed-wing planes allow longer missions but are more complex to operate and deploy.","A vertical take-off and landing (VTOL) UAV combines the positive aspects of both platforms: high maneuverability, and long mission time and, jointly with highly integrated control systems, simple operation and deployment.","Building upon the flexibility allowed by such a platform, we propose a method that combines advanced flight dynamics with high-performance consumer receivers to detect interference over large areas, with minimal interaction with the operator.","The proposed system can detect multiple interference sources and map their area of influence, gaining situational awareness of poor GNSS quality or denied environments.","Furthermore, it can estimate the relative heading and position of the interference source within tens of meters.","The proposed method is validated with real-life measurements, successfully mapping two interference-affected areas and exposing radio equipment causing involuntary in-band interference."],"url":"http://arxiv.org/abs/2405.07611v1","category":"cs.CR"}
{"created":"2024-05-13 09:59:59","title":"Transferable Neural Wavefunctions for Solids","abstract":"Deep-Learning-based Variational Monte Carlo (DL-VMC) has recently emerged as a highly accurate approach for finding approximate solutions to the many-electron Schr\\\"odinger equation. Despite its favorable scaling with the number of electrons, $\\mathcal{O}(n_\\text{el}^{4})$, the practical value of DL-VMC is limited by the high cost of optimizing the neural network weights for every system studied. To mitigate this problem, recent research has proposed optimizing a single neural network across multiple systems, reducing the cost per system. Here we extend this approach to solids, where similar but distinct calculations using different geometries, boundary conditions, and supercell sizes are often required. We show how to optimize a single ansatz across all of these variations, reducing the required number of optimization steps by an order of magnitude. Furthermore, we exploit the transfer capabilities of a pre-trained network. We successfully transfer a network, pre-trained on 2x2x2 supercells of LiH, to 3x3x3 supercells. This reduces the number of optimization steps required to simulate the large system by a factor of 50 compared to previous work.","sentences":["Deep-Learning-based Variational Monte Carlo (DL-VMC) has recently emerged as a highly accurate approach for finding approximate solutions to the many-electron Schr\\\"odinger equation.","Despite its favorable scaling with the number of electrons, $\\mathcal{O}(n_\\text{el}^{4})$, the practical value of DL-VMC is limited by the high cost of optimizing the neural network weights for every system studied.","To mitigate this problem, recent research has proposed optimizing a single neural network across multiple systems, reducing the cost per system.","Here we extend this approach to solids, where similar but distinct calculations using different geometries, boundary conditions, and supercell sizes are often required.","We show how to optimize a single ansatz across all of these variations, reducing the required number of optimization steps by an order of magnitude.","Furthermore, we exploit the transfer capabilities of a pre-trained network.","We successfully transfer a network, pre-trained on 2x2x2 supercells of LiH, to 3x3x3 supercells.","This reduces the number of optimization steps required to simulate the large system by a factor of 50 compared to previous work."],"url":"http://arxiv.org/abs/2405.07599v1","category":"physics.comp-ph"}
{"created":"2024-05-13 09:55:05","title":"Unconditionally decoherence-free quantum error mitigation by density matrix vectorization","abstract":"Fighting against noise is crucial for NISQ devices to demonstrate practical quantum applications. In this work, we give a new paradigm of quantum error mitigation based on the vectorization of density matrices. Different from the ideas of existing quantum error mitigation methods that try to distill noiseless information from noisy quantum states, our proposal directly changes the way of encoding information and maps the density matrices of noisy quantum states to noiseless pure states, which is realized by a novel and NISQ-friendly measurement protocol and a classical post-processing procedure. Our protocol requires no knowledge of the noise model, no ability to tune the noise strength, and no ancilla qubits for complicated controlled unitaries. Under our encoding, NISQ devices are always preparing pure quantum states which are highly desired resources for variational quantum algorithms to have good performance in many tasks. We show how this protocol can be well-fitted into variational quantum algorithms. We give several concrete ansatz constructions that are suitable for our proposal and do theoretical analysis on the sampling complexity, the expressibility, and the trainability. We also give a discussion on how this protocol is influenced by large noise and how it can be well combined with other quantum error mitigation protocols. The effectiveness of our proposal is demonstrated by various numerical experiments.","sentences":["Fighting against noise is crucial for NISQ devices to demonstrate practical quantum applications.","In this work, we give a new paradigm of quantum error mitigation based on the vectorization of density matrices.","Different from the ideas of existing quantum error mitigation methods that try to distill noiseless information from noisy quantum states, our proposal directly changes the way of encoding information and maps the density matrices of noisy quantum states to noiseless pure states, which is realized by a novel and NISQ-friendly measurement protocol and a classical post-processing procedure.","Our protocol requires no knowledge of the noise model, no ability to tune the noise strength, and no ancilla qubits for complicated controlled unitaries.","Under our encoding, NISQ devices are always preparing pure quantum states which are highly desired resources for variational quantum algorithms to have good performance in many tasks.","We show how this protocol can be well-fitted into variational quantum algorithms.","We give several concrete ansatz constructions that are suitable for our proposal and do theoretical analysis on the sampling complexity, the expressibility, and the trainability.","We also give a discussion on how this protocol is influenced by large noise and how it can be well combined with other quantum error mitigation protocols.","The effectiveness of our proposal is demonstrated by various numerical experiments."],"url":"http://arxiv.org/abs/2405.07592v1","category":"quant-ph"}
{"created":"2024-05-13 09:50:39","title":"Practical Computation of Graph VC-Dimension","abstract":"For any set system $H=(V,R), \\ R \\subseteq 2^V$, a subset $S \\subseteq V$ is called \\emph{shattered} if every $S' \\subseteq S$ results from the intersection of $S$ with some set in $\\R$. The \\emph{VC-dimension} of $H$ is the size of a largest shattered set in $V$. In this paper, we focus on the problem of computing the VC-dimension of graphs. In particular, given a graph $G=(V,E)$, the VC-dimension of $G$ is defined as the VC-dimension of $(V, \\mathcal N)$, where $\\mathcal N$ contains each subset of $V$ that can be obtained as the closed neighborhood of some vertex $v \\in V$ in $G$. Our main contribution is an algorithm for computing the VC-dimension of any graph, whose effectiveness is shown through experiments on various types of practical graphs, including graphs with millions of vertices. A key aspect of its efficiency resides in the fact that practical graphs have small VC-dimension, up to 8 in our experiments. As a side-product, we present several new bounds relating the graph VC-dimension to other classical graph theoretical notions. We also establish the $W[1]$-hardness of the graph VC-dimension problem by extending a previous result for arbitrary set systems.","sentences":["For any set system $H=(V,R), \\ R \\subseteq 2^V$, a subset $S \\subseteq V$ is called \\emph{shattered} if every $S' \\subseteq S$ results from the intersection of $S$ with some set in $\\R$. The \\emph{VC-dimension} of $H$ is the size of a largest shattered set in $V$. In this paper, we focus on the problem of computing the VC-dimension of graphs.","In particular, given a graph $G=(V,E)$, the VC-dimension of $G$ is defined as the VC-dimension of $(V, \\mathcal N)$, where $\\mathcal N$ contains each subset of $V$ that can be obtained as the closed neighborhood of some vertex $v \\in V$ in $G$. Our main contribution is an algorithm for computing the VC-dimension of any graph, whose effectiveness is shown through experiments on various types of practical graphs, including graphs with millions of vertices.","A key aspect of its efficiency resides in the fact that practical graphs have small VC-dimension, up to 8 in our experiments.","As a side-product, we present several new bounds relating the graph VC-dimension to other classical graph theoretical notions.","We also establish the $W[1]$-hardness of the graph VC-dimension problem by extending a previous result for arbitrary set systems."],"url":"http://arxiv.org/abs/2405.07588v1","category":"cs.DS"}
{"created":"2024-05-13 09:47:02","title":"On the Coexistence of eMBB and URLLC in the Cell-Free Massive MIMO Downlink","abstract":"We investigate the non-orthogonal coexistence between the ultra-reliable low-latency communication (URLLC) and the enhanced mobile broadband (eMBB) in the downlink of a cell-free massive multiple-input multiple-output (MIMO) system. We provide a unified information-theoretic framework that combines a finite-blocklength analysis of the URLLC error probability based on the use of mismatched decoding with an infinite-blocklength analysis of the eMBB spectral efficiency. Superposition coding and three levels of puncturing are considered as alternative downlink coexistence strategies to cope with the inter-service interference and the URLLC random activation pattern, under the assumption of imperfect pilot-based channel state information acquisition at the access points and statistical channel knowledge at the users. Numerical results shed light into the trade-off between eMBB and URLLC performances considering different precoding and power control strategies.","sentences":["We investigate the non-orthogonal coexistence between the ultra-reliable low-latency communication (URLLC) and the enhanced mobile broadband (eMBB) in the downlink of a cell-free massive multiple-input multiple-output (MIMO) system.","We provide a unified information-theoretic framework that combines a finite-blocklength analysis of the URLLC error probability based on the use of mismatched decoding with an infinite-blocklength analysis of the eMBB spectral efficiency.","Superposition coding and three levels of puncturing are considered as alternative downlink coexistence strategies to cope with the inter-service interference and the URLLC random activation pattern, under the assumption of imperfect pilot-based channel state information acquisition at the access points and statistical channel knowledge at the users.","Numerical results shed light into the trade-off between eMBB and URLLC performances considering different precoding and power control strategies."],"url":"http://arxiv.org/abs/2405.07585v1","category":"cs.IT"}
{"created":"2024-05-13 09:29:40","title":"PRANK: a singular value based noise filtering approach","abstract":"High quality measurements are paramount to a successful application of experimental techniques in structural dynamics. The presence of noise and disturbances can significantly distort the information stored in the data and, if not adequately treated, may result in erroneous findings and misleading predictions. A common technique to filter out noise relies on decomposing the dataset into singular components sorted by their degree of significance. Discarding low-value contributions helps to clean the data and remove spuriousness. This paper presents PRANK, a novel singular value-based reconstruction approach for multiple-response vibration datasets. PRANK integrates the effect of Principal Response Functions and Hankel filtering actions, resulting in an improved data reconstruction for both system poles and zeros. The mixed formulation, incorporating the e-15 algorithm for automatic truncation, is tested on both analytical and numerical examples, showcasing its robustness, efficiency and versatility. PRANK operates with both time- and frequency-based data. Applied to noisy full-field camera measurements, the filter delivered excellent performance, indicating its potential for various identification tasks and applications in vibration analysis.","sentences":["High quality measurements are paramount to a successful application of experimental techniques in structural dynamics.","The presence of noise and disturbances can significantly distort the information stored in the data and, if not adequately treated, may result in erroneous findings and misleading predictions.","A common technique to filter out noise relies on decomposing the dataset into singular components sorted by their degree of significance.","Discarding low-value contributions helps to clean the data and remove spuriousness.","This paper presents PRANK, a novel singular value-based reconstruction approach for multiple-response vibration datasets.","PRANK integrates the effect of Principal Response Functions and Hankel filtering actions, resulting in an improved data reconstruction for both system poles and zeros.","The mixed formulation, incorporating the e-15 algorithm for automatic truncation, is tested on both analytical and numerical examples, showcasing its robustness, efficiency and versatility.","PRANK operates with both time- and frequency-based data.","Applied to noisy full-field camera measurements, the filter delivered excellent performance, indicating its potential for various identification tasks and applications in vibration analysis."],"url":"http://arxiv.org/abs/2405.07578v1","category":"eess.SP"}
{"created":"2024-05-13 09:27:31","title":"Distributed Nash Equilibrium Seeking in Aggregative Games over Jointly Connected and Weight-Balanced Networks","abstract":"The problem of the distributed Nash equilibrium seeking for aggregative games has been studied over strongly connected and weight-balanced static networks and every time strongly connected and weight-balanced switching networks. In this paper, we further study the same problem over jointly connected and weight-balanced networks. The existing approaches critically rely on the connectedness of the network for constructing a Lyapunov function for their algorithms and theses approaches fail if the network is not connected. To overcome this difficulty, we propose an approach to show the exponential convergence of the output of the closed-loop system to the unknown Nash equilibrium (NE) point under a set of mild conditions.","sentences":["The problem of the distributed Nash equilibrium seeking for aggregative games has been studied over strongly connected and weight-balanced static networks and every time strongly connected and weight-balanced switching networks.","In this paper, we further study the same problem over jointly connected and weight-balanced networks.","The existing approaches critically rely on the connectedness of the network for constructing a Lyapunov function for their algorithms and theses approaches fail if the network is not connected.","To overcome this difficulty, we propose an approach to show the exponential convergence of the output of the closed-loop system to the unknown Nash equilibrium (NE) point under a set of mild conditions."],"url":"http://arxiv.org/abs/2405.07576v1","category":"math.OC"}
{"created":"2024-05-13 09:24:54","title":"Gate-controlled superconducting proximity effect of superconductor/ferromagnet van der Waals heterostructures","abstract":"The discovery of 2D materials opens up unprecedented opportunities to design new materials with specified properties. In many cases, the design guiding principle is based on one or another proximity effect, i.e. the nanoscale-penetration of electronic correlations from one material to another. In a few layer van der Waals (vdW) heterostructures the proximity regions occupy the entire system. Here we demonstrate that the physics of magnetic and superconducting proximity effects in 2D superconductor/ferromagnet vdW heterostructures is determined by the effects of interface hybridization of the electronic spectra of both materials. The degree of hybridization can be adjusted by gating, which makes it possible to achieve a high degree of controllability of the proximity effect. In particular, we show that this allows for electrical switching of superconductivity in such structures on and off, as well as for control of the amplitude and sign of the Zeeman splitting of superconducting spectra.","sentences":["The discovery of 2D materials opens up unprecedented opportunities to design new materials with specified properties.","In many cases, the design guiding principle is based on one or another proximity effect, i.e. the nanoscale-penetration of electronic correlations from one material to another.","In a few layer van der Waals (vdW) heterostructures the proximity regions occupy the entire system.","Here we demonstrate that the physics of magnetic and superconducting proximity effects in 2D superconductor/ferromagnet vdW heterostructures is determined by the effects of interface hybridization of the electronic spectra of both materials.","The degree of hybridization can be adjusted by gating, which makes it possible to achieve a high degree of controllability of the proximity effect.","In particular, we show that this allows for electrical switching of superconductivity in such structures on and off, as well as for control of the amplitude and sign of the Zeeman splitting of superconducting spectra."],"url":"http://arxiv.org/abs/2405.07575v1","category":"cond-mat.supr-con"}
{"created":"2024-05-13 09:07:05","title":"Approximation and decomposition of attractors of a Hopfield neural network system","abstract":"In this paper, the Parameter Switching (PS) algorithm is used to approximate numerically attractors of a Hopfield Neural Network (HNN) system. The PS algorithm is a convergent scheme designed for approximating attractors of an autonomous nonlinear system, depending linearly on a real parameter. Aided by the PS algorithm, it is shown that every attractor of the HNN system can be expressed as a convex combination of other attractors. The HNN system can easily be written in the form of a linear parameter dependence system, to which the PS algorithm can be applied. This work suggests the possibility to use the PS algorithm as a control-like or anticontrol-like method for chaos.","sentences":["In this paper, the Parameter Switching (PS) algorithm is used to approximate numerically attractors of a Hopfield Neural Network (HNN) system.","The PS algorithm is a convergent scheme designed for approximating attractors of an autonomous nonlinear system, depending linearly on a real parameter.","Aided by the PS algorithm, it is shown that every attractor of the HNN system can be expressed as a convex combination of other attractors.","The HNN system can easily be written in the form of a linear parameter dependence system, to which the PS algorithm can be applied.","This work suggests the possibility to use the PS algorithm as a control-like or anticontrol-like method for chaos."],"url":"http://arxiv.org/abs/2405.07567v1","category":"nlin.CD"}
{"created":"2024-05-13 08:34:43","title":"Interpreting S-Parameter Spectra in Coupled Resonant Systems: The Role of Probing Configurations","abstract":"The S-parameter $S_{21}$ is widely used to characterize the resonant properties of various systems. However, we demonstrates that the reliability of $S_{21}$ as a true indicator of system resonances depends heavily on the specific probing setup employed. While point-probe or weak probes preserve the integrity of the $S_{21}$ spectrum and accurately reflect system resonances, multi-point and strong probes can introduce significant discrepancies. These discrepancies can manifest as misleading features such as repulsive level anti-crossing or attractive level crossing, even in systems that are fundamentally uncoupled. This highlights the critical importance of selecting appropriate probing techniques to ensure a precise evaluation of system resonances.","sentences":["The S-parameter $S_{21}$ is widely used to characterize the resonant properties of various systems.","However, we demonstrates that the reliability of $S_{21}$ as a true indicator of system resonances depends heavily on the specific probing setup employed.","While point-probe or weak probes preserve the integrity of the $S_{21}$ spectrum and accurately reflect system resonances, multi-point and strong probes can introduce significant discrepancies.","These discrepancies can manifest as misleading features such as repulsive level anti-crossing or attractive level crossing, even in systems that are fundamentally uncoupled.","This highlights the critical importance of selecting appropriate probing techniques to ensure a precise evaluation of system resonances."],"url":"http://arxiv.org/abs/2405.07555v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-13 08:31:49","title":"On Joint Marginal Expected Shortfall and Associated Contribution Risk Measures","abstract":"Systemic risk is the risk that a company- or industry-level risk could trigger a huge collapse of another or even the whole institution. Various systemic risk measures have been proposed in the literature to quantify the domino and (relative) spillover effects induced by systemic risks such as the well-known CoVaR, CoES, MES and CoD risk measures, and associated contribution measures. This paper proposes another new type of systemic risk measure, called the joint marginal expected shortfall (JMES), to measure whether the MES of one entity's risk-taking adds to another one or the overall risk conditioned on the event that the entity is already in some specified distress level. We further introduce two useful systemic risk contribution measures based on the difference function or relative ratio function of the JMES and the conventional ES, respectively. Some basic properties of these proposed measures are studied such as monotonicity, comonotonic additivity, non-identifiability and non-elicitability. For both risk measures and two different vectors of bivariate risks, we establish sufficient conditions imposed on copula structure, stress levels, and stochastic orders to compare these new measures. We further provide some numerical examples to illustrate our main findings. A real application in analyzing the risk contagion among several stock market indices is implemented to show the performances of our proposed measures compared with other commonly used measures including CoVaR, CoES, MES, and their associated contribution measures.","sentences":["Systemic risk is the risk that a company- or industry-level risk could trigger a huge collapse of another or even the whole institution.","Various systemic risk measures have been proposed in the literature to quantify the domino and (relative) spillover effects induced by systemic risks such as the well-known CoVaR, CoES, MES and CoD risk measures, and associated contribution measures.","This paper proposes another new type of systemic risk measure, called the joint marginal expected shortfall (JMES), to measure whether the MES of one entity's risk-taking adds to another one or the overall risk conditioned on the event that the entity is already in some specified distress level.","We further introduce two useful systemic risk contribution measures based on the difference function or relative ratio function of the JMES and the conventional ES, respectively.","Some basic properties of these proposed measures are studied such as monotonicity, comonotonic additivity, non-identifiability and non-elicitability.","For both risk measures and two different vectors of bivariate risks, we establish sufficient conditions imposed on copula structure, stress levels, and stochastic orders to compare these new measures.","We further provide some numerical examples to illustrate our main findings.","A real application in analyzing the risk contagion among several stock market indices is implemented to show the performances of our proposed measures compared with other commonly used measures including CoVaR, CoES, MES, and their associated contribution measures."],"url":"http://arxiv.org/abs/2405.07549v1","category":"q-fin.RM"}
{"created":"2024-05-13 08:29:20","title":"Existence of non-Abelian vortices in a coupled 4D-2D quantum field theory","abstract":"Vortices produce locally concentrated field configurations and are solutions to the nonlinear partial differential equations systems of complicated structures. In this paper, we establish the existence and uniqueness for solutions of the gauged non-Abelian vortices in a coupled 4D-2D quantum field theory by researching the nonlinear elliptic equations systems with exponential terms in $\\mathbb{R}^{2}$ using the calculus of variations. In addition, we obtain the asymptotic behavior of the solutions at infinity and the quantized integrals in $\\mathbb{R}^{2}$.","sentences":["Vortices produce locally concentrated field configurations and are solutions to the nonlinear partial differential equations systems of complicated structures.","In this paper, we establish the existence and uniqueness for solutions of the gauged non-Abelian vortices in a coupled 4D-2D quantum field theory by researching the nonlinear elliptic equations systems with exponential terms in $\\mathbb{R}^{2}$ using the calculus of variations.","In addition, we obtain the asymptotic behavior of the solutions at infinity and the quantized integrals in $\\mathbb{R}^{2}$."],"url":"http://arxiv.org/abs/2405.07548v1","category":"math.AP"}
{"created":"2024-05-13 08:27:07","title":"Deviations from random matrix entanglement statistics for kicked quantum chaotic spin-$1/2$ chains","abstract":"It is commonly expected that for quantum chaotic many body systems the statistical properties approach those of random matrices when increasing the system size. We demonstrate for various kicked spin-$1/2$ chain models that the average eigenstate entanglement indeed approaches the random matrix result. However, the distribution of the eigenstate entanglement differs significantly. While for autonomous systems such deviations are expected, they are surprising for the more scrambling kicked systems. We attribute the origin of the deviations to the local two-dimensional Hilbert spaces. This is also supported by similar deviations occurring in a local random matrix model with global diagonal coupling.","sentences":["It is commonly expected that for quantum chaotic many body systems the statistical properties approach those of random matrices when increasing the system size.","We demonstrate for various kicked spin-$1/2$ chain models that the average eigenstate entanglement indeed approaches the random matrix result.","However, the distribution of the eigenstate entanglement differs significantly.","While for autonomous systems such deviations are expected, they are surprising for the more scrambling kicked systems.","We attribute the origin of the deviations to the local two-dimensional Hilbert spaces.","This is also supported by similar deviations occurring in a local random matrix model with global diagonal coupling."],"url":"http://arxiv.org/abs/2405.07545v1","category":"quant-ph"}
{"created":"2024-05-13 08:19:02","title":"Mirroring the Parking Target: An Optimal-Control-Based Parking Motion Planner with Strengthened Parking Reliability and Faster Parking Completion","abstract":"Automated Parking Assist (APA) systems are now facing great challenges of low adoption in applications, due to users' concerns about parking capability, reliability, and completion efficiency. To upgrade the conventional APA planners and enhance user's acceptance, this research proposes an optimal-control-based parking motion planner. Its highlight lies in its control logic: planning trajectories by mirroring the parking target. This method enables: i) parking capability in narrow spaces; ii) better parking reliability by expanding Operation Design Domain (ODD); iii) faster completion of parking process; iv) enhanced computational efficiency; v) universal to all types of parking. A comprehensive evaluation is conducted. Results demonstrate the proposed planner does enhance parking success rate by 40.6%, improve parking completion efficiency by 18.0%, and expand ODD by 86.1%. It shows its superiority in difficult parking cases, such as the parallel parking scenario and narrow spaces. Moreover, the average computation time of the proposed planner is 74 milliseconds. Results indicate that the proposed planner is ready for real-time commercial applications.","sentences":["Automated Parking Assist (APA) systems are now facing great challenges of low adoption in applications, due to users' concerns about parking capability, reliability, and completion efficiency.","To upgrade the conventional APA planners and enhance user's acceptance, this research proposes an optimal-control-based parking motion planner.","Its highlight lies in its control logic: planning trajectories by mirroring the parking target.","This method enables: i) parking capability in narrow spaces; ii) better parking reliability by expanding Operation Design Domain (ODD); iii) faster completion of parking process; iv) enhanced computational efficiency; v) universal to all types of parking.","A comprehensive evaluation is conducted.","Results demonstrate the proposed planner does enhance parking success rate by 40.6%, improve parking completion efficiency by 18.0%, and expand ODD by 86.1%.","It shows its superiority in difficult parking cases, such as the parallel parking scenario and narrow spaces.","Moreover, the average computation time of the proposed planner is 74 milliseconds.","Results indicate that the proposed planner is ready for real-time commercial applications."],"url":"http://arxiv.org/abs/2405.07538v1","category":"cs.RO"}
{"created":"2024-05-13 08:06:39","title":"A study of layered holographic superconductor","abstract":"We have investigated a holographic model of a multi-layered superconductor in (2+1)-dimensions using the AdS/CFT correspondence. This correspondence allows us to study strongly interacting condensed matter systems through a weakly interacting gravitational theory. Our study focused on the effects of a finite system size on the superconductor's properties. We observed significant variations in the critical temperature and critical magnetic field depending on the number of layers and the spacing between them. Notably, our results capture the transition from a two-dimensional (2D) to a three-dimensional (3D) behavior in the limit of a large number of closely spaced layers. This transition signifies a change in how the superconductivity operates within real material.","sentences":["We have investigated a holographic model of a multi-layered superconductor in (2+1)-dimensions using the AdS/CFT correspondence.","This correspondence allows us to study strongly interacting condensed matter systems through a weakly interacting gravitational theory.","Our study focused on the effects of a finite system size on the superconductor's properties.","We observed significant variations in the critical temperature and critical magnetic field depending on the number of layers and the spacing between them.","Notably, our results capture the transition from a two-dimensional (2D) to a three-dimensional (3D) behavior in the limit of a large number of closely spaced layers.","This transition signifies a change in how the superconductivity operates within real material."],"url":"http://arxiv.org/abs/2405.07535v1","category":"hep-th"}
{"created":"2024-05-13 08:03:32","title":"DID Connect: Authentication in TLS with Decentralized Identifiers and Verifiable Credentials","abstract":"Authentication in TLS is predominately carried out with X.509 digital certificates issued by certificate authorities (CA). The centralized nature of current public key infrastructures, however, comes along with severe risks, such as single points of failure and susceptibility to cyber-attacks, potentially undermining the security and trustworthiness of the entire system. With Decentralized Identifiers (DID) alongside distributed ledger technology, it becomes technically feasible to prove ownership of a unique identifier without requiring an attestation of the proof's public key by a centralized and therefore vulnerable CA. This article presents DID Connect, a novel authentication scheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant way with self-issued X.509 certificates that are equipped with ledger-anchored DIDs instead of CA-issued identifiers. It facilitates the exchange of tamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable Credentials after the TLS handshake to complete the authentication with a full identification of the communication partner. A prototypical implementation shows comparable TLS handshake durations of DID Connect if verification material is cached and reasonable prolongations if it is obtained from a ledger. The significant speed improvement of the resulting TLS channel over a widely used, DID-based alternative transport protocol on the application layer demonstrates the potential of DID Connect to become a viable solution for the establishment of secure and trustful end-to-end communication links with decentrally managed digital identities.","sentences":["Authentication in TLS is predominately carried out with X.509 digital certificates issued by certificate authorities (CA).","The centralized nature of current public key infrastructures, however, comes along with severe risks, such as single points of failure and susceptibility to cyber-attacks, potentially undermining the security and trustworthiness of the entire system.","With Decentralized Identifiers (DID) alongside distributed ledger technology, it becomes technically feasible to prove ownership of a unique identifier without requiring an attestation of the proof's public key by a centralized and therefore vulnerable CA.","This article presents DID Connect, a novel authentication scheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant way with self-issued X.509 certificates that are equipped with ledger-anchored DIDs instead of CA-issued identifiers.","It facilitates the exchange of tamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable Credentials after the TLS handshake to complete the authentication with a full identification of the communication partner.","A prototypical implementation shows comparable TLS handshake durations of DID Connect if verification material is cached and reasonable prolongations if it is obtained from a ledger.","The significant speed improvement of the resulting TLS channel over a widely used, DID-based alternative transport protocol on the application layer demonstrates the potential of DID Connect to become a viable solution for the establishment of secure and trustful end-to-end communication links with decentrally managed digital identities."],"url":"http://arxiv.org/abs/2405.07533v1","category":"cs.CR"}
{"created":"2024-05-13 07:45:20","title":"HybridHash: Hybrid Convolutional and Self-Attention Deep Hashing for Image Retrieval","abstract":"Deep image hashing aims to map input images into simple binary hash codes via deep neural networks and thus enable effective large-scale image retrieval. Recently, hybrid networks that combine convolution and Transformer have achieved superior performance on various computer tasks and have attracted extensive attention from researchers. Nevertheless, the potential benefits of such hybrid networks in image retrieval still need to be verified. To this end, we propose a hybrid convolutional and self-attention deep hashing method known as HybridHash. Specifically, we propose a backbone network with stage-wise architecture in which the block aggregation function is introduced to achieve the effect of local self-attention and reduce the computational complexity. The interaction module has been elaborately designed to promote the communication of information between image blocks and to enhance the visual representations. We have conducted comprehensive experiments on three widely used datasets: CIFAR-10, NUS-WIDE and IMAGENET. The experimental results demonstrate that the method proposed in this paper has superior performance with respect to state-of-the-art deep hashing methods. Source code is available https://github.com/shuaichaochao/HybridHash.","sentences":["Deep image hashing aims to map input images into simple binary hash codes via deep neural networks and thus enable effective large-scale image retrieval.","Recently, hybrid networks that combine convolution and Transformer have achieved superior performance on various computer tasks and have attracted extensive attention from researchers.","Nevertheless, the potential benefits of such hybrid networks in image retrieval still need to be verified.","To this end, we propose a hybrid convolutional and self-attention deep hashing method known as HybridHash.","Specifically, we propose a backbone network with stage-wise architecture in which the block aggregation function is introduced to achieve the effect of local self-attention and reduce the computational complexity.","The interaction module has been elaborately designed to promote the communication of information between image blocks and to enhance the visual representations.","We have conducted comprehensive experiments on three widely used datasets: CIFAR-10, NUS-WIDE and IMAGENET.","The experimental results demonstrate that the method proposed in this paper has superior performance with respect to state-of-the-art deep hashing methods.","Source code is available https://github.com/shuaichaochao/HybridHash."],"url":"http://arxiv.org/abs/2405.07524v1","category":"cs.CV"}
{"created":"2024-05-13 07:21:38","title":"KMT-2023-BLG-1866Lb: Microlensing super-Earth around an M dwarf host","abstract":"We investigate the nature of the short-term anomaly that appears in the lensing light curve of KMT-2023-BLG-1866. The anomaly was only partly covered due to its short duration, less than a day, coupled with cloudy weather conditions and restricted nighttime duration. Considering intricacy of interpreting partially covered signals, we thoroughly explore all potential degenerate solutions. Through this process, we identify three planetary scenarios that equally well account for the observed anomaly. These scenarios are characterized by the specific planetary parameters: $(s, q)_{\\rm inner} = [0.9740 \\pm 0.0083, (2.46 \\pm 1.07) \\times 10^{-5}]$, $(s, q)_{\\rm intermediate} = [0.9779 \\pm 0.0017, (1.56 \\pm 0.25)\\times 10^{-5}]$, and $(s, q)_{\\rm outer} = [0.9894 \\pm 0.0107, (2.31 \\pm 1.29)\\times 10^{-5}]$, where $s$ and $q$ denote the projected separation (scaled to the Einstein radius) and mass ratio between the planet and its host, respectively. We identify that the ambiguity between the inner and outer solutions stems from the inner-outer degeneracy, while the similarity between the intermediate solution and the others is due to an accidental degeneracy caused by incomplete anomaly coverage. Through Bayesian analysis utilizing the constraints derived from measured lensing observables and blending flux, our estimation indicates that the lens system comprises a very low-mass planet orbiting an early M-type star situated approximately (6.2 -- 6.5)~kpc from Earth in terms of median posterior values for the different solutions. The median mass of the planet host is in the range of (0.48 -- 0.51)~$M_\\odot$, and that of the planet's mass spans a range of (2.6 -- 4.0)~$M_{\\rm E}$, varying across different solutions. The detection of KMT-2023-BLG-1866Lb signifies the extension of the lensing surveys to very low-mass planets that have been difficult to be detected from earlier surveys.","sentences":["We investigate the nature of the short-term anomaly that appears in the lensing light curve of KMT-2023-BLG-1866.","The anomaly was only partly covered due to its short duration, less than a day, coupled with cloudy weather conditions and restricted nighttime duration.","Considering intricacy of interpreting partially covered signals, we thoroughly explore all potential degenerate solutions.","Through this process, we identify three planetary scenarios that equally well account for the observed anomaly.","These scenarios are characterized by the specific planetary parameters: $(s, q)_{\\rm inner} =","[0.9740 \\pm 0.0083, (2.46 \\pm 1.07) \\times 10^{-5}]$, $(s, q)_{\\rm intermediate} =","[0.9779 \\pm 0.0017, (1.56 \\pm 0.25)\\times 10^{-5}]$, and $(s, q)_{\\rm outer} =","[0.9894 \\pm 0.0107, (2.31 \\pm 1.29)\\times 10^{-5}]$, where $s$ and $q$ denote the projected separation (scaled to the Einstein radius) and mass ratio between the planet and its host, respectively.","We identify that the ambiguity between the inner and outer solutions stems from the inner-outer degeneracy, while the similarity between the intermediate solution and the others is due to an accidental degeneracy caused by incomplete anomaly coverage.","Through Bayesian analysis utilizing the constraints derived from measured lensing observables and blending flux, our estimation indicates that the lens system comprises a very low-mass planet orbiting an early M-type star situated approximately (6.2 -- 6.5)~kpc from Earth in terms of median posterior values for the different solutions.","The median mass of the planet host is in the range of (0.48 -- 0.51)~$M_\\odot$, and that of the planet's mass spans a range of (2.6 -- 4.0)~$M_{\\rm E}$, varying across different solutions.","The detection of KMT-2023-BLG-1866Lb signifies the extension of the lensing surveys to very low-mass planets that have been difficult to be detected from earlier surveys."],"url":"http://arxiv.org/abs/2405.07514v1","category":"astro-ph.EP"}
{"created":"2024-05-13 07:12:01","title":"Bifurcation analysis of the problem of a \"rubber\" ellipsoid of revolution rolling on a plane","abstract":"This paper is concerned with the problem of an ellipsoid of revolution rolling on a horizontal plane under the assumption that there is no slipping at the point of contact and no spinning about the vertical. A reduction of the equations of motion to a fixed level set of first integrals is performed. Permanent rotations corresponding to the rolling of an ellipsoid in a circle or in a straight line are found. A linear stability analysis of permanent rotations is carried out. A complete classification of possible trajectories of the reduced system is performed using a bifurcation analysis. A classification of the trajectories of the center of mass of the ellipsoid depending on parameter values and initial conditions is performed.","sentences":["This paper is concerned with the problem of an ellipsoid of revolution rolling on a horizontal plane under the assumption that there is no slipping at the point of contact and no spinning about the vertical.","A reduction of the equations of motion to a fixed level set of first integrals is performed.","Permanent rotations corresponding to the rolling of an ellipsoid in a circle or in a straight line are found.","A linear stability analysis of permanent rotations is carried out.","A complete classification of possible trajectories of the reduced system is performed using a bifurcation analysis.","A classification of the trajectories of the center of mass of the ellipsoid depending on parameter values and initial conditions is performed."],"url":"http://arxiv.org/abs/2405.07511v1","category":"math.DS"}
{"created":"2024-05-13 07:07:54","title":"Revealing the value of Repository Centrality in lifespan prediction of Open Source Software Projects","abstract":"Background: Open Source Software is the building block of modern software. However, the prevalence of project deprecation in the open source world weakens the integrity of the downstream systems and the broad ecosystem. Therefore it calls for efforts in monitoring and predicting project deprecations, empowering stakeholders to take proactive measures. Challenge: Existing techniques mainly focus on static features on a point in time to make predictions, resulting in limited effects. Goal: We propose a novel metric from the user-repository network, and leverage the metric to fit project deprecation predictors and prove its real-life implications. Method: We establish a comprehensive dataset containing 103,354 non-fork GitHub OSS projects spanning from 2011 to 2023. We propose repository centrality, a family of HITS weights that captures shifts in the popularity of a repository in the repository-user star network. Further with the metric, we utilize the advancements in gradient boosting and deep learning to fit survival analysis models to predict project lifespan or its survival hazard. Results: Our study reveals a correlation between the HITS centrality metrics and the repository deprecation risk. A drop in the HITS weights of a repository indicates a decline in its centrality and prevalence, leading to an increase in its deprecation risk and a decrease in its expected lifespan. Our predictive models powered by repository centrality and other repository features achieve satisfactory accuracy on the test set, with repository centrality being the most significant feature among all. Implications: This research offers a novel perspective on understanding the effect of prevalence on the deprecation of OSS repositories. Our approach to predict repository deprecation help detect health status of project and take actions in advance, fostering a more resilient OSS ecosystem.","sentences":["Background: Open Source Software is the building block of modern software.","However, the prevalence of project deprecation in the open source world weakens the integrity of the downstream systems and the broad ecosystem.","Therefore it calls for efforts in monitoring and predicting project deprecations, empowering stakeholders to take proactive measures.","Challenge:","Existing techniques mainly focus on static features on a point in time to make predictions, resulting in limited effects.","Goal: We propose a novel metric from the user-repository network, and leverage the metric to fit project deprecation predictors and prove its real-life implications.","Method: We establish a comprehensive dataset containing 103,354 non-fork GitHub OSS projects spanning from 2011 to 2023.","We propose repository centrality, a family of HITS weights that captures shifts in the popularity of a repository in the repository-user star network.","Further with the metric, we utilize the advancements in gradient boosting and deep learning to fit survival analysis models to predict project lifespan or its survival hazard.","Results:","Our study reveals a correlation between the HITS centrality metrics and the repository deprecation risk.","A drop in the HITS weights of a repository indicates a decline in its centrality and prevalence, leading to an increase in its deprecation risk and a decrease in its expected lifespan.","Our predictive models powered by repository centrality and other repository features achieve satisfactory accuracy on the test set, with repository centrality being the most significant feature among all.","Implications: This research offers a novel perspective on understanding the effect of prevalence on the deprecation of OSS repositories.","Our approach to predict repository deprecation help detect health status of project and take actions in advance, fostering a more resilient OSS ecosystem."],"url":"http://arxiv.org/abs/2405.07508v1","category":"cs.SE"}
{"created":"2024-05-13 07:00:30","title":"Chronoblox: Chronophotographic Sequential Graph Visualization","abstract":"We introduce Chronoblox, a system for visualizing dynamic graphs. Chronoblox consists of a chronophotography of a sequence of graph snapshots based on a single embedding space common to all time periods. The goal of Chronoblox is to project all snapshots onto a common visualization space so as to represent both local and global dynamics at a glance. In this short paper, we review both the embedding and spatialization strategies. We then explain the way in which Chronoblox translates micro to meso structural evolution visually. We finally evaluate our approach using a synthetic network before illustrating it on a real world retweet network.","sentences":["We introduce Chronoblox, a system for visualizing dynamic graphs.","Chronoblox consists of a chronophotography of a sequence of graph snapshots based on a single embedding space common to all time periods.","The goal of Chronoblox is to project all snapshots onto a common visualization space so as to represent both local and global dynamics at a glance.","In this short paper, we review both the embedding and spatialization strategies.","We then explain the way in which Chronoblox translates micro to meso structural evolution visually.","We finally evaluate our approach using a synthetic network before illustrating it on a real world retweet network."],"url":"http://arxiv.org/abs/2405.07506v1","category":"cs.HC"}
{"created":"2024-05-13 06:57:49","title":"A cyclic proof system for Guarded Kleene Algebra with Tests (full version)","abstract":"Guarded Kleene Algebra with Tests (GKAT for short) is an efficient fragment of Kleene Algebra with Tests, suitable for reasoning about simple imperative while-programs. Following earlier work by Das and Pous on Kleene Algebra, we study GKAT from a proof-theoretical perspective. The deterministic nature of GKAT allows for a non-well-founded sequent system whose set of regular proofs is complete with respect to the guarded language model. This is unlike the situation with Kleene Algebra, where hypersequents are required. Moreover, the decision procedure induced by proof search runs in NLOGSPACE, whereas that of Kleene Algebra is in PSPACE.","sentences":["Guarded Kleene Algebra with Tests (GKAT for short) is an efficient fragment of Kleene Algebra with Tests, suitable for reasoning about simple imperative while-programs.","Following earlier work by Das and Pous on Kleene Algebra, we study GKAT from a proof-theoretical perspective.","The deterministic nature of GKAT allows for a non-well-founded sequent system whose set of regular proofs is complete with respect to the guarded language model.","This is unlike the situation with Kleene Algebra, where hypersequents are required.","Moreover, the decision procedure induced by proof search runs in NLOGSPACE, whereas that of Kleene Algebra is in PSPACE."],"url":"http://arxiv.org/abs/2405.07505v1","category":"cs.LO"}
{"created":"2024-05-13 06:32:57","title":"Oedipus: LLM-enchanced Reasoning CAPTCHA Solver","abstract":"CAPTCHAs have become a ubiquitous tool in safeguarding applications from automated bots. Over time, the arms race between CAPTCHA development and evasion techniques has led to increasingly sophisticated and diverse designs. The latest iteration, reasoning CAPTCHAs, exploits tasks that are intuitively simple for humans but challenging for conventional AI technologies, thereby enhancing security measures.   Driven by the evolving AI capabilities, particularly the advancements in Large Language Models (LLMs), we investigate the potential of multimodal LLMs to solve modern reasoning CAPTCHAs. Our empirical analysis reveals that, despite their advanced reasoning capabilities, LLMs struggle to solve these CAPTCHAs effectively. In response, we introduce Oedipus, an innovative end-to-end framework for automated reasoning CAPTCHA solving. Central to this framework is a novel strategy that dissects the complex and human-easy-AI-hard tasks into a sequence of simpler and AI-easy steps. This is achieved through the development of a Domain Specific Language (DSL) for CAPTCHAs that guides LLMs in generating actionable sub-steps for each CAPTCHA challenge. The DSL is customized to ensure that each unit operation is a highly solvable subtask revealed in our previous empirical study. These sub-steps are then tackled sequentially using the Chain-of-Thought (CoT) methodology.   Our evaluation shows that Oedipus effectively resolves the studied CAPTCHAs, achieving an average success rate of 63.5\\%. Remarkably, it also shows adaptability to the most recent CAPTCHA designs introduced in late 2023, which are not included in our initial study. This prompts a discussion on future strategies for designing reasoning CAPTCHAs that can effectively counter advanced AI solutions.","sentences":["CAPTCHAs have become a ubiquitous tool in safeguarding applications from automated bots.","Over time, the arms race between CAPTCHA development and evasion techniques has led to increasingly sophisticated and diverse designs.","The latest iteration, reasoning CAPTCHAs, exploits tasks that are intuitively simple for humans but challenging for conventional AI technologies, thereby enhancing security measures.   ","Driven by the evolving AI capabilities, particularly the advancements in Large Language Models (LLMs), we investigate the potential of multimodal LLMs to solve modern reasoning CAPTCHAs.","Our empirical analysis reveals that, despite their advanced reasoning capabilities, LLMs struggle to solve these CAPTCHAs effectively.","In response, we introduce Oedipus, an innovative end-to-end framework for automated reasoning CAPTCHA solving.","Central to this framework is a novel strategy that dissects the complex and human-easy-AI-hard tasks into a sequence of simpler and AI-easy steps.","This is achieved through the development of a Domain Specific Language (DSL) for CAPTCHAs that guides LLMs in generating actionable sub-steps for each CAPTCHA challenge.","The DSL is customized to ensure that each unit operation is a highly solvable subtask revealed in our previous empirical study.","These sub-steps are then tackled sequentially using the Chain-of-Thought (CoT) methodology.   ","Our evaluation shows that Oedipus effectively resolves the studied CAPTCHAs, achieving an average success rate of 63.5\\%.","Remarkably, it also shows adaptability to the most recent CAPTCHA designs introduced in late 2023, which are not included in our initial study.","This prompts a discussion on future strategies for designing reasoning CAPTCHAs that can effectively counter advanced AI solutions."],"url":"http://arxiv.org/abs/2405.07496v1","category":"cs.CR"}
{"created":"2024-05-13 06:13:16","title":"Laser-painted cavity-mediated interactions in a quantum gas","abstract":"Experimental platforms based on ultracold atomic gases have significantly advanced the quantum simulation of complex systems, yet the exploration of phenomena driven by long-range interactions remains a formidable challenge. Currently available methods utilizing dipolar quantum gases or multi-mode cavities allow to implement long-range interactions with a $1/r^3$ character or with a spatial profile fixed by the mode-structure of the vacuum electromagnetic field surrounding the atoms, respectively. Here we propose an experimental scheme employing laser-painted cavity-mediated interactions, which enables the realization of atom-atom interactions that are fully tunable in range, shape, and sign. Our approach combines the versatility of cavity quantum electrodynamics with the precision of laser manipulation, thus providing a highly flexible platform for simulating and understanding long-range interactions in quantum many-body systems. Our analytical predictions are supported by numerical simulations describing the full dynamics of atoms, laser, and cavity. The latter demonstrate that there is a wide and experimentally accessible parameter regime where our protocol optimally works. The methodology not only paves the way for exploring new territories in quantum simulation but also enhances the understanding of fundamental physics, potentially leading to the discovery of novel quantum states and phases.","sentences":["Experimental platforms based on ultracold atomic gases have significantly advanced the quantum simulation of complex systems, yet the exploration of phenomena driven by long-range interactions remains a formidable challenge.","Currently available methods utilizing dipolar quantum gases or multi-mode cavities allow to implement long-range interactions with a $1/r^3$ character or with a spatial profile fixed by the mode-structure of the vacuum electromagnetic field surrounding the atoms, respectively.","Here we propose an experimental scheme employing laser-painted cavity-mediated interactions, which enables the realization of atom-atom interactions that are fully tunable in range, shape, and sign.","Our approach combines the versatility of cavity quantum electrodynamics with the precision of laser manipulation, thus providing a highly flexible platform for simulating and understanding long-range interactions in quantum many-body systems.","Our analytical predictions are supported by numerical simulations describing the full dynamics of atoms, laser, and cavity.","The latter demonstrate that there is a wide and experimentally accessible parameter regime where our protocol optimally works.","The methodology not only paves the way for exploring new territories in quantum simulation but also enhances the understanding of fundamental physics, potentially leading to the discovery of novel quantum states and phases."],"url":"http://arxiv.org/abs/2405.07492v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-13 06:10:48","title":"Bifurcation Patterns and Chaos Control in Discrete-Time Coral Reef Model","abstract":"The reduction in coral reef densities, characterized by the proliferation of macroalgae, has emerged as a global threat. In this paper, we present a discrete-time coral reef dynamical model that incorporates macroalgae. We explore all ecologically possible equilibrium points for the proposed model. The conditions for the local stability of the interior equilibrium point are analyzed, which represents the coexistence of both coral and macroalgae. Furthermore, we investigate the model's behavior using the center manifold theorem and bifurcation theory. Our analysis reveals that the model undergoes codimension-one bifurcations, specifically period-doubling and Neimark-Sacker bifurcations. To address the chaos resulting from the emergence of the Neimark-Sacker bifurcation, we apply the OGY feedback control method and a hybrid control methodology. Finally, we provide numerical simulations not only to validate the obtained results but also to demonstrate the complex dynamic behaviors that arise. These behaviors include reversal period-doubling bifurcation, period-4, 8, and 24 bubble bifurcations, as well as chaotic behavior.","sentences":["The reduction in coral reef densities, characterized by the proliferation of macroalgae, has emerged as a global threat.","In this paper, we present a discrete-time coral reef dynamical model that incorporates macroalgae.","We explore all ecologically possible equilibrium points for the proposed model.","The conditions for the local stability of the interior equilibrium point are analyzed, which represents the coexistence of both coral and macroalgae.","Furthermore, we investigate the model's behavior using the center manifold theorem and bifurcation theory.","Our analysis reveals that the model undergoes codimension-one bifurcations, specifically period-doubling and Neimark-Sacker bifurcations.","To address the chaos resulting from the emergence of the Neimark-Sacker bifurcation, we apply the OGY feedback control method and a hybrid control methodology.","Finally, we provide numerical simulations not only to validate the obtained results but also to demonstrate the complex dynamic behaviors that arise.","These behaviors include reversal period-doubling bifurcation, period-4, 8, and 24 bubble bifurcations, as well as chaotic behavior."],"url":"http://arxiv.org/abs/2405.07491v1","category":"math.DS"}
{"created":"2024-05-13 06:04:26","title":"Predictive Modeling of Flexible EHD Pumps using Kolmogorov-Arnold Networks","abstract":"We present a novel approach to predicting the pressure and flow rate of flexible electrohydrodynamic pumps using the Kolmogorov-Arnold Network. Inspired by the Kolmogorov-Arnold representation theorem, KAN replaces fixed activation functions with learnable spline-based activation functions, enabling it to approximate complex nonlinear functions more effectively than traditional models like Multi-Layer Perceptron and Random Forest. We evaluated KAN on a dataset of flexible EHD pump parameters and compared its performance against RF, and MLP models. KAN achieved superior predictive accuracy, with Mean Squared Errors of 12.186 and 0.001 for pressure and flow rate predictions, respectively. The symbolic formulas extracted from KAN provided insights into the nonlinear relationships between input parameters and pump performance. These findings demonstrate that KAN offers exceptional accuracy and interpretability, making it a promising alternative for predictive modeling in electrohydrodynamic pumping.","sentences":["We present a novel approach to predicting the pressure and flow rate of flexible electrohydrodynamic pumps using the Kolmogorov-Arnold Network.","Inspired by the Kolmogorov-Arnold representation theorem, KAN replaces fixed activation functions with learnable spline-based activation functions, enabling it to approximate complex nonlinear functions more effectively than traditional models like Multi-Layer Perceptron and Random Forest.","We evaluated KAN on a dataset of flexible EHD pump parameters and compared its performance against RF, and MLP models.","KAN achieved superior predictive accuracy, with Mean Squared Errors of 12.186 and 0.001 for pressure and flow rate predictions, respectively.","The symbolic formulas extracted from KAN provided insights into the nonlinear relationships between input parameters and pump performance.","These findings demonstrate that KAN offers exceptional accuracy and interpretability, making it a promising alternative for predictive modeling in electrohydrodynamic pumping."],"url":"http://arxiv.org/abs/2405.07488v1","category":"cs.LG"}
{"created":"2024-05-13 06:00:19","title":"A Room-Temperature Solid-State Maser Amplifier","abstract":"Masers once represented the state-of-the-art in low noise microwave amplification technology, but eventually became obsolete due to their need for cryogenic cooling. Masers based on solid-state spin systems perform most effectively as amplifiers, since they provide a large density of spins and can therefore operate at relatively high powers. Whilst solid-state masers oscillators have been demonstrated at room temperature, continuous-wave amplification in these systems has only ever been realized at cryogenic temperatures. Here we report on a continuous-wave solid-state maser amplifier operating at room temperature. We achieve this feat using a practical setup that includes an ensemble of nitrogen-vacancy center spins in a diamond crystal, a strong permanent magnet and simple laser diode. We describe important amplifier characteristics including gain, bandwidth, compression power and noise temperature and discuss the prospects of realizing a room-temperature near-quantum-noise-limited amplifier with this system. Finally, we show that in a different mode of operation the spins can be used to cool the system noise in an external circuit to cryogenic levels, all without the requirement for physical cooling.","sentences":["Masers once represented the state-of-the-art in low noise microwave amplification technology, but eventually became obsolete due to their need for cryogenic cooling.","Masers based on solid-state spin systems perform most effectively as amplifiers, since they provide a large density of spins and can therefore operate at relatively high powers.","Whilst solid-state masers oscillators have been demonstrated at room temperature, continuous-wave amplification in these systems has only ever been realized at cryogenic temperatures.","Here we report on a continuous-wave solid-state maser amplifier operating at room temperature.","We achieve this feat using a practical setup that includes an ensemble of nitrogen-vacancy center spins in a diamond crystal, a strong permanent magnet and simple laser diode.","We describe important amplifier characteristics including gain, bandwidth, compression power and noise temperature and discuss the prospects of realizing a room-temperature near-quantum-noise-limited amplifier with this system.","Finally, we show that in a different mode of operation the spins can be used to cool the system noise in an external circuit to cryogenic levels, all without the requirement for physical cooling."],"url":"http://arxiv.org/abs/2405.07486v1","category":"quant-ph"}
{"created":"2024-05-13 05:55:14","title":"A Class of Convex Optimization-Based Recursive Algorithms for Identification of Stochastic Systems","abstract":"Focusing on identification, this paper develops a class of convex optimization-based criteria and correspondingly the recursive algorithms to estimate the parameter vector $\\theta^{*}$ of a stochastic dynamic system. Not only do the criteria include the classical least-squares estimator but also the $L_l=|\\cdot|^l, l\\geq 1$, the Huber, the Log-cosh, and the Quantile costs as special cases. First, we prove that the minimizers of the convex optimization-based criteria converge to $\\theta^{*}$ with probability one. Second, the recursive algorithms are proposed to find the estimates, which minimize the convex optimization-based criteria, and it is shown that these estimates also converge to the true parameter vector with probability one. Numerical examples are given, justifying the performance of the proposed algorithms including the strong consistency of the estimates, the robustness against outliers in the observations, and higher efficiency in online computation compared with the kernel-based regularization method due to the recursive nature.","sentences":["Focusing on identification, this paper develops a class of convex optimization-based criteria and correspondingly the recursive algorithms to estimate the parameter vector $\\theta^{*}$ of a stochastic dynamic system.","Not only do the criteria include the classical least-squares estimator but also the $L_l=|\\cdot|^l, l\\geq 1$, the Huber, the Log-cosh, and the Quantile costs as special cases.","First, we prove that the minimizers of the convex optimization-based criteria converge to $\\theta^{*}$ with probability one.","Second, the recursive algorithms are proposed to find the estimates, which minimize the convex optimization-based criteria, and it is shown that these estimates also converge to the true parameter vector with probability one.","Numerical examples are given, justifying the performance of the proposed algorithms including the strong consistency of the estimates, the robustness against outliers in the observations, and higher efficiency in online computation compared with the kernel-based regularization method due to the recursive nature."],"url":"http://arxiv.org/abs/2405.07483v1","category":"math.OC"}
{"created":"2024-05-13 05:41:55","title":"Enhancing 3D Object Detection by Using Neural Network with Self-adaptive Thresholding","abstract":"Robust 3D object detection remains a pivotal concern in the domain of autonomous field robotics. Despite notable enhancements in detection accuracy across standard datasets, real-world urban environments, characterized by their unstructured and dynamic nature, frequently precipitate an elevated incidence of false positives, thereby undermining the reliability of existing detection paradigms. In this context, our study introduces an advanced post-processing algorithm that modulates detection thresholds dynamically relative to the distance from the ego object. Traditional perception systems typically utilize a uniform threshold, which often leads to decreased efficacy in detecting distant objects. In contrast, our proposed methodology employs a Neural Network with a self-adaptive thresholding mechanism that significantly attenuates false negatives while concurrently diminishing false positives, particularly in complex urban settings. Empirical results substantiate that our algorithm not only augments the performance of 3D object detection models in diverse urban and adverse weather scenarios but also establishes a new benchmark for adaptive thresholding techniques in field robotics.","sentences":["Robust 3D object detection remains a pivotal concern in the domain of autonomous field robotics.","Despite notable enhancements in detection accuracy across standard datasets, real-world urban environments, characterized by their unstructured and dynamic nature, frequently precipitate an elevated incidence of false positives, thereby undermining the reliability of existing detection paradigms.","In this context, our study introduces an advanced post-processing algorithm that modulates detection thresholds dynamically relative to the distance from the ego object.","Traditional perception systems typically utilize a uniform threshold, which often leads to decreased efficacy in detecting distant objects.","In contrast, our proposed methodology employs a Neural Network with a self-adaptive thresholding mechanism that significantly attenuates false negatives while concurrently diminishing false positives, particularly in complex urban settings.","Empirical results substantiate that our algorithm not only augments the performance of 3D object detection models in diverse urban and adverse weather scenarios but also establishes a new benchmark for adaptive thresholding techniques in field robotics."],"url":"http://arxiv.org/abs/2405.07479v1","category":"cs.RO"}
{"created":"2024-05-13 05:30:34","title":"Coded Event-triggered Control for Nonlinear Systems","abstract":"This paper studies a Coded Event-triggered Control (CEC) for a class of nonlinear systems under any initial condition. To reduce communication burden, the CEC is designed from the encoding-decoding viewpoint by which only $m$-length string is transmitted for each communication between CEC and actuator. If a more general Entry Capture Problem is encountered, such control design will be rather complicated yet challenging where the performance constraints are satisfied some time after (rather than from the beginning of) system operation, rendering normally employed prescribed performance control invalid because they may be not defined in the initial interval. By introducing auxiliary functions, we develop a Self-adjustable Prescribed Performance (SPP) mechanism which can flexibly adjust the symmetric or asymmetric performance boundaries to accommodate different initial conditions, providing an effective solution for the underlying tracking problem. In this way, the resulted CEC can not only consume less communication resources but also regulate the tracking error under any initial condition into an allowable set before a given time in a bounded and customizable manner. Simulation results verify and clarify the theoretical findings.","sentences":["This paper studies a Coded Event-triggered Control (CEC) for a class of nonlinear systems under any initial condition.","To reduce communication burden, the CEC is designed from the encoding-decoding viewpoint by which only $m$-length string is transmitted for each communication between CEC and actuator.","If a more general Entry Capture Problem is encountered, such control design will be rather complicated yet challenging where the performance constraints are satisfied some time after (rather than from the beginning of) system operation, rendering normally employed prescribed performance control invalid because they may be not defined in the initial interval.","By introducing auxiliary functions, we develop a Self-adjustable Prescribed Performance (SPP) mechanism which can flexibly adjust the symmetric or asymmetric performance boundaries to accommodate different initial conditions, providing an effective solution for the underlying tracking problem.","In this way, the resulted CEC can not only consume less communication resources but also regulate the tracking error under any initial condition into an allowable set before a given time in a bounded and customizable manner.","Simulation results verify and clarify the theoretical findings."],"url":"http://arxiv.org/abs/2405.07478v1","category":"eess.SY"}
{"created":"2024-05-13 05:26:44","title":"Revisiting visualization of spiral states in a wide-gap spherical Couette flow","abstract":"A pioneering study conducted by Egbers and Rath [Acta Mech. 111 pp. 125--140 (1995)] experimentally captured spiral waves to elucidate the transition in the wide-gap spherical Couette flow. However, the physical field quantities of the spiral waves corresponding to light patterns of various intensities, as obtained in the experiment, remain unclear, and we have yet to move beyond the understanding that the reflected light from shear-sensitive flake tracers responds to a flow that appears at the transition. In this study, the experiment to visualize spiral waves using aluminum flakes, as performed by Egbers and Rath, was numerically reproduced by solving the translational and rotational motions of the particles in a spiral wave. First, the spiral wave in a spherical Couette flow with an aspect ratio $\\eta=1/2$ was numerically calculated using the Newton--Raphson method. Subsequently, the image that was numerically reproduced from the spiral wave was compared with an experimentally visualized image. The torque acting on the inner sphere and the phase angular velocity of the spiral waves with various wavenumbers were provided. Attempts have been made to determine the instantaneous physical quantity to which the light and dark patterns obtained in the visualization corresponded, and the orientation motion of the flakes developed in the advective history of the flow is essential to yield favorable results. Exploring the correlation between flow visualization results and shear structures may provide a new avenue for quantitatively estimating spatial structures and time scales in complex and quickly time-varying flow fields, such as turbulence.","sentences":["A pioneering study conducted by Egbers and Rath","[Acta Mech. 111 pp.","125--140 (1995)] experimentally captured spiral waves to elucidate the transition in the wide-gap spherical Couette flow.","However, the physical field quantities of the spiral waves corresponding to light patterns of various intensities, as obtained in the experiment, remain unclear, and we have yet to move beyond the understanding that the reflected light from shear-sensitive flake tracers responds to a flow that appears at the transition.","In this study, the experiment to visualize spiral waves using aluminum flakes, as performed by Egbers and Rath, was numerically reproduced by solving the translational and rotational motions of the particles in a spiral wave.","First, the spiral wave in a spherical Couette flow with an aspect ratio $\\eta=1/2$ was numerically calculated using the Newton--Raphson method.","Subsequently, the image that was numerically reproduced from the spiral wave was compared with an experimentally visualized image.","The torque acting on the inner sphere and the phase angular velocity of the spiral waves with various wavenumbers were provided.","Attempts have been made to determine the instantaneous physical quantity to which the light and dark patterns obtained in the visualization corresponded, and the orientation motion of the flakes developed in the advective history of the flow is essential to yield favorable results.","Exploring the correlation between flow visualization results and shear structures may provide a new avenue for quantitatively estimating spatial structures and time scales in complex and quickly time-varying flow fields, such as turbulence."],"url":"http://arxiv.org/abs/2405.07477v1","category":"physics.flu-dyn"}
{"created":"2024-05-13 05:18:07","title":"GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting Editing with Image Prompting","abstract":"The increasing prominence of e-commerce has underscored the importance of Virtual Try-On (VTON). However, previous studies predominantly focus on the 2D realm and rely heavily on extensive data for training. Research on 3D VTON primarily centers on garment-body shape compatibility, a topic extensively covered in 2D VTON. Thanks to advances in 3D scene editing, a 2D diffusion model has now been adapted for 3D editing via multi-viewpoint editing. In this work, we propose GaussianVTON, an innovative 3D VTON pipeline integrating Gaussian Splatting (GS) editing with 2D VTON. To facilitate a seamless transition from 2D to 3D VTON, we propose, for the first time, the use of only images as editing prompts for 3D editing. To further address issues, e.g., face blurring, garment inaccuracy, and degraded viewpoint quality during editing, we devise a three-stage refinement strategy to gradually mitigate potential issues. Furthermore, we introduce a new editing strategy termed Edit Recall Reconstruction (ERR) to tackle the limitations of previous editing strategies in leading to complex geometric changes. Our comprehensive experiments demonstrate the superiority of GaussianVTON, offering a novel perspective on 3D VTON while also establishing a novel starting point for image-prompting 3D scene editing.","sentences":["The increasing prominence of e-commerce has underscored the importance of Virtual Try-On (VTON).","However, previous studies predominantly focus on the 2D realm and rely heavily on extensive data for training.","Research on 3D VTON primarily centers on garment-body shape compatibility, a topic extensively covered in 2D VTON.","Thanks to advances in 3D scene editing, a 2D diffusion model has now been adapted for 3D editing via multi-viewpoint editing.","In this work, we propose GaussianVTON, an innovative 3D VTON pipeline integrating Gaussian Splatting (GS) editing with 2D VTON.","To facilitate a seamless transition from 2D to 3D VTON, we propose, for the first time, the use of only images as editing prompts for 3D editing.","To further address issues, e.g., face blurring, garment inaccuracy, and degraded viewpoint quality during editing, we devise a three-stage refinement strategy to gradually mitigate potential issues.","Furthermore, we introduce a new editing strategy termed Edit Recall Reconstruction (ERR) to tackle the limitations of previous editing strategies in leading to complex geometric changes.","Our comprehensive experiments demonstrate the superiority of GaussianVTON, offering a novel perspective on 3D VTON while also establishing a novel starting point for image-prompting 3D scene editing."],"url":"http://arxiv.org/abs/2405.07472v1","category":"cs.CV"}
{"created":"2024-05-13 05:12:24","title":"Magnetic field dependence of critical currents of cross-type Josephson junctions with inhomogeneous critical current density under oblique magnetic fields","abstract":"Studies of the magnetic interference of sandwich-type Josephson junctions in which perpendicular or oblique magnetic fields are applied to the junction plane have received less attention than those where the applied magnetic fields are parallel. Recently, it has been theoretically demonstrated that a variety of magnetic interferences of the critical currents appear when oblique magnetic fields are applied to a cross-type junction with homogeneous critical current density. We theoretically investigated the effect of the inhomogeneous critical current density in the junction plane, and found that more complicated magnetic interferences appeared. We considered the distribution of the current density flowing through the junction plane to explore the cause of these complex magnetic interferences.","sentences":["Studies of the magnetic interference of sandwich-type Josephson junctions in which perpendicular or oblique magnetic fields are applied to the junction plane have received less attention than those where the applied magnetic fields are parallel.","Recently, it has been theoretically demonstrated that a variety of magnetic interferences of the critical currents appear when oblique magnetic fields are applied to a cross-type junction with homogeneous critical current density.","We theoretically investigated the effect of the inhomogeneous critical current density in the junction plane, and found that more complicated magnetic interferences appeared.","We considered the distribution of the current density flowing through the junction plane to explore the cause of these complex magnetic interferences."],"url":"http://arxiv.org/abs/2405.07470v1","category":"cond-mat.supr-con"}
{"created":"2024-05-13 05:10:30","title":"Phase coding semi-quantum key distribution system based on the Single-state protocol","abstract":"Semi-quantum key distribution (SQKD) allows sharing random keys between a quantum user and a classical user. However, implementing classical user operations is challenging, posing a hurdle to achieving the Single-state protocol. By using the \"selective modulation\" method, the feasibility of SQKD is verified in principle. The proposal of the selective modulation method enables the realization of other protocols for SQKD. To advance experimental progress in SQKD, we propose and implement a phase-encoded semi-quantum key distribution system based on the Single-state protocol and the \"selective modulation\" method. The system operates at a frequency of 100MHz and an average photon number of 0.1. The interference contrast achieved 96.52%, the average quantum bit error rate was 1.19%, and the raw key rate reached 88Kbps. Our experimental results demonstrate the feasibility and stability of the proposed phase-encoded semi-quantum key distribution system. Furthermore, by leveraging the \"selective modulation\" scheme proposed in this paper, we develop a comprehensive theoretical description of selective modulation. Through an analysis of quantum state evolution, we assess the security of our system, ultimately demonstrating its resilience against attacks targeting quantum states. The classical user of our system requires only two optical devices, significantly reducing the equipment requirements and enhancing its application potential. This work validates the feasibility of semi-quantum key distribution experiments and provides ideas for future research on semi-quantum key distribution experiments and security studies.","sentences":["Semi-quantum key distribution (SQKD) allows sharing random keys between a quantum user and a classical user.","However, implementing classical user operations is challenging, posing a hurdle to achieving the Single-state protocol.","By using the \"selective modulation\" method, the feasibility of SQKD is verified in principle.","The proposal of the selective modulation method enables the realization of other protocols for SQKD.","To advance experimental progress in SQKD, we propose and implement a phase-encoded semi-quantum key distribution system based on the Single-state protocol and the \"selective modulation\" method.","The system operates at a frequency of 100MHz and an average photon number of 0.1.","The interference contrast achieved 96.52%, the average quantum bit error rate was 1.19%, and the raw key rate reached 88Kbps.","Our experimental results demonstrate the feasibility and stability of the proposed phase-encoded semi-quantum key distribution system.","Furthermore, by leveraging the \"selective modulation\" scheme proposed in this paper, we develop a comprehensive theoretical description of selective modulation.","Through an analysis of quantum state evolution, we assess the security of our system, ultimately demonstrating its resilience against attacks targeting quantum states.","The classical user of our system requires only two optical devices, significantly reducing the equipment requirements and enhancing its application potential.","This work validates the feasibility of semi-quantum key distribution experiments and provides ideas for future research on semi-quantum key distribution experiments and security studies."],"url":"http://arxiv.org/abs/2405.07469v1","category":"quant-ph"}
{"created":"2024-05-13 17:50:02","title":"Unveiling the Pockels Coefficient of Ferroelectric Nitride ScAlN","abstract":"Nitride ferroelectrics have recently emerged as promising alternatives to oxide ferroelectrics due to their compatibility with mainstream semiconductor processing. ScAlN, in particular, has exhibited remarkable piezoelectric coupling strength ($K^2$) comparable to that of lithium niobate (LN), making it a valuable choice for RF filters in wireless communications. Recently, ScAlN has sparked interest in its use for nanophotonic devices, chiefly due to its large bandgap facilitating operation in blue wavelengths coupled with promises of enhanced nonlinear optical properties such as a large second-order susceptibility ($\\chi^{(2)}$). It is still an open question whether ScAlN can outperform oxide ferroelectrics concerning the Pockels effect -- an electro-optic coupling extensively utilized in optical communications devices. In this paper, we present a comprehensive theoretical analysis and experimental demonstration of ScAlN's Pockels effect. Our findings reveal that the electro-optic coupling of ScAlN, despite being weak at low Sc concentration, may be significantly enhanced at high levels of Sc doping, which points the direction of continued research efforts to unlock the full potential of ScAlN.","sentences":["Nitride ferroelectrics have recently emerged as promising alternatives to oxide ferroelectrics due to their compatibility with mainstream semiconductor processing.","ScAlN, in particular, has exhibited remarkable piezoelectric coupling strength ($K^2$) comparable to that of lithium niobate (LN), making it a valuable choice for RF filters in wireless communications.","Recently, ScAlN has sparked interest in its use for nanophotonic devices, chiefly due to its large bandgap facilitating operation in blue wavelengths coupled with promises of enhanced nonlinear optical properties such as a large second-order susceptibility ($\\chi^{(2)}$).","It is still an open question whether ScAlN can outperform oxide ferroelectrics concerning the Pockels effect -- an electro-optic coupling extensively utilized in optical communications devices.","In this paper, we present a comprehensive theoretical analysis and experimental demonstration of ScAlN's Pockels effect.","Our findings reveal that the electro-optic coupling of ScAlN, despite being weak at low Sc concentration, may be significantly enhanced at high levels of Sc doping, which points the direction of continued research efforts to unlock the full potential of ScAlN."],"url":"http://arxiv.org/abs/2405.07978v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 16:41:32","title":"Robust Quantum Sensing with Multiparameter Decorrelation","abstract":"The performance of a quantum sensor is fundamentally limited by noise. This noise is particularly damaging when it becomes correlated with the readout of a target signal, caused by fluctuations of the sensor's operating parameters. These uncertainties limit sensitivity in a way that can be understood with multiparameter estimation theory. We develop a new approach, adaptable to any quantum platform, for designing robust sensing protocols that leverages multiparameter estimation theory and machine learning to decorrelate a target signal from fluctuating off-target (``nuisance'') parameters. Central to our approach is the identification of information-theoretic goals that guide a machine learning agent through an otherwise intractably large space of potential sensing protocols. As an illustrative example, we apply our approach to a reconfigurable optical lattice to design an accelerometer whose sensitivity is decorrelated from lattice depth noise. We demonstrate the effect of decorrelation on outcomes and Bayesian inferencing through statistical analysis in parameter space, and discuss implications for future applications in quantum metrology and computing.","sentences":["The performance of a quantum sensor is fundamentally limited by noise.","This noise is particularly damaging when it becomes correlated with the readout of a target signal, caused by fluctuations of the sensor's operating parameters.","These uncertainties limit sensitivity in a way that can be understood with multiparameter estimation theory.","We develop a new approach, adaptable to any quantum platform, for designing robust sensing protocols that leverages multiparameter estimation theory and machine learning to decorrelate a target signal from fluctuating off-target (``nuisance'') parameters.","Central to our approach is the identification of information-theoretic goals that guide a machine learning agent through an otherwise intractably large space of potential sensing protocols.","As an illustrative example, we apply our approach to a reconfigurable optical lattice to design an accelerometer whose sensitivity is decorrelated from lattice depth noise.","We demonstrate the effect of decorrelation on outcomes and Bayesian inferencing through statistical analysis in parameter space, and discuss implications for future applications in quantum metrology and computing."],"url":"http://arxiv.org/abs/2405.07907v1","category":"quant-ph"}
{"created":"2024-05-13 16:00:04","title":"A formal categorical approach to the homotopy theory of dg categories","abstract":"We introduce a bicategory which refines the localization of the category of dg categories with respect to quasi-equivalences and investigate its properties via formal category theory. Concretely, we first introduce the bicategory of dg categories $\\mathsf{DBimod}$ whose Hom categories are the derived categories of dg bimodules and then define the desired bicategory as the sub-bicategory $\\mathsf{DBimod}^\\text{rqr}$ consisting only of right quasi-representable dg bimodules. The first half of the paper is devoted to the study of adjunctions and equivalences in these bicategories. We then show that the embedding $\\mathsf{DBimod}^\\text{rqr} \\hookrightarrow \\mathsf{DBimod}$ is a proarrow equipment in the sense of Richard J. Wood, which is a framework for formal category theory and makes it possible to talk about (weighted) (co)limits in an abstract way. Thus we obtain the notion of homotopical (co)limits in a dg category, including homotopical shifts and cones, by which we obtain a formal categorical characterization of pretriangulated dg categories. As an immediate application we give a conceptual proof of the fact that the pretriangulatedness is preserved under the gluing procedure.","sentences":["We introduce a bicategory which refines the localization of the category of dg categories with respect to quasi-equivalences and investigate its properties via formal category theory.","Concretely, we first introduce the bicategory of dg categories $\\mathsf{DBimod}$ whose Hom categories are the derived categories of dg bimodules and then define the desired bicategory as the sub-bicategory $\\mathsf{DBimod}^\\text{rqr}$ consisting only of right quasi-representable dg bimodules.","The first half of the paper is devoted to the study of adjunctions and equivalences in these bicategories.","We then show that the embedding $\\mathsf{DBimod}^\\text{rqr} \\hookrightarrow \\mathsf{DBimod}$ is a proarrow equipment in the sense of Richard J. Wood, which is a framework for formal category theory and makes it possible to talk about (weighted) (co)limits in an abstract way.","Thus we obtain the notion of homotopical (co)limits in a dg category, including homotopical shifts and cones, by which we obtain a formal categorical characterization of pretriangulated dg categories.","As an immediate application we give a conceptual proof of the fact that the pretriangulatedness is preserved under the gluing procedure."],"url":"http://arxiv.org/abs/2405.07873v1","category":"math.CT"}
{"created":"2024-05-13 15:58:13","title":"Mapping the Invisible: A Framework for Tracking COVID-19 Spread Among College Students with Google Location Data","abstract":"The COVID-19 pandemic and the implementation of social distancing policies have rapidly changed people's visiting patterns, as reflected in mobility data that tracks mobility traffic using location trackers on cell phones. However, the frequency and duration of concurrent occupancy at specific locations govern the transmission rather than the number of customers visiting. Therefore, understanding how people interact in different locations is crucial to target policies, inform contact tracing, and prevention strategies. This study proposes an efficient way to reduce the spread of the virus among on-campus university students by developing a self-developed Google History Location Extractor and Indicator software based on real-world human mobility data. The platform enables policymakers and researchers to explore the possibility of future developments in the epidemic's spread and simulate the outcomes of human mobility and epidemic state under different epidemic control policies. It offers functions for determining potential contacts, assessing individual infection risks, and evaluating the effectiveness of on-campus policies. The proposed multi-functional platform facilitates the screening process by more accurately targeting potential virus carriers and aids in making informed decisions on epidemic control policies, ultimately contributing to preventing and managing future outbreaks.","sentences":["The COVID-19 pandemic and the implementation of social distancing policies have rapidly changed people's visiting patterns, as reflected in mobility data that tracks mobility traffic using location trackers on cell phones.","However, the frequency and duration of concurrent occupancy at specific locations govern the transmission rather than the number of customers visiting.","Therefore, understanding how people interact in different locations is crucial to target policies, inform contact tracing, and prevention strategies.","This study proposes an efficient way to reduce the spread of the virus among on-campus university students by developing a self-developed Google History Location Extractor and Indicator software based on real-world human mobility data.","The platform enables policymakers and researchers to explore the possibility of future developments in the epidemic's spread and simulate the outcomes of human mobility and epidemic state under different epidemic control policies.","It offers functions for determining potential contacts, assessing individual infection risks, and evaluating the effectiveness of on-campus policies.","The proposed multi-functional platform facilitates the screening process by more accurately targeting potential virus carriers and aids in making informed decisions on epidemic control policies, ultimately contributing to preventing and managing future outbreaks."],"url":"http://arxiv.org/abs/2405.07870v1","category":"cs.SE"}
{"created":"2024-05-13 15:46:11","title":"Uniform Inference for Subsampled Moment Regression","abstract":"We propose a method for constructing a confidence region for the solution to a conditional moment equation. The method is built around a class of algorithms for nonparametric regression based on subsampled kernels. This class includes random forest regression. We bound the error in the confidence region's nominal coverage probability, under the restriction that the conditional moment equation of interest satisfies a local orthogonality condition. The method is applicable to the construction of confidence regions for conditional average treatment effects in randomized experiments, among many other similar problems encountered in applied economics and causal inference. As a by-product, we obtain several new order-explicit results on the concentration and normal approximation of high-dimensional $U$-statistics.","sentences":["We propose a method for constructing a confidence region for the solution to a conditional moment equation.","The method is built around a class of algorithms for nonparametric regression based on subsampled kernels.","This class includes random forest regression.","We bound the error in the confidence region's nominal coverage probability, under the restriction that the conditional moment equation of interest satisfies a local orthogonality condition.","The method is applicable to the construction of confidence regions for conditional average treatment effects in randomized experiments, among many other similar problems encountered in applied economics and causal inference.","As a by-product, we obtain several new order-explicit results on the concentration and normal approximation of high-dimensional $U$-statistics."],"url":"http://arxiv.org/abs/2405.07860v1","category":"econ.EM"}
{"created":"2024-05-13 15:44:40","title":"Radiogenomic biomarkers for immunotherapy in glioblastoma: A systematic review of magnetic resonance imaging studies","abstract":"Immunotherapy is an effective precision medicine treatment for several cancers. Imaging signatures of the underlying genome (radiogenomics) in glioblastoma patients may serve as preoperative biomarkers of the tumor-host immune apparatus. Validated biomarkers would have the potential to stratify patients during immunotherapy clinical trials, and if trials are beneficial, facilitate personalized neo-adjuvant treatment. The increased use of whole genome sequencing data, and the advances in bioinformatics and machine learning make such developments plausible. We performed a systematic review to determine the extent of development and validation of immune-related radiogenomic biomarkers for glioblastoma. A systematic review was performed following PRISMA guidelines using the PubMed, Medline, and Embase databases. Qualitative analysis was performed by incorporating the QUADAS 2 tool and CLAIM checklist. PROSPERO registered CRD42022340968. Extracted data were insufficiently homogenous to perform a meta-analysis. Results Nine studies, all retrospective, were included. Biomarkers extracted from magnetic resonance imaging volumes of interest included apparent diffusion coefficient values, relative cerebral blood volume values, and image-derived features. These biomarkers correlated with genomic markers from tumor cells or immune cells or with patient survival. The majority of studies had a high risk of bias and applicability concerns regarding the index test performed. Radiogenomic immune biomarkers have the potential to provide early treatment options to patients with glioblastoma. Targeted immunotherapy, stratified by these biomarkers, has the potential to allow individualized neo-adjuvant precision treatment options in clinical trials. However, there are no prospective studies validating these biomarkers, and interpretation is limited due to study bias with little evidence of generalizability.","sentences":["Immunotherapy is an effective precision medicine treatment for several cancers.","Imaging signatures of the underlying genome (radiogenomics) in glioblastoma patients may serve as preoperative biomarkers of the tumor-host immune apparatus.","Validated biomarkers would have the potential to stratify patients during immunotherapy clinical trials, and if trials are beneficial, facilitate personalized neo-adjuvant treatment.","The increased use of whole genome sequencing data, and the advances in bioinformatics and machine learning make such developments plausible.","We performed a systematic review to determine the extent of development and validation of immune-related radiogenomic biomarkers for glioblastoma.","A systematic review was performed following PRISMA guidelines using the PubMed, Medline, and","Embase databases.","Qualitative analysis was performed by incorporating the QUADAS 2 tool and CLAIM checklist.","PROSPERO registered CRD42022340968.","Extracted data were insufficiently homogenous to perform a meta-analysis.","Results Nine studies, all retrospective, were included.","Biomarkers extracted from magnetic resonance imaging volumes of interest included apparent diffusion coefficient values, relative cerebral blood volume values, and image-derived features.","These biomarkers correlated with genomic markers from tumor cells or immune cells or with patient survival.","The majority of studies had a high risk of bias and applicability concerns regarding the index test performed.","Radiogenomic immune biomarkers have the potential to provide early treatment options to patients with glioblastoma.","Targeted immunotherapy, stratified by these biomarkers, has the potential to allow individualized neo-adjuvant precision treatment options in clinical trials.","However, there are no prospective studies validating these biomarkers, and interpretation is limited due to study bias with little evidence of generalizability."],"url":"http://arxiv.org/abs/2405.07858v1","category":"q-bio.TO"}
{"created":"2024-05-13 15:40:56","title":"Using Multiparametric MRI with Optimized Synthetic Correlated Diffusion Imaging to Enhance Breast Cancer Pathologic Complete Response Prediction","abstract":"In 2020, 685,000 deaths across the world were attributed to breast cancer, underscoring the critical need for innovative and effective breast cancer treatment. Neoadjuvant chemotherapy has recently gained popularity as a promising treatment strategy for breast cancer, attributed to its efficacy in shrinking large tumors and leading to pathologic complete response. However, the current process to recommend neoadjuvant chemotherapy relies on the subjective evaluation of medical experts which contain inherent biases and significant uncertainty. A recent study, utilizing volumetric deep radiomic features extracted from synthetic correlated diffusion imaging (CDI$^s$), demonstrated significant potential in noninvasive breast cancer pathologic complete response prediction. Inspired by the positive outcomes of optimizing CDI$^s$ for prostate cancer delineation, this research investigates the application of optimized CDI$^s$ to enhance breast cancer pathologic complete response prediction. Using multiparametric MRI that fuses optimized CDI$^s$ with diffusion-weighted imaging (DWI), we obtain a leave-one-out cross-validation accuracy of 93.28%, over 5.5% higher than that previously reported.","sentences":["In 2020, 685,000 deaths across the world were attributed to breast cancer, underscoring the critical need for innovative and effective breast cancer treatment.","Neoadjuvant chemotherapy has recently gained popularity as a promising treatment strategy for breast cancer, attributed to its efficacy in shrinking large tumors and leading to pathologic complete response.","However, the current process to recommend neoadjuvant chemotherapy relies on the subjective evaluation of medical experts which contain inherent biases and significant uncertainty.","A recent study, utilizing volumetric deep radiomic features extracted from synthetic correlated diffusion imaging (CDI$^s$), demonstrated significant potential in noninvasive breast cancer pathologic complete response prediction.","Inspired by the positive outcomes of optimizing CDI$^s$ for prostate cancer delineation, this research investigates the application of optimized CDI$^s$ to enhance breast cancer pathologic complete response prediction.","Using multiparametric MRI that fuses optimized CDI$^s$ with diffusion-weighted imaging (DWI), we obtain a leave-one-out cross-validation accuracy of 93.28%, over 5.5% higher than that previously reported."],"url":"http://arxiv.org/abs/2405.07854v1","category":"eess.IV"}
{"created":"2024-05-13 15:30:35","title":"Sample Selection Bias in Machine Learning for Healthcare","abstract":"While machine learning algorithms hold promise for personalised medicine, their clinical adoption remains limited. One critical factor contributing to this restraint is sample selection bias (SSB) which refers to the study population being less representative of the target population, leading to biased and potentially harmful decisions. Despite being well-known in the literature, SSB remains scarcely studied in machine learning for healthcare. Moreover, the existing techniques try to correct the bias by balancing distributions between the study and the target populations, which may result in a loss of predictive performance. To address these problems, our study illustrates the potential risks associated with SSB by examining SSB's impact on the performance of machine learning algorithms. Most importantly, we propose a new research direction for addressing SSB, based on the target population identification rather than the bias correction. Specifically, we propose two independent networks (T-Net) and a multitasking network (MT-Net) for addressing SSB, where one network/task identifies the target subpopulation which is representative of the study population and the second makes predictions for the identified subpopulation. Our empirical results with synthetic and semi-synthetic datasets highlight that SSB can lead to a large drop in the performance of an algorithm for the target population as compared with the study population, as well as a substantial difference in the performance for the target subpopulations that are representative of the selected and the non-selected patients from the study population. Furthermore, our proposed techniques demonstrate robustness across various settings, including different dataset sizes, event rates, and selection rates, outperforming the existing bias correction techniques.","sentences":["While machine learning algorithms hold promise for personalised medicine, their clinical adoption remains limited.","One critical factor contributing to this restraint is sample selection bias (SSB) which refers to the study population being less representative of the target population, leading to biased and potentially harmful decisions.","Despite being well-known in the literature, SSB remains scarcely studied in machine learning for healthcare.","Moreover, the existing techniques try to correct the bias by balancing distributions between the study and the target populations, which may result in a loss of predictive performance.","To address these problems, our study illustrates the potential risks associated with SSB by examining SSB's impact on the performance of machine learning algorithms.","Most importantly, we propose a new research direction for addressing SSB, based on the target population identification rather than the bias correction.","Specifically, we propose two independent networks (T-Net) and a multitasking network (MT-Net) for addressing SSB, where one network/task identifies the target subpopulation which is representative of the study population and the second makes predictions for the identified subpopulation.","Our empirical results with synthetic and semi-synthetic datasets highlight that SSB can lead to a large drop in the performance of an algorithm for the target population as compared with the study population, as well as a substantial difference in the performance for the target subpopulations that are representative of the selected and the non-selected patients from the study population.","Furthermore, our proposed techniques demonstrate robustness across various settings, including different dataset sizes, event rates, and selection rates, outperforming the existing bias correction techniques."],"url":"http://arxiv.org/abs/2405.07841v1","category":"cs.LG"}
{"created":"2024-05-13 15:22:15","title":"Forecasting with Hyper-Trees","abstract":"This paper introduces the concept of Hyper-Trees and offers a new direction in applying tree-based models to time series data. Unlike conventional applications of decision trees that forecast time series directly, Hyper-Trees are designed to learn the parameters of a target time series model. Our framework leverages the gradient-based nature of boosted trees, which allows us to extend the concept of Hyper-Networks to Hyper-Trees and to induce a time-series inductive bias to tree models. By relating the parameters of a target time series model to features, Hyper-Trees address the challenge of parameter non-stationarity and enable tree-based forecasts to extend beyond their initial training range. With our research, we aim to explore the effectiveness of Hyper-Trees across various forecasting scenarios and to expand the application of gradient boosted decision trees past their conventional use in time series forecasting.","sentences":["This paper introduces the concept of Hyper-Trees and offers a new direction in applying tree-based models to time series data.","Unlike conventional applications of decision trees that forecast time series directly, Hyper-Trees are designed to learn the parameters of a target time series model.","Our framework leverages the gradient-based nature of boosted trees, which allows us to extend the concept of Hyper-Networks to Hyper-Trees and to induce a time-series inductive bias to tree models.","By relating the parameters of a target time series model to features, Hyper-Trees address the challenge of parameter non-stationarity and enable tree-based forecasts to extend beyond their initial training range.","With our research, we aim to explore the effectiveness of Hyper-Trees across various forecasting scenarios and to expand the application of gradient boosted decision trees past their conventional use in time series forecasting."],"url":"http://arxiv.org/abs/2405.07836v1","category":"cs.LG"}
{"created":"2024-05-13 14:39:33","title":"Geometric model for vector bundles via infinite marked strips","abstract":"We present a geometric model for the category of vector bundles over the weighted projective line of type (2,2,n). This model is based on the orbit space of an infinite marked strip under a specific group action. By establishing a bijection between indecomposable bundles and orbits of line segments on the strip, we interpret dimensions of extension spaces as intersection indices of these line segment orbits. Furthermore, our investigation yields geometric interpretations for various aspects, including the slope of indecomposable bundles, the Picard group action, vector bundle duality, projective covers and injective hulls of extension bundles.","sentences":["We present a geometric model for the category of vector bundles over the weighted projective line of type (2,2,n).","This model is based on the orbit space of an infinite marked strip under a specific group action.","By establishing a bijection between indecomposable bundles and orbits of line segments on the strip, we interpret dimensions of extension spaces as intersection indices of these line segment orbits.","Furthermore, our investigation yields geometric interpretations for various aspects, including the slope of indecomposable bundles, the Picard group action, vector bundle duality, projective covers and injective hulls of extension bundles."],"url":"http://arxiv.org/abs/2405.07793v1","category":"math.RT"}
{"created":"2024-05-13 14:12:33","title":"$\u03b1$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning","abstract":"Multitask Learning is a Machine Learning paradigm that aims to train a range of (usually related) tasks with the help of a shared model. While the goal is often to improve the joint performance of all training tasks, another approach is to focus on the performance of a specific target task, while treating the remaining ones as auxiliary data from which to possibly leverage positive transfer towards the target during training. In such settings, it becomes important to estimate the positive or negative influence auxiliary tasks will have on the target. While many ways have been proposed to estimate task weights before or during training they typically rely on heuristics or extensive search of the weighting space. We propose a novel method called $\\alpha$-Variable Importance Learning ($\\alpha$VIL) that is able to adjust task weights dynamically during model training, by making direct use of task-specific updates of the underlying model's parameters between training epochs. Experiments indicate that $\\alpha$VIL is able to outperform other Multitask Learning approaches in a variety of settings. To our knowledge, this is the first attempt at making direct use of model updates for task weight estimation.","sentences":["Multitask Learning is a Machine Learning paradigm that aims to train a range of (usually related) tasks with the help of a shared model.","While the goal is often to improve the joint performance of all training tasks, another approach is to focus on the performance of a specific target task, while treating the remaining ones as auxiliary data from which to possibly leverage positive transfer towards the target during training.","In such settings, it becomes important to estimate the positive or negative influence auxiliary tasks will have on the target.","While many ways have been proposed to estimate task weights before or during training they typically rely on heuristics or extensive search of the weighting space.","We propose a novel method called $\\alpha$-Variable Importance Learning ($\\alpha$VIL) that is able to adjust task weights dynamically during model training, by making direct use of task-specific updates of the underlying model's parameters between training epochs.","Experiments indicate that $\\alpha$VIL is able to outperform other Multitask Learning approaches in a variety of settings.","To our knowledge, this is the first attempt at making direct use of model updates for task weight estimation."],"url":"http://arxiv.org/abs/2405.07769v1","category":"cs.LG"}
{"created":"2024-05-13 14:00:02","title":"CAGES: Cost-Aware Gradient Entropy Search for Efficient Local Multi-Fidelity Bayesian Optimization","abstract":"Bayesian optimization (BO) is a popular approach for optimizing expensive-to-evaluate black-box objective functions. An important challenge in BO is its application to high-dimensional search spaces due in large part to the curse of dimensionality. One way to overcome this challenge is to focus on local BO methods that aim to efficiently learn gradients, which have shown strong empirical performance on a variety of high-dimensional problems including policy search in reinforcement learning (RL). However, current local BO methods assume access to only a single high-fidelity information source whereas, in many engineering and control problems, one has access to multiple cheaper approximations of the objective. We propose a novel algorithm, Cost-Aware Gradient Entropy Search (CAGES), for local BO of multi-fidelity black-box functions. CAGES makes no assumption about the relationship between different information sources, making it more flexible than other multi-fidelity methods. It also employs a new type of information-theoretic acquisition function, which enables systematic identification of samples that maximize the information gain about the unknown gradient per cost of the evaluation. We demonstrate CAGES can achieve significant performance improvements compared to other state-of-the-art methods on a variety of synthetic and benchmark RL problems.","sentences":["Bayesian optimization (BO) is a popular approach for optimizing expensive-to-evaluate black-box objective functions.","An important challenge in BO is its application to high-dimensional search spaces due in large part to the curse of dimensionality.","One way to overcome this challenge is to focus on local BO methods that aim to efficiently learn gradients, which have shown strong empirical performance on a variety of high-dimensional problems including policy search in reinforcement learning (RL).","However, current local BO methods assume access to only a single high-fidelity information source whereas, in many engineering and control problems, one has access to multiple cheaper approximations of the objective.","We propose a novel algorithm, Cost-Aware Gradient Entropy Search (CAGES), for local BO of multi-fidelity black-box functions.","CAGES makes no assumption about the relationship between different information sources, making it more flexible than other multi-fidelity methods.","It also employs a new type of information-theoretic acquisition function, which enables systematic identification of samples that maximize the information gain about the unknown gradient per cost of the evaluation.","We demonstrate CAGES can achieve significant performance improvements compared to other state-of-the-art methods on a variety of synthetic and benchmark RL problems."],"url":"http://arxiv.org/abs/2405.07760v1","category":"cs.LG"}
{"created":"2024-05-13 13:56:25","title":"Minimax rates in variance and covariance changepoint testing","abstract":"We study the detection of a change in the spatial covariance matrix of $n$ independent sub-Gaussian random variables of dimension $p$. Our first contribution is to show that $\\log\\log(8n)$ is the exact minimax testing rate for a change in variance when $p=1$, thereby giving a complete characterization of the problem for univariate data. Our second contribution is to derive a lower bound on the minimax testing rate under the operator norm, taking a certain notion of sparsity into account. In the low- to moderate-dimensional region of the parameter space, we are able to match the lower bound from above with an optimal test based on sparse eigenvalues. In the remaining region of the parameter space, where the dimensionality is high, the minimax lower bound implies that changepoint testing is very difficult. As our third contribution, we propose a computationally feasible variant of the optimal multivariate test for a change in covariance, which is also adaptive to the nominal noise level and the sparsity level of the change.","sentences":["We study the detection of a change in the spatial covariance matrix of $n$ independent sub-Gaussian random variables of dimension $p$. Our first contribution is to show that $\\log\\log(8n)$ is the exact minimax testing rate for a change in variance when $p=1$, thereby giving a complete characterization of the problem for univariate data.","Our second contribution is to derive a lower bound on the minimax testing rate under the operator norm, taking a certain notion of sparsity into account.","In the low- to moderate-dimensional region of the parameter space, we are able to match the lower bound from above with an optimal test based on sparse eigenvalues.","In the remaining region of the parameter space, where the dimensionality is high, the minimax lower bound implies that changepoint testing is very difficult.","As our third contribution, we propose a computationally feasible variant of the optimal multivariate test for a change in covariance, which is also adaptive to the nominal noise level and the sparsity level of the change."],"url":"http://arxiv.org/abs/2405.07757v1","category":"math.ST"}
{"created":"2024-05-13 13:30:43","title":"TOPress3D: 3D topology optimization with design-dependent pressure loads in MATLAB","abstract":"This paper introduces ``TOPress3D,\" a 3D topology optimization MATLAB code for structures subjected to design-dependent pressure loads. With a primary focus on pedagogical objectives, the code provides an easy learning experience, making it a valuable tool and practical gateway for newcomers, students, and researchers towards this topic. TOPress3D uses Darcy's law with a drainage term to link the given pressure load to design variables, which is converted to consistent nodal loads. Compliance minimization subjected to volume constraint optimization problems with pressure loads are solved. Load sensitivities arising due to design-dependent nature of the loads are evaluated using the adjoint-variable approach. The method of moving asymptotes is used to update the design variables. TOPress3D is constituted by six main parts. Each is described in detail. The code is also tailored to solve different problems. The robustness and success of the code are demonstrated while designing a few pressure load-bearing structures. The code is provided in Appendix B and is available with extensions in the supplementary material and publicly at \\url{https://github.com/PrabhatIn/TOPress3D}.","sentences":["This paper introduces ``TOPress3D,\" a 3D topology optimization MATLAB code for structures subjected to design-dependent pressure loads.","With a primary focus on pedagogical objectives, the code provides an easy learning experience, making it a valuable tool and practical gateway for newcomers, students, and researchers towards this topic.","TOPress3D uses Darcy's law with a drainage term to link the given pressure load to design variables, which is converted to consistent nodal loads.","Compliance minimization subjected to volume constraint optimization problems with pressure loads are solved.","Load sensitivities arising due to design-dependent nature of the loads are evaluated using the adjoint-variable approach.","The method of moving asymptotes is used to update the design variables.","TOPress3D is constituted by six main parts.","Each is described in detail.","The code is also tailored to solve different problems.","The robustness and success of the code are demonstrated while designing a few pressure load-bearing structures.","The code is provided in Appendix B and is available with extensions in the supplementary material and publicly at \\url{https://github.com/PrabhatIn/TOPress3D}."],"url":"http://arxiv.org/abs/2405.07733v1","category":"cs.CE"}
{"created":"2024-05-13 13:04:55","title":"On the Adversarial Robustness of Learning-based Image Compression Against Rate-Distortion Attacks","abstract":"Despite demonstrating superior rate-distortion (RD) performance, learning-based image compression (LIC) algorithms have been found to be vulnerable to malicious perturbations in recent studies. Adversarial samples in these studies are designed to attack only one dimension of either bitrate or distortion, targeting a submodel with a specific compression ratio. However, adversaries in real-world scenarios are neither confined to singular dimensional attacks nor always have control over compression ratios. This variability highlights the inadequacy of existing research in comprehensively assessing the adversarial robustness of LIC algorithms in practical applications. To tackle this issue, this paper presents two joint rate-distortion attack paradigms at both submodel and algorithm levels, i.e., Specific-ratio Rate-Distortion Attack (SRDA) and Agnostic-ratio Rate-Distortion Attack (ARDA). Additionally, a suite of multi-granularity assessment tools is introduced to evaluate the attack results from various perspectives. On this basis, extensive experiments on eight prominent LIC algorithms are conducted to offer a thorough analysis of their inherent vulnerabilities. Furthermore, we explore the efficacy of two defense techniques in improving the performance under joint rate-distortion attacks. The findings from these experiments can provide a valuable reference for the development of compression algorithms with enhanced adversarial robustness.","sentences":["Despite demonstrating superior rate-distortion (RD) performance, learning-based image compression (LIC) algorithms have been found to be vulnerable to malicious perturbations in recent studies.","Adversarial samples in these studies are designed to attack only one dimension of either bitrate or distortion, targeting a submodel with a specific compression ratio.","However, adversaries in real-world scenarios are neither confined to singular dimensional attacks nor always have control over compression ratios.","This variability highlights the inadequacy of existing research in comprehensively assessing the adversarial robustness of LIC algorithms in practical applications.","To tackle this issue, this paper presents two joint rate-distortion attack paradigms at both submodel and algorithm levels, i.e., Specific-ratio Rate-Distortion Attack (SRDA) and Agnostic-ratio Rate-Distortion Attack (ARDA).","Additionally, a suite of multi-granularity assessment tools is introduced to evaluate the attack results from various perspectives.","On this basis, extensive experiments on eight prominent LIC algorithms are conducted to offer a thorough analysis of their inherent vulnerabilities.","Furthermore, we explore the efficacy of two defense techniques in improving the performance under joint rate-distortion attacks.","The findings from these experiments can provide a valuable reference for the development of compression algorithms with enhanced adversarial robustness."],"url":"http://arxiv.org/abs/2405.07717v1","category":"eess.IV"}
{"created":"2024-05-13 12:52:58","title":"Secure Aggregation Meets Sparsification in Decentralized Learning","abstract":"Decentralized learning (DL) faces increased vulnerability to privacy breaches due to sophisticated attacks on machine learning (ML) models. Secure aggregation is a computationally efficient cryptographic technique that enables multiple parties to compute an aggregate of their private data while keeping their individual inputs concealed from each other and from any central aggregator. To enhance communication efficiency in DL, sparsification techniques are used, selectively sharing only the most crucial parameters or gradients in a model, thereby maintaining efficiency without notably compromising accuracy. However, applying secure aggregation to sparsified models in DL is challenging due to the transmission of disjoint parameter sets by distinct nodes, which can prevent masks from canceling out effectively. This paper introduces CESAR, a novel secure aggregation protocol for DL designed to be compatible with existing sparsification mechanisms. CESAR provably defends against honest-but-curious adversaries and can be formally adapted to counteract collusion between them. We provide a foundational understanding of the interaction between the sparsification carried out by the nodes and the proportion of the parameters shared under CESAR in both colluding and non-colluding environments, offering analytical insight into the working and applicability of the protocol. Experiments on a network with 48 nodes in a 3-regular topology show that with random subsampling, CESAR is always within 0.5% accuracy of decentralized parallel stochastic gradient descent (D-PSGD), while adding only 11% of data overhead. Moreover, it surpasses the accuracy on TopK by up to 0.3% on independent and identically distributed (IID) data.","sentences":["Decentralized learning (DL) faces increased vulnerability to privacy breaches due to sophisticated attacks on machine learning (ML) models.","Secure aggregation is a computationally efficient cryptographic technique that enables multiple parties to compute an aggregate of their private data while keeping their individual inputs concealed from each other and from any central aggregator.","To enhance communication efficiency in DL, sparsification techniques are used, selectively sharing only the most crucial parameters or gradients in a model, thereby maintaining efficiency without notably compromising accuracy.","However, applying secure aggregation to sparsified models in DL is challenging due to the transmission of disjoint parameter sets by distinct nodes, which can prevent masks from canceling out effectively.","This paper introduces CESAR, a novel secure aggregation protocol for DL designed to be compatible with existing sparsification mechanisms.","CESAR provably defends against honest-but-curious adversaries and can be formally adapted to counteract collusion between them.","We provide a foundational understanding of the interaction between the sparsification carried out by the nodes and the proportion of the parameters shared under CESAR in both colluding and non-colluding environments, offering analytical insight into the working and applicability of the protocol.","Experiments on a network with 48 nodes in a 3-regular topology show that with random subsampling, CESAR is always within 0.5% accuracy of decentralized parallel stochastic gradient descent (D-PSGD), while adding only 11% of data overhead.","Moreover, it surpasses the accuracy on TopK by up to 0.3% on independent and identically distributed (IID) data."],"url":"http://arxiv.org/abs/2405.07708v1","category":"cs.LG"}
{"created":"2024-05-13 12:23:23","title":"Convergence analysis of three semi-discrete numerical schemes for nonlocal geometric flows including perimeter terms","abstract":"We present and analyze three distinct semi-discrete schemes for solving nonlocal geometric flows incorporating perimeter terms. These schemes are based on the finite difference method, the finite element method, and the finite element method with a specific tangential motion. We offer rigorous proofs of quadratic convergence under $H^1$-norm for the first scheme and linear convergence under $H^1$-norm for the latter two schemes. All error estimates rely on the observation that the error of the nonlocal term can be controlled by the error of the local term. Furthermore, we explore the relationship between the convergence under $L^\\infty$-norm and manifold distance. Extensive numerical experiments are conducted to verify the convergence analysis, and demonstrate the accuracy of our schemes under various norms for different types of nonlocal flows.","sentences":["We present and analyze three distinct semi-discrete schemes for solving nonlocal geometric flows incorporating perimeter terms.","These schemes are based on the finite difference method, the finite element method, and the finite element method with a specific tangential motion.","We offer rigorous proofs of quadratic convergence under $H^1$-norm for the first scheme and linear convergence under $H^1$-norm for the latter two schemes.","All error estimates rely on the observation that the error of the nonlocal term can be controlled by the error of the local term.","Furthermore, we explore the relationship between the convergence under $L^\\infty$-norm and manifold distance.","Extensive numerical experiments are conducted to verify the convergence analysis, and demonstrate the accuracy of our schemes under various norms for different types of nonlocal flows."],"url":"http://arxiv.org/abs/2405.07690v1","category":"math.NA"}
{"created":"2024-05-13 11:59:44","title":"Duality-based single-level reformulations of bilevel optimization problems","abstract":"Usually, bilevel optimization problems need to be transformed into single-level ones in order to derive optimality conditions and solution algorithms. Among the available approaches, the replacement of the lower-level problem by means of duality relations became popular quite recently. We revisit three realizations of this idea which are based on the lower-level Lagrange, Wolfe, and Mond--Weir dual problem. The resulting single-level surrogate problems are equivalent to the original bilevel optimization problem from the viewpoint of global minimizers under mild assumptions. However, all these reformulations suffer from the appearance of so-called implicit variables, i.e., surrogate variables which do not enter the objective function but appear in the feasible set for modeling purposes. Treating implicit variables as explicit ones has been shown to be problematic when locally optimal solutions, stationary points, and applicable constraint qualifications are compared to the original problem. Indeed, we illustrate that the same difficulties have to be faced when using these duality-based reformulations. Furthermore, we show that the Mangasarian-Fromovitz constraint qualification is likely to be violated at each feasible point of these reformulations, contrasting assertions in some recently published papers.","sentences":["Usually, bilevel optimization problems need to be transformed into single-level ones in order to derive optimality conditions and solution algorithms.","Among the available approaches, the replacement of the lower-level problem by means of duality relations became popular quite recently.","We revisit three realizations of this idea which are based on the lower-level Lagrange, Wolfe, and Mond--Weir dual problem.","The resulting single-level surrogate problems are equivalent to the original bilevel optimization problem from the viewpoint of global minimizers under mild assumptions.","However, all these reformulations suffer from the appearance of so-called implicit variables, i.e., surrogate variables which do not enter the objective function but appear in the feasible set for modeling purposes.","Treating implicit variables as explicit ones has been shown to be problematic when locally optimal solutions, stationary points, and applicable constraint qualifications are compared to the original problem.","Indeed, we illustrate that the same difficulties have to be faced when using these duality-based reformulations.","Furthermore, we show that the Mangasarian-Fromovitz constraint qualification is likely to be violated at each feasible point of these reformulations, contrasting assertions in some recently published papers."],"url":"http://arxiv.org/abs/2405.07672v1","category":"math.OC"}
{"created":"2024-05-13 11:58:03","title":"Action of the axial $U(1)$ non-invertible symmetry on the 't~Hooft line operator: A simple argument","abstract":"Employing the modified Villain lattice formulation of the axion QED, we present an alternative and much simpler derivation of the conclusion of~Ref.~\\cite{Honda:2024sdz} that the sweep of the axial $U(1)$ non-invertible symmetry operator over the (non-genuine) gauge invariant 't~Hooft line operator with an integer magnetic charge does not leave any effect. The point is that such a 't~Hooft line can be represented by a boundary of a (non-topological) defect that is invariant under the axial transformation on the axion field.","sentences":["Employing the modified Villain lattice formulation of the axion QED, we present an alternative and much simpler derivation of the conclusion of~Ref.~\\cite{Honda:2024sdz} that the sweep of the axial $U(1)$ non-invertible symmetry operator over the (non-genuine) gauge invariant 't~Hooft line operator with an integer magnetic charge does not leave any effect.","The point is that such a 't~Hooft line can be represented by a boundary of a (non-topological) defect that is invariant under the axial transformation on the axion field."],"url":"http://arxiv.org/abs/2405.07669v1","category":"hep-lat"}
{"created":"2024-05-13 11:44:57","title":"Sign Stitching: A Novel Approach to Sign Language Production","abstract":"Sign Language Production (SLP) is a challenging task, given the limited resources available and the inherent diversity within sign data. As a result, previous works have suffered from the problem of regression to the mean, leading to under-articulated and incomprehensible signing. In this paper, we propose using dictionary examples and a learnt codebook of facial expressions to create expressive sign language sequences. However, simply concatenating signs and adding the face creates robotic and unnatural sequences. To address this we present a 7-step approach to effectively stitch sequences together. First, by normalizing each sign into a canonical pose, cropping, and stitching we create a continuous sequence. Then, by applying filtering in the frequency domain and resampling each sign, we create cohesive natural sequences that mimic the prosody found in the original data. We leverage a SignGAN model to map the output to a photo-realistic signer and present a complete Text-to-Sign (T2S) SLP pipeline. Our evaluation demonstrates the effectiveness of the approach, showcasing state-of-the-art performance across all datasets. Finally, a user evaluation shows our approach outperforms the baseline model and is capable of producing realistic sign language sequences.","sentences":["Sign Language Production (SLP) is a challenging task, given the limited resources available and the inherent diversity within sign data.","As a result, previous works have suffered from the problem of regression to the mean, leading to under-articulated and incomprehensible signing.","In this paper, we propose using dictionary examples and a learnt codebook of facial expressions to create expressive sign language sequences.","However, simply concatenating signs and adding the face creates robotic and unnatural sequences.","To address this we present a 7-step approach to effectively stitch sequences together.","First, by normalizing each sign into a canonical pose, cropping, and stitching we create a continuous sequence.","Then, by applying filtering in the frequency domain and resampling each sign, we create cohesive natural sequences that mimic the prosody found in the original data.","We leverage a SignGAN model to map the output to a photo-realistic signer and present a complete Text-to-Sign (T2S) SLP pipeline.","Our evaluation demonstrates the effectiveness of the approach, showcasing state-of-the-art performance across all datasets.","Finally, a user evaluation shows our approach outperforms the baseline model and is capable of producing realistic sign language sequences."],"url":"http://arxiv.org/abs/2405.07663v1","category":"cs.CV"}
{"created":"2024-05-13 11:13:17","title":"CDFormer:When Degradation Prediction Embraces Diffusion Model for Blind Image Super-Resolution","abstract":"Existing Blind image Super-Resolution (BSR) methods focus on estimating either kernel or degradation information, but have long overlooked the essential content details. In this paper, we propose a novel BSR approach, Content-aware Degradation-driven Transformer (CDFormer), to capture both degradation and content representations. However, low-resolution images cannot provide enough content details, and thus we introduce a diffusion-based module $CDFormer_{diff}$ to first learn Content Degradation Prior (CDP) in both low- and high-resolution images, and then approximate the real distribution given only low-resolution information. Moreover, we apply an adaptive SR network $CDFormer_{SR}$ that effectively utilizes CDP to refine features. Compared to previous diffusion-based SR methods, we treat the diffusion model as an estimator that can overcome the limitations of expensive sampling time and excessive diversity. Experiments show that CDFormer can outperform existing methods, establishing a new state-of-the-art performance on various benchmarks under blind settings. Codes and models will be available at \\href{https://github.com/I2-Multimedia-Lab/CDFormer}{https://github.com/I2-Multimedia-Lab/CDFormer}.","sentences":["Existing Blind image Super-Resolution (BSR) methods focus on estimating either kernel or degradation information, but have long overlooked the essential content details.","In this paper, we propose a novel BSR approach, Content-aware Degradation-driven Transformer (CDFormer), to capture both degradation and content representations.","However, low-resolution images cannot provide enough content details, and thus we introduce a diffusion-based module $CDFormer_{diff}$ to first learn Content Degradation","Prior (CDP) in both low- and high-resolution images, and then approximate the real distribution given only low-resolution information.","Moreover, we apply an adaptive SR network $CDFormer_{SR}$ that effectively utilizes CDP to refine features.","Compared to previous diffusion-based SR methods, we treat the diffusion model as an estimator that can overcome the limitations of expensive sampling time and excessive diversity.","Experiments show that CDFormer can outperform existing methods, establishing a new state-of-the-art performance on various benchmarks under blind settings.","Codes and models will be available at \\href{https://github.com/I2-Multimedia-Lab/CDFormer}{https://github.com/I2-Multimedia-Lab/CDFormer}."],"url":"http://arxiv.org/abs/2405.07648v1","category":"cs.CV"}
{"created":"2024-05-13 11:03:28","title":"Tidal disruption event AT2020ocn: early-time X-ray flares caused by a possible disc alignment process","abstract":"A tidal disruption event (TDE) may occur when a star is torn apart by the tidal force of a black hole (BH). Eventually, an accretion disc is thought to form out of stellar debris falling back towards the BH. If the star's orbital angular momentum vector prior to disruption is not aligned with the BH spin angular momentum vector, the disc will be tilted with respect to the BH equatorial plane. The disc will eventually be drawn into the BH equatorial plane due to a combination of the Bardeen-Petterson effect and internal torques. Here, we analyse the X-ray and UV observations of the TDE AT2020ocn obtained by Swift, XMM-Newton, and NICER. The X-ray light curve shows strong flares during the first $\\approx100$ days, while, over the same period, the UV emission decays gradually. We find that the X-ray flares can be explained by a model that also explains the spectral evolution. This model includes a slim disc viewed under a variable inclination plus an inverse-Comptonisation component processing the slim disc emission. A scenario where the ongoing Lense-Thirring precession during the disc alignment process is responsible for the observed inclination variations is consistent with the data. In later observations, we find that the X-ray spectrum of AT2020ocn becomes harder, while the mass accretion rate remains at super-Eddington levels, suggesting the formation of a corona in line with accretion onto other compact objects. We constrain the BH mass to be $(7^{+13}_{-3})\\times10^{5}$ M$_\\odot$ at the 1$\\sigma$ (68%) confidence level.","sentences":["A tidal disruption event (TDE) may occur when a star is torn apart by the tidal force of a black hole (BH).","Eventually, an accretion disc is thought to form out of stellar debris falling back towards the BH.","If the star's orbital angular momentum vector prior to disruption is not aligned with the BH spin angular momentum vector, the disc will be tilted with respect to the BH equatorial plane.","The disc will eventually be drawn into the BH equatorial plane due to a combination of the Bardeen-Petterson effect and internal torques.","Here, we analyse the X-ray and UV observations of the TDE AT2020ocn obtained by Swift, XMM-Newton, and NICER.","The X-ray light curve shows strong flares during the first $\\approx100$ days, while, over the same period, the UV emission decays gradually.","We find that the X-ray flares can be explained by a model that also explains the spectral evolution.","This model includes a slim disc viewed under a variable inclination plus an inverse-Comptonisation component processing the slim disc emission.","A scenario where the ongoing Lense-Thirring precession during the disc alignment process is responsible for the observed inclination variations is consistent with the data.","In later observations, we find that the X-ray spectrum of AT2020ocn becomes harder, while the mass accretion rate remains at super-Eddington levels, suggesting the formation of a corona in line with accretion onto other compact objects.","We constrain the BH mass to be $(7^{+13}_{-3})\\times10^{5}$ M$_\\odot$ at the 1$\\sigma$ (68%) confidence level."],"url":"http://arxiv.org/abs/2405.07642v1","category":"astro-ph.HE"}
{"created":"2024-05-13 10:30:33","title":"COBias and Debias: Minimizing Language Model Pairwise Accuracy Bias via Nonlinear Integer Programming","abstract":"For language model classification, would you prefer having only one workable class or having every class working? The latter makes more practical uses. Especially for large language models (LLMs), the fact that they achieve a fair overall accuracy by in-context learning (ICL) obscures a large difference in individual class accuracies. In this work, we uncover and tackle language models' imbalance in per-class prediction accuracy by reconceptualizing it as the Contextual Oddity Bias (COBias), and we are the first to engage nonlinear integer programming (NIP) to debias it. Briefly, COBias refers to the difference in accuracy by a class A compared to its ''odd'' class, which holds the majority wrong predictions of class A. With the COBias metric, we reveal that LLMs of varied scales and families exhibit large per-class accuracy differences. Then we propose Debiasing as Nonlinear Integer Programming (DNIP) to correct ICL per-class probabilities for lower bias and higher overall accuracy. Our optimization objective is directly based on the evaluation scores by COBias and accuracy metrics, solved by simulated annealing. Evaluations on three LLMs across seven NLP classification tasks show that DNIP simultaneously achieves significant COBias reduction ($-27\\%$) and accuracy improvement ($+12\\%$) over the conventional ICL approach, suggesting that modeling pairwise class accuracy differences is a direction in pushing forward more accurate, more reliable LLM predictions.","sentences":["For language model classification, would you prefer having only one workable class or having every class working?","The latter makes more practical uses.","Especially for large language models (LLMs), the fact that they achieve a fair overall accuracy by in-context learning (ICL) obscures a large difference in individual class accuracies.","In this work, we uncover and tackle language models' imbalance in per-class prediction accuracy by reconceptualizing it as the Contextual Oddity Bias (COBias), and we are the first to engage nonlinear integer programming (NIP) to debias it.","Briefly, COBias refers to the difference in accuracy by a class A compared to its ''odd'' class, which holds the majority wrong predictions of class A. With the COBias metric, we reveal that LLMs of varied scales and families exhibit large per-class accuracy differences.","Then we propose Debiasing as Nonlinear Integer Programming (DNIP) to correct ICL per-class probabilities for lower bias and higher overall accuracy.","Our optimization objective is directly based on the evaluation scores by COBias and accuracy metrics, solved by simulated annealing.","Evaluations on three LLMs across seven NLP classification tasks show that DNIP simultaneously achieves significant COBias reduction ($-27\\%$) and accuracy improvement ($+12\\%$) over the conventional ICL approach, suggesting that modeling pairwise class accuracy differences is a direction in pushing forward more accurate, more reliable LLM predictions."],"url":"http://arxiv.org/abs/2405.07623v1","category":"cs.CL"}
{"created":"2024-05-13 10:09:24","title":"Empirical Application Insights on Industrial Data and Service Aspects of Digital Twin Networks","abstract":"Digital twin networks (DTNs) serve as an emerging facilitator in the industrial networking sector, enabling the management of new classes of services, which require tailored support for improved resource utilization, low latencies and accurate data fidelity. In this paper, we explore the intersection between theoretical recommendations and practical implications of applying DTNs to industrial networked environments, sharing empirical findings and lessons learned from our ongoing work. To this end, we first provide experimental examples from selected aspects of data representations and fidelity, mixed-criticality workload support, and application-driven services. Then, we introduce an architectural framework for DTNs, exposing a more practical extension of existing standards; notably the ITU-T Y.3090 (2022) recommendation. Specifically, we explore and discuss the dual nature of DTNs, meant as a digital twin of the network and a network of digital twins, allowing the co-existence of both paradigms.","sentences":["Digital twin networks (DTNs) serve as an emerging facilitator in the industrial networking sector, enabling the management of new classes of services, which require tailored support for improved resource utilization, low latencies and accurate data fidelity.","In this paper, we explore the intersection between theoretical recommendations and practical implications of applying DTNs to industrial networked environments, sharing empirical findings and lessons learned from our ongoing work.","To this end, we first provide experimental examples from selected aspects of data representations and fidelity, mixed-criticality workload support, and application-driven services.","Then, we introduce an architectural framework for DTNs, exposing a more practical extension of existing standards; notably the ITU-T Y.3090 (2022) recommendation.","Specifically, we explore and discuss the dual nature of DTNs, meant as a digital twin of the network and a network of digital twins, allowing the co-existence of both paradigms."],"url":"http://arxiv.org/abs/2405.07605v1","category":"cs.NI"}
{"created":"2024-05-13 09:59:08","title":"Filling Riemann surfaces by hyperbolic Schottky manifolds of negative volume","abstract":"We provide conditions under which a Riemann surface $X$ is the asymptotic boundary of a convex co-compact hyperbolic manifold, homeomorphic to a handlebody, of negative renormalized volume. We prove that this is the case when there are on $X$ enough closed curves of short enough hyperbolic length.","sentences":["We provide conditions under which a Riemann surface $X$ is the asymptotic boundary of a convex co-compact hyperbolic manifold, homeomorphic to a handlebody, of negative renormalized volume.","We prove that this is the case when there are on $X$ enough closed curves of short enough hyperbolic length."],"url":"http://arxiv.org/abs/2405.07598v1","category":"math.DG"}
{"created":"2024-05-13 09:58:45","title":"Local Mutual-Information Differential Privacy","abstract":"Local mutual-information differential privacy (LMIDP) is a privacy notion that aims to quantify the reduction of uncertainty about the input data when the output of a privacy-preserving mechanism is revealed. We study the relation of LMIDP with local differential privacy (LDP), the de facto standard notion of privacy in context-independent (CI) scenarios, and with local information privacy (LIP), the state-of-the-art notion for context-dependent settings. We establish explicit conversion rules, i.e., bounds on the privacy parameters for a LMIDP mechanism to also satisfy LDP/LIP, and vice versa. We use our bounds to formally verify that LMIDP is a weak privacy notion. We also show that uncorrelated Gaussian noise is the best-case noise in terms of CI-LMIDP if both the input data and the noise are subject to an average power constraint.","sentences":["Local mutual-information differential privacy (LMIDP) is a privacy notion that aims to quantify the reduction of uncertainty about the input data when the output of a privacy-preserving mechanism is revealed.","We study the relation of LMIDP with local differential privacy (LDP), the de facto standard notion of privacy in context-independent (CI) scenarios, and with local information privacy (LIP), the state-of-the-art notion for context-dependent settings.","We establish explicit conversion rules, i.e., bounds on the privacy parameters for a LMIDP mechanism to also satisfy LDP/LIP, and vice versa.","We use our bounds to formally verify that LMIDP is a weak privacy notion.","We also show that uncorrelated Gaussian noise is the best-case noise in terms of CI-LMIDP if both the input data and the noise are subject to an average power constraint."],"url":"http://arxiv.org/abs/2405.07596v1","category":"cs.IT"}
{"created":"2024-05-13 09:54:54","title":"A Partially Defined Game with Payments","abstract":"We investigate a new problem that can be solved by using the theory of a partially defined game. We consider the situation described below: first, we assume that the worth of the grand and singleton coalitions is only known. It take some amount of costs to obtain worth of larger coalitions. If it is performed, then players make a payment from the worth of the grand coalition. That is, the worth of the grand coalition is reduced by examinations of coalitional worth. The problem of a partially defined game with payments is finding the solution of partially defined games at each point and the best exiting rule of examinations of coalitional worth.","sentences":["We investigate a new problem that can be solved by using the theory of a partially defined game.","We consider the situation described below: first, we assume that the worth of the grand and singleton coalitions is only known.","It take some amount of costs to obtain worth of larger coalitions.","If it is performed, then players make a payment from the worth of the grand coalition.","That is, the worth of the grand coalition is reduced by examinations of coalitional worth.","The problem of a partially defined game with payments is finding the solution of partially defined games at each point and the best exiting rule of examinations of coalitional worth."],"url":"http://arxiv.org/abs/2405.07591v1","category":"cs.GT"}
{"created":"2024-05-13 09:52:50","title":"Entanglement Swapping in Orbit: a Satellite Quantum Link Case Study","abstract":"Satellite quantum communication is a promising way to build long distance quantum links, making it an essential complement to optical fiber for quantum internetworking beyond metropolitan scales. A satellite point to point optical link differs from the more common fiber links in many ways, both quantitative (higher latency, strong losses) and qualitative (nonconstant parameter values during satellite passage, intermittency of the link, impossibility to set repeaters between the satellite and the ground station). We study here the performance of a quantum link between two ground stations, using a quantum-memory-equipped satellite as a quantum repeater. In contrast with quantum key distribution satellite links, the number of available quantum memory slots m, together with the unavoidable round-trip communication latency t of at least a few milliseconds, severely reduces the effective average repetition rate to m/t -- at most a few kilohertz for foreseeable quantum memories. Our study uses two approaches, which validate each other: 1) a simple analytical model of the effective rate of the quantum link; 2) an event-based simulation using the open source Quantum Internet Simulation Package (QuISP). The important differences between satellite and fiber links led us to modify QuISP itself. This work paves the way to the study of hybrid satellite- and fiber-based quantum repeater networks interconnecting different metropolitan areas.","sentences":["Satellite quantum communication is a promising way to build long distance quantum links, making it an essential complement to optical fiber for quantum internetworking beyond metropolitan scales.","A satellite point to point optical link differs from the more common fiber links in many ways, both quantitative (higher latency, strong losses) and qualitative (nonconstant parameter values during satellite passage, intermittency of the link, impossibility to set repeaters between the satellite and the ground station).","We study here the performance of a quantum link between two ground stations, using a quantum-memory-equipped satellite as a quantum repeater.","In contrast with quantum key distribution satellite links, the number of available quantum memory slots m, together with the unavoidable round-trip communication latency t of at least a few milliseconds, severely reduces the effective average repetition rate to m/t -- at most a few kilohertz for foreseeable quantum memories.","Our study uses two approaches, which validate each other: 1) a simple analytical model of the effective rate of the quantum link; 2) an event-based simulation using the open source Quantum Internet Simulation Package (QuISP).","The important differences between satellite and fiber links led us to modify QuISP itself.","This work paves the way to the study of hybrid satellite- and fiber-based quantum repeater networks interconnecting different metropolitan areas."],"url":"http://arxiv.org/abs/2405.07589v1","category":"quant-ph"}
{"created":"2024-05-13 09:22:19","title":"MaskFuser: Masked Fusion of Joint Multi-Modal Tokenization for End-to-End Autonomous Driving","abstract":"Current multi-modality driving frameworks normally fuse representation by utilizing attention between single-modality branches. However, the existing networks still suppress the driving performance as the Image and LiDAR branches are independent and lack a unified observation representation. Thus, this paper proposes MaskFuser, which tokenizes various modalities into a unified semantic feature space and provides a joint representation for further behavior cloning in driving contexts. Given the unified token representation, MaskFuser is the first work to introduce cross-modality masked auto-encoder training. The masked training enhances the fusion representation by reconstruction on masked tokens. Architecturally, a hybrid-fusion network is proposed to combine advantages from both early and late fusion: For the early fusion stage, modalities are fused by performing monotonic-to-BEV translation attention between branches; Late fusion is performed by tokenizing various modalities into a unified token space with shared encoding on it. MaskFuser respectively reaches a driving score of 49.05 and route completion of 92.85% on the CARLA LongSet6 benchmark evaluation, which improves the best of previous baselines by 1.74 and 3.21%. The introduced masked fusion increases driving stability under damaged sensory inputs. MaskFuser outperforms the best of previous baselines on driving score by 6.55 (27.8%), 1.53 (13.8%), 1.57 (30.9%), respectively given sensory masking ratios 25%, 50%, and 75%.","sentences":["Current multi-modality driving frameworks normally fuse representation by utilizing attention between single-modality branches.","However, the existing networks still suppress the driving performance as the Image and LiDAR branches are independent and lack a unified observation representation.","Thus, this paper proposes MaskFuser, which tokenizes various modalities into a unified semantic feature space and provides a joint representation for further behavior cloning in driving contexts.","Given the unified token representation, MaskFuser is the first work to introduce cross-modality masked auto-encoder training.","The masked training enhances the fusion representation by reconstruction on masked tokens.","Architecturally, a hybrid-fusion network is proposed to combine advantages from both early and late fusion: For the early fusion stage, modalities are fused by performing monotonic-to-BEV translation attention between branches; Late fusion is performed by tokenizing various modalities into a unified token space with shared encoding on it.","MaskFuser respectively reaches a driving score of 49.05 and route completion of 92.85% on the CARLA LongSet6 benchmark evaluation, which improves the best of previous baselines by 1.74 and 3.21%.","The introduced masked fusion increases driving stability under damaged sensory inputs.","MaskFuser outperforms the best of previous baselines on driving score by 6.55 (27.8%), 1.53 (13.8%), 1.57 (30.9%), respectively given sensory masking ratios 25%, 50%, and 75%."],"url":"http://arxiv.org/abs/2405.07573v1","category":"cs.CV"}
{"created":"2024-05-13 09:07:31","title":"Networked ISAC for Low-Altitude Economy: Transmit Beamforming and UAV Trajectory Design","abstract":"This paper studies the exploitation of networked integrated sensing and communications (ISAC) to support low-altitude economy (LAE), in which a set of networked ground base stations (GBSs) transmit wireless signals to cooperatively communicate with multiple authorized unmanned aerial vehicles (UAVs) and concurrently use the echo signals to detect the invasion of unauthorized objects in interested airspace. Under this setup, we jointly design the cooperative transmit beamforming at multiple GBSs together with the trajectory control of authorized UAVs and their GBS associations, for enhancing the authorized UAVs' communication performance while ensuring the sensing requirements for airspace monitoring. In particular, our objective is to maximize the average sum rate of authorized UAVs over a particular flight period, subject to the minimum illumination power constraints for sensing over the interested airspace, the maximum transmit power constraints at individual GBSs, and the flight constraints at UAVs. This problem is non-convex and challenging to solve, due to the involvement of integer variables and the coupling of optimization variables. To solve this non-convex problem, we propose an efficient algorithm by using the techniques of alternating optimization (AO), successive convex approximation (SCA), and semi-definite relaxation (SDR). Numerical results show that the obtained transmit beamforming and UAV trajectory designs in the proposed algorithm efficiently balance the tradeoff between the sensing and communication performances, thus significantly outperforming various benchmarks.","sentences":["This paper studies the exploitation of networked integrated sensing and communications (ISAC) to support low-altitude economy (LAE), in which a set of networked ground base stations (GBSs) transmit wireless signals to cooperatively communicate with multiple authorized unmanned aerial vehicles (UAVs) and concurrently use the echo signals to detect the invasion of unauthorized objects in interested airspace.","Under this setup, we jointly design the cooperative transmit beamforming at multiple GBSs together with the trajectory control of authorized UAVs and their GBS associations, for enhancing the authorized UAVs' communication performance while ensuring the sensing requirements for airspace monitoring.","In particular, our objective is to maximize the average sum rate of authorized UAVs over a particular flight period, subject to the minimum illumination power constraints for sensing over the interested airspace, the maximum transmit power constraints at individual GBSs, and the flight constraints at UAVs.","This problem is non-convex and challenging to solve, due to the involvement of integer variables and the coupling of optimization variables.","To solve this non-convex problem, we propose an efficient algorithm by using the techniques of alternating optimization (AO), successive convex approximation (SCA), and semi-definite relaxation (SDR).","Numerical results show that the obtained transmit beamforming and UAV trajectory designs in the proposed algorithm efficiently balance the tradeoff between the sensing and communication performances, thus significantly outperforming various benchmarks."],"url":"http://arxiv.org/abs/2405.07568v1","category":"eess.SP"}
{"created":"2024-05-13 09:00:40","title":"Growth of GeO2 on R-plane and C-plane Sapphires by MOCVD","abstract":"Rutile Germanium Dioxide (GeO2) has been recently theoretically identified as an ultrawide bandgap (UWBG) semiconductor with bandgap 4.68 eV similar to Ga2O3 but having bipolar dopability and ~2x higher electron mobility, Baliga figure of merit (BFOM) and thermal conductivity than Ga2O3. Bulk crystal growth is rapidly moving towards making large sized native substrates available. These outstanding material properties position GeO2 as a highly attractive UWBG semiconductor for various applications. However, the epitaxial growth in the most advantageous polymorph (rutile), ensuring controlled phase, pristine surface/interface quality, precise microstructure, and optimal functional properties, is still in its infancy. In this work, we explored growth of GeO2 by metal-organic chemical vapor deposition (MOCVD) on both C- and R-plane sapphire. Utilizing tetramethylgermane (TMGe) as a precursor, we have investigated the influences of different parameters on the film properties, including growth temperature, chamber pressure, TMGe flow rate, oxygen flow rate, shroud gas flow rate, and rotation speed. The total pressure emerged as a crucial parameter while growth attempts at low total pressure resulted in no films for a wide range of temperatures, precursor flow rate, argon flow rates, and susceptor rotation rate. A phase diagram, derived from our experimental findings, delineates the growth windows for GeO2 films on sapphire substrates. This study serves as a pioneering guide for the MOCVD growth of GeO2 films.","sentences":["Rutile Germanium Dioxide (GeO2) has been recently theoretically identified as an ultrawide bandgap (UWBG) semiconductor with bandgap 4.68 eV similar to Ga2O3 but having bipolar dopability and ~2x higher electron mobility, Baliga figure of merit (BFOM) and thermal conductivity than Ga2O3.","Bulk crystal growth is rapidly moving towards making large sized native substrates available.","These outstanding material properties position GeO2 as a highly attractive UWBG semiconductor for various applications.","However, the epitaxial growth in the most advantageous polymorph (rutile), ensuring controlled phase, pristine surface/interface quality, precise microstructure, and optimal functional properties, is still in its infancy.","In this work, we explored growth of GeO2 by metal-organic chemical vapor deposition (MOCVD) on both C- and R-plane sapphire.","Utilizing tetramethylgermane (TMGe) as a precursor, we have investigated the influences of different parameters on the film properties, including growth temperature, chamber pressure, TMGe flow rate, oxygen flow rate, shroud gas flow rate, and rotation speed.","The total pressure emerged as a crucial parameter while growth attempts at low total pressure resulted in no films for a wide range of temperatures, precursor flow rate, argon flow rates, and susceptor rotation rate.","A phase diagram, derived from our experimental findings, delineates the growth windows for GeO2 films on sapphire substrates.","This study serves as a pioneering guide for the MOCVD growth of GeO2 films."],"url":"http://arxiv.org/abs/2405.07564v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 07:35:24","title":"Dehazing Remote Sensing and UAV Imagery: A Review of Deep Learning, Prior-based, and Hybrid Approaches","abstract":"High-quality images are crucial in remote sensing and UAV applications, but atmospheric haze can severely degrade image quality, making image dehazing a critical research area. Since the introduction of deep convolutional neural networks, numerous approaches have been proposed, and even more have emerged with the development of vision transformers and contrastive/few-shot learning. Simultaneously, papers describing dehazing architectures applicable to various Remote Sensing (RS) domains are also being published. This review goes beyond the traditional focus on benchmarked haze datasets, as we also explore the application of dehazing techniques to remote sensing and UAV datasets, providing a comprehensive overview of both deep learning and prior-based approaches in these domains. We identify key challenges, including the lack of large-scale RS datasets and the need for more robust evaluation metrics, and outline potential solutions and future research directions to address them. This review is the first, to our knowledge, to provide comprehensive discussions on both existing and very recent dehazing approaches (as of 2024) on benchmarked and RS datasets, including UAV-based imagery.","sentences":["High-quality images are crucial in remote sensing and UAV applications, but atmospheric haze can severely degrade image quality, making image dehazing a critical research area.","Since the introduction of deep convolutional neural networks, numerous approaches have been proposed, and even more have emerged with the development of vision transformers and contrastive/few-shot learning.","Simultaneously, papers describing dehazing architectures applicable to various Remote Sensing (RS) domains are also being published.","This review goes beyond the traditional focus on benchmarked haze datasets, as we also explore the application of dehazing techniques to remote sensing and UAV datasets, providing a comprehensive overview of both deep learning and prior-based approaches in these domains.","We identify key challenges, including the lack of large-scale RS datasets and the need for more robust evaluation metrics, and outline potential solutions and future research directions to address them.","This review is the first, to our knowledge, to provide comprehensive discussions on both existing and very recent dehazing approaches (as of 2024) on benchmarked and RS datasets, including UAV-based imagery."],"url":"http://arxiv.org/abs/2405.07520v1","category":"cs.CV"}
{"created":"2024-05-13 07:34:55","title":"Stability equivalence for stochastic differential equations, stochastic differential delay equations and their corresponding Euler-Maruyama methods in $G$-framework","abstract":"In this paper, we investigate the stability equivalence problem for stochastic differential delay equations, the auxiliary stochastic differential equations and their corresponding Euler-Maruyama (EM) methods under $G$-framework. More precisely, for $p\\geq 2$, we prove the equivalence of practical exponential stability in $p$-th moment sense among stochastic differential delay equations driven by $G$-Brownian motion ($G$-SDDEs), the auxiliary stochastic differential equations driven by $G$-Brownian motion ($G$-SDEs), and their corresponding Euler-Maruyama methods, provided the delay or the step size is small enough. Thus, we can carry out careful simulations to examine the practical exponential stability of the underlying $G$-SDDE or $G$-SDE under some reasonable assumptions.","sentences":["In this paper, we investigate the stability equivalence problem for stochastic differential delay equations, the auxiliary stochastic differential equations and their corresponding Euler-Maruyama (EM) methods under $G$-framework.","More precisely, for $p\\geq 2$, we prove the equivalence of practical exponential stability in $p$-th moment sense among stochastic differential delay equations driven by $G$-Brownian motion ($G$-SDDEs), the auxiliary stochastic differential equations driven by $G$-Brownian motion ($G$-SDEs), and their corresponding Euler-Maruyama methods, provided the delay or the step size is small enough.","Thus, we can carry out careful simulations to examine the practical exponential stability of the underlying $G$-SDDE or $G$-SDE under some reasonable assumptions."],"url":"http://arxiv.org/abs/2405.07519v1","category":"math.PR"}
{"created":"2024-05-13 07:32:32","title":"Side-polished Silica-Fluoride Multimode Fibre Pump Combiner for Mid-IR Fibre Lasers and Amplifiers","abstract":"Side-pumping fibre combiners offer several advantages in fibre laser design, including distributed pump absorption, reduced heat load, and improved flexibility and reliability. These benefits are particularly important for all-fibre lasers and amplifiers operating in the mid-IR wavelength range and based on soft-glass optical fibres. However, conventional fabrication methods face limitations due to significant differences in the thermal properties of pump-delivering silica fibres and signal-guiding fluoride-based fibres. To address these challenges, this work introduces a design for a fuse-less side-polished (D-shaped) fibre-based pump combiner comprising multimode silica and double-clad fluoride-based fibres. The results demonstrate stable coupling efficiency exceeding 80% at a 980-nm wavelength over 8 hours of continuous operation under active thermal control. The developed pump combiner has also been successfully integrated into a linear Er-doped fibre laser cavity, showing continuous-wave generation at 2731 or 2781-nm central wavelength with an output power of 0.87 W. Overall, this innovative approach presents a simple, repeatable, and reproducible pump combiner design that opens up new possibilities for leveraging fibre-based component technology in soft glass matrices and other emerging fibres with unique compositions.","sentences":["Side-pumping fibre combiners offer several advantages in fibre laser design, including distributed pump absorption, reduced heat load, and improved flexibility and reliability.","These benefits are particularly important for all-fibre lasers and amplifiers operating in the mid-IR wavelength range and based on soft-glass optical fibres.","However, conventional fabrication methods face limitations due to significant differences in the thermal properties of pump-delivering silica fibres and signal-guiding fluoride-based fibres.","To address these challenges, this work introduces a design for a fuse-less side-polished (D-shaped) fibre-based pump combiner comprising multimode silica and double-clad fluoride-based fibres.","The results demonstrate stable coupling efficiency exceeding 80% at a 980-nm wavelength over 8 hours of continuous operation under active thermal control.","The developed pump combiner has also been successfully integrated into a linear Er-doped fibre laser cavity, showing continuous-wave generation at 2731 or 2781-nm central wavelength with an output power of 0.87 W. Overall, this innovative approach presents a simple, repeatable, and reproducible pump combiner design that opens up new possibilities for leveraging fibre-based component technology in soft glass matrices and other emerging fibres with unique compositions."],"url":"http://arxiv.org/abs/2405.07517v1","category":"physics.optics"}
{"created":"2024-05-13 06:35:51","title":"Distributed Quantum Computation with Minimum Circuit Execution Time over Quantum Networks","abstract":"Present quantum computers are constrained by limited qubit capacity and restricted physical connectivity, leading to challenges in large-scale quantum computations. Distributing quantum computations across a network of quantum computers is a promising way to circumvent these challenges and facilitate large quantum computations. However, distributed quantum computations require entanglements (to execute remote gates) which can incur significant generation latency and, thus, lead to decoherence of qubits. In this work, we consider the problem of distributing quantum circuits across a quantum network to minimize the execution time. The problem entails mapping the circuit qubits to network memories, including within each computer since limited connectivity within computers can affect the circuit execution time. We provide two-step solutions for the above problem: In the first step, we allocate qubits to memories to minimize the estimated execution time; for this step, we design an efficient algorithm based on an approximation algorithm for the max-quadratic-assignment problem. In the second step, we determine an efficient execution scheme, including generating required entanglements with minimum latency under the network resource and decoherence constraints; for this step, we develop two algorithms with appropriate performance guarantees under certain settings or assumptions. We consider multiple protocols for executing remote gates, viz., telegates and cat-entanglements. With extensive simulations over NetSquid, a quantum network simulator, we demonstrate the effectiveness of our developed techniques and show that they outperform a scheme based on prior work by up to 95%.","sentences":["Present quantum computers are constrained by limited qubit capacity and restricted physical connectivity, leading to challenges in large-scale quantum computations.","Distributing quantum computations across a network of quantum computers is a promising way to circumvent these challenges and facilitate large quantum computations.","However, distributed quantum computations require entanglements (to execute remote gates) which can incur significant generation latency and, thus, lead to decoherence of qubits.","In this work, we consider the problem of distributing quantum circuits across a quantum network to minimize the execution time.","The problem entails mapping the circuit qubits to network memories, including within each computer since limited connectivity within computers can affect the circuit execution time.","We provide two-step solutions for the above problem: In the first step, we allocate qubits to memories to minimize the estimated execution time; for this step, we design an efficient algorithm based on an approximation algorithm for the max-quadratic-assignment problem.","In the second step, we determine an efficient execution scheme, including generating required entanglements with minimum latency under the network resource and decoherence constraints; for this step, we develop two algorithms with appropriate performance guarantees under certain settings or assumptions.","We consider multiple protocols for executing remote gates, viz., telegates and cat-entanglements.","With extensive simulations over NetSquid, a quantum network simulator, we demonstrate the effectiveness of our developed techniques and show that they outperform a scheme based on prior work by up to 95%."],"url":"http://arxiv.org/abs/2405.07499v1","category":"quant-ph"}
{"created":"2024-05-13 06:30:13","title":"A systematic study of the ultra-fast outflow responses to luminosity variations in active galactic nuclei","abstract":"The extreme velocities and high ionization states of ultra-fast outflows (UFOs) make them a promising candidate for AGN feedback on the evolution of the host galaxy. However, their exact underlying driving mechanism is not yet fully understood. Given that the variability of UFOs may be used to distinguish among different launching mechanisms, we aim to search for and characterize the responses of the UFO properties to the variable irradiating luminosity. We performed a high-resolution spectroscopy of archival XMM-Newton observations on six highly-accreting NLS1 galaxies. The state-of-the-art methods of the blind Gaussian line scan and photoionization model scan are used to identify UFO solutions. We search for ionized winds and investigate the structure of ionized winds and their responses to the luminosity variations. The powerful photoionization model scan reveals three previously unreported UFOs in RE J1034+396, PG 1244+026 and I ZW 1, and two new WAs in RE J1034+396. The entrained UFOs are discovered in 4 (66%) AGN, supporting the shocked outflow interpretation for AGN ionized winds. 2 out of 7 (28%) UFOs seem to respond to the continuum and 3 (43%) UFOs hint at a radiatively accelerated nature. Combined with published works, we do not find any correlations between UFO responses and AGN properties except for a tentative ($\\sim1.8\\sigma$) anti-correlation between the UFO acceleration and the Eddington ratio, to be confirmed by further observations and an enlarged sample. The kinetic energy of UFOs, mostly detected in soft X-rays, is found to have a large uncertainty. We, therefore, cannot conclude whether soft X-ray UFOs have sufficient energy to drive the AGN feedback, although they are very promising based on some reasonable assumptions.","sentences":["The extreme velocities and high ionization states of ultra-fast outflows (UFOs) make them a promising candidate for AGN feedback on the evolution of the host galaxy.","However, their exact underlying driving mechanism is not yet fully understood.","Given that the variability of UFOs may be used to distinguish among different launching mechanisms, we aim to search for and characterize the responses of the UFO properties to the variable irradiating luminosity.","We performed a high-resolution spectroscopy of archival XMM-Newton observations on six highly-accreting NLS1 galaxies.","The state-of-the-art methods of the blind Gaussian line scan and photoionization model scan are used to identify UFO solutions.","We search for ionized winds and investigate the structure of ionized winds and their responses to the luminosity variations.","The powerful photoionization model scan reveals three previously unreported UFOs in RE J1034+396, PG 1244","+026","and I ZW 1, and two new WAs in RE J1034+396.","The entrained UFOs are discovered in 4 (66%) AGN, supporting the shocked outflow interpretation for AGN ionized winds.","2 out of 7 (28%) UFOs seem to respond to the continuum and 3 (43%) UFOs hint at a radiatively accelerated nature.","Combined with published works, we do not find any correlations between UFO responses and AGN properties except for a tentative ($\\sim1.8\\sigma$) anti-correlation between the UFO acceleration and the Eddington ratio, to be confirmed by further observations and an enlarged sample.","The kinetic energy of UFOs, mostly detected in soft X-rays, is found to have a large uncertainty.","We, therefore, cannot conclude whether soft X-ray UFOs have sufficient energy to drive the AGN feedback, although they are very promising based on some reasonable assumptions."],"url":"http://arxiv.org/abs/2405.07494v1","category":"astro-ph.HE"}
{"created":"2024-05-13 06:16:15","title":"Variable-Length Secret Key Agreement via Random Stopping Time","abstract":"We consider a key agreement setting where two parties observe correlated random sources, and want to agree on a secret key via public discussions. In order to allow the key length to adapt to the realizations of the random sources, we allow the key to be of variable length, subject to a novel variable-length version of the uniformity constraint based on random stopping time. We propose simple, computationally efficient key agreement schemes under the new constraint. The proposed scheme can be considered as the key agreement analogue of variable-length source coding via Huffman coding, and the Knuth-Yao random number generator.","sentences":["We consider a key agreement setting where two parties observe correlated random sources, and want to agree on a secret key via public discussions.","In order to allow the key length to adapt to the realizations of the random sources, we allow the key to be of variable length, subject to a novel variable-length version of the uniformity constraint based on random stopping time.","We propose simple, computationally efficient key agreement schemes under the new constraint.","The proposed scheme can be considered as the key agreement analogue of variable-length source coding via Huffman coding, and the Knuth-Yao random number generator."],"url":"http://arxiv.org/abs/2405.07493v1","category":"cs.IT"}
{"created":"2024-05-13 05:13:28","title":"On treewidth and maximum cliques","abstract":"We construct classes of graphs that are variants of the so-called layered wheel. One of their key properties is that while the treewidth is bounded by a function of the clique number, the construction can be adjusted to make the dependance grow arbitrarily. Some of these classes provide counter-examples to several conjectures. In particular, the construction includes hereditary classes of graphs whose treewidth is bounded by a function of the clique number while the tree-independence number is unbounded, thus disproving a conjecture of Dallard, Milani\\v{c} and \\v{S}torgel [Treewidth versus clique number. II. Tree-independence number. Journal of Combinatorial Theory, Series B, 164:404-442, 2024.]. The construction can be further adjusted to provide, for any fixed integer $c$, graphs of arbitrarily large treewidth that contain no $K_c$-free graphs of high treewidth, thus disproving a conjecture of Hajebi [Chordal graphs, even-hole-free graphs and sparse obstructions to bounded treewidth, arXiv:2401.01299, 2024].","sentences":["We construct classes of graphs that are variants of the so-called layered wheel.","One of their key properties is that while the treewidth is bounded by a function of the clique number, the construction can be adjusted to make the dependance grow arbitrarily.","Some of these classes provide counter-examples to several conjectures.","In particular, the construction includes hereditary classes of graphs whose treewidth is bounded by a function of the clique number while the tree-independence number is unbounded, thus disproving a conjecture of Dallard, Milani\\v{c} and \\v{S}torgel","[Treewidth versus clique number.","II.","Tree-independence number.","Journal of Combinatorial Theory, Series B, 164:404-442, 2024.].","The construction can be further adjusted to provide, for any fixed integer $c$, graphs of arbitrarily large treewidth that contain no $K_c$-free graphs of high treewidth, thus disproving a conjecture of Hajebi [Chordal graphs, even-hole-free graphs and sparse obstructions to bounded treewidth, arXiv:2401.01299, 2024]."],"url":"http://arxiv.org/abs/2405.07471v1","category":"math.CO"}
{"created":"2024-05-13 04:56:28","title":"Ginzburg-Landau simulations of three-terminal operation of a superconducting nanowire cryotron","abstract":"Superconducting nanowire cryotrons (nTrons) are expected to be used as interfaces for super-high-performance hybrid devices in which superconductor and semiconductor circuits are combined. However, nTrons are still under development, and diverse analyses of these devices are needed. Accordingly, we have developed a numerical technique to simulate the three-terminal operation of an nTron by using the finite element method to solve the time-dependent Ginzburg-Landau (TDGL) equation and the heat-diffusion equation. Simulations using this technique offer understanding of the dynamics of the order parameter, the thermal behavior, and the characteristics of three-terminal operation, and the TDGL model reproduces qualitatively the results of nTron experiments. In addition, we investigated how some geometric and physical parameters (the design elements) affect the operation characteristics. The TDGL model has far fewer free parameters compared with the lumped-element electrothermal model commonly used for simulating superconducting devices. Furthermore, the TDGL model provides time-dependent visual information about the superconducting state and the normal state, thereby offering insights into the relationship between nTron geometry and three-terminal operation. These simulation results offer a route to nTron optimization and the development of nTron applications.","sentences":["Superconducting nanowire cryotrons (nTrons) are expected to be used as interfaces for super-high-performance hybrid devices in which superconductor and semiconductor circuits are combined.","However, nTrons are still under development, and diverse analyses of these devices are needed.","Accordingly, we have developed a numerical technique to simulate the three-terminal operation of an nTron by using the finite element method to solve the time-dependent Ginzburg-Landau (TDGL) equation and the heat-diffusion equation.","Simulations using this technique offer understanding of the dynamics of the order parameter, the thermal behavior, and the characteristics of three-terminal operation, and the TDGL model reproduces qualitatively the results of nTron experiments.","In addition, we investigated how some geometric and physical parameters (the design elements) affect the operation characteristics.","The TDGL model has far fewer free parameters compared with the lumped-element electrothermal model commonly used for simulating superconducting devices.","Furthermore, the TDGL model provides time-dependent visual information about the superconducting state and the normal state, thereby offering insights into the relationship between nTron geometry and three-terminal operation.","These simulation results offer a route to nTron optimization and the development of nTron applications."],"url":"http://arxiv.org/abs/2405.07466v1","category":"cond-mat.supr-con"}
{"created":"2024-05-13 04:15:14","title":"Magnetoelectric domain engineering from micrometer to \u00c5ngstr\u00f8m scales","abstract":"The functionality of magnetoelectric multiferroics depends on the formation, size, and coupling of their magnetic and electric domains. Knowing the parameters guiding these criteria is a key effort in the emerging field of magnetoelectric domain engineering. Here we show, using a combination of piezoresponse-force microscopy, non-linear optics, and x-ray scattering, that the correlation length setting the size of the ferroelectric domains in the multiferroic hexagonal manganites can be engineered from the micron range down to a few unit cells under the substitution of Mn$^{3+}$ ions with Al$^{3+}$ ions. The magnetoelectric coupling mechanism between the antiferromagnetic Mn$^{3+}$ order and the distortive-ferroelectric order remains intact even at substantial replacement of Mn$^{3+}$ by Al$^{3+}$. Hence, chemical substitution proves to be an effective tool for domain-size engineering in one of the most studied classes of multiferroics.","sentences":["The functionality of magnetoelectric multiferroics depends on the formation, size, and coupling of their magnetic and electric domains.","Knowing the parameters guiding these criteria is a key effort in the emerging field of magnetoelectric domain engineering.","Here we show, using a combination of piezoresponse-force microscopy, non-linear optics, and x-ray scattering, that the correlation length setting the size of the ferroelectric domains in the multiferroic hexagonal manganites can be engineered from the micron range down to a few unit cells under the substitution of Mn$^{3+}$ ions with Al$^{3+}$ ions.","The magnetoelectric coupling mechanism between the antiferromagnetic Mn$^{3+}$ order and the distortive-ferroelectric order remains intact even at substantial replacement of Mn$^{3+}$ by Al$^{3+}$. Hence, chemical substitution proves to be an effective tool for domain-size engineering in one of the most studied classes of multiferroics."],"url":"http://arxiv.org/abs/2405.07457v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 03:52:45","title":"On Securing Analog Lagrange Coded Computing from Colluding Adversaries","abstract":"Analog Lagrange Coded Computing (ALCC) is a recently proposed coded computing paradigm wherein certain computations over analog datasets can be efficiently performed using distributed worker nodes through floating point implementation. While ALCC is known to preserve privacy of data from the workers, it is not resilient to adversarial workers that return erroneous computation results. Pointing at this security vulnerability, we focus on securing ALCC from a wide range of non-colluding and colluding adversarial workers. As a foundational step, we make use of error-correction algorithms for Discrete Fourier Transform (DFT) codes to build novel algorithms to nullify the erroneous computations returned from the adversaries. Furthermore, when such a robust ALCC is implemented in practical settings, we show that the presence of precision errors in the system can be exploited by the adversaries to propose novel colluding attacks to degrade the computation accuracy. As the main takeaway, we prove a counter-intuitive result that not all the adversaries should inject noise in their computations in order to optimally degrade the accuracy of the ALCC framework. This is the first work of its kind to address the vulnerability of ALCC against colluding adversaries.","sentences":["Analog Lagrange Coded Computing (ALCC) is a recently proposed coded computing paradigm wherein certain computations over analog datasets can be efficiently performed using distributed worker nodes through floating point implementation.","While ALCC is known to preserve privacy of data from the workers, it is not resilient to adversarial workers that return erroneous computation results.","Pointing at this security vulnerability, we focus on securing ALCC from a wide range of non-colluding and colluding adversarial workers.","As a foundational step, we make use of error-correction algorithms for Discrete Fourier Transform (DFT) codes to build novel algorithms to nullify the erroneous computations returned from the adversaries.","Furthermore, when such a robust ALCC is implemented in practical settings, we show that the presence of precision errors in the system can be exploited by the adversaries to propose novel colluding attacks to degrade the computation accuracy.","As the main takeaway, we prove a counter-intuitive result that not all the adversaries should inject noise in their computations in order to optimally degrade the accuracy of the ALCC framework.","This is the first work of its kind to address the vulnerability of ALCC against colluding adversaries."],"url":"http://arxiv.org/abs/2405.07454v1","category":"cs.IT"}
{"created":"2024-05-13 03:18:27","title":"Locality-Preserving Free-Form Deformation","abstract":"This paper proposes a method to estimate the locations of grid handles in free-form deformation (FFD) while preserving the local shape characteristics of the 2D/3D input model embedded into the grid, named locality-preserving FFD (lp-FFD). Users first specify some vertex locations in the input model and grid handle locations. The system then optimizes all locations of grid handles by minimizing the distortion of the input model's mesh elements. The proposed method is fast and stable, allowing the user to directly and indirectly make the deformed shape of mesh model and grid. This paper shows some examples of deformation results to demonstrate the robustness of our lp-FFD. In addition, we conducted a user study and confirm our lp-FFD's efficiency and effectiveness in shape deformation is higher than those of existing methods used in commercial software.","sentences":["This paper proposes a method to estimate the locations of grid handles in free-form deformation (FFD) while preserving the local shape characteristics of the 2D/3D input model embedded into the grid, named locality-preserving FFD (lp-FFD).","Users first specify some vertex locations in the input model and grid handle locations.","The system then optimizes all locations of grid handles by minimizing the distortion of the input model's mesh elements.","The proposed method is fast and stable, allowing the user to directly and indirectly make the deformed shape of mesh model and grid.","This paper shows some examples of deformation results to demonstrate the robustness of our lp-FFD.","In addition, we conducted a user study and confirm our lp-FFD's efficiency and effectiveness in shape deformation is higher than those of existing methods used in commercial software."],"url":"http://arxiv.org/abs/2405.07450v1","category":"cs.GR"}
{"created":"2024-05-13 02:58:59","title":"Maximizing Information Gain in Privacy-Aware Active Learning of Email Anomalies","abstract":"Redacted emails satisfy most privacy requirements but they make it more difficult to detect anomalous emails that may be indicative of data exfiltration. In this paper we develop an enhanced method of Active Learning using an information gain maximizing heuristic, and we evaluate its effectiveness in a real world setting where only redacted versions of email could be labeled by human analysts due to privacy concerns. In the first case study we examined how Active Learning should be carried out. We found that model performance was best when a single highly skilled (in terms of the labelling task) analyst provided the labels. In the second case study we used confidence ratings to estimate the labeling uncertainty of analysts and then prioritized instances for labeling based on the expected information gain (the difference between model uncertainty and analyst uncertainty) that would be provided by labelling each instance. We found that the information maximization gain heuristic improved model performance over existing sampling methods for Active Learning. Based on the results obtained, we recommend that analysts should be screened, and possibly trained, prior to implementation of Active Learning in cybersecurity applications. We also recommend that the information gain maximizing sample method (based on expert confidence) should be used in early stages of Active Learning, providing that well-calibrated confidence can be obtained. We also note that the expertise of analysts should be assessed prior to Active Learning, as we found that analysts with lower labelling skill had poorly calibrated (over-) confidence in their labels.","sentences":["Redacted emails satisfy most privacy requirements but they make it more difficult to detect anomalous emails that may be indicative of data exfiltration.","In this paper we develop an enhanced method of Active Learning using an information gain maximizing heuristic, and we evaluate its effectiveness in a real world setting where only redacted versions of email could be labeled by human analysts due to privacy concerns.","In the first case study we examined how Active Learning should be carried out.","We found that model performance was best when a single highly skilled (in terms of the labelling task) analyst provided the labels.","In the second case study we used confidence ratings to estimate the labeling uncertainty of analysts and then prioritized instances for labeling based on the expected information gain (the difference between model uncertainty and analyst uncertainty) that would be provided by labelling each instance.","We found that the information maximization gain heuristic improved model performance over existing sampling methods for Active Learning.","Based on the results obtained, we recommend that analysts should be screened, and possibly trained, prior to implementation of Active Learning in cybersecurity applications.","We also recommend that the information gain maximizing sample method (based on expert confidence) should be used in early stages of Active Learning, providing that well-calibrated confidence can be obtained.","We also note that the expertise of analysts should be assessed prior to Active Learning, as we found that analysts with lower labelling skill had poorly calibrated (over-) confidence in their labels."],"url":"http://arxiv.org/abs/2405.07440v1","category":"cs.HC"}
{"created":"2024-05-13 02:33:47","title":"Towards improved software visualisation of parameterised REE patterns: Introducing REEkit for geological analysis","abstract":"Modern geological studies and mineral exploration techniques rely heavily on being able to digitally visualise and interpret data. Rare earth elements (REEs) are vital for renewable energy technologies. REE concentrations, when normalised to a standard material, show unique geometric curves (or patterns) in geological samples due to their similar chemical properties. The lambda technique can be used to describe these patterns and turn them into points - making it easier to visualise and interpret larger datasets. Lambdas have the potential to help industry understand intricate sample relationships and the geological and economic importance of their data.   This study explored the use of lambdas through the evaluation of various visualisation methods to determine their usefulness in mineral exploration. The 'REEkit' platform facilitated the evaluation of the different visualisation methods and gauged industry interest and acceptance of such a service. Qualitative data was gathered through contextual inquiry, utilising semi-structured interviews and an observational session with 10 participants. Conceptual thematic analysis was applied to extract key findings.   This study found that two critical factors for successful lambda data visualisation in the mineral exploration industry are familiarity and clarity: visualisations that were familiar and commonplace for users allowed for better analysis and clear communication to non-technical audiences. This included visualisations such as the 3D scatter plot and scatter plot matrix. Furthermore, visualisations that complemented each other and seamlessly integrated into the same workflow provided diverse perspectives on the data. Important aspects included understanding population grouping versus data distribution, achieved through combinations such as scatter plot and density contour plot, or 3D scatter plot and violin plot.","sentences":["Modern geological studies and mineral exploration techniques rely heavily on being able to digitally visualise and interpret data.","Rare earth elements (REEs) are vital for renewable energy technologies.","REE concentrations, when normalised to a standard material, show unique geometric curves (or patterns) in geological samples due to their similar chemical properties.","The lambda technique can be used to describe these patterns and turn them into points - making it easier to visualise and interpret larger datasets.","Lambdas have the potential to help industry understand intricate sample relationships and the geological and economic importance of their data.   ","This study explored the use of lambdas through the evaluation of various visualisation methods to determine their usefulness in mineral exploration.","The 'REEkit' platform facilitated the evaluation of the different visualisation methods and gauged industry interest and acceptance of such a service.","Qualitative data was gathered through contextual inquiry, utilising semi-structured interviews and an observational session with 10 participants.","Conceptual thematic analysis was applied to extract key findings.   ","This study found that two critical factors for successful lambda data visualisation in the mineral exploration industry are familiarity and clarity: visualisations that were familiar and commonplace for users allowed for better analysis and clear communication to non-technical audiences.","This included visualisations such as the 3D scatter plot and scatter plot matrix.","Furthermore, visualisations that complemented each other and seamlessly integrated into the same workflow provided diverse perspectives on the data.","Important aspects included understanding population grouping versus data distribution, achieved through combinations such as scatter plot and density contour plot, or 3D scatter plot and violin plot."],"url":"http://arxiv.org/abs/2405.07438v1","category":"cs.HC"}
{"created":"2024-05-13 02:27:45","title":"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention","abstract":"Today, the acquisition of various behavioral log data has enabled deeper understanding of customer preferences and future behaviors in the marketing field. In particular, multimodal deep learning has achieved highly accurate predictions by combining multiple types of data. Many of these studies utilize with feature fusion to construct multimodal models, which combines extracted representations from each modality. However, since feature fusion treats information from each modality equally, it is difficult to perform flexible analysis such as the attention mechanism that has been used extensively in recent years. Therefore, this study proposes a context-aware multimodal deep learning model that combines Bidirectional Encoder Representations from Transformers (BERT) and cross-attention Transformer, which dynamically changes the attention of deep-contextualized word representations based on background information such as consumer demographic and lifestyle variables. We conduct a comprehensive analysis and demonstrate the effectiveness of our model by comparing it with six reference models in three categories using behavioral logs stored on an online platform. In addition, we present an efficient multimodal learning method by comparing the learning efficiency depending on the optimizers and the prediction accuracy depending on the number of tokens in the text data.","sentences":["Today, the acquisition of various behavioral log data has enabled deeper understanding of customer preferences and future behaviors in the marketing field.","In particular, multimodal deep learning has achieved highly accurate predictions by combining multiple types of data.","Many of these studies utilize with feature fusion to construct multimodal models, which combines extracted representations from each modality.","However, since feature fusion treats information from each modality equally, it is difficult to perform flexible analysis such as the attention mechanism that has been used extensively in recent years.","Therefore, this study proposes a context-aware multimodal deep learning model that combines Bidirectional Encoder Representations from Transformers (BERT) and cross-attention Transformer, which dynamically changes the attention of deep-contextualized word representations based on background information such as consumer demographic and lifestyle variables.","We conduct a comprehensive analysis and demonstrate the effectiveness of our model by comparing it with six reference models in three categories using behavioral logs stored on an online platform.","In addition, we present an efficient multimodal learning method by comparing the learning efficiency depending on the optimizers and the prediction accuracy depending on the number of tokens in the text data."],"url":"http://arxiv.org/abs/2405.07435v1","category":"cs.CE"}
{"created":"2024-05-13 01:36:52","title":"Exploring the Effects of User-Agent and User-Designer Similarity in Virtual Human Design to Promote Mental Health Intentions for College Students","abstract":"Virtual humans (i.e., embodied conversational agents) have the potential to support college students' mental health, particularly in Science, Technology, Engineering, and Mathematics (STEM) fields where students are at a heightened risk of mental disorders such as anxiety and depression. A comprehensive understanding of students, considering their cultural characteristics, experiences, and expectations, is crucial for creating timely and effective virtual human interventions. To this end, we conducted a user study with 481 computer science students from a major university in North America, exploring how they co-designed virtual humans to support mental health conversations for students similar to them. Our findings suggest that computer science students who engage in co-design processes of virtual humans tend to create agents that closely resemble them demographically--agent-designer demographic similarity. Key factors influencing virtual human design included age, gender, ethnicity, and the matching between appearance and voice. We also observed that the demographic characteristics of virtual human designers, especially ethnicity and gender, tend to be associated with those of the virtual humans they designed. Finally, we provide insights concerning the impact of user-designer demographic similarity in virtual humans' effectiveness in promoting mental health conversations when designers' characteristics are shared explicitly or implicitly. Understanding how virtual humans' characteristics serve users' experiences in mental wellness conversations and the similarity-attraction effects between agents, users, and designers may help tailor virtual humans' design to enhance their acceptance and increase their counseling effectiveness.","sentences":["Virtual humans (i.e., embodied conversational agents) have the potential to support college students' mental health, particularly in Science, Technology, Engineering, and Mathematics (STEM) fields where students are at a heightened risk of mental disorders such as anxiety and depression.","A comprehensive understanding of students, considering their cultural characteristics, experiences, and expectations, is crucial for creating timely and effective virtual human interventions.","To this end, we conducted a user study with 481 computer science students from a major university in North America, exploring how they co-designed virtual humans to support mental health conversations for students similar to them.","Our findings suggest that computer science students who engage in co-design processes of virtual humans tend to create agents that closely resemble them demographically--agent-designer demographic similarity.","Key factors influencing virtual human design included age, gender, ethnicity, and the matching between appearance and voice.","We also observed that the demographic characteristics of virtual human designers, especially ethnicity and gender, tend to be associated with those of the virtual humans they designed.","Finally, we provide insights concerning the impact of user-designer demographic similarity in virtual humans' effectiveness in promoting mental health conversations when designers' characteristics are shared explicitly or implicitly.","Understanding how virtual humans' characteristics serve users' experiences in mental wellness conversations and the similarity-attraction effects between agents, users, and designers may help tailor virtual humans' design to enhance their acceptance and increase their counseling effectiveness."],"url":"http://arxiv.org/abs/2405.07418v1","category":"cs.HC"}
{"created":"2024-05-13 01:34:16","title":"Identifying Hate Speech Peddlers in Online Platforms. A Bayesian Social Learning Approach for Large Language Model Driven Decision-Makers","abstract":"This paper studies the problem of autonomous agents performing Bayesian social learning for sequential detection when the observations of the state belong to a high-dimensional space and are expensive to analyze. Specifically, when the observations are textual, the Bayesian agent can use a large language model (LLM) as a map to get a low-dimensional private observation. The agent performs Bayesian learning and takes an action that minimizes the expected cost and is visible to subsequent agents. We prove that a sequence of such Bayesian agents herd in finite time to the public belief and take the same action disregarding the private observations. We propose a stopping time formulation for quickest time herding in social learning and optimally balance privacy and herding. Structural results are shown on the threshold nature of the optimal policy to the stopping time problem. We illustrate the application of our framework when autonomous Bayesian detectors aim to sequentially identify if a user is a hate speech peddler on an online platform by parsing text observations using an LLM. We numerically validate our results on real-world hate speech datasets. We show that autonomous Bayesian agents designed to flag hate speech peddlers in online platforms herd and misclassify the users when the public prior is strong. We also numerically show the effect of a threshold policy in delaying herding.","sentences":["This paper studies the problem of autonomous agents performing Bayesian social learning for sequential detection when the observations of the state belong to a high-dimensional space and are expensive to analyze.","Specifically, when the observations are textual, the Bayesian agent can use a large language model (LLM) as a map to get a low-dimensional private observation.","The agent performs Bayesian learning and takes an action that minimizes the expected cost and is visible to subsequent agents.","We prove that a sequence of such Bayesian agents herd in finite time to the public belief and take the same action disregarding the private observations.","We propose a stopping time formulation for quickest time herding in social learning and optimally balance privacy and herding.","Structural results are shown on the threshold nature of the optimal policy to the stopping time problem.","We illustrate the application of our framework when autonomous Bayesian detectors aim to sequentially identify if a user is a hate speech peddler on an online platform by parsing text observations using an LLM.","We numerically validate our results on real-world hate speech datasets.","We show that autonomous Bayesian agents designed to flag hate speech peddlers in online platforms herd and misclassify the users when the public prior is strong.","We also numerically show the effect of a threshold policy in delaying herding."],"url":"http://arxiv.org/abs/2405.07417v1","category":"cs.SI"}
{"created":"2024-05-13 01:29:48","title":"Structured Reinforcement Learning for Incentivized Stochastic Covert Optimization","abstract":"This paper studies how a stochastic gradient algorithm (SG) can be controlled to hide the estimate of the local stationary point from an eavesdropper. Such problems are of significant interest in distributed optimization settings like federated learning and inventory management. A learner queries a stochastic oracle and incentivizes the oracle to obtain noisy gradient measurements and perform SG. The oracle probabilistically returns either a noisy gradient of the function} or a non-informative measurement, depending on the oracle state and incentive. The learner's query and incentive are visible to an eavesdropper who wishes to estimate the stationary point. This paper formulates the problem of the learner performing covert optimization by dynamically incentivizing the stochastic oracle and obfuscating the eavesdropper as a finite-horizon Markov decision process (MDP). Using conditions for interval-dominance on the cost and transition probability structure, we show that the optimal policy for the MDP has a monotone threshold structure. We propose searching for the optimal stationary policy with the threshold structure using a stochastic approximation algorithm and a multi-armed bandit approach. The effectiveness of our methods is numerically demonstrated on a covert federated learning hate-speech classification task.","sentences":["This paper studies how a stochastic gradient algorithm (SG) can be controlled to hide the estimate of the local stationary point from an eavesdropper.","Such problems are of significant interest in distributed optimization settings like federated learning and inventory management.","A learner queries a stochastic oracle and incentivizes the oracle to obtain noisy gradient measurements and perform SG.","The oracle probabilistically returns either a noisy gradient of the function} or a non-informative measurement, depending on the oracle state and incentive.","The learner's query and incentive are visible to an eavesdropper who wishes to estimate the stationary point.","This paper formulates the problem of the learner performing covert optimization by dynamically incentivizing the stochastic oracle and obfuscating the eavesdropper as a finite-horizon Markov decision process (MDP).","Using conditions for interval-dominance on the cost and transition probability structure, we show that the optimal policy for the MDP has a monotone threshold structure.","We propose searching for the optimal stationary policy with the threshold structure using a stochastic approximation algorithm and a multi-armed bandit approach.","The effectiveness of our methods is numerically demonstrated on a covert federated learning hate-speech classification task."],"url":"http://arxiv.org/abs/2405.07415v1","category":"cs.LG"}
{"created":"2024-05-13 01:19:51","title":"Non-intrusive optimal experimental design for large-scale nonlinear Bayesian inverse problems using a Bayesian approximation error approach","abstract":"We consider optimal experimental design (OED) for nonlinear inverse problems within the Bayesian framework. Optimizing the data acquisition process for large-scale nonlinear Bayesian inverse problems is a computationally challenging task since the posterior is typically intractable and commonly-encountered optimality criteria depend on the observed data. Since these challenges are not present in OED for linear Bayesian inverse problems, we propose an approach based on first linearizing the associated forward problem and then optimizing the experimental design. Replacing an accurate but costly model with some linear surrogate, while justified for certain problems, can lead to incorrect posteriors and sub-optimal designs if model discrepancy is ignored. To avoid this, we use the Bayesian approximation error (BAE) approach to formulate an A-optimal design objective for sensor selection that is aware of the model error. In line with recent developments, we prove that this uncertainty-aware objective is independent of the exact choice of linearization. This key observation facilitates the formulation of an uncertainty-aware OED objective function using a completely trivial linear map, the zero map, as a surrogate to the forward dynamics. The base methodology is also extended to marginalized OED problems, accommodating uncertainties arising from both linear approximations and unknown auxiliary parameters. Our approach only requires parameter and data sample pairs, hence it is particularly well-suited for black box forward models. We demonstrate the effectiveness of our method for finding optimal designs in an idealized subsurface flow inverse problem and for tsunami detection.","sentences":["We consider optimal experimental design (OED) for nonlinear inverse problems within the Bayesian framework.","Optimizing the data acquisition process for large-scale nonlinear Bayesian inverse problems is a computationally challenging task since the posterior is typically intractable and commonly-encountered optimality criteria depend on the observed data.","Since these challenges are not present in OED for linear Bayesian inverse problems, we propose an approach based on first linearizing the associated forward problem and then optimizing the experimental design.","Replacing an accurate but costly model with some linear surrogate, while justified for certain problems, can lead to incorrect posteriors and sub-optimal designs if model discrepancy is ignored.","To avoid this, we use the Bayesian approximation error (BAE) approach to formulate an A-optimal design objective for sensor selection that is aware of the model error.","In line with recent developments, we prove that this uncertainty-aware objective is independent of the exact choice of linearization.","This key observation facilitates the formulation of an uncertainty-aware OED objective function using a completely trivial linear map, the zero map, as a surrogate to the forward dynamics.","The base methodology is also extended to marginalized OED problems, accommodating uncertainties arising from both linear approximations and unknown auxiliary parameters.","Our approach only requires parameter and data sample pairs, hence it is particularly well-suited for black box forward models.","We demonstrate the effectiveness of our method for finding optimal designs in an idealized subsurface flow inverse problem and for tsunami detection."],"url":"http://arxiv.org/abs/2405.07412v1","category":"math.NA"}
{"created":"2024-05-13 01:06:15","title":"Bayesian Spatially Clustered Compositional Regression: Linking intersectoral GDP contributions to Gini Coefficients","abstract":"The Gini coefficient is an universally used measurement of income inequality. Intersectoral GDP contributions reveal the economic development of different sectors of the national economy. Linking intersectoral GDP contributions to Gini coefficients will provide better understandings of how the Gini coefficient is influenced by different industries. In this paper, a compositional regression with spatially clustered coefficients is proposed to explore heterogeneous effects over spatial locations under nonparametric Bayesian framework. Specifically, a Markov random field constraint mixture of finite mixtures prior is designed for Bayesian log contrast regression with compostional covariates, which allows for both spatially contiguous clusters and discontinous clusters. In addition, an efficient Markov chain Monte Carlo algorithm for posterior sampling that enables simultaneous inference on both cluster configurations and cluster-wise parameters is designed. The compelling empirical performance of the proposed method is demonstrated via extensive simulation studies and an application to 51 states of United States from 2019 Bureau of Economic Analysis.","sentences":["The Gini coefficient is an universally used measurement of income inequality.","Intersectoral GDP contributions reveal the economic development of different sectors of the national economy.","Linking intersectoral GDP contributions to Gini coefficients will provide better understandings of how the Gini coefficient is influenced by different industries.","In this paper, a compositional regression with spatially clustered coefficients is proposed to explore heterogeneous effects over spatial locations under nonparametric Bayesian framework.","Specifically, a Markov random field constraint mixture of finite mixtures prior is designed for Bayesian log contrast regression with compostional covariates, which allows for both spatially contiguous clusters and discontinous clusters.","In addition, an efficient Markov chain Monte Carlo algorithm for posterior sampling that enables simultaneous inference on both cluster configurations and cluster-wise parameters is designed.","The compelling empirical performance of the proposed method is demonstrated via extensive simulation studies and an application to 51 states of United States from 2019 Bureau of Economic Analysis."],"url":"http://arxiv.org/abs/2405.07408v1","category":"stat.ME"}
{"created":"2024-05-13 00:40:28","title":"Emergent universality of free fall from quantum mechanics","abstract":"Classical and quantum mechanical descriptions of motion are fundamentally different. The universality of free fall (UFF) is a distinguishing feature of the classical motion (which has been verified with astonishing precision), while quantum theory tell us only about probabilities and uncertainties thus breaking the UFF. There are strong reasons to believe that the classical description must emerge, under plausible hypothesis, from quantum mechanics. In this Essay we show that the UFF is an emergent phenomenon: the coarse-grained quantum distribution for high energy levels leads to the classical distribution as the lowest order plus quantum corrections. We estimate the size of these corrections on the E\\\"otv\\\"os parameter and discuss the physical implications.","sentences":["Classical and quantum mechanical descriptions of motion are fundamentally different.","The universality of free fall (UFF) is a distinguishing feature of the classical motion (which has been verified with astonishing precision), while quantum theory tell us only about probabilities and uncertainties thus breaking the UFF.","There are strong reasons to believe that the classical description must emerge, under plausible hypothesis, from quantum mechanics.","In this Essay we show that the UFF is an emergent phenomenon: the coarse-grained quantum distribution for high energy levels leads to the classical distribution as the lowest order plus quantum corrections.","We estimate the size of these corrections on the E\\\"otv\\\"os parameter and discuss the physical implications."],"url":"http://arxiv.org/abs/2405.07403v1","category":"quant-ph"}
{"created":"2024-05-12 23:32:31","title":"The Spike-and-Slab Quantile LASSO for Robust Variable Selection in Cancer Genomics Studies","abstract":"Data irregularity in cancer genomics studies has been widely observed in the form of outliers and heavy-tailed distributions in the complex traits. In the past decade, robust variable selection methods have emerged as powerful alternatives to the non-robust ones to identify important genes associated with heterogeneous disease traits and build superior predictive models. In this study, to keep the remarkable features of the quantile LASSO and fully Bayesian regularized quantile regression while overcoming their disadvantage in the analysis of high-dimensional genomics data, we propose the spike-and-slab quantile LASSO through a fully Bayesian spike-and-slab formulation under the robust likelihood by adopting the asymmetric Laplace distribution (ALD). The proposed robust method has inherited the prominent properties of selective shrinkage and self-adaptivity to the sparsity pattern from the spike-and-slab LASSO (Ro\\v{c}kov\\'a and George, 2018). Furthermore, the spike-and-slab quantile LASSO has a computational advantage to locate the posterior modes via soft-thresholding rule guided Expectation-Maximization (EM) steps in the coordinate descent framework, a phenomenon rarely observed for robust regularization with non-differentiable loss functions. We have conducted comprehensive simulation studies with a variety of heavy-tailed errors in both homogeneous and heterogeneous model settings to demonstrate the superiority of the spike-and-slab quantile LASSO over its competing methods. The advantage of the proposed method has been further demonstrated in case studies of the lung adenocarcinomas (LUAD) and skin cutaneous melanoma (SKCM) data from The Cancer Genome Atlas (TCGA).","sentences":["Data irregularity in cancer genomics studies has been widely observed in the form of outliers and heavy-tailed distributions in the complex traits.","In the past decade, robust variable selection methods have emerged as powerful alternatives to the non-robust ones to identify important genes associated with heterogeneous disease traits and build superior predictive models.","In this study, to keep the remarkable features of the quantile LASSO and fully Bayesian regularized quantile regression while overcoming their disadvantage in the analysis of high-dimensional genomics data, we propose the spike-and-slab quantile LASSO through a fully Bayesian spike-and-slab formulation under the robust likelihood by adopting the asymmetric Laplace distribution (ALD).","The proposed robust method has inherited the prominent properties of selective shrinkage and self-adaptivity to the sparsity pattern from the spike-and-slab LASSO (Ro\\v{c}kov\\'a and George, 2018).","Furthermore, the spike-and-slab quantile LASSO has a computational advantage to locate the posterior modes via soft-thresholding rule guided Expectation-Maximization (EM) steps in the coordinate descent framework, a phenomenon rarely observed for robust regularization with non-differentiable loss functions.","We have conducted comprehensive simulation studies with a variety of heavy-tailed errors in both homogeneous and heterogeneous model settings to demonstrate the superiority of the spike-and-slab quantile LASSO over its competing methods.","The advantage of the proposed method has been further demonstrated in case studies of the lung adenocarcinomas (LUAD) and skin cutaneous melanoma (SKCM) data from The Cancer Genome Atlas (TCGA)."],"url":"http://arxiv.org/abs/2405.07397v1","category":"stat.ME"}
{"created":"2024-05-12 23:15:21","title":"Intrinsic Fairness-Accuracy Tradeoffs under Equalized Odds","abstract":"With the growing adoption of machine learning (ML) systems in areas like law enforcement, criminal justice, finance, hiring, and admissions, it is increasingly critical to guarantee the fairness of decisions assisted by ML. In this paper, we study the tradeoff between fairness and accuracy under the statistical notion of equalized odds. We present a new upper bound on the accuracy (that holds for any classifier), as a function of the fairness budget. In addition, our bounds also exhibit dependence on the underlying statistics of the data, labels and the sensitive group attributes. We validate our theoretical upper bounds through empirical analysis on three real-world datasets: COMPAS, Adult, and Law School. Specifically, we compare our upper bound to the tradeoffs that are achieved by various existing fair classifiers in the literature. Our results show that achieving high accuracy subject to a low-bias could be fundamentally limited based on the statistical disparity across the groups.","sentences":["With the growing adoption of machine learning (ML) systems in areas like law enforcement, criminal justice, finance, hiring, and admissions, it is increasingly critical to guarantee the fairness of decisions assisted by ML.","In this paper, we study the tradeoff between fairness and accuracy under the statistical notion of equalized odds.","We present a new upper bound on the accuracy (that holds for any classifier), as a function of the fairness budget.","In addition, our bounds also exhibit dependence on the underlying statistics of the data, labels and the sensitive group attributes.","We validate our theoretical upper bounds through empirical analysis on three real-world datasets: COMPAS, Adult, and Law School.","Specifically, we compare our upper bound to the tradeoffs that are achieved by various existing fair classifiers in the literature.","Our results show that achieving high accuracy subject to a low-bias could be fundamentally limited based on the statistical disparity across the groups."],"url":"http://arxiv.org/abs/2405.07393v1","category":"cs.LG"}
{"created":"2024-05-12 22:13:36","title":"Search for lepton-flavor-violating $\u03c4^- \\to \u03bc^-\u03bc^+\u03bc^-$ decays at Belle II","abstract":"We present the result of a search for the charged-lepton-flavor violating decay $\\tau^- \\to \\mu^-\\mu^+\\mu^-$ using a $424fb^{-1}$ sample of data recorded by the Belle II experiment at the SuperKEKB $e^{-}e^{+}$ collider. The selection of $e^{-}e^{+}\\to\\tau^+\\tau^-$ events is based on an inclusive reconstruction of the non-signal tau decay, and on a boosted decision tree to suppress background. We observe one signal candidate, which is compatible with the expectation from background processes. We set a $90\\%$ confidence level upper limit of $1.9 \\times 10^{-8}$ on the branching fraction of the \\taumu decay, which is the most stringent bound to date.","sentences":["We present the result of a search for the charged-lepton-flavor violating decay $\\tau^- \\to \\mu^-\\mu^+\\mu^-$ using a $424fb^{-1}$ sample of data recorded by the Belle II experiment at the SuperKEKB $e^{-}e^{+}$ collider.","The selection of $e^{-}e^{+}\\to\\tau^+\\tau^-$ events is based on an inclusive reconstruction of the non-signal tau decay, and on a boosted decision tree to suppress background.","We observe one signal candidate, which is compatible with the expectation from background processes.","We set a $90\\%$ confidence level upper limit of $1.9 \\times 10^{-8}$ on the branching fraction of the \\taumu decay, which is the most stringent bound to date."],"url":"http://arxiv.org/abs/2405.07386v1","category":"hep-ex"}
{"created":"2024-05-12 22:00:01","title":"Water-enhancing gels exhibiting heat-activated formation of silica aerogels for protection of critical infrastructure during catastrophic wildfire","abstract":"A promising strategy to address the pressing challenges with wildfire, particularly in the wildland-urban interface (WUI), involves developing new approaches for preventing and controlling wildfire within wildlands. Among sprayable fire-retardant materials, water-enhancing gels have emerged as exceptionally effective for protecting civil infrastructure. They possess favorable wetting and viscoelastic properties that reduce the likelihood of ignition, maintaining strong adherence to a wide array of surfaces after application. Although current water-enhancing hydrogels effectively maintain surface wetness by creating a barricade, they rapidly desiccate and lose efficacy under high heat and wind typical of wildfire conditions. To address this limitation, we developed unique biomimetic hydrogel materials from sustainable cellulosic polymers crosslinked by colloidal silica particles that exhibit ideal viscoelastic properties and facile manufacturing. Under heat activation, the hydrogel transitions into a highly porous and thermally insulative silica aerogel coating in situ, providing a robust protective layer against ignition of substrates, even when the hydrogel fire suppressant becomes completely desiccated. By confirming the mechanical properties, substrate adherence, and enhanced substrate protection against fire, these heat-activatable biomimetic hydrogels emerge as promising candidates for next-generation water-enhancing fire suppressants. These advancements have the potential to dramatically improve our ability to protect homes and critical infrastructure during wildfire.","sentences":["A promising strategy to address the pressing challenges with wildfire, particularly in the wildland-urban interface (WUI), involves developing new approaches for preventing and controlling wildfire within wildlands.","Among sprayable fire-retardant materials, water-enhancing gels have emerged as exceptionally effective for protecting civil infrastructure.","They possess favorable wetting and viscoelastic properties that reduce the likelihood of ignition, maintaining strong adherence to a wide array of surfaces after application.","Although current water-enhancing hydrogels effectively maintain surface wetness by creating a barricade, they rapidly desiccate and lose efficacy under high heat and wind typical of wildfire conditions.","To address this limitation, we developed unique biomimetic hydrogel materials from sustainable cellulosic polymers crosslinked by colloidal silica particles that exhibit ideal viscoelastic properties and facile manufacturing.","Under heat activation, the hydrogel transitions into a highly porous and thermally insulative silica aerogel coating in situ, providing a robust protective layer against ignition of substrates, even when the hydrogel fire suppressant becomes completely desiccated.","By confirming the mechanical properties, substrate adherence, and enhanced substrate protection against fire, these heat-activatable biomimetic hydrogels emerge as promising candidates for next-generation water-enhancing fire suppressants.","These advancements have the potential to dramatically improve our ability to protect homes and critical infrastructure during wildfire."],"url":"http://arxiv.org/abs/2405.07384v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-12 21:39:14","title":"Permissible four-strategy quantum extensions of classical games","abstract":"The study focuses on strategic-form games extended in the Eisert-Wilkens-Lewenstein scheme by two unitary operations. Conditions are determined under which the pair of unitary operators, along with classical strategies, form a game invariant under isomorphic transformations of the input classical game. These conditions are then applied to determine these operators, resulting in five main classes of games satisfying the isomorphism criterion, and a theorem is proved providing a practical criterion for this isomorphism. The interdependencies between different classes of extensions are identified, including limit cases in which one class transforms into another.","sentences":["The study focuses on strategic-form games extended in the Eisert-Wilkens-Lewenstein scheme by two unitary operations.","Conditions are determined under which the pair of unitary operators, along with classical strategies, form a game invariant under isomorphic transformations of the input classical game.","These conditions are then applied to determine these operators, resulting in five main classes of games satisfying the isomorphism criterion, and a theorem is proved providing a practical criterion for this isomorphism.","The interdependencies between different classes of extensions are identified, including limit cases in which one class transforms into another."],"url":"http://arxiv.org/abs/2405.07380v1","category":"quant-ph"}
{"created":"2024-05-12 21:28:59","title":"Hell is Paved with Good Intentions: The Intricate Relationship Between Cognitive Biases and Dark Patterns","abstract":"Throughout the past decade, research in HCI has identified numerous instances of dark patterns in digital interfaces. These efforts have led to a well-fostered typology describing harmful strategies users struggle to navigate. However, an in-depth understanding of the underlying mechanisms that deceive, coerce, or manipulate users is missing. We explore the interplay between cognitive biases and dark patterns to address this gap. To that end, we conducted four focus groups with experts (N=15) in psychology and dark pattern scholarship, inquiring how they conceptualise the relation between cognitive biases and dark patterns. Based on our results, we constructed the \"Relationship Model of Cognitive Biases and Dark Patterns\" which illustrates how cognitive bias and deceptive design patterns relate and identifies opportune moments for ethical reconsideration and user protection mechanisms. Our insights contribute to the current discourse by emphasising ethical design decisions and their implications in the field of HCI.","sentences":["Throughout the past decade, research in HCI has identified numerous instances of dark patterns in digital interfaces.","These efforts have led to a well-fostered typology describing harmful strategies users struggle to navigate.","However, an in-depth understanding of the underlying mechanisms that deceive, coerce, or manipulate users is missing.","We explore the interplay between cognitive biases and dark patterns to address this gap.","To that end, we conducted four focus groups with experts (N=15) in psychology and dark pattern scholarship, inquiring how they conceptualise the relation between cognitive biases and dark patterns.","Based on our results, we constructed the \"Relationship Model of Cognitive Biases and Dark Patterns\" which illustrates how cognitive bias and deceptive design patterns relate and identifies opportune moments for ethical reconsideration and user protection mechanisms.","Our insights contribute to the current discourse by emphasising ethical design decisions and their implications in the field of HCI."],"url":"http://arxiv.org/abs/2405.07378v1","category":"cs.HC"}
{"created":"2024-05-12 20:05:55","title":"Accelerating QM/MM simulations of electrochemical interfaces through machine learning of electronic charge densities","abstract":"A crucial aspect in the simulation of electrochemical interfaces consists in treating the distribution of electronic charge of electrode materials that are put in contact with an electrolyte solution. Recently, it has been shown how a machine-learning method that specifically targets the electronic charge density, also known as SALTED, can be used to predict the long-range response of metal electrodes in model electrochemical cells. In this work, we provide a full integration of SALTED with MetalWalls, a program for performing classical simulations of electrochemical systems. We do so by deriving a spherical harmonics extension of the Ewald summation method, which allows us to efficiently compute the electric field originated by the predicted electrode charge distribution. We show how to use this method to drive the molecular dynamics of an aqueous electrolyte solution under the quantum electric field of a gold electrode, which is matched to the accuracy of density-functional theory. Notably, we find that the resulting atomic forces present a small error of the order of 1 meV/{\\AA}, demonstrating the great effectiveness of adopting an electron-density path in predicting the electrostatics of the system. Upon running the data-driven dynamics over about 3 ns, we observe qualitative differences in the interfacial distribution of the electrolyte with respect to the results of a classical simulation. By greatly accelerating quantum-mechanics/molecular-mechanics approaches applied to electrochemical systems, our method opens the door to nanoseconds timescales in the accurate atomistic description of the electrical double layer.","sentences":["A crucial aspect in the simulation of electrochemical interfaces consists in treating the distribution of electronic charge of electrode materials that are put in contact with an electrolyte solution.","Recently, it has been shown how a machine-learning method that specifically targets the electronic charge density, also known as SALTED, can be used to predict the long-range response of metal electrodes in model electrochemical cells.","In this work, we provide a full integration of SALTED with MetalWalls, a program for performing classical simulations of electrochemical systems.","We do so by deriving a spherical harmonics extension of the Ewald summation method, which allows us to efficiently compute the electric field originated by the predicted electrode charge distribution.","We show how to use this method to drive the molecular dynamics of an aqueous electrolyte solution under the quantum electric field of a gold electrode, which is matched to the accuracy of density-functional theory.","Notably, we find that the resulting atomic forces present a small error of the order of 1 meV/{\\AA}, demonstrating the great effectiveness of adopting an electron-density path in predicting the electrostatics of the system.","Upon running the data-driven dynamics over about 3 ns, we observe qualitative differences in the interfacial distribution of the electrolyte with respect to the results of a classical simulation.","By greatly accelerating quantum-mechanics/molecular-mechanics approaches applied to electrochemical systems, our method opens the door to nanoseconds timescales in the accurate atomistic description of the electrical double layer."],"url":"http://arxiv.org/abs/2405.07370v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-12 19:53:16","title":"TOI-2447 b / NGTS-29 b: a 69-day Saturn around a Solar analogue","abstract":"Discovering transiting exoplanets with relatively long orbital periods ($>$10 days) is crucial to facilitate the study of cool exoplanet atmospheres ($T_{\\rm eq} < 700 K$) and to understand exoplanet formation and inward migration further out than typical transiting exoplanets. In order to discover these longer period transiting exoplanets, long-term photometric and radial velocity campaigns are required. We report the discovery of TOI-2447 b ($=$ NGTS-29b), a Saturn-mass transiting exoplanet orbiting a bright (T=10.0) Solar-type star (T$_{\\rm eff}$=5730 K). TOI-2447 b was identified as a transiting exoplanet candidate from a single transit event of 1.3% depth and 7.29 h duration in $TESS$ Sector 31 and a prior transit event from 2017 in NGTS data. Four further transit events were observed with NGTS photometry which revealed an orbital period of P=69.34 days. The transit events establish a radius for TOI-2447 b of $0.865 \\pm 0.010\\rm R_{\\rm J}$, while radial velocity measurements give a mass of $0.386 \\pm 0.025 \\rm M_{\\rm J}$. The equilibrium temperature of the planet is $414$ K, making it much cooler than the majority of $TESS$ planet discoveries. We also detect a transit signal in NGTS data not caused by TOI-2447 b, along with transit timing variations and evidence for a $\\sim$150 day signal in radial velocity measurements. It is likely that the system hosts additional planets, but further photometry and radial velocity campaigns will be needed to determine their parameters with confidence. TOI-2447 b/NGTS-29b joins a small but growing population of cool giants that will provide crucial insights into giant planet composition and formation mechanisms.","sentences":["Discovering transiting exoplanets with relatively long orbital periods ($>$10 days) is crucial to facilitate the study of cool exoplanet atmospheres ($T_{\\rm eq} < 700 K$) and to understand exoplanet formation and inward migration further out than typical transiting exoplanets.","In order to discover these longer period transiting exoplanets, long-term photometric and radial velocity campaigns are required.","We report the discovery of TOI-2447 b ($=$ NGTS-29b), a Saturn-mass transiting exoplanet orbiting a bright (T=10.0) Solar-type star (T$_{\\rm eff}$=5730 K).","TOI-2447 b was identified as a transiting exoplanet candidate from a single transit event of 1.3% depth and 7.29 h duration in $TESS$ Sector 31 and a prior transit event from 2017 in NGTS data.","Four further transit events were observed with NGTS photometry which revealed an orbital period of P=69.34 days.","The transit events establish a radius for TOI-2447 b of $0.865 \\pm 0.010\\rm R_{\\rm J}$, while radial velocity measurements give a mass of $0.386 \\pm 0.025 \\rm M_{\\rm J}$.","The equilibrium temperature of the planet is $414$ K, making it much cooler than the majority of $TESS$ planet discoveries.","We also detect a transit signal in NGTS data not caused by TOI-2447 b, along with transit timing variations and evidence for a $\\sim$150 day signal in radial velocity measurements.","It is likely that the system hosts additional planets, but further photometry and radial velocity campaigns will be needed to determine their parameters with confidence.","TOI-2447 b/NGTS-29b joins a small but growing population of cool giants that will provide crucial insights into giant planet composition and formation mechanisms."],"url":"http://arxiv.org/abs/2405.07367v1","category":"astro-ph.EP"}
{"created":"2024-05-12 19:36:11","title":"BoQ: A Place is Worth a Bag of Learnable Queries","abstract":"In visual place recognition, accurately identifying and matching images of locations under varying environmental conditions and viewpoints remains a significant challenge. In this paper, we introduce a new technique, called Bag-of-Queries (BoQ), which learns a set of global queries designed to capture universal place-specific attributes. Unlike existing methods that employ self-attention and generate the queries directly from the input features, BoQ employs distinct learnable global queries, which probe the input features via cross-attention, ensuring consistent information aggregation. In addition, our technique provides an interpretable attention mechanism and integrates with both CNN and Vision Transformer backbones. The performance of BoQ is demonstrated through extensive experiments on 14 large-scale benchmarks. It consistently outperforms current state-of-the-art techniques including NetVLAD, MixVPR and EigenPlaces. Moreover, as a global retrieval technique (one-stage), BoQ surpasses two-stage retrieval methods, such as Patch-NetVLAD, TransVPR and R2Former, all while being orders of magnitude faster and more efficient. The code and model weights are publicly available at https://github.com/amaralibey/Bag-of-Queries.","sentences":["In visual place recognition, accurately identifying and matching images of locations under varying environmental conditions and viewpoints remains a significant challenge.","In this paper, we introduce a new technique, called Bag-of-Queries (BoQ), which learns a set of global queries designed to capture universal place-specific attributes.","Unlike existing methods that employ self-attention and generate the queries directly from the input features, BoQ employs distinct learnable global queries, which probe the input features via cross-attention, ensuring consistent information aggregation.","In addition, our technique provides an interpretable attention mechanism and integrates with both CNN and Vision Transformer backbones.","The performance of BoQ is demonstrated through extensive experiments on 14 large-scale benchmarks.","It consistently outperforms current state-of-the-art techniques including NetVLAD, MixVPR and EigenPlaces.","Moreover, as a global retrieval technique (one-stage), BoQ surpasses two-stage retrieval methods, such as Patch-NetVLAD, TransVPR and R2Former, all while being orders of magnitude faster and more efficient.","The code and model weights are publicly available at https://github.com/amaralibey/Bag-of-Queries."],"url":"http://arxiv.org/abs/2405.07364v1","category":"cs.CV"}
{"created":"2024-05-12 18:45:11","title":"A Value Driven Framework for Cybersecurity Innovation in Transportation & Infrastructure","abstract":"This paper introduces a value-driven cybersecurity innovation framework for the transportation and infrastructure sectors, as opposed to the traditional market-centric approaches that have dominated the field. Recontextualizing innovation categories into sustaining, incremental, disruptive, and transformative, we aim to foster a culture of self-innovation within organizations, enabling a strategic focus on cybersecurity measures that directly contribute to business value and strategic goals. This approach enhances operational effectiveness and efficiency of cyber defences primarily, while also aligns cybersecurity initiatives with mission-critical objectives. We detail a practical method for evaluating the business value of cybersecurity innovations and present a pragmatic approach for organizations to funnel innovative ideas in a structured and repeatable manner. The framework is designed to reinforce cybersecurity capabilities against an evolving cyber threat landscape while maintaining infrastructural integrity. Shifting the focus from general market appeal to sector-specific needs, our framework provides cybersecurity leaders with the strategic cyber-foresight necessary for prioritizing impactful initiatives, thereby making cybersecurity a core business enabler rather than a burden.","sentences":["This paper introduces a value-driven cybersecurity innovation framework for the transportation and infrastructure sectors, as opposed to the traditional market-centric approaches that have dominated the field.","Recontextualizing innovation categories into sustaining, incremental, disruptive, and transformative, we aim to foster a culture of self-innovation within organizations, enabling a strategic focus on cybersecurity measures that directly contribute to business value and strategic goals.","This approach enhances operational effectiveness and efficiency of cyber defences primarily, while also aligns cybersecurity initiatives with mission-critical objectives.","We detail a practical method for evaluating the business value of cybersecurity innovations and present a pragmatic approach for organizations to funnel innovative ideas in a structured and repeatable manner.","The framework is designed to reinforce cybersecurity capabilities against an evolving cyber threat landscape while maintaining infrastructural integrity.","Shifting the focus from general market appeal to sector-specific needs, our framework provides cybersecurity leaders with the strategic cyber-foresight necessary for prioritizing impactful initiatives, thereby making cybersecurity a core business enabler rather than a burden."],"url":"http://arxiv.org/abs/2405.07358v1","category":"cs.CR"}
{"created":"2024-05-12 18:23:45","title":"Distributed Lov\u00e1sz Local Lemma under Bandwidth Limitations","abstract":"The constructive Lov\\'{a}sz Local Lemma has become a central tool for designing efficient distributed algorithms. While it has been extensively studied in the classic LOCAL model that uses unlimited bandwidth, much less is known in the bandwidth-restricted CONGEST model.   In this paper, we present bandwidth- and time-efficient algorithms for various subclasses of LLL problems, including a large class of subgraph sampling problems that are naturally formulated as LLLs. Lastly, we use our LLLs to design efficient CONGEST algorithms for coloring sparse and triangle-free graphs with few colors. These coloring algorithms are exponentially faster than previous LOCAL model algorithms.","sentences":["The constructive Lov\\'{a}sz Local Lemma has become a central tool for designing efficient distributed algorithms.","While it has been extensively studied in the classic LOCAL model that uses unlimited bandwidth, much less is known in the bandwidth-restricted CONGEST model.   ","In this paper, we present bandwidth- and time-efficient algorithms for various subclasses of LLL problems, including a large class of subgraph sampling problems that are naturally formulated as LLLs.","Lastly, we use our LLLs to design efficient CONGEST algorithms for coloring sparse and triangle-free graphs with few colors.","These coloring algorithms are exponentially faster than previous LOCAL model algorithms."],"url":"http://arxiv.org/abs/2405.07353v1","category":"cs.DS"}
{"created":"2024-05-12 17:44:01","title":"Critical probabilities for positively associated, finite-range dependent percolation models","abstract":"On a locally finite, infinite tree $T$, let $p_c(T)$ denote the critical probability for Bernoulli percolation. We prove that every positively associated, finite-range dependent percolation model on $T$ with marginals $p > p_c(T)$ must percolate. Among finite-range dependent models on trees, positive association is thus a favourable property for percolation to occur.   On general graphs of bounded degree, Liggett, Schonmann and Stacey (1997) proved that finite-range dependent percolation models with sufficiently large marginals stochastically dominate product measures. Under the additional assumption of positive association, we prove that stochastic domination actually holds for arbitrary marginals. Our result thereby generalises Proposition 3.4 in Liggett, Schonmann and Stacey (1997) which was restricted to the special case $G = \\mathbb{Z}$.   Studying the class of 1-independent percolation models has proven useful in bounding critical probabilities of various percolation models via renormalization. In many cases, the renormalized model is not only 1-independent but also positively associated. This motivates us to introduce the smallest parameter $p_a^+(G)$ such that every positively associated, 1-independent bond percolation model on a graph $G$ with marginals $p > p_a^+(G)$ percolates. We obtain quantitative upper and lower bounds on $p_a^+(\\mathbb{Z}^2)$ and on $p_a^+(\\mathbb{Z}^n)$ as $n\\to \\infty$, and also study the case of oriented bond percolation. In proving these results, we revisit several techniques originally developed for Bernoulli percolation, which become applicable thanks to a simple but seemingly new way of combining positive association with finite-range dependence.","sentences":["On a locally finite, infinite tree $T$, let $p_c(T)$ denote the critical probability for Bernoulli percolation.","We prove that every positively associated, finite-range dependent percolation model on $T$ with marginals $p > p_c(T)$ must percolate.","Among finite-range dependent models on trees, positive association is thus a favourable property for percolation to occur.   ","On general graphs of bounded degree, Liggett, Schonmann and Stacey (1997) proved that finite-range dependent percolation models with sufficiently large marginals stochastically dominate product measures.","Under the additional assumption of positive association, we prove that stochastic domination actually holds for arbitrary marginals.","Our result thereby generalises Proposition 3.4 in Liggett, Schonmann and Stacey (1997) which was restricted to the special case $G = \\mathbb{Z}$.   Studying the class of 1-independent percolation models has proven useful in bounding critical probabilities of various percolation models via renormalization.","In many cases, the renormalized model is not only 1-independent but also positively associated.","This motivates us to introduce the smallest parameter $p_a^+(G)$ such that every positively associated, 1-independent bond percolation model on a graph $G$ with marginals $p > p_a^+(G)$ percolates.","We obtain quantitative upper and lower bounds on $p_a^+(\\mathbb{Z}^2)$ and on $p_a^+(\\mathbb{Z}^n)$ as $n\\to \\infty$, and also study the case of oriented bond percolation.","In proving these results, we revisit several techniques originally developed for Bernoulli percolation, which become applicable thanks to a simple but seemingly new way of combining positive association with finite-range dependence."],"url":"http://arxiv.org/abs/2405.07345v1","category":"math.PR"}
{"created":"2024-05-12 17:40:27","title":"Graph neural networks for power grid operational risk assessment under evolving grid topology","abstract":"This article investigates the ability of graph neural networks (GNNs) to identify risky conditions in a power grid over the subsequent few hours, without explicit, high-resolution information regarding future generator on/off status (grid topology) or power dispatch decisions. The GNNs are trained using supervised learning, to predict the power grid's aggregated bus-level (either zonal or system-level) or individual branch-level state under different power supply and demand conditions. The variability of the stochastic grid variables (wind/solar generation and load demand), and their statistical correlations, are rigorously considered while generating the inputs for the training data. The outputs in the training data, obtained by solving numerous mixed-integer linear programming (MILP) optimal power flow problems, correspond to system-level, zonal and transmission line-level quantities of interest (QoIs). The QoIs predicted by the GNNs are used to conduct hours-ahead, sampling-based reliability and risk assessment w.r.t. zonal and system-level (load shedding) as well as branch-level (overloading) failure events. The proposed methodology is demonstrated for three synthetic grids with sizes ranging from 118 to 2848 buses. Our results demonstrate that GNNs are capable of providing fast and accurate prediction of QoIs and can be good proxies for computationally expensive MILP algorithms. The excellent accuracy of GNN-based reliability and risk assessment suggests that GNN models can substantially improve situational awareness by quickly providing rigorous reliability and risk estimates.","sentences":["This article investigates the ability of graph neural networks (GNNs) to identify risky conditions in a power grid over the subsequent few hours, without explicit, high-resolution information regarding future generator on/off status (grid topology) or power dispatch decisions.","The GNNs are trained using supervised learning, to predict the power grid's aggregated bus-level (either zonal or system-level) or individual branch-level state under different power supply and demand conditions.","The variability of the stochastic grid variables (wind/solar generation and load demand), and their statistical correlations, are rigorously considered while generating the inputs for the training data.","The outputs in the training data, obtained by solving numerous mixed-integer linear programming (MILP) optimal power flow problems, correspond to system-level, zonal and transmission line-level quantities of interest (QoIs).","The QoIs predicted by the GNNs are used to conduct hours-ahead, sampling-based reliability and risk assessment w.r.t. zonal and system-level (load shedding) as well as branch-level (overloading) failure events.","The proposed methodology is demonstrated for three synthetic grids with sizes ranging from 118 to 2848 buses.","Our results demonstrate that GNNs are capable of providing fast and accurate prediction of QoIs and can be good proxies for computationally expensive MILP algorithms.","The excellent accuracy of GNN-based reliability and risk assessment suggests that GNN models can substantially improve situational awareness by quickly providing rigorous reliability and risk estimates."],"url":"http://arxiv.org/abs/2405.07343v1","category":"eess.SY"}
{"created":"2024-05-12 17:36:40","title":"On the equivalence between n-state spin and vertex models on the square lattice","abstract":"In this paper we investigate a correspondence among spin and vertex models with the same number of local states on the square lattice with toroidal boundary conditions. We argue that the partition functions of an arbitrary $n$-state spin model and of a certain specific $n$-state vertex model coincide for finite lattice sizes. The equivalent vertex model has $n^3$ non-null Boltzmann weights and their relationship with the edge weights of the spin model is explicitly presented. In particular, the Ising model in a magnetic field is mapped to an eight-vertex model whose weights configurations combine both even and odd number of incoming and outcoming arrows at a vertex. We have studied the Yang-Baxter algebra for such mixed eight-vertex model when the weights are invariant under arrows reversing. We find that while the Lax operator lie on the same elliptic curve of the even eight-vertex model the respective $\\mathrm{R}$-matrix can not be presented in terms of the difference of two rapidities. We also argue that the spin-vertex equivalence may be used to imbed an integrable spin model in the realm of the quantum inverse scattering framework. As an example, we show how to determine the $\\mathrm{R}$-matrix of the 27-vertex model equivalent to a three-state spin model devised by Fateev and Zamolodchikov.","sentences":["In this paper we investigate a correspondence among spin and vertex models with the same number of local states on the square lattice with toroidal boundary conditions.","We argue that the partition functions of an arbitrary $n$-state spin model and of a certain specific $n$-state vertex model coincide for finite lattice sizes.","The equivalent vertex model has $n^3$ non-null Boltzmann weights and their relationship with the edge weights of the spin model is explicitly presented.","In particular, the Ising model in a magnetic field is mapped to an eight-vertex model whose weights configurations combine both even and odd number of incoming and outcoming arrows at a vertex.","We have studied the Yang-Baxter algebra for such mixed eight-vertex model when the weights are invariant under arrows reversing.","We find that while the Lax operator lie on the same elliptic curve of the even eight-vertex model the respective $\\mathrm{R}$-matrix can not be presented in terms of the difference of two rapidities.","We also argue that the spin-vertex equivalence may be used to imbed an integrable spin model in the realm of the quantum inverse scattering framework.","As an example, we show how to determine the $\\mathrm{R}$-matrix of the 27-vertex model equivalent to a three-state spin model devised by Fateev and Zamolodchikov."],"url":"http://arxiv.org/abs/2405.07341v1","category":"math-ph"}
{"created":"2024-05-12 17:21:57","title":"Explainable Convolutional Neural Networks for Retinal Fundus Classification and Cutting-Edge Segmentation Models for Retinal Blood Vessels from Fundus Images","abstract":"Our research focuses on the critical field of early diagnosis of disease by examining retinal blood vessels in fundus images. While automatic segmentation of retinal blood vessels holds promise for early detection, accurate analysis remains challenging due to the limitations of existing methods, which often lack discrimination power and are susceptible to influences from pathological regions. Our research in fundus image analysis advances deep learning-based classification using eight pre-trained CNN models. To enhance interpretability, we utilize Explainable AI techniques such as Grad-CAM, Grad-CAM++, Score-CAM, Faster Score-CAM, and Layer CAM. These techniques illuminate the decision-making processes of the models, fostering transparency and trust in their predictions. Expanding our exploration, we investigate ten models, including TransUNet with ResNet backbones, Attention U-Net with DenseNet and ResNet backbones, and Swin-UNET. Incorporating diverse architectures such as ResNet50V2, ResNet101V2, ResNet152V2, and DenseNet121 among others, this comprehensive study deepens our insights into attention mechanisms for enhanced fundus image analysis. Among the evaluated models for fundus image classification, ResNet101 emerged with the highest accuracy, achieving an impressive 94.17%. On the other end of the spectrum, EfficientNetB0 exhibited the lowest accuracy among the models, achieving a score of 88.33%. Furthermore, in the domain of fundus image segmentation, Swin-Unet demonstrated a Mean Pixel Accuracy of 86.19%, showcasing its effectiveness in accurately delineating regions of interest within fundus images. Conversely, Attention U-Net with DenseNet201 backbone exhibited the lowest Mean Pixel Accuracy among the evaluated models, achieving a score of 75.87%.","sentences":["Our research focuses on the critical field of early diagnosis of disease by examining retinal blood vessels in fundus images.","While automatic segmentation of retinal blood vessels holds promise for early detection, accurate analysis remains challenging due to the limitations of existing methods, which often lack discrimination power and are susceptible to influences from pathological regions.","Our research in fundus image analysis advances deep learning-based classification using eight pre-trained CNN models.","To enhance interpretability, we utilize Explainable AI techniques such as Grad-CAM, Grad-CAM++, Score-CAM, Faster Score-CAM, and Layer CAM.","These techniques illuminate the decision-making processes of the models, fostering transparency and trust in their predictions.","Expanding our exploration, we investigate ten models, including TransUNet with ResNet backbones, Attention U-Net with DenseNet and ResNet backbones, and Swin-UNET.","Incorporating diverse architectures such as ResNet50V2, ResNet101V2, ResNet152V2, and DenseNet121 among others, this comprehensive study deepens our insights into attention mechanisms for enhanced fundus image analysis.","Among the evaluated models for fundus image classification, ResNet101 emerged with the highest accuracy, achieving an impressive 94.17%.","On the other end of the spectrum, EfficientNetB0 exhibited the lowest accuracy among the models, achieving a score of 88.33%.","Furthermore, in the domain of fundus image segmentation, Swin-Unet demonstrated a Mean Pixel Accuracy of 86.19%, showcasing its effectiveness in accurately delineating regions of interest within fundus images.","Conversely, Attention U-Net with DenseNet201 backbone exhibited the lowest Mean Pixel Accuracy among the evaluated models, achieving a score of 75.87%."],"url":"http://arxiv.org/abs/2405.07338v1","category":"eess.IV"}
{"created":"2024-05-12 17:11:50","title":"Data Trading Combination Auction Mechanism based on the Exponential Mechanism","abstract":"With the widespread application of machine learning technology in recent years, the demand for training data has increased significantly, leading to the emergence of research areas such as data trading. The work in this field is still in the developmental stage. Different buyers have varying degrees of demand for various types of data, and auctions play a role in such scenarios due to their authenticity and fairness. Recent related work has proposed combination auction mechanisms for different domains. However, such mechanisms have not addressed the privacy concerns of buyers. In this paper, we design a \\textit{Data Trading Combination Auction Mechanism based on the exponential mechanism} (DCAE) to protect buyers' bidding privacy from being leaked. We apply the exponential mechanism to select the final settlement price for the auction and generate a probability distribution based on the relationship between the price and the revenue. In the experimental aspect, we consider the selection of different mechanisms under two scenarios, and the experimental results show that this method can ensure high auction revenue and protect buyers' privacy from being violated.","sentences":["With the widespread application of machine learning technology in recent years, the demand for training data has increased significantly, leading to the emergence of research areas such as data trading.","The work in this field is still in the developmental stage.","Different buyers have varying degrees of demand for various types of data, and auctions play a role in such scenarios due to their authenticity and fairness.","Recent related work has proposed combination auction mechanisms for different domains.","However, such mechanisms have not addressed the privacy concerns of buyers.","In this paper, we design a \\textit{Data Trading Combination Auction Mechanism based on the exponential mechanism} (DCAE) to protect buyers' bidding privacy from being leaked.","We apply the exponential mechanism to select the final settlement price for the auction and generate a probability distribution based on the relationship between the price and the revenue.","In the experimental aspect, we consider the selection of different mechanisms under two scenarios, and the experimental results show that this method can ensure high auction revenue and protect buyers' privacy from being violated."],"url":"http://arxiv.org/abs/2405.07336v1","category":"cs.GT"}
{"created":"2024-05-12 17:07:16","title":"Tremor Reduction for Accessible Ray Based Interaction in VR Applications","abstract":"Comparative to conventional 2D interaction methods, virtual reality (VR) demonstrates an opportunity for unique interface and interaction design decisions. Currently, this poses a challenge when developing an accessible VR experience as existing interaction techniques may not be usable by all users. It was discovered that many traditional 2D interface interaction methods have been directly converted to work in a VR space with little alteration to the input mechanism, such as the use of a laser pointer designed to that of a traditional cursor. It is recognized that distanceindependent millimetres can support designers in developing interfaces that scale in virtual worlds. Relevantly, Fitts law states that as distance increases, user movements are increasingly slower and performed less accurately. In this paper we propose the use of a low pass filter, to normalize user input noise, alleviating fine motor requirements during ray-based interaction. A development study was conducted to understand the feasibility of implementing such a filter and explore its effects on end users experience. It demonstrates how an algorithm can provide an opportunity for a more accurate and consequently less frustrating experience by filtering and reducing involuntary hand tremors. Further discussion on existing VR design philosophies is also conducted, analysing evidence that supports multisensory feedback and psychological models. The completed study can be downloaded from GitHub.","sentences":["Comparative to conventional 2D interaction methods, virtual reality (VR) demonstrates an opportunity for unique interface and interaction design decisions.","Currently, this poses a challenge when developing an accessible VR experience as existing interaction techniques may not be usable by all users.","It was discovered that many traditional 2D interface interaction methods have been directly converted to work in a VR space with little alteration to the input mechanism, such as the use of a laser pointer designed to that of a traditional cursor.","It is recognized that distanceindependent millimetres can support designers in developing interfaces that scale in virtual worlds.","Relevantly, Fitts law states that as distance increases, user movements are increasingly slower and performed less accurately.","In this paper we propose the use of a low pass filter, to normalize user input noise, alleviating fine motor requirements during ray-based interaction.","A development study was conducted to understand the feasibility of implementing such a filter and explore its effects on end users experience.","It demonstrates how an algorithm can provide an opportunity for a more accurate and consequently less frustrating experience by filtering and reducing involuntary hand tremors.","Further discussion on existing VR design philosophies is also conducted, analysing evidence that supports multisensory feedback and psychological models.","The completed study can be downloaded from GitHub."],"url":"http://arxiv.org/abs/2405.07335v1","category":"cs.HC"}
{"created":"2024-05-12 17:02:48","title":"Quantum Mini-Apps: A Framework for Developing and Benchmarking Quantum-HPC Applications","abstract":"With the increasing maturity and scale of quantum hardware and its integration into HPC systems, there is a need to develop robust techniques for developing, characterizing, and benchmarking quantum-HPC applications and middleware systems. This requires a better understanding of interaction, coupling, and common execution patterns between quantum and classical workload tasks and components. This paper identifies six quantum-HPC execution motifs - recurring execution patterns characterized by distinct coupling and interaction modes. These motifs provide the basis for a suite of quantum mini-apps - simplified application prototypes that encapsulate essential characteristics of production systems. To support these developments, we introduce a mini-app framework that offers the necessary abstractions for creating and executing mini-apps across heterogeneous quantum-HPC infrastructure, making it a valuable tool for performance characterizations and middleware development.","sentences":["With the increasing maturity and scale of quantum hardware and its integration into HPC systems, there is a need to develop robust techniques for developing, characterizing, and benchmarking quantum-HPC applications and middleware systems.","This requires a better understanding of interaction, coupling, and common execution patterns between quantum and classical workload tasks and components.","This paper identifies six quantum-HPC execution motifs - recurring execution patterns characterized by distinct coupling and interaction modes.","These motifs provide the basis for a suite of quantum mini-apps - simplified application prototypes that encapsulate essential characteristics of production systems.","To support these developments, we introduce a mini-app framework that offers the necessary abstractions for creating and executing mini-apps across heterogeneous quantum-HPC infrastructure, making it a valuable tool for performance characterizations and middleware development."],"url":"http://arxiv.org/abs/2405.07333v1","category":"quant-ph"}
{"created":"2024-05-12 16:23:52","title":"Power Evaluation of IOT Application Layer Protocols","abstract":"The Internet of Things has affected all aspects of daily life, and the number of IoT devices is increasing day by day. According to forecasts, the number of Internet of Things devices will reach one trillion devices by 2035. The increase in the number of devices connected to the Internet will cause various concerns. One of the most important concerns is the energy and power consumption of these devices. Although Internet of Things modules are low in energy consumption, their widespread and large-scale use has made the issue of power consumption become the most important challenge in this field. For this reason, it is necessary to use communication protocols that, in addition to establishing efficient communication, impose minimal power consumption on the network. In this paper, application layer protocols such as MQTT, MQTT-SN, CoAP, and HTTP are simulated using the tools available in the Contiki operating system, including COOJA and Powertrace, and they { are evaluated} and compared with each other in terms of power consumption. According to the simulations performed by the mentioned tools, the MQTT-SN protocol was the least consuming protocol in terms of power consumption. After that, the CoAP protocol is placed, and with a slight difference, the MQTT protocol, which consumes more than MQTT-SN. Finally, the HTTP protocol consumes the most power, which makes it unsuitable for communication in the Internet of Things","sentences":["The Internet of Things has affected all aspects of daily life, and the number of IoT devices is increasing day by day.","According to forecasts, the number of Internet of Things devices will reach one trillion devices by 2035.","The increase in the number of devices connected to the Internet will cause various concerns.","One of the most important concerns is the energy and power consumption of these devices.","Although Internet of Things modules are low in energy consumption, their widespread and large-scale use has made the issue of power consumption become the most important challenge in this field.","For this reason, it is necessary to use communication protocols that, in addition to establishing efficient communication, impose minimal power consumption on the network.","In this paper, application layer protocols such as MQTT, MQTT-SN, CoAP, and HTTP are simulated using the tools available in the Contiki operating system, including COOJA and Powertrace, and they { are evaluated} and compared with each other in terms of power consumption.","According to the simulations performed by the mentioned tools, the MQTT-SN protocol was the least consuming protocol in terms of power consumption.","After that, the CoAP protocol is placed, and with a slight difference, the MQTT protocol, which consumes more than MQTT-SN.","Finally, the HTTP protocol consumes the most power, which makes it unsuitable for communication in the Internet of Things"],"url":"http://arxiv.org/abs/2405.07326v1","category":"cs.NI"}
{"created":"2024-05-12 16:17:41","title":"Computational analysis of US Congressional speeches reveals a shift from evidence to intuition","abstract":"Pursuit of honest and truthful decision-making is crucial for governance and accountability in democracies. However, people sometimes take different perspectives of what it means to be honest and how to pursue truthfulness. Here we explore a continuum of perspectives from evidence-based reasoning, rooted in ascertainable facts and data, at one end, to intuitive decisions that are driven by feelings and subjective interpretations, at the other. We analyze the linguistic traces of those contrasting perspectives in Congressional speeches from 1879 to 2022. We find that evidence-based language has continued to decline since the mid-1970s, together with a decline in legislative productivity. The decline was accompanied by increasing partisan polarization in Congress and rising income inequality in society. Results highlight the importance of evidence-based language in political decision-making.","sentences":["Pursuit of honest and truthful decision-making is crucial for governance and accountability in democracies.","However, people sometimes take different perspectives of what it means to be honest and how to pursue truthfulness.","Here we explore a continuum of perspectives from evidence-based reasoning, rooted in ascertainable facts and data, at one end, to intuitive decisions that are driven by feelings and subjective interpretations, at the other.","We analyze the linguistic traces of those contrasting perspectives in Congressional speeches from 1879 to 2022.","We find that evidence-based language has continued to decline since the mid-1970s, together with a decline in legislative productivity.","The decline was accompanied by increasing partisan polarization in Congress and rising income inequality in society.","Results highlight the importance of evidence-based language in political decision-making."],"url":"http://arxiv.org/abs/2405.07323v1","category":"econ.GN"}
{"created":"2024-05-12 16:16:25","title":"Status and Prospects of the JUNO Experiment","abstract":"The Jiangmen Underground Neutrino Observatory (JUNO) is a multi-purpose neutrino experiment currently under construction in China. It is located 52.5km away from two nuclear power plants in a newly constructed 700-m-deep underground laboratory. JUNO will be the largest liquid scintillator (LS) detector in the world comprising 20 kt of ultrapure LS filled in an acrylic sphere. Its main goal is to determine the neutrino mass ordering by measuring the energy spectrum of reactor neutrinos with highest accuracy. In addition, JUNO will cover precision measurements of oscillation parameters and several aspects in the field of astroparticle physics. Data taking will start in late 2024.","sentences":["The Jiangmen Underground Neutrino Observatory (JUNO) is a multi-purpose neutrino experiment currently under construction in China.","It is located 52.5km away from two nuclear power plants in a newly constructed 700-m-deep underground laboratory.","JUNO will be the largest liquid scintillator (LS) detector in the world comprising 20 kt of ultrapure LS filled in an acrylic sphere.","Its main goal is to determine the neutrino mass ordering by measuring the energy spectrum of reactor neutrinos with highest accuracy.","In addition, JUNO will cover precision measurements of oscillation parameters and several aspects in the field of astroparticle physics.","Data taking will start in late 2024."],"url":"http://arxiv.org/abs/2405.07321v1","category":"physics.ins-det"}
{"created":"2024-05-12 15:55:43","title":"VALID: a Validated Algorithm for Learning in Decentralized Networks with Possible Adversarial Presence","abstract":"We introduce the paradigm of validated decentralized learning for undirected networks with heterogeneous data and possible adversarial infiltration. We require (a) convergence to a global empirical loss minimizer when adversaries are absent, and (b) either detection of adversarial presence of convergence to an admissible consensus irrespective of the adversarial configuration. To this end, we propose the VALID protocol which, to the best of our knowledge, is the first to achieve a validated learning guarantee. Moreover, VALID offers an O(1/T) convergence rate (under pertinent regularity assumptions), and computational and communication complexities comparable to non-adversarial distributed stochastic gradient descent. Remarkably, VALID retains optimal performance metrics in adversary-free environments, sidestepping the robustness penalties observed in prior byzantine-robust methods. A distinctive aspect of our study is a heterogeneity metric based on the norms of individual agents' gradients computed at the global empirical loss minimizer. This not only provides a natural statistic for detecting significant byzantine disruptions but also allows us to prove the optimality of VALID in wide generality. Lastly, our numerical results reveal that, in the absence of adversaries, VALID converges faster than state-of-the-art byzantine robust algorithms, while when adversaries are present, VALID terminates with each honest either converging to an admissible consensus of declaring adversarial presence in the network.","sentences":["We introduce the paradigm of validated decentralized learning for undirected networks with heterogeneous data and possible adversarial infiltration.","We require (a) convergence to a global empirical loss minimizer when adversaries are absent, and (b) either detection of adversarial presence of convergence to an admissible consensus irrespective of the adversarial configuration.","To this end, we propose the VALID protocol which, to the best of our knowledge, is the first to achieve a validated learning guarantee.","Moreover, VALID offers an O(1/T) convergence rate (under pertinent regularity assumptions), and computational and communication complexities comparable to non-adversarial distributed stochastic gradient descent.","Remarkably, VALID retains optimal performance metrics in adversary-free environments, sidestepping the robustness penalties observed in prior byzantine-robust methods.","A distinctive aspect of our study is a heterogeneity metric based on the norms of individual agents' gradients computed at the global empirical loss minimizer.","This not only provides a natural statistic for detecting significant byzantine disruptions but also allows us to prove the optimality of VALID in wide generality.","Lastly, our numerical results reveal that, in the absence of adversaries, VALID converges faster than state-of-the-art byzantine robust algorithms, while when adversaries are present, VALID terminates with each honest either converging to an admissible consensus of declaring adversarial presence in the network."],"url":"http://arxiv.org/abs/2405.07316v1","category":"cs.LG"}
{"created":"2024-05-12 15:44:47","title":"Machine Learning-Based Protection and Fault Identification of 100% Inverter-Based Microgrids","abstract":"100% inverter-based renewable units are becoming more prevalent, introducing new challenges in the protection of microgrids that incorporate these resources. This is particularly due to low fault currents and bidirectional flows. Previous work has studied the protection of microgrids with high penetration of inverter-interfaced distributed generators; however, very few have studied the protection of a 100% inverter-based microgrid. This work proposes machine learning (ML)-based protection solutions using local electrical measurements that consider implementation challenges and effectively combine short-circuit fault detection and type identification. A decision tree method is used to analyze a wide range of fault scenarios. PSCAD/EMTDC simulation environment is used to create a dataset for training and testing the proposed method. The effectiveness of the proposed methods is examined under seven distinct fault types, each featuring varying fault resistance, in a 100% inverter-based microgrid consisting of four inverters.","sentences":["100% inverter-based renewable units are becoming more prevalent, introducing new challenges in the protection of microgrids that incorporate these resources.","This is particularly due to low fault currents and bidirectional flows.","Previous work has studied the protection of microgrids with high penetration of inverter-interfaced distributed generators; however, very few have studied the protection of a 100% inverter-based microgrid.","This work proposes machine learning (ML)-based protection solutions using local electrical measurements that consider implementation challenges and effectively combine short-circuit fault detection and type identification.","A decision tree method is used to analyze a wide range of fault scenarios.","PSCAD/EMTDC simulation environment is used to create a dataset for training and testing the proposed method.","The effectiveness of the proposed methods is examined under seven distinct fault types, each featuring varying fault resistance, in a 100% inverter-based microgrid consisting of four inverters."],"url":"http://arxiv.org/abs/2405.07310v1","category":"eess.SY"}
{"created":"2024-05-12 15:36:49","title":"China's plug-in hybrid electric vehicles transition: an operational carbon perspective","abstract":"Assessing the energy and emissions of representative plug-in hybrid electric vehicle (PHEV) model operations is crucial for accelerating carbon neutrality transitions in China's passenger car sector. This study makes the first attempt to create a bottom-up model to measure the real-world energy use and carbon dioxide (CO2) emissions of China's top twenty selling PHEV model operations across different geographical regions during 2020-2022. The results indicate that (1) the actual electricity intensity for the best-selling PEHV models (20.2-38.2 kilowatt-hour [kWh]/100 kilometers [km]) was 30-40% higher than the New European Driving Cycle (NEDC) values, and the actual gasoline intensity (4.7 to 23.5 liters [L]/100 km) was 3-6 times greater than the NEDC values. (2) The overall energy consumption of the best-selling models exhibited variations among various geographical regions, and the total gasoline equivalent was twice as high in southern China (1283 mega-liters, 2020-2022) than in northern China and the Yangtze River Middle Reach. (3) The top-selling models emitted 4.9 mega-tons (Mt) of CO2 nationwide from 2020-2022, 1.9 Mt from electricity and 3 Mt from gasoline. In northern China, carbon emissions per vehicle were more than 1.2 times greater than those in other regions. Furthermore, targeted policy implications for expediting the carbon-neutral transition within the passenger vehicles are proposed. Overall, this study reviews and compares national and regional benchmark data and performance data for PHEV operations. Its objective is to bolster national decarbonization initiatives, ensuring low emissions and expediting the transportation sector's transition toward a net-zero era.","sentences":["Assessing the energy and emissions of representative plug-in hybrid electric vehicle (PHEV) model operations is crucial for accelerating carbon neutrality transitions in China's passenger car sector.","This study makes the first attempt to create a bottom-up model to measure the real-world energy use and carbon dioxide (CO2) emissions of China's top twenty selling PHEV model operations across different geographical regions during 2020-2022.","The results indicate that (1) the actual electricity intensity for the best-selling PEHV models (","20.2-38.2 kilowatt-hour","[kWh]/100 kilometers [km]) was 30-40% higher than the New European Driving Cycle (NEDC) values, and the actual gasoline intensity (4.7 to 23.5 liters [L]/100 km) was 3-6 times greater than the NEDC values.","(2) The overall energy consumption of the best-selling models exhibited variations among various geographical regions, and the total gasoline equivalent was twice as high in southern China (1283 mega-liters, 2020-2022) than in northern China and the Yangtze River Middle Reach.","(3) The top-selling models emitted 4.9 mega-tons (Mt) of CO2 nationwide from 2020-2022, 1.9 Mt from electricity and 3 Mt from gasoline.","In northern China, carbon emissions per vehicle were more than 1.2 times greater than those in other regions.","Furthermore, targeted policy implications for expediting the carbon-neutral transition within the passenger vehicles are proposed.","Overall, this study reviews and compares national and regional benchmark data and performance data for PHEV operations.","Its objective is to bolster national decarbonization initiatives, ensuring low emissions and expediting the transportation sector's transition toward a net-zero era."],"url":"http://arxiv.org/abs/2405.07308v1","category":"econ.GN"}
{"created":"2024-05-12 15:18:54","title":"Finding a Way Through the Social Media Labyrinth: Guiding Design Through User Expectations","abstract":"Social networking services (SNS) have become integral to modern life to create and maintain meaningful relationships. Nevertheless, their historic growth of features has led to labyrinthine user interfaces (UIs) that often result in frustration among users - for instance, when trying to control privacy-related settings. This paper aims to mitigate labyrinthine UIs by studying users' expectations (N=21) through an online card sorting exercise based on 58 common SNS UI features, teaching us about their expectations regarding the importance of specific UI features and the frequency with which they use them. Our findings offer a valuable understanding of the relationship between the importance and frequency of UI features and provide design considerations for six identified UI feature groups. Through these findings, we inform the design and development of user-centred alternatives to current SNS interfaces that enable users to successfully navigate SNS and feel in control over their data by meeting their expectations.","sentences":["Social networking services (SNS) have become integral to modern life to create and maintain meaningful relationships.","Nevertheless, their historic growth of features has led to labyrinthine user interfaces (UIs) that often result in frustration among users - for instance, when trying to control privacy-related settings.","This paper aims to mitigate labyrinthine UIs by studying users' expectations (N=21) through an online card sorting exercise based on 58 common SNS UI features, teaching us about their expectations regarding the importance of specific UI features and the frequency with which they use them.","Our findings offer a valuable understanding of the relationship between the importance and frequency of UI features and provide design considerations for six identified UI feature groups.","Through these findings, we inform the design and development of user-centred alternatives to current SNS interfaces that enable users to successfully navigate SNS and feel in control over their data by meeting their expectations."],"url":"http://arxiv.org/abs/2405.07305v1","category":"cs.HC"}
{"created":"2024-05-12 15:07:54","title":"Bayesian Inference for Small-Angle Scattering Data in Core-Shell Samples","abstract":"Small-angle scattering (SAS) techniques, which utilize neutrons and X-rays, are employed in various scientific fields, including materials science, biochemistry, and polymer physics. During the analysis of SAS data, model parameters that contain information about the sample are estimated by fitting the observational data to a model of sample. Previous research has demonstrated the effectiveness of Bayesian inference in analyzing SAS data using a sphere model. However, compared with the sphere model, the core-shell model, which represents functional nanoparticles, offers higher application potential and greater analytical value. Therefore, in this study, we propose an analytical method for the more complex and practical core-shell model based on Bayesian inference. Through numerical experiments, we evaluated the performance of this method under different conditions, including measurement times, number of data points, and differences in scattering length density. As a result, we clarify the conditions under which accurate estimations are possible.","sentences":["Small-angle scattering (SAS) techniques, which utilize neutrons and X-rays, are employed in various scientific fields, including materials science, biochemistry, and polymer physics.","During the analysis of SAS data, model parameters that contain information about the sample are estimated by fitting the observational data to a model of sample.","Previous research has demonstrated the effectiveness of Bayesian inference in analyzing SAS data using a sphere model.","However, compared with the sphere model, the core-shell model, which represents functional nanoparticles, offers higher application potential and greater analytical value.","Therefore, in this study, we propose an analytical method for the more complex and practical core-shell model based on Bayesian inference.","Through numerical experiments, we evaluated the performance of this method under different conditions, including measurement times, number of data points, and differences in scattering length density.","As a result, we clarify the conditions under which accurate estimations are possible."],"url":"http://arxiv.org/abs/2405.07302v1","category":"physics.app-ph"}
{"created":"2024-05-12 14:01:05","title":"Erasing Concepts from Text-to-Image Diffusion Models with Few-shot Unlearning","abstract":"Generating images from text has become easier because of the scaling of diffusion models and advancements in the field of vision and language. These models are trained using vast amounts of data from the Internet. Hence, they often contain undesirable content such as copyrighted material. As it is challenging to remove such data and retrain the models, methods for erasing specific concepts from pre-trained models have been investigated. We propose a novel concept-erasure method that updates the text encoder using few-shot unlearning in which a few real images are used. The discussion regarding the generated images after erasing a concept has been lacking. While there are methods for specifying the transition destination for concepts, the validity of the specified concepts is unclear. Our method implicitly achieves this by transitioning to the latent concepts inherent in the model or the images. Our method can erase a concept within 10 s, making concept erasure more accessible than ever before. Implicitly transitioning to related concepts leads to more natural concept erasure. We applied the proposed method to various concepts and confirmed that concept erasure can be achieved tens to hundreds of times faster than with current methods. By varying the parameters to be updated, we obtained results suggesting that, like previous research, knowledge is primarily accumulated in the feed-forward networks of the text encoder.","sentences":["Generating images from text has become easier because of the scaling of diffusion models and advancements in the field of vision and language.","These models are trained using vast amounts of data from the Internet.","Hence, they often contain undesirable content such as copyrighted material.","As it is challenging to remove such data and retrain the models, methods for erasing specific concepts from pre-trained models have been investigated.","We propose a novel concept-erasure method that updates the text encoder using few-shot unlearning in which a few real images are used.","The discussion regarding the generated images after erasing a concept has been lacking.","While there are methods for specifying the transition destination for concepts, the validity of the specified concepts is unclear.","Our method implicitly achieves this by transitioning to the latent concepts inherent in the model or the images.","Our method can erase a concept within 10 s, making concept erasure more accessible than ever before.","Implicitly transitioning to related concepts leads to more natural concept erasure.","We applied the proposed method to various concepts and confirmed that concept erasure can be achieved tens to hundreds of times faster than with current methods.","By varying the parameters to be updated, we obtained results suggesting that, like previous research, knowledge is primarily accumulated in the feed-forward networks of the text encoder."],"url":"http://arxiv.org/abs/2405.07288v1","category":"cs.CV"}
{"created":"2024-05-12 13:55:40","title":"A Short Note on a Flexible Cholesky Parameterization of Correlation Matrices","abstract":"We propose a Cholesky factor parameterization of correlation matrices that facilitates a priori restrictions on the correlation matrix. It is a smooth and differentiable transform that allows additional boundary constraints on the correlation values. Our particular motivation is random sampling under positivity constraints on the space of correlation matrices using MCMC methods.","sentences":["We propose a Cholesky factor parameterization of correlation matrices that facilitates a priori restrictions on the correlation matrix.","It is a smooth and differentiable transform that allows additional boundary constraints on the correlation values.","Our particular motivation is random sampling under positivity constraints on the space of correlation matrices using MCMC methods."],"url":"http://arxiv.org/abs/2405.07286v1","category":"stat.CO"}
{"created":"2024-05-12 13:36:07","title":"Branching Narratives: Character Decision Points Detection","abstract":"This paper presents the Character Decision Points Detection (CHADPOD) task, a task of identification of points within narratives where characters make decisions that may significantly influence the story's direction. We propose a novel dataset based on CYOA-like games graphs to be used as a benchmark for such a task. We provide a comparative analysis of different models' performance on this task, including a couple of LLMs and several MLMs as baselines, achieving up to 89% accuracy. This underscores the complexity of narrative analysis, showing the challenges associated with understanding character-driven story dynamics. Additionally, we show how such a model can be applied to the existing text to produce linear segments divided by potential branching points, demonstrating the practical application of our findings in narrative analysis.","sentences":["This paper presents the Character Decision Points Detection (CHADPOD) task, a task of identification of points within narratives where characters make decisions that may significantly influence the story's direction.","We propose a novel dataset based on CYOA-like games graphs to be used as a benchmark for such a task.","We provide a comparative analysis of different models' performance on this task, including a couple of LLMs and several MLMs as baselines, achieving up to 89% accuracy.","This underscores the complexity of narrative analysis, showing the challenges associated with understanding character-driven story dynamics.","Additionally, we show how such a model can be applied to the existing text to produce linear segments divided by potential branching points, demonstrating the practical application of our findings in narrative analysis."],"url":"http://arxiv.org/abs/2405.07282v1","category":"cs.CL"}
{"created":"2024-05-12 12:57:12","title":"Statistical Study of Planetesimal-Driven Migration","abstract":"Recent exoplanet observations have revealed a diversity of exoplanetary systems, which suggests the ubiquity of radial planetary migration. One powerful known mechanism of planetary migration is planetesimal-driven migration (PDM), which makes planets undergo significant migration through gravitational scattering with planetesimals. Here, we present the results of our high-resolution self-consistent $N$-body simulations of PDM, in which gravitational interactions among planetesimals, the gas drag, and Type-I migration are all taken into account. Our results show that even small protoplanets can actively migrate through PDM. Moreover, a fair fraction of them migrate outward. This outward migration can give a solution for the ''planet migration problem'' caused by Type-I migration and explain the origin of Jovian planets.","sentences":["Recent exoplanet observations have revealed a diversity of exoplanetary systems, which suggests the ubiquity of radial planetary migration.","One powerful known mechanism of planetary migration is planetesimal-driven migration (PDM), which makes planets undergo significant migration through gravitational scattering with planetesimals.","Here, we present the results of our high-resolution self-consistent $N$-body simulations of PDM, in which gravitational interactions among planetesimals, the gas drag, and Type-I migration are all taken into account.","Our results show that even small protoplanets can actively migrate through PDM.","Moreover, a fair fraction of them migrate outward.","This outward migration can give a solution for the ''planet migration problem'' caused by Type-I migration and explain the origin of Jovian planets."],"url":"http://arxiv.org/abs/2405.07279v1","category":"astro-ph.EP"}
{"created":"2024-05-12 12:49:00","title":"Distribution-Preserving Integrated Sensing and Communication with Secure Reconstruction","abstract":"Distribution-preserving integrated sensing and communication with secure reconstruction is investigated in this paper. In addition to the distortion constraint, we impose another constraint on the distance between the reconstructed sequence distribution and the original state distribution to force the system to preserve the statistical property of the channel states. An inner bound of the distribution-preserving capacity-distortion region is provided with some capacity region results under special cases. A numerical example demonstrates the tradeoff between the communication rate, reconstruction distortion and distribution preservation. Furthermore, we consider the case that the reconstructed sequence should be kept secret from an eavesdropper who also observes the channel output. An inner bound of the tradeoff region and a capacity-achieving special case are presented.","sentences":["Distribution-preserving integrated sensing and communication with secure reconstruction is investigated in this paper.","In addition to the distortion constraint, we impose another constraint on the distance between the reconstructed sequence distribution and the original state distribution to force the system to preserve the statistical property of the channel states.","An inner bound of the distribution-preserving capacity-distortion region is provided with some capacity region results under special cases.","A numerical example demonstrates the tradeoff between the communication rate, reconstruction distortion and distribution preservation.","Furthermore, we consider the case that the reconstructed sequence should be kept secret from an eavesdropper who also observes the channel output.","An inner bound of the tradeoff region and a capacity-achieving special case are presented."],"url":"http://arxiv.org/abs/2405.07275v1","category":"cs.IT"}
{"created":"2024-05-12 12:44:05","title":"Timely Offloading in Mobile Edge Cloud Systems","abstract":"Future real-time applications like smart cities will use complex Machine Learning (ML) models for a variety of tasks. Timely status information is required for these applications to be reliable. Offloading computation to a mobile edge cloud (MEC) can reduce the completion time of these tasks. However, using the MEC may come at a cost such as related to use of a cloud service or privacy. In this paper, we consider a source that generates time-stamped status updates for delivery to a monitor after processing by the mobile device or MEC. We study how a scheduler must forward these updates to achieve timely updates at the monitor but also limit MEC usage. We measure timeliness at the monitor using the age of information (AoI) metric. We formulate this problem as an infinite horizon Markov decision process (MDP) with an average cost criterion. We prove that an optimal scheduling policy has an age-threshold structure that depends on how long an update has been in service.","sentences":["Future real-time applications like smart cities will use complex Machine Learning (ML) models for a variety of tasks.","Timely status information is required for these applications to be reliable.","Offloading computation to a mobile edge cloud (MEC) can reduce the completion time of these tasks.","However, using the MEC may come at a cost such as related to use of a cloud service or privacy.","In this paper, we consider a source that generates time-stamped status updates for delivery to a monitor after processing by the mobile device or MEC.","We study how a scheduler must forward these updates to achieve timely updates at the monitor but also limit MEC usage.","We measure timeliness at the monitor using the age of information (AoI) metric.","We formulate this problem as an infinite horizon Markov decision process (MDP) with an average cost criterion.","We prove that an optimal scheduling policy has an age-threshold structure that depends on how long an update has been in service."],"url":"http://arxiv.org/abs/2405.07274v1","category":"eess.SY"}
{"created":"2024-05-12 12:08:05","title":"Span-Aggregatable, Contextualized Word Embeddings for Effective Phrase Mining","abstract":"Dense vector representations for sentences made significant progress in recent years as can be seen on sentence similarity tasks. Real-world phrase retrieval applications, on the other hand, still encounter challenges for effective use of dense representations. We show that when target phrases reside inside noisy context, representing the full sentence with a single dense vector, is not sufficient for effective phrase retrieval. We therefore look into the notion of representing multiple, sub-sentence, consecutive word spans, each with its own dense vector. We show that this technique is much more effective for phrase mining, yet requires considerable compute to obtain useful span representations. Accordingly, we make an argument for contextualized word/token embeddings that can be aggregated for arbitrary word spans while maintaining the span's semantic meaning. We introduce a modification to the common contrastive loss used for sentence embeddings that encourages word embeddings to have this property. To demonstrate the effect of this method we present a dataset based on the STS-B dataset with additional generated text, that requires finding the best matching paraphrase residing in a larger context and report the degree of similarity to the origin phrase. We demonstrate on this dataset, how our proposed method can achieve better results without significant increase to compute.","sentences":["Dense vector representations for sentences made significant progress in recent years as can be seen on sentence similarity tasks.","Real-world phrase retrieval applications, on the other hand, still encounter challenges for effective use of dense representations.","We show that when target phrases reside inside noisy context, representing the full sentence with a single dense vector, is not sufficient for effective phrase retrieval.","We therefore look into the notion of representing multiple, sub-sentence, consecutive word spans, each with its own dense vector.","We show that this technique is much more effective for phrase mining, yet requires considerable compute to obtain useful span representations.","Accordingly, we make an argument for contextualized word/token embeddings that can be aggregated for arbitrary word spans while maintaining the span's semantic meaning.","We introduce a modification to the common contrastive loss used for sentence embeddings that encourages word embeddings to have this property.","To demonstrate the effect of this method we present a dataset based on the STS-B dataset with additional generated text, that requires finding the best matching paraphrase residing in a larger context and report the degree of similarity to the origin phrase.","We demonstrate on this dataset, how our proposed method can achieve better results without significant increase to compute."],"url":"http://arxiv.org/abs/2405.07263v1","category":"cs.CL"}
{"created":"2024-05-12 11:50:39","title":"CiMLoop: A Flexible, Accurate, and Fast Compute-In-Memory Modeling Tool","abstract":"Compute-In-Memory (CiM) is a promising solution to accelerate Deep Neural Networks (DNNs) as it can avoid energy-intensive DNN weight movement and use memory arrays to perform low-energy, high-density computations. These benefits have inspired research across the CiM stack, but CiM research often focuses on only one level of the stack (i.e., devices, circuits, architecture, workload, or mapping) or only one design point (e.g., one fabricated chip). There is a need for a full-stack modeling tool to evaluate design decisions in the context of full systems (e.g., see how a circuit impacts system energy) and to perform rapid early-stage exploration of the CiM co-design space.   To address this need, we propose CiMLoop: an open-source tool to model diverse CiM systems and explore decisions across the CiM stack. CiMLoop introduces (1) a flexible specification that lets users describe, model, and map workloads to both circuits and architecture, (2) an accurate energy model that captures the interaction between DNN operand values, hardware data representations, and analog/digital values propagated by circuits, and (3) a fast statistical model that can explore the design space orders-of-magnitude more quickly than other high-accuracy models.   Using CiMLoop, researchers can evaluate design choices at different levels of the CiM stack, co-design across all levels, fairly compare different implementations, and rapidly explore the design space.","sentences":["Compute-In-Memory (CiM) is a promising solution to accelerate Deep Neural Networks (DNNs) as it can avoid energy-intensive DNN weight movement and use memory arrays to perform low-energy, high-density computations.","These benefits have inspired research across the CiM stack, but CiM research often focuses on only one level of the stack (i.e., devices, circuits, architecture, workload, or mapping) or only one design point (e.g., one fabricated chip).","There is a need for a full-stack modeling tool to evaluate design decisions in the context of full systems (e.g., see how a circuit impacts system energy) and to perform rapid early-stage exploration of the CiM co-design space.   ","To address this need, we propose CiMLoop: an open-source tool to model diverse CiM systems and explore decisions across the CiM stack.","CiMLoop introduces (1) a flexible specification that lets users describe, model, and map workloads to both circuits and architecture, (2) an accurate energy model that captures the interaction between DNN operand values, hardware data representations, and analog/digital values propagated by circuits, and (3) a fast statistical model that can explore the design space orders-of-magnitude more quickly than other high-accuracy models.   ","Using CiMLoop, researchers can evaluate design choices at different levels of the CiM stack, co-design across all levels, fairly compare different implementations, and rapidly explore the design space."],"url":"http://arxiv.org/abs/2405.07259v1","category":"cs.AR"}
{"created":"2024-05-12 11:30:01","title":"Leveraging Fixed and Dynamic Pseudo-labels for Semi-supervised Medical Image Segmentation","abstract":"Semi-supervised medical image segmentation has gained growing interest due to its ability to utilize unannotated data. The current state-of-the-art methods mostly rely on pseudo-labeling within a co-training framework. These methods depend on a single pseudo-label for training, but these labels are not as accurate as the ground truth of labeled data. Relying solely on one pseudo-label often results in suboptimal results. To this end, we propose a novel approach where multiple pseudo-labels for the same unannotated image are used to learn from the unlabeled data: the conventional fixed pseudo-label and the newly introduced dynamic pseudo-label. By incorporating multiple pseudo-labels for the same unannotated image into the co-training framework, our approach provides a more robust training approach that improves model performance and generalization capabilities. We validate our novel approach on three semi-supervised medical benchmark segmentation datasets, the Left Atrium dataset, the Pancreas-CT dataset, and the Brats-2019 dataset. Our approach significantly outperforms state-of-the-art methods over multiple medical benchmark segmentation datasets with different labeled data ratios. We also present several ablation experiments to demonstrate the effectiveness of various components used in our approach.","sentences":["Semi-supervised medical image segmentation has gained growing interest due to its ability to utilize unannotated data.","The current state-of-the-art methods mostly rely on pseudo-labeling within a co-training framework.","These methods depend on a single pseudo-label for training, but these labels are not as accurate as the ground truth of labeled data.","Relying solely on one pseudo-label often results in suboptimal results.","To this end, we propose a novel approach where multiple pseudo-labels for the same unannotated image are used to learn from the unlabeled data: the conventional fixed pseudo-label and the newly introduced dynamic pseudo-label.","By incorporating multiple pseudo-labels for the same unannotated image into the co-training framework, our approach provides a more robust training approach that improves model performance and generalization capabilities.","We validate our novel approach on three semi-supervised medical benchmark segmentation datasets, the Left Atrium dataset, the Pancreas-CT dataset, and the Brats-2019 dataset.","Our approach significantly outperforms state-of-the-art methods over multiple medical benchmark segmentation datasets with different labeled data ratios.","We also present several ablation experiments to demonstrate the effectiveness of various components used in our approach."],"url":"http://arxiv.org/abs/2405.07256v1","category":"eess.IV"}
{"created":"2024-05-12 11:20:31","title":"Deep Learning-aided Parametric Sparse Channel Estimation for Terahertz Massive MIMO Systems","abstract":"Terahertz (THz) communications is considered as one of key solutions to support extremely high data demand in 6G. One main difficulty of the THz communication is the severe signal attenuation caused by the foliage loss, oxygen/atmospheric absorption, body and hand losses. To compensate for the severe path loss, multiple-input-multiple-output (MIMO) antenna array-based beamforming has been widely used. Since the beams should be aligned with the signal propagation path to achieve the maximum beamforming gain, acquisition of accurate channel knowledge, i.e., channel estimation, is of great importance. An aim of this paper is to propose a new type of deep learning (DL)-based parametric channel estimation technique. In our work, DL figures out the mapping function between the received pilot signal and the sparse channel parameters characterizing the spherical domain channel. By exploiting the long short-term memory (LSTM), we can efficiently extract the temporally correlated features of sparse channel parameters and thus make an accurate estimation with relatively small pilot overhead. From the numerical experiments, we show that the proposed scheme is effective in estimating the near-field THz MIMO channel in THz downlink environments.","sentences":["Terahertz (THz) communications is considered as one of key solutions to support extremely high data demand in 6G. One main difficulty of the THz communication is the severe signal attenuation caused by the foliage loss, oxygen/atmospheric absorption, body and hand losses.","To compensate for the severe path loss, multiple-input-multiple-output (MIMO) antenna array-based beamforming has been widely used.","Since the beams should be aligned with the signal propagation path to achieve the maximum beamforming gain, acquisition of accurate channel knowledge, i.e., channel estimation, is of great importance.","An aim of this paper is to propose a new type of deep learning (DL)-based parametric channel estimation technique.","In our work, DL figures out the mapping function between the received pilot signal and the sparse channel parameters characterizing the spherical domain channel.","By exploiting the long short-term memory (LSTM), we can efficiently extract the temporally correlated features of sparse channel parameters and thus make an accurate estimation with relatively small pilot overhead.","From the numerical experiments, we show that the proposed scheme is effective in estimating the near-field THz MIMO channel in THz downlink environments."],"url":"http://arxiv.org/abs/2405.07255v1","category":"eess.SP"}
{"created":"2024-05-12 11:16:05","title":"Universal Batch Learning Under The Misspecification Setting","abstract":"In this paper we consider the problem of universal {\\em batch} learning in a misspecification setting with log-loss. In this setting the hypothesis class is a set of models $\\Theta$. However, the data is generated by an unknown distribution that may not belong to this set but comes from a larger set of models $\\Phi \\supset \\Theta$. Given a training sample, a universal learner is requested to predict a probability distribution for the next outcome and a log-loss is incurred. The universal learner performance is measured by the regret relative to the best hypothesis matching the data, chosen from $\\Theta$. Utilizing the minimax theorem and information theoretical tools, we derive the optimal universal learner, a mixture over the set of the data generating distributions, and get a closed form expression for the min-max regret. We show that this regret can be considered as a constrained version of the conditional capacity between the data and its generating distributions set. We present tight bounds for this min-max regret, implying that the complexity of the problem is dominated by the richness of the hypotheses models $\\Theta$ and not by the data generating distributions set $\\Phi$. We develop an extension to the Arimoto-Blahut algorithm for numerical evaluation of the regret and its capacity achieving prior distribution. We demonstrate our results for the case where the observations come from a $K$-parameters multinomial distributions while the hypothesis class $\\Theta$ is only a subset of this family of distributions.","sentences":["In this paper we consider the problem of universal {\\em batch} learning in a misspecification setting with log-loss.","In this setting the hypothesis class is a set of models $\\Theta$. However, the data is generated by an unknown distribution that may not belong to this set but comes from a larger set of models $\\Phi \\supset \\Theta$. Given a training sample, a universal learner is requested to predict a probability distribution for the next outcome and a log-loss is incurred.","The universal learner performance is measured by the regret relative to the best hypothesis matching the data, chosen from $\\Theta$. Utilizing the minimax theorem and information theoretical tools, we derive the optimal universal learner, a mixture over the set of the data generating distributions, and get a closed form expression for the min-max regret.","We show that this regret can be considered as a constrained version of the conditional capacity between the data and its generating distributions set.","We present tight bounds for this min-max regret, implying that the complexity of the problem is dominated by the richness of the hypotheses models $\\Theta$ and not by the data generating distributions set $\\Phi$. We develop an extension to the Arimoto-Blahut algorithm for numerical evaluation of the regret and its capacity achieving prior distribution.","We demonstrate our results for the case where the observations come from a $K$-parameters multinomial distributions while the hypothesis class $\\Theta$ is only a subset of this family of distributions."],"url":"http://arxiv.org/abs/2405.07252v1","category":"cs.LG"}
{"created":"2024-05-12 10:55:06","title":"Towards Cloud Efficiency with Large-scale Workload Characterization","abstract":"Cloud providers introduce features (e.g., Spot VMs, Harvest VMs, and Burstable VMs) and optimizations (e.g., oversubscription, auto-scaling, power harvesting, and overclocking) to improve efficiency and reliability. To effectively utilize these features, it's crucial to understand the characteristics of workloads running in the cloud. However, workload characteristics can be complex and depend on multiple signals, making manual characterization difficult and unscalable. In this study, we conduct the first large-scale examination of first-party workloads at Microsoft to understand their characteristics. Through an empirical study, we aim to answer the following questions: (1) What are the critical workload characteristics that impact efficiency and reliability on cloud platforms? (2) How do these characteristics vary across different workloads? (3) How can cloud platforms leverage these insights to efficiently characterize all workloads at scale? This study provides a deeper understanding of workload characteristics and their impact on cloud performance, which can aid in optimizing cloud services. Additionally, it identifies potential areas for future research.","sentences":["Cloud providers introduce features (e.g., Spot VMs, Harvest VMs, and Burstable VMs) and optimizations (e.g., oversubscription, auto-scaling, power harvesting, and overclocking) to improve efficiency and reliability.","To effectively utilize these features, it's crucial to understand the characteristics of workloads running in the cloud.","However, workload characteristics can be complex and depend on multiple signals, making manual characterization difficult and unscalable.","In this study, we conduct the first large-scale examination of first-party workloads at Microsoft to understand their characteristics.","Through an empirical study, we aim to answer the following questions: (1) What are the critical workload characteristics that impact efficiency and reliability on cloud platforms?","(2) How do these characteristics vary across different workloads?","(3) How can cloud platforms leverage these insights to efficiently characterize all workloads at scale?","This study provides a deeper understanding of workload characteristics and their impact on cloud performance, which can aid in optimizing cloud services.","Additionally, it identifies potential areas for future research."],"url":"http://arxiv.org/abs/2405.07250v1","category":"cs.DC"}
{"created":"2024-05-12 10:48:18","title":"Regular nilpotent partial Hessenberg varieties","abstract":"Regular nilpotent Hessenberg varieties are subvarieties of full flag varieties, while regular nilpotent partial Hessenberg varieties are subvarieties of partial flag varieties. In this manuscript we first provide a summand formula and a product formula for the Poincar\\'e polynomial of regular nilpotent partial Hessenberg varieties. It is well-known that there is an isomorphism between the cohomology rings of partial flag varieties and the invariants in the cohomology rings of full flag varieties under an action of a certain Weyl group by Bernstein-Gelfand-Gelfand. We generalize this result to regular nilpotent partial Hessenberg varieties. More concretely, we give an isomorphism between the cohomology rings of regular nilpotent partial Hessenberg varieties and the invariant subrings of the cohomology rings of regular nilpotent Hessenberg varieties under the certain Weyl group action. Furthermore, we provide a description of the cohomology rings for regular nilpotent partial Hessenberg varieties in terms of the invariants in the logarithmic derivation modules of ideal arrangements, which is a genelarization of the result by Abe-Masuda-Murai-Sato with the author.","sentences":["Regular nilpotent Hessenberg varieties are subvarieties of full flag varieties, while regular nilpotent partial Hessenberg varieties are subvarieties of partial flag varieties.","In this manuscript we first provide a summand formula and a product formula for the Poincar\\'e polynomial of regular nilpotent partial Hessenberg varieties.","It is well-known that there is an isomorphism between the cohomology rings of partial flag varieties and the invariants in the cohomology rings of full flag varieties under an action of a certain Weyl group by Bernstein-Gelfand-Gelfand.","We generalize this result to regular nilpotent partial Hessenberg varieties.","More concretely, we give an isomorphism between the cohomology rings of regular nilpotent partial Hessenberg varieties and the invariant subrings of the cohomology rings of regular nilpotent Hessenberg varieties under the certain Weyl group action.","Furthermore, we provide a description of the cohomology rings for regular nilpotent partial Hessenberg varieties in terms of the invariants in the logarithmic derivation modules of ideal arrangements, which is a genelarization of the result by Abe-Masuda-Murai-Sato with the author."],"url":"http://arxiv.org/abs/2405.07247v1","category":"math.AG"}
{"created":"2024-05-12 10:11:54","title":"Fight like a Woman: Domestic Violence and Female Judges in Brazil","abstract":"This article investigates the impact of judges' gender on the outcome of domestic violence cases. Using data From S\\~ao Paulo, Brazil, between 2010 and 2019, we compare conviction rates by judge's gender and find that a domestic violence case assigned to a female judge is 31% (10 p.p) more likely to result in conviction than a case assigned to a male judge with similar career characteristics. To show that this decision gap rises due to different gender perspectives about domestic violence instead of rising due to female judges being tougher than male judges, we compare it against gender conviction rate gaps in similar types of crimes. We find that the gender conviction rate gap for domestic violence cases is significantly larger than the same gap for other misdemeanor cases (3 p.p. larger) and for other physical assault cases (8 p.p. larger). Lastly, we find evidence that at least two channels explain this gender conviction rate gap for domestic violence cases: gender-based differences in evidence interpretation and gender-based sentencing criteria.","sentences":["This article investigates the impact of judges' gender on the outcome of domestic violence cases.","Using data From S\\~ao Paulo, Brazil, between 2010 and 2019, we compare conviction rates by judge's gender and find that a domestic violence case assigned to a female judge is 31% (10 p.p) more likely to result in conviction than a case assigned to a male judge with similar career characteristics.","To show that this decision gap rises due to different gender perspectives about domestic violence instead of rising due to female judges being tougher than male judges, we compare it against gender conviction rate gaps in similar types of crimes.","We find that the gender conviction rate gap for domestic violence cases is significantly larger than the same gap for other misdemeanor cases (3 p.p. larger) and for other physical assault cases (8 p.p. larger).","Lastly, we find evidence that at least two channels explain this gender conviction rate gap for domestic violence cases: gender-based differences in evidence interpretation and gender-based sentencing criteria."],"url":"http://arxiv.org/abs/2405.07240v1","category":"econ.GN"}
{"created":"2024-05-12 10:10:34","title":"Active self-disassembly enhances the yield of self-assembled structures","abstract":"We introduce a lattice model to probe the effect of active self-disassembly on equilibrium self-assembly. Surprisingly, we find conditions under which active self-disassembly enhances the yield of a target structure above that achieved by self-assembly alone when the latter is already favoured thermodynamically. We discuss biological implications of our findings.","sentences":["We introduce a lattice model to probe the effect of active self-disassembly on equilibrium self-assembly.","Surprisingly, we find conditions under which active self-disassembly enhances the yield of a target structure above that achieved by self-assembly alone when the latter is already favoured thermodynamically.","We discuss biological implications of our findings."],"url":"http://arxiv.org/abs/2405.07239v1","category":"cond-mat.soft"}
{"created":"2024-05-12 09:39:16","title":"Does gravitational wave assist vacuum steering and Bell nonlocality?","abstract":"We study quantum steering and Bell nonlocality harvested by the local interaction of two Unruh-DeWitt detectors with the vacuum massless scalar field, both in the presence of gravitational waves and in Minkowski spacetime. It is shown that quantum steerability under the influence of gravitational waves can be greater than or less than quantum steerability in Minkowski spacetime, which means that the gravitational waves can amplify or degrade the harvested steering. In particular, a resonance effect occurs when the energy gap of the detector is tuned to the frequency of the gravitational wave. We also find that the harvesting-achievable separation range of vacuum steering can be expanded or reduced by the presence of gravitational waves, which depends on the energy gap, the gravitational wave frequency, and the duration of the gravitational wave action. It is interesting to note that two detector systems that satisfy the Bell inequality, regardless of the existence of gravitational waves, indicating that steering harvesting cannot be considered to be nonlocal.","sentences":["We study quantum steering and Bell nonlocality harvested by the local interaction of two Unruh-DeWitt detectors with the vacuum massless scalar field, both in the presence of gravitational waves and in Minkowski spacetime.","It is shown that quantum steerability under the influence of gravitational waves can be greater than or less than quantum steerability in Minkowski spacetime, which means that the gravitational waves can amplify or degrade the harvested steering.","In particular, a resonance effect occurs when the energy gap of the detector is tuned to the frequency of the gravitational wave.","We also find that the harvesting-achievable separation range of vacuum steering can be expanded or reduced by the presence of gravitational waves, which depends on the energy gap, the gravitational wave frequency, and the duration of the gravitational wave action.","It is interesting to note that two detector systems that satisfy the Bell inequality, regardless of the existence of gravitational waves, indicating that steering harvesting cannot be considered to be nonlocal."],"url":"http://arxiv.org/abs/2405.07235v1","category":"gr-qc"}
{"created":"2024-05-12 09:09:44","title":"Information capacity of quantum communication under natural physical assumptions","abstract":"The quantum prepare-and-measure scenario has been studied under various physical assumptions on the emitted states. Here, we first discuss how different assumptions are conceptually and formally related. We then identify one that can serve as a relaxation of all others, corresponding to a limitation on the one-shot accessible information of the state ensemble. This motivates us to study the optimal state discrimination probability of a source subject to these various physical assumptions. We derive general and tight bounds for states restricted by their quantum dimension, their vacuum component, an arbitrary uniform overlap, the magnitude of higher-dimensional signals and the experimenter's trust in their device. Our results constitute a first step towards a more unified picture of semi-device-independent quantum information processing.","sentences":["The quantum prepare-and-measure scenario has been studied under various physical assumptions on the emitted states.","Here, we first discuss how different assumptions are conceptually and formally related.","We then identify one that can serve as a relaxation of all others, corresponding to a limitation on the one-shot accessible information of the state ensemble.","This motivates us to study the optimal state discrimination probability of a source subject to these various physical assumptions.","We derive general and tight bounds for states restricted by their quantum dimension, their vacuum component, an arbitrary uniform overlap, the magnitude of higher-dimensional signals and the experimenter's trust in their device.","Our results constitute a first step towards a more unified picture of semi-device-independent quantum information processing."],"url":"http://arxiv.org/abs/2405.07231v1","category":"quant-ph"}
{"created":"2024-05-12 09:09:39","title":"Acoustic Positioning for Deep Sea Neutrino Telescopes with a System of Piezo Sensors Integrated into Glass Spheres","abstract":"Position calibration in the deep sea is typically done by means of acoustic multilateration using three or more acoustic emitters installed at known positions. Rather than using hydrophones as receivers that are exposed to the ambient pressure, the sound signals can be coupled to piezo ceramics glued to the inside of existing containers for electronics or measuring instruments of a deep sea infrastructure. The ANTARES neutrino telescope operated from 2006 until 2022 in the Mediterranean Sea at a depth exceeding 2000m. It comprised nearly 900 glass spheres with 432mm diameter and 15mm thickness, equipped with photomultiplier tubes to detect Cherenkov light from tracks of charged elementary particles. In an experimental setup within ANTARES, piezo sensors have been glued to the inside of such - otherwise empty - glass spheres. These sensors recorded signals from acoustic emitters with frequencies from 46545 to 60235Hz. Two waves propagating through the glass sphere are found as a result of the excitation by the waves in the water. These can be qualitatively associated with symmetric and asymmetric Lamb-like waves of zeroth order: a fast (early) one with $v_e \\approx 5$mm/$\\mu$s and a slow (late) one with $v_\\ell \\approx 2$mm/$\\mu$s. Taking these findings into account improves the accuracy of the position calibration. The results can be transferred to the KM3NeT neutrino telescope, currently under construction at multiple sites in the Mediterranean Sea, for which the concept of piezo sensors glued to the inside of glass spheres has been adapted for monitoring the positions of the photomultiplier tubes.","sentences":["Position calibration in the deep sea is typically done by means of acoustic multilateration using three or more acoustic emitters installed at known positions.","Rather than using hydrophones as receivers that are exposed to the ambient pressure, the sound signals can be coupled to piezo ceramics glued to the inside of existing containers for electronics or measuring instruments of a deep sea infrastructure.","The ANTARES neutrino telescope operated from 2006 until 2022 in the Mediterranean Sea at a depth exceeding 2000m. It comprised nearly 900 glass spheres with 432mm diameter and 15mm thickness, equipped with photomultiplier tubes to detect Cherenkov light from tracks of charged elementary particles.","In an experimental setup within ANTARES, piezo sensors have been glued to the inside of such - otherwise empty - glass spheres.","These sensors recorded signals from acoustic emitters with frequencies from 46545 to 60235Hz.","Two waves propagating through the glass sphere are found as a result of the excitation by the waves in the water.","These can be qualitatively associated with symmetric and asymmetric Lamb-like waves of zeroth order: a fast (early) one with $v_e \\approx 5$mm/$\\mu$s and a slow (late) one with $v_\\ell \\approx 2$mm/$\\mu$s.","Taking these findings into account improves the accuracy of the position calibration.","The results can be transferred to the KM3NeT neutrino telescope, currently under construction at multiple sites in the Mediterranean Sea, for which the concept of piezo sensors glued to the inside of glass spheres has been adapted for monitoring the positions of the photomultiplier tubes."],"url":"http://arxiv.org/abs/2405.07230v1","category":"astro-ph.IM"}
{"created":"2024-05-12 09:09:30","title":"MM-InstructEval: Zero-Shot Evaluation of (Multimodal) Large Language Models on Multimodal Reasoning Tasks","abstract":"The rising popularity of multimodal large language models (MLLMs) has sparked a significant increase in research dedicated to evaluating these models. However, current evaluation studies predominantly concentrate on the ability of models to comprehend and reason within a unimodal (vision-only) context, overlooking critical performance evaluations in complex multimodal reasoning tasks that integrate both visual and text contexts. Furthermore, tasks that demand reasoning across multiple modalities pose greater challenges and require a deep understanding of multimodal contexts. In this paper, we introduce a comprehensive assessment framework named MM-InstructEval, which integrates a diverse array of metrics to provide an extensive evaluation of the performance of various models and instructions across a broad range of multimodal reasoning tasks with vision-text contexts. MM-InstructEval enhances the research on the performance of MLLMs in complex multimodal reasoning tasks, facilitating a more thorough and holistic zero-shot evaluation of MLLMs. We firstly utilize the \"Best Performance\" metric to determine the upper performance limit of each model across various datasets. The \"Mean Relative Gain\" metric provides an analysis of the overall performance across different models and instructions, while the \"Stability\" metric evaluates their sensitivity to variations. Historically, the research has focused on evaluating models independently or solely assessing instructions, overlooking the interplay between models and instructions. To address this gap, we introduce the \"Adaptability\" metric, designed to quantify the degree of adaptability between models and instructions. Evaluations are conducted on 31 models (23 MLLMs) across 16 multimodal datasets, covering 6 tasks, with 10 distinct instructions. The extensive analysis enables us to derive novel insights.","sentences":["The rising popularity of multimodal large language models (MLLMs) has sparked a significant increase in research dedicated to evaluating these models.","However, current evaluation studies predominantly concentrate on the ability of models to comprehend and reason within a unimodal (vision-only) context, overlooking critical performance evaluations in complex multimodal reasoning tasks that integrate both visual and text contexts.","Furthermore, tasks that demand reasoning across multiple modalities pose greater challenges and require a deep understanding of multimodal contexts.","In this paper, we introduce a comprehensive assessment framework named MM-InstructEval, which integrates a diverse array of metrics to provide an extensive evaluation of the performance of various models and instructions across a broad range of multimodal reasoning tasks with vision-text contexts.","MM-InstructEval enhances the research on the performance of MLLMs in complex multimodal reasoning tasks, facilitating a more thorough and holistic zero-shot evaluation of MLLMs.","We firstly utilize the \"Best Performance\" metric to determine the upper performance limit of each model across various datasets.","The \"Mean Relative Gain\" metric provides an analysis of the overall performance across different models and instructions, while the \"Stability\" metric evaluates their sensitivity to variations.","Historically, the research has focused on evaluating models independently or solely assessing instructions, overlooking the interplay between models and instructions.","To address this gap, we introduce the \"Adaptability\" metric, designed to quantify the degree of adaptability between models and instructions.","Evaluations are conducted on 31 models (23 MLLMs) across 16 multimodal datasets, covering 6 tasks, with 10 distinct instructions.","The extensive analysis enables us to derive novel insights."],"url":"http://arxiv.org/abs/2405.07229v1","category":"cs.MM"}
{"created":"2024-05-12 08:58:35","title":"A geometric decomposition of finite games: Convergence vs. recurrence under no-regret learning","abstract":"In view of the complexity of the dynamics of no-regret learning in games, we seek to decompose a finite game into simpler components where the day-to-day behavior of the dynamics is well understood. A natural starting point for this is Helmholtz's theorem, which resolves a vector field into a potential and an incompressible component. However, the geometry of no-regret dynamics - and, in particular, the dynamics of exponential / multiplicative weights (EW) schemes - is not compatible with the Euclidean underpinnings of Helmholtz's theorem, leading us to consider a Riemannian framework based on the Shahshahani metric. Using this geometric construction, we introduce the class of incompressible games, and we prove the following results: First, in addition to being volume-preserving, the continuous-time EW dynamics in incompressible games admit a constant of motion and are Poincar\\'e recurrent - i.e., almost every trajectory of play comes arbitrarily close to its starting point infinitely often. Second, we establish a deep connection with a well-known decomposition of games into a potential and harmonic component (where the players' objectives are aligned and anti-aligned respectively): a game is incompressible if and only if it is harmonic, implying in turn that the EW dynamics lead to Poincar\\'e recurrence in harmonic games.","sentences":["In view of the complexity of the dynamics of no-regret learning in games, we seek to decompose a finite game into simpler components where the day-to-day behavior of the dynamics is well understood.","A natural starting point for this is Helmholtz's theorem, which resolves a vector field into a potential and an incompressible component.","However, the geometry of no-regret dynamics - and, in particular, the dynamics of exponential / multiplicative weights (EW) schemes - is not compatible with the Euclidean underpinnings of Helmholtz's theorem, leading us to consider a Riemannian framework based on the Shahshahani metric.","Using this geometric construction, we introduce the class of incompressible games, and we prove the following results: First, in addition to being volume-preserving, the continuous-time EW dynamics in incompressible games admit a constant of motion and are Poincar\\'e recurrent - i.e., almost every trajectory of play comes arbitrarily close to its starting point infinitely often.","Second, we establish a deep connection with a well-known decomposition of games into a potential and harmonic component (where the players' objectives are aligned and anti-aligned respectively): a game is incompressible if and only if it is harmonic, implying in turn that the EW dynamics lead to Poincar\\'e recurrence in harmonic games."],"url":"http://arxiv.org/abs/2405.07224v1","category":"cs.GT"}
{"created":"2024-05-12 08:50:23","title":"Color confinement due to spontaneous breaking of magnetic $U(1)_m^8$","abstract":"The violation of non-Abelian Bianchi identity is equal to 8 Abelian monopole currents of the Dirac type satisfying Abelian conservation rules kinematically. There exist magnetic $U(1)_m^8$ symmetries in non-Abelian $SU(3)$ QCD. When the magnetic $U(1)_m^8$ symmetries are broken spontaneously, only states which are invariant under all $U(1)_e$ subgroups of $SU(3)$ can exist as a physical state. Such states are $SU(3)$ singlets. The QCD vacuum in the confinement phase is characterized by one long percolating monopole loop running over the whole lattice volume in both quenched and full QCD. The long loop in full QCD is on average a few times longer in comparison with that in quenched QCD case. Surprisingly, the monopole behaviors in full QCD seem independent of the bare quark mass suggesting irrelevance of Abelian monopoles to the chiral symmetry breaking mechanism.   Existence of such Abelian magnetic monopoles in the continuum limit is studied in detail in $SU(3)$ by means of a block spin transformation of monopoles and the inverse Monte-Carlo method. The monopole density $\\rho$ and the infrared effective monopole action $S(k)$ of $n$ blocked monopoles are determined for $a(\\beta)=(0.04\\sim 2)$fm and $n=1\\sim 12$ blockings on $48^4$ lattice in quenched QCD and for $a(\\beta)=0.0846(7)$fm and $n=1\\sim 24$ on $96^4$ in full QCD at $m_\\pi=146$MeV. Originally $\\rho$ and $S(k)$ are a two-point function of $a(\\beta)$ and the number of times of the blocking transformation $n$. However, both are found to be a function of $b=na(\\beta)$ alone in the quenched QCD which suggests the existence of the continuum limit. In the full QCD, the renormalization flow is observed similarly but the scaling is not yet proved. The distributions of the long loops show that monopole condensation occurs due to the entropy dominance over the energy for all $b$ considered.","sentences":["The violation of non-Abelian Bianchi identity is equal to 8 Abelian monopole currents of the Dirac type satisfying Abelian conservation rules kinematically.","There exist magnetic $U(1)_m^8$ symmetries in non-Abelian $SU(3)$ QCD.","When the magnetic $U(1)_m^8$ symmetries are broken spontaneously, only states which are invariant under all $U(1)_e$ subgroups of $SU(3)$ can exist as a physical state.","Such states are $SU(3)$ singlets.","The QCD vacuum in the confinement phase is characterized by one long percolating monopole loop running over the whole lattice volume in both quenched and full QCD.","The long loop in full QCD is on average a few times longer in comparison with that in quenched QCD case.","Surprisingly, the monopole behaviors in full QCD seem independent of the bare quark mass suggesting irrelevance of Abelian monopoles to the chiral symmetry breaking mechanism.   ","Existence of such Abelian magnetic monopoles in the continuum limit is studied in detail in $SU(3)$ by means of a block spin transformation of monopoles and the inverse Monte-Carlo method.","The monopole density $\\rho$ and the infrared effective monopole action $S(k)$ of $n$ blocked monopoles are determined for $a(\\beta)=(0.04\\sim 2)$fm and $n=1\\sim 12$ blockings on $48^4$ lattice in quenched QCD and for $a(\\beta)=0.0846(7)$fm and $n=1\\sim 24$ on $96^4$ in full QCD at $m_\\pi=146$MeV. Originally $\\rho$ and $S(k)$ are a two-point function of $a(\\beta)$ and the number of times of the blocking transformation $n$. However, both are found to be a function of $b=na(\\beta)$ alone in the quenched QCD which suggests the existence of the continuum limit.","In the full QCD, the renormalization flow is observed similarly but the scaling is not yet proved.","The distributions of the long loops show that monopole condensation occurs due to the entropy dominance over the energy for all $b$ considered."],"url":"http://arxiv.org/abs/2405.07221v1","category":"hep-lat"}
{"created":"2024-05-13 17:58:14","title":"Improved LARS algorithm for adaptive LASSO in the linear regression model","abstract":"The adaptive LASSO has been used for consistent variable selection in place of LASSO in the linear regression model. In this article, we propose a modified LARS algorithm to combine adaptive LASSO with some biased estimators, namely the Almost Unbiased Ridge Estimator (AURE), Liu Estimator (LE), Almost Unbiased Liu Estimator (AULE), Principal Component Regression Estimator (PCRE), r-k class estimator, and r-d class estimator. Furthermore, we examine the performance of the proposed algorithm using a Monte Carlo simulation study and real-world examples.","sentences":["The adaptive LASSO has been used for consistent variable selection in place of LASSO in the linear regression model.","In this article, we propose a modified LARS algorithm to combine adaptive LASSO with some biased estimators, namely the Almost Unbiased Ridge Estimator (AURE), Liu Estimator (LE), Almost Unbiased Liu Estimator (AULE), Principal Component Regression Estimator (PCRE), r-k class estimator, and r-d class estimator.","Furthermore, we examine the performance of the proposed algorithm using a Monte Carlo simulation study and real-world examples."],"url":"http://arxiv.org/abs/2405.07985v1","category":"stat.ME"}
{"created":"2024-05-13 17:09:03","title":"Authentic Hand Avatar from a Phone Scan via Universal Hand Model","abstract":"The authentic 3D hand avatar with every identifiable information, such as hand shapes and textures, is necessary for immersive experiences in AR/VR. In this paper, we present a universal hand model (UHM), which 1) can universally represent high-fidelity 3D hand meshes of arbitrary identities (IDs) and 2) can be adapted to each person with a short phone scan for the authentic hand avatar. For effective universal hand modeling, we perform tracking and modeling at the same time, while previous 3D hand models perform them separately. The conventional separate pipeline suffers from the accumulated errors from the tracking stage, which cannot be recovered in the modeling stage. On the other hand, ours does not suffer from the accumulated errors while having a much more concise overall pipeline. We additionally introduce a novel image matching loss function to address a skin sliding during the tracking and modeling, while existing works have not focused on it much. Finally, using learned priors from our UHM, we effectively adapt our UHM to each person's short phone scan for the authentic hand avatar.","sentences":["The authentic 3D hand avatar with every identifiable information, such as hand shapes and textures, is necessary for immersive experiences in AR/VR.","In this paper, we present a universal hand model (UHM), which 1) can universally represent high-fidelity 3D hand meshes of arbitrary identities (IDs) and 2) can be adapted to each person with a short phone scan for the authentic hand avatar.","For effective universal hand modeling, we perform tracking and modeling at the same time, while previous 3D hand models perform them separately.","The conventional separate pipeline suffers from the accumulated errors from the tracking stage, which cannot be recovered in the modeling stage.","On the other hand, ours does not suffer from the accumulated errors while having a much more concise overall pipeline.","We additionally introduce a novel image matching loss function to address a skin sliding during the tracking and modeling, while existing works have not focused on it much.","Finally, using learned priors from our UHM, we effectively adapt our UHM to each person's short phone scan for the authentic hand avatar."],"url":"http://arxiv.org/abs/2405.07933v1","category":"cs.CV"}
{"created":"2024-05-13 16:57:52","title":"Adaptive first-order methods with enhanced worst-case rates","abstract":"The Optimized Gradient Method (OGM), its strongly convex extension, the Information Theoretical Exact Method (ITEM), as well as the related Triple Momentum Method (TMM) have superior convergence guarantees when compared to the Fast Gradient Method but lack adaptivity and their derivation is incompatible with composite problems. In this work we introduce a slightly modified version of the estimate sequence that can be used to simultaneously derive OGM, ITEM and TMM while adding memory along with the ability to dynamically adjust the convergence guarantees at runtime. Our framework can be extended to the composite setup and we use it to construct an Enhanced Accelerated Composite Gradient Method equipped with fully-adaptive line-search.","sentences":["The Optimized Gradient Method (OGM), its strongly convex extension, the Information Theoretical Exact Method (ITEM), as well as the related Triple Momentum Method (TMM) have superior convergence guarantees when compared to the Fast Gradient Method but lack adaptivity and their derivation is incompatible with composite problems.","In this work we introduce a slightly modified version of the estimate sequence that can be used to simultaneously derive OGM, ITEM and TMM while adding memory along with the ability to dynamically adjust the convergence guarantees at runtime.","Our framework can be extended to the composite setup and we use it to construct an Enhanced Accelerated Composite Gradient Method equipped with fully-adaptive line-search."],"url":"http://arxiv.org/abs/2405.07926v1","category":"math.OC"}
{"created":"2024-05-13 15:20:31","title":"Adaptive Human-Swarm Interaction based on Workload Measurement using Functional Near-Infrared Spectroscopy","abstract":"One of the challenges of human-swarm interaction (HSI) is how to manage the operator's workload. In order to do this, we propose a novel neurofeedback technique for the real-time measurement of workload using functional near-infrared spectroscopy (fNIRS). The objective is to develop a baseline for workload measurement in human-swarm interaction using fNIRS and to develop an interface that dynamically adapts to the operator's workload. The proposed method consists of using fNIRS device to measure brain activity, process this through a machine learning algorithm, and pass it on to the HSI interface. By dynamically adapting the HSI interface, the swarm operator's workload could be reduced and the performance improved.","sentences":["One of the challenges of human-swarm interaction (HSI) is how to manage the operator's workload.","In order to do this, we propose a novel neurofeedback technique for the real-time measurement of workload using functional near-infrared spectroscopy (fNIRS).","The objective is to develop a baseline for workload measurement in human-swarm interaction using fNIRS and to develop an interface that dynamically adapts to the operator's workload.","The proposed method consists of using fNIRS device to measure brain activity, process this through a machine learning algorithm, and pass it on to the HSI interface.","By dynamically adapting the HSI interface, the swarm operator's workload could be reduced and the performance improved."],"url":"http://arxiv.org/abs/2405.07834v1","category":"cs.RO"}
{"created":"2024-05-13 12:57:03","title":"Valence Quark PDFs of the Proton from Two-Current Correlations in Lattice QCD","abstract":"Following previous works on that topic, we consider Euclidean hadronic matrix elements in position space of two spatially separated local currents on the lattice, in order to extract the $x$-dependence of parton distribution functions (PDFs). The corresponding approach is often referred to by the term lattice cross section (LCS). In this work we will consider valence quark PDFs of an unpolarized proton. We adapt the previously established formalism to our choice of operators. The calculation of the two-current matrix elements requires the evaluation of four-point functions. The corresponding calculation is carried out on a $n_f = 2+1$ gauge ensemble with lattice spacing $a = 0.0856~\\mathrm{fm}$ and pseudoscalar masses $m_\\pi = 355~\\mathrm{MeV}$, $m_K = 441~\\mathrm{MeV}$. The four-point functions have been evaluated in a previous project. The lattice data is converted to the $\\overline{\\mathrm{MS}}$-scheme at a scale $\\mu=2~\\mathrm{GeV}$ and improved w.r.t. lattice artifacts. We use a common model as fit ansatz for the lattice data in order to extract the PDFs.","sentences":["Following previous works on that topic, we consider Euclidean hadronic matrix elements in position space of two spatially separated local currents on the lattice, in order to extract the $x$-dependence of parton distribution functions (PDFs).","The corresponding approach is often referred to by the term lattice cross section (LCS).","In this work we will consider valence quark PDFs of an unpolarized proton.","We adapt the previously established formalism to our choice of operators.","The calculation of the two-current matrix elements requires the evaluation of four-point functions.","The corresponding calculation is carried out on a $n_f = 2+1$ gauge ensemble with lattice spacing $a = 0.0856~\\mathrm{fm}$ and pseudoscalar masses $m_\\pi = 355~\\mathrm{MeV}$, $m_K = 441~\\mathrm{MeV}$.","The four-point functions have been evaluated in a previous project.","The lattice data is converted to the $\\overline{\\mathrm{MS}}$-scheme at a scale $\\mu=2~\\mathrm{GeV}$ and improved w.r.t. lattice artifacts.","We use a common model as fit ansatz for the lattice data in order to extract the PDFs."],"url":"http://arxiv.org/abs/2405.07712v1","category":"hep-lat"}
{"created":"2024-05-13 12:32:45","title":"MonoMAE: Enhancing Monocular 3D Detection through Depth-Aware Masked Autoencoders","abstract":"Monocular 3D object detection aims for precise 3D localization and identification of objects from a single-view image. Despite its recent progress, it often struggles while handling pervasive object occlusions that tend to complicate and degrade the prediction of object dimensions, depths, and orientations. We design MonoMAE, a monocular 3D detector inspired by Masked Autoencoders that addresses the object occlusion issue by masking and reconstructing objects in the feature space. MonoMAE consists of two novel designs. The first is depth-aware masking that selectively masks certain parts of non-occluded object queries in the feature space for simulating occluded object queries for network training. It masks non-occluded object queries by balancing the masked and preserved query portions adaptively according to the depth information. The second is lightweight query completion that works with the depth-aware masking to learn to reconstruct and complete the masked object queries. With the proposed object occlusion and completion, MonoMAE learns enriched 3D representations that achieve superior monocular 3D detection performance qualitatively and quantitatively for both occluded and non-occluded objects. Additionally, MonoMAE learns generalizable representations that can work well in new domains.","sentences":["Monocular 3D object detection aims for precise 3D localization and identification of objects from a single-view image.","Despite its recent progress, it often struggles while handling pervasive object occlusions that tend to complicate and degrade the prediction of object dimensions, depths, and orientations.","We design MonoMAE, a monocular 3D detector inspired by Masked Autoencoders that addresses the object occlusion issue by masking and reconstructing objects in the feature space.","MonoMAE consists of two novel designs.","The first is depth-aware masking that selectively masks certain parts of non-occluded object queries in the feature space for simulating occluded object queries for network training.","It masks non-occluded object queries by balancing the masked and preserved query portions adaptively according to the depth information.","The second is lightweight query completion that works with the depth-aware masking to learn to reconstruct and complete the masked object queries.","With the proposed object occlusion and completion, MonoMAE learns enriched 3D representations that achieve superior monocular 3D detection performance qualitatively and quantitatively for both occluded and non-occluded objects.","Additionally, MonoMAE learns generalizable representations that can work well in new domains."],"url":"http://arxiv.org/abs/2405.07696v1","category":"cs.CV"}
{"created":"2024-05-13 12:30:09","title":"High-energy neutrinos from late-time jets of gamma-ray bursts with cocoon photons","abstract":"In gamma-ray bursts (GRBs), $\\sim$ 100 - 1000 s after the prompt emission, afterglow observations have consistently shown X-ray excesses detected in the form of flares (XFs; in long GRBs) or extended emission (EEs; in short GRBs). These observations are interpreted as emissions from jets launched by late central engine activity. However, the characteristics of these late-time jets, particularly the dissipation radius ($r_{\\rm diss}$), Lorentz factor ($\\Gamma$), and cosmic-ray loading factor ($\\xi_p$), remain unknown despite their importance. Here, in order to understand the properties of the late-time jets with future multi-messenger observations, we estimate the detectability of neutrinos associated with late-time emissions for a wide range of $r_{\\rm diss}$ and $\\Gamma$, assuming $\\xi_p=10$. We take into account external seed photons from the cocoon around the jets, which can enhance the neutrino production through photohadronic interaction in the jet dissipation region. Our results are still consistent with the upper limit obtained by IceCube. Our calculations indicate a promising prospect for neutrino detection with IceCube-Gen2 through the stacking of $\\sim 1000-2000$ events, for a wide range of $r_{\\rm diss}$ and $\\Gamma$. We found that setting an optimal energy threshold of 10 TeV can significantly reduce noise without negatively affecting neutrino detection. Furthermore, even in the case of non-detection, we show that meaningful constraints on the characteristics of the late-time jets can be obtained.","sentences":["In gamma-ray bursts (GRBs), $\\sim$ 100 - 1000 s after the prompt emission, afterglow observations have consistently shown X-ray excesses detected in the form of flares (XFs; in long GRBs) or extended emission (EEs; in short GRBs).","These observations are interpreted as emissions from jets launched by late central engine activity.","However, the characteristics of these late-time jets, particularly the dissipation radius ($r_{\\rm diss}$), Lorentz factor ($\\Gamma$), and cosmic-ray loading factor ($\\xi_p$), remain unknown despite their importance.","Here, in order to understand the properties of the late-time jets with future multi-messenger observations, we estimate the detectability of neutrinos associated with late-time emissions for a wide range of $r_{\\rm diss}$ and $\\Gamma$, assuming $\\xi_p=10$. We take into account external seed photons from the cocoon around the jets, which can enhance the neutrino production through photohadronic interaction in the jet dissipation region.","Our results are still consistent with the upper limit obtained by IceCube.","Our calculations indicate a promising prospect for neutrino detection with IceCube-Gen2 through the stacking of $\\sim 1000-2000$ events, for a wide range of $r_{\\rm diss}$ and $\\Gamma$. We found that setting an optimal energy threshold of 10 TeV can significantly reduce noise without negatively affecting neutrino detection.","Furthermore, even in the case of non-detection, we show that meaningful constraints on the characteristics of the late-time jets can be obtained."],"url":"http://arxiv.org/abs/2405.07695v1","category":"astro-ph.HE"}
{"created":"2024-05-13 10:26:53","title":"New Low-Dissipation Central-Upwind Schemes. Part II","abstract":"The low-dissipation central-upwind (LDCU) schemes have been recently introduced in [A. Kurganov and R. Xin, J. Sci. Comput., 96 (2023), Paper No. 56] as a modification of the central-upwind (CU) schemes from [{\\sc A. Kurganov and C. T. Lin, Commun. Comput. Phys., 2 (2007), pp. 141-163}]. The LDCU schemes achieve much higher resolution of contact waves and many (two-dimensional) structures resulting from complicated wave interaction. However, the LDCU schemes sometimes produce more oscillatory results compared with the CU schemes, especially near the computational domain boundaries.   In this paper, we propose a very simple -- yet systematic -- modification of the LDCU schemes, which completely eliminates the aforementioned oscillations almost without affecting the quality of the computed solution.","sentences":["The low-dissipation central-upwind (LDCU) schemes have been recently introduced in [A. Kurganov and R. Xin, J. Sci.","Comput., 96 (2023), Paper No. 56] as a modification of the central-upwind (CU) schemes from [{\\sc A. Kurganov and C. T. Lin, Commun.","Comput.","Phys., 2 (2007), pp. 141-163}].","The LDCU schemes achieve much higher resolution of contact waves and many (two-dimensional) structures resulting from complicated wave interaction.","However, the LDCU schemes sometimes produce more oscillatory results compared with the CU schemes, especially near the computational domain boundaries.   ","In this paper, we propose a very simple -- yet systematic -- modification of the LDCU schemes, which completely eliminates the aforementioned oscillations almost without affecting the quality of the computed solution."],"url":"http://arxiv.org/abs/2405.07620v1","category":"math.NA"}
{"created":"2024-05-13 08:33:00","title":"Space Domain based Ecological Cooperative and Adaptive Cruise Control on Rolling Terrain","abstract":"Ecological Cooperative and Adaptive Cruise Control (Eco-CACC) is widely focused to enhance sustainability of CACC. However, state-of-the-art Eco-CACC studies are still facing challenges in adopting on rolling terrain. Furthermore, they cannot ensure both ecology optimality and computational efficiency. Hence, this paper proposes a nonlinear optimal control based Eco-CACC controller. It has the following features: i) enhancing performance across rolling terrains by modeling in space domain; ii) enhancing fuel efficiency via globally optimizing all vehicle's fuel consumptions; iii) ensuring computational efficiency by developing a differential dynamic programming-based solving method for the non-linear optimal control problem; iv) ensuring string stability through theoretically proving and experimentally validating. The performance of the proposed Eco-CACC controller was evaluated. Results showed that the proposed Eco-CACC controller can improve average fuel saving by 37.67% at collector road and about 17.30% at major arterial.","sentences":["Ecological Cooperative and Adaptive Cruise Control (Eco-CACC) is widely focused to enhance sustainability of CACC.","However, state-of-the-art Eco-CACC studies are still facing challenges in adopting on rolling terrain.","Furthermore, they cannot ensure both ecology optimality and computational efficiency.","Hence, this paper proposes a nonlinear optimal control based Eco-CACC controller.","It has the following features: i) enhancing performance across rolling terrains by modeling in space domain; ii) enhancing fuel efficiency via globally optimizing all vehicle's fuel consumptions; iii) ensuring computational efficiency by developing a differential dynamic programming-based solving method for the non-linear optimal control problem; iv) ensuring string stability through theoretically proving and experimentally validating.","The performance of the proposed Eco-CACC controller was evaluated.","Results showed that the proposed Eco-CACC controller can improve average fuel saving by 37.67% at collector road and about 17.30% at major arterial."],"url":"http://arxiv.org/abs/2405.07553v1","category":"cs.RO"}
{"created":"2024-05-13 07:20:21","title":"Fine-tuning the SwissBERT Encoder Model for Embedding Sentences and Documents","abstract":"Encoder models trained for the embedding of sentences or short documents have proven useful for tasks such as semantic search and topic modeling. In this paper, we present a version of the SwissBERT encoder model that we specifically fine-tuned for this purpose. SwissBERT contains language adapters for the four national languages of Switzerland -- German, French, Italian, and Romansh -- and has been pre-trained on a large number of news articles in those languages. Using contrastive learning based on a subset of these articles, we trained a fine-tuned version, which we call SentenceSwissBERT. Multilingual experiments on document retrieval and text classification in a Switzerland-specific setting show that SentenceSwissBERT surpasses the accuracy of the original SwissBERT model and of a comparable baseline. The model is openly available for research use.","sentences":["Encoder models trained for the embedding of sentences or short documents have proven useful for tasks such as semantic search and topic modeling.","In this paper, we present a version of the SwissBERT encoder model that we specifically fine-tuned for this purpose.","SwissBERT contains language adapters for the four national languages of Switzerland -- German, French, Italian, and Romansh -- and has been pre-trained on a large number of news articles in those languages.","Using contrastive learning based on a subset of these articles, we trained a fine-tuned version, which we call SentenceSwissBERT.","Multilingual experiments on document retrieval and text classification in a Switzerland-specific setting show that SentenceSwissBERT surpasses the accuracy of the original SwissBERT model and of a comparable baseline.","The model is openly available for research use."],"url":"http://arxiv.org/abs/2405.07513v1","category":"cs.CL"}
{"created":"2024-05-13 06:43:11","title":"Optimized Generation of Entanglement by Real-Time Ordering of Swapping Operations","abstract":"Long-distance quantum communication in quantum networks faces significant challenges due to the constraints imposed by the no-cloning theorem. Most existing quantum communication protocols rely on the a priori distribution of entanglement pairs (EPs), a process known to incur considerable latency due to its stochastic nature. In this work, we consider the problem of minimizing the latency of establishing an EP across a pair of nodes in a quantum network. While prior research has primarily focused on minimizing the expected generation latency by selecting {\\em static} entanglement routes and/or swapping trees in advance, our approach considers a real-time adaptive strategy -- wherein the order of entanglement-swapping operations (hence, the swapping tree used) is progressively determined at runtime based on the runtime success/failure of the stochastic events. In this context, we present a greedy algorithm that iteratively determines the best route and/or entanglement-swapping operation to perform at each stage based on the current network. We evaluate our schemes on randomly generated networks and observe a reduction in latency of up to 40% from the optimal offline approach.","sentences":["Long-distance quantum communication in quantum networks faces significant challenges due to the constraints imposed by the no-cloning theorem.","Most existing quantum communication protocols rely on the a priori distribution of entanglement pairs (EPs), a process known to incur considerable latency due to its stochastic nature.","In this work, we consider the problem of minimizing the latency of establishing an EP across a pair of nodes in a quantum network.","While prior research has primarily focused on minimizing the expected generation latency by selecting {\\em static} entanglement routes and/or swapping trees in advance, our approach considers a real-time adaptive strategy -- wherein the order of entanglement-swapping operations (hence, the swapping tree used) is progressively determined at runtime based on the runtime success/failure of the stochastic events.","In this context, we present a greedy algorithm that iteratively determines the best route and/or entanglement-swapping operation to perform at each stage based on the current network.","We evaluate our schemes on randomly generated networks and observe a reduction in latency of up to 40% from the optimal offline approach."],"url":"http://arxiv.org/abs/2405.07501v1","category":"quant-ph"}
{"created":"2024-05-13 05:48:35","title":"Text Grouping Adapter: Adapting Pre-trained Text Detector for Layout Analysis","abstract":"Significant progress has been made in scene text detection models since the rise of deep learning, but scene text layout analysis, which aims to group detected text instances as paragraphs, has not kept pace. Previous works either treated text detection and grouping using separate models, or train a model from scratch while using a unified one. All of them have not yet made full use of the already well-trained text detectors and easily obtainable detection datasets. In this paper, we present Text Grouping Adapter (TGA), a module that can enable the utilization of various pre-trained text detectors to learn layout analysis, allowing us to adopt a well-trained text detector right off the shelf or just fine-tune it efficiently. Designed to be compatible with various text detector architectures, TGA takes detected text regions and image features as universal inputs to assemble text instance features. To capture broader contextual information for layout analysis, we propose to predict text group masks from text instance features by one-to-many assignment. Our comprehensive experiments demonstrate that, even with frozen pre-trained models, incorporating our TGA into various pre-trained text detectors and text spotters can achieve superior layout analysis performance, simultaneously inheriting generalized text detection ability from pre-training. In the case of full parameter fine-tuning, we can further improve layout analysis performance.","sentences":["Significant progress has been made in scene text detection models since the rise of deep learning, but scene text layout analysis, which aims to group detected text instances as paragraphs, has not kept pace.","Previous works either treated text detection and grouping using separate models, or train a model from scratch while using a unified one.","All of them have not yet made full use of the already well-trained text detectors and easily obtainable detection datasets.","In this paper, we present Text Grouping Adapter (TGA), a module that can enable the utilization of various pre-trained text detectors to learn layout analysis, allowing us to adopt a well-trained text detector right off the shelf or just fine-tune it efficiently.","Designed to be compatible with various text detector architectures, TGA takes detected text regions and image features as universal inputs to assemble text instance features.","To capture broader contextual information for layout analysis, we propose to predict text group masks from text instance features by one-to-many assignment.","Our comprehensive experiments demonstrate that, even with frozen pre-trained models, incorporating our TGA into various pre-trained text detectors and text spotters can achieve superior layout analysis performance, simultaneously inheriting generalized text detection ability from pre-training.","In the case of full parameter fine-tuning, we can further improve layout analysis performance."],"url":"http://arxiv.org/abs/2405.07481v1","category":"cs.CV"}
{"created":"2024-05-13 04:21:00","title":"DualFocus: A Unified Framework for Integrating Positive and Negative Descriptors in Text-based Person Retrieval","abstract":"Text-based person retrieval (TPR) aims to retrieve images of a person from an extensive array of candidates based on a given textual description. The core challenge lies in mapping visual and textual data into a unified latent space. While existing TPR methods concentrate on recognizing explicit and positive characteristics, they often neglect the critical influence of negative descriptors, resulting in potential false positives that fulfill positive criteria but could be excluded by negative descriptors. To alleviate these issues, we introduce DualFocus, a unified framework for integrating positive and negative descriptors to enhance the interpretative accuracy of vision-language foundational models regarding textual queries. DualFocus employs Dual (Positive/Negative) Attribute Prompt Learning (DAPL), which integrates Dual Image-Attribute Contrastive (DIAC) Learning and Sensitive Image-Attributes Matching (SIAM) Learning. This way DualFocus enhances the detection of unseen attributes, thereby boosting retrieval precision. To further achieve a balance between coarse and fine-grained alignment of visual and textual embeddings, we propose the Dynamic Tokenwise Similarity (DTS) loss, which refines the representation of both matching and non-matching descriptions, thereby enhancing the matching process through a detailed and adaptable similarity assessment. By focusing on token-level comparisons, DualFocus significantly outperforms existing techniques in both precision and robustness. The experiment results highlight DualFocus's superior performance on CUHK-PEDES, ICFG-PEDES, and RSTPReid.","sentences":["Text-based person retrieval (TPR) aims to retrieve images of a person from an extensive array of candidates based on a given textual description.","The core challenge lies in mapping visual and textual data into a unified latent space.","While existing TPR methods concentrate on recognizing explicit and positive characteristics, they often neglect the critical influence of negative descriptors, resulting in potential false positives that fulfill positive criteria but could be excluded by negative descriptors.","To alleviate these issues, we introduce DualFocus, a unified framework for integrating positive and negative descriptors to enhance the interpretative accuracy of vision-language foundational models regarding textual queries.","DualFocus employs Dual (Positive/Negative) Attribute Prompt Learning (DAPL), which integrates Dual Image-Attribute Contrastive (DIAC) Learning and Sensitive Image-Attributes Matching (SIAM) Learning.","This way DualFocus enhances the detection of unseen attributes, thereby boosting retrieval precision.","To further achieve a balance between coarse and fine-grained alignment of visual and textual embeddings, we propose the Dynamic Tokenwise Similarity (DTS) loss, which refines the representation of both matching and non-matching descriptions, thereby enhancing the matching process through a detailed and adaptable similarity assessment.","By focusing on token-level comparisons, DualFocus significantly outperforms existing techniques in both precision and robustness.","The experiment results highlight DualFocus's superior performance on CUHK-PEDES, ICFG-PEDES, and RSTPReid."],"url":"http://arxiv.org/abs/2405.07459v1","category":"cs.CV"}
{"created":"2024-05-13 02:07:51","title":"JointLoc: A Real-time Visual Localization Framework for Planetary UAVs Based on Joint Relative and Absolute Pose Estimation","abstract":"Unmanned aerial vehicles (UAVs) visual localization in planetary aims to estimate the absolute pose of the UAV in the world coordinate system through satellite maps and images captured by on-board cameras. However, since planetary scenes often lack significant landmarks and there are modal differences between satellite maps and UAV images, the accuracy and real-time performance of UAV positioning will be reduced. In order to accurately determine the position of the UAV in a planetary scene in the absence of the global navigation satellite system (GNSS), this paper proposes JointLoc, which estimates the real-time UAV position in the world coordinate system by adaptively fusing the absolute 2-degree-of-freedom (2-DoF) pose and the relative 6-degree-of-freedom (6-DoF) pose. Extensive comparative experiments were conducted on a proposed planetary UAV image cross-modal localization dataset, which contains three types of typical Martian topography generated via a simulation engine as well as real Martian UAV images from the Ingenuity helicopter. JointLoc achieved a root-mean-square error of 0.237m in the trajectories of up to 1,000m, compared to 0.594m and 0.557m for ORB-SLAM2 and ORB-SLAM3 respectively. The source code will be available at https://github.com/LuoXubo/JointLoc.","sentences":["Unmanned aerial vehicles (UAVs) visual localization in planetary aims to estimate the absolute pose of the UAV in the world coordinate system through satellite maps and images captured by on-board cameras.","However, since planetary scenes often lack significant landmarks and there are modal differences between satellite maps and UAV images, the accuracy and real-time performance of UAV positioning will be reduced.","In order to accurately determine the position of the UAV in a planetary scene in the absence of the global navigation satellite system (GNSS), this paper proposes JointLoc, which estimates the real-time UAV position in the world coordinate system by adaptively fusing the absolute 2-degree-of-freedom (2-DoF) pose and the relative 6-degree-of-freedom (6-DoF) pose.","Extensive comparative experiments were conducted on a proposed planetary UAV image cross-modal localization dataset, which contains three types of typical Martian topography generated via a simulation engine as well as real Martian UAV images from the Ingenuity helicopter.","JointLoc achieved a root-mean-square error of 0.237m in the trajectories of up to 1,000m, compared to 0.594m and 0.557m for ORB-SLAM2 and ORB-SLAM3 respectively.","The source code will be available at https://github.com/LuoXubo/JointLoc."],"url":"http://arxiv.org/abs/2405.07429v1","category":"cs.RO"}
{"created":"2024-05-12 23:34:06","title":"Semi-Supervised Weed Detection for Rapid Deployment and Enhanced Efficiency","abstract":"Weeds present a significant challenge in agriculture, causing yield loss and requiring expensive control measures. Automatic weed detection using computer vision and deep learning offers a promising solution. However, conventional deep learning methods often require large amounts of labelled training data, which can be costly and time-consuming to acquire. This paper introduces a novel method for semi-supervised weed detection, comprising two main components. Firstly, a multi-scale feature representation technique is employed to capture distinctive weed features across different scales. Secondly, we propose an adaptive pseudo-label assignment strategy, leveraging a small set of labelled images during training. This strategy dynamically assigns confidence scores to pseudo-labels generated from unlabeled data. Additionally, our approach integrates epoch-corresponding and mixed pseudo-labels to further enhance the learning process. Experimental results on the COCO dataset and five prominent weed datasets -- CottonWeedDet12, CropAndWeed, Palmer amaranth, RadishWheat, and RoboWeedMap -- illustrate that our method achieves state-of-the-art performance in weed detection, even with significantly less labelled data compared to existing techniques. This approach holds the potential to alleviate the labelling burden and enhance the feasibility and deployment speed of deep learning for weed detection in real-world agricultural scenarios.","sentences":["Weeds present a significant challenge in agriculture, causing yield loss and requiring expensive control measures.","Automatic weed detection using computer vision and deep learning offers a promising solution.","However, conventional deep learning methods often require large amounts of labelled training data, which can be costly and time-consuming to acquire.","This paper introduces a novel method for semi-supervised weed detection, comprising two main components.","Firstly, a multi-scale feature representation technique is employed to capture distinctive weed features across different scales.","Secondly, we propose an adaptive pseudo-label assignment strategy, leveraging a small set of labelled images during training.","This strategy dynamically assigns confidence scores to pseudo-labels generated from unlabeled data.","Additionally, our approach integrates epoch-corresponding and mixed pseudo-labels to further enhance the learning process.","Experimental results on the COCO dataset and five prominent weed datasets -- CottonWeedDet12, CropAndWeed, Palmer amaranth, RadishWheat, and RoboWeedMap -- illustrate that our method achieves state-of-the-art performance in weed detection, even with significantly less labelled data compared to existing techniques.","This approach holds the potential to alleviate the labelling burden and enhance the feasibility and deployment speed of deep learning for weed detection in real-world agricultural scenarios."],"url":"http://arxiv.org/abs/2405.07399v1","category":"cs.CV"}
{"created":"2024-05-12 20:45:52","title":"Advocating Feedback Control for Human-Earth System Applications","abstract":"This paper proposes a feedback control perspective for Human-Earth Systems (HESs) which essentially are complex systems that capture the interactions between humans and nature. Recent attention in HES research has been directed towards devising strategies for climate change mitigation and adaptation, aimed at achieving environmental and societal objectives. However, existing approaches heavily rely on HES models, which inherently suffer from inaccuracies due to the complexity of the system. Moreover, overly detailed models often prove impractical for optimization tasks. We propose a framework inheriting from feedback control strategies the robustness against model errors, because inaccuracies are mitigated using measurements retrieved from the field. The framework comprises two nested control loops. The outer loop computes the optimal inputs to the HES, which are then implemented by actuators controlled in the inner loop. Potential fields of applications are also identified.","sentences":["This paper proposes a feedback control perspective for Human-Earth Systems (HESs) which essentially are complex systems that capture the interactions between humans and nature.","Recent attention in HES research has been directed towards devising strategies for climate change mitigation and adaptation, aimed at achieving environmental and societal objectives.","However, existing approaches heavily rely on HES models, which inherently suffer from inaccuracies due to the complexity of the system.","Moreover, overly detailed models often prove impractical for optimization tasks.","We propose a framework inheriting from feedback control strategies the robustness against model errors, because inaccuracies are mitigated using measurements retrieved from the field.","The framework comprises two nested control loops.","The outer loop computes the optimal inputs to the HES, which are then implemented by actuators controlled in the inner loop.","Potential fields of applications are also identified."],"url":"http://arxiv.org/abs/2405.07376v1","category":"eess.SY"}
{"created":"2024-05-12 13:48:18","title":"BeautyMap: Binary-Encoded Adaptable Ground Matrix for Dynamic Points Removal in Global Maps","abstract":"Global point clouds that correctly represent the static environment features can facilitate accurate localization and robust path planning. However, dynamic objects introduce undesired ghost tracks that are mixed up with the static environment. Existing dynamic removal methods normally fail to balance the performance in computational efficiency and accuracy. In response, we present BeautyMap to efficiently remove the dynamic points while retaining static features for high-fidelity global maps. Our approach utilizes a binary-encoded matrix to efficiently extract the environment features. With a bit-wise comparison between matrices of each frame and the corresponding map region, we can extract potential dynamic regions. Then we use coarse to fine hierarchical segmentation of the $z$-axis to handle terrain variations. The final static restoration module accounts for the range-visibility of each single scan and protects static points out of sight. Comparative experiments underscore BeautyMap's superior performance in both accuracy and efficiency against other dynamic points removal methods. The code is open-sourced at https://github.com/MKJia/BeautyMap.","sentences":["Global point clouds that correctly represent the static environment features can facilitate accurate localization and robust path planning.","However, dynamic objects introduce undesired ghost tracks that are mixed up with the static environment.","Existing dynamic removal methods normally fail to balance the performance in computational efficiency and accuracy.","In response, we present BeautyMap to efficiently remove the dynamic points while retaining static features for high-fidelity global maps.","Our approach utilizes a binary-encoded matrix to efficiently extract the environment features.","With a bit-wise comparison between matrices of each frame and the corresponding map region, we can extract potential dynamic regions.","Then we use coarse to fine hierarchical segmentation of the $z$-axis to handle terrain variations.","The final static restoration module accounts for the range-visibility of each single scan and protects static points out of sight.","Comparative experiments underscore BeautyMap's superior performance in both accuracy and efficiency against other dynamic points removal methods.","The code is open-sourced at https://github.com/MKJia/BeautyMap."],"url":"http://arxiv.org/abs/2405.07283v1","category":"cs.RO"}
{"created":"2024-05-12 11:46:10","title":"Memory-corrected quantum repeaters with adaptive syndrome identification","abstract":"We address the challenge of incorporating encoded quantum memories into an exact secret key rate analysis for small and intermediate-scale quantum repeaters. To this end, we introduce the check matrix model and quantify the resilience of stabilizer codes of up to eleven qubits against Pauli noise, obtaining analytical expressions for effective logical error probabilities. Generally, we find that the five-qubit and Steane codes either outperform more complex, larger codes in the experimentally relevant parameter regimes or have a lower resource overhead. Subsequently, we apply our results to calculate lower bounds on the asymptotic secret key rate in memory-corrected quantum repeaters when using the five-qubit or Steane codes on the memory qubits. The five-qubit code drastically increases the effective memory coherence time, reducing a phase flip probability of $1\\%$ to $0.001\\%$ when employing an error syndrome identification adapted to the quantum noise channel. Furthermore, it mitigates the impact of faulty Bell state measurements and imperfect state preparation, lowering the minimally required depolarization parameter for non-zero secret key rates in an eight-segment repeater from $98.4\\%$ to $96.4\\%$. As a result, the memory-corrected quantum repeater can often generate secret keys in experimental parameter regimes where the unencoded repeater fails to produce a secret key. In an eight-segment repeater, one can even achieve non-vanishing secret key rates up to distances of 2000 km for memory coherence times of $t_c = 10$ s or less using multiplexing. Assuming a zero-distance link-coupling efficiency $p_0 = 0.7$, a depolarization parameter $\\mu = 0.99$, $t_c = 10$ s, and an 800 km total repeater length, we obtain a secret key rate of 4.85 Hz, beating both the unencoded repeater that provides 1.25 Hz and ideal twin-field quantum key distribution with 0.71 Hz at GHz clock rates.","sentences":["We address the challenge of incorporating encoded quantum memories into an exact secret key rate analysis for small and intermediate-scale quantum repeaters.","To this end, we introduce the check matrix model and quantify the resilience of stabilizer codes of up to eleven qubits against Pauli noise, obtaining analytical expressions for effective logical error probabilities.","Generally, we find that the five-qubit and Steane codes either outperform more complex, larger codes in the experimentally relevant parameter regimes or have a lower resource overhead.","Subsequently, we apply our results to calculate lower bounds on the asymptotic secret key rate in memory-corrected quantum repeaters when using the five-qubit or Steane codes on the memory qubits.","The five-qubit code drastically increases the effective memory coherence time, reducing a phase flip probability of $1\\%$ to $0.001\\%$ when employing an error syndrome identification adapted to the quantum noise channel.","Furthermore, it mitigates the impact of faulty Bell state measurements and imperfect state preparation, lowering the minimally required depolarization parameter for non-zero secret key rates in an eight-segment repeater from $98.4\\%$ to $96.4\\%$. As a result, the memory-corrected quantum repeater can often generate secret keys in experimental parameter regimes where the unencoded repeater fails to produce a secret key.","In an eight-segment repeater, one can even achieve non-vanishing secret key rates up to distances of 2000 km for memory coherence times of $t_c = 10$ s or less using multiplexing.","Assuming a zero-distance link-coupling efficiency $p_0 = 0.7$, a depolarization parameter $\\mu = 0.99$, $t_c = 10$ s, and an 800 km total repeater length, we obtain a secret key rate of 4.85 Hz, beating both the unencoded repeater that provides 1.25 Hz and ideal twin-field quantum key distribution with 0.71 Hz at GHz clock rates."],"url":"http://arxiv.org/abs/2405.07258v1","category":"quant-ph"}
{"created":"2024-05-12 10:13:36","title":"Case Study of Novelty, Complexity, and Adaptation in a Multicellular System","abstract":"Continuing generation of novelty, complexity, and adaptation are well-established as core aspects of open-ended evolution. However, it has yet to be firmly established to what extent these phenomena are coupled and by what means they interact. In this work, we track the co-evolution of novelty, complexity, and adaptation in a case study from the DISHTINY simulation system, which is designed to study the evolution of digital multicellularity. In this case study, we describe ten qualitatively distinct multicellular morphologies, several of which exhibit asymmetrical growth and distinct life stages. We contextualize the evolutionary history of these morphologies with measurements of complexity and adaptation. Our case study suggests a loose -- sometimes divergent -- relationship can exist among novelty, complexity, and adaptation.","sentences":["Continuing generation of novelty, complexity, and adaptation are well-established as core aspects of open-ended evolution.","However, it has yet to be firmly established to what extent these phenomena are coupled and by what means they interact.","In this work, we track the co-evolution of novelty, complexity, and adaptation in a case study from the DISHTINY simulation system, which is designed to study the evolution of digital multicellularity.","In this case study, we describe ten qualitatively distinct multicellular morphologies, several of which exhibit asymmetrical growth and distinct life stages.","We contextualize the evolutionary history of these morphologies with measurements of complexity and adaptation.","Our case study suggests a loose -- sometimes divergent -- relationship can exist among novelty, complexity, and adaptation."],"url":"http://arxiv.org/abs/2405.07241v1","category":"cs.NE"}
{"created":"2024-05-12 10:10:13","title":"Handwriting Anomalies and Learning Disabilities through Recurrent Neural Networks and Geometric Pattern Analysis","abstract":"Dyslexia and dysgraphia are learning disabilities that significantly impact reading, writing, and language processing capabilities. Dyslexia primarily affects reading, manifesting as difficulties in word recognition and phonological processing, where individuals struggle to connect sounds with corresponding letters. Dysgraphia, on the other side, affects writing abilities, leading to problems with letter formation, spacing, and alignment. Coexistence of these disorders complicates diagnosis. This necessitates a nuanced approach that can adapt to these changes and still accurately identify and differentiate between these disorders. This study utilizes advanced geometrical patterns and recurrent neural networks (RNN) to identify handwriting anomalies indicative of dyslexia and dysgraphia. Handwriting standardized features followed by feature extraction that focuses on baseline deviations, letter connectivity, stroke thickness and other anomalies into RNN based autoencoder to identify irregularities. Initial results show the challenge associated with complex pattern adaptation.","sentences":["Dyslexia and dysgraphia are learning disabilities that significantly impact reading, writing, and language processing capabilities.","Dyslexia primarily affects reading, manifesting as difficulties in word recognition and phonological processing, where individuals struggle to connect sounds with corresponding letters.","Dysgraphia, on the other side, affects writing abilities, leading to problems with letter formation, spacing, and alignment.","Coexistence of these disorders complicates diagnosis.","This necessitates a nuanced approach that can adapt to these changes and still accurately identify and differentiate between these disorders.","This study utilizes advanced geometrical patterns and recurrent neural networks (RNN) to identify handwriting anomalies indicative of dyslexia and dysgraphia.","Handwriting standardized features followed by feature extraction that focuses on baseline deviations, letter connectivity, stroke thickness and other anomalies into RNN based autoencoder to identify irregularities.","Initial results show the challenge associated with complex pattern adaptation."],"url":"http://arxiv.org/abs/2405.07238v1","category":"q-bio.QM"}
{"created":"2024-05-12 09:58:03","title":"Adaptive control of recurrent neural networks using conceptors","abstract":"Recurrent Neural Networks excel at predicting and generating complex high-dimensional temporal patterns. Due to their inherent nonlinear dynamics and memory, they can learn unbounded temporal dependencies from data. In a Machine Learning setting, the network's parameters are adapted during a training phase to match the requirements of a given task/problem increasing its computational capabilities. After the training, the network parameters are kept fixed to exploit the learned computations. The static parameters thereby render the network unadaptive to changing conditions, such as external or internal perturbation. In this manuscript, we demonstrate how keeping parts of the network adaptive even after the training enhances its functionality and robustness. Here, we utilize the conceptor framework and conceptualize an adaptive control loop analyzing the network's behavior continuously and adjusting its time-varying internal representation to follow a desired target. We demonstrate how the added adaptivity of the network supports the computational functionality in three distinct tasks: interpolation of temporal patterns, stabilization against partial network degradation, and robustness against input distortion. Our results highlight the potential of adaptive networks in machine learning beyond training, enabling them to not only learn complex patterns but also dynamically adjust to changing environments, ultimately broadening their applicability.","sentences":["Recurrent Neural Networks excel at predicting and generating complex high-dimensional temporal patterns.","Due to their inherent nonlinear dynamics and memory, they can learn unbounded temporal dependencies from data.","In a Machine Learning setting, the network's parameters are adapted during a training phase to match the requirements of a given task/problem increasing its computational capabilities.","After the training, the network parameters are kept fixed to exploit the learned computations.","The static parameters thereby render the network unadaptive to changing conditions, such as external or internal perturbation.","In this manuscript, we demonstrate how keeping parts of the network adaptive even after the training enhances its functionality and robustness.","Here, we utilize the conceptor framework and conceptualize an adaptive control loop analyzing the network's behavior continuously and adjusting its time-varying internal representation to follow a desired target.","We demonstrate how the added adaptivity of the network supports the computational functionality in three distinct tasks: interpolation of temporal patterns, stabilization against partial network degradation, and robustness against input distortion.","Our results highlight the potential of adaptive networks in machine learning beyond training, enabling them to not only learn complex patterns but also dynamically adjust to changing environments, ultimately broadening their applicability."],"url":"http://arxiv.org/abs/2405.07236v1","category":"cs.LG"}
{"created":"2024-05-12 08:28:52","title":"Magnetic-Guided Flexible Origami Robot toward Long-Term Phototherapy of H. pylori in the Stomach","abstract":"Helicobacter pylori, a pervasive bacterial infection associated with gastrointestinal disorders such as gastritis, peptic ulcer disease, and gastric cancer, impacts approximately 50% of the global population. The efficacy of standard clinical eradication therapies is diminishing due to the rise of antibiotic-resistant strains, necessitating alternative treatment strategies. Photodynamic therapy (PDT) emerges as a promising prospect in this context. This study presents the development and implementation of a magnetically-guided origami robot, incorporating flexible printed circuit units for sustained and stable phototherapy of Helicobacter pylori. Each integrated unit is equipped with wireless charging capabilities, producing an optimal power output that can concurrently illuminate up to 15 LEDs at their maximum intensity. Crucially, these units can be remotely manipulated via a magnetic field, facilitating both translational and rotational movements. We propose an open-loop manual control sequence that allows the formation of a stable, compliant triangular structure through the interaction of internal magnets. This adaptable configuration is uniquely designed to withstand the dynamic squeezing environment prevalent in real-world gastric applications. The research herein represents a significant stride in leveraging technology for innovative medical solutions, particularly in the management of antibiotic-resistant Helicobacter pylori infections.","sentences":["Helicobacter pylori, a pervasive bacterial infection associated with gastrointestinal disorders such as gastritis, peptic ulcer disease, and gastric cancer, impacts approximately 50% of the global population.","The efficacy of standard clinical eradication therapies is diminishing due to the rise of antibiotic-resistant strains, necessitating alternative treatment strategies.","Photodynamic therapy (PDT) emerges as a promising prospect in this context.","This study presents the development and implementation of a magnetically-guided origami robot, incorporating flexible printed circuit units for sustained and stable phototherapy of Helicobacter pylori.","Each integrated unit is equipped with wireless charging capabilities, producing an optimal power output that can concurrently illuminate up to 15 LEDs at their maximum intensity.","Crucially, these units can be remotely manipulated via a magnetic field, facilitating both translational and rotational movements.","We propose an open-loop manual control sequence that allows the formation of a stable, compliant triangular structure through the interaction of internal magnets.","This adaptable configuration is uniquely designed to withstand the dynamic squeezing environment prevalent in real-world gastric applications.","The research herein represents a significant stride in leveraging technology for innovative medical solutions, particularly in the management of antibiotic-resistant Helicobacter pylori infections."],"url":"http://arxiv.org/abs/2405.07216v1","category":"eess.SY"}
{"created":"2024-05-12 07:10:26","title":"Adaptive-TMLE for the Average Treatment Effect based on Randomized Controlled Trial Augmented with Real-World Data","abstract":"We consider the problem of estimating the average treatment effect (ATE) when both randomized control trial (RCT) data and real-world data (RWD) are available. We decompose the ATE estimand as the difference between a pooled-ATE estimand that integrates RCT and RWD and a bias estimand that captures the conditional effect of RCT enrollment on the outcome. We introduce an adaptive targeted minimum loss-based estimation (A-TMLE) framework to estimate them. We prove that the A-TMLE estimator is root-n-consistent and asymptotically normal. Moreover, in finite sample, it achieves the super-efficiency one would obtain had one known the oracle model for the conditional effect of the RCT enrollment on the outcome. Consequently, the smaller the working model of the bias induced by the RWD is, the greater our estimator's efficiency, while our estimator will always be at least as efficient as an efficient estimator that uses the RCT data only. A-TMLE outperforms existing methods in simulations by having smaller mean-squared-error and 95% confidence intervals. A-TMLE could help utilize RWD to improve the efficiency of randomized trial results without biasing the estimates of intervention effects. This approach could allow for smaller, faster trials, decreasing the time until patients can receive effective treatments.","sentences":["We consider the problem of estimating the average treatment effect (ATE) when both randomized control trial (RCT) data and real-world data (RWD) are available.","We decompose the ATE estimand as the difference between a pooled-ATE estimand that integrates RCT and RWD and a bias estimand that captures the conditional effect of RCT enrollment on the outcome.","We introduce an adaptive targeted minimum loss-based estimation (A-TMLE) framework to estimate them.","We prove that the A-TMLE estimator is root-n-consistent and asymptotically normal.","Moreover, in finite sample, it achieves the super-efficiency one would obtain had one known the oracle model for the conditional effect of the RCT enrollment on the outcome.","Consequently, the smaller the working model of the bias induced by the RWD is, the greater our estimator's efficiency, while our estimator will always be at least as efficient as an efficient estimator that uses the RCT data only.","A-TMLE outperforms existing methods in simulations by having smaller mean-squared-error and 95% confidence intervals.","A-TMLE could help utilize RWD to improve the efficiency of randomized trial results without biasing the estimates of intervention effects.","This approach could allow for smaller, faster trials, decreasing the time until patients can receive effective treatments."],"url":"http://arxiv.org/abs/2405.07186v1","category":"stat.ME"}
{"created":"2024-05-12 06:16:48","title":"Capacity Maximization for Base Station with Hybrid Fixed and Movable Antennas","abstract":"Six-dimensional movable antenna (6DMA) is an effective solution for enhancing wireless network capacity through the adjustment of both 3D positions and 3D rotations of distributed antennas/antenna surfaces. Although freely positioning/rotating 6DMA surfaces offers the greatest flexibility and thus highest capacity improvement, its implementation may be challenging in practice due to the drastic architecture change required for existing base stations (BSs), which predominantly adopt fixed-position antenna (FPA) arrays (e.g., sector antenna arrays). Thus, we introduce in this letter a new BS architecture called hybrid fixed and movable antennas (HFMA), which consists of both conventional FPA arrays and position/rotation-adjustable 6DMA surfaces. For ease of implementation, we consider that all 6DMA surfaces can rotate along a circular track above the FPA arrays. We aim to maximize the network capacity via optimizing the rotation angles of all 6DMA surfaces based on the users' spatial distribution. Since this problem is combinatorial and its optimal solution requires prohibitively high computational complexity via exhaustive search, we propose an alternative adaptive Markov Chain Monte Carlo based method to solve it more efficiently. Finally, we present simulation results that show significant performance gains achieved by our proposed design over various benchmark schemes.","sentences":["Six-dimensional movable antenna (6DMA) is an effective solution for enhancing wireless network capacity through the adjustment of both 3D positions and 3D rotations of distributed antennas/antenna surfaces.","Although freely positioning/rotating 6DMA surfaces offers the greatest flexibility and thus highest capacity improvement, its implementation may be challenging in practice due to the drastic architecture change required for existing base stations (BSs), which predominantly adopt fixed-position antenna (FPA) arrays (e.g., sector antenna arrays).","Thus, we introduce in this letter a new BS architecture called hybrid fixed and movable antennas (HFMA), which consists of both conventional FPA arrays and position/rotation-adjustable 6DMA surfaces.","For ease of implementation, we consider that all 6DMA surfaces can rotate along a circular track above the FPA arrays.","We aim to maximize the network capacity via optimizing the rotation angles of all 6DMA surfaces based on the users' spatial distribution.","Since this problem is combinatorial and its optimal solution requires prohibitively high computational complexity via exhaustive search, we propose an alternative adaptive Markov Chain Monte Carlo based method to solve it more efficiently.","Finally, we present simulation results that show significant performance gains achieved by our proposed design over various benchmark schemes."],"url":"http://arxiv.org/abs/2405.07176v1","category":"cs.IT"}
{"created":"2024-05-12 06:08:54","title":"On-Demand Model and Client Deployment in Federated Learning with Deep Reinforcement Learning","abstract":"In Federated Learning (FL), the limited accessibility of data from diverse locations and user types poses a significant challenge due to restricted user participation. Expanding client access and diversifying data enhance models by incorporating diverse perspectives, thereby enhancing adaptability. However, challenges arise in dynamic and mobile environments where certain devices may become inaccessible as FL clients, impacting data availability and client selection methods. To address this, we propose an On-Demand solution, deploying new clients using Docker Containers on-the-fly. Our On-Demand solution, employing Deep Reinforcement Learning (DRL), targets client availability and selection, while considering data shifts, and container deployment complexities. It employs an autonomous end-to-end solution for handling model deployment and client selection. The DRL strategy uses a Markov Decision Process (MDP) framework, with a Master Learner and a Joiner Learner. The designed cost functions represent the complexity of the dynamic client deployment and selection. Simulated tests show that our architecture can easily adjust to changes in the environment and respond to On-Demand requests. This underscores its ability to improve client availability, capability, accuracy, and learning efficiency, surpassing heuristic and tabular reinforcement learning solutions.","sentences":["In Federated Learning (FL), the limited accessibility of data from diverse locations and user types poses a significant challenge due to restricted user participation.","Expanding client access and diversifying data enhance models by incorporating diverse perspectives, thereby enhancing adaptability.","However, challenges arise in dynamic and mobile environments where certain devices may become inaccessible as FL clients, impacting data availability and client selection methods.","To address this, we propose an On-Demand solution, deploying new clients using Docker Containers on-the-fly.","Our On-Demand solution, employing Deep Reinforcement Learning (DRL), targets client availability and selection, while considering data shifts, and container deployment complexities.","It employs an autonomous end-to-end solution for handling model deployment and client selection.","The DRL strategy uses a Markov Decision Process (MDP) framework, with a Master Learner and a Joiner Learner.","The designed cost functions represent the complexity of the dynamic client deployment and selection.","Simulated tests show that our architecture can easily adjust to changes in the environment and respond to On-Demand requests.","This underscores its ability to improve client availability, capability, accuracy, and learning efficiency, surpassing heuristic and tabular reinforcement learning solutions."],"url":"http://arxiv.org/abs/2405.07175v1","category":"cs.LG"}
{"created":"2024-05-12 06:02:09","title":"Observability and Incident Response in Managed Serverless Environments Using Ontology-Based Log Monitoring","abstract":"In a fully managed serverless environment, the cloud service provider is responsible for securing the cloud infrastructure, thereby reducing the operational and maintenance efforts of application developers. However, this environment limits the use of existing cybersecurity frameworks and tools, which reduces observability and situational awareness capabilities (e.g., risk assessment, incident response). In addition, existing security frameworks for serverless applications do not generalize well to all application architectures and usually require adaptation, specialized expertise, etc. for use in fully managed serverless environments. In this paper, we introduce a three-layer security scheme for applications deployed in fully managed serverless environments. The first two layers involve a unique ontology based solely on serverless logs which is used to transform them into a unified application activity knowledge graph. In the third layer, we address the need for observability and situational awareness capabilities by implementing two situational awareness tools that utilizes the graph-based representation: 1) An incident response dashboard that leverages the ontology to visualize and examine application activity logs in the context of cybersecurity alerts. Our user study showed that the dashboard enabled participants to respond more accurately and quickly to new security alerts than the baseline tool. 2) A criticality of asset (CoA) risk assessment framework that enables efficient expert-based prioritization in cybersecurity contexts.","sentences":["In a fully managed serverless environment, the cloud service provider is responsible for securing the cloud infrastructure, thereby reducing the operational and maintenance efforts of application developers.","However, this environment limits the use of existing cybersecurity frameworks and tools, which reduces observability and situational awareness capabilities (e.g., risk assessment, incident response).","In addition, existing security frameworks for serverless applications do not generalize well to all application architectures and usually require adaptation, specialized expertise, etc. for use in fully managed serverless environments.","In this paper, we introduce a three-layer security scheme for applications deployed in fully managed serverless environments.","The first two layers involve a unique ontology based solely on serverless logs which is used to transform them into a unified application activity knowledge graph.","In the third layer, we address the need for observability and situational awareness capabilities by implementing two situational awareness tools that utilizes the graph-based representation: 1) An incident response dashboard that leverages the ontology to visualize and examine application activity logs in the context of cybersecurity alerts.","Our user study showed that the dashboard enabled participants to respond more accurately and quickly to new security alerts than the baseline tool.","2) A criticality of asset (CoA) risk assessment framework that enables efficient expert-based prioritization in cybersecurity contexts."],"url":"http://arxiv.org/abs/2405.07172v1","category":"cs.CR"}
{"created":"2024-05-12 05:57:37","title":"Enhanced Online Test-time Adaptation with Feature-Weight Cosine Alignment","abstract":"Online Test-Time Adaptation (OTTA) has emerged as an effective strategy to handle distributional shifts, allowing on-the-fly adaptation of pre-trained models to new target domains during inference, without the need for source data. We uncovered that the widely studied entropy minimization (EM) method for OTTA, suffers from noisy gradients due to ambiguity near decision boundaries and incorrect low-entropy predictions. To overcome these limitations, this paper introduces a novel cosine alignment optimization approach with a dual-objective loss function that refines the precision of class predictions and adaptability to novel domains. Specifically, our method optimizes the cosine similarity between feature vectors and class weight vectors, enhancing the precision of class predictions and the model's adaptability to novel domains. Our method outperforms state-of-the-art techniques and sets a new benchmark in multiple datasets, including CIFAR-10-C, CIFAR-100-C, ImageNet-C, Office-Home, and DomainNet datasets, demonstrating high accuracy and robustness against diverse corruptions and domain shifts.","sentences":["Online Test-Time Adaptation (OTTA) has emerged as an effective strategy to handle distributional shifts, allowing on-the-fly adaptation of pre-trained models to new target domains during inference, without the need for source data.","We uncovered that the widely studied entropy minimization (EM) method for OTTA, suffers from noisy gradients due to ambiguity near decision boundaries and incorrect low-entropy predictions.","To overcome these limitations, this paper introduces a novel cosine alignment optimization approach with a dual-objective loss function that refines the precision of class predictions and adaptability to novel domains.","Specifically, our method optimizes the cosine similarity between feature vectors and class weight vectors, enhancing the precision of class predictions and the model's adaptability to novel domains.","Our method outperforms state-of-the-art techniques and sets a new benchmark in multiple datasets, including CIFAR-10-C, CIFAR-100-C, ImageNet-C, Office-Home, and DomainNet datasets, demonstrating high accuracy and robustness against diverse corruptions and domain shifts."],"url":"http://arxiv.org/abs/2405.07171v1","category":"cs.CV"}
{"created":"2024-05-12 04:54:27","title":"Convective meta-thermal dispersion for self-adaptive cooling enhancement","abstract":"Improving the heat transfer coefficient is crucial across various energy utilization processes for maintaining device safety and stability with high energy efficiency. However, in scenarios with limited heat capacity flow rates, increasing the thermal conductivity of encapsulated internal heat source (IHS) packaging can paradoxically impede heat transfer. Herein, we introduced a convective-meta thermal dispersion (CMTD) strategy applicable throughout the energy domain. By integrating low thermal conductivity materials into high thermal conductivity package structures, we disrupted tangential heat flow while preserving efficient radial heat transport. Through this approach, a notable reduction in tangential temperature within the fluid channel was achieved, effectively lowering the IHS temperature. Remarkably, this cooling mechanism does not need additional energy input, thermal property enhancements, or expanded heat transfer areas, which are often prerequisites in existing technologies. Moreover, spontaneous enhancement phenomena emerged under constrained heat transfer conditions, termed self-adaptive cooling enhancement. Our investigations revealed, under steady-state conditions, a maximum 24.5% decrease in IHS average temperature, while transient conditions exhibited a maximum 32.3% increase in heat transfer between the IHS and cooling fluid, validating the efficacy of the CMTD strategy. These findings offer a promising pathway for efficient thermal management in various thermal energy utilization fields with high power density such as nuclear fission and fusion and contributed to a deeper understanding of fundamental fluid-solid heat transfer mechanisms across the energy science.","sentences":["Improving the heat transfer coefficient is crucial across various energy utilization processes for maintaining device safety and stability with high energy efficiency.","However, in scenarios with limited heat capacity flow rates, increasing the thermal conductivity of encapsulated internal heat source (IHS) packaging can paradoxically impede heat transfer.","Herein, we introduced a convective-meta thermal dispersion (CMTD) strategy applicable throughout the energy domain.","By integrating low thermal conductivity materials into high thermal conductivity package structures, we disrupted tangential heat flow while preserving efficient radial heat transport.","Through this approach, a notable reduction in tangential temperature within the fluid channel was achieved, effectively lowering the IHS temperature.","Remarkably, this cooling mechanism does not need additional energy input, thermal property enhancements, or expanded heat transfer areas, which are often prerequisites in existing technologies.","Moreover, spontaneous enhancement phenomena emerged under constrained heat transfer conditions, termed self-adaptive cooling enhancement.","Our investigations revealed, under steady-state conditions, a maximum 24.5% decrease in IHS average temperature, while transient conditions exhibited a maximum 32.3% increase in heat transfer between the IHS and cooling fluid, validating the efficacy of the CMTD strategy.","These findings offer a promising pathway for efficient thermal management in various thermal energy utilization fields with high power density such as nuclear fission and fusion and contributed to a deeper understanding of fundamental fluid-solid heat transfer mechanisms across the energy science."],"url":"http://arxiv.org/abs/2405.07161v1","category":"physics.app-ph"}
{"created":"2024-05-12 04:18:10","title":"Enhancing Multi-modal Learning: Meta-learned Cross-modal Knowledge Distillation for Handling Missing Modalities","abstract":"In multi-modal learning, some modalities are more influential than others, and their absence can have a significant impact on classification/segmentation accuracy. Hence, an important research question is if it is possible for trained multi-modal models to have high accuracy even when influential modalities are absent from the input data. In this paper, we propose a novel approach called Meta-learned Cross-modal Knowledge Distillation (MCKD) to address this research question. MCKD adaptively estimates the importance weight of each modality through a meta-learning process. These dynamically learned modality importance weights are used in a pairwise cross-modal knowledge distillation process to transfer the knowledge from the modalities with higher importance weight to the modalities with lower importance weight. This cross-modal knowledge distillation produces a highly accurate model even with the absence of influential modalities. Differently from previous methods in the field, our approach is designed to work in multiple tasks (e.g., segmentation and classification) with minimal adaptation. Experimental results on the Brain tumor Segmentation Dataset 2018 (BraTS2018) and the Audiovision-MNIST classification dataset demonstrate the superiority of MCKD over current state-of-the-art models. Particularly in BraTS2018, we achieve substantial improvements of 3.51\\% for enhancing tumor, 2.19\\% for tumor core, and 1.14\\% for the whole tumor in terms of average segmentation Dice score.","sentences":["In multi-modal learning, some modalities are more influential than others, and their absence can have a significant impact on classification/segmentation accuracy.","Hence, an important research question is if it is possible for trained multi-modal models to have high accuracy even when influential modalities are absent from the input data.","In this paper, we propose a novel approach called Meta-learned Cross-modal Knowledge Distillation (MCKD) to address this research question.","MCKD adaptively estimates the importance weight of each modality through a meta-learning process.","These dynamically learned modality importance weights are used in a pairwise cross-modal knowledge distillation process to transfer the knowledge from the modalities with higher importance weight to the modalities with lower importance weight.","This cross-modal knowledge distillation produces a highly accurate model even with the absence of influential modalities.","Differently from previous methods in the field, our approach is designed to work in multiple tasks (e.g., segmentation and classification) with minimal adaptation.","Experimental results on the Brain tumor Segmentation Dataset 2018 (BraTS2018) and the Audiovision-MNIST classification dataset demonstrate the superiority of MCKD over current state-of-the-art models.","Particularly in BraTS2018, we achieve substantial improvements of 3.51\\% for enhancing tumor, 2.19\\% for tumor core, and 1.14\\% for the whole tumor in terms of average segmentation Dice score."],"url":"http://arxiv.org/abs/2405.07155v1","category":"cs.CV"}
{"created":"2024-05-12 03:44:00","title":"Asymptotic behavior for the fast diffusion equation with absorption and singularity","abstract":"This paper is concerned with the weak solution for the fast diffusion equation with absorption and singularity in the form of $u_t=\\triangle u^m -u^p$. We first prove the existence and decay estimate of weak solution when the fast diffusion index satisfies $0<m<1$ and the absorption index is $p>1$. Then we show the asymptotic convergence of weak solution to the corresponding Barenblatt solution for $\\frac{n-1}{n}<m<1$ and $p>m+\\frac{2}{n}$ via the entropy dissipation method combining the generalized Shannon's inequality and Csisz$\\mathrm{\\acute{a}}$r-Kullback inequality. The singularity of spatial diffusion causes us the technical challenges for the asymptotic behavior of weak solution.","sentences":["This paper is concerned with the weak solution for the fast diffusion equation with absorption and singularity in the form of $u_t=\\triangle u^m -u^p$.","We first prove the existence and decay estimate of weak solution when the fast diffusion index satisfies $0<m<1$ and the absorption index is $p>1$. Then we show the asymptotic convergence of weak solution to the corresponding Barenblatt solution for $\\frac{n-1}{n}<m<1$ and $p>m+\\frac{2}{n}$ via the entropy dissipation method combining the generalized Shannon's inequality and Csisz$\\mathrm{\\acute{a}}$r-Kullback inequality.","The singularity of spatial diffusion causes us the technical challenges for the asymptotic behavior of weak solution."],"url":"http://arxiv.org/abs/2405.07150v1","category":"math.AP"}
{"created":"2024-05-12 03:34:01","title":"Investigate the efficiency of incompressible flow simulations on CPUs and GPUs with BSAMR","abstract":"Adaptive mesh refinement (AMR) is a classical technique about local refinement in space where needed, thus effectively reducing computational costs for HPC-based physics simulations. Although AMR has been used for many years, little reproducible research discusses the impact of software-based parameters on block-structured AMR (BSAMR) efficiency and how to choose them. This article primarily does parametric studies to investigate the computational efficiency of incompressible flows on a block-structured adaptive mesh. The parameters include refining block size, refining frequency, maximum level, and cycling method. A new projection skipping (PS) method is proposed, which brings insights about when and where the projections on coarser levels are safe to be omitted. We conduct extensive tests on different CPUs/GPUs for various 2D/3D incompressible flow cases, including bubble, RT instability, Taylor Green vortex, etc. Several valuable empirical conclusions are obtained to help guide simulations with BSAMR. Codes and all profiling data are available on GitHub.","sentences":["Adaptive mesh refinement (AMR) is a classical technique about local refinement in space where needed, thus effectively reducing computational costs for HPC-based physics simulations.","Although AMR has been used for many years, little reproducible research discusses the impact of software-based parameters on block-structured AMR (BSAMR) efficiency and how to choose them.","This article primarily does parametric studies to investigate the computational efficiency of incompressible flows on a block-structured adaptive mesh.","The parameters include refining block size, refining frequency, maximum level, and cycling method.","A new projection skipping (PS) method is proposed, which brings insights about when and where the projections on coarser levels are safe to be omitted.","We conduct extensive tests on different CPUs/GPUs for various 2D/3D incompressible flow cases, including bubble, RT instability, Taylor Green vortex, etc.","Several valuable empirical conclusions are obtained to help guide simulations with BSAMR.","Codes and all profiling data are available on GitHub."],"url":"http://arxiv.org/abs/2405.07148v1","category":"physics.flu-dyn"}
{"created":"2024-05-12 03:31:27","title":"Randomized algorithms for computing the tensor train approximation and their applications","abstract":"In this paper, we focus on the fixed TT-rank and precision problems of finding an approximation of the tensor train (TT) decomposition of a tensor. Note that the TT-SVD and TT-cross are two well-known algorithms for these two problems. Firstly, by combining the random projection technique with the power scheme, we obtain two types of randomized algorithms for the fixed TT-rank problem. Secondly, by using the non-asymptotic theory of sub-random Gaussian matrices, we derive the upper bounds of the proposed randomized algorithms. Thirdly, we deduce a new deterministic strategy to estimate the desired TT-rank with a given tolerance and another adaptive randomized algorithm that finds a low TT-rank representation satisfying a given tolerance, and is beneficial when the target TT-rank is not known in advance. We finally illustrate the accuracy of the proposed algorithms via some test tensors from synthetic and real databases. In particular, for the fixed TT-rank problem, the proposed algorithms can be several times faster than the TT-SVD, and the accuracy of the proposed algorithms and the TT-SVD are comparable for several test tensors.","sentences":["In this paper, we focus on the fixed TT-rank and precision problems of finding an approximation of the tensor train (TT) decomposition of a tensor.","Note that the TT-SVD and TT-cross are two well-known algorithms for these two problems.","Firstly, by combining the random projection technique with the power scheme, we obtain two types of randomized algorithms for the fixed TT-rank problem.","Secondly, by using the non-asymptotic theory of sub-random Gaussian matrices, we derive the upper bounds of the proposed randomized algorithms.","Thirdly, we deduce a new deterministic strategy to estimate the desired TT-rank with a given tolerance and another adaptive randomized algorithm that finds a low TT-rank representation satisfying a given tolerance, and is beneficial when the target TT-rank is not known in advance.","We finally illustrate the accuracy of the proposed algorithms via some test tensors from synthetic and real databases.","In particular, for the fixed TT-rank problem, the proposed algorithms can be several times faster than the TT-SVD, and the accuracy of the proposed algorithms and the TT-SVD are comparable for several test tensors."],"url":"http://arxiv.org/abs/2405.07147v1","category":"math.NA"}
{"created":"2024-05-12 03:17:21","title":"TRAIL: Cross-Shard Validation for Cryptocurrency Byzantine Shard Protection","abstract":"We present TRAIL: an algorithm that uses a novel consensus procedure to tolerate failed or malicious shards within a blockchain-based cryptocurrency. Our algorithm takes a new approach of selecting validator shards for each transaction from those that previously held the assets being transferred. This approach ensures the algorithm's robustness and efficiency. TRAIL is presented using PBFT for internal shard transaction processing and a modified version of PBFT for external cross-shard validation. We describe TRAIL, prove it correct, analyze its message complexity, and evaluate its performance. We propose various TRAIL optimizations: we describe how it can be adapted to other Byzantine-tolerant consensus algorithms, how a complete system may be built on the basis of it, and how TRAIL can be applied to existing and future sharded blockchains.","sentences":["We present TRAIL: an algorithm that uses a novel consensus procedure to tolerate failed or malicious shards within a blockchain-based cryptocurrency.","Our algorithm takes a new approach of selecting validator shards for each transaction from those that previously held the assets being transferred.","This approach ensures the algorithm's robustness and efficiency.","TRAIL is presented using PBFT for internal shard transaction processing and a modified version of PBFT for external cross-shard validation.","We describe TRAIL, prove it correct, analyze its message complexity, and evaluate its performance.","We propose various TRAIL optimizations: we describe how it can be adapted to other Byzantine-tolerant consensus algorithms, how a complete system may be built on the basis of it, and how TRAIL can be applied to existing and future sharded blockchains."],"url":"http://arxiv.org/abs/2405.07146v1","category":"cs.DC"}
{"created":"2024-05-12 02:02:43","title":"Oscillating-mode gap: an indicator of phase transition in open quantum many-body systems","abstract":"It presents a significant challenge to elucidate the relationship between the phases of open quantum many-body systems and the spectral structure of their governing Liouvillian, which determines how the density matrix evolves. Previous studies have focused on the Liouvillian gap, defined as the decay rate of the most slowly-decaying mode, as a key indicator of dissipative phase transition, noting its closure in symmetry-broken phases and opening in disordered phases. In this work, we propose an additional spectral gap, termed the oscillating-mode gap, defined as the decay rate of the most slowly-decaying oscillating mode. Through the analysis of a prototype dissipative boson system, we demonstrate the necessity of both the Liouvillian gap and the oscillating-mode gap for the comprehensive characterization of the system's phases and the transitions between them.","sentences":["It presents a significant challenge to elucidate the relationship between the phases of open quantum many-body systems and the spectral structure of their governing Liouvillian, which determines how the density matrix evolves.","Previous studies have focused on the Liouvillian gap, defined as the decay rate of the most slowly-decaying mode, as a key indicator of dissipative phase transition, noting its closure in symmetry-broken phases and opening in disordered phases.","In this work, we propose an additional spectral gap, termed the oscillating-mode gap, defined as the decay rate of the most slowly-decaying oscillating mode.","Through the analysis of a prototype dissipative boson system, we demonstrate the necessity of both the Liouvillian gap and the oscillating-mode gap for the comprehensive characterization of the system's phases and the transitions between them."],"url":"http://arxiv.org/abs/2405.07132v1","category":"quant-ph"}
{"created":"2024-05-13 17:14:06","title":"Compact moduli of Calabi-Yau cones and Sasaki-Einstein spaces","abstract":"We construct proper moduli algebraic spaces of K-polystable $\\mathbb{Q}$-Fano cones (a.k.a. Calabi-Yau cones) or equivalently their links i.e., Sasaki-Einstein manifolds with singularities.   As a byproduct, it gives alternative algebraic construction of proper K-moduli of $\\mathbb{Q}$-Fano varieties. In contrast to the previous algebraic proof of its properness ([BHLLX, LXZ]), we do not use the $\\delta$-invariants ([FO, BJ]) nor the $L^2$-normalized Donaldson-Futaki invariants. We use the local normalized volume of [Li] and the higher $\\Theta$-stable reduction instead.","sentences":["We construct proper moduli algebraic spaces of K-polystable $\\mathbb{Q}$-Fano cones (a.k.a. Calabi-Yau cones) or equivalently their links i.e., Sasaki-Einstein manifolds with singularities.   ","As a byproduct, it gives alternative algebraic construction of proper K-moduli of $\\mathbb{Q}$-Fano varieties.","In contrast to the previous algebraic proof of its properness ([BHLLX, LXZ]), we do not use the $\\delta$-invariants ([FO, BJ]) nor the $L^2$-normalized Donaldson-Futaki invariants.","We use the local normalized volume of [Li] and the higher $\\Theta$-stable reduction instead."],"url":"http://arxiv.org/abs/2405.07939v1","category":"math.AG"}
{"created":"2024-05-13 16:42:55","title":"A Unification of Exchangeability and Continuous Exposure and Confounder Measurement Errors: Probabilistic Exchangeability","abstract":"Exchangeability concerning a continuous exposure, X, implies no confounding bias when identifying average exposure effects of X, AEE(X). When X is measured with error (Xep), two challenges arise in identifying AEE(X). Firstly, exchangeability regarding Xep does not equal exchangeability regarding X. Secondly, the necessity of the non-differential error assumption (NDEA), overly stringent in practice, remains uncertain. To address them, this article proposes unifying exchangeability and exposure and confounder measurement errors with three novel concepts. The first, Probabilistic Exchangeability (PE), states that the outcomes of those with Xep=e are probabilistically exchangeable with the outcomes of those truly exposed to X=eT. The relationship between AEE(Xep) and AEE(X) in risk difference and ratio scales is mathematically expressed as a probabilistic certainty, termed exchangeability probability (Pe). Squared Pe (Pe.sq) quantifies the extent to which AEE(Xep) differs from AEE(X) due to exposure measurement error through mechanisms not akin to confounding mechanisms. The coefficient of determination (R.sq) in the regression of X against Xep may sometimes be sufficient to measure Pe.sq. The second concept, Emergent Pseudo Confounding (EPC), describes the bias introduced by exposure measurement error through mechanisms akin to confounding mechanisms. PE can hold when EPC is controlled for, which is weaker than NDEA. The third, Emergent Confounding, describes when bias due to confounder measurement error arises. Adjustment for E(P)C can be performed like confounding adjustment to ensure PE. This paper provides formal justifications for using AEE(Xep) and maximum insight into potential divergence of AEE(Xep) from AEE(X) and how to measure it.","sentences":["Exchangeability concerning a continuous exposure, X, implies no confounding bias when identifying average exposure effects of X, AEE(X).","When X is measured with error (Xep), two challenges arise in identifying AEE(X).","Firstly, exchangeability regarding Xep does not equal exchangeability regarding X. Secondly, the necessity of the non-differential error assumption (NDEA), overly stringent in practice, remains uncertain.","To address them, this article proposes unifying exchangeability and exposure and confounder measurement errors with three novel concepts.","The first, Probabilistic Exchangeability (PE), states that the outcomes of those with Xep=e are probabilistically exchangeable with the outcomes of those truly exposed to X=eT. The relationship between AEE(Xep) and AEE(X) in risk difference and ratio scales is mathematically expressed as a probabilistic certainty, termed exchangeability probability (Pe).","Squared Pe (Pe.sq) quantifies the extent to which AEE(Xep) differs from AEE(X) due to exposure measurement error through mechanisms not akin to confounding mechanisms.","The coefficient of determination (R.sq) in the regression of X against Xep may sometimes be sufficient to measure Pe.sq.","The second concept, Emergent Pseudo Confounding (EPC), describes the bias introduced by exposure measurement error through mechanisms akin to confounding mechanisms.","PE can hold when EPC is controlled for, which is weaker than NDEA.","The third, Emergent Confounding, describes when bias due to confounder measurement error arises.","Adjustment for E(P)C can be performed like confounding adjustment to ensure PE.","This paper provides formal justifications for using AEE(Xep) and maximum insight into potential divergence of AEE(Xep) from AEE(X) and how to measure it."],"url":"http://arxiv.org/abs/2405.07910v2","category":"math.ST"}
{"created":"2024-05-13 16:27:12","title":"All Nodes are created Not Equal: Node-Specific Layer Aggregation and Filtration for GNN","abstract":"The ever-designed Graph Neural Networks, though opening a promising path for the modeling of the graph-structure data, unfortunately introduce two daunting obstacles to their deployment on devices. (I) Most of existing GNNs are shallow, due mostly to the over-smoothing and gradient-vanish problem as they go deeper as convolutional architectures. (II) The vast majority of GNNs adhere to the homophily assumption, where the central node and its adjacent nodes share the same label. This assumption often poses challenges for many GNNs working with heterophilic graphs. Addressing the aforementioned issue has become a looming challenge in enhancing the robustness and scalability of GNN applications. In this paper, we take a comprehensive and systematic approach to overcoming the two aforementioned challenges for the first time. We propose a Node-Specific Layer Aggregation and Filtration architecture, termed NoSAF, a framework capable of filtering and processing information from each individual nodes. NoSAF introduces the concept of \"All Nodes are Created Not Equal\" into every layer of deep networks, aiming to provide a reliable information filter for each layer's nodes to sieve out information beneficial for the subsequent layer. By incorporating a dynamically updated codebank, NoSAF dynamically optimizes the optimal information outputted downwards at each layer. This effectively overcomes heterophilic issues and aids in deepening the network. To compensate for the information loss caused by the continuous filtering in NoSAF, we also propose NoSAF-D (Deep), which incorporates a compensation mechanism that replenishes information in every layer of the model, allowing NoSAF to perform meaningful computations even in very deep layers.","sentences":["The ever-designed Graph Neural Networks, though opening a promising path for the modeling of the graph-structure data, unfortunately introduce two daunting obstacles to their deployment on devices.","(I) Most of existing GNNs are shallow, due mostly to the over-smoothing and gradient-vanish problem as they go deeper as convolutional architectures.","(II)","The vast majority of GNNs adhere to the homophily assumption, where the central node and its adjacent nodes share the same label.","This assumption often poses challenges for many GNNs working with heterophilic graphs.","Addressing the aforementioned issue has become a looming challenge in enhancing the robustness and scalability of GNN applications.","In this paper, we take a comprehensive and systematic approach to overcoming the two aforementioned challenges for the first time.","We propose a Node-Specific Layer Aggregation and Filtration architecture, termed NoSAF, a framework capable of filtering and processing information from each individual nodes.","NoSAF introduces the concept of \"All Nodes are Created Not Equal\" into every layer of deep networks, aiming to provide a reliable information filter for each layer's nodes to sieve out information beneficial for the subsequent layer.","By incorporating a dynamically updated codebank, NoSAF dynamically optimizes the optimal information outputted downwards at each layer.","This effectively overcomes heterophilic issues and aids in deepening the network.","To compensate for the information loss caused by the continuous filtering in NoSAF, we also propose NoSAF-D (Deep), which incorporates a compensation mechanism that replenishes information in every layer of the model, allowing NoSAF to perform meaningful computations even in very deep layers."],"url":"http://arxiv.org/abs/2405.07892v1","category":"cs.LG"}
{"created":"2024-05-13 16:24:53","title":"The fermionic massless modular Hamiltonian","abstract":"We provide an explicit expression for the modular hamiltonian of the von Neumann algebras associated to the unit double cone for the (fermionic) quantum field theories of the 2-component Weyl (helicity 1/2) field, and of the 4-component massless Dirac and Majorana fields. To this end, we represent the one particle spaces of these theories in terms of solutions of the corresponding wave equations, and obtain the action of the modular group on them. As an application, we compute the relative entropy between the vacuum of the massless Majorana field and one particle states associated to waves with Cauchy data localized in the spatial unit ball.","sentences":["We provide an explicit expression for the modular hamiltonian of the von Neumann algebras associated to the unit double cone for the (fermionic) quantum field theories of the 2-component Weyl (helicity 1/2) field, and of the 4-component massless Dirac and Majorana fields.","To this end, we represent the one particle spaces of these theories in terms of solutions of the corresponding wave equations, and obtain the action of the modular group on them.","As an application, we compute the relative entropy between the vacuum of the massless Majorana field and one particle states associated to waves with Cauchy data localized in the spatial unit ball."],"url":"http://arxiv.org/abs/2405.07888v1","category":"math-ph"}
{"created":"2024-05-13 15:57:27","title":"Enhancing Clinically Significant Prostate Cancer Prediction in T2-weighted Images through Transfer Learning from Breast Cancer","abstract":"In 2020, prostate cancer saw a staggering 1.4 million new cases, resulting in over 375,000 deaths. The accurate identification of clinically significant prostate cancer is crucial for delivering effective treatment to patients. Consequently, there has been a surge in research exploring the application of deep neural networks to predict clinical significance based on magnetic resonance images. However, these networks demand extensive datasets to attain optimal performance. Recently, transfer learning emerged as a technique that leverages acquired features from a domain with richer data to enhance the performance of a domain with limited data. In this paper, we investigate the improvement of clinically significant prostate cancer prediction in T2-weighted images through transfer learning from breast cancer. The results demonstrate a remarkable improvement of over 30% in leave-one-out cross-validation accuracy.","sentences":["In 2020, prostate cancer saw a staggering 1.4 million new cases, resulting in over 375,000 deaths.","The accurate identification of clinically significant prostate cancer is crucial for delivering effective treatment to patients.","Consequently, there has been a surge in research exploring the application of deep neural networks to predict clinical significance based on magnetic resonance images.","However, these networks demand extensive datasets to attain optimal performance.","Recently, transfer learning emerged as a technique that leverages acquired features from a domain with richer data to enhance the performance of a domain with limited data.","In this paper, we investigate the improvement of clinically significant prostate cancer prediction in T2-weighted images through transfer learning from breast cancer.","The results demonstrate a remarkable improvement of over 30% in leave-one-out cross-validation accuracy."],"url":"http://arxiv.org/abs/2405.07869v1","category":"eess.IV"}
{"created":"2024-05-13 15:55:57","title":"Phonon Assisted Exciton Processes in Two-Dimensional Tungsten Monocarbide","abstract":"n this study, we utilize a rigorous ab initio-based finite momentum Bethe-Salpeter equation to investigate the photoluminescence emission in two-dimensional hexagonal tungsten carbide (h-WC). This thermodynamically stable monolayer exhibits an indirect optical gap, resulting in phonon-assisted emission. We observe that light absorption is a direct process centered around the direct quasiparticle gap, while light emission is indirect and requires modes between $\\Gamma$-$M$ in the phonon dispersion. The emission lines feature prominent phonon replicas at cryogenic temperatures, particularly near-infrared wavelengths (1.09 and 1.17 eV), and we observe exciton thermalization with the crystal beyond 25 K. Additionally, non-radiative recombination is a remarkably fast process, occurring at order of a few femtoseconds (4.8 fs at 0 K and 2.8 fs at 300 K) compared to radiative recombination (2.3 ps at 0 K and 214 ns at 300 K). These optical characteristics of 2D h-WC may facilitate the promise of photon-emitter devices for near-infrared signal communication.","sentences":["n this study, we utilize a rigorous ab initio-based finite momentum Bethe-Salpeter equation to investigate the photoluminescence emission in two-dimensional hexagonal tungsten carbide (h-WC).","This thermodynamically stable monolayer exhibits an indirect optical gap, resulting in phonon-assisted emission.","We observe that light absorption is a direct process centered around the direct quasiparticle gap, while light emission is indirect and requires modes between $\\Gamma$-$M$ in the phonon dispersion.","The emission lines feature prominent phonon replicas at cryogenic temperatures, particularly near-infrared wavelengths (1.09 and 1.17 eV), and we observe exciton thermalization with the crystal beyond 25 K. Additionally, non-radiative recombination is a remarkably fast process, occurring at order of a few femtoseconds (4.8 fs at 0 K and 2.8 fs at 300 K) compared to radiative recombination (2.3 ps at 0 K and 214 ns at 300 K).","These optical characteristics of 2D h-WC may facilitate the promise of photon-emitter devices for near-infrared signal communication."],"url":"http://arxiv.org/abs/2405.07867v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 15:36:04","title":"SceneFactory: A Workflow-centric and Unified Framework for Incremental Scene Modeling","abstract":"We present SceneFactory, a workflow-centric and unified framework for incremental scene modeling, that supports conveniently a wide range of applications, such as (unposed and/or uncalibrated) multi-view depth estimation, LiDAR completion, (dense) RGB-D/RGB-L/Mono//Depth-only reconstruction and SLAM. The workflow-centric design uses multiple blocks as the basis for building different production lines. The supported applications, i.e., productions avoid redundancy in their designs. Thus, the focus is on each block itself for independent expansion. To support all input combinations, our implementation consists of four building blocks in SceneFactory: (1) Mono-SLAM, (2) depth estimation, (3) flexion and (4) scene reconstruction. Furthermore, we propose an unposed & uncalibrated multi-view depth estimation model (U2-MVD) to estimate dense geometry. U2-MVD exploits dense bundle adjustment for solving for poses, intrinsics, and inverse depth. Then a semantic-awared ScaleCov step is introduced to complete the multi-view depth. Relying on U2-MVD, SceneFactory both supports user-friendly 3D creation (with just images) and bridges the applications of Dense RGB-D and Dense Mono. For high quality surface and color reconstruction, we propose due-purpose Multi-resolutional Neural Points (DM-NPs) for the first surface accessible Surface Color Field design, where we introduce Improved Point Rasterization (IPR) for point cloud based surface query.   We implement and experiment with SceneFactory to demonstrate its broad practicability and high flexibility. Its quality also competes or exceeds the tightly-coupled state of the art approaches in all tasks. We contribute the code to the community (https://jarrome.github.io/).","sentences":["We present SceneFactory, a workflow-centric and unified framework for incremental scene modeling, that supports conveniently a wide range of applications, such as (unposed and/or uncalibrated) multi-view depth estimation, LiDAR completion, (dense) RGB-D/RGB-L/Mono//Depth-only reconstruction and SLAM.","The workflow-centric design uses multiple blocks as the basis for building different production lines.","The supported applications, i.e., productions avoid redundancy in their designs.","Thus, the focus is on each block itself for independent expansion.","To support all input combinations, our implementation consists of four building blocks in SceneFactory: (1) Mono-SLAM, (2) depth estimation, (3) flexion and (4) scene reconstruction.","Furthermore, we propose an unposed & uncalibrated multi-view depth estimation model (U2-MVD) to estimate dense geometry.","U2-MVD exploits dense bundle adjustment for solving for poses, intrinsics, and inverse depth.","Then a semantic-awared ScaleCov step is introduced to complete the multi-view depth.","Relying on U2-MVD, SceneFactory both supports user-friendly 3D creation (with just images) and bridges the applications of Dense RGB-D and Dense Mono.","For high quality surface and color reconstruction, we propose due-purpose Multi-resolutional Neural Points (DM-NPs) for the first surface accessible Surface Color Field design, where we introduce Improved Point Rasterization (IPR) for point cloud based surface query.   ","We implement and experiment with SceneFactory to demonstrate its broad practicability and high flexibility.","Its quality also competes or exceeds the tightly-coupled state of the art approaches in all tasks.","We contribute the code to the community (https://jarrome.github.io/)."],"url":"http://arxiv.org/abs/2405.07847v1","category":"cs.CV"}
{"created":"2024-05-13 15:25:11","title":"Open-vocabulary Auditory Neural Decoding Using fMRI-prompted LLM","abstract":"Decoding language information from brain signals represents a vital research area within brain-computer interfaces, particularly in the context of deciphering the semantic information from the fMRI signal. However, many existing efforts concentrate on decoding small vocabulary sets, leaving space for the exploration of open vocabulary continuous text decoding. In this paper, we introduce a novel method, the \\textbf{Brain Prompt GPT (BP-GPT)}. By using the brain representation that is extracted from the fMRI as a prompt, our method can utilize GPT-2 to decode fMRI signals into stimulus text. Further, we introduce a text-to-text baseline and align the fMRI prompt to the text prompt. By introducing the text-to-text baseline, our BP-GPT can extract a more robust brain prompt and promote the decoding of pre-trained LLM. We evaluate our BP-GPT on the open-source auditory semantic decoding dataset and achieve a significant improvement up to $4.61\\%$ on METEOR and $2.43\\%$ on BERTScore across all the subjects compared to the state-of-the-art method. The experimental results demonstrate that using brain representation as a prompt to further drive LLM for auditory neural decoding is feasible and effective.","sentences":["Decoding language information from brain signals represents a vital research area within brain-computer interfaces, particularly in the context of deciphering the semantic information from the fMRI signal.","However, many existing efforts concentrate on decoding small vocabulary sets, leaving space for the exploration of open vocabulary continuous text decoding.","In this paper, we introduce a novel method, the \\textbf{Brain Prompt GPT (BP-GPT)}.","By using the brain representation that is extracted from the fMRI as a prompt, our method can utilize GPT-2 to decode fMRI signals into stimulus text.","Further, we introduce a text-to-text baseline and align the fMRI prompt to the text prompt.","By introducing the text-to-text baseline, our BP-GPT can extract a more robust brain prompt and promote the decoding of pre-trained LLM.","We evaluate our BP-GPT on the open-source auditory semantic decoding dataset and achieve a significant improvement up to $4.61\\%$ on METEOR and $2.43\\%$ on BERTScore across all the subjects compared to the state-of-the-art method.","The experimental results demonstrate that using brain representation as a prompt to further drive LLM for auditory neural decoding is feasible and effective."],"url":"http://arxiv.org/abs/2405.07840v1","category":"cs.HC"}
{"created":"2024-05-13 15:01:18","title":"Local Adjoints for Simultaneous Preaccumulations with Shared Inputs","abstract":"In shared-memory parallel automatic differentiation, shared inputs among simultaneous thread-local preaccumulations lead to data races if Jacobians are accumulated with a single, shared vector of adjoint variables. In this work, we discuss the benefits and tradeoffs of re-enabling such preaccumulations by a transition to suitable local adjoint variables. In particular, we assess the performance of mapped local adjoints in discrete adjoint computations in the multiphysics simulation suite SU2.","sentences":["In shared-memory parallel automatic differentiation, shared inputs among simultaneous thread-local preaccumulations lead to data races if Jacobians are accumulated with a single, shared vector of adjoint variables.","In this work, we discuss the benefits and tradeoffs of re-enabling such preaccumulations by a transition to suitable local adjoint variables.","In particular, we assess the performance of mapped local adjoints in discrete adjoint computations in the multiphysics simulation suite SU2."],"url":"http://arxiv.org/abs/2405.07819v1","category":"cs.MS"}
{"created":"2024-05-13 14:44:43","title":"Oscillations in the elastic high energy amplitude","abstract":"We discuss the oscillations in the elastic $pp$ differential cross section seen in the TOTEM data at $\\sqrt{s}=13$~TeV on the top of the usual smooth behaviour.","sentences":["We discuss the oscillations in the elastic $pp$ differential cross section seen in the TOTEM data at $\\sqrt{s}=13$~TeV on the top of the usual smooth behaviour."],"url":"http://arxiv.org/abs/2405.07802v1","category":"hep-ph"}
{"created":"2024-05-13 14:17:35","title":"Characterizing virulence differences in a parasitoid wasp through comparative transcriptomic and proteomic","abstract":"Background: Two strains of the endoparasitoid Cotesia typhae present a differential parasitism success on the host, Sesamia nonagrioides. One is virulent on both permissive and resistant host populations, and the other only on the permissive host. This interaction provides a very interesting frame for studying virulence factors. Here, we used a combination of comparative transcriptomic and proteomic analyses to unravel the molecular basis underlying virulence differences between the strains.Results: First, we report that virulence genes are mostly expressed during the nymphal stage of the parasitoid. Especially, proviral genes are broadly up-regulated at this stage, while their expression is only expected in the host. Parasitoid gene expression in the host increases with time, indicating the production of more virulence factors. Secondly, comparison between strains reveals differences in venom composition, with 12 proteins showing differential abundance. Proviral expression in the host displays a strong temporal variability, along with differential patterns between strains. Notably, a subset of proviral genes including protein-tyrosine phosphatases is specifically over-expressed in the resistant host parasitized by the less virulent strain, 24 hours after parasitism. This result particularly hints at host modulation of proviral expression.Conclusions: This study sheds light on the temporal expression of virulence factors of Cotesia typhae, both in the host and in the parasitoid. It also identifies potential molecular candidates driving differences in parasitism success between two strains. Together, those findings provide a path for further exploration of virulence mechanisms in parasitoid wasps, and offer insights into host-parasitoid coevolution.","sentences":["Background: Two strains of the endoparasitoid Cotesia typhae present a differential parasitism success on the host, Sesamia nonagrioides.","One is virulent on both permissive and resistant host populations, and the other only on the permissive host.","This interaction provides a very interesting frame for studying virulence factors.","Here, we used a combination of comparative transcriptomic and proteomic analyses to unravel the molecular basis underlying virulence differences between the strains.","Results:","First, we report that virulence genes are mostly expressed during the nymphal stage of the parasitoid.","Especially, proviral genes are broadly up-regulated at this stage, while their expression is only expected in the host.","Parasitoid gene expression in the host increases with time, indicating the production of more virulence factors.","Secondly, comparison between strains reveals differences in venom composition, with 12 proteins showing differential abundance.","Proviral expression in the host displays a strong temporal variability, along with differential patterns between strains.","Notably, a subset of proviral genes including protein-tyrosine phosphatases is specifically over-expressed in the resistant host parasitized by the less virulent strain, 24 hours after parasitism.","This result particularly hints at host modulation of proviral expression.","Conclusions: This study sheds light on the temporal expression of virulence factors of Cotesia typhae, both in the host and in the parasitoid.","It also identifies potential molecular candidates driving differences in parasitism success between two strains.","Together, those findings provide a path for further exploration of virulence mechanisms in parasitoid wasps, and offer insights into host-parasitoid coevolution."],"url":"http://arxiv.org/abs/2405.07772v1","category":"q-bio.GN"}
{"created":"2024-05-13 13:46:02","title":"Neural Network Compression for Reinforcement Learning Tasks","abstract":"In real applications of Reinforcement Learning (RL), such as robotics, low latency and energy efficient inference is very desired. The use of sparsity and pruning for optimizing Neural Network inference, and particularly to improve energy and latency efficiency, is a standard technique. In this work, we perform a systematic investigation of applying these optimization techniques for different RL algorithms in different RL environments, yielding up to a 400-fold reduction in the size of neural networks.","sentences":["In real applications of Reinforcement Learning (RL), such as robotics, low latency and energy efficient inference is very desired.","The use of sparsity and pruning for optimizing Neural Network inference, and particularly to improve energy and latency efficiency, is a standard technique.","In this work, we perform a systematic investigation of applying these optimization techniques for different RL algorithms in different RL environments, yielding up to a 400-fold reduction in the size of neural networks."],"url":"http://arxiv.org/abs/2405.07748v1","category":"cs.LG"}
{"created":"2024-05-13 12:01:54","title":"An Empirical Study on the Robustness of Massively Multilingual Neural Machine Translation","abstract":"Massively multilingual neural machine translation (MMNMT) has been proven to enhance the translation quality of low-resource languages. In this paper, we empirically investigate the translation robustness of Indonesian-Chinese translation in the face of various naturally occurring noise. To assess this, we create a robustness evaluation benchmark dataset for Indonesian-Chinese translation. This dataset is automatically translated into Chinese using four NLLB-200 models of different sizes. We conduct both automatic and human evaluations. Our in-depth analysis reveal the correlations between translation error types and the types of noise present, how these correlations change across different model sizes, and the relationships between automatic evaluation indicators and human evaluation indicators. The dataset is publicly available at https://github.com/tjunlp-lab/ID-ZH-MTRobustEval.","sentences":["Massively multilingual neural machine translation (MMNMT) has been proven to enhance the translation quality of low-resource languages.","In this paper, we empirically investigate the translation robustness of Indonesian-Chinese translation in the face of various naturally occurring noise.","To assess this, we create a robustness evaluation benchmark dataset for Indonesian-Chinese translation.","This dataset is automatically translated into Chinese using four NLLB-200 models of different sizes.","We conduct both automatic and human evaluations.","Our in-depth analysis reveal the correlations between translation error types and the types of noise present, how these correlations change across different model sizes, and the relationships between automatic evaluation indicators and human evaluation indicators.","The dataset is publicly available at https://github.com/tjunlp-lab/ID-ZH-MTRobustEval."],"url":"http://arxiv.org/abs/2405.07673v1","category":"cs.CL"}
{"created":"2024-05-13 10:26:28","title":"Analysis of the rate of convergence of an over-parametrized convolutional neural network image classifier learned by gradient descent","abstract":"Image classification based on over-parametrized convolutional neural networks with a global average-pooling layer is considered. The weights of the network are learned by gradient descent. A bound on the rate of convergence of the difference between the misclassification risk of the newly introduced convolutional neural network estimate and the minimal possible value is derived.","sentences":["Image classification based on over-parametrized convolutional neural networks with a global average-pooling layer is considered.","The weights of the network are learned by gradient descent.","A bound on the rate of convergence of the difference between the misclassification risk of the newly introduced convolutional neural network estimate and the minimal possible value is derived."],"url":"http://arxiv.org/abs/2405.07619v1","category":"stat.ML"}
{"created":"2024-05-13 10:21:29","title":"Expansions of the Potts model partition function along deletions and contractions","abstract":"We establish two expansions of the Potts model partition function of a graph. One is along the deletions of a graph, a rewritten formula given in Biggs (1977). The other is along the contractions of a graph. Then, we specialize the partition function to the chromatic or flow polynomial by the M\\\"obius inversion formula, and prove two known equations of the two polynomials. One expresses the chromatic polynomial as a weighted sum of flow polynomials of deletions, the other expresses the flow polynomial as a weighted sum of chromatic polynomials of contractions. The proof of the former by Biggs formula is due to Bychkov et al. (2021). The two expressions are considered to be dual in the sense of their forms, and transfer to each other with plane duality. This relation also holds in our expansions of the Potts model partition function. We clarify this duality by using matroid duality. Partition functions can be extended to matroids, and the two expansions can also be extended. The two expansions transfer to each other with matroid duality, so in addition to an elementary combinatorial proof of the two, we give another proof by the \"duality\" relation between them.","sentences":["We establish two expansions of the Potts model partition function of a graph.","One is along the deletions of a graph, a rewritten formula given in Biggs (1977).","The other is along the contractions of a graph.","Then, we specialize the partition function to the chromatic or flow polynomial by the M\\\"obius inversion formula, and prove two known equations of the two polynomials.","One expresses the chromatic polynomial as a weighted sum of flow polynomials of deletions, the other expresses the flow polynomial as a weighted sum of chromatic polynomials of contractions.","The proof of the former by Biggs formula is due to Bychkov et al. (2021).","The two expressions are considered to be dual in the sense of their forms, and transfer to each other with plane duality.","This relation also holds in our expansions of the Potts model partition function.","We clarify this duality by using matroid duality.","Partition functions can be extended to matroids, and the two expansions can also be extended.","The two expansions transfer to each other with matroid duality, so in addition to an elementary combinatorial proof of the two, we give another proof by the \"duality\" relation between them."],"url":"http://arxiv.org/abs/2405.07612v1","category":"math.CO"}
{"created":"2024-05-13 08:54:15","title":"The holonomy of spherically symmetric projective Finsler metrics of constant curvature","abstract":"In this paper, we investigate the holonomy group of $n$-dimensional projective Finsler metrics of constant curvature. We establish that in the spherically symmetric case, the holonomy group is maximal, and for a simply connected manifold it is isomorphic to $Diff_o({\\mathbb S^{n-1}})$, the connected component of the identity of the group of smooth diffeomorphism on the $(n-1)$-dimensional sphere. In particular, the holonomy group of the n-dimensional standard Funk metric and the Bryant-Shen metrics are maximal and isomorphic to $Diff_o({\\mathbb S^{n-1}})$. These results are the firsts describing explicitly the holonomy group of n-dimensional Finsler manifolds in the non-Berwaldian (that is when the canonical connection is non-linear) case.","sentences":["In this paper, we investigate the holonomy group of $n$-dimensional projective Finsler metrics of constant curvature.","We establish that in the spherically symmetric case, the holonomy group is maximal, and for a simply connected manifold it is isomorphic to $Diff_o({\\mathbb S^{n-1}})$, the connected component of the identity of the group of smooth diffeomorphism on the $(n-1)$-dimensional sphere.","In particular, the holonomy group of the n-dimensional standard Funk metric and the Bryant-Shen metrics are maximal and isomorphic to $Diff_o({\\mathbb S^{n-1}})$. These results are the firsts describing explicitly the holonomy group of n-dimensional Finsler manifolds in the non-Berwaldian (that is when the canonical connection is non-linear) case."],"url":"http://arxiv.org/abs/2405.07563v1","category":"math.DG"}
{"created":"2024-05-13 07:31:16","title":"Support-Query Prototype Fusion Network for Few-shot Medical Image Segmentation","abstract":"In recent years, deep learning based on Convolutional Neural Networks (CNNs) has achieved remarkable success in many applications. However, their heavy reliance on extensive labeled data and limited generalization ability to unseen classes pose challenges to their suitability for medical image processing tasks. Few-shot learning, which utilizes a small amount of labeled data to generalize to unseen classes, has emerged as a critical research area, attracting substantial attention. Currently, most studies employ a prototype-based approach, in which prototypical networks are used to construct prototypes from the support set, guiding the processing of the query set to obtain the final results. While effective, this approach heavily relies on the support set while neglecting the query set, resulting in notable disparities within the model classes. To mitigate this drawback, we propose a novel Support-Query Prototype Fusion Network (SQPFNet). SQPFNet initially generates several support prototypes for the foreground areas of the support images, thus producing a coarse segmentation mask. Subsequently, a query prototype is constructed based on the coarse segmentation mask, additionally exploiting pattern information in the query set. Thus, SQPFNet constructs high-quality support-query fused prototypes, upon which the query image is segmented to obtain the final refined query mask. Evaluation results on two public datasets, SABS and CMR, show that SQPFNet achieves state-of-the-art performance.","sentences":["In recent years, deep learning based on Convolutional Neural Networks (CNNs) has achieved remarkable success in many applications.","However, their heavy reliance on extensive labeled data and limited generalization ability to unseen classes pose challenges to their suitability for medical image processing tasks.","Few-shot learning, which utilizes a small amount of labeled data to generalize to unseen classes, has emerged as a critical research area, attracting substantial attention.","Currently, most studies employ a prototype-based approach, in which prototypical networks are used to construct prototypes from the support set, guiding the processing of the query set to obtain the final results.","While effective, this approach heavily relies on the support set while neglecting the query set, resulting in notable disparities within the model classes.","To mitigate this drawback, we propose a novel Support-Query Prototype Fusion Network (SQPFNet).","SQPFNet initially generates several support prototypes for the foreground areas of the support images, thus producing a coarse segmentation mask.","Subsequently, a query prototype is constructed based on the coarse segmentation mask, additionally exploiting pattern information in the query set.","Thus, SQPFNet constructs high-quality support-query fused prototypes, upon which the query image is segmented to obtain the final refined query mask.","Evaluation results on two public datasets, SABS and CMR, show that SQPFNet achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2405.07516v1","category":"cs.CV"}
{"created":"2024-05-13 07:00:42","title":"A frequency-domain enhanced multi-view network for metal fatigue life prediction","abstract":"Fatigue damages and failure widely exist in engineering structures. However, predicting fatigue life for various structural materials subjected to multiaxial loading paths remains a challenging problem. A novel multi-view deep learning model incorporating frequency-domain analysis for fatigue life prediction is proposed. The model consists of two main analytical components: one for analyzing multiaxial fatigue loading paths and the other for examining the mechanical properties of materials and specimen geometrical characteristics. In the module analyzing multiaxial fatigue loading paths, convolutional neural network (CNN), long short-term memory network (LSTM), and FNet are connected in parallel to extract features individually. Features of materials and specimens are extracted through fully connected neural networks (FCNNs). Subsequently, the features from these two parts are thoroughly integrated based on attention mechanisms, and connected to multiple FCNNs to accomplish fatigue life prediction. A fatigue experimental database comprising 557 samples, spanning 46 multiaxial loading paths and 19 metal materials, has been established for model training and testing. Additionally, 6 materials were respectively used as test sets to evaluate the extrapolation ability of the model. The results suggest that the proposed model exhibits robust predictive performance and extrapolation capabilities. We anticipate that the multi-view approach, along with its accuracy and applicability, can provide an unparalleled alternative for researchers in the field of engineering fatigue and beyond.","sentences":["Fatigue damages and failure widely exist in engineering structures.","However, predicting fatigue life for various structural materials subjected to multiaxial loading paths remains a challenging problem.","A novel multi-view deep learning model incorporating frequency-domain analysis for fatigue life prediction is proposed.","The model consists of two main analytical components: one for analyzing multiaxial fatigue loading paths and the other for examining the mechanical properties of materials and specimen geometrical characteristics.","In the module analyzing multiaxial fatigue loading paths, convolutional neural network (CNN), long short-term memory network (LSTM), and FNet are connected in parallel to extract features individually.","Features of materials and specimens are extracted through fully connected neural networks (FCNNs).","Subsequently, the features from these two parts are thoroughly integrated based on attention mechanisms, and connected to multiple FCNNs to accomplish fatigue life prediction.","A fatigue experimental database comprising 557 samples, spanning 46 multiaxial loading paths and 19 metal materials, has been established for model training and testing.","Additionally, 6 materials were respectively used as test sets to evaluate the extrapolation ability of the model.","The results suggest that the proposed model exhibits robust predictive performance and extrapolation capabilities.","We anticipate that the multi-view approach, along with its accuracy and applicability, can provide an unparalleled alternative for researchers in the field of engineering fatigue and beyond."],"url":"http://arxiv.org/abs/2405.07507v1","category":"physics.app-ph"}
{"created":"2024-05-13 05:58:01","title":"Extended Bose-Einstein condensate dark matter in $f(Q)$ gravity","abstract":"In this article, we attempt to explore the dark sector of the universe i.e. dark matter and dark energy, where the dark energy components are related to the modified $f(Q)$ Lagrangian, particularly a power law function $f(Q)= \\gamma \\left(\\frac{Q}{Q_0}\\right)^n$, while the dark matter component is described by the Extended Bose-Einstein Condensate (EBEC) equation of state for dark matter, specifically, $p = \\alpha \\rho + \\beta \\rho^2$. We find the corresponding Friedmann-like equations and the continuity equation for both dark components along with an interacting term, specifically $\\mathcal{Q} = 3b^2H \\rho$, which signifies the energy exchange between the dark sector of the universe. Further, we derive the analytical expression of the Hubble function, and then we find the best-fit values of free parameters utilizing the Bayesian analysis to estimate the posterior probability and the Markov Chain Monte Carlo (MCMC) sampling technique corresponding to CC+Pantheon+SH0ES samples. In addition, to examine the robustness of our MCMC analysis, we perform a statistical assessment using the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). Further from the evolutionary profile of the deceleration parameter and the energy density, we obtain a transition from the decelerated epoch to the accelerated expansion phase, with the present deceleration parameter value as $q(z=0)=q_0=-0.56^{+0.04}_{-0.03}$ ($68 \\%$ confidence limit), that is quite consistent with cosmological observations. In addition, we find the expected positive behavior of the effective energy density. Finally, by examining the sound speed parameter, we find that the assumed theoretical $f(Q)$ model is thermodynamically stable.","sentences":["In this article, we attempt to explore the dark sector of the universe i.e. dark matter and dark energy, where the dark energy components are related to the modified $f(Q)$ Lagrangian, particularly a power law function $f(Q)= \\gamma \\left(\\frac{Q}{Q_0}\\right)^n$, while the dark matter component is described by the Extended Bose-Einstein Condensate (EBEC) equation of state for dark matter, specifically, $p = \\alpha \\rho +","\\beta \\rho^2$. We find the corresponding Friedmann-like equations and the continuity equation for both dark components along with an interacting term, specifically $\\mathcal{Q} = 3b^2H \\rho$, which signifies the energy exchange between the dark sector of the universe.","Further, we derive the analytical expression of the Hubble function, and then we find the best-fit values of free parameters utilizing the Bayesian analysis to estimate the posterior probability and the Markov Chain Monte Carlo (MCMC) sampling technique corresponding to CC+Pantheon+SH0ES samples.","In addition, to examine the robustness of our MCMC analysis, we perform a statistical assessment using the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC).","Further from the evolutionary profile of the deceleration parameter and the energy density, we obtain a transition from the decelerated epoch to the accelerated expansion phase, with the present deceleration parameter value as $q(z=0)=q_0=-0.56^{+0.04}_{-0.03}$ ($68 \\%$ confidence limit), that is quite consistent with cosmological observations.","In addition, we find the expected positive behavior of the effective energy density.","Finally, by examining the sound speed parameter, we find that the assumed theoretical $f(Q)$ model is thermodynamically stable."],"url":"http://arxiv.org/abs/2405.07484v1","category":"gr-qc"}
{"created":"2024-05-13 05:18:23","title":"Intrinsic Rewards for Exploration without Harm from Observational Noise: A Simulation Study Based on the Free Energy Principle","abstract":"In Reinforcement Learning (RL), artificial agents are trained to maximize numerical rewards by performing tasks. Exploration is essential in RL because agents must discover information before exploiting it. Two rewards encouraging efficient exploration are the entropy of action policy and curiosity for information gain. Entropy is well-established in literature, promoting randomized action selection. Curiosity is defined in a broad variety of ways in literature, promoting discovery of novel experiences. One example, prediction error curiosity, rewards agents for discovering observations they cannot accurately predict. However, such agents may be distracted by unpredictable observational noises known as curiosity traps. Based on the Free Energy Principle (FEP), this paper proposes hidden state curiosity, which rewards agents by the KL divergence between the predictive prior and posterior probabilities of latent variables. We trained six types of agents to navigate mazes: baseline agents without rewards for entropy or curiosity, and agents rewarded for entropy and/or either prediction error curiosity or hidden state curiosity. We find entropy and curiosity result in efficient exploration, especially both employed together. Notably, agents with hidden state curiosity demonstrate resilience against curiosity traps, which hinder agents with prediction error curiosity. This suggests implementing the FEP may enhance the robustness and generalization of RL models, potentially aligning the learning processes of artificial and biological agents.","sentences":["In Reinforcement Learning (RL), artificial agents are trained to maximize numerical rewards by performing tasks.","Exploration is essential in RL because agents must discover information before exploiting it.","Two rewards encouraging efficient exploration are the entropy of action policy and curiosity for information gain.","Entropy is well-established in literature, promoting randomized action selection.","Curiosity is defined in a broad variety of ways in literature, promoting discovery of novel experiences.","One example, prediction error curiosity, rewards agents for discovering observations they cannot accurately predict.","However, such agents may be distracted by unpredictable observational noises known as curiosity traps.","Based on the Free Energy Principle (FEP), this paper proposes hidden state curiosity, which rewards agents by the KL divergence between the predictive prior and posterior probabilities of latent variables.","We trained six types of agents to navigate mazes: baseline agents without rewards for entropy or curiosity, and agents rewarded for entropy and/or either prediction error curiosity or hidden state curiosity.","We find entropy and curiosity result in efficient exploration, especially both employed together.","Notably, agents with hidden state curiosity demonstrate resilience against curiosity traps, which hinder agents with prediction error curiosity.","This suggests implementing the FEP may enhance the robustness and generalization of RL models, potentially aligning the learning processes of artificial and biological agents."],"url":"http://arxiv.org/abs/2405.07473v1","category":"cs.LG"}
{"created":"2024-05-13 04:54:21","title":"Deception in Differential Games: Information Limiting Strategy to Induce Dilemma","abstract":"Can deception exist in differential games? We provide a case study for a Turret-Attacker differential game, where two Attackers seek to score points by reaching a target region while a Turret tries to minimize the score by aligning itself with the Attackers before they reach the target. In contrast to the original problem solved with complete information, we assume that the Turret only has partial information about the maximum speed of the Attackers. We investigate whether there is any incentive for the Attackers to move slower than their maximum speed in order to ``deceive'' the Turret into taking suboptimal actions. We first describe the existence of a dilemma that the Turret may face. Then we derive a set of initial conditions from which the Attackers can force the Turret into a situation where it must take a guess.","sentences":["Can deception exist in differential games?","We provide a case study for a Turret-Attacker differential game, where two Attackers seek to score points by reaching a target region while a Turret tries to minimize the score by aligning itself with the Attackers before they reach the target.","In contrast to the original problem solved with complete information, we assume that the Turret only has partial information about the maximum speed of the Attackers.","We investigate whether there is any incentive for the Attackers to move slower than their maximum speed in order to ``deceive'' the Turret into taking suboptimal actions.","We first describe the existence of a dilemma that the Turret may face.","Then we derive a set of initial conditions from which the Attackers can force the Turret into a situation where it must take a guess."],"url":"http://arxiv.org/abs/2405.07465v1","category":"cs.GT"}
{"created":"2024-05-13 04:48:05","title":"Feynman Paradox about the Josephson effect and a sawtooth current in the double junction","abstract":"We revisit the Feynman approach to the Josephson effect, which employs a pair of linear coupling equations for its modeling. It is found that while the exact solutions can account for the AC Josephson effect when the coupling strength is significantly less than the voltage, they fail to produce the DC Josephson effect in any practical scenario. To address this fundamental discrepancy, we derive the coupled Ginzburg-Landau (GL) equations for two interconnected superconductors based on BCS theory. These equations reveal that the nonlinear coupling, which is overlooked in the Feynman method, is crucial in describing the spontaneous symmetry breaking in superconductors, a critical factor for achieving the DC Josephson effect. When the coupled GL equations are applied to a double junction, a sawtooth current pattern emerges, a result unattainable via the Feynman approach.","sentences":["We revisit the Feynman approach to the Josephson effect, which employs a pair of linear coupling equations for its modeling.","It is found that while the exact solutions can account for the AC Josephson effect when the coupling strength is significantly less than the voltage, they fail to produce the DC Josephson effect in any practical scenario.","To address this fundamental discrepancy, we derive the coupled Ginzburg-Landau (GL) equations for two interconnected superconductors based on BCS theory.","These equations reveal that the nonlinear coupling, which is overlooked in the Feynman method, is crucial in describing the spontaneous symmetry breaking in superconductors, a critical factor for achieving the DC Josephson effect.","When the coupled GL equations are applied to a double junction, a sawtooth current pattern emerges, a result unattainable via the Feynman approach."],"url":"http://arxiv.org/abs/2405.07462v1","category":"cond-mat.supr-con"}
{"created":"2024-05-13 04:08:18","title":"New sources of ghost fields in $k$-essence theories for black-bounce solutions","abstract":"In the present study, we generalize the possible ghost field configurations within the framework of $k$-essence theory to the Simpson-Visser metric area function $\\Sigma^2=x^2+a^2$. Our analysis encompasses field configurations for the region-defined metric function $dA_\\pm$ as well as the general solution that asymptotically behaves as Schwarzschild-de Sitter for $x\\to-\\infty$. Specifically, we investigate two scalar field configurations and define the associated potential for each one. Through rigorous calculations, we verify that all equations of motion are satisfied. Notably, our findings indicate that even when proposing new configurations of ghost scalar fields, the energy conditions remain unchanged. This result serves to validate the wormhole solutions obtained in previous studies.","sentences":["In the present study, we generalize the possible ghost field configurations within the framework of $k$-essence theory to the Simpson-Visser metric area function $\\Sigma^2=x^2+a^2$. Our analysis encompasses field configurations for the region-defined metric function $dA_\\pm$ as well as the general solution that asymptotically behaves as Schwarzschild-de Sitter for $x\\to-\\infty$. Specifically, we investigate two scalar field configurations and define the associated potential for each one.","Through rigorous calculations, we verify that all equations of motion are satisfied.","Notably, our findings indicate that even when proposing new configurations of ghost scalar fields, the energy conditions remain unchanged.","This result serves to validate the wormhole solutions obtained in previous studies."],"url":"http://arxiv.org/abs/2405.07455v1","category":"gr-qc"}
{"created":"2024-05-13 03:45:20","title":"An Effectiveness Study Across Baseline and Neural Network-based Force Estimation Methods on the da Vinci Research Kit Si System","abstract":"In this study, we further investigate the robustness and generalization ability of an neural network (NN) based force estimation method, using the da Vinci Research Kit Si (dVRK-Si). To evaluate our method's performance, we compare the force estimation accuracy with several baseline methods. We conduct comparative studies between the dVRK classic and dVRK-Si systems to benchmark the effectiveness of these approaches.   We conclude that the NN-based method provides comparable force estimation accuracy across the two systems, as the average root mean square error (RMSE) over the average range of force ratio is approximately 3.07% for the dVRK classic, and 5.27% for the dVRK-Si. On the dVRK-Si, the force estimation RMSEs for all the baseline methods are 2 to 4 times larger than the NN-based method in all directions. One possible reason is, we made assumptions in the baseline methods that static forces remain the same or dynamics is time-invariant. These assumptions may hold for the dVRK Classic, as it has pre-loaded weight and maintains horizontal self balance. Since the dVRK-Si configuration does not have this property, assumptions do not hold anymore, therefore the NN-based method significantly outperforms.","sentences":["In this study, we further investigate the robustness and generalization ability of an neural network (NN) based force estimation method, using the da Vinci Research Kit Si (dVRK-Si).","To evaluate our method's performance, we compare the force estimation accuracy with several baseline methods.","We conduct comparative studies between the dVRK classic and dVRK-Si systems to benchmark the effectiveness of these approaches.   ","We conclude that the NN-based method provides comparable force estimation accuracy across the two systems, as the average root mean square error (RMSE) over the average range of force ratio is approximately 3.07% for the dVRK classic, and 5.27% for the dVRK-Si.","On the dVRK-Si, the force estimation RMSEs for all the baseline methods are 2 to 4 times larger than the NN-based method in all directions.","One possible reason is, we made assumptions in the baseline methods that static forces remain the same or dynamics is time-invariant.","These assumptions may hold for the dVRK Classic, as it has pre-loaded weight and maintains horizontal self balance.","Since the dVRK-Si configuration does not have this property, assumptions do not hold anymore, therefore the NN-based method significantly outperforms."],"url":"http://arxiv.org/abs/2405.07453v1","category":"cs.RO"}
{"created":"2024-05-13 02:59:50","title":"Reducing Spatial Discretization Error on Coarse CFD Simulations Using an OpenFOAM-Embedded Deep Learning Framework","abstract":"We propose a method for reducing the spatial discretization error of coarse computational fluid dynamics (CFD) problems by enhancing the quality of low-resolution simulations using a deep learning model fed with high-quality data. We substitute the default differencing scheme for the convection term by a feed-forward neural network that interpolates velocities from cell centers to face values to produce velocities that approximate the fine-mesh data well. The deep learning framework incorporates the open-source CFD code OpenFOAM, resulting in an end-to-end differentiable model. We automatically differentiate the CFD physics using a discrete adjoint code version. We present a fast communication method between TensorFlow (Python) and OpenFOAM (c++) that accelerates the training process. We applied the model to the flow past a square cylinder problem, reducing the error to about 50% for simulations outside the training distribution compared to the traditional solver in the x- and y-velocity components using an 8x coarser mesh. The training is affordable in terms of time and data samples since the architecture exploits the local features of the physics while generating stable predictions for mid-term simulations.","sentences":["We propose a method for reducing the spatial discretization error of coarse computational fluid dynamics (CFD) problems by enhancing the quality of low-resolution simulations using a deep learning model fed with high-quality data.","We substitute the default differencing scheme for the convection term by a feed-forward neural network that interpolates velocities from cell centers to face values to produce velocities that approximate the fine-mesh data well.","The deep learning framework incorporates the open-source CFD code OpenFOAM, resulting in an end-to-end differentiable model.","We automatically differentiate the CFD physics using a discrete adjoint code version.","We present a fast communication method between TensorFlow (Python) and OpenFOAM (c++) that accelerates the training process.","We applied the model to the flow past a square cylinder problem, reducing the error to about 50% for simulations outside the training distribution compared to the traditional solver in the x- and y-velocity components using an 8x coarser mesh.","The training is affordable in terms of time and data samples since the architecture exploits the local features of the physics while generating stable predictions for mid-term simulations."],"url":"http://arxiv.org/abs/2405.07441v1","category":"cs.LG"}
{"created":"2024-05-13 02:00:19","title":"Existence of stationary vortex patches for the gSQG in bounded domains","abstract":"In this paper we show the existence of time-periodic vortex patches for the generalized surface quasi-geostrophic equation within a bounded domain. This construction is carried out for values of $\\gamma$ in the range of $(1,2)$. The resulting vortex patches possess a fixed vorticity and total flux, and they are located in the neighborhood of critical points that are non-degenerate for the Kirchhoff--Routh equation. The proof is accomplished through a combination of analyzing the linearization of the contour dynamics equation and employing the implicit function theorem as well as carefully selected function spaces.","sentences":["In this paper we show the existence of time-periodic vortex patches for the generalized surface quasi-geostrophic equation within a bounded domain.","This construction is carried out for values of $\\gamma$ in the range of $(1,2)$. The resulting vortex patches possess a fixed vorticity and total flux, and they are located in the neighborhood of critical points that are non-degenerate for the Kirchhoff--Routh equation.","The proof is accomplished through a combination of analyzing the linearization of the contour dynamics equation and employing the implicit function theorem as well as carefully selected function spaces."],"url":"http://arxiv.org/abs/2405.07427v1","category":"math.AP"}
{"created":"2024-05-13 01:42:26","title":"Necessity of orthogonal basis vectors for the two-anyon problem in one-dimensional lattice","abstract":"Few-body physics for anyons has been intensively studied within the anyon-Hubbard model, including the quantum walk and Bloch oscillations of two-anyon states. However, the known theoretical proposal and experimental simulations of two-anyon states in one-dimensional lattice have been carried out by expanding the wavefunction in terms of non-orthogonal basis vectors, which introduces extra non-physical degrees of freedom. In the present work, we deduce the finite difference equations for the two-anyon state in the one-dimensional lattice by solving the Schr\\\"odinger equation with orthogonal basis vectors. Such an orthogonal scheme gives all the orthogonal physical eigenstates for the time-independent two-anyon Schr\\\"odinger equation, while the conventional (non-orthogonal) method produces a lot of non-physical redundant eigen-solutions whose components violate the anyonic relations. The dynamical property of the two-anyon states in a sufficiently large lattice has been investigated and compared in both the orthogonal and conventional schemes, which proves to depend crucially on the initial states. When the initial states with two anyons on the same site or (next-)neighboring sites are suitably chosen to be in accordance with the anyonic coefficient relation, we observe exactly the same dynamical behavior in the two schemes, including the revival probability, the probability density function, and the two-body correlation, otherwise, the conventional scheme will produce erroneous results which not any more describe anyons. The period of the Bloch oscillation in the pseudo-fermionic limit is found to be twice that in the bosonic limit, while the oscillations disappear for statistical parameters in between. Our findings are vital for quantum simulations of few-body physics with anyons in the lattice.","sentences":["Few-body physics for anyons has been intensively studied within the anyon-Hubbard model, including the quantum walk and Bloch oscillations of two-anyon states.","However, the known theoretical proposal and experimental simulations of two-anyon states in one-dimensional lattice have been carried out by expanding the wavefunction in terms of non-orthogonal basis vectors, which introduces extra non-physical degrees of freedom.","In the present work, we deduce the finite difference equations for the two-anyon state in the one-dimensional lattice by solving the Schr\\\"odinger equation with orthogonal basis vectors.","Such an orthogonal scheme gives all the orthogonal physical eigenstates for the time-independent two-anyon Schr\\\"odinger equation, while the conventional (non-orthogonal) method produces a lot of non-physical redundant eigen-solutions whose components violate the anyonic relations.","The dynamical property of the two-anyon states in a sufficiently large lattice has been investigated and compared in both the orthogonal and conventional schemes, which proves to depend crucially on the initial states.","When the initial states with two anyons on the same site or (next-)neighboring sites are suitably chosen to be in accordance with the anyonic coefficient relation, we observe exactly the same dynamical behavior in the two schemes, including the revival probability, the probability density function, and the two-body correlation, otherwise, the conventional scheme will produce erroneous results which not any more describe anyons.","The period of the Bloch oscillation in the pseudo-fermionic limit is found to be twice that in the bosonic limit, while the oscillations disappear for statistical parameters in between.","Our findings are vital for quantum simulations of few-body physics with anyons in the lattice."],"url":"http://arxiv.org/abs/2405.07424v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-13 01:31:16","title":"A replica theory for the dynamic glass transition of hardspheres with continuous polydispersity","abstract":"Glassy soft matter is often continuously polydisperse, in which the sizes or various properties of the constituent particles are distributed continuously. However, most of the microscopic theories of the glass transition focus on the monodisperse particles. Here, we developed a replica theory for the dynamic glass transition of continuously polydisperse hardspheres. We focused on the limit of infinite spatial dimension, where replica theory becomes exact. In theory, the cage size $A$, which plays the role of an order parameter, appears to depend on the particle size $\\sigma$, and thus, the effective free energy, the so-called Franz-Parisi potential, is a functional of $A(\\sigma)$. We applied this theory to two fundamental systems: a nearly monodisperse system and an exponential distribution system. We found that dynamic decoupling occurs in both cases; the critical particle size $\\sigma^{\\ast}$ emerges, and larger particles with $\\sigma \\geq \\sigma^{\\ast}$ vitrify, while smaller particles $\\sigma < \\sigma^{\\ast}$ remain mobile. Moreover, the cage size $A(\\sigma)$ exhibits a critical behavior at $\\sigma \\simeq \\sigma^{\\ast}$, originating from spinodal instability of $\\sigma^{\\ast}$-sized particles. We discuss the implications of these results for finite dimensional systems.","sentences":["Glassy soft matter is often continuously polydisperse, in which the sizes or various properties of the constituent particles are distributed continuously.","However, most of the microscopic theories of the glass transition focus on the monodisperse particles.","Here, we developed a replica theory for the dynamic glass transition of continuously polydisperse hardspheres.","We focused on the limit of infinite spatial dimension, where replica theory becomes exact.","In theory, the cage size $A$, which plays the role of an order parameter, appears to depend on the particle size $\\sigma$, and thus, the effective free energy, the so-called Franz-Parisi potential, is a functional of $A(\\sigma)$. We applied this theory to two fundamental systems: a nearly monodisperse system and an exponential distribution system.","We found that dynamic decoupling occurs in both cases; the critical particle size $\\sigma^{\\ast}$ emerges, and larger particles with $\\sigma \\geq \\sigma^{\\ast}$ vitrify, while smaller particles $\\sigma < \\sigma^{\\ast}$ remain mobile.","Moreover, the cage size $A(\\sigma)$ exhibits a critical behavior at $\\sigma \\simeq \\sigma^{\\ast}$, originating from spinodal instability of $\\sigma^{\\ast}$-sized particles.","We discuss the implications of these results for finite dimensional systems."],"url":"http://arxiv.org/abs/2405.07416v1","category":"cond-mat.soft"}
{"created":"2024-05-12 23:19:41","title":"An Unstructured Body-of-Revolution Electromagnetic Particle-in-Cell Algorithm with Radial Perfectly Matched Layers and Dual Polarizations","abstract":"A novel electromagnetic particle-in-cell algorithm has been developed for fully kinetic plasma simulations on unstructured (irregular) meshes in complex body-of-revolution geometries. The algorithm, implemented in the BORPIC++ code, utilizes a set of field scalings and a coordinate mapping, reducing the Maxwell field problem in a cylindrical system to a Cartesian finite element Maxwell solver in the meridian plane. The latter obviates the cylindrical coordinate singularity in the symmetry axis. The choice of an unstructured finite element discretization enhances the geometrical flexibility of the BORPIC++ solver compared to the more traditional finite difference solvers. Symmetries in Maxwell's equations are explored to decompose the problem into two dual polarization states with isomorphic representations that enable code reuse. The particle-in-cell scatter and gather steps preserve charge-conservation at the discrete level. Our previous algorithm (BORPIC+) discretized the E and B field components of TE-phi and TM-phi polarizations on the finite element (primal) mesh. A cylindrical perfectly matched layer is implemented as a boundary condition in the radial direction to simulate open space problems, with periodic boundary conditions in the axial direction. We investigate effects of charged particles moving next to the cylindrical perfectly matched layer. We model azimuthal currents arising from rotational motion of charged rings, which produce TM-phi polarized fields. Several numerical examples are provided to illustrate the first application of the algorithm.","sentences":["A novel electromagnetic particle-in-cell algorithm has been developed for fully kinetic plasma simulations on unstructured (irregular) meshes in complex body-of-revolution geometries.","The algorithm, implemented in the BORPIC++ code, utilizes a set of field scalings and a coordinate mapping, reducing the Maxwell field problem in a cylindrical system to a Cartesian finite element Maxwell solver in the meridian plane.","The latter obviates the cylindrical coordinate singularity in the symmetry axis.","The choice of an unstructured finite element discretization enhances the geometrical flexibility of the BORPIC++ solver compared to the more traditional finite difference solvers.","Symmetries in Maxwell's equations are explored to decompose the problem into two dual polarization states with isomorphic representations that enable code reuse.","The particle-in-cell scatter and gather steps preserve charge-conservation at the discrete level.","Our previous algorithm (BORPIC+) discretized the E and B field components of TE-phi and TM-phi polarizations on the finite element (primal) mesh.","A cylindrical perfectly matched layer is implemented as a boundary condition in the radial direction to simulate open space problems, with periodic boundary conditions in the axial direction.","We investigate effects of charged particles moving next to the cylindrical perfectly matched layer.","We model azimuthal currents arising from rotational motion of charged rings, which produce TM-phi polarized fields.","Several numerical examples are provided to illustrate the first application of the algorithm."],"url":"http://arxiv.org/abs/2405.07396v1","category":"math.NA"}
{"created":"2024-05-12 22:48:35","title":"Finite Diffeomorphism Theorem for manifolds with lower Ricci curvature and bounded energy","abstract":"In this paper we prove that the space $\\cM(n,\\rv,D,\\Lambda):=\\{(M^n,g) \\text{ closed }: ~~\\Ric\\ge -(n-1),~\\Vol(M)\\ge \\rv>0, \\diam(M)\\le D \\text{ and } \\int_{M}|\\Rm|^{n/2}\\le \\Lambda\\}$ has at most $C(n,\\rv,D,\\Lambda)$ many diffeomorphism types. This removes the upper Ricci curvature bound of Anderson-Cheeger's finite diffeomorphism theorem in \\cite{AnCh}. Furthermore, if $M$ is K\\\"ahler surface, the Riemann curvature $L^2$ bound could be replaced by the scalar curvature $L^2$ bound.","sentences":["In this paper we prove that the space $\\cM(n,\\rv,D,\\Lambda):=\\{(M^n,g) \\text{ closed }: ~~\\Ric\\ge -(n-1),~\\Vol(M)\\ge \\rv>0, \\diam(M)\\le D \\text{ and } \\int_{M}|\\Rm|^{n/2}\\le \\Lambda\\}$ has at most $C(n,\\rv,D,\\Lambda)$ many diffeomorphism types.","This removes the upper Ricci curvature bound of Anderson-Cheeger's finite diffeomorphism theorem in \\cite{AnCh}.","Furthermore, if $M$ is K\\\"ahler surface, the Riemann curvature $L^2$ bound could be replaced by the scalar curvature $L^2$ bound."],"url":"http://arxiv.org/abs/2405.07390v1","category":"math.DG"}
{"created":"2024-05-12 21:51:08","title":"Networked Control with Hybrid Automatic Repeat Request Protocols","abstract":"We study feedback control of a dynamical process over a lossy channel equipped with a hybrid automatic repeat request protocol that connects a sensor to an actuator. The dynamical process is modeled by a Gauss-Markov process, and the lossy channel by a packet-erasure channel with ideal feedback. We suppose that data is communicated in the format of packets with negligible quantization error. In such a networked control system, whenever a packet loss occurs, there exists a tradeoff between transmitting new sensory information with a lower success probability and retransmitting previously failed sensory information with a higher success probability. In essence, an inherent tradeoff between freshness and reliability. To address this tradeoff, we consider a linear-quadratic-regulator performance index, which penalizes state deviations and control efforts over a finite horizon, and jointly design optimal policies for an encoder and a decoder, which are collocated with the sensor and the actuator, respectively. Our emphasis here lies specifically on designing switching and control policies, rather than error-correcting codes. We derive the structural properties of the optimal encoding and decoding policies. We show that the former is a threshold switching policy and the latter is a certainty-equivalent control policy. In addition, we specify the iterative equations that the encoder and the decoder need to solve in order to implement the optimal policies.","sentences":["We study feedback control of a dynamical process over a lossy channel equipped with a hybrid automatic repeat request protocol that connects a sensor to an actuator.","The dynamical process is modeled by a Gauss-Markov process, and the lossy channel by a packet-erasure channel with ideal feedback.","We suppose that data is communicated in the format of packets with negligible quantization error.","In such a networked control system, whenever a packet loss occurs, there exists a tradeoff between transmitting new sensory information with a lower success probability and retransmitting previously failed sensory information with a higher success probability.","In essence, an inherent tradeoff between freshness and reliability.","To address this tradeoff, we consider a linear-quadratic-regulator performance index, which penalizes state deviations and control efforts over a finite horizon, and jointly design optimal policies for an encoder and a decoder, which are collocated with the sensor and the actuator, respectively.","Our emphasis here lies specifically on designing switching and control policies, rather than error-correcting codes.","We derive the structural properties of the optimal encoding and decoding policies.","We show that the former is a threshold switching policy and the latter is a certainty-equivalent control policy.","In addition, we specify the iterative equations that the encoder and the decoder need to solve in order to implement the optimal policies."],"url":"http://arxiv.org/abs/2405.07381v1","category":"cs.IT"}
{"created":"2024-05-12 21:32:26","title":"Feedback stabilization via a quantum projection filter","abstract":"This paper considers a simplified model of open quantum systems undergoing imperfect measurements obtained via a projection filter approach. We use this approximate filter in the feedback stabilization problem specifically in the case of Quantum Non-Demolition (QND) measurements. The feedback design relies on the structure of the exponential family utilized for the projection process. We demonstrate that the introduced feedback guarantees exponential convergence of the original filter equation toward a predefined target state, corresponding to an eigenstate of the measurement operator.","sentences":["This paper considers a simplified model of open quantum systems undergoing imperfect measurements obtained via a projection filter approach.","We use this approximate filter in the feedback stabilization problem specifically in the case of Quantum Non-Demolition (QND) measurements.","The feedback design relies on the structure of the exponential family utilized for the projection process.","We demonstrate that the introduced feedback guarantees exponential convergence of the original filter equation toward a predefined target state, corresponding to an eigenstate of the measurement operator."],"url":"http://arxiv.org/abs/2405.07379v1","category":"quant-ph"}
{"created":"2024-05-12 18:45:30","title":"Forecasting with an N-dimensional Langevin Equation and a Neural-Ordinary Differential Equation","abstract":"Accurate prediction of electricity day-ahead prices is essential in competitive electricity markets. Although stationary electricity-price forecasting techniques have received considerable attention, research on non-stationary methods is comparatively scarce, despite the common prevalence of non-stationary features in electricity markets. Specifically, existing non-stationary techniques will often aim to address individual non-stationary features in isolation, leaving aside the exploration of concurrent multiple non-stationary effects. Our overarching objective here is the formulation of a framework to systematically model and forecast non-stationary electricity-price time series, encompassing the broader scope of non-stationary behavior. For this purpose we develop a data-driven model that combines an N-dimensional Langevin equation (LE) with a neural-ordinary differential equation (NODE). The LE captures fine-grained details of the electricity-price behavior in stationary regimes but is inadequate for non-stationary conditions. To overcome this inherent limitation, we adopt a NODE approach to learn, and at the same time predict, the difference between the actual electricity-price time series and the simulated price trajectories generated by the LE. By learning this difference, the NODE reconstructs the non-stationary components of the time series that the LE is not able to capture. We exemplify the effectiveness of our framework using the Spanish electricity day-ahead market as a prototypical case study. Our findings reveal that the NODE nicely complements the LE, providing a comprehensive strategy to tackle both stationary and non-stationary electricity-price behavior. The framework's dependability and robustness is demonstrated through different non-stationary scenarios by comparing it against a range of basic naive methods.","sentences":["Accurate prediction of electricity day-ahead prices is essential in competitive electricity markets.","Although stationary electricity-price forecasting techniques have received considerable attention, research on non-stationary methods is comparatively scarce, despite the common prevalence of non-stationary features in electricity markets.","Specifically, existing non-stationary techniques will often aim to address individual non-stationary features in isolation, leaving aside the exploration of concurrent multiple non-stationary effects.","Our overarching objective here is the formulation of a framework to systematically model and forecast non-stationary electricity-price time series, encompassing the broader scope of non-stationary behavior.","For this purpose we develop a data-driven model that combines an N-dimensional Langevin equation (LE) with a neural-ordinary differential equation (NODE).","The LE captures fine-grained details of the electricity-price behavior in stationary regimes but is inadequate for non-stationary conditions.","To overcome this inherent limitation, we adopt a NODE approach to learn, and at the same time predict, the difference between the actual electricity-price time series and the simulated price trajectories generated by the LE.","By learning this difference, the NODE reconstructs the non-stationary components of the time series that the LE is not able to capture.","We exemplify the effectiveness of our framework using the Spanish electricity day-ahead market as a prototypical case study.","Our findings reveal that the NODE nicely complements the LE, providing a comprehensive strategy to tackle both stationary and non-stationary electricity-price behavior.","The framework's dependability and robustness is demonstrated through different non-stationary scenarios by comparing it against a range of basic naive methods."],"url":"http://arxiv.org/abs/2405.07359v1","category":"cs.LG"}
{"created":"2024-05-12 18:42:20","title":"Exploring late-time cosmic acceleration: A study of a linear $f(T)$ cosmological model using observational data","abstract":"Understanding the evolution of dark energy poses a significant challenge in modern cosmology, as it is responsible for the universe's accelerated expansion. In this study, we focus on a specific $f(T)$ cosmological model and analyze its behavior using observational data, including 31 data points from the CC dataset, 1048 points from the Pantheon SNe Ia samples, and 6 points from the BAO dataset. By considering a linear $f(T)$ model with an additional constant term, we derive the expression for the Hubble parameter as a function of cosmic redshift for non-relativistic pressureless matter. We obtain the best-fit values for the Hubble constant, $H_0$, and the model parameters $\\alpha$ and $\\beta$, indicating a stable model capable of explaining late-time cosmic acceleration without invoking a dark energy component. This is achieved through modifying field equations to account for the observed accelerated expansion of the universe.","sentences":["Understanding the evolution of dark energy poses a significant challenge in modern cosmology, as it is responsible for the universe's accelerated expansion.","In this study, we focus on a specific $f(T)$ cosmological model and analyze its behavior using observational data, including 31 data points from the CC dataset, 1048 points from the Pantheon SNe Ia samples, and 6 points from the BAO dataset.","By considering a linear $f(T)$ model with an additional constant term, we derive the expression for the Hubble parameter as a function of cosmic redshift for non-relativistic pressureless matter.","We obtain the best-fit values for the Hubble constant, $H_0$, and the model parameters $\\alpha$ and $\\beta$, indicating a stable model capable of explaining late-time cosmic acceleration without invoking a dark energy component.","This is achieved through modifying field equations to account for the observed accelerated expansion of the universe."],"url":"http://arxiv.org/abs/2405.07357v1","category":"astro-ph.CO"}
{"created":"2024-05-12 16:54:57","title":"Stochastic Bandits with ReLU Neural Networks","abstract":"We study the stochastic bandit problem with ReLU neural network structure. We show that a $\\tilde{O}(\\sqrt{T})$ regret guarantee is achievable by considering bandits with one-layer ReLU neural networks; to the best of our knowledge, our work is the first to achieve such a guarantee. In this specific setting, we propose an OFU-ReLU algorithm that can achieve this upper bound. The algorithm first explores randomly until it reaches a linear regime, and then implements a UCB-type linear bandit algorithm to balance exploration and exploitation. Our key insight is that we can exploit the piecewise linear structure of ReLU activations and convert the problem into a linear bandit in a transformed feature space, once we learn the parameters of ReLU relatively accurately during the exploration stage. To remove dependence on model parameters, we design an OFU-ReLU+ algorithm based on a batching strategy, which can provide the same theoretical guarantee.","sentences":["We study the stochastic bandit problem with ReLU neural network structure.","We show that a $\\tilde{O}(\\sqrt{T})$ regret guarantee is achievable by considering bandits with one-layer ReLU neural networks; to the best of our knowledge, our work is the first to achieve such a guarantee.","In this specific setting, we propose an OFU-ReLU algorithm that can achieve this upper bound.","The algorithm first explores randomly until it reaches a linear regime, and then implements a UCB-type linear bandit algorithm to balance exploration and exploitation.","Our key insight is that we can exploit the piecewise linear structure of ReLU activations and convert the problem into a linear bandit in a transformed feature space, once we learn the parameters of ReLU relatively accurately during the exploration stage.","To remove dependence on model parameters, we design an OFU-ReLU+ algorithm based on a batching strategy, which can provide the same theoretical guarantee."],"url":"http://arxiv.org/abs/2405.07331v1","category":"cs.LG"}
{"created":"2024-05-12 16:37:49","title":"An algorithm for distributed time delay identification based on a mixed Erlang kernel approximation and the linear chain trick","abstract":"Time delays are ubiquitous in industry and nature, and they significantly affect both transient dynamics and stability properties. Consequently, it is often necessary to identify and account for the delays when, e.g., designing a model-based control strategy. However, identifying delays in differential equations is not straightforward and requires specialized methods. Therefore, we propose an algorithm for identifying distributed delays in delay differential equations (DDEs) that only involves simulation of ordinary differential equations (ODEs). Specifically, we 1) approximate the kernel in the DDEs (also called the memory function) by the probability density function of a mixed Erlang distribution and 2) use the linear chain trick (LCT) to transform the resulting DDEs into ODEs. Finally, the parameters in the kernel approximation are estimated as the solution to a dynamical least-squares problem, and we use a single-shooting approach to approximate this solution. We demonstrate the efficacy of the algorithm using numerical examples that involve the logistic equation and a point reactor kinetics model of a molten salt nuclear fission reactor.","sentences":["Time delays are ubiquitous in industry and nature, and they significantly affect both transient dynamics and stability properties.","Consequently, it is often necessary to identify and account for the delays when, e.g., designing a model-based control strategy.","However, identifying delays in differential equations is not straightforward and requires specialized methods.","Therefore, we propose an algorithm for identifying distributed delays in delay differential equations (DDEs) that only involves simulation of ordinary differential equations (ODEs).","Specifically, we 1) approximate the kernel in the DDEs (also called the memory function) by the probability density function of a mixed Erlang distribution and 2) use the linear chain trick (LCT) to transform the resulting DDEs into ODEs.","Finally, the parameters in the kernel approximation are estimated as the solution to a dynamical least-squares problem, and we use a single-shooting approach to approximate this solution.","We demonstrate the efficacy of the algorithm using numerical examples that involve the logistic equation and a point reactor kinetics model of a molten salt nuclear fission reactor."],"url":"http://arxiv.org/abs/2405.07328v1","category":"math.DS"}
{"created":"2024-05-12 16:17:35","title":"A Gromov-Witten approach to $G$-equivariant birational invariants","abstract":"In arXiv:2404.19088, we initiated a program linking birational invariants with smooth ones and offering new interpretations of classical invariants, such as the Kervaire-Milnor invariants. Here, we rely on the profound geometric reasoning provided by Lupercio and Uribe in the early 2000s to establish a connection between Chen-Ruan cohomology and the $G$-birational invariants introduced by Kontsevich, Pestun, and Tschinkel in recent pioneering work, along with presenting applications. The final section of this paper explores conjectures and preliminary results regarding gerbes, connections over orbifolds, discrete torsion, and potential approaches via motivic integration and twisted K-theory for $G$-birationality. Combined with the theory of atoms by Katzarkov, Kontsevich, Pantev, and Yu, the proposal in this paper program will lead to a theory of equivariant atoms.","sentences":["In arXiv:2404.19088, we initiated a program linking birational invariants with smooth ones and offering new interpretations of classical invariants, such as the Kervaire-Milnor invariants.","Here, we rely on the profound geometric reasoning provided by Lupercio and Uribe in the early 2000s to establish a connection between Chen-Ruan cohomology and the $G$-birational invariants introduced by Kontsevich, Pestun, and Tschinkel in recent pioneering work, along with presenting applications.","The final section of this paper explores conjectures and preliminary results regarding gerbes, connections over orbifolds, discrete torsion, and potential approaches via motivic integration and twisted K-theory for $G$-birationality.","Combined with the theory of atoms by Katzarkov, Kontsevich, Pantev, and Yu, the proposal in this paper program will lead to a theory of equivariant atoms."],"url":"http://arxiv.org/abs/2405.07322v1","category":"math.AG"}
{"created":"2024-05-12 15:50:31","title":"A Sharp condition on global wellposedness of Chern-Simon-Shr\u00f6dinger equation","abstract":"In this work, we derive a sharp condition on the mass of the initial data for the global existence of the Chern-Simon-Shr\\\"odinger equation. As a corollary, we prove that if the strength of interaction is less than the Bogomolny bound, then, for a large enough mass of initial data, there exists a globally defined solution. On the other hand, for the interactions which are above the Bogomolny bound, the critical mass condition on the initial data for the global existence depends on the strength of the self-interacting field. Then, we show that the states with the initial critical mass and zero energy are standing wave solutions and globally well-posed. Moreover, they are static if the self-interacting field is large enough and non-static for small self-interacting field.","sentences":["In this work, we derive a sharp condition on the mass of the initial data for the global existence of the Chern-Simon-Shr\\\"odinger equation.","As a corollary, we prove that if the strength of interaction is less than the Bogomolny bound, then, for a large enough mass of initial data, there exists a globally defined solution.","On the other hand, for the interactions which are above the Bogomolny bound, the critical mass condition on the initial data for the global existence depends on the strength of the self-interacting field.","Then, we show that the states with the initial critical mass and zero energy are standing wave solutions and globally well-posed.","Moreover, they are static if the self-interacting field is large enough and non-static for small self-interacting field."],"url":"http://arxiv.org/abs/2405.07315v1","category":"math.AP"}
{"created":"2024-05-12 15:48:12","title":"Scattering cross section of the long gravitino wave in Schwarzschild spacetime","abstract":"The scattering problem of the gravitino wave in Schwarzschild spacetime is explored. We employ the perturbative method to calculate the differential scattering cross section in the long wavelength limit, and demonstrate that it satisfies the same rule of the spin dependance as those of the scalar, neutrino, electromagnetic waves.","sentences":["The scattering problem of the gravitino wave in Schwarzschild spacetime is explored.","We employ the perturbative method to calculate the differential scattering cross section in the long wavelength limit, and demonstrate that it satisfies the same rule of the spin dependance as those of the scalar, neutrino, electromagnetic waves."],"url":"http://arxiv.org/abs/2405.07313v1","category":"gr-qc"}
{"created":"2024-05-12 15:22:19","title":"Point Resampling and Ray Transformation Aid to Editable NeRF Models","abstract":"In NeRF-aided editing tasks, object movement presents difficulties in supervision generation due to the introduction of variability in object positions. Moreover, the removal operations of certain scene objects often lead to empty regions, presenting challenges for NeRF models in inpainting them effectively. We propose an implicit ray transformation strategy, allowing for direct manipulation of the 3D object's pose by operating on the neural-point in NeRF rays. To address the challenge of inpainting potential empty regions, we present a plug-and-play inpainting module, dubbed differentiable neural-point resampling (DNR), which interpolates those regions in 3D space at the original ray locations within the implicit space, thereby facilitating object removal & scene inpainting tasks. Importantly, employing DNR effectively narrows the gap between ground truth and predicted implicit features, potentially increasing the mutual information (MI) of the features across rays. Then, we leverage DNR and ray transformation to construct a point-based editable NeRF pipeline PR^2T-NeRF. Results primarily evaluated on 3D object removal & inpainting tasks indicate that our pipeline achieves state-of-the-art performance. In addition, our pipeline supports high-quality rendering visualization for diverse editing operations without necessitating extra supervision.","sentences":["In NeRF-aided editing tasks, object movement presents difficulties in supervision generation due to the introduction of variability in object positions.","Moreover, the removal operations of certain scene objects often lead to empty regions, presenting challenges for NeRF models in inpainting them effectively.","We propose an implicit ray transformation strategy, allowing for direct manipulation of the 3D object's pose by operating on the neural-point in NeRF rays.","To address the challenge of inpainting potential empty regions, we present a plug-and-play inpainting module, dubbed differentiable neural-point resampling (DNR), which interpolates those regions in 3D space at the original ray locations within the implicit space, thereby facilitating object removal & scene inpainting tasks.","Importantly, employing DNR effectively narrows the gap between ground truth and predicted implicit features, potentially increasing the mutual information (MI) of the features across rays.","Then, we leverage DNR and ray transformation to construct a point-based editable NeRF pipeline PR^2T-NeRF.","Results primarily evaluated on 3D object removal & inpainting tasks indicate that our pipeline achieves state-of-the-art performance.","In addition, our pipeline supports high-quality rendering visualization for diverse editing operations without necessitating extra supervision."],"url":"http://arxiv.org/abs/2405.07306v1","category":"cs.CV"}
{"created":"2024-05-12 14:58:18","title":"Influence of initial correlations on evolution over time of an open quantum system","abstract":"A novel approach to accounting for the influence of initial system-bath correlations on the dynamics of an open quantum system, based on the conventional projection operator technique, is suggested. To avoid the difficulties of treating the initial correlations, the conventional Nakajima-Zwanzig inhomogeneous generalized master equations (GMEs) for a system's reduced statistical operator and correlation function are exactly converted into the homogeneous GMEs (HGMEs), which take into account the initial correlations in the kernel governing the evolution of these HGMEs. In the second order (Born) approximation in the system-bath interaction, the obtained HGMEs are local in time and valid at all timescales. They are further specialized for a realistic equilibrium Gibbs initial (at $t=t_0$) system+bath state (for a system reduced statistical operator an external force at $t>t_0$ is applied) and then for a bath of oscillators (Boson field). As an example, the evolution of a selected quantum oscillator (a localized mode) interacting with a Boson field (Fano-like model) is considered at different timescales. It is shown explicitly how the initial correlations influence the oscillator evolution process. In particular, it is shown that the equilibrium system's correlation function acquires at the large timescale the additional constant phase factor conditioned by survived initial system-bath correlations.","sentences":["A novel approach to accounting for the influence of initial system-bath correlations on the dynamics of an open quantum system, based on the conventional projection operator technique, is suggested.","To avoid the difficulties of treating the initial correlations, the conventional Nakajima-Zwanzig inhomogeneous generalized master equations (GMEs) for a system's reduced statistical operator and correlation function are exactly converted into the homogeneous GMEs (HGMEs), which take into account the initial correlations in the kernel governing the evolution of these HGMEs.","In the second order (Born) approximation in the system-bath interaction, the obtained HGMEs are local in time and valid at all timescales.","They are further specialized for a realistic equilibrium Gibbs initial (at $t=t_0$) system+bath state (for a system reduced statistical operator an external force at $t>t_0$ is applied) and then for a bath of oscillators (Boson field).","As an example, the evolution of a selected quantum oscillator (a localized mode) interacting with a Boson field (Fano-like model) is considered at different timescales.","It is shown explicitly how the initial correlations influence the oscillator evolution process.","In particular, it is shown that the equilibrium system's correlation function acquires at the large timescale the additional constant phase factor conditioned by survived initial system-bath correlations."],"url":"http://arxiv.org/abs/2405.07299v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-12 14:55:58","title":"Crystal-like thermal transport in amorphous carbon","abstract":"Thermal transport properties of amorphous carbon has attracted increasing attention due to its extreme thermal properties: It has been reported to have among the highest thermal conductivity for bulk amorphous solids up to $\\sim$ 37 Wm\\textsuperscript{-1}K\\textsuperscript{-1}, comparable to crystalline sapphire ($\\alpha$-Al\\textsubscript{2}O\\textsubscript{3}). Further, large density dependence in thermal conductivity demonstrates a potential for largely tunable thermal conductivity. However, mechanism behind the high thermal conductivity and its large density dependence remains elusive due to many variables at play. In this work, we perform large-scale ($\\sim$ 10\\textsuperscript{5} atoms) molecular dynamics simulations utilizing a machine learning potential based on neural networks. Through spectral decomposition of thermal conductivity which enables a quantum correction to classical heat capacity, we find that propagating vibrational excitations govern thermal transport in amorphous carbon ($\\sim$ 100 \\% of thermal conductivity) in sharp contrast to the conventional wisdom that diffusive vibrational excitations dominate thermal transport in amorphous solids. Instead, this remarkable behavior resembles thermal transport in simple crystals. Moreover, our temperature dependent spectral diffusivity and velocity current correlation analyses reveal that the density dependent thermal conductivity originates from anharmonicity sensitive propagating excitations. Our work suggests a novel insight and design principle into developing mechanically hard, thermally conductive amorphous solids.","sentences":["Thermal transport properties of amorphous carbon has attracted increasing attention due to its extreme thermal properties: It has been reported to have among the highest thermal conductivity for bulk amorphous solids up to $\\sim$ 37 Wm\\textsuperscript{-1}K\\textsuperscript{-1}, comparable to crystalline sapphire ($\\alpha$-Al\\textsubscript{2}O\\textsubscript{3}).","Further, large density dependence in thermal conductivity demonstrates a potential for largely tunable thermal conductivity.","However, mechanism behind the high thermal conductivity and its large density dependence remains elusive due to many variables at play.","In this work, we perform large-scale ($\\sim$ 10\\textsuperscript{5} atoms)","molecular dynamics simulations utilizing a machine learning potential based on neural networks.","Through spectral decomposition of thermal conductivity which enables a quantum correction to classical heat capacity, we find that propagating vibrational excitations govern thermal transport in amorphous carbon ($\\sim$ 100 \\% of thermal conductivity) in sharp contrast to the conventional wisdom that diffusive vibrational excitations dominate thermal transport in amorphous solids.","Instead, this remarkable behavior resembles thermal transport in simple crystals.","Moreover, our temperature dependent spectral diffusivity and velocity current correlation analyses reveal that the density dependent thermal conductivity originates from anharmonicity sensitive propagating excitations.","Our work suggests a novel insight and design principle into developing mechanically hard, thermally conductive amorphous solids."],"url":"http://arxiv.org/abs/2405.07298v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-12 14:06:05","title":"Robust Beamforming with Gradient-based Liquid Neural Network","abstract":"Millimeter-wave (mmWave) multiple-input multiple-output (MIMO) communication with the advanced beamforming technologies is a key enabler to meet the growing demands of future mobile communication. However, the dynamic nature of cellular channels in large-scale urban mmWave MIMO communication scenarios brings substantial challenges, particularly in terms of complexity and robustness. To address these issues, we propose a robust gradient-based liquid neural network (GLNN) framework that utilizes ordinary differential equation-based liquid neurons to solve the beamforming problem. Specifically, our proposed GLNN framework takes gradients of the optimization objective function as inputs to extract the high-order channel feature information, and then introduces a residual connection to mitigate the training burden. Furthermore, we use the manifold learning technique to compress the search space of the beamforming problem. These designs enable the GLNN to effectively maintain low complexity while ensuring strong robustness to noisy and highly dynamic channels. Extensive simulation results demonstrate that the GLNN can achieve 4.15% higher spectral efficiency than that of typical iterative algorithms, and reduce the time consumption to only 1.61% that of conventional methods.","sentences":["Millimeter-wave (mmWave) multiple-input multiple-output (MIMO) communication with the advanced beamforming technologies is a key enabler to meet the growing demands of future mobile communication.","However, the dynamic nature of cellular channels in large-scale urban mmWave MIMO communication scenarios brings substantial challenges, particularly in terms of complexity and robustness.","To address these issues, we propose a robust gradient-based liquid neural network (GLNN) framework that utilizes ordinary differential equation-based liquid neurons to solve the beamforming problem.","Specifically, our proposed GLNN framework takes gradients of the optimization objective function as inputs to extract the high-order channel feature information, and then introduces a residual connection to mitigate the training burden.","Furthermore, we use the manifold learning technique to compress the search space of the beamforming problem.","These designs enable the GLNN to effectively maintain low complexity while ensuring strong robustness to noisy and highly dynamic channels.","Extensive simulation results demonstrate that the GLNN can achieve 4.15% higher spectral efficiency than that of typical iterative algorithms, and reduce the time consumption to only 1.61% that of conventional methods."],"url":"http://arxiv.org/abs/2405.07291v1","category":"cs.IT"}
{"created":"2024-05-12 13:54:29","title":"Solutions of time fractional anomalous diffusion equations with coefficients depending on both time and space variables","abstract":"We derive explicit solutions for time-fractional anomalous diffusion equations with diffusivity coefficients that depend on both space and time variables. These solutions are expressed in Fox-H and generalized Wright functions, which are commonly used in anomalous diffusion equations. Our study represents a significant advancement in our understanding of anomalous diffusion with potential applications in a wide range of fields.","sentences":["We derive explicit solutions for time-fractional anomalous diffusion equations with diffusivity coefficients that depend on both space and time variables.","These solutions are expressed in Fox-H and generalized Wright functions, which are commonly used in anomalous diffusion equations.","Our study represents a significant advancement in our understanding of anomalous diffusion with potential applications in a wide range of fields."],"url":"http://arxiv.org/abs/2405.07285v1","category":"math.AP"}
{"created":"2024-05-12 12:20:26","title":"Architecture-Level Modeling of Photonic Deep Neural Network Accelerators","abstract":"Photonics is a promising technology to accelerate Deep Neural Networks as it can use optical interconnects to reduce data movement energy and it enables low-energy, high-throughput optical-analog computations.   To realize these benefits in a full system (accelerator + DRAM), designers must ensure that the benefits of using the electrical, optical, analog, and digital domains exceed the costs of converting data between domains. Designers must also consider system-level energy costs such as data fetch from DRAM. Converting data and accessing DRAM can consume significant energy, so to evaluate and explore the photonic system space, there is a need for a tool that can model these full-system considerations.   In this work, we show that similarities between Compute-in-Memory (CiM) and photonics let us use CiM system modeling tools to accurately model photonics systems. Bringing modeling tools to photonics enables evaluation of photonic research in a full-system context, rapid design space exploration, co-design, and comparison between systems.   Using our open-source model, we show that cross-domain conversion and DRAM can consume a significant portion of photonic system energy. We then demonstrate optimizations that reduce conversions and DRAM accesses to improve photonic system energy efficiency by up to 3x.","sentences":["Photonics is a promising technology to accelerate Deep Neural Networks as it can use optical interconnects to reduce data movement energy and it enables low-energy, high-throughput optical-analog computations.   ","To realize these benefits in a full system (accelerator + DRAM), designers must ensure that the benefits of using the electrical, optical, analog, and digital domains exceed the costs of converting data between domains.","Designers must also consider system-level energy costs such as data fetch from DRAM.","Converting data and accessing DRAM can consume significant energy, so to evaluate and explore the photonic system space, there is a need for a tool that can model these full-system considerations.   ","In this work, we show that similarities between Compute-in-Memory (CiM) and photonics let us use CiM system modeling tools to accurately model photonics systems.","Bringing modeling tools to photonics enables evaluation of photonic research in a full-system context, rapid design space exploration, co-design, and comparison between systems.   ","Using our open-source model, we show that cross-domain conversion and DRAM can consume a significant portion of photonic system energy.","We then demonstrate optimizations that reduce conversions and DRAM accesses to improve photonic system energy efficiency by up to 3x."],"url":"http://arxiv.org/abs/2405.07266v1","category":"cs.ET"}
{"created":"2024-05-12 10:53:58","title":"A continuum geometric approach for inverse design of origami structures","abstract":"Miura-Ori, a celebrated origami pattern that facilitates functionality in matter, has found multiple applications in the field of mechanical metamaterials. Modifications of Miura-Ori pattern can produce curved configurations during folding, thereby enhancing its potential functionalities. Thus, a key challenge in designing generalized Miura-Ori structures is to tailor their folding patterns to achieve desired geometries. In this work, we address this inverse-design problem by developing a new continuum framework for the differential geometry of generalized Miura-Ori. By assuming that the perturbation to the classical Miura-Ori is slowly varying in space, we derive analytical relations between geometrical properties and the perturbation field. These relationships are shown to be invertible, allowing us to design complex curved geometries. Our framework enables porting knowledge, methods and tools from continuum theories of matter and differential geometry to the field of origami metamaterials.","sentences":["Miura-Ori, a celebrated origami pattern that facilitates functionality in matter, has found multiple applications in the field of mechanical metamaterials.","Modifications of Miura-Ori pattern can produce curved configurations during folding, thereby enhancing its potential functionalities.","Thus, a key challenge in designing generalized Miura-Ori structures is to tailor their folding patterns to achieve desired geometries.","In this work, we address this inverse-design problem by developing a new continuum framework for the differential geometry of generalized Miura-Ori.","By assuming that the perturbation to the classical Miura-Ori is slowly varying in space, we derive analytical relations between geometrical properties and the perturbation field.","These relationships are shown to be invertible, allowing us to design complex curved geometries.","Our framework enables porting knowledge, methods and tools from continuum theories of matter and differential geometry to the field of origami metamaterials."],"url":"http://arxiv.org/abs/2405.07249v1","category":"cond-mat.soft"}
{"created":"2024-05-12 10:35:19","title":"Ecology, Spatial Structure, and Selection Pressure Induce Strong Signatures in Phylogenetic Structure","abstract":"Evolutionary dynamics are shaped by a variety of fundamental, generic drivers, including spatial structure, ecology, and selection pressure. These drivers impact the trajectory of evolution, and have been hypothesized to influence phylogenetic structure. Here, we set out to assess (1) if spatial structure, ecology, and selection pressure leave detectable signatures in phylogenetic structure, (2) the extent, in particular, to which ecology can be detected and discerned in the presence of spatial structure, and (3) the extent to which these phylogenetic signatures generalize across evolutionary systems. To this end, we analyze phylogenies generated by manipulating spatial structure, ecology, and selection pressure within three computational models of varied scope and sophistication. We find that selection pressure, spatial structure, and ecology have characteristic effects on phylogenetic metrics, although these effects are complex and not always intuitive. Signatures have some consistency across systems when using equivalent taxonomic unit definitions (e.g., individual, genotype, species). Further, we find that sufficiently strong ecology can be detected in the presence of spatial structure. We also find that, while low-resolution phylogenetic reconstructions can bias some phylogenetic metrics, high-resolution reconstructions recapitulate them faithfully. Although our results suggest potential for evolutionary inference of spatial structure, ecology, and selection pressure through phylogenetic analysis, further methods development is needed to distinguish these drivers' phylometric signatures from each other and to appropriately normalize phylogenetic metrics. With such work, phylogenetic analysis could provide a versatile toolkit to study large-scale evolving populations.","sentences":["Evolutionary dynamics are shaped by a variety of fundamental, generic drivers, including spatial structure, ecology, and selection pressure.","These drivers impact the trajectory of evolution, and have been hypothesized to influence phylogenetic structure.","Here, we set out to assess (1) if spatial structure, ecology, and selection pressure leave detectable signatures in phylogenetic structure, (2) the extent, in particular, to which ecology can be detected and discerned in the presence of spatial structure, and (3) the extent to which these phylogenetic signatures generalize across evolutionary systems.","To this end, we analyze phylogenies generated by manipulating spatial structure, ecology, and selection pressure within three computational models of varied scope and sophistication.","We find that selection pressure, spatial structure, and ecology have characteristic effects on phylogenetic metrics, although these effects are complex and not always intuitive.","Signatures have some consistency across systems when using equivalent taxonomic unit definitions (e.g., individual, genotype, species).","Further, we find that sufficiently strong ecology can be detected in the presence of spatial structure.","We also find that, while low-resolution phylogenetic reconstructions can bias some phylogenetic metrics, high-resolution reconstructions recapitulate them faithfully.","Although our results suggest potential for evolutionary inference of spatial structure, ecology, and selection pressure through phylogenetic analysis, further methods development is needed to distinguish these drivers' phylometric signatures from each other and to appropriately normalize phylogenetic metrics.","With such work, phylogenetic analysis could provide a versatile toolkit to study large-scale evolving populations."],"url":"http://arxiv.org/abs/2405.07245v1","category":"q-bio.PE"}
{"created":"2024-05-12 09:34:30","title":"Exclusive $J/\u03a8$ and $\u03c1^0$ vector meson production using BK evolution theory","abstract":"Exclusive diffractive processes, such as exclusive vector meson production, serve as excellent probes of hadron structure within the perturbative regime of quantum chromodynamics (QCD). The exclusive process involving light and heavy vector mesons, $e p \\rightarrow e V(V=J/\\Psi,\\rho,\\phi)$, has been investigated at the HERA accelerator facility. In this study, we focus on the theoretical prediction of exclusive $J/\\Psi$ and $\\rho^0$ vector meson production. Employing the color dipole description of deep inelastic scattering (DIS), we calculate both the differential cross-section and total cross-section of $J/\\Psi$ and $\\rho^0$ vector mesons, employing an analytical solution of the Balitsky-Kovchegov (BK) equation. Furthermore, we present the ratio of the longitudinal to the transverse cross-section for $J/\\Psi$ and $\\rho^0$ as a function of $Q^2$. Two well-known vector meson wave function models, Boosted Gaussian (BG) and Gaus-LC, have been integrated into our analysis, showing slight sensitivity to the chosen vector meson wave functions. Our theoretical predictions agree well with the available experimental data for vector meson production. The analytical solution of the BK equation proves reliable for the theoretical prediction of exclusive vector mesons within a specific range of $Q^2$.","sentences":["Exclusive diffractive processes, such as exclusive vector meson production, serve as excellent probes of hadron structure within the perturbative regime of quantum chromodynamics (QCD).","The exclusive process involving light and heavy vector mesons, $e p \\rightarrow e V(V=J/\\Psi,\\rho,\\phi)$, has been investigated at the HERA accelerator facility.","In this study, we focus on the theoretical prediction of exclusive $J/\\Psi$ and $\\rho^0$ vector meson production.","Employing the color dipole description of deep inelastic scattering (DIS), we calculate both the differential cross-section and total cross-section of $J/\\Psi$ and $\\rho^0$ vector mesons, employing an analytical solution of the Balitsky-Kovchegov (BK) equation.","Furthermore, we present the ratio of the longitudinal to the transverse cross-section for $J/\\Psi$ and $\\rho^0$ as a function of $Q^2$. Two well-known vector meson wave function models, Boosted Gaussian (BG) and Gaus-LC, have been integrated into our analysis, showing slight sensitivity to the chosen vector meson wave functions.","Our theoretical predictions agree well with the available experimental data for vector meson production.","The analytical solution of the BK equation proves reliable for the theoretical prediction of exclusive vector mesons within a specific range of $Q^2$."],"url":"http://arxiv.org/abs/2405.07234v1","category":"hep-ph"}
{"created":"2024-05-12 09:07:54","title":"Improved convergence rates for the Hele-Shaw limit in the presence of confining potentials","abstract":"Nowadays a vast literature is available on the Hele-Shaw or incompressible limit for nonlinear degenerate diffusion equations. This problem has attracted a lot of attention due to its applications to tissue growth and crowd motion modelling as it constitutes a way to link soft congestion (or compressible) models to hard congestion (or incompressible) descriptions. In this paper, we address the question of estimating the rate of this asymptotics in the presence of external drifts. In particular, we provide improved results in the 2-Wasserstein distance which are global in time thanks to the contractivity property that holds for strictly convex potentials.","sentences":["Nowadays a vast literature is available on the Hele-Shaw or incompressible limit for nonlinear degenerate diffusion equations.","This problem has attracted a lot of attention due to its applications to tissue growth and crowd motion modelling as it constitutes a way to link soft congestion (or compressible) models to hard congestion (or incompressible) descriptions.","In this paper, we address the question of estimating the rate of this asymptotics in the presence of external drifts.","In particular, we provide improved results in the 2-Wasserstein distance which are global in time thanks to the contractivity property that holds for strictly convex potentials."],"url":"http://arxiv.org/abs/2405.07227v1","category":"math.AP"}
{"created":"2024-05-12 08:23:48","title":"Interior pointwise regularity for elliptic and parabolic equations in divergence form and applications to nodal sets","abstract":"In this paper, we obtain the interior pointwise $C^{k,\\alpha}$ ($k\\geq 0$, $0<\\alpha<1$) regularity for weak solutions of elliptic and parabolic equations in divergence form. The compactness method and perturbation technique are employed. The pointwise regularity is proved in a very simple way and the results are optimal. In addition, these pointwise regularity can be used to characterize the structure of the nodal sets of solutions.","sentences":["In this paper, we obtain the interior pointwise $C^{k,\\alpha}$ ($k\\geq 0$, $0<\\alpha<1$) regularity for weak solutions of elliptic and parabolic equations in divergence form.","The compactness method and perturbation technique are employed.","The pointwise regularity is proved in a very simple way and the results are optimal.","In addition, these pointwise regularity can be used to characterize the structure of the nodal sets of solutions."],"url":"http://arxiv.org/abs/2405.07214v1","category":"math.AP"}
{"created":"2024-05-12 08:20:44","title":"A complete pair of solvents of a quadratic matrix pencil","abstract":"Let $B$ and $C$ be square complex matrices. The differential equation \\begin{equation*} x''(t)+Bx'(t)+Cx(t)=f(t) \\end{equation*} is considered. A solvent is a matrix solution $X$ of the equation $X^2+BX+C=\\mathbf0$. A pair of solvents $X$ and $Z$ is called complete if the matrix $X-Z$ is invertible. Knowing a complete pair of solvents $X$ and $Z$ allows us to reduce the solution of the initial value problem to the calculation of two matrix exponentials $e^{Xt}$ and $e^{Zt}$. The problem of finding a complete pair $X$ and $Z$, which leads to small rounding errors in solving the differential equation, is discussed.","sentences":["Let $B$ and $C$ be square complex matrices.","The differential equation \\begin{equation*} x''(t)+Bx'(t)+Cx(t)=f(t) \\end{equation*} is considered.","A solvent is a matrix solution $X$ of the equation $X^2+BX+C=\\mathbf0$. A pair of solvents $X$ and $Z$ is called complete if the matrix $X-Z$ is invertible.","Knowing a complete pair of solvents $X$ and $Z$ allows us to reduce the solution of the initial value problem to the calculation of two matrix exponentials $e^{Xt}$ and $e^{Zt}$. The problem of finding a complete pair $X$ and $Z$, which leads to small rounding errors in solving the differential equation, is discussed."],"url":"http://arxiv.org/abs/2405.07210v1","category":"math.NA"}
{"created":"2024-05-12 07:55:26","title":"Pointwise regularity for locally uniformly elliptic equations and applications","abstract":"In this paper, we study the regularity for viscosity solutions of locally uniformly elliptic equations and obtain a series of interior pointwise $C^{k,\\alpha}$ ($k\\geq 1$, $0<\\alpha<1$) regularity with smallness assumptions on the solution and the right-hand term. As applications, we obtain various interior pointwise regularity for several classical elliptic equations, i.e., the prescribed mean curvature equation, the Monge-Amp\\`{e}re equation, the $k$-Hessian equations, the $k$-Hessian quotient equations and the Lagrangian mean curvature equation. Moreover, the smallness assumptions are necessary in most cases (Remark 2.6, Remark 3.5, Remark 4.7, Remark 5.4 and Remark 6.5).","sentences":["In this paper, we study the regularity for viscosity solutions of locally uniformly elliptic equations and obtain a series of interior pointwise $C^{k,\\alpha}$ ($k\\geq 1$, $0<\\alpha<1$) regularity with smallness assumptions on the solution and the right-hand term.","As applications, we obtain various interior pointwise regularity for several classical elliptic equations, i.e., the prescribed mean curvature equation, the Monge-Amp\\`{e}re equation, the $k$-Hessian equations, the $k$-Hessian quotient equations and the Lagrangian mean curvature equation.","Moreover, the smallness assumptions are necessary in most cases (Remark 2.6, Remark 3.5, Remark 4.7, Remark 5.4 and Remark 6.5)."],"url":"http://arxiv.org/abs/2405.07199v1","category":"math.AP"}
{"created":"2024-05-12 07:53:28","title":"Dephasing-induced mobility edges in quasicrystals","abstract":"Mobility edges (ME), separating Anderson-localized states from extended states, are known to arise in the single-particle energy spectrum of certain one-dimensional lattices with aperiodic order. Dephasing and decoherence effects are widely acknowledged to spoil Anderson localization and to enhance transport, suggesting that ME and localization are unlikely to be observable in the presence of dephasing. Here it is shown that, contrary to such a wisdom, ME can be created by pure dephasing effects in quasicrystals in which all states are delocalized under coherent dynamics. Since the lifetimes of localized states induced by dephasing effects can be extremely long, rather counter-intuitively decoherence can enhance localization of excitation in the lattice. The results are illustrated by considering photonic quantum walks in synthetic mesh lattices.","sentences":["Mobility edges (ME), separating Anderson-localized states from extended states, are known to arise in the single-particle energy spectrum of certain one-dimensional lattices with aperiodic order.","Dephasing and decoherence effects are widely acknowledged to spoil Anderson localization and to enhance transport, suggesting that ME and localization are unlikely to be observable in the presence of dephasing.","Here it is shown that, contrary to such a wisdom, ME can be created by pure dephasing effects in quasicrystals in which all states are delocalized under coherent dynamics.","Since the lifetimes of localized states induced by dephasing effects can be extremely long, rather counter-intuitively decoherence can enhance localization of excitation in the lattice.","The results are illustrated by considering photonic quantum walks in synthetic mesh lattices."],"url":"http://arxiv.org/abs/2405.07198v1","category":"quant-ph"}
{"created":"2024-05-12 06:56:00","title":"Birth, interactions, and evolution over topography of solitons in Serre-Green-Naghdi model","abstract":"New evidence of surprising robustness of solitary-wave solutions of the Serre-Green-Naghdi (SGN) equations is presented on the basis of high-resolution numerical simulations conducted using a novel well-balanced finite-volume method. SGN solitons exhibit a striking resemblance with their celebrated Korteweg-deVries (KdV) counterparts. Co-moving solitons are shown to exit intact from double and triple collisions with a remarkably small wave-wake residual. The counter-propagating solitons experiencing frontal collisions and solitons hitting a wall, non-existing in KdV case configurations, are shown to also recover, but with a much larger than in co-moving case residual, confirming with higher precision the results known in the literature. Multiple SGN solitons emerging from localized initial conditions are exhibited, and it is demonstrated that SGN solitons survive hitting localized topographic obstacles, and generate secondary solitons when they encounter a rising escarpment.","sentences":["New evidence of surprising robustness of solitary-wave solutions of the Serre-Green-Naghdi (SGN) equations is presented on the basis of high-resolution numerical simulations conducted using a novel well-balanced finite-volume method.","SGN solitons exhibit a striking resemblance with their celebrated Korteweg-deVries (KdV) counterparts.","Co-moving solitons are shown to exit intact from double and triple collisions with a remarkably small wave-wake residual.","The counter-propagating solitons experiencing frontal collisions and solitons hitting a wall, non-existing in KdV case configurations, are shown to also recover, but with a much larger than in co-moving case residual, confirming with higher precision the results known in the literature.","Multiple SGN solitons emerging from localized initial conditions are exhibited, and it is demonstrated that SGN solitons survive hitting localized topographic obstacles, and generate secondary solitons when they encounter a rising escarpment."],"url":"http://arxiv.org/abs/2405.07182v1","category":"nlin.PS"}
{"created":"2024-05-12 06:08:21","title":"CRSFL: Cluster-based Resource-aware Split Federated Learning for Continuous Authentication","abstract":"In the ever-changing world of technology, continuous authentication and comprehensive access management are essential during user interactions with a device. Split Learning (SL) and Federated Learning (FL) have recently emerged as promising technologies for training a decentralized Machine Learning (ML) model. With the increasing use of smartphones and Internet of Things (IoT) devices, these distributed technologies enable users with limited resources to complete neural network model training with server assistance and collaboratively combine knowledge between different nodes. In this study, we propose combining these technologies to address the continuous authentication challenge while protecting user privacy and limiting device resource usage. However, the model's training is slowed due to SL sequential training and resource differences between IoT devices with different specifications. Therefore, we use a cluster-based approach to group devices with similar capabilities to mitigate the impact of slow devices while filtering out the devices incapable of training the model. In addition, we address the efficiency and robustness of training ML models by using SL and FL techniques to train the clients simultaneously while analyzing the overhead burden of the process. Following clustering, we select the best set of clients to participate in training through a Genetic Algorithm (GA) optimized on a carefully designed list of objectives. The performance of our proposed framework is compared to baseline methods, and the advantages are demonstrated using a real-life UMDAA-02-FD face detection dataset. The results show that CRSFL, our proposed approach, maintains high accuracy and reduces the overhead burden in continuous authentication scenarios while preserving user privacy.","sentences":["In the ever-changing world of technology, continuous authentication and comprehensive access management are essential during user interactions with a device.","Split Learning (SL) and Federated Learning (FL) have recently emerged as promising technologies for training a decentralized Machine Learning (ML) model.","With the increasing use of smartphones and Internet of Things (IoT) devices, these distributed technologies enable users with limited resources to complete neural network model training with server assistance and collaboratively combine knowledge between different nodes.","In this study, we propose combining these technologies to address the continuous authentication challenge while protecting user privacy and limiting device resource usage.","However, the model's training is slowed due to SL sequential training and resource differences between IoT devices with different specifications.","Therefore, we use a cluster-based approach to group devices with similar capabilities to mitigate the impact of slow devices while filtering out the devices incapable of training the model.","In addition, we address the efficiency and robustness of training ML models by using SL and FL techniques to train the clients simultaneously while analyzing the overhead burden of the process.","Following clustering, we select the best set of clients to participate in training through a Genetic Algorithm (GA) optimized on a carefully designed list of objectives.","The performance of our proposed framework is compared to baseline methods, and the advantages are demonstrated using a real-life UMDAA-02-FD face detection dataset.","The results show that CRSFL, our proposed approach, maintains high accuracy and reduces the overhead burden in continuous authentication scenarios while preserving user privacy."],"url":"http://arxiv.org/abs/2405.07174v1","category":"cs.CV"}
{"created":"2024-05-12 04:43:46","title":"Dual role of longitudinal optical phonons for generation of coherent oscillations in gallium arsenide under optical pumping","abstract":"We present a novel and simple picture of the generation dynamics of coherent longitudinal optical (LO) phonons and LO-phonon-plasmon-coupled (LOPC) modes by the ultrafast infrared pump-pulses in gallium arsenide (GaAs) employing the low-temperature approximation. LO phonons exhibit a pronounced coupling with plasmons formed by the optically excited electrons in the excited states of GaAs. This coupling results in the coherent oscillation of the LOPC modes in the excited states. The pump pulse also induces stimulated Raman scattering, which generates the coherent LO-phonon oscillation in the ground state. This picture is incorporated into a simplified model, and the time evolution of the density operator is calculated using the Lindblad-type quantum master equation. The theoretical results explain well the reported experimental results on the coherent oscillation of LO phonons and LOPC modes observed through transient reflection measurements. Above all, our model provides a natural reason for the simultaneous manifestation of the LO phonons and the LOPC modes.","sentences":["We present a novel and simple picture of the generation dynamics of coherent longitudinal optical (LO) phonons and LO-phonon-plasmon-coupled (LOPC) modes by the ultrafast infrared pump-pulses in gallium arsenide (GaAs) employing the low-temperature approximation.","LO phonons exhibit a pronounced coupling with plasmons formed by the optically excited electrons in the excited states of GaAs.","This coupling results in the coherent oscillation of the LOPC modes in the excited states.","The pump pulse also induces stimulated Raman scattering, which generates the coherent LO-phonon oscillation in the ground state.","This picture is incorporated into a simplified model, and the time evolution of the density operator is calculated using the Lindblad-type quantum master equation.","The theoretical results explain well the reported experimental results on the coherent oscillation of LO phonons and LOPC modes observed through transient reflection measurements.","Above all, our model provides a natural reason for the simultaneous manifestation of the LO phonons and the LOPC modes."],"url":"http://arxiv.org/abs/2405.07159v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-12 04:15:52","title":"Searching for $|V_{cd}|$ through the exclusive decay $D_s^+ \\to K^0e^+\u03bd_e$ within QCD Sum Rules","abstract":"In this paper, we carry out an investigation into the semileptonic decays $D_s^+ \\to K^0\\ell^+\\nu_\\ell$ with $\\ell=(e,\\mu)$ by employing the QCD light-cone sum rules approach. The vector transition form factor (TFF) $f_+^{D_s^+ K^0}(q^2)$ for $D_s^+\\to K^0$ decay is calculated while considering its next-to-leading order contribution. Subsequently, we briefly introduce the twist-2, 3 kaon distribution amplitudes, which are calculated by using QCD sum rules within the framework of the background field theory. At the large recoil point, the TFF has $f_+^{D_s^+ K^0}(0)=0.692_{-0.026}^{+0.027}$. Then, we extrapolate $f_+^{D_s^+ K^0}(q^2)$ to the whole physical $q^2$-region via the simplified $z(q^2,t)$-series expansion, and the behavior of TFF $f_+^{D_s^+ K^0}(q^2)$ is exhibited in the numerical results part, including the theoretical and experimental predictions for comparison. In addition, we compute the differential branching fraction $\\mathcal{B}(D_s^+ \\to K^0\\ell^+\\nu_\\ell)$ with the electron and muon channels, which are expected to be $\\mathcal{B}(D_s^+ \\to K^0e^+\\nu_e)=3.379_{-0.275}^{+0.301}\\times 10^{-3}$ and $\\mathcal{B}(D_s^+ \\to K^0\\mu^+\\nu_\\mu)=3.351_{-0.273}^{+0.299}\\times 10^{-3}$ as well as contained other results for comparison. Our results show good agreement with the BESIII measurements and theoretical predictions. Furthermore, we present our prediction with respect to the CKM matrix element $|V_{cd}|$ by using the $\\mathcal{B}(D_s^+ \\to K^0e^+\\nu_e)$ result from BESIII Collaboration, yielding $|V_{cd}|=0.221_{-0.010}^{+0.008}$. Finally, we provide the ratio between $D_s^+ \\to K^0e^+\\nu_e$ and $D_s^+ \\to \\eta e^+\\nu_e$ channels, i.e. $\\mathcal{R}_{K^0/\\eta}^e=0.144_{-0.020}^{+0.028}$.","sentences":["In this paper, we carry out an investigation into the semileptonic decays $D_s^+ \\to K^0\\ell^+\\nu_\\ell$ with $\\ell=(e,\\mu)$ by employing the QCD light-cone sum rules approach.","The vector transition form factor (TFF) $f_+^{D_s^+ K^0}(q^2)$ for $D_s^+\\to K^0$ decay is calculated while considering its next-to-leading order contribution.","Subsequently, we briefly introduce the twist-2, 3 kaon distribution amplitudes, which are calculated by using QCD sum rules within the framework of the background field theory.","At the large recoil point, the TFF has $f_+^{D_s^+ K^0}(0)=0.692_{-0.026}^{+0.027}$. Then, we extrapolate $f_+^{D_s^+ K^0}(q^2)$ to the whole physical $q^2$-region via the simplified $z(q^2,t)$-series expansion, and the behavior of TFF $f_+^{D_s^+ K^0}(q^2)$ is exhibited in the numerical results part, including the theoretical and experimental predictions for comparison.","In addition, we compute the differential branching fraction $\\mathcal{B}(D_s^+ \\to K^0\\ell^+\\nu_\\ell)$ with the electron and muon channels, which are expected to be $\\mathcal{B}(D_s^+ \\to K^0e^+\\nu_e)=3.379_{-0.275}^{+0.301}\\times 10^{-3}$ and $\\mathcal{B}(D_s^+ \\to K^0\\mu^+\\nu_\\mu)=3.351_{-0.273}^{+0.299}\\times 10^{-3}$ as well as contained other results for comparison.","Our results show good agreement with the BESIII measurements and theoretical predictions.","Furthermore, we present our prediction with respect to the CKM matrix element $|V_{cd}|$ by using the $\\mathcal{B}(D_s^+ \\to K^0e^+\\nu_e)$ result from BESIII Collaboration, yielding $|V_{cd}|=0.221_{-0.010}^{+0.008}$. Finally, we provide the ratio between $D_s^+ \\to K^0e^+\\nu_e$ and $D_s^+ \\to \\eta e^+\\nu_e$ channels, i.e. $\\mathcal{R}_{K^0/\\eta}^e=0.144_{-0.020}^{+0.028}$."],"url":"http://arxiv.org/abs/2405.07154v1","category":"hep-ph"}
{"created":"2024-05-12 03:36:47","title":"Asymptotic profiles for Choquard equations with general critical nonlinearities","abstract":"In this paper, we study asymptotic behavior of positive ground state solutions for the nonlinear Choquard equation: \\begin{equation}\\label{0.1} -\\Delta u+\\varepsilon u=\\big(I_{\\alpha}\\ast F(u)\\big)F'(u),\\quad u\\in H^1(\\mathbb R^N), \\end{equation} where $F(u)=|u|^{\\frac{N+\\alpha}{N-2}}+G(u)$, $N\\geq3$ is an integer, $I_{\\alpha}$ is the Riesz potential of order $\\alpha\\in(0,N)$, and $\\varepsilon>0$ is a parameter. Under some mild subcritical growth assumptions on $G(u)$, we show that as $\\varepsilon \\to \\infty$, the ground state solutions of \\eqref{0.1}, after a suitable rescaling, converge to a particular solution of the critical Choquard equation $-\\Delta u=\\frac{N+\\alpha}{N-2}(I_{\\alpha}*|u|^{\\frac{N+\\alpha}{N-2}})|u|^{\\frac{N+\\alpha}{N-2}-2}u$. We establish a novel sharp asymptotic characterisation of such a rescaling, which depends in a non-trivial way on the asymptotic behavior of $G(u)$ at infinity and the space dimension $N=3$, $N=4$ or $N\\geq5$.","sentences":["In this paper, we study asymptotic behavior of positive ground state solutions for the nonlinear Choquard equation: \\begin{equation}\\label{0.1} -\\Delta u+\\varepsilon u=\\big(I_{\\alpha}\\ast F(u)\\big)F'(u),\\quad u\\in H^1(\\mathbb R^N), \\end{equation} where $F(u)=|u|^{\\frac{N+\\alpha}{N-2}}+G(u)$, $N\\geq3$ is an integer, $I_{\\alpha}$ is the Riesz potential of order $\\alpha\\in(0,N)$, and $\\varepsilon>0$ is a parameter.","Under some mild subcritical growth assumptions on $G(u)$, we show that as $\\varepsilon \\to \\infty$, the ground state solutions of \\eqref{0.1}, after a suitable rescaling, converge to a particular solution of the critical Choquard equation $-\\Delta u=\\frac{N+\\alpha}{N-2}(I_{\\alpha}*|u|^{\\frac{N+\\alpha}{N-2}})|u|^{\\frac{N+\\alpha}{N-2}-2}u$.","We establish a novel sharp asymptotic characterisation of such a rescaling, which depends in a non-trivial way on the asymptotic behavior of $G(u)$ at infinity and the space dimension $N=3$, $N=4$ or $N\\geq5$."],"url":"http://arxiv.org/abs/2405.07149v1","category":"math.AP"}
{"created":"2024-05-12 02:29:03","title":"Reduced Krylov Basis Methods for Parametric Partial Differential Equations","abstract":"This work is on a user-friendly reduced basis method for solving a family of parametric PDEs by preconditioned Krylov subspace methods including the conjugate gradient method, generalized minimum residual method, and bi-conjugate gradient method. The proposed methods use a preconditioned Krylov subspace method for a high-fidelity discretization of one parameter instance to generate orthogonal basis vectors of the reduced basis subspace. Then large-scale discrete parameter-dependent problems are approximately solved in the low-dimensional Krylov subspace. As shown in the theory and experiments, only a small number of Krylov subspace iterations are needed to simultaneously generate approximate solutions of a family of high-fidelity and large-scale systems in the reduced basis subspace. This reduces the computational cost dramatically because (1) to construct the reduced basis vectors, we only solve one large-scale problem in the high-fidelity level; and (2) the family of large-scale problems restricted to the reduced basis subspace have much smaller sizes.","sentences":["This work is on a user-friendly reduced basis method for solving a family of parametric PDEs by preconditioned Krylov subspace methods including the conjugate gradient method, generalized minimum residual method, and bi-conjugate gradient method.","The proposed methods use a preconditioned Krylov subspace method for a high-fidelity discretization of one parameter instance to generate orthogonal basis vectors of the reduced basis subspace.","Then large-scale discrete parameter-dependent problems are approximately solved in the low-dimensional Krylov subspace.","As shown in the theory and experiments, only a small number of Krylov subspace iterations are needed to simultaneously generate approximate solutions of a family of high-fidelity and large-scale systems in the reduced basis subspace.","This reduces the computational cost dramatically because (1) to construct the reduced basis vectors, we only solve one large-scale problem in the high-fidelity level; and (2) the family of large-scale problems restricted to the reduced basis subspace have much smaller sizes."],"url":"http://arxiv.org/abs/2405.07139v1","category":"math.NA"}
{"created":"2024-05-12 01:32:32","title":"On uniqueness of KP soliton structures","abstract":"We consider the Kadomtsev-Petviashvili II (KP) model placed in $\\mathbb R_t \\times \\mathbb R_{x,y}^2$, in the case of smooth data that are not necessarily in a Sobolev space. In this paper, the subclass of smooth solutions we study is of ``soliton type'', characterized by a phase $\\Theta=\\Theta(t,x,y)$ and a unidimensional profile $F$. In particular, every classical KP soliton and multi-soliton falls into this category with suitable $\\Theta$ and $F$. We establish concrete characterizations of KP solitons by means of a natural set of nonlinear differential equations and inclusions of functionals of Wronskian, Airy and Heat types, among others. These functional equations only depend on the new variables $\\Theta$ and $F$. A distinct characteristic of this set of functionals is its special and rigid structure tailored to the considered soliton. By analyzing $\\Theta$ and $F$, we establish the uniqueness of line-solitons, multi-solitons, and other degenerate solutions among a large class of KP solutions. Our results are also valid for other 2D dispersive models such as the quadratic and cubic Zakharov-Kuznetsov equations.","sentences":["We consider the Kadomtsev-Petviashvili II (KP) model placed in $\\mathbb R_t \\times \\mathbb R_{x,y}^2$, in the case of smooth data that are not necessarily in a Sobolev space.","In this paper, the subclass of smooth solutions we study is of ``soliton type'', characterized by a phase $\\Theta=\\Theta(t,x,y)$ and a unidimensional profile $F$. In particular, every classical KP soliton and multi-soliton falls into this category with suitable $\\Theta$ and $F$. We establish concrete characterizations of KP solitons by means of a natural set of nonlinear differential equations and inclusions of functionals of Wronskian, Airy and Heat types, among others.","These functional equations only depend on the new variables $\\Theta$ and $F$. A distinct characteristic of this set of functionals is its special and rigid structure tailored to the considered soliton.","By analyzing $\\Theta$ and $F$, we establish the uniqueness of line-solitons, multi-solitons, and other degenerate solutions among a large class of KP solutions.","Our results are also valid for other 2D dispersive models such as the quadratic and cubic Zakharov-Kuznetsov equations."],"url":"http://arxiv.org/abs/2405.07125v1","category":"math.AP"}
{"created":"2024-05-12 01:22:05","title":"Vertex Shader Domain Warping with Automatic Differentiation","abstract":"Domain warping is a technique commonly used in creative coding to distort graphics and add visual interest to a work. The approach has the potential to be used in 3D art as mesh vertices can be efficiently warped using a vertex shader in a WebGL pipeline. However, 3D models packaged for the web typically come with baked-in normal vectors, and these need to be updated when vertex positions change for lighting calculations to work. This is typically done via finite differences, which requires parameter tuning to achieve optimal visual fidelity. We present a method for 3D domain warping that works with automatic differentiation, allowing exact normals to be used without any tuning while still benefiting from hardware acceleration.","sentences":["Domain warping is a technique commonly used in creative coding to distort graphics and add visual interest to a work.","The approach has the potential to be used in 3D art as mesh vertices can be efficiently warped using a vertex shader in a WebGL pipeline.","However, 3D models packaged for the web typically come with baked-in normal vectors, and these need to be updated when vertex positions change for lighting calculations to work.","This is typically done via finite differences, which requires parameter tuning to achieve optimal visual fidelity.","We present a method for 3D domain warping that works with automatic differentiation, allowing exact normals to be used without any tuning while still benefiting from hardware acceleration."],"url":"http://arxiv.org/abs/2405.07124v1","category":"cs.GR"}
{"created":"2024-05-12 00:57:37","title":"Quasiparticle and Excitonic Structures of Few-layer and Bulk GaSe: Interlayer Coupling, Self-energy, and Electron-hole Interaction","abstract":"Metal monochalcogenide GaSe is a classic layered semiconductor that has received increasing research interest due to its highly tunable electronic and optical properties for ultrathin electronics applications. Despite intense research efforts, a systematic understanding of the layer-dependent electronic and optical properties of GaSe remains to be established, and there appear significant discrepancies between different experiments. We have performed GW plus Bethe-Salpeter equation (BSE) calculations for few-layer and bulk GaSe, aiming at understanding the effects of interlayer coupling and dielectric screening on excited state properties of GaSe, and how the electronic and optical properties evolve from strongly two-dimensional (2D) like to intermediate thick layers, and to three-dimensional (3D) bulk character. Using a new definition of the exciton binding energy, we are able to calculate the binding energies of all excitonic states. Our results reveal an interesting correlation between the binding energy of an exciton and the spread of its wave function in the real and momentum spaces. We find that the existence of (nearly) parallel valence and conduction bands facilitates the formation of excitonic states that spread out in the momentum space. Thus, these excitons tend to be more localized in real space and have large exciton binding energies. The interlayer coupling substantially suppresses the Mexican-hat-like dispersion of the top valence band seen in monolayer system, explaining the greatly enhanced photoluminescence (PL) as layer thickness increases. Our results also help resolve apparent discrepancies between different experiments. After including the quasiparticle and excitonic effects as well the optical activities of excitons, our results compare well with available experimental results.","sentences":["Metal monochalcogenide GaSe is a classic layered semiconductor that has received increasing research interest due to its highly tunable electronic and optical properties for ultrathin electronics applications.","Despite intense research efforts, a systematic understanding of the layer-dependent electronic and optical properties of GaSe remains to be established, and there appear significant discrepancies between different experiments.","We have performed GW plus Bethe-Salpeter equation (BSE) calculations for few-layer and bulk GaSe, aiming at understanding the effects of interlayer coupling and dielectric screening on excited state properties of GaSe, and how the electronic and optical properties evolve from strongly two-dimensional (2D) like to intermediate thick layers, and to three-dimensional (3D) bulk character.","Using a new definition of the exciton binding energy, we are able to calculate the binding energies of all excitonic states.","Our results reveal an interesting correlation between the binding energy of an exciton and the spread of its wave function in the real and momentum spaces.","We find that the existence of (nearly) parallel valence and conduction bands facilitates the formation of excitonic states that spread out in the momentum space.","Thus, these excitons tend to be more localized in real space and have large exciton binding energies.","The interlayer coupling substantially suppresses the Mexican-hat-like dispersion of the top valence band seen in monolayer system, explaining the greatly enhanced photoluminescence (PL) as layer thickness increases.","Our results also help resolve apparent discrepancies between different experiments.","After including the quasiparticle and excitonic effects as well the optical activities of excitons, our results compare well with available experimental results."],"url":"http://arxiv.org/abs/2405.07120v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 17:47:40","title":"Sensitivity Analysis for Active Sampling, with Applications to the Simulation of Analog Circuits","abstract":"We propose an active sampling flow, with the use-case of simulating the impact of combined variations on analog circuits. In such a context, given the large number of parameters, it is difficult to fit a surrogate model and to efficiently explore the space of design features.   By combining a drastic dimension reduction using sensitivity analysis and Bayesian surrogate modeling, we obtain a flexible active sampling flow. On synthetic and real datasets, this flow outperforms the usual Monte-Carlo sampling which often forms the foundation of design space exploration.","sentences":["We propose an active sampling flow, with the use-case of simulating the impact of combined variations on analog circuits.","In such a context, given the large number of parameters, it is difficult to fit a surrogate model and to efficiently explore the space of design features.   ","By combining a drastic dimension reduction using sensitivity analysis and Bayesian surrogate modeling, we obtain a flexible active sampling flow.","On synthetic and real datasets, this flow outperforms the usual Monte-Carlo sampling which often forms the foundation of design space exploration."],"url":"http://arxiv.org/abs/2405.07971v1","category":"stat.ML"}
{"created":"2024-05-13 15:57:19","title":"Boostlet.js: Image processing plugins for the web via JavaScript injection","abstract":"Can web-based image processing and visualization tools easily integrate into existing websites without significant time and effort? Our Boostlet.js library addresses this challenge by providing an open-source, JavaScript-based web framework to enable additional image processing functionalities. Boostlet examples include kernel filtering, image captioning, data visualization, segmentation, and web-optimized machine-learning models. To achieve this, Boostlet.js uses a browser bookmark to inject a user-friendly plugin selection tool called PowerBoost into any host website. Boostlet also provides on-site access to a standard API independent of any visualization framework for pixel data and scene manipulation. Web-based Boostlets provide a modular architecture and client-side processing capabilities to apply advanced image-processing techniques using consumer-level hardware. The code is open-source and available.","sentences":["Can web-based image processing and visualization tools easily integrate into existing websites without significant time and effort?","Our Boostlet.js library addresses this challenge by providing an open-source, JavaScript-based web framework to enable additional image processing functionalities.","Boostlet examples include kernel filtering, image captioning, data visualization, segmentation, and web-optimized machine-learning models.","To achieve this, Boostlet.js uses a browser bookmark to inject a user-friendly plugin selection tool called PowerBoost into any host website.","Boostlet also provides on-site access to a standard API independent of any visualization framework for pixel data and scene manipulation.","Web-based Boostlets provide a modular architecture and client-side processing capabilities to apply advanced image-processing techniques using consumer-level hardware.","The code is open-source and available."],"url":"http://arxiv.org/abs/2405.07868v1","category":"cs.CV"}
{"created":"2024-05-13 14:44:02","title":"Data Imputation by Pursuing Better Classification: A Supervised Kernel-Based Method","abstract":"Data imputation, the process of filling in missing feature elements for incomplete data sets, plays a crucial role in data-driven learning. A fundamental belief is that data imputation is helpful for learning performance, and it follows that the pursuit of better classification can guide the data imputation process. While some works consider using label information to assist in this task, their simplistic utilization of labels lacks flexibility and may rely on strict assumptions. In this paper, we propose a new framework that effectively leverages supervision information to complete missing data in a manner conducive to classification. Specifically, this framework operates in two stages. Firstly, it leverages labels to supervise the optimization of similarity relationships among data, represented by the kernel matrix, with the goal of enhancing classification accuracy. To mitigate overfitting that may occur during this process, a perturbation variable is introduced to improve the robustness of the framework. Secondly, the learned kernel matrix serves as additional supervision information to guide data imputation through regression, utilizing the block coordinate descent method. The superiority of the proposed method is evaluated on four real-world data sets by comparing it with state-of-the-art imputation methods. Remarkably, our algorithm significantly outperforms other methods when the data is missing more than 60\\% of the features","sentences":["Data imputation, the process of filling in missing feature elements for incomplete data sets, plays a crucial role in data-driven learning.","A fundamental belief is that data imputation is helpful for learning performance, and it follows that the pursuit of better classification can guide the data imputation process.","While some works consider using label information to assist in this task, their simplistic utilization of labels lacks flexibility and may rely on strict assumptions.","In this paper, we propose a new framework that effectively leverages supervision information to complete missing data in a manner conducive to classification.","Specifically, this framework operates in two stages.","Firstly, it leverages labels to supervise the optimization of similarity relationships among data, represented by the kernel matrix, with the goal of enhancing classification accuracy.","To mitigate overfitting that may occur during this process, a perturbation variable is introduced to improve the robustness of the framework.","Secondly, the learned kernel matrix serves as additional supervision information to guide data imputation through regression, utilizing the block coordinate descent method.","The superiority of the proposed method is evaluated on four real-world data sets by comparing it with state-of-the-art imputation methods.","Remarkably, our algorithm significantly outperforms other methods when the data is missing more than 60\\% of the features"],"url":"http://arxiv.org/abs/2405.07800v1","category":"cs.LG"}
{"created":"2024-05-13 14:35:30","title":"DEPTH: Discourse Education through Pre-Training Hierarchically","abstract":"Language Models (LMs) often struggle with linguistic understanding at the discourse level, even though discourse patterns such as coherence, cohesion, and narrative flow are prevalent in their pre-training data. Current methods address these challenges only after the pre-training phase, relying on expensive human annotated data to align the model. To improve the discourse capabilities of LMs already at the pre-training stage, we introduce DEPTH, an encoder-decoder model that learns to represent sentences using a discourse-oriented pre-training objective. DEPTH combines hierarchical sentence representations with two objectives: (1) Sentence Un-Shuffling, and (2) Span-Corruption. This approach trains the model to represent both sub-word-level and sentence-level dependencies over a massive amount of unstructured text. When trained either from scratch or continuing from a pre-trained T5 checkpoint, DEPTH learns semantic and discourse-level representations faster than T5, outperforming it in span-corruption loss despite the additional sentence-un-shuffling objective. Evaluations on the GLUE, DiscoEval, and NI benchmarks demonstrate DEPTH's ability to quickly learn diverse downstream tasks, which require syntactic, semantic, and discourse capabilities. Overall, our approach extends the discourse capabilities of T5, while minimally impacting other natural language understanding (NLU) capabilities in the resulting LM.","sentences":["Language Models (LMs) often struggle with linguistic understanding at the discourse level, even though discourse patterns such as coherence, cohesion, and narrative flow are prevalent in their pre-training data.","Current methods address these challenges only after the pre-training phase, relying on expensive human annotated data to align the model.","To improve the discourse capabilities of LMs already at the pre-training stage, we introduce DEPTH, an encoder-decoder model that learns to represent sentences using a discourse-oriented pre-training objective.","DEPTH combines hierarchical sentence representations with two objectives: (1) Sentence Un-Shuffling, and (2) Span-Corruption.","This approach trains the model to represent both sub-word-level and sentence-level dependencies over a massive amount of unstructured text.","When trained either from scratch or continuing from a pre-trained T5 checkpoint, DEPTH learns semantic and discourse-level representations faster than T5, outperforming it in span-corruption loss despite the additional sentence-un-shuffling objective.","Evaluations on the GLUE, DiscoEval, and NI benchmarks demonstrate DEPTH's ability to quickly learn diverse downstream tasks, which require syntactic, semantic, and discourse capabilities.","Overall, our approach extends the discourse capabilities of T5, while minimally impacting other natural language understanding (NLU) capabilities in the resulting LM."],"url":"http://arxiv.org/abs/2405.07788v1","category":"cs.CL"}
{"created":"2024-05-13 11:40:59","title":"Signal Enhancement in Distributed Acoustic Sensing Data Using a Guided Unsupervised Deep Learning Network","abstract":"Distributed Acoustic Sensing (DAS) is a promising technology introducing a new paradigm in the acquisition of high-resolution seismic data. However, DAS data often show weak signals compared to the background noise, especially in tough installation environments. In this study, we propose a new approach to denoise DAS data that leverages an unsupervised deep learning (DL) model, eliminating the need for labeled training data. The DL model aims to reconstruct the DAS signal while simultaneously attenuating DAS noise. The input DAS data undergo band-pass filtering to eliminate high-frequency content. Subsequently, a continuous wavelet transform (CWT) is performed, and the finest scale is used to guide the DL model in reconstructing the DAS signal. First, we extract 2D patches from both the band-pass filtered data and the CWT scale of the data. Then, these patches are converted using an unrolling mechanism into 1D vectors to form the input of the DL model. The architecture of the proposed DL network is composed of several fully-connected layers. A self-attention layer is further included in each layer to extract the spatial relation between the band-pass filtered data and the CWT scale. Through an iterative process, the DL model tunes its parameters to suppress DAS noise, with the band-pass filtered data serving as the target for the network. We employ the log cosh as a loss function for the DL model, enhancing its robustness against erratic noise. The denoising performance of the proposed framework is validated using field examples from the San Andreas Fault Observatory at Depth (SAFOD) and Frontier Observatory for Research in Geothermal Energy (FORGE) datasets, where the data are recorded by a fiber-optic cable. Comparative analyses against three benchmark methods reveal the robust denoising performance of the proposed framework.","sentences":["Distributed Acoustic Sensing (DAS) is a promising technology introducing a new paradigm in the acquisition of high-resolution seismic data.","However, DAS data often show weak signals compared to the background noise, especially in tough installation environments.","In this study, we propose a new approach to denoise DAS data that leverages an unsupervised deep learning (DL) model, eliminating the need for labeled training data.","The DL model aims to reconstruct the DAS signal while simultaneously attenuating DAS noise.","The input DAS data undergo band-pass filtering to eliminate high-frequency content.","Subsequently, a continuous wavelet transform (CWT) is performed, and the finest scale is used to guide the DL model in reconstructing the DAS signal.","First, we extract 2D patches from both the band-pass filtered data and the CWT scale of the data.","Then, these patches are converted using an unrolling mechanism into 1D vectors to form the input of the DL model.","The architecture of the proposed DL network is composed of several fully-connected layers.","A self-attention layer is further included in each layer to extract the spatial relation between the band-pass filtered data and the CWT scale.","Through an iterative process, the DL model tunes its parameters to suppress DAS noise, with the band-pass filtered data serving as the target for the network.","We employ the log cosh as a loss function for the DL model, enhancing its robustness against erratic noise.","The denoising performance of the proposed framework is validated using field examples from the San Andreas Fault Observatory at Depth (SAFOD) and Frontier Observatory for Research in Geothermal Energy (FORGE) datasets, where the data are recorded by a fiber-optic cable.","Comparative analyses against three benchmark methods reveal the robust denoising performance of the proposed framework."],"url":"http://arxiv.org/abs/2405.07660v1","category":"physics.geo-ph"}
{"created":"2024-05-13 11:13:49","title":"Efficient Matrix Factorization Via Householder Reflections","abstract":"Motivated by orthogonal dictionary learning problems, we propose a novel method for matrix factorization, where the data matrix $\\mathbf{Y}$ is a product of a Householder matrix $\\mathbf{H}$ and a binary matrix $\\mathbf{X}$. First, we show that the exact recovery of the factors $\\mathbf{H}$ and $\\mathbf{X}$ from $\\mathbf{Y}$ is guaranteed with $\\Omega(1)$ columns in $\\mathbf{Y}$ . Next, we show approximate recovery (in the $l\\infty$ sense) can be done in polynomial time($O(np)$) with $\\Omega(\\log n)$ columns in $\\mathbf{Y}$ . We hope the techniques in this work help in developing alternate algorithms for orthogonal dictionary learning.","sentences":["Motivated by orthogonal dictionary learning problems, we propose a novel method for matrix factorization, where the data matrix $\\mathbf{Y}$ is a product of a Householder matrix $\\mathbf{H}$ and a binary matrix $\\mathbf{X}$. First, we show that the exact recovery of the factors $\\mathbf{H}$ and $\\mathbf{X}$ from $\\mathbf{Y}$ is guaranteed with $\\Omega(1)$ columns in $\\mathbf{Y}$ .","Next, we show approximate recovery (in the $l\\infty$ sense) can be done in polynomial time($O(np)$) with $\\Omega(\\log n)$ columns in $\\mathbf{Y}$ .","We hope the techniques in this work help in developing alternate algorithms for orthogonal dictionary learning."],"url":"http://arxiv.org/abs/2405.07649v1","category":"eess.SP"}
{"created":"2024-05-13 10:51:01","title":"Near-Optimal Regret in Linear MDPs with Aggregate Bandit Feedback","abstract":"In many real-world applications, it is hard to provide a reward signal in each step of a Reinforcement Learning (RL) process and more natural to give feedback when an episode ends. To this end, we study the recently proposed model of RL with Aggregate Bandit Feedback (RL-ABF), where the agent only observes the sum of rewards at the end of an episode instead of each reward individually. Prior work studied RL-ABF only in tabular settings, where the number of states is assumed to be small. In this paper, we extend ABF to linear function approximation and develop two efficient algorithms with near-optimal regret guarantees: a value-based optimistic algorithm built on a new randomization technique with a Q-functions ensemble, and a policy optimization algorithm that uses a novel hedging scheme over the ensemble.","sentences":["In many real-world applications, it is hard to provide a reward signal in each step of a Reinforcement Learning (RL) process and more natural to give feedback when an episode ends.","To this end, we study the recently proposed model of RL with Aggregate Bandit Feedback (RL-ABF), where the agent only observes the sum of rewards at the end of an episode instead of each reward individually.","Prior work studied RL-ABF only in tabular settings, where the number of states is assumed to be small.","In this paper, we extend ABF to linear function approximation and develop two efficient algorithms with near-optimal regret guarantees: a value-based optimistic algorithm built on a new randomization technique with a Q-functions ensemble, and a policy optimization algorithm that uses a novel hedging scheme over the ensemble."],"url":"http://arxiv.org/abs/2405.07637v1","category":"cs.LG"}
{"created":"2024-05-13 10:08:20","title":"Improving classifier-based effort-aware software defect prediction by reducing ranking errors","abstract":"Context: Software defect prediction utilizes historical data to direct software quality assurance resources to potentially problematic components. Effort-aware (EA) defect prediction prioritizes more bug-like components by taking cost-effectiveness into account. In other words, it is a ranking problem, however, existing ranking strategies based on classification, give limited consideration to ranking errors. Objective: Improve the performance of classifier-based EA ranking methods by focusing on ranking errors. Method: We propose a ranking score calculation strategy called EA-Z which sets a lower bound to avoid near-zero ranking errors. We investigate four primary EA ranking strategies with 16 classification learners, and conduct the experiments for EA-Z and the other four existing strategies. Results: Experimental results from 72 data sets show EA-Z is the best ranking score calculation strategy in terms of Recall@20% and Popt when considering all 16 learners. For particular learners, imbalanced ensemble learner UBag-svm and UBst-rf achieve top performance with EA-Z. Conclusion: Our study indicates the effectiveness of reducing ranking errors for classifier-based effort-aware defect prediction. We recommend using EA-Z with imbalanced ensemble learning.","sentences":["Context: Software defect prediction utilizes historical data to direct software quality assurance resources to potentially problematic components.","Effort-aware (EA) defect prediction prioritizes more bug-like components by taking cost-effectiveness into account.","In other words, it is a ranking problem, however, existing ranking strategies based on classification, give limited consideration to ranking errors.","Objective: Improve the performance of classifier-based EA ranking methods by focusing on ranking errors.","Method: We propose a ranking score calculation strategy called EA-Z which sets a lower bound to avoid near-zero ranking errors.","We investigate four primary EA ranking strategies with 16 classification learners, and conduct the experiments for EA-Z and the other four existing strategies.","Results: Experimental results from 72 data sets show EA-Z is the best ranking score calculation strategy in terms of Recall@20% and Popt when considering all 16 learners.","For particular learners, imbalanced ensemble learner UBag-svm and UBst-rf achieve top performance with EA-Z. Conclusion: Our study indicates the effectiveness of reducing ranking errors for classifier-based effort-aware defect prediction.","We recommend using EA-Z with imbalanced ensemble learning."],"url":"http://arxiv.org/abs/2405.07604v1","category":"cs.SE"}
{"created":"2024-05-13 08:32:22","title":"Distributed High-Dimensional Quantile Regression: Estimation Efficiency and Support Recovery","abstract":"In this paper, we focus on distributed estimation and support recovery for high-dimensional linear quantile regression. Quantile regression is a popular alternative tool to the least squares regression for robustness against outliers and data heterogeneity. However, the non-smoothness of the check loss function poses big challenges to both computation and theory in the distributed setting. To tackle these problems, we transform the original quantile regression into the least-squares optimization. By applying a double-smoothing approach, we extend a previous Newton-type distributed approach without the restrictive independent assumption between the error term and covariates. An efficient algorithm is developed, which enjoys high computation and communication efficiency. Theoretically, the proposed distributed estimator achieves a near-oracle convergence rate and high support recovery accuracy after a constant number of iterations. Extensive experiments on synthetic examples and a real data application further demonstrate the effectiveness of the proposed method.","sentences":["In this paper, we focus on distributed estimation and support recovery for high-dimensional linear quantile regression.","Quantile regression is a popular alternative tool to the least squares regression for robustness against outliers and data heterogeneity.","However, the non-smoothness of the check loss function poses big challenges to both computation and theory in the distributed setting.","To tackle these problems, we transform the original quantile regression into the least-squares optimization.","By applying a double-smoothing approach, we extend a previous Newton-type distributed approach without the restrictive independent assumption between the error term and covariates.","An efficient algorithm is developed, which enjoys high computation and communication efficiency.","Theoretically, the proposed distributed estimator achieves a near-oracle convergence rate and high support recovery accuracy after a constant number of iterations.","Extensive experiments on synthetic examples and a real data application further demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2405.07552v1","category":"stat.ML"}
{"created":"2024-05-13 08:04:56","title":"BharatBench: Dataset for data-driven weather forecasting over India","abstract":"Advanced weather and climate models use numerical techniques on grided meshes to simulate atmospheric and ocean dynamics, which are computationally expensive. Data-driven approaches are gaining popularity in weather and climate modeling, with a broad scope of applications. Although Machine Learning (ML) has been employed in this domain, significant progress has occurred in the past decade, leading to ML applications that are now competitive with traditional numerical methods. This study presents a user-friendly dataset for data-driven medium-range weather forecasting focused on India. The dataset is derived from IMDAA reanalysis datasets and optimized for ML applications. The study provides clear evaluation metrics and a few baseline scores from simple linear regression techniques and deep learning models. The dataset can be found at https://www.kaggle.com/datasets/maslab/bharatbench, while the codes are available at https://github.com/MASLABnitrkl/BharatBench. We hope this dataset will boost data-driven weather forecasting over India. We also address limitations in the current evaluation process and future challenges in data-driven weather forecasting.","sentences":["Advanced weather and climate models use numerical techniques on grided meshes to simulate atmospheric and ocean dynamics, which are computationally expensive.","Data-driven approaches are gaining popularity in weather and climate modeling, with a broad scope of applications.","Although Machine Learning (ML) has been employed in this domain, significant progress has occurred in the past decade, leading to ML applications that are now competitive with traditional numerical methods.","This study presents a user-friendly dataset for data-driven medium-range weather forecasting focused on India.","The dataset is derived from IMDAA reanalysis datasets and optimized for ML applications.","The study provides clear evaluation metrics and a few baseline scores from simple linear regression techniques and deep learning models.","The dataset can be found at https://www.kaggle.com/datasets/maslab/bharatbench, while the codes are available at https://github.com/MASLABnitrkl/BharatBench.","We hope this dataset will boost data-driven weather forecasting over India.","We also address limitations in the current evaluation process and future challenges in data-driven weather forecasting."],"url":"http://arxiv.org/abs/2405.07534v1","category":"physics.ao-ph"}
{"created":"2024-05-13 07:10:53","title":"PeRFlow: Piecewise Rectified Flow as Universal Plug-and-Play Accelerator","abstract":"We present Piecewise Rectified Flow (PeRFlow), a flow-based method for accelerating diffusion models. PeRFlow divides the sampling process of generative flows into several time windows and straightens the trajectories in each interval via the reflow operation, thereby approaching piecewise linear flows. PeRFlow achieves superior performance in a few-step generation. Moreover, through dedicated parameterizations, the obtained PeRFlow models show advantageous transfer ability, serving as universal plug-and-play accelerators that are compatible with various workflows based on the pre-trained diffusion models. The implementations of training and inference are fully open-sourced. https://github.com/magic-research/piecewise-rectified-flow","sentences":["We present Piecewise Rectified Flow (PeRFlow), a flow-based method for accelerating diffusion models.","PeRFlow divides the sampling process of generative flows into several time windows and straightens the trajectories in each interval via the reflow operation, thereby approaching piecewise linear flows.","PeRFlow achieves superior performance in a few-step generation.","Moreover, through dedicated parameterizations, the obtained PeRFlow models show advantageous transfer ability, serving as universal plug-and-play accelerators that are compatible with various workflows based on the pre-trained diffusion models.","The implementations of training and inference are fully open-sourced.","https://github.com/magic-research/piecewise-rectified-flow"],"url":"http://arxiv.org/abs/2405.07510v1","category":"cs.LG"}
{"created":"2024-05-13 06:33:06","title":"Towards Subgraph Isomorphism Counting with Graph Kernels","abstract":"Subgraph isomorphism counting is known as #P-complete and requires exponential time to find the accurate solution. Utilizing representation learning has been shown as a promising direction to represent substructures and approximate the solution. Graph kernels that implicitly capture the correlations among substructures in diverse graphs have exhibited great discriminative power in graph classification, so we pioneeringly investigate their potential in counting subgraph isomorphisms and further explore the augmentation of kernel capability through various variants, including polynomial and Gaussian kernels. Through comprehensive analysis, we enhance the graph kernels by incorporating neighborhood information. Finally, we present the results of extensive experiments to demonstrate the effectiveness of the enhanced graph kernels and discuss promising directions for future research.","sentences":["Subgraph isomorphism counting is known as #P-complete and requires exponential time to find the accurate solution.","Utilizing representation learning has been shown as a promising direction to represent substructures and approximate the solution.","Graph kernels that implicitly capture the correlations among substructures in diverse graphs have exhibited great discriminative power in graph classification, so we pioneeringly investigate their potential in counting subgraph isomorphisms and further explore the augmentation of kernel capability through various variants, including polynomial and Gaussian kernels.","Through comprehensive analysis, we enhance the graph kernels by incorporating neighborhood information.","Finally, we present the results of extensive experiments to demonstrate the effectiveness of the enhanced graph kernels and discuss promising directions for future research."],"url":"http://arxiv.org/abs/2405.07497v1","category":"cs.LG"}
{"created":"2024-05-13 06:08:09","title":"Sparse Domain Transfer via Elastic Net Regularization","abstract":"Transportation of samples across different domains is a central task in several machine learning problems. A sensible requirement for domain transfer tasks in computer vision and language domains is the sparsity of the transportation map, i.e., the transfer algorithm aims to modify the least number of input features while transporting samples across the source and target domains. In this work, we propose Elastic Net Optimal Transport (ENOT) to address the sparse distribution transfer problem. The ENOT framework utilizes the $L_1$-norm and $L_2$-norm regularization mechanisms to find a sparse and stable transportation map between the source and target domains. To compute the ENOT transport map, we consider the dual formulation of the ENOT optimization task and prove that the sparsified gradient of the optimal potential function in the ENOT's dual representation provides the ENOT transport map. Furthermore, we demonstrate the application of the ENOT framework to perform feature selection for sparse domain transfer. We present the numerical results of applying ENOT to several domain transfer problems for synthetic Gaussian mixtures and real image and text data. Our empirical results indicate the success of the ENOT framework in identifying a sparse domain transport map.","sentences":["Transportation of samples across different domains is a central task in several machine learning problems.","A sensible requirement for domain transfer tasks in computer vision and language domains is the sparsity of the transportation map, i.e., the transfer algorithm aims to modify the least number of input features while transporting samples across the source and target domains.","In this work, we propose Elastic Net Optimal Transport (ENOT) to address the sparse distribution transfer problem.","The ENOT framework utilizes the $L_1$-norm and $L_2$-norm regularization mechanisms to find a sparse and stable transportation map between the source and target domains.","To compute the ENOT transport map, we consider the dual formulation of the ENOT optimization task and prove that the sparsified gradient of the optimal potential function in the ENOT's dual representation provides the ENOT transport map.","Furthermore, we demonstrate the application of the ENOT framework to perform feature selection for sparse domain transfer.","We present the numerical results of applying ENOT to several domain transfer problems for synthetic Gaussian mixtures and real image and text data.","Our empirical results indicate the success of the ENOT framework in identifying a sparse domain transport map."],"url":"http://arxiv.org/abs/2405.07489v1","category":"cs.LG"}
{"created":"2024-05-13 05:48:37","title":"Marginal Fairness Sliced Wasserstein Barycenter","abstract":"The sliced Wasserstein barycenter (SWB) is a widely acknowledged method for efficiently generalizing the averaging operation within probability measure spaces. However, achieving marginal fairness SWB, ensuring approximately equal distances from the barycenter to marginals, remains unexplored. The uniform weighted SWB is not necessarily the optimal choice to obtain the desired marginal fairness barycenter due to the heterogeneous structure of marginals and the non-optimality of the optimization. As the first attempt to tackle the problem, we define the marginal fairness sliced Wasserstein barycenter (MFSWB) as a constrained SWB problem. Due to the computational disadvantages of the formal definition, we propose two hyperparameter-free and computationally tractable surrogate MFSWB problems that implicitly minimize the distances to marginals and encourage marginal fairness at the same time. To further improve the efficiency, we perform slicing distribution selection and obtain the third surrogate definition by introducing a new slicing distribution that focuses more on marginally unfair projecting directions. We discuss the relationship of the three proposed problems and their relationship to sliced multi-marginal Wasserstein distance. Finally, we conduct experiments on finding 3D point-clouds averaging, color harmonization, and training of sliced Wasserstein autoencoder with class-fairness representation to show the favorable performance of the proposed surrogate MFSWB problems.","sentences":["The sliced Wasserstein barycenter (SWB) is a widely acknowledged method for efficiently generalizing the averaging operation within probability measure spaces.","However, achieving marginal fairness SWB, ensuring approximately equal distances from the barycenter to marginals, remains unexplored.","The uniform weighted SWB is not necessarily the optimal choice to obtain the desired marginal fairness barycenter due to the heterogeneous structure of marginals and the non-optimality of the optimization.","As the first attempt to tackle the problem, we define the marginal fairness sliced Wasserstein barycenter (MFSWB) as a constrained SWB problem.","Due to the computational disadvantages of the formal definition, we propose two hyperparameter-free and computationally tractable surrogate MFSWB problems that implicitly minimize the distances to marginals and encourage marginal fairness at the same time.","To further improve the efficiency, we perform slicing distribution selection and obtain the third surrogate definition by introducing a new slicing distribution that focuses more on marginally unfair projecting directions.","We discuss the relationship of the three proposed problems and their relationship to sliced multi-marginal Wasserstein distance.","Finally, we conduct experiments on finding 3D point-clouds averaging, color harmonization, and training of sliced Wasserstein autoencoder with class-fairness representation to show the favorable performance of the proposed surrogate MFSWB problems."],"url":"http://arxiv.org/abs/2405.07482v1","category":"stat.ML"}
{"created":"2024-05-13 04:59:32","title":"MCS-SQL: Leveraging Multiple Prompts and Multiple-Choice Selection For Text-to-SQL Generation","abstract":"Recent advancements in large language models (LLMs) have enabled in-context learning (ICL)-based methods that significantly outperform fine-tuning approaches for text-to-SQL tasks. However, their performance is still considerably lower than that of human experts on benchmarks that include complex schemas and queries, such as BIRD. This study considers the sensitivity of LLMs to the prompts and introduces a novel approach that leverages multiple prompts to explore a broader search space for possible answers and effectively aggregate them. Specifically, we robustly refine the database schema through schema linking using multiple prompts. Thereafter, we generate various candidate SQL queries based on the refined schema and diverse prompts. Finally, the candidate queries are filtered based on their confidence scores, and the optimal query is obtained through a multiple-choice selection that is presented to the LLM. When evaluated on the BIRD and Spider benchmarks, the proposed method achieved execution accuracies of 65.5\\% and 89.6\\%, respectively, significantly outperforming previous ICL-based methods. Moreover, we established a new SOTA performance on the BIRD in terms of both the accuracy and efficiency of the generated queries.","sentences":["Recent advancements in large language models (LLMs) have enabled in-context learning (ICL)-based methods that significantly outperform fine-tuning approaches for text-to-SQL tasks.","However, their performance is still considerably lower than that of human experts on benchmarks that include complex schemas and queries, such as BIRD.","This study considers the sensitivity of LLMs to the prompts and introduces a novel approach that leverages multiple prompts to explore a broader search space for possible answers and effectively aggregate them.","Specifically, we robustly refine the database schema through schema linking using multiple prompts.","Thereafter, we generate various candidate SQL queries based on the refined schema and diverse prompts.","Finally, the candidate queries are filtered based on their confidence scores, and the optimal query is obtained through a multiple-choice selection that is presented to the LLM.","When evaluated on the BIRD and Spider benchmarks, the proposed method achieved execution accuracies of 65.5\\% and 89.6\\%, respectively, significantly outperforming previous ICL-based methods.","Moreover, we established a new SOTA performance on the BIRD in terms of both the accuracy and efficiency of the generated queries."],"url":"http://arxiv.org/abs/2405.07467v1","category":"cs.CL"}
{"created":"2024-05-13 04:15:59","title":"Examining Humanness as a Metaphor to Design Voice User Interfaces","abstract":"Voice User Interfaces (VUIs) increasingly leverage 'humanness' as a foundational design metaphor, adopting roles like 'assistants,' 'teachers,' and 'secretaries' to foster natural interactions. Yet, this approach can sometimes misalign user trust and reinforce societal stereotypes, leading to socio-technical challenges that might impede long-term engagement. This paper explores an alternative approach to navigate these challenges-incorporating non-human metaphors in VUI design. We report on a study with 240 participants examining the effects of human versus non-human metaphors on user perceptions within health and finance domains. Results indicate a preference for the human metaphor (doctor) over the non-human (health encyclopedia) in health contexts for its perceived enjoyability and likeability. In finance, however, user perceptions do not significantly differ between human (financial advisor) and non-human (calculator) metaphors. Importantly, our research reveals that the explicit awareness of a metaphor's use influences adoption intentions, with a marked preference for non-human metaphors when their metaphorical nature is not disclosed. These findings highlight context-specific conversation design strategies required in integrating non-human metaphors into VUI design, suggesting tradeoffs and design considerations that could enhance user engagement and adoption.","sentences":["Voice User Interfaces (VUIs) increasingly leverage 'humanness' as a foundational design metaphor, adopting roles like 'assistants,' 'teachers,' and 'secretaries' to foster natural interactions.","Yet, this approach can sometimes misalign user trust and reinforce societal stereotypes, leading to socio-technical challenges that might impede long-term engagement.","This paper explores an alternative approach to navigate these challenges-incorporating non-human metaphors in VUI design.","We report on a study with 240 participants examining the effects of human versus non-human metaphors on user perceptions within health and finance domains.","Results indicate a preference for the human metaphor (doctor) over the non-human (health encyclopedia) in health contexts for its perceived enjoyability and likeability.","In finance, however, user perceptions do not significantly differ between human (financial advisor) and non-human (calculator) metaphors.","Importantly, our research reveals that the explicit awareness of a metaphor's use influences adoption intentions, with a marked preference for non-human metaphors when their metaphorical nature is not disclosed.","These findings highlight context-specific conversation design strategies required in integrating non-human metaphors into VUI design, suggesting tradeoffs and design considerations that could enhance user engagement and adoption."],"url":"http://arxiv.org/abs/2405.07458v1","category":"cs.HC"}
{"created":"2024-05-13 04:12:03","title":"Boosting House Price Estimations with Multi-Head Gated Attention","abstract":"Evaluating house prices is crucial for various stakeholders, including homeowners, investors, and policymakers. However, traditional spatial interpolation methods have limitations in capturing the complex spatial relationships that affect property values. To address these challenges, we have developed a new method called Multi-Head Gated Attention for spatial interpolation. Our approach builds upon attention-based interpolation models and incorporates multiple attention heads and gating mechanisms to capture spatial dependencies and contextual information better. Importantly, our model produces embeddings that reduce the dimensionality of the data, enabling simpler models like linear regression to outperform complex ensembling models. We conducted extensive experiments to compare our model with baseline methods and the original attention-based interpolation model. The results show a significant improvement in the accuracy of house price predictions, validating the effectiveness of our approach. This research advances the field of spatial interpolation and provides a robust tool for more precise house price evaluation. Our GitHub repository.contains the data and code for all datasets, which are available for researchers and practitioners interested in replicating or building upon our work.","sentences":["Evaluating house prices is crucial for various stakeholders, including homeowners, investors, and policymakers.","However, traditional spatial interpolation methods have limitations in capturing the complex spatial relationships that affect property values.","To address these challenges, we have developed a new method called Multi-Head Gated Attention for spatial interpolation.","Our approach builds upon attention-based interpolation models and incorporates multiple attention heads and gating mechanisms to capture spatial dependencies and contextual information better.","Importantly, our model produces embeddings that reduce the dimensionality of the data, enabling simpler models like linear regression to outperform complex ensembling models.","We conducted extensive experiments to compare our model with baseline methods and the original attention-based interpolation model.","The results show a significant improvement in the accuracy of house price predictions, validating the effectiveness of our approach.","This research advances the field of spatial interpolation and provides a robust tool for more precise house price evaluation.","Our GitHub repository.contains the data and code for all datasets, which are available for researchers and practitioners interested in replicating or building upon our work."],"url":"http://arxiv.org/abs/2405.07456v1","category":"cs.LG"}
{"created":"2024-05-13 03:27:02","title":"PLA-SGCN: Protein-Ligand Binding Affinity Prediction by Integrating Similar Pairs and Semi-supervised Graph Convolutional Network","abstract":"The protein-ligand binding affinity (PLA) prediction goal is to predict whether or not the ligand could bind to a protein sequence. Recently, in PLA prediction, deep learning has received much attention. Two steps are involved in deep learning-based approaches: feature extraction and task prediction step. Many deep learning-based approaches concentrate on introducing new feature extraction networks or integrating auxiliary knowledge like protein-protein interaction networks or gene ontology knowledge. Then, a task prediction network is designed simply using some fully connected layers. This paper aims to integrate retrieved similar hard protein-ligand pairs in PLA prediction (i.e., task prediction step) using a semi-supervised graph convolutional network (GCN). Hard protein-ligand pairs are retrieved for each input query sample based on the manifold smoothness constraint. Then, a graph is learned automatically in which each node is a protein-ligand pair, and each edge represents the similarity between pairs. In other words, an end-to-end framework is proposed that simultaneously retrieves hard similar samples, learns protein-ligand descriptor, learns the graph topology of the input sample with retrieved similar hard samples (learn adjacency matrix), and learns a semi-supervised GCN to predict the binding affinity (as task predictor). The training step adjusts the parameter values, and in the inference step, the learned model is fine-tuned for each input sample. To evaluate the proposed approach, it is applied to the four well-known PDBbind, Davis, KIBA, and BindingDB datasets. The results show that the proposed method significantly performs better than the comparable approaches.","sentences":["The protein-ligand binding affinity (PLA) prediction goal is to predict whether or not the ligand could bind to a protein sequence.","Recently, in PLA prediction, deep learning has received much attention.","Two steps are involved in deep learning-based approaches: feature extraction and task prediction step.","Many deep learning-based approaches concentrate on introducing new feature extraction networks or integrating auxiliary knowledge like protein-protein interaction networks or gene ontology knowledge.","Then, a task prediction network is designed simply using some fully connected layers.","This paper aims to integrate retrieved similar hard protein-ligand pairs in PLA prediction (i.e., task prediction step) using a semi-supervised graph convolutional network (GCN).","Hard protein-ligand pairs are retrieved for each input query sample based on the manifold smoothness constraint.","Then, a graph is learned automatically in which each node is a protein-ligand pair, and each edge represents the similarity between pairs.","In other words, an end-to-end framework is proposed that simultaneously retrieves hard similar samples, learns protein-ligand descriptor, learns the graph topology of the input sample with retrieved similar hard samples (learn adjacency matrix), and learns a semi-supervised GCN to predict the binding affinity (as task predictor).","The training step adjusts the parameter values, and in the inference step, the learned model is fine-tuned for each input sample.","To evaluate the proposed approach, it is applied to the four well-known PDBbind, Davis, KIBA, and BindingDB datasets.","The results show that the proposed method significantly performs better than the comparable approaches."],"url":"http://arxiv.org/abs/2405.07452v1","category":"q-bio.QM"}
{"created":"2024-05-13 03:03:04","title":"Motion Keyframe Interpolation for Any Human Skeleton via Temporally Consistent Point Cloud Sampling and Reconstruction","abstract":"In the character animation field, modern supervised keyframe interpolation models have demonstrated exceptional performance in constructing natural human motions from sparse pose definitions. As supervised models, large motion datasets are necessary to facilitate the learning process; however, since motion is represented with fixed hierarchical skeletons, such datasets are incompatible for skeletons outside the datasets' native configurations. Consequently, the expected availability of a motion dataset for desired skeletons severely hinders the feasibility of learned interpolation in practice. To combat this limitation, we propose Point Cloud-based Motion Representation Learning (PC-MRL), an unsupervised approach to enabling cross-compatibility between skeletons for motion interpolation learning. PC-MRL consists of a skeleton obfuscation strategy using temporal point cloud sampling, and an unsupervised skeleton reconstruction method from point clouds. We devise a temporal point-wise K-nearest neighbors loss for unsupervised learning. Moreover, we propose First-frame Offset Quaternion (FOQ) and Rest Pose Augmentation (RPA) strategies to overcome necessary limitations of our unsupervised point cloud-to-skeletal motion process. Comprehensive experiments demonstrate the effectiveness of PC-MRL in motion interpolation for desired skeletons without supervision from native datasets.","sentences":["In the character animation field, modern supervised keyframe interpolation models have demonstrated exceptional performance in constructing natural human motions from sparse pose definitions.","As supervised models, large motion datasets are necessary to facilitate the learning process; however, since motion is represented with fixed hierarchical skeletons, such datasets are incompatible for skeletons outside the datasets' native configurations.","Consequently, the expected availability of a motion dataset for desired skeletons severely hinders the feasibility of learned interpolation in practice.","To combat this limitation, we propose Point Cloud-based Motion Representation Learning (PC-MRL), an unsupervised approach to enabling cross-compatibility between skeletons for motion interpolation learning.","PC-MRL consists of a skeleton obfuscation strategy using temporal point cloud sampling, and an unsupervised skeleton reconstruction method from point clouds.","We devise a temporal point-wise K-nearest neighbors loss for unsupervised learning.","Moreover, we propose First-frame Offset Quaternion (FOQ) and Rest Pose Augmentation (RPA) strategies to overcome necessary limitations of our unsupervised point cloud-to-skeletal motion process.","Comprehensive experiments demonstrate the effectiveness of PC-MRL in motion interpolation for desired skeletons without supervision from native datasets."],"url":"http://arxiv.org/abs/2405.07444v1","category":"cs.CV"}
{"created":"2024-05-13 02:18:49","title":"Compressed Online Learning of Conditional Mean Embedding","abstract":"The conditional mean embedding (CME) encodes Markovian stochastic kernels through their actions on probability distributions embedded within the reproducing kernel Hilbert spaces (RKHS). The CME plays a key role in several well-known machine learning tasks such as reinforcement learning, analysis of dynamical systems, etc. We present an algorithm to learn the CME incrementally from data via an operator-valued stochastic gradient descent. As is well-known, function learning in RKHS suffers from scalability challenges from large data. We utilize a compression mechanism to counter the scalability challenge. The core contribution of this paper is a finite-sample performance guarantee on the last iterate of the online compressed operator learning algorithm with fast-mixing Markovian samples, when the target CME may not be contained in the hypothesis space. We illustrate the efficacy of our algorithm by applying it to the analysis of an example dynamical system.","sentences":["The conditional mean embedding (CME) encodes Markovian stochastic kernels through their actions on probability distributions embedded within the reproducing kernel Hilbert spaces (RKHS).","The CME plays a key role in several well-known machine learning tasks such as reinforcement learning, analysis of dynamical systems, etc.","We present an algorithm to learn the CME incrementally from data via an operator-valued stochastic gradient descent.","As is well-known, function learning in RKHS suffers from scalability challenges from large data.","We utilize a compression mechanism to counter the scalability challenge.","The core contribution of this paper is a finite-sample performance guarantee on the last iterate of the online compressed operator learning algorithm with fast-mixing Markovian samples, when the target CME may not be contained in the hypothesis space.","We illustrate the efficacy of our algorithm by applying it to the analysis of an example dynamical system."],"url":"http://arxiv.org/abs/2405.07432v1","category":"stat.ML"}
{"created":"2024-05-13 01:39:25","title":"Indoor and Outdoor Crowd Density Level Estimation with Video Analysis through Machine Learning Models","abstract":"Crowd density level estimation is an essential aspect of crowd safety since it helps to identify areas of probable overcrowding and required conditions. Nowadays, AI systems can help in various sectors. Here for safety purposes or many for public service crowd detection, tracking or estimating crowd level is essential. So we decided to build an AI project to fulfil the purpose. This project can detect crowds from images, videos, or webcams. From these images, videos, or webcams, this system can detect, track and identify humans. This system also can estimate the crowd level. Though this project is simple, it is very effective, user-friendly, and less costly. Also, we trained our system with a dataset. So our system also can predict the crowd. Though the AI system is not a hundred percent accurate, this project is more than 97 percent accurate. We also represent the dataset in a graphical way.","sentences":["Crowd density level estimation is an essential aspect of crowd safety since it helps to identify areas of probable overcrowding and required conditions.","Nowadays, AI systems can help in various sectors.","Here for safety purposes or many for public service crowd detection, tracking or estimating crowd level is essential.","So we decided to build an AI project to fulfil the purpose.","This project can detect crowds from images, videos, or webcams.","From these images, videos, or webcams, this system can detect, track and identify humans.","This system also can estimate the crowd level.","Though this project is simple, it is very effective, user-friendly, and less costly.","Also, we trained our system with a dataset.","So our system also can predict the crowd.","Though the AI system is not a hundred percent accurate, this project is more than 97 percent accurate.","We also represent the dataset in a graphical way."],"url":"http://arxiv.org/abs/2405.07419v1","category":"cs.CR"}
{"created":"2024-05-12 23:00:53","title":"NGD-SLAM: Towards Real-Time SLAM for Dynamic Environments without GPU","abstract":"Accurate and robust camera tracking in dynamic environments presents a significant challenge for visual SLAM (Simultaneous Localization and Mapping). Recent progress in this field often involves the use of deep learning techniques to generate mask for dynamic objects, which usually require GPUs to operate in real-time (30 fps). Therefore, this paper proposes a novel visual SLAM system for dynamic environments that obtains real-time performance on CPU by incorporating a mask prediction mechanism, which allows the deep learning method and the camera tracking to run entirely in parallel at different frequencies such that neither waits for the result from the other. Based on this, it further introduces a dual-stage optical flow tracking approach and employs a hybrid usage of optical flow and ORB features, which significantly enhance the efficiency and robustness of the system. Compared with state-of-the-art methods, this system maintains high localization accuracy in dynamic environments while achieving a tracking frame rate of 56 fps on a single laptop CPU without any hardware acceleration, thus proving that deep learning methods are still feasible for dynamic SLAM even without GPU support. Based on the available information, this is the first SLAM system to achieve this.","sentences":["Accurate and robust camera tracking in dynamic environments presents a significant challenge for visual SLAM (Simultaneous Localization and Mapping).","Recent progress in this field often involves the use of deep learning techniques to generate mask for dynamic objects, which usually require GPUs to operate in real-time (30 fps).","Therefore, this paper proposes a novel visual SLAM system for dynamic environments that obtains real-time performance on CPU by incorporating a mask prediction mechanism, which allows the deep learning method and the camera tracking to run entirely in parallel at different frequencies such that neither waits for the result from the other.","Based on this, it further introduces a dual-stage optical flow tracking approach and employs a hybrid usage of optical flow and ORB features, which significantly enhance the efficiency and robustness of the system.","Compared with state-of-the-art methods, this system maintains high localization accuracy in dynamic environments while achieving a tracking frame rate of 56 fps on a single laptop CPU without any hardware acceleration, thus proving that deep learning methods are still feasible for dynamic SLAM even without GPU support.","Based on the available information, this is the first SLAM system to achieve this."],"url":"http://arxiv.org/abs/2405.07392v1","category":"cs.RO"}
{"created":"2024-05-12 18:25:38","title":"SoccerNet-Echoes: A Soccer Game Audio Commentary Dataset","abstract":"The application of Automatic Speech Recognition (ASR) technology in soccer offers numerous opportunities for sports analytics. Specifically, extracting audio commentaries with ASR provides valuable insights into the events of the game, and opens the door to several downstream applications such as automatic highlight generation. This paper presents SoccerNet-Echoes, an augmentation of the SoccerNet dataset with automatically generated transcriptions of audio commentaries from soccer game broadcasts, enhancing video content with rich layers of textual information derived from the game audio using ASR. These textual commentaries, generated using the Whisper model and translated with Google Translate, extend the usefulness of the SoccerNet dataset in diverse applications such as enhanced action spotting, automatic caption generation, and game summarization. By incorporating textual data alongside visual and auditory content, SoccerNet-Echoes aims to serve as a comprehensive resource for the development of algorithms specialized in capturing the dynamics of soccer games. We detail the methods involved in the curation of this dataset and the integration of ASR. We also highlight the implications of a multimodal approach in sports analytics, and how the enriched dataset can support diverse applications, thus broadening the scope of research and development in the field of sports analytics.","sentences":["The application of Automatic Speech Recognition (ASR) technology in soccer offers numerous opportunities for sports analytics.","Specifically, extracting audio commentaries with ASR provides valuable insights into the events of the game, and opens the door to several downstream applications such as automatic highlight generation.","This paper presents SoccerNet-Echoes, an augmentation of the SoccerNet dataset with automatically generated transcriptions of audio commentaries from soccer game broadcasts, enhancing video content with rich layers of textual information derived from the game audio using ASR.","These textual commentaries, generated using the Whisper model and translated with Google Translate, extend the usefulness of the SoccerNet dataset in diverse applications such as enhanced action spotting, automatic caption generation, and game summarization.","By incorporating textual data alongside visual and auditory content, SoccerNet-Echoes aims to serve as a comprehensive resource for the development of algorithms specialized in capturing the dynamics of soccer games.","We detail the methods involved in the curation of this dataset and the integration of ASR.","We also highlight the implications of a multimodal approach in sports analytics, and how the enriched dataset can support diverse applications, thus broadening the scope of research and development in the field of sports analytics."],"url":"http://arxiv.org/abs/2405.07354v1","category":"cs.SD"}
{"created":"2024-05-12 17:54:50","title":"MedConceptsQA -- Open Source Medical Concepts QA Benchmark","abstract":"We present MedConceptsQA, a dedicated open source benchmark for medical concepts question answering. The benchmark comprises of questions of various medical concepts across different vocabularies: diagnoses, procedures, and drugs. The questions are categorized into three levels of difficulty: easy, medium, and hard. We conducted evaluations of the benchmark using various Large Language Models. Our findings show that pre-trained clinical Large Language Models achieved accuracy levels close to random guessing on this benchmark, despite being pre-trained on medical data. However, GPT-4 achieves an absolute average improvement of nearly 27%-37% (27% for zero-shot learning and 37% for few-shot learning) when compared to clinical Large Language Models. Our benchmark serves as a valuable resource for evaluating the understanding and reasoning of medical concepts by Large Language Models. Our benchmark is available at https://huggingface.co/datasets/ofir408/MedConceptsQA","sentences":["We present MedConceptsQA, a dedicated open source benchmark for medical concepts question answering.","The benchmark comprises of questions of various medical concepts across different vocabularies: diagnoses, procedures, and drugs.","The questions are categorized into three levels of difficulty: easy, medium, and hard.","We conducted evaluations of the benchmark using various Large Language Models.","Our findings show that pre-trained clinical Large Language Models achieved accuracy levels close to random guessing on this benchmark, despite being pre-trained on medical data.","However, GPT-4 achieves an absolute average improvement of nearly 27%-37% (27% for zero-shot learning and 37% for few-shot learning) when compared to clinical Large Language Models.","Our benchmark serves as a valuable resource for evaluating the understanding and reasoning of medical concepts by Large Language Models.","Our benchmark is available at https://huggingface.co/datasets/ofir408/MedConceptsQA"],"url":"http://arxiv.org/abs/2405.07348v1","category":"cs.CL"}
{"created":"2024-05-12 15:46:52","title":"Nonparametric Control-Koopman Operator Learning: Flexible and Scalable Models for Prediction and Control","abstract":"Linearity of Koopman operators and simplicity of their estimators coupled with model-reduction capabilities has lead to their great popularity in applications for learning dynamical systems. While nonparametric Koopman operator learning in infinite-dimensional reproducing kernel Hilbert spaces is well understood for autonomous systems, its control system analogues are largely unexplored. Addressing systems with control inputs in a principled manner is crucial for fully data-driven learning of controllers, especially since existing approaches commonly resort to representational heuristics or parametric models of limited expressiveness and scalability. We address the aforementioned challenge by proposing a universal framework via control-affine reproducing kernels that enables direct estimation of a single operator even for control systems. The proposed approach, called control-Koopman operator regression (cKOR), is thus completely analogous to Koopman operator regression of the autonomous case. First in the literature, we present a nonparametric framework for learning Koopman operator representations of nonlinear control-affine systems that does not suffer from the curse of control input dimensionality. This allows for reformulating the infinite-dimensional learning problem in a finite-dimensional space based solely on data without apriori loss of precision due to a restriction to a finite span of functions or inputs as in other approaches. For enabling applications to large-scale control systems, we also enhance the scalability of control-Koopman operator estimators by leveraging random projections (sketching). The efficacy of our novel cKOR approach is demonstrated on both forecasting and control tasks.","sentences":["Linearity of Koopman operators and simplicity of their estimators coupled with model-reduction capabilities has lead to their great popularity in applications for learning dynamical systems.","While nonparametric Koopman operator learning in infinite-dimensional reproducing kernel Hilbert spaces is well understood for autonomous systems, its control system analogues are largely unexplored.","Addressing systems with control inputs in a principled manner is crucial for fully data-driven learning of controllers, especially since existing approaches commonly resort to representational heuristics or parametric models of limited expressiveness and scalability.","We address the aforementioned challenge by proposing a universal framework via control-affine reproducing kernels that enables direct estimation of a single operator even for control systems.","The proposed approach, called control-Koopman operator regression (cKOR), is thus completely analogous to Koopman operator regression of the autonomous case.","First in the literature, we present a nonparametric framework for learning Koopman operator representations of nonlinear control-affine systems that does not suffer from the curse of control input dimensionality.","This allows for reformulating the infinite-dimensional learning problem in a finite-dimensional space based solely on data without apriori loss of precision due to a restriction to a finite span of functions or inputs as in other approaches.","For enabling applications to large-scale control systems, we also enhance the scalability of control-Koopman operator estimators by leveraging random projections (sketching).","The efficacy of our novel cKOR approach is demonstrated on both forecasting and control tasks."],"url":"http://arxiv.org/abs/2405.07312v1","category":"eess.SY"}
{"created":"2024-05-12 13:51:11","title":"Zero Shot Context-Based Object Segmentation using SLIP (SAM+CLIP)","abstract":"We present SLIP (SAM+CLIP), an enhanced architecture for zero-shot object segmentation. SLIP combines the Segment Anything Model (SAM) \\cite{kirillov2023segment} with the Contrastive Language-Image Pretraining (CLIP) \\cite{radford2021learning}. By incorporating text prompts into SAM using CLIP, SLIP enables object segmentation without prior training on specific classes or categories. We fine-tune CLIP on a Pokemon dataset, allowing it to learn meaningful image-text representations. SLIP demonstrates the ability to recognize and segment objects in images based on contextual information from text prompts, expanding the capabilities of SAM for versatile object segmentation. Our experiments demonstrate the effectiveness of the SLIP architecture in segmenting objects in images based on textual cues. The integration of CLIP's text-image understanding capabilities into SAM expands the capabilities of the original architecture and enables more versatile and context-aware object segmentation.","sentences":["We present SLIP (SAM+CLIP), an enhanced architecture for zero-shot object segmentation.","SLIP combines the Segment Anything Model (SAM) \\cite{kirillov2023segment} with the Contrastive Language-Image Pretraining (CLIP) \\cite{radford2021learning}.","By incorporating text prompts into SAM using CLIP, SLIP enables object segmentation without prior training on specific classes or categories.","We fine-tune CLIP on a Pokemon dataset, allowing it to learn meaningful image-text representations.","SLIP demonstrates the ability to recognize and segment objects in images based on contextual information from text prompts, expanding the capabilities of SAM for versatile object segmentation.","Our experiments demonstrate the effectiveness of the SLIP architecture in segmenting objects in images based on textual cues.","The integration of CLIP's text-image understanding capabilities into SAM expands the capabilities of the original architecture and enables more versatile and context-aware object segmentation."],"url":"http://arxiv.org/abs/2405.07284v1","category":"cs.CV"}
{"created":"2024-05-12 12:55:40","title":"Human-interpretable clustering of short-text using large language models","abstract":"Large language models have seen extraordinary growth in popularity due to their human-like content generation capabilities. We show that these models can also be used to successfully cluster human-generated content, with success defined through the measures of distinctiveness and interpretability. This success is validated by both human reviewers and ChatGPT, providing an automated means to close the 'validation gap' that has challenged short-text clustering. Comparing the machine and human approaches we identify the biases inherent in each, and question the reliance on human-coding as the 'gold standard'. We apply our methodology to Twitter bios and find characteristic ways humans describe themselves, agreeing well with prior specialist work, but with interesting differences characteristic of the medium used to express identity.","sentences":["Large language models have seen extraordinary growth in popularity due to their human-like content generation capabilities.","We show that these models can also be used to successfully cluster human-generated content, with success defined through the measures of distinctiveness and interpretability.","This success is validated by both human reviewers and ChatGPT, providing an automated means to close the 'validation gap' that has challenged short-text clustering.","Comparing the machine and human approaches we identify the biases inherent in each, and question the reliance on human-coding as the 'gold standard'.","We apply our methodology to Twitter bios and find characteristic ways humans describe themselves, agreeing well with prior specialist work, but with interesting differences characteristic of the medium used to express identity."],"url":"http://arxiv.org/abs/2405.07278v1","category":"cs.CL"}
{"created":"2024-05-12 10:00:10","title":"Soft Contact Simulation and Manipulation Learning of Deformable Objects with Vision-based Tactile Sensor","abstract":"Deformable object manipulation is a classical and challenging research area in robotics. Compared with rigid object manipulation, this problem is more complex due to the deformation properties including elastic, plastic, and elastoplastic deformation. In this paper, we describe a new deformable object manipulation method including soft contact simulation, manipulation learning, and sim-to-real transfer. We propose a novel approach utilizing Vision-Based Tactile Sensors (VBTSs) as the end-effector in simulation to produce observations like relative position, squeezed area, and object contour, which are transferable to real robots. For a more realistic contact simulation, a new simulation environment including elastic, plastic, and elastoplastic deformations is created. We utilize RL strategies to train agents in the simulation, and expert demonstrations are applied for challenging tasks. Finally, we build a real experimental platform to complete the sim-to-real transfer and achieve a 90% success rate on difficult tasks such as cylinder and sphere. To test the robustness of our method, we use plasticine of different hardness and sizes to repeat the tasks including cylinder and sphere. The experimental results show superior performances of deformable object manipulation with the proposed method.","sentences":["Deformable object manipulation is a classical and challenging research area in robotics.","Compared with rigid object manipulation, this problem is more complex due to the deformation properties including elastic, plastic, and elastoplastic deformation.","In this paper, we describe a new deformable object manipulation method including soft contact simulation, manipulation learning, and sim-to-real transfer.","We propose a novel approach utilizing Vision-Based Tactile Sensors (VBTSs) as the end-effector in simulation to produce observations like relative position, squeezed area, and object contour, which are transferable to real robots.","For a more realistic contact simulation, a new simulation environment including elastic, plastic, and elastoplastic deformations is created.","We utilize RL strategies to train agents in the simulation, and expert demonstrations are applied for challenging tasks.","Finally, we build a real experimental platform to complete the sim-to-real transfer and achieve a 90% success rate on difficult tasks such as cylinder and sphere.","To test the robustness of our method, we use plasticine of different hardness and sizes to repeat the tasks including cylinder and sphere.","The experimental results show superior performances of deformable object manipulation with the proposed method."],"url":"http://arxiv.org/abs/2405.07237v1","category":"cs.RO"}
{"created":"2024-05-12 09:29:59","title":"A Flow is a Stream of Packets: A Stream-Structured Data Approach for DDoS Detection","abstract":"Distributed Denial of Service (DDoS) attacks are getting increasingly harmful to the Internet, showing no signs of slowing down. Developing an accurate detection mechanism to thwart DDoS attacks is still a big challenge due to the rich variety of these attacks and the emergence of new attack vectors. In this paper, we propose a new tree-based DDoS detection approach that operates on a flow as a stream structure, rather than the traditional fixed-size record structure containing aggregated flow statistics. Although aggregated flow records have gained popularity over the past decade, providing an effective means for flow-based intrusion detection by inspecting only a fraction of the total traffic volume, they are inherently constrained. Their detection precision is limited not only by the lack of packet payloads, but also by their structure, which is unable to model fine-grained inter-packet relations, such as packet order and temporal relations. Additionally, inferring aggregated flow statistics must wait for the complete flow to end. Here we show that considering flow inputs as variable-length streams composed of their associated packet headers, allows for very accurate and fast detection of malicious flows. We evaluate our proposed strategy on the CICDDoS2019 and CICIDS2017 datasets, which contain a comprehensive variety of DDoS attacks. Our approach matches or exceeds existing machine learning techniques' accuracy, including state-of-the-art deep learning methods. Furthermore, our method achieves significantly earlier detection, e.g., with CICDDoS2019 detection based on the first 2 packets, which corresponds to an average time-saving of 99.79% and uses only 4--6% of the traffic volume.","sentences":["Distributed Denial of Service (DDoS) attacks are getting increasingly harmful to the Internet, showing no signs of slowing down.","Developing an accurate detection mechanism to thwart DDoS attacks is still a big challenge due to the rich variety of these attacks and the emergence of new attack vectors.","In this paper, we propose a new tree-based DDoS detection approach that operates on a flow as a stream structure, rather than the traditional fixed-size record structure containing aggregated flow statistics.","Although aggregated flow records have gained popularity over the past decade, providing an effective means for flow-based intrusion detection by inspecting only a fraction of the total traffic volume, they are inherently constrained.","Their detection precision is limited not only by the lack of packet payloads, but also by their structure, which is unable to model fine-grained inter-packet relations, such as packet order and temporal relations.","Additionally, inferring aggregated flow statistics must wait for the complete flow to end.","Here we show that considering flow inputs as variable-length streams composed of their associated packet headers, allows for very accurate and fast detection of malicious flows.","We evaluate our proposed strategy on the CICDDoS2019 and CICIDS2017 datasets, which contain a comprehensive variety of DDoS attacks.","Our approach matches or exceeds existing machine learning techniques' accuracy, including state-of-the-art deep learning methods.","Furthermore, our method achieves significantly earlier detection, e.g., with CICDDoS2019 detection based on the first 2 packets, which corresponds to an average time-saving of 99.79% and uses only 4--6% of the traffic volume."],"url":"http://arxiv.org/abs/2405.07232v1","category":"cs.CR"}
{"created":"2024-05-13 17:55:52","title":"Identifying the minimal sets of distance restraints for FRET-assisted protein structural modeling","abstract":"Proteins naturally occur in crowded cellular environments and interact with other proteins, nucleic acids, and organelles. Since most previous experimental protein structure determination techniques require that proteins occur in idealized, non-physiological environments, the effects of realistic cellular environments on protein structure are largely unexplored. Recently, F\\\"{o}rster resonance energy transfer (FRET) has been shown to be an effective experimental method for investigating protein structure in vivo. Inter-residue distances measured in vivo can be incorporated as restraints in molecular dynamics (MD) simulations to model protein structural dynamics in vivo. Since most FRET studies only obtain inter-residue separations for a small number of amino acid pairs, it is important to determine the minimum number of restraints in the MD simulations that are required to achieve a given root-mean-square deviation (RMSD) from the experimental structural ensemble. Further, what is the optimal method for selecting these inter-residue restraints? Here, we implement several methods for selecting the most important FRET pairs and determine the number of pairs $N_{r}$ that are needed to induce conformational changes in proteins between two experimentally determined structures. We find that enforcing only a small fraction of restraints, $N_{r}/N \\lesssim 0.08$, where $N$ is the number of amino acids, can induce the conformational changes. These results establish the efficacy of FRET-assisted MD simulations for atomic scale structural modeling of proteins in vivo.","sentences":["Proteins naturally occur in crowded cellular environments and interact with other proteins, nucleic acids, and organelles.","Since most previous experimental protein structure determination techniques require that proteins occur in idealized, non-physiological environments, the effects of realistic cellular environments on protein structure are largely unexplored.","Recently, F\\\"{o}rster resonance energy transfer (FRET) has been shown to be an effective experimental method for investigating protein structure in vivo.","Inter-residue distances measured in vivo can be incorporated as restraints in molecular dynamics (MD) simulations to model protein structural dynamics in vivo.","Since most FRET studies only obtain inter-residue separations for a small number of amino acid pairs, it is important to determine the minimum number of restraints in the MD simulations that are required to achieve a given root-mean-square deviation (RMSD) from the experimental structural ensemble.","Further, what is the optimal method for selecting these inter-residue restraints?","Here, we implement several methods for selecting the most important FRET pairs and determine the number of pairs $N_{r}$ that are needed to induce conformational changes in proteins between two experimentally determined structures.","We find that enforcing only a small fraction of restraints, $N_{r}/N \\lesssim 0.08$, where $N$ is the number of amino acids, can induce the conformational changes.","These results establish the efficacy of FRET-assisted MD simulations for atomic scale structural modeling of proteins in vivo."],"url":"http://arxiv.org/abs/2405.07983v1","category":"physics.bio-ph"}
{"created":"2024-05-13 16:25:43","title":"Subspace-Informed Matrix Completion","abstract":"In this work, we consider the matrix completion problem, where the objective is to reconstruct a low-rank matrix from a few observed entries. A commonly employed approach involves nuclear norm minimization. For this method to succeed, the number of observed entries needs to scale at least proportional to both the rank of the ground-truth matrix and the coherence parameter. While the only prior information is oftentimes the low-rank nature of the ground-truth matrix, in various real-world scenarios, additional knowledge about the ground-truth low-rank matrix is available. For instance, in collaborative filtering, Netflix problem, and dynamic channel estimation in wireless communications, we have partial or full knowledge about the signal subspace in advance. Specifically, we are aware of some subspaces that form multiple angles with the column and row spaces of the ground-truth matrix. Leveraging this valuable information has the potential to significantly reduce the required number of observations. To this end, we introduce a multi-weight nuclear norm optimization problem that concurrently promotes the low-rank property as well the information about the available subspaces. The proposed weights are tailored to penalize each angle corresponding to each basis of the prior subspace independently. We further propose an optimal weight selection strategy by minimizing the coherence parameter of the ground-truth matrix, which is equivalent to minimizing the required number of observations. Simulation results validate the advantages of incorporating multiple weights in the completion procedure. Specifically, our proposed multi-weight optimization problem demonstrates a substantial reduction in the required number of observations compared to the state-of-the-art methods.","sentences":["In this work, we consider the matrix completion problem, where the objective is to reconstruct a low-rank matrix from a few observed entries.","A commonly employed approach involves nuclear norm minimization.","For this method to succeed, the number of observed entries needs to scale at least proportional to both the rank of the ground-truth matrix and the coherence parameter.","While the only prior information is oftentimes the low-rank nature of the ground-truth matrix, in various real-world scenarios, additional knowledge about the ground-truth low-rank matrix is available.","For instance, in collaborative filtering, Netflix problem, and dynamic channel estimation in wireless communications, we have partial or full knowledge about the signal subspace in advance.","Specifically, we are aware of some subspaces that form multiple angles with the column and row spaces of the ground-truth matrix.","Leveraging this valuable information has the potential to significantly reduce the required number of observations.","To this end, we introduce a multi-weight nuclear norm optimization problem that concurrently promotes the low-rank property as well the information about the available subspaces.","The proposed weights are tailored to penalize each angle corresponding to each basis of the prior subspace independently.","We further propose an optimal weight selection strategy by minimizing the coherence parameter of the ground-truth matrix, which is equivalent to minimizing the required number of observations.","Simulation results validate the advantages of incorporating multiple weights in the completion procedure.","Specifically, our proposed multi-weight optimization problem demonstrates a substantial reduction in the required number of observations compared to the state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.07890v1","category":"eess.SP"}
{"created":"2024-05-13 15:48:26","title":"Improving Breast Cancer Grade Prediction with Multiparametric MRI Created Using Optimized Synthetic Correlated Diffusion Imaging","abstract":"Breast cancer was diagnosed for over 7.8 million women between 2015 to 2020. Grading plays a vital role in breast cancer treatment planning. However, the current tumor grading method involves extracting tissue from patients, leading to stress, discomfort, and high medical costs. A recent paper leveraging volumetric deep radiomic features from synthetic correlated diffusion imaging (CDI$^s$) for breast cancer grade prediction showed immense promise for noninvasive methods for grading. Motivated by the impact of CDI$^s$ optimization for prostate cancer delineation, this paper examines using optimized CDI$^s$ to improve breast cancer grade prediction. We fuse the optimized CDI$^s$ signal with diffusion-weighted imaging (DWI) to create a multiparametric MRI for each patient. Using a larger patient cohort and training across all the layers of a pretrained MONAI model, we achieve a leave-one-out cross-validation accuracy of 95.79%, over 8% higher compared to that previously reported.","sentences":["Breast cancer was diagnosed for over 7.8 million women between 2015 to 2020.","Grading plays a vital role in breast cancer treatment planning.","However, the current tumor grading method involves extracting tissue from patients, leading to stress, discomfort, and high medical costs.","A recent paper leveraging volumetric deep radiomic features from synthetic correlated diffusion imaging (CDI$^s$) for breast cancer grade prediction showed immense promise for noninvasive methods for grading.","Motivated by the impact of CDI$^s$ optimization for prostate cancer delineation, this paper examines using optimized CDI$^s$ to improve breast cancer grade prediction.","We fuse the optimized CDI$^s$ signal with diffusion-weighted imaging (DWI) to create a multiparametric MRI for each patient.","Using a larger patient cohort and training across all the layers of a pretrained MONAI model, we achieve a leave-one-out cross-validation accuracy of 95.79%, over 8% higher compared to that previously reported."],"url":"http://arxiv.org/abs/2405.07861v1","category":"eess.IV"}
{"created":"2024-05-13 14:48:57","title":"Goal-oriented compression for $L_p$-norm-type goal functions: Application to power consumption scheduling","abstract":"Conventional data compression schemes aim at implementing a trade-off between the rate required to represent the compressed data and the resulting distortion between the original and reconstructed data. However, in more and more applications, what is desired is not reconstruction accuracy but the quality of the realization of a certain task by the receiver. In this paper, the receiver task is modeled by an optimization problem whose parameters have to be compressed by the transmitter. Motivated by applications such as the smart grid, this paper focuses on a goal function which is of $L_p$-norm-type. The aim is to design the precoding, quantization, and decoding stages such that the maximum of the goal function obtained with the compressed version of the parameters is as close as possible to the maximum obtained without compression. The numerical analysis, based on real smart grid signals, clearly shows the benefits of the proposed approach compared to the conventional distortion-based compression paradigm.","sentences":["Conventional data compression schemes aim at implementing a trade-off between the rate required to represent the compressed data and the resulting distortion between the original and reconstructed data.","However, in more and more applications, what is desired is not reconstruction accuracy but the quality of the realization of a certain task by the receiver.","In this paper, the receiver task is modeled by an optimization problem whose parameters have to be compressed by the transmitter.","Motivated by applications such as the smart grid, this paper focuses on a goal function which is of $L_p$-norm-type.","The aim is to design the precoding, quantization, and decoding stages such that the maximum of the goal function obtained with the compressed version of the parameters is as close as possible to the maximum obtained without compression.","The numerical analysis, based on real smart grid signals, clearly shows the benefits of the proposed approach compared to the conventional distortion-based compression paradigm."],"url":"http://arxiv.org/abs/2405.07808v1","category":"eess.SP"}
{"created":"2024-05-13 13:56:46","title":"Significant improvement in sensitivity of an anomalous Nernst heat flux sensor by composite structure","abstract":"Heat flux sensors (HFS) have attracted significant interest for their potential in managing waste heat efficiently. A recently proposed HFS, that works on the basis of the anomalous Nernst effect (ANE), offers several advantages in its simple structure leading to easy fabrication, low cost, and reduced thermal resistance. However, enhancing sensitivity through traditional material selection is now challenging due to a small number of materials satisfying the required coexistence of a large transverse Seebeck coefficient and low thermal conductivity. In this study, by utilizing composite structures and optimizing the device geometry, we have achieved a substantial improvement in the sensitivity of an ANE-based HFS. We developed composite structures comprised of a plastic substrate with an uneven surface and three-dimensional (3D) uneven TbCo films, fabricated using nanoimprint techniques and sputtering. This approach resulted in a sensitivity that is approximately four times greater than that observed in previous studies. Importantly, this method is independent of the material properties and can significantly enhance the sensitivity. Our findings could lead to the development of highly sensitive HFS devices and open new avenues for the fabrication of 3D devices.","sentences":["Heat flux sensors (HFS) have attracted significant interest for their potential in managing waste heat efficiently.","A recently proposed HFS, that works on the basis of the anomalous Nernst effect (ANE), offers several advantages in its simple structure leading to easy fabrication, low cost, and reduced thermal resistance.","However, enhancing sensitivity through traditional material selection is now challenging due to a small number of materials satisfying the required coexistence of a large transverse Seebeck coefficient and low thermal conductivity.","In this study, by utilizing composite structures and optimizing the device geometry, we have achieved a substantial improvement in the sensitivity of an ANE-based HFS.","We developed composite structures comprised of a plastic substrate with an uneven surface and three-dimensional (3D) uneven TbCo films, fabricated using nanoimprint techniques and sputtering.","This approach resulted in a sensitivity that is approximately four times greater than that observed in previous studies.","Importantly, this method is independent of the material properties and can significantly enhance the sensitivity.","Our findings could lead to the development of highly sensitive HFS devices and open new avenues for the fabrication of 3D devices."],"url":"http://arxiv.org/abs/2405.07758v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-13 13:55:19","title":"High-resolution dynamic consistency analysis of photonic time-delay reservoir computer","abstract":"We numerically investigate a time-delayed reservoir computer architecture based on a single mode laser diode with optical injection and optical feedback. Through a high-resolution parametric analysis, we reveal unforeseen regions of high dynamical consistency. We demonstrate furthermore that the best computing performance is not achieved at the edge of consistency as previously suggested in a coarser parametric analysis. This region of high consistency and optimal reservoir performances are highly sensitive to the data input modulation format","sentences":["We numerically investigate a time-delayed reservoir computer architecture based on a single mode laser diode with optical injection and optical feedback.","Through a high-resolution parametric analysis, we reveal unforeseen regions of high dynamical consistency.","We demonstrate furthermore that the best computing performance is not achieved at the edge of consistency as previously suggested in a coarser parametric analysis.","This region of high consistency and optimal reservoir performances are highly sensitive to the data input modulation format"],"url":"http://arxiv.org/abs/2405.07756v1","category":"physics.optics"}
{"created":"2024-05-13 13:38:40","title":"Optimal discrete Hardy-Rellich-Birman inequalities","abstract":"We prove sufficient conditions on a parameter sequence to determine optimal weights in inequalities for an integer power $\\ell$ of the discrete Laplacian on the half-line. By a concrete choice of the parameter sequence, we obtain explicit optimal discrete Rellich ($\\ell=2$) and Birman ($\\ell\\geq3$) weights. For $\\ell=1$, we rediscover the optimal Hardy weight of Keller-Pinchover-Pogorzelski. For $\\ell=2$, we improve upon the best known Rellich weights due to Gerhat-Krej\\v{c}i\\v{r}\\'{i}k-\\v{S}tampach and Huang-Ye. For $\\ell\\geq3$, our main result proves a conjecture by Gerhat-Krej\\v{c}i\\v{r}\\'{i}k-\\v{S}tampach and improves the discrete analogue of the classical Birman weight due to Huang-Ye to the optimal.","sentences":["We prove sufficient conditions on a parameter sequence to determine optimal weights in inequalities for an integer power $\\ell$ of the discrete Laplacian on the half-line.","By a concrete choice of the parameter sequence, we obtain explicit optimal discrete Rellich ($\\ell=2$) and Birman ($\\ell\\geq3$) weights.","For $\\ell=1$, we rediscover the optimal Hardy weight of Keller-Pinchover-Pogorzelski.","For $\\ell=2$, we improve upon the best known Rellich weights due to Gerhat-Krej\\v{c}i\\v{r}\\'{i}k-\\v{S}tampach and Huang-Ye.","For $\\ell\\geq3$, our main result proves a conjecture by Gerhat-Krej\\v{c}i\\v{r}\\'{i}k-\\v{S}tampach and improves the discrete analogue of the classical Birman weight due to Huang-Ye to the optimal."],"url":"http://arxiv.org/abs/2405.07742v1","category":"math.CA"}
{"created":"2024-05-13 13:34:42","title":"A Low-rank Projected Proximal Gradient Method for Spectral Compressed Sensing","abstract":"This paper presents a new approach to the recovery of a spectrally sparse signal (SSS) from partially observed entries, focusing on challenges posed by large-scale data and heavy noise environments. The SSS reconstruction can be formulated as a non-convex low-rank Hankel recovery problem. Traditional formulations for SSS recovery often suffer from reconstruction inaccuracies due to unequally weighted norms and over-relaxation of the Hankel structure in noisy conditions. Moreover, a critical limitation of standard proximal gradient (PG) methods for solving the optimization problem is their slow convergence. We overcome this by introducing a more accurate formulation and a Low-rank Projected Proximal Gradient (LPPG) method, designed to efficiently converge to stationary points through a two-step process. The first step involves a modified PG approach, allowing for a constant step size independent of signal size, which significantly accelerates the gradient descent phase. The second step employs a subspace projection strategy, optimizing within a low-rank matrix space to further decrease the objective function. Both steps of the LPPG method are meticulously tailored to exploit the intrinsic low-rank and Hankel structures of the problem, thereby enhancing computational efficiency. Our numerical simulations reveal a substantial improvement in both the efficiency and recovery accuracy of the LPPG method compared to existing benchmark algorithms. This performance gain is particularly pronounced in scenarios with significant noise, demonstrating the method's robustness and applicability to large-scale SSS recovery tasks.","sentences":["This paper presents a new approach to the recovery of a spectrally sparse signal (SSS) from partially observed entries, focusing on challenges posed by large-scale data and heavy noise environments.","The SSS reconstruction can be formulated as a non-convex low-rank Hankel recovery problem.","Traditional formulations for SSS recovery often suffer from reconstruction inaccuracies due to unequally weighted norms and over-relaxation of the Hankel structure in noisy conditions.","Moreover, a critical limitation of standard proximal gradient (PG) methods for solving the optimization problem is their slow convergence.","We overcome this by introducing a more accurate formulation and a Low-rank Projected Proximal Gradient (LPPG) method, designed to efficiently converge to stationary points through a two-step process.","The first step involves a modified PG approach, allowing for a constant step size independent of signal size, which significantly accelerates the gradient descent phase.","The second step employs a subspace projection strategy, optimizing within a low-rank matrix space to further decrease the objective function.","Both steps of the LPPG method are meticulously tailored to exploit the intrinsic low-rank and Hankel structures of the problem, thereby enhancing computational efficiency.","Our numerical simulations reveal a substantial improvement in both the efficiency and recovery accuracy of the LPPG method compared to existing benchmark algorithms.","This performance gain is particularly pronounced in scenarios with significant noise, demonstrating the method's robustness and applicability to large-scale SSS recovery tasks."],"url":"http://arxiv.org/abs/2405.07739v1","category":"eess.SP"}
{"created":"2024-05-13 13:23:11","title":"Optimal bolometer transfer function deconvolution for CMB experiments through maximum likelihood mapmaking","abstract":"We revisit the impact of finite time responses of bolometric detectors used for deep observations of the cosmic microwave background (CMB). Until now, bolometer transfer functions have been accounted for through a two-step procedure by first deconvolving an estimate of their Fourier-space representation from the raw time-ordered data (TOD), and then averaging the deconvolved TOD into pixelized maps. However, for many experiments, including the Planck High Frequency Instrument (HFI), it is necessary to apply an additional low-pass filter to avoid an excessive noise boost, which leads to an asymmetric effective beam. In this paper we demonstrate that this effect can be avoided if the transfer function deconvolution and pixelization operations are performed simultaneously through integrated maximum likelihood mapmaking. The resulting algorithm is structurally identical to the artDeco algorithm introduced by Keih\\\"anen & Reinecke (2012) for beam deconvolution. We illustrate the relevance of this method with simulated Planck HFI 143 GHz data, and find that the resulting effective beam is both more symmetric than with the two-step procedure, resulting in a sky-averaged ellipticity that is 64 % lower, and an effective beam full-width-at-half-maximum (FWHM) that is 2.3 % smaller. Similar improvements are expected for any other bolometer-based CMB experiments with long time constants.","sentences":["We revisit the impact of finite time responses of bolometric detectors used for deep observations of the cosmic microwave background (CMB).","Until now, bolometer transfer functions have been accounted for through a two-step procedure by first deconvolving an estimate of their Fourier-space representation from the raw time-ordered data (TOD), and then averaging the deconvolved TOD into pixelized maps.","However, for many experiments, including the Planck High Frequency Instrument (HFI), it is necessary to apply an additional low-pass filter to avoid an excessive noise boost, which leads to an asymmetric effective beam.","In this paper we demonstrate that this effect can be avoided if the transfer function deconvolution and pixelization operations are performed simultaneously through integrated maximum likelihood mapmaking.","The resulting algorithm is structurally identical to the artDeco algorithm introduced by Keih\\\"anen & Reinecke (2012) for beam deconvolution.","We illustrate the relevance of this method with simulated Planck HFI 143 GHz data, and find that the resulting effective beam is both more symmetric than with the two-step procedure, resulting in a sky-averaged ellipticity that is 64 % lower, and an effective beam full-width-at-half-maximum (FWHM) that is 2.3 % smaller.","Similar improvements are expected for any other bolometer-based CMB experiments with long time constants."],"url":"http://arxiv.org/abs/2405.07729v1","category":"astro-ph.CO"}
{"created":"2024-05-13 13:14:01","title":"Symmetric Clifford twirling for cost-optimal quantum error mitigation in early FTQC regime","abstract":"Twirling noise affecting quantum gates is essential in understanding and controlling errors, but applicable operations to noise are usually restricted by symmetries inherent in quantum gates. In this Letter, we propose symmetric Clifford twirling, a Clifford twirling utilizing only symmetric Clifford operators that commute with certain Pauli subgroups. We fully characterize how each Pauli noise is converted through the twirling and show that certain Pauli noise can be scrambled to a noise exponentially close to the global white noise. We further demonstrate that the effective noise of some highly structured circuits, such as Trotterized Hamiltonian simulation circuits, is scrambled to global white noise, and even a single use of CNOT gate can significantly accelerate the scrambling. These findings enable us to mitigate errors in non-Clifford operations with minimal sampling overhead in the early stages of fault-tolerant quantum computing, where executing non-Clifford operations is expected to be significantly more challenging than Clifford operations. Furthermore, they offer new insights into various fields of physics where randomness and symmetry play crucial roles.","sentences":["Twirling noise affecting quantum gates is essential in understanding and controlling errors, but applicable operations to noise are usually restricted by symmetries inherent in quantum gates.","In this Letter, we propose symmetric Clifford twirling, a Clifford twirling utilizing only symmetric Clifford operators that commute with certain Pauli subgroups.","We fully characterize how each Pauli noise is converted through the twirling and show that certain Pauli noise can be scrambled to a noise exponentially close to the global white noise.","We further demonstrate that the effective noise of some highly structured circuits, such as Trotterized Hamiltonian simulation circuits, is scrambled to global white noise, and even a single use of CNOT gate can significantly accelerate the scrambling.","These findings enable us to mitigate errors in non-Clifford operations with minimal sampling overhead in the early stages of fault-tolerant quantum computing, where executing non-Clifford operations is expected to be significantly more challenging than Clifford operations.","Furthermore, they offer new insights into various fields of physics where randomness and symmetry play crucial roles."],"url":"http://arxiv.org/abs/2405.07720v1","category":"quant-ph"}
{"created":"2024-05-13 10:47:31","title":"Piecewise omnigenous stellarators","abstract":"In omnigeneous magnetic fields, charged particles are perfectly confined in the absence of collisions and turbulence. For this reason, the magnetic configuration is optimized to be close to omnigenity in any candidate for a stellarator fusion reactor. However, approaching omnigenity imposes severe constraints on the spatial variation of the magnetic field. In particular, the topology of the contours of constant magnetic-field-strength on each magnetic surface must be such that there are no particles transitioning between different types of wells. This, in turn, usually leads to complicated plasma shapes and coils. This Letter presents a new family of optimized fields that display tokamak-like collisional energy transport while having transitioning particles. This result radically broadens the space of accessible reactor-relevant configurations.","sentences":["In omnigeneous magnetic fields, charged particles are perfectly confined in the absence of collisions and turbulence.","For this reason, the magnetic configuration is optimized to be close to omnigenity in any candidate for a stellarator fusion reactor.","However, approaching omnigenity imposes severe constraints on the spatial variation of the magnetic field.","In particular, the topology of the contours of constant magnetic-field-strength on each magnetic surface must be such that there are no particles transitioning between different types of wells.","This, in turn, usually leads to complicated plasma shapes and coils.","This Letter presents a new family of optimized fields that display tokamak-like collisional energy transport while having transitioning particles.","This result radically broadens the space of accessible reactor-relevant configurations."],"url":"http://arxiv.org/abs/2405.07634v1","category":"physics.plasm-ph"}
{"created":"2024-05-13 10:38:05","title":"Substitutability, equilibrium transport, and matching models","abstract":"This chapter explores the role of substitutability in economic models, particularly in the context of optimal transport and matching models. In equilibrium models with substitutability, market-clearing prices can often be recovered using coordinate update methods such as Jacobi's algorithm. We provide a detailed mathematical analysis of models with substitutability through the lens of Z- and M-functions, in particular regarding their role in ensuring the convergence of Jacobi's algorithm. The chapter proceeds by studying matching models using substitutability, first focusing on models with (imperfectly) transferable utility, and then on models with non-transferable utility. In both cases, the text reviews theoretical implications as well as computational approaches (Sinkhorn, Gale--Shapley), and highlights a practical economic application.","sentences":["This chapter explores the role of substitutability in economic models, particularly in the context of optimal transport and matching models.","In equilibrium models with substitutability, market-clearing prices can often be recovered using coordinate update methods such as Jacobi's algorithm.","We provide a detailed mathematical analysis of models with substitutability through the lens of Z- and M-functions, in particular regarding their role in ensuring the convergence of Jacobi's algorithm.","The chapter proceeds by studying matching models using substitutability, first focusing on models with (imperfectly) transferable utility, and then on models with non-transferable utility.","In both cases, the text reviews theoretical implications as well as computational approaches (Sinkhorn, Gale--Shapley), and highlights a practical economic application."],"url":"http://arxiv.org/abs/2405.07628v1","category":"econ.TH"}
{"created":"2024-05-13 10:35:23","title":"Towards Robust Benchmarking of Quantum Optimization Algorithms","abstract":"Benchmarking the performance of quantum optimization algorithms is crucial for identifying utility for industry-relevant use cases. Benchmarking processes vary between optimization applications and depend on user-specified goals. The heuristic nature of quantum algorithms poses challenges, especially when comparing to classical counterparts. A key problem in existing benchmarking frameworks is the lack of equal effort in optimizing for the best quantum and, respectively, classical approaches. This paper presents a comprehensive set of guidelines comprising universal steps towards fair benchmarks. We discuss (1) application-specific algorithm choice, ensuring every solver is provided with the most fitting mathematical formulation of a problem; (2) the selection of benchmark data, including hard instances and real-world samples; (3) the choice of a suitable holistic figure of merit, like time-to-solution or solution quality within time constraints; and (4) equitable hyperparameter training to eliminate bias towards a particular method. The proposed guidelines are tested across three benchmarking scenarios, utilizing the Max-Cut (MC) and Travelling Salesperson Problem (TSP). The benchmarks employ classical mathematical algorithms, such as Branch-and-Cut (BNC) solvers, classical heuristics, Quantum Annealing (QA), and the Quantum Approximate Optimization Algorithm (QAOA).","sentences":["Benchmarking the performance of quantum optimization algorithms is crucial for identifying utility for industry-relevant use cases.","Benchmarking processes vary between optimization applications and depend on user-specified goals.","The heuristic nature of quantum algorithms poses challenges, especially when comparing to classical counterparts.","A key problem in existing benchmarking frameworks is the lack of equal effort in optimizing for the best quantum and, respectively, classical approaches.","This paper presents a comprehensive set of guidelines comprising universal steps towards fair benchmarks.","We discuss (1) application-specific algorithm choice, ensuring every solver is provided with the most fitting mathematical formulation of a problem; (2) the selection of benchmark data, including hard instances and real-world samples; (3) the choice of a suitable holistic figure of merit, like time-to-solution or solution quality within time constraints; and (4) equitable hyperparameter training to eliminate bias towards a particular method.","The proposed guidelines are tested across three benchmarking scenarios, utilizing the Max-Cut (MC) and Travelling Salesperson Problem (TSP).","The benchmarks employ classical mathematical algorithms, such as Branch-and-Cut (BNC) solvers, classical heuristics, Quantum Annealing (QA), and the Quantum Approximate Optimization Algorithm (QAOA)."],"url":"http://arxiv.org/abs/2405.07624v1","category":"quant-ph"}
{"created":"2024-05-13 10:19:25","title":"FNCC: Fast Notification Congestion Control in Data Center Networks","abstract":"Congestion control plays a pivotal role in large-scale data centers, facilitating ultra-low latency, high bandwidth, and optimal utilization. Even with the deployment of data center congestion control mechanisms such as DCQCN and HPCC, these algorithms often respond to congestion sluggishly. This sluggishness is primarily due to the slow notification of congestion. It takes almost one round-trip time (RTT) for the congestion information to reach the sender. In this paper, we introduce the Fast Notification Congestion Control (FNCC) mechanism, which achieves sub-RTT notification. FNCC leverages the acknowledgment packet (ACK) from the return path to carry in-network telemetry (INT) information of the request path, offering the sender more timely and accurate INT. To further accelerate the responsiveness of last-hop congestion control, we propose that the receiver notifies the sender of the number of concurrent congested flows, which can be used to adjust the congested flows to a fair rate quickly. Our experimental results demonstrate that FNCC reduces flow completion time by 27.4% and 88.9% compared to HPCC and DCQCN, respectively. Moreover, FNCC triggers minimal pause frames and maintains high utilization even at 400Gbps.","sentences":["Congestion control plays a pivotal role in large-scale data centers, facilitating ultra-low latency, high bandwidth, and optimal utilization.","Even with the deployment of data center congestion control mechanisms such as DCQCN and HPCC, these algorithms often respond to congestion sluggishly.","This sluggishness is primarily due to the slow notification of congestion.","It takes almost one round-trip time (RTT) for the congestion information to reach the sender.","In this paper, we introduce the Fast Notification Congestion Control (FNCC) mechanism, which achieves sub-RTT notification.","FNCC leverages the acknowledgment packet (ACK) from the return path to carry in-network telemetry (INT) information of the request path, offering the sender more timely and accurate INT.","To further accelerate the responsiveness of last-hop congestion control, we propose that the receiver notifies the sender of the number of concurrent congested flows, which can be used to adjust the congested flows to a fair rate quickly.","Our experimental results demonstrate that FNCC reduces flow completion time by 27.4% and 88.9% compared to HPCC and DCQCN, respectively.","Moreover, FNCC triggers minimal pause frames and maintains high utilization even at 400Gbps."],"url":"http://arxiv.org/abs/2405.07608v1","category":"cs.NI"}
{"created":"2024-05-13 08:49:27","title":"Preliminary Design of Detector Assembly for DIXE","abstract":"Diffuse X-ray Explorer (DIXE) is a proposed X-ray spectroscopic survey experiment for the China Space Station. Its detector assembly (DA) contains the transition edge sensor (TES) microcalorimeter and readout electronics based on the superconducting quantum interference device (SQUID) on the cold stage. The cold stage is thermally connected to the ADR stage, and a Kevlar suspension is used to stabilize and isolate it from the 4 K environment. TES and SQUID are both sensitive to the magnetic field, so a hybrid shielding structure consisting of an outer Cryoperm shield and an inner niobium shield is used to attenuate the magnetic field. In addition, IR/optical/UV photons can produce shot noise and thus degrade the energy resolution of the TES microcalorimeter. A blocking filter assembly is designed to minimize the effects. In it, five filters are mounted at different temperature stages, reducing the probability of IR/optical/UV photons reaching the detector through multiple reflections between filters and absorption. This paper will describe the preliminary design of the detector assembly and its optimization.","sentences":["Diffuse X-ray Explorer (DIXE) is a proposed X-ray spectroscopic survey experiment for the China Space Station.","Its detector assembly (DA) contains the transition edge sensor (TES) microcalorimeter and readout electronics based on the superconducting quantum interference device (SQUID) on the cold stage.","The cold stage is thermally connected to the ADR stage, and a Kevlar suspension is used to stabilize and isolate it from the 4 K environment.","TES and SQUID are both sensitive to the magnetic field, so a hybrid shielding structure consisting of an outer Cryoperm shield and an inner niobium shield is used to attenuate the magnetic field.","In addition, IR/optical/UV photons can produce shot noise and thus degrade the energy resolution of the TES microcalorimeter.","A blocking filter assembly is designed to minimize the effects.","In it, five filters are mounted at different temperature stages, reducing the probability of IR/optical/UV photons reaching the detector through multiple reflections between filters and absorption.","This paper will describe the preliminary design of the detector assembly and its optimization."],"url":"http://arxiv.org/abs/2405.07559v1","category":"astro-ph.IM"}
{"created":"2024-05-13 02:49:14","title":"A Fast Radio Burst monitor with a Compact All-Sky Phased Array (CASPA)","abstract":"Fast Radio Bursts (FRBs) are short-duration radio transients that occur at random times in host galaxies distributed all over the sky. Large field of view instruments can play a critical role in the blind search for rare FRBs. We present a concept for an all-sky FRB monitor using a compact all-sky phased array (CASPA), which can efficiently achieve an extremely large field of view of $\\sim10^4$ square degrees. Such a system would allow us to conduct a continuous, blind FRB search covering the entire southern sky. Using the measured FRB luminosity function, we investigate the detection rate for this all-sky phased array and compare the result to a number of other proposed large field-of-view instruments. We predict a rate of a few FRB detections per week and determine the dispersion measure and redshift distributions of these detectable FRBs. This instrument is optimal for detecting FRBs in the nearby Universe and for extending the high-end of the FRB luminosity function through finding ultraluminous events. Additionally, this instrument can be used to shadow the new gravitational-wave observing runs, detect high energy events triggered from Galactic magnetars and search for other bright, but currently unknown transient signals.","sentences":["Fast Radio Bursts (FRBs) are short-duration radio transients that occur at random times in host galaxies distributed all over the sky.","Large field of view instruments can play a critical role in the blind search for rare FRBs.","We present a concept for an all-sky FRB monitor using a compact all-sky phased array (CASPA), which can efficiently achieve an extremely large field of view of $\\sim10^4$ square degrees.","Such a system would allow us to conduct a continuous, blind FRB search covering the entire southern sky.","Using the measured FRB luminosity function, we investigate the detection rate for this all-sky phased array and compare the result to a number of other proposed large field-of-view instruments.","We predict a rate of a few FRB detections per week and determine the dispersion measure and redshift distributions of these detectable FRBs.","This instrument is optimal for detecting FRBs in the nearby Universe and for extending the high-end of the FRB luminosity function through finding ultraluminous events.","Additionally, this instrument can be used to shadow the new gravitational-wave observing runs, detect high energy events triggered from Galactic magnetars and search for other bright, but currently unknown transient signals."],"url":"http://arxiv.org/abs/2405.07439v1","category":"astro-ph.IM"}
{"created":"2024-05-13 02:27:27","title":"Concurrent aggregate queries","abstract":"Concurrent data structures serve as fundamental building blocks for concurrent computing. Many concurrent counterparts have been designed for basic sequential mechanisms; however, one notable omission is a concurrent tree that supports aggregate queries. Aggregate queries essentially compile succinct information about a range of data items, for example, calculating the average salary of employees in their 30s. Such queries play an essential role in various applications and are commonly taught in undergraduate data structures courses. In this paper, we formalize a type of aggregate queries that can be efficiently supported by concurrent trees and present a design for implementing these queries on concurrent trees. We bring two algorithms implementing this design, where one optimizes for tree update time, while the other optimizes for aggregate query time. We analyze their correctness and complexity, demonstrating the trade-offs between query time and update time.","sentences":["Concurrent data structures serve as fundamental building blocks for concurrent computing.","Many concurrent counterparts have been designed for basic sequential mechanisms; however, one notable omission is a concurrent tree that supports aggregate queries.","Aggregate queries essentially compile succinct information about a range of data items, for example, calculating the average salary of employees in their 30s.","Such queries play an essential role in various applications and are commonly taught in undergraduate data structures courses.","In this paper, we formalize a type of aggregate queries that can be efficiently supported by concurrent trees and present a design for implementing these queries on concurrent trees.","We bring two algorithms implementing this design, where one optimizes for tree update time, while the other optimizes for aggregate query time.","We analyze their correctness and complexity, demonstrating the trade-offs between query time and update time."],"url":"http://arxiv.org/abs/2405.07434v1","category":"cs.DC"}
{"created":"2024-05-13 01:21:02","title":"Unraveling Anisotropic Hybridizations of Solid-state Electrolyte Nano-films in Li-ion Batteries","abstract":"Li2WO4 (LWO) is recognized for its potential as a solid-state electrolyte and it has demonstrated the ability to enhance the electrochemical performance of LiCoO2 (LCO) cathodes in Li-ion batteries. However, prior investigations into LWO have predominantly involved polycrystalline structures, thereby lacking a comprehensive understanding of its behavior when interfaced with single crystal systems, particularly those intricately connected to LCO. In this study, we employ pulsed laser deposition (PLD) to epitaxially synthesize LWO nano-films on LCO layers with different orientations. Based on a series of high-resolution synchrotron-based techniques including X-ray absorption spectroscopy (XAS) and X-ray photoemission spectroscopy (XPS), the electronic structure of LWO is carefully scrutinized where a higher main energy level of W5d(eg)-O2p orbitals hybridization in LWO/LCO(104) as compared to LWO/LCO(003) has been observed. This experimental finding is further validated by a comprehensive set of density of states calculations. Furthermore, detailed polarized XAS characterization unveils distinct anisotropy between the two oriented LWO configurations. This comprehensive scientific investigation, harnessing the capabilities of synchrotron-based techniques, provides invaluable insights for future studies, offering guidance for the optimized utilization of LWO as a solid-state electrolyte or modification layer for LCO cathodes in high-powered Li-ion batteries.","sentences":["Li2WO4 (LWO) is recognized for its potential as a solid-state electrolyte and it has demonstrated the ability to enhance the electrochemical performance of LiCoO2 (LCO) cathodes in Li-ion batteries.","However, prior investigations into LWO have predominantly involved polycrystalline structures, thereby lacking a comprehensive understanding of its behavior when interfaced with single crystal systems, particularly those intricately connected to LCO.","In this study, we employ pulsed laser deposition (PLD) to epitaxially synthesize LWO nano-films on LCO layers with different orientations.","Based on a series of high-resolution synchrotron-based techniques including X-ray absorption spectroscopy (XAS) and X-ray photoemission spectroscopy (XPS), the electronic structure of LWO is carefully scrutinized where a higher main energy level of W5d(eg)-O2p orbitals hybridization in LWO/LCO(104) as compared to LWO/LCO(003) has been observed.","This experimental finding is further validated by a comprehensive set of density of states calculations.","Furthermore, detailed polarized XAS characterization unveils distinct anisotropy between the two oriented LWO configurations.","This comprehensive scientific investigation, harnessing the capabilities of synchrotron-based techniques, provides invaluable insights for future studies, offering guidance for the optimized utilization of LWO as a solid-state electrolyte or modification layer for LCO cathodes in high-powered Li-ion batteries."],"url":"http://arxiv.org/abs/2405.07413v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 01:09:40","title":"Non-unique Hamiltonians for Discrete Symplectic Dynamics","abstract":"An outstanding property of any Hamiltonian system is the symplecticity of its flow, namely, the continuous trajectory preserves volume in phase space. Given a symplectic but discrete trajectory generated by a transition matrix applied at a fixed time-increment ($\\tau > 0$), it was generally believed that there exists a unique Hamiltonian producing a continuous trajectory that coincides at all discrete times ($t = n\\tau$ with $n$ integers) as long as $\\tau$ is small enough. However, it is now exactly demonstrated that, for any given discrete symplectic dynamics of a harmonic oscillator, there exists an infinite number of real-valued Hamiltonians for any small value of $\\tau$ and an infinite number of complex-valued Hamiltonians for any large value of $\\tau$. In addition, when the transition matrix is similar to a Jordan normal form with the supradiagonal element of $1$ and the two identical diagonal elements of either $1$ or $-1$, only one solution to the Hamiltonian is found for the case with the diagonal elements of $1$, but no solution can be found for the other case.","sentences":["An outstanding property of any Hamiltonian system is the symplecticity of its flow, namely, the continuous trajectory preserves volume in phase space.","Given a symplectic but discrete trajectory generated by a transition matrix applied at a fixed time-increment ($\\tau > 0$), it was generally believed that there exists a unique Hamiltonian producing a continuous trajectory that coincides at all discrete times ($t = n\\tau$ with $n$ integers) as long as $\\tau$ is small enough.","However, it is now exactly demonstrated that, for any given discrete symplectic dynamics of a harmonic oscillator, there exists an infinite number of real-valued Hamiltonians for any small value of $\\tau$ and an infinite number of complex-valued Hamiltonians for any large value of $\\tau$. In addition, when the transition matrix is similar to a Jordan normal form with the supradiagonal element of $1$ and the two identical diagonal elements of either $1$ or $-1$, only one solution to the Hamiltonian is found for the case with the diagonal elements of $1$, but no solution can be found for the other case."],"url":"http://arxiv.org/abs/2405.07410v1","category":"math-ph"}
{"created":"2024-05-12 22:27:25","title":"Observed quantum particles system with graphon interaction","abstract":"In this paper, we consider a system of heterogeneously interacting quantum particles subject to indirect continuous measurement. The interaction is assumed to be of the mean-field type. We derive a new limiting quantum graphon system, prove the well-posedness of this system, and establish a stability result.","sentences":["In this paper, we consider a system of heterogeneously interacting quantum particles subject to indirect continuous measurement.","The interaction is assumed to be of the mean-field type.","We derive a new limiting quantum graphon system, prove the well-posedness of this system, and establish a stability result."],"url":"http://arxiv.org/abs/2405.07389v1","category":"math.AP"}
{"created":"2024-05-12 20:01:45","title":"A New Algorithm for Computing $\u03b1$-Capacity","abstract":"The problem of computing $\\alpha$-capacity for $\\alpha>1$ is equivalent to that of computing the correct decoding exponent. Various algorithms for computing them have been proposed, such as Arimoto and Jitsumatsu--Oohama algorithm. In this study, we propose a novel alternating optimization algorithm for computing the $\\alpha$-capacity for $\\alpha>1$ based on a variational characterization of the Augustin--Csisz{\\'a}r mutual information. A comparison of the convergence performance of these algorithms is demonstrated through numerical examples.","sentences":["The problem of computing $\\alpha$-capacity for $\\alpha>1$ is equivalent to that of computing the correct decoding exponent.","Various algorithms for computing them have been proposed, such as Arimoto and Jitsumatsu--Oohama algorithm.","In this study, we propose a novel alternating optimization algorithm for computing the $\\alpha$-capacity for $\\alpha>1$ based on a variational characterization of the Augustin--Csisz{\\'a}r mutual information.","A comparison of the convergence performance of these algorithms is demonstrated through numerical examples."],"url":"http://arxiv.org/abs/2405.07368v1","category":"cs.IT"}
{"created":"2024-05-12 13:07:36","title":"Movable Antennas Aided Multicast MISO Communication Systems","abstract":"A novel multicast communication system with movable antennas (MAs) is proposed, where the antenna position optimization is exploited to enhance the transmission rate. Specifically, an MA-assisted two-user multicast multiple-input single-input system is considered. The joint optimization of the transmit beamforming vector and transmit MA positions is studied by modeling the motion of the MA elements as discrete movements. A low-complexity greedy search-based algorithm is proposed to tackle this non-convex inter-programming problem. A branch-and-bound (BAB)-based method is proposed to achieve the optimal multicast rate with a reduced time complexity than the brute-force search by assuming the two users suffer similar line-of-sight path losses. Numerical results reveal that the proposed MA systems significantly improve the multicast rate compared to conventional fixed-position antennas (FPAs)-based systems.","sentences":["A novel multicast communication system with movable antennas (MAs) is proposed, where the antenna position optimization is exploited to enhance the transmission rate.","Specifically, an MA-assisted two-user multicast multiple-input single-input system is considered.","The joint optimization of the transmit beamforming vector and transmit MA positions is studied by modeling the motion of the MA elements as discrete movements.","A low-complexity greedy search-based algorithm is proposed to tackle this non-convex inter-programming problem.","A branch-and-bound (BAB)-based method is proposed to achieve the optimal multicast rate with a reduced time complexity than the brute-force search by assuming the two users suffer similar line-of-sight path losses.","Numerical results reveal that the proposed MA systems significantly improve the multicast rate compared to conventional fixed-position antennas (FPAs)-based systems."],"url":"http://arxiv.org/abs/2405.07281v1","category":"eess.SP"}
{"created":"2024-05-12 12:34:00","title":"The comparative study of high efficiency of Tm^{3+}-doped fiber laser at 1.72 \u03bcm for different pump schemes","abstract":"In this study, we revealed the impact of the pumping scheme, fiber length, pumping power, and reflectivity of the output fiber Bragg grating on the performance of a Tm^3+ -doped fiber laser (TDFL) operating at a wavelength of 1.72 \\mu m. Using numerical simulations, we optimized the output power and reduced losses due to reabsorption; as well as amplified spontaneous emission (ASE) at approximately 1820 nm. The Tm^3+ -doped fiber was bi-directionally pumped at 1570 nm to enhance the pump absorption. The simulations suggest that a maximum power of 5.96W at 1.72 \\mu m and a slope efficiency of 64 % are achievable using a Tm^{3+}-doped silica fiber with a bi-directional pump of 4 W forward and 6 W backward.","sentences":["In this study, we revealed the impact of the pumping scheme, fiber length, pumping power, and reflectivity of the output fiber Bragg grating on the performance of a Tm^3+ -doped fiber laser (TDFL) operating at a wavelength of 1.72 \\mu m. Using numerical simulations, we optimized the output power and reduced losses due to reabsorption; as well as amplified spontaneous emission (ASE) at approximately 1820 nm.","The Tm^3+ -doped fiber was bi-directionally pumped at 1570 nm to enhance the pump absorption.","The simulations suggest that a maximum power of 5.96W at 1.72 \\mu m and a slope efficiency of 64 % are achievable using a Tm^{3+}-doped silica fiber with a bi-directional pump of 4 W forward and 6 W backward."],"url":"http://arxiv.org/abs/2405.07269v1","category":"physics.optics"}
{"created":"2024-05-12 12:01:59","title":"String stability and guaranteed safety via funnel cruise control for vehicle platoons","abstract":"We study decentralized control strategies for platoons of autonomous vehicles with heterogeneous and nonlinear dynamics. Based on ideas from funnel control, we present a novel decentralized control algorithm which is able to guarantee a safety distance between any two vehicles, a good traffic flow and it achieves string stability of the controlled platoon. We illustrate the performance of the controller by simulations of two extreme scenarios.","sentences":["We study decentralized control strategies for platoons of autonomous vehicles with heterogeneous and nonlinear dynamics.","Based on ideas from funnel control, we present a novel decentralized control algorithm which is able to guarantee a safety distance between any two vehicles, a good traffic flow and it achieves string stability of the controlled platoon.","We illustrate the performance of the controller by simulations of two extreme scenarios."],"url":"http://arxiv.org/abs/2405.07262v1","category":"math.OC"}
{"created":"2024-05-12 10:48:18","title":"ZX Graphical Calculus for Continuous-Variable Quantum Processes","abstract":"Continuous-variable (CV) quantum information processing is a promising candidate for large-scale fault-tolerant quantum computation. However, analysis of CV quantum process relies mostly on direct computation of the evolution of operators in the Heisenberg picture, and the features of CV space has yet to be thoroughly investigated in an intuitive manner. One key ingredient for further exploration of CV quantum computing is the construction of a computational model that brings visual intuition and new tools for analysis. In this paper, we delve into a graphical computational model, inspired by a similar model for qubit-based systems called the ZX~calculus, that enables the representation of arbitrary CV quantum process as a simple directed graph. We demonstrate the utility of our model as a graphical tool to comprehend CV processes intuitively by showing how equivalences between two distinct quantum processes can be proven as a sequence of diagrammatic transformations in certain cases. We also examine possible applications of our model, such as measurement-based quantum computing, characterization of Gaussian and non-Gaussian processes, and circuit optimization.","sentences":["Continuous-variable (CV) quantum information processing is a promising candidate for large-scale fault-tolerant quantum computation.","However, analysis of CV quantum process relies mostly on direct computation of the evolution of operators in the Heisenberg picture, and the features of CV space has yet to be thoroughly investigated in an intuitive manner.","One key ingredient for further exploration of CV quantum computing is the construction of a computational model that brings visual intuition and new tools for analysis.","In this paper, we delve into a graphical computational model, inspired by a similar model for qubit-based systems called the ZX~calculus, that enables the representation of arbitrary CV quantum process as a simple directed graph.","We demonstrate the utility of our model as a graphical tool to comprehend CV processes intuitively by showing how equivalences between two distinct quantum processes can be proven as a sequence of diagrammatic transformations in certain cases.","We also examine possible applications of our model, such as measurement-based quantum computing, characterization of Gaussian and non-Gaussian processes, and circuit optimization."],"url":"http://arxiv.org/abs/2405.07246v1","category":"quant-ph"}
{"created":"2024-05-12 08:22:41","title":"Equivariant QAOA and the Duel of the Mixers","abstract":"Constructing an optimal mixer for Quantum Approximate Optimization Algorithm (QAOA) Hamiltonian is crucial for enhancing the performance of QAOA in solving combinatorial optimization problems. We present a systematic methodology for constructing the QAOA tailored mixer Hamiltonian, ensuring alignment with the inherent symmetries of classical optimization problem objectives. The key to our approach is to identify an operator that commutes with the action of the group of symmetries on the QAOA underlying Hilbert space and meets the essential technical criteria for effective mixer Hamiltonian functionality. We offer a construction method specifically tailored to the symmetric group $S_d$, prevalent in a variety of combinatorial optimization problems. By rigorously validating the required properties, providing a concrete formula and corresponding quantum circuit for implementation, we establish the viability of the proposed mixer Hamiltonian. Furthermore, we demonstrate that the classical mixer $B$ commutes only with a subgroup of $S_d$ of significantly smaller order than the group itself, enhancing the efficiency of the proposed approach. To evaluate the effectiveness of our methodology, we compare two QAOA variants utilizing different mixer Hamiltonians: conventional $B=\\sum X_i$ and the newly proposed $H_M$ in edge coloring and graph partitioning problems across various graphs. We observe statistically significant differences in mean values, with the new variant consistently demonstrating superior performance across multiple independent simulations. Additionally, we analyze the phenomenon of poor performance in alternative warm-start QAOA variants, providing a conceptual explanation supported by recent literature findings.","sentences":["Constructing an optimal mixer for Quantum Approximate Optimization Algorithm (QAOA) Hamiltonian is crucial for enhancing the performance of QAOA in solving combinatorial optimization problems.","We present a systematic methodology for constructing the QAOA tailored mixer Hamiltonian, ensuring alignment with the inherent symmetries of classical optimization problem objectives.","The key to our approach is to identify an operator that commutes with the action of the group of symmetries on the QAOA underlying Hilbert space and meets the essential technical criteria for effective mixer Hamiltonian functionality.","We offer a construction method specifically tailored to the symmetric group $S_d$, prevalent in a variety of combinatorial optimization problems.","By rigorously validating the required properties, providing a concrete formula and corresponding quantum circuit for implementation, we establish the viability of the proposed mixer Hamiltonian.","Furthermore, we demonstrate that the classical mixer $B$ commutes only with a subgroup of $S_d$ of significantly smaller order than the group itself, enhancing the efficiency of the proposed approach.","To evaluate the effectiveness of our methodology, we compare two QAOA variants utilizing different mixer Hamiltonians: conventional $B=\\sum X_i$ and the newly proposed $H_M$ in edge coloring and graph partitioning problems across various graphs.","We observe statistically significant differences in mean values, with the new variant consistently demonstrating superior performance across multiple independent simulations.","Additionally, we analyze the phenomenon of poor performance in alternative warm-start QAOA variants, providing a conceptual explanation supported by recent literature findings."],"url":"http://arxiv.org/abs/2405.07211v1","category":"quant-ph"}
{"created":"2024-05-12 07:48:23","title":"Qsyn: A Developer-Friendly Quantum Circuit Synthesis Framework for NISQ Era and Beyond","abstract":"In this paper, we introduce a new quantum circuit synthesis (QCS) framework, Qsyn, for developers to research, develop, test, experiment, and then contribute their QCS algorithms and tools to the framework. Our framework is more developer-friendly than other modern QCS frameworks in three aspects: (1) We design a rich command-line interface so that developers can easily design various testing scenarios and flexibly conduct experiments on their algorithms. (2) We offer detailed access to many data representations on different abstract levels of quantum circuits so that developers can optimize their algorithms to the extreme. (3) We define a rigid developing flow and environment so that developers can ensure their development qualities with the best modern software engineering practices. We illustrate the friendliness of our framework with a showcase of developing a T-Count Optimization algorithm and demonstrate our performance superiority with fair comparisons to other modern QCS frameworks.","sentences":["In this paper, we introduce a new quantum circuit synthesis (QCS) framework, Qsyn, for developers to research, develop, test, experiment, and then contribute their QCS algorithms and tools to the framework.","Our framework is more developer-friendly than other modern QCS frameworks in three aspects: (1) We design a rich command-line interface so that developers can easily design various testing scenarios and flexibly conduct experiments on their algorithms.","(2) We offer detailed access to many data representations on different abstract levels of quantum circuits so that developers can optimize their algorithms to the extreme.","(3) We define a rigid developing flow and environment so that developers can ensure their development qualities with the best modern software engineering practices.","We illustrate the friendliness of our framework with a showcase of developing a T-Count Optimization algorithm and demonstrate our performance superiority with fair comparisons to other modern QCS frameworks."],"url":"http://arxiv.org/abs/2405.07197v1","category":"quant-ph"}
{"created":"2024-05-12 07:46:00","title":"Permissioned Blockchain-based Framework for Ranking Synthetic Data Generators","abstract":"Synthetic data generation is increasingly recognized as a crucial solution to address data related challenges such as scarcity, bias, and privacy concerns. As synthetic data proliferates, the need for a robust evaluation framework to select a synthetic data generator becomes more pressing given the variety of options available. In this research study, we investigate two primary questions: 1) How can we select the most suitable synthetic data generator from a set of options for a specific purpose? 2) How can we make the selection process more transparent, accountable, and auditable? To address these questions, we introduce a novel approach in which the proposed ranking algorithm is implemented as a smart contract within a permissioned blockchain framework called Sawtooth. Through comprehensive experiments and comparisons with state-of-the-art baseline ranking solutions, our framework demonstrates its effectiveness in providing nuanced rankings that consider both desirable and undesirable properties. Furthermore, our framework serves as a valuable tool for selecting the optimal synthetic data generators for specific needs while ensuring compliance with data protection principles.","sentences":["Synthetic data generation is increasingly recognized as a crucial solution to address data related challenges such as scarcity, bias, and privacy concerns.","As synthetic data proliferates, the need for a robust evaluation framework to select a synthetic data generator becomes more pressing given the variety of options available.","In this research study, we investigate two primary questions: 1) How can we select the most suitable synthetic data generator from a set of options for a specific purpose?","2) How can we make the selection process more transparent, accountable, and auditable?","To address these questions, we introduce a novel approach in which the proposed ranking algorithm is implemented as a smart contract within a permissioned blockchain framework called Sawtooth.","Through comprehensive experiments and comparisons with state-of-the-art baseline ranking solutions, our framework demonstrates its effectiveness in providing nuanced rankings that consider both desirable and undesirable properties.","Furthermore, our framework serves as a valuable tool for selecting the optimal synthetic data generators for specific needs while ensuring compliance with data protection principles."],"url":"http://arxiv.org/abs/2405.07196v1","category":"cs.DB"}
{"created":"2024-05-12 07:21:37","title":"A hybrid meta-heuristic approach for channel estimation in OFDM MIMO","abstract":"In wireless communication Multiple Input Multiple Output (MIMO) technology has brought significant improvement in service by adopting Orthogonal Frequency Division Multiplexing (OFDM), a digital modulation technique. To achieve great performance with MIMO efficiently gathering channel state information (CSI) plays a vital role. Among different approach of channel estimation techniques data-aided channel estimation is more reliable. The existing methods of data-aided channel estimation are Least Square (LS) and Minimum Mean Square Error (MMSE) methods which do not achieve a great performance. Moreover, MMSE is little complex and has higher computational cost. That is why many attempts have been done previously to optimize the methods with help of meta heuristics and also other ways. In this paper we have tried to optimize LS estimation with a combined algorithm of Genetic Algorithm (GA) and Particle Swarm Optimization (PSO). The proposed algorithm has outperformed LS and MMSE. And it gives similar result if we optimize LS with standard PSO but in less numbers of iteration.","sentences":["In wireless communication Multiple Input Multiple Output (MIMO) technology has brought significant improvement in service by adopting Orthogonal Frequency Division Multiplexing (OFDM), a digital modulation technique.","To achieve great performance with MIMO efficiently gathering channel state information (CSI) plays a vital role.","Among different approach of channel estimation techniques data-aided channel estimation is more reliable.","The existing methods of data-aided channel estimation are Least Square (LS) and Minimum Mean Square Error (MMSE) methods which do not achieve a great performance.","Moreover, MMSE is little complex and has higher computational cost.","That is why many attempts have been done previously to optimize the methods with help of meta heuristics and also other ways.","In this paper we have tried to optimize LS estimation with a combined algorithm of Genetic Algorithm (GA) and Particle Swarm Optimization (PSO).","The proposed algorithm has outperformed LS and MMSE.","And it gives similar result if we optimize LS with standard PSO but in less numbers of iteration."],"url":"http://arxiv.org/abs/2405.07189v1","category":"cs.NI"}
{"created":"2024-05-12 06:57:43","title":"A Study of Quantitative Correlations Between Crucial Bio-markers and the Optimal Drug Regimen of Type-I Lepra Reaction","abstract":"Leprosy (Hansen's) is a disease caused by Mycobacterium leprae. This disease slowly leads to occurrence of leprae reactions which mainly damage peripheral nervous system which cause loss of organs. We can prevent occurring leprae reactions by monitoring the bio-markers involved in it. Motivated by these observations in this research work we do a exhaustive study dealing with the quantitative correlations between crucial bio-markers and the Multi Drug Thearphy (MDT) used in treating the type I lepra reaction. We frame and study a complex 11 compartment model dealing with the the concentrations of plasma $c_1(t)$ and effective drug action $c_2(t)$, susceptible schwann cells $S(t)$, infected schwann cells $I(t)$, bacterial load $B(t)$, and five cytokines pivotal in Type-1 Lepra reaction: IFN-$\\gamma$, TNF-$\\alpha$, IL-$10$, IL-$12$, IL-$15$, and IL-$17$. We explore exhaustively and establish the quantitative correlations with respect to the optimal drug dosage of the MDT drugs such as rifampin, clofazimine \\& dapsone and the crucial bio-markers involved in type I lepra reaction. We conclude this work by reitrating the fact that the optimal drug dosage of the MDT drugs found through these optimal control studies and the dosage prescribed as per WHO guidelines are almost the same.","sentences":["Leprosy (Hansen's) is a disease caused by Mycobacterium leprae.","This disease slowly leads to occurrence of leprae reactions which mainly damage peripheral nervous system which cause loss of organs.","We can prevent occurring leprae reactions by monitoring the bio-markers involved in it.","Motivated by these observations in this research work we do a exhaustive study dealing with the quantitative correlations between crucial bio-markers and the Multi Drug Thearphy (MDT) used in treating the type I lepra reaction.","We frame and study a complex 11 compartment model dealing with the the concentrations of plasma $c_1(t)$ and effective drug action $c_2(t)$, susceptible schwann cells $S(t)$, infected schwann cells $I(t)$, bacterial load $B(t)$, and five cytokines pivotal in Type-1 Lepra reaction: IFN-$\\gamma$, TNF-$\\alpha$, IL-$10$, IL-$12$, IL-$15$, and IL-$17$. We explore exhaustively and establish the quantitative correlations with respect to the optimal drug dosage of the MDT drugs such as rifampin, clofazimine \\& dapsone and the crucial bio-markers involved in type I lepra reaction.","We conclude this work by reitrating the fact that the optimal drug dosage of the MDT drugs found through these optimal control studies and the dosage prescribed as per WHO guidelines are almost the same."],"url":"http://arxiv.org/abs/2405.07183v1","category":"math.DS"}
{"created":"2024-05-12 06:48:24","title":"Repairing Reed-Solomon Codes with Side Information","abstract":"We generalize the problem of recovering a lost/erased symbol in a Reed-Solomon code to the scenario in which some side information about the lost symbol is known. The side information is represented as a set $S$ of linearly independent combinations of the sub-symbols of the lost symbol. When $S = \\varnothing$, this reduces to the standard problem of repairing a single codeword symbol. When $S$ is a set of sub-symbols of the erased one, this becomes the repair problem with partially lost/erased symbol. We first establish that the minimum repair bandwidth depends on $|S|$ and not the content of $S$ and construct a lower bound on the repair bandwidth of a linear repair scheme with side information $S$. We then consider the well-known subspace-polynomial repair schemes and show that their repair bandwidths can be optimized by choosing the right subspaces. Finally, we demonstrate several parameter regimes where the optimal bandwidths can be achieved for full-length Reed-Solomon codes.","sentences":["We generalize the problem of recovering a lost/erased symbol in a Reed-Solomon code to the scenario in which some side information about the lost symbol is known.","The side information is represented as a set $S$ of linearly independent combinations of the sub-symbols of the lost symbol.","When $S = \\varnothing$, this reduces to the standard problem of repairing a single codeword symbol.","When $S$ is a set of sub-symbols of the erased one, this becomes the repair problem with partially lost/erased symbol.","We first establish that the minimum repair bandwidth depends on $|S|$ and not the content of $S$ and construct a lower bound on the repair bandwidth of a linear repair scheme with side information $S$. We then consider the well-known subspace-polynomial repair schemes and show that their repair bandwidths can be optimized by choosing the right subspaces.","Finally, we demonstrate several parameter regimes where the optimal bandwidths can be achieved for full-length Reed-Solomon codes."],"url":"http://arxiv.org/abs/2405.07180v1","category":"cs.IT"}
{"created":"2024-05-12 06:40:31","title":"Particle transport in open polygonal billiards: a scattering map","abstract":"Polygonal billiards exhibit a rich and complex dynamical behavior. In recent years polygonal billiards have attracted great attention due to their application in the understanding of anomalous transport, but also at the fundamental level, due to its connections with diverse fields in mathematics. We explore this complexity and its consequences on the properties of particle transport in infinitely long channels made of the repetitions of an elementary open polygonal cell. Borrowing ideas from the Zemlyakov-Katok construction, we construct an interval exchange transformation classified by the singular directions of the discontinuities of the billiard flow over the translation surface associated to the elementary cell. From this, we derive an exact expression of a scattering map of the cell connecting the outgoing flow of trajectories with the unconstrained incoming flow. The scattering map is defined over a partition of the coordinate space, characterized by different families of trajectories. Furthermore, we obtain an analytical expression for the average speed of propagation of ballistic modes, describing with high accuracy the speed of propagation of ballistic fronts appearing in the tails of the distribution of the particle displacement. The symbolic hierarchy of the trajectories forming these ballistic fronts is also discussed.","sentences":["Polygonal billiards exhibit a rich and complex dynamical behavior.","In recent years polygonal billiards have attracted great attention due to their application in the understanding of anomalous transport, but also at the fundamental level, due to its connections with diverse fields in mathematics.","We explore this complexity and its consequences on the properties of particle transport in infinitely long channels made of the repetitions of an elementary open polygonal cell.","Borrowing ideas from the Zemlyakov-Katok construction, we construct an interval exchange transformation classified by the singular directions of the discontinuities of the billiard flow over the translation surface associated to the elementary cell.","From this, we derive an exact expression of a scattering map of the cell connecting the outgoing flow of trajectories with the unconstrained incoming flow.","The scattering map is defined over a partition of the coordinate space, characterized by different families of trajectories.","Furthermore, we obtain an analytical expression for the average speed of propagation of ballistic modes, describing with high accuracy the speed of propagation of ballistic fronts appearing in the tails of the distribution of the particle displacement.","The symbolic hierarchy of the trajectories forming these ballistic fronts is also discussed."],"url":"http://arxiv.org/abs/2405.07179v1","category":"nlin.CD"}
{"created":"2024-05-12 05:11:23","title":"Modeling Pedestrian Intrinsic Uncertainty for Multimodal Stochastic Trajectory Prediction via Energy Plan Denoising","abstract":"Pedestrian trajectory prediction plays a pivotal role in the realms of autonomous driving and smart cities. Despite extensive prior research employing sequence and generative models, the unpredictable nature of pedestrians, influenced by their social interactions and individual preferences, presents challenges marked by uncertainty and multimodality. In response, we propose the Energy Plan Denoising (EPD) model for stochastic trajectory prediction. EPD initially provides a coarse estimation of the distribution of future trajectories, termed the Plan, utilizing the Langevin Energy Model. Subsequently, it refines this estimation through denoising via the Probabilistic Diffusion Model. By initiating denoising with the Plan, EPD effectively reduces the need for iterative steps, thereby enhancing efficiency. Furthermore, EPD differs from conventional approaches by modeling the distribution of trajectories instead of individual trajectories. This allows for the explicit modeling of pedestrian intrinsic uncertainties and eliminates the need for multiple denoising operations. A single denoising operation produces a distribution from which multiple samples can be drawn, significantly enhancing efficiency. Moreover, EPD's fine-tuning of the Plan contributes to improved model performance. We validate EPD on two publicly available datasets, where it achieves state-of-the-art results. Additionally, ablation experiments underscore the contributions of individual modules, affirming the efficacy of the proposed approach.","sentences":["Pedestrian trajectory prediction plays a pivotal role in the realms of autonomous driving and smart cities.","Despite extensive prior research employing sequence and generative models, the unpredictable nature of pedestrians, influenced by their social interactions and individual preferences, presents challenges marked by uncertainty and multimodality.","In response, we propose the Energy Plan Denoising (EPD) model for stochastic trajectory prediction.","EPD initially provides a coarse estimation of the distribution of future trajectories, termed the Plan, utilizing the Langevin Energy Model.","Subsequently, it refines this estimation through denoising via the Probabilistic Diffusion Model.","By initiating denoising with the Plan, EPD effectively reduces the need for iterative steps, thereby enhancing efficiency.","Furthermore, EPD differs from conventional approaches by modeling the distribution of trajectories instead of individual trajectories.","This allows for the explicit modeling of pedestrian intrinsic uncertainties and eliminates the need for multiple denoising operations.","A single denoising operation produces a distribution from which multiple samples can be drawn, significantly enhancing efficiency.","Moreover, EPD's fine-tuning of the Plan contributes to improved model performance.","We validate EPD on two publicly available datasets, where it achieves state-of-the-art results.","Additionally, ablation experiments underscore the contributions of individual modules, affirming the efficacy of the proposed approach."],"url":"http://arxiv.org/abs/2405.07164v1","category":"cs.CV"}
{"created":"2024-05-12 03:49:07","title":"Group Complete-$\\{s\\}$ Pliable Index Coding","abstract":"This paper introduces a novel class of PICOD($t$) problems referred to as $g$-group complete-$S$ PICOD($t$) problems. It constructs a multi-stage achievability scheme to generate pliable index codes for group complete PICOD problems when $S = \\{s\\}$ is a singleton set. Using the maximum acyclic induced subgraph bound, lower bounds on the broadcast rate are derived for singleton $S$, which establishes the optimality of the achievability scheme for a range of values for $t$ and for any $g$ and $s$. For all other values, it is shown that the achievability scheme is optimal among the restricted class of broadcast codes.","sentences":["This paper introduces a novel class of PICOD($t$) problems referred to as $g$-group complete-$S$ PICOD($t$) problems.","It constructs a multi-stage achievability scheme to generate pliable index codes for group complete PICOD problems when $S = \\{s\\}$ is a singleton set.","Using the maximum acyclic induced subgraph bound, lower bounds on the broadcast rate are derived for singleton $S$, which establishes the optimality of the achievability scheme for a range of values for $t$ and for any $g$ and $s$. For all other values, it is shown that the achievability scheme is optimal among the restricted class of broadcast codes."],"url":"http://arxiv.org/abs/2405.07151v1","category":"cs.IT"}
{"created":"2024-05-12 02:04:28","title":"Bifractality in one-dimensional Wolf-Villain model","abstract":"We introduce a multifractal optimal detrended fluctuation analysis to study the scaling properties of the one-dimensional Wolf-Villain (WV) model for surface growth. This model produces mounded surface morphologies for long time scales (up to $10^9$ monolayers) and its universality class remains controversial. Our results for the multifractal exponent $\\tau(q)$ reveal an effective local roughness exponent consistent with a transient given by the molecular beam epitaxy (MBE) growth regime and Edward-Wilkinson (EW) universality class for negative and positive $q$-values, respectively. Therefore, although the results corroborate that long-wavelength fluctuations belong to the EW class in the hydrodynamic limit, as conjectured in the recent literature, a bifractal signature of the WV model with an MBE regime at short wavelengths was observed.","sentences":["We introduce a multifractal optimal detrended fluctuation analysis to study the scaling properties of the one-dimensional Wolf-Villain (WV) model for surface growth.","This model produces mounded surface morphologies for long time scales (up to $10^9$ monolayers) and its universality class remains controversial.","Our results for the multifractal exponent $\\tau(q)$ reveal an effective local roughness exponent consistent with a transient given by the molecular beam epitaxy (MBE) growth regime and Edward-Wilkinson (EW) universality class for negative and positive $q$-values, respectively.","Therefore, although the results corroborate that long-wavelength fluctuations belong to the EW class in the hydrodynamic limit, as conjectured in the recent literature, a bifractal signature of the WV model with an MBE regime at short wavelengths was observed."],"url":"http://arxiv.org/abs/2405.07133v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-12 01:44:19","title":"Circuit Design of Two-Step Quantum Search Algorithm for Solving Traveling Salesman Problems","abstract":"Quantum search algorithms, such as Grover's algorithm, are expected to efficiently solve constrained combinatorial optimization problems. However, implementing a quantum search algorithm for solving the traveling salesman problem (TSP) on a circuit poses a potential challenge because current quantum search algorithms for TSP assume that an initial state of equal superposition of feasible solution states satisfying the constraint is already prepared a priori. The time complexity of brute-force preparation of the initial state increases exponentially with the factorial growth of feasible solutions, posing a considerable obstacle in designing quantum circuits for large-scale TSP. To overcome this problem, we propose a two-step quantum search algorithm with two distinct operators for preparing the initial state and solving TSP. The algorithm first amplifies an equal superposition state of all feasible solutions of TSP and subsequently amplifies the optimal solution states among these feasible solution states. Our algorithm, encoded in the higher-order unconstrained binary optimization (HOBO) representation, notably reduces the required number of qubits, enabling efficient preparation of the initial state with a unified circuit design and solving TSP with a quadratic speedup in the absence of prior knowledge of feasible solutions.","sentences":["Quantum search algorithms, such as Grover's algorithm, are expected to efficiently solve constrained combinatorial optimization problems.","However, implementing a quantum search algorithm for solving the traveling salesman problem (TSP) on a circuit poses a potential challenge because current quantum search algorithms for TSP assume that an initial state of equal superposition of feasible solution states satisfying the constraint is already prepared a priori.","The time complexity of brute-force preparation of the initial state increases exponentially with the factorial growth of feasible solutions, posing a considerable obstacle in designing quantum circuits for large-scale TSP.","To overcome this problem, we propose a two-step quantum search algorithm with two distinct operators for preparing the initial state and solving TSP.","The algorithm first amplifies an equal superposition state of all feasible solutions of TSP and subsequently amplifies the optimal solution states among these feasible solution states.","Our algorithm, encoded in the higher-order unconstrained binary optimization (HOBO) representation, notably reduces the required number of qubits, enabling efficient preparation of the initial state with a unified circuit design and solving TSP with a quadratic speedup in the absence of prior knowledge of feasible solutions."],"url":"http://arxiv.org/abs/2405.07129v1","category":"quant-ph"}
