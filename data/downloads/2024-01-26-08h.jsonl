{"created":"2024-01-25 18:59:58","title":"Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities","abstract":"We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model via a proposed method named Cross-Modal Re-parameterization, which exploits the auxiliary weights without any inference costs. On the image, point cloud, video, and audio recognition tasks, we observe significant and consistent performance improvements with irrelevant data from other modalities. The code and models are available at https://github.com/AILab-CVC/M2PT.","sentences":["We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets.","We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities.","We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models.","In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities.","As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model via a proposed method named Cross-Modal Re-parameterization, which exploits the auxiliary weights without any inference costs.","On the image, point cloud, video, and audio recognition tasks, we observe significant and consistent performance improvements with irrelevant data from other modalities.","The code and models are available at https://github.com/AILab-CVC/M2PT."],"url":"http://arxiv.org/abs/2401.14405v1","category":"cs.CV"}
{"created":"2024-01-25 18:59:57","title":"Deconstructing Denoising Diffusion Models for Self-Supervised Learning","abstract":"In this study, we examine the representation learning abilities of Denoising Diffusion Models (DDM) that were originally purposed for image generation. Our philosophy is to deconstruct a DDM, gradually transforming it into a classical Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore how various components of modern DDMs influence self-supervised representation learning. We observe that only a very few modern components are critical for learning good representations, while many others are nonessential. Our study ultimately arrives at an approach that is highly simplified and to a large extent resembles a classical DAE. We hope our study will rekindle interest in a family of classical methods within the realm of modern self-supervised learning.","sentences":["In this study, we examine the representation learning abilities of Denoising Diffusion Models (DDM) that were originally purposed for image generation.","Our philosophy is to deconstruct a DDM, gradually transforming it into a classical Denoising Autoencoder (DAE).","This deconstructive procedure allows us to explore how various components of modern DDMs influence self-supervised representation learning.","We observe that only a very few modern components are critical for learning good representations, while many others are nonessential.","Our study ultimately arrives at an approach that is highly simplified and to a large extent resembles a classical DAE.","We hope our study will rekindle interest in a family of classical methods within the realm of modern self-supervised learning."],"url":"http://arxiv.org/abs/2401.14404v1","category":"cs.CV"}
{"created":"2024-01-25 18:59:44","title":"Adaptive Mobile Manipulation for Articulated Objects In the Open World","abstract":"Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution. We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD. In our experiments we utilize 20 articulate objects across 4 buildings in the CMU campus. With less than an hour of online learning for each object, the system is able to increase success rate from 50% of BC pre-training to 95% using online adaptation. Video results at https://open-world-mobilemanip.github.io/","sentences":["Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem.","However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area.","In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments.","The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution.","We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD.","In our experiments we utilize 20 articulate objects across 4 buildings in the CMU campus.","With less than an hour of online learning for each object, the system is able to increase success rate from 50% of BC pre-training to 95% using online adaptation.","Video results at https://open-world-mobilemanip.github.io/"],"url":"http://arxiv.org/abs/2401.14403v1","category":"cs.RO"}
{"created":"2024-01-25 18:59:42","title":"Range-Agnostic Multi-View Depth Estimation With Keyframe Selection","abstract":"Methods for 3D reconstruction from posed frames require prior knowledge about the scene metric range, usually to recover matching cues along the epipolar lines and narrow the search range. However, such prior might not be directly available or estimated inaccurately in real scenarios -- e.g., outdoor 3D reconstruction from video sequences -- therefore heavily hampering performance. In this paper, we focus on multi-view depth estimation without requiring prior knowledge about the metric range of the scene by proposing RAMDepth, an efficient and purely 2D framework that reverses the depth estimation and matching steps order. Moreover, we demonstrate the capability of our framework to provide rich insights about the quality of the views used for prediction. Additional material can be found on our project page https://andreaconti.github.io/projects/range_agnostic_multi_view_depth.","sentences":["Methods for 3D reconstruction from posed frames require prior knowledge about the scene metric range, usually to recover matching cues along the epipolar lines and narrow the search range.","However, such prior might not be directly available or estimated inaccurately in real scenarios -- e.g., outdoor 3D reconstruction from video sequences -- therefore heavily hampering performance.","In this paper, we focus on multi-view depth estimation without requiring prior knowledge about the metric range of the scene by proposing RAMDepth, an efficient and purely 2D framework that reverses the depth estimation and matching steps order.","Moreover, we demonstrate the capability of our framework to provide rich insights about the quality of the views used for prediction.","Additional material can be found on our project page https://andreaconti.github.io/projects/range_agnostic_multi_view_depth."],"url":"http://arxiv.org/abs/2401.14401v1","category":"cs.CV"}
{"created":"2024-01-25 18:59:32","title":"Modular Adaptation of Multilingual Encoders to Written Swiss German Dialect","abstract":"Creating neural text encoders for written Swiss German is challenging due to a dearth of training data combined with dialectal variation. In this paper, we build on several existing multilingual encoders and adapt them to Swiss German using continued pre-training. Evaluation on three diverse downstream tasks shows that simply adding a Swiss German adapter to a modular encoder achieves 97.5% of fully monolithic adaptation performance. We further find that for the task of retrieving Swiss German sentences given Standard German queries, adapting a character-level model is more effective than the other adaptation strategies. We release our code and the models trained for our experiments at https://github.com/ZurichNLP/swiss-german-text-encoders","sentences":["Creating neural text encoders for written Swiss German is challenging due to a dearth of training data combined with dialectal variation.","In this paper, we build on several existing multilingual encoders and adapt them to Swiss German using continued pre-training.","Evaluation on three diverse downstream tasks shows that simply adding a Swiss German adapter to a modular encoder achieves 97.5% of fully monolithic adaptation performance.","We further find that for the task of retrieving Swiss German sentences given Standard German queries, adapting a character-level model is more effective than the other adaptation strategies.","We release our code and the models trained for our experiments at https://github.com/ZurichNLP/swiss-german-text-encoders"],"url":"http://arxiv.org/abs/2401.14400v1","category":"cs.CL"}
{"created":"2024-01-25 18:57:36","title":"pix2gestalt: Amodal Segmentation by Synthesizing Wholes","abstract":"We introduce pix2gestalt, a framework for zero-shot amodal segmentation, which learns to estimate the shape and appearance of whole objects that are only partially visible behind occlusions. By capitalizing on large-scale diffusion models and transferring their representations to this task, we learn a conditional diffusion model for reconstructing whole objects in challenging zero-shot cases, including examples that break natural and physical priors, such as art. As training data, we use a synthetically curated dataset containing occluded objects paired with their whole counterparts. Experiments show that our approach outperforms supervised baselines on established benchmarks. Our model can furthermore be used to significantly improve the performance of existing object recognition and 3D reconstruction methods in the presence of occlusions.","sentences":["We introduce pix2gestalt, a framework for zero-shot amodal segmentation, which learns to estimate the shape and appearance of whole objects that are only partially visible behind occlusions.","By capitalizing on large-scale diffusion models and transferring their representations to this task, we learn a conditional diffusion model for reconstructing whole objects in challenging zero-shot cases, including examples that break natural and physical priors, such as art.","As training data, we use a synthetically curated dataset containing occluded objects paired with their whole counterparts.","Experiments show that our approach outperforms supervised baselines on established benchmarks.","Our model can furthermore be used to significantly improve the performance of existing object recognition and 3D reconstruction methods in the presence of occlusions."],"url":"http://arxiv.org/abs/2401.14398v1","category":"cs.CV"}
{"created":"2024-01-25 18:52:18","title":"O(1) Insertion for Random Walk d-ary Cuckoo Hashing up to the Load Threshold","abstract":"The random walk $d$-ary cuckoo hashing algorithm was defined by Fotakis, Pagh, Sanders, and Spirakis to generalize and improve upon the standard cuckoo hashing algorithm of Pagh and Rodler. Random walk $d$-ary cuckoo hashing has low space overhead, guaranteed fast access, and fast in practice insertion time. In this paper, we give a theoretical insertion time bound for this algorithm. More precisely, for every $d\\ge 3$ hashes, let $c_d^*$ be the sharp threshold for the load factor at which a valid assignment of $cm$ objects to a hash table of size $m$ likely exists. We show that for any $d\\ge 4$ hashes and load factor $c<c_d^*$, the expectation of the random walk insertion time is $O(1)$, that is, a constant depending only on $d$ and $c$ but not $m$.","sentences":["The random walk $d$-ary cuckoo hashing algorithm was defined by Fotakis, Pagh, Sanders, and Spirakis to generalize and improve upon the standard cuckoo hashing algorithm of Pagh and Rodler.","Random walk $d$-ary cuckoo hashing has low space overhead, guaranteed fast access, and fast in practice insertion time.","In this paper, we give a theoretical insertion time bound for this algorithm.","More precisely, for every $d\\ge 3$ hashes, let $c_d^*$ be the sharp threshold for the load factor at which a valid assignment of $cm$ objects to a hash table of size $m$ likely exists.","We show that for any $d\\ge 4$ hashes and load factor $c<c_d^*$, the expectation of the random walk insertion time is $O(1)$, that is, a constant depending only on $d$ and $c$ but not $m$."],"url":"http://arxiv.org/abs/2401.14394v1","category":"cs.DS"}
{"created":"2024-01-25 18:50:20","title":"Summing up perturbation series around superintegrable point","abstract":"We work out explicit formulas for correlators in the Gaussian matrix model perturbed by a logarithmic potential, i.e. by inserting Miwa variables. In this paper, we concentrate on the example of a single Miwa variable. The ordinary Gaussian model is superintegrable, i.e. the average of the Schur functions $S_Q$ is an explicit function of the Young diagram $Q$. The question is what happens to this property after perturbation. We show that the entire perturbation series can be nicely summed up into a kind of Borel transform of a universal exponential function, while the dependence on $R$ enters through a polynomial factor in front of this exponential. Moreover, these polynomials can be described explicitly through a single additional structure, which we call ``truncation'' of the Young diagram $Q$. It is unclear if one can call this an extended superintegrability, but at least it is a tremendously simple deformation of it. Moreover, the vanishing Gaussian correlators remain vanishing and, hence, are not deformed at all.","sentences":["We work out explicit formulas for correlators in the Gaussian matrix model perturbed by a logarithmic potential, i.e. by inserting Miwa variables.","In this paper, we concentrate on the example of a single Miwa variable.","The ordinary Gaussian model is superintegrable, i.e. the average of the Schur functions $S_Q$ is an explicit function of the Young diagram $Q$. The question is what happens to this property after perturbation.","We show that the entire perturbation series can be nicely summed up into a kind of Borel transform of a universal exponential function, while the dependence on $R$ enters through a polynomial factor in front of this exponential.","Moreover, these polynomials can be described explicitly through a single additional structure, which we call ``truncation'' of the Young diagram $Q$. It is unclear if one can call this an extended superintegrability, but at least it is a tremendously simple deformation of it.","Moreover, the vanishing Gaussian correlators remain vanishing and, hence, are not deformed at all."],"url":"http://arxiv.org/abs/2401.14392v1","category":"hep-th"}
{"created":"2024-01-25 18:49:57","title":"Rethinking Patch Dependence for Masked Autoencoders","abstract":"In this work, we re-examine inter-patch dependencies in the decoding mechanism of masked autoencoders (MAE). We decompose this decoding mechanism for masked patch reconstruction in MAE into self-attention and cross-attention. Our investigations suggest that self-attention between mask patches is not essential for learning good representations. To this end, we propose a novel pretraining framework: Cross-Attention Masked Autoencoders (CrossMAE). CrossMAE's decoder leverages only cross-attention between masked and visible tokens, with no degradation in downstream performance. This design also enables decoding only a small subset of mask tokens, boosting efficiency. Furthermore, each decoder block can now leverage different encoder features, resulting in improved representation learning. CrossMAE matches MAE in performance with 2.5 to 3.7$\\times$ less decoding compute. It also surpasses MAE on ImageNet classification and COCO instance segmentation under the same compute. Code and models: https://crossmae.github.io","sentences":["In this work, we re-examine inter-patch dependencies in the decoding mechanism of masked autoencoders (MAE).","We decompose this decoding mechanism for masked patch reconstruction in MAE into self-attention and cross-attention.","Our investigations suggest that self-attention between mask patches is not essential for learning good representations.","To this end, we propose a novel pretraining framework: Cross-Attention Masked Autoencoders (CrossMAE).","CrossMAE's decoder leverages only cross-attention between masked and visible tokens, with no degradation in downstream performance.","This design also enables decoding only a small subset of mask tokens, boosting efficiency.","Furthermore, each decoder block can now leverage different encoder features, resulting in improved representation learning.","CrossMAE matches MAE in performance with 2.5 to 3.7$\\times$ less decoding compute.","It also surpasses MAE on ImageNet classification and COCO instance segmentation under the same compute.","Code and models: https://crossmae.github.io"],"url":"http://arxiv.org/abs/2401.14391v1","category":"cs.CV"}
{"created":"2024-01-25 18:47:23","title":"Smooth Ranking SVM via Cutting-Plane Method","abstract":"The most popular classification algorithms are designed to maximize classification accuracy during training. However, this strategy may fail in the presence of class imbalance since it is possible to train models with high accuracy by overfitting to the majority class. On the other hand, the Area Under the Curve (AUC) is a widely used metric to compare classification performance of different algorithms when there is a class imbalance, and various approaches focusing on the direct optimization of this metric during training have been proposed. Among them, SVM-based formulations are especially popular as this formulation allows incorporating different regularization strategies easily. In this work, we develop a prototype learning approach that relies on cutting-plane method, similar to Ranking SVM, to maximize AUC. Our algorithm learns simpler models by iteratively introducing cutting planes, thus overfitting is prevented in an unconventional way. Furthermore, it penalizes the changes in the weights at each iteration to avoid large jumps that might be observed in the test performance, thus facilitating a smooth learning process. Based on the experiments conducted on 73 binary classification datasets, our method yields the best test AUC in 25 datasets among its relevant competitors.","sentences":["The most popular classification algorithms are designed to maximize classification accuracy during training.","However, this strategy may fail in the presence of class imbalance since it is possible to train models with high accuracy by overfitting to the majority class.","On the other hand, the Area Under the Curve (AUC) is a widely used metric to compare classification performance of different algorithms when there is a class imbalance, and various approaches focusing on the direct optimization of this metric during training have been proposed.","Among them, SVM-based formulations are especially popular as this formulation allows incorporating different regularization strategies easily.","In this work, we develop a prototype learning approach that relies on cutting-plane method, similar to Ranking SVM, to maximize AUC.","Our algorithm learns simpler models by iteratively introducing cutting planes, thus overfitting is prevented in an unconventional way.","Furthermore, it penalizes the changes in the weights at each iteration to avoid large jumps that might be observed in the test performance, thus facilitating a smooth learning process.","Based on the experiments conducted on 73 binary classification datasets, our method yields the best test AUC in 25 datasets among its relevant competitors."],"url":"http://arxiv.org/abs/2401.14388v1","category":"cs.LG"}
{"created":"2024-01-25 18:46:35","title":"Inconsistency Masks: Removing the Uncertainty from Input-Pseudo-Label Pairs","abstract":"Generating sufficient labeled data is a significant hurdle in the efficient execution of deep learning projects, especially in uncharted territories of image segmentation where labeling demands extensive time, unlike classification tasks. Our study confronts this challenge, operating in an environment constrained by limited hardware resources and the lack of extensive datasets or pre-trained models. We introduce the novel use of Inconsistency Masks (IM) to effectively filter uncertainty in image-pseudo-label pairs, substantially elevating segmentation quality beyond traditional semi-supervised learning techniques. By integrating IM with other methods, we demonstrate remarkable binary segmentation performance on the ISIC 2018 dataset, starting with just 10% labeled data. Notably, three of our hybrid models outperform those trained on the fully labeled dataset. Our approach consistently achieves exceptional results across three additional datasets and shows further improvement when combined with other techniques. For comprehensive and robust evaluation, this paper includes an extensive analysis of prevalent semi-supervised learning strategies, all trained under identical starting conditions. The full code is available at: https://github.com/MichaelVorndran/InconsistencyMasks","sentences":["Generating sufficient labeled data is a significant hurdle in the efficient execution of deep learning projects, especially in uncharted territories of image segmentation where labeling demands extensive time, unlike classification tasks.","Our study confronts this challenge, operating in an environment constrained by limited hardware resources and the lack of extensive datasets or pre-trained models.","We introduce the novel use of Inconsistency Masks (IM) to effectively filter uncertainty in image-pseudo-label pairs, substantially elevating segmentation quality beyond traditional semi-supervised learning techniques.","By integrating IM with other methods, we demonstrate remarkable binary segmentation performance on the ISIC 2018 dataset, starting with just 10% labeled data.","Notably, three of our hybrid models outperform those trained on the fully labeled dataset.","Our approach consistently achieves exceptional results across three additional datasets and shows further improvement when combined with other techniques.","For comprehensive and robust evaluation, this paper includes an extensive analysis of prevalent semi-supervised learning strategies, all trained under identical starting conditions.","The full code is available at: https://github.com/MichaelVorndran/InconsistencyMasks"],"url":"http://arxiv.org/abs/2401.14387v1","category":"cs.CV"}
{"created":"2024-01-25 18:43:24","title":"Entropic Quantum Central Limit Theorem and Quantum Inverse Sumset Theorem","abstract":"We establish an entropic, quantum central limit theorem and quantum inverse sumset theorem in discrete-variable quantum systems describing qudits or qubits. Both results are enabled by using our recently-discovered quantum convolution. We show that the exponential rate of convergence of the entropic central limit theorem is bounded by the magic gap. We also establish an ``quantum, entropic inverse sumset theorem,'' by introducing a quantum doubling constant. Furthermore, we introduce a ``quantum Ruzsa divergence'', and we pose a conjecture called ``convolutional strong subaddivity,'' which leads to the triangle inequality for the quantum Ruzsa divergence. A byproduct of this work is a magic measure to quantify the nonstabilizer nature of a state, based on the quantum Ruzsa divergence.","sentences":["We establish an entropic, quantum central limit theorem and quantum inverse sumset theorem in discrete-variable quantum systems describing qudits or qubits.","Both results are enabled by using our recently-discovered quantum convolution.","We show that the exponential rate of convergence of the entropic central limit theorem is bounded by the magic gap.","We also establish an ``quantum, entropic inverse sumset theorem,'' by introducing a quantum doubling constant.","Furthermore, we introduce a ``quantum Ruzsa divergence'', and we pose a conjecture called ``convolutional strong subaddivity,'' which leads to the triangle inequality for the quantum Ruzsa divergence.","A byproduct of this work is a magic measure to quantify the nonstabilizer nature of a state, based on the quantum Ruzsa divergence."],"url":"http://arxiv.org/abs/2401.14385v1","category":"quant-ph"}
{"created":"2024-01-25 18:40:35","title":"A Sum-of-Squares Hierarchy in the Absence of Pointwise Proofs I: Energy Certificates","abstract":"We devise a parameterized family of distributions, the high-entropy step distributions (HES), which are expressive enough to capture near-optima of spherical spin glass models in the full Replica Symmetry Breaking (fRSB) regime and yet permit low-degree Sum-of-Squares (SoS) certificates that no such distribution can achieve value slightly larger than the true optimum. This yields a SoS optimization program and rounding scheme that attains near-optimal solutions for spherical spin glasses in the fRSB regime. In other regimes, the same results occur at the ALG value, which is a conjectured best-value attainable by any polynomial time algorithm. These SoS programs optimize over families of distributions of possible solutions, and circumvent the oft-cited impossibility of providing a low-degree SoS proof of concentration of measure by instead proving the same bounds only in expectation on solution distributions that can be produced by the chosen rounding algorithm. The new SoS hierarchy does not make any specific reference to the spherical spin glass problem, and we conjecture that it can be applied to a broad range of average-case problems to obtain value that is optimal among polynomial-time algorithms. We give evidence for this with examples of ensembles that provably fool certain local iterative algorithms but for which there is either proof or evidence that the SoS program is better. This opens the door to addressing a question posed by Barak about the possible optimality of SoS on average-case optimization problems, and by Schramm about reductions between different families of algorithms for average-case problems. In this paper, we give low-degree SoS proofs certifying key properties about HES distributions as well as the ALG threshold for spherical spin glasses. The rounding algorithm is introduced and analyzed in a companion paper.","sentences":["We devise a parameterized family of distributions, the high-entropy step distributions (HES), which are expressive enough to capture near-optima of spherical spin glass models in the full Replica Symmetry Breaking (fRSB) regime and yet permit low-degree Sum-of-Squares (SoS) certificates that no such distribution can achieve value slightly larger than the true optimum.","This yields a SoS optimization program and rounding scheme that attains near-optimal solutions for spherical spin glasses in the fRSB regime.","In other regimes, the same results occur at the ALG value, which is a conjectured best-value attainable by any polynomial time algorithm.","These SoS programs optimize over families of distributions of possible solutions, and circumvent the oft-cited impossibility of providing a low-degree SoS proof of concentration of measure by instead proving the same bounds only in expectation on solution distributions that can be produced by the chosen rounding algorithm.","The new SoS hierarchy does not make any specific reference to the spherical spin glass problem, and we conjecture that it can be applied to a broad range of average-case problems to obtain value that is optimal among polynomial-time algorithms.","We give evidence for this with examples of ensembles that provably fool certain local iterative algorithms but for which there is either proof or evidence that the SoS program is better.","This opens the door to addressing a question posed by Barak about the possible optimality of SoS on average-case optimization problems, and by Schramm about reductions between different families of algorithms for average-case problems.","In this paper, we give low-degree SoS proofs certifying key properties about HES distributions as well as the ALG threshold for spherical spin glasses.","The rounding algorithm is introduced and analyzed in a companion paper."],"url":"http://arxiv.org/abs/2401.14383v1","category":"cs.CC"}
{"created":"2024-01-25 18:37:17","title":"An Orthogonal Polynomial Kernel-Based Machine Learning Model for Differential-Algebraic Equations","abstract":"The recent introduction of the Least-Squares Support Vector Regression (LS-SVR) algorithm for solving differential and integral equations has sparked interest. In this study, we expand the application of this algorithm to address systems of differential-algebraic equations (DAEs). Our work presents a novel approach to solving general DAEs in an operator format by establishing connections between the LS-SVR machine learning model, weighted residual methods, and Legendre orthogonal polynomials. To assess the effectiveness of our proposed method, we conduct simulations involving various DAE scenarios, such as nonlinear systems, fractional-order derivatives, integro-differential, and partial DAEs. Finally, we carry out comparisons between our proposed method and currently established state-of-the-art approaches, demonstrating its reliability and effectiveness.","sentences":["The recent introduction of the Least-Squares Support Vector Regression (LS-SVR) algorithm for solving differential and integral equations has sparked interest.","In this study, we expand the application of this algorithm to address systems of differential-algebraic equations (DAEs).","Our work presents a novel approach to solving general DAEs in an operator format by establishing connections between the LS-SVR machine learning model, weighted residual methods, and Legendre orthogonal polynomials.","To assess the effectiveness of our proposed method, we conduct simulations involving various DAE scenarios, such as nonlinear systems, fractional-order derivatives, integro-differential, and partial DAEs.","Finally, we carry out comparisons between our proposed method and currently established state-of-the-art approaches, demonstrating its reliability and effectiveness."],"url":"http://arxiv.org/abs/2401.14382v1","category":"math.NA"}
{"created":"2024-01-25 18:36:10","title":"Manifold GCN: Diffusion-based Convolutional Neural Network for Manifold-valued Graphs","abstract":"We propose two graph neural network layers for graphs with features in a Riemannian manifold. First, based on a manifold-valued graph diffusion equation, we construct a diffusion layer that can be applied to an arbitrary number of nodes and graph connectivity patterns. Second, we model a tangent multilayer perceptron by transferring ideas from the vector neuron framework to our general setting. Both layers are equivariant with respect to node permutations and isometries of the feature manifold. These properties have been shown to lead to a beneficial inductive bias in many deep learning tasks. Numerical examples on synthetic data as well as on triangle meshes of the right hippocampus to classify Alzheimer's disease demonstrate the very good performance of our layers.","sentences":["We propose two graph neural network layers for graphs with features in a Riemannian manifold.","First, based on a manifold-valued graph diffusion equation, we construct a diffusion layer that can be applied to an arbitrary number of nodes and graph connectivity patterns.","Second, we model a tangent multilayer perceptron by transferring ideas from the vector neuron framework to our general setting.","Both layers are equivariant with respect to node permutations and isometries of the feature manifold.","These properties have been shown to lead to a beneficial inductive bias in many deep learning tasks.","Numerical examples on synthetic data as well as on triangle meshes of the right hippocampus to classify Alzheimer's disease demonstrate the very good performance of our layers."],"url":"http://arxiv.org/abs/2401.14381v1","category":"cs.LG"}
{"created":"2024-01-25 18:30:46","title":"UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models","abstract":"In contemporary design practices, the integration of computer vision and generative artificial intelligence (genAI) represents a transformative shift towards more interactive and inclusive processes. These technologies offer new dimensions of image analysis and generation, which are particularly relevant in the context of urban landscape reconstruction. This paper presents a novel workflow encapsulated within a prototype application, designed to leverage the synergies between advanced image segmentation and diffusion models for a comprehensive approach to urban design. Our methodology encompasses the OneFormer model for detailed image segmentation and the Stable Diffusion XL (SDXL) diffusion model, implemented through ControlNet, for generating images from textual descriptions. Validation results indicated a high degree of performance by the prototype application, showcasing significant accuracy in both object detection and text-to-image generation. This was evidenced by superior Intersection over Union (IoU) and CLIP scores across iterative evaluations for various categories of urban landscape features. Preliminary testing included utilising UrbanGenAI as an educational tool enhancing the learning experience in design pedagogy, and as a participatory instrument facilitating community-driven urban planning. Early results suggested that UrbanGenAI not only advances the technical frontiers of urban landscape reconstruction but also provides significant pedagogical and participatory planning benefits. The ongoing development of UrbanGenAI aims to further validate its effectiveness across broader contexts and integrate additional features such as real-time feedback mechanisms and 3D modelling capabilities. Keywords: generative AI; panoptic image segmentation; diffusion models; urban landscape design; design pedagogy; co-design","sentences":["In contemporary design practices, the integration of computer vision and generative artificial intelligence (genAI) represents a transformative shift towards more interactive and inclusive processes.","These technologies offer new dimensions of image analysis and generation, which are particularly relevant in the context of urban landscape reconstruction.","This paper presents a novel workflow encapsulated within a prototype application, designed to leverage the synergies between advanced image segmentation and diffusion models for a comprehensive approach to urban design.","Our methodology encompasses the OneFormer model for detailed image segmentation and the Stable Diffusion XL (SDXL) diffusion model, implemented through ControlNet, for generating images from textual descriptions.","Validation results indicated a high degree of performance by the prototype application, showcasing significant accuracy in both object detection and text-to-image generation.","This was evidenced by superior Intersection over Union (IoU) and CLIP scores across iterative evaluations for various categories of urban landscape features.","Preliminary testing included utilising UrbanGenAI as an educational tool enhancing the learning experience in design pedagogy, and as a participatory instrument facilitating community-driven urban planning.","Early results suggested that UrbanGenAI not only advances the technical frontiers of urban landscape reconstruction but also provides significant pedagogical and participatory planning benefits.","The ongoing development of UrbanGenAI aims to further validate its effectiveness across broader contexts and integrate additional features such as real-time feedback mechanisms and 3D modelling capabilities.","Keywords: generative AI; panoptic image segmentation; diffusion models; urban landscape design; design pedagogy; co-design"],"url":"http://arxiv.org/abs/2401.14379v1","category":"cs.CV"}
{"created":"2024-01-25 18:27:53","title":"Bonding Grammars","abstract":"We introduce bonding grammars, a graph grammar formalism developed to model DNA computation by means of graph transformations. It is a modification of fusion grammars introduced by Kreowski, Kuske and Lye in 2017. Bonding is a graph transformation that consists of merging two hyperedges into a single larger one. We show why bonding better reflects interaction between DNA molecules than fusion. We prove that bonding grammars naturally generalise regular sticker systems. We also study the relation between bonding grammars and hyperedge replacement grammars proving that each of these kinds of grammars generates a language the other one cannot generate. Finally, we prove that the membership problem for bonding grammars is NP-complete and, moreover, that some bonding grammar generates an NP-complete set.","sentences":["We introduce bonding grammars, a graph grammar formalism developed to model DNA computation by means of graph transformations.","It is a modification of fusion grammars introduced by Kreowski, Kuske and Lye in 2017.","Bonding is a graph transformation that consists of merging two hyperedges into a single larger one.","We show why bonding better reflects interaction between DNA molecules than fusion.","We prove that bonding grammars naturally generalise regular sticker systems.","We also study the relation between bonding grammars and hyperedge replacement grammars proving that each of these kinds of grammars generates a language the other one cannot generate.","Finally, we prove that the membership problem for bonding grammars is NP-complete and, moreover, that some bonding grammar generates an NP-complete set."],"url":"http://arxiv.org/abs/2401.14377v1","category":"cs.FL"}
{"created":"2024-01-25 18:26:29","title":"The GraphTempo Framework for Exploring the Evolution of a Graph through Pattern Aggregation","abstract":"When the focus is on the relationships or interactions between entities, graphs offer an intuitive model for many real-world data. Such graphs are usually large and change over time, thus, requiring models and strategies that explore their evolution. We study the evolution of aggregated graphs and introduce the GraphTempo model that allows temporal and attribute aggregation not only on node level by grouping individual nodes, but on a pattern level as well, where subgraphs are grouped together. Furthermore, We propose an efficient strategy for exploring the evolution of the graph based on identifying time intervals of significant growth, shrinkage or stability. Finally, we evaluate the efficiency and effectiveness of the proposed approach using three real graphs.","sentences":["When the focus is on the relationships or interactions between entities, graphs offer an intuitive model for many real-world data.","Such graphs are usually large and change over time, thus, requiring models and strategies that explore their evolution.","We study the evolution of aggregated graphs and introduce the GraphTempo model that allows temporal and attribute aggregation not only on node level by grouping individual nodes, but on a pattern level as well, where subgraphs are grouped together.","Furthermore, We propose an efficient strategy for exploring the evolution of the graph based on identifying time intervals of significant growth, shrinkage or stability.","Finally, we evaluate the efficiency and effectiveness of the proposed approach using three real graphs."],"url":"http://arxiv.org/abs/2401.14375v1","category":"cs.SI"}
{"created":"2024-01-25 18:24:13","title":"TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation","abstract":"The recent advances in natural language processing have predominantly favored well-resourced English-centric models, resulting in a significant gap with low-resource languages. In this work, we introduce the language model TURNA, which is developed for the low-resource language Turkish and is capable of both natural language understanding and generation tasks. TURNA is pretrained with an encoder-decoder architecture based on the unified framework UL2 with a diverse corpus that we specifically curated for this purpose. We evaluated TURNA with three generation tasks and five understanding tasks for Turkish. The results show that TURNA outperforms several multilingual models in both understanding and generation tasks, and competes with monolingual Turkish models in understanding tasks. TURNA is made available at https://huggingface.co/boun-tabi-LMG/TURNA .","sentences":["The recent advances in natural language processing have predominantly favored well-resourced English-centric models, resulting in a significant gap with low-resource languages.","In this work, we introduce the language model TURNA, which is developed for the low-resource language Turkish and is capable of both natural language understanding and generation tasks.","TURNA is pretrained with an encoder-decoder architecture based on the unified framework UL2 with a diverse corpus that we specifically curated for this purpose.","We evaluated TURNA with three generation tasks and five understanding tasks for Turkish.","The results show that TURNA outperforms several multilingual models in both understanding and generation tasks, and competes with monolingual Turkish models in understanding tasks.","TURNA is made available at https://huggingface.co/boun-tabi-LMG/TURNA ."],"url":"http://arxiv.org/abs/2401.14373v1","category":"cs.CL"}
{"created":"2024-01-25 18:20:37","title":"Efficient Optimisation of Physical Reservoir Computers using only a Delayed Input","abstract":"We present an experimental validation of a recently proposed optimization technique for reservoir computing, using an optoelectronic setup. Reservoir computing is a robust framework for signal processing applications, and the development of efficient optimization approaches remains a key challenge. The technique we address leverages solely a delayed version of the input signal to identify the optimal operational region of the reservoir, simplifying the traditionally time-consuming task of hyperparameter tuning. We verify the effectiveness of this approach on different benchmark tasks and reservoir operating conditions.","sentences":["We present an experimental validation of a recently proposed optimization technique for reservoir computing, using an optoelectronic setup.","Reservoir computing is a robust framework for signal processing applications, and the development of efficient optimization approaches remains a key challenge.","The technique we address leverages solely a delayed version of the input signal to identify the optimal operational region of the reservoir, simplifying the traditionally time-consuming task of hyperparameter tuning.","We verify the effectiveness of this approach on different benchmark tasks and reservoir operating conditions."],"url":"http://arxiv.org/abs/2401.14371v1","category":"cs.ET"}
{"created":"2024-01-25 18:16:52","title":"Anomalous localization in spin-chain with tilted interactions","abstract":"The localization properties of a disorder-free spin chain with inhomogeneous interactions are studied. In particular, we consider interaction strength growing linearly along the chain for systems with different interaction ranges. Using exact diagonalization we find the participation ratio of all eigenstates which allows us to quantify the localization volume in the Hilbert space. Surprisingly the localization volume changes nonmonotonically with the interaction range. The model for the infinite interaction range resembles the Schwinger model of lattice gauge theory in staggered formalism. The model studied may be implemented in state-of-the-art cold atomic devices and could reveal hidden features in disorder-free confinement phenomena.","sentences":["The localization properties of a disorder-free spin chain with inhomogeneous interactions are studied.","In particular, we consider interaction strength growing linearly along the chain for systems with different interaction ranges.","Using exact diagonalization we find the participation ratio of all eigenstates which allows us to quantify the localization volume in the Hilbert space.","Surprisingly the localization volume changes nonmonotonically with the interaction range.","The model for the infinite interaction range resembles the Schwinger model of lattice gauge theory in staggered formalism.","The model studied may be implemented in state-of-the-art cold atomic devices and could reveal hidden features in disorder-free confinement phenomena."],"url":"http://arxiv.org/abs/2401.14369v1","category":"cond-mat.dis-nn"}
{"created":"2024-01-25 18:16:40","title":"Spectral Gaps of 2D and 3D Many-body Quantum Systems in the Thermodynamic Limit","abstract":"We present an expression for the spectral gap, opening up new possibilities for performing and accelerating spectral calculations of quantum many-body systems. We develop and demonstrate one such possibility in the context of tensor network simulations. Our approach requires only minor modifications of the widely used Simple Update method and is computationally lightweight relative to other approaches. We validate it by computing spectral gaps of the 2D and 3D transverse-field Ising models and find strong agreement with previously reported perturbation theory results.","sentences":["We present an expression for the spectral gap, opening up new possibilities for performing and accelerating spectral calculations of quantum many-body systems.","We develop and demonstrate one such possibility in the context of tensor network simulations.","Our approach requires only minor modifications of the widely used Simple Update method and is computationally lightweight relative to other approaches.","We validate it by computing spectral gaps of the 2D and 3D transverse-field Ising models and find strong agreement with previously reported perturbation theory results."],"url":"http://arxiv.org/abs/2401.14368v1","category":"quant-ph"}
{"created":"2024-01-25 18:14:57","title":"Genie: Achieving Human Parity in Content-Grounded Datasets Generation","abstract":"The lack of high-quality data for content-grounded generation tasks has been identified as a major obstacle to advancing these tasks. To address this gap, we propose Genie, a novel method for automatically generating high-quality content-grounded data. It consists of three stages: (a) Content Preparation, (b) Generation: creating task-specific examples from the content (e.g., question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure the quality and faithfulness of the generated data. We showcase this methodology by generating three large-scale synthetic data, making wishes, for Long-Form Question-Answering (LFQA), summarization, and information extraction. In a human evaluation, our generated data was found to be natural and of high quality. Furthermore, we compare models trained on our data with models trained on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for Summarization. We show that our models are on par with or outperforming models trained on human-generated data and consistently outperforming them in faithfulness. Finally, we applied our method to create LFQA data within the medical domain and compared a model trained on it with models trained on other domains.","sentences":["The lack of high-quality data for content-grounded generation tasks has been identified as a major obstacle to advancing these tasks.","To address this gap, we propose Genie, a novel method for automatically generating high-quality content-grounded data.","It consists of three stages: (a) Content Preparation, (b) Generation: creating task-specific examples from the content (e.g., question-answer pairs or summaries).","(c) Filtering mechanism aiming to ensure the quality and faithfulness of the generated data.","We showcase this methodology by generating three large-scale synthetic data, making wishes, for Long-Form Question-Answering (LFQA), summarization, and information extraction.","In a human evaluation, our generated data was found to be natural and of high quality.","Furthermore, we compare models trained on our data with models trained on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for Summarization.","We show that our models are on par with or outperforming models trained on human-generated data and consistently outperforming them in faithfulness.","Finally, we applied our method to create LFQA data within the medical domain and compared a model trained on it with models trained on other domains."],"url":"http://arxiv.org/abs/2401.14367v1","category":"cs.CL"}
{"created":"2024-01-25 18:08:53","title":"The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support","abstract":"People experiencing severe distress increasingly use Large Language Model (LLM) chatbots as mental health support tools. Discussions on social media have described how engagements were lifesaving for some, but evidence suggests that general-purpose LLM chatbots also have notable risks that could endanger the welfare of users if not designed responsibly. In this study, we investigate the lived experiences of people who have used LLM chatbots for mental health support. We build on interviews with 21 individuals from globally diverse backgrounds to analyze how users create unique support roles for their chatbots, fill in gaps in everyday care, and navigate associated cultural limitations when seeking support from chatbots. We ground our analysis in psychotherapy literature around effective support, and introduce the concept of therapeutic alignment, or aligning AI with therapeutic values for mental health contexts. Our study offers recommendations for how designers can approach the ethical and effective use of LLM chatbots and other AI mental health support tools in mental health care.","sentences":["People experiencing severe distress increasingly use Large Language Model (LLM) chatbots as mental health support tools.","Discussions on social media have described how engagements were lifesaving for some, but evidence suggests that general-purpose LLM chatbots also have notable risks that could endanger the welfare of users if not designed responsibly.","In this study, we investigate the lived experiences of people who have used LLM chatbots for mental health support.","We build on interviews with 21 individuals from globally diverse backgrounds to analyze how users create unique support roles for their chatbots, fill in gaps in everyday care, and navigate associated cultural limitations when seeking support from chatbots.","We ground our analysis in psychotherapy literature around effective support, and introduce the concept of therapeutic alignment, or aligning AI with therapeutic values for mental health contexts.","Our study offers recommendations for how designers can approach the ethical and effective use of LLM chatbots and other AI mental health support tools in mental health care."],"url":"http://arxiv.org/abs/2401.14362v1","category":"cs.HC"}
{"created":"2024-01-25 18:07:50","title":"MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE Serving","abstract":"This paper presents MoE-Infinity, a cost-efficient mixture-of-expert (MoE) serving system that realizes activation-aware expert offloading. MoE-Infinity features sequence-level expert activation tracing, a new approach adept at identifying sparse activations and capturing the temporal locality of MoE inference. By analyzing these traces, MoE-Infinity performs novel activation-aware expert prefetching and caching, substantially reducing the latency overheads usually associated with offloading experts for improved cost performance. Extensive experiments in a cluster show that MoE-Infinity outperforms numerous existing systems and approaches, reducing latency by 4 - 20X and decreasing deployment costs by over 8X for various MoEs. MoE-Infinity's source code is publicly available at https://github.com/TorchMoE/MoE-Infinity","sentences":["This paper presents MoE-Infinity, a cost-efficient mixture-of-expert (MoE) serving system that realizes activation-aware expert offloading.","MoE-Infinity features sequence-level expert activation tracing, a new approach adept at identifying sparse activations and capturing the temporal locality of MoE inference.","By analyzing these traces, MoE-Infinity performs novel activation-aware expert prefetching and caching, substantially reducing the latency overheads usually associated with offloading experts for improved cost performance.","Extensive experiments in a cluster show that MoE-Infinity outperforms numerous existing systems and approaches, reducing latency by 4 - 20X and decreasing deployment costs by over 8X for various MoEs.","MoE-Infinity's source code is publicly available at https://github.com/TorchMoE/MoE-Infinity"],"url":"http://arxiv.org/abs/2401.14361v1","category":"cs.LG"}
{"created":"2024-01-25 18:06:19","title":"A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis on Noisy Bengali Texts","abstract":"While Bengali is considered a language with limited resources, sentiment analysis has been a subject of extensive research in the literature. Nevertheless, there is a scarcity of exploration into sentiment analysis specifically in the realm of noisy Bengali texts. In this paper, we introduce a dataset (NC-SentNoB) that we annotated manually to identify ten different types of noise found in a pre-existing sentiment analysis dataset comprising of around 15K noisy Bengali texts. At first, given an input noisy text, we identify the noise type, addressing this as a multi-label classification task. Then, we introduce baseline noise reduction methods to alleviate noise prior to conducting sentiment analysis. Finally, we assess the performance of fine-tuned sentiment analysis models with both noisy and noise-reduced texts to make comparisons. The experimental findings indicate that the noise reduction methods utilized are not satisfactory, highlighting the need for more suitable noise reduction methods in future research endeavors. We have made the implementation and dataset presented in this paper publicly available at https://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bengali-Texts","sentences":["While Bengali is considered a language with limited resources, sentiment analysis has been a subject of extensive research in the literature.","Nevertheless, there is a scarcity of exploration into sentiment analysis specifically in the realm of noisy Bengali texts.","In this paper, we introduce a dataset (NC-SentNoB) that we annotated manually to identify ten different types of noise found in a pre-existing sentiment analysis dataset comprising of around 15K noisy Bengali texts.","At first, given an input noisy text, we identify the noise type, addressing this as a multi-label classification task.","Then, we introduce baseline noise reduction methods to alleviate noise prior to conducting sentiment analysis.","Finally, we assess the performance of fine-tuned sentiment analysis models with both noisy and noise-reduced texts to make comparisons.","The experimental findings indicate that the noise reduction methods utilized are not satisfactory, highlighting the need for more suitable noise reduction methods in future research endeavors.","We have made the implementation and dataset presented in this paper publicly available at https://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bengali-Texts"],"url":"http://arxiv.org/abs/2401.14360v1","category":"cs.CL"}
{"created":"2024-01-25 18:01:48","title":"Exact surface energy of the Hubbard model with unparallel boundary magnetic fields","abstract":"In this paper, we study the exact physical quantities in the thermodynamic limit of the one-dimensional Hubbard model with unparallel boundary magnetic fields based on the off-diagonal Bethe ansatz solution. At the half-filling, we obtain the different patterns of Bethe roots of the reduced Bethe ansatz equations for the different boundary parameters. According to them, we obtain the densities of states, ground state energy density and surface energy. Our results show that the system has the stable boundary bound states when the boundary magnetic fields satisfy some constraints.","sentences":["In this paper, we study the exact physical quantities in the thermodynamic limit of the one-dimensional Hubbard model with unparallel boundary magnetic fields based on the off-diagonal Bethe ansatz solution.","At the half-filling, we obtain the different patterns of Bethe roots of the reduced Bethe ansatz equations for the different boundary parameters.","According to them, we obtain the densities of states, ground state energy density and surface energy.","Our results show that the system has the stable boundary bound states when the boundary magnetic fields satisfy some constraints."],"url":"http://arxiv.org/abs/2401.14356v1","category":"math-ph"}
{"created":"2024-01-25 17:58:51","title":"Learning Robust Generalizable Radiance Field with Visibility and Feature Augmented Point Representation","abstract":"This paper introduces a novel paradigm for the generalizable neural radiance field (NeRF). Previous generic NeRF methods combine multiview stereo techniques with image-based neural rendering for generalization, yielding impressive results, while suffering from three issues. First, occlusions often result in inconsistent feature matching. Then, they deliver distortions and artifacts in geometric discontinuities and locally sharp shapes due to their individual process of sampled points and rough feature aggregation. Third, their image-based representations experience severe degradations when source views are not near enough to the target view. To address challenges, we propose the first paradigm that constructs the generalizable neural field based on point-based rather than image-based rendering, which we call the Generalizable neural Point Field (GPF). Our approach explicitly models visibilities by geometric priors and augments them with neural features. We propose a novel nonuniform log sampling strategy to improve both rendering speed and reconstruction quality. Moreover, we present a learnable kernel spatially augmented with features for feature aggregations, mitigating distortions at places with drastically varying geometries. Besides, our representation can be easily manipulated. Experiments show that our model can deliver better geometries, view consistencies, and rendering quality than all counterparts and benchmarks on three datasets in both generalization and finetuning settings, preliminarily proving the potential of the new paradigm for generalizable NeRF.","sentences":["This paper introduces a novel paradigm for the generalizable neural radiance field (NeRF).","Previous generic NeRF methods combine multiview stereo techniques with image-based neural rendering for generalization, yielding impressive results, while suffering from three issues.","First, occlusions often result in inconsistent feature matching.","Then, they deliver distortions and artifacts in geometric discontinuities and locally sharp shapes due to their individual process of sampled points and rough feature aggregation.","Third, their image-based representations experience severe degradations when source views are not near enough to the target view.","To address challenges, we propose the first paradigm that constructs the generalizable neural field based on point-based rather than image-based rendering, which we call the Generalizable neural Point Field (GPF).","Our approach explicitly models visibilities by geometric priors and augments them with neural features.","We propose a novel nonuniform log sampling strategy to improve both rendering speed and reconstruction quality.","Moreover, we present a learnable kernel spatially augmented with features for feature aggregations, mitigating distortions at places with drastically varying geometries.","Besides, our representation can be easily manipulated.","Experiments show that our model can deliver better geometries, view consistencies, and rendering quality than all counterparts and benchmarks on three datasets in both generalization and finetuning settings, preliminarily proving the potential of the new paradigm for generalizable NeRF."],"url":"http://arxiv.org/abs/2401.14354v1","category":"cs.CV"}
{"created":"2024-01-25 17:58:40","title":"Initial data for Minkowski stability with arbitrary decay","abstract":"We construct and parametrize solutions to the constraint equations of general relativity in a neighborhood of Minkowski spacetime with arbitrary prescribed decay properties at infinity. We thus provide a large class of initial data for the results on stability of Minkowski which include a mass term in the asymptotics. Due to the symmetries of Minkowski, a naive linear perturbation fails. Our construction is based on a simplified conformal method, a reduction to transverse traceless perturbations and a nonlinear fixed point argument where we face linear obstructions coming from the cokernels of both the linearized constraint operator and the Laplace operator. To tackle these obstructions, we introduce a well-chosen truncated black hole around which to perturb. The control of the parameters of the truncated black hole is the most technical part of the proof, since its center of mass and angular momentum could be arbitrarily large.","sentences":["We construct and parametrize solutions to the constraint equations of general relativity in a neighborhood of Minkowski spacetime with arbitrary prescribed decay properties at infinity.","We thus provide a large class of initial data for the results on stability of Minkowski which include a mass term in the asymptotics.","Due to the symmetries of Minkowski, a naive linear perturbation fails.","Our construction is based on a simplified conformal method, a reduction to transverse traceless perturbations and a nonlinear fixed point argument where we face linear obstructions coming from the cokernels of both the linearized constraint operator and the Laplace operator.","To tackle these obstructions, we introduce a well-chosen truncated black hole around which to perturb.","The control of the parameters of the truncated black hole is the most technical part of the proof, since its center of mass and angular momentum could be arbitrarily large."],"url":"http://arxiv.org/abs/2401.14353v1","category":"math.AP"}
{"created":"2024-01-25 17:55:28","title":"Skyline-based exploration of temporal property graphs","abstract":"In this paper, we focus on temporal property graphs, that is, property graphs whose labeled nodes and edges as well as the values of the properties associated with them may change with time. For instance, consider a bibliographic network, with nodes representing authors and conferences with properties such as gender and location respectively, and edges representing collaboration between authors and publications in conferences. A key challenge in studying temporal graphs lies in detecting interesting events in their evolution, defined as time intervals of significant stability, growth, or shrinkage. To address this challenge, we build aggregated graphs, where nodes are grouped based on the values of their properties, and seek events at the aggregated level, for example, time intervals of significant growth in the collaborations between authors of the same gender. To locate such events, we propose a novel approach based on unified evolution skylines. A unified evolution skyline assesses the significance of an event in conjunction with the duration of the interval in which the event occurs. Significance is measured by a set of counts, where each count refers to the number of graph elements that remain stable, are created, or deleted, for a specific property value. For example, for property gender, we measure the number of female-female, female-male, and male-male collaborations. Lastly, we share experimental findings that highlight the efficiency and effectiveness of our approach.","sentences":["In this paper, we focus on temporal property graphs, that is, property graphs whose labeled nodes and edges as well as the values of the properties associated with them may change with time.","For instance, consider a bibliographic network, with nodes representing authors and conferences with properties such as gender and location respectively, and edges representing collaboration between authors and publications in conferences.","A key challenge in studying temporal graphs lies in detecting interesting events in their evolution, defined as time intervals of significant stability, growth, or shrinkage.","To address this challenge, we build aggregated graphs, where nodes are grouped based on the values of their properties, and seek events at the aggregated level, for example, time intervals of significant growth in the collaborations between authors of the same gender.","To locate such events, we propose a novel approach based on unified evolution skylines.","A unified evolution skyline assesses the significance of an event in conjunction with the duration of the interval in which the event occurs.","Significance is measured by a set of counts, where each count refers to the number of graph elements that remain stable, are created, or deleted, for a specific property value.","For example, for property gender, we measure the number of female-female, female-male, and male-male collaborations.","Lastly, we share experimental findings that highlight the efficiency and effectiveness of our approach."],"url":"http://arxiv.org/abs/2401.14352v1","category":"cs.SI"}
{"created":"2024-01-25 17:55:07","title":"ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models","abstract":"This paper presents ServerlessLLM, a locality-enhanced serverless inference system for Large Language Models (LLMs). ServerlessLLM exploits the substantial capacity and bandwidth of storage and memory devices available on GPU servers, thereby reducing costly remote checkpoint downloads and achieving efficient checkpoint loading. ServerlessLLM achieves this through three main contributions: (i) fast LLM checkpoint loading via a novel loading-optimized checkpoint format design, coupled with an efficient multi-tier checkpoint loading system; (ii) locality-driven LLM inference with live migration, which allows ServerlessLLM to effectively achieve locality-driven server allocation while preserving the low latency of ongoing LLM inference; and (iii) locality-aware server allocation, enabling ServerlessLLM to evaluate the status of each server in a cluster and effectively schedule model startup time to capitalize on local checkpoint placement. Our comprehensive experiments, which include microbenchmarks and real-world traces, show that ServerlessLLM surpasses state-of-the-art systems by 10 - 200X in latency performance when running various LLM inference workloads.","sentences":["This paper presents ServerlessLLM, a locality-enhanced serverless inference system for Large Language Models (LLMs).","ServerlessLLM exploits the substantial capacity and bandwidth of storage and memory devices available on GPU servers, thereby reducing costly remote checkpoint downloads and achieving efficient checkpoint loading.","ServerlessLLM achieves this through three main contributions: (i) fast LLM checkpoint loading via a novel loading-optimized checkpoint format design, coupled with an efficient multi-tier checkpoint loading system; (ii) locality-driven LLM inference with live migration, which allows ServerlessLLM to effectively achieve locality-driven server allocation while preserving the low latency of ongoing LLM inference; and (iii) locality-aware server allocation, enabling ServerlessLLM to evaluate the status of each server in a cluster and effectively schedule model startup time to capitalize on local checkpoint placement.","Our comprehensive experiments, which include microbenchmarks and real-world traces, show that ServerlessLLM surpasses state-of-the-art systems by 10 - 200X in latency performance when running various LLM inference workloads."],"url":"http://arxiv.org/abs/2401.14351v1","category":"cs.LG"}
{"created":"2024-01-25 17:54:45","title":"5G Network Security Practices: An Overview and Survey","abstract":"This document provides an overview of 5G network security, describing various components of the 5G core network architecture and what kind of security services are offered by these 5G components. It also explores the potential security risks and vulnerabilities presented by the security architecture in 5G and recommends some of the best practices for the 5G network admins to consider while deploying a secure 5G network, based on the surveyed documents from the European government's efforts in commercializing the IoT devices and securing supply chain over 5G networks.","sentences":["This document provides an overview of 5G network security, describing various components of the 5G core network architecture and what kind of security services are offered by these 5G components.","It also explores the potential security risks and vulnerabilities presented by the security architecture in 5G and recommends some of the best practices for the 5G network admins to consider while deploying a secure 5G network, based on the surveyed documents from the European government's efforts in commercializing the IoT devices and securing supply chain over 5G networks."],"url":"http://arxiv.org/abs/2401.14350v1","category":"cs.NI"}
{"created":"2024-01-25 17:50:05","title":"Learning to navigate efficiently and precisely in real environments","abstract":"In the context of autonomous navigation of terrestrial robots, the creation of realistic models for agent dynamics and sensing is a widespread habit in the robotics literature and in commercial applications, where they are used for model based control and/or for localization and mapping. The more recent Embodied AI literature, on the other hand, focuses on modular or end-to-end agents trained in simulators like Habitat or AI-Thor, where the emphasis is put on photo-realistic rendering and scene diversity, but high-fidelity robot motion is assigned a less privileged role. The resulting sim2real gap significantly impacts transfer of the trained models to real robotic platforms. In this work we explore end-to-end training of agents in simulation in settings which minimize the sim2real gap both, in sensing and in actuation. Our agent directly predicts (discretized) velocity commands, which are maintained through closed-loop control in the real robot. The behavior of the real robot (including the underlying low-level controller) is identified and simulated in a modified Habitat simulator. Noise models for odometry and localization further contribute in lowering the sim2real gap. We evaluate on real navigation scenarios, explore different localization and point goal calculation methods and report significant gains in performance and robustness compared to prior work.","sentences":["In the context of autonomous navigation of terrestrial robots, the creation of realistic models for agent dynamics and sensing is a widespread habit in the robotics literature and in commercial applications, where they are used for model based control and/or for localization and mapping.","The more recent Embodied AI literature, on the other hand, focuses on modular or end-to-end agents trained in simulators like Habitat or AI-Thor, where the emphasis is put on photo-realistic rendering and scene diversity, but high-fidelity robot motion is assigned a less privileged role.","The resulting sim2real gap significantly impacts transfer of the trained models to real robotic platforms.","In this work we explore end-to-end training of agents in simulation in settings which minimize the sim2real gap both, in sensing and in actuation.","Our agent directly predicts (discretized) velocity commands, which are maintained through closed-loop control in the real robot.","The behavior of the real robot (including the underlying low-level controller) is identified and simulated in a modified Habitat simulator.","Noise models for odometry and localization further contribute in lowering the sim2real gap.","We evaluate on real navigation scenarios, explore different localization and point goal calculation methods and report significant gains in performance and robustness compared to prior work."],"url":"http://arxiv.org/abs/2401.14349v1","category":"cs.RO"}
{"created":"2024-01-25 17:48:11","title":"Evolving higher-order synergies reveals a trade-off between stability and information integration capacity in complex systems","abstract":"There has recently been an explosion of interest in how \"higher-order\" structures emerge in complex systems. This \"emergent\" organization has been found in a variety of natural and artificial systems, although at present the field lacks a unified understanding of what the consequences of higher-order synergies and redundancies are for systems. Typical research treat the presence (or absence) of synergistic information as a dependent variable and report changes in the level of synergy in response to some change in the system. Here, we attempt to flip the script: rather than treating higher-order information as a dependent variable, we use evolutionary optimization to evolve boolean networks with significant higher-order redundancies, synergies, or statistical complexity. We then analyse these evolved populations of networks using established tools for characterizing discrete dynamics: the number of attractors, average transient length, and Derrida coefficient. We also assess the capacity of the systems to integrate information. We find that high-synergy systems are unstable and chaotic, but with a high capacity to integrate information. In contrast, evolved redundant systems are extremely stable, but have negligible capacity to integrate information. Finally, the complex systems that balance integration and segregation (known as Tononi-Sporns-Edelman complexity) show features of both chaosticity and stability, with a greater capacity to integrate information than the redundant systems while being more stable than the random and synergistic systems. We conclude that there may be a fundamental trade-off between the robustness of a systems dynamics and its capacity to integrate information (which inherently requires flexibility and sensitivity), and that certain kinds of complexity naturally balance this trade-off.","sentences":["There has recently been an explosion of interest in how \"higher-order\" structures emerge in complex systems.","This \"emergent\" organization has been found in a variety of natural and artificial systems, although at present the field lacks a unified understanding of what the consequences of higher-order synergies and redundancies are for systems.","Typical research treat the presence (or absence) of synergistic information as a dependent variable and report changes in the level of synergy in response to some change in the system.","Here, we attempt to flip the script: rather than treating higher-order information as a dependent variable, we use evolutionary optimization to evolve boolean networks with significant higher-order redundancies, synergies, or statistical complexity.","We then analyse these evolved populations of networks using established tools for characterizing discrete dynamics: the number of attractors, average transient length, and Derrida coefficient.","We also assess the capacity of the systems to integrate information.","We find that high-synergy systems are unstable and chaotic, but with a high capacity to integrate information.","In contrast, evolved redundant systems are extremely stable, but have negligible capacity to integrate information.","Finally, the complex systems that balance integration and segregation (known as Tononi-Sporns-Edelman complexity) show features of both chaosticity and stability, with a greater capacity to integrate information than the redundant systems while being more stable than the random and synergistic systems.","We conclude that there may be a fundamental trade-off between the robustness of a systems dynamics and its capacity to integrate information (which inherently requires flexibility and sensitivity), and that certain kinds of complexity naturally balance this trade-off."],"url":"http://arxiv.org/abs/2401.14347v1","category":"cs.IT"}
{"created":"2024-01-25 17:44:14","title":"From the Choi Formalism in Infinite Dimensions to Unique Decompositions of Generators of Completely Positive Dynamical Semigroups","abstract":"Given any separable complex Hilbert space, any trace-class operator $B$ which does not have purely imaginary trace, and any generator $L$ of a norm-continuous one-parameter semigroup of completely positive maps we prove that there exists a unique bounded operator $K$ and a unique completely positive map $\\Phi$ such that (i) $L=K(\\cdot)+(\\cdot)K^*+\\Phi$, (ii) the superoperator $\\Phi(B^*(\\cdot)B)$ is trace class and has vanishing trace, and (iii) ${\\rm tr}(B^*K)$ is a real number. Central to our proof is a modified version of the Choi formalism which relates completely positive maps to positive semi-definite operators. We characterize when this correspondence is injective and surjective, respectively, which in turn explains why the proof idea of our main result cannot extend to non-separable Hilbert spaces. In particular, we find examples of positive semi-definite operators which have empty pre-image under the Choi formalism as soon as the underlying Hilbert space is infinite-dimensional.","sentences":["Given any separable complex Hilbert space, any trace-class operator $B$ which does not have purely imaginary trace, and any generator $L$ of a norm-continuous one-parameter semigroup of completely positive maps we prove that there exists a unique bounded operator $K$ and a unique completely positive map $\\Phi$ such that (i) $L=K(\\cdot)+(\\cdot)K^*+\\Phi$, (ii) the superoperator $\\Phi(B^*(\\cdot)B)$ is trace class and has vanishing trace, and (iii) ${\\rm tr}(B^*K)$ is a real number.","Central to our proof is a modified version of the Choi formalism which relates completely positive maps to positive semi-definite operators.","We characterize when this correspondence is injective and surjective, respectively, which in turn explains why the proof idea of our main result cannot extend to non-separable Hilbert spaces.","In particular, we find examples of positive semi-definite operators which have empty pre-image under the Choi formalism as soon as the underlying Hilbert space is infinite-dimensional."],"url":"http://arxiv.org/abs/2401.14344v1","category":"math.FA"}
{"created":"2024-01-25 17:43:39","title":"Class-attribute Priors: Adapting Optimization to Heterogeneity and Fairness Objective","abstract":"Modern classification problems exhibit heterogeneities across individual classes: Each class may have unique attributes, such as sample size, label quality, or predictability (easy vs difficult), and variable importance at test-time. Without care, these heterogeneities impede the learning process, most notably, when optimizing fairness objectives. Confirming this, under a gaussian mixture setting, we show that the optimal SVM classifier for balanced accuracy needs to be adaptive to the class attributes. This motivates us to propose CAP: An effective and general method that generates a class-specific learning strategy (e.g. hyperparameter) based on the attributes of that class. This way, optimization process better adapts to heterogeneities. CAP leads to substantial improvements over the naive approach of assigning separate hyperparameters to each class. We instantiate CAP for loss function design and post-hoc logit adjustment, with emphasis on label-imbalanced problems. We show that CAP is competitive with prior art and its flexibility unlocks clear benefits for fairness objectives beyond balanced accuracy. Finally, we evaluate CAP on problems with label noise as well as weighted test objectives to showcase how CAP can jointly adapt to different heterogeneities.","sentences":["Modern classification problems exhibit heterogeneities across individual classes: Each class may have unique attributes, such as sample size, label quality, or predictability (easy vs difficult), and variable importance at test-time.","Without care, these heterogeneities impede the learning process, most notably, when optimizing fairness objectives.","Confirming this, under a gaussian mixture setting, we show that the optimal SVM classifier for balanced accuracy needs to be adaptive to the class attributes.","This motivates us to propose CAP: An effective and general method that generates a class-specific learning strategy (e.g. hyperparameter) based on the attributes of that class.","This way, optimization process better adapts to heterogeneities.","CAP leads to substantial improvements over the naive approach of assigning separate hyperparameters to each class.","We instantiate CAP for loss function design and post-hoc logit adjustment, with emphasis on label-imbalanced problems.","We show that CAP is competitive with prior art and its flexibility unlocks clear benefits for fairness objectives beyond balanced accuracy.","Finally, we evaluate CAP on problems with label noise as well as weighted test objectives to showcase how CAP can jointly adapt to different heterogeneities."],"url":"http://arxiv.org/abs/2401.14343v1","category":"cs.LG"}
{"created":"2024-01-25 17:43:13","title":"Probing Quantum Entanglement from Quantum Correction to Newtonian Potential Energy","abstract":"Inspired by string theory ideas, we probe quantum entanglement from the gravitational potential energy. Concretely, we reconsider the study of quantum corrections to the Newtonian potential energy by treating a massive two-particle system $m_{1}$ and $m_{2}$ with size dimensions $r_{1}$ ad $% r_{2}$ where the two particles separated by a distance $d$ are under only their mutual classical gravitational interaction $V_{r}\\left( r_{1}\\text{, }% r_{2}\\right) $. Exploring such a size-dependent gravitational behavior and taking the limit $r_{1}$, $r_{2}\\ll d$, we investigate the associated quantum biparticle state and express its evolution after an interaction time $\\tau $. Among others, we show that the two masses cannot be separable due to the induced gravitational entanglement in terms of the accumulated quantum phase $\\delta \\phi =\\delta V_{g}\\tau /\\hbar $. By analogy with the classical gravity, we derive the expression of the resulting extremely weak entanglement force from the corresponding gravitational entanglement energy. Then, we provide certain entanglement diagnostics.","sentences":["Inspired by string theory ideas, we probe quantum entanglement from the gravitational potential energy.","Concretely, we reconsider the study of quantum corrections to the Newtonian potential energy by treating a massive two-particle system $m_{1}$ and $m_{2}$ with size dimensions $r_{1}$ ad $% r_{2}$ where the two particles separated by a distance $d$ are under only their mutual classical gravitational interaction $V_{r}\\left( r_{1}\\text{, }% r_{2}\\right) $.","Exploring such a size-dependent gravitational behavior and taking the limit $r_{1}$, $r_{2}\\ll d$, we investigate the associated quantum biparticle state and express its evolution after an interaction time $\\tau $.","Among others, we show that the two masses cannot be separable due to the induced gravitational entanglement in terms of the accumulated quantum phase $\\delta \\phi =\\delta V_{g}\\tau /\\hbar $.","By analogy with the classical gravity, we derive the expression of the resulting extremely weak entanglement force from the corresponding gravitational entanglement energy.","Then, we provide certain entanglement diagnostics."],"url":"http://arxiv.org/abs/2401.14342v1","category":"quant-ph"}
{"created":"2024-01-25 17:42:41","title":"Efficient Construction of Long Orientable Sequences","abstract":"An orientable sequence of order $n$ is a cyclic binary sequence such that each length-$n$ substring appears at most once \\emph{in either direction}. Maximal length orientable sequences are known only for $n\\leq 7$, and a trivial upper bound on their length is $2^{n-1} - 2^{\\lfloor(n-1)/2\\rfloor}$. This paper presents the first efficient algorithm to construct orientable sequences with asymptotically optimal length; more specifically, our algorithm constructs orientable sequences via cycle-joining and a successor-rule approach requiring $O(n)$ time per symbol and $O(n)$ space. This answers a longstanding open question from Dai, Martin, Robshaw, Wild [Cryptography and Coding III (1993)]. Our sequences are applied to find new longest-known orientable sequences for $n\\leq 20$.","sentences":["An orientable sequence of order $n$ is a cyclic binary sequence such that each length-$n$ substring appears at most once \\emph{in either direction}.","Maximal length orientable sequences are known only for $n\\leq 7$, and a trivial upper bound on their length is $2^{n-1} - 2^{\\lfloor(n-1)/2\\rfloor}$.","This paper presents the first efficient algorithm to construct orientable sequences with asymptotically optimal length; more specifically, our algorithm constructs orientable sequences via cycle-joining and a successor-rule approach requiring $O(n)$ time per symbol and $O(n)$ space.","This answers a longstanding open question from Dai, Martin, Robshaw, Wild [Cryptography and Coding III (1993)].","Our sequences are applied to find new longest-known orientable sequences for $n\\leq 20$."],"url":"http://arxiv.org/abs/2401.14341v1","category":"cs.DS"}
{"created":"2024-01-25 17:39:47","title":"Estimation of partially known Gaussian graphical models with score-based structural priors","abstract":"We propose a novel algorithm for the support estimation of partially known Gaussian graphical models that incorporates prior information about the underlying graph. In contrast to classical approaches that provide a point estimate based on a maximum likelihood or a maximum a posteriori criterion using (simple) priors on the precision matrix, we consider a prior on the graph and rely on annealed Langevin diffusion to generate samples from the posterior distribution. Since the Langevin sampler requires access to the score function of the underlying graph prior, we use graph neural networks to effectively estimate the score from a graph dataset (either available beforehand or generated from a known distribution). Numerical experiments demonstrate the benefits of our approach.","sentences":["We propose a novel algorithm for the support estimation of partially known Gaussian graphical models that incorporates prior information about the underlying graph.","In contrast to classical approaches that provide a point estimate based on a maximum likelihood or a maximum a posteriori criterion using (simple) priors on the precision matrix, we consider a prior on the graph and rely on annealed Langevin diffusion to generate samples from the posterior distribution.","Since the Langevin sampler requires access to the score function of the underlying graph prior, we use graph neural networks to effectively estimate the score from a graph dataset (either available beforehand or generated from a known distribution).","Numerical experiments demonstrate the benefits of our approach."],"url":"http://arxiv.org/abs/2401.14340v1","category":"stat.ML"}
{"created":"2024-01-25 17:37:40","title":"Quantum Variational Algorithms for the Allocation of Resources in a Cloud/Edge Architecture","abstract":"Modern Cloud/Edge architectures need to orchestrate multiple layers of heterogeneous computing nodes, including pervasive sensors/actuators, distributed Edge/Fog nodes, centralized data centers and quantum devices. The optimal assignment and scheduling of computation on the different nodes is a very difficult problem, with NP-hard complexity. In this paper, we explore the possibility of solving this problem with variational quantum algorithms, which can become a viable alternative to classical algorithms in the near future. In particular, we compare the performances, in terms of success probability, of two algorithms, i.e., Quantum Approximate Optimization Algorithm (QAOA) and Variational Quantum Eigensolver (VQE). The simulation experiments, performed for a set of simple problems, show that the VQE algorithm ensures better performances when it is equipped with appropriate circuit ansatzes that are able to restrict the search space. Moreover, experiments executed on real quantum hardware show that the execution time, when increasing the size of the problem, grows much more slowly than the trend obtained with classical computation, which is known to be exponential.","sentences":["Modern Cloud/Edge architectures need to orchestrate multiple layers of heterogeneous computing nodes, including pervasive sensors/actuators, distributed Edge/Fog nodes, centralized data centers and quantum devices.","The optimal assignment and scheduling of computation on the different nodes is a very difficult problem, with NP-hard complexity.","In this paper, we explore the possibility of solving this problem with variational quantum algorithms, which can become a viable alternative to classical algorithms in the near future.","In particular, we compare the performances, in terms of success probability, of two algorithms, i.e., Quantum Approximate Optimization Algorithm (QAOA) and Variational Quantum Eigensolver (VQE).","The simulation experiments, performed for a set of simple problems, show that the VQE algorithm ensures better performances when it is equipped with appropriate circuit ansatzes that are able to restrict the search space.","Moreover, experiments executed on real quantum hardware show that the execution time, when increasing the size of the problem, grows much more slowly than the trend obtained with classical computation, which is known to be exponential."],"url":"http://arxiv.org/abs/2401.14339v1","category":"quant-ph"}
{"created":"2024-01-25 17:34:34","title":"Progressive Multi-task Anti-Noise Learning and Distilling Frameworks for Fine-grained Vehicle Recognition","abstract":"Fine-grained vehicle recognition (FGVR) is an essential fundamental technology for intelligent transportation systems, but very difficult because of its inherent intra-class variation. Most previous FGVR studies only focus on the intra-class variation caused by different shooting angles, positions, etc., while the intra-class variation caused by image noise has received little attention. This paper proposes a progressive multi-task anti-noise learning (PMAL) framework and a progressive multi-task distilling (PMD) framework to solve the intra-class variation problem in FGVR due to image noise. The PMAL framework achieves high recognition accuracy by treating image denoising as an additional task in image recognition and progressively forcing a model to learn noise invariance. The PMD framework transfers the knowledge of the PMAL-trained model into the original backbone network, which produces a model with about the same recognition accuracy as the PMAL-trained model, but without any additional overheads over the original backbone network. Combining the two frameworks, we obtain models that significantly exceed previous state-of-the-art methods in recognition accuracy on two widely-used, standard FGVR datasets, namely Stanford Cars, and CompCars, as well as three additional surveillance image-based vehicle-type classification datasets, namely Beijing Institute of Technology (BIT)-Vehicle, Vehicle Type Image Data 2 (VTID2), and Vehicle Images Dataset for Make Model Recognition (VIDMMR), without any additional overheads over the original backbone networks. The source code is available at https://github.com/Dichao-Liu/Anti-noise_FGVR","sentences":["Fine-grained vehicle recognition (FGVR) is an essential fundamental technology for intelligent transportation systems, but very difficult because of its inherent intra-class variation.","Most previous FGVR studies only focus on the intra-class variation caused by different shooting angles, positions, etc., while the intra-class variation caused by image noise has received little attention.","This paper proposes a progressive multi-task anti-noise learning (PMAL) framework and a progressive multi-task distilling (PMD) framework to solve the intra-class variation problem in FGVR due to image noise.","The PMAL framework achieves high recognition accuracy by treating image denoising as an additional task in image recognition and progressively forcing a model to learn noise invariance.","The PMD framework transfers the knowledge of the PMAL-trained model into the original backbone network, which produces a model with about the same recognition accuracy as the PMAL-trained model, but without any additional overheads over the original backbone network.","Combining the two frameworks, we obtain models that significantly exceed previous state-of-the-art methods in recognition accuracy on two widely-used, standard FGVR datasets, namely Stanford Cars, and CompCars, as well as three additional surveillance image-based vehicle-type classification datasets, namely Beijing Institute of Technology (BIT)-Vehicle, Vehicle Type Image Data 2 (VTID2), and Vehicle Images Dataset for Make Model Recognition (VIDMMR), without any additional overheads over the original backbone networks.","The source code is available at https://github.com/Dichao-Liu/Anti-noise_FGVR"],"url":"http://arxiv.org/abs/2401.14336v1","category":"cs.CV"}
{"created":"2024-01-25 17:31:38","title":"Rotating effects on the photoionization cross-section of a 2D quantum ring","abstract":"In this letter, we investigate the nonrelativistic quantum motion of a charged particle within a rotating frame, taking into account the Aharonov-Bohm (AB) effect and a uniform magnetic field. Our analysis entails the derivation of the equation of motion and the corresponding radial equation to describe the system. Solving the resulting radial equation enables us to determine the eigenvalues and eigenfunctions, providing a clear expression for the energy levels. Furthermore, our numerical analysis highlights the substantial influence of rotation on both energy levels and optical properties. Specifically, we evaluate the photoionization cross-section (PCS) with and without the effects of rotation. To elucidate the impact of rotation on the photoionization process of the system, we present graphics that offer an appealing visualization of the intrinsic nature of the physics involved.","sentences":["In this letter, we investigate the nonrelativistic quantum motion of a charged particle within a rotating frame, taking into account the Aharonov-Bohm (AB) effect and a uniform magnetic field.","Our analysis entails the derivation of the equation of motion and the corresponding radial equation to describe the system.","Solving the resulting radial equation enables us to determine the eigenvalues and eigenfunctions, providing a clear expression for the energy levels.","Furthermore, our numerical analysis highlights the substantial influence of rotation on both energy levels and optical properties.","Specifically, we evaluate the photoionization cross-section (PCS) with and without the effects of rotation.","To elucidate the impact of rotation on the photoionization process of the system, we present graphics that offer an appealing visualization of the intrinsic nature of the physics involved."],"url":"http://arxiv.org/abs/2401.14333v1","category":"cond-mat.mes-hall"}
{"created":"2024-01-25 17:30:08","title":"SunBlock: Cloudless Protection for IoT Systems","abstract":"With an increasing number of Internet of Things (IoT) devices present in homes, there is a rise in the number of potential information leakage channels and their associated security threats and privacy risks. Despite a long history of attacks on IoT devices in unprotected home networks, the problem of accurate, rapid detection and prevention of such attacks remains open. Many existing IoT protection solutions are cloud-based, sometimes ineffective, and might share consumer data with unknown third parties. This paper investigates the potential for effective IoT threat detection locally, on a home router, using AI tools combined with classic rule-based traffic-filtering algorithms. Our results show that with a slight rise of router hardware resources caused by machine learning and traffic filtering logic, a typical home router instrumented with our solution is able to effectively detect risks and protect a typical home IoT network, equaling or outperforming existing popular solutions, without any effects on benign IoT functionality, and without relying on cloud services and third parties.","sentences":["With an increasing number of Internet of Things (IoT) devices present in homes, there is a rise in the number of potential information leakage channels and their associated security threats and privacy risks.","Despite a long history of attacks on IoT devices in unprotected home networks, the problem of accurate, rapid detection and prevention of such attacks remains open.","Many existing IoT protection solutions are cloud-based, sometimes ineffective, and might share consumer data with unknown third parties.","This paper investigates the potential for effective IoT threat detection locally, on a home router, using AI tools combined with classic rule-based traffic-filtering algorithms.","Our results show that with a slight rise of router hardware resources caused by machine learning and traffic filtering logic, a typical home router instrumented with our solution is able to effectively detect risks and protect a typical home IoT network, equaling or outperforming existing popular solutions, without any effects on benign IoT functionality, and without relying on cloud services and third parties."],"url":"http://arxiv.org/abs/2401.14332v1","category":"cs.CR"}
{"created":"2024-01-25 17:27:18","title":"Pulsar Timing Array source ensembles","abstract":"The stochastic gravitational wave background for pulsar timing arrays is often modeled by a Gaussian ensemble which is isotropic and unpolarized. However, the Universe has a discrete set of polarized gravitational wave sources at specific sky locations. Can we trust that the Gaussian ensemble is an accurate description? To investigate this, we explicitly construct an ensemble containing $N$ individual binary sources with circular orbits. The orbital inclination angles are randomly distributed, hence the individual sources are elliptically polarized. We then compute the first two moments of the Hellings and Downs correlation, as well as the pulsar-averaged correlation mean and (cosmic) variance. The first moments are the same as for a previously studied ensemble of circularly polarized sources. However, the second moments, and hence the variances, are different for the two ensembles. While neither discrete source model is exactly described by a Gaussian ensemble, we show that in the limit of large $N$, the differences are small.","sentences":["The stochastic gravitational wave background for pulsar timing arrays is often modeled by a Gaussian ensemble which is isotropic and unpolarized.","However, the Universe has a discrete set of polarized gravitational wave sources at specific sky locations.","Can we trust that the Gaussian ensemble is an accurate description?","To investigate this, we explicitly construct an ensemble containing $N$ individual binary sources with circular orbits.","The orbital inclination angles are randomly distributed, hence the individual sources are elliptically polarized.","We then compute the first two moments of the Hellings and Downs correlation, as well as the pulsar-averaged correlation mean and (cosmic) variance.","The first moments are the same as for a previously studied ensemble of circularly polarized sources.","However, the second moments, and hence the variances, are different for the two ensembles.","While neither discrete source model is exactly described by a Gaussian ensemble, we show that in the limit of large $N$, the differences are small."],"url":"http://arxiv.org/abs/2401.14329v1","category":"gr-qc"}
{"created":"2024-01-25 17:24:20","title":"Tripartite entanglement and tripartite steering in three-qubit pure states induced by vacuum-one-photon superpositions","abstract":"Utilizing a tritter with variable parameter $T$ and induced by vacuum-one-photon superpositions $\\left\\vert 0\\right\\rangle +\\alpha \\left\\vert 1\\right\\rangle $ with $\\alpha =\\left\\vert \\alpha \\right\\vert e^{i\\phi }$, we generate a class of three-qubit pure states. These states take the form of $\\left\\vert \\psi \\right\\rangle _{123}=c_{0}\\left\\vert 000\\right\\rangle +c_{1}\\left\\vert 100\\right\\rangle +c_{2}\\left\\vert 010\\right\\rangle +c_{3}\\left\\vert 001\\right\\rangle $. The coefficients ($ c_{0}$, $c_{1}$, $c_{2}$, and $c_{3}$) can be manipulated through interaction parameters ($\\left\\vert \\alpha \\right\\vert $, $\\phi $, and $T$). In line with Xie and Eberly's work[Phys. Rev. Lett. 127, 040403 (2021)], we investigate the genuine tripartite entanglement for $\\left\\vert \\psi \\right\\rangle _{123}$ using the concurrence triangle measure. Drawing on Hao et al.'s research [Phys. Rev. Lett. 128, 120402 (2021)], we examine tripartite steering for $\\left\\vert \\psi \\right\\rangle _{123}$ under certain measurements based on the uncertainty relations criterion. We identify nine potential configurations exhibiting varying steerability across different parameter spaces. It is important to highlight that while the state $% \\left\\vert \\psi \\right\\rangle _{123}$ exhibits entanglement, steering remains unattainable in a substantial portion of the parameter space.","sentences":["Utilizing a tritter with variable parameter $T$ and induced by vacuum-one-photon superpositions $\\left\\vert 0\\right\\rangle +\\alpha \\left\\vert 1\\right\\rangle $ with $\\alpha =\\left\\vert \\alpha \\right\\vert e^{i\\phi }$, we generate a class of three-qubit pure states.","These states take the form of $\\left\\vert \\psi \\right\\rangle _{123}=c_{0}\\left\\vert 000\\right\\rangle +c_{1}\\left\\vert 100\\right\\rangle +c_{2}\\left\\vert 010\\right\\rangle","+c_{3}\\left\\vert 001\\right\\rangle $.","The coefficients ($ c_{0}$, $c_{1}$, $c_{2}$, and $c_{3}$) can be manipulated through interaction parameters ($\\left\\vert \\alpha \\right\\vert $, $\\phi $, and $T$).","In line with Xie and Eberly's work[Phys.","Rev. Lett.","127, 040403 (2021)], we investigate the genuine tripartite entanglement for $\\left\\vert \\psi \\right\\rangle _{123}$ using the concurrence triangle measure.","Drawing on Hao et al.'s research [Phys. Rev. Lett.","128, 120402 (2021)], we examine tripartite steering for $\\left\\vert \\psi \\right\\rangle _{123}$ under certain measurements based on the uncertainty relations criterion.","We identify nine potential configurations exhibiting varying steerability across different parameter spaces.","It is important to highlight that while the state $% \\left\\vert \\psi \\right\\rangle _{123}$ exhibits entanglement, steering remains unattainable in a substantial portion of the parameter space."],"url":"http://arxiv.org/abs/2401.14328v1","category":"quant-ph"}
{"created":"2024-01-25 17:21:35","title":"Unlocking Past Information: Temporal Embeddings in Cooperative Bird's Eye View Prediction","abstract":"Accurate and comprehensive semantic segmentation of Bird's Eye View (BEV) is essential for ensuring safe and proactive navigation in autonomous driving. Although cooperative perception has exceeded the detection capabilities of single-agent systems, prevalent camera-based algorithms in cooperative perception neglect valuable information derived from historical observations. This limitation becomes critical during sensor failures or communication issues as cooperative perception reverts to single-agent perception, leading to degraded performance and incomplete BEV segmentation maps. This paper introduces TempCoBEV, a temporal module designed to incorporate historical cues into current observations, thereby improving the quality and reliability of BEV map segmentations. We propose an importance-guided attention architecture to effectively integrate temporal information that prioritizes relevant properties for BEV map segmentation. TempCoBEV is an independent temporal module that seamlessly integrates into state-of-the-art camera-based cooperative perception models. We demonstrate through extensive experiments on the OPV2V dataset that TempCoBEV performs better than non-temporal models in predicting current and future BEV map segmentations, particularly in scenarios involving communication failures. We show the efficacy of TempCoBEV and its capability to integrate historical cues into the current BEV map, improving predictions under optimal communication conditions by up to 2% and under communication failures by up to 19%. The code will be published on GitHub.","sentences":["Accurate and comprehensive semantic segmentation of Bird's Eye View (BEV) is essential for ensuring safe and proactive navigation in autonomous driving.","Although cooperative perception has exceeded the detection capabilities of single-agent systems, prevalent camera-based algorithms in cooperative perception neglect valuable information derived from historical observations.","This limitation becomes critical during sensor failures or communication issues as cooperative perception reverts to single-agent perception, leading to degraded performance and incomplete BEV segmentation maps.","This paper introduces TempCoBEV, a temporal module designed to incorporate historical cues into current observations, thereby improving the quality and reliability of BEV map segmentations.","We propose an importance-guided attention architecture to effectively integrate temporal information that prioritizes relevant properties for BEV map segmentation.","TempCoBEV is an independent temporal module that seamlessly integrates into state-of-the-art camera-based cooperative perception models.","We demonstrate through extensive experiments on the OPV2V dataset that TempCoBEV performs better than non-temporal models in predicting current and future BEV map segmentations, particularly in scenarios involving communication failures.","We show the efficacy of TempCoBEV and its capability to integrate historical cues into the current BEV map, improving predictions under optimal communication conditions by up to 2% and under communication failures by up to 19%.","The code will be published on GitHub."],"url":"http://arxiv.org/abs/2401.14325v1","category":"cs.CV"}
{"created":"2024-01-25 17:20:19","title":"Scalable Tree-based Register Automata Learning","abstract":"Existing active automata learning (AAL) algorithms have demonstrated their potential in capturing the behavior of complex systems (e.g., in analyzing network protocol implementations). The most widely used AAL algorithms generate finite state machine models, such as Mealy machines. For many analysis tasks, however, it is crucial to generate richer classes of models that also show how relations between data parameters affect system behavior. Such models have shown potential to uncover critical bugs, but their learning algorithms do not scale beyond small and well curated experiments. In this paper, we present $SL^\\lambda$, an effective and scalable register automata (RA) learning algorithm that significantly reduces the number of tests required for inferring models. It achieves this by combining a tree-based cost-efficient data structure with mechanisms for computing short and restricted tests. We have implemented $SL^\\lambda$ as a new algorithm in RALib. We evaluate its performance by comparing it against $SL^*$, the current state-of-the-art RA learning algorithm, in a series of experiments, and show superior performance and substantial asymptotic improvements in bigger systems.","sentences":["Existing active automata learning (AAL) algorithms have demonstrated their potential in capturing the behavior of complex systems (e.g., in analyzing network protocol implementations).","The most widely used AAL algorithms generate finite state machine models, such as Mealy machines.","For many analysis tasks, however, it is crucial to generate richer classes of models that also show how relations between data parameters affect system behavior.","Such models have shown potential to uncover critical bugs, but their learning algorithms do not scale beyond small and well curated experiments.","In this paper, we present $SL^\\lambda$, an effective and scalable register automata (RA) learning algorithm that significantly reduces the number of tests required for inferring models.","It achieves this by combining a tree-based cost-efficient data structure with mechanisms for computing short and restricted tests.","We have implemented $SL^\\lambda$ as a new algorithm in RALib.","We evaluate its performance by comparing it against $SL^*$, the current state-of-the-art RA learning algorithm, in a series of experiments, and show superior performance and substantial asymptotic improvements in bigger systems."],"url":"http://arxiv.org/abs/2401.14324v1","category":"cs.FL"}
{"created":"2024-01-25 17:19:49","title":"Common Randomness Generation from Finite Compound Sources","abstract":"We investigate the problem of generating common randomness (CR) from finite compound sources aided by unidirectional communication over rate-limited perfect channels. The two communicating parties, often referred to as terminals, observe independent and identically distributed (i.i.d.) samples of a finite compound source and aim to agree on a common random variable with a high probability for every possible realization of the source state. Both parties know the set of source states as well as their statistics. However, they are unaware of the actual realization of the source state. We establish a single-letter lower and upper bound on the compound CR capacity for the specified model. Furthermore, we present two special scenarios where the established bounds coincide.","sentences":["We investigate the problem of generating common randomness (CR) from finite compound sources aided by unidirectional communication over rate-limited perfect channels.","The two communicating parties, often referred to as terminals, observe independent and identically distributed (i.i.d.)","samples of a finite compound source and aim to agree on a common random variable with a high probability for every possible realization of the source state.","Both parties know the set of source states as well as their statistics.","However, they are unaware of the actual realization of the source state.","We establish a single-letter lower and upper bound on the compound CR capacity for the specified model.","Furthermore, we present two special scenarios where the established bounds coincide."],"url":"http://arxiv.org/abs/2401.14323v1","category":"cs.IT"}
{"created":"2024-01-25 17:19:22","title":"Generalized People Diversity: Learning a Human Perception-Aligned Diversity Representation for People Images","abstract":"Capturing the diversity of people in images is challenging: recent literature tends to focus on diversifying one or two attributes, requiring expensive attribute labels or building classifiers. We introduce a diverse people image ranking method which more flexibly aligns with human notions of people diversity in a less prescriptive, label-free manner. The Perception-Aligned Text-derived Human representation Space (PATHS) aims to capture all or many relevant features of people-related diversity, and, when used as the representation space in the standard Maximal Marginal Relevance (MMR) ranking algorithm, is better able to surface a range of types of people-related diversity (e.g. disability, cultural attire). PATHS is created in two stages. First, a text-guided approach is used to extract a person-diversity representation from a pre-trained image-text model. Then this representation is fine-tuned on perception judgments from human annotators so that it captures the aspects of people-related similarity that humans find most salient. Empirical results show that the PATHS method achieves diversity better than baseline methods, according to side-by-side ratings from human annotators.","sentences":["Capturing the diversity of people in images is challenging: recent literature tends to focus on diversifying one or two attributes, requiring expensive attribute labels or building classifiers.","We introduce a diverse people image ranking method which more flexibly aligns with human notions of people diversity in a less prescriptive, label-free manner.","The Perception-Aligned Text-derived Human representation Space (PATHS) aims to capture all or many relevant features of people-related diversity, and, when used as the representation space in the standard Maximal Marginal Relevance (MMR) ranking algorithm, is better able to surface a range of types of people-related diversity (e.g. disability, cultural attire).","PATHS is created in two stages.","First, a text-guided approach is used to extract a person-diversity representation from a pre-trained image-text model.","Then this representation is fine-tuned on perception judgments from human annotators so that it captures the aspects of people-related similarity that humans find most salient.","Empirical results show that the PATHS method achieves diversity better than baseline methods, according to side-by-side ratings from human annotators."],"url":"http://arxiv.org/abs/2401.14322v1","category":"cs.CV"}
{"created":"2024-01-25 17:18:33","title":"Quantifying Software Correctness by Combining Architecture Modeling and Formal Program Analysis","abstract":"Most formal methods see the correctness of a software system as a binary decision. However, proving the correctness of complex systems completely is difficult because they are composed of multiple components, usage scenarios, and environments. We present QuAC, a modular approach for quantifying the correctness of service-oriented software systems by combining software architecture modeling with deductive verification. Our approach is based on a model of the service-oriented architecture and the probabilistic usage scenarios of the system. The correctness of a single service is approximated by a coverage region, which is a formula describing which inputs for that service are proven to not lead to an erroneous execution. The coverage regions can be determined by a combination of various analyses, e.g., formal verification, expert estimations, or testing. The coverage regions and the software model are then combined into a probabilistic program. From this, we can compute the probability that under a given usage profile no service is called outside its coverage region. If the coverage region is large enough, then instead of attempting to get 100% coverage, which may be prohibitively expensive, run-time verification or testing approaches may be used to deal with inputs outside the coverage region. We also present an implementation of QuAC for Java using the modeling tool Palladio and the deductive verification tool KeY. We demonstrate its usability by applying it to a software simulation of an energy system.","sentences":["Most formal methods see the correctness of a software system as a binary decision.","However, proving the correctness of complex systems completely is difficult because they are composed of multiple components, usage scenarios, and environments.","We present QuAC, a modular approach for quantifying the correctness of service-oriented software systems by combining software architecture modeling with deductive verification.","Our approach is based on a model of the service-oriented architecture and the probabilistic usage scenarios of the system.","The correctness of a single service is approximated by a coverage region, which is a formula describing which inputs for that service are proven to not lead to an erroneous execution.","The coverage regions can be determined by a combination of various analyses, e.g., formal verification, expert estimations, or testing.","The coverage regions and the software model are then combined into a probabilistic program.","From this, we can compute the probability that under a given usage profile no service is called outside its coverage region.","If the coverage region is large enough, then instead of attempting to get 100% coverage, which may be prohibitively expensive, run-time verification or testing approaches may be used to deal with inputs outside the coverage region.","We also present an implementation of QuAC for Java using the modeling tool Palladio and the deductive verification tool KeY.","We demonstrate its usability by applying it to a software simulation of an energy system."],"url":"http://arxiv.org/abs/2401.14320v1","category":"cs.SE"}
{"created":"2024-01-25 17:13:51","title":"A Quantum \"Lifting Theorem\" for Constructions of Pseudorandom Generators from Random Oracles","abstract":"We study the (quantum) security of pseudorandom generators (PRGs) constructed from random oracles. We prove a ``lifting theorem'' showing, roughly, that if such a PRG is unconditionally secure against classical adversaries making polynomially many queries to the random oracle, then it is also (unconditionally) secure against quantum adversaries in the same sense. As a result of independent interest, we also show that any pseudo-deterministic quantum-oracle algorithm (i.e., a quantum algorithm that with high probability returns the same value on repeated executions) can be simulated by a computationally unbounded but query bounded classical-oracle algorithm with only a polynomial blowup in the number of queries. This implies as a corollary that our lifting theorem holds even for PRGs that themselves make quantum queries to the random oracle.","sentences":["We study the (quantum) security of pseudorandom generators (PRGs) constructed from random oracles.","We prove a ``lifting theorem'' showing, roughly, that if such a PRG is unconditionally secure against classical adversaries making polynomially many queries to the random oracle, then it is also (unconditionally) secure against quantum adversaries in the same sense.","As a result of independent interest, we also show that any pseudo-deterministic quantum-oracle algorithm (i.e., a quantum algorithm that with high probability returns the same value on repeated executions) can be simulated by a computationally unbounded but query bounded classical-oracle algorithm with only a polynomial blowup in the number of queries.","This implies as a corollary that our lifting theorem holds even for PRGs that themselves make quantum queries to the random oracle."],"url":"http://arxiv.org/abs/2401.14319v1","category":"cs.CR"}
{"created":"2024-01-25 17:08:13","title":"Maximizing the Minimum Eigenvalue in Constant Dimension","abstract":"In an instance of the minimum eigenvalue problem, we are given a collection of $n$ vectors $v_1,\\ldots, v_n \\subset {\\mathbb{R}^d}$, and the goal is to pick a subset $B\\subseteq [n]$ of given vectors to maximize the minimum eigenvalue of the matrix $\\sum_{i\\in B} v_i v_i^{\\top} $. Often, additional combinatorial constraints such as cardinality constraint $\\left(|B|\\leq k\\right)$ or matroid constraint ($B$ is a basis of a matroid defined on $[n]$) must be satisfied by the chosen set of vectors. The minimum eigenvalue problem with matroid constraints models a wide variety of problems including the Santa Clause problem, the E-design problem, and the constructive Kadison-Singer problem.   In this paper, we give a randomized algorithm that finds a set $B\\subseteq [n]$ subject to any matroid constraint whose minimum eigenvalue is at least $(1-\\epsilon)$ times the optimum, with high probability. The running time of the algorithm is $O\\left( n^{O(d\\log(d)/\\epsilon^2)}\\right)$. In particular, our results give a polynomial time asymptotic scheme when the dimension of the vectors is constant. Our algorithm uses a convex programming relaxation of the problem after guessing a rescaling which allows us to apply pipage rounding and matrix Chernoff inequalities to round to a good solution. The key new component is a structural lemma which enables us to \"guess'' the appropriate rescaling, which could be of independent interest. Our approach generalizes the approximation guarantee to monotone, homogeneous functions and as such we can maximize $\\det(\\sum_{i\\in B} v_i v_i^\\top)^{1/d}$, or minimize any norm of the eigenvalues of the matrix $\\left(\\sum_{i\\in B} v_i v_i^\\top\\right)^{-1} $, with the same running time under some mild assumptions. As a byproduct, we also get a simple algorithm for an algorithmic version of Kadison-Singer problem.","sentences":["In an instance of the minimum eigenvalue problem, we are given a collection of $n$ vectors $v_1,\\ldots, v_n","\\subset {\\mathbb{R}^d}$, and the goal is to pick a subset $B\\subseteq","[n]$ of given vectors to maximize the minimum eigenvalue of the matrix $\\sum_{i\\in B} v_i v_i^{\\top} $.","Often, additional combinatorial constraints such as cardinality constraint $\\left(|B|\\leq k\\right)$ or matroid constraint ($B$ is a basis of a matroid defined on $[n]$) must be satisfied by the chosen set of vectors.","The minimum eigenvalue problem with matroid constraints models a wide variety of problems including the Santa Clause problem, the E-design problem, and the constructive Kadison-Singer problem.   ","In this paper, we give a randomized algorithm that finds a set $B\\subseteq [n]$ subject to any matroid constraint whose minimum eigenvalue is at least $(1-\\epsilon)$ times the optimum, with high probability.","The running time of the algorithm is $O\\left( n^{O(d\\log(d)/\\epsilon^2)}\\right)$. In particular, our results give a polynomial time asymptotic scheme when the dimension of the vectors is constant.","Our algorithm uses a convex programming relaxation of the problem after guessing a rescaling which allows us to apply pipage rounding and matrix Chernoff inequalities to round to a good solution.","The key new component is a structural lemma which enables us to \"guess'' the appropriate rescaling, which could be of independent interest.","Our approach generalizes the approximation guarantee to monotone, homogeneous functions and as such we can maximize $\\det(\\sum_{i\\in B} v_i v_i^\\top)^{1/d}$, or minimize any norm of the eigenvalues of the matrix $\\left(\\sum_{i\\in B} v_i v_i^\\top\\right)^{-1} $, with the same running time under some mild assumptions.","As a byproduct, we also get a simple algorithm for an algorithmic version of Kadison-Singer problem."],"url":"http://arxiv.org/abs/2401.14317v1","category":"cs.DS"}
{"created":"2024-01-25 17:03:02","title":"MultiTest: Physical-Aware Object Insertion for Testing Multi-sensor Fusion Perception Systems","abstract":"Multi-sensor fusion stands as a pivotal technique in addressing numerous safety-critical tasks and applications, e.g., self-driving cars and automated robotic arms. With the continuous advancement in data-driven artificial intelligence (AI), MSF's potential for sensing and understanding intricate external environments has been further amplified, bringing a profound impact on intelligent systems and specifically on their perception systems. Similar to traditional software, adequate testing is also required for AI-enabled MSF systems. Yet, existing testing methods primarily concentrate on single-sensor perception systems (e.g., image-/point cloud-based object detection systems). There remains a lack of emphasis on generating multi-modal test cases for MSF systems. To address these limitations, we design and implement MultiTest, a fitness-guided metamorphic testing method for complex MSF perception systems. MultiTest employs a physical-aware approach to synthesize realistic multi-modal object instances and insert them into critical positions of background images and point clouds. A fitness metric is designed to guide and boost the test generation process. We conduct extensive experiments with five SOTA perception systems to evaluate MultiTest from the perspectives of: (1) generated test cases' realism, (2) fault detection capabilities, and (3) performance improvement. The results show that MultiTest can generate realistic and modality-consistent test data and effectively detect hundreds of diverse faults of an MSF system under test. Moreover, retraining an MSF system on the test cases generated by MultiTest can improve the system's robustness.","sentences":["Multi-sensor fusion stands as a pivotal technique in addressing numerous safety-critical tasks and applications, e.g., self-driving cars and automated robotic arms.","With the continuous advancement in data-driven artificial intelligence (AI), MSF's potential for sensing and understanding intricate external environments has been further amplified, bringing a profound impact on intelligent systems and specifically on their perception systems.","Similar to traditional software, adequate testing is also required for AI-enabled MSF systems.","Yet, existing testing methods primarily concentrate on single-sensor perception systems (e.g., image-/point cloud-based object detection systems).","There remains a lack of emphasis on generating multi-modal test cases for MSF systems.","To address these limitations, we design and implement MultiTest, a fitness-guided metamorphic testing method for complex MSF perception systems.","MultiTest employs a physical-aware approach to synthesize realistic multi-modal object instances and insert them into critical positions of background images and point clouds.","A fitness metric is designed to guide and boost the test generation process.","We conduct extensive experiments with five SOTA perception systems to evaluate MultiTest from the perspectives of: (1) generated test cases' realism, (2) fault detection capabilities, and (3) performance improvement.","The results show that MultiTest can generate realistic and modality-consistent test data and effectively detect hundreds of diverse faults of an MSF system under test.","Moreover, retraining an MSF system on the test cases generated by MultiTest can improve the system's robustness."],"url":"http://arxiv.org/abs/2401.14314v1","category":"cs.SE"}
{"created":"2024-01-25 16:54:55","title":"The soaring kite: a tale of two punctured tori","abstract":"We consider the 5-mass kite family of self-energy Feynman integrals and present a systematic approach for constructing an epsilon-form basis, along with its differential equation pulled back onto the moduli space of two tori. Each torus is associated with one of the two distinct elliptic curves this family depends on. We demonstrate how the locations of relevant punctures, which are required to parametrize the full image of the kinematic space onto this moduli space, can be extracted from integrals over maximal cuts. A boundary value is provided such that the differential equation is systematically solved in terms of iterated integrals over g-kernels and modular forms. Then, the numerical evaluation of the master integrals is discussed, and important challenges in that regard are emphasized. In an appendix, we introduce new relations between g-kernels.","sentences":["We consider the 5-mass kite family of self-energy Feynman integrals and present a systematic approach for constructing an epsilon-form basis, along with its differential equation pulled back onto the moduli space of two tori.","Each torus is associated with one of the two distinct elliptic curves this family depends on.","We demonstrate how the locations of relevant punctures, which are required to parametrize the full image of the kinematic space onto this moduli space, can be extracted from integrals over maximal cuts.","A boundary value is provided such that the differential equation is systematically solved in terms of iterated integrals over g-kernels and modular forms.","Then, the numerical evaluation of the master integrals is discussed, and important challenges in that regard are emphasized.","In an appendix, we introduce new relations between g-kernels."],"url":"http://arxiv.org/abs/2401.14307v1","category":"hep-th"}
{"created":"2024-01-25 16:50:57","title":"On Some Complexity Results for Even Linear Languages","abstract":"We deal with a normal form for context-free grammars, called Dyck normal form. This normal form is a syntactical restriction of the Chomsky normal form, in which the two nonterminals occurring on the right-hand side of a rule are paired nonterminals. This pairwise property, along with several other terminal rewriting conditions, makes it possible to define a homomorphism from Dyck words to words generated by a grammar in Dyck normal form. We prove that for each context-free language L, there exist an integer K and a homomorphism phi such that L=phi(D'_K), where D'_K is a subset of D_K and D_K is the one-sided Dyck language over K letters. As an application we give an alternative proof of the inclusion of the class of even linear languages in AC1.","sentences":["We deal with a normal form for context-free grammars, called Dyck normal form.","This normal form is a syntactical restriction of the Chomsky normal form, in which the two nonterminals occurring on the right-hand side of a rule are paired nonterminals.","This pairwise property, along with several other terminal rewriting conditions, makes it possible to define a homomorphism from Dyck words to words generated by a grammar in Dyck normal form.","We prove that for each context-free language L, there exist an integer K and a homomorphism phi such that L=phi(D'_K), where D'_K is a subset of D_K and D_K is the one-sided Dyck language over K letters.","As an application we give an alternative proof of the inclusion of the class of even linear languages in AC1."],"url":"http://arxiv.org/abs/2401.14303v1","category":"cs.FL"}
{"created":"2024-01-25 16:42:45","title":"Characterising the Haar measure on the $p$-adic rotation groups via inverse limits of measure spaces","abstract":"We determine the Haar measure on the compact $p$-adic special orthogonal groups of rotations $\\mathrm{SO}(d)_p$ in dimension $d=2,3$, by exploiting the machinery of inverse limits of measure spaces, for every prime $p>2$. We characterise $\\mathrm{SO}(d)_p$ as inverse limits of finite groups, of which we provide parametrisations and orders, together with an equivalent description through a multivariable Hensel lifting. Supplying these finite groups with their normalised counting measures, we get an inverse family of Haar measure spaces for each $\\mathrm{SO}(d)_p$. Finally, we constructively prove the existence of the so-called inverse limit measure of these inverse families, which is explicitly computable, and prove that it gives the Haar measure on $\\mathrm{SO}(d)_p$. Our results pave the way towards the study of the irreducible projective unitary representations of the $p$-adic rotation groups, with potential applications to the recently proposed $p$-adic quantum information theory.","sentences":["We determine the Haar measure on the compact $p$-adic special orthogonal groups of rotations $\\mathrm{SO}(d)_p$ in dimension $d=2,3$, by exploiting the machinery of inverse limits of measure spaces, for every prime $p>2$. We characterise $\\mathrm{SO}(d)_p$ as inverse limits of finite groups, of which we provide parametrisations and orders, together with an equivalent description through a multivariable Hensel lifting.","Supplying these finite groups with their normalised counting measures, we get an inverse family of Haar measure spaces for each $\\mathrm{SO}(d)_p$. Finally, we constructively prove the existence of the so-called inverse limit measure of these inverse families, which is explicitly computable, and prove that it gives the Haar measure on $\\mathrm{SO}(d)_p$. Our results pave the way towards the study of the irreducible projective unitary representations of the $p$-adic rotation groups, with potential applications to the recently proposed $p$-adic quantum information theory."],"url":"http://arxiv.org/abs/2401.14298v1","category":"math-ph"}
{"created":"2024-01-25 16:38:06","title":"\"All of Me\": Mining Users' Attributes from their Public Spotify Playlists","abstract":"In the age of digital music streaming, playlists on platforms like Spotify have become an integral part of individuals' musical experiences. People create and publicly share their own playlists to express their musical tastes, promote the discovery of their favorite artists, and foster social connections. These publicly accessible playlists transcend the boundaries of mere musical preferences: they serve as sources of rich insights into users' attributes and identities. For example, the musical preferences of elderly individuals may lean more towards Frank Sinatra, while Billie Eilish remains a favored choice among teenagers. These playlists thus become windows into the diverse and evolving facets of one's musical identity.   In this work, we investigate the relationship between Spotify users' attributes and their public playlists. In particular, we focus on identifying recurring musical characteristics associated with users' individual attributes, such as demographics, habits, or personality traits. To this end, we conducted an online survey involving 739 Spotify users, yielding a dataset of 10,286 publicly shared playlists encompassing over 200,000 unique songs and 55,000 artists. Through extensive statistical analyses, we first assess a deep connection between a user's Spotify playlists and their real-life attributes. For instance, we found individuals high in openness often create playlists featuring a diverse array of artists, while female users prefer Pop and K-pop music genres. Building upon these observed associations, we create accurate predictive models for users' attributes, presenting a novel DeepSet application that outperforms baselines in most of these users' attributes.","sentences":["In the age of digital music streaming, playlists on platforms like Spotify have become an integral part of individuals' musical experiences.","People create and publicly share their own playlists to express their musical tastes, promote the discovery of their favorite artists, and foster social connections.","These publicly accessible playlists transcend the boundaries of mere musical preferences: they serve as sources of rich insights into users' attributes and identities.","For example, the musical preferences of elderly individuals may lean more towards Frank Sinatra, while Billie Eilish remains a favored choice among teenagers.","These playlists thus become windows into the diverse and evolving facets of one's musical identity.   ","In this work, we investigate the relationship between Spotify users' attributes and their public playlists.","In particular, we focus on identifying recurring musical characteristics associated with users' individual attributes, such as demographics, habits, or personality traits.","To this end, we conducted an online survey involving 739 Spotify users, yielding a dataset of 10,286 publicly shared playlists encompassing over 200,000 unique songs and 55,000 artists.","Through extensive statistical analyses, we first assess a deep connection between a user's Spotify playlists and their real-life attributes.","For instance, we found individuals high in openness often create playlists featuring a diverse array of artists, while female users prefer Pop and K-pop music genres.","Building upon these observed associations, we create accurate predictive models for users' attributes, presenting a novel DeepSet application that outperforms baselines in most of these users' attributes."],"url":"http://arxiv.org/abs/2401.14296v1","category":"cs.CR"}
{"created":"2024-01-25 16:34:00","title":"Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts","abstract":"The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxonomy of structure-enhanced LLM reasoning schemes. We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others. We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context. Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost. We also outline theoretical underpinnings, relationships between prompting and others parts of the LLM ecosystem such as knowledge bases, and the associated research challenges. Our work will help to advance future prompt engineering techniques.","sentences":["The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques.","Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph.","As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing.","To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes.","For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts.","We then build the first taxonomy of structure-enhanced LLM reasoning schemes.","We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others.","We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context.","Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost.","We also outline theoretical underpinnings, relationships between prompting and others parts of the LLM ecosystem such as knowledge bases, and the associated research challenges.","Our work will help to advance future prompt engineering techniques."],"url":"http://arxiv.org/abs/2401.14295v1","category":"cs.CL"}
{"created":"2024-01-25 16:30:22","title":"AST-2: Single and bi-layered 2-D acoustic soft tactile skin","abstract":"This paper aims to present an innovative and cost-effective design for Acoustic Soft Tactile (AST) Skin, with the primary goal of significantly enhancing the accuracy of 2-D tactile feature estimation. The existing challenge lies in achieving precise tactile feature estimation, especially concerning contact geometry characteristics, using cost-effective solutions. We hypothesise that by harnessing acoustic energy through dedicated acoustic channels in 2 layers beneath the sensing surface and analysing amplitude modulation, we can effectively decode interactions on the sensory surface, thereby improving tactile feature estimation. Our approach involves the distinct separation of hardware components responsible for emitting and receiving acoustic signals, resulting in a modular and highly customizable skin design. Practical tests demonstrate the effectiveness of this novel design, achieving remarkable precision in estimating contact normal forces (MAE < 0.8 N), 2D contact localisation (MAE < 0.7 mm), and contact surface diameter (MAE < 0.3 mm). In conclusion, the AST skin, with its innovative design and modular architecture, successfully addresses the challenge of tactile feature estimation. The presented results showcase its ability to precisely estimate various tactile features, making it a practical and cost-effective solution for robotic applications.","sentences":["This paper aims to present an innovative and cost-effective design for Acoustic Soft Tactile (AST) Skin, with the primary goal of significantly enhancing the accuracy of 2-D tactile feature estimation.","The existing challenge lies in achieving precise tactile feature estimation, especially concerning contact geometry characteristics, using cost-effective solutions.","We hypothesise that by harnessing acoustic energy through dedicated acoustic channels in 2 layers beneath the sensing surface and analysing amplitude modulation, we can effectively decode interactions on the sensory surface, thereby improving tactile feature estimation.","Our approach involves the distinct separation of hardware components responsible for emitting and receiving acoustic signals, resulting in a modular and highly customizable skin design.","Practical tests demonstrate the effectiveness of this novel design, achieving remarkable precision in estimating contact normal forces (MAE < 0.8 N), 2D contact localisation (MAE < 0.7 mm), and contact surface diameter (MAE < 0.3 mm).","In conclusion, the AST skin, with its innovative design and modular architecture, successfully addresses the challenge of tactile feature estimation.","The presented results showcase its ability to precisely estimate various tactile features, making it a practical and cost-effective solution for robotic applications."],"url":"http://arxiv.org/abs/2401.14292v1","category":"cs.RO"}
{"created":"2024-01-25 16:29:11","title":"Quantum Electrometer for Time-Resolved Material Science at the Atomic Lattice Scale","abstract":"The detection of individual charges plays a crucial role in fundamental material science and the advancement of classical and quantum high-performance technologies that operate with low noise. However, resolving charges at the lattice scale in a time-resolved manner has not been achieved so far. Here, we present the development of an electrometer, leveraging on the spectroscopy of an optically-active spin defect embedded in a solid-state material with a non-linear Stark response. By applying our approach to diamond, a widely used platform for quantum technology applications, we successfully localize charge traps, quantify their impact on transport dynamics and noise generation, analyze relevant material properties, and develop strategies for material optimization.","sentences":["The detection of individual charges plays a crucial role in fundamental material science and the advancement of classical and quantum high-performance technologies that operate with low noise.","However, resolving charges at the lattice scale in a time-resolved manner has not been achieved so far.","Here, we present the development of an electrometer, leveraging on the spectroscopy of an optically-active spin defect embedded in a solid-state material with a non-linear Stark response.","By applying our approach to diamond, a widely used platform for quantum technology applications, we successfully localize charge traps, quantify their impact on transport dynamics and noise generation, analyze relevant material properties, and develop strategies for material optimization."],"url":"http://arxiv.org/abs/2401.14290v1","category":"physics.app-ph"}
{"created":"2024-01-25 16:18:51","title":"Equivalence of Applicative Functors and Multifunctors","abstract":"McBride and Paterson introduced Applicative functors to Haskell, which are equivalent to the lax monoidal functors (with strength) of category theory. Applicative functors F are presented via idiomatic application $\\_\\circledast\\_ : F (A \\to B) \\to F A \\to F B$ and laws that are a bit hard to remember. Capriotti and Kaposi observed that applicative functors can be conceived as multifunctors, i.e., by a family liftA$_n$ : $(A_1 \\to ... \\to A_n \\to C) \\to F A_1 \\to ... \\to F A_n \\to F C$ of zipWith-like functions that generalize pure $(n=0)$, fmap $(n=1)$ and liftA2 $(n=2)$. This reduces the associated laws to just the first functor law and a uniform scheme of second (multi)functor laws, i.e., a composition law for liftA. In this note, we rigorously prove that applicative functors are in fact equivalent to multifunctors, by interderiving their laws.","sentences":["McBride and Paterson introduced Applicative functors to Haskell, which are equivalent to the lax monoidal functors (with strength) of category theory.","Applicative functors F are presented via idiomatic application $\\_\\circledast\\_ :","F (A \\to B) \\to F A \\to F B$ and laws that are a bit hard to remember.","Capriotti and Kaposi observed that applicative functors can be conceived as multifunctors, i.e., by a family liftA$_n$ : $(A_1 \\to ... \\to A_n \\to C) \\to F A_1 \\to ...","\\to F A_n \\to F C$ of zipWith-like functions that generalize pure $(n=0)$, fmap $(n=1)$ and liftA2 $(n=2)$.","This reduces the associated laws to just the first functor law and a uniform scheme of second (multi)functor laws, i.e., a composition law for liftA. In this note, we rigorously prove that applicative functors are in fact equivalent to multifunctors, by interderiving their laws."],"url":"http://arxiv.org/abs/2401.14286v1","category":"cs.PL"}
{"created":"2024-01-25 16:18:11","title":"POUR-Net: A Population-Prior-Aided Over-Under-Representation Network for Low-Count PET Attenuation Map Generation","abstract":"Low-dose PET offers a valuable means of minimizing radiation exposure in PET imaging. However, the prevalent practice of employing additional CT scans for generating attenuation maps (u-map) for PET attenuation correction significantly elevates radiation doses. To address this concern and further mitigate radiation exposure in low-dose PET exams, we propose POUR-Net - an innovative population-prior-aided over-under-representation network that aims for high-quality attenuation map generation from low-dose PET. First, POUR-Net incorporates an over-under-representation network (OUR-Net) to facilitate efficient feature extraction, encompassing both low-resolution abstracted and fine-detail features, for assisting deep generation on the full-resolution level. Second, complementing OUR-Net, a population prior generation machine (PPGM) utilizing a comprehensive CT-derived u-map dataset, provides additional prior information to aid OUR-Net generation. The integration of OUR-Net and PPGM within a cascade framework enables iterative refinement of $\\mu$-map generation, resulting in the production of high-quality $\\mu$-maps. Experimental results underscore the effectiveness of POUR-Net, showing it as a promising solution for accurate CT-free low-count PET attenuation correction, which also surpasses the performance of previous baseline methods.","sentences":["Low-dose PET offers a valuable means of minimizing radiation exposure in PET imaging.","However, the prevalent practice of employing additional CT scans for generating attenuation maps (u-map) for PET attenuation correction significantly elevates radiation doses.","To address this concern and further mitigate radiation exposure in low-dose PET exams, we propose POUR-Net - an innovative population-prior-aided over-under-representation network that aims for high-quality attenuation map generation from low-dose PET.","First, POUR-Net incorporates an over-under-representation network (OUR-Net) to facilitate efficient feature extraction, encompassing both low-resolution abstracted and fine-detail features, for assisting deep generation on the full-resolution level.","Second, complementing OUR-Net, a population prior generation machine (PPGM) utilizing a comprehensive CT-derived u-map dataset, provides additional prior information to aid OUR-Net generation.","The integration of OUR-Net and PPGM within a cascade framework enables iterative refinement of $\\mu$-map generation, resulting in the production of high-quality $\\mu$-maps.","Experimental results underscore the effectiveness of POUR-Net, showing it as a promising solution for accurate CT-free low-count PET attenuation correction, which also surpasses the performance of previous baseline methods."],"url":"http://arxiv.org/abs/2401.14285v1","category":"cs.CV"}
{"created":"2024-01-25 16:15:56","title":"Bridging Education and Development: IDEs as Interactive Learning Platforms","abstract":"In this work, we introduce a novel approach to programming education - in-IDE courses implemented for IntelliJ-based IDEs via the JetBrains Academy Plugin. The primary objective of this approach is to address the challenge of familiarizing students with industrial technologies by moving all theory and practical materials to a professional IDE. This approach allows students to immediately use modern industrial tools as they are fully integrated into the learning process. We have already applied this approach in over 40 courses, and it successfully educates students across diverse topics such as Plugin Development, Algorithms, Data Analysis, and Language mastery in various programming languages, including Kotlin, Java, C++, and Python. Along with the paper, we are providing the community not only with a new way of learning and a set of ready-made courses but also a collection of helpful resources to assist educators in getting started with the plugin. Finally, we describe in detail an IDE plugin development course that demonstrates how the in-IDE approach covers complex topics easily.","sentences":["In this work, we introduce a novel approach to programming education - in-IDE courses implemented for IntelliJ-based IDEs via the JetBrains Academy Plugin.","The primary objective of this approach is to address the challenge of familiarizing students with industrial technologies by moving all theory and practical materials to a professional IDE.","This approach allows students to immediately use modern industrial tools as they are fully integrated into the learning process.","We have already applied this approach in over 40 courses, and it successfully educates students across diverse topics such as Plugin Development, Algorithms, Data Analysis, and Language mastery in various programming languages, including Kotlin, Java, C++, and Python.","Along with the paper, we are providing the community not only with a new way of learning and a set of ready-made courses but also a collection of helpful resources to assist educators in getting started with the plugin.","Finally, we describe in detail an IDE plugin development course that demonstrates how the in-IDE approach covers complex topics easily."],"url":"http://arxiv.org/abs/2401.14284v1","category":"cs.SE"}
{"created":"2024-01-25 16:15:27","title":"Information Leakage Detection through Approximate Bayes-optimal Prediction","abstract":"In today's data-driven world, the proliferation of publicly available information intensifies the challenge of information leakage (IL), raising security concerns. IL involves unintentionally exposing secret (sensitive) information to unauthorized parties via systems' observable information. Conventional statistical approaches, which estimate mutual information (MI) between observable and secret information for detecting IL, face challenges such as the curse of dimensionality, convergence, computational complexity, and MI misestimation. Furthermore, emerging supervised machine learning (ML) methods, though effective, are limited to binary system-sensitive information and lack a comprehensive theoretical framework. To address these limitations, we establish a theoretical framework using statistical learning theory and information theory to accurately quantify and detect IL. We demonstrate that MI can be accurately estimated by approximating the log-loss and accuracy of the Bayes predictor. As the Bayes predictor is typically unknown in practice, we propose to approximate it with the help of automated machine learning (AutoML). First, we compare our MI estimation approaches against current baselines, using synthetic data sets generated using the multivariate normal (MVN) distribution with known MI. Second, we introduce a cut-off technique using one-sided statistical tests to detect IL, employing the Holm-Bonferroni correction to increase confidence in detection decisions. Our study evaluates IL detection performance on real-world data sets, highlighting the effectiveness of the Bayes predictor's log-loss estimation, and finds our proposed method to effectively estimate MI on synthetic data sets and thus detect ILs accurately.","sentences":["In today's data-driven world, the proliferation of publicly available information intensifies the challenge of information leakage (IL), raising security concerns.","IL involves unintentionally exposing secret (sensitive) information to unauthorized parties via systems' observable information.","Conventional statistical approaches, which estimate mutual information (MI) between observable and secret information for detecting IL, face challenges such as the curse of dimensionality, convergence, computational complexity, and MI misestimation.","Furthermore, emerging supervised machine learning (ML) methods, though effective, are limited to binary system-sensitive information and lack a comprehensive theoretical framework.","To address these limitations, we establish a theoretical framework using statistical learning theory and information theory to accurately quantify and detect IL.","We demonstrate that MI can be accurately estimated by approximating the log-loss and accuracy of the Bayes predictor.","As the Bayes predictor is typically unknown in practice, we propose to approximate it with the help of automated machine learning (AutoML).","First, we compare our MI estimation approaches against current baselines, using synthetic data sets generated using the multivariate normal (MVN) distribution with known MI.","Second, we introduce a cut-off technique using one-sided statistical tests to detect IL, employing the Holm-Bonferroni correction to increase confidence in detection decisions.","Our study evaluates IL detection performance on real-world data sets, highlighting the effectiveness of the Bayes predictor's log-loss estimation, and finds our proposed method to effectively estimate MI on synthetic data sets and thus detect ILs accurately."],"url":"http://arxiv.org/abs/2401.14283v1","category":"stat.ML"}
{"created":"2024-01-25 16:11:41","title":"RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization","abstract":"This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Latin scripts. We propose an innovative approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Focusing on Hindi, we demonstrate through Hindi-to-English translation and sentiment analysis tasks that romanized text not only significantly improves inference efficiency due to its lower fertility compared to native text but also achieves competitive performance with limited pre-training. Additionally, our novel multi-script prompting approach, which combines romanized and native texts, shows promise in further enhancing task performance. These findings suggest the potential of romanization in bridging the language gap for LLM applications, with future work aimed at expanding this approach to more languages and tasks.","sentences":["This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Latin scripts.","We propose an innovative approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment.","Focusing on Hindi, we demonstrate through Hindi-to-English translation and sentiment analysis tasks that romanized text not only significantly improves inference efficiency due to its lower fertility compared to native text but also achieves competitive performance with limited pre-training.","Additionally, our novel multi-script prompting approach, which combines romanized and native texts, shows promise in further enhancing task performance.","These findings suggest the potential of romanization in bridging the language gap for LLM applications, with future work aimed at expanding this approach to more languages and tasks."],"url":"http://arxiv.org/abs/2401.14280v1","category":"cs.CL"}
{"created":"2024-01-25 16:10:33","title":"ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT","abstract":"Technical question and answering (Q&A) sites such as Stack Overflow have become an important source for software developers to seek knowledge. However, code snippets on Q&A sites are usually uncompilable and semantically incomplete for compilation due to unresolved types and missing dependent libraries, which raises the obstacle for users to reuse or analyze Q&A code snippets. Prior approaches either are not designed for synthesizing compilable code or suffer from a low compilation success rate. To address this problem, we propose ZS4C, a lightweight approach to perform zero-shot synthesis of compilable code from incomplete code snippets using Large Language Model (LLM). ZS4C operates in two stages. In the first stage, ZS4C utilizes an LLM, i.e., ChatGPT, to identify missing import statements for a given code snippet, leveraging our designed task-specific prompt template. In the second stage, ZS4C fixes compilation errors caused by incorrect import statements and syntax errors through collaborative work between ChatGPT and a compiler. We thoroughly evaluated ZS4C on a widely used benchmark called StatType-SO against the SOTA approach SnR. Compared with SnR, ZS4C improves the compilation rate from 63% to 87.6%, with a 39.3% improvement. On average, ZS4C can infer more accurate import statements than SnR, with an improvement of 6.6% in the F1.","sentences":["Technical question and answering (Q&A) sites such as Stack Overflow have become an important source for software developers to seek knowledge.","However, code snippets on Q&A sites are usually uncompilable and semantically incomplete for compilation due to unresolved types and missing dependent libraries, which raises the obstacle for users to reuse or analyze Q&A code snippets.","Prior approaches either are not designed for synthesizing compilable code or suffer from a low compilation success rate.","To address this problem, we propose ZS4C, a lightweight approach to perform zero-shot synthesis of compilable code from incomplete code snippets using Large Language Model (LLM).","ZS4C operates in two stages.","In the first stage, ZS4C utilizes an LLM, i.e., ChatGPT, to identify missing import statements for a given code snippet, leveraging our designed task-specific prompt template.","In the second stage, ZS4C fixes compilation errors caused by incorrect import statements and syntax errors through collaborative work between ChatGPT and a compiler.","We thoroughly evaluated ZS4C on a widely used benchmark called StatType-SO against the SOTA approach SnR. Compared with SnR, ZS4C improves the compilation rate from 63% to 87.6%, with a 39.3% improvement.","On average, ZS4C can infer more accurate import statements than SnR, with an improvement of 6.6% in the F1."],"url":"http://arxiv.org/abs/2401.14279v1","category":"cs.SE"}
{"created":"2024-01-25 16:09:59","title":"CHIRON: Accelerating Node Synchronization without Security Trade-offs in Distributed Ledgers","abstract":"Blockchain performance has historically faced challenges posed by the throughput limitations of consensus algorithms. Recent breakthroughs in research have successfully alleviated these constraints by introducing a modular architecture that decouples consensus from execution. The move toward independent optimization of the consensus layer has shifted attention to the execution layer.   While concurrent transaction execution is a promising solution for increasing throughput, practical challenges persist. Its effectiveness varies based on the workloads, and the associated increased hardware requirements raise concerns about undesirable centralization. This increased requirement results in full nodes and stragglers synchronizing from signed checkpoints, decreasing the trustless nature of blockchain systems.   In response to these challenges, this paper introduces Chiron, a system designed to extract execution hints for the acceleration of straggling and full nodes. Notably, Chiron achieves this without compromising the security of the system or introducing overhead on the critical path of consensus. Evaluation results demonstrate a notable speedup of up to 30%, effectively addressing the gap between theoretical research and practical deployment. The quantification of this speedup is achieved through realistic blockchain benchmarks derived from a comprehensive analysis of Ethereum and Solana workloads, constituting an independent contribution.","sentences":["Blockchain performance has historically faced challenges posed by the throughput limitations of consensus algorithms.","Recent breakthroughs in research have successfully alleviated these constraints by introducing a modular architecture that decouples consensus from execution.","The move toward independent optimization of the consensus layer has shifted attention to the execution layer.   ","While concurrent transaction execution is a promising solution for increasing throughput, practical challenges persist.","Its effectiveness varies based on the workloads, and the associated increased hardware requirements raise concerns about undesirable centralization.","This increased requirement results in full nodes and stragglers synchronizing from signed checkpoints, decreasing the trustless nature of blockchain systems.   ","In response to these challenges, this paper introduces Chiron, a system designed to extract execution hints for the acceleration of straggling and full nodes.","Notably, Chiron achieves this without compromising the security of the system or introducing overhead on the critical path of consensus.","Evaluation results demonstrate a notable speedup of up to 30%, effectively addressing the gap between theoretical research and practical deployment.","The quantification of this speedup is achieved through realistic blockchain benchmarks derived from a comprehensive analysis of Ethereum and Solana workloads, constituting an independent contribution."],"url":"http://arxiv.org/abs/2401.14278v1","category":"cs.DC"}
{"created":"2024-01-25 16:09:44","title":"An Instance-Based Approach to the Trace Reconstruction Problem","abstract":"In the trace reconstruction problem, one observes the output of passing a binary string $s \\in \\{0,1\\}^n$ through a deletion channel $T$ times and wishes to recover $s$ from the resulting $T$ \"traces.\" Most of the literature has focused on characterizing the hardness of this problem in terms of the number of traces $T$ needed for perfect reconstruction either in the worst case or in the average case (over input sequences $s$). In this paper, we propose an alternative, instance-based approach to the problem. We define the \"Levenshtein difficulty\" of a problem instance $(s,T)$ as the probability that the resulting traces do not provide enough information for correct recovery with full certainty. One can then try to characterize, for a specific $s$, how $T$ needs to scale in order for the Levenshtein difficulty to go to zero, and seek reconstruction algorithms that match this scaling for each $s$. For a class of binary strings with alternating long runs, we precisely characterize the scaling of $T$ for which the Levenshtein difficulty goes to zero. For this class, we also prove that a simple \"Las Vegas algorithm\" has an error probability that decays to zero with the same rate as that with which the Levenshtein difficulty tends to zero.","sentences":["In the trace reconstruction problem, one observes the output of passing a binary string $s \\in \\{0,1\\}^n$ through a deletion channel $T$ times and wishes to recover $s$ from the resulting $T$ \"traces.\"","Most of the literature has focused on characterizing the hardness of this problem in terms of the number of traces $T$ needed for perfect reconstruction either in the worst case or in the average case (over input sequences $s$).","In this paper, we propose an alternative, instance-based approach to the problem.","We define the \"Levenshtein difficulty\" of a problem instance $(s,T)$ as the probability that the resulting traces do not provide enough information for correct recovery with full certainty.","One can then try to characterize, for a specific $s$, how $T$ needs to scale in order for the Levenshtein difficulty to go to zero, and seek reconstruction algorithms that match this scaling for each $s$. For a class of binary strings with alternating long runs, we precisely characterize the scaling of $T$ for which the Levenshtein difficulty goes to zero.","For this class, we also prove that a simple \"Las Vegas algorithm\" has an error probability that decays to zero with the same rate as that with which the Levenshtein difficulty tends to zero."],"url":"http://arxiv.org/abs/2401.14277v1","category":"cs.IT"}
{"created":"2024-01-25 16:07:59","title":"libcdict: fast dictionaries in C","abstract":"A common requirement in science is to store and share large sets of simulation data in an efficient, nested, flexible and human-readable way. Such datasets contain number counts and distributions, i.e. histograms and maps, of arbitrary dimension and variable type, e.g. floating-point number, integer or character string. Modern high-level programming languages like Perl and Python have associated arrays, knowns as dictionaries or hashes, respectively, to fulfil this storage need. Low-level languages used more commonly for fast computational simulations, such as C and Fortran, lack this functionality. We present libcdict, a C dictionary library, to solve this problem. Libcdict provides C and Fortran application programming interfaces (APIs) to native dictionaries, called cdicts, and functions for cdicts to load and save these as JSON and hence for easy interpretation in other software and languages like Perl, Python and R.","sentences":["A common requirement in science is to store and share large sets of simulation data in an efficient, nested, flexible and human-readable way.","Such datasets contain number counts and distributions, i.e. histograms and maps, of arbitrary dimension and variable type, e.g. floating-point number, integer or character string.","Modern high-level programming languages like Perl and Python have associated arrays, knowns as dictionaries or hashes, respectively, to fulfil this storage need.","Low-level languages used more commonly for fast computational simulations, such as C and Fortran, lack this functionality.","We present libcdict, a C dictionary library, to solve this problem.","Libcdict provides C and Fortran application programming interfaces (APIs) to native dictionaries, called cdicts, and functions for cdicts to load and save these as JSON and hence for easy interpretation in other software and languages like Perl, Python and R."],"url":"http://arxiv.org/abs/2401.14272v1","category":"cs.DS"}
{"created":"2024-01-25 16:05:44","title":"Viscoelasticty with physics-augmented neural networks: Model formulation and training methods without prescribed internal variables","abstract":"We present an approach for the data-driven modeling of nonlinear viscoelastic materials at small strains which is based on physics-augmented neural networks (NNs) and requires only stress and strain paths for training. The model is built on the concept of generalized standard materials and is therefore thermodynamically consistent by construction. It consists of a free energy and a dissipation potential, which can be either expressed by the components of their tensor arguments or by a suitable set of invariants. The two potentials are described by fully/partially input convex neural networks. For training of the NN model by paths of stress and strain, an efficient and flexible training method based on a recurrent cell, particularly a long short-term memory cell, is developed to automatically generate the internal variable(s) during the training process. The proposed method is benchmarked and thoroughly compared with existing approaches. These include a method that obtains the internal variable by integrating the evolution equation over the entire sequence, while the other method uses an an auxiliary feedforward neural network for the internal variable(s). Databases for training are generated by using a conventional nonlinear viscoelastic reference model, where 3D and 2D plane strain data with either ideal or noisy stresses are generated. The coordinate-based and the invariant-based formulation are compared and the advantages of the latter are demonstrated. Afterwards, the invariant-based model is calibrated by applying the three training methods using ideal or noisy stress data. All methods yield good results, but differ in computation time and usability for large data sets. The presented training method based on a recurrent cell turns out to be particularly robust and widely applicable and thus represents a promising approach for the calibration of other types of models as well.","sentences":["We present an approach for the data-driven modeling of nonlinear viscoelastic materials at small strains which is based on physics-augmented neural networks (NNs) and requires only stress and strain paths for training.","The model is built on the concept of generalized standard materials and is therefore thermodynamically consistent by construction.","It consists of a free energy and a dissipation potential, which can be either expressed by the components of their tensor arguments or by a suitable set of invariants.","The two potentials are described by fully/partially input convex neural networks.","For training of the NN model by paths of stress and strain, an efficient and flexible training method based on a recurrent cell, particularly a long short-term memory cell, is developed to automatically generate the internal variable(s) during the training process.","The proposed method is benchmarked and thoroughly compared with existing approaches.","These include a method that obtains the internal variable by integrating the evolution equation over the entire sequence, while the other method uses an an auxiliary feedforward neural network for the internal variable(s).","Databases for training are generated by using a conventional nonlinear viscoelastic reference model, where 3D and 2D plane strain data with either ideal or noisy stresses are generated.","The coordinate-based and the invariant-based formulation are compared and the advantages of the latter are demonstrated.","Afterwards, the invariant-based model is calibrated by applying the three training methods using ideal or noisy stress data.","All methods yield good results, but differ in computation time and usability for large data sets.","The presented training method based on a recurrent cell turns out to be particularly robust and widely applicable and thus represents a promising approach for the calibration of other types of models as well."],"url":"http://arxiv.org/abs/2401.14270v1","category":"cs.CE"}
{"created":"2024-01-25 16:02:56","title":"GPTVoiceTasker: LLM-Powered Virtual Assistant for Smartphone","abstract":"Virtual assistants have the potential to play an important role in helping users achieves different tasks. However, these systems face challenges in their real-world usability, characterized by inefficiency and struggles in grasping user intentions. Leveraging recent advances in Large Language Models (LLMs), we introduce GptVoiceTasker, a virtual assistant poised to enhance user experiences and task efficiency on mobile devices. GptVoiceTasker excels at intelligently deciphering user commands and executing relevant device interactions to streamline task completion. The system continually learns from historical user commands to automate subsequent usages, further enhancing execution efficiency. Our experiments affirm GptVoiceTasker's exceptional command interpretation abilities and the precision of its task automation module. In our user study, GptVoiceTasker boosted task efficiency in real-world scenarios by 34.85%, accompanied by positive participant feedback. We made GptVoiceTasker open-source, inviting further research into LLMs utilization for diverse tasks through prompt engineering and leveraging user usage data to improve efficiency.","sentences":["Virtual assistants have the potential to play an important role in helping users achieves different tasks.","However, these systems face challenges in their real-world usability, characterized by inefficiency and struggles in grasping user intentions.","Leveraging recent advances in Large Language Models (LLMs), we introduce GptVoiceTasker, a virtual assistant poised to enhance user experiences and task efficiency on mobile devices.","GptVoiceTasker excels at intelligently deciphering user commands and executing relevant device interactions to streamline task completion.","The system continually learns from historical user commands to automate subsequent usages, further enhancing execution efficiency.","Our experiments affirm GptVoiceTasker's exceptional command interpretation abilities and the precision of its task automation module.","In our user study, GptVoiceTasker boosted task efficiency in real-world scenarios by 34.85%, accompanied by positive participant feedback.","We made GptVoiceTasker open-source, inviting further research into LLMs utilization for diverse tasks through prompt engineering and leveraging user usage data to improve efficiency."],"url":"http://arxiv.org/abs/2401.14268v1","category":"cs.HC"}
{"created":"2024-01-25 16:01:49","title":"Transformers and Cortical Waves: Encoders for Pulling In Context Across Time","abstract":"The capabilities of transformer networks such as ChatGPT and other Large Language Models (LLMs) have captured the world's attention. The crucial computational mechanism underlying their performance relies on transforming a complete input sequence - for example, all the words in a sentence into a long \"encoding vector\" - that allows transformers to learn long-range temporal dependencies in naturalistic sequences. Specifically, \"self-attention\" applied to this encoding vector enhances temporal context in transformers by computing associations between pairs of words in the input sequence. We suggest that waves of neural activity, traveling across single cortical regions or across multiple regions at the whole-brain scale, could implement a similar encoding principle. By encapsulating recent input history into a single spatial pattern at each moment in time, cortical waves may enable temporal context to be extracted from sequences of sensory inputs, the same computational principle used in transformers.","sentences":["The capabilities of transformer networks such as ChatGPT and other Large Language Models (LLMs) have captured the world's attention.","The crucial computational mechanism underlying their performance relies on transforming a complete input sequence - for example, all the words in a sentence into a long \"encoding vector\" - that allows transformers to learn long-range temporal dependencies in naturalistic sequences.","Specifically, \"self-attention\" applied to this encoding vector enhances temporal context in transformers by computing associations between pairs of words in the input sequence.","We suggest that waves of neural activity, traveling across single cortical regions or across multiple regions at the whole-brain scale, could implement a similar encoding principle.","By encapsulating recent input history into a single spatial pattern at each moment in time, cortical waves may enable temporal context to be extracted from sequences of sensory inputs, the same computational principle used in transformers."],"url":"http://arxiv.org/abs/2401.14267v1","category":"cs.CL"}
{"created":"2024-01-25 16:00:01","title":"Worst-Case Per-User Error Bound for Asynchronous Unsourced Multiple Access","abstract":"This work considers an asynchronous $\\textsf{K}_a$-active-user unsourced multiple access channel (AUMAC) with the worst-case asynchronicity. The transmitted messages must be decoded within $n$ channel uses, while some codewords are not completely received due to asynchronicities. We consider a constraint of the largest allowed delay of the transmission. The AUMAC lacks the permutation-invariant property of the synchronous UMAC since different permutations of the same codewords with a fixed asynchronicity are distinguishable. Hence, the analyses require calculating all $2^{\\textsf{K}_a}-1$ combinations of erroneously decoded messages. Moreover, transmitters cannot adapt the corresponding codebooks according to asynchronicity due to a lack of information on asynchronicities. To overcome this challenge, a uniform bound of the per-user probability of error (PUPE) is derived by investigating the worst-case of the asynchronous patterns with the delay constraint. Numerical results show the trade-off between the energy-per-bit and the number of active users for different delay constraints. In addition, although the asynchronous transmission reduces interference, the required energy-per-bit increases as the receiver decodes with incompletely received codewords, compared to the synchronous case.","sentences":["This work considers an asynchronous $\\textsf{K}_a$-active-user unsourced multiple access channel (AUMAC) with the worst-case asynchronicity.","The transmitted messages must be decoded within $n$ channel uses, while some codewords are not completely received due to asynchronicities.","We consider a constraint of the largest allowed delay of the transmission.","The AUMAC lacks the permutation-invariant property of the synchronous UMAC since different permutations of the same codewords with a fixed asynchronicity are distinguishable.","Hence, the analyses require calculating all $2^{\\textsf{K}_a}-1$ combinations of erroneously decoded messages.","Moreover, transmitters cannot adapt the corresponding codebooks according to asynchronicity due to a lack of information on asynchronicities.","To overcome this challenge, a uniform bound of the per-user probability of error (PUPE) is derived by investigating the worst-case of the asynchronous patterns with the delay constraint.","Numerical results show the trade-off between the energy-per-bit and the number of active users for different delay constraints.","In addition, although the asynchronous transmission reduces interference, the required energy-per-bit increases as the receiver decodes with incompletely received codewords, compared to the synchronous case."],"url":"http://arxiv.org/abs/2401.14265v1","category":"cs.IT"}
{"created":"2024-01-25 15:54:06","title":"Conservation laws and the foundations of quantum mechanics","abstract":"In a recent paper, PNAS, 118, e1921529118 (2021), it was argued that while the standard definition of conservation laws in quantum mechanics, which is of a statistical character, is perfectly valid, it misses essential features of nature and it can and must be revisited to address the issue of conservation/non-conservation in individual cases. Specifically, in the above paper an experiment was presented in which it can be proven that in some individual cases energy is not conserved, despite being conserved statistically. It was felt however that this is worrisome, and that something must be wrong if there are individual instances in which conservation doesn't hold, even though this is not required by the standard conservation law. Here we revisit that experiment and show that although its results are correct, there is a way to circumvent them and ensure individual case conservation in that situation. The solution is however quite unusual, challenging one of the basic assumptions of quantum mechanics, namely that any quantum state can be prepared, and it involves a time-holistic, double non-conservation effect. Our results bring new light on the role of the preparation stage of the initial state of a particle and on the interplay of conservation laws and frames of reference. We also conjecture that when such a full analysis of any conservation experiment is performed, conservation is obeyed in every individual case.","sentences":["In a recent paper, PNAS, 118, e1921529118 (2021), it was argued that while the standard definition of conservation laws in quantum mechanics, which is of a statistical character, is perfectly valid, it misses essential features of nature and it can and must be revisited to address the issue of conservation/non-conservation in individual cases.","Specifically, in the above paper an experiment was presented in which it can be proven that in some individual cases energy is not conserved, despite being conserved statistically.","It was felt however that this is worrisome, and that something must be wrong if there are individual instances in which conservation doesn't hold, even though this is not required by the standard conservation law.","Here we revisit that experiment and show that although its results are correct, there is a way to circumvent them and ensure individual case conservation in that situation.","The solution is however quite unusual, challenging one of the basic assumptions of quantum mechanics, namely that any quantum state can be prepared, and it involves a time-holistic, double non-conservation effect.","Our results bring new light on the role of the preparation stage of the initial state of a particle and on the interplay of conservation laws and frames of reference.","We also conjecture that when such a full analysis of any conservation experiment is performed, conservation is obeyed in every individual case."],"url":"http://arxiv.org/abs/2401.14261v1","category":"quant-ph"}
{"created":"2024-01-25 15:50:13","title":"Mpemba effects in nonequilibrium open quantum systems","abstract":"Originally, the Mpemba effect (MPE) is referred to the faster icing of a higher-temperature system than a system of a lower temperature. This concept was later generalized to anomalous decays of certain system quantities to the equilibrium states. In this study, we investigate the scenario when a system has no such equilibrium state to approach. Instead, the system is put in contact with two different baths, and only a nonequilibrium state exists, sustained by constant energy injection from the surrounding thermal baths. Firstly, we show that the nonequilibrium conditions can dramatically enlarge the parameter regimes where the MPE emerges. Secondly, we demonstrate that the anomalous MPEs and inverse MPEs emerge in the evolution of quantum correlations in the two-site fermionic system and that nonequilibrium conditions can expedite or delay the MPEs. Thirdly, we show that the nonequilibrium-induced quantum coherence can have considerable contributions to the emergence of the MPE which the conventional Lindbladian dynamics fails to capture.","sentences":["Originally, the Mpemba effect (MPE) is referred to the faster icing of a higher-temperature system than a system of a lower temperature.","This concept was later generalized to anomalous decays of certain system quantities to the equilibrium states.","In this study, we investigate the scenario when a system has no such equilibrium state to approach.","Instead, the system is put in contact with two different baths, and only a nonequilibrium state exists, sustained by constant energy injection from the surrounding thermal baths.","Firstly, we show that the nonequilibrium conditions can dramatically enlarge the parameter regimes where the MPE emerges.","Secondly, we demonstrate that the anomalous MPEs and inverse MPEs emerge in the evolution of quantum correlations in the two-site fermionic system and that nonequilibrium conditions can expedite or delay the MPEs.","Thirdly, we show that the nonequilibrium-induced quantum coherence can have considerable contributions to the emergence of the MPE which the conventional Lindbladian dynamics fails to capture."],"url":"http://arxiv.org/abs/2401.14259v1","category":"quant-ph"}
{"created":"2024-01-25 15:49:12","title":"Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation","abstract":"Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents with fine-grained sketch control while being high-fidelity to text prompts. Extensive results show that our method achieves state-of-the-art performance in terms of sketch similarity and text alignment.","sentences":["Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description.","However, the generated objects are stochastic and lack fine-grained control.","Sketches provide a cheap approach to introduce such fine-grained control.","Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity.","In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation.","Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF).","We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF.","In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method.","We demonstrate that our method can synthesize 3D consistent contents with fine-grained sketch control while being high-fidelity to text prompts.","Extensive results show that our method achieves state-of-the-art performance in terms of sketch similarity and text alignment."],"url":"http://arxiv.org/abs/2401.14257v1","category":"cs.CV"}
{"created":"2024-01-25 15:47:18","title":"Producing Plankton Classifiers that are Robust to Dataset Shift","abstract":"Modern plankton high-throughput monitoring relies on deep learning classifiers for species recognition in water ecosystems. Despite satisfactory nominal performances, a significant challenge arises from Dataset Shift, which causes performances to drop during deployment. In our study, we integrate the ZooLake dataset with manually-annotated images from 10 independent days of deployment, serving as test cells to benchmark Out-Of-Dataset (OOD) performances. Our analysis reveals instances where classifiers, initially performing well in In-Dataset conditions, encounter notable failures in practical scenarios. For example, a MobileNet with a 92% nominal test accuracy shows a 77% OOD accuracy. We systematically investigate conditions leading to OOD performance drops and propose a preemptive assessment method to identify potential pitfalls when classifying new data, and pinpoint features in OOD images that adversely impact classification. We present a three-step pipeline: (i) identifying OOD degradation compared to nominal test performance, (ii) conducting a diagnostic analysis of degradation causes, and (iii) providing solutions. We find that ensembles of BEiT vision transformers, with targeted augmentations addressing OOD robustness, geometric ensembling, and rotation-based test-time augmentation, constitute the most robust model, which we call BEsT model. It achieves an 83% OOD accuracy, with errors concentrated on container classes. Moreover, it exhibits lower sensitivity to dataset shift, and reproduces well the plankton abundances. Our proposed pipeline is applicable to generic plankton classifiers, contingent on the availability of suitable test cells. By identifying critical shortcomings and offering practical procedures to fortify models against dataset shift, our study contributes to the development of more reliable plankton classification technologies.","sentences":["Modern plankton high-throughput monitoring relies on deep learning classifiers for species recognition in water ecosystems.","Despite satisfactory nominal performances, a significant challenge arises from Dataset Shift, which causes performances to drop during deployment.","In our study, we integrate the ZooLake dataset with manually-annotated images from 10 independent days of deployment, serving as test cells to benchmark Out-Of-Dataset (OOD) performances.","Our analysis reveals instances where classifiers, initially performing well in In-Dataset conditions, encounter notable failures in practical scenarios.","For example, a MobileNet with a 92% nominal test accuracy shows a 77% OOD accuracy.","We systematically investigate conditions leading to OOD performance drops and propose a preemptive assessment method to identify potential pitfalls when classifying new data, and pinpoint features in OOD images that adversely impact classification.","We present a three-step pipeline: (i) identifying OOD degradation compared to nominal test performance, (ii) conducting a diagnostic analysis of degradation causes, and (iii) providing solutions.","We find that ensembles of BEiT vision transformers, with targeted augmentations addressing OOD robustness, geometric ensembling, and rotation-based test-time augmentation, constitute the most robust model, which we call BEsT model.","It achieves an 83% OOD accuracy, with errors concentrated on container classes.","Moreover, it exhibits lower sensitivity to dataset shift, and reproduces well the plankton abundances.","Our proposed pipeline is applicable to generic plankton classifiers, contingent on the availability of suitable test cells.","By identifying critical shortcomings and offering practical procedures to fortify models against dataset shift, our study contributes to the development of more reliable plankton classification technologies."],"url":"http://arxiv.org/abs/2401.14256v1","category":"cs.CV"}
{"created":"2024-01-25 15:45:28","title":"Interpretable Solutions for Breast Cancer Diagnosis with Grammatical Evolution and Data Augmentation","abstract":"Medical imaging diagnosis increasingly relies on Machine Learning (ML) models. This is a task that is often hampered by severely imbalanced datasets, where positive cases can be quite rare. Their use is further compromised by their limited interpretability, which is becoming increasingly important. While post-hoc interpretability techniques such as SHAP and LIME have been used with some success on so-called black box models, the use of inherently understandable models makes such endeavors more fruitful. This paper addresses these issues by demonstrating how a relatively new synthetic data generation technique, STEM, can be used to produce data to train models produced by Grammatical Evolution (GE) that are inherently understandable. STEM is a recently introduced combination of the Synthetic Minority Oversampling Technique (SMOTE), Edited Nearest Neighbour (ENN), and Mixup; it has previously been successfully used to tackle both between class and within class imbalance issues. We test our technique on the Digital Database for Screening Mammography (DDSM) and the Wisconsin Breast Cancer (WBC) datasets and compare Area Under the Curve (AUC) results with an ensemble of the top three performing classifiers from a set of eight standard ML classifiers with varying degrees of interpretability. We demonstrate that the GE-derived models present the best AUC while still maintaining interpretable solutions.","sentences":["Medical imaging diagnosis increasingly relies on Machine Learning (ML) models.","This is a task that is often hampered by severely imbalanced datasets, where positive cases can be quite rare.","Their use is further compromised by their limited interpretability, which is becoming increasingly important.","While post-hoc interpretability techniques such as SHAP and LIME have been used with some success on so-called black box models, the use of inherently understandable models makes such endeavors more fruitful.","This paper addresses these issues by demonstrating how a relatively new synthetic data generation technique, STEM, can be used to produce data to train models produced by Grammatical Evolution (GE) that are inherently understandable.","STEM is a recently introduced combination of the Synthetic Minority Oversampling Technique (SMOTE), Edited Nearest Neighbour (ENN), and Mixup; it has previously been successfully used to tackle both between class and within class imbalance issues.","We test our technique on the Digital Database for Screening Mammography (DDSM) and the Wisconsin Breast Cancer (WBC) datasets and compare Area Under the Curve (AUC) results with an ensemble of the top three performing classifiers from a set of eight standard ML classifiers with varying degrees of interpretability.","We demonstrate that the GE-derived models present the best AUC while still maintaining interpretable solutions."],"url":"http://arxiv.org/abs/2401.14255v1","category":"cs.LG"}
{"created":"2024-01-25 15:42:36","title":"On mission Twitter Profiles: A Study of Selective Toxic Behavior","abstract":"The argument for persistent social media influence campaigns, often funded by malicious entities, is gaining traction. These entities utilize instrumented profiles to disseminate divisive content and disinformation, shaping public perception. Despite ample evidence of these instrumented profiles, few identification methods exist to locate them in the wild. To evade detection and appear genuine, small clusters of instrumented profiles engage in unrelated discussions, diverting attention from their true goals. This strategic thematic diversity conceals their selective polarity towards certain topics and fosters public trust.   This study aims to characterize profiles potentially used for influence operations, termed 'on-mission profiles,' relying solely on thematic content diversity within unlabeled data. Distinguishing this work is its focus on content volume and toxicity towards specific themes. Longitudinal data from 138K Twitter or X, profiles and 293M tweets enables profiling based on theme diversity. High thematic diversity groups predominantly produce toxic content concerning specific themes, like politics, health, and news classifying them as 'on-mission' profiles.   Using the identified ``on-mission\" profiles, we design a classifier for unseen, unlabeled data. Employing a linear SVM model, we train and test it on an 80/20% split of the most diverse profiles. The classifier achieves a flawless 100% accuracy, facilitating the discovery of previously unknown ``on-mission\" profiles in the wild.","sentences":["The argument for persistent social media influence campaigns, often funded by malicious entities, is gaining traction.","These entities utilize instrumented profiles to disseminate divisive content and disinformation, shaping public perception.","Despite ample evidence of these instrumented profiles, few identification methods exist to locate them in the wild.","To evade detection and appear genuine, small clusters of instrumented profiles engage in unrelated discussions, diverting attention from their true goals.","This strategic thematic diversity conceals their selective polarity towards certain topics and fosters public trust.   ","This study aims to characterize profiles potentially used for influence operations, termed 'on-mission profiles,' relying solely on thematic content diversity within unlabeled data.","Distinguishing this work is its focus on content volume and toxicity towards specific themes.","Longitudinal data from 138K Twitter or X, profiles and 293M tweets enables profiling based on theme diversity.","High thematic diversity groups predominantly produce toxic content concerning specific themes, like politics, health, and news classifying them as 'on-mission' profiles.   ","Using the identified ``on-mission\" profiles, we design a classifier for unseen, unlabeled data.","Employing a linear SVM model, we train and test it on an 80/20% split of the most diverse profiles.","The classifier achieves a flawless 100% accuracy, facilitating the discovery of previously unknown ``on-mission\" profiles in the wild."],"url":"http://arxiv.org/abs/2401.14252v1","category":"cs.CY"}
{"created":"2024-01-25 15:41:29","title":"On quasi-local angular momentum and the construction of axial vector fields","abstract":"A method is introduced which, for the first time, allows us to construct axial vector fields without which formal definitions of quasi-local angular momentum, in general, would remain empty. The introduced method is practical, it can be used to construct all such axial vector fields, and it allows the quasi-local angular momentum to be represented by a triple vector in three-dimensional Euclidean space. We also derive balance relations which allow us to monitor the variation of the magnitude and direction of this vector, and also to monitor the angular momentum transports in generic spacetimes without symmetries.","sentences":["A method is introduced which, for the first time, allows us to construct axial vector fields without which formal definitions of quasi-local angular momentum, in general, would remain empty.","The introduced method is practical, it can be used to construct all such axial vector fields, and it allows the quasi-local angular momentum to be represented by a triple vector in three-dimensional Euclidean space.","We also derive balance relations which allow us to monitor the variation of the magnitude and direction of this vector, and also to monitor the angular momentum transports in generic spacetimes without symmetries."],"url":"http://arxiv.org/abs/2401.14251v1","category":"gr-qc"}
{"created":"2024-01-25 15:40:19","title":"JUMP: A joint multimodal registration pipeline for neuroimaging with minimal preprocessing","abstract":"We present a pipeline for unbiased and robust multimodal registration of neuroimaging modalities with minimal pre-processing. While typical multimodal studies need to use multiple independent processing pipelines, with diverse options and hyperparameters, we propose a single and structured framework to jointly process different image modalities. The use of state-of-the-art learning-based techniques enables fast inferences, which makes the presented method suitable for large-scale and/or multi-cohort datasets with a diverse number of modalities per session. The pipeline currently works with structural MRI, resting state fMRI and amyloid PET images. We show the predictive power of the derived biomarkers using in a case-control study and study the cross-modal relationship between different image modalities. The code can be found in https: //github.com/acasamitjana/JUMP.","sentences":["We present a pipeline for unbiased and robust multimodal registration of neuroimaging modalities with minimal pre-processing.","While typical multimodal studies need to use multiple independent processing pipelines, with diverse options and hyperparameters, we propose a single and structured framework to jointly process different image modalities.","The use of state-of-the-art learning-based techniques enables fast inferences, which makes the presented method suitable for large-scale and/or multi-cohort datasets with a diverse number of modalities per session.","The pipeline currently works with structural MRI, resting state fMRI and amyloid PET images.","We show the predictive power of the derived biomarkers using in a case-control study and study the cross-modal relationship between different image modalities.","The code can be found in https: //github.com/acasamitjana/JUMP."],"url":"http://arxiv.org/abs/2401.14250v1","category":"cs.CV"}
{"created":"2024-01-25 15:36:49","title":"Contract Usage and Evolution in Android Mobile Applications","abstract":"Formal contracts and assertions are effective methods to enhance software quality by enforcing preconditions, postconditions, and invariants. Previous research has demonstrated the value of contracts in traditional software development contexts. However, the adoption and impact of contracts in the context of mobile application development, particularly of Android applications, remain unexplored.   To address this, we present the first large-scale empirical study on the presence and use of contracts in Android applications, written in Java or Kotlin. We consider different types of contract elements divided into five categories: conditional runtime exceptions, APIs, annotations, assertions, and other. We analyzed 2,390 Android applications from the F-Droid repository and processed more than 51,749 KLOC to determine 1) how and to what extent contracts are used, 2) how contract usage evolves, and 3) whether contracts are used safely in the context of program evolution and inheritance. Our findings include: 1) although most applications do not specify contracts, annotation-based approaches are the most popular among practitioners; 2) applications that use contracts continue to use them in later versions, but the number of methods increases at a higher rate than the number of contracts; and 3) there are many potentially unsafe specification changes when applications evolve and in subtyping relationships, which indicates a lack of specification stability. Our findings show that it would be desirable to have libraries that standardize contract specifications in Java and Kotlin, and tools that aid practitioners in writing stronger contracts and in detecting contract violations in the context of program evolution and inheritance.","sentences":["Formal contracts and assertions are effective methods to enhance software quality by enforcing preconditions, postconditions, and invariants.","Previous research has demonstrated the value of contracts in traditional software development contexts.","However, the adoption and impact of contracts in the context of mobile application development, particularly of Android applications, remain unexplored.   ","To address this, we present the first large-scale empirical study on the presence and use of contracts in Android applications, written in Java or Kotlin.","We consider different types of contract elements divided into five categories: conditional runtime exceptions, APIs, annotations, assertions, and other.","We analyzed 2,390 Android applications from the F-Droid repository and processed more than 51,749 KLOC to determine 1) how and to what extent contracts are used, 2) how contract usage evolves, and 3) whether contracts are used safely in the context of program evolution and inheritance.","Our findings include: 1) although most applications do not specify contracts, annotation-based approaches are the most popular among practitioners; 2) applications that use contracts continue to use them in later versions, but the number of methods increases at a higher rate than the number of contracts; and 3) there are many potentially unsafe specification changes when applications evolve and in subtyping relationships, which indicates a lack of specification stability.","Our findings show that it would be desirable to have libraries that standardize contract specifications in Java and Kotlin, and tools that aid practitioners in writing stronger contracts and in detecting contract violations in the context of program evolution and inheritance."],"url":"http://arxiv.org/abs/2401.14244v1","category":"cs.SE"}
{"created":"2024-01-25 15:36:45","title":"Variational Neural and Tensor Network Approximations of Thermal States","abstract":"We introduce a variational Monte Carlo algorithm for approximating finite-temperature quantum many-body systems, based on the minimization of a modified free energy. We employ a variety of trial states -- both tensor networks as well as neural networks -- as variational ans\\\"atze for our numerical optimization. We benchmark and compare different constructions in the above classes, both for one- and two-dimensional problems, with systems made of up to \\(N=100\\) spins. Despite excellent results in one dimension, our results suggest that the numerical ans\\\"atze employed have certain expressive limitations for tackling more challenging two-dimensional systems.","sentences":["We introduce a variational Monte Carlo algorithm for approximating finite-temperature quantum many-body systems, based on the minimization of a modified free energy.","We employ a variety of trial states -- both tensor networks as well as neural networks -- as variational ans\\\"atze for our numerical optimization.","We benchmark and compare different constructions in the above classes, both for one- and two-dimensional problems, with systems made of up to \\(N=100\\) spins.","Despite excellent results in one dimension, our results suggest that the numerical ans\\\"atze employed have certain expressive limitations for tackling more challenging two-dimensional systems."],"url":"http://arxiv.org/abs/2401.14243v1","category":"quant-ph"}
{"created":"2024-01-25 15:33:20","title":"Improving Natural Language Capability of Code Large Language Model","abstract":"Code large language models (Code LLMs) have demonstrated remarkable performance in code generation. Nonetheless, most existing works focus on boosting code LLMs from the perspective of programming capabilities, while their natural language capabilities receive less attention. To fill this gap, we thus propose a novel framework, comprising two modules: AttentionExtractor, which is responsible for extracting key phrases from the user's natural language requirements, and AttentionCoder, which leverages these extracted phrases to generate target code to solve the requirement. This framework pioneers an innovative idea by seamlessly integrating code LLMs with traditional natural language processing tools. To validate the effectiveness of the framework, we craft a new code generation benchmark, called MultiNL-H, covering five natural languages. Extensive experimental results demonstrate the effectiveness of our proposed framework.","sentences":["Code large language models (Code LLMs) have demonstrated remarkable performance in code generation.","Nonetheless, most existing works focus on boosting code LLMs from the perspective of programming capabilities, while their natural language capabilities receive less attention.","To fill this gap, we thus propose a novel framework, comprising two modules: AttentionExtractor, which is responsible for extracting key phrases from the user's natural language requirements, and AttentionCoder, which leverages these extracted phrases to generate target code to solve the requirement.","This framework pioneers an innovative idea by seamlessly integrating code LLMs with traditional natural language processing tools.","To validate the effectiveness of the framework, we craft a new code generation benchmark, called MultiNL-H, covering five natural languages.","Extensive experimental results demonstrate the effectiveness of our proposed framework."],"url":"http://arxiv.org/abs/2401.14242v1","category":"cs.CL"}
{"created":"2024-01-25 15:29:56","title":"New Algorithms for Computing Sibson Capacity and Arimoto Capacity","abstract":"The Arimoto capacity and Sibson capacity, which are based on the Arimoto and Sibson mutual information (MI) of order {\\alpha}, respectively, are well-known generalizations of the channel capacity C. In this study, we derive novel alternating optimization algorithms for computing these capacities by providing new max characterizations of the Arimoto MI and Sibson MI. Moreover, we prove that all iterative algorithms for computing these capacities are equivalent under appropriate conditions imposed on their initial distributions","sentences":["The Arimoto capacity and Sibson capacity, which are based on the Arimoto and Sibson mutual information (MI) of order {\\alpha}, respectively, are well-known generalizations of the channel capacity C.","In this study, we derive novel alternating optimization algorithms for computing these capacities by providing new max characterizations of the Arimoto MI and Sibson MI.","Moreover, we prove that all iterative algorithms for computing these capacities are equivalent under appropriate conditions imposed on their initial distributions"],"url":"http://arxiv.org/abs/2401.14241v1","category":"cs.IT"}
{"created":"2024-01-25 15:28:07","title":"Enhanced Labeling Technique for Reddit Text and Fine-Tuned Longformer Models for Classifying Depression Severity in English and Luganda","abstract":"Depression is a global burden and one of the most challenging mental health conditions to control. Experts can detect its severity early using the Beck Depression Inventory (BDI) questionnaire, administer appropriate medication to patients, and impede its progression. Due to the fear of potential stigmatization, many patients turn to social media platforms like Reddit for advice and assistance at various stages of their journey. This research extracts text from Reddit to facilitate the diagnostic process. It employs a proposed labeling approach to categorize the text and subsequently fine-tunes the Longformer model. The model's performance is compared against baseline models, including Naive Bayes, Random Forest, Support Vector Machines, and Gradient Boosting. Our findings reveal that the Longformer model outperforms the baseline models in both English (48%) and Luganda (45%) languages on a custom-made dataset.","sentences":["Depression is a global burden and one of the most challenging mental health conditions to control.","Experts can detect its severity early using the Beck Depression Inventory (BDI) questionnaire, administer appropriate medication to patients, and impede its progression.","Due to the fear of potential stigmatization, many patients turn to social media platforms like Reddit for advice and assistance at various stages of their journey.","This research extracts text from Reddit to facilitate the diagnostic process.","It employs a proposed labeling approach to categorize the text and subsequently fine-tunes the Longformer model.","The model's performance is compared against baseline models, including Naive Bayes, Random Forest, Support Vector Machines, and Gradient Boosting.","Our findings reveal that the Longformer model outperforms the baseline models in both English (48%) and Luganda (45%) languages on a custom-made dataset."],"url":"http://arxiv.org/abs/2401.14240v1","category":"cs.CL"}
{"created":"2024-01-25 15:21:53","title":"Exploring the Unexplored: Understanding the Impact of Layer Adjustments on Image Classification","abstract":"This paper investigates how adjustments to deep learning architectures impact model performance in image classification. Small-scale experiments generate initial insights although the trends observed are not consistent with the entire dataset. Filtering operations in the image processing pipeline are crucial, with image filtering before pre-processing yielding better results. The choice and order of layers as well as filter placement significantly impact model performance. This study provides valuable insights into optimizing deep learning models, with potential avenues for future research including collaborative platforms.","sentences":["This paper investigates how adjustments to deep learning architectures impact model performance in image classification.","Small-scale experiments generate initial insights although the trends observed are not consistent with the entire dataset.","Filtering operations in the image processing pipeline are crucial, with image filtering before pre-processing yielding better results.","The choice and order of layers as well as filter placement significantly impact model performance.","This study provides valuable insights into optimizing deep learning models, with potential avenues for future research including collaborative platforms."],"url":"http://arxiv.org/abs/2401.14236v1","category":"cs.CV"}
{"created":"2024-01-25 15:16:47","title":"Strongly k-recursive sequences","abstract":"Drawing inspiration from a recent paper of Heuberger, Krenn, and Lipnik, we define the class of strongly k-recursive sequences. We show that every k-automatic sequence is strongly $k$-recursive, therefore k-recursive, and discuss that the converse is not true.   We also show that the class of strongly k-recursive sequences is a proper subclass of the class of k-regular sequences, and we present some explicit examples. We then extend the proof techniques to answer the same question for the class of k-recursive sequences.","sentences":["Drawing inspiration from a recent paper of Heuberger, Krenn, and Lipnik, we define the class of strongly k-recursive sequences.","We show that every k-automatic sequence is strongly $k$-recursive, therefore k-recursive, and discuss that the converse is not true.   ","We also show that the class of strongly k-recursive sequences is a proper subclass of the class of k-regular sequences, and we present some explicit examples.","We then extend the proof techniques to answer the same question for the class of k-recursive sequences."],"url":"http://arxiv.org/abs/2401.14231v1","category":"cs.FL"}
{"created":"2024-01-25 15:11:07","title":"Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods","abstract":"As the cost of training ever larger language models has grown, so has the interest in reusing previously learnt knowledge. Transfer learning methods have shown how reusing non-task-specific knowledge can help in subsequent task-specific learning. In this paper, we investigate the inverse: porting whole functional modules that encode task-specific knowledge from one model to another. We designed a study comprising 1,440 training/testing runs to test the portability of modules trained by parameter-efficient finetuning (PEFT) techniques, using sentiment analysis as an example task. We test portability in a wide range of scenarios, involving different PEFT techniques and different pretrained host models, among other dimensions. We compare the performance of ported modules with that of equivalent modules trained (i) from scratch, and (ii) from parameters sampled from the same distribution as the ported module. We find that the ported modules far outperform the two alternatives tested, but that there are interesting performance differences between the four PEFT techniques. We conclude that task-specific knowledge in the form of structurally modular sets of parameters as produced by PEFT techniques is highly portable, but that degree of success depends on type of PEFT and on differences between originating and receiving pretrained models.","sentences":["As the cost of training ever larger language models has grown, so has the interest in reusing previously learnt knowledge.","Transfer learning methods have shown how reusing non-task-specific knowledge can help in subsequent task-specific learning.","In this paper, we investigate the inverse: porting whole functional modules that encode task-specific knowledge from one model to another.","We designed a study comprising 1,440 training/testing runs to test the portability of modules trained by parameter-efficient finetuning (PEFT) techniques, using sentiment analysis as an example task.","We test portability in a wide range of scenarios, involving different PEFT techniques and different pretrained host models, among other dimensions.","We compare the performance of ported modules with that of equivalent modules trained (i) from scratch, and (ii) from parameters sampled from the same distribution as the ported module.","We find that the ported modules far outperform the two alternatives tested, but that there are interesting performance differences between the four PEFT techniques.","We conclude that task-specific knowledge in the form of structurally modular sets of parameters as produced by PEFT techniques is highly portable, but that degree of success depends on type of PEFT and on differences between originating and receiving pretrained models."],"url":"http://arxiv.org/abs/2401.14228v1","category":"cs.CL"}
{"created":"2024-01-25 15:07:19","title":"Periodically Forced Nonlinear Oscillatory Acoustic Vacuum","abstract":"In this work, we study the in-plane oscillations of a finite lattice of particles coupled by linear springs under distributed harmonic excitation. Melnikov-type analysis is applied for the persistence of periodic oscillations of a reduced system.","sentences":["In this work, we study the in-plane oscillations of a finite lattice of particles coupled by linear springs under distributed harmonic excitation.","Melnikov-type analysis is applied for the persistence of periodic oscillations of a reduced system."],"url":"http://arxiv.org/abs/2401.14227v1","category":"math-ph"}
{"created":"2024-01-25 15:06:40","title":"Sample Efficient Reinforcement Learning by Automatically Learning to Compose Subtasks","abstract":"Improving sample efficiency is central to Reinforcement Learning (RL), especially in environments where the rewards are sparse. Some recent approaches have proposed to specify reward functions as manually designed or learned reward structures whose integrations in the RL algorithms are claimed to significantly improve the learning efficiency. Manually designed reward structures can suffer from inaccuracy and existing automatically learning methods are often computationally intractable for complex tasks. The integration of inaccurate or partial reward structures in RL algorithms fail to learn optimal policies. In this work, we propose an RL algorithm that can automatically structure the reward function for sample efficiency, given a set of labels that signify subtasks. Given such minimal knowledge about the task, we train a high-level policy that selects optimal sub-tasks in each state together with a low-level policy that efficiently learns to complete each sub-task. We evaluate our algorithm in a variety of sparse-reward environments. The experiment results show that our approach significantly outperforms the state-of-art baselines as the difficulty of the task increases.","sentences":["Improving sample efficiency is central to Reinforcement Learning (RL), especially in environments where the rewards are sparse.","Some recent approaches have proposed to specify reward functions as manually designed or learned reward structures whose integrations in the RL algorithms are claimed to significantly improve the learning efficiency.","Manually designed reward structures can suffer from inaccuracy and existing automatically learning methods are often computationally intractable for complex tasks.","The integration of inaccurate or partial reward structures in RL algorithms fail to learn optimal policies.","In this work, we propose an RL algorithm that can automatically structure the reward function for sample efficiency, given a set of labels that signify subtasks.","Given such minimal knowledge about the task, we train a high-level policy that selects optimal sub-tasks in each state together with a low-level policy that efficiently learns to complete each sub-task.","We evaluate our algorithm in a variety of sparse-reward environments.","The experiment results show that our approach significantly outperforms the state-of-art baselines as the difficulty of the task increases."],"url":"http://arxiv.org/abs/2401.14226v1","category":"cs.LG"}
{"created":"2024-01-25 15:05:22","title":"On the well-posedness of inverse problems under information field theory: application to model-form error detection","abstract":"We derive properties of information field theory (IFT) as applied to inverse problems. The results here can be extended to methodologies which can be seen as limiting cases of IFT, such as Gaussian process regression and physics-informed machine learning. We first define the concept of a well-posed inverse problem within the context of IFT, and pose a few useful theorems for conditions in which an inverse problem becomes well-posed. Using the Gaussian random field interpretation of IFT, we show how identifying parameters of a covariance kernel becomes a well-posed inverse problem under certain conditions. An expression for the Hessian of the inverse problem log posterior is derived to construct the results. A specific focus is placed on the inverse problem of detecting model-form error. We provide an example where the physics are assumed to be the Poisson equation and prove conditions for which identifying model-form error in this case becomes a well-posed inverse problem under IFT.","sentences":["We derive properties of information field theory (IFT) as applied to inverse problems.","The results here can be extended to methodologies which can be seen as limiting cases of IFT, such as Gaussian process regression and physics-informed machine learning.","We first define the concept of a well-posed inverse problem within the context of IFT, and pose a few useful theorems for conditions in which an inverse problem becomes well-posed.","Using the Gaussian random field interpretation of IFT, we show how identifying parameters of a covariance kernel becomes a well-posed inverse problem under certain conditions.","An expression for the Hessian of the inverse problem log posterior is derived to construct the results.","A specific focus is placed on the inverse problem of detecting model-form error.","We provide an example where the physics are assumed to be the Poisson equation and prove conditions for which identifying model-form error in this case becomes a well-posed inverse problem under IFT."],"url":"http://arxiv.org/abs/2401.14224v1","category":"math-ph"}
{"created":"2024-01-25 14:55:13","title":"The explicit form of the unitary representation of the Poincar\u00e9 group for vector-valued wave functions (massive and massless), with applications to photon's localization and position operators","abstract":"We geometrically derive the explicit form of the Unitary representation of the Poincare group and use it to apply speed-of-light boosts to simple polarization basis to end up with Hawton-Baylis photon position operator with commuting components. We give explicit formulas for other photon boost eigenmodes. We investigate the underlying affine connections on the light cone in momentum space and find that while Pryce connection is metric semi-symmetric, the flat Hawton-Baylis connection is not semi-symmetric. Finally we discuss localizability of photon states localized on closed loops and show that photon states on the circle, both unnormalized improper states and finite norm wave packet smeared over washer-like regions are strictly localized with respect to Hawton-Baylis operators with commuting components and also with respect to the noncommutative Jauch-Piron-Amrein POV measure.","sentences":["We geometrically derive the explicit form of the Unitary representation of the Poincare group and use it to apply speed-of-light boosts to simple polarization basis to end up with Hawton-Baylis photon position operator with commuting components.","We give explicit formulas for other photon boost eigenmodes.","We investigate the underlying affine connections on the light cone in momentum space and find that while Pryce connection is metric semi-symmetric, the flat Hawton-Baylis connection is not semi-symmetric.","Finally we discuss localizability of photon states localized on closed loops and show that photon states on the circle, both unnormalized improper states and finite norm wave packet smeared over washer-like regions are strictly localized with respect to Hawton-Baylis operators with commuting components and also with respect to the noncommutative Jauch-Piron-Amrein POV measure."],"url":"http://arxiv.org/abs/2401.14217v1","category":"quant-ph"}
{"created":"2024-01-25 14:54:33","title":"Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement","abstract":"Memorizing and utilizing speakers' personas is a common practice for response generation in long-term conversations. Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality. This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation. While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies. As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement. The supplementary video of our work is available at https://caffeine-15bbf.web.app/.","sentences":["Memorizing and utilizing speakers' personas is a common practice for response generation in long-term conversations.","Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality.","This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation.","While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies.","As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement.","The supplementary video of our work is available at https://caffeine-15bbf.web.app/."],"url":"http://arxiv.org/abs/2401.14215v1","category":"cs.CL"}
{"created":"2024-01-25 14:54:07","title":"A Quantitative Version of More Capable Channel Comparison","abstract":"This paper introduces a quantitative generalization of the ``more capable'' comparison of broadcast channels, which is termed ``more capable with advantage''. Some basic properties are demonstrated (including tensorization on product channels), and a characterisation is given for the cases of Binary Symmetric Channel (BSC) and Binary Erasure Channel (BEC).   It is then applied to two problems. First, a list decoding bound on the BSC is given that applies to transitive codes that achieve capacity on the BEC. Second, new lower bounds on entropy rates of binary hidden Markov processes are derived.","sentences":["This paper introduces a quantitative generalization of the ``more capable'' comparison of broadcast channels, which is termed ``more capable with advantage''.","Some basic properties are demonstrated (including tensorization on product channels), and a characterisation is given for the cases of Binary Symmetric Channel (BSC) and Binary Erasure Channel (BEC).   ","It is then applied to two problems.","First, a list decoding bound on the BSC is given that applies to transitive codes that achieve capacity on the BEC.","Second, new lower bounds on entropy rates of binary hidden Markov processes are derived."],"url":"http://arxiv.org/abs/2401.14214v1","category":"cs.IT"}
{"created":"2024-01-25 14:53:30","title":"Explicitly Representing Syntax Improves Sentence-to-layout Prediction of Unexpected Situations","abstract":"Recognizing visual entities in a natural language sentence and arranging them in a 2D spatial layout require a compositional understanding of language and space. This task of layout prediction is valuable in text-to-image synthesis as it allows localized and controlled in-painting of the image. In this comparative study it is shown that we can predict layouts from language representations that implicitly or explicitly encode sentence syntax, if the sentences mention similar entity-relationships to the ones seen during training. To test compositional understanding, we collect a test set of grammatically correct sentences and layouts describing compositions of entities and relations that unlikely have been seen during training. Performance on this test set substantially drops, showing that current models rely on correlations in the training data and have difficulties in understanding the structure of the input sentences. We propose a novel structural loss function that better enforces the syntactic structure of the input sentence and show large performance gains in the task of 2D spatial layout prediction conditioned on text. The loss has the potential to be used in other generation tasks where a tree-like structure underlies the conditioning modality. Code, trained models and the USCOCO evaluation set will be made available via github.","sentences":["Recognizing visual entities in a natural language sentence and arranging them in a 2D spatial layout require a compositional understanding of language and space.","This task of layout prediction is valuable in text-to-image synthesis as it allows localized and controlled in-painting of the image.","In this comparative study it is shown that we can predict layouts from language representations that implicitly or explicitly encode sentence syntax, if the sentences mention similar entity-relationships to the ones seen during training.","To test compositional understanding, we collect a test set of grammatically correct sentences and layouts describing compositions of entities and relations that unlikely have been seen during training.","Performance on this test set substantially drops, showing that current models rely on correlations in the training data and have difficulties in understanding the structure of the input sentences.","We propose a novel structural loss function that better enforces the syntactic structure of the input sentence and show large performance gains in the task of 2D spatial layout prediction conditioned on text.","The loss has the potential to be used in other generation tasks where a tree-like structure underlies the conditioning modality.","Code, trained models and the USCOCO evaluation set will be made available via github."],"url":"http://arxiv.org/abs/2401.14212v1","category":"cs.CL"}
{"created":"2024-01-25 14:49:15","title":"Communication-Efficient Federated Learning through Adaptive Weight Clustering and Server-Side Distillation","abstract":"Federated Learning (FL) is a promising technique for the collaborative training of deep neural networks across multiple devices while preserving data privacy. Despite its potential benefits, FL is hindered by excessive communication costs due to repeated server-client communication during training. To address this challenge, model compression techniques, such as sparsification and weight clustering are applied, which often require modifying the underlying model aggregation schemes or involve cumbersome hyperparameter tuning, with the latter not only adjusts the model's compression rate but also limits model's potential for continuous improvement over growing data. In this paper, we propose FedCompress, a novel approach that combines dynamic weight clustering and server-side knowledge distillation to reduce communication costs while learning highly generalizable models. Through a comprehensive evaluation on diverse public datasets, we demonstrate the efficacy of our approach compared to baselines in terms of communication costs and inference speed. We will make our implementation public upon acceptance.","sentences":["Federated Learning (FL) is a promising technique for the collaborative training of deep neural networks across multiple devices while preserving data privacy.","Despite its potential benefits, FL is hindered by excessive communication costs due to repeated server-client communication during training.","To address this challenge, model compression techniques, such as sparsification and weight clustering are applied, which often require modifying the underlying model aggregation schemes or involve cumbersome hyperparameter tuning, with the latter not only adjusts the model's compression rate but also limits model's potential for continuous improvement over growing data.","In this paper, we propose FedCompress, a novel approach that combines dynamic weight clustering and server-side knowledge distillation to reduce communication costs while learning highly generalizable models.","Through a comprehensive evaluation on diverse public datasets, we demonstrate the efficacy of our approach compared to baselines in terms of communication costs and inference speed.","We will make our implementation public upon acceptance."],"url":"http://arxiv.org/abs/2401.14211v1","category":"cs.LG"}
{"created":"2024-01-25 14:48:08","title":"At the junction between deep learning and statistics of extremes: formalizing the landslide hazard definition","abstract":"The most adopted definition of landslide hazard combines spatial information about landslide location (susceptibility), threat (intensity), and frequency (return period). Only the first two elements are usually considered and estimated when working over vast areas. Even then, separate models constitute the standard, with frequency being rarely investigated. Frequency and intensity are intertwined and depend on each other because larger events occur less frequently and vice versa. However, due to the lack of multi-temporal inventories and joint statistical models, modelling such properties via a unified hazard model has always been challenging and has yet to be attempted. Here, we develop a unified model to estimate landslide hazard at the slope unit level to address such gaps. We employed deep learning, combined with a model motivated by extreme-value theory to analyse an inventory of 30 years of observed rainfall-triggered landslides in Nepal and assess landslide hazard for multiple return periods. We also use our model to further explore landslide hazard for the same return periods under different climate change scenarios up to the end of the century. Our results show that the proposed model performs excellently and can be used to model landslide hazard in a unified manner. Geomorphologically, we find that under both climate change scenarios (SSP245 and SSP885), landslide hazard is likely to increase up to two times on average in the lower Himalayan regions while remaining the same in the middle Himalayan region whilst decreasing slightly in the upper Himalayan region areas.","sentences":["The most adopted definition of landslide hazard combines spatial information about landslide location (susceptibility), threat (intensity), and frequency (return period).","Only the first two elements are usually considered and estimated when working over vast areas.","Even then, separate models constitute the standard, with frequency being rarely investigated.","Frequency and intensity are intertwined and depend on each other because larger events occur less frequently and vice versa.","However, due to the lack of multi-temporal inventories and joint statistical models, modelling such properties via a unified hazard model has always been challenging and has yet to be attempted.","Here, we develop a unified model to estimate landslide hazard at the slope unit level to address such gaps.","We employed deep learning, combined with a model motivated by extreme-value theory to analyse an inventory of 30 years of observed rainfall-triggered landslides in Nepal and assess landslide hazard for multiple return periods.","We also use our model to further explore landslide hazard for the same return periods under different climate change scenarios up to the end of the century.","Our results show that the proposed model performs excellently and can be used to model landslide hazard in a unified manner.","Geomorphologically, we find that under both climate change scenarios (SSP245 and SSP885), landslide hazard is likely to increase up to two times on average in the lower Himalayan regions while remaining the same in the middle Himalayan region whilst decreasing slightly in the upper Himalayan region areas."],"url":"http://arxiv.org/abs/2401.14210v1","category":"cs.LG"}
{"created":"2024-01-25 14:48:05","title":"Computational General Relativity in the Wolfram Language using Gravitas II: ADM Formalism and Numerical Relativity","abstract":"This is the second in a series of two articles introducing the Gravitas computational general relativity framework, in which we now focus upon the design and capabilities of Gravitas's numerical subsystem, including its ability to perform general 3+1 decompositions of spacetime via the ADM formalism, its support for the definition and construction of arbitrary Cauchy surfaces as initial data, its support for the definition and enforcement of arbitrary gauge and coordinate conditions, its various algorithms for ensuring the satisfaction of the ADM Hamiltonian and momentum constraints, and its unique adaptive refinement algorithms based on hypergraph rewriting via Wolfram model evolution. Particular attention is paid to the seamless integration between Gravitas's symbolic and numerical subsystems, its ability to configure, run, analyze and visualize complex numerical relativity simulations and their outputs within a single notebook environment, and its capabilities for handling generic curvilinear coordinate systems and spacetimes with general (and often highly non-trivial) topologies using its specialized and highly efficient hypergraph-based numerical algorithms. We also provide illustrations of Gravitas's functionality for the visualization of hypergraph geometries and spacetime embedding diagrams, the ability for Gravitas's symbolic and numerical subsystems to be used in concert for the extraction of gravitational wave signals and other crucial simulation data, and Gravitas's in-built library of standard initial data, matter distributions and gauge conditions. We conclude by demonstrating how the numerical subsystem can be used to set up, run, visualize and analyze a standard yet nevertheless reasonably challenging numerical relativity test case: a binary black hole collision and merger within a vacuum spacetime (including the extraction of its outgoing gravitational wave profile).","sentences":["This is the second in a series of two articles introducing the Gravitas computational general relativity framework, in which we now focus upon the design and capabilities of Gravitas's numerical subsystem, including its ability to perform general 3+1 decompositions of spacetime via the ADM formalism, its support for the definition and construction of arbitrary Cauchy surfaces as initial data, its support for the definition and enforcement of arbitrary gauge and coordinate conditions, its various algorithms for ensuring the satisfaction of the ADM Hamiltonian and momentum constraints, and its unique adaptive refinement algorithms based on hypergraph rewriting via Wolfram model evolution.","Particular attention is paid to the seamless integration between Gravitas's symbolic and numerical subsystems, its ability to configure, run, analyze and visualize complex numerical relativity simulations and their outputs within a single notebook environment, and its capabilities for handling generic curvilinear coordinate systems and spacetimes with general (and often highly non-trivial) topologies using its specialized and highly efficient hypergraph-based numerical algorithms.","We also provide illustrations of Gravitas's functionality for the visualization of hypergraph geometries and spacetime embedding diagrams, the ability for Gravitas's symbolic and numerical subsystems to be used in concert for the extraction of gravitational wave signals and other crucial simulation data, and Gravitas's in-built library of standard initial data, matter distributions and gauge conditions.","We conclude by demonstrating how the numerical subsystem can be used to set up, run, visualize and analyze a standard yet nevertheless reasonably challenging numerical relativity test case: a binary black hole collision and merger within a vacuum spacetime (including the extraction of its outgoing gravitational wave profile)."],"url":"http://arxiv.org/abs/2401.14209v1","category":"gr-qc"}
{"created":"2024-01-25 14:45:44","title":"Quantum information recovery from black hole with projective measurement","abstract":"We studied the Hayden-Preskill thought experiment with the local projective measurement. Compared to the original model, the measurement is applied on the Hawking radiation that was emitted after throwing the quantum diary into the black hole. Within this setup, we explored various aspects of this model, including the information recovery from the black hole, the relation to the black hole final state proposal, the relation between the Yoshida-Kitaev protocol and Petz recovery map, the effects of the decoherence, and the quantum simulations of the decoding protocols. These aspects may provide us new insights into the non-perturbative nature of quantum black holes.","sentences":["We studied the Hayden-Preskill thought experiment with the local projective measurement.","Compared to the original model, the measurement is applied on the Hawking radiation that was emitted after throwing the quantum diary into the black hole.","Within this setup, we explored various aspects of this model, including the information recovery from the black hole, the relation to the black hole final state proposal, the relation between the Yoshida-Kitaev protocol and Petz recovery map, the effects of the decoherence, and the quantum simulations of the decoding protocols.","These aspects may provide us new insights into the non-perturbative nature of quantum black holes."],"url":"http://arxiv.org/abs/2401.14207v1","category":"gr-qc"}
{"created":"2024-01-25 14:40:58","title":"Exploiting Liver CT scans in Colorectal Carcinoma genomics mutation classification","abstract":"The liver is the most involved organ by distant metastasis in colon-rectal cancer (CRC) patients and it comes necessary to be aware of the mutational status of the lesions to correctly design the best individual treatment. So far, efforts have been made in order to develop non-invasive and real-time methods that permit the analysis of the whole tumor, using new artificial intelligence tools to analyze the tumor's image obtained by Computed Tomography (CT) scan. In order to address the current medical workflow, that is biopsy analysis-based, we propose the first DeepLearning-based exploration, to our knowledge, of such classification approach from the patient medical imaging. We propose i) a solid pipeline for managing undersized datasets of available CT scans and ii) a baseline study for genomics mutation diagnosis support for preemptive patient follow-up. Our method is able to identify CRC RAS mutation family from CT images with 0.73 F1 score.","sentences":["The liver is the most involved organ by distant metastasis in colon-rectal cancer (CRC) patients and it comes necessary to be aware of the mutational status of the lesions to correctly design the best individual treatment.","So far, efforts have been made in order to develop non-invasive and real-time methods that permit the analysis of the whole tumor, using new artificial intelligence tools to analyze the tumor's image obtained by Computed Tomography (CT) scan.","In order to address the current medical workflow, that is biopsy analysis-based, we propose the first DeepLearning-based exploration, to our knowledge, of such classification approach from the patient medical imaging.","We propose i) a solid pipeline for managing undersized datasets of available CT scans and ii) a baseline study for genomics mutation diagnosis support for preemptive patient follow-up.","Our method is able to identify CRC RAS mutation family from CT images with 0.73 F1 score."],"url":"http://arxiv.org/abs/2401.14206v1","category":"eess.IV"}
{"created":"2024-01-25 14:22:49","title":"Speeding up Fermionic Lattice Calculations with Photonic Accelerated Inverters","abstract":"Lattice field theory (LFT) is the standard non-perturbative method to perform numerical calculations of quantum field theory. However, the typical bottleneck of fermionic lattice calculations is the inversion of the Dirac matrix. This inversion is solved by iterative methods, like the conjugate gradient algorithm, where matrix-vector multiplications (MVMs) are the main operation. Photonic integrated circuits excel in performing quick and energy-efficient MVMs, but at the same time, they are known to have low accuracy. This can be overcome by using mixed precision methods. In this paper, we explore the idea of using photonic technology to fulfil the demand for computational power of fermionic lattice calculations. These methods have the potential to reduce computation costs by one order of magnitude. Because of the hybrid nature of these methods, we call these 'photonic accelerated inverters (PAIs)'.","sentences":["Lattice field theory (LFT) is the standard non-perturbative method to perform numerical calculations of quantum field theory.","However, the typical bottleneck of fermionic lattice calculations is the inversion of the Dirac matrix.","This inversion is solved by iterative methods, like the conjugate gradient algorithm, where matrix-vector multiplications (MVMs) are the main operation.","Photonic integrated circuits excel in performing quick and energy-efficient MVMs, but at the same time, they are known to have low accuracy.","This can be overcome by using mixed precision methods.","In this paper, we explore the idea of using photonic technology to fulfil the demand for computational power of fermionic lattice calculations.","These methods have the potential to reduce computation costs by one order of magnitude.","Because of the hybrid nature of these methods, we call these 'photonic accelerated inverters (PAIs)'."],"url":"http://arxiv.org/abs/2401.14200v1","category":"hep-lat"}
{"created":"2024-01-25 14:21:14","title":"MTRGL:Effective Temporal Correlation Discerning through Multi-modal Temporal Relational Graph Learning","abstract":"In this study, we explore the synergy of deep learning and financial market applications, focusing on pair trading. This market-neutral strategy is integral to quantitative finance and is apt for advanced deep-learning techniques. A pivotal challenge in pair trading is discerning temporal correlations among entities, necessitating the integration of diverse data modalities. Addressing this, we introduce a novel framework, Multi-modal Temporal Relation Graph Learning (MTRGL). MTRGL combines time series data and discrete features into a temporal graph and employs a memory-based temporal graph neural network. This approach reframes temporal correlation identification as a temporal graph link prediction task, which has shown empirical success. Our experiments on real-world datasets confirm the superior performance of MTRGL, emphasizing its promise in refining automated pair trading strategies.","sentences":["In this study, we explore the synergy of deep learning and financial market applications, focusing on pair trading.","This market-neutral strategy is integral to quantitative finance and is apt for advanced deep-learning techniques.","A pivotal challenge in pair trading is discerning temporal correlations among entities, necessitating the integration of diverse data modalities.","Addressing this, we introduce a novel framework, Multi-modal Temporal Relation Graph Learning (MTRGL).","MTRGL combines time series data and discrete features into a temporal graph and employs a memory-based temporal graph neural network.","This approach reframes temporal correlation identification as a temporal graph link prediction task, which has shown empirical success.","Our experiments on real-world datasets confirm the superior performance of MTRGL, emphasizing its promise in refining automated pair trading strategies."],"url":"http://arxiv.org/abs/2401.14199v1","category":"cs.LG"}
{"created":"2024-01-25 14:17:53","title":"DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence","abstract":"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.","sentences":["The rapid development of large language models has revolutionized code intelligence in software development.","However, the predominance of closed-source models has restricted extensive research and development.","To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens.","These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling.","Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5.","Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use."],"url":"http://arxiv.org/abs/2401.14196v1","category":"cs.SE"}
{"created":"2024-01-25 14:07:34","title":"Parameter-Efficient Conversational Recommender System as a Language Processing Task","abstract":"Conversational recommender systems (CRS) aim to recommend relevant items to users by eliciting user preference through natural language conversation. Prior work often utilizes external knowledge graphs for items' semantic information, a language model for dialogue generation, and a recommendation module for ranking relevant items. This combination of multiple components suffers from a cumbersome training process, and leads to semantic misalignment issues between dialogue generation and item recommendation. In this paper, we represent items in natural language and formulate CRS as a natural language processing task. Accordingly, we leverage the power of pre-trained language models to encode items, understand user intent via conversation, perform item recommendation through semantic matching, and generate dialogues. As a unified model, our PECRS (Parameter-Efficient CRS), can be optimized in a single stage, without relying on non-textual metadata such as a knowledge graph. Experiments on two benchmark CRS datasets, ReDial and INSPIRED, demonstrate the effectiveness of PECRS on recommendation and conversation. Our code is available at: https://github.com/Ravoxsg/efficient_unified_crs.","sentences":["Conversational recommender systems (CRS) aim to recommend relevant items to users by eliciting user preference through natural language conversation.","Prior work often utilizes external knowledge graphs for items' semantic information, a language model for dialogue generation, and a recommendation module for ranking relevant items.","This combination of multiple components suffers from a cumbersome training process, and leads to semantic misalignment issues between dialogue generation and item recommendation.","In this paper, we represent items in natural language and formulate CRS as a natural language processing task.","Accordingly, we leverage the power of pre-trained language models to encode items, understand user intent via conversation, perform item recommendation through semantic matching, and generate dialogues.","As a unified model, our PECRS (Parameter-Efficient CRS), can be optimized in a single stage, without relying on non-textual metadata such as a knowledge graph.","Experiments on two benchmark CRS datasets, ReDial and INSPIRED, demonstrate the effectiveness of PECRS on recommendation and conversation.","Our code is available at: https://github.com/Ravoxsg/efficient_unified_crs."],"url":"http://arxiv.org/abs/2401.14194v1","category":"cs.CL"}
{"created":"2024-01-25 14:03:15","title":"How Can Large Language Models Understand Spatial-Temporal Data?","abstract":"While Large Language Models (LLMs) dominate tasks like natural language processing and computer vision, harnessing their power for spatial-temporal forecasting remains challenging. The disparity between sequential text and complex spatial-temporal data hinders this application. To address this issue, this paper introduces STG-LLM, an innovative approach empowering LLMs for spatial-temporal forecasting. We tackle the data mismatch by proposing: 1) STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph data into concise tokens capturing both spatial and temporal relationships; 2) STG-Adapter: This minimalistic adapter, consisting of linear encoding and decoding layers, bridges the gap between tokenized data and LLM comprehension. By fine-tuning only a small set of parameters, it can effectively grasp the semantics of tokens generated by STG-Tokenizer, while preserving the original natural language understanding capabilities of LLMs. Extensive experiments on diverse spatial-temporal benchmark datasets show that STG-LLM successfully unlocks LLM potential for spatial-temporal forecasting. Remarkably, our approach achieves competitive performance on par with dedicated SOTA methods.","sentences":["While Large Language Models (LLMs) dominate tasks like natural language processing and computer vision, harnessing their power for spatial-temporal forecasting remains challenging.","The disparity between sequential text and complex spatial-temporal data hinders this application.","To address this issue, this paper introduces STG-LLM, an innovative approach empowering LLMs for spatial-temporal forecasting.","We tackle the data mismatch by proposing: 1) STG-Tokenizer:","This spatial-temporal graph tokenizer transforms intricate graph data into concise tokens capturing both spatial and temporal relationships; 2) STG-Adapter:","This minimalistic adapter, consisting of linear encoding and decoding layers, bridges the gap between tokenized data and LLM comprehension.","By fine-tuning only a small set of parameters, it can effectively grasp the semantics of tokens generated by STG-Tokenizer, while preserving the original natural language understanding capabilities of LLMs.","Extensive experiments on diverse spatial-temporal benchmark datasets show that STG-LLM successfully unlocks LLM potential for spatial-temporal forecasting.","Remarkably, our approach achieves competitive performance on par with dedicated SOTA methods."],"url":"http://arxiv.org/abs/2401.14192v1","category":"cs.LG"}
{"created":"2024-01-25 13:47:22","title":"TDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down Fusion","abstract":"Audio-visual speech separation has gained significant traction in recent years due to its potential applications in various fields such as speech recognition, diarization, scene analysis and assistive technologies. Designing a lightweight audio-visual speech separation network is important for low-latency applications, but existing methods often require higher computational costs and more parameters to achieve better separation performance. In this paper, we present an audio-visual speech separation model called Top-Down-Fusion Net (TDFNet), a state-of-the-art (SOTA) model for audio-visual speech separation, which builds upon the architecture of TDANet, an audio-only speech separation method. TDANet serves as the architectural foundation for the auditory and visual networks within TDFNet, offering an efficient model with fewer parameters. On the LRS2-2Mix dataset, TDFNet achieves a performance increase of up to 10\\% across all performance metrics compared with the previous SOTA method CTCNet. Remarkably, these results are achieved using fewer parameters and only 28\\% of the multiply-accumulate operations (MACs) of CTCNet. In essence, our method presents a highly effective and efficient solution to the challenges of speech separation within the audio-visual domain, making significant strides in harnessing visual information optimally.","sentences":["Audio-visual speech separation has gained significant traction in recent years due to its potential applications in various fields such as speech recognition, diarization, scene analysis and assistive technologies.","Designing a lightweight audio-visual speech separation network is important for low-latency applications, but existing methods often require higher computational costs and more parameters to achieve better separation performance.","In this paper, we present an audio-visual speech separation model called Top-Down-Fusion Net (TDFNet), a state-of-the-art (SOTA) model for audio-visual speech separation, which builds upon the architecture of TDANet, an audio-only speech separation method.","TDANet serves as the architectural foundation for the auditory and visual networks within TDFNet, offering an efficient model with fewer parameters.","On the LRS2-2Mix dataset, TDFNet achieves a performance increase of up to 10\\% across all performance metrics compared with the previous SOTA method CTCNet.","Remarkably, these results are achieved using fewer parameters and only 28\\% of the multiply-accumulate operations (MACs) of CTCNet.","In essence, our method presents a highly effective and efficient solution to the challenges of speech separation within the audio-visual domain, making significant strides in harnessing visual information optimally."],"url":"http://arxiv.org/abs/2401.14185v1","category":"cs.SD"}
{"created":"2024-01-25 13:46:21","title":"Friendly Attacks to Improve Channel Coding Reliability","abstract":"This paper introduces a novel approach called \"friendly attack\" aimed at enhancing the performance of error correction channel codes. Inspired by the concept of adversarial attacks, our method leverages the idea of introducing slight perturbations to the neural network input, resulting in a substantial impact on the network's performance. By introducing small perturbations to fixed-point modulated codewords before transmission, we effectively improve the decoder's performance without violating the input power constraint. The perturbation design is accomplished by a modified iterative fast gradient method. This study investigates various decoder architectures suitable for computing gradients to obtain the desired perturbations. Specifically, we consider belief propagation (BP) for LDPC codes; the error correcting code transformer, BP and neural BP (NBP) for polar codes, and neural BCJR for convolutional codes. We demonstrate that the proposed friendly attack method can improve the reliability across different channels, modulations, codes, and decoders. This method allows us to increase the reliability of communication with a legacy receiver by simply modifying the transmitted codeword appropriately.","sentences":["This paper introduces a novel approach called \"friendly attack\" aimed at enhancing the performance of error correction channel codes.","Inspired by the concept of adversarial attacks, our method leverages the idea of introducing slight perturbations to the neural network input, resulting in a substantial impact on the network's performance.","By introducing small perturbations to fixed-point modulated codewords before transmission, we effectively improve the decoder's performance without violating the input power constraint.","The perturbation design is accomplished by a modified iterative fast gradient method.","This study investigates various decoder architectures suitable for computing gradients to obtain the desired perturbations.","Specifically, we consider belief propagation (BP) for LDPC codes; the error correcting code transformer, BP and neural BP (NBP) for polar codes, and neural BCJR for convolutional codes.","We demonstrate that the proposed friendly attack method can improve the reliability across different channels, modulations, codes, and decoders.","This method allows us to increase the reliability of communication with a legacy receiver by simply modifying the transmitted codeword appropriately."],"url":"http://arxiv.org/abs/2401.14184v1","category":"cs.IT"}
{"created":"2024-01-25 13:41:34","title":"Deep Neural Networks as Variational Solutions for Correlated Open Quantum Systems","abstract":"In this work we apply deep neural networks to find the non-equilibrium steady state solution to correlated open quantum many-body systems. Motivated by the ongoing search to find more powerful representations of (mixed) quantum states, we design a simple prototypical convolutional neural network and show that parametrizing the density matrix directly with more powerful models can yield better variational ansatz functions and improve upon results reached by neural density operator based on the restricted Boltzmann machine. Hereby we give up the explicit restriction to positive semi-definite density matrices. However, this is fulfilled again to good approximation by optimizing the parameters. The great advantage of this approach is that it opens up the possibility of exploring more complex network architectures that can be tailored to specific physical properties. We show how translation invariance can be enforced effortlessly and reach better results with fewer parameters. We present results for the dissipative one-dimensional transverse-field Ising model and a two-dimensional dissipative Heisenberg model compared to exact values.","sentences":["In this work we apply deep neural networks to find the non-equilibrium steady state solution to correlated open quantum many-body systems.","Motivated by the ongoing search to find more powerful representations of (mixed) quantum states, we design a simple prototypical convolutional neural network and show that parametrizing the density matrix directly with more powerful models can yield better variational ansatz functions and improve upon results reached by neural density operator based on the restricted Boltzmann machine.","Hereby we give up the explicit restriction to positive semi-definite density matrices.","However, this is fulfilled again to good approximation by optimizing the parameters.","The great advantage of this approach is that it opens up the possibility of exploring more complex network architectures that can be tailored to specific physical properties.","We show how translation invariance can be enforced effortlessly and reach better results with fewer parameters.","We present results for the dissipative one-dimensional transverse-field Ising model and a two-dimensional dissipative Heisenberg model compared to exact values."],"url":"http://arxiv.org/abs/2401.14179v1","category":"quant-ph"}
{"created":"2024-01-25 13:39:54","title":"Copilot Refinement: Addressing Code Smells in Copilot-Generated Python Code","abstract":"As one of the most popular dynamic languages, Python experiences a decrease in readability and maintainability when code smells are present. Recent advancements in Large Language Models have sparked growing interest in AI-enabled tools for both code generation and refactoring. GitHub Copilot is one such tool that has gained widespread usage. Copilot Chat, released on September 2023, functions as an interactive tool aims at facilitating natural language-powered coding. However, limited attention has been given to understanding code smells in Copilot-generated Python code and Copilot's ability to fix the code smells it generates. To this end, we built a dataset comprising 102 code smells in Copilot-generated Python code. Our aim is to first explore the occurrence of code smells in Copilot-generated Python code and then evaluate the effectiveness of Copilot in fixing these code smells employing different prompts. The results show that 8 out of 10 types of Python smells can be detected in Copilot-generated Python code, among which Multiply-Nested Container is the most common one. For these code smells, Copilot Chat achieves a highest fixing rate of 87.1%, showing promise in fixing Python code smells generated by Copilot itself. Besides, the effectiveness of Copilot Chat in fixing these smells can be improved with the provision of more detailed prompts. However, using Copilot Chat to fix these smells might introduce new code smells.","sentences":["As one of the most popular dynamic languages, Python experiences a decrease in readability and maintainability when code smells are present.","Recent advancements in Large Language Models have sparked growing interest in AI-enabled tools for both code generation and refactoring.","GitHub Copilot is one such tool that has gained widespread usage.","Copilot Chat, released on September 2023, functions as an interactive tool aims at facilitating natural language-powered coding.","However, limited attention has been given to understanding code smells in Copilot-generated Python code and Copilot's ability to fix the code smells it generates.","To this end, we built a dataset comprising 102 code smells in Copilot-generated Python code.","Our aim is to first explore the occurrence of code smells in Copilot-generated Python code and then evaluate the effectiveness of Copilot in fixing these code smells employing different prompts.","The results show that 8 out of 10 types of Python smells can be detected in Copilot-generated Python code, among which Multiply-Nested Container is the most common one.","For these code smells, Copilot Chat achieves a highest fixing rate of 87.1%, showing promise in fixing Python code smells generated by Copilot itself.","Besides, the effectiveness of Copilot Chat in fixing these smells can be improved with the provision of more detailed prompts.","However, using Copilot Chat to fix these smells might introduce new code smells."],"url":"http://arxiv.org/abs/2401.14176v1","category":"cs.SE"}
{"created":"2024-01-25 13:34:33","title":"The Boundaries of Tractability in Hierarchical Task Network Planning","abstract":"We study the complexity-theoretic boundaries of tractability for three classical problems in the context of Hierarchical Task Network Planning: the validation of a provided plan, whether an executable plan exists, and whether a given state can be reached by some plan. We show that all three problems can be solved in polynomial time on primitive task networks of constant partial order width (and a generalization thereof), whereas for the latter two problems this holds only under a provably necessary restriction to the state space. Next, we obtain an algorithmic meta-theorem along with corresponding lower bounds to identify tight conditions under which general polynomial-time solvability results can be lifted from primitive to general task networks. Finally, we enrich our investigation by analyzing the parameterized complexity of the three considered problems, and show that (1) fixed-parameter tractability for all three problems can be achieved by replacing the partial order width with the vertex cover number of the network as the parameter, and (2) other classical graph-theoretic parameters of the network (including treewidth, treedepth, and the aforementioned partial order width) do not yield fixed-parameter tractability for any of the three problems.","sentences":["We study the complexity-theoretic boundaries of tractability for three classical problems in the context of Hierarchical Task Network Planning: the validation of a provided plan, whether an executable plan exists, and whether a given state can be reached by some plan.","We show that all three problems can be solved in polynomial time on primitive task networks of constant partial order width (and a generalization thereof), whereas for the latter two problems this holds only under a provably necessary restriction to the state space.","Next, we obtain an algorithmic meta-theorem along with corresponding lower bounds to identify tight conditions under which general polynomial-time solvability results can be lifted from primitive to general task networks.","Finally, we enrich our investigation by analyzing the parameterized complexity of the three considered problems, and show that (1) fixed-parameter tractability for all three problems can be achieved by replacing the partial order width with the vertex cover number of the network as the parameter, and (2) other classical graph-theoretic parameters of the network (including treewidth, treedepth, and the aforementioned partial order width) do not yield fixed-parameter tractability for any of the three problems."],"url":"http://arxiv.org/abs/2401.14174v1","category":"cs.CC"}
{"created":"2024-01-25 13:28:53","title":"Predicting Hypoxia in Brain Tumors from Multiparametric MRI","abstract":"This research paper presents a novel approach to the prediction of hypoxia in brain tumors, using multi-parametric Magnetic Resonance Imaging (MRI). Hypoxia, a condition characterized by low oxygen levels, is a common feature of malignant brain tumors associated with poor prognosis. Fluoromisonidazole Positron Emission Tomography (FMISO PET) is a well-established method for detecting hypoxia in vivo, but it is expensive and not widely available. Our study proposes the use of MRI, a more accessible and cost-effective imaging modality, to predict FMISO PET signals. We investigate deep learning models (DL) trained on the ACRIN 6684 dataset, a resource that contains paired MRI and FMISO PET images from patients with brain tumors. Our trained models effectively learn the complex relationships between the MRI features and the corresponding FMISO PET signals, thereby enabling the prediction of hypoxia from MRI scans alone. The results show a strong correlation between the predicted and actual FMISO PET signals, with an overall PSNR score above 29.6 and a SSIM score greater than 0.94, confirming MRI as a promising option for hypoxia prediction in brain tumors. This approach could significantly improve the accessibility of hypoxia detection in clinical settings, with the potential for more timely and targeted treatments.","sentences":["This research paper presents a novel approach to the prediction of hypoxia in brain tumors, using multi-parametric Magnetic Resonance Imaging (MRI).","Hypoxia, a condition characterized by low oxygen levels, is a common feature of malignant brain tumors associated with poor prognosis.","Fluoromisonidazole Positron Emission Tomography (FMISO PET) is a well-established method for detecting hypoxia in vivo, but it is expensive and not widely available.","Our study proposes the use of MRI, a more accessible and cost-effective imaging modality, to predict FMISO PET signals.","We investigate deep learning models (DL) trained on the ACRIN 6684 dataset, a resource that contains paired MRI and FMISO PET images from patients with brain tumors.","Our trained models effectively learn the complex relationships between the MRI features and the corresponding FMISO PET signals, thereby enabling the prediction of hypoxia from MRI scans alone.","The results show a strong correlation between the predicted and actual FMISO PET signals, with an overall PSNR score above 29.6 and a SSIM score greater than 0.94, confirming MRI as a promising option for hypoxia prediction in brain tumors.","This approach could significantly improve the accessibility of hypoxia detection in clinical settings, with the potential for more timely and targeted treatments."],"url":"http://arxiv.org/abs/2401.14171v1","category":"eess.IV"}
{"created":"2024-01-25 13:27:03","title":"Vivim: a Video Vision Mamba for Medical Video Object Segmentation","abstract":"Traditional convolutional neural networks have a limited receptive field while transformer-based networks are mediocre in constructing long-term dependency from the perspective of computational complexity. Such the bottleneck poses a significant challenge when processing long video sequences in video analysis tasks. Very recently, the state space models (SSMs) with efficient hardware-aware designs, famous by Mamba, have exhibited impressive achievements in long sequence modeling, which facilitates the development of deep neural networks on many vision tasks. To better capture available cues in video frames, this paper presents a generic Video Vision Mamba-based framework for medical video object segmentation tasks, named Vivim. Our Vivim can effectively compress the long-term spatiotemporal representation into sequences at varying scales by our designed Temporal Mamba Block. Compared to existing video-level Transformer-based methods, our model maintains excellent segmentation results with better speed performance. Extensive experiments on the breast US dataset demonstrate the effectiveness and efficiency of our Vivim. The code for Vivim is available at: https://github.com/scott-yjyang/Vivim.","sentences":["Traditional convolutional neural networks have a limited receptive field while transformer-based networks are mediocre in constructing long-term dependency from the perspective of computational complexity.","Such the bottleneck poses a significant challenge when processing long video sequences in video analysis tasks.","Very recently, the state space models (SSMs) with efficient hardware-aware designs, famous by Mamba, have exhibited impressive achievements in long sequence modeling, which facilitates the development of deep neural networks on many vision tasks.","To better capture available cues in video frames, this paper presents a generic Video Vision Mamba-based framework for medical video object segmentation tasks, named Vivim.","Our Vivim can effectively compress the long-term spatiotemporal representation into sequences at varying scales by our designed Temporal Mamba Block.","Compared to existing video-level Transformer-based methods, our model maintains excellent segmentation results with better speed performance.","Extensive experiments on the breast US dataset demonstrate the effectiveness and efficiency of our Vivim.","The code for Vivim is available at: https://github.com/scott-yjyang/Vivim."],"url":"http://arxiv.org/abs/2401.14168v1","category":"cs.CV"}
{"created":"2024-01-25 13:20:47","title":"BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction","abstract":"As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models (PLMs), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives. While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns. From the perspective of distribution analyses, we disclose that the intrinsic issues behind the phenomenon are the over-multitudinous conceptual knowledge contained in PLMs and the abridged knowledge for target downstream domains, which jointly result in that PLMs mis-locate the knowledge distributions corresponding to the target domains in the universal knowledge embedding space. To this end, we intuitively explore to approximate the unabridged target domains of downstream tasks in a debiased manner, and then abstract such domains to generate discriminative prompts, thereby providing the de-ambiguous guidance for PLMs. Guided by such an intuition, we propose a simple yet effective approach, namely BayesPrompt, to learn prompts that contain the domain discriminative information against the interference from domain-irrelevant knowledge. BayesPrompt primitively leverages known distributions to approximate the debiased factual distributions of target domains and further uniformly samples certain representative features from the approximated distributions to generate the ultimate prompts for PLMs. We provide theoretical insights with the connection to domain adaptation. Empirically, our method achieves state-of-the-art performance on benchmarks.","sentences":["As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models (PLMs), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives.","While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns.","From the perspective of distribution analyses, we disclose that the intrinsic issues behind the phenomenon are the over-multitudinous conceptual knowledge contained in PLMs and the abridged knowledge for target downstream domains, which jointly result in that PLMs mis-locate the knowledge distributions corresponding to the target domains in the universal knowledge embedding space.","To this end, we intuitively explore to approximate the unabridged target domains of downstream tasks in a debiased manner, and then abstract such domains to generate discriminative prompts, thereby providing the de-ambiguous guidance for PLMs.","Guided by such an intuition, we propose a simple yet effective approach, namely BayesPrompt, to learn prompts that contain the domain discriminative information against the interference from domain-irrelevant knowledge.","BayesPrompt primitively leverages known distributions to approximate the debiased factual distributions of target domains and further uniformly samples certain representative features from the approximated distributions to generate the ultimate prompts for PLMs.","We provide theoretical insights with the connection to domain adaptation.","Empirically, our method achieves state-of-the-art performance on benchmarks."],"url":"http://arxiv.org/abs/2401.14166v1","category":"cs.CL"}
{"created":"2024-01-25 13:12:46","title":"A Mathematical Theory of Semantic Communication: Overview","abstract":"Semantic communication initiates a new direction for future communication. In this paper, we aim to establish a systematic framework of semantic information theory (SIT). First, we propose a semantic communication model and define the synonymous mapping to indicate the critical relationship between semantic information and syntactic information. Based on this core concept, we introduce the measures of semantic information, such as semantic entropy $H_s(\\tilde{U})$, up/down semantic mutual information $I^s(\\tilde{X};\\tilde{Y})$ $(I_s(\\tilde{X};\\tilde{Y}))$, semantic capacity $C_s=\\max_{p(x)}I^s(\\tilde{X};\\tilde{Y})$, and semantic rate-distortion function $R_s(D)=\\min_{p(\\hat{x}|x):\\mathbb{E}d_s(\\tilde{x},\\hat{\\tilde{x}})\\leq D}I_s(\\tilde{X};\\hat{\\tilde{X}})$. Furthermore, we prove three coding theorems of SIT, that is, the semantic source coding theorem, semantic channel coding theorem, and semantic rate-distortion coding theorem. We find that the limits of information theory are extended by using synonymous mapping, that is, $H_s(\\tilde{U})\\leq H(U)$, $C_s\\geq C$ and $R_s(D)\\leq R(D)$. All these works composite the basis of semantic information theory. In summary, the theoretic framework proposed in this paper is a natural extension of classic information theory and may reveal great performance potential for future communication.","sentences":["Semantic communication initiates a new direction for future communication.","In this paper, we aim to establish a systematic framework of semantic information theory (SIT).","First, we propose a semantic communication model and define the synonymous mapping to indicate the critical relationship between semantic information and syntactic information.","Based on this core concept, we introduce the measures of semantic information, such as semantic entropy $H_s(\\tilde{U})$, up/down semantic mutual information $I^s(\\tilde{X};\\tilde{Y})$ $(I_s(\\tilde{X};\\tilde{Y}))$, semantic capacity $C_s=\\max_{p(x)}I^s(\\tilde{X};\\tilde{Y})$, and semantic rate-distortion function $R_s(D)=\\min_{p(\\hat{x}|x):\\mathbb{E}d_s(\\tilde{x},\\hat{\\tilde{x}})\\leq D}I_s(\\tilde{X};\\hat{\\tilde{X}})$.","Furthermore, we prove three coding theorems of SIT, that is, the semantic source coding theorem, semantic channel coding theorem, and semantic rate-distortion coding theorem.","We find that the limits of information theory are extended by using synonymous mapping, that is, $H_s(\\tilde{U})\\leq H(U)$, $C_s\\geq C$ and $R_s(D)\\leq R(D)$. All these works composite the basis of semantic information theory.","In summary, the theoretic framework proposed in this paper is a natural extension of classic information theory and may reveal great performance potential for future communication."],"url":"http://arxiv.org/abs/2401.14160v1","category":"cs.IT"}
{"created":"2024-01-25 13:12:09","title":"Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks","abstract":"We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.","sentences":["We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM).","This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models.","As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline.","For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything.","Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis.","Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models."],"url":"http://arxiv.org/abs/2401.14159v1","category":"cs.CV"}
{"created":"2024-01-25 13:07:34","title":"Alleviating Structural Distribution Shift in Graph Anomaly Detection","abstract":"Graph anomaly detection (GAD) is a challenging binary classification problem due to its different structural distribution between anomalies and normal nodes -- abnormal nodes are a minority, therefore holding high heterophily and low homophily compared to normal nodes. Furthermore, due to various time factors and the annotation preferences of human experts, the heterophily and homophily can change across training and testing data, which is called structural distribution shift (SDS) in this paper. The mainstream methods are built on graph neural networks (GNNs), benefiting the classification of normals from aggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and suffering from poor generalization.   This work solves the problem from a feature view. We observe that the degree of SDS varies between anomalies and normal nodes. Hence to address the issue, the key lies in resisting high heterophily for anomalies meanwhile benefiting the learning of normals from homophily. We tease out the anomaly features on which we constrain to mitigate the effect of heterophilous neighbors and make them invariant. We term our proposed framework as Graph Decomposition Network (GDN). Extensive experiments are conducted on two benchmark datasets, and the proposed framework achieves a remarkable performance boost in GAD, especially in an SDS environment where anomalies have largely different structural distribution across training and testing environments. Codes are open-sourced in https://github.com/blacksingular/wsdm_GDN.","sentences":["Graph anomaly detection (GAD) is a challenging binary classification problem due to its different structural distribution between anomalies and normal nodes -- abnormal nodes are a minority, therefore holding high heterophily and low homophily compared to normal nodes.","Furthermore, due to various time factors and the annotation preferences of human experts, the heterophily and homophily can change across training and testing data, which is called structural distribution shift (SDS) in this paper.","The mainstream methods are built on graph neural networks (GNNs), benefiting the classification of normals from aggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and suffering from poor generalization.   ","This work solves the problem from a feature view.","We observe that the degree of SDS varies between anomalies and normal nodes.","Hence to address the issue, the key lies in resisting high heterophily for anomalies meanwhile benefiting the learning of normals from homophily.","We tease out the anomaly features on which we constrain to mitigate the effect of heterophilous neighbors and make them invariant.","We term our proposed framework as Graph Decomposition Network (GDN).","Extensive experiments are conducted on two benchmark datasets, and the proposed framework achieves a remarkable performance boost in GAD, especially in an SDS environment where anomalies have largely different structural distribution across training and testing environments.","Codes are open-sourced in https://github.com/blacksingular/wsdm_GDN."],"url":"http://arxiv.org/abs/2401.14155v1","category":"cs.LG"}
{"created":"2024-01-25 13:05:06","title":"Agent-based Simulation with Netlogo to Evaluate AmI Scenarios","abstract":"In this paper an agent-based simulation is developed in order to evaluate an AmI scenario based on agents. Many AmI applications are implemented through agents but they are not compared to any other existing alternative in order to evaluate the relative benefits of using them. The proposal simulation environment developed in Netlogo analyse such benefits using two evaluation criteria: First, measuring agent satisfaction of different types of desires along the execution. Second, measuring time savings obtained through a correct use of context information.   So, here, a previously suggested agent architecture, an ontology and a 12-steps protocol to provide AmI services in airports, is evaluated using a NetLogo simulation environment. The present work uses a NetLogo model considering scalability problems of this application domain but using FIPA and BDI extensions to be coherent with our previous works and our previous JADE implementation of them.   The NetLogo model presented simulates an airport with agent users passing through several zones located in a specific order in a map: passport controls, check-in counters of airline companies, boarding gates, different types of shopping. Although initial data in simulations are generated randomly, and the model is just an approximation of real-world airports, the definition of this case of use of Ambient Intelligence through NetLogo agents opens an interesting way to evaluate the benefits of using Ambient Intelligence, which is a significant contribution to the final development of them.","sentences":["In this paper an agent-based simulation is developed in order to evaluate an AmI scenario based on agents.","Many AmI applications are implemented through agents but they are not compared to any other existing alternative in order to evaluate the relative benefits of using them.","The proposal simulation environment developed in Netlogo analyse such benefits using two evaluation criteria: First, measuring agent satisfaction of different types of desires along the execution.","Second, measuring time savings obtained through a correct use of context information.   ","So, here, a previously suggested agent architecture, an ontology and a 12-steps protocol to provide AmI services in airports, is evaluated using a NetLogo simulation environment.","The present work uses a NetLogo model considering scalability problems of this application domain but using FIPA and BDI extensions to be coherent with our previous works and our previous JADE implementation of them.   ","The NetLogo model presented simulates an airport with agent users passing through several zones located in a specific order in a map: passport controls, check-in counters of airline companies, boarding gates, different types of shopping.","Although initial data in simulations are generated randomly, and the model is just an approximation of real-world airports, the definition of this case of use of Ambient Intelligence through NetLogo agents opens an interesting way to evaluate the benefits of using Ambient Intelligence, which is a significant contribution to the final development of them."],"url":"http://arxiv.org/abs/2401.14153v1","category":"cs.AI"}
{"created":"2024-01-25 13:03:20","title":"True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning","abstract":"Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the actor and critic share one frozen LLM equipped with low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in both classical decision-making environment, Overcooked, and simulated household environment, VirtualHome. ii) Benefiting from LLMs' open-vocabulary feature, TWOSOME shows superior generalization ability to unseen tasks. iii) Under our framework, there is no significant loss of the LLMs' original ability during online PPO finetuning.","sentences":["Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments.","On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations.","To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments.","Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies.","Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles.","Finally, we design a novel parameter-efficient training architecture where the actor and critic share one frozen LLM equipped with low-rank adapters (LoRA) updated by PPO.","We conduct extensive experiments to evaluate TWOSOME.","i) TWOSOME exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in both classical decision-making environment, Overcooked, and simulated household environment, VirtualHome. ii) Benefiting from LLMs' open-vocabulary feature, TWOSOME shows superior generalization ability to unseen tasks.","iii) Under our framework, there is no significant loss of the LLMs' original ability during online PPO finetuning."],"url":"http://arxiv.org/abs/2401.14151v1","category":"cs.LG"}
{"created":"2024-01-25 13:02:39","title":"Polarized and bright telecom C-band single-photon source from InP-based quantum dots coupled to elliptical Bragg gratings","abstract":"Bright, polarized, and high-purity single-photon sources in telecom wavelengths are crucial components in long-distance quantum communication, optical quantum computation and quantum networks. Semiconductor InAs/InP quantum dots (QDs) combined with photonic cavities provide a competitive path leading to optimal single-photon sources in this range. Here, we demonstrate a bright and polarized single-photon source operating in the telecom C-band based on an elliptical Bragg grating (EBG) cavity. With a significant Purcell enhancement of 5.25$\\pm$0.05, the device achieves a polarization ratio of 0.986, single-photon purity of g^2 (0)=0.078$\\pm$0.016 and single-polarized photon collection efficiency of ~ 24% at the first lens (NA=0.65) without blinking. These findings suggest that C-band QD-based single-photon sources are potential candidates for advancing quantum communication.","sentences":["Bright, polarized, and high-purity single-photon sources in telecom wavelengths are crucial components in long-distance quantum communication, optical quantum computation and quantum networks.","Semiconductor InAs/InP quantum dots (QDs) combined with photonic cavities provide a competitive path leading to optimal single-photon sources in this range.","Here, we demonstrate a bright and polarized single-photon source operating in the telecom C-band based on an elliptical Bragg grating (EBG) cavity.","With a significant Purcell enhancement of 5.25$\\pm$0.05, the device achieves a polarization ratio of 0.986, single-photon purity of g^2 (0)=0.078$\\pm$0.016 and single-polarized photon collection efficiency of ~ 24% at the first lens (NA=0.65) without blinking.","These findings suggest that C-band QD-based single-photon sources are potential candidates for advancing quantum communication."],"url":"http://arxiv.org/abs/2401.14150v1","category":"quant-ph"}
{"created":"2024-01-25 12:59:13","title":"Developing a High-Performance Process Mining Library with Java and Python Bindings in Rust","abstract":"The most commonly used open-source process mining software tools today are ProM and PM4Py, written in Java and Python, respectively. Such high-level, often interpreted, programming languages trade off performance with memory safety and ease-of-use. In contrast, traditional compiled languages, like C or C++, can achieve top performance but often suffer from instability related to unsafe memory management. Lately, Rust emerged as a highly performant, compiled programming language with inherent memory safety. In this paper, we describe our approach to developing a shared process mining library in Rust with bindings to both Java and Python, allowing full integration into the existing ecosystems, like ProM and PM4Py. By facilitating interoperability, our methodology enables researchers or industry to develop novel algorithms in Rust once and make them accessible to the entire community while also achieving superior performance.","sentences":["The most commonly used open-source process mining software tools today are ProM and PM4Py, written in Java and Python, respectively.","Such high-level, often interpreted, programming languages trade off performance with memory safety and ease-of-use.","In contrast, traditional compiled languages, like C or C++, can achieve top performance but often suffer from instability related to unsafe memory management.","Lately, Rust emerged as a highly performant, compiled programming language with inherent memory safety.","In this paper, we describe our approach to developing a shared process mining library in Rust with bindings to both Java and Python, allowing full integration into the existing ecosystems, like ProM and PM4Py.","By facilitating interoperability, our methodology enables researchers or industry to develop novel algorithms in Rust once and make them accessible to the entire community while also achieving superior performance."],"url":"http://arxiv.org/abs/2401.14149v1","category":"cs.SE"}
{"created":"2024-01-25 12:55:48","title":"LanDA: Language-Guided Multi-Source Domain Adaptation","abstract":"Multi-Source Domain Adaptation (MSDA) aims to mitigate changes in data distribution when transferring knowledge from multiple labeled source domains to an unlabeled target domain. However, existing MSDA techniques assume target domain images are available, yet overlook image-rich semantic information. Consequently, an open question is whether MSDA can be guided solely by textual cues in the absence of target domain images. By employing a multimodal model with a joint image and language embedding space, we propose a novel language-guided MSDA approach, termed LanDA, based on optimal transfer theory, which facilitates the transfer of multiple source domains to a new target domain, requiring only a textual description of the target domain without needing even a single target domain image, while retaining task-relevant information. We present extensive experiments across different transfer scenarios using a suite of relevant benchmarks, demonstrating that LanDA outperforms standard fine-tuning and ensemble approaches in both target and source domains.","sentences":["Multi-Source Domain Adaptation (MSDA) aims to mitigate changes in data distribution when transferring knowledge from multiple labeled source domains to an unlabeled target domain.","However, existing MSDA techniques assume target domain images are available, yet overlook image-rich semantic information.","Consequently, an open question is whether MSDA can be guided solely by textual cues in the absence of target domain images.","By employing a multimodal model with a joint image and language embedding space, we propose a novel language-guided MSDA approach, termed LanDA, based on optimal transfer theory, which facilitates the transfer of multiple source domains to a new target domain, requiring only a textual description of the target domain without needing even a single target domain image, while retaining task-relevant information.","We present extensive experiments across different transfer scenarios using a suite of relevant benchmarks, demonstrating that LanDA outperforms standard fine-tuning and ensemble approaches in both target and source domains."],"url":"http://arxiv.org/abs/2401.14148v1","category":"cs.CV"}
{"created":"2024-01-25 12:53:40","title":"Concept: Dynamic Risk Assessment for AI-Controlled Robotic Systems","abstract":"AI-controlled robotic systems pose a risk to human workers and the environment. Classical risk assessment methods cannot adequately describe such black box systems. Therefore, new methods for a dynamic risk assessment of such AI-controlled systems are required. In this paper, we introduce the concept of a new dynamic risk assessment approach for AI-controlled robotic systems. The approach pipelines five blocks: (i) a Data Logging that logs the data of the given simulation, (ii) a Skill Detection that automatically detects the executed skills with a deep learning technique, (iii) a Behavioral Analysis that creates the behavioral profile of the robotic systems, (iv) a Risk Model Generation that automatically transforms the behavioral profile and risk data containing the failure probabilities of robotic hardware components into advanced hybrid risk models, and (v) Risk Model Solvers for the numerical evaluation of the generated hybrid risk models.   Keywords: Dynamic Risk Assessment, Hybrid Risk Models, M2M Transformation, ROS, AI-Controlled Robotic Systems, Deep Learning, Reinforcement Learning","sentences":["AI-controlled robotic systems pose a risk to human workers and the environment.","Classical risk assessment methods cannot adequately describe such black box systems.","Therefore, new methods for a dynamic risk assessment of such AI-controlled systems are required.","In this paper, we introduce the concept of a new dynamic risk assessment approach for AI-controlled robotic systems.","The approach pipelines five blocks: (i) a Data Logging that logs the data of the given simulation, (ii) a Skill Detection that automatically detects the executed skills with a deep learning technique, (iii) a Behavioral Analysis that creates the behavioral profile of the robotic systems, (iv) a Risk Model Generation that automatically transforms the behavioral profile and risk data containing the failure probabilities of robotic hardware components into advanced hybrid risk models, and (v) Risk Model Solvers for the numerical evaluation of the generated hybrid risk models.   ","Keywords: Dynamic Risk Assessment, Hybrid Risk Models, M2M Transformation, ROS, AI-Controlled Robotic Systems, Deep Learning, Reinforcement Learning"],"url":"http://arxiv.org/abs/2401.14147v1","category":"cs.RO"}
{"created":"2024-01-25 12:49:33","title":"Mathematical Tri-State Model for Bee Shimmering Propagation Dynamics","abstract":"Bees undergo a self-organised process known as shimmering, where they form emergent patterns when they interact with each other on the nest surface as a defence mechanism in response to predator attacks. Many experimental studies have empirically investigated how the transfer of information to neighbouring bees propagates in various shimmering processes by measuring shimmering wave strength. However, there is no analytical modelling of the collective defence mechanism in nature. Here we introduce the first analytical tri-state Inactive-Active-Relapse (IAR) model to formulate the intrinsic process of bee shimmering. The major shimmering behaviour is shown to emerge under theoretical conditions which is demonstrated numerically and visually by simulating 1,000,000 bee agents, while the number of agents is scalable. Furthermore, we elaborate on these mathematical results to construct a wave strength function to demonstrate the accuracy of shimmering dynamics. The constructed wave strength function can be adapted to peak between 50-150ms which supports the experimental studies. Our results provide a foundation for further theoretical understanding of bee shimmering wave dynamics and could serve as inspiration for modelling other self-organised phenomena across scientific applications.","sentences":["Bees undergo a self-organised process known as shimmering, where they form emergent patterns when they interact with each other on the nest surface as a defence mechanism in response to predator attacks.","Many experimental studies have empirically investigated how the transfer of information to neighbouring bees propagates in various shimmering processes by measuring shimmering wave strength.","However, there is no analytical modelling of the collective defence mechanism in nature.","Here we introduce the first analytical tri-state Inactive-Active-Relapse (IAR) model to formulate the intrinsic process of bee shimmering.","The major shimmering behaviour is shown to emerge under theoretical conditions which is demonstrated numerically and visually by simulating 1,000,000 bee agents, while the number of agents is scalable.","Furthermore, we elaborate on these mathematical results to construct a wave strength function to demonstrate the accuracy of shimmering dynamics.","The constructed wave strength function can be adapted to peak between 50-150ms which supports the experimental studies.","Our results provide a foundation for further theoretical understanding of bee shimmering wave dynamics and could serve as inspiration for modelling other self-organised phenomena across scientific applications."],"url":"http://arxiv.org/abs/2401.14145v1","category":"nlin.AO"}
{"created":"2024-01-25 12:46:37","title":"Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations","abstract":"Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., \"yellow breast\") does not help correct highly correlated concepts (e.g., \"yellow belly\"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label \"Kentucky Warbler\" and a concept \"black bill\", what is the probability that the model correctly predicts another concept \"black crown\"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural networks to define the joint energy of candidate (input, concept, class) tuples. With such a unified interface, prediction, concept correction, and conditional dependency quantification are then represented as conditional probabilities, which are generated by composing different energy functions. Our ECBMs address both limitations of existing CBMs, providing higher accuracy and richer concept interpretations. Empirical results show that our approach outperforms the state-of-the-art on real-world datasets.","sentences":["Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models.","They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts.","However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., \"yellow breast\") does not help correct highly correlated concepts (e.g., \"yellow belly\"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label \"Kentucky Warbler\" and a concept \"black bill\", what is the probability that the model correctly predicts another concept \"black crown\"), therefore failing to provide deeper insight into how a black-box model works.","In response to these limitations, we propose Energy-based Concept Bottleneck Models (ECBMs).","Our ECBMs use a set of neural networks to define the joint energy of candidate (input, concept, class) tuples.","With such a unified interface, prediction, concept correction, and conditional dependency quantification are then represented as conditional probabilities, which are generated by composing different energy functions.","Our ECBMs address both limitations of existing CBMs, providing higher accuracy and richer concept interpretations.","Empirical results show that our approach outperforms the state-of-the-art on real-world datasets."],"url":"http://arxiv.org/abs/2401.14142v1","category":"cs.CV"}
{"created":"2024-01-25 12:46:04","title":"Exploring the Distinctive Tweeting Patterns of Toxic Twitter Users","abstract":"In the pursuit of bolstering user safety, social media platforms deploy active moderation strategies, including content removal and user suspension. These measures target users engaged in discussions marked by hate speech or toxicity, often linked to specific keywords or hashtags. Nonetheless, the increasing prevalence of toxicity indicates that certain users adeptly circumvent these measures. This study examines consistently toxic users on Twitter (rebranded as X) Rather than relying on traditional methods based on specific topics or hashtags, we employ a novel approach based on patterns of toxic tweets, yielding deeper insights into their behavior. We analyzed 38 million tweets from the timelines of 12,148 Twitter users and identified the top 1,457 users who consistently exhibit toxic behavior, relying on metrics like the Gini index and Toxicity score. By comparing their posting patterns to those of non-consistently toxic users, we have uncovered distinctive temporal patterns, including contiguous activity spans, inter-tweet intervals (referred to as 'Burstiness'), and churn analysis. These findings provide strong evidence for the existence of a unique tweeting pattern associated with toxic behavior on Twitter. Crucially, our methodology transcends Twitter and can be adapted to various social media platforms, facilitating the identification of consistently toxic users based on their posting behavior. This research contributes to ongoing efforts to combat online toxicity and offers insights for refining moderation strategies in the digital realm. We are committed to open research and will provide our code and data to the research community.","sentences":["In the pursuit of bolstering user safety, social media platforms deploy active moderation strategies, including content removal and user suspension.","These measures target users engaged in discussions marked by hate speech or toxicity, often linked to specific keywords or hashtags.","Nonetheless, the increasing prevalence of toxicity indicates that certain users adeptly circumvent these measures.","This study examines consistently toxic users on Twitter (rebranded as X) Rather than relying on traditional methods based on specific topics or hashtags, we employ a novel approach based on patterns of toxic tweets, yielding deeper insights into their behavior.","We analyzed 38 million tweets from the timelines of 12,148 Twitter users and identified the top 1,457 users who consistently exhibit toxic behavior, relying on metrics like the Gini index and Toxicity score.","By comparing their posting patterns to those of non-consistently toxic users, we have uncovered distinctive temporal patterns, including contiguous activity spans, inter-tweet intervals (referred to as 'Burstiness'), and churn analysis.","These findings provide strong evidence for the existence of a unique tweeting pattern associated with toxic behavior on Twitter.","Crucially, our methodology transcends Twitter and can be adapted to various social media platforms, facilitating the identification of consistently toxic users based on their posting behavior.","This research contributes to ongoing efforts to combat online toxicity and offers insights for refining moderation strategies in the digital realm.","We are committed to open research and will provide our code and data to the research community."],"url":"http://arxiv.org/abs/2401.14141v1","category":"cs.SI"}
{"created":"2024-01-25 12:32:21","title":"Expression-aware video inpainting for HMD removal in XR applications","abstract":"Head-mounted displays (HMDs) serve as indispensable devices for observing extended reality (XR) environments and virtual content. However, HMDs present an obstacle to external recording techniques as they block the upper face of the user. This limitation significantly affects social XR applications, specifically teleconferencing, where facial features and eye gaze information play a vital role in creating an immersive user experience. In this study, we propose a new network for expression-aware video inpainting for HMD removal (EVI-HRnet) based on generative adversarial networks (GANs). Our model effectively fills in missing information with regard to facial landmarks and a single occlusion-free reference image of the user. The framework and its components ensure the preservation of the user's identity across frames using the reference frame. To further improve the level of realism of the inpainted output, we introduce a novel facial expression recognition (FER) loss function for emotion preservation. Our results demonstrate the remarkable capability of the proposed framework to remove HMDs from facial videos while maintaining the subject's facial expression and identity. Moreover, the outputs exhibit temporal consistency along the inpainted frames. This lightweight framework presents a practical approach for HMD occlusion removal, with the potential to enhance various collaborative XR applications without the need for additional hardware.","sentences":["Head-mounted displays (HMDs) serve as indispensable devices for observing extended reality (XR) environments and virtual content.","However, HMDs present an obstacle to external recording techniques as they block the upper face of the user.","This limitation significantly affects social XR applications, specifically teleconferencing, where facial features and eye gaze information play a vital role in creating an immersive user experience.","In this study, we propose a new network for expression-aware video inpainting for HMD removal (EVI-HRnet) based on generative adversarial networks (GANs).","Our model effectively fills in missing information with regard to facial landmarks and a single occlusion-free reference image of the user.","The framework and its components ensure the preservation of the user's identity across frames using the reference frame.","To further improve the level of realism of the inpainted output, we introduce a novel facial expression recognition (FER) loss function for emotion preservation.","Our results demonstrate the remarkable capability of the proposed framework to remove HMDs from facial videos while maintaining the subject's facial expression and identity.","Moreover, the outputs exhibit temporal consistency along the inpainted frames.","This lightweight framework presents a practical approach for HMD occlusion removal, with the potential to enhance various collaborative XR applications without the need for additional hardware."],"url":"http://arxiv.org/abs/2401.14136v1","category":"cs.CV"}
{"created":"2024-01-25 12:31:41","title":"Convolutional Neural Networks can achieve binary bail judgement classification","abstract":"There is an evident lack of implementation of Machine Learning (ML) in the legal domain in India, and any research that does take place in this domain is usually based on data from the higher courts of law and works with English data. The lower courts and data from the different regional languages of India are often overlooked. In this paper, we deploy a Convolutional Neural Network (CNN) architecture on a corpus of Hindi legal documents. We perform a bail Prediction task with the help of a CNN model and achieve an overall accuracy of 93\\% which is an improvement on the benchmark accuracy, set by Kapoor et al. (2022), albeit in data from 20 districts of the Indian state of Uttar Pradesh.","sentences":["There is an evident lack of implementation of Machine Learning (ML) in the legal domain in India, and any research that does take place in this domain is usually based on data from the higher courts of law and works with English data.","The lower courts and data from the different regional languages of India are often overlooked.","In this paper, we deploy a Convolutional Neural Network (CNN) architecture on a corpus of Hindi legal documents.","We perform a bail Prediction task with the help of a CNN model and achieve an overall accuracy of 93\\% which is an improvement on the benchmark accuracy, set by Kapoor et al. (2022), albeit in data from 20 districts of the Indian state of Uttar Pradesh."],"url":"http://arxiv.org/abs/2401.14135v1","category":"cs.CL"}
{"created":"2024-01-25 12:27:03","title":"Enabling Cross-Camera Collaboration for Video Analytics on Distributed Smart Cameras","abstract":"Overlapping cameras offer exciting opportunities to view a scene from different angles, allowing for more advanced, comprehensive and robust analysis. However, existing visual analytics systems for multi-camera streams are mostly limited to (i) per-camera processing and aggregation and (ii) workload-agnostic centralized processing architectures. In this paper, we present Argus, a distributed video analytics system with cross-camera collaboration on smart cameras. We identify multi-camera, multi-target tracking as the primary task of multi-camera video analytics and develop a novel technique that avoids redundant, processing-heavy identification tasks by leveraging object-wise spatio-temporal association in the overlapping fields of view across multiple cameras. We further develop a set of techniques to perform these operations across distributed cameras without cloud support at low latency by (i) dynamically ordering the camera and object inspection sequence and (ii) flexibly distributing the workload across smart cameras, taking into account network transmission and heterogeneous computational capacities. Evaluation of three real-world overlapping camera datasets with two Nvidia Jetson devices shows that Argus reduces the number of object identifications and end-to-end latency by up to 7.13x and 2.19x (4.86x and 1.60x compared to the state-of-the-art), while achieving comparable tracking quality.","sentences":["Overlapping cameras offer exciting opportunities to view a scene from different angles, allowing for more advanced, comprehensive and robust analysis.","However, existing visual analytics systems for multi-camera streams are mostly limited to (i) per-camera processing and aggregation and (ii) workload-agnostic centralized processing architectures.","In this paper, we present Argus, a distributed video analytics system with cross-camera collaboration on smart cameras.","We identify multi-camera, multi-target tracking as the primary task of multi-camera video analytics and develop a novel technique that avoids redundant, processing-heavy identification tasks by leveraging object-wise spatio-temporal association in the overlapping fields of view across multiple cameras.","We further develop a set of techniques to perform these operations across distributed cameras without cloud support at low latency by (i) dynamically ordering the camera and object inspection sequence and (ii) flexibly distributing the workload across smart cameras, taking into account network transmission and heterogeneous computational capacities.","Evaluation of three real-world overlapping camera datasets with two Nvidia Jetson devices shows that Argus reduces the number of object identifications and end-to-end latency by up to 7.13x and 2.19x (4.86x and 1.60x compared to the state-of-the-art), while achieving comparable tracking quality."],"url":"http://arxiv.org/abs/2401.14132v1","category":"cs.CV"}
{"created":"2024-01-25 12:23:22","title":"Equivariant Manifold Neural ODEs and Differential Invariants","abstract":"In this paper we develop a manifestly geometric framework for equivariant manifold neural ordinary differential equations (NODEs), and use it to analyse their modelling capabilities for symmetric data. First, we consider the action of a Lie group $G$ on a smooth manifold $M$ and establish the equivalence between equivariance of vector fields, symmetries of the corresponding Cauchy problems, and equivariance of the associated NODEs. We also propose a novel formulation of the equivariant NODEs in terms of the differential invariants of the action of $G$ on $M$, based on Lie theory for symmetries of differential equations, which provides an efficient parameterisation of the space of equivariant vector fields in a way that is agnostic to both the manifold $M$ and the symmetry group $G$. Second, we construct augmented manifold NODEs, through embeddings into equivariant flows, and show that they are universal approximators of equivariant diffeomorphisms on any path-connected $M$. Furthermore, we show that the augmented NODEs can be incorporated in the geometric framework and parameterised using higher order differential invariants. Finally, we consider the induced action of $G$ on different fields on $M$ and show how it can be used to generalise previous work, on, e.g., continuous normalizing flows, to equivariant models in any geometry.","sentences":["In this paper we develop a manifestly geometric framework for equivariant manifold neural ordinary differential equations (NODEs), and use it to analyse their modelling capabilities for symmetric data.","First, we consider the action of a Lie group $G$ on a smooth manifold $M$ and establish the equivalence between equivariance of vector fields, symmetries of the corresponding Cauchy problems, and equivariance of the associated NODEs.","We also propose a novel formulation of the equivariant NODEs in terms of the differential invariants of the action of $G$ on $M$, based on Lie theory for symmetries of differential equations, which provides an efficient parameterisation of the space of equivariant vector fields in a way that is agnostic to both the manifold $M$ and the symmetry group $G$. Second, we construct augmented manifold NODEs, through embeddings into equivariant flows, and show that they are universal approximators of equivariant diffeomorphisms on any path-connected $M$. Furthermore, we show that the augmented NODEs can be incorporated in the geometric framework and parameterised using higher order differential invariants.","Finally, we consider the induced action of $G$ on different fields on $M$ and show how it can be used to generalise previous work, on, e.g., continuous normalizing flows, to equivariant models in any geometry."],"url":"http://arxiv.org/abs/2401.14131v1","category":"cs.LG"}
{"created":"2024-01-25 12:18:46","title":"Attention-based Efficient Classification for 3D MRI Image of Alzheimer's Disease","abstract":"Early diagnosis of Alzheimer Diagnostics (AD) is a challenging task due to its subtle and complex clinical symptoms. Deep learning-assisted medical diagnosis using image recognition techniques has become an important research topic in this field. The features have to accurately capture main variations of anatomical brain structures. However, time-consuming is expensive for feature extraction by deep learning training. This study proposes a novel Alzheimer's disease detection model based on Convolutional Neural Networks. The model utilizes a pre-trained ResNet network as the backbone, incorporating post-fusion algorithm for 3D medical images and attention mechanisms. The experimental results indicate that the employed 2D fusion algorithm effectively improves the model's training expense. And the introduced attention mechanism accurately weights important regions in images, further enhancing the model's diagnostic accuracy.","sentences":["Early diagnosis of Alzheimer Diagnostics (AD) is a challenging task due to its subtle and complex clinical symptoms.","Deep learning-assisted medical diagnosis using image recognition techniques has become an important research topic in this field.","The features have to accurately capture main variations of anatomical brain structures.","However, time-consuming is expensive for feature extraction by deep learning training.","This study proposes a novel Alzheimer's disease detection model based on Convolutional Neural Networks.","The model utilizes a pre-trained ResNet network as the backbone, incorporating post-fusion algorithm for 3D medical images and attention mechanisms.","The experimental results indicate that the employed 2D fusion algorithm effectively improves the model's training expense.","And the introduced attention mechanism accurately weights important regions in images, further enhancing the model's diagnostic accuracy."],"url":"http://arxiv.org/abs/2401.14130v1","category":"eess.IV"}
{"created":"2024-01-25 12:16:10","title":"Performance Analysis for Near-Field ISAC: A Holographic MIMO Design","abstract":"A near-field holographic multiple-input multiple-output (MIMO) based integrated sensing and communications (ISAC) framework is proposed for both downlink and uplink scenarios, where spherical wave-based model is considered to capture the characteristics of the near field. The coupling effect introduced by the densely spaced antennas of the holographic MIMO are characterized by spatially correlated Rayleigh fading. Based on the proposed framework, by considering both instantaneous channel state information (CSI) and statistical CSI, closed-form expressions are derived for sensing rates (SRs), communication rates (CRs), and outage probabilities under different ISAC designs. Further insights are gained by examining high signal-to-noise ratio slopes and diversity orders. Specifically, 1) for the downlink case, a sensing-centric (S-C) design and a communications-centric (C-C) design are investigated based on different beamforming strategies, and a Pareto optimal design is proposed to characterize the attainable SR-CR region; and 2) for the uplink case, the S-C design and the C-C design are distinguished by the interference cancellation order of the communication signal and the sensing signal, and the rate region is obtained through a time-sharing strategy. Numerical results reveal that the proposed ISAC system achieves more extensive rate regions than the conventional frequency-division sensing and communications system, highlighting its superior performance.","sentences":["A near-field holographic multiple-input multiple-output (MIMO) based integrated sensing and communications (ISAC) framework is proposed for both downlink and uplink scenarios, where spherical wave-based model is considered to capture the characteristics of the near field.","The coupling effect introduced by the densely spaced antennas of the holographic MIMO are characterized by spatially correlated Rayleigh fading.","Based on the proposed framework, by considering both instantaneous channel state information (CSI) and statistical CSI, closed-form expressions are derived for sensing rates (SRs), communication rates (CRs), and outage probabilities under different ISAC designs.","Further insights are gained by examining high signal-to-noise ratio slopes and diversity orders.","Specifically, 1) for the downlink case, a sensing-centric (S-C) design and a communications-centric (C-C) design are investigated based on different beamforming strategies, and a Pareto optimal design is proposed to characterize the attainable SR-CR region; and 2) for the uplink case, the S-C design and the C-C design are distinguished by the interference cancellation order of the communication signal and the sensing signal, and the rate region is obtained through a time-sharing strategy.","Numerical results reveal that the proposed ISAC system achieves more extensive rate regions than the conventional frequency-division sensing and communications system, highlighting its superior performance."],"url":"http://arxiv.org/abs/2401.14129v1","category":"cs.IT"}
{"created":"2024-01-25 12:16:03","title":"Homoclinic chaos in a pair of parametrically-driven coupled SQUIDs","abstract":"An rf superconducting quantum interference device (SQUID) consists of a superconducting ring interrupted by a Josephson junction (JJ). When driven by an alternating magnetic field, the induced supercurrents around the ring are determined by the JJ through the celebrated Josephson relations. This system exhibits rich nonlinear behavior, including chaotic effects. We study the dynamics of a pair of parametrically-driven coupled SQUIDs arranged in series. We take advantage of the weak damping that characterizes these systems to perform a multiple-scales analysis and obtain amplitude equations, describing the slow dynamics of the system. This picture allows us to expose the existence of homoclinic orbits in the dynamics of the integrable part of the slow equations of motion. Using high-dimensional Melnikov theory, we are able to obtain explicit parameter values for which these orbits persist in the full system, consisting of both Hamiltonian and non-Hamiltonian perturbations, to form so-called Silnikov orbits, indicating a loss of integrability and the existence of chaos.","sentences":["An rf superconducting quantum interference device (SQUID) consists of a superconducting ring interrupted by a Josephson junction (JJ).","When driven by an alternating magnetic field, the induced supercurrents around the ring are determined by the JJ through the celebrated Josephson relations.","This system exhibits rich nonlinear behavior, including chaotic effects.","We study the dynamics of a pair of parametrically-driven coupled SQUIDs arranged in series.","We take advantage of the weak damping that characterizes these systems to perform a multiple-scales analysis and obtain amplitude equations, describing the slow dynamics of the system.","This picture allows us to expose the existence of homoclinic orbits in the dynamics of the integrable part of the slow equations of motion.","Using high-dimensional Melnikov theory, we are able to obtain explicit parameter values for which these orbits persist in the full system, consisting of both Hamiltonian and non-Hamiltonian perturbations, to form so-called Silnikov orbits, indicating a loss of integrability and the existence of chaos."],"url":"http://arxiv.org/abs/2401.14128v1","category":"nlin.CD"}
{"created":"2024-01-25 12:14:21","title":"An Indexed Linear Logic for Idempotent Intersection Types (Long version)","abstract":"Indexed Linear Logic has been introduced by Ehrhard and Bucciarelli, it can be seen as a logical presentation of non-idempotent intersection types extended through the relational semantics to the full linear logic. We introduce an idempotent variant of Indexed Linear Logic. We give a fine-grained reformulation of the syntax by exposing implicit parameters and by unifying several operations on formulae via the notion of base change. Idempotency is achieved by means of an appropriate subtyping relation. We carry on an in-depth study of indLL as a logic, showing how it determines a refinement of classical linear logic and establishing a terminating cut-elimination procedure. Cut-elimination is proved to be confluent up to an appropriate congruence induced by the subtyping relation.","sentences":["Indexed Linear Logic has been introduced by Ehrhard and Bucciarelli, it can be seen as a logical presentation of non-idempotent intersection types extended through the relational semantics to the full linear logic.","We introduce an idempotent variant of Indexed Linear Logic.","We give a fine-grained reformulation of the syntax by exposing implicit parameters and by unifying several operations on formulae via the notion of base change.","Idempotency is achieved by means of an appropriate subtyping relation.","We carry on an in-depth study of indLL as a logic, showing how it determines a refinement of classical linear logic and establishing a terminating cut-elimination procedure.","Cut-elimination is proved to be confluent up to an appropriate congruence induced by the subtyping relation."],"url":"http://arxiv.org/abs/2401.14126v1","category":"cs.LO"}
{"created":"2024-01-25 12:04:53","title":"Incorporating Exemplar Optimization into Training with Dual Networks for Human Mesh Recovery","abstract":"We propose a novel optimization-based human mesh recovery method from a single image. Given a test exemplar, previous approaches optimize the pre-trained regression network to minimize the 2D re-projection loss, which however suffer from over-/under-fitting problems. This is because the ``exemplar optimization'' at testing time has too weak relation to the pre-training process, and the exemplar optimization loss function is different from the training loss function. (1) We incorporate exemplar optimization into the training stage. During training, our method first executes exemplar optimization and subsequently proceeds with training-time optimization. The exemplar optimization may run into a wrong direction, while the subsequent training optimization serves to correct the deviation. Involved in training, the exemplar optimization learns to adapt its behavior to training data, thereby acquires generalibility to test exemplars. (2) We devise a dual-network architecture to convey the novel training paradigm, which is composed of a main regression network and an auxiliary network, in which we can formulate the exemplar optimization loss function in the same form as the training loss function. This further enhances the compatibility between the exemplar and training optimizations. Experiments demonstrate that our exemplar optimization after the novel training scheme significantly outperforms state-of-the-art approaches.","sentences":["We propose a novel optimization-based human mesh recovery method from a single image.","Given a test exemplar, previous approaches optimize the pre-trained regression network to minimize the 2D re-projection loss, which however suffer from over-/under-fitting problems.","This is because the ``exemplar optimization'' at testing time has too weak relation to the pre-training process, and the exemplar optimization loss function is different from the training loss function.","(1) We incorporate exemplar optimization into the training stage.","During training, our method first executes exemplar optimization and subsequently proceeds with training-time optimization.","The exemplar optimization may run into a wrong direction, while the subsequent training optimization serves to correct the deviation.","Involved in training, the exemplar optimization learns to adapt its behavior to training data, thereby acquires generalibility to test exemplars.","(2) We devise a dual-network architecture to convey the novel training paradigm, which is composed of a main regression network and an auxiliary network, in which we can formulate the exemplar optimization loss function in the same form as the training loss function.","This further enhances the compatibility between the exemplar and training optimizations.","Experiments demonstrate that our exemplar optimization after the novel training scheme significantly outperforms state-of-the-art approaches."],"url":"http://arxiv.org/abs/2401.14121v1","category":"cs.CV"}
{"created":"2024-01-25 11:54:44","title":"Evaluation of POSIT Arithmetic with Accelerators","abstract":"We present an evaluation of 32-bit POSIT arithmetic through its implementation as accelerators on FPGAs and GPUs. POSIT, a floating-point number format, adaptively changes the size of its fractional part. We developed hardware designs for FPGAs and software for GPUs to accelerate linear algebra operations using Posit(32,2) arithmetic. Our FPGA- and GPU-based accelerators in Posit(32,2) arithmetic significantly accelerated the Cholesky and LU decomposition algorithms for dense matrices. In terms of numerical accuracy, Posit(32,2) arithmetic is approximately 0.5 - 1.0 digits more accurate than the standard 32-bit format, especially when the norm of the elements of the input matrix is close to 1. Evaluating power consumption, we observed that the power efficiency of the accelerators ranged between 0.043 - 0.076 Gflops/watts for the LU decomposition in Posit(32,2) arithmetic. The power efficiency of the latest GPUs as accelerators of Posit(32,2) arithmetic is better than that of the evaluated FPGA chip.","sentences":["We present an evaluation of 32-bit POSIT arithmetic through its implementation as accelerators on FPGAs and GPUs.","POSIT, a floating-point number format, adaptively changes the size of its fractional part.","We developed hardware designs for FPGAs and software for GPUs to accelerate linear algebra operations using Posit(32,2) arithmetic.","Our FPGA- and GPU-based accelerators in Posit(32,2) arithmetic significantly accelerated the Cholesky and LU decomposition algorithms for dense matrices.","In terms of numerical accuracy, Posit(32,2) arithmetic is approximately 0.5 - 1.0 digits more accurate than the standard 32-bit format, especially when the norm of the elements of the input matrix is close to 1.","Evaluating power consumption, we observed that the power efficiency of the accelerators ranged between 0.043 - 0.076 Gflops/watts for the LU decomposition in Posit(32,2) arithmetic.","The power efficiency of the latest GPUs as accelerators of Posit(32,2) arithmetic is better than that of the evaluated FPGA chip."],"url":"http://arxiv.org/abs/2401.14117v1","category":"cs.DC"}
{"created":"2024-01-25 11:53:32","title":"Uniqueness of photon sphere for Reissner-Nordstr\u00f6m electric-magnetic system","abstract":"Uniqueness of static, asymptotically flat, non-extremal {\\it photon sphere} in Einstein-Maxwell spacetime with electric and magnetic charges has been proved. Using conformal positive energy theorem, as well as, the positive mass theorem and adequate conformal transformations, we envisage the two alternative ways of proving that the exterior region of a certain radius of the studied static {\\it photon sphere}, is characterized by ADM mass, electric and magnetic charges.","sentences":["Uniqueness of static, asymptotically flat, non-extremal {\\it photon sphere} in Einstein-Maxwell spacetime with electric and magnetic charges has been proved.","Using conformal positive energy theorem, as well as, the positive mass theorem and adequate conformal transformations, we envisage the two alternative ways of proving that the exterior region of a certain radius of the studied static {\\it photon sphere}, is characterized by ADM mass, electric and magnetic charges."],"url":"http://arxiv.org/abs/2401.14116v1","category":"gr-qc"}
{"created":"2024-01-25 11:50:43","title":"MIFI: MultI-camera Feature Integration for Roust 3D Distracted Driver Activity Recognition","abstract":"Distracted driver activity recognition plays a critical role in risk aversion-particularly beneficial in intelligent transportation systems. However, most existing methods make use of only the video from a single view and the difficulty-inconsistent issue is neglected. Different from them, in this work, we propose a novel MultI-camera Feature Integration (MIFI) approach for 3D distracted driver activity recognition by jointly modeling the data from different camera views and explicitly re-weighting examples based on their degree of difficulty. Our contributions are two-fold: (1) We propose a simple but effective multi-camera feature integration framework and provide three types of feature fusion techniques. (2) To address the difficulty-inconsistent problem in distracted driver activity recognition, a periodic learning method, named example re-weighting that can jointly learn the easy and hard samples, is presented. The experimental results on the 3MDAD dataset demonstrate that the proposed MIFI can consistently boost performance compared to single-view models.","sentences":["Distracted driver activity recognition plays a critical role in risk aversion-particularly beneficial in intelligent transportation systems.","However, most existing methods make use of only the video from a single view and the difficulty-inconsistent issue is neglected.","Different from them, in this work, we propose a novel MultI-camera Feature Integration (MIFI) approach for 3D distracted driver activity recognition by jointly modeling the data from different camera views and explicitly re-weighting examples based on their degree of difficulty.","Our contributions are two-fold: (1) We propose a simple but effective multi-camera feature integration framework and provide three types of feature fusion techniques.","(2) To address the difficulty-inconsistent problem in distracted driver activity recognition, a periodic learning method, named example re-weighting that can jointly learn the easy and hard samples, is presented.","The experimental results on the 3MDAD dataset demonstrate that the proposed MIFI can consistently boost performance compared to single-view models."],"url":"http://arxiv.org/abs/2401.14115v1","category":"cs.CV"}
{"created":"2024-01-25 11:47:58","title":"On the Affinity, Rationality, and Diversity of Hierarchical Topic Modeling","abstract":"Hierarchical topic modeling aims to discover latent topics from a corpus and organize them into a hierarchy to understand documents with desirable semantic granularity. However, existing work struggles with producing topic hierarchies of low affinity, rationality, and diversity, which hampers document understanding. To overcome these challenges, we in this paper propose Transport Plan and Context-aware Hierarchical Topic Model (TraCo). Instead of early simple topic dependencies, we propose a transport plan dependency method. It constrains dependencies to ensure their sparsity and balance, and also regularizes topic hierarchy building with them. This improves affinity and diversity of hierarchies. We further propose a context-aware disentangled decoder. Rather than previously entangled decoding, it distributes different semantic granularity to topics at different levels by disentangled decoding. This facilitates the rationality of hierarchies. Experiments on benchmark datasets demonstrate that our method surpasses state-of-the-art baselines, effectively improving the affinity, rationality, and diversity of hierarchical topic modeling with better performance on downstream tasks.","sentences":["Hierarchical topic modeling aims to discover latent topics from a corpus and organize them into a hierarchy to understand documents with desirable semantic granularity.","However, existing work struggles with producing topic hierarchies of low affinity, rationality, and diversity, which hampers document understanding.","To overcome these challenges, we in this paper propose Transport Plan and Context-aware Hierarchical Topic Model (TraCo).","Instead of early simple topic dependencies, we propose a transport plan dependency method.","It constrains dependencies to ensure their sparsity and balance, and also regularizes topic hierarchy building with them.","This improves affinity and diversity of hierarchies.","We further propose a context-aware disentangled decoder.","Rather than previously entangled decoding, it distributes different semantic granularity to topics at different levels by disentangled decoding.","This facilitates the rationality of hierarchies.","Experiments on benchmark datasets demonstrate that our method surpasses state-of-the-art baselines, effectively improving the affinity, rationality, and diversity of hierarchical topic modeling with better performance on downstream tasks."],"url":"http://arxiv.org/abs/2401.14113v1","category":"cs.CL"}
{"created":"2024-01-25 11:46:38","title":"FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design","abstract":"Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications. However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference. It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with irregular bit-width and (2) high runtime overhead of weight de-quantization. To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width. We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called FP6-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved. Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a single GPU, achieving 1.69x-2.65x higher normalized inference throughput than the FP16 baseline. The source code will be publicly available soon.","sentences":["Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications.","However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference.","It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with irregular bit-width and (2) high runtime overhead of weight de-quantization.","To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width.","We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called FP6-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved.","Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a single GPU, achieving 1.69x-2.65x higher normalized inference throughput than the FP16 baseline.","The source code will be publicly available soon."],"url":"http://arxiv.org/abs/2401.14112v1","category":"cs.LG"}
{"created":"2024-01-25 11:46:31","title":"Scene Graph to Image Synthesis: Integrating CLIP Guidance with Graph Conditioning in Diffusion Models","abstract":"Advancements in generative models have sparked significant interest in generating images while adhering to specific structural guidelines. Scene graph to image generation is one such task of generating images which are consistent with the given scene graph. However, the complexity of visual scenes poses a challenge in accurately aligning objects based on specified relations within the scene graph. Existing methods approach this task by first predicting a scene layout and generating images from these layouts using adversarial training. In this work, we introduce a novel approach to generate images from scene graphs which eliminates the need of predicting intermediate layouts. We leverage pre-trained text-to-image diffusion models and CLIP guidance to translate graph knowledge into images. Towards this, we first pre-train our graph encoder to align graph features with CLIP features of corresponding images using a GAN based training. Further, we fuse the graph features with CLIP embedding of object labels present in the given scene graph to create a graph consistent CLIP guided conditioning signal. In the conditioning input, object embeddings provide coarse structure of the image and graph features provide structural alignment based on relationships among objects. Finally, we fine tune a pre-trained diffusion model with the graph consistent conditioning signal with reconstruction and CLIP alignment loss. Elaborate experiments reveal that our method outperforms existing methods on standard benchmarks of COCO-stuff and Visual Genome dataset.","sentences":["Advancements in generative models have sparked significant interest in generating images while adhering to specific structural guidelines.","Scene graph to image generation is one such task of generating images which are consistent with the given scene graph.","However, the complexity of visual scenes poses a challenge in accurately aligning objects based on specified relations within the scene graph.","Existing methods approach this task by first predicting a scene layout and generating images from these layouts using adversarial training.","In this work, we introduce a novel approach to generate images from scene graphs which eliminates the need of predicting intermediate layouts.","We leverage pre-trained text-to-image diffusion models and CLIP guidance to translate graph knowledge into images.","Towards this, we first pre-train our graph encoder to align graph features with CLIP features of corresponding images using a GAN based training.","Further, we fuse the graph features with CLIP embedding of object labels present in the given scene graph to create a graph consistent CLIP guided conditioning signal.","In the conditioning input, object embeddings provide coarse structure of the image and graph features provide structural alignment based on relationships among objects.","Finally, we fine tune a pre-trained diffusion model with the graph consistent conditioning signal with reconstruction and CLIP alignment loss.","Elaborate experiments reveal that our method outperforms existing methods on standard benchmarks of COCO-stuff and Visual Genome dataset."],"url":"http://arxiv.org/abs/2401.14111v1","category":"cs.CV"}
{"created":"2024-01-25 11:46:01","title":"Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators","abstract":"The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune high-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits accumulators, with no significant degradation in accuracy. Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy.","sentences":["The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients).","However, current hardware still relies on high-accuracy core operations.","Most significant is the operation of accumulating products.","This high-precision accumulation operation is gradually becoming the main computational bottleneck.","This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance.","In this work, we present a simple method to train and fine-tune high-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits accumulators, with no significant degradation in accuracy.","Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy."],"url":"http://arxiv.org/abs/2401.14110v1","category":"cs.LG"}
{"created":"2024-01-25 11:45:21","title":"CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks","abstract":"Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly in generative Artificial Intelligence (AI), but their immense size poses significant challenges, such as huge training and inference costs, substantial energy demands, and limitations for on-site deployment. Traditional compression methods such as pruning, distillation, and low-rank approximation focus on reducing the effective number of neurons in the network, while quantization focuses on reducing the numerical precision of individual weights to reduce the model size while keeping the number of neurons fixed. While these compression methods have been relatively successful in practice, there's no compelling reason to believe that truncating the number of neurons is an optimal strategy. In this context, this paper introduces CompactifAI, an innovative LLM compression approach using quantum-inspired Tensor Networks that focuses on the model's correlation space instead, allowing for a more controlled, refined and interpretable model compression. Our method is versatile and can be implemented with - or on top of - other compression techniques. As a benchmark, we demonstrate that CompactifAI alone enables compression of the LlaMA-2 7B model to only $30\\%$ of its original size while recovering over $90\\%$ of the original accuracy after a brief distributed retraining.","sentences":["Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly in generative Artificial Intelligence (AI), but their immense size poses significant challenges, such as huge training and inference costs, substantial energy demands, and limitations for on-site deployment.","Traditional compression methods such as pruning, distillation, and low-rank approximation focus on reducing the effective number of neurons in the network, while quantization focuses on reducing the numerical precision of individual weights to reduce the model size while keeping the number of neurons fixed.","While these compression methods have been relatively successful in practice, there's no compelling reason to believe that truncating the number of neurons is an optimal strategy.","In this context, this paper introduces CompactifAI, an innovative LLM compression approach using quantum-inspired Tensor Networks that focuses on the model's correlation space instead, allowing for a more controlled, refined and interpretable model compression.","Our method is versatile and can be implemented with - or on top of - other compression techniques.","As a benchmark, we demonstrate that CompactifAI alone enables compression of the LlaMA-2 7B model to only $30\\%$ of its original size while recovering over $90\\%$ of the original accuracy after a brief distributed retraining."],"url":"http://arxiv.org/abs/2401.14109v1","category":"cs.CL"}
{"created":"2024-01-25 11:44:12","title":"Travelling waves in nonlinear magneto-inductive lattices","abstract":"We consider a lattice equation modelling one-dimensional metamaterials formed by a discrete array of nonlinear resonators. We focus on periodic travelling waves due to the presence of a periodic force. The existence and uniqueness results of periodic travelling waves of the system are presented. Our analytical results are found to be in good agreement with direct numerical computations","sentences":["We consider a lattice equation modelling one-dimensional metamaterials formed by a discrete array of nonlinear resonators.","We focus on periodic travelling waves due to the presence of a periodic force.","The existence and uniqueness results of periodic travelling waves of the system are presented.","Our analytical results are found to be in good agreement with direct numerical computations"],"url":"http://arxiv.org/abs/2401.14108v1","category":"math-ph"}
{"created":"2024-01-25 11:43:35","title":"Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement","abstract":"Wearable technologies enable continuous monitoring of various health metrics, such as physical activity, heart rate, sleep, and stress levels. A key challenge with wearable data is obtaining quality labels. Unlike modalities like video where the videos themselves can be effectively used to label objects or events, wearable data do not contain obvious cues about the physical manifestation of the users and usually require rich metadata. As a result, label noise can become an increasingly thorny issue when labeling such data. In this paper, we propose a novel solution to address noisy label learning, entitled Few-Shot Human-in-the-Loop Refinement (FHLR). Our method initially learns a seed model using weak labels. Next, it fine-tunes the seed model using a handful of expert corrections. Finally, it achieves better generalizability and robustness by merging the seed and fine-tuned models via weighted parameter averaging. We evaluate our approach on four challenging tasks and datasets, and compare it against eight competitive baselines designed to deal with noisy labels. We show that FHLR achieves significantly better performance when learning from noisy labels and achieves state-of-the-art by a large margin, with up to 19% accuracy improvement under symmetric and asymmetric noise. Notably, we find that FHLR is particularly robust to increased label noise, unlike prior works that suffer from severe performance degradation. Our work not only achieves better generalization in high-stakes health sensing benchmarks but also sheds light on how noise affects commonly-used models.","sentences":["Wearable technologies enable continuous monitoring of various health metrics, such as physical activity, heart rate, sleep, and stress levels.","A key challenge with wearable data is obtaining quality labels.","Unlike modalities like video where the videos themselves can be effectively used to label objects or events, wearable data do not contain obvious cues about the physical manifestation of the users and usually require rich metadata.","As a result, label noise can become an increasingly thorny issue when labeling such data.","In this paper, we propose a novel solution to address noisy label learning, entitled Few-Shot Human-in-the-Loop Refinement (FHLR).","Our method initially learns a seed model using weak labels.","Next, it fine-tunes the seed model using a handful of expert corrections.","Finally, it achieves better generalizability and robustness by merging the seed and fine-tuned models via weighted parameter averaging.","We evaluate our approach on four challenging tasks and datasets, and compare it against eight competitive baselines designed to deal with noisy labels.","We show that FHLR achieves significantly better performance when learning from noisy labels and achieves state-of-the-art by a large margin, with up to 19% accuracy improvement under symmetric and asymmetric noise.","Notably, we find that FHLR is particularly robust to increased label noise, unlike prior works that suffer from severe performance degradation.","Our work not only achieves better generalization in high-stakes health sensing benchmarks but also sheds light on how noise affects commonly-used models."],"url":"http://arxiv.org/abs/2401.14107v1","category":"cs.LG"}
{"created":"2024-01-25 11:40:51","title":"Epimorphisms and Acyclic Types in Univalent Mathematics","abstract":"We characterize the epimorphisms in homotopy type theory (HoTT) as the fiberwise acyclic maps and develop a type-theoretic treatment of acyclic maps and types in the context of synthetic homotopy theory. We present examples and applications in group theory, such as the acyclicity of the Higman group, through the identification of groups with $0$-connected, pointed $1$-types. Many of our results are formalized as part of the agda-unimath library.","sentences":["We characterize the epimorphisms in homotopy type theory (HoTT) as the fiberwise acyclic maps and develop a type-theoretic treatment of acyclic maps and types in the context of synthetic homotopy theory.","We present examples and applications in group theory, such as the acyclicity of the Higman group, through the identification of groups with $0$-connected, pointed $1$-types.","Many of our results are formalized as part of the agda-unimath library."],"url":"http://arxiv.org/abs/2401.14106v1","category":"cs.LO"}
{"created":"2024-01-25 11:32:02","title":"Inverse source problem for discrete Helmholtz equation","abstract":"We consider multi-frequency inverse source problem for the discrete Helmholtz operator on the square lattice $\\mathbb{Z}^d$, $d \\ge 1$. We consider this problem for the cases with and without phase information. We prove uniqueness results and present examples of non-uniqueness for this problem for the case of compactly supported source function. Relations with inverse scattering problem for the discrete Schr\\\"{o}dinger operators in the Born approximation are also provided.","sentences":["We consider multi-frequency inverse source problem for the discrete Helmholtz operator on the square lattice $\\mathbb{Z}^d$, $d \\ge 1$.","We consider this problem for the cases with and without phase information.","We prove uniqueness results and present examples of non-uniqueness for this problem for the case of compactly supported source function.","Relations with inverse scattering problem for the discrete Schr\\\"{o}dinger operators in the Born approximation are also provided."],"url":"http://arxiv.org/abs/2401.14103v1","category":"math.AP"}
{"created":"2024-01-25 11:27:37","title":"Few-magnon excitations in a frustrated spin-$S$ ferromagnetic chain","abstract":"We study few-magnon excitations in a finite-size spin-$S$ ferromagnetic nearest-neighbor (NN) XXZ chain with additional antiferromagnetic next-nearest-neighbor (NNN) interaction $J'$ and single-ion (SI) anisotropy $D$. Using a set of exact two-magnon Bloch states, the two-magnon problem is mapped to a single-particle one on an effective open chain with both NN and NNN hoppings. For the commensurate momentum $k=-\\pi$, the effective chain is decoupled into two NN open chains that can be exactly solved via a plane-wave ansatz. Based on this, we identify in the $\\Delta'-D/|J'|$ plane (with $\\Delta'$ the anisotropy parameter for the NNN coupling) the regions supporting the SI or NNN exchange two-magnon bound states near the edge of the band. We prove that there always exists a lower-energy NN exchange two-magnon bound state near the band edge. For $S=1/2$, we numerically calculate the $n$-magnon spectra for $n\\leq5$ by using a spin-operator matrix element method. The corresponding $n$-magnon commensurate instability regions are determined for finite chains and consistent results with prior literature are observed.","sentences":["We study few-magnon excitations in a finite-size spin-$S$ ferromagnetic nearest-neighbor (NN) XXZ chain with additional antiferromagnetic next-nearest-neighbor (NNN) interaction $J'$ and single-ion (SI) anisotropy $D$. Using a set of exact two-magnon Bloch states, the two-magnon problem is mapped to a single-particle one on an effective open chain with both NN and NNN hoppings.","For the commensurate momentum $k=-\\pi$, the effective chain is decoupled into two NN open chains that can be exactly solved via a plane-wave ansatz.","Based on this, we identify in the $\\Delta'-D/|J'|$ plane (with $\\Delta'$ the anisotropy parameter for the NNN coupling) the regions supporting the SI or NNN exchange two-magnon bound states near the edge of the band.","We prove that there always exists a lower-energy NN exchange two-magnon bound state near the band edge.","For $S=1/2$, we numerically calculate the $n$-magnon spectra for $n\\leq5$ by using a spin-operator matrix element method.","The corresponding $n$-magnon commensurate instability regions are determined for finite chains and consistent results with prior literature are observed."],"url":"http://arxiv.org/abs/2401.14101v1","category":"cond-mat.str-el"}
