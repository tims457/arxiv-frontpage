{"created":"2024-06-06 17:59:58","title":"Stereo-Depth Fusion through Virtual Pattern Projection","abstract":"This paper presents a novel general-purpose stereo and depth data fusion paradigm that mimics the active stereo principle by replacing the unreliable physical pattern projector with a depth sensor. It works by projecting virtual patterns consistent with the scene geometry onto the left and right images acquired by a conventional stereo camera, using the sparse hints obtained from a depth sensor, to facilitate the visual correspondence. Purposely, any depth sensing device can be seamlessly plugged into our framework, enabling the deployment of a virtual active stereo setup in any possible environment and overcoming the severe limitations of physical pattern projection, such as the limited working range and environmental conditions. Exhaustive experiments on indoor and outdoor datasets featuring both long and close range, including those providing raw, unfiltered depth hints from off-the-shelf depth sensors, highlight the effectiveness of our approach in notably boosting the robustness and accuracy of algorithms and deep stereo without any code modification and even without re-training. Additionally, we assess the performance of our strategy on active stereo evaluation datasets with conventional pattern projection. Indeed, in all these scenarios, our virtual pattern projection paradigm achieves state-of-the-art performance. The source code is available at: https://github.com/bartn8/vppstereo.","sentences":["This paper presents a novel general-purpose stereo and depth data fusion paradigm that mimics the active stereo principle by replacing the unreliable physical pattern projector with a depth sensor.","It works by projecting virtual patterns consistent with the scene geometry onto the left and right images acquired by a conventional stereo camera, using the sparse hints obtained from a depth sensor, to facilitate the visual correspondence.","Purposely, any depth sensing device can be seamlessly plugged into our framework, enabling the deployment of a virtual active stereo setup in any possible environment and overcoming the severe limitations of physical pattern projection, such as the limited working range and environmental conditions.","Exhaustive experiments on indoor and outdoor datasets featuring both long and close range, including those providing raw, unfiltered depth hints from off-the-shelf depth sensors, highlight the effectiveness of our approach in notably boosting the robustness and accuracy of algorithms and deep stereo without any code modification and even without re-training.","Additionally, we assess the performance of our strategy on active stereo evaluation datasets with conventional pattern projection.","Indeed, in all these scenarios, our virtual pattern projection paradigm achieves state-of-the-art performance.","The source code is available at: https://github.com/bartn8/vppstereo."],"url":"http://arxiv.org/abs/2406.04345v1","category":"cs.CV"}
{"created":"2024-06-06 17:59:52","title":"Interpreting the Second-Order Effects of Neurons in CLIP","abstract":"We interpret the function of individual neurons in CLIP by automatically describing them using text. Analyzing the direct effects (i.e. the flow from a neuron through the residual stream to the output) or the indirect effects (overall contribution) fails to capture the neurons' function in CLIP. Therefore, we present the \"second-order lens\", analyzing the effect flowing from a neuron through the later attention heads, directly to the output. We find that these effects are highly selective: for each neuron, the effect is significant for <2% of the images. Moreover, each effect can be approximated by a single direction in the text-image space of CLIP. We describe neurons by decomposing these directions into sparse sets of text representations. The sets reveal polysemantic behavior - each neuron corresponds to multiple, often unrelated, concepts (e.g. ships and cars). Exploiting this neuron polysemy, we mass-produce \"semantic\" adversarial examples by generating images with concepts spuriously correlated to the incorrect class. Additionally, we use the second-order effects for zero-shot segmentation and attribute discovery in images. Our results indicate that a scalable understanding of neurons can be used for model deception and for introducing new model capabilities.","sentences":["We interpret the function of individual neurons in CLIP by automatically describing them using text.","Analyzing the direct effects (i.e. the flow from a neuron through the residual stream to the output) or the indirect effects (overall contribution) fails to capture the neurons' function in CLIP.","Therefore, we present the \"second-order lens\", analyzing the effect flowing from a neuron through the later attention heads, directly to the output.","We find that these effects are highly selective: for each neuron, the effect is significant for <2% of the images.","Moreover, each effect can be approximated by a single direction in the text-image space of CLIP.","We describe neurons by decomposing these directions into sparse sets of text representations.","The sets reveal polysemantic behavior - each neuron corresponds to multiple, often unrelated, concepts (e.g. ships and cars).","Exploiting this neuron polysemy, we mass-produce \"semantic\" adversarial examples by generating images with concepts spuriously correlated to the incorrect class.","Additionally, we use the second-order effects for zero-shot segmentation and attribute discovery in images.","Our results indicate that a scalable understanding of neurons can be used for model deception and for introducing new model capabilities."],"url":"http://arxiv.org/abs/2406.04341v1","category":"cs.CV"}
{"created":"2024-06-06 17:59:47","title":"Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion","abstract":"In recent years, there has been rapid development in 3D generation models, opening up new possibilities for applications such as simulating the dynamic movements of 3D objects and customizing their behaviors. However, current 3D generative models tend to focus only on surface features such as color and shape, neglecting the inherent physical properties that govern the behavior of objects in the real world. To accurately simulate physics-aligned dynamics, it is essential to predict the physical properties of materials and incorporate them into the behavior prediction process. Nonetheless, predicting the diverse materials of real-world objects is still challenging due to the complex nature of their physical attributes. In this paper, we propose \\textbf{Physics3D}, a novel method for learning various physical properties of 3D objects through a video diffusion model. Our approach involves designing a highly generalizable physical simulation system based on a viscoelastic material model, which enables us to simulate a wide range of materials with high-fidelity capabilities. Moreover, we distill the physical priors from a video diffusion model that contains more understanding of realistic object materials. Extensive experiments demonstrate the effectiveness of our method with both elastic and plastic materials. Physics3D shows great potential for bridging the gap between the physical world and virtual neural space, providing a better integration and application of realistic physical principles in virtual environments. Project page: https://liuff19.github.io/Physics3D.","sentences":["In recent years, there has been rapid development in 3D generation models, opening up new possibilities for applications such as simulating the dynamic movements of 3D objects and customizing their behaviors.","However, current 3D generative models tend to focus only on surface features such as color and shape, neglecting the inherent physical properties that govern the behavior of objects in the real world.","To accurately simulate physics-aligned dynamics, it is essential to predict the physical properties of materials and incorporate them into the behavior prediction process.","Nonetheless, predicting the diverse materials of real-world objects is still challenging due to the complex nature of their physical attributes.","In this paper, we propose \\textbf{Physics3D}, a novel method for learning various physical properties of 3D objects through a video diffusion model.","Our approach involves designing a highly generalizable physical simulation system based on a viscoelastic material model, which enables us to simulate a wide range of materials with high-fidelity capabilities.","Moreover, we distill the physical priors from a video diffusion model that contains more understanding of realistic object materials.","Extensive experiments demonstrate the effectiveness of our method with both elastic and plastic materials.","Physics3D shows great potential for bridging the gap between the physical world and virtual neural space, providing a better integration and application of realistic physical principles in virtual environments.","Project page: https://liuff19.github.io/Physics3D."],"url":"http://arxiv.org/abs/2406.04338v1","category":"cs.CV"}
{"created":"2024-06-06 17:59:47","title":"RoboMamba: Multimodal State Space Model for Efficient Robot Reasoning and Manipulation","abstract":"A fundamental objective in robot manipulation is to enable models to comprehend visual scenes and execute actions. Although existing robot Multimodal Large Language Models (MLLMs) can handle a range of basic tasks, they still face challenges in two areas: 1) inadequate reasoning ability to tackle complex tasks, and 2) high computational costs for MLLM fine-tuning and inference. The recently proposed state space model (SSM) known as Mamba demonstrates promising capabilities in non-trivial sequence modeling with linear inference complexity. Inspired by this, we introduce RoboMamba, an end-to-end robotic MLLM that leverages the Mamba model to deliver both robotic reasoning and action capabilities, while maintaining efficient fine-tuning and inference. Specifically, we first integrate the vision encoder with Mamba, aligning visual data with language embedding through co-training, empowering our model with visual common sense and robot-related reasoning. To further equip RoboMamba with action pose prediction abilities, we explore an efficient fine-tuning strategy with a simple policy head. We find that once RoboMamba possesses sufficient reasoning capability, it can acquire manipulation skills with minimal fine-tuning parameters (0.1\\% of the model) and time (20 minutes). In experiments, RoboMamba demonstrates outstanding reasoning capabilities on general and robotic evaluation benchmarks. Meanwhile, our model showcases impressive pose prediction results in both simulation and real-world experiments, achieving inference speeds 7 times faster than existing robot MLLMs. Our project web page: https://sites.google.com/view/robomamba-web","sentences":["A fundamental objective in robot manipulation is to enable models to comprehend visual scenes and execute actions.","Although existing robot Multimodal Large Language Models (MLLMs) can handle a range of basic tasks, they still face challenges in two areas: 1) inadequate reasoning ability to tackle complex tasks, and 2) high computational costs for MLLM fine-tuning and inference.","The recently proposed state space model (SSM) known as Mamba demonstrates promising capabilities in non-trivial sequence modeling with linear inference complexity.","Inspired by this, we introduce RoboMamba, an end-to-end robotic MLLM that leverages the Mamba model to deliver both robotic reasoning and action capabilities, while maintaining efficient fine-tuning and inference.","Specifically, we first integrate the vision encoder with Mamba, aligning visual data with language embedding through co-training, empowering our model with visual common sense and robot-related reasoning.","To further equip RoboMamba with action pose prediction abilities, we explore an efficient fine-tuning strategy with a simple policy head.","We find that once RoboMamba possesses sufficient reasoning capability, it can acquire manipulation skills with minimal fine-tuning parameters (0.1\\% of the model) and time (20 minutes).","In experiments, RoboMamba demonstrates outstanding reasoning capabilities on general and robotic evaluation benchmarks.","Meanwhile, our model showcases impressive pose prediction results in both simulation and real-world experiments, achieving inference speeds 7 times faster than existing robot MLLMs.","Our project web page: https://sites.google.com/view/robomamba-web"],"url":"http://arxiv.org/abs/2406.04339v1","category":"cs.CV"}
{"created":"2024-06-06 17:59:44","title":"Coherent Zero-Shot Visual Instruction Generation","abstract":"Despite the advances in text-to-image synthesis, particularly with diffusion models, generating visual instructions that require consistent representation and smooth state transitions of objects across sequential steps remains a formidable challenge. This paper introduces a simple, training-free framework to tackle the issues, capitalizing on the advancements in diffusion models and large language models (LLMs). Our approach systematically integrates text comprehension and image generation to ensure visual instructions are visually appealing and maintain consistency and accuracy throughout the instruction sequence. We validate the effectiveness by testing multi-step instructions and comparing the text alignment and consistency with several baselines. Our experiments show that our approach can visualize coherent and visually pleasing instructions","sentences":["Despite the advances in text-to-image synthesis, particularly with diffusion models, generating visual instructions that require consistent representation and smooth state transitions of objects across sequential steps remains a formidable challenge.","This paper introduces a simple, training-free framework to tackle the issues, capitalizing on the advancements in diffusion models and large language models (LLMs).","Our approach systematically integrates text comprehension and image generation to ensure visual instructions are visually appealing and maintain consistency and accuracy throughout the instruction sequence.","We validate the effectiveness by testing multi-step instructions and comparing the text alignment and consistency with several baselines.","Our experiments show that our approach can visualize coherent and visually pleasing instructions"],"url":"http://arxiv.org/abs/2406.04337v1","category":"cs.CV"}
{"created":"2024-06-06 17:59:39","title":"Particles and their fluids in $f(R,T)$ gravity","abstract":"According to the von Laue condition, the volume-averaged pressure inside particles of fixed mass and structure vanishes in the Minkowski limit of general relativity. Here we show that this condition is in general not fulfilled in the context of $f(R,T)$ gravity, or of other theories of gravity in which the linear momentum is not conserved in this limit (here, $R$ and $T$ represent the Ricci scalar and the trace of the energy-momentum tensor, respectively). In particular, we show that dust -- a perfect fluid whose particles are at rest in the fluid's proper frame -- cannot in general be described as pressureless in the context of these theories. We further discuss the implications of our findings for the form of the on-shell Lagrangian of an ideal gas.","sentences":["According to the von Laue condition, the volume-averaged pressure inside particles of fixed mass and structure vanishes in the Minkowski limit of general relativity.","Here we show that this condition is in general not fulfilled in the context of $f(R,T)$ gravity, or of other theories of gravity in which the linear momentum is not conserved in this limit (here, $R$ and $T$ represent the Ricci scalar and the trace of the energy-momentum tensor, respectively).","In particular, we show that dust -- a perfect fluid whose particles are at rest in the fluid's proper frame -- cannot in general be described as pressureless in the context of these theories.","We further discuss the implications of our findings for the form of the on-shell Lagrangian of an ideal gas."],"url":"http://arxiv.org/abs/2406.04335v1","category":"gr-qc"}
{"created":"2024-06-06 17:59:23","title":"BitsFusion: 1.99 bits Weight Quantization of Diffusion Model","abstract":"Diffusion-based image generation models have achieved great success in recent years by showing the capability of synthesizing high-quality content. However, these models contain a huge number of parameters, resulting in a significantly large model size. Saving and transferring them is a major bottleneck for various applications, especially those running on resource-constrained devices. In this work, we develop a novel weight quantization method that quantizes the UNet from Stable Diffusion v1.5 to 1.99 bits, achieving a model with 7.9X smaller size while exhibiting even better generation quality than the original one. Our approach includes several novel techniques, such as assigning optimal bits to each layer, initializing the quantized model for better performance, and improving the training strategy to dramatically reduce quantization error. Furthermore, we extensively evaluate our quantized model across various benchmark datasets and through human evaluation to demonstrate its superior generation quality.","sentences":["Diffusion-based image generation models have achieved great success in recent years by showing the capability of synthesizing high-quality content.","However, these models contain a huge number of parameters, resulting in a significantly large model size.","Saving and transferring them is a major bottleneck for various applications, especially those running on resource-constrained devices.","In this work, we develop a novel weight quantization method that quantizes the UNet from Stable Diffusion v1.5 to 1.99 bits, achieving a model with 7.9X smaller size while exhibiting even better generation quality than the original one.","Our approach includes several novel techniques, such as assigning optimal bits to each layer, initializing the quantized model for better performance, and improving the training strategy to dramatically reduce quantization error.","Furthermore, we extensively evaluate our quantized model across various benchmark datasets and through human evaluation to demonstrate its superior generation quality."],"url":"http://arxiv.org/abs/2406.04333v1","category":"cs.CV"}
{"created":"2024-06-06 17:59:10","title":"Simplified and Generalized Masked Diffusion for Discrete Data","abstract":"Masked (or absorbing) diffusion is actively explored as an alternative to autoregressive models for generative modeling of discrete data. However, existing work in this area has been hindered by unnecessarily complex model formulations and unclear relationships between different perspectives, leading to suboptimal parameterization, training objectives, and ad hoc adjustments to counteract these issues. In this work, we aim to provide a simple and general framework that unlocks the full potential of masked diffusion models. We show that the continuous-time variational objective of masked diffusion models is a simple weighted integral of cross-entropy losses. Our framework also enables training generalized masked diffusion models with state-dependent masking schedules. When evaluated by perplexity, our models trained on OpenWebText surpass prior diffusion language models at GPT-2 scale and demonstrate superior performance on 4 out of 5 zero-shot language modeling tasks. Furthermore, our models vastly outperform previous discrete diffusion models on pixel-level image modeling, achieving 2.78~(CIFAR-10) and 3.42 (ImageNet 64$\\times$64) bits per dimension that are comparable or better than autoregressive models of similar sizes.","sentences":["Masked (or absorbing) diffusion is actively explored as an alternative to autoregressive models for generative modeling of discrete data.","However, existing work in this area has been hindered by unnecessarily complex model formulations and unclear relationships between different perspectives, leading to suboptimal parameterization, training objectives, and ad hoc adjustments to counteract these issues.","In this work, we aim to provide a simple and general framework that unlocks the full potential of masked diffusion models.","We show that the continuous-time variational objective of masked diffusion models is a simple weighted integral of cross-entropy losses.","Our framework also enables training generalized masked diffusion models with state-dependent masking schedules.","When evaluated by perplexity, our models trained on OpenWebText surpass prior diffusion language models at GPT-2 scale and demonstrate superior performance on 4 out of 5 zero-shot language modeling tasks.","Furthermore, our models vastly outperform previous discrete diffusion models on pixel-level image modeling, achieving 2.78~(CIFAR-10) and 3.42 (ImageNet 64$\\times$64) bits per dimension that are comparable or better than autoregressive models of similar sizes."],"url":"http://arxiv.org/abs/2406.04329v1","category":"cs.LG"}
{"created":"2024-06-06 17:59:10","title":"PaCE: Parsimonious Concept Engineering for Large Language Models","abstract":"Large Language Models (LLMs) are being used for a wide variety of tasks. While they are capable of generating human-like responses, they can also produce undesirable output including potentially harmful information, racist or sexist language, and hallucinations. Alignment methods are designed to reduce such undesirable output, via techniques such as fine-tuning, prompt engineering, and representation engineering. However, existing methods face several challenges: some require costly fine-tuning for every alignment task; some do not adequately remove undesirable concepts, failing alignment; some remove benign concepts, lowering the linguistic capabilities of LLMs. To address these issues, we propose Parsimonious Concept Engineering (PaCE), a novel activation engineering framework for alignment. First, to sufficiently model the concepts, we construct a large-scale concept dictionary in the activation space, in which each atom corresponds to a semantic concept. Then, given any alignment task, we instruct a concept partitioner to efficiently annotate the concepts as benign or undesirable. Finally, at inference time, we decompose the LLM activations along the concept dictionary via sparse coding, to accurately represent the activation as a linear combination of the benign and undesirable components. By removing the latter ones from the activation, we reorient the behavior of LLMs towards alignment goals. We conduct experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising, and show that PaCE achieves state-of-the-art alignment performance while maintaining linguistic capabilities.","sentences":["Large Language Models (LLMs) are being used for a wide variety of tasks.","While they are capable of generating human-like responses, they can also produce undesirable output including potentially harmful information, racist or sexist language, and hallucinations.","Alignment methods are designed to reduce such undesirable output, via techniques such as fine-tuning, prompt engineering, and representation engineering.","However, existing methods face several challenges: some require costly fine-tuning for every alignment task; some do not adequately remove undesirable concepts, failing alignment; some remove benign concepts, lowering the linguistic capabilities of LLMs.","To address these issues, we propose Parsimonious Concept Engineering (PaCE), a novel activation engineering framework for alignment.","First, to sufficiently model the concepts, we construct a large-scale concept dictionary in the activation space, in which each atom corresponds to a semantic concept.","Then, given any alignment task, we instruct a concept partitioner to efficiently annotate the concepts as benign or undesirable.","Finally, at inference time, we decompose the LLM activations along the concept dictionary via sparse coding, to accurately represent the activation as a linear combination of the benign and undesirable components.","By removing the latter ones from the activation, we reorient the behavior of LLMs towards alignment goals.","We conduct experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising, and show that PaCE achieves state-of-the-art alignment performance while maintaining linguistic capabilities."],"url":"http://arxiv.org/abs/2406.04331v1","category":"cs.CL"}
{"created":"2024-06-06 17:58:54","title":"ShareGPT4Video: Improving Video Understanding and Generation with Better Captions","abstract":"We present the ShareGPT4Video series, aiming to facilitate the video understanding of large video-language models (LVLMs) and the video generation of text-to-video models (T2VMs) via dense and precise captions. The series comprises: 1) ShareGPT4Video, 40K GPT4V annotated dense captions of videos with various lengths and sources, developed through carefully designed data filtering and annotating strategy. 2) ShareCaptioner-Video, an efficient and capable captioning model for arbitrary videos, with 4.8M high-quality aesthetic videos annotated by it. 3) ShareGPT4Video-8B, a simple yet superb LVLM that reached SOTA performance on three advancing video benchmarks. To achieve this, taking aside the non-scalable costly human annotators, we find using GPT4V to caption video with a naive multi-frame or frame-concatenation input strategy leads to less detailed and sometimes temporal-confused results. We argue the challenge of designing a high-quality video captioning strategy lies in three aspects: 1) Inter-frame precise temporal change understanding. 2) Intra-frame detailed content description. 3) Frame-number scalability for arbitrary-length videos. To this end, we meticulously designed a differential video captioning strategy, which is stable, scalable, and efficient for generating captions for videos with arbitrary resolution, aspect ratios, and length. Based on it, we construct ShareGPT4Video, which contains 40K high-quality videos spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. Based on ShareGPT4Video, we further develop ShareCaptioner-Video, a superior captioner capable of efficiently generating high-quality captions for arbitrary videos...","sentences":["We present the ShareGPT4Video series, aiming to facilitate the video understanding of large video-language models (LVLMs) and the video generation of text-to-video models (T2VMs) via dense and precise captions.","The series comprises: 1) ShareGPT4Video, 40K GPT4V annotated dense captions of videos with various lengths and sources, developed through carefully designed data filtering and annotating strategy.","2) ShareCaptioner-Video, an efficient and capable captioning model for arbitrary videos, with 4.8M high-quality aesthetic videos annotated by it.","3) ShareGPT4Video-8B, a simple yet superb LVLM that reached SOTA performance on three advancing video benchmarks.","To achieve this, taking aside the non-scalable costly human annotators, we find using GPT4V to caption video with a naive multi-frame or frame-concatenation input strategy leads to less detailed and sometimes temporal-confused results.","We argue the challenge of designing a high-quality video captioning strategy lies in three aspects: 1) Inter-frame precise temporal change understanding.","2) Intra-frame detailed content description.","3) Frame-number scalability for arbitrary-length videos.","To this end, we meticulously designed a differential video captioning strategy, which is stable, scalable, and efficient for generating captions for videos with arbitrary resolution, aspect ratios, and length.","Based on it, we construct ShareGPT4Video, which contains 40K high-quality videos spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events.","Based on ShareGPT4Video, we further develop ShareCaptioner-Video, a superior captioner capable of efficiently generating high-quality captions for arbitrary videos..."],"url":"http://arxiv.org/abs/2406.04325v1","category":"cs.CV"}
{"created":"2024-06-06 17:58:27","title":"SF-V: Single Forward Video Generation Model","abstract":"Diffusion-based video generation models have demonstrated remarkable success in obtaining high-fidelity videos through the iterative denoising process. However, these models require multiple denoising steps during sampling, resulting in high computational costs. In this work, we propose a novel approach to obtain single-step video generation models by leveraging adversarial training to fine-tune pre-trained video diffusion models. We show that, through the adversarial training, the multi-steps video diffusion model, i.e., Stable Video Diffusion (SVD), can be trained to perform single forward pass to synthesize high-quality videos, capturing both temporal and spatial dependencies in the video data. Extensive experiments demonstrate that our method achieves competitive generation quality of synthesized videos with significantly reduced computational overhead for the denoising process (i.e., around $23\\times$ speedup compared with SVD and $6\\times$ speedup compared with existing works, with even better generation quality), paving the way for real-time video synthesis and editing. More visualization results are made publicly available at https://snap-research.github.io/SF-V.","sentences":["Diffusion-based video generation models have demonstrated remarkable success in obtaining high-fidelity videos through the iterative denoising process.","However, these models require multiple denoising steps during sampling, resulting in high computational costs.","In this work, we propose a novel approach to obtain single-step video generation models by leveraging adversarial training to fine-tune pre-trained video diffusion models.","We show that, through the adversarial training, the multi-steps video diffusion model, i.e., Stable Video Diffusion (SVD), can be trained to perform single forward pass to synthesize high-quality videos, capturing both temporal and spatial dependencies in the video data.","Extensive experiments demonstrate that our method achieves competitive generation quality of synthesized videos with significantly reduced computational overhead for the denoising process (i.e., around $23\\times$ speedup compared with SVD and $6\\times$ speedup compared with existing works, with even better generation quality), paving the way for real-time video synthesis and editing.","More visualization results are made publicly available at https://snap-research.github.io/SF-V."],"url":"http://arxiv.org/abs/2406.04324v1","category":"cs.CV"}
{"created":"2024-06-06 17:58:15","title":"DIRECT-3D: Learning Direct Text-to-3D Generation on Massive Noisy 3D Data","abstract":"We present DIRECT-3D, a diffusion-based 3D generative model for creating high-quality 3D assets (represented by Neural Radiance Fields) from text prompts. Unlike recent 3D generative models that rely on clean and well-aligned 3D data, limiting them to single or few-class generation, our model is directly trained on extensive noisy and unaligned `in-the-wild' 3D assets, mitigating the key challenge (i.e., data scarcity) in large-scale 3D generation. In particular, DIRECT-3D is a tri-plane diffusion model that integrates two innovations: 1) A novel learning framework where noisy data are filtered and aligned automatically during the training process. Specifically, after an initial warm-up phase using a small set of clean data, an iterative optimization is introduced in the diffusion process to explicitly estimate the 3D pose of objects and select beneficial data based on conditional density. 2) An efficient 3D representation that is achieved by disentangling object geometry and color features with two separate conditional diffusion models that are optimized hierarchically. Given a prompt input, our model generates high-quality, high-resolution, realistic, and complex 3D objects with accurate geometric details in seconds. We achieve state-of-the-art performance in both single-class generation and text-to-3D generation. We also demonstrate that DIRECT-3D can serve as a useful 3D geometric prior of objects, for example to alleviate the well-known Janus problem in 2D-lifting methods such as DreamFusion. The code and models are available for research purposes at: https://github.com/qihao067/direct3d.","sentences":["We present DIRECT-3D, a diffusion-based 3D generative model for creating high-quality 3D assets (represented by Neural Radiance Fields) from text prompts.","Unlike recent 3D generative models that rely on clean and well-aligned 3D data, limiting them to single or few-class generation, our model is directly trained on extensive noisy and unaligned `in-the-wild' 3D assets, mitigating the key challenge (i.e., data scarcity) in large-scale 3D generation.","In particular, DIRECT-3D is a tri-plane diffusion model that integrates two innovations: 1) A novel learning framework where noisy data are filtered and aligned automatically during the training process.","Specifically, after an initial warm-up phase using a small set of clean data, an iterative optimization is introduced in the diffusion process to explicitly estimate the 3D pose of objects and select beneficial data based on conditional density.","2) An efficient 3D representation that is achieved by disentangling object geometry and color features with two separate conditional diffusion models that are optimized hierarchically.","Given a prompt input, our model generates high-quality, high-resolution, realistic, and complex 3D objects with accurate geometric details in seconds.","We achieve state-of-the-art performance in both single-class generation and text-to-3D generation.","We also demonstrate that DIRECT-3D can serve as a useful 3D geometric prior of objects, for example to alleviate the well-known Janus problem in 2D-lifting methods such as DreamFusion.","The code and models are available for research purposes at: https://github.com/qihao067/direct3d."],"url":"http://arxiv.org/abs/2406.04322v1","category":"cs.CV"}
{"created":"2024-06-06 17:58:15","title":"ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories","abstract":"Training autonomous agents with sparse rewards is a long-standing problem in online reinforcement learning (RL), due to low data efficiency. Prior work overcomes this challenge by extracting useful knowledge from offline data, often accomplished through the learning of action distribution from offline data and utilizing the learned distribution to facilitate online RL. However, since the offline data are given and fixed, the extracted knowledge is inherently limited, making it difficult to generalize to new tasks. We propose a novel approach that leverages offline data to learn a generative diffusion model, coined as Adaptive Trajectory Diffuser (ATraDiff). This model generates synthetic trajectories, serving as a form of data augmentation and consequently enhancing the performance of online RL methods. The key strength of our diffuser lies in its adaptability, allowing it to effectively handle varying trajectory lengths and mitigate distribution shifts between online and offline data. Because of its simplicity, ATraDiff seamlessly integrates with a wide spectrum of RL methods. Empirical evaluation shows that ATraDiff consistently achieves state-of-the-art performance across a variety of environments, with particularly pronounced improvements in complicated settings. Our code and demo video are available at https://atradiff.github.io .","sentences":["Training autonomous agents with sparse rewards is a long-standing problem in online reinforcement learning (RL), due to low data efficiency.","Prior work overcomes this challenge by extracting useful knowledge from offline data, often accomplished through the learning of action distribution from offline data and utilizing the learned distribution to facilitate online RL.","However, since the offline data are given and fixed, the extracted knowledge is inherently limited, making it difficult to generalize to new tasks.","We propose a novel approach that leverages offline data to learn a generative diffusion model, coined as Adaptive Trajectory Diffuser (ATraDiff).","This model generates synthetic trajectories, serving as a form of data augmentation and consequently enhancing the performance of online RL methods.","The key strength of our diffuser lies in its adaptability, allowing it to effectively handle varying trajectory lengths and mitigate distribution shifts between online and offline data.","Because of its simplicity, ATraDiff seamlessly integrates with a wide spectrum of RL methods.","Empirical evaluation shows that ATraDiff consistently achieves state-of-the-art performance across a variety of environments, with particularly pronounced improvements in complicated settings.","Our code and demo video are available at https://atradiff.github.io ."],"url":"http://arxiv.org/abs/2406.04323v1","category":"cs.LG"}
{"created":"2024-06-06 17:58:11","title":"VidMuse: A Simple Video-to-Music Generation Framework with Long-Short-Term Modeling","abstract":"In this work, we systematically study music generation conditioned solely on the video. First, we present a large-scale dataset comprising 190K video-music pairs, including various genres such as movie trailers, advertisements, and documentaries. Furthermore, we propose VidMuse, a simple framework for generating music aligned with video inputs. VidMuse stands out by producing high-fidelity music that is both acoustically and semantically aligned with the video. By incorporating local and global visual cues, VidMuse enables the creation of musically coherent audio tracks that consistently match the video content through Long-Short-Term modeling. Through extensive experiments, VidMuse outperforms existing models in terms of audio quality, diversity, and audio-visual alignment. The code and datasets will be available at https://github.com/ZeyueT/VidMuse/.","sentences":["In this work, we systematically study music generation conditioned solely on the video.","First, we present a large-scale dataset comprising 190K video-music pairs, including various genres such as movie trailers, advertisements, and documentaries.","Furthermore, we propose VidMuse, a simple framework for generating music aligned with video inputs.","VidMuse stands out by producing high-fidelity music that is both acoustically and semantically aligned with the video.","By incorporating local and global visual cues, VidMuse enables the creation of musically coherent audio tracks that consistently match the video content through Long-Short-Term modeling.","Through extensive experiments, VidMuse outperforms existing models in terms of audio quality, diversity, and audio-visual alignment.","The code and datasets will be available at https://github.com/ZeyueT/VidMuse/."],"url":"http://arxiv.org/abs/2406.04321v1","category":"cs.CV"}
{"created":"2024-06-06 17:58:09","title":"Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models","abstract":"Modeling multivariate time series is a well-established problem with a wide range of applications from healthcare to financial markets. Traditional State Space Models (SSMs) are classical approaches for univariate time series modeling due to their simplicity and expressive power to represent linear dependencies. They, however, have fundamentally limited expressive power to capture non-linear dependencies, are slow in practice, and fail to model the inter-variate information flow. Despite recent attempts to improve the expressive power of SSMs by using deep structured SSMs, the existing methods are either limited to univariate time series, fail to model complex patterns (e.g., seasonal patterns), fail to dynamically model the dependencies of variate and time dimensions, and/or are input-independent. We present Chimera that uses two input-dependent 2-D SSM heads with different discretization processes to learn long-term progression and seasonal patterns. To improve the efficiency of complex 2D recurrence, we present a fast training using a new 2-dimensional parallel selective scan. We further present and discuss 2-dimensional Mamba and Mamba-2 as the spacial cases of our 2D SSM. Our experimental evaluation shows the superior performance of Chimera on extensive and diverse benchmarks, including ECG and speech time series classification, long-term and short-term time series forecasting, and time series anomaly detection.","sentences":["Modeling multivariate time series is a well-established problem with a wide range of applications from healthcare to financial markets.","Traditional State Space Models (SSMs) are classical approaches for univariate time series modeling due to their simplicity and expressive power to represent linear dependencies.","They, however, have fundamentally limited expressive power to capture non-linear dependencies, are slow in practice, and fail to model the inter-variate information flow.","Despite recent attempts to improve the expressive power of SSMs by using deep structured SSMs, the existing methods are either limited to univariate time series, fail to model complex patterns (e.g., seasonal patterns), fail to dynamically model the dependencies of variate and time dimensions, and/or are input-independent.","We present Chimera that uses two input-dependent 2-D SSM heads with different discretization processes to learn long-term progression and seasonal patterns.","To improve the efficiency of complex 2D recurrence, we present a fast training using a new 2-dimensional parallel selective scan.","We further present and discuss 2-dimensional Mamba and Mamba-2 as the spacial cases of our 2D SSM.","Our experimental evaluation shows the superior performance of Chimera on extensive and diverse benchmarks, including ECG and speech time series classification, long-term and short-term time series forecasting, and time series anomaly detection."],"url":"http://arxiv.org/abs/2406.04320v1","category":"cs.LG"}
{"created":"2024-06-06 17:58:08","title":"Invariant quasimorphisms and generalized mixed Bavard duality","abstract":"This article provides an expository account of the celebrated duality theorem of Bavard and three its strengthenings. The Bavard duality theorem connects scl (stable commutator length) and quasimorphisms on a group. Calegari extended the framework from a group element to a chain on the group, and established the generalized Bavard duality. Kawasaki, Kimura, Matsushita and Mimura studied the setting of a pair of a group and its normal subgroup, and obtained the mixed Bavard duality. The first half of the present article is devoted to an introduction to these three Bavard dualities. In the latter half, we present a new strengthening, the generalized mixed Bavard duality, and provide a self-contained proof of it. This third strengthening recovers all of the Bavard dualities treated in the first half; thus, we supply complete proofs of these four Bavard dualities in a unified manner. In addition, we state several results on the space $\\mathrm{W}(G,N)$ of non-extendable quasimorphisms, which is related to the comparison problem between scl and mixed scl via the mixed Bavard duality.","sentences":["This article provides an expository account of the celebrated duality theorem of Bavard and three its strengthenings.","The Bavard duality theorem connects scl (stable commutator length) and quasimorphisms on a group.","Calegari extended the framework from a group element to a chain on the group, and established the generalized Bavard duality.","Kawasaki, Kimura, Matsushita and Mimura studied the setting of a pair of a group and its normal subgroup, and obtained the mixed Bavard duality.","The first half of the present article is devoted to an introduction to these three Bavard dualities.","In the latter half, we present a new strengthening, the generalized mixed Bavard duality, and provide a self-contained proof of it.","This third strengthening recovers all of the Bavard dualities treated in the first half; thus, we supply complete proofs of these four Bavard dualities in a unified manner.","In addition, we state several results on the space $\\mathrm{W}(G,N)$ of non-extendable quasimorphisms, which is related to the comparison problem between scl and mixed scl via the mixed Bavard duality."],"url":"http://arxiv.org/abs/2406.04319v1","category":"math.GT"}
{"created":"2024-06-06 17:58:00","title":"Adaptive Sampling of k-Space in Magnetic Resonance for Rapid Pathology Prediction","abstract":"Magnetic Resonance (MR) imaging, despite its proven diagnostic utility, remains an inaccessible imaging modality for disease surveillance at the population level. A major factor rendering MR inaccessible is lengthy scan times. An MR scanner collects measurements associated with the underlying anatomy in the Fourier space, also known as the k-space. Creating a high-fidelity image requires collecting large quantities of such measurements, increasing the scan time. Traditionally to accelerate an MR scan, image reconstruction from under-sampled k-space data is the method of choice. However, recent works show the feasibility of bypassing image reconstruction and directly learning to detect disease directly from a sparser learned subset of the k-space measurements. In this work, we propose Adaptive Sampling for MR (ASMR), a sampling method that learns an adaptive policy to sequentially select k-space samples to optimize for target disease detection. On 6 out of 8 pathology classification tasks spanning the Knee, Brain, and Prostate MR scans, ASMR reaches within 2% of the performance of a fully sampled classifier while using only 8% of the k-space, as well as outperforming prior state-of-the-art work in k-space sampling such as EMRT, LOUPE, and DPS.","sentences":["Magnetic Resonance (MR) imaging, despite its proven diagnostic utility, remains an inaccessible imaging modality for disease surveillance at the population level.","A major factor rendering MR inaccessible is lengthy scan times.","An MR scanner collects measurements associated with the underlying anatomy in the Fourier space, also known as the k-space.","Creating a high-fidelity image requires collecting large quantities of such measurements, increasing the scan time.","Traditionally to accelerate an MR scan, image reconstruction from under-sampled k-space data is the method of choice.","However, recent works show the feasibility of bypassing image reconstruction and directly learning to detect disease directly from a sparser learned subset of the k-space measurements.","In this work, we propose Adaptive Sampling for MR (ASMR), a sampling method that learns an adaptive policy to sequentially select k-space samples to optimize for target disease detection.","On 6 out of 8 pathology classification tasks spanning the Knee, Brain, and Prostate MR scans, ASMR reaches within 2% of the performance of a fully sampled classifier while using only 8% of the k-space, as well as outperforming prior state-of-the-art work in k-space sampling such as EMRT, LOUPE, and DPS."],"url":"http://arxiv.org/abs/2406.04318v1","category":"cs.LG"}
{"created":"2024-06-06 17:57:49","title":"Regularized KL-Divergence for Well-Defined Function-Space Variational Inference in Bayesian neural networks","abstract":"Bayesian neural networks (BNN) promise to combine the predictive performance of neural networks with principled uncertainty modeling important for safety-critical systems and decision making. However, posterior uncertainty estimates depend on the choice of prior, and finding informative priors in weight-space has proven difficult. This has motivated variational inference (VI) methods that pose priors directly on the function generated by the BNN rather than on weights. In this paper, we address a fundamental issue with such function-space VI approaches pointed out by Burt et al. (2020), who showed that the objective function (ELBO) is negative infinite for most priors of interest. Our solution builds on generalized VI (Knoblauch et al., 2019) with the regularized KL divergence (Quang, 2019) and is, to the best of our knowledge, the first well-defined variational objective for function-space inference in BNNs with Gaussian process (GP) priors. Experiments show that our method incorporates the properties specified by the GP prior on synthetic and small real-world data sets, and provides competitive uncertainty estimates for regression, classification and out-of-distribution detection compared to BNN baselines with both function and weight-space priors.","sentences":["Bayesian neural networks (BNN) promise to combine the predictive performance of neural networks with principled uncertainty modeling important for safety-critical systems and decision making.","However, posterior uncertainty estimates depend on the choice of prior, and finding informative priors in weight-space has proven difficult.","This has motivated variational inference (VI) methods that pose priors directly on the function generated by the BNN rather than on weights.","In this paper, we address a fundamental issue with such function-space VI approaches pointed out by Burt et al. (2020), who showed that the objective function (ELBO) is negative infinite for most priors of interest.","Our solution builds on generalized VI (Knoblauch et al., 2019) with the regularized KL divergence (Quang, 2019) and is, to the best of our knowledge, the first well-defined variational objective for function-space inference in BNNs with Gaussian process (GP) priors.","Experiments show that our method incorporates the properties specified by the GP prior on synthetic and small real-world data sets, and provides competitive uncertainty estimates for regression, classification and out-of-distribution detection compared to BNN baselines with both function and weight-space priors."],"url":"http://arxiv.org/abs/2406.04317v1","category":"cs.LG"}
{"created":"2024-06-06 17:57:17","title":"An FIO-based approach to $L^p$-bounds for the wave equation on $2$-step Carnot groups: the case of M\u00e9tivier groups","abstract":"Let $\\mathcal{L}$ be a homogeneous left-invariant sub-Laplacian on a $2$-step Carnot group. We devise a new geometric approach to sharp fixed-time $L^p$-bounds with loss of derivatives for the wave equation driven by $\\mathcal{L}$, based on microlocal analysis and highlighting the role of the underlying sub-Riemannian geodesic flow. A major challenge here stems from the fact that, differently from the Riemannian case, the conjugate locus of a point on a sub-Riemannian manifold may cluster at the point itself, thus making it indispensable to deal with caustics even when studying small-time wave propagation.   Our analysis of the wave propagator on a $2$-step Carnot group allows us to reduce microlocally to two conic regions in frequency space: an anti-FIO region, which seems not amenable to FIO techniques, and an FIO region. For the latter, we construct a parametrix by means of FIOs with complex phase, by adapting a construction from the elliptic setting due to Laptev, Safarov and Vassiliev, which remains valid beyond caustics. A substantial problem arising here is that, after a natural decomposition and scalings, one must deal with the long-time behaviour and control of $L^1$-norms of the corresponding contributions to the wave propagator, a new phenomenon that is specific to sub-elliptic settings.   For the class of M\\'etivier groups, we show how our approach, in combination with a variation of the key method of Seeger, Sogge and Stein for proving $L^p$-estimates for FIOs, yields $L^p$-bounds for the wave equation, which are sharp up to the endpoint regularity. In particular, we extend previously known results for distinguished sub-Laplacians on groups of Heisenberg type, by means of a more general and robust approach. The study of the wave equation on wider classes of $2$-step Carnot groups via this approach will pose further challenges that we plan to address in subsequent works.","sentences":["Let $\\mathcal{L}$ be a homogeneous left-invariant sub-Laplacian on a $2$-step Carnot group.","We devise a new geometric approach to sharp fixed-time $L^p$-bounds with loss of derivatives for the wave equation driven by $\\mathcal{L}$, based on microlocal analysis and highlighting the role of the underlying sub-Riemannian geodesic flow.","A major challenge here stems from the fact that, differently from the Riemannian case, the conjugate locus of a point on a sub-Riemannian manifold may cluster at the point itself, thus making it indispensable to deal with caustics even when studying small-time wave propagation.   ","Our analysis of the wave propagator on a $2$-step Carnot group allows us to reduce microlocally to two conic regions in frequency space: an anti-FIO region, which seems not amenable to FIO techniques, and an FIO region.","For the latter, we construct a parametrix by means of FIOs with complex phase, by adapting a construction from the elliptic setting due to Laptev, Safarov and Vassiliev, which remains valid beyond caustics.","A substantial problem arising here is that, after a natural decomposition and scalings, one must deal with the long-time behaviour and control of $L^1$-norms of the corresponding contributions to the wave propagator, a new phenomenon that is specific to sub-elliptic settings.   ","For the class of M\\'etivier groups, we show how our approach, in combination with a variation of the key method of Seeger, Sogge and Stein for proving $L^p$-estimates for FIOs, yields $L^p$-bounds for the wave equation, which are sharp up to the endpoint regularity.","In particular, we extend previously known results for distinguished sub-Laplacians on groups of Heisenberg type, by means of a more general and robust approach.","The study of the wave equation on wider classes of $2$-step Carnot groups via this approach will pose further challenges that we plan to address in subsequent works."],"url":"http://arxiv.org/abs/2406.04315v1","category":"math.AP"}
{"created":"2024-06-06 17:57:09","title":"Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step","abstract":"Recently, Direct Preference Optimization (DPO) has extended its success from aligning large language models (LLMs) to aligning text-to-image diffusion models with human preferences. Unlike most existing DPO methods that assume all diffusion steps share a consistent preference order with the final generated images, we argue that this assumption neglects step-specific denoising performance and that preference labels should be tailored to each step's contribution. To address this limitation, we propose Step-aware Preference Optimization (SPO), a novel post-training approach that independently evaluates and adjusts the denoising performance at each step, using a step-aware preference model and a step-wise resampler to ensure accurate step-aware supervision. Specifically, at each denoising step, we sample a pool of images, find a suitable win-lose pair, and, most importantly, randomly select a single image from the pool to initialize the next denoising step. This step-wise resampler process ensures the next win-lose image pair comes from the same image, making the win-lose comparison independent of the previous step. To assess the preferences at each step, we train a separate step-aware preference model that can be applied to both noisy and clean images. Our experiments with Stable Diffusion v1.5 and SDXL demonstrate that SPO significantly outperforms the latest Diffusion-DPO in aligning generated images with complex, detailed prompts and enhancing aesthetics, while also achieving more than 20x times faster in training efficiency. Code and model: https://rockeycoss.github.io/spo.github.io/","sentences":["Recently, Direct Preference Optimization (DPO) has extended its success from aligning large language models (LLMs) to aligning text-to-image diffusion models with human preferences.","Unlike most existing DPO methods that assume all diffusion steps share a consistent preference order with the final generated images, we argue that this assumption neglects step-specific denoising performance and that preference labels should be tailored to each step's contribution.","To address this limitation, we propose Step-aware Preference Optimization (SPO), a novel post-training approach that independently evaluates and adjusts the denoising performance at each step, using a step-aware preference model and a step-wise resampler to ensure accurate step-aware supervision.","Specifically, at each denoising step, we sample a pool of images, find a suitable win-lose pair, and, most importantly, randomly select a single image from the pool to initialize the next denoising step.","This step-wise resampler process ensures the next win-lose image pair comes from the same image, making the win-lose comparison independent of the previous step.","To assess the preferences at each step, we train a separate step-aware preference model that can be applied to both noisy and clean images.","Our experiments with Stable Diffusion v1.5 and SDXL demonstrate that SPO significantly outperforms the latest Diffusion-DPO in aligning generated images with complex, detailed prompts and enhancing aesthetics, while also achieving more than 20x times faster in training efficiency.","Code and model: https://rockeycoss.github.io/spo.github.io/"],"url":"http://arxiv.org/abs/2406.04314v1","category":"cs.CV"}
{"created":"2024-06-06 17:57:04","title":"Improving Alignment and Robustness with Short Circuiting","abstract":"AI systems can take harmful actions and are highly vulnerable to adversarial attacks. We present an approach, inspired by recent advances in representation engineering, that \"short-circuits\" models as they respond with harmful outputs. Existing techniques aimed at improving alignment, such as refusal training, are often bypassed. Techniques such as adversarial training try to plug these holes by countering specific attacks. As an alternative to refusal training and adversarial training, short-circuiting directly controls the representations that are responsible for harmful outputs in the first place. Our technique can be applied to both text-only and multimodal language models to prevent the generation of harmful outputs without sacrificing utility -- even in the presence of powerful unseen attacks. Notably, while adversarial robustness in standalone image recognition remains an open challenge, short-circuiting allows the larger multimodal system to reliably withstand image \"hijacks\" that aim to produce harmful content. Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack. Our approach represents a significant step forward in the development of reliable safeguards to harmful behavior and adversarial attacks.","sentences":["AI systems can take harmful actions and are highly vulnerable to adversarial attacks.","We present an approach, inspired by recent advances in representation engineering, that \"short-circuits\" models as they respond with harmful outputs.","Existing techniques aimed at improving alignment, such as refusal training, are often bypassed.","Techniques such as adversarial training try to plug these holes by countering specific attacks.","As an alternative to refusal training and adversarial training, short-circuiting directly controls the representations that are responsible for harmful outputs in the first place.","Our technique can be applied to both text-only and multimodal language models to prevent the generation of harmful outputs without sacrificing utility -- even in the presence of powerful unseen attacks.","Notably, while adversarial robustness in standalone image recognition remains an open challenge, short-circuiting allows the larger multimodal system to reliably withstand image \"hijacks\" that aim to produce harmful content.","Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack.","Our approach represents a significant step forward in the development of reliable safeguards to harmful behavior and adversarial attacks."],"url":"http://arxiv.org/abs/2406.04313v1","category":"cs.LG"}
{"created":"2024-06-06 17:56:40","title":"ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise Optimization","abstract":"Text-to-Image (T2I) models have made significant advancements in recent years, but they still struggle to accurately capture intricate details specified in complex compositional prompts. While fine-tuning T2I models with reward objectives has shown promise, it suffers from \"reward hacking\" and may not generalize well to unseen prompt distributions. In this work, we propose Reward-based Noise Optimization (ReNO), a novel approach that enhances T2I models at inference by optimizing the initial noise based on the signal from one or multiple human preference reward models. Remarkably, solving this optimization problem with gradient ascent for 50 iterations yields impressive results on four different one-step models across two competitive benchmarks, T2I-CompBench and GenEval. Within a computational budget of 20-50 seconds, ReNO-enhanced one-step models consistently surpass the performance of all current open-source Text-to-Image models. Extensive user studies demonstrate that our model is preferred nearly twice as often compared to the popular SDXL model and is on par with the proprietary Stable Diffusion 3 with 8B parameters. Moreover, given the same computational resources, a ReNO-optimized one-step model outperforms widely-used open-source models such as SDXL and PixArt-$\\alpha$, highlighting the efficiency and effectiveness of ReNO in enhancing T2I model performance at inference time. Code is available at https://github.com/ExplainableML/ReNO.","sentences":["Text-to-Image (T2I) models have made significant advancements in recent years, but they still struggle to accurately capture intricate details specified in complex compositional prompts.","While fine-tuning T2I models with reward objectives has shown promise, it suffers from \"reward hacking\" and may not generalize well to unseen prompt distributions.","In this work, we propose Reward-based Noise Optimization (ReNO), a novel approach that enhances T2I models at inference by optimizing the initial noise based on the signal from one or multiple human preference reward models.","Remarkably, solving this optimization problem with gradient ascent for 50 iterations yields impressive results on four different one-step models across two competitive benchmarks, T2I-CompBench and GenEval.","Within a computational budget of 20-50 seconds, ReNO-enhanced one-step models consistently surpass the performance of all current open-source Text-to-Image models.","Extensive user studies demonstrate that our model is preferred nearly twice as often compared to the popular SDXL model and is on par with the proprietary Stable Diffusion 3 with 8B parameters.","Moreover, given the same computational resources, a ReNO-optimized one-step model outperforms widely-used open-source models such as SDXL and PixArt-$\\alpha$, highlighting the efficiency and effectiveness of ReNO in enhancing T2I model performance at inference time.","Code is available at https://github.com/ExplainableML/ReNO."],"url":"http://arxiv.org/abs/2406.04312v1","category":"cs.CV"}
{"created":"2024-06-06 17:55:55","title":"Neural Networks Assisted Metropolis-Hastings for Bayesian Estimation of Critical Exponent on Elliptic Black Hole Solution in 4D Using Quantum Perturbation Theory","abstract":"The critical gravitational collapse is known to produce continuous self-similar solutions characterized by the Choptuik critical exponent, $\\gamma$. We examine all solutions within the complete domains of the linear perturbation equations, considering the numerical measurement errors. Specifically, we study quantum perturbation theory for the four-dimensional Einstein-axion-dilaton system of the elliptic class of $\\text{SL}(2,\\mathbb{R})$ transformations. We developed a novel artificial neural network-assisted Metropolis-Hastings based on quantum perturbation theory to find the critical exponent in a Bayesian framework. Unlike existing methods, this new probabilistic approach identifies the available deterministic solutions and explores the range of physically distinguishable critical exponents that may arise due to numerical measurement errors.","sentences":["The critical gravitational collapse is known to produce continuous self-similar solutions characterized by the Choptuik critical exponent, $\\gamma$. We examine all solutions within the complete domains of the linear perturbation equations, considering the numerical measurement errors.","Specifically, we study quantum perturbation theory for the four-dimensional Einstein-axion-dilaton system of the elliptic class of $\\text{SL}(2,\\mathbb{R})$ transformations.","We developed a novel artificial neural network-assisted Metropolis-Hastings based on quantum perturbation theory to find the critical exponent in a Bayesian framework.","Unlike existing methods, this new probabilistic approach identifies the available deterministic solutions and explores the range of physically distinguishable critical exponents that may arise due to numerical measurement errors."],"url":"http://arxiv.org/abs/2406.04310v1","category":"hep-th"}
{"created":"2024-06-06 17:54:26","title":"High-precision and low-depth eigenstate property estimation: theory and resource estimation","abstract":"Estimating the eigenstate properties of quantum many-body systems is a long-standing, challenging problem for both classical and quantum computing. For the task of eigenstate preparation, quantum signal processing (QSP) has established near-optimal query complexity $O( \\Delta^{-1} \\log(\\epsilon^{-1}) )$ by querying the block encoding of the Hamiltonian $H$ where $\\Delta$ is the energy gap and $\\epsilon$ is the target precision. However, QSP is challenging for both near-term noisy quantum computers and early fault-tolerant quantum computers (FTQC), which are limited by the number of logical qubits and circuit depth. To date, early FTQC algorithms have focused on querying the perfect time evolution $e^{-iHt}$. It remains uncertain whether early FTQC algorithms can maintain good asymptotic scaling at the gate level. Moreover, when considering qubit connectivity, the circuit depth of existing FTQC algorithms may scale suboptimally with system size. Here, we present a full-stack design of a random sampling algorithm for estimating the eigenenergy and the observable expectations on the eigenstates, which can achieve high precision and good system size scaling. The gate complexity has a logarithmic dependence on precision $ {O}(\\log^{1+o(1)} (1/\\epsilon))$ for generic Hamiltonians, which cannot achieved by methods using Trottersiation to realise $e^{-iHt}$ like in QETU. For $n$-qubit lattice Hamiltonians, our method achieves near-optimal system size dependence with the gate complexity $O(n^{1+o(1)})$. When restricting the qubit connectivity to a linear nearest-neighbour architecture, The method shows advantages in circuit depth, with $O(n^{o(1)})$ for lattice models and $O(n^{2+o(1)})$ for electronic structure problems. We compare the resource requirements (CNOT gates, T gates and qubit numbers) by phase estimation, QSP, and QETU, in lattice and molecular problems.","sentences":["Estimating the eigenstate properties of quantum many-body systems is a long-standing, challenging problem for both classical and quantum computing.","For the task of eigenstate preparation, quantum signal processing (QSP) has established near-optimal query complexity $O( \\Delta^{-1} \\log(\\epsilon^{-1}) )","$ by querying the block encoding of the Hamiltonian $H$ where $\\Delta$ is the energy gap and $\\epsilon$ is the target precision.","However, QSP is challenging for both near-term noisy quantum computers and early fault-tolerant quantum computers (FTQC), which are limited by the number of logical qubits and circuit depth.","To date, early FTQC algorithms have focused on querying the perfect time evolution $e^{-iHt}$. It remains uncertain whether early FTQC algorithms can maintain good asymptotic scaling at the gate level.","Moreover, when considering qubit connectivity, the circuit depth of existing FTQC algorithms may scale suboptimally with system size.","Here, we present a full-stack design of a random sampling algorithm for estimating the eigenenergy and the observable expectations on the eigenstates, which can achieve high precision and good system size scaling.","The gate complexity has a logarithmic dependence on precision $ {O}(\\log^{1+o(1)} (1/\\epsilon))$ for generic Hamiltonians, which cannot achieved by methods using Trottersiation to realise $e^{-iHt}$ like in QETU.","For $n$-qubit lattice Hamiltonians, our method achieves near-optimal system size dependence with the gate complexity $O(n^{1+o(1)})$. When restricting the qubit connectivity to a linear nearest-neighbour architecture, The method shows advantages in circuit depth, with $O(n^{o(1)})$ for lattice models and $O(n^{2+o(1)})$ for electronic structure problems.","We compare the resource requirements (CNOT gates, T gates and qubit numbers) by phase estimation, QSP, and QETU, in lattice and molecular problems."],"url":"http://arxiv.org/abs/2406.04307v1","category":"quant-ph"}
{"created":"2024-06-06 17:53:34","title":"Semantically Diverse Language Generation for Uncertainty Estimation in Language Models","abstract":"Large language models (LLMs) can suffer from hallucinations when generating text. These hallucinations impede various applications in society and industry by making LLMs untrustworthy. Current LLMs generate text in an autoregressive fashion by predicting and appending text tokens. When an LLM is uncertain about the semantic meaning of the next tokens to generate, it is likely to start hallucinating. Thus, it has been suggested that hallucinations stem from predictive uncertainty. We introduce Semantically Diverse Language Generation (SDLG) to quantify predictive uncertainty in LLMs. SDLG steers the LLM to generate semantically diverse yet likely alternatives for an initially generated text. This approach provides a precise measure of aleatoric semantic uncertainty, detecting whether the initial text is likely to be hallucinated. Experiments on question-answering tasks demonstrate that SDLG consistently outperforms existing methods while being the most computationally efficient, setting a new standard for uncertainty estimation in LLMs.","sentences":["Large language models (LLMs) can suffer from hallucinations when generating text.","These hallucinations impede various applications in society and industry by making LLMs untrustworthy.","Current LLMs generate text in an autoregressive fashion by predicting and appending text tokens.","When an LLM is uncertain about the semantic meaning of the next tokens to generate, it is likely to start hallucinating.","Thus, it has been suggested that hallucinations stem from predictive uncertainty.","We introduce Semantically Diverse Language Generation (SDLG) to quantify predictive uncertainty in LLMs.","SDLG steers the LLM to generate semantically diverse yet likely alternatives for an initially generated text.","This approach provides a precise measure of aleatoric semantic uncertainty, detecting whether the initial text is likely to be hallucinated.","Experiments on question-answering tasks demonstrate that SDLG consistently outperforms existing methods while being the most computationally efficient, setting a new standard for uncertainty estimation in LLMs."],"url":"http://arxiv.org/abs/2406.04306v1","category":"cs.LG"}
{"created":"2024-06-06 17:52:05","title":"Quixer: A Quantum Transformer Model","abstract":"Progress in the realisation of reliable large-scale quantum computers has motivated research into the design of quantum machine learning models. We present Quixer: a novel quantum transformer model which utilises the Linear Combination of Unitaries and Quantum Singular Value Transform primitives as building blocks. Quixer operates by preparing a superposition of tokens and applying a trainable non-linear transformation to this mix. We present the first results for a quantum transformer model applied to a practical language modelling task, obtaining results competitive with an equivalent classical baseline. In addition, we include resource estimates for evaluating the model on quantum hardware, and provide an open-source implementation for classical simulation. We conclude by highlighting the generality of Quixer, showing that its parameterised components can be substituted with fixed structures to yield new classes of quantum transformers.","sentences":["Progress in the realisation of reliable large-scale quantum computers has motivated research into the design of quantum machine learning models.","We present Quixer: a novel quantum transformer model which utilises the Linear Combination of Unitaries and Quantum Singular Value Transform primitives as building blocks.","Quixer operates by preparing a superposition of tokens and applying a trainable non-linear transformation to this mix.","We present the first results for a quantum transformer model applied to a practical language modelling task, obtaining results competitive with an equivalent classical baseline.","In addition, we include resource estimates for evaluating the model on quantum hardware, and provide an open-source implementation for classical simulation.","We conclude by highlighting the generality of Quixer, showing that its parameterised components can be substituted with fixed structures to yield new classes of quantum transformers."],"url":"http://arxiv.org/abs/2406.04305v1","category":"quant-ph"}
{"created":"2024-06-06 17:49:21","title":"Vision-LSTM: xLSTM as Generic Vision Backbone","abstract":"Transformers are widely used as generic backbones in computer vision, despite initially introduced for natural language processing. Recently, the Long Short-Term Memory (LSTM) has been extended to a scalable and performant architecture - the xLSTM - which overcomes long-standing LSTM limitations via exponential gating and parallelizable matrix memory structure. In this report, we introduce Vision-LSTM (ViL), an adaption of the xLSTM building blocks to computer vision. ViL comprises a stack of xLSTM blocks where odd blocks process the sequence of patch tokens from top to bottom while even blocks go from bottom to top. Experiments show that ViL holds promise to be further deployed as new generic backbone for computer vision architectures.","sentences":["Transformers are widely used as generic backbones in computer vision, despite initially introduced for natural language processing.","Recently, the Long Short-Term Memory (LSTM) has been extended to a scalable and performant architecture - the xLSTM - which overcomes long-standing LSTM limitations via exponential gating and parallelizable matrix memory structure.","In this report, we introduce Vision-LSTM (ViL), an adaption of the xLSTM building blocks to computer vision.","ViL comprises a stack of xLSTM blocks where odd blocks process the sequence of patch tokens from top to bottom while even blocks go from bottom to top.","Experiments show that ViL holds promise to be further deployed as new generic backbone for computer vision architectures."],"url":"http://arxiv.org/abs/2406.04303v1","category":"cs.CV"}
{"created":"2024-06-06 17:47:48","title":"Neural Surface Reconstruction from Sparse Views Using Epipolar Geometry","abstract":"This paper addresses the challenge of reconstructing surfaces from sparse view inputs, where ambiguity and occlusions due to missing information pose significant hurdles. We present a novel approach, named EpiS, that incorporates Epipolar information into the reconstruction process. Existing methods in sparse-view neural surface learning have mainly focused on mean and variance considerations using cost volumes for feature extraction. In contrast, our method aggregates coarse information from the cost volume into Epipolar features extracted from multiple source views, enabling the generation of fine-grained Signal Distance Function (SDF)-aware features. Additionally, we employ an attention mechanism along the line dimension to facilitate feature fusion based on the SDF feature. Furthermore, to address the information gaps in sparse conditions, we integrate depth information from monocular depth estimation using global and local regularization techniques. The global regularization utilizes a triplet loss function, while the local regularization employs a derivative loss function. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods, especially in cases with sparse and generalizable conditions.","sentences":["This paper addresses the challenge of reconstructing surfaces from sparse view inputs, where ambiguity and occlusions due to missing information pose significant hurdles.","We present a novel approach, named EpiS, that incorporates Epipolar information into the reconstruction process.","Existing methods in sparse-view neural surface learning have mainly focused on mean and variance considerations using cost volumes for feature extraction.","In contrast, our method aggregates coarse information from the cost volume into Epipolar features extracted from multiple source views, enabling the generation of fine-grained Signal Distance Function (SDF)-aware features.","Additionally, we employ an attention mechanism along the line dimension to facilitate feature fusion based on the SDF feature.","Furthermore, to address the information gaps in sparse conditions, we integrate depth information from monocular depth estimation using global and local regularization techniques.","The global regularization utilizes a triplet loss function, while the local regularization employs a derivative loss function.","Extensive experiments demonstrate that our approach outperforms state-of-the-art methods, especially in cases with sparse and generalizable conditions."],"url":"http://arxiv.org/abs/2406.04301v1","category":"cs.CV"}
{"created":"2024-06-06 17:47:41","title":"Text-to-Drive: Diverse Driving Behavior Synthesis via Large Language Models","abstract":"Generating varied scenarios through simulation is crucial for training and evaluating safety-critical systems, such as autonomous vehicles. Yet, the task of modeling the trajectories of other vehicles to simulate diverse and meaningful close interactions remains prohibitively costly. Adopting language descriptions to generate driving behaviors emerges as a promising strategy, offering a scalable and intuitive method for human operators to simulate a wide range of driving interactions. However, the scarcity of large-scale annotated language-trajectory data makes this approach challenging.   To address this gap, we propose Text-to-Drive (T2D) to synthesize diverse driving behaviors via Large Language Models (LLMs). We introduce a knowledge-driven approach that operates in two stages. In the first stage, we employ the embedded knowledge of LLMs to generate diverse language descriptions of driving behaviors for a scene. Then, we leverage LLM's reasoning capabilities to synthesize these behaviors in simulation. At its core, T2D employs an LLM to construct a state chart that maps low-level states to high-level abstractions. This strategy aids in downstream tasks such as summarizing low-level observations, assessing policy alignment with behavior description, and shaping the auxiliary reward, all without needing human supervision. With our knowledge-driven approach, we demonstrate that T2D generates more diverse trajectories compared to other baselines and offers a natural language interface that allows for interactive incorporation of human preference. Please check our website for more examples: https://text-to-drive.github.io/","sentences":["Generating varied scenarios through simulation is crucial for training and evaluating safety-critical systems, such as autonomous vehicles.","Yet, the task of modeling the trajectories of other vehicles to simulate diverse and meaningful close interactions remains prohibitively costly.","Adopting language descriptions to generate driving behaviors emerges as a promising strategy, offering a scalable and intuitive method for human operators to simulate a wide range of driving interactions.","However, the scarcity of large-scale annotated language-trajectory data makes this approach challenging.   ","To address this gap, we propose Text-to-Drive (T2D) to synthesize diverse driving behaviors via Large Language Models (LLMs).","We introduce a knowledge-driven approach that operates in two stages.","In the first stage, we employ the embedded knowledge of LLMs to generate diverse language descriptions of driving behaviors for a scene.","Then, we leverage LLM's reasoning capabilities to synthesize these behaviors in simulation.","At its core, T2D employs an LLM to construct a state chart that maps low-level states to high-level abstractions.","This strategy aids in downstream tasks such as summarizing low-level observations, assessing policy alignment with behavior description, and shaping the auxiliary reward, all without needing human supervision.","With our knowledge-driven approach, we demonstrate that T2D generates more diverse trajectories compared to other baselines and offers a natural language interface that allows for interactive incorporation of human preference.","Please check our website for more examples: https://text-to-drive.github.io/"],"url":"http://arxiv.org/abs/2406.04300v1","category":"cs.RO"}
{"created":"2024-06-06 17:42:37","title":"Measuring and Addressing Indexical Bias in Information Retrieval","abstract":"Information Retrieval (IR) systems are designed to deliver relevant content, but traditional systems may not optimize rankings for fairness, neutrality, or the balance of ideas. Consequently, IR can often introduce indexical biases, or biases in the positional order of documents. Although indexical bias can demonstrably affect people's opinion, voting patterns, and other behaviors, these issues remain understudied as the field lacks reliable metrics and procedures for automatically measuring indexical bias. Towards this end, we introduce the PAIR framework, which supports automatic bias audits for ranked documents or entire IR systems. After introducing DUO, the first general-purpose automatic bias metric, we run an extensive evaluation of 8 IR systems on a new corpus of 32k synthetic and 4.7k natural documents, with 4k queries spanning 1.4k controversial issue topics. A human behavioral study validates our approach, showing that our bias metric can help predict when and how indexical bias will shift a reader's opinion.","sentences":["Information Retrieval (IR) systems are designed to deliver relevant content, but traditional systems may not optimize rankings for fairness, neutrality, or the balance of ideas.","Consequently, IR can often introduce indexical biases, or biases in the positional order of documents.","Although indexical bias can demonstrably affect people's opinion, voting patterns, and other behaviors, these issues remain understudied as the field lacks reliable metrics and procedures for automatically measuring indexical bias.","Towards this end, we introduce the PAIR framework, which supports automatic bias audits for ranked documents or entire IR systems.","After introducing DUO, the first general-purpose automatic bias metric, we run an extensive evaluation of 8 IR systems on a new corpus of 32k synthetic and 4.7k natural documents, with 4k queries spanning 1.4k controversial issue topics.","A human behavioral study validates our approach, showing that our bias metric can help predict when and how indexical bias will shift a reader's opinion."],"url":"http://arxiv.org/abs/2406.04298v1","category":"cs.IR"}
{"created":"2024-06-06 17:40:22","title":"Translation symmetry restoration under random unitary dynamics","abstract":"The finite parts of a large, locally interacting many-body system prepared out-of-equilibrium eventually equilibrate. Characterising the underlying mechanisms of this process and its timescales, however, is particularly hard as it requires to decouple universal features from observable-specific ones. Recently, new insight came by studying how certain symmetries of the dynamics that are broken by the initial state are restored at the level of the reduced state of a given subsystem. This provides a high level, observable-independent probe. Until now this idea has been applied to the restoration of internal symmetries, e.g. U(1) symmetries related to charge conservation. Here we show that that the same logic can be applied to the restoration of space-time symmetries, and hence can be used to characterise the relaxation of fully generic systems. We illustrate this idea by considering the paradigmatic example of \"generic\" many-body dynamics, i.e. a local random unitary circuit. We show that, surprisingly, the restoration of translation symmetry in these systems only happens on time-scales proportional to the subsystem's volume. In fact, for large enough subsystems the time of symmetry restoration becomes initial-state independent (as long as the latter breaks the symmetry at time zero) and coincides with the thermalisation time. For intermediate subsystems, however, one can observe the so-called \"quantum Mpemba effect\", where the state of the system restores a symmetry faster if it is initially more asymmetric.","sentences":["The finite parts of a large, locally interacting many-body system prepared out-of-equilibrium eventually equilibrate.","Characterising the underlying mechanisms of this process and its timescales, however, is particularly hard as it requires to decouple universal features from observable-specific ones.","Recently, new insight came by studying how certain symmetries of the dynamics that are broken by the initial state are restored at the level of the reduced state of a given subsystem.","This provides a high level, observable-independent probe.","Until now this idea has been applied to the restoration of internal symmetries, e.g. U(1) symmetries related to charge conservation.","Here we show that that the same logic can be applied to the restoration of space-time symmetries, and hence can be used to characterise the relaxation of fully generic systems.","We illustrate this idea by considering the paradigmatic example of \"generic\" many-body dynamics, i.e. a local random unitary circuit.","We show that, surprisingly, the restoration of translation symmetry in these systems only happens on time-scales proportional to the subsystem's volume.","In fact, for large enough subsystems the time of symmetry restoration becomes initial-state independent (as long as the latter breaks the symmetry at time zero) and coincides with the thermalisation time.","For intermediate subsystems, however, one can observe the so-called \"quantum Mpemba effect\", where the state of the system restores a symmetry faster if it is initially more asymmetric."],"url":"http://arxiv.org/abs/2406.04296v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-06 17:39:09","title":"Everything to the Synthetic: Diffusion-driven Test-time Adaptation via Synthetic-Domain Alignment","abstract":"Test-time adaptation (TTA) aims to enhance the performance of source-domain pretrained models when tested on unknown shifted target domains. Traditional TTA methods primarily adapt model weights based on target data streams, making model performance sensitive to the amount and order of target data. Recently, diffusion-driven TTA methods have demonstrated strong performance by using an unconditional diffusion model, which is also trained on the source domain to transform target data into synthetic data as a source domain projection. This allows the source model to make predictions without weight adaptation. In this paper, we argue that the domains of the source model and the synthetic data in diffusion-driven TTA methods are not aligned. To adapt the source model to the synthetic domain of the unconditional diffusion model, we introduce a Synthetic-Domain Alignment (SDA) framework to fine-tune the source model with synthetic data. Specifically, we first employ a conditional diffusion model to generate labeled samples, creating a synthetic dataset. Subsequently, we use the aforementioned unconditional diffusion model to add noise to and denoise each sample before fine-tuning. This process mitigates the potential domain gap between the conditional and unconditional models. Extensive experiments across various models and benchmarks demonstrate that SDA achieves superior domain alignment and consistently outperforms existing diffusion-driven TTA methods. Our code is available at https://github.com/SHI-Labs/Diffusion-Driven-Test-Time-Adaptation-via-Synthetic-Domain-Alignment.","sentences":["Test-time adaptation (TTA) aims to enhance the performance of source-domain pretrained models when tested on unknown shifted target domains.","Traditional TTA methods primarily adapt model weights based on target data streams, making model performance sensitive to the amount and order of target data.","Recently, diffusion-driven TTA methods have demonstrated strong performance by using an unconditional diffusion model, which is also trained on the source domain to transform target data into synthetic data as a source domain projection.","This allows the source model to make predictions without weight adaptation.","In this paper, we argue that the domains of the source model and the synthetic data in diffusion-driven TTA methods are not aligned.","To adapt the source model to the synthetic domain of the unconditional diffusion model, we introduce a Synthetic-Domain Alignment (SDA) framework to fine-tune the source model with synthetic data.","Specifically, we first employ a conditional diffusion model to generate labeled samples, creating a synthetic dataset.","Subsequently, we use the aforementioned unconditional diffusion model to add noise to and denoise each sample before fine-tuning.","This process mitigates the potential domain gap between the conditional and unconditional models.","Extensive experiments across various models and benchmarks demonstrate that SDA achieves superior domain alignment and consistently outperforms existing diffusion-driven TTA methods.","Our code is available at https://github.com/SHI-Labs/Diffusion-Driven-Test-Time-Adaptation-via-Synthetic-Domain-Alignment."],"url":"http://arxiv.org/abs/2406.04295v1","category":"cs.CV"}
{"created":"2024-06-06 17:37:47","title":"VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval","abstract":"Multi-modal retrieval becomes increasingly popular in practice. However, the existing retrievers are mostly text-oriented, which lack the capability to process visual information. Despite the presence of vision-language models like CLIP, the current methods are severely limited in representing the text-only and image-only data. In this work, we present a new embedding model VISTA for universal multi-modal retrieval. Our work brings forth threefold technical contributions. Firstly, we introduce a flexible architecture which extends a powerful text encoder with the image understanding capability by introducing visual token embeddings. Secondly, we develop two data generation strategies, which bring high-quality composed image-text to facilitate the training of the embedding model. Thirdly, we introduce a multi-stage training algorithm, which first aligns the visual token embedding with the text encoder using massive weakly labeled data, and then develops multi-modal representation capability using the generated composed image-text data. In our experiments, VISTA achieves superior performances across a variety of multi-modal retrieval tasks in both zero-shot and supervised settings. Our model, data, and source code are available at https://github.com/FlagOpen/FlagEmbedding.","sentences":["Multi-modal retrieval becomes increasingly popular in practice.","However, the existing retrievers are mostly text-oriented, which lack the capability to process visual information.","Despite the presence of vision-language models like CLIP, the current methods are severely limited in representing the text-only and image-only data.","In this work, we present a new embedding model VISTA for universal multi-modal retrieval.","Our work brings forth threefold technical contributions.","Firstly, we introduce a flexible architecture which extends a powerful text encoder with the image understanding capability by introducing visual token embeddings.","Secondly, we develop two data generation strategies, which bring high-quality composed image-text to facilitate the training of the embedding model.","Thirdly, we introduce a multi-stage training algorithm, which first aligns the visual token embedding with the text encoder using massive weakly labeled data, and then develops multi-modal representation capability using the generated composed image-text data.","In our experiments, VISTA achieves superior performances across a variety of multi-modal retrieval tasks in both zero-shot and supervised settings.","Our model, data, and source code are available at https://github.com/FlagOpen/FlagEmbedding."],"url":"http://arxiv.org/abs/2406.04292v1","category":"cs.IR"}
{"created":"2024-06-06 17:29:57","title":"ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions","abstract":"We present ABEX, a novel and effective generative data augmentation methodology for low-resource Natural Language Understanding (NLU) tasks. ABEX is based on ABstract-and-EXpand, a novel paradigm for generating diverse forms of an input document -- we first convert a document into its concise, abstract description and then generate new documents based on expanding the resultant abstraction. To learn the task of expanding abstract descriptions, we first train BART on a large-scale synthetic dataset with abstract-document pairs. Next, to generate abstract descriptions for a document, we propose a simple, controllable, and training-free method based on editing AMR graphs. ABEX brings the best of both worlds: by expanding from abstract representations, it preserves the original semantic properties of the documents, like style and meaning, thereby maintaining alignment with the original label and data distribution. At the same time, the fundamental process of elaborating on abstract descriptions facilitates diverse generations. We demonstrate the effectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource settings. ABEX outperforms all our baselines qualitatively with improvements of 0.04% - 38.8%. Qualitatively, ABEX outperforms all prior methods from literature in terms of context and length diversity.","sentences":["We present ABEX, a novel and effective generative data augmentation methodology for low-resource Natural Language Understanding (NLU) tasks.","ABEX is based on ABstract-and-EXpand, a novel paradigm for generating diverse forms of an input document -- we first convert a document into its concise, abstract description and then generate new documents based on expanding the resultant abstraction.","To learn the task of expanding abstract descriptions, we first train BART on a large-scale synthetic dataset with abstract-document pairs.","Next, to generate abstract descriptions for a document, we propose a simple, controllable, and training-free method based on editing AMR graphs.","ABEX brings the best of both worlds: by expanding from abstract representations, it preserves the original semantic properties of the documents, like style and meaning, thereby maintaining alignment with the original label and data distribution.","At the same time, the fundamental process of elaborating on abstract descriptions facilitates diverse generations.","We demonstrate the effectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource settings.","ABEX outperforms all our baselines qualitatively with improvements of 0.04% - 38.8%.","Qualitatively, ABEX outperforms all prior methods from literature in terms of context and length diversity."],"url":"http://arxiv.org/abs/2406.04286v1","category":"cs.CL"}
{"created":"2024-06-06 17:28:11","title":"A Statistical Characterization of Wireless Channels Conditioned on Side Information","abstract":"Statistical prior channel knowledge, such as the wide-sense-stationary-uncorrelated-scattering (WSSUS) property, and additional side information both can be used to enhance physical layer applications in wireless communication. Generally, the wireless channel's strongly fluctuating path phases and WSSUS property characterize the channel by a zero mean and Toeplitz-structured covariance matrices in different domains. In this work, we derive a framework to comprehensively categorize side information based on whether it preserves or abandons these statistical features conditioned on the given side information. To accomplish this, we combine insights from a generic channel model with the representation of wireless channels as probabilistic graphs. Additionally, we exemplify several applications, ranging from channel modeling to estimation and clustering, which demonstrate how the proposed framework can practically enhance physical layer methods utilizing machine learning (ML).","sentences":["Statistical prior channel knowledge, such as the wide-sense-stationary-uncorrelated-scattering (WSSUS) property, and additional side information both can be used to enhance physical layer applications in wireless communication.","Generally, the wireless channel's strongly fluctuating path phases and WSSUS property characterize the channel by a zero mean and Toeplitz-structured covariance matrices in different domains.","In this work, we derive a framework to comprehensively categorize side information based on whether it preserves or abandons these statistical features conditioned on the given side information.","To accomplish this, we combine insights from a generic channel model with the representation of wireless channels as probabilistic graphs.","Additionally, we exemplify several applications, ranging from channel modeling to estimation and clustering, which demonstrate how the proposed framework can practically enhance physical layer methods utilizing machine learning (ML)."],"url":"http://arxiv.org/abs/2406.04282v1","category":"eess.SP"}
{"created":"2024-06-06 17:27:09","title":"Total-Duration-Aware Duration Modeling for Text-to-Speech Systems","abstract":"Accurate control of the total duration of generated speech by adjusting the speech rate is crucial for various text-to-speech (TTS) applications. However, the impact of adjusting the speech rate on speech quality, such as intelligibility and speaker characteristics, has been underexplored. In this work, we propose a novel total-duration-aware (TDA) duration model for TTS, where phoneme durations are predicted not only from the text input but also from an additional input of the total target duration. We also propose a MaskGIT-based duration model that enhances the diversity and quality of the predicted phoneme durations. Our results demonstrate that the proposed TDA duration models achieve better intelligibility and speaker similarity for various speech rate configurations compared to the baseline models. We also show that the proposed MaskGIT-based model can generate phoneme durations with higher quality and diversity compared to its regression or flow-matching counterparts.","sentences":["Accurate control of the total duration of generated speech by adjusting the speech rate is crucial for various text-to-speech (TTS) applications.","However, the impact of adjusting the speech rate on speech quality, such as intelligibility and speaker characteristics, has been underexplored.","In this work, we propose a novel total-duration-aware (TDA) duration model for TTS, where phoneme durations are predicted not only from the text input but also from an additional input of the total target duration.","We also propose a MaskGIT-based duration model that enhances the diversity and quality of the predicted phoneme durations.","Our results demonstrate that the proposed TDA duration models achieve better intelligibility and speaker similarity for various speech rate configurations compared to the baseline models.","We also show that the proposed MaskGIT-based model can generate phoneme durations with higher quality and diversity compared to its regression or flow-matching counterparts."],"url":"http://arxiv.org/abs/2406.04281v1","category":"eess.AS"}
{"created":"2024-06-06 17:26:40","title":"xMIL: Insightful Explanations for Multiple Instance Learning in Histopathology","abstract":"Multiple instance learning (MIL) is an effective and widely used approach for weakly supervised machine learning. In histopathology, MIL models have achieved remarkable success in tasks like tumor detection, biomarker prediction, and outcome prognostication. However, MIL explanation methods are still lagging behind, as they are limited to small bag sizes or disregard instance interactions. We revisit MIL through the lens of explainable AI (XAI) and introduce xMIL, a refined framework with more general assumptions. We demonstrate how to obtain improved MIL explanations using layer-wise relevance propagation (LRP) and conduct extensive evaluation experiments on three toy settings and four real-world histopathology datasets. Our approach consistently outperforms previous explanation attempts with particularly improved faithfulness scores on challenging biomarker prediction tasks. Finally, we showcase how xMIL explanations enable pathologists to extract insights from MIL models, representing a significant advance for knowledge discovery and model debugging in digital histopathology.","sentences":["Multiple instance learning (MIL) is an effective and widely used approach for weakly supervised machine learning.","In histopathology, MIL models have achieved remarkable success in tasks like tumor detection, biomarker prediction, and outcome prognostication.","However, MIL explanation methods are still lagging behind, as they are limited to small bag sizes or disregard instance interactions.","We revisit MIL through the lens of explainable AI (XAI) and introduce xMIL, a refined framework with more general assumptions.","We demonstrate how to obtain improved MIL explanations using layer-wise relevance propagation (LRP) and conduct extensive evaluation experiments on three toy settings and four real-world histopathology datasets.","Our approach consistently outperforms previous explanation attempts with particularly improved faithfulness scores on challenging biomarker prediction tasks.","Finally, we showcase how xMIL explanations enable pathologists to extract insights from MIL models, representing a significant advance for knowledge discovery and model debugging in digital histopathology."],"url":"http://arxiv.org/abs/2406.04280v1","category":"cs.LG"}
{"created":"2024-06-06 17:26:00","title":"Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People","abstract":"Conversational tones -- the manners and attitudes in which speakers communicate -- are essential to effective communication. Amidst the increasing popularization of Large Language Models (LLMs) over recent years, it becomes necessary to characterize the divergences in their conversational tones relative to humans. However, existing investigations of conversational modalities rely on pre-existing taxonomies or text corpora, which suffer from experimenter bias and may not be representative of real-world distributions for the studies' psycholinguistic domains. Inspired by methods from cognitive science, we propose an iterative method for simultaneously eliciting conversational tones and sentences, where participants alternate between two tasks: (1) one participant identifies the tone of a given sentence and (2) a different participant generates a sentence based on that tone. We run 100 iterations of this process with human participants and GPT-4, then obtain a dataset of sentences and frequent conversational tones. In an additional experiment, humans and GPT-4 annotated all sentences with all tones. With data from 1,339 human participants, 33,370 human judgments, and 29,900 GPT-4 queries, we show how our approach can be used to create an interpretable geometric representation of relations between conversational tones in humans and GPT-4. This work demonstrates how combining ideas from machine learning and cognitive science can address challenges in human-computer interactions.","sentences":["Conversational tones -- the manners and attitudes in which speakers communicate -- are essential to effective communication.","Amidst the increasing popularization of Large Language Models (LLMs) over recent years, it becomes necessary to characterize the divergences in their conversational tones relative to humans.","However, existing investigations of conversational modalities rely on pre-existing taxonomies or text corpora, which suffer from experimenter bias and may not be representative of real-world distributions for the studies' psycholinguistic domains.","Inspired by methods from cognitive science, we propose an iterative method for simultaneously eliciting conversational tones and sentences, where participants alternate between two tasks: (1) one participant identifies the tone of a given sentence and (2) a different participant generates a sentence based on that tone.","We run 100 iterations of this process with human participants and GPT-4, then obtain a dataset of sentences and frequent conversational tones.","In an additional experiment, humans and GPT-4 annotated all sentences with all tones.","With data from 1,339 human participants, 33,370 human judgments, and 29,900 GPT-4 queries, we show how our approach can be used to create an interpretable geometric representation of relations between conversational tones in humans and GPT-4.","This work demonstrates how combining ideas from machine learning and cognitive science can address challenges in human-computer interactions."],"url":"http://arxiv.org/abs/2406.04278v1","category":"cs.CL"}
{"created":"2024-06-06 17:25:33","title":"VideoTetris: Towards Compositional Text-to-Video Generation","abstract":"Diffusion models have demonstrated great success in text-to-video (T2V) generation. However, existing methods may face challenges when handling complex (long) video generation scenarios that involve multiple objects or dynamic changes in object numbers. To address these limitations, we propose VideoTetris, a novel framework that enables compositional T2V generation. Specifically, we propose spatio-temporal compositional diffusion to precisely follow complex textual semantics by manipulating and composing the attention maps of denoising networks spatially and temporally. Moreover, we propose an enhanced video data preprocessing to enhance the training data regarding motion dynamics and prompt understanding, equipped with a new reference frame attention mechanism to improve the consistency of auto-regressive video generation. Extensive experiments demonstrate that our VideoTetris achieves impressive qualitative and quantitative results in compositional T2V generation. Code is available at: https://github.com/YangLing0818/VideoTetris","sentences":["Diffusion models have demonstrated great success in text-to-video (T2V) generation.","However, existing methods may face challenges when handling complex (long) video generation scenarios that involve multiple objects or dynamic changes in object numbers.","To address these limitations, we propose VideoTetris, a novel framework that enables compositional T2V generation.","Specifically, we propose spatio-temporal compositional diffusion to precisely follow complex textual semantics by manipulating and composing the attention maps of denoising networks spatially and temporally.","Moreover, we propose an enhanced video data preprocessing to enhance the training data regarding motion dynamics and prompt understanding, equipped with a new reference frame attention mechanism to improve the consistency of auto-regressive video generation.","Extensive experiments demonstrate that our VideoTetris achieves impressive qualitative and quantitative results in compositional T2V generation.","Code is available at: https://github.com/YangLing0818/VideoTetris"],"url":"http://arxiv.org/abs/2406.04277v1","category":"cs.CV"}
{"created":"2024-06-06 17:25:07","title":"Generative AI-in-the-loop: Integrating LLMs and GPTs into the Next Generation Networks","abstract":"In recent years, machine learning (ML) techniques have created numerous opportunities for intelligent mobile networks and have accelerated the automation of network operations. However, complex network tasks may involve variables and considerations even beyond the capacity of traditional ML algorithms. On the other hand, large language models (LLMs) have recently emerged, demonstrating near-human-level performance in cognitive tasks across various fields. However, they remain prone to hallucinations and often lack common sense in basic tasks. Therefore, they are regarded as assistive tools for humans. In this work, we propose the concept of \"generative AI-in-the-loop\" and utilize the semantic understanding, context awareness, and reasoning abilities of LLMs to assist humans in handling complex or unforeseen situations in mobile communication networks. We believe that combining LLMs and ML models allows both to leverage their respective capabilities and achieve better results than either model alone. To support this idea, we begin by analyzing the capabilities of LLMs and compare them with traditional ML algorithms. We then explore potential LLM-based applications in line with the requirements of next-generation networks. We further examine the integration of ML and LLMs, discussing how they can be used together in mobile networks. Unlike existing studies, our research emphasizes the fusion of LLMs with traditional ML-driven next-generation networks and serves as a comprehensive refinement of existing surveys. Finally, we provide a case study to enhance ML-based network intrusion detection with synthesized data generated by LLMs. Our case study further demonstrates the advantages of our proposed idea.","sentences":["In recent years, machine learning (ML) techniques have created numerous opportunities for intelligent mobile networks and have accelerated the automation of network operations.","However, complex network tasks may involve variables and considerations even beyond the capacity of traditional ML algorithms.","On the other hand, large language models (LLMs) have recently emerged, demonstrating near-human-level performance in cognitive tasks across various fields.","However, they remain prone to hallucinations and often lack common sense in basic tasks.","Therefore, they are regarded as assistive tools for humans.","In this work, we propose the concept of \"generative AI-in-the-loop\" and utilize the semantic understanding, context awareness, and reasoning abilities of LLMs to assist humans in handling complex or unforeseen situations in mobile communication networks.","We believe that combining LLMs and ML models allows both to leverage their respective capabilities and achieve better results than either model alone.","To support this idea, we begin by analyzing the capabilities of LLMs and compare them with traditional ML algorithms.","We then explore potential LLM-based applications in line with the requirements of next-generation networks.","We further examine the integration of ML and LLMs, discussing how they can be used together in mobile networks.","Unlike existing studies, our research emphasizes the fusion of LLMs with traditional ML-driven next-generation networks and serves as a comprehensive refinement of existing surveys.","Finally, we provide a case study to enhance ML-based network intrusion detection with synthesized data generated by LLMs.","Our case study further demonstrates the advantages of our proposed idea."],"url":"http://arxiv.org/abs/2406.04276v1","category":"cs.LG"}
{"created":"2024-06-06 17:23:54","title":"Interfacing Gottesman-Kitaev-Preskill Qubits to Quantum Memories","abstract":"Gottesman-Kitaev-Preskill (GKP) states have been demonstrated to pose significant advantages when utilized for fault-tolerant all optical continuous-variable quantum computing as well as for quantum communications links for entanglement distribution. However interfacing these systems to long-lived solid-state quantum memories has remained an open problem. Here we propose an interface between quantum memories and GKP qubit states based on a cavity-mediated controlled displacement gate. We characterize the quality of memory-GKP entanglement as a function of cavity parameters suggesting optimal regimes of operation for high-quality state transfer between either qubit states. We further extend this protocol to demonstrate the creation of GKP cluster states by avoiding the requirement of ancillary optical quadrature-squeezed light. Utilizing post-selected entanglement swapping operations for GKP qubits, we demonstrate the utility of our protocol for high-rate entanglement generation between quantum memories. Extensions and derivatives of our proposal could enable a wide variety of applications by utilizing the operational trade-offs for qubits encoded in memory and in the GKP basis.","sentences":["Gottesman-Kitaev-Preskill (GKP) states have been demonstrated to pose significant advantages when utilized for fault-tolerant all optical continuous-variable quantum computing as well as for quantum communications links for entanglement distribution.","However interfacing these systems to long-lived solid-state quantum memories has remained an open problem.","Here we propose an interface between quantum memories and GKP qubit states based on a cavity-mediated controlled displacement gate.","We characterize the quality of memory-GKP entanglement as a function of cavity parameters suggesting optimal regimes of operation for high-quality state transfer between either qubit states.","We further extend this protocol to demonstrate the creation of GKP cluster states by avoiding the requirement of ancillary optical quadrature-squeezed light.","Utilizing post-selected entanglement swapping operations for GKP qubits, we demonstrate the utility of our protocol for high-rate entanglement generation between quantum memories.","Extensions and derivatives of our proposal could enable a wide variety of applications by utilizing the operational trade-offs for qubits encoded in memory and in the GKP basis."],"url":"http://arxiv.org/abs/2406.04275v1","category":"quant-ph"}
{"created":"2024-06-06 17:23:49","title":"Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models","abstract":"This work studies the challenge of aligning large language models (LLMs) with offline preference data. We focus on alignment by Reinforcement Learning from Human Feedback (RLHF) in particular. While popular preference optimization methods exhibit good empirical performance in practice, they are not theoretically guaranteed to converge to the optimal policy and can provably fail when the data coverage is sparse by classical offline reinforcement learning (RL) results. On the other hand, a recent line of work has focused on theoretically motivated preference optimization methods with provable guarantees, but these are not computationally efficient for large-scale applications like LLM alignment. To bridge this gap, we propose SPAC, a new offline preference optimization method with self-play, inspired by the on-average pessimism technique from the offline RL literature, to be the first provable and scalable approach to LLM alignment. We both provide theoretical analysis for its convergence under single-policy concentrability for the general function approximation setting and demonstrate its competitive empirical performance for LLM alignment on a 7B Mistral model with Open LLM Leaderboard evaluations.","sentences":["This work studies the challenge of aligning large language models (LLMs) with offline preference data.","We focus on alignment by Reinforcement Learning from Human Feedback (RLHF) in particular.","While popular preference optimization methods exhibit good empirical performance in practice, they are not theoretically guaranteed to converge to the optimal policy and can provably fail when the data coverage is sparse by classical offline reinforcement learning (RL) results.","On the other hand, a recent line of work has focused on theoretically motivated preference optimization methods with provable guarantees, but these are not computationally efficient for large-scale applications like LLM alignment.","To bridge this gap, we propose SPAC, a new offline preference optimization method with self-play, inspired by the on-average pessimism technique from the offline RL literature, to be the first provable and scalable approach to LLM alignment.","We both provide theoretical analysis for its convergence under single-policy concentrability for the general function approximation setting and demonstrate its competitive empirical performance for LLM alignment on a 7B Mistral model with Open LLM Leaderboard evaluations."],"url":"http://arxiv.org/abs/2406.04274v1","category":"cs.LG"}
{"created":"2024-06-06 17:23:05","title":"ELFS: Enhancing Label-Free Coreset Selection via Clustering-based Pseudo-Labeling","abstract":"High-quality human-annotated data is crucial for modern deep learning pipelines, yet the human annotation process is both costly and time-consuming. Given a constrained human labeling budget, selecting an informative and representative data subset for labeling can significantly reduce human annotation effort. Well-performing state-of-the-art (SOTA) coreset selection methods require ground-truth labels over the whole dataset, failing to reduce the human labeling burden. Meanwhile, SOTA label-free coreset selection methods deliver inferior performance due to poor geometry-based scores. In this paper, we introduce ELFS, a novel label-free coreset selection method. ELFS employs deep clustering to estimate data difficulty scores without ground-truth labels. Furthermore, ELFS uses a simple but effective double-end pruning method to mitigate bias on calculated scores, which further improves the performance on selected coresets. We evaluate ELFS on five vision benchmarks and show that ELFS consistently outperforms SOTA label-free baselines. For instance, at a 90% pruning rate, ELFS surpasses the best-performing baseline by 5.3% on CIFAR10 and 7.1% on CIFAR100. Moreover, ELFS even achieves comparable performance to supervised coreset selection at low pruning rates (e.g., 30% and 50%) on CIFAR10 and ImageNet-1K.","sentences":["High-quality human-annotated data is crucial for modern deep learning pipelines, yet the human annotation process is both costly and time-consuming.","Given a constrained human labeling budget, selecting an informative and representative data subset for labeling can significantly reduce human annotation effort.","Well-performing state-of-the-art (SOTA) coreset selection methods require ground-truth labels over the whole dataset, failing to reduce the human labeling burden.","Meanwhile, SOTA label-free coreset selection methods deliver inferior performance due to poor geometry-based scores.","In this paper, we introduce ELFS, a novel label-free coreset selection method.","ELFS employs deep clustering to estimate data difficulty scores without ground-truth labels.","Furthermore, ELFS uses a simple but effective double-end pruning method to mitigate bias on calculated scores, which further improves the performance on selected coresets.","We evaluate ELFS on five vision benchmarks and show that ELFS consistently outperforms SOTA label-free baselines.","For instance, at a 90% pruning rate, ELFS surpasses the best-performing baseline by 5.3% on CIFAR10 and 7.1% on CIFAR100.","Moreover, ELFS even achieves comparable performance to supervised coreset selection at low pruning rates (e.g., 30% and 50%) on CIFAR10 and ImageNet-1K."],"url":"http://arxiv.org/abs/2406.04273v1","category":"cs.CV"}
{"created":"2024-06-06 17:22:08","title":"Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models","abstract":"We introduce Buffer of Thoughts (BoT), a novel and versatile thought-augmented reasoning approach for enhancing accuracy, efficiency and robustness of large language models (LLMs). Specifically, we propose meta-buffer to store a series of informative high-level thoughts, namely thought-template, distilled from the problem-solving processes across various tasks. Then for each problem, we retrieve a relevant thought-template and adaptively instantiate it with specific reasoning structures to conduct efficient reasoning. To guarantee the scalability and stability, we further propose buffer-manager to dynamically update the meta-buffer, thus enhancing the capacity of meta-buffer as more tasks are solved. We conduct extensive experiments on 10 challenging reasoning-intensive tasks, and achieve significant performance improvements over previous SOTA methods: 11% on Game of 24, 20% on Geometric Shapes and 51% on Checkmate-in-One. Further analysis demonstrate the superior generalization ability and model robustness of our BoT, while requiring only 12% of the cost of multi-query prompting methods (e.g., tree/graph of thoughts) on average. Notably, we find that our Llama3-8B+BoT has the potential to surpass Llama3-70B model. Our project is available at: https://github.com/YangLing0818/buffer-of-thought-llm","sentences":["We introduce Buffer of Thoughts (BoT), a novel and versatile thought-augmented reasoning approach for enhancing accuracy, efficiency and robustness of large language models (LLMs).","Specifically, we propose meta-buffer to store a series of informative high-level thoughts, namely thought-template, distilled from the problem-solving processes across various tasks.","Then for each problem, we retrieve a relevant thought-template and adaptively instantiate it with specific reasoning structures to conduct efficient reasoning.","To guarantee the scalability and stability, we further propose buffer-manager to dynamically update the meta-buffer, thus enhancing the capacity of meta-buffer as more tasks are solved.","We conduct extensive experiments on 10 challenging reasoning-intensive tasks, and achieve significant performance improvements over previous SOTA methods: 11% on Game of 24, 20% on Geometric Shapes and 51% on Checkmate-in-One.","Further analysis demonstrate the superior generalization ability and model robustness of our BoT, while requiring only 12% of the cost of multi-query prompting methods (e.g., tree/graph of thoughts) on average.","Notably, we find that our Llama3-8B+BoT has the potential to surpass Llama3-70B model.","Our project is available at: https://github.com/YangLing0818/buffer-of-thought-llm"],"url":"http://arxiv.org/abs/2406.04271v1","category":"cs.CL"}
{"created":"2024-06-06 17:15:02","title":"Open-Endedness is Essential for Artificial Superhuman Intelligence","abstract":"In recent years there has been a tremendous surge in the general capabilities of AI systems, mainly fuelled by training foundation models on internetscale data. Nevertheless, the creation of openended, ever self-improving AI remains elusive. In this position paper, we argue that the ingredients are now in place to achieve openendedness in AI systems with respect to a human observer. Furthermore, we claim that such open-endedness is an essential property of any artificial superhuman intelligence (ASI). We begin by providing a concrete formal definition of open-endedness through the lens of novelty and learnability. We then illustrate a path towards ASI via open-ended systems built on top of foundation models, capable of making novel, humanrelevant discoveries. We conclude by examining the safety implications of generally-capable openended AI. We expect that open-ended foundation models will prove to be an increasingly fertile and safety-critical area of research in the near future.","sentences":["In recent years there has been a tremendous surge in the general capabilities of AI systems, mainly fuelled by training foundation models on internetscale data.","Nevertheless, the creation of openended, ever self-improving AI remains elusive.","In this position paper, we argue that the ingredients are now in place to achieve openendedness in AI systems with respect to a human observer.","Furthermore, we claim that such open-endedness is an essential property of any artificial superhuman intelligence (ASI).","We begin by providing a concrete formal definition of open-endedness through the lens of novelty and learnability.","We then illustrate a path towards ASI via open-ended systems built on top of foundation models, capable of making novel, humanrelevant discoveries.","We conclude by examining the safety implications of generally-capable openended AI.","We expect that open-ended foundation models will prove to be an increasingly fertile and safety-critical area of research in the near future."],"url":"http://arxiv.org/abs/2406.04268v1","category":"cs.LG"}
{"created":"2024-06-06 17:12:48","title":"Matrices over polynomial rings approached by commutative algebra","abstract":"The main goal of the paper is the discussion of a deeper interaction between matrix theory over polynomial rings over a field and typical methods of commutative algebra and related algebraic geometry. This is intended in the sense of bringing numerical algebraic invariants into the picture of determinantal ideals, with an emphasis on non-generic ones. In particular, there is a strong focus on square sparse matrices and features of the dual variety to a determinantal hypersurface. Though the overall goal is not exhausted here, one provides several environments where the present treatment has a degree of success.","sentences":["The main goal of the paper is the discussion of a deeper interaction between matrix theory over polynomial rings over a field and typical methods of commutative algebra and related algebraic geometry.","This is intended in the sense of bringing numerical algebraic invariants into the picture of determinantal ideals, with an emphasis on non-generic ones.","In particular, there is a strong focus on square sparse matrices and features of the dual variety to a determinantal hypersurface.","Though the overall goal is not exhausted here, one provides several environments where the present treatment has a degree of success."],"url":"http://arxiv.org/abs/2406.04266v1","category":"math.AC"}
{"created":"2024-06-06 17:09:32","title":"MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding","abstract":"The evaluation of Long Video Understanding (LVU) performance poses an important but challenging research problem. Despite previous efforts, the existing video understanding benchmarks are severely constrained by several issues, especially the insufficient lengths of videos, a lack of diversity in video types and evaluation tasks, and the inappropriateness for evaluating LVU performances. To address the above problems, we propose a new benchmark, called MLVU (Multi-task Long Video Understanding Benchmark), for the comprehensive and in-depth evaluation of LVU. MLVU presents the following critical values: 1) The substantial and flexible extension of video lengths, which enables the benchmark to evaluate LVU performance across a wide range of durations. 2) The inclusion of various video genres, e.g., movies, surveillance footage, egocentric videos, cartoons, game videos, etc., which reflects the models' LVU performances in different scenarios. 3) The development of diversified evaluation tasks, which enables a comprehensive examination of MLLMs' key abilities in long-video understanding. The empirical study with 20 latest MLLMs reveals significant room for improvement in today's technique, as all existing methods struggle with most of the evaluation tasks and exhibit severe performance degradation when handling longer videos. Additionally, it suggests that factors such as context length, image-understanding quality, and the choice of LLM backbone can play critical roles in future advancements. We anticipate that MLVU will advance the research of long video understanding by providing a comprehensive and in-depth analysis of MLLMs.","sentences":["The evaluation of Long Video Understanding (LVU) performance poses an important but challenging research problem.","Despite previous efforts, the existing video understanding benchmarks are severely constrained by several issues, especially the insufficient lengths of videos, a lack of diversity in video types and evaluation tasks, and the inappropriateness for evaluating LVU performances.","To address the above problems, we propose a new benchmark, called MLVU (Multi-task Long Video Understanding Benchmark), for the comprehensive and in-depth evaluation of LVU.","MLVU presents the following critical values: 1) The substantial and flexible extension of video lengths, which enables the benchmark to evaluate LVU performance across a wide range of durations.","2)","The inclusion of various video genres, e.g., movies, surveillance footage, egocentric videos, cartoons, game videos, etc., which reflects the models' LVU performances in different scenarios.","3)","The development of diversified evaluation tasks, which enables a comprehensive examination of MLLMs' key abilities in long-video understanding.","The empirical study with 20 latest MLLMs reveals significant room for improvement in today's technique, as all existing methods struggle with most of the evaluation tasks and exhibit severe performance degradation when handling longer videos.","Additionally, it suggests that factors such as context length, image-understanding quality, and the choice of LLM backbone can play critical roles in future advancements.","We anticipate that MLVU will advance the research of long video understanding by providing a comprehensive and in-depth analysis of MLLMs."],"url":"http://arxiv.org/abs/2406.04264v1","category":"cs.CV"}
{"created":"2024-06-06 17:09:08","title":"No real advantage of photon subtraction and displacement in continuous variable measurement device independent quantum key distribution","abstract":"We critically analyse the role of single photon subtraction (SPS) and displacement in improving the performance of continuous variable measurement device independent quantum key distribution (CV-MDI-QKD). We consider CV-MDI-QKD with resource states generated by SPS on a displaced two-mode squeezed vacuum state. Optimizing the secret key rate with state parameters reveals that implementing SPS yields no benefits in improving the loss tolerance of CV-MDI-QKD. Additionally, we find that displacement too is not useful in improving the performance of CV-MDI-QKD. While our result is in contradistinction with the widely held belief in the field regarding the utility of SPS and displacement in CV-MDI-QKD, it also calls for a re-examination of the role of non-Gaussian operations in increasing the efficiency of various quantum information processing protocols.","sentences":["We critically analyse the role of single photon subtraction (SPS) and displacement in improving the performance of continuous variable measurement device independent quantum key distribution (CV-MDI-QKD).","We consider CV-MDI-QKD with resource states generated by SPS on a displaced two-mode squeezed vacuum state.","Optimizing the secret key rate with state parameters reveals that implementing SPS yields no benefits in improving the loss tolerance of CV-MDI-QKD.","Additionally, we find that displacement too is not useful in improving the performance of CV-MDI-QKD.","While our result is in contradistinction with the widely held belief in the field regarding the utility of SPS and displacement in CV-MDI-QKD, it also calls for a re-examination of the role of non-Gaussian operations in increasing the efficiency of various quantum information processing protocols."],"url":"http://arxiv.org/abs/2406.04263v1","category":"quant-ph"}
{"created":"2024-06-06 17:08:28","title":"Near-field Beam Training with Sparse DFT Codebook","abstract":"Extremely large-scale array (XL-array) has emerged as one promising technology to improve the spectral efficiency and spatial resolution of future sixth generation (6G) wireless systems.The upsurge in the antenna number antennas renders communication users more likely to be located in the near-field region, which requires a more accurate spherical (instead of planar) wavefront propagation modeling.This inevitably incurs unaffordable beam training overhead when performing a two-dimensional (2D) beam-search in both the angular and range domains.To address this issue, we first introduce a new sparse discrete Fourier transform (DFT) codebook, which exhibits the angular periodicity in the received beam pattern at the user, which motivates us to propose a three-phase beam training scheme.Specifically, in the first phase, we utilize the sparse DFT codebook for beam sweeping in an angular subspace and estimate candidate user angles according to the received beam pattern.Then, a central sub-array is activated to scan specific candidate angles for resolving the issue of angular ambiguity and identity the user angle.In the third phase, the polar-domain codebook is applied in the estimated angle to search the best effective range of the user.Finally, numerical results show that the proposed beam training scheme enabled by sparse DFT codebook achieves 98.67% reduction as compared with the exhaustive-search scheme, yet without compromising rate performance in the high signal-to-ratio (SNR) regime.","sentences":["Extremely large-scale array (XL-array) has emerged as one promising technology to improve the spectral efficiency and spatial resolution of future sixth generation (6G) wireless systems.","The upsurge in the antenna number antennas renders communication users more likely to be located in the near-field region, which requires a more accurate spherical (instead of planar) wavefront propagation modeling.","This inevitably incurs unaffordable beam training overhead when performing a two-dimensional (2D) beam-search in both the angular and range domains.","To address this issue, we first introduce a new sparse discrete Fourier transform (DFT) codebook, which exhibits the angular periodicity in the received beam pattern at the user, which motivates us to propose a three-phase beam training scheme.","Specifically, in the first phase, we utilize the sparse DFT codebook for beam sweeping in an angular subspace and estimate candidate user angles according to the received beam pattern.","Then, a central sub-array is activated to scan specific candidate angles for resolving the issue of angular ambiguity and identity the user angle.","In the third phase, the polar-domain codebook is applied in the estimated angle to search the best effective range of the user.","Finally, numerical results show that the proposed beam training scheme enabled by sparse DFT codebook achieves 98.67% reduction as compared with the exhaustive-search scheme, yet without compromising rate performance in the high signal-to-ratio (SNR) regime."],"url":"http://arxiv.org/abs/2406.04262v1","category":"eess.SP"}
{"created":"2024-06-06 17:04:50","title":"Topological Stability and Latschev-type Reconstruction Theorems for $\\boldsymbol{\\mathrm{CAT}(\u03ba)}$ Spaces","abstract":"We consider the problem of homotopy-type reconstruction of compact shapes $X\\subset\\mathbb{R}^N$ that are $\\mathrm{CAT}(\\kappa)$ in the intrinsic length metric. The reconstructed spaces are in the form of Vietoris--Rips complexes computed from a compact sample $S$, Hausdorff--close to the unknown shape $X$. Instead of the Euclidean metric on the sample, our reconstruction technique leverages a path-based metric to compute these complexes. As naturally emerging in the framework of reconstruction, we also study the Gromov--Hausdorff topological stability and finiteness problem for general compact $\\mathrm{CAT}(\\kappa)$ spaces. Our techniques provide novel sampling conditions alternative to the existing and commonly used techniques using weak feature size and $\\mu$--reach. In particular, we introduce a new parameter, called the {\\em restricted distortion}, which is a generalization of the well-known global distortion of embedding. We show examples of Euclidean subspaces, for which the known parameters such as the reach, $\\mu$--reach and weak features size vanish, whereas the restricted distortion is finite, making our reconstruction results applicable for such spaces.","sentences":["We consider the problem of homotopy-type reconstruction of compact shapes $X\\subset\\mathbb{R}^N$ that are $\\mathrm{CAT}(\\kappa)$ in the intrinsic length metric.","The reconstructed spaces are in the form of Vietoris--Rips complexes computed from a compact sample $S$, Hausdorff--close to the unknown shape $X$. Instead of the Euclidean metric on the sample, our reconstruction technique leverages a path-based metric to compute these complexes.","As naturally emerging in the framework of reconstruction, we also study the Gromov--Hausdorff topological stability and finiteness problem for general compact $\\mathrm{CAT}(\\kappa)$ spaces.","Our techniques provide novel sampling conditions alternative to the existing and commonly used techniques using weak feature size and $\\mu$--reach.","In particular, we introduce a new parameter, called the {\\em restricted distortion}, which is a generalization of the well-known global distortion of embedding.","We show examples of Euclidean subspaces, for which the known parameters such as the reach, $\\mu$--reach and weak features size vanish, whereas the restricted distortion is finite, making our reconstruction results applicable for such spaces."],"url":"http://arxiv.org/abs/2406.04259v1","category":"math.AT"}
{"created":"2024-06-06 17:04:40","title":"Quiver Hecke algebras from Floer homology in Couloumb branches","abstract":"Homology theories categorifying quantum group link invariants are known to be governed by the representation theory of quiver Hecke algebras, also called KLRW algebras. Here we show that certain cylindrical KLRW algebras, relevant in particular for cylindrical generalizations of link homology theories, can be realized by Lagrangian Floer homology in multiplicative Coulomb branches. This confirms a homological mirror symmetry prediction of the first author.","sentences":["Homology theories categorifying quantum group link invariants are known to be governed by the representation theory of quiver Hecke algebras, also called KLRW algebras.","Here we show that certain cylindrical KLRW algebras, relevant in particular for cylindrical generalizations of link homology theories, can be realized by Lagrangian Floer homology in multiplicative Coulomb branches.","This confirms a homological mirror symmetry prediction of the first author."],"url":"http://arxiv.org/abs/2406.04258v1","category":"math.SG"}
{"created":"2024-06-06 17:03:46","title":"The frequency process in a non-neutral two-type continuous-state branching process with competition and its genealogy","abstract":"We consider a population growth model given by a two-type continuous-state branching process with immigration and competition, introduced by Ma. We study the relative frequency of one of the types in the population when the total mass is forced to be constant at a dense set of times. The resulting process is described as the solution to an SDE, which we call the culled frequency process, generalizing the $\\Lambda$-asymmetric frequency process introduced by Caballero et al. We obtain conditions for the culled frequency process to have a moment dual and show that it is given by a branching-coalescing continuous-time Markov chain that describes the genealogy of the two-type CBI with competition. Finally, we obtain a large population limit of the culled frequency process, resulting in a deterministic ordinary differential equation (ODE). Two particular cases of the limiting ODE are studied to determine if general two-type branching mechanisms and general Malthusians can lead to the coexistence of the two types in the population.","sentences":["We consider a population growth model given by a two-type continuous-state branching process with immigration and competition, introduced by Ma.","We study the relative frequency of one of the types in the population when the total mass is forced to be constant at a dense set of times.","The resulting process is described as the solution to an SDE, which we call the culled frequency process, generalizing the $\\Lambda$-asymmetric frequency process introduced by Caballero et al.","We obtain conditions for the culled frequency process to have a moment dual and show that it is given by a branching-coalescing continuous-time Markov chain that describes the genealogy of the two-type CBI with competition.","Finally, we obtain a large population limit of the culled frequency process, resulting in a deterministic ordinary differential equation (ODE).","Two particular cases of the limiting ODE are studied to determine if general two-type branching mechanisms and general Malthusians can lead to the coexistence of the two types in the population."],"url":"http://arxiv.org/abs/2406.04255v1","category":"math.PR"}
{"created":"2024-06-06 17:00:10","title":"GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions","abstract":"We introduce a new generative approach for synthesizing 3D geometry and images from single-view collections. Most existing approaches predict volumetric density to render multi-view consistent images. By employing volumetric rendering using neural radiance fields, they inherit a key limitation: the generated geometry is noisy and unconstrained, limiting the quality and utility of the output meshes. To address this issue, we propose GeoGen, a new SDF-based 3D generative model trained in an end-to-end manner. Initially, we reinterpret the volumetric density as a Signed Distance Function (SDF). This allows us to introduce useful priors to generate valid meshes. However, those priors prevent the generative model from learning details, limiting the applicability of the method to real-world scenarios. To alleviate that problem, we make the transformation learnable and constrain the rendered depth map to be consistent with the zero-level set of the SDF. Through the lens of adversarial training, we encourage the network to produce higher fidelity details on the output meshes. For evaluation, we introduce a synthetic dataset of human avatars captured from 360-degree camera angles, to overcome the challenges presented by real-world datasets, which often lack 3D consistency and do not cover all camera angles. Our experiments on multiple datasets show that GeoGen produces visually and quantitatively better geometry than the previous generative models based on neural radiance fields.","sentences":["We introduce a new generative approach for synthesizing 3D geometry and images from single-view collections.","Most existing approaches predict volumetric density to render multi-view consistent images.","By employing volumetric rendering using neural radiance fields, they inherit a key limitation: the generated geometry is noisy and unconstrained, limiting the quality and utility of the output meshes.","To address this issue, we propose GeoGen, a new SDF-based 3D generative model trained in an end-to-end manner.","Initially, we reinterpret the volumetric density as a Signed Distance Function (SDF).","This allows us to introduce useful priors to generate valid meshes.","However, those priors prevent the generative model from learning details, limiting the applicability of the method to real-world scenarios.","To alleviate that problem, we make the transformation learnable and constrain the rendered depth map to be consistent with the zero-level set of the SDF.","Through the lens of adversarial training, we encourage the network to produce higher fidelity details on the output meshes.","For evaluation, we introduce a synthetic dataset of human avatars captured from 360-degree camera angles, to overcome the challenges presented by real-world datasets, which often lack 3D consistency and do not cover all camera angles.","Our experiments on multiple datasets show that GeoGen produces visually and quantitatively better geometry than the previous generative models based on neural radiance fields."],"url":"http://arxiv.org/abs/2406.04254v1","category":"cs.CV"}
{"created":"2024-06-06 16:58:00","title":"A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation","abstract":"3D modeling has long been an important area in computer vision and computer graphics. Recently, thanks to the breakthroughs in neural representations and generative models, we witnessed a rapid development of 3D modeling. 3D human modeling, lying at the core of many real-world applications, such as gaming and animation, has attracted significant attention. Over the past few years, a large body of work on creating 3D human avatars has been introduced, forming a new and abundant knowledge base for 3D human modeling. The scale of the literature makes it difficult for individuals to keep track of all the works. This survey aims to provide a comprehensive overview of these emerging techniques for 3D human avatar modeling, from both reconstruction and generation perspectives. Firstly, we review representative methods for 3D human reconstruction, including methods based on pixel-aligned implicit function, neural radiance field, and 3D Gaussian Splatting, etc. We then summarize representative methods for 3D human generation, especially those using large language models like CLIP, diffusion models, and various 3D representations, which demonstrate state-of-the-art performance. Finally, we discuss our reflection on existing methods and open challenges for 3D human avatar modeling, shedding light on future research.","sentences":["3D modeling has long been an important area in computer vision and computer graphics.","Recently, thanks to the breakthroughs in neural representations and generative models, we witnessed a rapid development of 3D modeling.","3D human modeling, lying at the core of many real-world applications, such as gaming and animation, has attracted significant attention.","Over the past few years, a large body of work on creating 3D human avatars has been introduced, forming a new and abundant knowledge base for 3D human modeling.","The scale of the literature makes it difficult for individuals to keep track of all the works.","This survey aims to provide a comprehensive overview of these emerging techniques for 3D human avatar modeling, from both reconstruction and generation perspectives.","Firstly, we review representative methods for 3D human reconstruction, including methods based on pixel-aligned implicit function, neural radiance field, and 3D Gaussian Splatting, etc.","We then summarize representative methods for 3D human generation, especially those using large language models like CLIP, diffusion models, and various 3D representations, which demonstrate state-of-the-art performance.","Finally, we discuss our reflection on existing methods and open challenges for 3D human avatar modeling, shedding light on future research."],"url":"http://arxiv.org/abs/2406.04253v1","category":"cs.CV"}
{"created":"2024-06-06 16:54:20","title":"Online learning of quantum processes","abstract":"Among recent insights into learning quantum states, online learning and shadow tomography procedures are notable for their ability to accurately predict expectation values even of adaptively chosen observables. In contrast to the state case, quantum process learning tasks with a similarly adaptive nature have received little attention. In this work, we investigate online learning tasks for quantum processes. Whereas online learning is infeasible for general quantum channels, we show that channels of bounded gate complexity as well as Pauli channels can be online learned in the regret and mistake-bounded models of online learning. In fact, we can online learn probabilistic mixtures of any exponentially large set of known channels. We also provide a provably sample-efficient shadow tomography procedure for Pauli channels. Our results extend beyond quantum channels to non-Markovian multi-time processes, with favorable regret and mistake bounds, as well as a shadow tomography procedure. We complement our online learning upper bounds with mistake as well as computational lower bounds. On the technical side, we make use of the multiplicative weights update algorithm, classical adaptive data analysis, and Bell sampling, as well as tools from the theory of quantum combs for multi-time quantum processes. Our work initiates a study of online learning for classes of quantum channels and, more generally, non-Markovian quantum processes. Given the importance of online learning for state shadow tomography, this may serve as a step towards quantum channel variants of adaptive shadow tomography.","sentences":["Among recent insights into learning quantum states, online learning and shadow tomography procedures are notable for their ability to accurately predict expectation values even of adaptively chosen observables.","In contrast to the state case, quantum process learning tasks with a similarly adaptive nature have received little attention.","In this work, we investigate online learning tasks for quantum processes.","Whereas online learning is infeasible for general quantum channels, we show that channels of bounded gate complexity as well as Pauli channels can be online learned in the regret and mistake-bounded models of online learning.","In fact, we can online learn probabilistic mixtures of any exponentially large set of known channels.","We also provide a provably sample-efficient shadow tomography procedure for Pauli channels.","Our results extend beyond quantum channels to non-Markovian multi-time processes, with favorable regret and mistake bounds, as well as a shadow tomography procedure.","We complement our online learning upper bounds with mistake as well as computational lower bounds.","On the technical side, we make use of the multiplicative weights update algorithm, classical adaptive data analysis, and Bell sampling, as well as tools from the theory of quantum combs for multi-time quantum processes.","Our work initiates a study of online learning for classes of quantum channels and, more generally, non-Markovian quantum processes.","Given the importance of online learning for state shadow tomography, this may serve as a step towards quantum channel variants of adaptive shadow tomography."],"url":"http://arxiv.org/abs/2406.04250v1","category":"quant-ph"}
{"created":"2024-06-06 16:52:42","title":"Conv-INR: Convolutional Implicit Neural Representation for Multimodal Visual Signals","abstract":"Implicit neural representation (INR) has recently emerged as a promising paradigm for signal representations. Typically, INR is parameterized by a multiplayer perceptron (MLP) which takes the coordinates as the inputs and generates corresponding attributes of a signal. However, MLP-based INRs face two critical issues: i) individually considering each coordinate while ignoring the connections; ii) suffering from the spectral bias thus failing to learn high-frequency components. While target visual signals usually exhibit strong local structures and neighborhood dependencies, and high-frequency components are significant in these signals, the issues harm the representational capacity of INRs. This paper proposes Conv-INR, the first INR model fully based on convolution. Due to the inherent attributes of convolution, Conv-INR can simultaneously consider adjacent coordinates and learn high-frequency components effectively. Compared to existing MLP-based INRs, Conv-INR has better representational capacity and trainability without requiring primary function expansion. We conduct extensive experiments on four tasks, including image fitting, CT/MRI reconstruction, and novel view synthesis, Conv-INR all significantly surpasses existing MLP-based INRs, validating the effectiveness. Finally, we raise three reparameterization methods that can further enhance the performance of the vanilla Conv-INR without introducing any extra inference cost.","sentences":["Implicit neural representation (INR) has recently emerged as a promising paradigm for signal representations.","Typically, INR is parameterized by a multiplayer perceptron (MLP) which takes the coordinates as the inputs and generates corresponding attributes of a signal.","However, MLP-based INRs face two critical issues: i) individually considering each coordinate while ignoring the connections; ii) suffering from the spectral bias thus failing to learn high-frequency components.","While target visual signals usually exhibit strong local structures and neighborhood dependencies, and high-frequency components are significant in these signals, the issues harm the representational capacity of INRs.","This paper proposes Conv-INR, the first INR model fully based on convolution.","Due to the inherent attributes of convolution, Conv-INR can simultaneously consider adjacent coordinates and learn high-frequency components effectively.","Compared to existing MLP-based INRs, Conv-INR has better representational capacity and trainability without requiring primary function expansion.","We conduct extensive experiments on four tasks, including image fitting, CT/MRI reconstruction, and novel view synthesis, Conv-INR all significantly surpasses existing MLP-based INRs, validating the effectiveness.","Finally, we raise three reparameterization methods that can further enhance the performance of the vanilla Conv-INR without introducing any extra inference cost."],"url":"http://arxiv.org/abs/2406.04249v1","category":"cs.CV"}
{"created":"2024-06-06 16:47:46","title":"Spectral flow and the conformal block expansion for strings in AdS$_3$","abstract":"We present a detailed study of spectrally flowed four-point functions in the SL(2,$\\mathbb{R}$) WZW model, focusing on their conformal block decomposition. Dei and Eberhardt conjectured a general formula relating these observables to their unflowed counterparts. Although the latter are not known in closed form, their conformal block expansion has been formally established. By combining this information with the integral transform that encodes the effect of spectral flow, we show how to describe a considerable number of $s$-channel exchanges, including cases with both flowed and unflowed intermediate states. For all such processes, we compute the normalization of the corresponding conformal blocks in terms of products of the recently derived flowed three-point functions with arbitrary spectral flow charges. Our results constitute a highly non-trivial consistency check, thus strongly supporting the aforementioned conjecture, and establishing its computational power.","sentences":["We present a detailed study of spectrally flowed four-point functions in the SL(2,$\\mathbb{R}$) WZW model, focusing on their conformal block decomposition.","Dei and Eberhardt conjectured a general formula relating these observables to their unflowed counterparts.","Although the latter are not known in closed form, their conformal block expansion has been formally established.","By combining this information with the integral transform that encodes the effect of spectral flow, we show how to describe a considerable number of $s$-channel exchanges, including cases with both flowed and unflowed intermediate states.","For all such processes, we compute the normalization of the corresponding conformal blocks in terms of products of the recently derived flowed three-point functions with arbitrary spectral flow charges.","Our results constitute a highly non-trivial consistency check, thus strongly supporting the aforementioned conjecture, and establishing its computational power."],"url":"http://arxiv.org/abs/2406.04247v1","category":"hep-th"}
{"created":"2024-06-06 16:44:08","title":"Online learning of a panoply of quantum objects","abstract":"In many quantum tasks, there is an unknown quantum object that one wishes to learn. An online strategy for this task involves adaptively refining a hypothesis to reproduce such an object or its measurement statistics. A common evaluation metric for such a strategy is its regret, or roughly the accumulated errors in hypothesis statistics. We prove a sublinear regret bound for learning over general subsets of positive semidefinite matrices via the regularized-follow-the-leader algorithm and apply it to various settings where one wishes to learn quantum objects. For concrete applications, we present a sublinear regret bound for learning quantum states, effects, channels, interactive measurements, strategies, co-strategies, and the collection of inner products of pure states. Our bound applies to many other quantum objects with compact, convex representations. In proving our regret bound, we establish various matrix analysis results useful in quantum information theory. This includes a generalization of Pinsker's inequality for arbitrary positive semidefinite operators with possibly different traces, which may be of independent interest and applicable to more general classes of divergences.","sentences":["In many quantum tasks, there is an unknown quantum object that one wishes to learn.","An online strategy for this task involves adaptively refining a hypothesis to reproduce such an object or its measurement statistics.","A common evaluation metric for such a strategy is its regret, or roughly the accumulated errors in hypothesis statistics.","We prove a sublinear regret bound for learning over general subsets of positive semidefinite matrices via the regularized-follow-the-leader algorithm and apply it to various settings where one wishes to learn quantum objects.","For concrete applications, we present a sublinear regret bound for learning quantum states, effects, channels, interactive measurements, strategies, co-strategies, and the collection of inner products of pure states.","Our bound applies to many other quantum objects with compact, convex representations.","In proving our regret bound, we establish various matrix analysis results useful in quantum information theory.","This includes a generalization of Pinsker's inequality for arbitrary positive semidefinite operators with possibly different traces, which may be of independent interest and applicable to more general classes of divergences."],"url":"http://arxiv.org/abs/2406.04245v1","category":"quant-ph"}
{"created":"2024-06-06 16:39:23","title":"Regular one-parameter groups, reflection positivity and their application to Hankel operators and standard subspaces","abstract":"Standard subspaces are a well studied object in algebraic quantum field theory (AQFT). Given a standard subspace ${\\tt V}$ of a Hilbert space $\\mathcal{H}$, one is interested in unitary one-parameter groups on $\\mathcal{H}$ with $U_t {\\tt V} \\subseteq {\\tt V}$ for every $t \\in \\mathbb{R}_+$. If $({\\tt V},U)$ is a non-degenerate standard pair on $\\mathcal{H}$, i.e. the self-adjoint infinitesimal generator of $U$ is a positive operator with trivial kernel, two classical results are given by Borchers' Theorem, relating non-degenerate standard pairs to positive energy representations of the affine group $\\mathrm{Aff}(\\mathbb{R})$ and the Longo--Witten Theorem, stating the the semigroup of unitary endomorphisms of ${\\tt V}$ can be identified with the semigroup of symmetric operator-valued inner functions on the upper half plane.   In this thesis we prove results similar to the theorems of Borchers and of Longo--Witten for a more general framework of unitary one-parameter groups without the assumption that their infinitesimal generator is positive. We replace this assumption by the weaker assumption that the triple $(\\mathcal{H},{\\tt V},U)$ is a so called real regular one-parameter group.","sentences":["Standard subspaces are a well studied object in algebraic quantum field theory (AQFT).","Given a standard subspace ${\\tt V}$ of a Hilbert space $\\mathcal{H}$, one is interested in unitary one-parameter groups on $\\mathcal{H}$ with $U_t {\\tt V} \\subseteq {\\tt V}$ for every $t \\in \\mathbb{R}_+$. If $({\\tt V},U)$ is a non-degenerate standard pair on $\\mathcal{H}$, i.e. the self-adjoint infinitesimal generator of $U$ is a positive operator with trivial kernel, two classical results are given by Borchers' Theorem, relating non-degenerate standard pairs to positive energy representations of the affine group $\\mathrm{Aff}(\\mathbb{R})$ and the Longo--Witten Theorem, stating the the semigroup of unitary endomorphisms of ${\\tt V}$ can be identified with the semigroup of symmetric operator-valued inner functions on the upper half plane.   ","In this thesis we prove results similar to the theorems of Borchers and of Longo--Witten for a more general framework of unitary one-parameter groups without the assumption that their infinitesimal generator is positive.","We replace this assumption by the weaker assumption that the triple $(\\mathcal{H},{\\tt V},U)$ is a so called real regular one-parameter group."],"url":"http://arxiv.org/abs/2406.04241v1","category":"math.FA"}
{"created":"2024-06-06 16:39:00","title":"Hypernetworks for Personalizing ASR to Atypical Speech","abstract":"Parameter-efficient fine-tuning (PEFT) for personalizing automatic speech recognition (ASR) has recently shown promise for adapting general population models to atypical speech. However, these approaches assume a priori knowledge of the atypical speech disorder being adapted for -- the diagnosis of which requires expert knowledge that is not always available. Even given this knowledge, data scarcity and high inter/intra-speaker variability further limit the effectiveness of traditional fine-tuning. To circumvent these challenges, we first identify the minimal set of model parameters required for ASR adaptation. Our analysis of each individual parameter's effect on adaptation performance allows us to reduce Word Error Rate (WER) by half while adapting 0.03\\% of all weights. Alleviating the need for cohort-specific models, we next propose the novel use of a meta-learned hypernetwork to generate highly individualized, utterance-level adaptations on-the-fly for a diverse set of atypical speech characteristics. Evaluating adaptation at the global, cohort and individual-level, we show that hypernetworks generalize better to out-of-distribution speakers, while maintaining an overall relative WER reduction of 75.2% using 0.1% of the full parameter budget.","sentences":["Parameter-efficient fine-tuning (PEFT) for personalizing automatic speech recognition (ASR) has recently shown promise for adapting general population models to atypical speech.","However, these approaches assume a priori knowledge of the atypical speech disorder being adapted for -- the diagnosis of which requires expert knowledge that is not always available.","Even given this knowledge, data scarcity and high inter/intra-speaker variability further limit the effectiveness of traditional fine-tuning.","To circumvent these challenges, we first identify the minimal set of model parameters required for ASR adaptation.","Our analysis of each individual parameter's effect on adaptation performance allows us to reduce Word Error Rate (WER) by half while adapting 0.03\\% of all weights.","Alleviating the need for cohort-specific models, we next propose the novel use of a meta-learned hypernetwork to generate highly individualized, utterance-level adaptations on-the-fly for a diverse set of atypical speech characteristics.","Evaluating adaptation at the global, cohort and individual-level, we show that hypernetworks generalize better to out-of-distribution speakers, while maintaining an overall relative WER reduction of 75.2% using 0.1% of the full parameter budget."],"url":"http://arxiv.org/abs/2406.04240v1","category":"cs.LG"}
{"created":"2024-06-06 16:38:53","title":"Solving Inverse Problems in Protein Space Using Diffusion-Based Priors","abstract":"The interaction of a protein with its environment can be understood and controlled via its 3D structure. Experimental methods for protein structure determination, such as X-ray crystallography or cryogenic electron microscopy, shed light on biological processes but introduce challenging inverse problems. Learning-based approaches have emerged as accurate and efficient methods to solve these inverse problems for 3D structure determination, but are specialized for a predefined type of measurement. Here, we introduce a versatile framework to turn raw biophysical measurements of varying types into 3D atomic models. Our method combines a physics-based forward model of the measurement process with a pretrained generative model providing a task-agnostic, data-driven prior. Our method outperforms posterior sampling baselines on both linear and non-linear inverse problems. In particular, it is the first diffusion-based method for refining atomic models from cryo-EM density maps.","sentences":["The interaction of a protein with its environment can be understood and controlled via its 3D structure.","Experimental methods for protein structure determination, such as X-ray crystallography or cryogenic electron microscopy, shed light on biological processes but introduce challenging inverse problems.","Learning-based approaches have emerged as accurate and efficient methods to solve these inverse problems for 3D structure determination, but are specialized for a predefined type of measurement.","Here, we introduce a versatile framework to turn raw biophysical measurements of varying types into 3D atomic models.","Our method combines a physics-based forward model of the measurement process with a pretrained generative model providing a task-agnostic, data-driven prior.","Our method outperforms posterior sampling baselines on both linear and non-linear inverse problems.","In particular, it is the first diffusion-based method for refining atomic models from cryo-EM density maps."],"url":"http://arxiv.org/abs/2406.04239v1","category":"cs.LG"}
{"created":"2024-06-06 16:36:22","title":"The evolution of the transverse-momentum dependent gluon distribution at small $x$","abstract":"Using the colour dipole picture for photon-nucleus interactions at small $x$ together with the Color Glass Condensate (CGC) effective theory, we demonstrate that the next-to-leading (NLO) order corrections to the cross-section for the inclusive production of a pair of hard jets encode not only the JIMWLK evolution with decreasing $x$, but also the DGLAP evolution of the gluon distribution function and the CSS evolution of the gluon transverse momentum dependent (TMD) distribution. The emergent CSS equation takes the form of a rate equation describing the evolution of the dijet distribution in the transverse momentum imbalance $K_\\perp$ when increasing the dijet relative momentum $P_\\perp$. All three types of evolution become important when both $P_\\perp$ and $K_\\perp$ are much larger than the nuclear saturation momentum $Q_s(x)$ and we propose a framework which encompasses all of them. The solution to the JIMWLK equation provides the source term for the DGLAP evolution with increasing $K_\\perp$, which in turn generates the initial condition for the CSS evolution with increasing $P_\\perp$.","sentences":["Using the colour dipole picture for photon-nucleus interactions at small $x$ together with the Color Glass Condensate (CGC) effective theory, we demonstrate that the next-to-leading (NLO) order corrections to the cross-section for the inclusive production of a pair of hard jets encode not only the JIMWLK evolution with decreasing $x$, but also the DGLAP evolution of the gluon distribution function and the CSS evolution of the gluon transverse momentum dependent (TMD) distribution.","The emergent CSS equation takes the form of a rate equation describing the evolution of the dijet distribution in the transverse momentum imbalance $K_\\perp$ when increasing the dijet relative momentum $P_\\perp$. All three types of evolution become important when both $P_\\perp$ and $K_\\perp$ are much larger than the nuclear saturation momentum $Q_s(x)$","and we propose a framework which encompasses all of them.","The solution to the JIMWLK equation provides the source term for the DGLAP evolution with increasing $K_\\perp$, which in turn generates the initial condition for the CSS evolution with increasing $P_\\perp$."],"url":"http://arxiv.org/abs/2406.04238v1","category":"hep-ph"}
{"created":"2024-06-06 16:35:36","title":"Understanding Information Storage and Transfer in Multi-modal Large Language Models","abstract":"Understanding the mechanisms of information storage and transfer in Transformer-based models is important for driving model understanding progress. Recent work has studied these mechanisms for Large Language Models (LLMs), revealing insights on how information is stored in a model's parameters and how information flows to and from these parameters in response to specific prompts. However, these studies have not yet been extended to Multi-modal Large Language Models (MLLMs). Given their expanding capabilities and real-world use, we start by studying one aspect of these models -- how MLLMs process information in a factual visual question answering task. We use a constraint-based formulation which views a visual question as having a set of visual or textual constraints that the model's generated answer must satisfy to be correct (e.g. What movie directed by the director in this photo has won a Golden Globe?). Under this setting, we contribute i) a method that extends causal information tracing from pure language to the multi-modal setting, and ii) VQA-Constraints, a test-bed of 9.7K visual questions annotated with constraints. We use these tools to study two open-source MLLMs, LLaVa and multi-modal Phi-2. Our key findings show that these MLLMs rely on MLP and self-attention blocks in much earlier layers for information storage, compared to LLMs whose mid-layer MLPs are more important. We also show that a consistent small subset of visual tokens output by the vision encoder are responsible for transferring information from the image to these causal blocks. We validate these mechanisms by introducing MultEdit, a model-editing algorithm that can correct errors and insert new long-tailed information into MLLMs by targeting these causal blocks.","sentences":["Understanding the mechanisms of information storage and transfer in Transformer-based models is important for driving model understanding progress.","Recent work has studied these mechanisms for Large Language Models (LLMs), revealing insights on how information is stored in a model's parameters and how information flows to and from these parameters in response to specific prompts.","However, these studies have not yet been extended to Multi-modal Large Language Models (MLLMs).","Given their expanding capabilities and real-world use, we start by studying one aspect of these models -- how MLLMs process information in a factual visual question answering task.","We use a constraint-based formulation which views a visual question as having a set of visual or textual constraints that the model's generated answer must satisfy to be correct (e.g. What movie directed by the director in this photo has won a Golden Globe?).","Under this setting, we contribute i) a method that extends causal information tracing from pure language to the multi-modal setting, and ii) VQA-Constraints, a test-bed of 9.7K visual questions annotated with constraints.","We use these tools to study two open-source MLLMs, LLaVa and multi-modal Phi-2.","Our key findings show that these MLLMs rely on MLP and self-attention blocks in much earlier layers for information storage, compared to LLMs whose mid-layer MLPs are more important.","We also show that a consistent small subset of visual tokens output by the vision encoder are responsible for transferring information from the image to these causal blocks.","We validate these mechanisms by introducing MultEdit, a model-editing algorithm that can correct errors and insert new long-tailed information into MLLMs by targeting these causal blocks."],"url":"http://arxiv.org/abs/2406.04236v1","category":"cs.CV"}
{"created":"2024-06-06 16:35:27","title":"Toward Artificial Open-Ended Evolution within Lenia using Quality-Diversity","abstract":"From the formation of snowflakes to the evolution of diverse life forms, emergence is ubiquitous in our universe. In the quest to understand how complexity can arise from simple rules, abstract computational models, such as cellular automata, have been developed to study self-organization. However, the discovery of self-organizing patterns in artificial systems is challenging and has largely relied on manual or semi-automatic search in the past. In this paper, we show that Quality-Diversity, a family of Evolutionary Algorithms, is an effective framework for the automatic discovery of diverse self-organizing patterns in complex systems. Quality-Diversity algorithms aim to evolve a large population of diverse individuals, each adapted to its ecological niche. Combined with Lenia, a family of continuous cellular automata, we demonstrate that our method is able to evolve a diverse population of lifelike self-organizing autonomous patterns. Our framework, called Leniabreeder, can leverage both manually defined diversity criteria to guide the search toward interesting areas, as well as unsupervised measures of diversity to broaden the scope of discoverable patterns. We demonstrate both qualitatively and quantitatively that Leniabreeder offers a powerful solution for discovering self-organizing patterns. The effectiveness of unsupervised Quality-Diversity methods combined with the rich landscape of Lenia exhibits a sustained generation of diversity and complexity characteristic of biological evolution. We provide empirical evidence that suggests unbounded diversity and argue that Leniabreeder is a step toward replicating open-ended evolution in silico.","sentences":["From the formation of snowflakes to the evolution of diverse life forms, emergence is ubiquitous in our universe.","In the quest to understand how complexity can arise from simple rules, abstract computational models, such as cellular automata, have been developed to study self-organization.","However, the discovery of self-organizing patterns in artificial systems is challenging and has largely relied on manual or semi-automatic search in the past.","In this paper, we show that Quality-Diversity, a family of Evolutionary Algorithms, is an effective framework for the automatic discovery of diverse self-organizing patterns in complex systems.","Quality-Diversity algorithms aim to evolve a large population of diverse individuals, each adapted to its ecological niche.","Combined with Lenia, a family of continuous cellular automata, we demonstrate that our method is able to evolve a diverse population of lifelike self-organizing autonomous patterns.","Our framework, called Leniabreeder, can leverage both manually defined diversity criteria to guide the search toward interesting areas, as well as unsupervised measures of diversity to broaden the scope of discoverable patterns.","We demonstrate both qualitatively and quantitatively that Leniabreeder offers a powerful solution for discovering self-organizing patterns.","The effectiveness of unsupervised Quality-Diversity methods combined with the rich landscape of Lenia exhibits a sustained generation of diversity and complexity characteristic of biological evolution.","We provide empirical evidence that suggests unbounded diversity and argue that Leniabreeder is a step toward replicating open-ended evolution in silico."],"url":"http://arxiv.org/abs/2406.04235v1","category":"cs.NE"}
{"created":"2024-06-06 16:31:47","title":"FairytaleQA Translated: Enabling Educational Question and Answer Generation in Less-Resourced Languages","abstract":"Question Answering (QA) datasets are crucial in assessing reading comprehension skills for both machines and humans. While numerous datasets have been developed in English for this purpose, a noticeable void exists in less-resourced languages. To alleviate this gap, our paper introduces machine-translated versions of FairytaleQA, a renowned QA dataset designed to assess and enhance narrative comprehension skills in young children. By employing fine-tuned, modest-scale models, we establish benchmarks for both Question Generation (QG) and QA tasks within the translated datasets. In addition, we present a case study proposing a model for generating question-answer pairs, with an evaluation incorporating quality metrics such as question well-formedness, answerability, relevance, and children suitability. Our evaluation prioritizes quantifying and describing error cases, along with providing directions for future work. This paper contributes to the advancement of QA and QG research in less-resourced languages, promoting accessibility and inclusivity in the development of these models for reading comprehension. The code and data is publicly available at github.com/bernardoleite/fairytaleqa-translated.","sentences":["Question Answering (QA) datasets are crucial in assessing reading comprehension skills for both machines and humans.","While numerous datasets have been developed in English for this purpose, a noticeable void exists in less-resourced languages.","To alleviate this gap, our paper introduces machine-translated versions of FairytaleQA, a renowned QA dataset designed to assess and enhance narrative comprehension skills in young children.","By employing fine-tuned, modest-scale models, we establish benchmarks for both Question Generation (QG) and QA tasks within the translated datasets.","In addition, we present a case study proposing a model for generating question-answer pairs, with an evaluation incorporating quality metrics such as question well-formedness, answerability, relevance, and children suitability.","Our evaluation prioritizes quantifying and describing error cases, along with providing directions for future work.","This paper contributes to the advancement of QA and QG research in less-resourced languages, promoting accessibility and inclusivity in the development of these models for reading comprehension.","The code and data is publicly available at github.com/bernardoleite/fairytaleqa-translated."],"url":"http://arxiv.org/abs/2406.04233v1","category":"cs.CL"}
{"created":"2024-06-06 16:31:22","title":"Quantifying Misalignment Between Agents","abstract":"Growing concerns about the AI alignment problem have emerged in recent years, with previous work focusing mainly on (1) qualitative descriptions of the alignment problem; (2) attempting to align AI actions with human interests by focusing on value specification and learning; and/or (3) focusing on a single agent or on humanity as a singular unit. Recent work in sociotechnical AI alignment has made some progress in defining alignment inclusively, but the field as a whole still lacks a systematic understanding of how to specify, describe, and analyze misalignment among entities, which may include individual humans, AI agents, and complex compositional entities such as corporations, nation-states, and so forth. Previous work on controversy in computational social science offers a mathematical model of contention among populations (of humans). In this paper, we adapt this contention model to the alignment problem, and show how misalignment can vary depending on the population of agents (human or otherwise) being observed, the domain in question, and the agents' probability-weighted preferences between possible outcomes. Our model departs from value specification approaches and focuses instead on the morass of complex, interlocking, sometimes contradictory goals that agents may have in practice. We apply our model by analyzing several case studies ranging from social media moderation to autonomous vehicle behavior. By applying our model with appropriately representative value data, AI engineers can ensure that their systems learn values maximally aligned with diverse human interests.","sentences":["Growing concerns about the AI alignment problem have emerged in recent years, with previous work focusing mainly on (1) qualitative descriptions of the alignment problem; (2) attempting to align AI actions with human interests by focusing on value specification and learning; and/or (3) focusing on a single agent or on humanity as a singular unit.","Recent work in sociotechnical AI alignment has made some progress in defining alignment inclusively, but the field as a whole still lacks a systematic understanding of how to specify, describe, and analyze misalignment among entities, which may include individual humans, AI agents, and complex compositional entities such as corporations, nation-states, and so forth.","Previous work on controversy in computational social science offers a mathematical model of contention among populations (of humans).","In this paper, we adapt this contention model to the alignment problem, and show how misalignment can vary depending on the population of agents (human or otherwise) being observed, the domain in question, and the agents' probability-weighted preferences between possible outcomes.","Our model departs from value specification approaches and focuses instead on the morass of complex, interlocking, sometimes contradictory goals that agents may have in practice.","We apply our model by analyzing several case studies ranging from social media moderation to autonomous vehicle behavior.","By applying our model with appropriately representative value data, AI engineers can ensure that their systems learn values maximally aligned with diverse human interests."],"url":"http://arxiv.org/abs/2406.04231v1","category":"cs.MA"}
{"created":"2024-06-06 16:30:41","title":"M3LEO: A Multi-Modal, Multi-Label Earth Observation Dataset Integrating Interferometric SAR and RGB Data","abstract":"Satellite-based remote sensing has revolutionised the way we address global challenges in a rapidly evolving world. Huge quantities of Earth Observation (EO) data are generated by satellite sensors daily, but processing these large datasets for use in ML pipelines is technically and computationally challenging. Specifically, different types of EO data are often hosted on a variety of platforms, with differing availability for Python preprocessing tools. In addition, spatial alignment across data sources and data tiling can present significant technical hurdles for novice users. While some preprocessed EO datasets exist, their content is often limited to optical or near-optical wavelength data, which is ineffective at night or in adverse weather conditions. Synthetic Aperture Radar (SAR), an active sensing technique based on microwave length radiation, offers a viable alternative. However, the application of machine learning to SAR has been limited due to a lack of ML-ready data and pipelines, particularly for the full diversity of SAR data, including polarimetry, coherence and interferometry. We introduce M3LEO, a multi-modal, multi-label EO dataset that includes polarimetric, interferometric, and coherence SAR data derived from Sentinel-1, alongside Sentinel-2 RGB imagery and a suite of labelled tasks for model evaluation. M3LEO spans 17.5TB and contains approximately 10M data chips across six geographic regions. The dataset is complemented by a flexible PyTorch Lightning framework, with configuration management using Hydra. We provide tools to process any dataset available on popular platforms such as Google Earth Engine for integration with our framework. Initial experiments validate the utility of our data and framework, showing that SAR imagery contains information additional to that extractable from RGB data. Data at huggingface.co/M3LEO, and code at github.com/spaceml-org/M3LEO.","sentences":["Satellite-based remote sensing has revolutionised the way we address global challenges in a rapidly evolving world.","Huge quantities of Earth Observation (EO) data are generated by satellite sensors daily, but processing these large datasets for use in ML pipelines is technically and computationally challenging.","Specifically, different types of EO data are often hosted on a variety of platforms, with differing availability for Python preprocessing tools.","In addition, spatial alignment across data sources and data tiling can present significant technical hurdles for novice users.","While some preprocessed EO datasets exist, their content is often limited to optical or near-optical wavelength data, which is ineffective at night or in adverse weather conditions.","Synthetic Aperture Radar (SAR), an active sensing technique based on microwave length radiation, offers a viable alternative.","However, the application of machine learning to SAR has been limited due to a lack of ML-ready data and pipelines, particularly for the full diversity of SAR data, including polarimetry, coherence and interferometry.","We introduce M3LEO, a multi-modal, multi-label EO dataset that includes polarimetric, interferometric, and coherence SAR data derived from Sentinel-1, alongside Sentinel-2 RGB imagery and a suite of labelled tasks for model evaluation.","M3LEO spans 17.5TB and contains approximately 10M data chips across six geographic regions.","The dataset is complemented by a flexible PyTorch Lightning framework, with configuration management using Hydra.","We provide tools to process any dataset available on popular platforms such as Google Earth Engine for integration with our framework.","Initial experiments validate the utility of our data and framework, showing that SAR imagery contains information additional to that extractable from RGB data.","Data at huggingface.co/M3LEO, and code at github.com/spaceml-org/M3LEO."],"url":"http://arxiv.org/abs/2406.04230v1","category":"cs.CV"}
{"created":"2024-06-06 16:29:25","title":"The CLRS-Text Algorithmic Reasoning Language Benchmark","abstract":"Eliciting reasoning capabilities from language models (LMs) is a critical direction on the path towards building intelligent systems. Most recent studies dedicated to reasoning focus on out-of-distribution performance on procedurally-generated synthetic benchmarks, bespoke-built to evaluate specific skills only. This trend makes results hard to transfer across publications, slowing down progress. Three years ago, a similar issue was identified and rectified in the field of neural algorithmic reasoning, with the advent of the CLRS benchmark. CLRS is a dataset generator comprising graph execution traces of classical algorithms from the Introduction to Algorithms textbook. Inspired by this, we propose CLRS-Text -- a textual version of these algorithmic traces. Out of the box, CLRS-Text is capable of procedurally generating trace data for thirty diverse, challenging algorithmic tasks across any desirable input distribution, while offering a standard pipeline in which any additional algorithmic tasks may be created in the benchmark. We fine-tune and evaluate various LMs as generalist executors on this benchmark, validating prior work and revealing a novel, interesting challenge for the LM reasoning community. Our code is available at https://github.com/google-deepmind/clrs/tree/master/clrs/_src/clrs_text.","sentences":["Eliciting reasoning capabilities from language models (LMs) is a critical direction on the path towards building intelligent systems.","Most recent studies dedicated to reasoning focus on out-of-distribution performance on procedurally-generated synthetic benchmarks, bespoke-built to evaluate specific skills only.","This trend makes results hard to transfer across publications, slowing down progress.","Three years ago, a similar issue was identified and rectified in the field of neural algorithmic reasoning, with the advent of the CLRS benchmark.","CLRS is a dataset generator comprising graph execution traces of classical algorithms from the Introduction to Algorithms textbook.","Inspired by this, we propose CLRS-Text -- a textual version of these algorithmic traces.","Out of the box, CLRS-Text is capable of procedurally generating trace data for thirty diverse, challenging algorithmic tasks across any desirable input distribution, while offering a standard pipeline in which any additional algorithmic tasks may be created in the benchmark.","We fine-tune and evaluate various LMs as generalist executors on this benchmark, validating prior work and revealing a novel, interesting challenge for the LM reasoning community.","Our code is available at https://github.com/google-deepmind/clrs/tree/master/clrs/_src/clrs_text."],"url":"http://arxiv.org/abs/2406.04229v1","category":"cs.LG"}
{"created":"2024-06-06 16:21:13","title":"Rank-2 wobbly bundles from special divisors on spectral curves","abstract":"Let $X$ be a compact connected Riemann surface of genus $g \\geq 3$ and $S \\rightarrow X$ a $2:1$ branched covering defined by a generic quadratic differential on $X$. In the following notes, we explore how taking direct images of line bundles defined by certain distinguished special divisors on $S$ gives rank-2 bundles having nilpotent Higgs fields on $X$. The key ingredient is the notion of Baker-Akhiezer divisors associated to Higgs bundles and choices of subbundles of the underlying rank-2 bundles.","sentences":["Let $X$ be a compact connected Riemann surface of genus $g \\geq 3$ and $S \\rightarrow X$ a $2:1$ branched covering defined by a generic quadratic differential on $X$. In the following notes, we explore how taking direct images of line bundles defined by certain distinguished special divisors on $S$ gives rank-2 bundles having nilpotent Higgs fields on $X$. The key ingredient is the notion of Baker-Akhiezer divisors associated to Higgs bundles and choices of subbundles of the underlying rank-2 bundles."],"url":"http://arxiv.org/abs/2406.04224v1","category":"math.AG"}
{"created":"2024-06-06 16:20:53","title":"Resolving the Module of Derivations on an $n \\times (n+1)$ Determinantal Ring","abstract":"We use the construction of the relative bar resolution via differential graded structures to obtain the minimal graded free resolution of $\\text{Der}_{R \\mid k}$, where $R$ is a determinantal ring defined by the maximal minors of an $n \\times (n+1)$ generic matrix and $k$ is its coefficient field. Along the way, we compute an explicit action of the Hilbert-Burch differential graded algebra on a differential graded module resolving the cokernel of the Jacobian matrix whose kernel is $\\text{Der}_{R \\mid k}$. As a consequence of the minimality of the resulting relative bar resolution, we get a minimal generating set for $\\text{Der}_{R \\mid k}$ as an $R$-module, which, while already known, has not been obtained via our methods.","sentences":["We use the construction of the relative bar resolution via differential graded structures to obtain the minimal graded free resolution of $\\text{Der}_{R \\mid k}$, where $R$ is a determinantal ring defined by the maximal minors of an $n \\times (n+1)$ generic matrix and $k$ is its coefficient field.","Along the way, we compute an explicit action of the Hilbert-Burch differential graded algebra on a differential graded module resolving the cokernel of the Jacobian matrix whose kernel is $\\text{Der}_{R \\mid k}$. As a consequence of the minimality of the resulting relative bar resolution, we get a minimal generating set for $\\text{Der}_{R \\mid k}$ as an $R$-module, which, while already known, has not been obtained via our methods."],"url":"http://arxiv.org/abs/2406.04223v1","category":"math.AC"}
{"created":"2024-06-06 16:20:27","title":"Coarse embeddability, $L^1$-compression and Percolations on General Graphs","abstract":"We show that a locally finite, connected graph has a coarse embedding into a Hilbert space if and only if there exist bond percolations with arbitrarily large marginals and two-point function vanishing at infinity. We further show that the decay is stretched exponential with stretching exponent $\\alpha\\in[0,1]$ if and only if the $L^1$-compression exponent of the graph is at least $\\alpha$, leading to a probabilistic characterization of this exponent. These results are new even in the particular setting of Cayley graphs of finitely generated groups. The proofs build on a new probabilistic method introduced recently by the authors to study group-invariant percolation on Cayley graphs [24,25], which is now extended to the general, non-symmetric situation of graphs to study their coarse embeddability and compression exponents.","sentences":["We show that a locally finite, connected graph has a coarse embedding into a Hilbert space if and only if there exist bond percolations with arbitrarily large marginals and two-point function vanishing at infinity.","We further show that the decay is stretched exponential with stretching exponent $\\alpha\\in[0,1]$ if and only if the $L^1$-compression exponent of the graph is at least $\\alpha$, leading to a probabilistic characterization of this exponent.","These results are new even in the particular setting of Cayley graphs of finitely generated groups.","The proofs build on a new probabilistic method introduced recently by the authors to study group-invariant percolation on Cayley graphs [24,25], which is now extended to the general, non-symmetric situation of graphs to study their coarse embeddability and compression exponents."],"url":"http://arxiv.org/abs/2406.04222v1","category":"math.PR"}
{"created":"2024-06-06 16:20:07","title":"Matching Anything by Segmenting Anything","abstract":"The robust association of the same objects across video frames in complex scenes is crucial for many applications, especially Multiple Object Tracking (MOT). Current methods predominantly rely on labeled domain-specific video datasets, which limits the cross-domain generalization of learned similarity embeddings. We propose MASA, a novel method for robust instance association learning, capable of matching any objects within videos across diverse domains without tracking labels. Leveraging the rich object segmentation from the Segment Anything Model (SAM), MASA learns instance-level correspondence through exhaustive data transformations. We treat the SAM outputs as dense object region proposals and learn to match those regions from a vast image collection. We further design a universal MASA adapter which can work in tandem with foundational segmentation or detection models and enable them to track any detected objects. Those combinations present strong zero-shot tracking ability in complex domains. Extensive tests on multiple challenging MOT and MOTS benchmarks indicate that the proposed method, using only unlabeled static images, achieves even better performance than state-of-the-art methods trained with fully annotated in-domain video sequences, in zero-shot association. Project Page: https://matchinganything.github.io/","sentences":["The robust association of the same objects across video frames in complex scenes is crucial for many applications, especially Multiple Object Tracking (MOT).","Current methods predominantly rely on labeled domain-specific video datasets, which limits the cross-domain generalization of learned similarity embeddings.","We propose MASA, a novel method for robust instance association learning, capable of matching any objects within videos across diverse domains without tracking labels.","Leveraging the rich object segmentation from the Segment Anything Model (SAM), MASA learns instance-level correspondence through exhaustive data transformations.","We treat the SAM outputs as dense object region proposals and learn to match those regions from a vast image collection.","We further design a universal MASA adapter which can work in tandem with foundational segmentation or detection models and enable them to track any detected objects.","Those combinations present strong zero-shot tracking ability in complex domains.","Extensive tests on multiple challenging MOT and MOTS benchmarks indicate that the proposed method, using only unlabeled static images, achieves even better performance than state-of-the-art methods trained with fully annotated in-domain video sequences, in zero-shot association.","Project Page: https://matchinganything.github.io/"],"url":"http://arxiv.org/abs/2406.04221v1","category":"cs.CV"}
{"created":"2024-06-06 16:18:30","title":"BEADs: Bias Evaluation Across Domains","abstract":"Recent improvements in large language models (LLMs) have significantly enhanced natural language processing (NLP) applications. However, these models can also inherit and perpetuate biases from their training data. Addressing this issue is crucial, yet many existing datasets do not offer evaluation across diverse NLP tasks. To tackle this, we introduce the Bias Evaluations Across Domains (BEADs) dataset, designed to support a wide range of NLP tasks, including text classification, bias entity recognition, bias quantification, and benign language generation. BEADs uses AI-driven annotation combined with experts' verification to provide reliable labels. This method overcomes the limitations of existing datasets that typically depend on crowd-sourcing, expert-only annotations with limited bias evaluations, or unverified AI labeling. Our empirical analysis shows that BEADs is effective in detecting and reducing biases across different language models, with smaller models fine-tuned on BEADs often outperforming LLMs in bias classification tasks. However, these models may still exhibit biases towards certain demographics. Fine-tuning LLMs with our benign language data also reduces biases while preserving the models' knowledge. Our findings highlight the importance of comprehensive bias evaluation and the potential of targeted fine-tuning for reducing the bias of LLMs. We are making BEADs publicly available at https://huggingface.co/datasets/shainar/BEAD   Warning: This paper contains examples that may be considered offensive.","sentences":["Recent improvements in large language models (LLMs) have significantly enhanced natural language processing (NLP) applications.","However, these models can also inherit and perpetuate biases from their training data.","Addressing this issue is crucial, yet many existing datasets do not offer evaluation across diverse NLP tasks.","To tackle this, we introduce the Bias Evaluations Across Domains (BEADs) dataset, designed to support a wide range of NLP tasks, including text classification, bias entity recognition, bias quantification, and benign language generation.","BEADs uses AI-driven annotation combined with experts' verification to provide reliable labels.","This method overcomes the limitations of existing datasets that typically depend on crowd-sourcing, expert-only annotations with limited bias evaluations, or unverified AI labeling.","Our empirical analysis shows that BEADs is effective in detecting and reducing biases across different language models, with smaller models fine-tuned on BEADs often outperforming LLMs in bias classification tasks.","However, these models may still exhibit biases towards certain demographics.","Fine-tuning LLMs with our benign language data also reduces biases while preserving the models' knowledge.","Our findings highlight the importance of comprehensive bias evaluation and the potential of targeted fine-tuning for reducing the bias of LLMs.","We are making BEADs publicly available at https://huggingface.co/datasets/shainar/BEAD   Warning: This paper contains examples that may be considered offensive."],"url":"http://arxiv.org/abs/2406.04220v1","category":"cs.CL"}
{"created":"2024-06-06 16:18:02","title":"Rethinking LLM and Linguistic Steganalysis: An Efficient Detection of Strongly Concealed Stego","abstract":"To detect stego (steganographic text) in complex scenarios, linguistic steganalysis (LS) with various motivations has been proposed and achieved excellent performance. However, with the development of generative steganography, some stegos have strong concealment, especially after the emergence of LLMs-based steganography, the existing LS has low detection or even cannot detect them. We designed a novel LS with two modes called LSGC. In the generation mode, we created an LS-task \"description\" and used the generation ability of LLM to explain whether texts to be detected are stegos. On this basis, we rethought the principle of LS and LLMs, and proposed the classification mode. In this mode, LSGC deleted the LS-task \"description\" and changed the \"causalLM\" LLMs to the \"sequenceClassification\" architecture. The LS features can be extracted by only one pass of the model, and a linear layer with initialization weights is added to obtain the classification probability. Experiments on strongly concealed stegos show that LSGC significantly improves detection and reaches SOTA performance. Additionally, LSGC in classification mode greatly reduces training time while maintaining high performance.","sentences":["To detect stego (steganographic text) in complex scenarios, linguistic steganalysis (LS) with various motivations has been proposed and achieved excellent performance.","However, with the development of generative steganography, some stegos have strong concealment, especially after the emergence of LLMs-based steganography, the existing LS has low detection or even cannot detect them.","We designed a novel LS with two modes called LSGC.","In the generation mode, we created an LS-task \"description\" and used the generation ability of LLM to explain whether texts to be detected are stegos.","On this basis, we rethought the principle of LS and LLMs, and proposed the classification mode.","In this mode, LSGC deleted the LS-task \"description\" and changed the \"causalLM\" LLMs to the \"sequenceClassification\" architecture.","The LS features can be extracted by only one pass of the model, and a linear layer with initialization weights is added to obtain the classification probability.","Experiments on strongly concealed stegos show that LSGC significantly improves detection and reaches SOTA performance.","Additionally, LSGC in classification mode greatly reduces training time while maintaining high performance."],"url":"http://arxiv.org/abs/2406.04218v1","category":"cs.CL"}
{"created":"2024-06-06 16:15:34","title":"What Do Language Models Learn in Context? The Structured Task Hypothesis","abstract":"Large language models (LLMs) exhibit an intriguing ability to learn a novel task from in-context examples presented in a demonstration, termed in-context learning (ICL). Understandably, a swath of research has been dedicated to uncovering the theories underpinning ICL. One popular hypothesis explains ICL by task selection. LLMs identify the task based on the demonstration and generalize it to the prompt. Another popular hypothesis is that ICL is a form of meta-learning, i.e., the models learn a learning algorithm at pre-training time and apply it to the demonstration. Finally, a third hypothesis argues that LLMs use the demonstration to select a composition of tasks learned during pre-training to perform ICL. In this paper, we empirically explore these three hypotheses that explain LLMs' ability to learn in context with a suite of experiments derived from common text classification tasks. We invalidate the first two hypotheses with counterexamples and provide evidence in support of the last hypothesis. Our results suggest an LLM could learn a novel task in context via composing tasks learned during pre-training.","sentences":["Large language models (LLMs) exhibit an intriguing ability to learn a novel task from in-context examples presented in a demonstration, termed in-context learning (ICL).","Understandably, a swath of research has been dedicated to uncovering the theories underpinning ICL.","One popular hypothesis explains ICL by task selection.","LLMs identify the task based on the demonstration and generalize it to the prompt.","Another popular hypothesis is that ICL is a form of meta-learning, i.e., the models learn a learning algorithm at pre-training time and apply it to the demonstration.","Finally, a third hypothesis argues that LLMs use the demonstration to select a composition of tasks learned during pre-training to perform ICL.","In this paper, we empirically explore these three hypotheses that explain LLMs' ability to learn in context with a suite of experiments derived from common text classification tasks.","We invalidate the first two hypotheses with counterexamples and provide evidence in support of the last hypothesis.","Our results suggest an LLM could learn a novel task in context via composing tasks learned during pre-training."],"url":"http://arxiv.org/abs/2406.04216v1","category":"cs.CL"}
{"created":"2024-06-06 16:14:54","title":"mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans","abstract":"It is very challenging to curate a dataset for language-specific knowledge and common sense in order to evaluate natural language understanding capabilities of language models. Due to the limitation in the availability of annotators, most current multilingual datasets are created through translation, which cannot evaluate such language-specific aspects. Therefore, we propose Multilingual CommonsenseQA (mCSQA) based on the construction process of CSQA but leveraging language models for a more efficient construction, e.g., by asking LM to generate questions/answers, refine answers and verify QAs followed by reduced human efforts for verification. Constructed dataset is a benchmark for cross-lingual language-transfer capabilities of multilingual LMs, and experimental results showed high language-transfer capabilities for questions that LMs could easily solve, but lower transfer capabilities for questions requiring deep knowledge or commonsense. This highlights the necessity of language-specific datasets for evaluation and training. Finally, our method demonstrated that multilingual LMs could create QA including language-specific knowledge, significantly reducing the dataset creation cost compared to manual creation. The datasets are available at https://huggingface.co/datasets/yusuke1997/mCSQA.","sentences":["It is very challenging to curate a dataset for language-specific knowledge and common sense in order to evaluate natural language understanding capabilities of language models.","Due to the limitation in the availability of annotators, most current multilingual datasets are created through translation, which cannot evaluate such language-specific aspects.","Therefore, we propose Multilingual CommonsenseQA (mCSQA) based on the construction process of CSQA but leveraging language models for a more efficient construction, e.g., by asking LM to generate questions/answers, refine answers and verify QAs followed by reduced human efforts for verification.","Constructed dataset is a benchmark for cross-lingual language-transfer capabilities of multilingual LMs, and experimental results showed high language-transfer capabilities for questions that LMs could easily solve, but lower transfer capabilities for questions requiring deep knowledge or commonsense.","This highlights the necessity of language-specific datasets for evaluation and training.","Finally, our method demonstrated that multilingual LMs could create QA including language-specific knowledge, significantly reducing the dataset creation cost compared to manual creation.","The datasets are available at https://huggingface.co/datasets/yusuke1997/mCSQA."],"url":"http://arxiv.org/abs/2406.04215v1","category":"cs.CL"}
{"created":"2024-06-06 16:14:16","title":"ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models","abstract":"Large Language Models (LLMs) are transforming diverse fields and gaining increasing influence as human proxies. This development underscores the urgent need for evaluating value orientations and understanding of LLMs to ensure their responsible integration into public-facing applications. This work introduces ValueBench, the first comprehensive psychometric benchmark for evaluating value orientations and value understanding in LLMs. ValueBench collects data from 44 established psychometric inventories, encompassing 453 multifaceted value dimensions. We propose an evaluation pipeline grounded in realistic human-AI interactions to probe value orientations, along with novel tasks for evaluating value understanding in an open-ended value space. With extensive experiments conducted on six representative LLMs, we unveil their shared and distinctive value orientations and exhibit their ability to approximate expert conclusions in value-related extraction and generation tasks. ValueBench is openly accessible at https://github.com/Value4AI/ValueBench.","sentences":["Large Language Models (LLMs) are transforming diverse fields and gaining increasing influence as human proxies.","This development underscores the urgent need for evaluating value orientations and understanding of LLMs to ensure their responsible integration into public-facing applications.","This work introduces ValueBench, the first comprehensive psychometric benchmark for evaluating value orientations and value understanding in LLMs.","ValueBench collects data from 44 established psychometric inventories, encompassing 453 multifaceted value dimensions.","We propose an evaluation pipeline grounded in realistic human-AI interactions to probe value orientations, along with novel tasks for evaluating value understanding in an open-ended value space.","With extensive experiments conducted on six representative LLMs, we unveil their shared and distinctive value orientations and exhibit their ability to approximate expert conclusions in value-related extraction and generation tasks.","ValueBench is openly accessible at https://github.com/Value4AI/ValueBench."],"url":"http://arxiv.org/abs/2406.04214v1","category":"cs.CL"}
{"created":"2024-06-06 16:10:08","title":"Stirling permutation codes. II","abstract":"In the context of Stirling polynomials, Gessel and Stanley introduced the definition of Stirling permutation, which has attracted extensive attention over the past decades. Recently, we introduced Stirling permutation code and provided numerous equidistribution results as applications. The purpose of the present work is to further analyse Stirling permutation code. First, we derive an expansion formula expressing the joint distribution of the types A and B descent statistics over the hyperoctahedral group, and we also find an interlacing property involving the zeros of its coefficient polynomials. Next, we prove a strong connection between signed permutations in the hyperoctahedral group and Stirling permutations. Furthermore, we investigate unified generalizations of the trivariate second-order Eulerian polynomials and ascent-plateau polynomials. Using Stirling permutation codes, we provide expansion formulas for eight-variable and seventeen-variable polynomials, which imply several $e$-positive expansions and clarify the connection among several statistics. Our results generalize the results of Dumont, B\\'ona, Janson, Haglund-Visontai and Chen-Fu.","sentences":["In the context of Stirling polynomials, Gessel and Stanley introduced the definition of Stirling permutation, which has attracted extensive attention over the past decades.","Recently, we introduced Stirling permutation code and provided numerous equidistribution results as applications.","The purpose of the present work is to further analyse","Stirling permutation code.","First, we derive an expansion formula expressing the joint distribution of the types A and B descent statistics over the hyperoctahedral group, and we also find an interlacing property involving the zeros of its coefficient polynomials.","Next, we prove a strong connection between signed permutations in the hyperoctahedral group and Stirling permutations.","Furthermore, we investigate unified generalizations of the trivariate second-order Eulerian polynomials and ascent-plateau polynomials.","Using Stirling permutation codes, we provide expansion formulas for eight-variable and seventeen-variable polynomials, which imply several $e$-positive expansions and clarify the connection among several statistics.","Our results generalize the results of Dumont, B\\'ona, Janson, Haglund-Visontai and Chen-Fu."],"url":"http://arxiv.org/abs/2406.04211v1","category":"math.CO"}
{"created":"2024-06-06 16:05:45","title":"Aligning Agents like Large Language Models","abstract":"Training agents to behave as desired in complex 3D environments from high-dimensional sensory information is challenging. Imitation learning from diverse human behavior provides a scalable approach for training an agent with a sensible behavioral prior, but such an agent may not perform the specific behaviors of interest when deployed. To address this issue, we draw an analogy between the undesirable behaviors of imitation learning agents and the unhelpful responses of unaligned large language models (LLMs). We then investigate how the procedure for aligning LLMs can be applied to aligning agents in a 3D environment from pixels. For our analysis, we utilize an academically illustrative part of a modern console game in which the human behavior distribution is multi-modal, but we want our agent to imitate a single mode of this behavior. We demonstrate that we can align our agent to consistently perform the desired mode, while providing insights and advice for successfully applying this approach to training agents. Project webpage at https://adamjelley.github.io/aligning-agents-like-llms .","sentences":["Training agents to behave as desired in complex 3D environments from high-dimensional sensory information is challenging.","Imitation learning from diverse human behavior provides a scalable approach for training an agent with a sensible behavioral prior, but such an agent may not perform the specific behaviors of interest when deployed.","To address this issue, we draw an analogy between the undesirable behaviors of imitation learning agents and the unhelpful responses of unaligned large language models (LLMs).","We then investigate how the procedure for aligning LLMs can be applied to aligning agents in a 3D environment from pixels.","For our analysis, we utilize an academically illustrative part of a modern console game in which the human behavior distribution is multi-modal, but we want our agent to imitate a single mode of this behavior.","We demonstrate that we can align our agent to consistently perform the desired mode, while providing insights and advice for successfully applying this approach to training agents.","Project webpage at https://adamjelley.github.io/aligning-agents-like-llms ."],"url":"http://arxiv.org/abs/2406.04208v1","category":"cs.LG"}
{"created":"2024-06-06 16:04:06","title":"Diffusion-based image inpainting with internal learning","abstract":"Diffusion models are now the undisputed state-of-the-art for image generation and image restoration. However, they require large amounts of computational power for training and inference. In this paper, we propose lightweight diffusion models for image inpainting that can be trained on a single image, or a few images. We show that our approach competes with large state-of-the-art models in specific cases. We also show that training a model on a single image is particularly relevant for image acquisition modality that differ from the RGB images of standard learning databases. We show results in three different contexts: texture images, line drawing images, and materials BRDF, for which we achieve state-of-the-art results in terms of realism, with a computational load that is greatly reduced compared to concurrent methods.","sentences":["Diffusion models are now the undisputed state-of-the-art for image generation and image restoration.","However, they require large amounts of computational power for training and inference.","In this paper, we propose lightweight diffusion models for image inpainting that can be trained on a single image, or a few images.","We show that our approach competes with large state-of-the-art models in specific cases.","We also show that training a model on a single image is particularly relevant for image acquisition modality that differ from the RGB images of standard learning databases.","We show results in three different contexts: texture images, line drawing images, and materials BRDF, for which we achieve state-of-the-art results in terms of realism, with a computational load that is greatly reduced compared to concurrent methods."],"url":"http://arxiv.org/abs/2406.04206v1","category":"cs.CV"}
{"created":"2024-06-06 16:02:51","title":"Why Study the Spherical Convexity of Non-Homogeneous Quadratic Functions, and What Makes It Surprising?","abstract":"This paper presents necessary, sufficient, and equivalent conditions for the spherical convexity of non-homogeneous quadratic functions. In addition to motivating this study and identifying useful criteria for determining whether such functions are spherically convex, we discovered surprising properties that distinguish spherically convex quadratic functions from their geodesically convex counterparts in both hyperbolic and Euclidean spaces. Since spherically convex functions over the entire sphere are constant, we restricted our focus to proper spherically convex subsets of the sphere. Although most of our results pertain to non-homogeneous quadratic functions on the spherically convex set of unit vectors with positive coordinates, we also present findings for more general spherically convex sets. Beyond the general non-homogeneous quadratic functions, we consider explicit special cases where the matrix in the function's definition is of a specific type, such as positive, diagonal, and Z-matrix.","sentences":["This paper presents necessary, sufficient, and equivalent conditions for the spherical convexity of non-homogeneous quadratic functions.","In addition to motivating this study and identifying useful criteria for determining whether such functions are spherically convex, we discovered surprising properties that distinguish spherically convex quadratic functions from their geodesically convex counterparts in both hyperbolic and Euclidean spaces.","Since spherically convex functions over the entire sphere are constant, we restricted our focus to proper spherically convex subsets of the sphere.","Although most of our results pertain to non-homogeneous quadratic functions on the spherically convex set of unit vectors with positive coordinates, we also present findings for more general spherically convex sets.","Beyond the general non-homogeneous quadratic functions, we consider explicit special cases where the matrix in the function's definition is of a specific type, such as positive, diagonal, and Z-matrix."],"url":"http://arxiv.org/abs/2406.04205v1","category":"math.OC"}
{"created":"2024-06-06 16:00:20","title":"Legal Documents Drafting with Fine-Tuned Pre-Trained Large Language Model","abstract":"With the development of large-scale Language Models (LLM), fine-tuning pre-trained LLM has become a mainstream paradigm for solving downstream tasks of natural language processing. However, training a language model in the legal field requires a large number of legal documents so that the language model can learn legal terminology and the particularity of the format of legal documents. The typical NLP approaches usually rely on many manually annotated data sets for training. However, in the legal field application, it is difficult to obtain a large number of manually annotated data sets, which restricts the typical method applied to the task of drafting legal documents. The experimental results of this paper show that not only can we leverage a large number of annotation-free legal documents without Chinese word segmentation to fine-tune a large-scale language model, but more importantly, it can fine-tune a pre-trained LLM on the local computer to achieve the generating legal document drafts task, and at the same time achieve the protection of information privacy and to improve information security issues.","sentences":["With the development of large-scale Language Models (LLM), fine-tuning pre-trained LLM has become a mainstream paradigm for solving downstream tasks of natural language processing.","However, training a language model in the legal field requires a large number of legal documents so that the language model can learn legal terminology and the particularity of the format of legal documents.","The typical NLP approaches usually rely on many manually annotated data sets for training.","However, in the legal field application, it is difficult to obtain a large number of manually annotated data sets, which restricts the typical method applied to the task of drafting legal documents.","The experimental results of this paper show that not only can we leverage a large number of annotation-free legal documents without Chinese word segmentation to fine-tune a large-scale language model, but more importantly, it can fine-tune a pre-trained LLM on the local computer to achieve the generating legal document drafts task, and at the same time achieve the protection of information privacy and to improve information security issues."],"url":"http://arxiv.org/abs/2406.04202v1","category":"cs.CL"}
{"created":"2024-06-06 15:59:17","title":"Towards Principled Superhuman AI for Multiplayer Symmetric Games","abstract":"Multiplayer games, when the number of players exceeds two, present unique challenges that fundamentally distinguish them from the extensively studied two-player zero-sum games. These challenges arise from the non-uniqueness of equilibria and the risk of agents performing highly suboptimally when adopting equilibrium strategies. While a line of recent works developed learning systems successfully achieving human-level or even superhuman performance in popular multiplayer games such as Mahjong, Poker, and Diplomacy, two critical questions remain unaddressed: (1) What is the correct solution concept that AI agents should find? and (2) What is the general algorithmic framework that provably solves all games within this class? This paper takes the first step towards solving these unique challenges of multiplayer games by provably addressing both questions in multiplayer symmetric normal-form games. We also demonstrate that many meta-algorithms developed in prior practical systems for multiplayer games can fail to achieve even the basic goal of obtaining agent's equal share of the total reward.","sentences":["Multiplayer games, when the number of players exceeds two, present unique challenges that fundamentally distinguish them from the extensively studied two-player zero-sum games.","These challenges arise from the non-uniqueness of equilibria and the risk of agents performing highly suboptimally when adopting equilibrium strategies.","While a line of recent works developed learning systems successfully achieving human-level or even superhuman performance in popular multiplayer games such as Mahjong, Poker, and Diplomacy, two critical questions remain unaddressed: (1) What is the correct solution concept that AI agents should find?","and (2) What is the general algorithmic framework that provably solves all games within this class?","This paper takes the first step towards solving these unique challenges of multiplayer games by provably addressing both questions in multiplayer symmetric normal-form games.","We also demonstrate that many meta-algorithms developed in prior practical systems for multiplayer games can fail to achieve even the basic goal of obtaining agent's equal share of the total reward."],"url":"http://arxiv.org/abs/2406.04201v1","category":"cs.LG"}
{"created":"2024-06-06 15:55:53","title":"DICE: Detecting In-distribution Contamination in LLM's Fine-tuning Phase for Math Reasoning","abstract":"The advancement of large language models (LLMs) relies on evaluation using public benchmarks, but data contamination can lead to overestimated performance. Previous researches focus on detecting contamination by determining whether the model has seen the exact same data during training. In this work, we argue that even training on data similar to benchmark data inflates performance on in-distribution tasks without improving overall capacity, which we called In-distribution contamination. To effectively detect in-distribution contamination, we propose DICE, a novel method that leverages the internal states of LLMs to locate-then-detect the contamination. DICE first identifies the most sensitive layer to contamination, then trains a classifier based on the internal states of that layer. Experiments reveal DICE's high accuracy in detecting in-distribution contamination across various LLMs and math reasoning datasets. We also show the generalization capability of the trained DICE detector, which is able to detect contamination across multiple benchmarks with similar distributions. Additionally, we find that the DICE detection scores are positively correlated with the performance of ten LLMs fine-tuned by either us or other organizations on four math reasoning datasets (with $R^2$ values between 0.6 and 0.75). This indicates that the in-distribution contamination problem potentially lead to an overestimation of the true capabilities of many existing models. The code and data are available at https://github.com/THU-KEG/DICE.","sentences":["The advancement of large language models (LLMs) relies on evaluation using public benchmarks, but data contamination can lead to overestimated performance.","Previous researches focus on detecting contamination by determining whether the model has seen the exact same data during training.","In this work, we argue that even training on data similar to benchmark data inflates performance on in-distribution tasks without improving overall capacity, which we called In-distribution contamination.","To effectively detect in-distribution contamination, we propose DICE, a novel method that leverages the internal states of LLMs to locate-then-detect the contamination.","DICE first identifies the most sensitive layer to contamination, then trains a classifier based on the internal states of that layer.","Experiments reveal DICE's high accuracy in detecting in-distribution contamination across various LLMs and math reasoning datasets.","We also show the generalization capability of the trained DICE detector, which is able to detect contamination across multiple benchmarks with similar distributions.","Additionally, we find that the DICE detection scores are positively correlated with the performance of ten LLMs fine-tuned by either us or other organizations on four math reasoning datasets (with $R^2$ values between 0.6 and 0.75).","This indicates that the in-distribution contamination problem potentially lead to an overestimation of the true capabilities of many existing models.","The code and data are available at https://github.com/THU-KEG/DICE."],"url":"http://arxiv.org/abs/2406.04197v1","category":"cs.CL"}
{"created":"2024-06-06 15:52:56","title":"Machine Learning-Driven Microwave Imaging for Soil Moisture Estimation near Leaky Pipe","abstract":"Characterizing soil moisture (SM) around drip irrigation pipes is crucial for precise and optimized farming. Machine learning (ML) approaches are particularly suitable for this task as they can reduce uncertainties caused by soil conditions and the drip pipe positions, using features extracted from relevant datasets. This letter addresses local moisture detection in the vicinity of dripping pipes using a portable microwave imaging system. The employed ML approach is fed with two dimensional images generated by two different microwave imaging techniques based on spatio-temporal measurements at various frequency bands. The study investigates the performance of K-Nearest Neighbor (KNN) and Convolutional Neural Networks (CNN) algorithms for moisture classification based on these images, both before and after performing soil clutter reduction. We also explore the potentials of CNN and KNN for moisture estimation around the plant roots and in the presence of pebbles. The results demonstrate the more accurate moisture estimation using CNN when it is applied after clutter reduction considering back projection algorithm (BPA) as the imaging technique.","sentences":["Characterizing soil moisture (SM) around drip irrigation pipes is crucial for precise and optimized farming.","Machine learning (ML) approaches are particularly suitable for this task as they can reduce uncertainties caused by soil conditions and the drip pipe positions, using features extracted from relevant datasets.","This letter addresses local moisture detection in the vicinity of dripping pipes using a portable microwave imaging system.","The employed ML approach is fed with two dimensional images generated by two different microwave imaging techniques based on spatio-temporal measurements at various frequency bands.","The study investigates the performance of K-Nearest Neighbor (KNN) and Convolutional Neural Networks (CNN) algorithms for moisture classification based on these images, both before and after performing soil clutter reduction.","We also explore the potentials of CNN and KNN for moisture estimation around the plant roots and in the presence of pebbles.","The results demonstrate the more accurate moisture estimation using CNN when it is applied after clutter reduction considering back projection algorithm (BPA) as the imaging technique."],"url":"http://arxiv.org/abs/2406.04193v1","category":"eess.IV"}
{"created":"2024-06-06 15:48:50","title":"Large-scale semi-discrete optimal transport with distributed Voronoi diagrams","abstract":"In this article, I propose a numerical method to solve semi-discrete optimal transport problems for gigantic pointsets ($10^8$ points and more). By pushing the limits by several orders of magnitude, it opens the path to new applications in cosmology, fluid simulation and data science to name but a few. The method is based on a new algorithm that computes (generalized) Voronoi diagrams in parallel and in a distributed way. First I make the simple observation that the cells defined by a subgraph of the Delaunay graph contain the Voronoi cells, and that one can deduce the missing edges from the intersections between those cells. Based on this observation, I introduce the Distributed Voronoi Diagram algorithm (DVD) that can be used on a cluster and that exchanges vertices between the nodes as need be. I also report early experimental results, demonstrating that the DVD algorithm has the potential to solve some giga-scale semi-discrete optimal transport problems encountered in computational cosmology.","sentences":["In this article, I propose a numerical method to solve semi-discrete optimal transport problems for gigantic pointsets ($10^8$ points and more).","By pushing the limits by several orders of magnitude, it opens the path to new applications in cosmology, fluid simulation and data science to name but a few.","The method is based on a new algorithm that computes (generalized)","Voronoi diagrams in parallel and in a distributed way.","First I make the simple observation that the cells defined by a subgraph of the Delaunay graph contain the Voronoi cells, and that one can deduce the missing edges from the intersections between those cells.","Based on this observation, I introduce the Distributed Voronoi Diagram algorithm (DVD) that can be used on a cluster and that exchanges vertices between the nodes as need be.","I also report early experimental results, demonstrating that the DVD algorithm has the potential to solve some giga-scale semi-discrete optimal transport problems encountered in computational cosmology."],"url":"http://arxiv.org/abs/2406.04192v1","category":"physics.comp-ph"}
{"created":"2024-06-06 15:47:06","title":"Strong Approximations for Empirical Processes Indexed by Lipschitz Functions","abstract":"This paper presents new uniform Gaussian strong approximations for empirical processes indexed by classes of functions based on $d$-variate random vectors ($d\\geq1$). First, a uniform Gaussian strong approximation is established for general empirical processes indexed by Lipschitz functions, encompassing and improving on all previous results in the literature. When specialized to the setting considered by Rio (1994), and certain constraints on the function class hold, our result improves the approximation rate $n^{-1/(2d)}$ to $n^{-1/\\max\\{d,2\\}}$, up to the same $\\operatorname{polylog} n$ term, where $n$ denotes the sample size. Remarkably, we establish a valid uniform Gaussian strong approximation at the optimal rate $n^{-1/2}\\log n$ for $d=2$, which was previously known to be valid only for univariate ($d=1$) empirical processes via the celebrated Hungarian construction (Koml\\'os et al., 1975). Second, a uniform Gaussian strong approximation is established for a class of multiplicative separable empirical processes indexed by Lipschitz functions, which address some outstanding problems in the literature (Chernozhukov et al., 2014, Section 3). In addition, two other uniform Gaussian strong approximation results are presented for settings where the function class takes the form of a sequence of Haar basis based on generalized quasi-uniform partitions. We demonstrate the improvements and usefulness of our new strong approximation results with several statistical applications to nonparametric density and regression estimation.","sentences":["This paper presents new uniform Gaussian strong approximations for empirical processes indexed by classes of functions based on $d$-variate random vectors ($d\\geq1$).","First, a uniform Gaussian strong approximation is established for general empirical processes indexed by Lipschitz functions, encompassing and improving on all previous results in the literature.","When specialized to the setting considered by Rio (1994), and certain constraints on the function class hold, our result improves the approximation rate $n^{-1/(2d)}$ to $n^{-1/\\max\\{d,2\\}}$, up to the same $\\operatorname{polylog} n$ term, where $n$ denotes the sample size.","Remarkably, we establish a valid uniform Gaussian strong approximation at the optimal rate $n^{-1/2}\\log n$ for $d=2$, which was previously known to be valid only for univariate ($d=1$) empirical processes via the celebrated Hungarian construction (Koml\\'os et al., 1975).","Second, a uniform Gaussian strong approximation is established for a class of multiplicative separable empirical processes indexed by Lipschitz functions, which address some outstanding problems in the literature (Chernozhukov et al., 2014, Section 3).","In addition, two other uniform Gaussian strong approximation results are presented for settings where the function class takes the form of a sequence of Haar basis based on generalized quasi-uniform partitions.","We demonstrate the improvements and usefulness of our new strong approximation results with several statistical applications to nonparametric density and regression estimation."],"url":"http://arxiv.org/abs/2406.04191v1","category":"math.ST"}
{"created":"2024-06-06 15:44:52","title":"Digital Twin Aided RIS Communication: Robust Beamforming and Interference Management","abstract":"Reconfigurable intelligent surfaces (RISs) are envisioned to play a key role in future wireless communication networks. However, channel estimation in RIS-aided wireless networks is challenging due to their passive nature and the large number of reflective elements, leading to high channel estimation overhead. Additionally, conventional methods like beam sweeping, which do not rely on explicit channel state information, often struggle in managing interference in multi-user networks. In this paper, we propose a novel approach that leverages digital twins (DTs) of the physical environments to approximate channels using electromagnetic 3D models and ray tracing, thus relaxing the need for channel estimation and extensive over-the-air computations in RIS-aided wireless networks. To address the digital twins channel approximation errors, we further refine this approach with a DT-specific robust transmission design that reliably meets minimum desired rates. The results show that our method secures these rates over 90% of the time, significantly outperforming beam sweeping, which achieves these rates less than 8% of the time due to its poor management of transmitting power and interference.","sentences":["Reconfigurable intelligent surfaces (RISs) are envisioned to play a key role in future wireless communication networks.","However, channel estimation in RIS-aided wireless networks is challenging due to their passive nature and the large number of reflective elements, leading to high channel estimation overhead.","Additionally, conventional methods like beam sweeping, which do not rely on explicit channel state information, often struggle in managing interference in multi-user networks.","In this paper, we propose a novel approach that leverages digital twins (DTs) of the physical environments to approximate channels using electromagnetic 3D models and ray tracing, thus relaxing the need for channel estimation and extensive over-the-air computations in RIS-aided wireless networks.","To address the digital twins channel approximation errors, we further refine this approach with a DT-specific robust transmission design that reliably meets minimum desired rates.","The results show that our method secures these rates over 90% of the time, significantly outperforming beam sweeping, which achieves these rates less than 8% of the time due to its poor management of transmitting power and interference."],"url":"http://arxiv.org/abs/2406.04188v1","category":"eess.SP"}
{"created":"2024-06-06 15:44:39","title":"A Multiscale Perspective on Maximum Marginal Likelihood Estimation","abstract":"In this paper, we provide a multiscale perspective on the problem of maximum marginal likelihood estimation. We consider and analyse a diffusion-based maximum marginal likelihood estimation scheme using ideas from multiscale dynamics. Our perspective is based on stochastic averaging; we make an explicit connection between ideas in applied probability and parameter inference in computational statistics. In particular, we consider a general class of coupled Langevin diffusions for joint inference of latent variables and parameters in statistical models, where the latent variables are sampled from a fast Langevin process (which acts as a sampler), and the parameters are updated using a slow Langevin process (which acts as an optimiser). We show that the resulting system of stochastic differential equations (SDEs) can be viewed as a two-time scale system. To demonstrate the utility of such a perspective, we show that the averaged parameter dynamics obtained in the limit of scale separation can be used to estimate the optimal parameter, within the strongly convex setting. We do this by using recent uniform-in-time non-asymptotic averaging bounds. Finally, we conclude by showing that the slow-fast algorithm we consider here, termed Slow-Fast Langevin Algorithm, performs on par with state-of-the-art methods on a variety of examples. We believe that the stochastic averaging approach we provide in this paper enables us to look at these algorithms from a fresh angle, as well as unlocking the path to develop and analyse new methods using well-established averaging principles.","sentences":["In this paper, we provide a multiscale perspective on the problem of maximum marginal likelihood estimation.","We consider and analyse a diffusion-based maximum marginal likelihood estimation scheme using ideas from multiscale dynamics.","Our perspective is based on stochastic averaging; we make an explicit connection between ideas in applied probability and parameter inference in computational statistics.","In particular, we consider a general class of coupled Langevin diffusions for joint inference of latent variables and parameters in statistical models, where the latent variables are sampled from a fast Langevin process (which acts as a sampler), and the parameters are updated using a slow Langevin process (which acts as an optimiser).","We show that the resulting system of stochastic differential equations (SDEs) can be viewed as a two-time scale system.","To demonstrate the utility of such a perspective, we show that the averaged parameter dynamics obtained in the limit of scale separation can be used to estimate the optimal parameter, within the strongly convex setting.","We do this by using recent uniform-in-time non-asymptotic averaging bounds.","Finally, we conclude by showing that the slow-fast algorithm we consider here, termed Slow-Fast Langevin Algorithm, performs on par with state-of-the-art methods on a variety of examples.","We believe that the stochastic averaging approach we provide in this paper enables us to look at these algorithms from a fresh angle, as well as unlocking the path to develop and analyse new methods using well-established averaging principles."],"url":"http://arxiv.org/abs/2406.04187v1","category":"stat.CO"}
{"created":"2024-06-06 15:43:20","title":"Difference Equations and Integral Families for Witten Diagrams","abstract":"We show that tree-level and one-loop Mellin space correlators in anti-de Sitter space obey certain difference equations, which are the direct analog to the differential equations for Feynman loop integrals in the flat space. Finite-difference relations, which we refer to as ``summation-by-parts relations'', in parallel with the integration-by-parts relations for Feynman loop integrals, are derived to reduce the integrals to a basis. We illustrate the general methodology by explicitly deriving the difference equations and summation-by-parts relations for various tree-level and one-loop Witten diagrams up to the four-point bubble level.","sentences":["We show that tree-level and one-loop Mellin space correlators in anti-de Sitter space obey certain difference equations, which are the direct analog to the differential equations for Feynman loop integrals in the flat space.","Finite-difference relations, which we refer to as ``summation-by-parts relations'', in parallel with the integration-by-parts relations for Feynman loop integrals, are derived to reduce the integrals to a basis.","We illustrate the general methodology by explicitly deriving the difference equations and summation-by-parts relations for various tree-level and one-loop Witten diagrams up to the four-point bubble level."],"url":"http://arxiv.org/abs/2406.04186v1","category":"hep-th"}
{"created":"2024-06-06 15:40:29","title":"Shield Synthesis for LTL Modulo Theories","abstract":"In recent years, Machine Learning (ML) models have achieved remarkable success in various domains. However, these models also tend to demonstrate unsafe behaviors, precluding their deployment in safety-critical systems. To cope with this issue, ample research focuses on developing methods that guarantee the safe behaviour of a given ML model. A prominent example is shielding which incorporates an external component (a \"shield\") that blocks unwanted behavior. Despite significant progress, shielding suffers from a main setback: it is currently geared towards properties encoded solely in propositional logics (e.g., LTL) and is unsuitable for richer logics. This, in turn, limits the widespread applicability of shielding in many real-world systems. In this work, we address this gap, and extend shielding to LTL modulo theories, by building upon recent advances in reactive synthesis modulo theories. This allowed us to develop a novel approach for generating shields conforming to complex safety specifications in these more expressive, logics. We evaluated our shields and demonstrate their ability to handle rich data with temporal dynamics. To the best of our knowledge, this is the first approach for synthesizing shields for such expressivity.","sentences":["In recent years, Machine Learning (ML) models have achieved remarkable success in various domains.","However, these models also tend to demonstrate unsafe behaviors, precluding their deployment in safety-critical systems.","To cope with this issue, ample research focuses on developing methods that guarantee the safe behaviour of a given ML model.","A prominent example is shielding which incorporates an external component (a \"shield\") that blocks unwanted behavior.","Despite significant progress, shielding suffers from a main setback: it is currently geared towards properties encoded solely in propositional logics (e.g., LTL) and is unsuitable for richer logics.","This, in turn, limits the widespread applicability of shielding in many real-world systems.","In this work, we address this gap, and extend shielding to LTL modulo theories, by building upon recent advances in reactive synthesis modulo theories.","This allowed us to develop a novel approach for generating shields conforming to complex safety specifications in these more expressive, logics.","We evaluated our shields and demonstrate their ability to handle rich data with temporal dynamics.","To the best of our knowledge, this is the first approach for synthesizing shields for such expressivity."],"url":"http://arxiv.org/abs/2406.04184v1","category":"cs.LO"}
{"created":"2024-06-06 15:38:16","title":"Exact solutions for analog Hawking effect in dielectric media","abstract":"In the framework of the analog Hawking radiation for dielectric media, we analyze a toy-model and also the 2D reduction of the Hopfield model for a specific monotone and realistic profile for the refractive index. We are able to provide exact solutions, which do not require any weak dispersion approximation. The theory of Fuchsian ordinary differential equations is the basic tool for recovering exact solutions, which are rigoroulsy identified, and involve the so-called generalized hypergeometric functions $_4F_3(\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4;\\beta_1,\\beta_2,\\beta_3;z)$. A complete set of connection formulas are available, both for the subcritical case and for the transcritical one, and also the Stokes phenomenon occurring in the problem is fully discussed. From the physical point of view, we focus on the problem of thermality. Under suitable conditions, the Hawking temperature is deduced, and we show that it is in fully agreement with the expression deduced in other frameworks under various approximations.","sentences":["In the framework of the analog Hawking radiation for dielectric media, we analyze a toy-model and also the 2D reduction of the Hopfield model for a specific monotone and realistic profile for the refractive index.","We are able to provide exact solutions, which do not require any weak dispersion approximation.","The theory of Fuchsian ordinary differential equations is the basic tool for recovering exact solutions, which are rigoroulsy identified, and involve the so-called generalized hypergeometric functions $_4F_3(\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4;\\beta_1,\\beta_2,\\beta_3;z)$.","A complete set of connection formulas are available, both for the subcritical case and for the transcritical one, and also the Stokes phenomenon occurring in the problem is fully discussed.","From the physical point of view, we focus on the problem of thermality.","Under suitable conditions, the Hawking temperature is deduced, and we show that it is in fully agreement with the expression deduced in other frameworks under various approximations."],"url":"http://arxiv.org/abs/2406.04181v1","category":"hep-th"}
{"created":"2024-06-06 15:36:32","title":"Cogenesis by a sliding pNGB with symmetry non-restoration","abstract":"We show that a pseudo-Nambu-Goldstone boson (pNGB) with an initial misalignment angle can drive successful spontaneous baryogenesis, and become a good dark matter candidate if the corresponding global symmetry is non-restored at high temperatures. Considering a dimension-five explicit breaking operator, we find that the pNGB starts its motion with a sliding across rapidly decreasing potential barriers during which the baryon asymmetry is generated and frozen, and later it oscillates as dark matter. It is predicted that the pNGB mass and decay constant are around $5\\,{\\rm eV}$ and $3\\times10^6\\,{\\rm GeV}$, respectively, while the radial mode has a light mass $O(10)\\,{\\rm MeV}$ and a small mixing $O(10^{-4})$ with the Higgs boson. Applied to the Majoron in the type-I seesaw model, the heaviest right-handed neutrino is required to be as light as $100\\,{\\rm GeV}$. These predictions can be tested at kaon experiments, heavy neutral lepton searches, LHC, and future colliders.","sentences":["We show that a pseudo-Nambu-Goldstone boson (pNGB) with an initial misalignment angle can drive successful spontaneous baryogenesis, and become a good dark matter candidate if the corresponding global symmetry is non-restored at high temperatures.","Considering a dimension-five explicit breaking operator, we find that the pNGB starts its motion with a sliding across rapidly decreasing potential barriers during which the baryon asymmetry is generated and frozen, and later it oscillates as dark matter.","It is predicted that the pNGB mass and decay constant are around $5\\,{\\rm eV}$ and $3\\times10^6\\,{\\rm GeV}$, respectively, while the radial mode has a light mass $O(10)\\,{\\rm MeV}$ and a small mixing $O(10^{-4})$ with the Higgs boson.","Applied to the Majoron in the type-I seesaw model, the heaviest right-handed neutrino is required to be as light as $100\\,{\\rm GeV}$.","These predictions can be tested at kaon experiments, heavy neutral lepton searches, LHC, and future colliders."],"url":"http://arxiv.org/abs/2406.04180v1","category":"hep-ph"}
{"created":"2024-06-06 15:35:41","title":"Encoding Semantic Priors into the Weights of Implicit Neural Representation","abstract":"Implicit neural representation (INR) has recently emerged as a promising paradigm for signal representations, which takes coordinates as inputs and generates corresponding signal values. Since these coordinates contain no semantic features, INR fails to take any semantic information into consideration. However, semantic information has been proven critical in many vision tasks, especially for visual signal representation. This paper proposes a reparameterization method termed as SPW, which encodes the semantic priors to the weights of INR, thus making INR contain semantic information implicitly and enhancing its representational capacity. Specifically, SPW uses the Semantic Neural Network (SNN) to extract both low- and high-level semantic information of the target visual signal and generates the semantic vector, which is input into the Weight Generation Network (WGN) to generate the weights of INR model. Finally, INR uses the generated weights with semantic priors to map the coordinates to the signal values. After training, we only retain the generated weights while abandoning both SNN and WGN, thus SPW introduces no extra costs in inference. Experimental results show that SPW can improve the performance of various INR models significantly on various tasks, including image fitting, CT reconstruction, MRI reconstruction, and novel view synthesis. Further experiments illustrate that model with SPW has lower weight redundancy and learns more novel representations, validating the effectiveness of SPW.","sentences":["Implicit neural representation (INR) has recently emerged as a promising paradigm for signal representations, which takes coordinates as inputs and generates corresponding signal values.","Since these coordinates contain no semantic features, INR fails to take any semantic information into consideration.","However, semantic information has been proven critical in many vision tasks, especially for visual signal representation.","This paper proposes a reparameterization method termed as SPW, which encodes the semantic priors to the weights of INR, thus making INR contain semantic information implicitly and enhancing its representational capacity.","Specifically, SPW uses the Semantic Neural Network (SNN) to extract both low- and high-level semantic information of the target visual signal and generates the semantic vector, which is input into the Weight Generation Network (WGN) to generate the weights of INR model.","Finally, INR uses the generated weights with semantic priors to map the coordinates to the signal values.","After training, we only retain the generated weights while abandoning both SNN and WGN, thus SPW introduces no extra costs in inference.","Experimental results show that SPW can improve the performance of various INR models significantly on various tasks, including image fitting, CT reconstruction, MRI reconstruction, and novel view synthesis.","Further experiments illustrate that model with SPW has lower weight redundancy and learns more novel representations, validating the effectiveness of SPW."],"url":"http://arxiv.org/abs/2406.04178v1","category":"cs.CV"}
{"created":"2024-06-06 15:35:25","title":"A Voxel-based Approach for Simulating Microbial Decomposition in Soil: Comparison with LBM and Improvement of Morphological Models","abstract":"This study presents a new computational approach for simulating the microbial decomposition of organic matter, from 3D micro-computed tomography (micro-CT) images of soil. The method employs a valuated graph of connected voxels to simulate transformation and diffusion processes involved in microbial decomposition within the complex soil matrix. The resulting model can be adapted to simulate any diffusion-transformation processes in porous media. We implemented parallelization strategies and explored different numerical methods, including implicit, explicit, synchronous, and asynchronous schemes. To validate our method, we compared simulation outputs with those provided by LBioS and by Mosaic models. LBioS uses a lattice-Boltzmann method for diffusion and Mosaic takes benefit of Pore Network Geometrical Modelling (PNGM) by means of geometrical primitives such as spheres and ellipsoids. This approach achieved comparable results to traditional LBM-based simulations, but required only one-fourth of the computing time. Compared to Mosaic simulation, the proposed method is slower but more accurate and does not require any calibration. Furthermore, we present a theoretical framework and an application example to enhance PNGM-based simulations. This is accomplished by approximating the diffusional conductance coefficients using stochastic gradient descent and data generated by the current approach.","sentences":["This study presents a new computational approach for simulating the microbial decomposition of organic matter, from 3D micro-computed tomography (micro-CT) images of soil.","The method employs a valuated graph of connected voxels to simulate transformation and diffusion processes involved in microbial decomposition within the complex soil matrix.","The resulting model can be adapted to simulate any diffusion-transformation processes in porous media.","We implemented parallelization strategies and explored different numerical methods, including implicit, explicit, synchronous, and asynchronous schemes.","To validate our method, we compared simulation outputs with those provided by LBioS and by Mosaic models.","LBioS uses a lattice-Boltzmann method for diffusion and Mosaic takes benefit of Pore Network Geometrical Modelling (PNGM) by means of geometrical primitives such as spheres and ellipsoids.","This approach achieved comparable results to traditional LBM-based simulations, but required only one-fourth of the computing time.","Compared to Mosaic simulation, the proposed method is slower but more accurate and does not require any calibration.","Furthermore, we present a theoretical framework and an application example to enhance PNGM-based simulations.","This is accomplished by approximating the diffusional conductance coefficients using stochastic gradient descent and data generated by the current approach."],"url":"http://arxiv.org/abs/2406.04177v1","category":"cs.CV"}
{"created":"2024-06-06 15:34:19","title":"An explanation for the absence of echoes in black hole light curve autocorrelations","abstract":"The observed radiation from hot gas accreting onto a black hole depends on both the details of the flow and the spacetime geometry. The lensing behavior of a black hole produces a distinctive pattern of autocorrelations within its photon ring that encodes its mass, spin, and inclination. In particular, the time autocorrelation of the light curve is expected to display a series of peaks produced by light echoes of the source, with each peak delayed by the characteristic time lapse $\\tau$ between light echoes. However, such peaks are absent from the light curves of observed black holes. Here, we develop an analytical model for such light curves that demonstrates how, even though light echoes always exist in the signal, they do not produce autocorrelation peaks if the characteristic correlation timescale $\\lambda_0$ of the source is greater than $\\tau$. We validate our model against simulated light curves of a stochastic accretion model ray traced with a general-relativistic code, and then fit the model to an observed light curve for Sgr A*. We infer that $\\lambda_0>\\tau$, providing an explanation for the absence of light echoes in the time autocorrelations of Sgr A* light curves. Our results highlight the importance for black hole parameter inference of spatially resolving the photon ring via future space-based interferometry.","sentences":["The observed radiation from hot gas accreting onto a black hole depends on both the details of the flow and the spacetime geometry.","The lensing behavior of a black hole produces a distinctive pattern of autocorrelations within its photon ring that encodes its mass, spin, and inclination.","In particular, the time autocorrelation of the light curve is expected to display a series of peaks produced by light echoes of the source, with each peak delayed by the characteristic time lapse $\\tau$ between light echoes.","However, such peaks are absent from the light curves of observed black holes.","Here, we develop an analytical model for such light curves that demonstrates how, even though light echoes always exist in the signal, they do not produce autocorrelation peaks if the characteristic correlation timescale $\\lambda_0$ of the source is greater than $\\tau$. We validate our model against simulated light curves of a stochastic accretion model ray traced with a general-relativistic code, and then fit the model to an observed light curve for Sgr A*.","We infer that $\\lambda_0>\\tau$, providing an explanation for the absence of light echoes in the time autocorrelations of Sgr A* light curves.","Our results highlight the importance for black hole parameter inference of spatially resolving the photon ring via future space-based interferometry."],"url":"http://arxiv.org/abs/2406.04176v1","category":"astro-ph.HE"}
{"created":"2024-06-06 15:32:29","title":"Confabulation: The Surprising Value of Large Language Model Hallucinations","abstract":"This paper presents a systematic defense of large language model (LLM) hallucinations or 'confabulations' as a potential resource instead of a categorically negative pitfall. The standard view is that confabulations are inherently problematic and AI research should eliminate this flaw. In this paper, we argue and empirically demonstrate that measurable semantic characteristics of LLM confabulations mirror a human propensity to utilize increased narrativity as a cognitive resource for sense-making and communication. In other words, it has potential value. Specifically, we analyze popular hallucination benchmarks and reveal that hallucinated outputs display increased levels of narrativity and semantic coherence relative to veridical outputs. This finding reveals a tension in our usually dismissive understandings of confabulation. It suggests, counter-intuitively, that the tendency for LLMs to confabulate may be intimately associated with a positive capacity for coherent narrative-text generation.","sentences":["This paper presents a systematic defense of large language model (LLM) hallucinations or 'confabulations' as a potential resource instead of a categorically negative pitfall.","The standard view is that confabulations are inherently problematic and AI research should eliminate this flaw.","In this paper, we argue and empirically demonstrate that measurable semantic characteristics of LLM confabulations mirror a human propensity to utilize increased narrativity as a cognitive resource for sense-making and communication.","In other words, it has potential value.","Specifically, we analyze popular hallucination benchmarks and reveal that hallucinated outputs display increased levels of narrativity and semantic coherence relative to veridical outputs.","This finding reveals a tension in our usually dismissive understandings of confabulation.","It suggests, counter-intuitively, that the tendency for LLMs to confabulate may be intimately associated with a positive capacity for coherent narrative-text generation."],"url":"http://arxiv.org/abs/2406.04175v1","category":"cs.CL"}
{"created":"2024-06-06 15:28:05","title":"Twists of twisted generalized Weyl algebras","abstract":"We study graded twisted tensor products and graded twists of twisted generalized Weyl algebras (TGWAs). We show that the class of TGWAs is closed under these operations assuming mild hypotheses. We generalize a result on cocycle equivalence amongst multiparameter quantized Weyl algebras to the setting of TGWAs. As another application we prove that certain TGWAs of type $A_2$ are noetherian.","sentences":["We study graded twisted tensor products and graded twists of twisted generalized Weyl algebras (TGWAs).","We show that the class of TGWAs is closed under these operations assuming mild hypotheses.","We generalize a result on cocycle equivalence amongst multiparameter quantized Weyl algebras to the setting of TGWAs.","As another application we prove that certain TGWAs of type $A_2$ are noetherian."],"url":"http://arxiv.org/abs/2406.04172v1","category":"math.RA"}
{"created":"2024-06-06 15:27:52","title":"Element-wise Multiplication Based Physics-informed Neural Networks","abstract":"As a promising framework for resolving partial differential equations (PDEs), physics-informed neural networks (PINNs) have received widespread attention from industrial and scientific fields. However, lack of expressive ability and initialization pathology issues are found to prevent the application of PINNs in complex PDEs. In this work, we propose Element-wise Multiplication Based Physics-informed Neural Networks (EM-PINNs) to resolve these issues. The element-wise multiplication operation is adopted to transform features into high-dimensional, non-linear spaces, which effectively enhance the expressive capability of PINNs. Benefiting from element-wise multiplication operation, EM-PINNs can eliminate the initialization pathologies of PINNs. The proposed structure is verified on various benchmarks. The results show that EM-PINNs have strong expressive ability.","sentences":["As a promising framework for resolving partial differential equations (PDEs), physics-informed neural networks (PINNs) have received widespread attention from industrial and scientific fields.","However, lack of expressive ability and initialization pathology issues are found to prevent the application of PINNs in complex PDEs.","In this work, we propose Element-wise Multiplication Based Physics-informed Neural Networks (EM-PINNs) to resolve these issues.","The element-wise multiplication operation is adopted to transform features into high-dimensional, non-linear spaces, which effectively enhance the expressive capability of PINNs.","Benefiting from element-wise multiplication operation, EM-PINNs can eliminate the initialization pathologies of PINNs.","The proposed structure is verified on various benchmarks.","The results show that EM-PINNs have strong expressive ability."],"url":"http://arxiv.org/abs/2406.04170v1","category":"cs.LG"}
{"created":"2024-06-06 15:20:37","title":"Essentially Sharp Estimates on the Entropy Regularization Error in Discrete Discounted Markov Decision Processes","abstract":"We study the error introduced by entropy regularization of infinite-horizon discrete discounted Markov decision processes. We show that this error decreases exponentially in the inverse regularization strength both in a weighted KL-divergence and in value with a problem-specific exponent. We provide a lower bound matching our upper bound up to a polynomial factor. Our proof relies on the correspondence of the solutions of entropy-regularized Markov decision processes with gradient flows of the unregularized reward with respect to a Riemannian metric common in natural policy gradient methods. Further, this correspondence allows us to identify the limit of the gradient flow as the generalized maximum entropy optimal policy, thereby characterizing the implicit bias of the Kakade gradient flow which corresponds to a time-continuous version of the natural policy gradient method. We use this to show that for entropy-regularized natural policy gradient methods the overall error decays exponentially in the square root of the number of iterations improving existing sublinear guarantees.","sentences":["We study the error introduced by entropy regularization of infinite-horizon discrete discounted Markov decision processes.","We show that this error decreases exponentially in the inverse regularization strength both in a weighted KL-divergence and in value with a problem-specific exponent.","We provide a lower bound matching our upper bound up to a polynomial factor.","Our proof relies on the correspondence of the solutions of entropy-regularized Markov decision processes with gradient flows of the unregularized reward with respect to a Riemannian metric common in natural policy gradient methods.","Further, this correspondence allows us to identify the limit of the gradient flow as the generalized maximum entropy optimal policy, thereby characterizing the implicit bias of the Kakade gradient flow which corresponds to a time-continuous version of the natural policy gradient method.","We use this to show that for entropy-regularized natural policy gradient methods the overall error decays exponentially in the square root of the number of iterations improving existing sublinear guarantees."],"url":"http://arxiv.org/abs/2406.04163v1","category":"math.OC"}
{"created":"2024-06-06 15:19:56","title":"Gravitational reheating in Starobinsky inflation","abstract":"We investigate the possibility of achieving post-inflationary reheating exclusively through the gravitational interaction in Starobinsky inflation, which itself assumes nothing but gravity. We consider the possibility that the reheating sector couples to gravity via a non-minimal coupling. Our analysis is performed both in a perturbative and in a non-perturbative approach, where particle production is computed from Bogoliubov coefficients. Our findings indicate that, for sufficiently large non-minimal coupling ($\\xi\\gtrsim~\\text{few}\\times 10$), non-perturbative particle production allows for temperatures of order $10^{12}$ GeV to be reached while also ensuring that the Universe becomes radiation-dominated. This shows that the gravitational interaction could be the sole responsible for reheating the Universe after inflation, without the need to assume other ad hoc inflaton interactions.","sentences":["We investigate the possibility of achieving post-inflationary reheating exclusively through the gravitational interaction in Starobinsky inflation, which itself assumes nothing but gravity.","We consider the possibility that the reheating sector couples to gravity via a non-minimal coupling.","Our analysis is performed both in a perturbative and in a non-perturbative approach, where particle production is computed from Bogoliubov coefficients.","Our findings indicate that, for sufficiently large non-minimal coupling ($\\xi\\gtrsim~\\text{few}\\times 10$), non-perturbative particle production allows for temperatures of order $10^{12}$ GeV to be reached while also ensuring that the Universe becomes radiation-dominated.","This shows that the gravitational interaction could be the sole responsible for reheating the Universe after inflation, without the need to assume other ad hoc inflaton interactions."],"url":"http://arxiv.org/abs/2406.04161v1","category":"gr-qc"}
{"created":"2024-06-06 15:19:15","title":"MARLander: A Local Path Planning for Drone Swarms using Multiagent Deep Reinforcement Learning","abstract":"Achieving safe and precise landings for a swarm of drones poses a significant challenge, primarily attributed to conventional control and planning methods. This paper presents the implementation of multi-agent deep reinforcement learning (MADRL) techniques for the precise landing of a drone swarm at relocated target locations. The system is trained in a realistic simulated environment with a maximum velocity of 3 m/s in training spaces of 4 x 4 x 4 m and deployed utilizing Crazyflie drones with a Vicon indoor localization system. The experimental results revealed that the proposed approach achieved a landing accuracy of 2.26 cm on stationary and 3.93 cm on moving platforms surpassing a baseline method used with a Proportional-integral-derivative (PID) controller with an Artificial Potential Field (APF). This research highlights drone landing technologies that eliminate the need for analytical centralized systems, potentially offering scalability and revolutionizing applications in logistics, safety, and rescue missions.","sentences":["Achieving safe and precise landings for a swarm of drones poses a significant challenge, primarily attributed to conventional control and planning methods.","This paper presents the implementation of multi-agent deep reinforcement learning (MADRL) techniques for the precise landing of a drone swarm at relocated target locations.","The system is trained in a realistic simulated environment with a maximum velocity of 3 m/s in training spaces of 4 x 4 x 4 m and deployed utilizing Crazyflie drones with a Vicon indoor localization system.","The experimental results revealed that the proposed approach achieved a landing accuracy of 2.26 cm on stationary and 3.93 cm on moving platforms surpassing a baseline method used with a Proportional-integral-derivative (PID) controller with an Artificial Potential Field (APF).","This research highlights drone landing technologies that eliminate the need for analytical centralized systems, potentially offering scalability and revolutionizing applications in logistics, safety, and rescue missions."],"url":"http://arxiv.org/abs/2406.04159v1","category":"cs.RO"}
{"created":"2024-06-06 15:18:59","title":"Sparse Multi-baseline SAR Cross-modal 3D Reconstruction of Vehicle Targets","abstract":"Multi-baseline SAR 3D imaging faces significant challenges due to data sparsity. In recent years, deep learning techniques have achieved notable success in enhancing the quality of sparse SAR 3D imaging. However, previous work typically rely on full-aperture high-resolution radar images to supervise the training of deep neural networks (DNNs), utilizing only single-modal information from radar data. Consequently, imaging performance is limited, and acquiring full-aperture data for multi-baseline SAR is costly and sometimes impractical in real-world applications. In this paper, we propose a Cross-Modal Reconstruction Network (CMR-Net), which integrates differentiable render and cross-modal supervision with optical images to reconstruct highly sparse multi-baseline SAR 3D images of vehicle targets into visually structured and high-resolution images. We meticulously designed the network architecture and training strategies to enhance network generalization capability. Remarkably, CMR-Net, trained solely on simulated data, demonstrates high-resolution reconstruction capabilities on both publicly available simulation datasets and real measured datasets, outperforming traditional sparse reconstruction algorithms based on compressed sensing and other learning-based methods. Additionally, using optical images as supervision provides a cost-effective way to build training datasets, reducing the difficulty of method dissemination. Our work showcases the broad prospects of deep learning in multi-baseline SAR 3D imaging and offers a novel path for researching radar imaging based on cross-modal learning theory.","sentences":["Multi-baseline SAR 3D imaging faces significant challenges due to data sparsity.","In recent years, deep learning techniques have achieved notable success in enhancing the quality of sparse SAR 3D imaging.","However, previous work typically rely on full-aperture high-resolution radar images to supervise the training of deep neural networks (DNNs), utilizing only single-modal information from radar data.","Consequently, imaging performance is limited, and acquiring full-aperture data for multi-baseline SAR is costly and sometimes impractical in real-world applications.","In this paper, we propose a Cross-Modal Reconstruction Network (CMR-Net), which integrates differentiable render and cross-modal supervision with optical images to reconstruct highly sparse multi-baseline SAR 3D images of vehicle targets into visually structured and high-resolution images.","We meticulously designed the network architecture and training strategies to enhance network generalization capability.","Remarkably, CMR-Net, trained solely on simulated data, demonstrates high-resolution reconstruction capabilities on both publicly available simulation datasets and real measured datasets, outperforming traditional sparse reconstruction algorithms based on compressed sensing and other learning-based methods.","Additionally, using optical images as supervision provides a cost-effective way to build training datasets, reducing the difficulty of method dissemination.","Our work showcases the broad prospects of deep learning in multi-baseline SAR 3D imaging and offers a novel path for researching radar imaging based on cross-modal learning theory."],"url":"http://arxiv.org/abs/2406.04158v1","category":"cs.CV"}
{"created":"2024-06-06 15:18:25","title":"Circuit-level fault tolerance of cat codes","abstract":"Bosonic codes offer the possibility of storing quantum information in a single infinite-dimensional physical system endowed with the capability to correct errors, thereby reducing the number of physical components needed to protect against noise. Much of the current efforts in bosonic codes are on correcting only loss errors, while deferring the correction of phase errors -- perhaps actively suppressed -- to subsequent layers of encoding with standard qubit codes. Rotationally symmetric bosonic codes, which include the well-known cat and binomial codes, are capable of simultaneous correction of both loss and phase errors, offer an alternate route that deals with arbitrary errors already at the base layer. Grimsmo et al. [PRX 10, 011058 (2020)] analyzed the family of such codes and proposed general error-correction circuits to correct both loss and phase errors, reporting high noise thresholds in the presence of loss and phase errors on the input, while the error-correction circuits remain noiseless. A proper assessment, however, requires consideration of circuit-level noise, where the individual circuit components can themselves be faulty and introduce errors on the encoded information. Here, we carry out such a circuit-level analysis, and assess the performance of the error-correction circuits for the storage of information encoded with cat codes. While the circuits of Grimsmo et al.~are formally fault tolerant even under circuit-level noise, the thresholds are significantly worse. We show how, through waiting-time optimization and the use of squeezing, we can restore the noise requirements to ones plausibly achievable with near-term quantum hardware. Our circuit-level analysis also reveals important features of the error-correction circuits not visible in the earlier ideal-circuit perspective.","sentences":["Bosonic codes offer the possibility of storing quantum information in a single infinite-dimensional physical system endowed with the capability to correct errors, thereby reducing the number of physical components needed to protect against noise.","Much of the current efforts in bosonic codes are on correcting only loss errors, while deferring the correction of phase errors -- perhaps actively suppressed -- to subsequent layers of encoding with standard qubit codes.","Rotationally symmetric bosonic codes, which include the well-known cat and binomial codes, are capable of simultaneous correction of both loss and phase errors, offer an alternate route that deals with arbitrary errors already at the base layer.","Grimsmo et al.","[PRX 10, 011058 (2020)] analyzed the family of such codes and proposed general error-correction circuits to correct both loss and phase errors, reporting high noise thresholds in the presence of loss and phase errors on the input, while the error-correction circuits remain noiseless.","A proper assessment, however, requires consideration of circuit-level noise, where the individual circuit components can themselves be faulty and introduce errors on the encoded information.","Here, we carry out such a circuit-level analysis, and assess the performance of the error-correction circuits for the storage of information encoded with cat codes.","While the circuits of Grimsmo et al.~are formally fault tolerant even under circuit-level noise, the thresholds are significantly worse.","We show how, through waiting-time optimization and the use of squeezing, we can restore the noise requirements to ones plausibly achievable with near-term quantum hardware.","Our circuit-level analysis also reveals important features of the error-correction circuits not visible in the earlier ideal-circuit perspective."],"url":"http://arxiv.org/abs/2406.04157v1","category":"quant-ph"}
{"created":"2024-06-06 15:17:51","title":"Pointer-Guided Pre-Training: Infusing Large Language Models with Paragraph-Level Contextual Awareness","abstract":"We introduce \"pointer-guided segment ordering\" (SO), a novel pre-training technique aimed at enhancing the contextual understanding of paragraph-level text representations in large language models. Our methodology leverages a self-attention-driven pointer network to restore the original sequence of shuffled text segments, addressing the challenge of capturing the structural coherence and contextual dependencies within documents. This pre-training approach is complemented by a fine-tuning methodology that incorporates dynamic sampling, augmenting the diversity of training instances and improving sample efficiency for various downstream applications. We evaluate our method on a diverse set of datasets, demonstrating its efficacy in tasks requiring sequential text classification across scientific literature and financial reporting domains. Our experiments show that pointer-guided pre-training significantly enhances the model's ability to understand complex document structures, leading to state-of-the-art performance in downstream classification tasks.","sentences":["We introduce \"pointer-guided segment ordering\" (SO), a novel pre-training technique aimed at enhancing the contextual understanding of paragraph-level text representations in large language models.","Our methodology leverages a self-attention-driven pointer network to restore the original sequence of shuffled text segments, addressing the challenge of capturing the structural coherence and contextual dependencies within documents.","This pre-training approach is complemented by a fine-tuning methodology that incorporates dynamic sampling, augmenting the diversity of training instances and improving sample efficiency for various downstream applications.","We evaluate our method on a diverse set of datasets, demonstrating its efficacy in tasks requiring sequential text classification across scientific literature and financial reporting domains.","Our experiments show that pointer-guided pre-training significantly enhances the model's ability to understand complex document structures, leading to state-of-the-art performance in downstream classification tasks."],"url":"http://arxiv.org/abs/2406.04156v1","category":"cs.CL"}
{"created":"2024-06-06 15:17:33","title":"Improving Physics-Augmented Continuum Neural Radiance Field-Based Geometry-Agnostic System Identification with Lagrangian Particle Optimization","abstract":"Geometry-agnostic system identification is a technique for identifying the geometry and physical properties of an object from video sequences without any geometric assumptions. Recently, physics-augmented continuum neural radiance fields (PAC-NeRF) has demonstrated promising results for this technique by utilizing a hybrid Eulerian-Lagrangian representation, in which the geometry is represented by the Eulerian grid representations of NeRF, the physics is described by a material point method (MPM), and they are connected via Lagrangian particles. However, a notable limitation of PAC-NeRF is that its performance is sensitive to the learning of the geometry from the first frames owing to its two-step optimization. First, the grid representations are optimized with the first frames of video sequences, and then the physical properties are optimized through video sequences utilizing the fixed first-frame grid representations. This limitation can be critical when learning of the geometric structure is difficult, for example, in a few-shot (sparse view) setting. To overcome this limitation, we propose Lagrangian particle optimization (LPO), in which the positions and features of particles are optimized through video sequences in Lagrangian space. This method allows for the optimization of the geometric structure across the entire video sequence within the physical constraints imposed by the MPM. The experimental results demonstrate that the LPO is useful for geometric correction and physical identification in sparse-view settings.","sentences":["Geometry-agnostic system identification is a technique for identifying the geometry and physical properties of an object from video sequences without any geometric assumptions.","Recently, physics-augmented continuum neural radiance fields (PAC-NeRF) has demonstrated promising results for this technique by utilizing a hybrid Eulerian-Lagrangian representation, in which the geometry is represented by the Eulerian grid representations of NeRF, the physics is described by a material point method (MPM), and they are connected via Lagrangian particles.","However, a notable limitation of PAC-NeRF is that its performance is sensitive to the learning of the geometry from the first frames owing to its two-step optimization.","First, the grid representations are optimized with the first frames of video sequences, and then the physical properties are optimized through video sequences utilizing the fixed first-frame grid representations.","This limitation can be critical when learning of the geometric structure is difficult, for example, in a few-shot (sparse view) setting.","To overcome this limitation, we propose Lagrangian particle optimization (LPO), in which the positions and features of particles are optimized through video sequences in Lagrangian space.","This method allows for the optimization of the geometric structure across the entire video sequence within the physical constraints imposed by the MPM.","The experimental results demonstrate that the LPO is useful for geometric correction and physical identification in sparse-view settings."],"url":"http://arxiv.org/abs/2406.04155v1","category":"cs.CV"}
{"created":"2024-06-06 15:17:16","title":"Two Erd\u0151s-Hajnal-type theorems for forbidden order-size pairs","abstract":"The celebrated Erd\\H{o}s-Hajnal conjecture says that any graph without a fixed induced subgraph $H$ contains a very large homogeneous set. A direct analog of this conjecture is not true for hypergraphs. In this paper we present two natural variants of this problem which do hold for hypergraphs. We show that for every $r \\geq 3$, $m \\geq m_0(r)$ and $0 \\leq f \\leq \\binom{m}{r}$, if an $r$-graph $G$ does not contain $m$ vertices spanning exactly $f$ edges, then $G$ contains much bigger homogeneous sets than what is guaranteed to exist in general $r$-graphs. We also prove that if a $3$-graph $G$ does not contain homogeneous sets of polynomial size, then for every $m \\geq 3$ there are $\\Omega(m^3)$ values of $f$ such that $G$ contains $m$ vertices spanning exactly $f$ edges. This makes progress on a conjecture of Axenovich, Brada\\v{c}, Gishboliner, Mubayi and Weber.","sentences":["The celebrated Erd\\H{o}s-Hajnal conjecture says that any graph without a fixed induced subgraph $H$ contains a very large homogeneous set.","A direct analog of this conjecture is not true for hypergraphs.","In this paper we present two natural variants of this problem which do hold for hypergraphs.","We show that for every $r \\geq 3$, $m \\geq m_0(r)$ and $0","\\leq f \\leq \\binom{m}{r}$, if an $r$-graph $G$ does not contain $m$ vertices spanning exactly $f$ edges, then $G$ contains much bigger homogeneous sets than what is guaranteed to exist in general $r$-graphs.","We also prove that if a $3$-graph $G$ does not contain homogeneous sets of polynomial size, then for every $m \\geq","3$ there are $\\Omega(m^3)$ values of $f$ such that $G$ contains $m$ vertices spanning exactly $f$ edges.","This makes progress on a conjecture of Axenovich, Brada\\v{c}, Gishboliner, Mubayi and Weber."],"url":"http://arxiv.org/abs/2406.04154v1","category":"math.CO"}
{"created":"2024-06-06 15:15:41","title":"AgentGym: Evolving Large Language Model-based Agents across Diverse Environments","abstract":"Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community. Large language models (LLMs) are considered a promising foundation to build such agents due to their generalized capabilities. Current approaches either have LLM-based agents imitate expert-provided trajectories step-by-step, requiring human supervision, which is hard to scale and limits environmental exploration; or they let agents explore and learn in isolated environments, resulting in specialist agents with limited generalization. In this paper, we take the first step towards building generally-capable LLM-based agents with self-evolution ability. We identify a trinity of ingredients: 1) diverse environments for agent exploration and learning, 2) a trajectory set to equip agents with basic capabilities and prior knowledge, and 3) an effective and scalable evolution method. We propose AgentGym, a new framework featuring a variety of environments and tasks for broad, real-time, uni-format, and concurrent agent exploration. AgentGym also includes a database with expanded instructions, a benchmark suite, and high-quality trajectories across environments. Next, we propose a novel method, AgentEvol, to investigate the potential of agent self-evolution beyond previously seen data across tasks and environments. Experimental results show that the evolved agents can achieve results comparable to SOTA models. We release the AgentGym suite, including the platform, dataset, benchmark, checkpoints, and algorithm implementations. The AgentGym suite is available on https://github.com/WooooDyy/AgentGym.","sentences":["Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community.","Large language models (LLMs) are considered a promising foundation to build such agents due to their generalized capabilities.","Current approaches either have LLM-based agents imitate expert-provided trajectories step-by-step, requiring human supervision, which is hard to scale and limits environmental exploration; or they let agents explore and learn in isolated environments, resulting in specialist agents with limited generalization.","In this paper, we take the first step towards building generally-capable LLM-based agents with self-evolution ability.","We identify a trinity of ingredients: 1) diverse environments for agent exploration and learning, 2) a trajectory set to equip agents with basic capabilities and prior knowledge, and 3) an effective and scalable evolution method.","We propose AgentGym, a new framework featuring a variety of environments and tasks for broad, real-time, uni-format, and concurrent agent exploration.","AgentGym also includes a database with expanded instructions, a benchmark suite, and high-quality trajectories across environments.","Next, we propose a novel method, AgentEvol, to investigate the potential of agent self-evolution beyond previously seen data across tasks and environments.","Experimental results show that the evolved agents can achieve results comparable to SOTA models.","We release the AgentGym suite, including the platform, dataset, benchmark, checkpoints, and algorithm implementations.","The AgentGym suite is available on https://github.com/WooooDyy/AgentGym."],"url":"http://arxiv.org/abs/2406.04151v1","category":"cs.AI"}
{"created":"2024-06-06 15:13:56","title":"Characterizing segregation in blast rock piles a deep-learning approach leveraging aerial image analysis","abstract":"Blasted rock material serves a critical role in various engineering applications, yet the phenomenon of segregation-where particle sizes vary significantly along the gradient of a quarry pile-presents challenges for optimizing quarry material storage and handling. This study introduces an advanced image analysis methodology to characterize such segregation of rock fragments. The accurate delineation of detailed rock fragment size distributions was achieved through the analysis of drone-captured imagery, coupled with the application of an enhanced Unet semantic segmentation model integrated with an expansion-based post-processing technique. The quarry slope was stratified into four vertical sections, with the size distribution of each section quantified via ellipsoid shape approximations. Our results disclose pronounced vertical segregation patterns, with finer particles concentrated in the upper slope regions and coarser particles in the lower. Utilizing relative characteristic diameters, we offered insight into the degree of segregation, thereby illustrating the spatial heterogeneity in fragment size more clearly. The techniques outlined in this study deliver a scalable and accurate method for assessing fragment size distribution, with the potential to better inform resource management and operational decisions in quarry management.","sentences":["Blasted rock material serves a critical role in various engineering applications, yet the phenomenon of segregation-where particle sizes vary significantly along the gradient of a quarry pile-presents challenges for optimizing quarry material storage and handling.","This study introduces an advanced image analysis methodology to characterize such segregation of rock fragments.","The accurate delineation of detailed rock fragment size distributions was achieved through the analysis of drone-captured imagery, coupled with the application of an enhanced Unet semantic segmentation model integrated with an expansion-based post-processing technique.","The quarry slope was stratified into four vertical sections, with the size distribution of each section quantified via ellipsoid shape approximations.","Our results disclose pronounced vertical segregation patterns, with finer particles concentrated in the upper slope regions and coarser particles in the lower.","Utilizing relative characteristic diameters, we offered insight into the degree of segregation, thereby illustrating the spatial heterogeneity in fragment size more clearly.","The techniques outlined in this study deliver a scalable and accurate method for assessing fragment size distribution, with the potential to better inform resource management and operational decisions in quarry management."],"url":"http://arxiv.org/abs/2406.04149v1","category":"eess.IV"}
{"created":"2024-06-06 15:10:27","title":"Every Answer Matters: Evaluating Commonsense with Probabilistic Measures","abstract":"Large language models have demonstrated impressive performance on commonsense tasks; however, these tasks are often posed as multiple-choice questions, allowing models to exploit systematic biases. Commonsense is also inherently probabilistic with multiple correct answers. The purpose of \"boiling water\" could be making tea and cooking, but it also could be killing germs. Existing tasks do not capture the probabilistic nature of common sense. To this end, we present commonsense frame completion (CFC), a new generative task that evaluates common sense via multiple open-ended generations. We also propose a method of probabilistic evaluation that strongly correlates with human judgments. Humans drastically outperform strong language model baselines on our dataset, indicating this approach is both a challenging and useful evaluation of machine common sense.","sentences":["Large language models have demonstrated impressive performance on commonsense tasks; however, these tasks are often posed as multiple-choice questions, allowing models to exploit systematic biases.","Commonsense is also inherently probabilistic with multiple correct answers.","The purpose of \"boiling water\" could be making tea and cooking, but it also could be killing germs.","Existing tasks do not capture the probabilistic nature of common sense.","To this end, we present commonsense frame completion (CFC), a new generative task that evaluates common sense via multiple open-ended generations.","We also propose a method of probabilistic evaluation that strongly correlates with human judgments.","Humans drastically outperform strong language model baselines on our dataset, indicating this approach is both a challenging and useful evaluation of machine common sense."],"url":"http://arxiv.org/abs/2406.04145v1","category":"cs.CL"}
{"created":"2024-06-06 15:08:41","title":"Redundancy-aware Action Spaces for Robot Learning","abstract":"Joint space and task space control are the two dominant action modes for controlling robot arms within the robot learning literature. Actions in joint space provide precise control over the robot's pose, but tend to suffer from inefficient training; actions in task space boast data-efficient training but sacrifice the ability to perform tasks in confined spaces due to limited control over the full joint configuration. This work analyses the criteria for designing action spaces for robot manipulation and introduces ER (End-effector Redundancy), a novel action space formulation that, by addressing the redundancies present in the manipulator, aims to combine the advantages of both joint and task spaces, offering fine-grained comprehensive control with overactuated robot arms whilst achieving highly efficient robot learning. We present two implementations of ER, ERAngle (ERA) and ERJoint (ERJ), and we show that ERJ in particular demonstrates superior performance across multiple settings, especially when precise control over the robot configuration is required. We validate our results both in simulated and real robotic environments.","sentences":["Joint space and task space control are the two dominant action modes for controlling robot arms within the robot learning literature.","Actions in joint space provide precise control over the robot's pose, but tend to suffer from inefficient training; actions in task space boast data-efficient training but sacrifice the ability to perform tasks in confined spaces due to limited control over the full joint configuration.","This work analyses the criteria for designing action spaces for robot manipulation and introduces ER (End-effector Redundancy), a novel action space formulation that, by addressing the redundancies present in the manipulator, aims to combine the advantages of both joint and task spaces, offering fine-grained comprehensive control with overactuated robot arms whilst achieving highly efficient robot learning.","We present two implementations of ER, ERAngle (ERA) and ERJoint (ERJ), and we show that ERJ in particular demonstrates superior performance across multiple settings, especially when precise control over the robot configuration is required.","We validate our results both in simulated and real robotic environments."],"url":"http://arxiv.org/abs/2406.04144v1","category":"cs.RO"}
{"created":"2024-06-06 15:08:16","title":"Do Language Models Understand Morality? Towards a Robust Detection of Moral Content","abstract":"The task of detecting moral values in text has significant implications in various fields, including natural language processing, social sciences, and ethical decision-making. Previously proposed supervised models often suffer from overfitting, leading to hyper-specialized moral classifiers that struggle to perform well on data from different domains. To address this issue, we introduce novel systems that leverage abstract concepts and common-sense knowledge acquired from Large Language Models and Natural Language Inference models during previous stages of training on multiple data sources. By doing so, we aim to develop versatile and robust methods for detecting moral values in real-world scenarios. Our approach uses the GPT 3.5 model as a zero-shot ready-made unsupervised multi-label classifier for moral values detection, eliminating the need for explicit training on labeled data. We compare it with a smaller NLI-based zero-shot model. The results show that the NLI approach achieves competitive results compared to the Davinci model. Furthermore, we conduct an in-depth investigation of the performance of supervised systems in the context of cross-domain multi-label moral value detection. This involves training supervised models on different domains to explore their effectiveness in handling data from different sources and comparing their performance with the unsupervised methods. Our contributions encompass a thorough analysis of both supervised and unsupervised methodologies for cross-domain value detection. We introduce the Davinci model as a state-of-the-art zero-shot unsupervised moral values classifier, pushing the boundaries of moral value detection without the need for explicit training on labeled data. Additionally, we perform a comparative evaluation of our approach with the supervised models, shedding light on their respective strengths and weaknesses.","sentences":["The task of detecting moral values in text has significant implications in various fields, including natural language processing, social sciences, and ethical decision-making.","Previously proposed supervised models often suffer from overfitting, leading to hyper-specialized moral classifiers that struggle to perform well on data from different domains.","To address this issue, we introduce novel systems that leverage abstract concepts and common-sense knowledge acquired from Large Language Models and Natural Language Inference models during previous stages of training on multiple data sources.","By doing so, we aim to develop versatile and robust methods for detecting moral values in real-world scenarios.","Our approach uses the GPT 3.5 model as a zero-shot ready-made unsupervised multi-label classifier for moral values detection, eliminating the need for explicit training on labeled data.","We compare it with a smaller NLI-based zero-shot model.","The results show that the NLI approach achieves competitive results compared to the Davinci model.","Furthermore, we conduct an in-depth investigation of the performance of supervised systems in the context of cross-domain multi-label moral value detection.","This involves training supervised models on different domains to explore their effectiveness in handling data from different sources and comparing their performance with the unsupervised methods.","Our contributions encompass a thorough analysis of both supervised and unsupervised methodologies for cross-domain value detection.","We introduce the Davinci model as a state-of-the-art zero-shot unsupervised moral values classifier, pushing the boundaries of moral value detection without the need for explicit training on labeled data.","Additionally, we perform a comparative evaluation of our approach with the supervised models, shedding light on their respective strengths and weaknesses."],"url":"http://arxiv.org/abs/2406.04143v1","category":"cs.CL"}
{"created":"2024-06-06 15:03:03","title":"On semiample vector bundles and parallelizable compact complex manifolds","abstract":"We provide a characterization of parallelizable compact complex manifolds and their quotients using holomorphic symmetric differentials. In particular we show that compact complex manifolds of Kodaira dimension 0 having strongly semiample cotangent bundle are parallelizable manifolds, while compact complex manifolds of Kodaira dimension 0 having weakly semiample cotangent bundle are quotients of parallelizable manifolds. The main constructions used involve considerations about semiampleness of vector bundles, which are themselves of interest. As a byproduct we prove that compact manifolds having Kodaira dimension 0 and weakly sermiample cotangent bundle have infinite fundamental group, and we conjecture that this should be the case for all compact complex manifolds not of general type with weakly semiample cotangent bundle.","sentences":["We provide a characterization of parallelizable compact complex manifolds and their quotients using holomorphic symmetric differentials.","In particular we show that compact complex manifolds of Kodaira dimension 0 having strongly semiample cotangent bundle are parallelizable manifolds, while compact complex manifolds of Kodaira dimension 0 having weakly semiample cotangent bundle are quotients of parallelizable manifolds.","The main constructions used involve considerations about semiampleness of vector bundles, which are themselves of interest.","As a byproduct we prove that compact manifolds having Kodaira dimension 0 and weakly sermiample cotangent bundle have infinite fundamental group, and we conjecture that this should be the case for all compact complex manifolds not of general type with weakly semiample cotangent bundle."],"url":"http://arxiv.org/abs/2406.04139v1","category":"math.AG"}
{"created":"2024-06-06 14:59:39","title":"The 3D-PC: a benchmark for visual perspective taking in humans and machines","abstract":"Visual perspective taking (VPT) is the ability to perceive and reason about the perspectives of others. It is an essential feature of human intelligence, which develops over the first decade of life and requires an ability to process the 3D structure of visual scenes. A growing number of reports have indicated that deep neural networks (DNNs) become capable of analyzing 3D scenes after training on large image datasets. We investigated if this emergent ability for 3D analysis in DNNs is sufficient for VPT with the 3D perception challenge (3D-PC): a novel benchmark for 3D perception in humans and DNNs. The 3D-PC is comprised of three 3D-analysis tasks posed within natural scene images: 1. a simple test of object depth order, 2. a basic VPT task (VPT-basic), and 3. another version of VPT (VPT-Strategy) designed to limit the effectiveness of \"shortcut\" visual strategies. We tested human participants (N=33) and linearly probed or text-prompted over 300 DNNs on the challenge and found that nearly all of the DNNs approached or exceeded human accuracy in analyzing object depth order. Surprisingly, DNN accuracy on this task correlated with their object recognition performance. In contrast, there was an extraordinary gap between DNNs and humans on VPT-basic. Humans were nearly perfect, whereas most DNNs were near chance. Fine-tuning DNNs on VPT-basic brought them close to human performance, but they, unlike humans, dropped back to chance when tested on VPT-perturb. Our challenge demonstrates that the training routines and architectures of today's DNNs are well-suited for learning basic 3D properties of scenes and objects but are ill-suited for reasoning about these properties like humans do. We release our 3D-PC datasets and code to help bridge this gap in 3D perception between humans and machines.","sentences":["Visual perspective taking (VPT) is the ability to perceive and reason about the perspectives of others.","It is an essential feature of human intelligence, which develops over the first decade of life and requires an ability to process the 3D structure of visual scenes.","A growing number of reports have indicated that deep neural networks (DNNs) become capable of analyzing 3D scenes after training on large image datasets.","We investigated if this emergent ability for 3D analysis in DNNs is sufficient for VPT with the 3D perception challenge (3D-PC): a novel benchmark for 3D perception in humans and DNNs.","The 3D-PC is comprised of three 3D-analysis tasks posed within natural scene images: 1. a simple test of object depth order, 2. a basic VPT task (VPT-basic), and 3.","another version of VPT (VPT-Strategy) designed to limit the effectiveness of \"shortcut\" visual strategies.","We tested human participants (N=33) and linearly probed or text-prompted over 300 DNNs on the challenge and found that nearly all of the DNNs approached or exceeded human accuracy in analyzing object depth order.","Surprisingly, DNN accuracy on this task correlated with their object recognition performance.","In contrast, there was an extraordinary gap between DNNs and humans on VPT-basic.","Humans were nearly perfect, whereas most DNNs were near chance.","Fine-tuning DNNs on VPT-basic brought them close to human performance, but they, unlike humans, dropped back to chance when tested on VPT-perturb.","Our challenge demonstrates that the training routines and architectures of today's DNNs are well-suited for learning basic 3D properties of scenes and objects but are ill-suited for reasoning about these properties like humans do.","We release our 3D-PC datasets and code to help bridge this gap in 3D perception between humans and machines."],"url":"http://arxiv.org/abs/2406.04138v1","category":"cs.CV"}
{"created":"2024-06-06 14:57:52","title":"Optimal Batched Linear Bandits","abstract":"We introduce the E$^4$ algorithm for the batched linear bandit problem, incorporating an Explore-Estimate-Eliminate-Exploit framework. With a proper choice of exploration rate, we prove E$^4$ achieves the finite-time minimax optimal regret with only $O(\\log\\log T)$ batches, and the asymptotically optimal regret with only $3$ batches as $T\\rightarrow\\infty$, where $T$ is the time horizon. We further prove a lower bound on the batch complexity of linear contextual bandits showing that any asymptotically optimal algorithm must require at least $3$ batches in expectation as $T\\rightarrow\\infty$, which indicates E$^4$ achieves the asymptotic optimality in regret and batch complexity simultaneously. To the best of our knowledge, E$^4$ is the first algorithm for linear bandits that simultaneously achieves the minimax and asymptotic optimality in regret with the corresponding optimal batch complexities. In addition, we show that with another choice of exploration rate E$^4$ achieves an instance-dependent regret bound requiring at most $O(\\log T)$ batches, and maintains the minimax optimality and asymptotic optimality. We conduct thorough experiments to evaluate our algorithm on randomly generated instances and the challenging \\textit{End of Optimism} instances \\citep{lattimore2017end} which were shown to be hard to learn for optimism based algorithms. Empirical results show that E$^4$ consistently outperforms baseline algorithms with respect to regret minimization, batch complexity, and computational efficiency.","sentences":["We introduce the E$^4$ algorithm for the batched linear bandit problem, incorporating an Explore-Estimate-Eliminate-Exploit framework.","With a proper choice of exploration rate, we prove E$^4$ achieves the finite-time minimax optimal regret with only $O(\\log\\log T)$ batches, and the asymptotically optimal regret with only $3$ batches as $T\\rightarrow\\infty$, where $T$ is the time horizon.","We further prove a lower bound on the batch complexity of linear contextual bandits showing that any asymptotically optimal algorithm must require at least $3$ batches in expectation as $T\\rightarrow\\infty$, which indicates E$^4$ achieves the asymptotic optimality in regret and batch complexity simultaneously.","To the best of our knowledge, E$^4$ is the first algorithm for linear bandits that simultaneously achieves the minimax and asymptotic optimality in regret with the corresponding optimal batch complexities.","In addition, we show that with another choice of exploration rate E$^4$ achieves an instance-dependent regret bound requiring at most $O(\\log T)$ batches, and maintains the minimax optimality and asymptotic optimality.","We conduct thorough experiments to evaluate our algorithm on randomly generated instances and the challenging \\textit{End of Optimism} instances \\citep{lattimore2017end} which were shown to be hard to learn for optimism based algorithms.","Empirical results show that E$^4$ consistently outperforms baseline algorithms with respect to regret minimization, batch complexity, and computational efficiency."],"url":"http://arxiv.org/abs/2406.04137v1","category":"cs.LG"}
{"created":"2024-06-06 14:57:48","title":"Legal Judgment Reimagined: PredEx and the Rise of Intelligent AI Interpretation in Indian Courts","abstract":"In the era of Large Language Models (LLMs), predicting judicial outcomes poses significant challenges due to the complexity of legal proceedings and the scarcity of expert-annotated datasets. Addressing this, we introduce \\textbf{Pred}iction with \\textbf{Ex}planation (\\texttt{PredEx}), the largest expert-annotated dataset for legal judgment prediction and explanation in the Indian context, featuring over 15,000 annotations. This groundbreaking corpus significantly enhances the training and evaluation of AI models in legal analysis, with innovations including the application of instruction tuning to LLMs. This method has markedly improved the predictive accuracy and explanatory depth of these models for legal judgments. We employed various transformer-based models, tailored for both general and Indian legal contexts. Through rigorous lexical, semantic, and expert assessments, our models effectively leverage \\texttt{PredEx} to provide precise predictions and meaningful explanations, establishing it as a valuable benchmark for both the legal profession and the NLP community.","sentences":["In the era of Large Language Models (LLMs), predicting judicial outcomes poses significant challenges due to the complexity of legal proceedings and the scarcity of expert-annotated datasets.","Addressing this, we introduce \\textbf{Pred}iction with \\textbf{Ex}planation (\\texttt{PredEx}), the largest expert-annotated dataset for legal judgment prediction and explanation in the Indian context, featuring over 15,000 annotations.","This groundbreaking corpus significantly enhances the training and evaluation of AI models in legal analysis, with innovations including the application of instruction tuning to LLMs.","This method has markedly improved the predictive accuracy and explanatory depth of these models for legal judgments.","We employed various transformer-based models, tailored for both general and Indian legal contexts.","Through rigorous lexical, semantic, and expert assessments, our models effectively leverage \\texttt{PredEx} to provide precise predictions and meaningful explanations, establishing it as a valuable benchmark for both the legal profession and the NLP community."],"url":"http://arxiv.org/abs/2406.04136v1","category":"cs.CL"}
{"created":"2024-06-06 14:57:44","title":"Beyond the Rotational Deathline: Radio Emission from Ultra-long Period Magnetars","abstract":"Motivated by the recent detection of ultra-long period radio transients, we investigate new models of coherent radio emission via low-altitude electron-positron pair production in neutron stars beyond rotationally-powered curvature radiation deathlines. We find that plastic motion (akin to 'continental drift') and qualitatively similar thermoelectric action by temperature gradients in the crusts of slowly rotating, highly magnetized neutron stars could impart mild local magnetospheric twists. Regardless of which mechanism drives twists, we find that particle acceleration initiates pair cascades across charge-starved gaps above a mild critical twist. Cascades are initiated via resonant inverse-Compton scattered photons or curvature radiation, and may produce broadband coherent radio emission. We compute the pair luminosity (maximum allowed radio luminosity) for these two channels, and derive deathlines and 'active zones' in $P-\\dot{P}$ space from a variety of considerations. We find these twist-initiated pair cascades only occur for magnetar-like field strengths $B \\gtrsim 10^{14}$ G and long periods: $P_{\\rm RICS} \\gtrsim 120 \\; (T/10^{6.5} {\\rm K})^{-5} \\, {\\rm sec}$ and $P_{\\rm curv} \\gtrsim 150 \\; ({\\rm v_{\\rm pl}}/10^{3} {\\, \\rm cm \\, yr^{-1}})^{-7/6} \\, {\\rm sec}$. Using a simplified geometric model, we find that plastic motion or thermoelectrically driven twists might naturally reproduce the observed luminosities, timescales, and timing signatures. We further derive 'active zones' in which rotationally-powered pair creation occurs via resonantly scattered photons, beyond standard curvature deathlines for pulsars. All cascades are generically accompanied by simultaneous (non-)thermal X-ray/UV counterparts which might be detectable with current instrumentation.","sentences":["Motivated by the recent detection of ultra-long period radio transients, we investigate new models of coherent radio emission via low-altitude electron-positron pair production in neutron stars beyond rotationally-powered curvature radiation deathlines.","We find that plastic motion (akin to 'continental drift') and qualitatively similar thermoelectric action by temperature gradients in the crusts of slowly rotating, highly magnetized neutron stars could impart mild local magnetospheric twists.","Regardless of which mechanism drives twists, we find that particle acceleration initiates pair cascades across charge-starved gaps above a mild critical twist.","Cascades are initiated via resonant inverse-Compton scattered photons or curvature radiation, and may produce broadband coherent radio emission.","We compute the pair luminosity (maximum allowed radio luminosity) for these two channels, and derive deathlines and 'active zones' in $P-\\dot{P}$ space from a variety of considerations.","We find these twist-initiated pair cascades only occur for magnetar-like field strengths $B \\gtrsim 10^{14}$ G and long periods: $P_{\\rm RICS} \\gtrsim 120 \\; (T/10^{6.5} {\\rm K})^{-5} \\, {\\rm sec}$ and $P_{\\rm curv} \\gtrsim 150 \\; ({\\rm v_{\\rm pl}}/10^{3} {\\, \\rm cm \\, yr^{-1}})^{-7/6} \\, {\\rm sec}$. Using a simplified geometric model, we find that plastic motion or thermoelectrically driven twists might naturally reproduce the observed luminosities, timescales, and timing signatures.","We further derive 'active zones' in which rotationally-powered pair creation occurs via resonantly scattered photons, beyond standard curvature deathlines for pulsars.","All cascades are generically accompanied by simultaneous (non-)thermal X-ray/UV counterparts which might be detectable with current instrumentation."],"url":"http://arxiv.org/abs/2406.04135v1","category":"astro-ph.HE"}
{"created":"2024-06-06 14:51:39","title":"Realizability of Subgroups by Subshifts of Finite Type","abstract":"We study the problem of realizing families of subgroups as the set of stabilizers of configurations from a subshift of finite type (SFT). This problem generalizes both the existence of strongly and weakly aperiodic SFTs. We show that a finitely generated normal subgroup is realizable if and only if the quotient by the subgroup admits a strongly aperiodic SFT. We also show that if a subgroup is realizable, its subgroup membership problem must be decidable. The article also contains the introduction of periodically rigid groups, which are groups for which every weakly aperiodic subshift of finite type is strongly aperiodic. We conjecture that the only finitely generated periodically rigid groups are virtually $\\mathbb{Z}$ groups and torsion-free virtually $\\mathbb{Z}^2$ groups. Finally, we show virtually nilpotent and polycyclic groups satisfy the conjecture.","sentences":["We study the problem of realizing families of subgroups as the set of stabilizers of configurations from a subshift of finite type (SFT).","This problem generalizes both the existence of strongly and weakly aperiodic SFTs.","We show that a finitely generated normal subgroup is realizable if and only if the quotient by the subgroup admits a strongly aperiodic SFT.","We also show that if a subgroup is realizable, its subgroup membership problem must be decidable.","The article also contains the introduction of periodically rigid groups, which are groups for which every weakly aperiodic subshift of finite type is strongly aperiodic.","We conjecture that the only finitely generated periodically rigid groups are virtually $\\mathbb{Z}$ groups and torsion-free virtually $\\mathbb{Z}^2$ groups.","Finally, we show virtually nilpotent and polycyclic groups satisfy the conjecture."],"url":"http://arxiv.org/abs/2406.04132v1","category":"math.DS"}
{"created":"2024-06-06 14:49:06","title":"Are We Done with MMLU?","abstract":"Maybe not. We identify and analyse errors in the popular Massive Multitask Language Understanding (MMLU) benchmark. Even though MMLU is widely adopted, our analysis demonstrates numerous ground truth errors that obscure the true capabilities of LLMs. For example, we find that 57% of the analysed questions in the Virology subset contain errors. To address this issue, we introduce a comprehensive framework for identifying dataset errors using a novel error taxonomy. Then, we create MMLU-Redux, which is a subset of 3,000 manually re-annotated questions across 30 MMLU subjects. Using MMLU-Redux, we demonstrate significant discrepancies with the model performance metrics that were originally reported. Our results strongly advocate for revising MMLU's error-ridden questions to enhance its future utility and reliability as a benchmark. Therefore, we open up MMLU-Redux for additional annotation https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux.","sentences":["Maybe not.","We identify and analyse errors in the popular Massive Multitask Language Understanding (MMLU) benchmark.","Even though MMLU is widely adopted, our analysis demonstrates numerous ground truth errors that obscure the true capabilities of LLMs.","For example, we find that 57% of the analysed questions in the Virology subset contain errors.","To address this issue, we introduce a comprehensive framework for identifying dataset errors using a novel error taxonomy.","Then, we create MMLU-Redux, which is a subset of 3,000 manually re-annotated questions across 30 MMLU subjects.","Using MMLU-Redux, we demonstrate significant discrepancies with the model performance metrics that were originally reported.","Our results strongly advocate for revising MMLU's error-ridden questions to enhance its future utility and reliability as a benchmark.","Therefore, we open up MMLU-Redux for additional annotation https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux."],"url":"http://arxiv.org/abs/2406.04127v1","category":"cs.CL"}
{"created":"2024-06-06 14:48:14","title":"A characterization of $(\u03bc,\u03bd)$-dichotomies via admissibility","abstract":"We present a characterization of $(\\mu,\\nu)$-dichotomies in terms of the admissibility of certain pairs of weighted spaces for nonautonomous discrete time dynamics acting on Banach spaces. Our general framework enables us to treat various settings in which no similar result has been previously obtained as well as to recover and refine several known results. We emphasize that our results hold without any bounded growth assumption and the statements make no use of Lyapunov norms. Moreover, as a consequence of our characterization, we study the robustness of $(\\mu, \\nu)$-dichotomies, i.e. we show that this notion persists under small but very general linear perturbations.","sentences":["We present a characterization of $(\\mu,\\nu)$-dichotomies in terms of the admissibility of certain pairs of weighted spaces for nonautonomous discrete time dynamics acting on Banach spaces.","Our general framework enables us to treat various settings in which no similar result has been previously obtained as well as to recover and refine several known results.","We emphasize that our results hold without any bounded growth assumption and the statements make no use of Lyapunov norms.","Moreover, as a consequence of our characterization, we study the robustness of $(\\mu, \\nu)$-dichotomies, i.e. we show that this notion persists under small but very general linear perturbations."],"url":"http://arxiv.org/abs/2406.04126v1","category":"math.DS"}
{"created":"2024-06-06 14:47:39","title":"Helsinki Speech Challenge 2024","abstract":"The Helsinki Speech Challenge 2024 (HSC2024) invites researchers to enhance and deconvolve speech audio recordings. We recorded a dataset that challenges participants to apply speech enhancement and inverse problems techniques to recorded speech data. This dataset includes paired samples of AI-generated clean speech and corresponding recordings, which feature varying levels of corruption, including frequency attenuation and reverberation. The challenge focuses on developing innovative deconvolution methods to accurately recover the original audio. The effectiveness of these methods will be quantitatively assessed using a speech recognition model, providing a relevant metric for evaluating enhancements in real-world scenarios.","sentences":["The Helsinki Speech Challenge 2024 (HSC2024) invites researchers to enhance and deconvolve speech audio recordings.","We recorded a dataset that challenges participants to apply speech enhancement and inverse problems techniques to recorded speech data.","This dataset includes paired samples of AI-generated clean speech and corresponding recordings, which feature varying levels of corruption, including frequency attenuation and reverberation.","The challenge focuses on developing innovative deconvolution methods to accurately recover the original audio.","The effectiveness of these methods will be quantitatively assessed using a speech recognition model, providing a relevant metric for evaluating enhancements in real-world scenarios."],"url":"http://arxiv.org/abs/2406.04123v1","category":"eess.AS"}
{"created":"2024-06-06 14:42:47","title":"On the Tensor Property of Bernstein-Sato Polynomial","abstract":"M. Popa proposed a question whether $b_{f\\cdot g}(s) = b_f(s)b_g(s)$ holds for $f\\in \\mathbb C[x_1,...,x_n]$ and $g\\in \\mathbb C[y_1,...,y_m]$, where $b_{f}$, $b_g$, and $b_{f\\cdot g}$ are corresponding Bernstein-Sato polynomials. It is a subtle problem that seems clear but turns out highly non-trivial. In this paper, we give a positive answer for Popa's question and generalize the result to arbitrary non-singular complex varieties $X$ and $Y$, and the tensor of effective divisors $D_1$ and $D_2$ on them respectively, and prove a generalized theorem. The theorem also leads to a multiplicative result of Igusa's strong monodromy conjecture. Moreover, we propose two analogous conjectures about Bernstein-Sato polynomials for ideals and prove them for monomial ideals.","sentences":["M. Popa proposed a question whether $b_{f\\cdot g}(s) = b_f(s)b_g(s)$ holds for $f\\in \\mathbb C[x_1,...,x_n]$ and $g\\in \\mathbb C[y_1,...,y_m]$, where $b_{f}$, $b_g$, and $b_{f\\cdot g}$ are corresponding Bernstein-Sato polynomials.","It is a subtle problem that seems clear but turns out highly non-trivial.","In this paper, we give a positive answer for Popa's question and generalize the result to arbitrary non-singular complex varieties $X$ and $Y$, and the tensor of effective divisors $D_1$ and $D_2$ on them respectively, and prove a generalized theorem.","The theorem also leads to a multiplicative result of Igusa's strong monodromy conjecture.","Moreover, we propose two analogous conjectures about Bernstein-Sato polynomials for ideals and prove them for monomial ideals."],"url":"http://arxiv.org/abs/2406.04121v1","category":"math.AG"}
{"created":"2024-06-06 14:41:36","title":"Microscale physics and macroscale convective heat transfer in supercritical fluids","abstract":"Driven by fundamental thermodynamic efficiency considerations, an emerging trend in the energy and propulsion systems is that the working fluid operates at a pressure above the critical pressure. Energy transport is thus accompanied by dramatic and strongly nonlinear variations of thermophysical properties, which cause abnormal heat transfer behavior and non-ideal gas effects. This situation raises a crucial challenge for the heat exchanger and turbomachinery design, overall energy and exergy efficiency. We aim to provide a multi-scale overview of the flow and thermal behavior of fluid above the critical point: microscopic physics and macroscopic transport. Microscopic physics, i.e. near-critical thermophysical properties, phase transition and fluid dynamics, are introduced. A particular focus will be on the supercritical pseudo boiling process, which is a generalization of classical liquid-vapor phase transitions to non-equilibrium supercritical states. These new perspectives lead to a revised view of the state space. Further, recent results demonstrated the possibility of stable supercritical fluid interfaces without surface tension. On the macroscale, recent progress on the modeling of turbulent flow and convective heat transfer of supercritical fluids are summarized. Direct numerical simulation is able to offer insights into the physics of thermal fluids. We start with a description of fundamental fluid mechanics problems related to supercritical fluids. In addition, the heat transfer deterioration in supercritical fluids is found to be closely connected to the flow relaminarization by the non-uniform body-force. Finally, various modeling approaches such as recently developed advanced Reynolds-averaged turbulence modeling as well as machine-learning methods are summarized.","sentences":["Driven by fundamental thermodynamic efficiency considerations, an emerging trend in the energy and propulsion systems is that the working fluid operates at a pressure above the critical pressure.","Energy transport is thus accompanied by dramatic and strongly nonlinear variations of thermophysical properties, which cause abnormal heat transfer behavior and non-ideal gas effects.","This situation raises a crucial challenge for the heat exchanger and turbomachinery design, overall energy and exergy efficiency.","We aim to provide a multi-scale overview of the flow and thermal behavior of fluid above the critical point: microscopic physics and macroscopic transport.","Microscopic physics, i.e. near-critical thermophysical properties, phase transition and fluid dynamics, are introduced.","A particular focus will be on the supercritical pseudo boiling process, which is a generalization of classical liquid-vapor phase transitions to non-equilibrium supercritical states.","These new perspectives lead to a revised view of the state space.","Further, recent results demonstrated the possibility of stable supercritical fluid interfaces without surface tension.","On the macroscale, recent progress on the modeling of turbulent flow and convective heat transfer of supercritical fluids are summarized.","Direct numerical simulation is able to offer insights into the physics of thermal fluids.","We start with a description of fundamental fluid mechanics problems related to supercritical fluids.","In addition, the heat transfer deterioration in supercritical fluids is found to be closely connected to the flow relaminarization by the non-uniform body-force.","Finally, various modeling approaches such as recently developed advanced Reynolds-averaged turbulence modeling as well as machine-learning methods are summarized."],"url":"http://arxiv.org/abs/2406.04119v1","category":"physics.flu-dyn"}
{"created":"2024-06-06 14:36:07","title":"Promoting Fairness and Diversity in Speech Datasets for Mental Health and Neurological Disorders Research","abstract":"Current research in machine learning and artificial intelligence is largely centered on modeling and performance evaluation, less so on data collection. However, recent research demonstrated that limitations and biases in data may negatively impact trustworthiness and reliability. These aspects are particularly impactful on sensitive domains such as mental health and neurological disorders, where speech data are used to develop AI applications aimed at improving the health of patients and supporting healthcare providers. In this paper, we chart the landscape of available speech datasets for this domain, to highlight possible pitfalls and opportunities for improvement and promote fairness and diversity. We present a comprehensive list of desiderata for building speech datasets for mental health and neurological disorders and distill it into a checklist focused on ethical concerns to foster more responsible research.","sentences":["Current research in machine learning and artificial intelligence is largely centered on modeling and performance evaluation, less so on data collection.","However, recent research demonstrated that limitations and biases in data may negatively impact trustworthiness and reliability.","These aspects are particularly impactful on sensitive domains such as mental health and neurological disorders, where speech data are used to develop AI applications aimed at improving the health of patients and supporting healthcare providers.","In this paper, we chart the landscape of available speech datasets for this domain, to highlight possible pitfalls and opportunities for improvement and promote fairness and diversity.","We present a comprehensive list of desiderata for building speech datasets for mental health and neurological disorders and distill it into a checklist focused on ethical concerns to foster more responsible research."],"url":"http://arxiv.org/abs/2406.04116v1","category":"cs.AI"}
{"created":"2024-06-06 14:31:38","title":"Harmonic generation with topological edge states and electron-electron interaction","abstract":"It has been found previously that the presence or absence of topological edge states in the Su-Schrieffer-Heeger (SSH) model has a huge impact on harmonic generation spectra. More specifically, the yield of harmonics for harmonic orders that correspond to photon energies below the band gap is many orders of magnitude different in the trivial and topological phase. It is shown in this work that this effect is still present if electron-electron interaction is taken into account, i.e., if a Hubbard term is added to the SSH Hamiltonian. To that end, finite SSH-Hubbard chains at half filling are considered that are short enough to be accessible to exact diagonalization but already showing edge states in the topological phase. We show that the huge difference in the harmonic yield between the trivial and the topological phase can be reproduced with few-level models employing only the many-body ground state and a few excited many-body states.","sentences":["It has been found previously that the presence or absence of topological edge states in the Su-Schrieffer-Heeger (SSH) model has a huge impact on harmonic generation spectra.","More specifically, the yield of harmonics for harmonic orders that correspond to photon energies below the band gap is many orders of magnitude different in the trivial and topological phase.","It is shown in this work that this effect is still present if electron-electron interaction is taken into account, i.e., if a Hubbard term is added to the SSH Hamiltonian.","To that end, finite SSH-Hubbard chains at half filling are considered that are short enough to be accessible to exact diagonalization but already showing edge states in the topological phase.","We show that the huge difference in the harmonic yield between the trivial and the topological phase can be reproduced with few-level models employing only the many-body ground state and a few excited many-body states."],"url":"http://arxiv.org/abs/2406.04114v1","category":"quant-ph"}
{"created":"2024-06-06 14:30:59","title":"Uncovering Limitations of Large Language Models in Information Seeking from Tables","abstract":"Tables are recognized for their high information density and widespread usage, serving as essential sources of information. Seeking information from tables (TIS) is a crucial capability for Large Language Models (LLMs), serving as the foundation of knowledge-based Q&A systems. However, this field presently suffers from an absence of thorough and reliable evaluation. This paper introduces a more reliable benchmark for Table Information Seeking (TabIS). To avoid the unreliable evaluation caused by text similarity-based metrics, TabIS adopts a single-choice question format (with two options per question) instead of a text generation format. We establish an effective pipeline for generating options, ensuring their difficulty and quality. Experiments conducted on 12 LLMs reveal that while the performance of GPT-4-turbo is marginally satisfactory, both other proprietary and open-source models perform inadequately. Further analysis shows that LLMs exhibit a poor understanding of table structures, and struggle to balance between TIS performance and robustness against pseudo-relevant tables (common in retrieval-augmented systems). These findings uncover the limitations and potential challenges of LLMs in seeking information from tables. We release our data and code to facilitate further research in this field.","sentences":["Tables are recognized for their high information density and widespread usage, serving as essential sources of information.","Seeking information from tables (TIS) is a crucial capability for Large Language Models (LLMs), serving as the foundation of knowledge-based Q&A systems.","However, this field presently suffers from an absence of thorough and reliable evaluation.","This paper introduces a more reliable benchmark for Table Information Seeking (TabIS).","To avoid the unreliable evaluation caused by text similarity-based metrics, TabIS adopts a single-choice question format (with two options per question) instead of a text generation format.","We establish an effective pipeline for generating options, ensuring their difficulty and quality.","Experiments conducted on 12 LLMs reveal that while the performance of GPT-4-turbo is marginally satisfactory, both other proprietary and open-source models perform inadequately.","Further analysis shows that LLMs exhibit a poor understanding of table structures, and struggle to balance between TIS performance and robustness against pseudo-relevant tables (common in retrieval-augmented systems).","These findings uncover the limitations and potential challenges of LLMs in seeking information from tables.","We release our data and code to facilitate further research in this field."],"url":"http://arxiv.org/abs/2406.04113v1","category":"cs.CL"}
{"created":"2024-06-06 14:29:49","title":"Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation","abstract":"While overparameterization in machine learning models offers great benefits in terms of optimization and generalization, it also leads to increased computational requirements as model sizes grow. In this work, we show that by leveraging the inherent low-dimensional structures of data and compressible dynamics within the model parameters, we can reap the benefits of overparameterization without the computational burdens. In practice, we demonstrate the effectiveness of this approach for deep low-rank matrix completion as well as fine-tuning language models. Our approach is grounded in theoretical findings for deep overparameterized low-rank matrix recovery, where we show that the learning dynamics of each weight matrix are confined to an invariant low-dimensional subspace. Consequently, we can construct and train compact, highly compressed factorizations possessing the same benefits as their overparameterized counterparts. In the context of deep matrix completion, our technique substantially improves training efficiency while retaining the advantages of overparameterization. For language model fine-tuning, we propose a method called \"Deep LoRA\", which improves the existing low-rank adaptation (LoRA) technique, leading to reduced overfitting and a simplified hyperparameter setup, while maintaining comparable efficiency. We validate the effectiveness of Deep LoRA on natural language tasks, particularly when fine-tuning with limited data.","sentences":["While overparameterization in machine learning models offers great benefits in terms of optimization and generalization, it also leads to increased computational requirements as model sizes grow.","In this work, we show that by leveraging the inherent low-dimensional structures of data and compressible dynamics within the model parameters, we can reap the benefits of overparameterization without the computational burdens.","In practice, we demonstrate the effectiveness of this approach for deep low-rank matrix completion as well as fine-tuning language models.","Our approach is grounded in theoretical findings for deep overparameterized low-rank matrix recovery, where we show that the learning dynamics of each weight matrix are confined to an invariant low-dimensional subspace.","Consequently, we can construct and train compact, highly compressed factorizations possessing the same benefits as their overparameterized counterparts.","In the context of deep matrix completion, our technique substantially improves training efficiency while retaining the advantages of overparameterization.","For language model fine-tuning, we propose a method called \"Deep LoRA\", which improves the existing low-rank adaptation (LoRA) technique, leading to reduced overfitting and a simplified hyperparameter setup, while maintaining comparable efficiency.","We validate the effectiveness of Deep LoRA on natural language tasks, particularly when fine-tuning with limited data."],"url":"http://arxiv.org/abs/2406.04112v1","category":"cs.LG"}
{"created":"2024-06-06 14:25:15","title":"Comparing second-order gravitational self-force and effective-one-body waveforms from inspiralling, quasi-circular black hole binaries with a non-spinning primary and a spinning secondary","abstract":"We present the first comparison of waveforms evaluated using the effective-one-body (EOB) approach and gravitational self-force (GSF) theory for inspiralling black hole binaries with a non-spinning primary and a spinning secondary. This paper belongs to a series of papers comparing the EOB model TEOBResumS to GSF results, where the latter are used to benchmark the EOB analytical choices in the large-mass-ratio regime. In this work, we explore the performance of two gauge choices for the gyro-gravitomagnetic functions GS, GS* entering the spin-orbit sector within the EOB dynamics. In particular, we consider the usual gauge of TEOBResumS, where GS and GS* only depend on the inverse radius and the radial momentum, and a different gauge where these functions also depend on the azimuthal momentum. The latter choice allows us to exploit as prefactor in GS* the complete expression GKS* for a spinning particle on Kerr. As done previously, we employ both waveform alignments in the time domain and a gauge-invariant frequency-domain analysis to gain a more complete understanding of the impact of the new analytical choice. The frequency-domain analysis is particularly useful in confirming that the gyro-gravitomagnetic functions in the new chosen gauge bring the EOB spin contribution at 1st post-adiabatic order closer to the GSF one. We finally implement the improved functions within the public code for TEOBResumS-Dal\\'i, which already incorporates eccentricity. In this way, we upgrade the EOB model for extreme-mass-ratio inspirals presented in our previous work.","sentences":["We present the first comparison of waveforms evaluated using the effective-one-body (EOB) approach and gravitational self-force (GSF) theory for inspiralling black hole binaries with a non-spinning primary and a spinning secondary.","This paper belongs to a series of papers comparing the EOB model TEOBResumS to GSF results, where the latter are used to benchmark the EOB analytical choices in the large-mass-ratio regime.","In this work, we explore the performance of two gauge choices for the gyro-gravitomagnetic functions GS, GS* entering the spin-orbit sector within the EOB dynamics.","In particular, we consider the usual gauge of TEOBResumS, where GS and GS* only depend on the inverse radius and the radial momentum, and a different gauge where these functions also depend on the azimuthal momentum.","The latter choice allows us to exploit as prefactor in GS* the complete expression GKS* for a spinning particle on Kerr.","As done previously, we employ both waveform alignments in the time domain and a gauge-invariant frequency-domain analysis to gain a more complete understanding of the impact of the new analytical choice.","The frequency-domain analysis is particularly useful in confirming that the gyro-gravitomagnetic functions in the new chosen gauge bring the EOB spin contribution at 1st post-adiabatic order closer to the GSF one.","We finally implement the improved functions within the public code for TEOBResumS-Dal\\'i, which already incorporates eccentricity.","In this way, we upgrade the EOB model for extreme-mass-ratio inspirals presented in our previous work."],"url":"http://arxiv.org/abs/2406.04108v1","category":"gr-qc"}
{"created":"2024-06-06 14:23:20","title":"A Practical Analysis Procedure on Generalizing Comparative Effectiveness in the Randomized Clinical Trial to the Real-world Trialeligible Population","abstract":"When evaluating the effectiveness of a drug, a Randomized Controlled Trial (RCT) is often considered the gold standard due to its perfect randomization. While RCT assures strong internal validity, its restricted external validity poses challenges in extending treatment effects to the broader real-world population due to possible heterogeneity in covariates. In this paper, we introduce a procedure to generalize the RCT findings to the real-world trial-eligible population based on the adaption of existing statistical methods. We utilized the augmented inversed probability of sampling weighting (AIPSW) estimator for the estimation and omitted variable bias framework to assess the robustness of the estimate against the assumption violation caused by potentially unmeasured confounders. We analyzed an RCT comparing the effectiveness of lowering hypertension between Songling Xuemaikang Capsule (SXC), a traditional Chinese medicine (TCM), and Losartan as an illustration. The generalization results indicated that although SXC is less effective in lowering blood pressure than Losartan on week 2, week 4, and week 6, there is no statistically significant difference among the trial-eligible population at week 8, and the generalization is robust against potential unmeasured confounders.","sentences":["When evaluating the effectiveness of a drug, a Randomized Controlled Trial (RCT) is often considered the gold standard due to its perfect randomization.","While RCT assures strong internal validity, its restricted external validity poses challenges in extending treatment effects to the broader real-world population due to possible heterogeneity in covariates.","In this paper, we introduce a procedure to generalize the RCT findings to the real-world trial-eligible population based on the adaption of existing statistical methods.","We utilized the augmented inversed probability of sampling weighting (AIPSW) estimator for the estimation and omitted variable bias framework to assess the robustness of the estimate against the assumption violation caused by potentially unmeasured confounders.","We analyzed an RCT comparing the effectiveness of lowering hypertension between Songling Xuemaikang Capsule (SXC), a traditional Chinese medicine (TCM), and Losartan as an illustration.","The generalization results indicated that although SXC is less effective in lowering blood pressure than Losartan on week 2, week 4, and week 6, there is no statistically significant difference among the trial-eligible population at week 8, and the generalization is robust against potential unmeasured confounders."],"url":"http://arxiv.org/abs/2406.04107v1","category":"stat.AP"}
{"created":"2024-06-06 14:23:10","title":"Explainability and Hate Speech: Structured Explanations Make Social Media Moderators Faster","abstract":"Content moderators play a key role in keeping the conversation on social media healthy. While the high volume of content they need to judge represents a bottleneck to the moderation pipeline, no studies have explored how models could support them to make faster decisions. There is, by now, a vast body of research into detecting hate speech, sometimes explicitly motivated by a desire to help improve content moderation, but published research using real content moderators is scarce. In this work we investigate the effect of explanations on the speed of real-world moderators. Our experiments show that while generic explanations do not affect their speed and are often ignored, structured explanations lower moderators' decision making time by 7.4%.","sentences":["Content moderators play a key role in keeping the conversation on social media healthy.","While the high volume of content they need to judge represents a bottleneck to the moderation pipeline, no studies have explored how models could support them to make faster decisions.","There is, by now, a vast body of research into detecting hate speech, sometimes explicitly motivated by a desire to help improve content moderation, but published research using real content moderators is scarce.","In this work we investigate the effect of explanations on the speed of real-world moderators.","Our experiments show that while generic explanations do not affect their speed and are often ignored, structured explanations lower moderators' decision making time by 7.4%."],"url":"http://arxiv.org/abs/2406.04106v1","category":"cs.CL"}
{"created":"2024-06-06 14:20:55","title":"Symplectic Methods in Deep Learning","abstract":"Deep learning is widely used in tasks including image recognition and generation, in learning dynamical systems from data and many more. It is important to construct learning architectures with theoretical guarantees to permit safety in the applications. There has been considerable progress in this direction lately. In particular, symplectic networks were shown to have the non vanishing gradient property, essential for numerical stability. On the other hand, architectures based on higher order numerical methods were shown to be efficient in many tasks where the learned function has an underlying dynamical structure. In this work we construct symplectic networks based on higher order explicit methods with non vanishing gradient property and test their efficiency on various examples.","sentences":["Deep learning is widely used in tasks including image recognition and generation, in learning dynamical systems from data and many more.","It is important to construct learning architectures with theoretical guarantees to permit safety in the applications.","There has been considerable progress in this direction lately.","In particular, symplectic networks were shown to have the non vanishing gradient property, essential for numerical stability.","On the other hand, architectures based on higher order numerical methods were shown to be efficient in many tasks where the learned function has an underlying dynamical structure.","In this work we construct symplectic networks based on higher order explicit methods with non vanishing gradient property and test their efficiency on various examples."],"url":"http://arxiv.org/abs/2406.04104v1","category":"math.NA"}
{"created":"2024-06-06 14:20:21","title":"Multistep Distillation of Diffusion Models via Moment Matching","abstract":"We present a new method for making diffusion models faster to sample. The method distills many-step diffusion models into few-step models by matching conditional expectations of the clean data given noisy data along the sampling trajectory. Our approach extends recently proposed one-step methods to the multi-step case, and provides a new perspective by interpreting these approaches in terms of moment matching. By using up to 8 sampling steps, we obtain distilled models that outperform not only their one-step versions but also their original many-step teacher models, obtaining new state-of-the-art results on the Imagenet dataset. We also show promising results on a large text-to-image model where we achieve fast generation of high resolution images directly in image space, without needing autoencoders or upsamplers.","sentences":["We present a new method for making diffusion models faster to sample.","The method distills many-step diffusion models into few-step models by matching conditional expectations of the clean data given noisy data along the sampling trajectory.","Our approach extends recently proposed one-step methods to the multi-step case, and provides a new perspective by interpreting these approaches in terms of moment matching.","By using up to 8 sampling steps, we obtain distilled models that outperform not only their one-step versions but also their original many-step teacher models, obtaining new state-of-the-art results on the Imagenet dataset.","We also show promising results on a large text-to-image model where we achieve fast generation of high resolution images directly in image space, without needing autoencoders or upsamplers."],"url":"http://arxiv.org/abs/2406.04103v1","category":"cs.LG"}
{"created":"2024-06-06 14:15:15","title":"Class-Aware Cartilage Segmentation for Autonomous US-CT Registration in Robotic Intercostal Ultrasound Imaging","abstract":"Ultrasound imaging has been widely used in clinical examinations owing to the advantages of being portable, real-time, and radiation-free. Considering the potential of extensive deployment of autonomous examination systems in hospitals, robotic US imaging has attracted increased attention. However, due to the inter-patient variations, it is still challenging to have an optimal path for each patient, particularly for thoracic applications with limited acoustic windows, e.g., intercostal liver imaging. To address this problem, a class-aware cartilage bone segmentation network with geometry-constraint post-processing is presented to capture patient-specific rib skeletons. Then, a dense skeleton graph-based non-rigid registration is presented to map the intercostal scanning path from a generic template to individual patients. By explicitly considering the high-acoustic impedance bone structures, the transferred scanning path can be precisely located in the intercostal space, enhancing the visibility of internal organs by reducing the acoustic shadow. To evaluate the proposed approach, the final path mapping performance is validated on five distinct CTs and two volunteer US data, resulting in ten pairs of CT-US combinations. Results demonstrate that the proposed graph-based registration method can robustly and precisely map the path from CT template to individual patients (Euclidean error: $2.21\\pm1.11~mm$).","sentences":["Ultrasound imaging has been widely used in clinical examinations owing to the advantages of being portable, real-time, and radiation-free.","Considering the potential of extensive deployment of autonomous examination systems in hospitals, robotic US imaging has attracted increased attention.","However, due to the inter-patient variations, it is still challenging to have an optimal path for each patient, particularly for thoracic applications with limited acoustic windows, e.g., intercostal liver imaging.","To address this problem, a class-aware cartilage bone segmentation network with geometry-constraint post-processing is presented to capture patient-specific rib skeletons.","Then, a dense skeleton graph-based non-rigid registration is presented to map the intercostal scanning path from a generic template to individual patients.","By explicitly considering the high-acoustic impedance bone structures, the transferred scanning path can be precisely located in the intercostal space, enhancing the visibility of internal organs by reducing the acoustic shadow.","To evaluate the proposed approach, the final path mapping performance is validated on five distinct CTs and two volunteer US data, resulting in ten pairs of CT-US combinations.","Results demonstrate that the proposed graph-based registration method can robustly and precisely map the path from CT template to individual patients (Euclidean error: $2.21\\pm1.11~mm$)."],"url":"http://arxiv.org/abs/2406.04100v1","category":"cs.CV"}
{"created":"2024-06-06 14:15:12","title":"Enhancing Weather Predictions: Super-Resolution via Deep Diffusion Models","abstract":"This study investigates the application of deep-learning diffusion models for the super-resolution of weather data, a novel approach aimed at enhancing the spatial resolution and detail of meteorological variables. Leveraging the capabilities of diffusion models, specifically the SR3 and ResDiff architectures, we present a methodology for transforming low-resolution weather data into high-resolution outputs. Our experiments, conducted using the WeatherBench dataset, focus on the super-resolution of the two-meter temperature variable, demonstrating the models' ability to generate detailed and accurate weather maps. The results indicate that the ResDiff model, further improved by incorporating physics-based modifications, significantly outperforms traditional SR3 methods in terms of Mean Squared Error (MSE), Structural Similarity Index (SSIM), and Peak Signal-to-Noise Ratio (PSNR). This research highlights the potential of diffusion models in meteorological applications, offering insights into their effectiveness, challenges, and prospects for future advancements in weather prediction and climate analysis.","sentences":["This study investigates the application of deep-learning diffusion models for the super-resolution of weather data, a novel approach aimed at enhancing the spatial resolution and detail of meteorological variables.","Leveraging the capabilities of diffusion models, specifically the SR3 and ResDiff architectures, we present a methodology for transforming low-resolution weather data into high-resolution outputs.","Our experiments, conducted using the WeatherBench dataset, focus on the super-resolution of the two-meter temperature variable, demonstrating the models' ability to generate detailed and accurate weather maps.","The results indicate that the ResDiff model, further improved by incorporating physics-based modifications, significantly outperforms traditional SR3 methods in terms of Mean Squared Error (MSE), Structural Similarity Index (SSIM), and Peak Signal-to-Noise Ratio (PSNR).","This research highlights the potential of diffusion models in meteorological applications, offering insights into their effectiveness, challenges, and prospects for future advancements in weather prediction and climate analysis."],"url":"http://arxiv.org/abs/2406.04099v1","category":"cs.LG"}
{"created":"2024-06-06 14:10:12","title":"Scaling and evaluating sparse autoencoders","abstract":"Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.","sentences":["Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer.","Since language models learn many concepts, autoencoders need to be very large to recover all relevant features.","However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents.","We propose using k-sparse autoencoders","[Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier.","Additionally, we find modifications that result in few dead latents, even at the largest scales we tried.","Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity.","We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects.","These metrics all generally improve with autoencoder size.","To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens.","We release training code and autoencoders for open-source models, as well as a visualizer."],"url":"http://arxiv.org/abs/2406.04093v1","category":"cs.LG"}
{"created":"2024-06-06 14:07:33","title":"Towards bit threads in general gravitational spacetimes","abstract":"The concept of the generalized entanglement wedge was recently proposed by Bousso and Penington, which states that any bulk gravitational region $a$ possesses an associated generalized entanglement wedge $E(a) \\supset a$ on a static Cauchy surface $M$ in general gravitational spacetimes, where $E(a)$ may contain an entanglement island $I(a)$. It suggests that the fine-grained entropy for bulk region $a$ is given by the generalized entropy $S_{\\text{gen}}(E(a))$. Motivated by this proposal, we extend the quantum bit thread description to general gravitational spacetimes, no longer limited to the AdS spacetime. By utilizing the convex optimization techniques, a dual flow description for the generalized entropy $S_{\\text{gen}}(E(a))$ of a bulk gravitational region $a$ is established on the static Cauchy surface $M$, such that $S_{\\text{gen}}(E(a))$ is equal to the maximum flux of any flow that starts from the boundary $\\partial M$ and ends at bulk region $a$, or equivalently, the maximum number of bit threads that connect the boundary $\\partial M$ to the bulk region $a$. In addition, the nesting property of flows is also proved. Thus the basic properties of the entropy for bulk regions, i.e. the monotonicity, subadditivity, Araki-Lieb inequality and strong subadditivity, can be verified from flow perspectives by using properties of flows, such as the nesting property. Moreover, in max thread configurations, we find that there exists some lower bounds on the bulk entanglement entropy of matter fields in the region $E(a)\\setminus a$, particularly on an entanglement island region $I(a) \\subset (E(a)\\setminus a)$, as required by the existence of a nontrivial generalized entanglement wedge. Our quantum bit thread formulation may provide a way to investigate more fine-grained entanglement structures in general spacetimes.","sentences":["The concept of the generalized entanglement wedge was recently proposed by Bousso and Penington, which states that any bulk gravitational region $a$ possesses an associated generalized entanglement wedge $E(a) \\supset a$ on a static Cauchy surface $M$ in general gravitational spacetimes, where $E(a)$ may contain an entanglement island $I(a)$.","It suggests that the fine-grained entropy for bulk region $a$ is given by the generalized entropy $S_{\\text{gen}}(E(a))$. Motivated by this proposal, we extend the quantum bit thread description to general gravitational spacetimes, no longer limited to the AdS spacetime.","By utilizing the convex optimization techniques, a dual flow description for the generalized entropy $S_{\\text{gen}}(E(a))$ of a bulk gravitational region $a$ is established on the static Cauchy surface $M$, such that $S_{\\text{gen}}(E(a))$ is equal to the maximum flux of any flow that starts from the boundary $\\partial M$ and ends at bulk region $a$, or equivalently, the maximum number of bit threads that connect the boundary $\\partial M$ to the bulk region $a$. In addition, the nesting property of flows is also proved.","Thus the basic properties of the entropy for bulk regions, i.e. the monotonicity, subadditivity, Araki-Lieb inequality and strong subadditivity, can be verified from flow perspectives by using properties of flows, such as the nesting property.","Moreover, in max thread configurations, we find that there exists some lower bounds on the bulk entanglement entropy of matter fields in the region $E(a)\\setminus a$, particularly on an entanglement island region $I(a) \\subset (E(a)\\setminus a)$, as required by the existence of a nontrivial generalized entanglement wedge.","Our quantum bit thread formulation may provide a way to investigate more fine-grained entanglement structures in general spacetimes."],"url":"http://arxiv.org/abs/2406.04092v1","category":"hep-th"}
{"created":"2024-06-06 13:59:51","title":"On Limitation of Transformer for Learning HMMs","abstract":"Despite the remarkable success of Transformer-based architectures in various sequential modeling tasks, such as natural language processing, computer vision, and robotics, their ability to learn basic sequential models, like Hidden Markov Models (HMMs), is still unclear. This paper investigates the performance of Transformers in learning HMMs and their variants through extensive experimentation and compares them to Recurrent Neural Networks (RNNs). We show that Transformers consistently underperform RNNs in both training speed and testing accuracy across all tested HMM models. There are even challenging HMM instances where Transformers struggle to learn, while RNNs can successfully do so. Our experiments further reveal the relation between the depth of Transformers and the longest sequence length it can effectively learn, based on the types and the complexity of HMMs. To address the limitation of transformers in modeling HMMs, we demonstrate that a variant of the Chain-of-Thought (CoT), called $\\textit{block CoT}$ in the training phase, can help transformers to reduce the evaluation error and to learn longer sequences at a cost of increasing the training time. Finally, we complement our empirical findings by theoretical results proving the expressiveness of transformers in approximating HMMs with logarithmic depth.","sentences":["Despite the remarkable success of Transformer-based architectures in various sequential modeling tasks, such as natural language processing, computer vision, and robotics, their ability to learn basic sequential models, like Hidden Markov Models (HMMs), is still unclear.","This paper investigates the performance of Transformers in learning HMMs and their variants through extensive experimentation and compares them to Recurrent Neural Networks (RNNs).","We show that Transformers consistently underperform RNNs in both training speed and testing accuracy across all tested HMM models.","There are even challenging HMM instances where Transformers struggle to learn, while RNNs can successfully do so.","Our experiments further reveal the relation between the depth of Transformers and the longest sequence length it can effectively learn, based on the types and the complexity of HMMs.","To address the limitation of transformers in modeling HMMs, we demonstrate that a variant of the Chain-of-Thought (CoT), called $\\textit{block CoT}$ in the training phase, can help transformers to reduce the evaluation error and to learn longer sequences at a cost of increasing the training time.","Finally, we complement our empirical findings by theoretical results proving the expressiveness of transformers in approximating HMMs with logarithmic depth."],"url":"http://arxiv.org/abs/2406.04089v1","category":"cs.LG"}
{"created":"2024-06-06 13:57:05","title":"A Survey of Language-Based Communication in Robotics","abstract":"Embodied robots which can interact with their environment and neighbours are increasingly being used as a test case to develop Artificial Intelligence. This creates a need for multimodal robot controllers which can operate across different types of information including text. Large Language Models are able to process and generate textual as well as audiovisual data and, more recently, robot actions. Language Models are increasingly being applied to robotic systems; these Language-Based robots leverage the power of language models in a variety of ways. Additionally, the use of language opens up multiple forms of information exchange between members of a human-robot team. This survey motivates the use of language models in robotics, and then delineates works based on the part of the overall control flow in which language is incorporated. Language can be used by human to task a robot, by a robot to inform a human, between robots as a human-like communication medium, and internally for a robot's planning and control. Applications of language-based robots are explored, and finally numerous limitations and challenges are discussed to provide a summary of the development needed for language-based robotics moving forward. Links to each paper and, if available, source code are made available in the accompanying site at https://uos-haris.online/sooratilab/papers/WillSurvey/LangRobotSurvey.php","sentences":["Embodied robots which can interact with their environment and neighbours are increasingly being used as a test case to develop Artificial Intelligence.","This creates a need for multimodal robot controllers which can operate across different types of information including text.","Large Language Models are able to process and generate textual as well as audiovisual data and, more recently, robot actions.","Language Models are increasingly being applied to robotic systems; these Language-Based robots leverage the power of language models in a variety of ways.","Additionally, the use of language opens up multiple forms of information exchange between members of a human-robot team.","This survey motivates the use of language models in robotics, and then delineates works based on the part of the overall control flow in which language is incorporated.","Language can be used by human to task a robot, by a robot to inform a human, between robots as a human-like communication medium, and internally for a robot's planning and control.","Applications of language-based robots are explored, and finally numerous limitations and challenges are discussed to provide a summary of the development needed for language-based robotics moving forward.","Links to each paper and, if available, source code are made available in the accompanying site at https://uos-haris.online/sooratilab/papers/WillSurvey/LangRobotSurvey.php"],"url":"http://arxiv.org/abs/2406.04086v1","category":"cs.RO"}
{"created":"2024-06-06 13:54:04","title":"Quadruple-well ferroelectricity and moderate switching barrier in defective wurtzite \u03b1-Al3S3: a first-principles study","abstract":"Wurtzite-type ferroelectrics are highly promising for next-generation microelectronic devices due to their ferroelectric properties and integration with exiting semiconductors. However, their high coercive fields, which are close to breakdown electric fields, need to be lowered. To deal with this issue and secure device reliability, much effort has been devoted to exploring novel wurtzite compounds with lower polarization switching barriers and implementing doping strategies. Here, we report first-principles calculations on polarization switching in cation-vacancy ordered wurtzite {\\alpha}-Al2S3, unveiling its uniaxial quadruple-well ferroelectricity and moderate switching barrier, 51 meV/cation, which is much lower than that of conventional wurtzite ferroelectrics. There are three important features relevant to the Al vacancies leading to the uncommon quadruple-well ferroelectricity and the moderate switching barrier: mitigation of cation-cation repulsion, structural flexibility that alleviates an in-plane lattice expansion, and formation of {\\sigma}-like bonding states consisting of Al 3pz and S 3pz orbitals. Biaxial compressive strain and Ga doping lower the switching barriers by up to 40%. This study encourages experimental investigation of the ferroelectric properties for defective wurtzite {\\alpha}-Al2S3 as a new promising material with unconventional and intriguing ferroelectricity and suggests a potential strategy for reducing switching barriers in wurtzite ferroelectrics: introducing cation vacancies.","sentences":["Wurtzite-type ferroelectrics are highly promising for next-generation microelectronic devices due to their ferroelectric properties and integration with exiting semiconductors.","However, their high coercive fields, which are close to breakdown electric fields, need to be lowered.","To deal with this issue and secure device reliability, much effort has been devoted to exploring novel wurtzite compounds with lower polarization switching barriers and implementing doping strategies.","Here, we report first-principles calculations on polarization switching in cation-vacancy ordered wurtzite {\\alpha}-Al2S3, unveiling its uniaxial quadruple-well ferroelectricity and moderate switching barrier, 51 meV/cation, which is much lower than that of conventional wurtzite ferroelectrics.","There are three important features relevant to the Al vacancies leading to the uncommon quadruple-well ferroelectricity and the moderate switching barrier: mitigation of cation-cation repulsion, structural flexibility that alleviates an in-plane lattice expansion, and formation of {\\sigma}-like bonding states consisting of Al 3pz and S 3pz orbitals.","Biaxial compressive strain and Ga doping lower the switching barriers by up to 40%.","This study encourages experimental investigation of the ferroelectric properties for defective wurtzite {\\alpha}-Al2S3 as a new promising material with unconventional and intriguing ferroelectricity and suggests a potential strategy for reducing switching barriers in wurtzite ferroelectrics: introducing cation vacancies."],"url":"http://arxiv.org/abs/2406.04084v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-06 13:51:44","title":"Leveraging automatic strategy discovery to teach people how to select better projects","abstract":"The decisions of individuals and organizations are often suboptimal because normative decision strategies are too demanding in the real world. Recent work suggests that some errors can be prevented by leveraging artificial intelligence to discover and teach prescriptive decision strategies that take people's constraints into account. So far, this line of research has been limited to simplified decision problems. This article is the first to extend this approach to a real-world decision problem, namely project selection. We develop a computational method (MGPS) that automatically discovers project selection strategies that are optimized for real people and develop an intelligent tutor that teaches the discovered strategies. We evaluated MGPS on a computational benchmark and tested the intelligent tutor in a training experiment with two control conditions. MGPS outperformed a state-of-the-art method and was more computationally efficient. Moreover, the intelligent tutor significantly improved people's decision strategies. Our results indicate that our method can improve human decision-making in naturalistic settings similar to real-world project selection, a first step towards applying strategy discovery to the real world.","sentences":["The decisions of individuals and organizations are often suboptimal because normative decision strategies are too demanding in the real world.","Recent work suggests that some errors can be prevented by leveraging artificial intelligence to discover and teach prescriptive decision strategies that take people's constraints into account.","So far, this line of research has been limited to simplified decision problems.","This article is the first to extend this approach to a real-world decision problem, namely project selection.","We develop a computational method (MGPS) that automatically discovers project selection strategies that are optimized for real people and develop an intelligent tutor that teaches the discovered strategies.","We evaluated MGPS on a computational benchmark and tested the intelligent tutor in a training experiment with two control conditions.","MGPS outperformed a state-of-the-art method and was more computationally efficient.","Moreover, the intelligent tutor significantly improved people's decision strategies.","Our results indicate that our method can improve human decision-making in naturalistic settings similar to real-world project selection, a first step towards applying strategy discovery to the real world."],"url":"http://arxiv.org/abs/2406.04082v1","category":"cs.AI"}
{"created":"2024-06-06 13:51:39","title":"Bootstrapping Expectiles in Reinforcement Learning","abstract":"Many classic Reinforcement Learning (RL) algorithms rely on a Bellman operator, which involves an expectation over the next states, leading to the concept of bootstrapping. To introduce a form of pessimism, we propose to replace this expectation with an expectile. In practice, this can be very simply done by replacing the $L_2$ loss with a more general expectile loss for the critic. Introducing pessimism in RL is desirable for various reasons, such as tackling the overestimation problem (for which classic solutions are double Q-learning or the twin-critic approach of TD3) or robust RL (where transitions are adversarial). We study empirically these two cases. For the overestimation problem, we show that the proposed approach, ExpectRL, provides better results than a classic twin-critic. On robust RL benchmarks, involving changes of the environment, we show that our approach is more robust than classic RL algorithms. We also introduce a variation of ExpectRL combined with domain randomization which is competitive with state-of-the-art robust RL agents. Eventually, we also extend \\ExpectRL with a mechanism for choosing automatically the expectile value, that is the degree of pessimism","sentences":["Many classic Reinforcement Learning (RL) algorithms rely on a Bellman operator, which involves an expectation over the next states, leading to the concept of bootstrapping.","To introduce a form of pessimism, we propose to replace this expectation with an expectile.","In practice, this can be very simply done by replacing the $L_2$ loss with a more general expectile loss for the critic.","Introducing pessimism in RL is desirable for various reasons, such as tackling the overestimation problem (for which classic solutions are double Q-learning or the twin-critic approach of TD3) or robust RL (where transitions are adversarial).","We study empirically these two cases.","For the overestimation problem, we show that the proposed approach, ExpectRL, provides better results than a classic twin-critic.","On robust RL benchmarks, involving changes of the environment, we show that our approach is more robust than classic RL algorithms.","We also introduce a variation of ExpectRL combined with domain randomization which is competitive with state-of-the-art robust RL agents.","Eventually, we also extend \\ExpectRL with a mechanism for choosing automatically the expectile value, that is the degree of pessimism"],"url":"http://arxiv.org/abs/2406.04081v1","category":"cs.LG"}
{"created":"2024-06-06 13:47:18","title":"How many sprays cover the space?","abstract":"For all \\( d \\geq 3 \\) we show that the cardinality of \\( \\mathbb{R} \\) is at most \\( \\aleph_n \\) if and only if \\( \\R^d \\) can be covered with \\( ( n + 1 ) ( d - 1 ) + 1 \\) sprays whose centers are in general position in a hyperplane. This extends previous results by Schmerl when \\( d = 2 \\).","sentences":["For all \\( d \\geq 3 \\) we show that the cardinality of \\( \\mathbb{R} \\) is at most \\( \\aleph_n \\) if and only if \\( \\R^d \\) can be covered with \\( ( n + 1 )","( d - 1 ) + 1 \\) sprays whose centers are in general position in a hyperplane.","This extends previous results by Schmerl when \\( d = 2 \\)."],"url":"http://arxiv.org/abs/2406.04078v1","category":"math.LO"}
{"created":"2024-06-06 13:46:25","title":"Why recommended visit intervals should be extracted when conducting longitudinal analyses using electronic health record data: examining visit mechanism and sensitivity to assessment not at random","abstract":"Electronic health records (EHRs) provide an efficient approach to generating rich longitudinal datasets. However, since patients visit as needed, the assessment times are typically irregular and may be related to the patient's health. Failing to account for this informative assessment process could result in biased estimates of the disease course. In this paper, we show how estimation of the disease trajectory can be enhanced by leveraging an underutilized piece of information that is often in the patient's EHR: physician-recommended intervals between visits. Specifically, we demonstrate how recommended intervals can be used in characterizing the assessment process, and in investigating the sensitivity of the results to assessment not at random (ANAR). We illustrate our proposed approach in a clinic-based cohort study of juvenile dermatomyositis (JDM). In this study, we found that the recommended intervals explained 78% of the variability in the assessment times. Under a specific case of ANAR where we assumed that a worsening in disease led to patients visiting earlier than recommended, the estimated population average disease activity trajectory was shifted downward relative to the trajectory assuming assessment at random. These results demonstrate the crucial role recommended intervals play in improving the rigour of the analysis by allowing us to assess both the plausibility of the AAR assumption and the sensitivity of the results to departures from this assumption. Thus, we advise that studies using irregular longitudinal data should extract recommended visit intervals and follow our procedure for incorporating them into analyses.","sentences":["Electronic health records (EHRs) provide an efficient approach to generating rich longitudinal datasets.","However, since patients visit as needed, the assessment times are typically irregular and may be related to the patient's health.","Failing to account for this informative assessment process could result in biased estimates of the disease course.","In this paper, we show how estimation of the disease trajectory can be enhanced by leveraging an underutilized piece of information that is often in the patient's EHR: physician-recommended intervals between visits.","Specifically, we demonstrate how recommended intervals can be used in characterizing the assessment process, and in investigating the sensitivity of the results to assessment not at random (ANAR).","We illustrate our proposed approach in a clinic-based cohort study of juvenile dermatomyositis (JDM).","In this study, we found that the recommended intervals explained 78% of the variability in the assessment times.","Under a specific case of ANAR where we assumed that a worsening in disease led to patients visiting earlier than recommended, the estimated population average disease activity trajectory was shifted downward relative to the trajectory assuming assessment at random.","These results demonstrate the crucial role recommended intervals play in improving the rigour of the analysis by allowing us to assess both the plausibility of the AAR assumption and the sensitivity of the results to departures from this assumption.","Thus, we advise that studies using irregular longitudinal data should extract recommended visit intervals and follow our procedure for incorporating them into analyses."],"url":"http://arxiv.org/abs/2406.04077v1","category":"stat.ME"}
{"created":"2024-06-06 13:41:37","title":"Estimation of Global Building Stocks by 2070: Unlocking Renovation Potential","abstract":"Buildings produce one-third of carbon emissions globally, however, data absence regarding global floorspace poses challenges in advancing building carbon neutrality. We compile the measured building stocks for 14 major economies and apply our global building stock model, GLOBUS, to evaluate future trends in stock turnover. Based on a scenario not considering renovation, by 2070 the building stock in developed economies will be ~1.4 times that of 2020 (100 billion m2); in developing economies it is expected to be 2.2 times that of 2020 (313 billion m2). Based on a techno-economic potential scenario, however, stocks in developed economies will decline to approximately 0.8 times the 2020 level, while stocks in developing economies will increase to nearly twice the 2020 level due to their fewer buildings currently. Overall, GLOBUS provides a way of calculating the global building stock, helping scientists, engineers, and policymakers conduct a range of investigation across various future scenarios.","sentences":["Buildings produce one-third of carbon emissions globally, however, data absence regarding global floorspace poses challenges in advancing building carbon neutrality.","We compile the measured building stocks for 14 major economies and apply our global building stock model, GLOBUS, to evaluate future trends in stock turnover.","Based on a scenario not considering renovation, by 2070 the building stock in developed economies will be ~1.4 times that of 2020 (100 billion m2); in developing economies it is expected to be 2.2 times that of 2020 (313 billion m2).","Based on a techno-economic potential scenario, however, stocks in developed economies will decline to approximately 0.8 times the 2020 level, while stocks in developing economies will increase to nearly twice the 2020 level due to their fewer buildings currently.","Overall, GLOBUS provides a way of calculating the global building stock, helping scientists, engineers, and policymakers conduct a range of investigation across various future scenarios."],"url":"http://arxiv.org/abs/2406.04074v1","category":"econ.GN"}
{"created":"2024-06-06 13:34:43","title":"Batch-in-Batch: a new adversarial training framework for initial perturbation and sample selection","abstract":"Adversarial training methods commonly generate independent initial perturbation for adversarial samples from a simple uniform distribution, and obtain the training batch for the classifier without selection. In this work, we propose a simple yet effective training framework called Batch-in-Batch (BB) to enhance models robustness. It involves specifically a joint construction of initial values that could simultaneously generates $m$ sets of perturbations from the original batch set to provide more diversity for adversarial samples; and also includes various sample selection strategies that enable the trained models to have smoother losses and avoid overconfident outputs. Through extensive experiments on three benchmark datasets (CIFAR-10, SVHN, CIFAR-100) with two networks (PreActResNet18 and WideResNet28-10) that are used in both the single-step (Noise-Fast Gradient Sign Method, N-FGSM) and multi-step (Projected Gradient Descent, PGD-10) adversarial training, we show that models trained within the BB framework consistently have higher adversarial accuracy across various adversarial settings, notably achieving over a 13% improvement on the SVHN dataset with an attack radius of 8/255 compared to the N-FGSM baseline model. Furthermore, experimental analysis of the efficiency of both the proposed initial perturbation method and sample selection strategies validates our insights. Finally, we show that our framework is cost-effective in terms of computational resources, even with a relatively large value of $m$.","sentences":["Adversarial training methods commonly generate independent initial perturbation for adversarial samples from a simple uniform distribution, and obtain the training batch for the classifier without selection.","In this work, we propose a simple yet effective training framework called Batch-in-Batch (BB) to enhance models robustness.","It involves specifically a joint construction of initial values that could simultaneously generates $m$ sets of perturbations from the original batch set to provide more diversity for adversarial samples; and also includes various sample selection strategies that enable the trained models to have smoother losses and avoid overconfident outputs.","Through extensive experiments on three benchmark datasets (CIFAR-10, SVHN, CIFAR-100) with two networks (PreActResNet18 and WideResNet28-10) that are used in both the single-step (Noise-Fast Gradient Sign Method, N-FGSM) and multi-step (Projected Gradient Descent, PGD-10) adversarial training, we show that models trained within the BB framework consistently have higher adversarial accuracy across various adversarial settings, notably achieving over a 13% improvement on the SVHN dataset with an attack radius of 8/255 compared to the N-FGSM baseline model.","Furthermore, experimental analysis of the efficiency of both the proposed initial perturbation method and sample selection strategies validates our insights.","Finally, we show that our framework is cost-effective in terms of computational resources, even with a relatively large value of $m$."],"url":"http://arxiv.org/abs/2406.04070v1","category":"cs.LG"}
{"created":"2024-06-06 13:33:45","title":"Reassessing How to Compare and Improve the Calibration of Machine Learning Models","abstract":"A machine learning model is calibrated if its predicted probability for an outcome matches the observed frequency for that outcome conditional on the model prediction. This property has become increasingly important as the impact of machine learning models has continued to spread to various domains. As a result, there are now a dizzying number of recent papers on measuring and improving the calibration of (specifically deep learning) models. In this work, we reassess the reporting of calibration metrics in the recent literature. We show that there exist trivial recalibration approaches that can appear seemingly state-of-the-art unless calibration and prediction metrics (i.e. test accuracy) are accompanied by additional generalization metrics such as negative log-likelihood. We then derive a calibration-based decomposition of Bregman divergences that can be used to both motivate a choice of calibration metric based on a generalization metric, and to detect trivial calibration. Finally, we apply these ideas to develop a new extension to reliability diagrams that can be used to jointly visualize calibration as well as the estimated generalization error of a model.","sentences":["A machine learning model is calibrated if its predicted probability for an outcome matches the observed frequency for that outcome conditional on the model prediction.","This property has become increasingly important as the impact of machine learning models has continued to spread to various domains.","As a result, there are now a dizzying number of recent papers on measuring and improving the calibration of (specifically deep learning) models.","In this work, we reassess the reporting of calibration metrics in the recent literature.","We show that there exist trivial recalibration approaches that can appear seemingly state-of-the-art unless calibration and prediction metrics (i.e. test accuracy) are accompanied by additional generalization metrics such as negative log-likelihood.","We then derive a calibration-based decomposition of Bregman divergences that can be used to both motivate a choice of calibration metric based on a generalization metric, and to detect trivial calibration.","Finally, we apply these ideas to develop a new extension to reliability diagrams that can be used to jointly visualize calibration as well as the estimated generalization error of a model."],"url":"http://arxiv.org/abs/2406.04068v1","category":"cs.LG"}
{"created":"2024-06-06 13:32:09","title":"Ask LLMs Directly, \"What shapes your bias?\": Measuring Social Bias in Large Language Models","abstract":"Social bias is shaped by the accumulation of social perceptions towards targets across various demographic identities. To fully understand such social bias in large language models (LLMs), it is essential to consider the composite of social perceptions from diverse perspectives among identities. Previous studies have either evaluated biases in LLMs by indirectly assessing the presence of sentiments towards demographic identities in the generated text or measuring the degree of alignment with given stereotypes. These methods have limitations in directly quantifying social biases at the level of distinct perspectives among identities. In this paper, we aim to investigate how social perceptions from various viewpoints contribute to the development of social bias in LLMs. To this end, we propose a novel strategy to intuitively quantify these social perceptions and suggest metrics that can evaluate the social biases within LLMs by aggregating diverse social perceptions. The experimental results show the quantitative demonstration of the social attitude in LLMs by examining social perception. The analysis we conducted shows that our proposed metrics capture the multi-dimensional aspects of social bias, enabling a fine-grained and comprehensive investigation of bias in LLMs.","sentences":["Social bias is shaped by the accumulation of social perceptions towards targets across various demographic identities.","To fully understand such social bias in large language models (LLMs), it is essential to consider the composite of social perceptions from diverse perspectives among identities.","Previous studies have either evaluated biases in LLMs by indirectly assessing the presence of sentiments towards demographic identities in the generated text or measuring the degree of alignment with given stereotypes.","These methods have limitations in directly quantifying social biases at the level of distinct perspectives among identities.","In this paper, we aim to investigate how social perceptions from various viewpoints contribute to the development of social bias in LLMs.","To this end, we propose a novel strategy to intuitively quantify these social perceptions and suggest metrics that can evaluate the social biases within LLMs by aggregating diverse social perceptions.","The experimental results show the quantitative demonstration of the social attitude in LLMs by examining social perception.","The analysis we conducted shows that our proposed metrics capture the multi-dimensional aspects of social bias, enabling a fine-grained and comprehensive investigation of bias in LLMs."],"url":"http://arxiv.org/abs/2406.04064v1","category":"cs.CL"}
{"created":"2024-06-06 13:26:23","title":"Solution to a conjecture on resistance distances of block tower graphs","abstract":"Let $G$ be a connected graph. The resistance distance between two vertices $u$ and $v$ of $G$, denoted by $R_{G}[u,v]$, is defined as the net effective resistance between them in the electric network constructed from $G$ by replacing each edge with a unit resistor. The resistance diameter of $G$, denoted by $D_{r}(G)$, is defined as the maximum resistance distance among all pairs of vertices of $G$. Let $P_n=a_1a_2\\ldots a_n$ be the $n$-vertex path graph and $C_{4}=b_{1}b_2b_3b_4b_{1}$ be the 4-cycle. Then the $n$-th block tower graph $G_n$ is defined as the the Cartesian product of $P_n$ and $C_4$, that is, $G_n=P_{n}\\square C_4$. Clearly, the vertex set of $G_n$ is $\\{(a_i,b_j)|i=1,\\ldots,n;j=1,\\ldots,4\\}$. In [Discrete Appl. Math. 320 (2022) 387--407], Evans and Francis proposed the following conjecture on resistance distances of $G_n$ and $G_{n+1}$: \\begin{equation*} \\lim_{n \\rightarrow \\infty}\\left(R_{G_{n+1}}[(a_{1},b_1),(a_{n+1},b_3)]-R_{G_{n}}[(a_{1},b_1),(a_{n},b_3)]\\right)=\\frac{1}{4}. \\end{equation*}   In this paper, combing algebraic methods and electric network approaches, we confirm and further generalize this conjecture. In addition, we determine all the pair of vertices that reach the resistance diameter of $G_{n}$, which enables us to give an equivalent explanation of the conjecture.","sentences":["Let $G$ be a connected graph.","The resistance distance between two vertices $u$ and $v$ of $G$, denoted by $R_{G}[u,v]$, is defined as the net effective resistance between them in the electric network constructed from $G$ by replacing each edge with a unit resistor.","The resistance diameter of $G$, denoted by $D_{r}(G)$, is defined as the maximum resistance distance among all pairs of vertices of $G$. Let","$P_n=a_1a_2\\ldots a_n$ be the $n$-vertex path graph and $C_{4}=b_{1}b_2b_3b_4b_{1}$ be the 4-cycle.","Then the $n$-th block tower graph $G_n$ is defined as the the Cartesian product of $P_n$ and $C_4$, that is, $G_n=P_{n}\\square C_4$. Clearly, the vertex set of $G_n$ is $\\{(a_i,b_j)|i=1,\\ldots,n;j=1,\\ldots,4\\}$.","In [Discrete Appl.","Math. 320 (2022) 387--407], Evans and Francis proposed the following conjecture on resistance distances of $G_n$ and $G_{n+1}$: \\begin{equation*} \\lim_{n \\rightarrow \\infty}\\left(R_{G_{n+1}}[(a_{1},b_1),(a_{n+1},b_3)]-R_{G_{n}}[(a_{1},b_1),(a_{n},b_3)]\\right)=\\frac{1}{4}.","\\end{equation*}   In this paper, combing algebraic methods and electric network approaches, we confirm and further generalize this conjecture.","In addition, we determine all the pair of vertices that reach the resistance diameter of $G_{n}$, which enables us to give an equivalent explanation of the conjecture."],"url":"http://arxiv.org/abs/2406.04060v1","category":"math.CO"}
{"created":"2024-06-06 13:26:09","title":"First-order and Berezinskii-Kosterlitz-Thouless phase transitions in two-dimensional generalized XY models","abstract":"The aim of this paper is to illustrate that generalized two-dimensional XY models (proposed by Romano and Zagrebnov) may also support a first-order phase transition. Two approaches are employed to accurately determine the critical parameter $q$ at which such a transition takes place. Furthermore, we show that the model is characterized by three distinct regions concerning both first-order and Berezinskii-Kosterlitz-Thouless phase transitions. Finally, the underlying mechanisms governing such transitions are presented, along with an estimation of the critical temperatures.","sentences":["The aim of this paper is to illustrate that generalized two-dimensional XY models (proposed by Romano and Zagrebnov) may also support a first-order phase transition.","Two approaches are employed to accurately determine the critical parameter $q$ at which such a transition takes place.","Furthermore, we show that the model is characterized by three distinct regions concerning both first-order and Berezinskii-Kosterlitz-Thouless phase transitions.","Finally, the underlying mechanisms governing such transitions are presented, along with an estimation of the critical temperatures."],"url":"http://arxiv.org/abs/2406.04059v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-06 13:25:42","title":"Watching Popular Musicians Learn by Ear: A Hypothesis-Generating Study of Human-Recording Interactions in YouTube Videos","abstract":"Popular musicians often learn music by ear. It is unclear what role technology plays for those with experience at this task. In search of opportunities for the development of novel human-recording interactions, we analyze 18 YouTube videos depicting real-world examples of by-ear learning, and discuss why, during this preliminary phase of research, online videos are appropriate data. From our observations we generate hypotheses that can inform future work. For example, a musician's scope of learning may influence what technological interactions would help them, they could benefit from tools that accommodate their working memory, and transcription does not appear to play a key role in ear learning. Based on these findings, we pose a number of research questions, and discuss their methodological considerations to guide future study.","sentences":["Popular musicians often learn music by ear.","It is unclear what role technology plays for those with experience at this task.","In search of opportunities for the development of novel human-recording interactions, we analyze 18 YouTube videos depicting real-world examples of by-ear learning, and discuss why, during this preliminary phase of research, online videos are appropriate data.","From our observations we generate hypotheses that can inform future work.","For example, a musician's scope of learning may influence what technological interactions would help them, they could benefit from tools that accommodate their working memory, and transcription does not appear to play a key role in ear learning.","Based on these findings, we pose a number of research questions, and discuss their methodological considerations to guide future study."],"url":"http://arxiv.org/abs/2406.04058v1","category":"cs.HC"}
{"created":"2024-06-06 13:21:28","title":"Leveraging SPD Matrices on Riemannian Manifolds in Quantum Classical Hybrid Models for Structural Health Monitoring","abstract":"Realtime finite element modeling of bridges assists modern structural health monitoring systems by providing comprehensive insights into structural integrity. This capability is essential for ensuring the safe operation of bridges and preventing sudden catastrophic failures. However, FEM computational cost and the need for realtime analysis pose significant challenges. Additionally, the input data is a 7 dimensional vector, while the output is a 1017 dimensional vector, making accurate and efficient analysis particularly difficult. In this study, we propose a novel hybrid quantum classical Multilayer Perceptron pipeline leveraging Symmetric Positive Definite matrices and Riemannian manifolds for effective data representation. To maintain the integrity of the qubit structure, we utilize SPD matrices, ensuring data representation is well aligned with the quantum computational framework. Additionally, the method leverages polynomial feature expansion to capture nonlinear relationships within the data. The proposed pipeline combines classical fully connected neural network layers with quantum circuit layers to enhance model performance and efficiency. Our experiments focused on various configurations of such hybrid models to identify the optimal structure for accurate and efficient realtime analysis. The best performing model achieved a Mean Squared Error of 0.00031, significantly outperforming traditional methods.","sentences":["Realtime finite element modeling of bridges assists modern structural health monitoring systems by providing comprehensive insights into structural integrity.","This capability is essential for ensuring the safe operation of bridges and preventing sudden catastrophic failures.","However, FEM computational cost and the need for realtime analysis pose significant challenges.","Additionally, the input data is a 7 dimensional vector, while the output is a 1017 dimensional vector, making accurate and efficient analysis particularly difficult.","In this study, we propose a novel hybrid quantum classical Multilayer Perceptron pipeline leveraging Symmetric Positive Definite matrices and Riemannian manifolds for effective data representation.","To maintain the integrity of the qubit structure, we utilize SPD matrices, ensuring data representation is well aligned with the quantum computational framework.","Additionally, the method leverages polynomial feature expansion to capture nonlinear relationships within the data.","The proposed pipeline combines classical fully connected neural network layers with quantum circuit layers to enhance model performance and efficiency.","Our experiments focused on various configurations of such hybrid models to identify the optimal structure for accurate and efficient realtime analysis.","The best performing model achieved a Mean Squared Error of 0.00031, significantly outperforming traditional methods."],"url":"http://arxiv.org/abs/2406.04055v1","category":"cs.LG"}
{"created":"2024-06-06 13:20:54","title":"Dynamics-based halo model for large scale structure","abstract":"Accurate modelling of the one-to-two halo transition has long been difficult to achieve. We demonstrate that physically motivated halo definitions that respect the bimodal phase-space distribution of dark matter particles near halos resolves this difficulty. Specifically, the two phase-space components are overlapping and correspond to: 1) particles \\it orbiting \\rm the halo; and 2) particles \\it infalling \\rm into the halo for the first time. Motivated by this decomposition, Garc\\'ia [R. Garc\\'ia et. al., MNRAS 521, 2464 (2023)] advocated for defining haloes as the collection of particles orbiting their self-generated potential. This definition identifies the traditional one-halo term of the halo--mass correlation function with the distribution of orbiting particles around a halo, while the two-halo term governs the distribution of infalling particles. We use dark matter simulations to demonstrate that the distribution of orbiting particles is finite and can be characterised by a single physical scale $r_{\\rm h}$, which we refer to as the \\it halo radius. \\rm The two-halo term is described using a simple yet accurate empirical model based on the Zel'dovich correlation function. We further demonstrate that the halo radius imprints itself on the distribution of infalling particles at small scales. Our final model for the halo--mass correlation function is accurate at the $\\approx 2\\%$ level for $r \\in [0.1,50]\\ h^{-1}\\ Mpc$. The Fourier transform of our best fit model describes the halo--mass power spectrum with comparable accuracy for $k\\in [0.06, 6.0]\\ h\\ Mpc^{-1}$.","sentences":["Accurate modelling of the one-to-two halo transition has long been difficult to achieve.","We demonstrate that physically motivated halo definitions that respect the bimodal phase-space distribution of dark matter particles near halos resolves this difficulty.","Specifically, the two phase-space components are overlapping and correspond to: 1) particles \\it orbiting \\rm the halo; and 2) particles \\it infalling \\rm into the halo for the first time.","Motivated by this decomposition, Garc\\'ia","[R. Garc\\'ia et. al., MNRAS 521, 2464 (2023)] advocated for defining haloes as the collection of particles orbiting their self-generated potential.","This definition identifies the traditional one-halo term of the halo--mass correlation function with the distribution of orbiting particles around a halo, while the two-halo term governs the distribution of infalling particles.","We use dark matter simulations to demonstrate that the distribution of orbiting particles is finite and can be characterised by a single physical scale $r_{\\rm h}$, which we refer to as the \\it halo radius.","\\rm The two-halo term is described using a simple yet accurate empirical model based on the Zel'dovich correlation function.","We further demonstrate that the halo radius imprints itself on the distribution of infalling particles at small scales.","Our final model for the halo--mass correlation function is accurate at the $\\approx 2\\%$ level for $r \\in","[0.1,50]\\ h^{-1}\\ Mpc$.","The Fourier transform of our best fit model describes the halo--mass power spectrum with comparable accuracy for $k\\in [0.06, 6.0]\\ h\\ Mpc^{-1}$."],"url":"http://arxiv.org/abs/2406.04054v1","category":"astro-ph.CO"}
{"created":"2024-06-06 13:17:44","title":"Multivector Neurons: Better and Faster O(n)-Equivariant Clifford Graph Neural Networks","abstract":"Most current deep learning models equivariant to $O(n)$ or $SO(n)$ either consider mostly scalar information such as distances and angles or have a very high computational complexity. In this work, we test a few novel message passing graph neural networks (GNNs) based on Clifford multivectors, structured similarly to other prevalent equivariant models in geometric deep learning. Our approach leverages efficient invariant scalar features while simultaneously performing expressive learning on multivector representations, particularly through the use of the equivariant geometric product operator. By integrating these elements, our methods outperform established efficient baseline models on an N-Body simulation task and protein denoising task while maintaining a high efficiency. In particular, we push the state-of-the-art error on the N-body dataset to 0.0035 (averaged over 3 runs); an 8% improvement over recent methods. Our implementation is available on Github.","sentences":["Most current deep learning models equivariant to $O(n)$ or $SO(n)$ either consider mostly scalar information such as distances and angles or have a very high computational complexity.","In this work, we test a few novel message passing graph neural networks (GNNs) based on Clifford multivectors, structured similarly to other prevalent equivariant models in geometric deep learning.","Our approach leverages efficient invariant scalar features while simultaneously performing expressive learning on multivector representations, particularly through the use of the equivariant geometric product operator.","By integrating these elements, our methods outperform established efficient baseline models on an N-Body simulation task and protein denoising task while maintaining a high efficiency.","In particular, we push the state-of-the-art error on the N-body dataset to 0.0035 (averaged over 3 runs); an 8% improvement over recent methods.","Our implementation is available on Github."],"url":"http://arxiv.org/abs/2406.04052v1","category":"cs.LG"}
{"created":"2024-06-06 13:17:24","title":"Continuously non-extendable mappings between generalized complex ellipsoids of different dimensions","abstract":"There exists a proper holomorphic mapping between balls of different dimensions such that it does not extend continuously to the boundary. The aim of this paper is to show the same phenomenon occurs for pseudoconvex domains of different dimensions.","sentences":["There exists a proper holomorphic mapping between balls of different dimensions such that it does not extend continuously to the boundary.","The aim of this paper is to show the same phenomenon occurs for pseudoconvex domains of different dimensions."],"url":"http://arxiv.org/abs/2406.04051v1","category":"math.CV"}
{"created":"2024-06-06 13:17:01","title":"Unraveling the mysteries of wormhole formation in Rastall-Rainbow gravity: A comprehensive study using the embedding approach","abstract":"The present work looks for the possible existence of static and spherically symmetric wormhole geometries in Rastall-Rainbow gravity. Since, the Rastall-Rainbow gravity model has been constructed with the combination of Rastall theory and the gravity's rainbow formalism. Taking advantage of the Karmarkar condition for embedding class one metrics, we solve the modified field equations analytically that describe wormholes for specific choice of redshift function. For specific parameter ranges, the solution represents a traversable wormhole that exhibits the violation of null energy condition and consequently the weak energy condition also. Furthermore, we focus on the wormhole stability via adiabatic sound velocity analysis. This model establishes a strong connection between two model parameters, namely, the Rastall parameters and the Rainbow functions, and how it affects the wormhole solution.","sentences":["The present work looks for the possible existence of static and spherically symmetric wormhole geometries in Rastall-Rainbow gravity.","Since, the Rastall-Rainbow gravity model has been constructed with the combination of Rastall theory and the gravity's rainbow formalism.","Taking advantage of the Karmarkar condition for embedding class one metrics, we solve the modified field equations analytically that describe wormholes for specific choice of redshift function.","For specific parameter ranges, the solution represents a traversable wormhole that exhibits the violation of null energy condition and consequently the weak energy condition also.","Furthermore, we focus on the wormhole stability via adiabatic sound velocity analysis.","This model establishes a strong connection between two model parameters, namely, the Rastall parameters and the Rainbow functions, and how it affects the wormhole solution."],"url":"http://arxiv.org/abs/2406.04049v1","category":"gr-qc"}
{"created":"2024-06-06 13:15:37","title":"ActionReasoningBench: Reasoning about Actions with and without Ramification Constraints","abstract":"Reasoning about actions and change (RAC) has historically driven the development of many early AI challenges, such as the frame problem, and many AI disciplines, including non-monotonic and commonsense reasoning. The role of RAC remains important even now, particularly for tasks involving dynamic environments, interactive scenarios, and commonsense reasoning. Despite the progress of Large Language Models (LLMs) in various AI domains, their performance on RAC is underexplored. To address this gap, we introduce a new benchmark, ActionReasoningBench, encompassing 13 domains and rigorously evaluating LLMs across eight different areas of RAC. These include - Object Tracking, Fluent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical RAC, Hallucination Detection, and Composite Questions. Furthermore, we also investigate the indirect effect of actions due to ramification constraints for every domain. Finally, we evaluate our benchmark using open-sourced and commercial state-of-the-art LLMs, including GPT-4o, Gemini-1.0-Pro, Llama2-7b-chat, Llama2-13b-chat, Llama3-8b-instruct, Gemma-2b-instruct, and Gemma-7b-instruct. Our findings indicate that these models face significant challenges across all categories included in our benchmark.","sentences":["Reasoning about actions and change (RAC) has historically driven the development of many early AI challenges, such as the frame problem, and many AI disciplines, including non-monotonic and commonsense reasoning.","The role of RAC remains important even now, particularly for tasks involving dynamic environments, interactive scenarios, and commonsense reasoning.","Despite the progress of Large Language Models (LLMs) in various AI domains, their performance on RAC is underexplored.","To address this gap, we introduce a new benchmark, ActionReasoningBench, encompassing 13 domains and rigorously evaluating LLMs across eight different areas of RAC.","These include - Object Tracking, Fluent Tracking, State Tracking, Action Executability, Effects of Actions, Numerical RAC, Hallucination Detection, and Composite Questions.","Furthermore, we also investigate the indirect effect of actions due to ramification constraints for every domain.","Finally, we evaluate our benchmark using open-sourced and commercial state-of-the-art LLMs, including GPT-4o, Gemini-1.0-Pro, Llama2-7b-chat, Llama2-13b-chat, Llama3-8b-instruct, Gemma-2b-instruct, and Gemma-7b-instruct.","Our findings indicate that these models face significant challenges across all categories included in our benchmark."],"url":"http://arxiv.org/abs/2406.04046v1","category":"cs.CC"}
{"created":"2024-06-06 13:15:37","title":"Slicing Mutual Information Generalization Bounds for Neural Networks","abstract":"The ability of machine learning (ML) algorithms to generalize well to unseen data has been studied through the lens of information theory, by bounding the generalization error with the input-output mutual information (MI), i.e., the MI between the training data and the learned hypothesis. Yet, these bounds have limited practicality for modern ML applications (e.g., deep learning), due to the difficulty of evaluating MI in high dimensions. Motivated by recent findings on the compressibility of neural networks, we consider algorithms that operate by slicing the parameter space, i.e., trained on random lower-dimensional subspaces. We introduce new, tighter information-theoretic generalization bounds tailored for such algorithms, demonstrating that slicing improves generalization. Our bounds offer significant computational and statistical advantages over standard MI bounds, as they rely on scalable alternative measures of dependence, i.e., disintegrated mutual information and $k$-sliced mutual information. Then, we extend our analysis to algorithms whose parameters do not need to exactly lie on random subspaces, by leveraging rate-distortion theory. This strategy yields generalization bounds that incorporate a distortion term measuring model compressibility under slicing, thereby tightening existing bounds without compromising performance or requiring model compression. Building on this, we propose a regularization scheme enabling practitioners to control generalization through compressibility. Finally, we empirically validate our results and achieve the computation of non-vacuous information-theoretic generalization bounds for neural networks, a task that was previously out of reach.","sentences":["The ability of machine learning (ML) algorithms to generalize well to unseen data has been studied through the lens of information theory, by bounding the generalization error with the input-output mutual information (MI), i.e., the MI between the training data and the learned hypothesis.","Yet, these bounds have limited practicality for modern ML applications (e.g., deep learning), due to the difficulty of evaluating MI in high dimensions.","Motivated by recent findings on the compressibility of neural networks, we consider algorithms that operate by slicing the parameter space, i.e., trained on random lower-dimensional subspaces.","We introduce new, tighter information-theoretic generalization bounds tailored for such algorithms, demonstrating that slicing improves generalization.","Our bounds offer significant computational and statistical advantages over standard MI bounds, as they rely on scalable alternative measures of dependence, i.e., disintegrated mutual information and $k$-sliced mutual information.","Then, we extend our analysis to algorithms whose parameters do not need to exactly lie on random subspaces, by leveraging rate-distortion theory.","This strategy yields generalization bounds that incorporate a distortion term measuring model compressibility under slicing, thereby tightening existing bounds without compromising performance or requiring model compression.","Building on this, we propose a regularization scheme enabling practitioners to control generalization through compressibility.","Finally, we empirically validate our results and achieve the computation of non-vacuous information-theoretic generalization bounds for neural networks, a task that was previously out of reach."],"url":"http://arxiv.org/abs/2406.04047v1","category":"stat.ML"}
{"created":"2024-06-06 13:12:28","title":"Holographic stress tensor correlators on higher genus Riemann surfaces","abstract":"In this work, we present a comprehensive study of holographic stress tensor correlators on general Riemann surfaces, extending beyond the previously well-studied torus cases to explore higher genus conformal field theories (CFTs) within the framework of the Anti-de Sitter/conformal field theory (AdS/CFT) correspondence. We develop a methodological approach to compute holographic stress tensor correlators, employing the Schottky uniformization technique to address the handlebody solutions for higher genus Riemann surfaces. Through rigorous calculations, we derive four-point stress tensor correlators, alongside recurrence relations for higher-point correlators, within the $\\mathrm{AdS}_3/\\mathrm{CFT}_2$ context. Additionally, our research delves into the holography of cutoff $\\mathrm{AdS}_3$ spaces, offering novel insights into the lower-point correlators of the $T\\bar{T}$-deformed theories on higher genus Riemann surfaces up to the first deformation order.","sentences":["In this work, we present a comprehensive study of holographic stress tensor correlators on general Riemann surfaces, extending beyond the previously well-studied torus cases to explore higher genus conformal field theories (CFTs) within the framework of the Anti-de Sitter/conformal field theory (AdS/CFT) correspondence.","We develop a methodological approach to compute holographic stress tensor correlators, employing the Schottky uniformization technique to address the handlebody solutions for higher genus Riemann surfaces.","Through rigorous calculations, we derive four-point stress tensor correlators, alongside recurrence relations for higher-point correlators, within the $\\mathrm{AdS}_3/\\mathrm{CFT}_2$ context.","Additionally, our research delves into the holography of cutoff $\\mathrm{AdS}_3$ spaces, offering novel insights into the lower-point correlators of the $T\\bar{T}$-deformed theories on higher genus Riemann surfaces up to the first deformation order."],"url":"http://arxiv.org/abs/2406.04042v1","category":"hep-th"}
{"created":"2024-06-06 13:10:37","title":"Linear Opinion Pooling for Uncertainty Quantification on Graphs","abstract":"We address the problem of uncertainty quantification for graph-structured data, or, more specifically, the problem to quantify the predictive uncertainty in (semi-supervised) node classification. Key questions in this regard concern the distinction between two different types of uncertainty, aleatoric and epistemic, and how to support uncertainty quantification by leveraging the structural information provided by the graph topology. Challenging assumptions and postulates of state-of-the-art methods, we propose a novel approach that represents (epistemic) uncertainty in terms of mixtures of Dirichlet distributions and refers to the established principle of linear opinion pooling for propagating information between neighbored nodes in the graph. The effectiveness of this approach is demonstrated in a series of experiments on a variety of graph-structured datasets.","sentences":["We address the problem of uncertainty quantification for graph-structured data, or, more specifically, the problem to quantify the predictive uncertainty in (semi-supervised) node classification.","Key questions in this regard concern the distinction between two different types of uncertainty, aleatoric and epistemic, and how to support uncertainty quantification by leveraging the structural information provided by the graph topology.","Challenging assumptions and postulates of state-of-the-art methods, we propose a novel approach that represents (epistemic) uncertainty in terms of mixtures of Dirichlet distributions and refers to the established principle of linear opinion pooling for propagating information between neighbored nodes in the graph.","The effectiveness of this approach is demonstrated in a series of experiments on a variety of graph-structured datasets."],"url":"http://arxiv.org/abs/2406.04041v1","category":"cs.LG"}
{"created":"2024-06-06 13:07:42","title":"Precise measurement of light-quark electroweak couplings at future colliders","abstract":"Electroweak Precision Measurements are stringent tests of the Standard Model and sensitive probes to New Physics. Accurate studies of the Z-boson couplings to the first-generation quarks could reveal potential discrepancies between the fundamental theory and experimental data. Future lepton colliders offering high statistics of Z bosons would be an excellent tool to perform such a measurement based on comparison of radiative and non-radiative hadronic decays of the Z boson. Due to the difference in quark charge, the relative contribution of the events with final-state radiation (FSR) directly reflects the ratio of up- and down-type quark decays. Such an analysis requires a proper distinction between photons coming from different sources, including initial-state radiation (ISR), FSR, parton showers and hadronisation. In our talk, we will show how to extract the values of the Z couplings to quarks and present preliminary results of the analysis for ILC.","sentences":["Electroweak Precision Measurements are stringent tests of the Standard Model and sensitive probes to New Physics.","Accurate studies of the Z-boson couplings to the first-generation quarks could reveal potential discrepancies between the fundamental theory and experimental data.","Future lepton colliders offering high statistics of Z bosons would be an excellent tool to perform such a measurement based on comparison of radiative and non-radiative hadronic decays of the Z boson.","Due to the difference in quark charge, the relative contribution of the events with final-state radiation (FSR) directly reflects the ratio of up- and down-type quark decays.","Such an analysis requires a proper distinction between photons coming from different sources, including initial-state radiation (ISR), FSR, parton showers and hadronisation.","In our talk, we will show how to extract the values of the Z couplings to quarks and present preliminary results of the analysis for ILC."],"url":"http://arxiv.org/abs/2406.04040v1","category":"hep-ph"}
{"created":"2024-06-06 13:05:32","title":"Shaping History: Advanced Machine Learning Techniques for the Analysis and Dating of Cuneiform Tablets over Three Millennia","abstract":"Cuneiform tablets, emerging in ancient Mesopotamia around the late fourth millennium BCE, represent one of humanity's earliest writing systems. Characterized by wedge-shaped marks on clay tablets, these artifacts provided insight into Mesopotamian civilization across various domains. Traditionally, the analysis and dating of these tablets rely on subjective assessment of shape and writing style, leading to uncertainties in pinpointing their exact temporal origins. Recent advances in digitization have revolutionized the study of cuneiform by enhancing accessibility and analytical capabilities. Our research uniquely focuses on the silhouette of tablets as significant indicators of their historical periods, diverging from most studies that concentrate on textual content. Utilizing an unprecedented dataset of over 94,000 images from the Cuneiform Digital Library Initiative collection, we apply deep learning methods to classify cuneiform tablets, covering over 3,000 years of history. By leveraging statistical, computational techniques, and generative modeling through Variational Auto-Encoders (VAEs), we achieve substantial advancements in the automatic classification of these ancient documents, focusing on the tablets' silhouettes as key predictors. Our classification approach begins with a Decision Tree using height-to-width ratios and culminates with a ResNet50 model, achieving a 61% macro F1-score for tablet silhouettes. Moreover, we introduce novel VAE-powered tools to enhance explainability and enable researchers to explore changes in tablet shapes across different eras and genres. This research contributes to document analysis and diplomatics by demonstrating the value of large-scale data analysis combined with statistical methods. These insights offer valuable tools for historians and epigraphists, enriching our understanding of cuneiform tablets and the cultures that produced them.","sentences":["Cuneiform tablets, emerging in ancient Mesopotamia around the late fourth millennium BCE, represent one of humanity's earliest writing systems.","Characterized by wedge-shaped marks on clay tablets, these artifacts provided insight into Mesopotamian civilization across various domains.","Traditionally, the analysis and dating of these tablets rely on subjective assessment of shape and writing style, leading to uncertainties in pinpointing their exact temporal origins.","Recent advances in digitization have revolutionized the study of cuneiform by enhancing accessibility and analytical capabilities.","Our research uniquely focuses on the silhouette of tablets as significant indicators of their historical periods, diverging from most studies that concentrate on textual content.","Utilizing an unprecedented dataset of over 94,000 images from the Cuneiform Digital Library Initiative collection, we apply deep learning methods to classify cuneiform tablets, covering over 3,000 years of history.","By leveraging statistical, computational techniques, and generative modeling through Variational Auto-Encoders (VAEs), we achieve substantial advancements in the automatic classification of these ancient documents, focusing on the tablets' silhouettes as key predictors.","Our classification approach begins with a Decision Tree using height-to-width ratios and culminates with a ResNet50 model, achieving a 61% macro F1-score for tablet silhouettes.","Moreover, we introduce novel VAE-powered tools to enhance explainability and enable researchers to explore changes in tablet shapes across different eras and genres.","This research contributes to document analysis and diplomatics by demonstrating the value of large-scale data analysis combined with statistical methods.","These insights offer valuable tools for historians and epigraphists, enriching our understanding of cuneiform tablets and the cultures that produced them."],"url":"http://arxiv.org/abs/2406.04039v1","category":"cs.CV"}
{"created":"2024-06-06 13:04:23","title":"A Road-Map for Transferring Software Engineering methods for Model-Based Early V&V of Behaviour to Systems Engineering","abstract":"In this paper we discuss the growing need for system behaviour to be validated and verified (V&V'ed) early in model-based systems engineering. Several aspects push companies towards integration of techniques, methods, and processes that promote specific and general V&V activities earlier to support more effective decision-making. As a result, there are incentives to introduce new technologies to remain competitive with the recently drastic changes in system complexity and heterogeneity. Performing V&V early on in development is a means of reducing risk for later error detection while moving key activities earlier in a process. We present a summary of the literature on early V&V and position existing challenges regarding potential solutions and future investigations. In particular, we reason that the software engineering community can act as a source for inspiration as many emerging technologies in the software domain are showing promise in the wider systems domain, and there already exist well formed methods for early V&V of software behaviour in the software modelling community. We conclude the paper with a road-map for future research and development for both researchers and practitioners to further develop the concepts discussed in the paper.","sentences":["In this paper we discuss the growing need for system behaviour to be validated and verified (V&V'ed) early in model-based systems engineering.","Several aspects push companies towards integration of techniques, methods, and processes that promote specific and general V&V activities earlier to support more effective decision-making.","As a result, there are incentives to introduce new technologies to remain competitive with the recently drastic changes in system complexity and heterogeneity.","Performing V&V early on in development is a means of reducing risk for later error detection while moving key activities earlier in a process.","We present a summary of the literature on early V&V and position existing challenges regarding potential solutions and future investigations.","In particular, we reason that the software engineering community can act as a source for inspiration as many emerging technologies in the software domain are showing promise in the wider systems domain, and there already exist well formed methods for early V&V of software behaviour in the software modelling community.","We conclude the paper with a road-map for future research and development for both researchers and practitioners to further develop the concepts discussed in the paper."],"url":"http://arxiv.org/abs/2406.04037v1","category":"cs.SE"}
{"created":"2024-06-06 13:03:51","title":"Spatio-temporal Early Prediction based on Multi-objective Reinforcement Learning","abstract":"Accuracy and timeliness are indeed often conflicting goals in prediction tasks. Premature predictions may yield a higher rate of false alarms, whereas delaying predictions to gather more information can render them too late to be useful. In applications such as wildfires, crimes, and traffic jams, timely predictions are vital for safeguarding human life and property. Consequently, finding a balance between accuracy and timeliness is crucial. In this paper, we propose a spatio-temporal early prediction model based on Multi-Objective reinforcement learning that can either implement an optimal policy given a preference or infer the preference based on a small number of samples. The model addresses two primary challenges: 1) enhancing the accuracy of early predictions and 2) providing the optimal policy for determining the most suitable prediction time for each area. Our method demonstrates superior performance on three large-scale real-world datasets, surpassing existing methods in early spatio-temporal prediction tasks.","sentences":["Accuracy and timeliness are indeed often conflicting goals in prediction tasks.","Premature predictions may yield a higher rate of false alarms, whereas delaying predictions to gather more information can render them too late to be useful.","In applications such as wildfires, crimes, and traffic jams, timely predictions are vital for safeguarding human life and property.","Consequently, finding a balance between accuracy and timeliness is crucial.","In this paper, we propose a spatio-temporal early prediction model based on Multi-Objective reinforcement learning that can either implement an optimal policy given a preference or infer the preference based on a small number of samples.","The model addresses two primary challenges: 1) enhancing the accuracy of early predictions and 2) providing the optimal policy for determining the most suitable prediction time for each area.","Our method demonstrates superior performance on three large-scale real-world datasets, surpassing existing methods in early spatio-temporal prediction tasks."],"url":"http://arxiv.org/abs/2406.04035v1","category":"cs.LG"}
{"created":"2024-06-06 13:03:20","title":"The geometry of intersecting codes and applications to additive combinatorics and factorization theory","abstract":"Intersecting codes are linear codes where every two nonzero codewords have non-trivially intersecting support. In this article we expand on the theory of this family of codes, by showing that nondegenerate intersecting codes correspond to sets of points (with multiplicites) in a projective space that are not contained in two hyperplanes. This correspondence allows the use of geometric arguments to demonstrate properties and provide constructions of intersecting codes. We improve on existing bounds on their length and provide explicit constructions of short intersecting codes. Finally, generalizing a link between coding theory and the theory of the Davenport constant (a combinatorial invariant of finite abelian groups), we provide new asymptotic bounds on the weighted $2$-wise Davenport constant. These bounds then yield results on factorizations in rings of algebraic integers and related structures.","sentences":["Intersecting codes are linear codes where every two nonzero codewords have non-trivially intersecting support.","In this article we expand on the theory of this family of codes, by showing that nondegenerate intersecting codes correspond to sets of points (with multiplicites) in a projective space that are not contained in two hyperplanes.","This correspondence allows the use of geometric arguments to demonstrate properties and provide constructions of intersecting codes.","We improve on existing bounds on their length and provide explicit constructions of short intersecting codes.","Finally, generalizing a link between coding theory and the theory of the Davenport constant (a combinatorial invariant of finite abelian groups), we provide new asymptotic bounds on the weighted $2$-wise Davenport constant.","These bounds then yield results on factorizations in rings of algebraic integers and related structures."],"url":"http://arxiv.org/abs/2406.04034v1","category":"math.CO"}
{"created":"2024-06-06 13:03:08","title":"Enumerating Galois extensions of number fields","abstract":"Let $k$ be a number field. We provide an asymptotic formula for the number of Galois extensions of $k$ with absolute discriminant bounded by some $X \\geq 1$, as $X\\to\\infty$. We also provide an asymptotic formula for the closely related count of extensions $K/k$ whose normal closure has discriminant bounded by $X$. The key behind these results is a new upper bound on the number of Galois extensions of $k$ with a given Galois group $G$ and discriminant bounded by $X$; we show the number of such extensions is $O_{[k:\\mathbb{Q}],G} (X^{ \\frac{4}{\\sqrt{|G|}}})$. This improves over the previous best bound $O_{k,G,\\epsilon}(X^{\\frac{3}{8}+\\epsilon})$ due to Ellenberg and Venkatesh. In particular, ours is the first bound for general $G$ with an exponent that decays as $|G| \\to \\infty$.","sentences":["Let $k$ be a number field.","We provide an asymptotic formula for the number of Galois extensions of $k$ with absolute discriminant bounded by some $X \\geq 1$, as $X\\to\\infty$. We also provide an asymptotic formula for the closely related count of extensions $K/k$ whose normal closure has discriminant bounded by $X$. The key behind these results is a new upper bound on the number of Galois extensions of $k$ with a given Galois group $G$ and discriminant bounded by $X$; we show the number of such extensions is $O_{[k:\\mathbb{Q}],G} (X^{ \\frac{4}{\\sqrt{|G|}}})$. This improves over the previous best bound $O_{k,G,\\epsilon}(X^{\\frac{3}{8}+\\epsilon})$ due to Ellenberg and Venkatesh.","In particular, ours is the first bound for general $G$ with an exponent that decays as $|G| \\to \\infty$."],"url":"http://arxiv.org/abs/2406.04033v1","category":"math.NT"}
{"created":"2024-06-06 13:02:00","title":"Zero-Painter: Training-Free Layout Control for Text-to-Image Synthesis","abstract":"We present Zero-Painter, a novel training-free framework for layout-conditional text-to-image synthesis that facilitates the creation of detailed and controlled imagery from textual prompts. Our method utilizes object masks and individual descriptions, coupled with a global text prompt, to generate images with high fidelity. Zero-Painter employs a two-stage process involving our novel Prompt-Adjusted Cross-Attention (PACA) and Region-Grouped Cross-Attention (ReGCA) blocks, ensuring precise alignment of generated objects with textual prompts and mask shapes. Our extensive experiments demonstrate that Zero-Painter surpasses current state-of-the-art methods in preserving textual details and adhering to mask shapes.","sentences":["We present Zero-Painter, a novel training-free framework for layout-conditional text-to-image synthesis that facilitates the creation of detailed and controlled imagery from textual prompts.","Our method utilizes object masks and individual descriptions, coupled with a global text prompt, to generate images with high fidelity.","Zero-Painter employs a two-stage process involving our novel Prompt-Adjusted Cross-Attention (PACA) and Region-Grouped Cross-Attention (ReGCA) blocks, ensuring precise alignment of generated objects with textual prompts and mask shapes.","Our extensive experiments demonstrate that Zero-Painter surpasses current state-of-the-art methods in preserving textual details and adhering to mask shapes."],"url":"http://arxiv.org/abs/2406.04032v1","category":"cs.CV"}
{"created":"2024-06-06 13:00:42","title":"Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt","abstract":"In the realm of large vision language models (LVLMs), jailbreak attacks serve as a red-teaming approach to bypass guardrails and uncover safety implications. Existing jailbreaks predominantly focus on the visual modality, perturbing solely visual inputs in the prompt for attacks. However, they fall short when confronted with aligned models that fuse visual and textual features simultaneously for generation. To address this limitation, this paper introduces the Bi-Modal Adversarial Prompt Attack (BAP), which executes jailbreaks by optimizing textual and visual prompts cohesively. Initially, we adversarially embed universally harmful perturbations in an image, guided by a few-shot query-agnostic corpus (e.g., affirmative prefixes and negative inhibitions). This process ensures that image prompt LVLMs to respond positively to any harmful queries. Subsequently, leveraging the adversarial image, we optimize textual prompts with specific harmful intent. In particular, we utilize a large language model to analyze jailbreak failures and employ chain-of-thought reasoning to refine textual prompts through a feedback-iteration manner. To validate the efficacy of our approach, we conducted extensive evaluations on various datasets and LVLMs, demonstrating that our method significantly outperforms other methods by large margins (+29.03% in attack success rate on average). Additionally, we showcase the potential of our attacks on black-box commercial LVLMs, such as Gemini and ChatGLM.","sentences":["In the realm of large vision language models (LVLMs), jailbreak attacks serve as a red-teaming approach to bypass guardrails and uncover safety implications.","Existing jailbreaks predominantly focus on the visual modality, perturbing solely visual inputs in the prompt for attacks.","However, they fall short when confronted with aligned models that fuse visual and textual features simultaneously for generation.","To address this limitation, this paper introduces the Bi-Modal Adversarial Prompt Attack (BAP), which executes jailbreaks by optimizing textual and visual prompts cohesively.","Initially, we adversarially embed universally harmful perturbations in an image, guided by a few-shot query-agnostic corpus (e.g., affirmative prefixes and negative inhibitions).","This process ensures that image prompt LVLMs to respond positively to any harmful queries.","Subsequently, leveraging the adversarial image, we optimize textual prompts with specific harmful intent.","In particular, we utilize a large language model to analyze jailbreak failures and employ chain-of-thought reasoning to refine textual prompts through a feedback-iteration manner.","To validate the efficacy of our approach, we conducted extensive evaluations on various datasets and LVLMs, demonstrating that our method significantly outperforms other methods by large margins (+29.03% in attack success rate on average).","Additionally, we showcase the potential of our attacks on black-box commercial LVLMs, such as Gemini and ChatGLM."],"url":"http://arxiv.org/abs/2406.04031v1","category":"cs.CV"}
{"created":"2024-06-06 12:59:46","title":"Pre-trained Transformer Uncovers Meaningful Patterns in Human Mobility Data","abstract":"We empirically demonstrate that a transformer pre-trained on country-scale unlabeled human mobility data learns embeddings capable, through fine-tuning, of developing a deep understanding of the target geography and its corresponding mobility patterns. Utilizing an adaptation framework, we evaluate the performance of our pre-trained embeddings in encapsulating a broad spectrum of concepts directly and indirectly related to human mobility. This includes basic notions, such as geographic location and distance, and extends to more complex constructs, such as administrative divisions and land cover. Our extensive empirical analysis reveals a substantial performance boost gained from pre-training, reaching up to 38% in tasks such as tree-cover regression. We attribute this result to the ability of the pre-training to uncover meaningful patterns hidden in the raw data, beneficial for modeling relevant high-level concepts. The pre-trained embeddings emerge as robust representations of regions and trajectories, potentially valuable for a wide range of downstream applications.","sentences":["We empirically demonstrate that a transformer pre-trained on country-scale unlabeled human mobility data learns embeddings capable, through fine-tuning, of developing a deep understanding of the target geography and its corresponding mobility patterns.","Utilizing an adaptation framework, we evaluate the performance of our pre-trained embeddings in encapsulating a broad spectrum of concepts directly and indirectly related to human mobility.","This includes basic notions, such as geographic location and distance, and extends to more complex constructs, such as administrative divisions and land cover.","Our extensive empirical analysis reveals a substantial performance boost gained from pre-training, reaching up to 38% in tasks such as tree-cover regression.","We attribute this result to the ability of the pre-training to uncover meaningful patterns hidden in the raw data, beneficial for modeling relevant high-level concepts.","The pre-trained embeddings emerge as robust representations of regions and trajectories, potentially valuable for a wide range of downstream applications."],"url":"http://arxiv.org/abs/2406.04029v1","category":"cs.CY"}
{"created":"2024-06-06 12:57:31","title":"Contrastive Sparse Autoencoders for Interpreting Planning of Chess-Playing Agents","abstract":"AI led chess systems to a superhuman level, yet these systems heavily rely on black-box algorithms. This is unsustainable in ensuring transparency to the end-user, particularly when these systems are responsible for sensitive decision-making. Recent interpretability work has shown that the inner representations of Deep Neural Networks (DNNs) were fathomable and contained human-understandable concepts. Yet, these methods are seldom contextualised and are often based on a single hidden state, which makes them unable to interpret multi-step reasoning, e.g. planning. In this respect, we propose contrastive sparse autoencoders (CSAE), a novel framework for studying pairs of game trajectories. Using CSAE, we are able to extract and interpret concepts that are meaningful to the chess-agent plans. We primarily focused on a qualitative analysis of the CSAE features before proposing an automated feature taxonomy. Furthermore, to evaluate the quality of our trained CSAE, we devise sanity checks to wave spurious correlations in our results.","sentences":["AI led chess systems to a superhuman level, yet these systems heavily rely on black-box algorithms.","This is unsustainable in ensuring transparency to the end-user, particularly when these systems are responsible for sensitive decision-making.","Recent interpretability work has shown that the inner representations of Deep Neural Networks (DNNs) were fathomable and contained human-understandable concepts.","Yet, these methods are seldom contextualised and are often based on a single hidden state, which makes them unable to interpret multi-step reasoning, e.g. planning.","In this respect, we propose contrastive sparse autoencoders (CSAE), a novel framework for studying pairs of game trajectories.","Using CSAE, we are able to extract and interpret concepts that are meaningful to the chess-agent plans.","We primarily focused on a qualitative analysis of the CSAE features before proposing an automated feature taxonomy.","Furthermore, to evaluate the quality of our trained CSAE, we devise sanity checks to wave spurious correlations in our results."],"url":"http://arxiv.org/abs/2406.04028v1","category":"cs.AI"}
{"created":"2024-06-06 12:55:50","title":"PowerPeeler: A Precise and General Dynamic Deobfuscation Method for PowerShell Scripts","abstract":"PowerShell is a powerful and versatile task automation tool. Unfortunately, it is also widely abused by cyber attackers. To bypass malware detection and hinder threat analysis, attackers often employ diverse techniques to obfuscate malicious PowerShell scripts. Existing deobfuscation tools suffer from the limitation of static analysis, which fails to simulate the real deobfuscation process accurately.   In this paper, we propose PowerPeeler. To the best of our knowledge, it is the first dynamic PowerShell script deobfuscation approach at the instruction level. It utilizes expression-related Abstract Syntax Tree (AST) nodes to identify potential obfuscated script pieces. Then, PowerPeeler correlates the AST nodes with their corresponding instructions and monitors the script's entire execution process. Subsequently, PowerPeeler dynamically tracks the execution of these instructions and records their execution results. Finally, PowerPeeler stringifies these results to replace the corresponding obfuscated script pieces and reconstruct the deobfuscated script.   To evaluate the effectiveness of PowerPeeler, we collect 1,736,669 real-world malicious PowerShell samples with diversity obfuscation methods. We compare PowerPeeler with five state-of-the-art deobfuscation tools and GPT-4. The evaluation results demonstrate that PowerPeeler can effectively handle all well-known obfuscation methods. Additionally, the deobfuscation correctness rate of PowerPeeler reaches 95%, significantly surpassing that of other tools. PowerPeeler not only recovers the highest amount of sensitive data but also maintains a semantic consistency over 97%, which is also the best. Moreover, PowerPeeler effectively obtains the largest quantity of valid deobfuscated results within a limited time frame. Furthermore, PowerPeeler is extendable and can be used as a helpful tool for other cyber security solutions.","sentences":["PowerShell is a powerful and versatile task automation tool.","Unfortunately, it is also widely abused by cyber attackers.","To bypass malware detection and hinder threat analysis, attackers often employ diverse techniques to obfuscate malicious PowerShell scripts.","Existing deobfuscation tools suffer from the limitation of static analysis, which fails to simulate the real deobfuscation process accurately.   ","In this paper, we propose PowerPeeler.","To the best of our knowledge, it is the first dynamic PowerShell script deobfuscation approach at the instruction level.","It utilizes expression-related Abstract Syntax Tree (AST) nodes to identify potential obfuscated script pieces.","Then, PowerPeeler correlates the AST nodes with their corresponding instructions and monitors the script's entire execution process.","Subsequently, PowerPeeler dynamically tracks the execution of these instructions and records their execution results.","Finally, PowerPeeler stringifies these results to replace the corresponding obfuscated script pieces and reconstruct the deobfuscated script.   ","To evaluate the effectiveness of PowerPeeler, we collect 1,736,669 real-world malicious PowerShell samples with diversity obfuscation methods.","We compare PowerPeeler with five state-of-the-art deobfuscation tools and GPT-4.","The evaluation results demonstrate that PowerPeeler can effectively handle all well-known obfuscation methods.","Additionally, the deobfuscation correctness rate of PowerPeeler reaches 95%, significantly surpassing that of other tools.","PowerPeeler not only recovers the highest amount of sensitive data but also maintains a semantic consistency over 97%, which is also the best.","Moreover, PowerPeeler effectively obtains the largest quantity of valid deobfuscated results within a limited time frame.","Furthermore, PowerPeeler is extendable and can be used as a helpful tool for other cyber security solutions."],"url":"http://arxiv.org/abs/2406.04027v1","category":"cs.CR"}
{"created":"2024-06-06 12:46:21","title":"American Sign Language Handshapes Reflect Pressures for Communicative Efficiency","abstract":"Communicative efficiency is a prominent theory in linguistics and cognitive science. While numerous studies have shown how the pressure to save energy is reflected in the form of spoken languages, few have explored this phenomenon in signed languages. In this paper, we show how handshapes in American Sign Language (ASL) reflect these efficiency pressures and we present new evidence of communicative efficiency in the visual-gestural modality.   We focus on handshapes that are used in both native ASL signs and signs borrowed from English to compare efficiency pressures from both ASL and English. First, we design new methodologies to quantify the articulatory effort required to produce handshapes as well as the perceptual effort needed to recognize them. Then, we compare correlations between communicative effort and usage statistics in ASL and English. Our findings reveal that frequent ASL handshapes are easier to produce and that pressures for communicative efficiency mostly come from ASL usage, not from English lexical borrowing.","sentences":["Communicative efficiency is a prominent theory in linguistics and cognitive science.","While numerous studies have shown how the pressure to save energy is reflected in the form of spoken languages, few have explored this phenomenon in signed languages.","In this paper, we show how handshapes in American Sign Language (ASL) reflect these efficiency pressures and we present new evidence of communicative efficiency in the visual-gestural modality.   ","We focus on handshapes that are used in both native ASL signs and signs borrowed from English to compare efficiency pressures from both ASL and English.","First, we design new methodologies to quantify the articulatory effort required to produce handshapes as well as the perceptual effort needed to recognize them.","Then, we compare correlations between communicative effort and usage statistics in ASL and English.","Our findings reveal that frequent ASL handshapes are easier to produce and that pressures for communicative efficiency mostly come from ASL usage, not from English lexical borrowing."],"url":"http://arxiv.org/abs/2406.04024v1","category":"cs.CL"}
{"created":"2024-06-06 12:46:09","title":"Orbits of the hyperoctahedral group as Euclidean designs","abstract":"The hyperoctahedral group $H$ in $n$ dimensions (the Weyl group of Lie type $B_n$) is the subgroup of the orthogonal group generated by all transpositions of coordinates and reflections with respect to coordinate hyperplanes. A finite set ${\\cal X} \\subset \\mathbb{R}^n$ with a weight function $w: {\\cal X} \\rightarrow \\mathbb{R}^+$ is called a Euclidean $t$-design, if $$\\sum_{r \\in R} W_r \\overline{f}_{S_{r}} = \\sum_{{\\bf x} \\in {\\cal X}} w({\\bf x}) f({\\bf x})$$ holds for every polynomial $f$ of total degree at most $t$; here $R$ is the set of norms of the points in ${\\cal X}$, $W_r$ is the total weight of all elements of ${\\cal X}$ with norm $r$, $S_r$ is the $n$-dimensional sphere of radius $r$ centered at the origin, and $\\overline{f}_{S_{r}}$ is the average of $f$ over $S_{r}$.   Here we consider Euclidean designs which are supported by orbits of the hyperoctahedral group. Namely, we prove that any Euclidean design on a union of generalized hyperoctahedra has strength (maximum $t$ for which it is a Euclidean design) equal to 3, 5, or 7. We find explicit necessary and sufficient conditions for when this strength is 5 and for when it is 7. In order to establish our classification, we translate the above definition of Euclidean designs to a single equation for $t=5$, a set of three equations for $t=7$, and a set of seven equations for $t=9$.   Neumaier and Seidel (1988), as well as Delsarte and Seidel (1989), proved a Fisher-type inequality $|{\\cal X}| \\geq N(n,p,t)$ for the minimum size of a Euclidean $t$-design in $\\mathbb{R}^n$ on $p=|R|$ concentric spheres (assuming that the design is antipodal if $t$ is odd). A Euclidean design with exactly $N(n,p,t)$ points is called tight. We exhibit new examples of antipodal tight Euclidean designs, supported by orbits of the hyperoctahedral group, for $N(n,p,t)=$(3,2,5), (3,3,7), and (4,2,7).","sentences":["The hyperoctahedral group $H$ in $n$ dimensions (the Weyl group of Lie type $B_n$) is the subgroup of the orthogonal group generated by all transpositions of coordinates and reflections with respect to coordinate hyperplanes.","A finite set ${\\cal X} \\subset \\mathbb{R}^n$ with a weight function $w: {\\cal X} \\rightarrow \\mathbb{R}^+$ is called a Euclidean $t$-design, if $$\\sum_{r \\in R} W_r \\overline{f}_{S_{r}} = \\sum_{{\\bf x} \\in {\\cal X}} w({\\bf x}) f({\\bf x})$$ holds for every polynomial $f$ of total degree at most $t$; here $R$ is the set of norms of the points in ${\\cal X}$, $W_r$ is the total weight of all elements of ${\\cal X}$ with norm $r$, $S_r$ is the $n$-dimensional sphere of radius $r$ centered at the origin, and $\\overline{f}_{S_{r}}$ is the average of $f$ over $S_{r}$.   ","Here we consider Euclidean designs which are supported by orbits of the hyperoctahedral group.","Namely, we prove that any Euclidean design on a union of generalized hyperoctahedra has strength (maximum $t$ for which it is a Euclidean design) equal to 3, 5, or 7.","We find explicit necessary and sufficient conditions for when this strength is 5 and for when it is 7.","In order to establish our classification, we translate the above definition of Euclidean designs to a single equation for $t=5$, a set of three equations for $t=7$, and a set of seven equations for $t=9$.   Neumaier and Seidel (1988), as well as Delsarte and Seidel (1989), proved a Fisher-type inequality $|{\\cal X}| \\geq N(n,p,t)$ for the minimum size of a Euclidean $t$-design in $\\mathbb{R}^n$ on $p=|R|$ concentric spheres (assuming that the design is antipodal if $t$ is odd).","A Euclidean design with exactly $N(n,p,t)$ points is called tight.","We exhibit new examples of antipodal tight Euclidean designs, supported by orbits of the hyperoctahedral group, for $N(n,p,t)=$(3,2,5), (3,3,7), and (4,2,7)."],"url":"http://arxiv.org/abs/2406.04023v1","category":"math.CO"}
{"created":"2024-06-06 12:45:15","title":"An investigation of anisotropy in the bubbly turbulent flow via direct numerical simulations","abstract":"This study explores the dynamics of dispersed bubbly turbulent flow in a channel using interface-resolved direct numerical simulation (DNS) with an efficient Coupled Level-Set Volume-of-Fluid (CLSVOF) solver. The influence of number of bubbles (96 and 192), flow direction, and Eotvos number was examined across eight distinct cases. The results indicate that in upward flows, bubbles tend to accumulate near the wall, with smaller Eotvos numbers bringing them closer to the wall and enhancing energy dissipation through increased turbulence and vorticity. This proximity causes the liquid phase velocity to attenuate, and the bubbles, being more spherical, induce more isotropic turbulence. Conversely, in downward flows, bubbles cluster in the middle of the channel and induce additional pseudo-turbulence in the channel center, which induce additional turbulent kinetic energy in the channel center. The study further examines budget of Turbulent Kinetic Energy (TKE) and the exact balance equation for the Reynolds stresses, revealing that near-wall bubble motion generates substantial velocity gradients, particularly in the wall-normal direction, significantly impacting the turbulence structure.","sentences":["This study explores the dynamics of dispersed bubbly turbulent flow in a channel using interface-resolved direct numerical simulation (DNS) with an efficient Coupled Level-Set Volume-of-Fluid (CLSVOF) solver.","The influence of number of bubbles (96 and 192), flow direction, and Eotvos number was examined across eight distinct cases.","The results indicate that in upward flows, bubbles tend to accumulate near the wall, with smaller Eotvos numbers bringing them closer to the wall and enhancing energy dissipation through increased turbulence and vorticity.","This proximity causes the liquid phase velocity to attenuate, and the bubbles, being more spherical, induce more isotropic turbulence.","Conversely, in downward flows, bubbles cluster in the middle of the channel and induce additional pseudo-turbulence in the channel center, which induce additional turbulent kinetic energy in the channel center.","The study further examines budget of Turbulent Kinetic Energy (TKE) and the exact balance equation for the Reynolds stresses, revealing that near-wall bubble motion generates substantial velocity gradients, particularly in the wall-normal direction, significantly impacting the turbulence structure."],"url":"http://arxiv.org/abs/2406.04019v1","category":"physics.flu-dyn"}
{"created":"2024-06-06 12:42:30","title":"Self-resonance after inflation: The case of $\u03b1$-attractor models","abstract":"This work aims to explain the amplification of curvature perturbations on small scales arising from the phenomenon of self-resonance. By making a series expansion of the inflationary potential and employing perturbative techniques to solve for the inflaton field, we reformulate the Mukhanov-Sasaki equation into a Hill equation. Then, we derive expressions for the Floquet exponents corresponding to a potential having both cubic and quartic terms in its expansion. Our analytical results are then compared with numerical computations. Moreover, we propose potential applications including the generation of Primordial Black Holes, Scalar-Induced Gravitational Waves, and Oscillons.","sentences":["This work aims to explain the amplification of curvature perturbations on small scales arising from the phenomenon of self-resonance.","By making a series expansion of the inflationary potential and employing perturbative techniques to solve for the inflaton field, we reformulate the Mukhanov-Sasaki equation into a Hill equation.","Then, we derive expressions for the Floquet exponents corresponding to a potential having both cubic and quartic terms in its expansion.","Our analytical results are then compared with numerical computations.","Moreover, we propose potential applications including the generation of Primordial Black Holes, Scalar-Induced Gravitational Waves, and Oscillons."],"url":"http://arxiv.org/abs/2406.04017v1","category":"hep-th"}
{"created":"2024-06-06 12:37:10","title":"Beyond Diagonal RIS-Aided Networks: Performance Analysis and Sectorization Tradeoff","abstract":"Reconfigurable intelligent surfaces (RISs) have emerged as a spectrum- and energy-efficient technology to enhance the coverage of wireless communications within the upcoming 6G networks. Recently, novel extensions of this technology, referred to as multi-sector beyond diagonal RIS (BD-RIS), have been proposed, where the configurable elements are divided into $L$ sectors $(L \\geq 2)$ and arranged as a polygon prism, with each sector covering $1/L$ space. This paper presents a performance analysis of a multi-user communication system assisted by a multi-sector BD-RIS operating in time-switching (TS) mode. Specifically, we derive closed-form expressions for the moment-generating function (MGF), probability density function (PDF), and cumulative density function (CDF) of the signal-to-noise ratio (SNR) per user. Furthermore, closed-form expressions for the outage probability, achievable spectral and energy efficiency, symbol error probability, and diversity order for the proposed system model are derived. Moreover, a comparison is performed with the simultaneously transmitting and reflecting (STAR)-RISs, a special case of multi-sector BD-RIS with two sectors. Our analysis shows that for a fixed number of elements, increasing the sectors improves outage performance at the expense of reduced diversity order compared to STAR-RIS. This trade-off is influenced by the Rician factors of the cascaded channel and the number of configurable elements per sector. However, this superiority in slope is observed at outage probability values below $10^{-5}$, which remains below practical operating ranges of communication systems. Additionally, simulations are provided to validate the accuracy of our theoretical analyses showing a notable $182\\%$ increase in spectral efficiency and a $238\\%$ increase in energy efficiency when transitioning from a 2-sector to a 6-sector configuration.","sentences":["Reconfigurable intelligent surfaces (RISs) have emerged as a spectrum- and energy-efficient technology to enhance the coverage of wireless communications within the upcoming 6G networks.","Recently, novel extensions of this technology, referred to as multi-sector beyond diagonal RIS (BD-RIS), have been proposed, where the configurable elements are divided into $L$ sectors $(L \\geq 2)$ and arranged as a polygon prism, with each sector covering $1/L$ space.","This paper presents a performance analysis of a multi-user communication system assisted by a multi-sector BD-RIS operating in time-switching (TS) mode.","Specifically, we derive closed-form expressions for the moment-generating function (MGF), probability density function (PDF), and cumulative density function (CDF) of the signal-to-noise ratio (SNR) per user.","Furthermore, closed-form expressions for the outage probability, achievable spectral and energy efficiency, symbol error probability, and diversity order for the proposed system model are derived.","Moreover, a comparison is performed with the simultaneously transmitting and reflecting (STAR)-RISs, a special case of multi-sector BD-RIS with two sectors.","Our analysis shows that for a fixed number of elements, increasing the sectors improves outage performance at the expense of reduced diversity order compared to STAR-RIS.","This trade-off is influenced by the Rician factors of the cascaded channel and the number of configurable elements per sector.","However, this superiority in slope is observed at outage probability values below $10^{-5}$, which remains below practical operating ranges of communication systems.","Additionally, simulations are provided to validate the accuracy of our theoretical analyses showing a notable $182\\%$ increase in spectral efficiency and a $238\\%$ increase in energy efficiency when transitioning from a 2-sector to a 6-sector configuration."],"url":"http://arxiv.org/abs/2406.04009v1","category":"eess.SP"}
{"created":"2024-06-06 12:31:34","title":"Fe-MoS$_2$ nanoenzyme with photothermal enhanced enzyme activity for glucose colorimetric detection","abstract":"With the development of nanotechnology, it has been discovered that some nanomaterials have the activity of mimicking enzymes. This type of inorganic nanomaterial with characteristics similar to natural enzymes is called nanoenzyme. Compared with natural enzymes, nanoenzymes have advantages such as low deactivation, good stability, low production and storage costs, surface modification, and large-scale preparation. They have a wide range of applications in fields such as human health, environmental safety, and biosensing. MoS$_2$ can replace natural peroxidase to catalyze the decomposition of H$_2$O$_2$ and is considered to have peroxidase like activity, making it a typical nanoenzyme material. In addition, the Fenton reaction between Fe$_2^+$ and H$_2$O$_2$ can also cause H$_2$O$_2$ to decompose. Both MoS$_2$ and Fe$_2^+$ can cause the decomposition of H$_2$O$_2$, resulting in the production of hydroxyl radicals (OH). Hydroxyl radicals can catalyze the oxidation of chromogenic substrates 3,3',5,5'-tetramethylbenzidine (TMB), generating blue single electron oxidation products (oxTMB) and characteristic absorption peaks at 652 nm. Based on this characteristic, we doped Fe$_2^+$ into MoS$_2$ to obtain Fe-MoS$_2$ composite materials. Firstly, natural glucose oxidase is used to decompose glucose into glucose lactone and hydrogen peroxide. Then, Fe-MoS$_2$ composite material is added, and TMB is added dropwise to observe the blue turning of the mixed solution. Based on the above principle, colorimetric detection of glucose can be achieved. In addition, under infrared irradiation, the activity of peroxidase like enzymes in Fe-MoS$_2$ is enhanced.","sentences":["With the development of nanotechnology, it has been discovered that some nanomaterials have the activity of mimicking enzymes.","This type of inorganic nanomaterial with characteristics similar to natural enzymes is called nanoenzyme.","Compared with natural enzymes, nanoenzymes have advantages such as low deactivation, good stability, low production and storage costs, surface modification, and large-scale preparation.","They have a wide range of applications in fields such as human health, environmental safety, and biosensing.","MoS$_2$ can replace natural peroxidase to catalyze the decomposition of H$_2$O$_2$ and is considered to have peroxidase like activity, making it a typical nanoenzyme material.","In addition, the Fenton reaction between Fe$_2^+$ and H$_2$O$_2$ can also cause H$_2$O$_2$ to decompose.","Both MoS$_2$ and Fe$_2^+$ can cause the decomposition of H$_2$O$_2$, resulting in the production of hydroxyl radicals (OH).","Hydroxyl radicals can catalyze the oxidation of chromogenic substrates 3,3',5,5'-tetramethylbenzidine (TMB), generating blue single electron oxidation products (oxTMB) and characteristic absorption peaks at 652 nm.","Based on this characteristic, we doped Fe$_2^+$ into MoS$_2$ to obtain Fe-MoS$_2$ composite materials.","Firstly, natural glucose oxidase is used to decompose glucose into glucose lactone and hydrogen peroxide.","Then, Fe-MoS$_2$ composite material is added, and TMB is added dropwise to observe the blue turning of the mixed solution.","Based on the above principle, colorimetric detection of glucose can be achieved.","In addition, under infrared irradiation, the activity of peroxidase like enzymes in Fe-MoS$_2$ is enhanced."],"url":"http://arxiv.org/abs/2406.04006v1","category":"physics.chem-ph"}
{"created":"2024-06-06 12:26:14","title":"T-Count Optimizing Genetic Algorithm for Quantum State Preparation","abstract":"Quantum state preparation is a crucial process within numerous quantum algorithms, and the need for efficient initialization of quantum registers is ever increasing as demand for useful quantum computing grows. The problem arises as the number of qubits to be initialized grows, the circuits required to implement the desired state also exponentially increase in size leading to loss of fidelity to noise. This is mainly due to the susceptibility to environmental effects of the non-Clifford T gate, whose use should thus be reduced as much as possible. In this paper, we present and utilize a genetic algorithm for state preparation circuits consisting of gates from the Clifford + T gate set and optimize them in T-Count as to reduce the impact of noise. Whilst the method presented here does not always produce the most accurate circuits in terms of fidelity, it can generate high-fidelity, non-trivial quantum states such as quantum Fourier transform states. In addition, our algorithm does automatically generate fault tolerantly implementable solutions where the number of the most error prone components is reduced. We present an evaluation of the algorithm when trialed against preparing random, Poisson probability distribution, W, GHZ, and quantum Fourier transform states. We also experimentally demonstrate the scalability issues as qubit count increases, which highlights the need for further optimization of the search process.","sentences":["Quantum state preparation is a crucial process within numerous quantum algorithms, and the need for efficient initialization of quantum registers is ever increasing as demand for useful quantum computing grows.","The problem arises as the number of qubits to be initialized grows, the circuits required to implement the desired state also exponentially increase in size leading to loss of fidelity to noise.","This is mainly due to the susceptibility to environmental effects of the non-Clifford T gate, whose use should thus be reduced as much as possible.","In this paper, we present and utilize a genetic algorithm for state preparation circuits consisting of gates from the Clifford + T gate set and optimize them in T-Count as to reduce the impact of noise.","Whilst the method presented here does not always produce the most accurate circuits in terms of fidelity, it can generate high-fidelity, non-trivial quantum states such as quantum Fourier transform states.","In addition, our algorithm does automatically generate fault tolerantly implementable solutions where the number of the most error prone components is reduced.","We present an evaluation of the algorithm when trialed against preparing random, Poisson probability distribution, W, GHZ, and quantum Fourier transform states.","We also experimentally demonstrate the scalability issues as qubit count increases, which highlights the need for further optimization of the search process."],"url":"http://arxiv.org/abs/2406.04004v1","category":"quant-ph"}
{"created":"2024-06-06 12:19:55","title":"Stochastic logic in biased coupled photonic probabilistic bits","abstract":"Optical computing often employs tailor-made hardware to implement specific algorithms, trading generality for improved performance in key aspects like speed and power efficiency. An important computing approach that is still missing its corresponding optical hardware is probabilistic computing, used e.g. for solving difficult combinatorial optimization problems. In this study, we propose an experimentally viable photonic approach to solve arbitrary probabilistic computing problems. Our method relies on the insight that coherent Ising machines composed of coupled and biased optical parametric oscillators can emulate stochastic logic. We demonstrate the feasibility of our approach by using numerical simulations equivalent to the full density matrix formulation of coupled optical parametric oscillators.","sentences":["Optical computing often employs tailor-made hardware to implement specific algorithms, trading generality for improved performance in key aspects like speed and power efficiency.","An important computing approach that is still missing its corresponding optical hardware is probabilistic computing, used e.g. for solving difficult combinatorial optimization problems.","In this study, we propose an experimentally viable photonic approach to solve arbitrary probabilistic computing problems.","Our method relies on the insight that coherent Ising machines composed of coupled and biased optical parametric oscillators can emulate stochastic logic.","We demonstrate the feasibility of our approach by using numerical simulations equivalent to the full density matrix formulation of coupled optical parametric oscillators."],"url":"http://arxiv.org/abs/2406.04000v1","category":"physics.optics"}
{"created":"2024-06-06 12:17:57","title":"Unveiling the Dynamics of Information Interplay in Supervised Learning","abstract":"In this paper, we use matrix information theory as an analytical tool to analyze the dynamics of the information interplay between data representations and classification head vectors in the supervised learning process. Specifically, inspired by the theory of Neural Collapse, we introduce matrix mutual information ratio (MIR) and matrix entropy difference ratio (HDR) to assess the interactions of data representation and class classification heads in supervised learning, and we determine the theoretical optimal values for MIR and HDR when Neural Collapse happens. Our experiments show that MIR and HDR can effectively explain many phenomena occurring in neural networks, for example, the standard supervised training dynamics, linear mode connectivity, and the performance of label smoothing and pruning. Additionally, we use MIR and HDR to gain insights into the dynamics of grokking, which is an intriguing phenomenon observed in supervised training, where the model demonstrates generalization capabilities long after it has learned to fit the training data. Furthermore, we introduce MIR and HDR as loss terms in supervised and semi-supervised learning to optimize the information interactions among samples and classification heads. The empirical results provide evidence of the method's effectiveness, demonstrating that the utilization of MIR and HDR not only aids in comprehending the dynamics throughout the training process but can also enhances the training procedure itself.","sentences":["In this paper, we use matrix information theory as an analytical tool to analyze the dynamics of the information interplay between data representations and classification head vectors in the supervised learning process.","Specifically, inspired by the theory of Neural Collapse, we introduce matrix mutual information ratio (MIR) and matrix entropy difference ratio (HDR) to assess the interactions of data representation and class classification heads in supervised learning, and we determine the theoretical optimal values for MIR and HDR when Neural Collapse happens.","Our experiments show that MIR and HDR can effectively explain many phenomena occurring in neural networks, for example, the standard supervised training dynamics, linear mode connectivity, and the performance of label smoothing and pruning.","Additionally, we use MIR and HDR to gain insights into the dynamics of grokking, which is an intriguing phenomenon observed in supervised training, where the model demonstrates generalization capabilities long after it has learned to fit the training data.","Furthermore, we introduce MIR and HDR as loss terms in supervised and semi-supervised learning to optimize the information interactions among samples and classification heads.","The empirical results provide evidence of the method's effectiveness, demonstrating that the utilization of MIR and HDR not only aids in comprehending the dynamics throughout the training process but can also enhances the training procedure itself."],"url":"http://arxiv.org/abs/2406.03999v1","category":"cs.LG"}
{"created":"2024-06-06 12:17:05","title":"HackAtari: Atari Learning Environments for Robust and Continual Reinforcement Learning","abstract":"Artificial agents' adaptability to novelty and alignment with intended behavior is crucial for their effective deployment. Reinforcement learning (RL) leverages novelty as a means of exploration, yet agents often struggle to handle novel situations, hindering generalization. To address these issues, we propose HackAtari, a framework introducing controlled novelty to the most common RL benchmark, the Atari Learning Environment. HackAtari allows us to create novel game scenarios (including simplification for curriculum learning), to swap the game elements' colors, as well as to introduce different reward signals for the agent. We demonstrate that current agents trained on the original environments include robustness failures, and evaluate HackAtari's efficacy in enhancing RL agents' robustness and aligning behavior through experiments using C51 and PPO. Overall, HackAtari can be used to improve the robustness of current and future RL algorithms, allowing Neuro-Symbolic RL, curriculum RL, causal RL, as well as LLM-driven RL. Our work underscores the significance of developing interpretable in RL agents.","sentences":["Artificial agents' adaptability to novelty and alignment with intended behavior is crucial for their effective deployment.","Reinforcement learning (RL) leverages novelty as a means of exploration, yet agents often struggle to handle novel situations, hindering generalization.","To address these issues, we propose HackAtari, a framework introducing controlled novelty to the most common RL benchmark, the Atari Learning Environment.","HackAtari allows us to create novel game scenarios (including simplification for curriculum learning), to swap the game elements' colors, as well as to introduce different reward signals for the agent.","We demonstrate that current agents trained on the original environments include robustness failures, and evaluate HackAtari's efficacy in enhancing RL agents' robustness and aligning behavior through experiments using C51 and PPO.","Overall, HackAtari can be used to improve the robustness of current and future RL algorithms, allowing Neuro-Symbolic RL, curriculum RL, causal RL, as well as LLM-driven RL.","Our work underscores the significance of developing interpretable in RL agents."],"url":"http://arxiv.org/abs/2406.03997v1","category":"cs.AI"}
{"created":"2024-06-06 12:15:51","title":"AC4MPC: Actor-Critic Reinforcement Learning for Nonlinear Model Predictive Control","abstract":"\\Ac{MPC} and \\ac{RL} are two powerful control strategies with, arguably, complementary advantages. In this work, we show how actor-critic \\ac{RL} techniques can be leveraged to improve the performance of \\ac{MPC}. The \\ac{RL} critic is used as an approximation of the optimal value function, and an actor roll-out provides an initial guess for primal variables of the \\ac{MPC}. A parallel control architecture is proposed where each \\ac{MPC} instance is solved twice for different initial guesses. Besides the actor roll-out initialization, a shifted initialization from the previous solution is used. Thereafter, the actor and the critic are again used to approximately evaluate the infinite horizon cost of these trajectories. The control actions from the lowest-cost trajectory are applied to the system at each time step. We establish that the proposed algorithm is guaranteed to outperform the original \\ac{RL} policy plus an error term that depends on the accuracy of the critic and decays with the horizon length of the \\ac{MPC} formulation. Moreover, we do not require globally optimal solutions for these guarantees to hold. The approach is demonstrated on an illustrative toy example and an \\ac{AD} overtaking scenario.","sentences":["\\Ac{MPC} and \\ac{RL} are two powerful control strategies with, arguably, complementary advantages.","In this work, we show how actor-critic \\ac{RL} techniques can be leveraged to improve the performance of \\ac{MPC}.","The \\ac{RL} critic is used as an approximation of the optimal value function, and an actor roll-out provides an initial guess for primal variables of the \\ac{MPC}.","A parallel control architecture is proposed where each \\ac{MPC} instance is solved twice for different initial guesses.","Besides the actor roll-out initialization, a shifted initialization from the previous solution is used.","Thereafter, the actor and the critic are again used to approximately evaluate the infinite horizon cost of these trajectories.","The control actions from the lowest-cost trajectory are applied to the system at each time step.","We establish that the proposed algorithm is guaranteed to outperform the original \\ac{RL} policy plus an error term that depends on the accuracy of the critic and decays with the horizon length of the \\ac{MPC} formulation.","Moreover, we do not require globally optimal solutions for these guarantees to hold.","The approach is demonstrated on an illustrative toy example and an \\ac{AD} overtaking scenario."],"url":"http://arxiv.org/abs/2406.03995v1","category":"eess.SY"}
{"created":"2024-06-06 12:08:43","title":"Assessing LLMs for Zero-shot Abstractive Summarization Through the Lens of Relevance Paraphrasing","abstract":"Large Language Models (LLMs) have achieved state-of-the-art performance at zero-shot generation of abstractive summaries for given articles. However, little is known about the robustness of such a process of zero-shot summarization. To bridge this gap, we propose relevance paraphrasing, a simple strategy that can be used to measure the robustness of LLMs as summarizers. The relevance paraphrasing approach identifies the most relevant sentences that contribute to generating an ideal summary, and then paraphrases these inputs to obtain a minimally perturbed dataset. Then, by evaluating model performance for summarization on both the original and perturbed datasets, we can assess the LLM's one aspect of robustness. We conduct extensive experiments with relevance paraphrasing on 4 diverse datasets, as well as 4 LLMs of different sizes (GPT-3.5-Turbo, Llama-2-13B, Mistral-7B, and Dolly-v2-7B). Our results indicate that LLMs are not consistent summarizers for the minimally perturbed articles, necessitating further improvements.","sentences":["Large Language Models (LLMs) have achieved state-of-the-art performance at zero-shot generation of abstractive summaries for given articles.","However, little is known about the robustness of such a process of zero-shot summarization.","To bridge this gap, we propose relevance paraphrasing, a simple strategy that can be used to measure the robustness of LLMs as summarizers.","The relevance paraphrasing approach identifies the most relevant sentences that contribute to generating an ideal summary, and then paraphrases these inputs to obtain a minimally perturbed dataset.","Then, by evaluating model performance for summarization on both the original and perturbed datasets, we can assess the LLM's one aspect of robustness.","We conduct extensive experiments with relevance paraphrasing on 4 diverse datasets, as well as 4 LLMs of different sizes (GPT-3.5-Turbo, Llama-2-13B, Mistral-7B, and Dolly-v2-7B).","Our results indicate that LLMs are not consistent summarizers for the minimally perturbed articles, necessitating further improvements."],"url":"http://arxiv.org/abs/2406.03993v1","category":"cs.CL"}
{"created":"2024-06-06 12:07:38","title":"Generalized Wedderburn Rank Reduction","abstract":"We generalize the Wedderburn rank reduction formula by replacing the inverse with the Moore--Penrose pseudoinverse. In particular, this allows one to remove the non--singularity of a certain matrix from assumptions. The results implies in a straightforward way Nystroem, CUR decompositions, meta-factorization, and a result of Ameli, Shadden. We investigate which properties of the matrix are inherited by the generalized Wedderburn reduction. Reductions leading to the best low-rank approximation are explicitly described in terms of singular vectors. We give a self--contained calculation of the range and the nullspace of the projection $A(BA)^+B$ and prove that any projection can be expressed in this way.","sentences":["We generalize the Wedderburn rank reduction formula by replacing the inverse with the Moore--Penrose pseudoinverse.","In particular, this allows one to remove the non--singularity of a certain matrix from assumptions.","The results implies in a straightforward way Nystroem, CUR decompositions, meta-factorization, and a result of Ameli, Shadden.","We investigate which properties of the matrix are inherited by the generalized Wedderburn reduction.","Reductions leading to the best low-rank approximation are explicitly described in terms of singular vectors.","We give a self--contained calculation of the range and the nullspace of the projection $A(BA)^+B$ and prove that any projection can be expressed in this way."],"url":"http://arxiv.org/abs/2406.03992v1","category":"math.NA"}
{"created":"2024-06-06 12:06:40","title":"Exact solution for a general FJC polyelectrolyte model with up to Next Nearest Neighbour Interactions","abstract":"This work presents a polyelectrolyte (PE) model based on a freely jointed chain (FJC) in which the beads are ionizable and the bonds are rigid. Electrostatic interactions considered are of the short range type up to next-nearest neighbours using a Debye-H\\\"uckel (DH) potential. Exact expressions for the conformational and protonation properties are obtained as a function of the pH using the transfer matrix method and the integration of the angular variables. For the finite cases with low N values, relevant differences arise on both the protonation curve and the end to end distance due to the end effects. However, when increasing N a rather rapid convergence is observed to the infinite chain limiting case.","sentences":["This work presents a polyelectrolyte (PE) model based on a freely jointed chain (FJC) in which the beads are ionizable and the bonds are rigid.","Electrostatic interactions considered are of the short range type up to next-nearest neighbours using a Debye-H\\\"uckel (DH) potential.","Exact expressions for the conformational and protonation properties are obtained as a function of the pH using the transfer matrix method and the integration of the angular variables.","For the finite cases with low N values, relevant differences arise on both the protonation curve and the end to end distance due to the end effects.","However, when increasing N a rather rapid convergence is observed to the infinite chain limiting case."],"url":"http://arxiv.org/abs/2406.03991v1","category":"cond-mat.soft"}
{"created":"2024-06-06 12:00:41","title":"On The Persona-based Summarization of Domain-Specific Documents","abstract":"In an ever-expanding world of domain-specific knowledge, the increasing complexity of consuming, and storing information necessitates the generation of summaries from large information repositories. However, every persona of a domain has different requirements of information and hence their summarization. For example, in the healthcare domain, a persona-based (such as Doctor, Nurse, Patient etc.) approach is imperative to deliver targeted medical information efficiently. Persona-based summarization of domain-specific information by humans is a high cognitive load task and is generally not preferred. The summaries generated by two different humans have high variability and do not scale in cost and subject matter expertise as domains and personas grow. Further, AI-generated summaries using generic Large Language Models (LLMs) may not necessarily offer satisfactory accuracy for different domains unless they have been specifically trained on domain-specific data and can also be very expensive to use in day-to-day operations. Our contribution in this paper is two-fold: 1) We present an approach to efficiently fine-tune a domain-specific small foundation LLM using a healthcare corpus and also show that we can effectively evaluate the summarization quality using AI-based critiquing. 2) We further show that AI-based critiquing has good concordance with Human-based critiquing of the summaries. Hence, such AI-based pipelines to generate domain-specific persona-based summaries can be easily scaled to other domains such as legal, enterprise documents, education etc. in a very efficient and cost-effective manner.","sentences":["In an ever-expanding world of domain-specific knowledge, the increasing complexity of consuming, and storing information necessitates the generation of summaries from large information repositories.","However, every persona of a domain has different requirements of information and hence their summarization.","For example, in the healthcare domain, a persona-based (such as Doctor, Nurse, Patient etc.)","approach is imperative to deliver targeted medical information efficiently.","Persona-based summarization of domain-specific information by humans is a high cognitive load task and is generally not preferred.","The summaries generated by two different humans have high variability and do not scale in cost and subject matter expertise as domains and personas grow.","Further, AI-generated summaries using generic Large Language Models (LLMs) may not necessarily offer satisfactory accuracy for different domains unless they have been specifically trained on domain-specific data and can also be very expensive to use in day-to-day operations.","Our contribution in this paper is two-fold: 1) We present an approach to efficiently fine-tune a domain-specific small foundation LLM using a healthcare corpus and also show that we can effectively evaluate the summarization quality using AI-based critiquing.","2) We further show that AI-based critiquing has good concordance with Human-based critiquing of the summaries.","Hence, such AI-based pipelines to generate domain-specific persona-based summaries can be easily scaled to other domains such as legal, enterprise documents, education etc.","in a very efficient and cost-effective manner."],"url":"http://arxiv.org/abs/2406.03986v1","category":"cs.CL"}
{"created":"2024-06-06 11:54:28","title":"Level statistics detect generalized symmetries","abstract":"Level statistics are a useful probe for detecting symmetries and distinguishing integrable and non-integrable systems. I show by way of several examples that level statistics detect the presence of generalized symmetries that go beyond conventional lattice symmetries and internal symmetries. I consider non-invertible symmetries through the example of Kramers-Wannier duality at an Ising critical point, symmetries with nonlocal generators through the example of a spin-$1$ anisotropic Heisenberg chain, and $q$-deformed symmetries through an example closely related to recent work on $q$-deformed SPT phases. In each case, conventional level statistics detect the generalized symmetries, and these symmetries must be resolved before seeing characteristic level repulsion in non-integrable systems. For the $q$-deformed symmetry, I discovered via level statistics a $q$-deformed generalization of inversion that is interesting in its own right and that may protect $q$-deformed SPT phases.","sentences":["Level statistics are a useful probe for detecting symmetries and distinguishing integrable and non-integrable systems.","I show by way of several examples that level statistics detect the presence of generalized symmetries that go beyond conventional lattice symmetries and internal symmetries.","I consider non-invertible symmetries through the example of Kramers-Wannier duality at an Ising critical point, symmetries with nonlocal generators through the example of a spin-$1$ anisotropic Heisenberg chain, and $q$-deformed symmetries through an example closely related to recent work on $q$-deformed SPT phases.","In each case, conventional level statistics detect the generalized symmetries, and these symmetries must be resolved before seeing characteristic level repulsion in non-integrable systems.","For the $q$-deformed symmetry, I discovered via level statistics a $q$-deformed generalization of inversion that is interesting in its own right and that may protect $q$-deformed SPT phases."],"url":"http://arxiv.org/abs/2406.03983v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-06 11:40:50","title":"PyExoCross: a Python program for generating spectra and cross-sections from molecular line lists","abstract":"PyExoCross is a Python adaptation of the ExoCross Fortran application, PyExoCross is designed for postprocessing the huge molecular line lists generated by the ExoMol project and other similar initiatives such as the HITRAN and HITEMP databases. PyExoCross generates absorption and emission stick spectra, cross-sections, and other properties (partition functions, specific heats, cooling functions, lifetimes, and oscillator strengths) based on molecular line lists. PyExoCross calculates cross-sections with four line profiles: Doppler, Gaussian, Lorentzian, and Voigt profiles in both sampling and binned methods; a number of options are available for computing Voigt profiles which we test for speed and accuracy. PyExoCross supports importing and exporting line lists in the ExoMol and HITRAN/HITEMP formats. PyExoCross also provides conversion between the ExoMol and HITRAN data formats. In addition, PyExoCross has extra code for users to automate the batch download of line list files from the ExoMol database.","sentences":["PyExoCross is a Python adaptation of the ExoCross Fortran application, PyExoCross is designed for postprocessing the huge molecular line lists generated by the ExoMol project and other similar initiatives such as the HITRAN and HITEMP databases.","PyExoCross generates absorption and emission stick spectra, cross-sections, and other properties (partition functions, specific heats, cooling functions, lifetimes, and oscillator strengths) based on molecular line lists.","PyExoCross calculates cross-sections with four line profiles: Doppler, Gaussian, Lorentzian, and Voigt profiles in both sampling and binned methods; a number of options are available for computing Voigt profiles which we test for speed and accuracy.","PyExoCross supports importing and exporting line lists in the ExoMol and HITRAN/HITEMP formats.","PyExoCross also provides conversion between the ExoMol and HITRAN data formats.","In addition, PyExoCross has extra code for users to automate the batch download of line list files from the ExoMol database."],"url":"http://arxiv.org/abs/2406.03977v1","category":"astro-ph.IM"}
{"created":"2024-06-06 11:33:56","title":"Operator learning based on sparse high-dimensional approximation","abstract":"We present a dimension-incremental method for function approximation in bounded orthonormal product bases to learn the solutions of various differential equations. Therefore, we deconstruct the source function of the differential equation into parameters like Fourier or Spline coefficients and treat the solution of the differential equation as a high-dimensional function w.r.t. the spatial variables, these parameters and also further possible parameters from the differential equation itself. Finally, we learn this function in the sense of sparse approximation in a suitable function space by detecting coefficients of the basis expansion with largest absolute value. Investigating the corresponding indices of the basis coefficients yields further insights on the structure of the solution as well as its dependency on the parameters and their interactions and allows for a reasonable generalization to even higher dimensions and therefore better resolutions of the deconstructed source function.","sentences":["We present a dimension-incremental method for function approximation in bounded orthonormal product bases to learn the solutions of various differential equations.","Therefore, we deconstruct the source function of the differential equation into parameters like Fourier or Spline coefficients and treat the solution of the differential equation as a high-dimensional function w.r.t.","the spatial variables, these parameters and also further possible parameters from the differential equation itself.","Finally, we learn this function in the sense of sparse approximation in a suitable function space by detecting coefficients of the basis expansion with largest absolute value.","Investigating the corresponding indices of the basis coefficients yields further insights on the structure of the solution as well as its dependency on the parameters and their interactions and allows for a reasonable generalization to even higher dimensions and therefore better resolutions of the deconstructed source function."],"url":"http://arxiv.org/abs/2406.03973v1","category":"math.NA"}
{"created":"2024-06-06 11:33:29","title":"Eigenpath traversal by Poisson-distributed phase randomisation","abstract":"We present a framework for quantum computation, similar to Adiabatic Quantum Computation (AQC), that is based on the quantum Zeno effect. By performing randomised dephasing operations at intervals determined by a Poisson process, we are able to track the eigenspace associated to a particular eigenvalue.   We derive a simple differential equation for the fidelity, leading to general theorems bounding the time complexity of a whole class of algorithms. We also use eigenstate filtering to optimise the scaling of the complexity in the error tolerance $\\epsilon$.   In many cases the bounds given by our general theorems are optimal, giving a time complexity of $O(1/\\Delta_m)$ with $\\Delta_m$ the minimum of the gap. This allows us to prove optimal results using very general features of problems, minimising the problem-specific insight necessary.   As two applications of our framework, we obtain optimal scaling for the Grover problem (i.e.\\ $O(\\sqrt{N})$ where $N$ is the database size) and the Quantum Linear System Problem (i.e.\\ $O(\\kappa\\log(1/\\epsilon))$ where $\\kappa$ is the condition number and $\\epsilon$ the error tolerance) by direct applications of our theorems.","sentences":["We present a framework for quantum computation, similar to Adiabatic Quantum Computation (AQC), that is based on the quantum Zeno effect.","By performing randomised dephasing operations at intervals determined by a Poisson process, we are able to track the eigenspace associated to a particular eigenvalue.   ","We derive a simple differential equation for the fidelity, leading to general theorems bounding the time complexity of a whole class of algorithms.","We also use eigenstate filtering to optimise the scaling of the complexity in the error tolerance $\\epsilon$.   In many cases the bounds given by our general theorems are optimal, giving a time complexity of $O(1/\\Delta_m)$ with $\\Delta_m$ the minimum of the gap.","This allows us to prove optimal results using very general features of problems, minimising the problem-specific insight necessary.   ","As two applications of our framework, we obtain optimal scaling for the Grover problem (i.e.\\ $O(\\sqrt{N})$ where $N$ is the database size) and the Quantum Linear System Problem (i.e.\\ $O(\\kappa\\log(1/\\epsilon))$ where $\\kappa$ is the condition number and $\\epsilon$ the error tolerance) by direct applications of our theorems."],"url":"http://arxiv.org/abs/2406.03972v1","category":"quant-ph"}
{"created":"2024-06-06 11:27:49","title":"Calculating Bayesian evidence for inflationary models using CONNECT","abstract":"Bayesian evidence is a standard method used for comparing the ability of different models to fit available data and is used extensively in cosmology. However, since the evidence calculation involves performing an integral of the likelihood function over the entire space of model parameters this can be prohibitively expensive in terms of both CPU and time consumption. For example, in the simplest $\\Lambda$CDM model and using CMB data from the Planck satellite, the dimensionality of the model space is over 30 (typically 6 cosmological parameters and 28 nuisance parameters). Even the simplest possible model requires $\\mathcal{O}(10^6)$ calls to an Einstein--Boltzmann solver such as CLASS or CAMB and takes several days.   Here we present calculations of Bayesian evidence using the CONNECT framework to calculate cosmological observables. We demonstrate that we can achieve results comparable to those obtained using Einstein--Boltzmann solvers, but at a minute fraction of the computational cost. As a test case, we then go on to compute Bayesian evidence ratios for a selection of slow-roll inflationary models.   In the setup presented here, the total computation time is completely dominated by the likelihood function calculation which now becomes the main bottleneck for increasing computation speed.","sentences":["Bayesian evidence is a standard method used for comparing the ability of different models to fit available data and is used extensively in cosmology.","However, since the evidence calculation involves performing an integral of the likelihood function over the entire space of model parameters this can be prohibitively expensive in terms of both CPU and time consumption.","For example, in the simplest $\\Lambda$CDM model and using CMB data from the Planck satellite, the dimensionality of the model space is over 30 (typically 6 cosmological parameters and 28 nuisance parameters).","Even the simplest possible model requires $\\mathcal{O}(10^6)$ calls to an Einstein--Boltzmann solver such as CLASS or CAMB and takes several days.   ","Here we present calculations of Bayesian evidence using the CONNECT framework to calculate cosmological observables.","We demonstrate that we can achieve results comparable to those obtained using Einstein--Boltzmann solvers, but at a minute fraction of the computational cost.","As a test case, we then go on to compute Bayesian evidence ratios for a selection of slow-roll inflationary models.   ","In the setup presented here, the total computation time is completely dominated by the likelihood function calculation which now becomes the main bottleneck for increasing computation speed."],"url":"http://arxiv.org/abs/2406.03968v1","category":"astro-ph.CO"}
{"created":"2024-06-06 11:26:49","title":"Model order reduction for discrete time-delay systems with inhomogeneous initial conditions","abstract":"We propose two kinds of model order reduction methods for discrete time-delay systems with inhomogeneous initial conditions. The peculiar properties of discrete Walsh functions are directly utilized to compute the Walsh coefficients of the systems, and the projection matrix is defined properly to generate reduced models by taking into account the non-zero initial conditions. It is shown that reduced models can preserve some Walsh coefficients of the expansion of the original systems. Further, the superposition principle is exploited to achieve a decomposition of the original systems, and a new definition of Gramians is proposed by combining the individual Gramians of each subsystem. As a result, the balanced truncation method is applied to systems with inhomogeneous initial conditions. We also provide a low-rank approximation to Gramians based on the discrete Laguerre polynomials, which enables an efficient execution of our approach. Numerical examples confirm the feasibility and effectiveness of the proposed methods.","sentences":["We propose two kinds of model order reduction methods for discrete time-delay systems with inhomogeneous initial conditions.","The peculiar properties of discrete Walsh functions are directly utilized to compute the Walsh coefficients of the systems, and the projection matrix is defined properly to generate reduced models by taking into account the non-zero initial conditions.","It is shown that reduced models can preserve some Walsh coefficients of the expansion of the original systems.","Further, the superposition principle is exploited to achieve a decomposition of the original systems, and a new definition of Gramians is proposed by combining the individual Gramians of each subsystem.","As a result, the balanced truncation method is applied to systems with inhomogeneous initial conditions.","We also provide a low-rank approximation to Gramians based on the discrete Laguerre polynomials, which enables an efficient execution of our approach.","Numerical examples confirm the feasibility and effectiveness of the proposed methods."],"url":"http://arxiv.org/abs/2406.03967v1","category":"math.OC"}
{"created":"2024-06-06 11:26:19","title":"QuickCurve: revisiting slightly non-planar 3D printing","abstract":"Additive manufacturing builds physical objects by accumulating layers upon layers of solidified material. This process is typically done with horizontal planar layers. However, fused filament printers have the capability to extrude material along 3D curves. The idea of depositing out-of-plane, also known as non-planar printing, has spawned a trend of research towards algorithms that could generate non-planar deposition paths automatically from a 3D object. In this paper we introduce a novel algorithm for this purpose. Our method optimizes for a curved slicing surface. This surface is intersected with the input model to extract non-planar layers, with the objective of accurately reproducing the model top surfaces while avoiding collisions. Our formulation leads to a simple and efficient approach that only requires solving for a single least-square problem. Notably, it does not require a tetrahedralization of the input or iterative solver passes, while being more general than simpler approaches. We further explore how to orient the paths to follow the principal curvatures of the surfaces, how to filter spurious tiny features damaging the results, and how to achieve a good compromise of mixing planar and non-planar strategies within the same part. We present a complete formulation and its implementation, and demonstrate our method on a variety of 3D printed models.","sentences":["Additive manufacturing builds physical objects by accumulating layers upon layers of solidified material.","This process is typically done with horizontal planar layers.","However, fused filament printers have the capability to extrude material along 3D curves.","The idea of depositing out-of-plane, also known as non-planar printing, has spawned a trend of research towards algorithms that could generate non-planar deposition paths automatically from a 3D object.","In this paper we introduce a novel algorithm for this purpose.","Our method optimizes for a curved slicing surface.","This surface is intersected with the input model to extract non-planar layers, with the objective of accurately reproducing the model top surfaces while avoiding collisions.","Our formulation leads to a simple and efficient approach that only requires solving for a single least-square problem.","Notably, it does not require a tetrahedralization of the input or iterative solver passes, while being more general than simpler approaches.","We further explore how to orient the paths to follow the principal curvatures of the surfaces, how to filter spurious tiny features damaging the results, and how to achieve a good compromise of mixing planar and non-planar strategies within the same part.","We present a complete formulation and its implementation, and demonstrate our method on a variety of 3D printed models."],"url":"http://arxiv.org/abs/2406.03966v1","category":"cs.GR"}
{"created":"2024-06-06 11:22:57","title":"More Bang For Your Buck(et): Fast and Space-efficient Hardware-accelerated Coarse-granular Indexing on GPUs","abstract":"In recent work, we have shown that NVIDIA's raytracing cores on RTX video cards can be exploited to realize hardware-accelerated lookups for GPU-resident database indexes. On a high level, the concept materializes all keys as triangles in a 3D scene and indexes them. Lookups are performed by firing rays into the scene and utilizing the index structure to detect hits in a hardware-accelerated fashion. While this approach called RTIndeX (or short RX) is indeed promising, it currently suffers from three limitations: (1) significant memory overhead per key, (2) slow range-lookups, and (3) poor updateability. In this work, we show that all three problems can be tackled by a single design change: Generalizing RX to become a coarse-granular index cgRX. Instead of indexing individual keys, cgRX indexes buckets of keys which are post-filtered after retrieval. This drastically reduces the memory overhead, leads to the generation of a smaller and more efficient index structure, and enables fast range-lookups as well as updates. We will see that representing the buckets in the 3D space such that the lookup of a key is performed both correctly and efficiently requires the careful orchestration of firing rays in a specific sequence. Our experimental evaluation shows that cgRX offers the most bang for the buck(et) by providing a throughput in relation to the memory footprint that is 1.5-3x higher than for the comparable range-lookup supporting baselines. At the same time, cgRX improves the range-lookup performance over RX by up to 2x and offers practical updateability that is up to 5.5x faster than rebuilding from scratch.","sentences":["In recent work, we have shown that NVIDIA's raytracing cores on RTX video cards can be exploited to realize hardware-accelerated lookups for GPU-resident database indexes.","On a high level, the concept materializes all keys as triangles in a 3D scene and indexes them.","Lookups are performed by firing rays into the scene and utilizing the index structure to detect hits in a hardware-accelerated fashion.","While this approach called RTIndeX (or short RX) is indeed promising, it currently suffers from three limitations: (1) significant memory overhead per key, (2) slow range-lookups, and (3) poor updateability.","In this work, we show that all three problems can be tackled by a single design change:","Generalizing RX to become a coarse-granular index cgRX.","Instead of indexing individual keys, cgRX indexes buckets of keys which are post-filtered after retrieval.","This drastically reduces the memory overhead, leads to the generation of a smaller and more efficient index structure, and enables fast range-lookups as well as updates.","We will see that representing the buckets in the 3D space such that the lookup of a key is performed both correctly and efficiently requires the careful orchestration of firing rays in a specific sequence.","Our experimental evaluation shows that cgRX offers the most bang for the buck(et) by providing a throughput in relation to the memory footprint that is 1.5-3x higher than for the comparable range-lookup supporting baselines.","At the same time, cgRX improves the range-lookup performance over RX by up to 2x and offers practical updateability that is up to 5.5x faster than rebuilding from scratch."],"url":"http://arxiv.org/abs/2406.03965v1","category":"cs.DB"}
{"created":"2024-06-06 11:17:21","title":"Quantum Speed Limits for Implementation of Unitary Transformations","abstract":"Quantum speed limits are the boundaries that define how quickly one quantum state can transform into another. Instead of focusing on the transformation between pairs of states, we provide bounds on the speed limit of quantum evolution by unitary operators in arbitrary dimensions. These do not depend on the initial and final state but depend only on the trace of the unitary operator that is to be implemented and the gross characteristics (average and variance) of the energy spectrum of the Hamiltonian which generates this unitary evolution. The bounds that we find can be thought of as the generalization of the Mandelstam-Tamm (TM) and the Margolus-Levitin (MT) bound for state transformations to implementations of unitary operators. We will discuss the application of these bounds in several classes of transformations that are of interest in quantum information processing.","sentences":["Quantum speed limits are the boundaries that define how quickly one quantum state can transform into another.","Instead of focusing on the transformation between pairs of states, we provide bounds on the speed limit of quantum evolution by unitary operators in arbitrary dimensions.","These do not depend on the initial and final state but depend only on the trace of the unitary operator that is to be implemented and the gross characteristics (average and variance) of the energy spectrum of the Hamiltonian which generates this unitary evolution.","The bounds that we find can be thought of as the generalization of the Mandelstam-Tamm (TM) and the Margolus-Levitin (MT) bound for state transformations to implementations of unitary operators.","We will discuss the application of these bounds in several classes of transformations that are of interest in quantum information processing."],"url":"http://arxiv.org/abs/2406.03964v1","category":"quant-ph"}
{"created":"2024-06-06 11:14:27","title":"A + B: A General Generator-Reader Framework for Optimizing LLMs to Unleash Synergy Potential","abstract":"Retrieval-Augmented Generation (RAG) is an effective solution to supplement necessary knowledge to large language models (LLMs). Targeting its bottleneck of retriever performance, \"generate-then-read\" pipeline is proposed to replace the retrieval stage with generation from the LLM itself. Although promising, this research direction is underexplored and still cannot work in the scenario when source knowledge is given. In this paper, we formalize a general \"A + B\" framework with varying combinations of foundation models and types for systematic investigation. We explore the efficacy of the base and chat versions of LLMs and found their different functionalities suitable for generator A and reader B, respectively. Their combinations consistently outperform single models, especially in complex scenarios. Furthermore, we extend the application of the \"A + B\" framework to scenarios involving source documents through continuous learning, enabling the direct integration of external knowledge into LLMs. This approach not only facilitates effective acquisition of new knowledge but also addresses the challenges of safety and helpfulness post-adaptation. The paper underscores the versatility of the \"A + B\" framework, demonstrating its potential to enhance the practical application of LLMs across various domains.","sentences":["Retrieval-Augmented Generation (RAG) is an effective solution to supplement necessary knowledge to large language models (LLMs).","Targeting its bottleneck of retriever performance, \"generate-then-read\" pipeline is proposed to replace the retrieval stage with generation from the LLM itself.","Although promising, this research direction is underexplored and still cannot work in the scenario when source knowledge is given.","In this paper, we formalize a general \"A + B\" framework with varying combinations of foundation models and types for systematic investigation.","We explore the efficacy of the base and chat versions of LLMs and found their different functionalities suitable for generator A and reader B, respectively.","Their combinations consistently outperform single models, especially in complex scenarios.","Furthermore, we extend the application of the \"A + B\" framework to scenarios involving source documents through continuous learning, enabling the direct integration of external knowledge into LLMs.","This approach not only facilitates effective acquisition of new knowledge but also addresses the challenges of safety and helpfulness post-adaptation.","The paper underscores the versatility of the \"A + B\" framework, demonstrating its potential to enhance the practical application of LLMs across various domains."],"url":"http://arxiv.org/abs/2406.03963v1","category":"cs.CL"}
{"created":"2024-06-06 11:13:44","title":"LDM-RSIC: Exploring Distortion Prior with Latent Diffusion Models for Remote Sensing Image Compression","abstract":"Deep learning-based image compression algorithms typically focus on designing encoding and decoding networks and improving the accuracy of entropy model estimation to enhance the rate-distortion (RD) performance. However, few algorithms leverage the compression distortion prior from existing compression algorithms to improve RD performance. In this paper, we propose a latent diffusion model-based remote sensing image compression (LDM-RSIC) method, which aims to enhance the final decoding quality of RS images by utilizing the generated distortion prior from a LDM. Our approach consists of two stages. In the first stage, a self-encoder learns prior from the high-quality input image. In the second stage, the prior is generated through an LDM, conditioned on the decoded image of an existing learning-based image compression algorithm, to be used as auxiliary information for generating the texture-rich enhanced image. To better utilize the prior, a channel attention and gate-based dynamic feature attention module (DFAM) is embedded into a Transformer-based multi-scale enhancement network (MEN) for image enhancement. Extensive experiments demonstrate the proposed LDM-RSIC significantly outperforms existing state-of-the-art traditional and learning-based image compression algorithms in terms of both subjective perception and objective metrics. Additionally, we use the LDM-based scheme to improve the traditional image compression algorithm JPEG2000 and obtain 32.00% bit savings on the DOTA testing set. The code will be available at https://github.com/mlkk518/LDM-RSIC.","sentences":["Deep learning-based image compression algorithms typically focus on designing encoding and decoding networks and improving the accuracy of entropy model estimation to enhance the rate-distortion (RD) performance.","However, few algorithms leverage the compression distortion prior from existing compression algorithms to improve RD performance.","In this paper, we propose a latent diffusion model-based remote sensing image compression (LDM-RSIC) method, which aims to enhance the final decoding quality of RS images by utilizing the generated distortion prior from a LDM.","Our approach consists of two stages.","In the first stage, a self-encoder learns prior from the high-quality input image.","In the second stage, the prior is generated through an LDM, conditioned on the decoded image of an existing learning-based image compression algorithm, to be used as auxiliary information for generating the texture-rich enhanced image.","To better utilize the prior, a channel attention and gate-based dynamic feature attention module (DFAM) is embedded into a Transformer-based multi-scale enhancement network (MEN) for image enhancement.","Extensive experiments demonstrate the proposed LDM-RSIC significantly outperforms existing state-of-the-art traditional and learning-based image compression algorithms in terms of both subjective perception and objective metrics.","Additionally, we use the LDM-based scheme to improve the traditional image compression algorithm JPEG2000 and obtain 32.00% bit savings on the DOTA testing set.","The code will be available at https://github.com/mlkk518/LDM-RSIC."],"url":"http://arxiv.org/abs/2406.03961v1","category":"eess.IV"}
{"created":"2024-06-06 11:04:35","title":"Exploring loop-induced first-order electroweak phase transition in the Higgs effective field theory","abstract":"The nearly aligned Higgs Effective Field Theory (naHEFT) is based on the general assumption: all deviations in the Higgs boson couplings are originated from quantum one-loop effects of new particles that are integrated out. If the new particles integrated out have the same non-decoupling property, physics of the electroweak symmetry breaking can be then described by several parameters in the naHEFT, so that there is a correlation among the Higgs boson couplings such as $h \\gamma \\gamma$, $hWW$ and $hhh$ couplings. In this paper, we analyze the strongly first-order electroweak phase transition (EWPT) with the condition of sphaleron decoupling and the completion condition of the phase transition, and investigate the relation among the deviations in the Higgs boson couplings and the dynamics of the EWPTs. We also take into account the gravitational wave spectrum as well as the primordial black hole predicted at the EWPT. We show that if the new particles integrated out include charged scalar states future precision measurements of the $h \\gamma \\gamma$ coupling can give a useful prediction on the $hhh$ coupling to realize the strongly first-order EWPT. We can explore the nature of EWPT and the new physics behind it by the combination of precision measurements of various Higgs boson couplings at future collider experiments, gravitational wave observations at future space-based interferometers and searches for primordial black holes.","sentences":["The nearly aligned Higgs Effective Field Theory (naHEFT) is based on the general assumption: all deviations in the Higgs boson couplings are originated from quantum one-loop effects of new particles that are integrated out.","If the new particles integrated out have the same non-decoupling property, physics of the electroweak symmetry breaking can be then described by several parameters in the naHEFT, so that there is a correlation among the Higgs boson couplings such as $h \\gamma \\gamma$, $hWW$ and $hhh$ couplings.","In this paper, we analyze the strongly first-order electroweak phase transition (EWPT) with the condition of sphaleron decoupling and the completion condition of the phase transition, and investigate the relation among the deviations in the Higgs boson couplings and the dynamics of the EWPTs.","We also take into account the gravitational wave spectrum as well as the primordial black hole predicted at the EWPT.","We show that if the new particles integrated out include charged scalar states future precision measurements of the $h \\gamma \\gamma$ coupling can give a useful prediction on the $hhh$ coupling to realize the strongly first-order EWPT.","We can explore the nature of EWPT and the new physics behind it by the combination of precision measurements of various Higgs boson couplings at future collider experiments, gravitational wave observations at future space-based interferometers and searches for primordial black holes."],"url":"http://arxiv.org/abs/2406.03957v1","category":"hep-ph"}
{"created":"2024-06-06 10:54:44","title":"Tox-BART: Leveraging Toxicity Attributes for Explanation Generation of Implicit Hate Speech","abstract":"Employing language models to generate explanations for an incoming implicit hate post is an active area of research. The explanation is intended to make explicit the underlying stereotype and aid content moderators. The training often combines top-k relevant knowledge graph (KG) tuples to provide world knowledge and improve performance on standard metrics. Interestingly, our study presents conflicting evidence for the role of the quality of KG tuples in generating implicit explanations. Consequently, simpler models incorporating external toxicity signals outperform KG-infused models. Compared to the KG-based setup, we observe a comparable performance for SBIC (LatentHatred) datasets with a performance variation of +0.44 (+0.49), +1.83 (-1.56), and -4.59 (+0.77) in BLEU, ROUGE-L, and BERTScore. Further human evaluation and error analysis reveal that our proposed setup produces more precise explanations than zero-shot GPT-3.5, highlighting the intricate nature of the task.","sentences":["Employing language models to generate explanations for an incoming implicit hate post is an active area of research.","The explanation is intended to make explicit the underlying stereotype and aid content moderators.","The training often combines top-k relevant knowledge graph (KG) tuples to provide world knowledge and improve performance on standard metrics.","Interestingly, our study presents conflicting evidence for the role of the quality of KG tuples in generating implicit explanations.","Consequently, simpler models incorporating external toxicity signals outperform KG-infused models.","Compared to the KG-based setup, we observe a comparable performance for SBIC (LatentHatred) datasets with a performance variation of +0.44 (+0.49), +1.83 (-1.56), and -4.59 (+0.77) in BLEU, ROUGE-L, and BERTScore.","Further human evaluation and error analysis reveal that our proposed setup produces more precise explanations than zero-shot GPT-3.5, highlighting the intricate nature of the task."],"url":"http://arxiv.org/abs/2406.03953v1","category":"cs.CL"}
{"created":"2024-06-06 10:52:59","title":"L-shadowing lemma for the Cauchy equation","abstract":"We prove that if the Cauchy problem $\\dot{u}=Au$ in a Banach space is hyperbolic, then the problem has the L-shadowing property. Conversely, if the space is finite-dimensional and the L-shadowing property is satisfied, then the problem is hyperbolic. This generalizes a previous result by Ombach \\cite{o, o1} for linear homeomorphisms. Some short applications are given.","sentences":["We prove that if the Cauchy problem $\\dot{u}=Au$ in a Banach space is hyperbolic, then the problem has the L-shadowing property.","Conversely, if the space is finite-dimensional and the L-shadowing property is satisfied, then the problem is hyperbolic.","This generalizes a previous result by Ombach \\cite{o, o1} for linear homeomorphisms.","Some short applications are given."],"url":"http://arxiv.org/abs/2406.03951v1","category":"math.AP"}
{"created":"2024-06-06 10:50:26","title":"UltraMedical: Building Specialized Generalists in Biomedicine","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains and are moving towards more specialized areas. Recent advanced proprietary models such as GPT-4 and Gemini have achieved significant advancements in biomedicine, which have also raised privacy and security challenges. The construction of specialized generalists hinges largely on high-quality datasets, enhanced by techniques like supervised fine-tuning and reinforcement learning from human or AI feedback, and direct preference optimization. However, these leading technologies (e.g., preference learning) are still significantly limited in the open source community due to the scarcity of specialized data. In this paper, we present the UltraMedical collections, which consist of high-quality manual and synthetic datasets in the biomedicine domain, featuring preference annotations across multiple advanced LLMs. By utilizing these datasets, we fine-tune a suite of specialized medical models based on Llama-3 series, demonstrating breathtaking capabilities across various medical benchmarks. Moreover, we develop powerful reward models skilled in biomedical and general reward benchmark, enhancing further online preference learning within the biomedical LLM community.","sentences":["Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains and are moving towards more specialized areas.","Recent advanced proprietary models such as GPT-4 and Gemini have achieved significant advancements in biomedicine, which have also raised privacy and security challenges.","The construction of specialized generalists hinges largely on high-quality datasets, enhanced by techniques like supervised fine-tuning and reinforcement learning from human or AI feedback, and direct preference optimization.","However, these leading technologies (e.g., preference learning) are still significantly limited in the open source community due to the scarcity of specialized data.","In this paper, we present the UltraMedical collections, which consist of high-quality manual and synthetic datasets in the biomedicine domain, featuring preference annotations across multiple advanced LLMs.","By utilizing these datasets, we fine-tune a suite of specialized medical models based on Llama-3 series, demonstrating breathtaking capabilities across various medical benchmarks.","Moreover, we develop powerful reward models skilled in biomedical and general reward benchmark, enhancing further online preference learning within the biomedical LLM community."],"url":"http://arxiv.org/abs/2406.03949v1","category":"cs.CL"}
{"created":"2024-06-06 16:28:04","title":"R-CONV: An Analytical Approach for Efficient Data Reconstruction via Convolutional Gradients","abstract":"In the effort to learn from extensive collections of distributed data, federated learning has emerged as a promising approach for preserving privacy by using a gradient-sharing mechanism instead of exchanging raw data. However, recent studies show that private training data can be leaked through many gradient attacks. While previous analytical-based attacks have successfully reconstructed input data from fully connected layers, their effectiveness diminishes when applied to convolutional layers. This paper introduces an advanced data leakage method to efficiently exploit convolutional layers' gradients. We present a surprising finding: even with non-fully invertible activation functions, such as ReLU, we can analytically reconstruct training samples from the gradients. To the best of our knowledge, this is the first analytical approach that successfully reconstructs convolutional layer inputs directly from the gradients, bypassing the need to reconstruct layers' outputs. Prior research has mainly concentrated on the weight constraints of convolution layers, overlooking the significance of gradient constraints. Our findings demonstrate that existing analytical methods used to estimate the risk of gradient attacks lack accuracy. In some layers, attacks can be launched with less than 5% of the reported constraints.","sentences":["In the effort to learn from extensive collections of distributed data, federated learning has emerged as a promising approach for preserving privacy by using a gradient-sharing mechanism instead of exchanging raw data.","However, recent studies show that private training data can be leaked through many gradient attacks.","While previous analytical-based attacks have successfully reconstructed input data from fully connected layers, their effectiveness diminishes when applied to convolutional layers.","This paper introduces an advanced data leakage method to efficiently exploit convolutional layers' gradients.","We present a surprising finding: even with non-fully invertible activation functions, such as ReLU, we can analytically reconstruct training samples from the gradients.","To the best of our knowledge, this is the first analytical approach that successfully reconstructs convolutional layer inputs directly from the gradients, bypassing the need to reconstruct layers' outputs.","Prior research has mainly concentrated on the weight constraints of convolution layers, overlooking the significance of gradient constraints.","Our findings demonstrate that existing analytical methods used to estimate the risk of gradient attacks lack accuracy.","In some layers, attacks can be launched with less than 5% of the reported constraints."],"url":"http://arxiv.org/abs/2406.04227v1","category":"cs.LG"}
{"created":"2024-06-06 14:12:56","title":"Multiplicity dependence of (multi-)strange hadrons in oxygen-oxygen collisions at $\\sqrt{s_{\\mathrm{NN}}}~=~7$ TeV using EPOS4 and AMPT","abstract":"LHC is anticipated to collect data from oxygen-oxygen ($O+O$) collisions at center-of-mass energy $\\sqrt{s_{\\mathrm{NN}}}~=~7$ TeV to further investigate the particle production mechanisms. In the present work, we report on the predictions of transverse momentum ($p_{\\rm T}$-spectra), $dN/dy$, yield ratios relative to pions, $p_{\\rm T}$-differential ratios of (multi-)strange hadrons in $O+O$ collisions at $\\sqrt{s_{\\mathrm{NN}}}~=~7$ TeV using EPOS4 and AMPT models. Both models differ fundamentally, EPOS4 incorporates a QGP phase, while AMPT focuses on pre-formed hadron interactions. We observed that AMPT qualitatively fails to predict strangeness enhancement in $O+O$ collisions, while EPOS4 predicts a significant enhancement. We also observed the hints of stronger radial flow in EPOS4 compared to AMPT in $O+O$ collisions at $\\sqrt{s_{\\mathrm{NN}}}~=~7$ TeV. AMPT incorporates some flow effects, but EPOS4's implementation of full hydrodynamic flow appears to be significantly more effective in reproducing the experimental data. Both models interestingly predict the final state multiplicity overlap with $pp$, $p+Pb$, and $Pb+Pb$ collisions. The upcoming data on $O+O$ collisions at the LHC is expected to play a crucial role both, constraining the model parameters and significantly refining our understanding of these theoretical models.","sentences":["LHC is anticipated to collect data from oxygen-oxygen ($O+O$) collisions at center-of-mass energy $\\sqrt{s_{\\mathrm{NN}}}~=~7$ TeV to further investigate the particle production mechanisms.","In the present work, we report on the predictions of transverse momentum ($p_{\\rm T}$-spectra), $dN/dy$, yield ratios relative to pions, $p_{\\rm T}$-differential ratios of (multi-)strange hadrons in $O+O$ collisions at $\\sqrt{s_{\\mathrm{NN}}}~=~7$ TeV using EPOS4 and AMPT models.","Both models differ fundamentally, EPOS4 incorporates a QGP phase, while AMPT focuses on pre-formed hadron interactions.","We observed that AMPT qualitatively fails to predict strangeness enhancement in $O+O$ collisions, while EPOS4 predicts a significant enhancement.","We also observed the hints of stronger radial flow in EPOS4 compared to AMPT in $O+O$ collisions at $\\sqrt{s_{\\mathrm{NN}}}~=~7$ TeV. AMPT incorporates some flow effects, but EPOS4's implementation of full hydrodynamic flow appears to be significantly more effective in reproducing the experimental data.","Both models interestingly predict the final state multiplicity overlap with $pp$, $p+Pb$, and $Pb+Pb$ collisions.","The upcoming data on $O+O$ collisions at the LHC is expected to play a crucial role both, constraining the model parameters and significantly refining our understanding of these theoretical models."],"url":"http://arxiv.org/abs/2406.04096v1","category":"hep-ph"}
{"created":"2024-06-06 13:57:15","title":"Contribution of young massive star clusters to Galactic diffuse $\u03b3$-ray emission","abstract":"Context: Young massive stellar clusters (YMSCs) have emerged as potential $\\gamma$-ray sources, after the recent association of a dozen YMSCs with extended $\\gamma$-ray emission. The large size of the detected halos, comparable to that of the wind-blown bubble expected around YMSCs, makes the $\\gamma$-ray detection of individual YMSCs rather challenging. As a result, the emission from most of the Galactic YMSCs could be unresolved, thus contributing to the diffuse $\\gamma$-ray radiation observed along the Galactic Plane.   Aims: In this study, we estimate the possible contribution to the Galactic diffuse $\\gamma$-ray emission from a synthetic population of YMSCs, and we compare it with observations obtained with different experiments, from 1 GeV to hundreds of TeV, in two regions of the Galactic Plane.   Methods: As the population of galactic YMSCs is only known locally, we evaluate the contribution of $\\gamma$-ray emission relying on the simulation of synthetic populations of YMSCs based on the observed properties of local clusters. We compute the $\\gamma$-ray emission from each cluster assuming that the radiation is purely hadronic in nature and produced by cosmic rays accelerated at the cluster's collective wind termination shock.   Results: We find that the $\\gamma$-ray emission from unresolved YMSCs can significantly contribute to the observed Galactic diffuse flux, especially in the inner part of the Galaxy. The result is independent of the assumed particle transport, but an important role is played by Wolf-Rayet stars. The predicted $\\gamma$-ray flux should be considered as a lower limit, given that our calculation neglects the contribution of supernovae exploding in YMSCs.","sentences":["Context: Young massive stellar clusters (YMSCs) have emerged as potential $\\gamma$-ray sources, after the recent association of a dozen YMSCs with extended $\\gamma$-ray emission.","The large size of the detected halos, comparable to that of the wind-blown bubble expected around YMSCs, makes the $\\gamma$-ray detection of individual YMSCs rather challenging.","As a result, the emission from most of the Galactic YMSCs could be unresolved, thus contributing to the diffuse $\\gamma$-ray radiation observed along the Galactic Plane.   ","Aims:","In this study, we estimate the possible contribution to the Galactic diffuse $\\gamma$-ray emission from a synthetic population of YMSCs, and we compare it with observations obtained with different experiments, from 1 GeV to hundreds of TeV, in two regions of the Galactic Plane.   ","Methods: As the population of galactic YMSCs is only known locally, we evaluate the contribution of $\\gamma$-ray emission relying on the simulation of synthetic populations of YMSCs based on the observed properties of local clusters.","We compute the $\\gamma$-ray emission from each cluster assuming that the radiation is purely hadronic in nature and produced by cosmic rays accelerated at the cluster's collective wind termination shock.   ","Results: We find that the $\\gamma$-ray emission from unresolved YMSCs can significantly contribute to the observed Galactic diffuse flux, especially in the inner part of the Galaxy.","The result is independent of the assumed particle transport, but an important role is played by Wolf-Rayet stars.","The predicted $\\gamma$-ray flux should be considered as a lower limit, given that our calculation neglects the contribution of supernovae exploding in YMSCs."],"url":"http://arxiv.org/abs/2406.04087v1","category":"astro-ph.HE"}
{"created":"2024-06-06 13:36:41","title":"Dynamic angular synchronization under smoothness constraints","abstract":"Given an undirected measurement graph $\\mathcal{H} = ([n], \\mathcal{E})$, the classical angular synchronization problem consists of recovering unknown angles $\\theta_1^*,\\dots,\\theta_n^*$ from a collection of noisy pairwise measurements of the form $(\\theta_i^* - \\theta_j^*) \\mod 2\\pi$, for all $\\{i,j\\} \\in \\mathcal{E}$. This problem arises in a variety of applications, including computer vision, time synchronization of distributed networks, and ranking from pairwise comparisons. In this paper, we consider a dynamic version of this problem where the angles, and also the measurement graphs evolve over $T$ time points. Assuming a smoothness condition on the evolution of the latent angles, we derive three algorithms for joint estimation of the angles over all time points. Moreover, for one of the algorithms, we establish non-asymptotic recovery guarantees for the mean-squared error (MSE) under different statistical models. In particular, we show that the MSE converges to zero as $T$ increases under milder conditions than in the static setting. This includes the setting where the measurement graphs are highly sparse and disconnected, and also when the measurement noise is large and can potentially increase with $T$. We complement our theoretical results with experiments on synthetic data.","sentences":["Given an undirected measurement graph $\\mathcal{H} = ([n], \\mathcal{E})$, the classical angular synchronization problem consists of recovering unknown angles $\\theta_1^*,\\dots,\\theta_n^*$ from a collection of noisy pairwise measurements of the form $(\\theta_i^* - \\theta_j^*) \\mod 2\\pi$, for all $\\{i,j\\} \\in \\mathcal{E}$. This problem arises in a variety of applications, including computer vision, time synchronization of distributed networks, and ranking from pairwise comparisons.","In this paper, we consider a dynamic version of this problem where the angles, and also the measurement graphs evolve over $T$ time points.","Assuming a smoothness condition on the evolution of the latent angles, we derive three algorithms for joint estimation of the angles over all time points.","Moreover, for one of the algorithms, we establish non-asymptotic recovery guarantees for the mean-squared error (MSE) under different statistical models.","In particular, we show that the MSE converges to zero as $T$ increases under milder conditions than in the static setting.","This includes the setting where the measurement graphs are highly sparse and disconnected, and also when the measurement noise is large and can potentially increase with $T$. We complement our theoretical results with experiments on synthetic data."],"url":"http://arxiv.org/abs/2406.04071v1","category":"stat.ML"}
{"created":"2024-06-06 12:33:23","title":"A Versatile Collage Visualization Technique","abstract":"Collage techniques are commonly used in visualization to organize a collection of geometric shapes, facilitating the representation of visual features holistically, as seen in word clouds or circular packing diagrams. Typically, packing methods rely on object-space optimization techniques, which often necessitate customizing the optimization process to suit the complexity of geometric primitives and the specific application requirements. In this paper, we introduce a versatile image-space collage technique designed to pack geometric elements into a given shape. Leveraging a differential renderer and image-space losses, our optimization process is highly efficient and can easily accommodate various loss functions. We demonstrate the diverse visual expressiveness of our approach across various visualization applications. The evaluation confirmed the benefits of our method in terms of both visual quality and time performance. The project page is https://szuviz.github.io/pixel-space-collage-technique/.","sentences":["Collage techniques are commonly used in visualization to organize a collection of geometric shapes, facilitating the representation of visual features holistically, as seen in word clouds or circular packing diagrams.","Typically, packing methods rely on object-space optimization techniques, which often necessitate customizing the optimization process to suit the complexity of geometric primitives and the specific application requirements.","In this paper, we introduce a versatile image-space collage technique designed to pack geometric elements into a given shape.","Leveraging a differential renderer and image-space losses, our optimization process is highly efficient and can easily accommodate various loss functions.","We demonstrate the diverse visual expressiveness of our approach across various visualization applications.","The evaluation confirmed the benefits of our method in terms of both visual quality and time performance.","The project page is https://szuviz.github.io/pixel-space-collage-technique/."],"url":"http://arxiv.org/abs/2406.04008v1","category":"cs.GR"}
{"created":"2024-06-06 12:15:39","title":"Exploring Topic Modelling of User Reviews as a Monitoring Mechanism for Emergent Issues Within Social VR Communities","abstract":"Users of social virtual reality (VR) platforms often use user reviews to document incidents of witnessed and/or experienced user harassment. However, at present, research has yet to be explore utilising this data as a monitoring mechanism to identify emergent issues within social VR communities. Such a system would be of much benefit to developers and researchers as it would enable the automatic identification of emergent issues as they occur, provide a means of longitudinally analysing harassment, and reduce the reliance on alternative, high cost, monitoring methodologies, e.g. observation or interview studies. To contribute towards the development of such a system, we collected approximately 40,000 Rec Room user reviews from the Steam storefront. We then analysed our dataset's sentiment, word/term frequencies, and conducted a topic modelling analysis of the negative reviews detected in our dataset. We report our approach was capable of longitudinally monitoring changes in review sentiment and identifying high level themes related to types of harassment known to occur in social VR platforms.","sentences":["Users of social virtual reality (VR) platforms often use user reviews to document incidents of witnessed and/or experienced user harassment.","However, at present, research has yet to be explore utilising this data as a monitoring mechanism to identify emergent issues within social VR communities.","Such a system would be of much benefit to developers and researchers as it would enable the automatic identification of emergent issues as they occur, provide a means of longitudinally analysing harassment, and reduce the reliance on alternative, high cost, monitoring methodologies, e.g. observation or interview studies.","To contribute towards the development of such a system, we collected approximately 40,000 Rec Room user reviews from the Steam storefront.","We then analysed our dataset's sentiment, word/term frequencies, and conducted a topic modelling analysis of the negative reviews detected in our dataset.","We report our approach was capable of longitudinally monitoring changes in review sentiment and identifying high level themes related to types of harassment known to occur in social VR platforms."],"url":"http://arxiv.org/abs/2406.03994v1","category":"cs.HC"}
{"created":"2024-06-06 10:17:52","title":"Beyond Similarity: Personalized Federated Recommendation with Composite Aggregation","abstract":"Federated recommendation aims to collect global knowledge by aggregating local models from massive devices, to provide recommendations while ensuring privacy. Current methods mainly leverage aggregation functions invented by federated vision community to aggregate parameters from similar clients, e.g., clustering aggregation. Despite considerable performance, we argue that it is suboptimal to apply them to federated recommendation directly. This is mainly reflected in the disparate model architectures. Different from structured parameters like convolutional neural networks in federated vision, federated recommender models usually distinguish itself by employing one-to-one item embedding table. Such a discrepancy induces the challenging embedding skew issue, which continually updates the trained embeddings but ignores the non-trained ones during aggregation, thus failing to predict future items accurately. To this end, we propose a personalized Federated recommendation model with Composite Aggregation (FedCA), which not only aggregates similar clients to enhance trained embeddings, but also aggregates complementary clients to update non-trained embeddings. Besides, we formulate the overall learning process into a unified optimization algorithm to jointly learn the similarity and complementarity. Extensive experiments on several real-world datasets substantiate the effectiveness of our proposed model. The source codes are available at https://github.com/hongleizhang/FedCA.","sentences":["Federated recommendation aims to collect global knowledge by aggregating local models from massive devices, to provide recommendations while ensuring privacy.","Current methods mainly leverage aggregation functions invented by federated vision community to aggregate parameters from similar clients, e.g., clustering aggregation.","Despite considerable performance, we argue that it is suboptimal to apply them to federated recommendation directly.","This is mainly reflected in the disparate model architectures.","Different from structured parameters like convolutional neural networks in federated vision, federated recommender models usually distinguish itself by employing one-to-one item embedding table.","Such a discrepancy induces the challenging embedding skew issue, which continually updates the trained embeddings but ignores the non-trained ones during aggregation, thus failing to predict future items accurately.","To this end, we propose a personalized Federated recommendation model with Composite Aggregation (FedCA), which not only aggregates similar clients to enhance trained embeddings, but also aggregates complementary clients to update non-trained embeddings.","Besides, we formulate the overall learning process into a unified optimization algorithm to jointly learn the similarity and complementarity.","Extensive experiments on several real-world datasets substantiate the effectiveness of our proposed model.","The source codes are available at https://github.com/hongleizhang/FedCA."],"url":"http://arxiv.org/abs/2406.03933v1","category":"cs.CR"}
{"created":"2024-06-06 10:03:42","title":"Knowledge Transfer, Knowledge Gaps, and Knowledge Silos in Citation Networks","abstract":"The advancement of science relies on the exchange of ideas across disciplines and the integration of diverse knowledge domains. However, tracking knowledge flows and interdisciplinary integration in rapidly evolving, multidisciplinary fields remains a significant challenge. This work introduces a novel network analysis framework to study the dynamics of knowledge transfer directly from citation data. By applying dynamic community detection to cumulative, time-evolving citation networks, we can identify research areas as groups of papers sharing knowledge sources and outputs. Our analysis characterises the life-cycles and knowledge transfer patterns of these dynamic communities over time. We demonstrate our approach through a case study of eXplainable Artificial Intelligence (XAI) research, an emerging interdisciplinary field at the intersection of machine learning, statistics, and psychology. Key findings include: (i) knowledge transfer between these important foundational topics and the contemporary topics in XAI research is limited, and the extent of knowledge transfer varies across different contemporary research topics; (ii) certain application domains exist as isolated \"knowledge silos\"; (iii) significant \"knowledge gaps\" are identified between related XAI research areas, suggesting opportunities for cross-pollination and improved knowledge integration. By mapping interdisciplinary integration and bridging knowledge gaps, this work can inform strategies to synthesise ideas from disparate sources and drive innovation. More broadly, our proposed framework enables new insights into the evolution of knowledge ecosystems directly from citation data, with applications spanning literature review, research planning, and science policy.","sentences":["The advancement of science relies on the exchange of ideas across disciplines and the integration of diverse knowledge domains.","However, tracking knowledge flows and interdisciplinary integration in rapidly evolving, multidisciplinary fields remains a significant challenge.","This work introduces a novel network analysis framework to study the dynamics of knowledge transfer directly from citation data.","By applying dynamic community detection to cumulative, time-evolving citation networks, we can identify research areas as groups of papers sharing knowledge sources and outputs.","Our analysis characterises the life-cycles and knowledge transfer patterns of these dynamic communities over time.","We demonstrate our approach through a case study of eXplainable Artificial Intelligence (XAI) research, an emerging interdisciplinary field at the intersection of machine learning, statistics, and psychology.","Key findings include: (i) knowledge transfer between these important foundational topics and the contemporary topics in XAI research is limited, and the extent of knowledge transfer varies across different contemporary research topics; (ii) certain application domains exist as isolated \"knowledge silos\"; (iii) significant \"knowledge gaps\" are identified between related XAI research areas, suggesting opportunities for cross-pollination and improved knowledge integration.","By mapping interdisciplinary integration and bridging knowledge gaps, this work can inform strategies to synthesise ideas from disparate sources and drive innovation.","More broadly, our proposed framework enables new insights into the evolution of knowledge ecosystems directly from citation data, with applications spanning literature review, research planning, and science policy."],"url":"http://arxiv.org/abs/2406.03921v1","category":"cs.SI"}
{"created":"2024-06-06 10:02:06","title":"Vectorized Conditional Neural Fields: A Framework for Solving Time-dependent Parametric Partial Differential Equations","abstract":"Transformer models are increasingly used for solving Partial Differential Equations (PDEs). Several adaptations have been proposed, all of which suffer from the typical problems of Transformers, such as quadratic memory and time complexity. Furthermore, all prevalent architectures for PDE solving lack at least one of several desirable properties of an ideal surrogate model, such as (i) generalization to PDE parameters not seen during training, (ii) spatial and temporal zero-shot super-resolution, (iii) continuous temporal extrapolation, (iv) support for 1D, 2D, and 3D PDEs, and (v) efficient inference for longer temporal rollouts. To address these limitations, we propose Vectorized Conditional Neural Fields (VCNeFs), which represent the solution of time-dependent PDEs as neural fields. Contrary to prior methods, however, VCNeFs compute, for a set of multiple spatio-temporal query points, their solutions in parallel and model their dependencies through attention mechanisms. Moreover, VCNeF can condition the neural field on both the initial conditions and the parameters of the PDEs. An extensive set of experiments demonstrates that VCNeFs are competitive with and often outperform existing ML-based surrogate models.","sentences":["Transformer models are increasingly used for solving Partial Differential Equations (PDEs).","Several adaptations have been proposed, all of which suffer from the typical problems of Transformers, such as quadratic memory and time complexity.","Furthermore, all prevalent architectures for PDE solving lack at least one of several desirable properties of an ideal surrogate model, such as (i) generalization to PDE parameters not seen during training, (ii) spatial and temporal zero-shot super-resolution, (iii) continuous temporal extrapolation, (iv) support for 1D, 2D, and 3D PDEs, and (v) efficient inference for longer temporal rollouts.","To address these limitations, we propose Vectorized Conditional Neural Fields (VCNeFs), which represent the solution of time-dependent PDEs as neural fields.","Contrary to prior methods, however, VCNeFs compute, for a set of multiple spatio-temporal query points, their solutions in parallel and model their dependencies through attention mechanisms.","Moreover, VCNeF can condition the neural field on both the initial conditions and the parameters of the PDEs.","An extensive set of experiments demonstrates that VCNeFs are competitive with and often outperform existing ML-based surrogate models."],"url":"http://arxiv.org/abs/2406.03919v1","category":"cs.LG"}
{"created":"2024-06-06 09:56:49","title":"ArMeme: Propagandistic Content in Arabic Memes","abstract":"With the rise of digital communication, memes have become a significant medium for cultural and political expression that is often used to mislead audiences. Identification of such misleading and persuasive multimodal content has become more important among various stakeholders, including social media platforms, policymakers, and the broader society as they often cause harm to individuals, organizations, and/or society. While there has been effort to develop AI-based automatic systems for resource-rich languages (e.g., English), it is relatively little to none for medium to low resource languages. In this study, we focused on developing an Arabic memes dataset with manual annotations of propagandistic content. We annotated ~6K Arabic memes collected from various social media platforms, which is a first resource for Arabic multimodal research. We provide a comprehensive analysis aiming to develop computational tools for their detection. We will make them publicly available for the community.","sentences":["With the rise of digital communication, memes have become a significant medium for cultural and political expression that is often used to mislead audiences.","Identification of such misleading and persuasive multimodal content has become more important among various stakeholders, including social media platforms, policymakers, and the broader society as they often cause harm to individuals, organizations, and/or society.","While there has been effort to develop AI-based automatic systems for resource-rich languages (e.g., English), it is relatively little to none for medium to low resource languages.","In this study, we focused on developing an Arabic memes dataset with manual annotations of propagandistic content.","We annotated ~6K Arabic memes collected from various social media platforms, which is a first resource for Arabic multimodal research.","We provide a comprehensive analysis aiming to develop computational tools for their detection.","We will make them publicly available for the community."],"url":"http://arxiv.org/abs/2406.03916v1","category":"cs.CL"}
{"created":"2024-06-06 09:51:30","title":"GenSafe: A Generalizable Safety Enhancer for Safe Reinforcement Learning Algorithms Based on Reduced Order Markov Decision Process Model","abstract":"Although deep reinforcement learning has demonstrated impressive achievements in controlling various autonomous systems, e.g., autonomous vehicles or humanoid robots, its inherent reliance on random exploration raises safety concerns in their real-world applications. To improve system safety during the learning process, a variety of Safe Reinforcement Learning (SRL) algorithms have been proposed, which usually incorporate safety constraints within the Constrained Markov Decision Process (CMDP) framework. However, the efficacy of these SRL algorithms often relies on accurate function approximations, a task that is notably challenging to accomplish in the early learning stages due to data insufficiency. To address this problem, we introduce a Genralizable Safety enhancer (GenSafe) in this work. Leveraging model order reduction techniques, we first construct a Reduced Order Markov Decision Process (ROMDP) as a low-dimensional proxy for the original cost function in CMDP. Then, by solving ROMDP-based constraints that are reformulated from the original cost constraints, the proposed GenSafe refines the actions taken by the agent to enhance the possibility of constraint satisfaction. Essentially, GenSafe acts as an additional safety layer for SRL algorithms, offering broad compatibility across diverse SRL approaches. The performance of GenSafe is examined on multiple SRL benchmark problems. The results show that, it is not only able to improve the safety performance, especially in the early learning phases, but also to maintain the task performance at a satisfactory level.","sentences":["Although deep reinforcement learning has demonstrated impressive achievements in controlling various autonomous systems, e.g., autonomous vehicles or humanoid robots, its inherent reliance on random exploration raises safety concerns in their real-world applications.","To improve system safety during the learning process, a variety of Safe Reinforcement Learning (SRL) algorithms have been proposed, which usually incorporate safety constraints within the Constrained Markov Decision Process (CMDP) framework.","However, the efficacy of these SRL algorithms often relies on accurate function approximations, a task that is notably challenging to accomplish in the early learning stages due to data insufficiency.","To address this problem, we introduce a Genralizable Safety enhancer (GenSafe) in this work.","Leveraging model order reduction techniques, we first construct a Reduced Order Markov Decision Process (ROMDP) as a low-dimensional proxy for the original cost function in CMDP.","Then, by solving ROMDP-based constraints that are reformulated from the original cost constraints, the proposed GenSafe refines the actions taken by the agent to enhance the possibility of constraint satisfaction.","Essentially, GenSafe acts as an additional safety layer for SRL algorithms, offering broad compatibility across diverse SRL approaches.","The performance of GenSafe is examined on multiple SRL benchmark problems.","The results show that, it is not only able to improve the safety performance, especially in the early learning phases, but also to maintain the task performance at a satisfactory level."],"url":"http://arxiv.org/abs/2406.03912v1","category":"cs.AI"}
{"created":"2024-06-06 09:38:06","title":"Data-Centric Label Smoothing for Explainable Glaucoma Screening from Eye Fundus Images","abstract":"As current computing capabilities increase, modern machine learning and computer vision system tend to increase in complexity, mostly by means of larger models and advanced optimization strategies. Although often neglected, in many problems there is also much to be gained by considering potential improvements in understanding and better leveraging already-available training data, including annotations. This so-called data-centric approach can lead to substantial performance increases, sometimes beyond what can be achieved by larger models. In this paper we adopt such an approach for the task of justifiable glaucoma screening from retinal images. In particular, we focus on how to combine information from multiple annotators of different skills into a tailored label smoothing scheme that allows us to better employ a large collection of fundus images, instead of discarding samples suffering from inter-rater variability. Internal validation results indicate that our bespoke label smoothing approach surpasses the performance of a standard resnet50 model and also the same model trained with conventional label smoothing techniques, in particular for the multi-label scenario of predicting clinical reasons of glaucoma likelihood in a highly imbalanced screening context. Our code is made available at github.com/agaldran/justraigs .","sentences":["As current computing capabilities increase, modern machine learning and computer vision system tend to increase in complexity, mostly by means of larger models and advanced optimization strategies.","Although often neglected, in many problems there is also much to be gained by considering potential improvements in understanding and better leveraging already-available training data, including annotations.","This so-called data-centric approach can lead to substantial performance increases, sometimes beyond what can be achieved by larger models.","In this paper we adopt such an approach for the task of justifiable glaucoma screening from retinal images.","In particular, we focus on how to combine information from multiple annotators of different skills into a tailored label smoothing scheme that allows us to better employ a large collection of fundus images, instead of discarding samples suffering from inter-rater variability.","Internal validation results indicate that our bespoke label smoothing approach surpasses the performance of a standard resnet50 model and also the same model trained with conventional label smoothing techniques, in particular for the multi-label scenario of predicting clinical reasons of glaucoma likelihood in a highly imbalanced screening context.","Our code is made available at github.com/agaldran/justraigs ."],"url":"http://arxiv.org/abs/2406.03903v1","category":"eess.IV"}
{"created":"2024-06-06 09:36:14","title":"HeSum: a Novel Dataset for Abstractive Text Summarization in Hebrew","abstract":"While large language models (LLMs) excel in various natural language tasks in English, their performance in lower-resourced languages like Hebrew, especially for generative tasks such as abstractive summarization, remains unclear. The high morphological richness in Hebrew adds further challenges due to the ambiguity in sentence comprehension and the complexities in meaning construction. In this paper, we address this resource and evaluation gap by introducing HeSum, a novel benchmark specifically designed for abstractive text summarization in Modern Hebrew. HeSum consists of 10,000 article-summary pairs sourced from Hebrew news websites written by professionals. Linguistic analysis confirms HeSum's high abstractness and unique morphological challenges. We show that HeSum presents distinct difficulties for contemporary state-of-the-art LLMs, establishing it as a valuable testbed for generative language technology in Hebrew, and MRLs generative challenges in general.","sentences":["While large language models (LLMs) excel in various natural language tasks in English, their performance in lower-resourced languages like Hebrew, especially for generative tasks such as abstractive summarization, remains unclear.","The high morphological richness in Hebrew adds further challenges due to the ambiguity in sentence comprehension and the complexities in meaning construction.","In this paper, we address this resource and evaluation gap by introducing HeSum, a novel benchmark specifically designed for abstractive text summarization in Modern Hebrew.","HeSum consists of 10,000 article-summary pairs sourced from Hebrew news websites written by professionals.","Linguistic analysis confirms HeSum's high abstractness and unique morphological challenges.","We show that HeSum presents distinct difficulties for contemporary state-of-the-art LLMs, establishing it as a valuable testbed for generative language technology in Hebrew, and MRLs generative challenges in general."],"url":"http://arxiv.org/abs/2406.03897v1","category":"cs.CL"}
{"created":"2024-06-06 09:28:08","title":"How Good is Zero-Shot MT Evaluation for Low Resource Indian Languages?","abstract":"While machine translation evaluation has been studied primarily for high-resource languages, there has been a recent interest in evaluation for low-resource languages due to the increasing availability of data and models. In this paper, we focus on a zero-shot evaluation setting focusing on low-resource Indian languages, namely Assamese, Kannada, Maithili, and Punjabi. We collect sufficient Multi-Dimensional Quality Metrics (MQM) and Direct Assessment (DA) annotations to create test sets and meta-evaluate a plethora of automatic evaluation metrics. We observe that even for learned metrics, which are known to exhibit zero-shot performance, the Kendall Tau and Pearson correlations with human annotations are only as high as 0.32 and 0.45. Synthetic data approaches show mixed results and overall do not help close the gap by much for these languages. This indicates that there is still a long way to go for low-resource evaluation.","sentences":["While machine translation evaluation has been studied primarily for high-resource languages, there has been a recent interest in evaluation for low-resource languages due to the increasing availability of data and models.","In this paper, we focus on a zero-shot evaluation setting focusing on low-resource Indian languages, namely Assamese, Kannada, Maithili, and Punjabi.","We collect sufficient Multi-Dimensional Quality Metrics (MQM) and Direct Assessment (DA) annotations to create test sets and meta-evaluate a plethora of automatic evaluation metrics.","We observe that even for learned metrics, which are known to exhibit zero-shot performance, the Kendall Tau and Pearson correlations with human annotations are only as high as 0.32 and 0.45.","Synthetic data approaches show mixed results and overall do not help close the gap by much for these languages.","This indicates that there is still a long way to go for low-resource evaluation."],"url":"http://arxiv.org/abs/2406.03893v1","category":"cs.CL"}
{"created":"2024-06-06 09:21:13","title":"Spontaneous Speech-Based Suicide Risk Detection Using Whisper and Large Language Models","abstract":"The early detection of suicide risk is important since it enables the intervention to prevent potential suicide attempts. This paper studies the automatic detection of suicide risk based on spontaneous speech from adolescents, and collects a Mandarin dataset with 15 hours of suicide speech from more than a thousand adolescents aged from ten to eighteen for our experiments. To leverage the diverse acoustic and linguistic features embedded in spontaneous speech, both the Whisper speech model and textual large language models (LLMs) are used for suicide risk detection. Both all-parameter finetuning and parameter-efficient finetuning approaches are used to adapt the pre-trained models for suicide risk detection, and multiple audio-text fusion approaches are evaluated to combine the representations of Whisper and the LLM. The proposed system achieves a detection accuracy of 0.807 and an F1-score of 0.846 on the test set with 119 subjects, indicating promising potential for real suicide risk detection applications.","sentences":["The early detection of suicide risk is important since it enables the intervention to prevent potential suicide attempts.","This paper studies the automatic detection of suicide risk based on spontaneous speech from adolescents, and collects a Mandarin dataset with 15 hours of suicide speech from more than a thousand adolescents aged from ten to eighteen for our experiments.","To leverage the diverse acoustic and linguistic features embedded in spontaneous speech, both the Whisper speech model and textual large language models (LLMs) are used for suicide risk detection.","Both all-parameter finetuning and parameter-efficient finetuning approaches are used to adapt the pre-trained models for suicide risk detection, and multiple audio-text fusion approaches are evaluated to combine the representations of Whisper and the LLM.","The proposed system achieves a detection accuracy of 0.807 and an F1-score of 0.846 on the test set with 119 subjects, indicating promising potential for real suicide risk detection applications."],"url":"http://arxiv.org/abs/2406.03882v1","category":"cs.CL"}
{"created":"2024-06-06 09:18:42","title":"Evaluating the IWSLT2023 Speech Translation Tasks: Human Annotations, Automatic Metrics, and Segmentation","abstract":"Human evaluation is a critical component in machine translation system development and has received much attention in text translation research. However, little prior work exists on the topic of human evaluation for speech translation, which adds additional challenges such as noisy data and segmentation mismatches. We take first steps to fill this gap by conducting a comprehensive human evaluation of the results of several shared tasks from the last International Workshop on Spoken Language Translation (IWSLT 2023). We propose an effective evaluation strategy based on automatic resegmentation and direct assessment with segment context. Our analysis revealed that: 1) the proposed evaluation strategy is robust and scores well-correlated with other types of human judgements; 2) automatic metrics are usually, but not always, well-correlated with direct assessment scores; and 3) COMET as a slightly stronger automatic metric than chrF, despite the segmentation noise introduced by the resegmentation step systems. We release the collected human-annotated data in order to encourage further investigation.","sentences":["Human evaluation is a critical component in machine translation system development and has received much attention in text translation research.","However, little prior work exists on the topic of human evaluation for speech translation, which adds additional challenges such as noisy data and segmentation mismatches.","We take first steps to fill this gap by conducting a comprehensive human evaluation of the results of several shared tasks from the last International Workshop on Spoken Language Translation (IWSLT 2023).","We propose an effective evaluation strategy based on automatic resegmentation and direct assessment with segment context.","Our analysis revealed that: 1) the proposed evaluation strategy is robust and scores well-correlated with other types of human judgements; 2) automatic metrics are usually, but not always, well-correlated with direct assessment scores; and 3) COMET as a slightly stronger automatic metric than chrF, despite the segmentation noise introduced by the resegmentation step systems.","We release the collected human-annotated data in order to encourage further investigation."],"url":"http://arxiv.org/abs/2406.03881v1","category":"cs.CL"}
{"created":"2024-06-06 09:17:40","title":"Memorization in deep learning: A survey","abstract":"Deep Learning (DL) powered by Deep Neural Networks (DNNs) has revolutionized various domains, yet understanding the intricacies of DNN decision-making and learning processes remains a significant challenge. Recent investigations have uncovered an interesting memorization phenomenon in which DNNs tend to memorize specific details from examples rather than learning general patterns, affecting model generalization, security, and privacy. This raises critical questions about the nature of generalization in DNNs and their susceptibility to security breaches. In this survey, we present a systematic framework to organize memorization definitions based on the generalization and security/privacy domains and summarize memorization evaluation methods at both the example and model levels. Through a comprehensive literature review, we explore DNN memorization behaviors and their impacts on security and privacy. We also introduce privacy vulnerabilities caused by memorization and the phenomenon of forgetting and explore its connection with memorization. Furthermore, we spotlight various applications leveraging memorization and forgetting mechanisms, including noisy label learning, privacy preservation, and model enhancement. This survey offers the first-in-kind understanding of memorization in DNNs, providing insights into its challenges and opportunities for enhancing AI development while addressing critical ethical concerns.","sentences":["Deep Learning (DL) powered by Deep Neural Networks (DNNs) has revolutionized various domains, yet understanding the intricacies of DNN decision-making and learning processes remains a significant challenge.","Recent investigations have uncovered an interesting memorization phenomenon in which DNNs tend to memorize specific details from examples rather than learning general patterns, affecting model generalization, security, and privacy.","This raises critical questions about the nature of generalization in DNNs and their susceptibility to security breaches.","In this survey, we present a systematic framework to organize memorization definitions based on the generalization and security/privacy domains and summarize memorization evaluation methods at both the example and model levels.","Through a comprehensive literature review, we explore DNN memorization behaviors and their impacts on security and privacy.","We also introduce privacy vulnerabilities caused by memorization and the phenomenon of forgetting and explore its connection with memorization.","Furthermore, we spotlight various applications leveraging memorization and forgetting mechanisms, including noisy label learning, privacy preservation, and model enhancement.","This survey offers the first-in-kind understanding of memorization in DNNs, providing insights into its challenges and opportunities for enhancing AI development while addressing critical ethical concerns."],"url":"http://arxiv.org/abs/2406.03880v1","category":"cs.LG"}
{"created":"2024-06-06 09:12:30","title":"Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving","abstract":"In an era marked by the rapid scaling of foundation models, autonomous driving technologies are approaching a transformative threshold where end-to-end autonomous driving (E2E-AD) emerges due to its potential of scaling up in the data-driven manner. However, existing E2E-AD methods are mostly evaluated under the open-loop log-replay manner with L2 errors and collision rate as metrics (e.g., in nuScenes), which could not fully reflect the driving performance of algorithms as recently acknowledged in the community. For those E2E-AD methods evaluated under the closed-loop protocol, they are tested in fixed routes (e.g., Town05Long and Longest6 in CARLA) with the driving score as metrics, which is known for high variance due to the unsmoothed metric function and large randomness in the long route. Besides, these methods usually collect their own data for training, which makes algorithm-level fair comparison infeasible.   To fulfill the paramount need of comprehensive, realistic, and fair testing environments for Full Self-Driving (FSD), we present Bench2Drive, the first benchmark for evaluating E2E-AD systems' multiple abilities in a closed-loop manner. Bench2Drive's official training data consists of 2 million fully annotated frames, collected from 10000 short clips uniformly distributed under 44 interactive scenarios (cut-in, overtaking, detour, etc), 23 weathers (sunny, foggy, rainy, etc), and 12 towns (urban, village, university, etc) in CARLA v2. Its evaluation protocol requires E2E-AD models to pass 44 interactive scenarios under different locations and weathers which sums up to 220 routes and thus provides a comprehensive and disentangled assessment about their driving capability under different situations. We implement state-of-the-art E2E-AD models and evaluate them in Bench2Drive, providing insights regarding current status and future directions.","sentences":["In an era marked by the rapid scaling of foundation models, autonomous driving technologies are approaching a transformative threshold where end-to-end autonomous driving (E2E-AD) emerges due to its potential of scaling up in the data-driven manner.","However, existing E2E-AD methods are mostly evaluated under the open-loop log-replay manner with L2 errors and collision rate as metrics (e.g., in nuScenes), which could not fully reflect the driving performance of algorithms as recently acknowledged in the community.","For those E2E-AD methods evaluated under the closed-loop protocol, they are tested in fixed routes (e.g., Town05Long and Longest6 in CARLA) with the driving score as metrics, which is known for high variance due to the unsmoothed metric function and large randomness in the long route.","Besides, these methods usually collect their own data for training, which makes algorithm-level fair comparison infeasible.   ","To fulfill the paramount need of comprehensive, realistic, and fair testing environments for Full Self-Driving (FSD), we present Bench2Drive, the first benchmark for evaluating E2E-AD systems' multiple abilities in a closed-loop manner.","Bench2Drive's official training data consists of 2 million fully annotated frames, collected from 10000 short clips uniformly distributed under 44 interactive scenarios (cut-in, overtaking, detour, etc), 23 weathers (sunny, foggy, rainy, etc), and 12 towns (urban, village, university, etc) in CARLA v2.","Its evaluation protocol requires E2E-AD models to pass 44 interactive scenarios under different locations and weathers which sums up to 220 routes and thus provides a comprehensive and disentangled assessment about their driving capability under different situations.","We implement state-of-the-art E2E-AD models and evaluate them in Bench2Drive, providing insights regarding current status and future directions."],"url":"http://arxiv.org/abs/2406.03877v1","category":"cs.RO"}
{"created":"2024-06-06 09:07:20","title":"Hidden collective oscillations in a disordered mean-field spin model with non-reciprocal interactions","abstract":"We study the effect of introducing separable quenched disorder on a non-equilibrium mean-field spin model exhibiting a phase transition to an oscillating state in the absence of disorder, due to non-reciprocal interactions. In the disordered model, the magnetisation and its time derivative no longer carry the signature of the phase transition to an oscillating state. However, thanks to the separable (Mattis-type) form of the disorder, the presence of oscillations can be revealed by introducing a specific, disorder-dependent observable. We also introduce generalised linear and non-linear susceptibilities associated either with the magnetisation or with its time derivative. While linear susceptibilities show no sign of a phase transition, the third-order susceptibilities present a clear signature of the onset of an oscillating phase. In addition, we show that the overlap distribution also provides evidence for the presence of oscillations, without explicit knowledge of the disorder.","sentences":["We study the effect of introducing separable quenched disorder on a non-equilibrium mean-field spin model exhibiting a phase transition to an oscillating state in the absence of disorder, due to non-reciprocal interactions.","In the disordered model, the magnetisation and its time derivative no longer carry the signature of the phase transition to an oscillating state.","However, thanks to the separable (Mattis-type) form of the disorder, the presence of oscillations can be revealed by introducing a specific, disorder-dependent observable.","We also introduce generalised linear and non-linear susceptibilities associated either with the magnetisation or with its time derivative.","While linear susceptibilities show no sign of a phase transition, the third-order susceptibilities present a clear signature of the onset of an oscillating phase.","In addition, we show that the overlap distribution also provides evidence for the presence of oscillations, without explicit knowledge of the disorder."],"url":"http://arxiv.org/abs/2406.03874v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-06 09:04:48","title":"Quantum Implicit Neural Representations","abstract":"Implicit neural representations have emerged as a powerful paradigm to represent signals such as images and sounds. This approach aims to utilize neural networks to parameterize the implicit function of the signal. However, when representing implicit functions, traditional neural networks such as ReLU-based multilayer perceptrons face challenges in accurately modeling high-frequency components of signals. Recent research has begun to explore the use of Fourier Neural Networks (FNNs) to overcome this limitation. In this paper, we propose Quantum Implicit Representation Network (QIREN), a novel quantum generalization of FNNs. Furthermore, through theoretical analysis, we demonstrate that QIREN possesses a quantum advantage over classical FNNs. Lastly, we conducted experiments in signal representation, image superresolution, and image generation tasks to show the superior performance of QIREN compared to state-of-the-art (SOTA) models. Our work not only incorporates quantum advantages into implicit neural representations but also uncovers a promising application direction for Quantum Neural Networks.","sentences":["Implicit neural representations have emerged as a powerful paradigm to represent signals such as images and sounds.","This approach aims to utilize neural networks to parameterize the implicit function of the signal.","However, when representing implicit functions, traditional neural networks such as ReLU-based multilayer perceptrons face challenges in accurately modeling high-frequency components of signals.","Recent research has begun to explore the use of Fourier Neural Networks (FNNs) to overcome this limitation.","In this paper, we propose Quantum Implicit Representation Network (QIREN), a novel quantum generalization of FNNs.","Furthermore, through theoretical analysis, we demonstrate that QIREN possesses a quantum advantage over classical FNNs.","Lastly, we conducted experiments in signal representation, image superresolution, and image generation tasks to show the superior performance of QIREN compared to state-of-the-art (SOTA) models.","Our work not only incorporates quantum advantages into implicit neural representations but also uncovers a promising application direction for Quantum Neural Networks."],"url":"http://arxiv.org/abs/2406.03873v1","category":"cs.LG"}
{"created":"2024-06-06 08:59:08","title":"GOOSE: Goal-Conditioned Reinforcement Learning for Safety-Critical Scenario Generation","abstract":"Scenario-based testing is considered state-of-the-art for verifying and validating Advanced Driver Assistance Systems (ADASs) and Automated Driving Systems (ADSs). However, the practical application of scenario-based testing requires an efficient method to generate or collect the scenarios that are needed for the safety assessment. In this paper, we propose Goal-conditioned Scenario Generation (GOOSE), a goal-conditioned reinforcement learning (RL) approach that automatically generates safety-critical scenarios to challenge ADASs or ADSs. In order to simultaneously set up and optimize scenarios, we propose to control vehicle trajectories at the scenario level. Each step in the RL framework corresponds to a scenario simulation. We use Non-Uniform Rational B-Splines (NURBS) for trajectory modeling. To guide the goal-conditioned agent, we formulate test-specific, constraint-based goals inspired by the OpenScenario Domain Specific Language(DSL). Through experiments conducted on multiple pre-crash scenarios derived from UN Regulation No. 157 for Active Lane Keeping Systems (ALKS), we demonstrate the effectiveness of GOOSE in generating scenarios that lead to safety-critical events.","sentences":["Scenario-based testing is considered state-of-the-art for verifying and validating Advanced Driver Assistance Systems (ADASs) and Automated Driving Systems (ADSs).","However, the practical application of scenario-based testing requires an efficient method to generate or collect the scenarios that are needed for the safety assessment.","In this paper, we propose Goal-conditioned Scenario Generation (GOOSE), a goal-conditioned reinforcement learning (RL) approach that automatically generates safety-critical scenarios to challenge ADASs or ADSs.","In order to simultaneously set up and optimize scenarios, we propose to control vehicle trajectories at the scenario level.","Each step in the RL framework corresponds to a scenario simulation.","We use Non-Uniform Rational B-Splines (NURBS) for trajectory modeling.","To guide the goal-conditioned agent, we formulate test-specific, constraint-based goals inspired by the OpenScenario Domain Specific Language(DSL).","Through experiments conducted on multiple pre-crash scenarios derived from UN Regulation No. 157 for Active Lane Keeping Systems (ALKS), we demonstrate the effectiveness of GOOSE in generating scenarios that lead to safety-critical events."],"url":"http://arxiv.org/abs/2406.03870v1","category":"cs.SE"}
{"created":"2024-06-06 08:51:26","title":"Semantic Similarity Score for Measuring Visual Similarity at Semantic Level","abstract":"Semantic communication, as a revolutionary communication architecture, is considered a promising novel communication paradigm. Unlike traditional symbol-based error-free communication systems, semantic-based visual communication systems extract, compress, transmit, and reconstruct images at the semantic level. However, widely used image similarity evaluation metrics, whether pixel-based MSE or PSNR or structure-based MS-SSIM, struggle to accurately measure the loss of semantic-level information of the source during system transmission. This presents challenges in evaluating the performance of visual semantic communication systems, especially when comparing them with traditional communication systems. To address this, we propose a semantic evaluation metric -- SeSS (Semantic Similarity Score), based on Scene Graph Generation and graph matching, which shifts the similarity scores between images into semantic-level graph matching scores. Meanwhile, semantic similarity scores for tens of thousands of image pairs are manually annotated to fine-tune the hyperparameters in the graph matching algorithm, aligning the metric more closely with human semantic perception. The performance of the SeSS is tested on different datasets, including (1)images transmitted by traditional and semantic communication systems at different compression rates, (2)images transmitted by traditional and semantic communication systems at different signal-to-noise ratios, (3)images generated by large-scale model with different noise levels introduced, and (4)cases of images subjected to certain special transformations. The experiments demonstrate the effectiveness of SeSS, indicating that the metric can measure the semantic-level differences in semantic-level information of images and can be used for evaluation in visual semantic communication systems.","sentences":["Semantic communication, as a revolutionary communication architecture, is considered a promising novel communication paradigm.","Unlike traditional symbol-based error-free communication systems, semantic-based visual communication systems extract, compress, transmit, and reconstruct images at the semantic level.","However, widely used image similarity evaluation metrics, whether pixel-based MSE or PSNR or structure-based MS-SSIM, struggle to accurately measure the loss of semantic-level information of the source during system transmission.","This presents challenges in evaluating the performance of visual semantic communication systems, especially when comparing them with traditional communication systems.","To address this, we propose a semantic evaluation metric -- SeSS (Semantic Similarity Score), based on Scene Graph Generation and graph matching, which shifts the similarity scores between images into semantic-level graph matching scores.","Meanwhile, semantic similarity scores for tens of thousands of image pairs are manually annotated to fine-tune the hyperparameters in the graph matching algorithm, aligning the metric more closely with human semantic perception.","The performance of the SeSS is tested on different datasets, including (1)images transmitted by traditional and semantic communication systems at different compression rates, (2)images transmitted by traditional and semantic communication systems at different signal-to-noise ratios, (3)images generated by large-scale model with different noise levels introduced, and (4)cases of images subjected to certain special transformations.","The experiments demonstrate the effectiveness of SeSS, indicating that the metric can measure the semantic-level differences in semantic-level information of images and can be used for evaluation in visual semantic communication systems."],"url":"http://arxiv.org/abs/2406.03865v1","category":"cs.CV"}
{"created":"2024-06-06 08:49:51","title":"Behavior-Targeted Attack on Reinforcement Learning with Limited Access to Victim's Policy","abstract":"This study considers the attack on reinforcement learning agents where the adversary aims to control the victim's behavior as specified by the adversary by adding adversarial modifications to the victim's state observation. While some attack methods reported success in manipulating the victim agent's behavior, these methods often rely on environment-specific heuristics. In addition, all existing attack methods require white-box access to the victim's policy. In this study, we propose a novel method for manipulating the victim agent in the black-box (i.e., the adversary is allowed to observe the victim's state and action only) and no-box (i.e., the adversary is allowed to observe the victim's state only) setting without requiring environment-specific heuristics. Our attack method is formulated as a bi-level optimization problem that is reduced to a distribution matching problem and can be solved by an existing imitation learning algorithm in the black-box and no-box settings. Empirical evaluations on several reinforcement learning benchmarks show that our proposed method has superior attack performance to baselines.","sentences":["This study considers the attack on reinforcement learning agents where the adversary aims to control the victim's behavior as specified by the adversary by adding adversarial modifications to the victim's state observation.","While some attack methods reported success in manipulating the victim agent's behavior, these methods often rely on environment-specific heuristics.","In addition, all existing attack methods require white-box access to the victim's policy.","In this study, we propose a novel method for manipulating the victim agent in the black-box (i.e., the adversary is allowed to observe the victim's state and action only) and no-box (i.e., the adversary is allowed to observe the victim's state only) setting without requiring environment-specific heuristics.","Our attack method is formulated as a bi-level optimization problem that is reduced to a distribution matching problem and can be solved by an existing imitation learning algorithm in the black-box and no-box settings.","Empirical evaluations on several reinforcement learning benchmarks show that our proposed method has superior attack performance to baselines."],"url":"http://arxiv.org/abs/2406.03862v1","category":"cs.LG"}
{"created":"2024-06-06 08:23:22","title":"Open Problem: Active Representation Learning","abstract":"In this work, we introduce the concept of Active Representation Learning, a novel class of problems that intertwines exploration and representation learning within partially observable environments. We extend ideas from Active Simultaneous Localization and Mapping (active SLAM), and translate them to scientific discovery problems, exemplified by adaptive microscopy. We explore the need for a framework that derives exploration skills from representations that are in some sense actionable, aiming to enhance the efficiency and effectiveness of data collection and model building in the natural sciences.","sentences":["In this work, we introduce the concept of Active Representation Learning, a novel class of problems that intertwines exploration and representation learning within partially observable environments.","We extend ideas from Active Simultaneous Localization and Mapping (active SLAM), and translate them to scientific discovery problems, exemplified by adaptive microscopy.","We explore the need for a framework that derives exploration skills from representations that are in some sense actionable, aiming to enhance the efficiency and effectiveness of data collection and model building in the natural sciences."],"url":"http://arxiv.org/abs/2406.03845v1","category":"cs.LG"}
{"created":"2024-06-06 08:21:30","title":"POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models","abstract":"Large language models (LLMs) have exhibited impressive abilities for multimodal content comprehension and reasoning with proper prompting in zero- or few-shot settings. Despite the proliferation of interactive systems developed to support prompt engineering for LLMs across various tasks, most have primarily focused on textual or visual inputs, thus neglecting the complex interplay between modalities within multimodal inputs. This oversight hinders the development of effective prompts that guide model multimodal reasoning processes by fully exploiting the rich context provided by multiple modalities. In this paper, we present POEM, a visual analytics system to facilitate efficient prompt engineering for enhancing the multimodal reasoning performance of LLMs. The system enables users to explore the interaction patterns across modalities at varying levels of detail for a comprehensive understanding of the multimodal knowledge elicited by various prompts. Through diverse recommendations of demonstration examples and instructional principles, POEM supports users in iteratively crafting and refining prompts to better align and enhance model knowledge with human insights. The effectiveness and efficiency of our system are validated through two case studies and interviews with experts.","sentences":["Large language models (LLMs) have exhibited impressive abilities for multimodal content comprehension and reasoning with proper prompting in zero- or few-shot settings.","Despite the proliferation of interactive systems developed to support prompt engineering for LLMs across various tasks, most have primarily focused on textual or visual inputs, thus neglecting the complex interplay between modalities within multimodal inputs.","This oversight hinders the development of effective prompts that guide model multimodal reasoning processes by fully exploiting the rich context provided by multiple modalities.","In this paper, we present POEM, a visual analytics system to facilitate efficient prompt engineering for enhancing the multimodal reasoning performance of LLMs.","The system enables users to explore the interaction patterns across modalities at varying levels of detail for a comprehensive understanding of the multimodal knowledge elicited by various prompts.","Through diverse recommendations of demonstration examples and instructional principles, POEM supports users in iteratively crafting and refining prompts to better align and enhance model knowledge with human insights.","The effectiveness and efficiency of our system are validated through two case studies and interviews with experts."],"url":"http://arxiv.org/abs/2406.03843v1","category":"cs.HC"}
{"created":"2024-06-06 08:13:02","title":"Proactive Detection of Physical Inter-rule Vulnerabilities in IoT Services Using a Deep Learning Approach","abstract":"Emerging Internet of Things (IoT) platforms provide sophisticated capabilities to automate IoT services by enabling occupants to create trigger-action rules. Multiple trigger-action rules can physically interact with each other via shared environment channels, such as temperature, humidity, and illumination. We refer to inter-rule interactions via shared environment channels as a physical inter-rule vulnerability. Such vulnerability can be exploited by attackers to launch attacks against IoT systems. We propose a new framework to proactively discover possible physical inter-rule interactions from user requirement specifications (i.e., descriptions) using a deep learning approach. Specifically, we utilize the Transformer model to generate trigger-action rules from their associated descriptions. We discover two types of physical inter-rule vulnerabilities and determine associated environment channels using natural language processing (NLP) tools. Given the extracted trigger-action rules and associated environment channels, an approach is proposed to identify hidden physical inter-rule vulnerabilities among them. Our experiment on 27983 IFTTT style rules shows that the Transformer can successfully extract trigger-action rules from descriptions with 95.22% accuracy. We also validate the effectiveness of our approach on 60 SmartThings official IoT apps and discover 99 possible physical inter-rule vulnerabilities.","sentences":["Emerging Internet of Things (IoT) platforms provide sophisticated capabilities to automate IoT services by enabling occupants to create trigger-action rules.","Multiple trigger-action rules can physically interact with each other via shared environment channels, such as temperature, humidity, and illumination.","We refer to inter-rule interactions via shared environment channels as a physical inter-rule vulnerability.","Such vulnerability can be exploited by attackers to launch attacks against IoT systems.","We propose a new framework to proactively discover possible physical inter-rule interactions from user requirement specifications (i.e., descriptions) using a deep learning approach.","Specifically, we utilize the Transformer model to generate trigger-action rules from their associated descriptions.","We discover two types of physical inter-rule vulnerabilities and determine associated environment channels using natural language processing (NLP) tools.","Given the extracted trigger-action rules and associated environment channels, an approach is proposed to identify hidden physical inter-rule vulnerabilities among them.","Our experiment on 27983 IFTTT style rules shows that the Transformer can successfully extract trigger-action rules from descriptions with 95.22% accuracy.","We also validate the effectiveness of our approach on 60 SmartThings official IoT apps and discover 99 possible physical inter-rule vulnerabilities."],"url":"http://arxiv.org/abs/2406.03836v1","category":"cs.CR"}
{"created":"2024-06-06 07:59:02","title":"Views about ChatGPT: Are human decision making and human learning necessary?","abstract":"Using individual-level survey data from 2024, this study investigated how respondent characteristics are associated with a subjective view of generative artificial intelligence (GAI). We asked 14 questions concerning respondents view about GAI, such as general view, faulty GAI, autonomous GEI, GAI replacing humans, and importance of human learning. Regression analysis based on the ordered logit model revealed that: (1) In some cases, the results of smartphone and computer usage times differed. Smartphone usage time was negatively correlated with the importance of human learning, whereas computer usage was not negatively correlated. (2) Managers and ordinary businesspeople have positive views of GAI. However, managers do not show a positive view about GAI being responsible for human decision making. (3) Teachers generally have a negative view about GAI replacing humans and no need of learning. They do not have negative views about GAI producing documents unless GAI is faulty. (4) Medical industry workers positively view GAI if it operates following their direction. However, they do not agree with the view that GAI replaces humans, and that human learning is unnecessary. (5) Females are less likely than men to have a positive view of GAI. In summary, views about GAI vary widely by the individual characteristics and condition of GAI, and by the question set.","sentences":["Using individual-level survey data from 2024, this study investigated how respondent characteristics are associated with a subjective view of generative artificial intelligence (GAI).","We asked 14 questions concerning respondents view about GAI, such as general view, faulty GAI, autonomous GEI, GAI replacing humans, and importance of human learning.","Regression analysis based on the ordered logit model revealed that: (1) In some cases, the results of smartphone and computer usage times differed.","Smartphone usage time was negatively correlated with the importance of human learning, whereas computer usage was not negatively correlated.","(2) Managers and ordinary businesspeople have positive views of GAI.","However, managers do not show a positive view about GAI being responsible for human decision making.","(3) Teachers generally have a negative view about GAI replacing humans and no need of learning.","They do not have negative views about GAI producing documents unless GAI is faulty.","(4) Medical industry workers positively view GAI if it operates following their direction.","However, they do not agree with the view that GAI replaces humans, and that human learning is unnecessary.","(5) Females are less likely than men to have a positive view of GAI.","In summary, views about GAI vary widely by the individual characteristics and condition of GAI, and by the question set."],"url":"http://arxiv.org/abs/2406.03823v1","category":"econ.GN"}
{"created":"2024-06-06 07:55:30","title":"A Survey on Intelligent Internet of Things: Applications, Security, Privacy, and Future Directions","abstract":"The rapid advances in the Internet of Things (IoT) have promoted a revolution in communication technology and offered various customer services. Artificial intelligence (AI) techniques have been exploited to facilitate IoT operations and maximize their potential in modern application scenarios. In particular, the convergence of IoT and AI has led to a new networking paradigm called Intelligent IoT (IIoT), which has the potential to significantly transform businesses and industrial domains. This paper presents a comprehensive survey of IIoT by investigating its significant applications in mobile networks, as well as its associated security and privacy issues. Specifically, we explore and discuss the roles of IIoT in a wide range of key application domains, from smart healthcare and smart cities to smart transportation and smart industries. Through such extensive discussions, we investigate important security issues in IIoT networks, where network attacks, confidentiality, integrity, and intrusion are analyzed, along with a discussion of potential countermeasures. Privacy issues in IIoT networks were also surveyed and discussed, including data, location, and model privacy leakage. Finally, we outline several key challenges and highlight potential research directions in this important area.","sentences":["The rapid advances in the Internet of Things (IoT) have promoted a revolution in communication technology and offered various customer services.","Artificial intelligence (AI) techniques have been exploited to facilitate IoT operations and maximize their potential in modern application scenarios.","In particular, the convergence of IoT and AI has led to a new networking paradigm called Intelligent IoT (IIoT), which has the potential to significantly transform businesses and industrial domains.","This paper presents a comprehensive survey of IIoT by investigating its significant applications in mobile networks, as well as its associated security and privacy issues.","Specifically, we explore and discuss the roles of IIoT in a wide range of key application domains, from smart healthcare and smart cities to smart transportation and smart industries.","Through such extensive discussions, we investigate important security issues in IIoT networks, where network attacks, confidentiality, integrity, and intrusion are analyzed, along with a discussion of potential countermeasures.","Privacy issues in IIoT networks were also surveyed and discussed, including data, location, and model privacy leakage.","Finally, we outline several key challenges and highlight potential research directions in this important area."],"url":"http://arxiv.org/abs/2406.03820v1","category":"cs.NI"}
{"created":"2024-06-06 07:40:00","title":"ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search","abstract":"Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data. This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning). In this paper, we develop a reinforced self-training approach, called ReST-MCTS*, based on integrating process reward guidance with tree search MCTS* for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models. ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer. These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training. We first show that the tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget. We then show that by using traces searched by this tree-search policy as training data, we can continuously enhance the three language models for multiple iterations, and outperform other self-training algorithms such as ReST$^\\text{EM}$ and Self-Rewarding LM.","sentences":["Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data.","This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning).","In this paper, we develop a reinforced self-training approach, called ReST-MCTS*, based on integrating process reward guidance with tree search MCTS* for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models.","ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning:","Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer.","These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training.","We first show that the tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget.","We then show that by using traces searched by this tree-search policy as training data, we can continuously enhance the three language models for multiple iterations, and outperform other self-training algorithms such as ReST$^\\text{EM}$ and Self-Rewarding LM."],"url":"http://arxiv.org/abs/2406.03816v1","category":"cs.CL"}
{"created":"2024-06-06 07:37:42","title":"How to Scale Inverse RL to Large State Spaces? A Provably Efficient Approach","abstract":"In online Inverse Reinforcement Learning (IRL), the learner can collect samples about the dynamics of the environment to improve its estimate of the reward function. Since IRL suffers from identifiability issues, many theoretical works on online IRL focus on estimating the entire set of rewards that explain the demonstrations, named the feasible reward set. However, none of the algorithms available in the literature can scale to problems with large state spaces. In this paper, we focus on the online IRL problem in Linear Markov Decision Processes (MDPs). We show that the structure offered by Linear MDPs is not sufficient for efficiently estimating the feasible set when the state space is large. As a consequence, we introduce the novel framework of rewards compatibility, which generalizes the notion of feasible set, and we develop CATY-IRL, a sample efficient algorithm whose complexity is independent of the cardinality of the state space in Linear MDPs. When restricted to the tabular setting, we demonstrate that CATY-IRL is minimax optimal up to logarithmic factors. As a by-product, we show that Reward-Free Exploration (RFE) enjoys the same worst-case rate, improving over the state-of-the-art lower bound. Finally, we devise a unifying framework for IRL and RFE that may be of independent interest.","sentences":["In online Inverse Reinforcement Learning (IRL), the learner can collect samples about the dynamics of the environment to improve its estimate of the reward function.","Since IRL suffers from identifiability issues, many theoretical works on online IRL focus on estimating the entire set of rewards that explain the demonstrations, named the feasible reward set.","However, none of the algorithms available in the literature can scale to problems with large state spaces.","In this paper, we focus on the online IRL problem in Linear Markov Decision Processes (MDPs).","We show that the structure offered by Linear MDPs is not sufficient for efficiently estimating the feasible set when the state space is large.","As a consequence, we introduce the novel framework of rewards compatibility, which generalizes the notion of feasible set, and we develop CATY-IRL, a sample efficient algorithm whose complexity is independent of the cardinality of the state space in Linear MDPs.","When restricted to the tabular setting, we demonstrate that CATY-IRL is minimax optimal up to logarithmic factors.","As a by-product, we show that Reward-Free Exploration (RFE) enjoys the same worst-case rate, improving over the state-of-the-art lower bound.","Finally, we devise a unifying framework for IRL and RFE that may be of independent interest."],"url":"http://arxiv.org/abs/2406.03812v1","category":"cs.LG"}
{"created":"2024-06-06 07:30:27","title":"Cross-variable Linear Integrated ENhanced Transformer for Photovoltaic power forecasting","abstract":"Photovoltaic (PV) power forecasting plays a crucial role in optimizing the operation and planning of PV systems, thereby enabling efficient energy management and grid integration. However, un certainties caused by fluctuating weather conditions and complex interactions between different variables pose significant challenges to accurate PV power forecasting. In this study, we propose PV-Client (Cross-variable Linear Integrated ENhanced Transformer for Photovoltaic power forecasting) to address these challenges and enhance PV power forecasting accuracy. PV-Client employs an ENhanced Transformer module to capture complex interactions of various features in PV systems, and utilizes a linear module to learn trend information in PV power. Diverging from conventional time series-based Transformer models that use cross-time Attention to learn dependencies between different time steps, the Enhanced Transformer module integrates cross-variable Attention to capture dependencies between PV power and weather factors. Furthermore, PV-Client streamlines the embedding and position encoding layers by replacing the Decoder module with a projection layer. Experimental results on three real-world PV power datasets affirm PV-Client's state-of-the-art (SOTA) performance in PV power forecasting. Specifically, PV-Client surpasses the second-best model GRU by 5.3% in MSE metrics and 0.9% in accuracy metrics at the Jingang Station. Similarly, PV-Client outperforms the second-best model SVR by 10.1% in MSE metrics and 0.2% in accuracy metrics at the Xinqingnian Station, and PV-Client exhibits superior performance compared to the second-best model SVR with enhancements of 3.4% in MSE metrics and 0.9% in accuracy metrics at the Hongxing Station.","sentences":["Photovoltaic (PV) power forecasting plays a crucial role in optimizing the operation and planning of PV systems, thereby enabling efficient energy management and grid integration.","However, un certainties caused by fluctuating weather conditions and complex interactions between different variables pose significant challenges to accurate PV power forecasting.","In this study, we propose PV-Client (Cross-variable Linear Integrated ENhanced Transformer for Photovoltaic power forecasting) to address these challenges and enhance PV power forecasting accuracy.","PV-Client employs an ENhanced Transformer module to capture complex interactions of various features in PV systems, and utilizes a linear module to learn trend information in PV power.","Diverging from conventional time series-based Transformer models that use cross-time Attention to learn dependencies between different time steps, the Enhanced Transformer module integrates cross-variable Attention to capture dependencies between PV power and weather factors.","Furthermore, PV-Client streamlines the embedding and position encoding layers by replacing the Decoder module with a projection layer.","Experimental results on three real-world PV power datasets affirm PV-Client's state-of-the-art (SOTA) performance in PV power forecasting.","Specifically, PV-Client surpasses the second-best model GRU by 5.3% in MSE metrics and 0.9% in accuracy metrics at the Jingang Station.","Similarly, PV-Client outperforms the second-best model SVR by 10.1% in MSE metrics and 0.2% in accuracy metrics at the Xinqingnian Station, and PV-Client exhibits superior performance compared to the second-best model SVR with enhancements of 3.4% in MSE metrics and 0.9% in accuracy metrics at the Hongxing Station."],"url":"http://arxiv.org/abs/2406.03808v1","category":"cs.LG"}
{"created":"2024-06-06 07:30:14","title":"Tool-Planner: Dynamic Solution Tree Planning for Large Language Model with Tool Clustering","abstract":"Large language models (LLMs) have demonstrated exceptional reasoning capabilities, enabling them to solve various complex problems. Recently, this ability has been applied to the paradigm of tool learning. Tool learning involves providing examples of tool usage and their corresponding functions, allowing LLMs to formulate plans and demonstrate the process of invoking and executing each tool. LLMs can address tasks that they cannot complete independently, thereby enhancing their potential across different tasks. However, this approach faces two key challenges. First, redundant error correction leads to unstable planning and long execution time. Additionally, designing a correct plan among multiple tools is also a challenge in tool learning. To address these issues, we propose Tool-Planner, a task-processing framework based on toolkits. Tool-Planner groups tools based on the API functions with the same function into a toolkit and allows LLMs to implement planning across the various toolkits. When a tool error occurs, the language model can reselect and adjust tools based on the toolkit. Experiments show that our approach demonstrates a high pass and win rate across different datasets and optimizes the planning scheme for tool learning in models such as GPT-4 and Claude 3, showcasing the potential of our method.","sentences":["Large language models (LLMs) have demonstrated exceptional reasoning capabilities, enabling them to solve various complex problems.","Recently, this ability has been applied to the paradigm of tool learning.","Tool learning involves providing examples of tool usage and their corresponding functions, allowing LLMs to formulate plans and demonstrate the process of invoking and executing each tool.","LLMs can address tasks that they cannot complete independently, thereby enhancing their potential across different tasks.","However, this approach faces two key challenges.","First, redundant error correction leads to unstable planning and long execution time.","Additionally, designing a correct plan among multiple tools is also a challenge in tool learning.","To address these issues, we propose Tool-Planner, a task-processing framework based on toolkits.","Tool-Planner groups tools based on the API functions with the same function into a toolkit and allows LLMs to implement planning across the various toolkits.","When a tool error occurs, the language model can reselect and adjust tools based on the toolkit.","Experiments show that our approach demonstrates a high pass and win rate across different datasets and optimizes the planning scheme for tool learning in models such as GPT-4 and Claude 3, showcasing the potential of our method."],"url":"http://arxiv.org/abs/2406.03807v1","category":"cs.AI"}
{"created":"2024-06-06 07:12:50","title":"Enhanced Semantic Segmentation Pipeline for WeatherProof Dataset Challenge","abstract":"This report describes the winning solution to the WeatherProof Dataset Challenge (CVPR 2024 UG2+ Track 3). Details regarding the challenge are available at https://cvpr2024ug2challenge.github.io/track3.html. We propose an enhanced semantic segmentation pipeline for this challenge. Firstly, we improve semantic segmentation models, using backbone pretrained with Depth Anything to improve UperNet model and SETRMLA model, and adding language guidance based on both weather and category information to InternImage model. Secondly, we introduce a new dataset WeatherProofExtra with wider viewing angle and employ data augmentation methods, including adverse weather and super-resolution. Finally, effective training strategies and ensemble method are applied to improve final performance further. Our solution is ranked 1st on the final leaderboard. Code will be available at https://github.com/KaneiGi/WeatherProofChallenge.","sentences":["This report describes the winning solution to the WeatherProof Dataset Challenge (CVPR 2024 UG2+ Track 3).","Details regarding the challenge are available at https://cvpr2024ug2challenge.github.io/track3.html.","We propose an enhanced semantic segmentation pipeline for this challenge.","Firstly, we improve semantic segmentation models, using backbone pretrained with Depth Anything to improve UperNet model and SETRMLA model, and adding language guidance based on both weather and category information to InternImage model.","Secondly, we introduce a new dataset WeatherProofExtra with wider viewing angle and employ data augmentation methods, including adverse weather and super-resolution.","Finally, effective training strategies and ensemble method are applied to improve final performance further.","Our solution is ranked 1st on the final leaderboard.","Code will be available at https://github.com/KaneiGi/WeatherProofChallenge."],"url":"http://arxiv.org/abs/2406.03799v1","category":"cs.CV"}
{"created":"2024-06-06 07:01:36","title":"Enhancing Graph U-Nets for Mesh-Agnostic Spatio-Temporal Flow Prediction","abstract":"This study aims to overcome the conventional deep-learning approaches based on convolutional neural networks, whose applicability to complex geometries and unstructured meshes is limited due to their inherent mesh dependency. We propose novel approaches to improve mesh-agnostic spatio-temporal prediction of transient flow fields using graph U-Nets, enabling accurate prediction on diverse mesh configurations. Key enhancements to the graph U-Net architecture, including the Gaussian mixture model convolutional operator and noise injection approaches, provide increased flexibility in modeling node dynamics: the former reduces prediction error by 95\\% compared to conventional convolutional operators, while the latter improves long-term prediction robustness, resulting in an error reduction of 86\\%. We also investigate transductive and inductive-learning perspectives of graph U-Nets with proposed improvements. In the transductive setting, they effectively predict quantities for unseen nodes within the trained graph. In the inductive setting, they successfully perform in mesh scenarios with different vortex-shedding periods, showing 98\\% improvement in predicting the future flow fields compared to a model trained without the inductive settings. It is found that graph U-Nets without pooling operations, i.e. without reducing and restoring the node dimensionality of the graph data, perform better in inductive settings due to their ability to learn from the detailed structure of each graph. Meanwhile, we also discover that the choice of normalization technique significantly impacts graph U-Net performance.","sentences":["This study aims to overcome the conventional deep-learning approaches based on convolutional neural networks, whose applicability to complex geometries and unstructured meshes is limited due to their inherent mesh dependency.","We propose novel approaches to improve mesh-agnostic spatio-temporal prediction of transient flow fields using graph U-Nets, enabling accurate prediction on diverse mesh configurations.","Key enhancements to the graph U-Net architecture, including the Gaussian mixture model convolutional operator and noise injection approaches, provide increased flexibility in modeling node dynamics: the former reduces prediction error by 95\\% compared to conventional convolutional operators, while the latter improves long-term prediction robustness, resulting in an error reduction of 86\\%.","We also investigate transductive and inductive-learning perspectives of graph U-Nets with proposed improvements.","In the transductive setting, they effectively predict quantities for unseen nodes within the trained graph.","In the inductive setting, they successfully perform in mesh scenarios with different vortex-shedding periods, showing 98\\% improvement in predicting the future flow fields compared to a model trained without the inductive settings.","It is found that graph U-Nets without pooling operations, i.e. without reducing and restoring the node dimensionality of the graph data, perform better in inductive settings due to their ability to learn from the detailed structure of each graph.","Meanwhile, we also discover that the choice of normalization technique significantly impacts graph U-Net performance."],"url":"http://arxiv.org/abs/2406.03789v1","category":"cs.LG"}
{"created":"2024-06-06 06:54:13","title":"Flips in colorful triangulations","abstract":"The associahedron is the graph $\\mathcal{G}_N$ that has as nodes all triangulations of a convex $N$-gon, and an edge between any two triangulations that differ in a flip operation, which consists of removing an edge shared by two triangles and replacing it by the other diagonal of the resulting 4-gon. In this paper, we consider a large collection of induced subgraphs of $\\mathcal{G}_N$ obtained by Ramsey-type colorability properties. Specifically, coloring the points of the $N$-gon red and blue alternatingly, we consider only colorful triangulations, namely triangulations in which every triangle has points in both colors, i.e., monochromatic triangles are forbidden. The resulting induced subgraph of $\\mathcal{G}_N$ on colorful triangulations is denoted by $\\mathcal{F}_N$. We prove that $\\mathcal{F}_N$ has a Hamilton cycle for all $N\\geq 8$, resolving a problem raised by Sagan, i.e., all colorful triangulations on $N$ points can be listed so that any two cyclically consecutive triangulations differ in a flip. In fact, we prove that for an arbitrary fixed coloring pattern of the $N$ points with at least 10 changes of color, the resulting subgraph of $\\mathcal{G}_N$ on colorful triangulations (for that coloring pattern) admits a Hamilton cycle. We also provide an efficient algorithm for computing a Hamilton path in $\\mathcal{F}_N$ that runs in time $\\mathcal{O}(1)$ on average per generated node. This algorithm is based on a new and algorithmic construction of a tree rotation Gray code for listing all $n$-vertex $k$-ary trees that runs in time $\\mathcal{O}(k)$ on average per generated tree.","sentences":["The associahedron is the graph $\\mathcal{G}_N$ that has as nodes all triangulations of a convex $N$-gon, and an edge between any two triangulations that differ in a flip operation, which consists of removing an edge shared by two triangles and replacing it by the other diagonal of the resulting 4-gon.","In this paper, we consider a large collection of induced subgraphs of $\\mathcal{G}_N$ obtained by Ramsey-type colorability properties.","Specifically, coloring the points of the $N$-gon red and blue alternatingly, we consider only colorful triangulations, namely triangulations in which every triangle has points in both colors, i.e., monochromatic triangles are forbidden.","The resulting induced subgraph of $\\mathcal{G}_N$ on colorful triangulations is denoted by $\\mathcal{F}_N$. We prove that $\\mathcal{F}_N$ has a Hamilton cycle for all $N\\geq 8$, resolving a problem raised by Sagan, i.e., all colorful triangulations on $N$ points can be listed so that any two cyclically consecutive triangulations differ in a flip.","In fact, we prove that for an arbitrary fixed coloring pattern of the $N$ points with at least 10 changes of color, the resulting subgraph of $\\mathcal{G}_N$ on colorful triangulations (for that coloring pattern) admits a Hamilton cycle.","We also provide an efficient algorithm for computing a Hamilton path in $\\mathcal{F}_N$ that runs in time $\\mathcal{O}(1)$ on average per generated node.","This algorithm is based on a new and algorithmic construction of a tree rotation Gray code for listing all $n$-vertex $k$-ary trees that runs in time $\\mathcal{O}(k)$ on average per generated tree."],"url":"http://arxiv.org/abs/2406.03783v1","category":"math.CO"}
{"created":"2024-06-06 06:50:23","title":"Gamma Ray AGNs: Estimating Redshifts and Blazar Classification using traditional Neural Networks with smart initialization and self-supervised learning","abstract":"Redshift estimation and the classification of gamma-ray AGNs represent crucial challenges in the field of gamma-ray astronomy. Recent efforts have been made to tackle these problems using traditional machine learning methods. However, the simplicity of existing algorithms, combined with their basic implementations, underscores an opportunity and a need for further advancement in this area. Our approach begins by implementing a Bayesian model for redshift estimation, which can account for uncertainty while providing predictions with the desired confidence level. Subsequently, we address the classification problem by leveraging intelligent initialization techniques and employing soft voting. Additionally, we explore several potential self-supervised algorithms in their conventional form. Lastly, in addition to generating predictions for data with missing outputs, we ensure that the theoretical assertions put forth by both algorithms mutually reinforce each other.","sentences":["Redshift estimation and the classification of gamma-ray AGNs represent crucial challenges in the field of gamma-ray astronomy.","Recent efforts have been made to tackle these problems using traditional machine learning methods.","However, the simplicity of existing algorithms, combined with their basic implementations, underscores an opportunity and a need for further advancement in this area.","Our approach begins by implementing a Bayesian model for redshift estimation, which can account for uncertainty while providing predictions with the desired confidence level.","Subsequently, we address the classification problem by leveraging intelligent initialization techniques and employing soft voting.","Additionally, we explore several potential self-supervised algorithms in their conventional form.","Lastly, in addition to generating predictions for data with missing outputs, we ensure that the theoretical assertions put forth by both algorithms mutually reinforce each other."],"url":"http://arxiv.org/abs/2406.03782v1","category":"astro-ph.HE"}
{"created":"2024-06-06 06:41:53","title":"Empirical Guidelines for Deploying LLMs onto Resource-constrained Edge Devices","abstract":"The scaling laws have become the de facto guidelines for designing large language models (LLMs), but they were studied under the assumption of unlimited computing resources for both training and inference. As LLMs are increasingly used as personalized intelligent assistants, their customization (i.e., learning through fine-tuning) and deployment onto resource-constrained edge devices will become more and more prevalent. An urging but open question is how a resource-constrained computing environment would affect the design choices for a personalized LLM. We study this problem empirically in this work. In particular, we consider the tradeoffs among a number of key design factors and their intertwined impacts on learning efficiency and accuracy. The factors include the learning methods for LLM customization, the amount of personalized data used for learning customization, the types and sizes of LLMs, the compression methods of LLMs, the amount of time afforded to learn, and the difficulty levels of the target use cases. Through extensive experimentation and benchmarking, we draw a number of surprisingly insightful guidelines for deploying LLMs onto resource-constrained devices. For example, an optimal choice between parameter learning and RAG may vary depending on the difficulty of the downstream task, the longer fine-tuning time does not necessarily help the model, and a compressed LLM may be a better choice than an uncompressed LLM to learn from limited personalized data.","sentences":["The scaling laws have become the de facto guidelines for designing large language models (LLMs), but they were studied under the assumption of unlimited computing resources for both training and inference.","As LLMs are increasingly used as personalized intelligent assistants, their customization (i.e., learning through fine-tuning) and deployment onto resource-constrained edge devices will become more and more prevalent.","An urging but open question is how a resource-constrained computing environment would affect the design choices for a personalized LLM.","We study this problem empirically in this work.","In particular, we consider the tradeoffs among a number of key design factors and their intertwined impacts on learning efficiency and accuracy.","The factors include the learning methods for LLM customization, the amount of personalized data used for learning customization, the types and sizes of LLMs, the compression methods of LLMs, the amount of time afforded to learn, and the difficulty levels of the target use cases.","Through extensive experimentation and benchmarking, we draw a number of surprisingly insightful guidelines for deploying LLMs onto resource-constrained devices.","For example, an optimal choice between parameter learning and RAG may vary depending on the difficulty of the downstream task, the longer fine-tuning time does not necessarily help the model, and a compressed LLM may be a better choice than an uncompressed LLM to learn from limited personalized data."],"url":"http://arxiv.org/abs/2406.03777v1","category":"cs.LG"}
{"created":"2024-06-06 06:40:19","title":"XL-HeadTags: Leveraging Multimodal Retrieval Augmentation for the Multilingual Generation of News Headlines and Tags","abstract":"Millions of news articles published online daily can overwhelm readers. Headlines and entity (topic) tags are essential for guiding readers to decide if the content is worth their time. While headline generation has been extensively studied, tag generation remains largely unexplored, yet it offers readers better access to topics of interest. The need for conciseness in capturing readers' attention necessitates improved content selection strategies for identifying salient and relevant segments within lengthy articles, thereby guiding language models effectively. To address this, we propose to leverage auxiliary information such as images and captions embedded in the articles to retrieve relevant sentences and utilize instruction tuning with variations to generate both headlines and tags for news articles in a multilingual context. To make use of the auxiliary information, we have compiled a dataset named XL-HeadTags, which includes 20 languages across 6 diverse language families. Through extensive evaluation, we demonstrate the effectiveness of our plug-and-play multimodal-multilingual retrievers for both tasks. Additionally, we have developed a suite of tools for processing and evaluating multilingual texts, significantly contributing to the research community by enabling more accurate and efficient analysis across languages.","sentences":["Millions of news articles published online daily can overwhelm readers.","Headlines and entity (topic) tags are essential for guiding readers to decide if the content is worth their time.","While headline generation has been extensively studied, tag generation remains largely unexplored, yet it offers readers better access to topics of interest.","The need for conciseness in capturing readers' attention necessitates improved content selection strategies for identifying salient and relevant segments within lengthy articles, thereby guiding language models effectively.","To address this, we propose to leverage auxiliary information such as images and captions embedded in the articles to retrieve relevant sentences and utilize instruction tuning with variations to generate both headlines and tags for news articles in a multilingual context.","To make use of the auxiliary information, we have compiled a dataset named XL-HeadTags, which includes 20 languages across 6 diverse language families.","Through extensive evaluation, we demonstrate the effectiveness of our plug-and-play multimodal-multilingual retrievers for both tasks.","Additionally, we have developed a suite of tools for processing and evaluating multilingual texts, significantly contributing to the research community by enabling more accurate and efficient analysis across languages."],"url":"http://arxiv.org/abs/2406.03776v1","category":"cs.CL"}
{"created":"2024-06-06 06:15:35","title":"Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective","abstract":"Pre-trained large language models (LLMs) based on Transformer have demonstrated striking in-context learning (ICL) abilities. With a few demonstration input-label pairs, they can predict the label for an unseen input without any parameter updates. In this paper, we show an exciting phenomenon that SVD-based weight pruning can enhance ICL performance, and more surprising, pruning weights in deep layers often results in more stable performance improvements in shallow layers. However, the underlying mechanism of those findings still remains an open question. To reveal those findings, we conduct an in-depth theoretical analysis by presenting the implicit gradient descent (GD) trajectories of ICL and giving the mutual information based generalization bounds of ICL via full implicit GD trajectories. This helps us reasonably explain the surprising experimental findings. Besides, based on all our experimental and theoretical insights, we intuitively propose a simple, model-compression and derivative-free algorithm for downstream tasks in enhancing ICL inference. Experiments on benchmark datasets and open source LLMs display the method effectiveness\\footnote{The code is available at \\url{https://github.com/chen123CtrlS/EnhancingICL_SVDPruning}}.","sentences":["Pre-trained large language models (LLMs) based on Transformer have demonstrated striking in-context learning (ICL) abilities.","With a few demonstration input-label pairs, they can predict the label for an unseen input without any parameter updates.","In this paper, we show an exciting phenomenon that SVD-based weight pruning can enhance ICL performance, and more surprising, pruning weights in deep layers often results in more stable performance improvements in shallow layers.","However, the underlying mechanism of those findings still remains an open question.","To reveal those findings, we conduct an in-depth theoretical analysis by presenting the implicit gradient descent (GD) trajectories of ICL and giving the mutual information based generalization bounds of ICL via full implicit GD trajectories.","This helps us reasonably explain the surprising experimental findings.","Besides, based on all our experimental and theoretical insights, we intuitively propose a simple, model-compression and derivative-free algorithm for downstream tasks in enhancing ICL inference.","Experiments on benchmark datasets and open source LLMs display the method effectiveness\\footnote{The code is available at \\url{https://github.com/chen123CtrlS/EnhancingICL_SVDPruning}}."],"url":"http://arxiv.org/abs/2406.03768v1","category":"cs.LG"}
{"created":"2024-06-06 04:57:29","title":"Instance Segmentation and Teeth Classification in Panoramic X-rays","abstract":"Teeth segmentation and recognition are critical in various dental applications and dental diagnosis. Automatic and accurate segmentation approaches have been made possible by integrating deep learning models. Although teeth segmentation has been studied in the past, only some techniques were able to effectively classify and segment teeth simultaneously. This article offers a pipeline of two deep learning models, U-Net and YOLOv8, which results in BB-UNet, a new architecture for the classification and segmentation of teeth on panoramic X-rays that is efficient and reliable. We have improved the quality and reliability of teeth segmentation by utilising the YOLOv8 and U-Net capabilities. The proposed networks have been evaluated using the mean average precision (mAP) and dice coefficient for YOLOv8 and BB-UNet, respectively. We have achieved a 3\\% increase in mAP score for teeth classification compared to existing methods, and a 10-15\\% increase in dice coefficient for teeth segmentation compared to U-Net across different categories of teeth. A new Dental dataset was created based on UFBA-UESC dataset with Bounding-Box and Polygon annotations of 425 dental panoramic X-rays. The findings of this research pave the way for a wider adoption of object detection models in the field of dental diagnosis.","sentences":["Teeth segmentation and recognition are critical in various dental applications and dental diagnosis.","Automatic and accurate segmentation approaches have been made possible by integrating deep learning models.","Although teeth segmentation has been studied in the past, only some techniques were able to effectively classify and segment teeth simultaneously.","This article offers a pipeline of two deep learning models, U-Net and YOLOv8, which results in BB-UNet, a new architecture for the classification and segmentation of teeth on panoramic X-rays that is efficient and reliable.","We have improved the quality and reliability of teeth segmentation by utilising the YOLOv8 and U-Net capabilities.","The proposed networks have been evaluated using the mean average precision (mAP) and dice coefficient for YOLOv8 and BB-UNet, respectively.","We have achieved a 3\\% increase in mAP score for teeth classification compared to existing methods, and a 10-15\\% increase in dice coefficient for teeth segmentation compared to U-Net across different categories of teeth.","A new Dental dataset was created based on UFBA-UESC dataset with Bounding-Box and Polygon annotations of 425 dental panoramic X-rays.","The findings of this research pave the way for a wider adoption of object detection models in the field of dental diagnosis."],"url":"http://arxiv.org/abs/2406.03747v1","category":"cs.CV"}
{"created":"2024-06-06 04:55:55","title":"Efficient Knowledge Infusion via KG-LLM Alignment","abstract":"To tackle the problem of domain-specific knowledge scarcity within large language models (LLMs), knowledge graph-retrievalaugmented method has been proven to be an effective and efficient technique for knowledge infusion. However, existing approaches face two primary challenges: knowledge mismatch between public available knowledge graphs and the specific domain of the task at hand, and poor information compliance of LLMs with knowledge graphs. In this paper, we leverage a small set of labeled samples and a large-scale corpus to efficiently construct domain-specific knowledge graphs by an LLM, addressing the issue of knowledge mismatch. Additionally, we propose a three-stage KG-LLM alignment strategyto enhance the LLM's capability to utilize information from knowledge graphs. We conduct experiments with a limited-sample setting on two biomedical question-answering datasets, and the results demonstrate that our approach outperforms existing baselines.","sentences":["To tackle the problem of domain-specific knowledge scarcity within large language models (LLMs), knowledge graph-retrievalaugmented method has been proven to be an effective and efficient technique for knowledge infusion.","However, existing approaches face two primary challenges: knowledge mismatch between public available knowledge graphs and the specific domain of the task at hand, and poor information compliance of LLMs with knowledge graphs.","In this paper, we leverage a small set of labeled samples and a large-scale corpus to efficiently construct domain-specific knowledge graphs by an LLM, addressing the issue of knowledge mismatch.","Additionally, we propose a three-stage KG-LLM alignment strategyto enhance the LLM's capability to utilize information from knowledge graphs.","We conduct experiments with a limited-sample setting on two biomedical question-answering datasets, and the results demonstrate that our approach outperforms existing baselines."],"url":"http://arxiv.org/abs/2406.03746v1","category":"cs.CL"}
{"created":"2024-06-06 04:12:57","title":"Credit Card Fraud Detection Using Advanced Transformer Model","abstract":"With the proliferation of various online and mobile payment systems, credit card fraud has emerged as a significant threat to financial security. This study focuses on innovative applications of the latest Transformer models for more robust and precise fraud detection. To ensure the reliability of the data, we meticulously processed the data sources, balancing the dataset to address the issue of data sparsity significantly. We also selected highly correlated vectors to strengthen the training process.To guarantee the reliability and practicality of the new Transformer model, we conducted performance comparisons with several widely adopted models, including Support Vector Machine (SVM), Random Forest, Neural Network, and Logistic Regression. We rigorously compared these models using metrics such as Precision, Recall, and F1 Score. Through these detailed analyses and comparisons, we present to the readers a highly efficient and powerful anti-fraud mechanism with promising prospects. The results demonstrate that the Transformer model not only excels in traditional applications but also shows great potential in niche areas like fraud detection, offering a substantial advancement in the field.","sentences":["With the proliferation of various online and mobile payment systems, credit card fraud has emerged as a significant threat to financial security.","This study focuses on innovative applications of the latest Transformer models for more robust and precise fraud detection.","To ensure the reliability of the data, we meticulously processed the data sources, balancing the dataset to address the issue of data sparsity significantly.","We also selected highly correlated vectors to strengthen the training process.","To guarantee the reliability and practicality of the new Transformer model, we conducted performance comparisons with several widely adopted models, including Support Vector Machine (SVM), Random Forest, Neural Network, and Logistic Regression.","We rigorously compared these models using metrics such as Precision, Recall, and F1 Score.","Through these detailed analyses and comparisons, we present to the readers a highly efficient and powerful anti-fraud mechanism with promising prospects.","The results demonstrate that the Transformer model not only excels in traditional applications but also shows great potential in niche areas like fraud detection, offering a substantial advancement in the field."],"url":"http://arxiv.org/abs/2406.03733v1","category":"cs.LG"}
{"created":"2024-06-06 04:05:54","title":"FastGAS: Fast Graph-based Annotation Selection for In-Context Learning","abstract":"In-context learning (ICL) empowers large language models (LLMs) to tackle new tasks by using a series of training instances as prompts. Since generating the prompts needs to sample from a vast pool of instances and annotate them (e.g., add labels in classification task), existing methods have proposed to select a subset of unlabeled examples for annotation, thus enhancing the quality of prompts and concurrently mitigating annotation costs. However, these methods often require a long time to select instances due to their complexity, hindering their practical viability. To address this limitation, we propose a graph-based selection method, FastGAS, designed to efficiently identify high-quality instances while minimizing computational overhead. Initially, we construct a data similarity graph based on instance similarities. Subsequently, employing a graph partitioning algorithm, we partition the graph into pieces. Within each piece (i.e., subgraph), we adopt a greedy approach to pick the most representative nodes. By aggregating nodes from diverse pieces and annotating the corresponding instances, we identify a set of diverse and representative instances for ICL. Compared to prior approaches, our method not only exhibits superior performance on different tasks but also significantly reduces selection time. In addition, we demonstrate the efficacy of our approach in LLMs of larger sizes.","sentences":["In-context learning (ICL) empowers large language models (LLMs) to tackle new tasks by using a series of training instances as prompts.","Since generating the prompts needs to sample from a vast pool of instances and annotate them (e.g., add labels in classification task), existing methods have proposed to select a subset of unlabeled examples for annotation, thus enhancing the quality of prompts and concurrently mitigating annotation costs.","However, these methods often require a long time to select instances due to their complexity, hindering their practical viability.","To address this limitation, we propose a graph-based selection method, FastGAS, designed to efficiently identify high-quality instances while minimizing computational overhead.","Initially, we construct a data similarity graph based on instance similarities.","Subsequently, employing a graph partitioning algorithm, we partition the graph into pieces.","Within each piece (i.e., subgraph), we adopt a greedy approach to pick the most representative nodes.","By aggregating nodes from diverse pieces and annotating the corresponding instances, we identify a set of diverse and representative instances for ICL.","Compared to prior approaches, our method not only exhibits superior performance on different tasks but also significantly reduces selection time.","In addition, we demonstrate the efficacy of our approach in LLMs of larger sizes."],"url":"http://arxiv.org/abs/2406.03730v1","category":"cs.LG"}
{"created":"2024-06-06 03:35:09","title":"Offline Multi-Objective Optimization","abstract":"Offline optimization aims to maximize a black-box objective function with a static dataset and has wide applications. In addition to the objective function being black-box and expensive to evaluate, numerous complex real-world problems entail optimizing multiple conflicting objectives, i.e., multi-objective optimization (MOO). Nevertheless, offline MOO has not progressed as much as offline single-objective optimization (SOO), mainly due to the lack of benchmarks like Design-Bench for SOO. To bridge this gap, we propose a first benchmark for offline MOO, covering a range of problems from synthetic to real-world tasks. This benchmark provides tasks, datasets, and open-source examples, which can serve as a foundation for method comparisons and advancements in offline MOO. Furthermore, we analyze how the current related methods can be adapted to offline MOO from four fundamental perspectives, including data, model architecture, learning algorithm, and search algorithm. Empirical results show improvements over the best value of the training set, demonstrating the effectiveness of offline MOO methods. As no particular method stands out significantly, there is still an open challenge in further enhancing the effectiveness of offline MOO. We finally discuss future challenges for offline MOO, with the hope of shedding some light on this emerging field. Our code is available at \\url{https://github.com/lamda-bbo/offline-moo}.","sentences":["Offline optimization aims to maximize a black-box objective function with a static dataset and has wide applications.","In addition to the objective function being black-box and expensive to evaluate, numerous complex real-world problems entail optimizing multiple conflicting objectives, i.e., multi-objective optimization (MOO).","Nevertheless, offline MOO has not progressed as much as offline single-objective optimization (SOO), mainly due to the lack of benchmarks like Design-Bench for SOO.","To bridge this gap, we propose a first benchmark for offline MOO, covering a range of problems from synthetic to real-world tasks.","This benchmark provides tasks, datasets, and open-source examples, which can serve as a foundation for method comparisons and advancements in offline MOO.","Furthermore, we analyze how the current related methods can be adapted to offline MOO from four fundamental perspectives, including data, model architecture, learning algorithm, and search algorithm.","Empirical results show improvements over the best value of the training set, demonstrating the effectiveness of offline MOO methods.","As no particular method stands out significantly, there is still an open challenge in further enhancing the effectiveness of offline MOO.","We finally discuss future challenges for offline MOO, with the hope of shedding some light on this emerging field.","Our code is available at \\url{https://github.com/lamda-bbo/offline-moo}."],"url":"http://arxiv.org/abs/2406.03722v1","category":"cs.LG"}
{"created":"2024-06-06 03:34:42","title":"Attribute-Aware Implicit Modality Alignment for Text Attribute Person Search","abstract":"Text attribute person search aims to find specific pedestrians through given textual attributes, which is very meaningful in the scene of searching for designated pedestrians through witness descriptions. The key challenge is the significant modality gap between textual attributes and images. Previous methods focused on achieving explicit representation and alignment through unimodal pre-trained models. Nevertheless, the absence of inter-modality correspondence in these models may lead to distortions in the local information of intra-modality. Moreover, these methods only considered the alignment of inter-modality and ignored the differences between different attribute categories. To mitigate the above problems, we propose an Attribute-Aware Implicit Modality Alignment (AIMA) framework to learn the correspondence of local representations between textual attributes and images and combine global representation matching to narrow the modality gap. Firstly, we introduce the CLIP model as the backbone and design prompt templates to transform attribute combinations into structured sentences. This facilitates the model's ability to better understand and match image details. Next, we design a Masked Attribute Prediction (MAP) module that predicts the masked attributes after the interaction of image and masked textual attribute features through multi-modal interaction, thereby achieving implicit local relationship alignment. Finally, we propose an Attribute-IoU Guided Intra-Modal Contrastive (A-IoU IMC) loss, aligning the distribution of different textual attributes in the embedding space with their IoU distribution, achieving better semantic arrangement. Extensive experiments on the Market-1501 Attribute, PETA, and PA100K datasets show that the performance of our proposed method significantly surpasses the current state-of-the-art methods.","sentences":["Text attribute person search aims to find specific pedestrians through given textual attributes, which is very meaningful in the scene of searching for designated pedestrians through witness descriptions.","The key challenge is the significant modality gap between textual attributes and images.","Previous methods focused on achieving explicit representation and alignment through unimodal pre-trained models.","Nevertheless, the absence of inter-modality correspondence in these models may lead to distortions in the local information of intra-modality.","Moreover, these methods only considered the alignment of inter-modality and ignored the differences between different attribute categories.","To mitigate the above problems, we propose an Attribute-Aware Implicit Modality Alignment (AIMA) framework to learn the correspondence of local representations between textual attributes and images and combine global representation matching to narrow the modality gap.","Firstly, we introduce the CLIP model as the backbone and design prompt templates to transform attribute combinations into structured sentences.","This facilitates the model's ability to better understand and match image details.","Next, we design a Masked Attribute Prediction (MAP) module that predicts the masked attributes after the interaction of image and masked textual attribute features through multi-modal interaction, thereby achieving implicit local relationship alignment.","Finally, we propose an Attribute-IoU Guided Intra-Modal Contrastive (A-IoU IMC) loss, aligning the distribution of different textual attributes in the embedding space with their IoU distribution, achieving better semantic arrangement.","Extensive experiments on the Market-1501 Attribute, PETA, and PA100K datasets show that the performance of our proposed method significantly surpasses the current state-of-the-art methods."],"url":"http://arxiv.org/abs/2406.03721v1","category":"cs.CV"}
{"created":"2024-06-06 03:29:05","title":"Generalization-Enhanced Code Vulnerability Detection via Multi-Task Instruction Fine-Tuning","abstract":"Code Pre-trained Models (CodePTMs) based vulnerability detection have achieved promising results over recent years. However, these models struggle to generalize as they typically learn superficial mapping from source code to labels instead of understanding the root causes of code vulnerabilities, resulting in poor performance in real-world scenarios beyond the training instances. To tackle this challenge, we introduce VulLLM, a novel framework that integrates multi-task learning with Large Language Models (LLMs) to effectively mine deep-seated vulnerability features. Specifically, we construct two auxiliary tasks beyond the vulnerability detection task. First, we utilize the vulnerability patches to construct a vulnerability localization task. Second, based on the vulnerability features extracted from patches, we leverage GPT-4 to construct a vulnerability interpretation task. VulLLM innovatively augments vulnerability classification by leveraging generative LLMs to understand complex vulnerability patterns, thus compelling the model to capture the root causes of vulnerabilities rather than overfitting to spurious features of a single task. The experiments conducted on six large datasets demonstrate that VulLLM surpasses seven state-of-the-art models in terms of effectiveness, generalization, and robustness.","sentences":["Code Pre-trained Models (CodePTMs) based vulnerability detection have achieved promising results over recent years.","However, these models struggle to generalize as they typically learn superficial mapping from source code to labels instead of understanding the root causes of code vulnerabilities, resulting in poor performance in real-world scenarios beyond the training instances.","To tackle this challenge, we introduce VulLLM, a novel framework that integrates multi-task learning with Large Language Models (LLMs) to effectively mine deep-seated vulnerability features.","Specifically, we construct two auxiliary tasks beyond the vulnerability detection task.","First, we utilize the vulnerability patches to construct a vulnerability localization task.","Second, based on the vulnerability features extracted from patches, we leverage GPT-4 to construct a vulnerability interpretation task.","VulLLM innovatively augments vulnerability classification by leveraging generative LLMs to understand complex vulnerability patterns, thus compelling the model to capture the root causes of vulnerabilities rather than overfitting to spurious features of a single task.","The experiments conducted on six large datasets demonstrate that VulLLM surpasses seven state-of-the-art models in terms of effectiveness, generalization, and robustness."],"url":"http://arxiv.org/abs/2406.03718v1","category":"cs.CR"}
{"created":"2024-06-06 03:14:59","title":"Pi-fusion: Physics-informed diffusion model for learning fluid dynamics","abstract":"Physics-informed deep learning has been developed as a novel paradigm for learning physical dynamics recently. While general physics-informed deep learning methods have shown early promise in learning fluid dynamics, they are difficult to generalize in arbitrary time instants in real-world scenario, where the fluid motion can be considered as a time-variant trajectory involved large-scale particles. Inspired by the advantage of diffusion model in learning the distribution of data, we first propose Pi-fusion, a physics-informed diffusion model for predicting the temporal evolution of velocity and pressure field in fluid dynamics. Physics-informed guidance sampling is proposed in the inference procedure of Pi-fusion to improve the accuracy and interpretability of learning fluid dynamics. Furthermore, we introduce a training strategy based on reciprocal learning to learn the quasiperiodical pattern of fluid motion and thus improve the generalizability of the model. The proposed approach are then evaluated on both synthetic and real-world dataset, by comparing it with state-of-the-art physics-informed deep learning methods. Experimental results show that the proposed approach significantly outperforms existing methods for predicting temporal evolution of velocity and pressure field, confirming its strong generalization by drawing probabilistic inference of forward process and physics-informed guidance sampling. The proposed Pi-fusion can also be generalized in learning other physical dynamics governed by partial differential equations.","sentences":["Physics-informed deep learning has been developed as a novel paradigm for learning physical dynamics recently.","While general physics-informed deep learning methods have shown early promise in learning fluid dynamics, they are difficult to generalize in arbitrary time instants in real-world scenario, where the fluid motion can be considered as a time-variant trajectory involved large-scale particles.","Inspired by the advantage of diffusion model in learning the distribution of data, we first propose Pi-fusion, a physics-informed diffusion model for predicting the temporal evolution of velocity and pressure field in fluid dynamics.","Physics-informed guidance sampling is proposed in the inference procedure of Pi-fusion to improve the accuracy and interpretability of learning fluid dynamics.","Furthermore, we introduce a training strategy based on reciprocal learning to learn the quasiperiodical pattern of fluid motion and thus improve the generalizability of the model.","The proposed approach are then evaluated on both synthetic and real-world dataset, by comparing it with state-of-the-art physics-informed deep learning methods.","Experimental results show that the proposed approach significantly outperforms existing methods for predicting temporal evolution of velocity and pressure field, confirming its strong generalization by drawing probabilistic inference of forward process and physics-informed guidance sampling.","The proposed Pi-fusion can also be generalized in learning other physical dynamics governed by partial differential equations."],"url":"http://arxiv.org/abs/2406.03711v1","category":"physics.flu-dyn"}
{"created":"2024-06-06 03:14:23","title":"TwinS: Revisiting Non-Stationarity in Multivariate Time Series Forecasting","abstract":"Recently, multivariate time series forecasting tasks have garnered increasing attention due to their significant practical applications, leading to the emergence of various deep forecasting models. However, real-world time series exhibit pronounced non-stationary distribution characteristics. These characteristics are not solely limited to time-varying statistical properties highlighted by non-stationary Transformer but also encompass three key aspects: nested periodicity, absence of periodic distributions, and hysteresis among time variables. In this paper, we begin by validating this theory through wavelet analysis and propose the Transformer-based TwinS model, which consists of three modules to address the non-stationary periodic distributions: Wavelet Convolution, Period-Aware Attention, and Channel-Temporal Mixed MLP. Specifically, The Wavelet Convolution models nested periods by scaling the convolution kernel size like wavelet transform. The Period-Aware Attention guides attention computation by generating period relevance scores through a convolutional sub-network. The Channel-Temporal Mixed MLP captures the overall relationships between time series through channel-time mixing learning. TwinS achieves SOTA performance compared to mainstream TS models, with a maximum improvement in MSE of 25.8\\% over PatchTST.","sentences":["Recently, multivariate time series forecasting tasks have garnered increasing attention due to their significant practical applications, leading to the emergence of various deep forecasting models.","However, real-world time series exhibit pronounced non-stationary distribution characteristics.","These characteristics are not solely limited to time-varying statistical properties highlighted by non-stationary Transformer but also encompass three key aspects: nested periodicity, absence of periodic distributions, and hysteresis among time variables.","In this paper, we begin by validating this theory through wavelet analysis and propose the Transformer-based TwinS model, which consists of three modules to address the non-stationary periodic distributions: Wavelet Convolution, Period-Aware Attention, and Channel-Temporal Mixed MLP.","Specifically, The Wavelet Convolution models nested periods by scaling the convolution kernel size like wavelet transform.","The Period-Aware Attention guides attention computation by generating period relevance scores through a convolutional sub-network.","The Channel-Temporal Mixed MLP captures the overall relationships between time series through channel-time mixing learning.","TwinS achieves SOTA performance compared to mainstream TS models, with a maximum improvement in MSE of 25.8\\% over PatchTST."],"url":"http://arxiv.org/abs/2406.03710v1","category":"cs.LG"}
{"created":"2024-06-06 03:06:46","title":"What Should Embeddings Embed? Autoregressive Models Represent Latent Generating Distributions","abstract":"Autoregressive language models have demonstrated a remarkable ability to extract latent structure from text. The embeddings from large language models have been shown to capture aspects of the syntax and semantics of language. But what {\\em should} embeddings represent? We connect the autoregressive prediction objective to the idea of constructing predictive sufficient statistics to summarize the information contained in a sequence of observations, and use this connection to identify three settings where the optimal content of embeddings can be identified: independent identically distributed data, where the embedding should capture the sufficient statistics of the data; latent state models, where the embedding should encode the posterior distribution over states given the data; and discrete hypothesis spaces, where the embedding should reflect the posterior distribution over hypotheses given the data. We then conduct empirical probing studies to show that transformers encode these three kinds of latent generating distributions, and that they perform well in out-of-distribution cases and without token memorization in these settings.","sentences":["Autoregressive language models have demonstrated a remarkable ability to extract latent structure from text.","The embeddings from large language models have been shown to capture aspects of the syntax and semantics of language.","But what {\\em should} embeddings represent?","We connect the autoregressive prediction objective to the idea of constructing predictive sufficient statistics to summarize the information contained in a sequence of observations, and use this connection to identify three settings where the optimal content of embeddings can be identified: independent identically distributed data, where the embedding should capture the sufficient statistics of the data; latent state models, where the embedding should encode the posterior distribution over states given the data; and discrete hypothesis spaces, where the embedding should reflect the posterior distribution over hypotheses given the data.","We then conduct empirical probing studies to show that transformers encode these three kinds of latent generating distributions, and that they perform well in out-of-distribution cases and without token memorization in these settings."],"url":"http://arxiv.org/abs/2406.03707v1","category":"cs.LG"}
{"created":"2024-06-06 02:43:21","title":"M-QALM: A Benchmark to Assess Clinical Reading Comprehension and Knowledge Recall in Large Language Models via Question Answering","abstract":"There is vivid research on adapting Large Language Models (LLMs) to perform a variety of tasks in high-stakes domains such as healthcare. Despite their popularity, there is a lack of understanding of the extent and contributing factors that allow LLMs to recall relevant knowledge and combine it with presented information in the clinical and biomedical domain: a fundamental pre-requisite for success on down-stream tasks. Addressing this gap, we use Multiple Choice and Abstractive Question Answering to conduct a large-scale empirical study on 22 datasets in three generalist and three specialist biomedical sub-domains. Our multifaceted analysis of the performance of 15 LLMs, further broken down by sub-domain, source of knowledge and model architecture, uncovers success factors such as instruction tuning that lead to improved recall and comprehension. We further show that while recently proposed domain-adapted models may lack adequate knowledge, directly fine-tuning on our collected medical knowledge datasets shows encouraging results, even generalising to unseen specialist sub-domains. We complement the quantitative results with a skill-oriented manual error analysis, which reveals a significant gap between the models' capabilities to simply recall necessary knowledge and to integrate it with the presented context. To foster research and collaboration in this field we share M-QALM, our resources, standardised methodology, and evaluation results, with the research community to facilitate further advancements in clinical knowledge representation learning within language models.","sentences":["There is vivid research on adapting Large Language Models (LLMs) to perform a variety of tasks in high-stakes domains such as healthcare.","Despite their popularity, there is a lack of understanding of the extent and contributing factors that allow LLMs to recall relevant knowledge and combine it with presented information in the clinical and biomedical domain: a fundamental pre-requisite for success on down-stream tasks.","Addressing this gap, we use Multiple Choice and Abstractive Question Answering to conduct a large-scale empirical study on 22 datasets in three generalist and three specialist biomedical sub-domains.","Our multifaceted analysis of the performance of 15 LLMs, further broken down by sub-domain, source of knowledge and model architecture, uncovers success factors such as instruction tuning that lead to improved recall and comprehension.","We further show that while recently proposed domain-adapted models may lack adequate knowledge, directly fine-tuning on our collected medical knowledge datasets shows encouraging results, even generalising to unseen specialist sub-domains.","We complement the quantitative results with a skill-oriented manual error analysis, which reveals a significant gap between the models' capabilities to simply recall necessary knowledge and to integrate it with the presented context.","To foster research and collaboration in this field we share M-QALM, our resources, standardised methodology, and evaluation results, with the research community to facilitate further advancements in clinical knowledge representation learning within language models."],"url":"http://arxiv.org/abs/2406.03699v1","category":"cs.CL"}
{"created":"2024-06-06 02:20:31","title":"Evaluating the World Model Implicit in a Generative Model","abstract":"Recent work suggests that large language models may implicitly learn world models. How should we assess this possibility? We formalize this question for the case where the underlying reality is governed by a deterministic finite automaton. This includes problems as diverse as simple logical reasoning, geographic navigation, game-playing, and chemistry. We propose new evaluation metrics for world model recovery inspired by the classic Myhill-Nerode theorem from language theory. We illustrate their utility in three domains: game playing, logic puzzles, and navigation. In all domains, the generative models we consider do well on existing diagnostics for assessing world models, but our evaluation metrics reveal their world models to be far less coherent than they appear. Such incoherence creates fragility: using a generative model to solve related but subtly different tasks can lead it to fail badly. Building generative models that meaningfully capture the underlying logic of the domains they model would be immensely valuable; our results suggest new ways to assess how close a given model is to that goal.","sentences":["Recent work suggests that large language models may implicitly learn world models.","How should we assess this possibility?","We formalize this question for the case where the underlying reality is governed by a deterministic finite automaton.","This includes problems as diverse as simple logical reasoning, geographic navigation, game-playing, and chemistry.","We propose new evaluation metrics for world model recovery inspired by the classic Myhill-Nerode theorem from language theory.","We illustrate their utility in three domains: game playing, logic puzzles, and navigation.","In all domains, the generative models we consider do well on existing diagnostics for assessing world models, but our evaluation metrics reveal their world models to be far less coherent than they appear.","Such incoherence creates fragility: using a generative model to solve related but subtly different tasks can lead it to fail badly.","Building generative models that meaningfully capture the underlying logic of the domains they model would be immensely valuable; our results suggest new ways to assess how close a given model is to that goal."],"url":"http://arxiv.org/abs/2406.03689v1","category":"cs.CL"}
{"created":"2024-06-06 01:51:09","title":"Multiscale Tests for Point Processes and Longitudinal Networks","abstract":"We propose a new testing framework applicable to both the two-sample problem on point processes and the community detection problem on rectangular arrays of point processes, which we refer to as longitudinal networks; the latter problem is useful in situations where we observe interactions among a group of individuals over time. Our framework is based on a multiscale discretization scheme that consider not just the global null but also a collection of nulls local to small regions in the domain; in the two-sample problem, the local rejections tell us where the intensity functions differ and in the longitudinal network problem, the local rejections tell us when the community structure is most salient. We provide theoretical analysis for the two-sample problem and show that our method has minimax optimal power under a Holder continuity condition. We provide extensive simulation and real data analysis demonstrating the practicality of our proposed method.","sentences":["We propose a new testing framework applicable to both the two-sample problem on point processes and the community detection problem on rectangular arrays of point processes, which we refer to as longitudinal networks; the latter problem is useful in situations where we observe interactions among a group of individuals over time.","Our framework is based on a multiscale discretization scheme that consider not just the global null but also a collection of nulls local to small regions in the domain; in the two-sample problem, the local rejections tell us where the intensity functions differ and in the longitudinal network problem, the local rejections tell us when the community structure is most salient.","We provide theoretical analysis for the two-sample problem and show that our method has minimax optimal power under a Holder continuity condition.","We provide extensive simulation and real data analysis demonstrating the practicality of our proposed method."],"url":"http://arxiv.org/abs/2406.03681v1","category":"stat.ME"}
{"created":"2024-06-06 01:49:29","title":"On the Effects of Data Scale on Computer Control Agents","abstract":"Autonomous agents that control computer interfaces to accomplish human tasks are emerging. Leveraging LLMs to power such agents has been of special interest, but unless fine-tuned on human-collected task demonstrations, performance is still relatively low. In this work we study whether fine-tuning alone is a viable approach for building real-world computer control agents. %In particularly, we investigate how performance measured on both high and low-level tasks in domain and out of domain scales as more training data is collected.   To this end we collect and release a new dataset, AndroidControl, consisting of 15,283 demonstrations of everyday tasks with Android apps. Compared to existing datasets, each AndroidControl task instance includes both high and low-level human-generated instructions, allowing us to explore the level of task complexity an agent can handle. Moreover, AndroidControl is the most diverse computer control dataset to date, including 15,283 unique tasks over 833 Android apps, thus allowing us to conduct in-depth analysis of the model performance in and out of the domain of the training data. Using the dataset, we find that when tested in domain fine-tuned models outperform zero and few-shot baselines and scale in such a way that robust performance might feasibly be obtained simply by collecting more data. Out of domain, performance scales significantly more slowly and suggests that in particular for high-level tasks, fine-tuning on more data alone may be insufficient for achieving robust out-of-domain performance.","sentences":["Autonomous agents that control computer interfaces to accomplish human tasks are emerging.","Leveraging LLMs to power such agents has been of special interest, but unless fine-tuned on human-collected task demonstrations, performance is still relatively low.","In this work we study whether fine-tuning alone is a viable approach for building real-world computer control agents.","%In particularly, we investigate how performance measured on both high and low-level tasks in domain and out of domain scales as more training data is collected.   ","To this end we collect and release a new dataset, AndroidControl, consisting of 15,283 demonstrations of everyday tasks with Android apps.","Compared to existing datasets, each AndroidControl task instance includes both high and low-level human-generated instructions, allowing us to explore the level of task complexity an agent can handle.","Moreover, AndroidControl is the most diverse computer control dataset to date, including 15,283 unique tasks over 833 Android apps, thus allowing us to conduct in-depth analysis of the model performance in and out of the domain of the training data.","Using the dataset, we find that when tested in domain fine-tuned models outperform zero and few-shot baselines and scale in such a way that robust performance might feasibly be obtained simply by collecting more data.","Out of domain, performance scales significantly more slowly and suggests that in particular for high-level tasks, fine-tuning on more data alone may be insufficient for achieving robust out-of-domain performance."],"url":"http://arxiv.org/abs/2406.03679v1","category":"cs.AI"}
{"created":"2024-06-06 01:46:49","title":"Reflective Policy Optimization","abstract":"On-policy reinforcement learning methods, like Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), often demand extensive data per update, leading to sample inefficiency. This paper introduces Reflective Policy Optimization (RPO), a novel on-policy extension that amalgamates past and future state-action information for policy optimization. This approach empowers the agent for introspection, allowing modifications to its actions within the current state. Theoretical analysis confirms that policy performance is monotonically improved and contracts the solution space, consequently expediting the convergence procedure. Empirical results demonstrate RPO's feasibility and efficacy in two reinforcement learning benchmarks, culminating in superior sample efficiency. The source code of this work is available at https://github.com/Edgargan/RPO.","sentences":["On-policy reinforcement learning methods, like Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), often demand extensive data per update, leading to sample inefficiency.","This paper introduces Reflective Policy Optimization (RPO), a novel on-policy extension that amalgamates past and future state-action information for policy optimization.","This approach empowers the agent for introspection, allowing modifications to its actions within the current state.","Theoretical analysis confirms that policy performance is monotonically improved and contracts the solution space, consequently expediting the convergence procedure.","Empirical results demonstrate RPO's feasibility and efficacy in two reinforcement learning benchmarks, culminating in superior sample efficiency.","The source code of this work is available at https://github.com/Edgargan/RPO."],"url":"http://arxiv.org/abs/2406.03678v1","category":"cs.LG"}
{"created":"2024-06-06 01:23:45","title":"Linguistically Conditioned Semantic Textual Similarity","abstract":"Semantic textual similarity (STS) is a fundamental NLP task that measures the semantic similarity between a pair of sentences. In order to reduce the inherent ambiguity posed from the sentences, a recent work called Conditional STS (C-STS) has been proposed to measure the sentences' similarity conditioned on a certain aspect. Despite the popularity of C-STS, we find that the current C-STS dataset suffers from various issues that could impede proper evaluation on this task. In this paper, we reannotate the C-STS validation set and observe an annotator discrepancy on 55% of the instances resulting from the annotation errors in the original label, ill-defined conditions, and the lack of clarity in the task definition. After a thorough dataset analysis, we improve the C-STS task by leveraging the models' capability to understand the conditions under a QA task setting. With the generated answers, we present an automatic error identification pipeline that is able to identify annotation errors from the C-STS data with over 80% F1 score. We also propose a new method that largely improves the performance over baselines on the C-STS data by training the models with the answers. Finally we discuss the conditionality annotation based on the typed-feature structure (TFS) of entity types. We show in examples that the TFS is able to provide a linguistic foundation for constructing C-STS data with new conditions.","sentences":["Semantic textual similarity (STS) is a fundamental NLP task that measures the semantic similarity between a pair of sentences.","In order to reduce the inherent ambiguity posed from the sentences, a recent work called Conditional STS (C-STS) has been proposed to measure the sentences' similarity conditioned on a certain aspect.","Despite the popularity of C-STS, we find that the current C-STS dataset suffers from various issues that could impede proper evaluation on this task.","In this paper, we reannotate the C-STS validation set and observe an annotator discrepancy on 55% of the instances resulting from the annotation errors in the original label, ill-defined conditions, and the lack of clarity in the task definition.","After a thorough dataset analysis, we improve the C-STS task by leveraging the models' capability to understand the conditions under a QA task setting.","With the generated answers, we present an automatic error identification pipeline that is able to identify annotation errors from the C-STS data with over 80% F1 score.","We also propose a new method that largely improves the performance over baselines on the C-STS data by training the models with the answers.","Finally we discuss the conditionality annotation based on the typed-feature structure (TFS) of entity types.","We show in examples that the TFS is able to provide a linguistic foundation for constructing C-STS data with new conditions."],"url":"http://arxiv.org/abs/2406.03673v1","category":"cs.CL"}
{"created":"2024-06-06 01:14:24","title":"PANDA: Expanded Width-Aware Message Passing Beyond Rewiring","abstract":"Recent research in the field of graph neural network (GNN) has identified a critical issue known as \"over-squashing,\" resulting from the bottleneck phenomenon in graph structures, which impedes the propagation of long-range information. Prior works have proposed a variety of graph rewiring concepts that aim at optimizing the spatial or spectral properties of graphs to promote the signal propagation. However, such approaches inevitably deteriorate the original graph topology, which may lead to a distortion of information flow. To address this, we introduce an expanded width-aware (PANDA) message passing, a new message passing paradigm where nodes with high centrality, a potential source of over-squashing, are selectively expanded in width to encapsulate the growing influx of signals from distant nodes. Experimental results show that our method outperforms existing rewiring methods, suggesting that selectively expanding the hidden state of nodes can be a compelling alternative to graph rewiring for addressing the over-squashing.","sentences":["Recent research in the field of graph neural network (GNN) has identified a critical issue known as \"over-squashing,\" resulting from the bottleneck phenomenon in graph structures, which impedes the propagation of long-range information.","Prior works have proposed a variety of graph rewiring concepts that aim at optimizing the spatial or spectral properties of graphs to promote the signal propagation.","However, such approaches inevitably deteriorate the original graph topology, which may lead to a distortion of information flow.","To address this, we introduce an expanded width-aware (PANDA) message passing, a new message passing paradigm where nodes with high centrality, a potential source of over-squashing, are selectively expanded in width to encapsulate the growing influx of signals from distant nodes.","Experimental results show that our method outperforms existing rewiring methods, suggesting that selectively expanding the hidden state of nodes can be a compelling alternative to graph rewiring for addressing the over-squashing."],"url":"http://arxiv.org/abs/2406.03671v1","category":"cs.LG"}
{"created":"2024-06-06 00:56:25","title":"3rd Place Solution for MOSE Track in CVPR 2024 PVUW workshop: Complex Video Object Segmentation","abstract":"Video Object Segmentation (VOS) is a vital task in computer vision, focusing on distinguishing foreground objects from the background across video frames. Our work draws inspiration from the Cutie model, and we investigate the effects of object memory, the total number of memory frames, and input resolution on segmentation performance. This report validates the effectiveness of our inference method on the coMplex video Object SEgmentation (MOSE) dataset, which features complex occlusions. Our experimental results demonstrate that our approach achieves a J\\&F score of 0.8139 on the test set, securing the third position in the final ranking. These findings highlight the robustness and accuracy of our method in handling challenging VOS scenarios.","sentences":["Video Object Segmentation (VOS) is a vital task in computer vision, focusing on distinguishing foreground objects from the background across video frames.","Our work draws inspiration from the Cutie model, and we investigate the effects of object memory, the total number of memory frames, and input resolution on segmentation performance.","This report validates the effectiveness of our inference method on the coMplex video Object SEgmentation (MOSE) dataset, which features complex occlusions.","Our experimental results demonstrate that our approach achieves a J\\&F score of 0.8139 on the test set, securing the third position in the final ranking.","These findings highlight the robustness and accuracy of our method in handling challenging VOS scenarios."],"url":"http://arxiv.org/abs/2406.03668v1","category":"cs.CV"}
{"created":"2024-06-06 00:50:22","title":"Towards Dynamic Trend Filtering through Trend Point Detection with Reinforcement Learning","abstract":"Trend filtering simplifies complex time series data by applying smoothness to filter out noise while emphasizing proximity to the original data. However, existing trend filtering methods fail to reflect abrupt changes in the trend due to `approximateness,' resulting in constant smoothness. This approximateness uniformly filters out the tail distribution of time series data, characterized by extreme values, including both abrupt changes and noise. In this paper, we propose Trend Point Detection formulated as a Markov Decision Process (MDP), a novel approach to identifying essential points that should be reflected in the trend, departing from approximations. We term these essential points as Dynamic Trend Points (DTPs) and extract trends by interpolating them. To identify DTPs, we utilize Reinforcement Learning (RL) within a discrete action space and a forecasting sum-of-squares loss function as a reward, referred to as the Dynamic Trend Filtering network (DTF-net). DTF-net integrates flexible noise filtering, preserving critical original subsequences while removing noise as required for other subsequences. We demonstrate that DTF-net excels at capturing abrupt changes compared to other trend filtering algorithms and enhances forecasting performance, as abrupt changes are predicted rather than smoothed out.","sentences":["Trend filtering simplifies complex time series data by applying smoothness to filter out noise while emphasizing proximity to the original data.","However, existing trend filtering methods fail to reflect abrupt changes in the trend due to `approximateness,' resulting in constant smoothness.","This approximateness uniformly filters out the tail distribution of time series data, characterized by extreme values, including both abrupt changes and noise.","In this paper, we propose Trend Point Detection formulated as a Markov Decision Process (MDP), a novel approach to identifying essential points that should be reflected in the trend, departing from approximations.","We term these essential points as Dynamic Trend Points (DTPs) and extract trends by interpolating them.","To identify DTPs, we utilize Reinforcement Learning (RL) within a discrete action space and a forecasting sum-of-squares loss function as a reward, referred to as the Dynamic Trend Filtering network (DTF-net).","DTF-net integrates flexible noise filtering, preserving critical original subsequences while removing noise as required for other subsequences.","We demonstrate that DTF-net excels at capturing abrupt changes compared to other trend filtering algorithms and enhances forecasting performance, as abrupt changes are predicted rather than smoothed out."],"url":"http://arxiv.org/abs/2406.03665v1","category":"cs.LG"}
{"created":"2024-06-05 23:53:30","title":"UrBAN: Urban Beehive Acoustics and PheNotyping Dataset","abstract":"In this paper, we present a multimodal dataset obtained from a honey bee colony in Montr\\'eal, Quebec, Canada, spanning the years of 2021 to 2022. This apiary comprised 10 beehives, with microphones recording more than 2000 hours of high quality raw audio, and also sensors capturing temperature, and humidity. Periodic hive inspections involved monitoring colony honey bee population changes, assessing queen-related conditions, and documenting overall hive health. Additionally, health metrics, such as Varroa mite infestation rates and winter mortality assessments were recorded, offering valuable insights into factors affecting hive health status and resilience. In this study, we first outline the data collection process, sensor data description, and dataset structure. Furthermore, we demonstrate a practical application of this dataset by extracting various features from the raw audio to predict colony population using the number of frames of bees as a proxy.","sentences":["In this paper, we present a multimodal dataset obtained from a honey bee colony in Montr\\'eal, Quebec, Canada, spanning the years of 2021 to 2022.","This apiary comprised 10 beehives, with microphones recording more than 2000 hours of high quality raw audio, and also sensors capturing temperature, and humidity.","Periodic hive inspections involved monitoring colony honey bee population changes, assessing queen-related conditions, and documenting overall hive health.","Additionally, health metrics, such as Varroa mite infestation rates and winter mortality assessments were recorded, offering valuable insights into factors affecting hive health status and resilience.","In this study, we first outline the data collection process, sensor data description, and dataset structure.","Furthermore, we demonstrate a practical application of this dataset by extracting various features from the raw audio to predict colony population using the number of frames of bees as a proxy."],"url":"http://arxiv.org/abs/2406.03657v1","category":"eess.AS"}
{"created":"2024-06-05 23:08:57","title":"Ensembling Portfolio Strategies for Long-Term Investments: A Distribution-Free Preference Framework for Decision-Making and Algorithms","abstract":"This paper investigates the problem of ensembling multiple strategies for sequential portfolios to outperform individual strategies in terms of long-term wealth. Due to the uncertainty of strategies' performances in the future market, which are often based on specific models and statistical assumptions, investors often mitigate risk and enhance robustness by combining multiple strategies, akin to common approaches in collective learning prediction. However, the absence of a distribution-free and consistent preference framework complicates decisions of combination due to the ambiguous objective. To address this gap, we introduce a novel framework for decision-making in combining strategies, irrespective of market conditions, by establishing the investor's preference between decisions and then forming a clear objective. Through this framework, we propose a combinatorial strategy construction, free from statistical assumptions, for any scale of component strategies, even infinite, such that it meets the determined criterion. Finally, we test the proposed strategy along with its accelerated variant and some other multi-strategies. The numerical experiments show results in favor of the proposed strategies, albeit with small tradeoffs in their Sharpe ratios, in which their cumulative wealths eventually exceed those of the best component strategies while the accelerated strategy significantly improves performance.","sentences":["This paper investigates the problem of ensembling multiple strategies for sequential portfolios to outperform individual strategies in terms of long-term wealth.","Due to the uncertainty of strategies' performances in the future market, which are often based on specific models and statistical assumptions, investors often mitigate risk and enhance robustness by combining multiple strategies, akin to common approaches in collective learning prediction.","However, the absence of a distribution-free and consistent preference framework complicates decisions of combination due to the ambiguous objective.","To address this gap, we introduce a novel framework for decision-making in combining strategies, irrespective of market conditions, by establishing the investor's preference between decisions and then forming a clear objective.","Through this framework, we propose a combinatorial strategy construction, free from statistical assumptions, for any scale of component strategies, even infinite, such that it meets the determined criterion.","Finally, we test the proposed strategy along with its accelerated variant and some other multi-strategies.","The numerical experiments show results in favor of the proposed strategies, albeit with small tradeoffs in their Sharpe ratios, in which their cumulative wealths eventually exceed those of the best component strategies while the accelerated strategy significantly improves performance."],"url":"http://arxiv.org/abs/2406.03652v1","category":"q-fin.PM"}
{"created":"2024-06-05 23:06:48","title":"Inductive Generalization in Reinforcement Learning from Specifications","abstract":"We present a novel inductive generalization framework for RL from logical specifications. Many interesting tasks in RL environments have a natural inductive structure. These inductive tasks have similar overarching goals but they differ inductively in low-level predicates and distributions. We present a generalization procedure that leverages this inductive relationship to learn a higher-order function, a policy generator, that generates appropriately adapted policies for instances of an inductive task in a zero-shot manner. An evaluation of the proposed approach on a set of challenging control benchmarks demonstrates the promise of our framework in generalizing to unseen policies for long-horizon tasks.","sentences":["We present a novel inductive generalization framework for RL from logical specifications.","Many interesting tasks in RL environments have a natural inductive structure.","These inductive tasks have similar overarching goals but they differ inductively in low-level predicates and distributions.","We present a generalization procedure that leverages this inductive relationship to learn a higher-order function, a policy generator, that generates appropriately adapted policies for instances of an inductive task in a zero-shot manner.","An evaluation of the proposed approach on a set of challenging control benchmarks demonstrates the promise of our framework in generalizing to unseen policies for long-horizon tasks."],"url":"http://arxiv.org/abs/2406.03651v1","category":"cs.LG"}
{"created":"2024-06-05 22:52:27","title":"Decision-focused Graph Neural Networks for Combinatorial Optimization","abstract":"In recent years, there has been notable interest in investigating combinatorial optimization (CO) problems by neural-based framework. An emerging strategy to tackle these challenging problems involves the adoption of graph neural networks (GNNs) as an alternative to traditional algorithms, a subject that has attracted considerable attention. Despite the growing popularity of GNNs and traditional algorithm solvers in the realm of CO, there is limited research on their integrated use and the correlation between them within an end-to-end framework. The primary focus of our work is to formulate a more efficient and precise framework for CO by employing decision-focused learning on graphs. Additionally, we introduce a decision-focused framework that utilizes GNNs to address CO problems with auxiliary support. To realize an end-to-end approach, we have designed two cascaded modules: (a) an unsupervised trained graph predictive model, and (b) a solver for quadratic binary unconstrained optimization. Empirical evaluations are conducted on various classical tasks, including maximum cut, maximum independent set, and minimum vertex cover. The experimental results on classical CO problems (i.e. MaxCut, MIS, and MVC) demonstrate the superiority of our method over both the standalone GNN approach and classical methods.","sentences":["In recent years, there has been notable interest in investigating combinatorial optimization (CO) problems by neural-based framework.","An emerging strategy to tackle these challenging problems involves the adoption of graph neural networks (GNNs) as an alternative to traditional algorithms, a subject that has attracted considerable attention.","Despite the growing popularity of GNNs and traditional algorithm solvers in the realm of CO, there is limited research on their integrated use and the correlation between them within an end-to-end framework.","The primary focus of our work is to formulate a more efficient and precise framework for CO by employing decision-focused learning on graphs.","Additionally, we introduce a decision-focused framework that utilizes GNNs to address CO problems with auxiliary support.","To realize an end-to-end approach, we have designed two cascaded modules: (a) an unsupervised trained graph predictive model, and (b) a solver for quadratic binary unconstrained optimization.","Empirical evaluations are conducted on various classical tasks, including maximum cut, maximum independent set, and minimum vertex cover.","The experimental results on classical CO problems (i.e. MaxCut, MIS, and MVC) demonstrate the superiority of our method over both the standalone GNN approach and classical methods."],"url":"http://arxiv.org/abs/2406.03647v1","category":"cs.LG"}
{"created":"2024-06-05 22:30:40","title":"Task and Motion Planning for Execution in the Real","abstract":"Task and motion planning represents a powerful set of hybrid planning methods that combine reasoning over discrete task domains and continuous motion generation. Traditional reasoning necessitates task domain models and enough information to ground actions to motion planning queries. Gaps in this knowledge often arise from sources like occlusion or imprecise modeling. This work generates task and motion plans that include actions cannot be fully grounded at planning time. During execution, such an action is handled by a provided human-designed or learned closed-loop behavior. Execution combines offline planned motions and online behaviors till reaching the task goal. Failures of behaviors are fed back as constraints to find new plans. Forty real-robot trials and motivating demonstrations are performed to evaluate the proposed framework and compare against state-of-the-art. Results show faster execution time, less number of actions, and more success in problems where diverse gaps arise. The experiment data is shared for researchers to simulate these settings. The work shows promise in expanding the applicable class of realistic partially grounded problems that robots can address.","sentences":["Task and motion planning represents a powerful set of hybrid planning methods that combine reasoning over discrete task domains and continuous motion generation.","Traditional reasoning necessitates task domain models and enough information to ground actions to motion planning queries.","Gaps in this knowledge often arise from sources like occlusion or imprecise modeling.","This work generates task and motion plans that include actions cannot be fully grounded at planning time.","During execution, such an action is handled by a provided human-designed or learned closed-loop behavior.","Execution combines offline planned motions and online behaviors till reaching the task goal.","Failures of behaviors are fed back as constraints to find new plans.","Forty real-robot trials and motivating demonstrations are performed to evaluate the proposed framework and compare against state-of-the-art.","Results show faster execution time, less number of actions, and more success in problems where diverse gaps arise.","The experiment data is shared for researchers to simulate these settings.","The work shows promise in expanding the applicable class of realistic partially grounded problems that robots can address."],"url":"http://arxiv.org/abs/2406.03641v1","category":"cs.RO"}
{"created":"2024-06-05 21:29:05","title":"Active ML for 6G: Towards Efficient Data Generation, Acquisition, and Annotation","abstract":"This paper explores the integration of active machine learning (ML) for 6G networks, an area that remains under-explored yet holds potential. Unlike passive ML systems, active ML can be made to interact with the network environment. It actively selects informative and representative data points for training, thereby reducing the volume of data needed while accelerating the learning process. While active learning research mainly focuses on data annotation, we call for a network-centric active learning framework that considers both annotation (i.e., what is the label) and data acquisition (i.e., which and how many samples to collect). Moreover, we explore the synergy between generative artificial intelligence (AI) and active learning to overcome existing limitations in both active learning and generative AI. This paper also features a case study on a mmWave throughput prediction problem to demonstrate the practical benefits and improved performance of active learning for 6G networks. Furthermore, we discuss how the implications of active learning extend to numerous 6G network use cases. We highlight the potential of active learning based 6G networks to enhance computational efficiency, data annotation and acquisition efficiency, adaptability, and overall network intelligence. We conclude with a discussion on challenges and future research directions for active learning in 6G networks, including development of novel query strategies, distributed learning integration, and inclusion of human- and machine-in-the-loop learning.","sentences":["This paper explores the integration of active machine learning (ML) for 6G networks, an area that remains under-explored yet holds potential.","Unlike passive ML systems, active ML can be made to interact with the network environment.","It actively selects informative and representative data points for training, thereby reducing the volume of data needed while accelerating the learning process.","While active learning research mainly focuses on data annotation, we call for a network-centric active learning framework that considers both annotation (i.e., what is the label) and data acquisition (i.e., which and how many samples to collect).","Moreover, we explore the synergy between generative artificial intelligence (AI) and active learning to overcome existing limitations in both active learning and generative AI.","This paper also features a case study on a mmWave throughput prediction problem to demonstrate the practical benefits and improved performance of active learning for 6G networks.","Furthermore, we discuss how the implications of active learning extend to numerous 6G network use cases.","We highlight the potential of active learning based 6G networks to enhance computational efficiency, data annotation and acquisition efficiency, adaptability, and overall network intelligence.","We conclude with a discussion on challenges and future research directions for active learning in 6G networks, including development of novel query strategies, distributed learning integration, and inclusion of human- and machine-in-the-loop learning."],"url":"http://arxiv.org/abs/2406.03630v1","category":"cs.NI"}
{"created":"2024-06-05 21:02:10","title":"Degrees of Freedom Matter: Inferring Dynamics from Point Trajectories","abstract":"Understanding the dynamics of generic 3D scenes is fundamentally challenging in computer vision, essential in enhancing applications related to scene reconstruction, motion tracking, and avatar creation. In this work, we address the task as the problem of inferring dense, long-range motion of 3D points. By observing a set of point trajectories, we aim to learn an implicit motion field parameterized by a neural network to predict the movement of novel points within the same domain, without relying on any data-driven or scene-specific priors. To achieve this, our approach builds upon the recently introduced dynamic point field model that learns smooth deformation fields between the canonical frame and individual observation frames. However, temporal consistency between consecutive frames is neglected, and the number of required parameters increases linearly with the sequence length due to per-frame modeling. To address these shortcomings, we exploit the intrinsic regularization provided by SIREN, and modify the input layer to produce a spatiotemporally smooth motion field. Additionally, we analyze the motion field Jacobian matrix, and discover that the motion degrees of freedom (DOFs) in an infinitesimal area around a point and the network hidden variables have different behaviors to affect the model's representational power. This enables us to improve the model representation capability while retaining the model compactness. Furthermore, to reduce the risk of overfitting, we introduce a regularization term based on the assumption of piece-wise motion smoothness. Our experiments assess the model's performance in predicting unseen point trajectories and its application in temporal mesh alignment with guidance. The results demonstrate its superiority and effectiveness. The code and data for the project are publicly available: \\url{https://yz-cnsdqz.github.io/eigenmotion/DOMA/}","sentences":["Understanding the dynamics of generic 3D scenes is fundamentally challenging in computer vision, essential in enhancing applications related to scene reconstruction, motion tracking, and avatar creation.","In this work, we address the task as the problem of inferring dense, long-range motion of 3D points.","By observing a set of point trajectories, we aim to learn an implicit motion field parameterized by a neural network to predict the movement of novel points within the same domain, without relying on any data-driven or scene-specific priors.","To achieve this, our approach builds upon the recently introduced dynamic point field model that learns smooth deformation fields between the canonical frame and individual observation frames.","However, temporal consistency between consecutive frames is neglected, and the number of required parameters increases linearly with the sequence length due to per-frame modeling.","To address these shortcomings, we exploit the intrinsic regularization provided by SIREN, and modify the input layer to produce a spatiotemporally smooth motion field.","Additionally, we analyze the motion field Jacobian matrix, and discover that the motion degrees of freedom (DOFs) in an infinitesimal area around a point and the network hidden variables have different behaviors to affect the model's representational power.","This enables us to improve the model representation capability while retaining the model compactness.","Furthermore, to reduce the risk of overfitting, we introduce a regularization term based on the assumption of piece-wise motion smoothness.","Our experiments assess the model's performance in predicting unseen point trajectories and its application in temporal mesh alignment with guidance.","The results demonstrate its superiority and effectiveness.","The code and data for the project are publicly available: \\url{https://yz-cnsdqz.github.io/eigenmotion/DOMA/}"],"url":"http://arxiv.org/abs/2406.03625v1","category":"cs.CV"}
{"created":"2024-06-05 20:06:59","title":"FedPylot: Navigating Federated Learning for Real-Time Object Detection in Internet of Vehicles","abstract":"The Internet of Vehicles (IoV) emerges as a pivotal component for autonomous driving and intelligent transportation systems (ITS), by enabling low-latency big data processing in a dense interconnected network that comprises vehicles, infrastructures, pedestrians and the cloud. Autonomous vehicles are heavily reliant on machine learning (ML) and can strongly benefit from the wealth of sensory data generated at the edge, which calls for measures to reconcile model training with preserving the privacy of sensitive user data. Federated learning (FL) stands out as a promising solution to train sophisticated ML models in vehicular networks while protecting the privacy of road users and mitigating communication overhead. This paper examines the federated optimization of the cutting-edge YOLOv7 model to tackle real-time object detection amid data heterogeneity, encompassing unbalancedness, concept drift, and label distribution skews. To this end, we introduce FedPylot, a lightweight MPI-based prototype to simulate federated object detection experiments on high-performance computing (HPC) systems, where we safeguard server-client communications using hybrid encryption. Our study factors in accuracy, communication cost, and inference speed, thereby presenting a balanced approach to the challenges faced by autonomous vehicles. We demonstrate promising results for the applicability of FL in IoV and hope that FedPylot will provide a basis for future research into federated real-time object detection. The source code is available at https://github.com/cyprienquemeneur/fedpylot.","sentences":["The Internet of Vehicles (IoV) emerges as a pivotal component for autonomous driving and intelligent transportation systems (ITS), by enabling low-latency big data processing in a dense interconnected network that comprises vehicles, infrastructures, pedestrians and the cloud.","Autonomous vehicles are heavily reliant on machine learning (ML) and can strongly benefit from the wealth of sensory data generated at the edge, which calls for measures to reconcile model training with preserving the privacy of sensitive user data.","Federated learning (FL) stands out as a promising solution to train sophisticated ML models in vehicular networks while protecting the privacy of road users and mitigating communication overhead.","This paper examines the federated optimization of the cutting-edge YOLOv7 model to tackle real-time object detection amid data heterogeneity, encompassing unbalancedness, concept drift, and label distribution skews.","To this end, we introduce FedPylot, a lightweight MPI-based prototype to simulate federated object detection experiments on high-performance computing (HPC) systems, where we safeguard server-client communications using hybrid encryption.","Our study factors in accuracy, communication cost, and inference speed, thereby presenting a balanced approach to the challenges faced by autonomous vehicles.","We demonstrate promising results for the applicability of FL in IoV and hope that FedPylot will provide a basis for future research into federated real-time object detection.","The source code is available at https://github.com/cyprienquemeneur/fedpylot."],"url":"http://arxiv.org/abs/2406.03611v1","category":"cs.LG"}
{"created":"2024-06-05 20:02:54","title":"Modulated Ringdown Comb Interferometry for next-generation high complexity trace gas sensing","abstract":"Gas samples relevant to health and environment typically contain a plethora of molecular species that span a huge concentration dynamic range. High-concentration molecules impose a strong absorption background that hinders robust identification of low-concentration species. While mid-infrared frequency comb spectroscopy with high-finesse cavity enhancement has realized many of the most sensitive multi-species trace gas detection to date, its robust performance requires gas samples to contain only weak absorption features to avoid dispersing cavity resonances from the comb line frequencies. Here we introduce a new technique that is free from this restriction, thus enabling the development of next-generation multi-species trace gas sensing with broad applicability to complex and dynamic molecular compositions. The principle of Modulated Ringdown Comb Interferometry is to resolve ringdown dynamics carried by massively parallel comb lines transmitted through a length-modulated cavity. This method leverages both periodicity of the field dynamics and Doppler frequency shifts introduced from a Michelson interferometer. Scalable enhancement of both spectral coverage and cavity finesse is enabled with dispersion immune and high-efficiency data collection. Built upon this platform, we realize in the mid-infrared a product of finesse and spectral coverage that is orders of magnitude better than all prior experiments. We demonstrate the power of this technique by measuring highly dispersive exhaled human breath samples over a vastly expanded spectral coverage of 1,010 cm-1 and with cavity finesse of 23,000. This allows for the first time simultaneous quantification of 20 distinct molecular species at > 1 part-per-trillion sensitivity with their concentrations varying by 7 orders of magnitude.","sentences":["Gas samples relevant to health and environment typically contain a plethora of molecular species that span a huge concentration dynamic range.","High-concentration molecules impose a strong absorption background that hinders robust identification of low-concentration species.","While mid-infrared frequency comb spectroscopy with high-finesse cavity enhancement has realized many of the most sensitive multi-species trace gas detection to date, its robust performance requires gas samples to contain only weak absorption features to avoid dispersing cavity resonances from the comb line frequencies.","Here we introduce a new technique that is free from this restriction, thus enabling the development of next-generation multi-species trace gas sensing with broad applicability to complex and dynamic molecular compositions.","The principle of Modulated Ringdown Comb Interferometry is to resolve ringdown dynamics carried by massively parallel comb lines transmitted through a length-modulated cavity.","This method leverages both periodicity of the field dynamics and Doppler frequency shifts introduced from a Michelson interferometer.","Scalable enhancement of both spectral coverage and cavity finesse is enabled with dispersion immune and high-efficiency data collection.","Built upon this platform, we realize in the mid-infrared a product of finesse and spectral coverage that is orders of magnitude better than all prior experiments.","We demonstrate the power of this technique by measuring highly dispersive exhaled human breath samples over a vastly expanded spectral coverage of 1,010 cm-1 and with cavity finesse of 23,000.","This allows for the first time simultaneous quantification of 20 distinct molecular species at > 1 part-per-trillion sensitivity with their concentrations varying by 7 orders of magnitude."],"url":"http://arxiv.org/abs/2406.03609v1","category":"physics.optics"}
{"created":"2024-06-05 19:47:35","title":"Knowledge-Infused Legal Wisdom: Navigating LLM Consultation through the Lens of Diagnostics and Positive-Unlabeled Reinforcement Learning","abstract":"The integration of generative Large Language Models (LLMs) into various applications, including the legal domain, has been accelerated by their expansive and versatile nature. However, when facing a legal case, users without a legal background often struggle to formulate professional queries and may inadvertently overlook critical legal factors when presenting their case narrative to LLMs. To address this issue, we propose the Diagnostic Legal Large Language Model (D3LM), which utilizes adaptive lawyer-like diagnostic questions to collect additional case information and then provides high-quality feedback. D3LM incorporates an innovative graph-based Positive-Unlabeled Reinforcement Learning (PURL) algorithm, enabling the generation of critical questions and enhancing user-LLM interactions. Moreover, an integrated LLM-based stopping criterion facilitates precise Court Views Generation (CVG). Our research also introduces a new English-language CVG dataset based on the US case law database, enriching the realm of LLM research and deployment with a vital dimension. D3LM surpasses classical LLMs by delivering outstanding performance and a remarkable user experience in the legal domain.","sentences":["The integration of generative Large Language Models (LLMs) into various applications, including the legal domain, has been accelerated by their expansive and versatile nature.","However, when facing a legal case, users without a legal background often struggle to formulate professional queries and may inadvertently overlook critical legal factors when presenting their case narrative to LLMs.","To address this issue, we propose the Diagnostic Legal Large Language Model (D3LM), which utilizes adaptive lawyer-like diagnostic questions to collect additional case information and then provides high-quality feedback.","D3LM incorporates an innovative graph-based Positive-Unlabeled Reinforcement Learning (PURL) algorithm, enabling the generation of critical questions and enhancing user-LLM interactions.","Moreover, an integrated LLM-based stopping criterion facilitates precise Court Views Generation (CVG).","Our research also introduces a new English-language CVG dataset based on the US case law database, enriching the realm of LLM research and deployment with a vital dimension.","D3LM surpasses classical LLMs by delivering outstanding performance and a remarkable user experience in the legal domain."],"url":"http://arxiv.org/abs/2406.03600v1","category":"cs.CL"}
{"created":"2024-06-05 19:45:10","title":"Hi5: 2D Hand Pose Estimation with Zero Human Annotation","abstract":"We propose a new large synthetic hand pose estimation dataset, Hi5, and a novel inexpensive method for collecting high-quality synthetic data that requires no human annotation or validation. Leveraging recent advancements in computer graphics, high-fidelity 3D hand models with diverse genders and skin colors, and dynamic environments and camera movements, our data synthesis pipeline allows precise control over data diversity and representation, ensuring robust and fair model training. We generate a dataset with 583,000 images with accurate pose annotation using a single consumer PC that closely represents real-world variability. Pose estimation models trained with Hi5 perform competitively on real-hand benchmarks while surpassing models trained with real data when tested on occlusions and perturbations. Our experiments show promising results for synthetic data as a viable solution for data representation problems in real datasets. Overall, this paper provides a promising new approach to synthetic data creation and annotation that can reduce costs and increase the diversity and quality of data for hand pose estimation.","sentences":["We propose a new large synthetic hand pose estimation dataset, Hi5, and a novel inexpensive method for collecting high-quality synthetic data that requires no human annotation or validation.","Leveraging recent advancements in computer graphics, high-fidelity 3D hand models with diverse genders and skin colors, and dynamic environments and camera movements, our data synthesis pipeline allows precise control over data diversity and representation, ensuring robust and fair model training.","We generate a dataset with 583,000 images with accurate pose annotation using a single consumer PC that closely represents real-world variability.","Pose estimation models trained with Hi5 perform competitively on real-hand benchmarks while surpassing models trained with real data when tested on occlusions and perturbations.","Our experiments show promising results for synthetic data as a viable solution for data representation problems in real datasets.","Overall, this paper provides a promising new approach to synthetic data creation and annotation that can reduce costs and increase the diversity and quality of data for hand pose estimation."],"url":"http://arxiv.org/abs/2406.03599v1","category":"cs.CV"}
{"created":"2024-06-05 19:31:19","title":"Why is \"Problems\" Predictive of Positive Sentiment? A Case Study of Explaining Unintuitive Features in Sentiment Classification","abstract":"Explainable AI (XAI) algorithms aim to help users understand how a machine learning model makes predictions. To this end, many approaches explain which input features are most predictive of a target label. However, such explanations can still be puzzling to users (e.g., in product reviews, the word \"problems\" is predictive of positive sentiment). If left unexplained, puzzling explanations can have negative impacts. Explaining unintuitive associations between an input feature and a target label is an underexplored area in XAI research. We take an initial effort in this direction using unintuitive associations learned by sentiment classifiers as a case study. We propose approaches for (1) automatically detecting associations that can appear unintuitive to users and (2) generating explanations to help users understand why an unintuitive feature is predictive. Results from a crowdsourced study (N=300) found that our proposed approaches can effectively detect and explain predictive but unintuitive features in sentiment classification.","sentences":["Explainable AI (XAI) algorithms aim to help users understand how a machine learning model makes predictions.","To this end, many approaches explain which input features are most predictive of a target label.","However, such explanations can still be puzzling to users (e.g., in product reviews, the word \"problems\" is predictive of positive sentiment).","If left unexplained, puzzling explanations can have negative impacts.","Explaining unintuitive associations between an input feature and a target label is an underexplored area in XAI research.","We take an initial effort in this direction using unintuitive associations learned by sentiment classifiers as a case study.","We propose approaches for (1) automatically detecting associations that can appear unintuitive to users and (2) generating explanations to help users understand why an unintuitive feature is predictive.","Results from a crowdsourced study (N=300) found that our proposed approaches can effectively detect and explain predictive but unintuitive features in sentiment classification."],"url":"http://arxiv.org/abs/2406.03594v1","category":"cs.HC"}
{"created":"2024-06-05 19:30:52","title":"Measuring Retrieval Complexity in Question Answering Systems","abstract":"In this paper, we investigate which questions are challenging for retrieval-based Question Answering (QA). We (i) propose retrieval complexity (RC), a novel metric conditioned on the completeness of retrieved documents, which measures the difficulty of answering questions, and (ii) propose an unsupervised pipeline to measure RC given an arbitrary retrieval system. Our proposed pipeline measures RC more accurately than alternative estimators, including LLMs, on six challenging QA benchmarks. Further investigation reveals that RC scores strongly correlate with both QA performance and expert judgment across five of the six studied benchmarks, indicating that RC is an effective measure of question difficulty. Subsequent categorization of high-RC questions shows that they span a broad set of question shapes, including multi-hop, compositional, and temporal QA, indicating that RC scores can categorize a new subset of complex questions. Our system can also have a major impact on retrieval-based systems by helping to identify more challenging questions on existing datasets.","sentences":["In this paper, we investigate which questions are challenging for retrieval-based Question Answering (QA).","We (i) propose retrieval complexity (RC), a novel metric conditioned on the completeness of retrieved documents, which measures the difficulty of answering questions, and (ii) propose an unsupervised pipeline to measure RC given an arbitrary retrieval system.","Our proposed pipeline measures RC more accurately than alternative estimators, including LLMs, on six challenging QA benchmarks.","Further investigation reveals that RC scores strongly correlate with both QA performance and expert judgment across five of the six studied benchmarks, indicating that RC is an effective measure of question difficulty.","Subsequent categorization of high-RC questions shows that they span a broad set of question shapes, including multi-hop, compositional, and temporal QA, indicating that RC scores can categorize a new subset of complex questions.","Our system can also have a major impact on retrieval-based systems by helping to identify more challenging questions on existing datasets."],"url":"http://arxiv.org/abs/2406.03592v1","category":"cs.CL"}
{"created":"2024-06-05 19:05:08","title":"CountCLIP -- [Re] Teaching CLIP to Count to Ten","abstract":"Large vision-language models (VLMs) are shown to learn rich joint image-text representations enabling high performances in relevant downstream tasks. However, they fail to showcase their quantitative understanding of objects, and they lack good counting-aware representation. This paper conducts a reproducibility study of 'Teaching CLIP to Count to Ten' (Paiss et al., 2023), which presents a method to finetune a CLIP model (Radford et al., 2021) to improve zero-shot counting accuracy in an image while maintaining the performance for zero-shot classification by introducing a counting-contrastive loss term. We improve the model's performance on a smaller subset of their training data with lower computational resources. We verify these claims by reproducing their study with our own code. The implementation can be found at https://github.com/SforAiDl/CountCLIP.","sentences":["Large vision-language models (VLMs) are shown to learn rich joint image-text representations enabling high performances in relevant downstream tasks.","However, they fail to showcase their quantitative understanding of objects, and they lack good counting-aware representation.","This paper conducts a reproducibility study of 'Teaching CLIP to Count to Ten' (Paiss et al., 2023), which presents a method to finetune a CLIP model (Radford et al., 2021) to improve zero-shot counting accuracy in an image while maintaining the performance for zero-shot classification by introducing a counting-contrastive loss term.","We improve the model's performance on a smaller subset of their training data with lower computational resources.","We verify these claims by reproducing their study with our own code.","The implementation can be found at https://github.com/SforAiDl/CountCLIP."],"url":"http://arxiv.org/abs/2406.03586v1","category":"cs.CV"}
{"created":"2024-06-05 19:01:43","title":"A Comparison of Recent Algorithms for Symbolic Regression to Genetic Programming","abstract":"Symbolic regression is a machine learning method with the goal to produce interpretable results. Unlike other machine learning methods such as, e.g. random forests or neural networks, which are opaque, symbolic regression aims to model and map data in a way that can be understood by scientists. Recent advancements, have attempted to bridge the gap between these two fields; new methodologies attempt to fuse the mapping power of neural networks and deep learning techniques with the explanatory power of symbolic regression. In this paper, we examine these new emerging systems and test the performance of an end-to-end transformer model for symbolic regression versus the reigning traditional methods based on genetic programming that have spearheaded symbolic regression throughout the years. We compare these systems on novel datasets to avoid bias to older methods who were improved on well-known benchmark datasets. Our results show that traditional GP methods as implemented e.g., by Operon still remain superior to two recently published symbolic regression methods.","sentences":["Symbolic regression is a machine learning method with the goal to produce interpretable results.","Unlike other machine learning methods such as, e.g. random forests or neural networks, which are opaque, symbolic regression aims to model and map data in a way that can be understood by scientists.","Recent advancements, have attempted to bridge the gap between these two fields; new methodologies attempt to fuse the mapping power of neural networks and deep learning techniques with the explanatory power of symbolic regression.","In this paper, we examine these new emerging systems and test the performance of an end-to-end transformer model for symbolic regression versus the reigning traditional methods based on genetic programming that have spearheaded symbolic regression throughout the years.","We compare these systems on novel datasets to avoid bias to older methods who were improved on well-known benchmark datasets.","Our results show that traditional GP methods as implemented e.g., by Operon still remain superior to two recently published symbolic regression methods."],"url":"http://arxiv.org/abs/2406.03585v1","category":"cs.LG"}
{"created":"2024-06-05 18:57:02","title":"Understanding the Limitations of Diffusion Concept Algebra Through Food","abstract":"Image generation techniques, particularly latent diffusion models, have exploded in popularity in recent years. Many techniques have been developed to manipulate and clarify the semantic concepts these large-scale models learn, offering crucial insights into biases and concept relationships. However, these techniques are often only validated in conventional realms of human or animal faces and artistic style transitions. The food domain offers unique challenges through complex compositions and regional biases, which can shed light on the limitations and opportunities within existing methods. Through the lens of food imagery, we analyze both qualitative and quantitative patterns within a concept traversal technique. We reveal measurable insights into the model's ability to capture and represent the nuances of culinary diversity, while also identifying areas where the model's biases and limitations emerge.","sentences":["Image generation techniques, particularly latent diffusion models, have exploded in popularity in recent years.","Many techniques have been developed to manipulate and clarify the semantic concepts these large-scale models learn, offering crucial insights into biases and concept relationships.","However, these techniques are often only validated in conventional realms of human or animal faces and artistic style transitions.","The food domain offers unique challenges through complex compositions and regional biases, which can shed light on the limitations and opportunities within existing methods.","Through the lens of food imagery, we analyze both qualitative and quantitative patterns within a concept traversal technique.","We reveal measurable insights into the model's ability to capture and represent the nuances of culinary diversity, while also identifying areas where the model's biases and limitations emerge."],"url":"http://arxiv.org/abs/2406.03582v1","category":"cs.CV"}
{"created":"2024-06-05 18:48:00","title":"Explaining the Contributing Factors for Vulnerability Detection in Machine Learning","abstract":"There is an increasing trend to mine vulnerabilities from software repositories and use machine learning techniques to automatically detect software vulnerabilities. A fundamental but unresolved research question is: how do different factors in the mining and learning process impact the accuracy of identifying vulnerabilities in software projects of varying characteristics? Substantial research has been dedicated in this area, including source code static analysis, software repository mining, and NLP-based machine learning. However, practitioners lack experience regarding the key factors for building a baseline model of the state-of-the-art. In addition, there lacks of experience regarding the transferability of the vulnerability signatures from project to project. This study investigates how the combination of different vulnerability features and three representative machine learning models impact the accuracy of vulnerability detection in 17 real-world projects. We examine two types of vulnerability representations: 1) code features extracted through NLP with varying tokenization strategies and three different embedding techniques (bag-of-words, word2vec, and fastText) and 2) a set of eight architectural metrics that capture the abstract design of the software systems. The three machine learning algorithms include a random forest model, a support vector machines model, and a residual neural network model. The analysis shows a recommended baseline model with signatures extracted through bag-of-words embedding, combined with the random forest, consistently increases the detection accuracy by about 4% compared to other combinations in all 17 projects. Furthermore, we observe the limitation of transferring vulnerability signatures across domains based on our experiments.","sentences":["There is an increasing trend to mine vulnerabilities from software repositories and use machine learning techniques to automatically detect software vulnerabilities.","A fundamental but unresolved research question is: how do different factors in the mining and learning process impact the accuracy of identifying vulnerabilities in software projects of varying characteristics?","Substantial research has been dedicated in this area, including source code static analysis, software repository mining, and NLP-based machine learning.","However, practitioners lack experience regarding the key factors for building a baseline model of the state-of-the-art.","In addition, there lacks of experience regarding the transferability of the vulnerability signatures from project to project.","This study investigates how the combination of different vulnerability features and three representative machine learning models impact the accuracy of vulnerability detection in 17 real-world projects.","We examine two types of vulnerability representations: 1) code features extracted through NLP with varying tokenization strategies and three different embedding techniques (bag-of-words, word2vec, and fastText) and 2) a set of eight architectural metrics that capture the abstract design of the software systems.","The three machine learning algorithms include a random forest model, a support vector machines model, and a residual neural network model.","The analysis shows a recommended baseline model with signatures extracted through bag-of-words embedding, combined with the random forest, consistently increases the detection accuracy by about 4% compared to other combinations in all 17 projects.","Furthermore, we observe the limitation of transferring vulnerability signatures across domains based on our experiments."],"url":"http://arxiv.org/abs/2406.03577v1","category":"cs.SE"}
{"created":"2024-06-05 18:45:45","title":"Enhancing Traffic Sign Recognition with Tailored Data Augmentation: Addressing Class Imbalance and Instance Scarcity","abstract":"This paper tackles critical challenges in traffic sign recognition (TSR), which is essential for road safety -- specifically, class imbalance and instance scarcity in datasets. We introduce tailored data augmentation techniques, including synthetic image generation, geometric transformations, and a novel obstacle-based augmentation method to enhance dataset quality for improved model robustness and accuracy. Our methodology incorporates diverse augmentation processes to accurately simulate real-world conditions, thereby expanding the training data's variety and representativeness. Our findings demonstrate substantial improvements in TSR models performance, offering significant implications for traffic sign recognition systems. This research not only addresses dataset limitations in TSR but also proposes a model for similar challenges across different regions and applications, marking a step forward in the field of computer vision and traffic sign recognition systems.","sentences":["This paper tackles critical challenges in traffic sign recognition (TSR), which is essential for road safety -- specifically, class imbalance and instance scarcity in datasets.","We introduce tailored data augmentation techniques, including synthetic image generation, geometric transformations, and a novel obstacle-based augmentation method to enhance dataset quality for improved model robustness and accuracy.","Our methodology incorporates diverse augmentation processes to accurately simulate real-world conditions, thereby expanding the training data's variety and representativeness.","Our findings demonstrate substantial improvements in TSR models performance, offering significant implications for traffic sign recognition systems.","This research not only addresses dataset limitations in TSR but also proposes a model for similar challenges across different regions and applications, marking a step forward in the field of computer vision and traffic sign recognition systems."],"url":"http://arxiv.org/abs/2406.03576v1","category":"cs.CV"}
{"created":"2024-06-05 18:00:09","title":"Robust Communication and Computation using Deep Learning via Joint Uncertainty Injection","abstract":"The convergence of communication and computation, along with the integration of machine learning and artificial intelligence, stand as key empowering pillars for the sixth-generation of communication systems (6G). This paper considers a network of one base station serving a number of devices simultaneously using spatial multiplexing. The paper then presents an innovative deep learning-based approach to simultaneously manage the transmit and computing powers, alongside computation allocation, amidst uncertainties in both channel and computing states information. More specifically, the paper aims at proposing a robust solution that minimizes the worst-case delay across the served devices subject to computation and power constraints. The paper uses a deep neural network (DNN)-based solution that maps estimated channels and computation requirements to optimized resource allocations. During training, uncertainty samples are injected after the DNN output to jointly account for both communication and computation estimation errors. The DNN is then trained via backpropagation using the robust utility, thus implicitly learning the uncertainty distributions. Our results validate the enhanced robust delay performance of the joint uncertainty injection versus the classical DNN approach, especially in high channel and computational uncertainty regimes.","sentences":["The convergence of communication and computation, along with the integration of machine learning and artificial intelligence, stand as key empowering pillars for the sixth-generation of communication systems (6G).","This paper considers a network of one base station serving a number of devices simultaneously using spatial multiplexing.","The paper then presents an innovative deep learning-based approach to simultaneously manage the transmit and computing powers, alongside computation allocation, amidst uncertainties in both channel and computing states information.","More specifically, the paper aims at proposing a robust solution that minimizes the worst-case delay across the served devices subject to computation and power constraints.","The paper uses a deep neural network (DNN)-based solution that maps estimated channels and computation requirements to optimized resource allocations.","During training, uncertainty samples are injected after the DNN output to jointly account for both communication and computation estimation errors.","The DNN is then trained via backpropagation using the robust utility, thus implicitly learning the uncertainty distributions.","Our results validate the enhanced robust delay performance of the joint uncertainty injection versus the classical DNN approach, especially in high channel and computational uncertainty regimes."],"url":"http://arxiv.org/abs/2406.03548v1","category":"cs.IT"}
{"created":"2024-06-05 18:00:02","title":"A Geometric View of Data Complexity: Efficient Local Intrinsic Dimension Estimation with Diffusion Models","abstract":"High-dimensional data commonly lies on low-dimensional submanifolds, and estimating the local intrinsic dimension (LID) of a datum -- i.e. the dimension of the submanifold it belongs to -- is a longstanding problem. LID can be understood as the number of local factors of variation: the more factors of variation a datum has, the more complex it tends to be. Estimating this quantity has proven useful in contexts ranging from generalization in neural networks to detection of out-of-distribution data, adversarial examples, and AI-generated text. The recent successes of deep generative models present an opportunity to leverage them for LID estimation, but current methods based on generative models produce inaccurate estimates, require more than a single pre-trained model, are computationally intensive, or do not exploit the best available deep generative models, i.e. diffusion models (DMs). In this work, we show that the Fokker-Planck equation associated with a DM can provide a LID estimator which addresses all the aforementioned deficiencies. Our estimator, called FLIPD, is compatible with all popular DMs, and outperforms existing baselines on LID estimation benchmarks. We also apply FLIPD on natural images where the true LID is unknown. Compared to competing estimators, FLIPD exhibits a higher correlation with non-LID measures of complexity, better matches a qualitative assessment of complexity, and is the only estimator to remain tractable with high-resolution images at the scale of Stable Diffusion.","sentences":["High-dimensional data commonly lies on low-dimensional submanifolds, and estimating the local intrinsic dimension (LID) of a datum -- i.e. the dimension of the submanifold it belongs to -- is a longstanding problem.","LID can be understood as the number of local factors of variation: the more factors of variation a datum has, the more complex it tends to be.","Estimating this quantity has proven useful in contexts ranging from generalization in neural networks to detection of out-of-distribution data, adversarial examples, and AI-generated text.","The recent successes of deep generative models present an opportunity to leverage them for LID estimation, but current methods based on generative models produce inaccurate estimates, require more than a single pre-trained model, are computationally intensive, or do not exploit the best available deep generative models, i.e. diffusion models (DMs).","In this work, we show that the Fokker-Planck equation associated with a DM can provide a LID estimator which addresses all the aforementioned deficiencies.","Our estimator, called FLIPD, is compatible with all popular DMs, and outperforms existing baselines on LID estimation benchmarks.","We also apply FLIPD on natural images where the true LID is unknown.","Compared to competing estimators, FLIPD exhibits a higher correlation with non-LID measures of complexity, better matches a qualitative assessment of complexity, and is the only estimator to remain tractable with high-resolution images at the scale of Stable Diffusion."],"url":"http://arxiv.org/abs/2406.03537v1","category":"cs.LG"}
{"created":"2024-06-05 17:53:55","title":"VideoPhy: Evaluating Physical Commonsense for Video Generation","abstract":"Recent advances in internet-scale video data pretraining have led to the development of text-to-video generative models that can create high-quality videos across a broad range of visual concepts and styles. Due to their ability to synthesize realistic motions and render complex objects, these generative models have the potential to become general-purpose simulators of the physical world. However, it is unclear how far we are from this goal with the existing text-to-video generative models. To this end, we present VideoPhy, a benchmark designed to assess whether the generated videos follow physical commonsense for real-world activities (e.g. marbles will roll down when placed on a slanted surface). Specifically, we curate a list of 688 captions that involve interactions between various material types in the physical world (e.g., solid-solid, solid-fluid, fluid-fluid). We then generate videos conditioned on these captions from diverse state-of-the-art text-to-video generative models, including open models (e.g., VideoCrafter2) and closed models (e.g., Lumiere from Google, Pika). Further, our human evaluation reveals that the existing models severely lack the ability to generate videos adhering to the given text prompts, while also lack physical commonsense. Specifically, the best performing model, Pika, generates videos that adhere to the caption and physical laws for only 19.7% of the instances. VideoPhy thus highlights that the video generative models are far from accurately simulating the physical world. Finally, we also supplement the dataset with an auto-evaluator, VideoCon-Physics, to assess semantic adherence and physical commonsense at scale.","sentences":["Recent advances in internet-scale video data pretraining have led to the development of text-to-video generative models that can create high-quality videos across a broad range of visual concepts and styles.","Due to their ability to synthesize realistic motions and render complex objects, these generative models have the potential to become general-purpose simulators of the physical world.","However, it is unclear how far we are from this goal with the existing text-to-video generative models.","To this end, we present VideoPhy, a benchmark designed to assess whether the generated videos follow physical commonsense for real-world activities (e.g. marbles will roll down when placed on a slanted surface).","Specifically, we curate a list of 688 captions that involve interactions between various material types in the physical world (e.g., solid-solid, solid-fluid, fluid-fluid).","We then generate videos conditioned on these captions from diverse state-of-the-art text-to-video generative models, including open models (e.g., VideoCrafter2) and closed models (e.g., Lumiere from Google, Pika).","Further, our human evaluation reveals that the existing models severely lack the ability to generate videos adhering to the given text prompts, while also lack physical commonsense.","Specifically, the best performing model, Pika, generates videos that adhere to the caption and physical laws for only 19.7% of the instances.","VideoPhy thus highlights that the video generative models are far from accurately simulating the physical world.","Finally, we also supplement the dataset with an auto-evaluator, VideoCon-Physics, to assess semantic adherence and physical commonsense at scale."],"url":"http://arxiv.org/abs/2406.03520v1","category":"cs.CV"}
{"created":"2024-06-05 16:39:32","title":"Buffered Asynchronous Secure Aggregation for Cross-Device Federated Learning","abstract":"Asynchronous federated learning (AFL) is an effective method to address the challenge of device heterogeneity in cross-device federated learning. However, AFL is usually incompatible with existing secure aggregation protocols used to protect user privacy in federated learning because most existing secure aggregation protocols are based on synchronous aggregation. To address this problem, we propose a novel secure aggregation protocol named buffered asynchronous secure aggregation (BASA) in this paper. Compared with existing protocols, BASA is fully compatible with AFL and provides secure aggregation under the condition that each user only needs one round of communication with the server without relying on any synchronous interaction among users. Based on BASA, we propose the first AFL method which achieves secure aggregation without extra requirements on hardware. We empirically demonstrate that BASA outperforms existing secure aggregation protocols for cross-device federated learning in terms of training efficiency and scalability.","sentences":["Asynchronous federated learning (AFL) is an effective method to address the challenge of device heterogeneity in cross-device federated learning.","However, AFL is usually incompatible with existing secure aggregation protocols used to protect user privacy in federated learning because most existing secure aggregation protocols are based on synchronous aggregation.","To address this problem, we propose a novel secure aggregation protocol named buffered asynchronous secure aggregation (BASA) in this paper.","Compared with existing protocols, BASA is fully compatible with AFL and provides secure aggregation under the condition that each user only needs one round of communication with the server without relying on any synchronous interaction among users.","Based on BASA, we propose the first AFL method which achieves secure aggregation without extra requirements on hardware.","We empirically demonstrate that BASA outperforms existing secure aggregation protocols for cross-device federated learning in terms of training efficiency and scalability."],"url":"http://arxiv.org/abs/2406.03516v1","category":"cs.CR"}
{"created":"2024-06-06 17:45:00","title":"NoisyGL: A Comprehensive Benchmark for Graph Neural Networks under Label Noise","abstract":"Graph Neural Networks (GNNs) exhibit strong potential in node classification task through a message-passing mechanism. However, their performance often hinges on high-quality node labels, which are challenging to obtain in real-world scenarios due to unreliable sources or adversarial attacks. Consequently, label noise is common in real-world graph data, negatively impacting GNNs by propagating incorrect information during training. To address this issue, the study of Graph Neural Networks under Label Noise (GLN) has recently gained traction. However, due to variations in dataset selection, data splitting, and preprocessing techniques, the community currently lacks a comprehensive benchmark, which impedes deeper understanding and further development of GLN. To fill this gap, we introduce NoisyGL in this paper, the first comprehensive benchmark for graph neural networks under label noise. NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy labeled graph data across various datasets, with unified experimental settings and interface. Our benchmark has uncovered several important insights that were missed in previous research, and we believe these findings will be highly beneficial for future studies. We hope our open-source benchmark library will foster further advancements in this field. The code of the benchmark can be found in https://github.com/eaglelab-zju/NoisyGL.","sentences":["Graph Neural Networks (GNNs) exhibit strong potential in node classification task through a message-passing mechanism.","However, their performance often hinges on high-quality node labels, which are challenging to obtain in real-world scenarios due to unreliable sources or adversarial attacks.","Consequently, label noise is common in real-world graph data, negatively impacting GNNs by propagating incorrect information during training.","To address this issue, the study of Graph Neural Networks under Label Noise (GLN) has recently gained traction.","However, due to variations in dataset selection, data splitting, and preprocessing techniques, the community currently lacks a comprehensive benchmark, which impedes deeper understanding and further development of GLN.","To fill this gap, we introduce NoisyGL in this paper, the first comprehensive benchmark for graph neural networks under label noise.","NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy labeled graph data across various datasets, with unified experimental settings and interface.","Our benchmark has uncovered several important insights that were missed in previous research, and we believe these findings will be highly beneficial for future studies.","We hope our open-source benchmark library will foster further advancements in this field.","The code of the benchmark can be found in https://github.com/eaglelab-zju/NoisyGL."],"url":"http://arxiv.org/abs/2406.04299v1","category":"cs.LG"}
{"created":"2024-06-06 17:37:39","title":"Stratified Prediction-Powered Inference for Hybrid Language Model Evaluation","abstract":"Prediction-powered inference (PPI) is a method that improves statistical estimates based on limited human-labeled data. PPI achieves this by combining small amounts of human-labeled data with larger amounts of data labeled by a reasonably accurate -- but potentially biased -- automatic system, in a way that results in tighter confidence intervals for certain parameters of interest (e.g., the mean performance of a language model). In this paper, we propose a method called Stratified Prediction-Powered Inference (StratPPI), in which we show that the basic PPI estimates can be considerably improved by employing simple data stratification strategies. Without making any assumptions on the underlying automatic labeling system or data distribution, we derive an algorithm for computing provably valid confidence intervals for population parameters (such as averages) that is based on stratified sampling. In particular, we show both theoretically and empirically that, with appropriate choices of stratification and sample allocation, our approach can provide substantially tighter confidence intervals than unstratified approaches. Specifically, StratPPI is expected to improve in cases where the performance of the autorater varies across different conditional distributions of the target data.","sentences":["Prediction-powered inference (PPI) is a method that improves statistical estimates based on limited human-labeled data.","PPI achieves this by combining small amounts of human-labeled data with larger amounts of data labeled by a reasonably accurate -- but potentially biased -- automatic system, in a way that results in tighter confidence intervals for certain parameters of interest (e.g., the mean performance of a language model).","In this paper, we propose a method called Stratified Prediction-Powered Inference (StratPPI), in which we show that the basic PPI estimates can be considerably improved by employing simple data stratification strategies.","Without making any assumptions on the underlying automatic labeling system or data distribution, we derive an algorithm for computing provably valid confidence intervals for population parameters (such as averages) that is based on stratified sampling.","In particular, we show both theoretically and empirically that, with appropriate choices of stratification and sample allocation, our approach can provide substantially tighter confidence intervals than unstratified approaches.","Specifically, StratPPI is expected to improve in cases where the performance of the autorater varies across different conditional distributions of the target data."],"url":"http://arxiv.org/abs/2406.04291v1","category":"cs.LG"}
{"created":"2024-06-06 17:34:24","title":"What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages","abstract":"What can large language models learn? By definition, language models (LM) are distributions over strings. Therefore, an intuitive way of addressing the above question is to formalize it as a matter of learnability of classes of distributions over strings. While prior work in this direction focused on assessing the theoretical limits, in contrast, we seek to understand the empirical learnability. Unlike prior empirical work, we evaluate neural LMs on their home turf-learning probabilistic languages-rather than as classifiers of formal languages. In particular, we investigate the learnability of regular LMs (RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs as a function of various complexity parameters of the RLM and the hidden state size of the neural LM. We find that the RLM rank, which corresponds to the size of linear space spanned by the logits of its conditional distributions, and the expected length of sampled strings are strong and significant predictors of learnability for both RNNs and Transformers. Several other predictors also reach significance, but with differing patterns between RNNs and Transformers.","sentences":["What can large language models learn?","By definition, language models (LM) are distributions over strings.","Therefore, an intuitive way of addressing the above question is to formalize it as a matter of learnability of classes of distributions over strings.","While prior work in this direction focused on assessing the theoretical limits, in contrast, we seek to understand the empirical learnability.","Unlike prior empirical work, we evaluate neural LMs on their home turf-learning probabilistic languages-rather than as classifiers of formal languages.","In particular, we investigate the learnability of regular LMs (RLMs) by RNN and Transformer LMs.","We empirically test the learnability of RLMs as a function of various complexity parameters of the RLM and the hidden state size of the neural LM.","We find that the RLM rank, which corresponds to the size of linear space spanned by the logits of its conditional distributions, and the expected length of sampled strings are strong and significant predictors of learnability for both RNNs and Transformers.","Several other predictors also reach significance, but with differing patterns between RNNs and Transformers."],"url":"http://arxiv.org/abs/2406.04289v1","category":"cs.CL"}
{"created":"2024-06-06 17:33:31","title":"Trials and Tribulations in the Reanalysis of KELT-24 b: a Case Study for the Importance of Stellar Modeling","abstract":"We present a new analysis of the KELT-24 system, comprising a well-aligned hot Jupiter, KELT-24~b, and a bright ($V=8.3$), nearby ($d=96.9~\\mathrm{pc}$) F-type host star. KELT-24~b was independently discovered by two groups in 2019, with each reporting best-fit stellar parameters that were notably inconsistent. Here, we present three independent analyses of the KELT-24 system, each incorporating a broad range of photometric and spectroscopic data, including eight sectors of TESS photometry and more than 200 new radial velocities (RVs) from MINERVA. Two of these analyses use KELT-24's observed spectral energy distribution (SED) through a direct comparison to stellar evolutionary models, while our third analysis assumes an unknown additional body contributing to the observed broadband photometry and excludes the SED. Ultimately, we find that the models that include the SED are a poor fit to the available data, so we adopt the system parameters derived without it. We also highlight a single transit-like event observed by TESS, deemed likely to be an eclipsing binary bound to KELT-24, that will require follow-up observations to confirm. We discuss the potential of these additional bodies in the KELT-24 system as a possible explanation for the discrepancies between the results of the different modeling approaches, and explore the system for longer-period planets that may be weakly evident in the RV observations. The comprehensive investigations that we present not only increase the fidelity of our understanding of the KELT-24 system, but also serve as a blueprint for future stellar modeling in global analyses of exoplanet systems.","sentences":["We present a new analysis of the KELT-24 system, comprising a well-aligned hot Jupiter, KELT-24~b, and a bright ($V=8.3$), nearby ($d=96.9~\\mathrm{pc}$) F-type host star.","KELT-24~b was independently discovered by two groups in 2019, with each reporting best-fit stellar parameters that were notably inconsistent.","Here, we present three independent analyses of the KELT-24 system, each incorporating a broad range of photometric and spectroscopic data, including eight sectors of TESS photometry and more than 200 new radial velocities (RVs) from MINERVA.","Two of these analyses use KELT-24's observed spectral energy distribution (SED) through a direct comparison to stellar evolutionary models, while our third analysis assumes an unknown additional body contributing to the observed broadband photometry and excludes the SED.","Ultimately, we find that the models that include the SED are a poor fit to the available data, so we adopt the system parameters derived without it.","We also highlight a single transit-like event observed by TESS, deemed likely to be an eclipsing binary bound to KELT-24, that will require follow-up observations to confirm.","We discuss the potential of these additional bodies in the KELT-24 system as a possible explanation for the discrepancies between the results of the different modeling approaches, and explore the system for longer-period planets that may be weakly evident in the RV observations.","The comprehensive investigations that we present not only increase the fidelity of our understanding of the KELT-24 system, but also serve as a blueprint for future stellar modeling in global analyses of exoplanet systems."],"url":"http://arxiv.org/abs/2406.04288v1","category":"astro-ph.EP"}
{"created":"2024-06-06 17:26:11","title":"Discovering reduced order model equations of many-body quantum systems using genetic programming: a technical report","abstract":"In this technical report we present first results of applying Genetic Programming (GP) to obtain equations for constructing reduced order models of quantum systems, with a particular interest in nuclear Density Functional Theory. We employ the reduced basis method to obtain reduced coordinates as the amplitudes of the reduced basis, and use GP to avoid the need of constructing the reduced equations through, for example, a Galerkin projection. The reduced order models constructed through GP show excellent accuracy and speed performance, including extrapolations in the controlling parameters, and show promise as an effective method for emulating computationally demanding calculations in nuclear physics.","sentences":["In this technical report we present first results of applying Genetic Programming (GP) to obtain equations for constructing reduced order models of quantum systems, with a particular interest in nuclear Density Functional Theory.","We employ the reduced basis method to obtain reduced coordinates as the amplitudes of the reduced basis, and use GP to avoid the need of constructing the reduced equations through, for example, a Galerkin projection.","The reduced order models constructed through GP show excellent accuracy and speed performance, including extrapolations in the controlling parameters, and show promise as an effective method for emulating computationally demanding calculations in nuclear physics."],"url":"http://arxiv.org/abs/2406.04279v1","category":"nucl-th"}
{"created":"2024-06-06 17:12:25","title":"Apsidal Precession in Binary Asteroids","abstract":"While the secondary in a binary asteroid plays an important role in the precession of the mutual orbit, this role has not been thoroughly studied. Given the complex spin-orbit coupled dynamics in binary asteroids, we use a numerical approach to study the relationship between the secondary's shape and spin and the apsidal precession rate of the orbit. Using this approach in conjunction with observations of Didymos, we find it is likely that Dimorphos was significantly reshaped as a result of the DART impact, with its new shape more elongated than the pre-impact shape. Finally, we show that non-principal axis rotation of the secondary can lead to a chaotic evolution of the longitude of the periapsis.","sentences":["While the secondary in a binary asteroid plays an important role in the precession of the mutual orbit, this role has not been thoroughly studied.","Given the complex spin-orbit coupled dynamics in binary asteroids, we use a numerical approach to study the relationship between the secondary's shape and spin and the apsidal precession rate of the orbit.","Using this approach in conjunction with observations of Didymos, we find it is likely that Dimorphos was significantly reshaped as a result of the DART impact, with its new shape more elongated than the pre-impact shape.","Finally, we show that non-principal axis rotation of the secondary can lead to a chaotic evolution of the longitude of the periapsis."],"url":"http://arxiv.org/abs/2406.04265v1","category":"astro-ph.EP"}
{"created":"2024-06-06 17:03:50","title":"Gradient Boosting for Hierarchical Data in Small Area Estimation","abstract":"This paper introduces Mixed Effect Gradient Boosting (MEGB), which combines the strengths of Gradient Boosting with Mixed Effects models to address complex, hierarchical data structures often encountered in statistical analysis. The methodological foundations, including a review of the Mixed Effects model and the Extreme Gradient Boosting method, leading to the introduction of MEGB are shown in detail. It highlights how MEGB can derive area-level mean estimations from unit-level data and calculate Mean Squared Error (MSE) estimates using a nonparametric bootstrap approach. The paper evaluates MEGB's performance through model-based and design-based simulation studies, comparing it against established estimators. The findings indicate that MEGB provides promising area mean estimations and may outperform existing small area estimators in various scenarios. The paper concludes with a discussion on future research directions, highlighting the possibility of extending MEGB's framework to accommodate different types of outcome variables or non-linear area level indicators.","sentences":["This paper introduces Mixed Effect Gradient Boosting (MEGB), which combines the strengths of Gradient Boosting with Mixed Effects models to address complex, hierarchical data structures often encountered in statistical analysis.","The methodological foundations, including a review of the Mixed Effects model and the Extreme Gradient Boosting method, leading to the introduction of MEGB are shown in detail.","It highlights how MEGB can derive area-level mean estimations from unit-level data and calculate Mean Squared Error (MSE) estimates using a nonparametric bootstrap approach.","The paper evaluates MEGB's performance through model-based and design-based simulation studies, comparing it against established estimators.","The findings indicate that MEGB provides promising area mean estimations and may outperform existing small area estimators in various scenarios.","The paper concludes with a discussion on future research directions, highlighting the possibility of extending MEGB's framework to accommodate different types of outcome variables or non-linear area level indicators."],"url":"http://arxiv.org/abs/2406.04256v1","category":"stat.ME"}
{"created":"2024-06-06 16:47:11","title":"Complementary polynomials in quantum signal processing","abstract":"Quantum signal processing is a framework for implementing polynomial functions on quantum computers. To implement a given polynomial $P$, one must first construct a corresponding complementary polynomial $Q$. Existing approaches to this problem employ numerical methods that are not amenable to explicit error analysis. We present a new approach to complementary polynomials using complex analysis. Our main mathematical result is a contour integral representation for a canonical complementary polynomial. The integral representation on the unit circle has a particularly simple and efficacious Fourier analytic interpretation, which we use to develop a Fast Fourier Transform-based algorithm for the efficient calculation of $Q$ in the monomial basis with explicit error guarantees. Numerical evidence that our algorithm outperforms the state-of-the-art optimization-based method for computing complementary polynomials is provided.","sentences":["Quantum signal processing is a framework for implementing polynomial functions on quantum computers.","To implement a given polynomial $P$, one must first construct a corresponding complementary polynomial $Q$. Existing approaches to this problem employ numerical methods that are not amenable to explicit error analysis.","We present a new approach to complementary polynomials using complex analysis.","Our main mathematical result is a contour integral representation for a canonical complementary polynomial.","The integral representation on the unit circle has a particularly simple and efficacious Fourier analytic interpretation, which we use to develop a Fast Fourier Transform-based algorithm for the efficient calculation of $Q$ in the monomial basis with explicit error guarantees.","Numerical evidence that our algorithm outperforms the state-of-the-art optimization-based method for computing complementary polynomials is provided."],"url":"http://arxiv.org/abs/2406.04246v1","category":"quant-ph"}
{"created":"2024-06-06 16:41:39","title":"Benchmark Data Contamination of Large Language Models: A Survey","abstract":"The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing. However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC). This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process. This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks. The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications.","sentences":["The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3, and Gemini has transformed the field of natural language processing.","However, it has also resulted in a significant issue known as Benchmark Data Contamination (BDC).","This occurs when language models inadvertently incorporate evaluation benchmark information from their training data, leading to inaccurate or unreliable performance during the evaluation phase of the process.","This paper reviews the complex challenge of BDC in LLM evaluation and explores alternative assessment methods to mitigate the risks associated with traditional benchmarks.","The paper also examines challenges and future directions in mitigating BDC risks, highlighting the complexity of the issue and the need for innovative solutions to ensure the reliability of LLM evaluation in real-world applications."],"url":"http://arxiv.org/abs/2406.04244v1","category":"cs.CL"}
{"created":"2024-06-06 16:41:10","title":"Policy Optimization in Control: Geometry and Algorithmic Implications","abstract":"This survey explores the geometric perspective on policy optimization within the realm of feedback control systems, emphasizing the intrinsic relationship between control design and optimization. By adopting a geometric viewpoint, we aim to provide a nuanced understanding of how various ``complete parameterization'' -- referring to the policy parameters together with its Riemannian geometry -- of control design problems, influence stability and performance of local search algorithms. The paper is structured to address key themes such as policy parameterization, the topology and geometry of stabilizing policies, and their implications for various (non-convex) dynamic performance measures. We focus on a few iconic control design problems, including the Linear Quadratic Regulator (LQR), Linear Quadratic Gaussian (LQG) control, and $\\mathcal{H}_\\infty$ control. In particular, we first discuss the topology and Riemannian geometry of stabilizing policies, distinguishing between their static and dynamic realizations. Expanding on this geometric perspective, we then explore structural properties of the aforementioned performance measures and their interplay with the geometry of stabilizing policies in presence of policy constraints; along the way, we address issues such as spurious stationary points, symmetries of dynamic feedback policies, and (non-)smoothness of the corresponding performance measures. We conclude the survey with algorithmic implications of policy optimization in feedback design.","sentences":["This survey explores the geometric perspective on policy optimization within the realm of feedback control systems, emphasizing the intrinsic relationship between control design and optimization.","By adopting a geometric viewpoint, we aim to provide a nuanced understanding of how various ``complete parameterization'' -- referring to the policy parameters together with its Riemannian geometry -- of control design problems, influence stability and performance of local search algorithms.","The paper is structured to address key themes such as policy parameterization, the topology and geometry of stabilizing policies, and their implications for various (non-convex) dynamic performance measures.","We focus on a few iconic control design problems, including the Linear Quadratic Regulator (LQR), Linear Quadratic Gaussian (LQG) control, and $\\mathcal{H}_\\infty$ control.","In particular, we first discuss the topology and Riemannian geometry of stabilizing policies, distinguishing between their static and dynamic realizations.","Expanding on this geometric perspective, we then explore structural properties of the aforementioned performance measures and their interplay with the geometry of stabilizing policies in presence of policy constraints; along the way, we address issues such as spurious stationary points, symmetries of dynamic feedback policies, and (non-)smoothness of the corresponding performance measures.","We conclude the survey with algorithmic implications of policy optimization in feedback design."],"url":"http://arxiv.org/abs/2406.04243v1","category":"math.OC"}
{"created":"2024-06-06 16:31:32","title":"Multidimensional Stability of Planar Travelling Waves for Stochastically Perturbed Reaction-Diffusion Systems","abstract":"We consider reaction-diffusion systems with multiplicative noise on a spatial domain of dimension two or higher. The noise process is white in time, coloured in space, and invariant under translations. In the deterministic setting, multidimensional stability of planar waves on the whole space $\\mathbb R^d$ has been studied by many. Inspired by previous works on the real line, we establish the multidimensional stability of planar waves on a cylindrical domain on time scales that are exponentially long with respect to the noise strength. This is achieved by means of a stochastic phase tracking mechanism that can be maintained over such long time scales. The corresponding mild formulation of our problem features stochastic integrals with respect to anticipating integrands, which hence cannot be understood within the well-established setting of It\\^o-integrals. To circumvent this problem, we exploit and extend recently developed theory concerning forward integrals.","sentences":["We consider reaction-diffusion systems with multiplicative noise on a spatial domain of dimension two or higher.","The noise process is white in time, coloured in space, and invariant under translations.","In the deterministic setting, multidimensional stability of planar waves on the whole space $\\mathbb R^d$ has been studied by many.","Inspired by previous works on the real line, we establish the multidimensional stability of planar waves on a cylindrical domain on time scales that are exponentially long with respect to the noise strength.","This is achieved by means of a stochastic phase tracking mechanism that can be maintained over such long time scales.","The corresponding mild formulation of our problem features stochastic integrals with respect to anticipating integrands, which hence cannot be understood within the well-established setting of It\\^o-integrals.","To circumvent this problem, we exploit and extend recently developed theory concerning forward integrals."],"url":"http://arxiv.org/abs/2406.04232v1","category":"math.AP"}
{"created":"2024-06-06 16:28:56","title":"Early-time resonances in the three-dimensional wall-bounded axisymmetric Euler and related equations","abstract":"We investigate the complex-time analytic structure of solutions of the 3D-axisymmetric, wall-bounded, incompressible Euler equations, by starting with the initial data proposed in Luo and Hou (2014), to study a possible finite-time singularity. We use our pseudospectral Fourier-Chebyshev method, with quadruple-precision arithmetic, to compute the time-Taylor series coefficients of the flow fields, up to a high order. We show that the resulting approximations display early-time resonances; the initial spatial location of these structures is different from that for the tygers, which we have obtained in Kolluru et al. (2022). We then perform asymptotic analysis of the Taylor-series coefficients, by using generalised ratio methods, to extract the location and nature of the convergence-limiting singularities and demonstrate that these singularities are distributed around the origin, in the complex-t2 plane, along two curves that resemble the shape of an eye. We obtain similar results for the 1D wall-approximation (of the full 3D-axisymmetric Euler equation) called the 1D HL model, for which we use Fourier-pseudospectral methods to compute the time-Taylor series coefficients of the flow fields. Our work examines the link between tygers, in Galerkin-truncated pseudospectral studies, and early-time resonances, in truncated time-Taylor expansions of solutions of PDEs, such as those we consider.","sentences":["We investigate the complex-time analytic structure of solutions of the 3D-axisymmetric, wall-bounded, incompressible Euler equations, by starting with the initial data proposed in Luo and Hou (2014), to study a possible finite-time singularity.","We use our pseudospectral Fourier-Chebyshev method, with quadruple-precision arithmetic, to compute the time-Taylor series coefficients of the flow fields, up to a high order.","We show that the resulting approximations display early-time resonances; the initial spatial location of these structures is different from that for the tygers, which we have obtained in Kolluru et al. (2022).","We then perform asymptotic analysis of the Taylor-series coefficients, by using generalised ratio methods, to extract the location and nature of the convergence-limiting singularities and demonstrate that these singularities are distributed around the origin, in the complex-t2 plane, along two curves that resemble the shape of an eye.","We obtain similar results for the 1D wall-approximation (of the full 3D-axisymmetric Euler equation) called the 1D HL model, for which we use Fourier-pseudospectral methods to compute the time-Taylor series coefficients of the flow fields.","Our work examines the link between tygers, in Galerkin-truncated pseudospectral studies, and early-time resonances, in truncated time-Taylor expansions of solutions of PDEs, such as those we consider."],"url":"http://arxiv.org/abs/2406.04228v1","category":"physics.flu-dyn"}
{"created":"2024-06-06 16:17:50","title":"Optomechanical Backaction in the Bistable Regime","abstract":"With a variety of realisations, optomechanics utilizes its light matter interaction to test fundamental physics. By coupling the phonons of a mechanical resonator to the photons in a high quality cavity, control of increasingly macroscopic objects has become feasible. In such systems, state manipulation of the mechanical mode is achieved by driving the cavity. To be able to achieve high drive powers the system is typically designed such that it remains in a linear response regime when driven. A nonlinear response and especially bistability in a driven cavity is often considered detrimentally to cooling and state preparation in optomechanical systems and is avoided in experiments. Here we show, that with an intrinsic nonlinear cavity backaction cooling of a mechanical resonator is feasible operating deeply within the nonlinear regime of the cavity. With our theory taking the nonlinearity into account, precise predictions on backaction cooling can be achieved even with a cavity beyond the bifurcation point, where the cavity photon number spectrum starts to deviate from a typical Lorentzian shape.","sentences":["With a variety of realisations, optomechanics utilizes its light matter interaction to test fundamental physics.","By coupling the phonons of a mechanical resonator to the photons in a high quality cavity, control of increasingly macroscopic objects has become feasible.","In such systems, state manipulation of the mechanical mode is achieved by driving the cavity.","To be able to achieve high drive powers the system is typically designed such that it remains in a linear response regime when driven.","A nonlinear response and especially bistability in a driven cavity is often considered detrimentally to cooling and state preparation in optomechanical systems and is avoided in experiments.","Here we show, that with an intrinsic nonlinear cavity backaction cooling of a mechanical resonator is feasible operating deeply within the nonlinear regime of the cavity.","With our theory taking the nonlinearity into account, precise predictions on backaction cooling can be achieved even with a cavity beyond the bifurcation point, where the cavity photon number spectrum starts to deviate from a typical Lorentzian shape."],"url":"http://arxiv.org/abs/2406.04217v1","category":"quant-ph"}
{"created":"2024-06-06 16:11:39","title":"Sound Event Bounding Boxes","abstract":"Sound event detection is the task of recognizing sounds and determining their extent (onset/offset times) within an audio clip. Existing systems commonly predict sound presence confidence in short time frames. Then, thresholding produces binary frame-level presence decisions, with the extent of individual events determined by merging consecutive positive frames. In this paper, we show that frame-level thresholding degrades the prediction of the event extent by coupling it with the system's sound presence confidence. We propose to decouple the prediction of event extent and confidence by introducing SEBBs, which format each sound event prediction as a tuple of a class type, extent, and overall confidence. We also propose a change-detection-based algorithm to convert legacy frame-level outputs into SEBBs. We find the algorithm significantly improves the performance of DCASE 2023 Challenge systems, boosting the state of the art from .644 to .686 PSDS1.","sentences":["Sound event detection is the task of recognizing sounds and determining their extent (onset/offset times) within an audio clip.","Existing systems commonly predict sound presence confidence in short time frames.","Then, thresholding produces binary frame-level presence decisions, with the extent of individual events determined by merging consecutive positive frames.","In this paper, we show that frame-level thresholding degrades the prediction of the event extent by coupling it with the system's sound presence confidence.","We propose to decouple the prediction of event extent and confidence by introducing SEBBs, which format each sound event prediction as a tuple of a class type, extent, and overall confidence.","We also propose a change-detection-based algorithm to convert legacy frame-level outputs into SEBBs.","We find the algorithm significantly improves the performance of DCASE 2023 Challenge systems, boosting the state of the art from .644 to .686 PSDS1."],"url":"http://arxiv.org/abs/2406.04212v1","category":"eess.AS"}
{"created":"2024-06-06 16:07:34","title":"Controlling wall particle interactions with activity","abstract":"We calculate the effective forces on hard disks near walls embedded inside active nematic liquid crystals. When the disks are sufficiently close to the wall and the flows are sufficiently slow, we can obtain exact expressions for the effective forces. We find these forces and the dynamics of disks near the wall depend both on the properties of the active nematic and on the anchoring conditions on the disks and the wall. Our results show that the presence of active stresses attract planar anchored disks to walls if the activity is extensile, and repel them if contractile. For normal anchored disks the reverse is true; they are attracted in contractile systems, and repelled in extensile ones.","sentences":["We calculate the effective forces on hard disks near walls embedded inside active nematic liquid crystals.","When the disks are sufficiently close to the wall and the flows are sufficiently slow, we can obtain exact expressions for the effective forces.","We find these forces and the dynamics of disks near the wall depend both on the properties of the active nematic and on the anchoring conditions on the disks and the wall.","Our results show that the presence of active stresses attract planar anchored disks to walls if the activity is extensile, and repel them if contractile.","For normal anchored disks the reverse is true; they are attracted in contractile systems, and repelled in extensile ones."],"url":"http://arxiv.org/abs/2406.04209v1","category":"cond-mat.soft"}
{"created":"2024-06-06 16:02:29","title":"Fundamental effective temperature measurements for eclipsing binary stars -- V. The circumbinary planet system EBLM J0608-59","abstract":"EBLM J0608-59 / TOI-1338 / BEBOP-1 is a 12th-magnitude, F9V star in an eclipsing binary with a much fainter M-dwarf companion on a wide, eccentric orbit (P=14.6 d). The binary is orbited by two circumbinary planets: one transiting on a 95-day orbit and one non-transiting on a 215-day orbit. We have used high-precision photometry from the TESS mission combined with direct mass measurements for the two stars published recently to measure the following model-independent radii: $R_1 = 1.32 \\pm 0.02 R_{\\odot}$, $R_2 = 0.309 \\pm 0.004 R_{\\odot}$. Using $R_1$ and the parallax from Gaia EDR3 we find that this star's angular diameter is $\\theta = 0.0309 \\pm 0.0005$ mas. The apparent bolometric flux of the primary star corrected for both extinction and the contribution from the M-dwarf ($<0.4$%) is ${\\mathcal F}_{\\oplus,0} = (0.417\\pm 0.005)\\times10^{-9} {\\rm \\,erg\\,cm}^{-2} {\\rm \\,s}^{-1}$. Hence, this F9V star has an effective temperature $T_{\\rm eff,1} = 6031{\\rm\\,K} \\pm 46{\\rm \\,K\\,(rnd.)} \\pm 10 {\\rm \\,K\\,(sys.)}$. EBLM J0608-59 is an ideal benchmark star that can be added to the sample of such systems we are establishing for \"end-to-end\" tests of the stellar parameters measured by large-scale spectroscopic surveys.","sentences":["EBLM J0608-59 / TOI-1338 / BEBOP-1 is a 12th-magnitude, F9V star in an eclipsing binary with a much fainter M-dwarf companion on a wide, eccentric orbit (P=14.6 d).","The binary is orbited by two circumbinary planets: one transiting on a 95-day orbit and one non-transiting on a 215-day orbit.","We have used high-precision photometry from the TESS mission combined with direct mass measurements for the two stars published recently to measure the following model-independent radii: $R_1 = 1.32 \\pm 0.02 R_{\\odot}$, $R_2 = 0.309 \\pm 0.004 R_{\\odot}$. Using $R_1$ and the parallax from Gaia EDR3 we find that this star's angular diameter is $\\theta = 0.0309 \\pm 0.0005$ mas.","The apparent bolometric flux of the primary star corrected for both extinction and the contribution from the M-dwarf ($<0.4$%) is ${\\mathcal F}_{\\oplus,0} = (0.417\\pm 0.005)\\times10^{-9} {\\rm \\,erg\\,cm}^{-2} {\\rm \\,s}^{-1}$. Hence, this F9V star has an effective temperature $T_{\\rm eff,1} = 6031{\\rm\\,K} \\pm 46{\\rm \\,K\\,(rnd.)","} \\pm 10 {\\rm \\,K\\,(sys.)}$. EBLM J0608-59 is an ideal benchmark star that can be added to the sample of such systems we are establishing for \"end-to-end\" tests of the stellar parameters measured by large-scale spectroscopic surveys."],"url":"http://arxiv.org/abs/2406.04204v1","category":"astro-ph.SR"}
{"created":"2024-06-06 16:01:54","title":"Explicit Steady-State Approximations for Parallel Server Systems with Heterogeneous Servers","abstract":"The weighted-workload-task-allocation (WWTA) load-balancing policy is known to be throughput optimal for parallel server systems with heterogeneous servers. This work concerns the heavy traffic approximation of steady-state performance for parallel server systems operating under WWTA policy. Under a relaxed complete-resource-pooling condition, we prove that WWTA achieves a \"strong form\" of state-space collapse in heavy traffic and that the scaled workload for each server converges in distribution to an exponential random variable, whose parameter is explicitly given by system primitives. Various steady-state performance measures are shown to be approximated from this exponential random variable. Instead of proving a stochastic process limit followed by an interchange of limits - a method that dominates the literature, our method works directly with a pre-limit basic adjoint relationship (BAR) that characterizes the stationary distribution of each pre-limit system.","sentences":["The weighted-workload-task-allocation (WWTA) load-balancing policy is known to be throughput optimal for parallel server systems with heterogeneous servers.","This work concerns the heavy traffic approximation of steady-state performance for parallel server systems operating under WWTA policy.","Under a relaxed complete-resource-pooling condition, we prove that WWTA achieves a \"strong form\" of state-space collapse in heavy traffic and that the scaled workload for each server converges in distribution to an exponential random variable, whose parameter is explicitly given by system primitives.","Various steady-state performance measures are shown to be approximated from this exponential random variable.","Instead of proving a stochastic process limit followed by an interchange of limits - a method that dominates the literature, our method works directly with a pre-limit basic adjoint relationship (BAR) that characterizes the stationary distribution of each pre-limit system."],"url":"http://arxiv.org/abs/2406.04203v1","category":"math.PR"}
{"created":"2024-06-06 15:58:21","title":"Flow-induced Oscillations via Hopf Bifurcation in a Fluid-Solid Interaction Problem","abstract":"We furnish necessary and sufficient conditions for the occurrence of a Hopf bifurcation in a particularly significant fluid-structure problem, where a Navier-Stokes liquid interacts with a rigid body that is subject to an undamped elastic restoring force. The motion of the coupled system is driven by a uniform flow at spatial infinity, with constant dimensionless velocity $\\lambda>0$. In particular, if the relevant linearized operator meets suitable spectral properties, there exists a threshold $\\lambda_o>0$ above which a bifurcating time-periodic branch stems out of the branch of steady-state solutions. The most remarkable feature of our result is that no restriction is imposed on the frequency $\\omega$ of the bifurcating solution, which may thus coincide with one of the natural structural frequencies $\\omega_{\\sf n}$ of the body. Therefore, resonance cannot occur as a result of this bifurcation. However, when $\\omega\\to\\omega_{\\sf n}$, the amplitude of oscillations may become very large when the fluid density is negligible compared to the mass of the body. To our knowledge, our result is the first {\\it rigorous} investigation of the existence of a Hopf bifurcation in a fluid-structure interaction problem.","sentences":["We furnish necessary and sufficient conditions for the occurrence of a Hopf bifurcation in a particularly significant fluid-structure problem, where a Navier-Stokes liquid interacts with a rigid body that is subject to an undamped elastic restoring force.","The motion of the coupled system is driven by a uniform flow at spatial infinity, with constant dimensionless velocity $\\lambda>0$. In particular, if the relevant linearized operator meets suitable spectral properties, there exists a threshold $\\lambda_o>0$ above which a bifurcating time-periodic branch stems out of the branch of steady-state solutions.","The most remarkable feature of our result is that no restriction is imposed on the frequency $\\omega$ of the bifurcating solution, which may thus coincide with one of the natural structural frequencies $\\omega_{\\sf n}$ of the body.","Therefore, resonance cannot occur as a result of this bifurcation.","However, when $\\omega\\to\\omega_{\\sf n}$, the amplitude of oscillations may become very large when the fluid density is negligible compared to the mass of the body.","To our knowledge, our result is the first {\\it rigorous} investigation of the existence of a Hopf bifurcation in a fluid-structure interaction problem."],"url":"http://arxiv.org/abs/2406.04198v1","category":"math.AP"}
{"created":"2024-06-06 15:54:06","title":"Unified Rapid Mass Transfer","abstract":"We present a method to obtain rapid mass loss rates in binary systems, specifically at the onset of MT episodes. The method unifies atmospheric (underflow) and $L_1$ stream (overflow) mass rates in a single continuous procedure. The method uses averaged 3D properties of the binaries, such as effective binary potential and effective binary acceleration, to both evolve the donor and obtain properties of the matter at the $L_1$ plane. In the case of underflow, we obtain atmospheric stratification. Our method can be used for binaries with an extensive range of mass ratios, $0.01 \\le q \\le 100$, and can also be applied to hot donors. The considered examples show that the MT rates obtained with this revised formalism always differ from the optically thin and optically thick MT rates widely used during the computations of binary evolution.","sentences":["We present a method to obtain rapid mass loss rates in binary systems, specifically at the onset of MT episodes.","The method unifies atmospheric (underflow) and $L_1$ stream (overflow) mass rates in a single continuous procedure.","The method uses averaged 3D properties of the binaries, such as effective binary potential and effective binary acceleration, to both evolve the donor and obtain properties of the matter at the $L_1","$ plane.","In the case of underflow, we obtain atmospheric stratification.","Our method can be used for binaries with an extensive range of mass ratios, $0.01 \\le q \\le 100$, and can also be applied to hot donors.","The considered examples show that the MT rates obtained with this revised formalism always differ from the optically thin and optically thick MT rates widely used during the computations of binary evolution."],"url":"http://arxiv.org/abs/2406.04195v1","category":"astro-ph.SR"}
{"created":"2024-06-06 15:46:35","title":"Probing quantum complexity via universal saturation of stabilizer entropies","abstract":"Nonstabilizerness or `magic' is a key resource for quantum computing and a necessary condition for quantum advantage. Non-Clifford operations turn stabilizer states into resourceful states, where the amount of nonstabilizerness is quantified by resource measures such as stabilizer R\\'enyi entropies (SREs). Here, we show that SREs saturate their maximum value at a critical number of non-Clifford operations. Close to the critical point SREs show universal behavior. Remarkably, the derivative of the SRE crosses at the same point independent of the number of qubits and can be rescaled onto a single curve. We find that the critical point depends non-trivially on R\\'enyi index $\\alpha$. For random Clifford circuits doped with T-gates, the critical T-gate density scales independently of $\\alpha$. In contrast, for random Hamiltonian evolution, the critical time scales linearly with qubit number for $\\alpha>1$, while is a constant for $\\alpha<1$. This highlights that $\\alpha$-SREs reveal fundamentally different aspects of nonstabilizerness depending on $\\alpha$: $\\alpha$-SREs with $\\alpha<1$ relate to Clifford simulation complexity, while $\\alpha>1$ probe the distance to the closest stabilizer state and approximate state certification cost via Pauli measurements. As technical contributions, we observe that the Pauli spectrum of random evolution can be approximated by two highly concentrated peaks which allows us to compute its SRE. Further, we introduce a class of random evolution that can be expressed as random Clifford circuits and rotations, where we provide its exact SRE. Our results opens up new approaches to characterize the complexity of quantum systems.","sentences":["Nonstabilizerness or `magic' is a key resource for quantum computing and a necessary condition for quantum advantage.","Non-Clifford operations turn stabilizer states into resourceful states, where the amount of nonstabilizerness is quantified by resource measures such as stabilizer R\\'enyi entropies (SREs).","Here, we show that SREs saturate their maximum value at a critical number of non-Clifford operations.","Close to the critical point SREs show universal behavior.","Remarkably, the derivative of the SRE crosses at the same point independent of the number of qubits and can be rescaled onto a single curve.","We find that the critical point depends non-trivially on R\\'enyi index $\\alpha$. For random Clifford circuits doped with T-gates, the critical T-gate density scales independently of $\\alpha$. In contrast, for random Hamiltonian evolution, the critical time scales linearly with qubit number for $\\alpha>1$, while is a constant for $\\alpha<1$. This highlights that $\\alpha$-SREs reveal fundamentally different aspects of nonstabilizerness depending on $\\alpha$: $\\alpha$-SREs with $\\alpha<1$ relate to Clifford simulation complexity, while $\\alpha>1$ probe the distance to the closest stabilizer state and approximate state certification cost via Pauli measurements.","As technical contributions, we observe that the Pauli spectrum of random evolution can be approximated by two highly concentrated peaks which allows us to compute its SRE.","Further, we introduce a class of random evolution that can be expressed as random Clifford circuits and rotations, where we provide its exact SRE.","Our results opens up new approaches to characterize the complexity of quantum systems."],"url":"http://arxiv.org/abs/2406.04190v1","category":"quant-ph"}
{"created":"2024-06-06 15:41:12","title":"Numerical Optimization Study of a Constrained Hypersonic Reentry Vehicle","abstract":"The trajectory optimization of the atmospheric entry of a reusable launch vehicle is studied. The objective is to maximize the crossrange of the vehicle subject to two control-inequality path constraints, two state-inequality path constraints, and one mixed state-and-control inequality path constraint. In order to determine the complex switching structure in the activity of the path constraints, a recently developed method for solving state-path constrained optimal control problems is used. This recently developed method is designed to algorithmically locate the points of activation and deactivation in the path constraints and partition the domain of the independent variable into subdomains based on these activation and deactivation points. Additionally, in a domain where a state-inequality path constraint is found to be active, the method algorithmically determines and enforces the additional necessary conditions that apply on the constrained arc. A multiple-domain formulation of Legendre-Gauss-Radau direct collocation is then employed to transcribe the optimal control problem into a large sparse nonlinear programming problem. Two studies are performed which analyze a variety of problem formulations of the hypersonic reusable launch vehicle. Key features of the constrained trajectories are presented, and the method used is shown to obtain highly accurate solutions with minimal user intervention.","sentences":["The trajectory optimization of the atmospheric entry of a reusable launch vehicle is studied.","The objective is to maximize the crossrange of the vehicle subject to two control-inequality path constraints, two state-inequality path constraints, and one mixed state-and-control inequality path constraint.","In order to determine the complex switching structure in the activity of the path constraints, a recently developed method for solving state-path constrained optimal control problems is used.","This recently developed method is designed to algorithmically locate the points of activation and deactivation in the path constraints and partition the domain of the independent variable into subdomains based on these activation and deactivation points.","Additionally, in a domain where a state-inequality path constraint is found to be active, the method algorithmically determines and enforces the additional necessary conditions that apply on the constrained arc.","A multiple-domain formulation of Legendre-Gauss-Radau direct collocation is then employed to transcribe the optimal control problem into a large sparse nonlinear programming problem.","Two studies are performed which analyze a variety of problem formulations of the hypersonic reusable launch vehicle.","Key features of the constrained trajectories are presented, and the method used is shown to obtain highly accurate solutions with minimal user intervention."],"url":"http://arxiv.org/abs/2406.04185v1","category":"math.OC"}
{"created":"2024-06-06 15:39:44","title":"Manipulating the Relaxation Time of Boundary-Dissipative Systems through Bond Dissipation","abstract":"Relaxation time plays a crucial role in describing the relaxation processes of quantum systems. We study the effect of a type of bond dissipation on the relaxation time of boundary dissipative systems and find that it can change the scaling of the relaxation time $T_c\\sim L^{z}$ from $z=3$ to a value significantly less than $3$. We further reveal that the reason such bond dissipation can significantly reduce the relaxation time is that it can selectively target specific states. For Anderson localized systems, the scaling behavior of the relaxation time changes from an exponential form to a power-law form as the system size varies. This is because the bond dissipation we consider can not only select specific states but also disrupt the localization properties. Our work reveals that in open systems, one type of dissipation can be used to regulate the effects produced by another type of dissipation.","sentences":["Relaxation time plays a crucial role in describing the relaxation processes of quantum systems.","We study the effect of a type of bond dissipation on the relaxation time of boundary dissipative systems and find that it can change the scaling of the relaxation time $T_c\\sim L^{z}$ from $z=3$ to a value significantly less than $3$. We further reveal that the reason such bond dissipation can significantly reduce the relaxation time is that it can selectively target specific states.","For Anderson localized systems, the scaling behavior of the relaxation time changes from an exponential form to a power-law form as the system size varies.","This is because the bond dissipation we consider can not only select specific states but also disrupt the localization properties.","Our work reveals that in open systems, one type of dissipation can be used to regulate the effects produced by another type of dissipation."],"url":"http://arxiv.org/abs/2406.04183v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-06 15:36:13","title":"On the zeros of partition functions with multi-spin interactions","abstract":"Let $X_1, \\ldots, X_n$ be probability spaces, let $X$ be their direct product, let $\\phi_1, \\ldots, \\phi_m: X \\longrightarrow {\\Bbb C}$ be random variables, each depending only on a few coordinates of a point $x=(x_1, \\ldots, x_n)$, and let $f=\\phi_1 + \\ldots + \\phi_m$. The expectation $E\\thinspace e^{\\lambda f}$, where $\\lambda \\in {\\Bbb C}$, appears in statistical physics as the partition function of a system with multi-spin interactions, and also in combinatorics and computer science, where it is known as the partition function of edge-coloring models, tensor network contractions or a Holant polynomial. Assuming that each $\\phi_i$ is 1-Lipschitz in the Hamming metric of $X$, that each $\\phi_i(x)$ depends on at most $r \\geq 2$ coordinates $x_1, \\ldots, x_n$ of $x \\in X$, and that for each $j$ there are at most $c \\geq 1$ functions $\\phi_i$ that depend on the coordinate $x_j$, we prove that $E\\thinspace e^{\\lambda f} \\ne 0$ provided $| \\lambda | \\leq \\ (3 c \\sqrt{r-1})^{-1}$ and that the bound is sharp up to a logarithmic in $r$ factor. As a corollary, the value of the expectation can be efficiently approximated, provided $\\lambda$ lies in a slightly smaller disc.","sentences":["Let $X_1, \\ldots, X_n$ be probability spaces, let $X$ be their direct product, let $\\phi_1, \\ldots, \\phi_m:","X \\longrightarrow {\\Bbb C}$ be random variables, each depending only on a few coordinates of a point $x=(x_1, \\ldots, x_n)$, and let $f=\\phi_1 + \\ldots + \\phi_m$.","The expectation $E\\thinspace e^{\\lambda f}$, where $\\lambda \\in {\\Bbb C}$, appears in statistical physics as the partition function of a system with multi-spin interactions, and also in combinatorics and computer science, where it is known as the partition function of edge-coloring models, tensor network contractions or a Holant polynomial.","Assuming that each $\\phi_i$ is 1-Lipschitz in the Hamming metric of $X$, that each $\\phi_i(x)$ depends on at most $r \\geq 2$ coordinates $x_1, \\ldots, x_n$ of $x \\in X$, and that for each $j$ there are at most $c \\geq 1$ functions $\\phi_i$ that depend on the coordinate $x_j$, we prove that $E\\thinspace e^{\\lambda f} \\ne 0$ provided $| \\lambda | \\leq \\ (3 c \\sqrt{r-1})^{-1}$","and that the bound is sharp up to a logarithmic in $r$ factor.","As a corollary, the value of the expectation can be efficiently approximated, provided $\\lambda$ lies in a slightly smaller disc."],"url":"http://arxiv.org/abs/2406.04179v1","category":"math.PR"}
{"created":"2024-06-06 15:20:25","title":"Stability of equilibria and bifurcations for a fluid-solid interaction problem","abstract":"We study certain significant properties of the equilibrium configurations of a rigid body subject to an undamped elastic restoring force, in the stream of a viscous liquid in an unbounded 3D domain. The motion of the coupled system is driven by a uniform flow at spatial infinity, with constant dimensionless velocity $\\lambda$. We show that if $\\lambda$ is below a critical value, $\\lambda_c$ (say), there is a unique and stable time-independent configuration, where the body is in equilibrium and the flow is steady. We also prove that, if $\\lambda<\\lambda_c$, no oscillatory flow may occur. Successively, we investigate possible loss of uniqueness by providing necessary and sufficient conditions for the occurrence of a steady bifurcation at some $\\lambda_s\\ge \\lambda_c$.","sentences":["We study certain significant properties of the equilibrium configurations of a rigid body subject to an undamped elastic restoring force, in the stream of a viscous liquid in an unbounded 3D domain.","The motion of the coupled system is driven by a uniform flow at spatial infinity, with constant dimensionless velocity $\\lambda$. We show that if $\\lambda$ is below a critical value, $\\lambda_c$ (say), there is a unique and stable time-independent configuration, where the body is in equilibrium and the flow is steady.","We also prove that, if $\\lambda<\\lambda_c$, no oscillatory flow may occur.","Successively, we investigate possible loss of uniqueness by providing necessary and sufficient conditions for the occurrence of a steady bifurcation at some $\\lambda_s\\ge \\lambda_c$."],"url":"http://arxiv.org/abs/2406.04162v1","category":"math.AP"}
{"created":"2024-06-06 15:19:19","title":"Disk Evolution Study Through Imaging of Nearby Young Stars (DESTINYS): PDS 111, an old T Tauri star with a young-looking disk","abstract":"The interplay between T Tauri stars and their circumstellar disks, and how this impacts the onset of planet formation has yet to be established. We studied a seemingly old T Tauri star, PDS 111, and its disk. We analyzed optical, infrared, and sub-millimeter observations obtained with VLT/X-shooter, Mercator/HERMES, TESS, VLT/SPHERE, and ALMA, providing a new view on PDS 111 and its protoplanetary disk. The multi-epoch spectroscopy yields photospheric lines to classify the star, and emission lines to study variability in the hot inner disk and to determine the mass-accretion rate. The SPHERE and ALMA observations are used to characterize the dust distribution of the small and large grains, respectively. PDS 111 is a weak-line T Tauri star with spectral type G2, exhibits strong H$\\alpha$ variability and with a low mass-accretion rate of $1-5\\times10^{-10}$\\,M$_{\\odot}$\\,yr$^{-1}$. We measured an age of the system of 15.9$^{+1.7}_{-3.7}$ Myr using pre-main sequence tracks. The SPHERE observations show a strongly flaring disk with an asymmetric substructure. The ALMA observations reveal a 30 au cavity in the dust continuum emission with a low contrast asymmetry in the South-West of the disk and a dust disk mass of 45.8\\,$M_\\oplus$. The $^{12}$CO radial extension is at least three times larger than that of the dust emission. Although the measured age is younger than suggested in literature, PDS 111 still seems relatively old; this provides insight into disk properties at an advanced stage of pre-main sequence evolution. The characteristics of this disk are very similar to its younger counterparts: strongly flaring, an average disk mass, a typical radial extent of the disk gas and dust, and the presence of common substructures. This suggests that disk evolution has not significantly changed the disk properties. These results show similarities with the \"Peter Pan disks\" around M-dwarfs.","sentences":["The interplay between T Tauri stars and their circumstellar disks, and how this impacts the onset of planet formation has yet to be established.","We studied a seemingly old T Tauri star, PDS 111, and its disk.","We analyzed optical, infrared, and sub-millimeter observations obtained with VLT/X-shooter, Mercator/HERMES, TESS, VLT/SPHERE, and ALMA, providing a new view on PDS 111 and its protoplanetary disk.","The multi-epoch spectroscopy yields photospheric lines to classify the star, and emission lines to study variability in the hot inner disk and to determine the mass-accretion rate.","The SPHERE and ALMA observations are used to characterize the dust distribution of the small and large grains, respectively.","PDS 111 is a weak-line T Tauri star with spectral type G2, exhibits strong H$\\alpha$ variability and with a low mass-accretion rate of $1-5\\times10^{-10}$\\,M$_{\\odot}$\\,yr$^{-1}$. We measured an age of the system of 15.9$^{+1.7}_{-3.7}$ Myr using pre-main sequence tracks.","The SPHERE observations show a strongly flaring disk with an asymmetric substructure.","The ALMA observations reveal a 30 au cavity in the dust continuum emission with a low contrast asymmetry in the South-West of the disk and a dust disk mass of 45.8\\,$M_\\oplus$. The $^{12}$CO radial extension is at least three times larger than that of the dust emission.","Although the measured age is younger than suggested in literature, PDS 111 still seems relatively old; this provides insight into disk properties at an advanced stage of pre-main sequence evolution.","The characteristics of this disk are very similar to its younger counterparts: strongly flaring, an average disk mass, a typical radial extent of the disk gas and dust, and the presence of common substructures.","This suggests that disk evolution has not significantly changed the disk properties.","These results show similarities with the \"Peter Pan disks\" around M-dwarfs."],"url":"http://arxiv.org/abs/2406.04160v1","category":"astro-ph.EP"}
{"created":"2024-06-06 15:16:44","title":"Position: How Regulation Will Change Software Security Research","abstract":"Software security has been an important research topic over the years. The community has proposed processes and tools for secure software development and security analysis. However, a significant number of vulnerabilities remains in real-world software-driven systems and products.   To alleviate this problem, legislation is being established to oblige manufacturers, for example, to comply with essential security requirements and to establish appropriate development practices. We argue that software engineering research needs to provide better tools and support that helps industry comply with the new standards while retaining effcient processes. We argue for a stronger cooperation between legal scholars and computer scientists, and for bridging the gap between higher-level regulation and code-level engineering.","sentences":["Software security has been an important research topic over the years.","The community has proposed processes and tools for secure software development and security analysis.","However, a significant number of vulnerabilities remains in real-world software-driven systems and products.   ","To alleviate this problem, legislation is being established to oblige manufacturers, for example, to comply with essential security requirements and to establish appropriate development practices.","We argue that software engineering research needs to provide better tools and support that helps industry comply with the new standards while retaining effcient processes.","We argue for a stronger cooperation between legal scholars and computer scientists, and for bridging the gap between higher-level regulation and code-level engineering."],"url":"http://arxiv.org/abs/2406.04152v1","category":"cs.SE"}
{"created":"2024-06-06 15:04:36","title":"Coding Over Coupon Collector Channels for Combinatorial Motif-Based DNA Storage","abstract":"Encoding information in combinations of pre-synthesised deoxyribonucleic acid (DNA) strands (referred to as motifs) is an interesting approach to DNA storage that could potentially circumvent the prohibitive costs of nucleotide-by-nucleotide DNA synthesis. Based on our analysis of an empirical data set from HelixWorks, we propose two channel models for this setup (with and without interference) and analyse their fundamental limits. We propose a coding scheme that approaches those limits by leveraging all information available at the output of the channel, in contrast to earlier schemes developed for a similar setup by Preuss et al. We highlight an important connection between channel capacity curves and the fundamental trade-off between synthesis (writing) and sequencing (reading), and offer a way to mitigate an exponential growth in decoding complexity with the size of the motif library.","sentences":["Encoding information in combinations of pre-synthesised deoxyribonucleic acid (DNA) strands (referred to as motifs) is an interesting approach to DNA storage that could potentially circumvent the prohibitive costs of nucleotide-by-nucleotide DNA synthesis.","Based on our analysis of an empirical data set from HelixWorks, we propose two channel models for this setup (with and without interference) and analyse their fundamental limits.","We propose a coding scheme that approaches those limits by leveraging all information available at the output of the channel, in contrast to earlier schemes developed for a similar setup by Preuss et al.","We highlight an important connection between channel capacity curves and the fundamental trade-off between synthesis (writing) and sequencing (reading), and offer a way to mitigate an exponential growth in decoding complexity with the size of the motif library."],"url":"http://arxiv.org/abs/2406.04141v1","category":"cs.IT"}
{"created":"2024-06-06 14:51:31","title":"KIC 4150611: A quadruply eclipsing heptuple star system with a g-mode period-spacing pattern Eclipse modelling of the triple and spectroscopic analysis","abstract":"KIC 4150611 is a high-order multiple composed of a triple system composed of the F1V primary (Aa), which is eclipsed on a 94.2d period by a tight 1.52d binary composed of two dim K/M dwarfs (Ab1, Ab2), which also eclipse each other; an 8.65d eccentric, eclipsing binary composed of two G stars (Ba, Bb); and another faint eclipsing binary composed of two stars of unknown spectral type (Ca and Cb). In addition to its many eclipses, the system is an SB3 spectroscopic multiple (Aa, Ba, and Bb) and the primary (Aa) is a hybrid pulsator. We employ a novel photometric analysis of the complicated eclipse geometry of Aa to obtain orbital and stellar properties of the triple. We acquired 51 TRES spectra at the Fred L. Whipple Observatory, calculating radial velocities and orbital elements of Aa (SB1) and the B binary (SB2). These spectra and radial velocities are used to perform spectral disentangling for Aa, Ba, and Bb. Spectral modelling is applied to the disentangled spectrum of Aa to obtain atmospheric properties. We obtain precise stellar properties of the triple, including the mass ratios (MAa/(MAb1 + MAb2) = 3.61 +/- 0.01, MAb1/MAb2 = 1.113 +/- 0.001), separation ratio (aAab/aAb1Ab2 = 21.81 +/- 0.01), orbital periods (PAab = 94.29486 +/- 0.00008d, PAb1Ab2 = 1.522248 +/- 0.000001d), and stellar radii (RAa = 1.64 +/- 0.06 Rsun, RAb1 = 0.42 +/- 0.01 Rsun, RAb2 = 0.38 +/- 0.01 Rsun). Radial velocity fitting and spectral disentangling arrive at orbital elements for Aa, Ba, and Bb in excellent agreement with each other and with previous results in the literature. Spectral modelling on the disentangled spectrum of Aa provides constraints on the effective temperature (Teff = 7280 +/- 70 K), surface gravity (log(g) = 4.14 +/- 0.18 dex), micro-turbulent velocity (vmicro = 3.61 +/- 0.19 km s-1), rotation velocity (v sin i = 127 +/- 4 km s-1), and metallicity ([M/H] = -0.23 +/- 0.06).","sentences":["KIC 4150611 is a high-order multiple composed of a triple system composed of the F1V primary (Aa), which is eclipsed on a 94.2d period by a tight 1.52d binary composed of two dim K/M dwarfs (Ab1, Ab2), which also eclipse each other; an 8.65d eccentric, eclipsing binary composed of two G stars (Ba, Bb); and another faint eclipsing binary composed of two stars of unknown spectral type (Ca and Cb).","In addition to its many eclipses, the system is an SB3 spectroscopic multiple (Aa, Ba, and Bb) and the primary (Aa) is a hybrid pulsator.","We employ a novel photometric analysis of the complicated eclipse geometry of Aa to obtain orbital and stellar properties of the triple.","We acquired 51 TRES spectra at the Fred L. Whipple Observatory, calculating radial velocities and orbital elements of Aa (SB1) and the B binary (SB2).","These spectra and radial velocities are used to perform spectral disentangling for Aa, Ba, and Bb.","Spectral modelling is applied to the disentangled spectrum of Aa to obtain atmospheric properties.","We obtain precise stellar properties of the triple, including the mass ratios (MAa/(MAb1 + MAb2) = 3.61 +/- 0.01, MAb1/MAb2 = 1.113 +/- 0.001), separation ratio (aAab/aAb1Ab2 = 21.81 +/- 0.01), orbital periods (PAab = 94.29486 +/- 0.00008d, PAb1Ab2 = 1.522248 +/- 0.000001d), and stellar radii (RAa = 1.64 +/- 0.06 Rsun, RAb1 = 0.42 +/- 0.01 Rsun, RAb2","= 0.38 +/- 0.01 Rsun).","Radial velocity fitting and spectral disentangling arrive at orbital elements for Aa, Ba, and Bb in excellent agreement with each other and with previous results in the literature.","Spectral modelling on the disentangled spectrum of Aa provides constraints on the effective temperature (Teff = 7280 +/- 70 K), surface gravity (log(g) = 4.14 +/- 0.18 dex), micro-turbulent velocity (vmicro = 3.61 +/- 0.19 km s-1),","rotation velocity (v sin","i = 127 +/- 4 km s-1), and metallicity ([M/H] = -0.23 +/- 0.06)."],"url":"http://arxiv.org/abs/2406.04131v1","category":"astro-ph.SR"}
{"created":"2024-06-06 14:50:47","title":"An overview of systems-theoretic guarantees in data-driven model predictive control","abstract":"The development of control methods based on data has seen a surge of interest in recent years. When applying data-driven controllers in real-world applications, providing theoretical guarantees for the closed-loop system is of crucial importance to ensure reliable operation. In this review, we provide an overview of data-driven model predictive control (MPC) methods for controlling unknown systems with guarantees on systems-theoretic properties such as stability, robustness, and constraint satisfaction. The considered approaches rely on the Fundamental Lemma from behavioral theory in order to predict input-output trajectories directly from data. We cover various setups, ranging from linear systems and noise-free data to more realistic formulations with noise and nonlinearities, and we provide an overview of different techniques to ensure guarantees for the closed-loop system. Moreover, we discuss avenues for future research that may further improve the theoretical understanding and practical applicability of data-driven MPC.","sentences":["The development of control methods based on data has seen a surge of interest in recent years.","When applying data-driven controllers in real-world applications, providing theoretical guarantees for the closed-loop system is of crucial importance to ensure reliable operation.","In this review, we provide an overview of data-driven model predictive control (MPC) methods for controlling unknown systems with guarantees on systems-theoretic properties such as stability, robustness, and constraint satisfaction.","The considered approaches rely on the Fundamental Lemma from behavioral theory in order to predict input-output trajectories directly from data.","We cover various setups, ranging from linear systems and noise-free data to more realistic formulations with noise and nonlinearities, and we provide an overview of different techniques to ensure guarantees for the closed-loop system.","Moreover, we discuss avenues for future research that may further improve the theoretical understanding and practical applicability of data-driven MPC."],"url":"http://arxiv.org/abs/2406.04130v1","category":"eess.SY"}
{"created":"2024-06-06 14:50:15","title":"LenslessFace: An End-to-End Optimized Lensless System for Privacy-Preserving Face Verification","abstract":"Lensless cameras, innovatively replacing traditional lenses for ultra-thin, flat optics, encode light directly onto sensors, producing images that are not immediately recognizable. This compact, lightweight, and cost-effective imaging solution offers inherent privacy advantages, making it attractive for privacy-sensitive applications like face verification. Typical lensless face verification adopts a two-stage process of reconstruction followed by verification, incurring privacy risks from reconstructed faces and high computational costs. This paper presents an end-to-end optimization approach for privacy-preserving face verification directly on encoded lensless captures, ensuring that the entire software pipeline remains encoded with no visible faces as intermediate results. To achieve this, we propose several techniques to address unique challenges from the lensless setup which precludes traditional face detection and alignment. Specifically, we propose a face center alignment scheme, an augmentation curriculum to build robustness against variations, and a knowledge distillation method to smooth optimization and enhance performance. Evaluations under both simulation and real environment demonstrate our method outperforms two-stage lensless verification while enhancing privacy and efficiency. Project website: \\url{lenslessface.github.io}.","sentences":["Lensless cameras, innovatively replacing traditional lenses for ultra-thin, flat optics, encode light directly onto sensors, producing images that are not immediately recognizable.","This compact, lightweight, and cost-effective imaging solution offers inherent privacy advantages, making it attractive for privacy-sensitive applications like face verification.","Typical lensless face verification adopts a two-stage process of reconstruction followed by verification, incurring privacy risks from reconstructed faces and high computational costs.","This paper presents an end-to-end optimization approach for privacy-preserving face verification directly on encoded lensless captures, ensuring that the entire software pipeline remains encoded with no visible faces as intermediate results.","To achieve this, we propose several techniques to address unique challenges from the lensless setup which precludes traditional face detection and alignment.","Specifically, we propose a face center alignment scheme, an augmentation curriculum to build robustness against variations, and a knowledge distillation method to smooth optimization and enhance performance.","Evaluations under both simulation and real environment demonstrate our method outperforms two-stage lensless verification while enhancing privacy and efficiency.","Project website: \\url{lenslessface.github.io}."],"url":"http://arxiv.org/abs/2406.04129v1","category":"cs.CV"}
{"created":"2024-06-06 14:38:20","title":"Light Curve Models of Convective Common Envelopes","abstract":"Common envelopes are thought to be the main method for producing tight binaries in the universe as the orbital period shrinks by several orders of magnitude during this phase. Despite their importance for various evolutionary channels, direct detections are rare, and thus observational constraints on common envelope physics are often inferred from post-CE populations. Population constraints suggest that the CE phase must be highly inefficient at using orbital energy to drive envelope ejection for low-mass systems and highly efficient for high-mass systems. Such a dichotomy has been explained by an interplay between convection, radiation and orbital decay. If convective transport to the surface occurs faster than the orbit decays, the CE self-regulates and radiatively cools. Once the orbit shrinks such that convective transport is slow compared to orbital decay, a burst occurs as the release of orbital energy can be far in excess of that required to unbind the envelope. With the anticipation of first light for the Rubin Observatory, we calculate light curve models for convective common envelopes and provide the time evolution of apparent magnitudes for the Rubin filters. Convection imparts a distinct signature in the light curves and lengthens the timescales during which they are observable. Given Rubin limiting magnitudes, convective CEs should be detectable out to distances of ~8 Mpc at a rate of ~0.3 per day and provide an intriguing observational test of common envelope physics.","sentences":["Common envelopes are thought to be the main method for producing tight binaries in the universe as the orbital period shrinks by several orders of magnitude during this phase.","Despite their importance for various evolutionary channels, direct detections are rare, and thus observational constraints on common envelope physics are often inferred from post-CE populations.","Population constraints suggest that the CE phase must be highly inefficient at using orbital energy to drive envelope ejection for low-mass systems and highly efficient for high-mass systems.","Such a dichotomy has been explained by an interplay between convection, radiation and orbital decay.","If convective transport to the surface occurs faster than the orbit decays, the CE self-regulates and radiatively cools.","Once the orbit shrinks such that convective transport is slow compared to orbital decay, a burst occurs as the release of orbital energy can be far in excess of that required to unbind the envelope.","With the anticipation of first light for the Rubin Observatory, we calculate light curve models for convective common envelopes and provide the time evolution of apparent magnitudes for the Rubin filters.","Convection imparts a distinct signature in the light curves and lengthens the timescales during which they are observable.","Given Rubin limiting magnitudes, convective CEs should be detectable out to distances of ~8 Mpc at a rate of ~0.3 per day and provide an intriguing observational test of common envelope physics."],"url":"http://arxiv.org/abs/2406.04118v1","category":"astro-ph.SR"}
{"created":"2024-06-06 14:27:57","title":"Giant and anisotropic enhancement of spin-charge conversion in double Rashba interface graphene-based quantum system","abstract":"The ever-increasing demand for efficient data storage and processing has fueled the search for novel memory devices. Spintronics offers an alternative fast and efficient solution using spin-to-charge interconversion. In this work, we demonstrate a remarkable thirty-four-fold increase in spin-to-charge current conversion when incorporating a 2D epitaxial graphene monolayer between iron and platinum layers by exploring spin-pumping on-chip devices. Furthermore, we find that the spin conversion is also anisotropic. We attribute this enhancement and anisotropy to the asymmetric Rashba contributions driven by an unbalanced spin accumulation at the differently hybridized top and bottom graphene interfaces, as highlighted by ad-hoc first-principles theory. The improvement in spin-to-charge conversion as well as its anisotropy reveals the importance of interfaces in hybrid 2D-thin film systems opening up new possibilities for engineering spin conversion in 2D materials, leading to potential advances in memory, logic applications, or unconventional computing.","sentences":["The ever-increasing demand for efficient data storage and processing has fueled the search for novel memory devices.","Spintronics offers an alternative fast and efficient solution using spin-to-charge interconversion.","In this work, we demonstrate a remarkable thirty-four-fold increase in spin-to-charge current conversion when incorporating a 2D epitaxial graphene monolayer between iron and platinum layers by exploring spin-pumping on-chip devices.","Furthermore, we find that the spin conversion is also anisotropic.","We attribute this enhancement and anisotropy to the asymmetric Rashba contributions driven by an unbalanced spin accumulation at the differently hybridized top and bottom graphene interfaces, as highlighted by ad-hoc first-principles theory.","The improvement in spin-to-charge conversion as well as its anisotropy reveals the importance of interfaces in hybrid 2D-thin film systems opening up new possibilities for engineering spin conversion in 2D materials, leading to potential advances in memory, logic applications, or unconventional computing."],"url":"http://arxiv.org/abs/2406.04110v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-06 14:26:35","title":"Intention and Face in Dialog","abstract":"The notion of face described by Brown and Levinson (1987) has been studied in great detail, but a critical aspect of the framework, that which focuses on how intentions mediate the planning of turns which impose upon face, has received far less attention. We present an analysis of three computational systems trained for classifying both intention and politeness, focusing on how the former influences the latter. In politeness theory, agents attend to the desire to have their wants appreciated (positive face), and a complementary desire to act unimpeded and maintain freedom (negative face). Similar to speech acts, utterances can perform so-called face acts which can either raise or threaten the positive or negative face of the speaker or hearer. We begin by using an existing corpus to train a model which classifies face acts, achieving a new SoTA in the process. We then observe that every face act has an underlying intention that motivates it and perform additional experiments integrating dialog act annotations to provide these intentions by proxy. Our analysis finds that dialog acts improve performance on face act detection for minority classes and points to a close relationship between aspects of face and intent.","sentences":["The notion of face described by Brown and Levinson (1987) has been studied in great detail, but a critical aspect of the framework, that which focuses on how intentions mediate the planning of turns which impose upon face, has received far less attention.","We present an analysis of three computational systems trained for classifying both intention and politeness, focusing on how the former influences the latter.","In politeness theory, agents attend to the desire to have their wants appreciated (positive face), and a complementary desire to act unimpeded and maintain freedom (negative face).","Similar to speech acts, utterances can perform so-called face acts which can either raise or threaten the positive or negative face of the speaker or hearer.","We begin by using an existing corpus to train a model which classifies face acts, achieving a new SoTA in the process.","We then observe that every face act has an underlying intention that motivates it and perform additional experiments integrating dialog act annotations to provide these intentions by proxy.","Our analysis finds that dialog acts improve performance on face act detection for minority classes and points to a close relationship between aspects of face and intent."],"url":"http://arxiv.org/abs/2406.04109v1","category":"cs.CL"}
{"created":"2024-06-06 14:06:49","title":"Computing Floquet quasienergies in finite and extended systems: Role of electromagnetic and quantum-geometric gauges","abstract":"We present an approach to compute the Floquet quasienergy spectrum of time-periodic systems. The method allows to characterize the light-matter interaction in finite and extended structures by carefully addressing the resolution of the position operator. In periodic systems we discuss the role of the quantum-geometric gauge freedom of Bloch states and employ a Wannier-based scheme to compute the required matrix elements. As a consequence, the method is accurate and applicable to a broad range of systems, from atoms and molecules to cold atomic gases and materials described by density functional theory, as well as model systems. We demonstrate the applicability of the approach by studying two cases: a particle trapped in a one-dimensional box and the semiconducting material BC$_2$N. We employ the first example to provide a numerical proof of the invariance of the Floquet quasienergy spectrum with respect to the choice of electromagnetic gauge. The analysis of BC$_2$N then serves to illustrate the physical effects described by the quasienergies, such as multiphoton resonances, and their expected range of occurrence in real materials in terms of external electric field and frequency of the drive pulse.","sentences":["We present an approach to compute the Floquet quasienergy spectrum of time-periodic systems.","The method allows to characterize the light-matter interaction in finite and extended structures by carefully addressing the resolution of the position operator.","In periodic systems we discuss the role of the quantum-geometric gauge freedom of Bloch states and employ a Wannier-based scheme to compute the required matrix elements.","As a consequence, the method is accurate and applicable to a broad range of systems, from atoms and molecules to cold atomic gases and materials described by density functional theory, as well as model systems.","We demonstrate the applicability of the approach by studying two cases: a particle trapped in a one-dimensional box and the semiconducting material BC$_2$N. We employ the first example to provide a numerical proof of the invariance of the Floquet quasienergy spectrum with respect to the choice of electromagnetic gauge.","The analysis of BC$_2$N then serves to illustrate the physical effects described by the quasienergies, such as multiphoton resonances, and their expected range of occurrence in real materials in terms of external electric field and frequency of the drive pulse."],"url":"http://arxiv.org/abs/2406.04091v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-06 13:49:33","title":"Quantum Simulations with Bilayer 2D Bose Gases in Multiple-RF-dressed Potentials","abstract":"Multiple-RF (MRF) dressing allows trapping of ultracold atoms in novel spatial geometries, such as highly controllable bilayer structures for 2D ultracold gases, providing unique opportunities for the investigation of 2D quantum systems both in and out of equilibrium. Here, we give an overview of the recent developments of MRF-dressed atom experiments, illustrated by the detailed studies of universal relaxation dynamics across the Berezinskii-Kosterlitz-Thouless critical point enabled by coherent splitting quench protocols and detection of correlations via spatially selective matter-wave interferometry.","sentences":["Multiple-RF (MRF) dressing allows trapping of ultracold atoms in novel spatial geometries, such as highly controllable bilayer structures for 2D ultracold gases, providing unique opportunities for the investigation of 2D quantum systems both in and out of equilibrium.","Here, we give an overview of the recent developments of MRF-dressed atom experiments, illustrated by the detailed studies of universal relaxation dynamics across the Berezinskii-Kosterlitz-Thouless critical point enabled by coherent splitting quench protocols and detection of correlations via spatially selective matter-wave interferometry."],"url":"http://arxiv.org/abs/2406.04080v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-06 13:48:10","title":"Conic Surfaces and Transformations for X-Ray Beamline Optics Modeling","abstract":"Optical surfaces represented by second-degree polynomials (quadratic or conics) are ubiquitous in optics. We revisit the equations of the conic shapes in the context of grazing incidence optics, gathering together the curves commonly used in x-ray instruments and synchrotron beamlines. We present the equations for paraboloids, ellipsoids, and hyperboloids in a common and consistent notation. We develop the transformations from centered systems that are commonly used to describe conics and their axes of symmetry, to local coordinate systems centered on the off-axis mirror surfaces. The equations presented are directly applicable to ray tracing, fabrication, and metrology calculations. They can also be used to study misalignments, movement tolerances, and aberrations of optical surfaces.","sentences":["Optical surfaces represented by second-degree polynomials (quadratic or conics) are ubiquitous in optics.","We revisit the equations of the conic shapes in the context of grazing incidence optics, gathering together the curves commonly used in x-ray instruments and synchrotron beamlines.","We present the equations for paraboloids, ellipsoids, and hyperboloids in a common and consistent notation.","We develop the transformations from centered systems that are commonly used to describe conics and their axes of symmetry, to local coordinate systems centered on the off-axis mirror surfaces.","The equations presented are directly applicable to ray tracing, fabrication, and metrology calculations.","They can also be used to study misalignments, movement tolerances, and aberrations of optical surfaces."],"url":"http://arxiv.org/abs/2406.04079v1","category":"physics.optics"}
{"created":"2024-06-06 13:34:18","title":"Hyperbolicity of smooth logarithmic and orbifold pairs in $\\mathbb{P}^n$","abstract":"We derive a necessary and sufficient condition on a hyperplane arrangement in $\\mathbb{P}^n$ for the associated logarithmic cotangent bundle to be ample modulo boundary. We extend this result to the orbifold setting and give some applications concerning hyperbolicity of pairs.","sentences":["We derive a necessary and sufficient condition on a hyperplane arrangement in $\\mathbb{P}^n$ for the associated logarithmic cotangent bundle to be ample modulo boundary.","We extend this result to the orbifold setting and give some applications concerning hyperbolicity of pairs."],"url":"http://arxiv.org/abs/2406.04069v1","category":"math.AG"}
{"created":"2024-06-06 13:32:26","title":"Low-temperature spin dynamics and absence of magnetic order in layered $\u03b1$-RuI$_3$","abstract":"The triangular-lattice system $\\alpha$-RuI$_3$ is isostructural to the widely-studied $\\alpha$-RuCl$_3$ compound which was identified as a potential Kitaev system but exhibits, instead of spin liquid behaviour, a magnetically ordered zig-zag ground state which sets in below 14~K. Here we show experimentally that, in contrast, the spins in $\\alpha$-RuI$_3$ remain dynamic down to at least 50~mK. We study the spin dynamics using muon-spin relaxation methods and determine the presence of low-frequency fluctuations which are characteristic of a two-dimensional system.","sentences":["The triangular-lattice system $\\alpha$-RuI$_3$ is isostructural to the widely-studied $\\alpha$-RuCl$_3$ compound which was identified as a potential Kitaev system but exhibits, instead of spin liquid behaviour, a magnetically ordered zig-zag ground state which sets in below 14~K. Here we show experimentally that, in contrast, the spins in $\\alpha$-RuI$_3$ remain dynamic down to at least 50~mK. We study the spin dynamics using muon-spin relaxation methods and determine the presence of low-frequency fluctuations which are characteristic of a two-dimensional system."],"url":"http://arxiv.org/abs/2406.04065v1","category":"cond-mat.str-el"}
{"created":"2024-06-06 13:32:06","title":"Ultrasolid Homotopical Algebra","abstract":"Solid modules over $\\mathbb{Q}$ or $\\mathbb{F}_p$, introduced by Clausen and Scholze, are a well-behaved variant of complete topological vector spaces that forms a symmetric monoidal Grothendieck abelian category. For a discrete field $k$, we construct the category of ultrasolid $k$-modules, which specialises to solid modules over $\\mathbb{Q}$ or $\\mathbb{F}_p$. In this setting, we show some commutative algebra results like an ultrasolid variant of Nakayama's lemma. We also explore higher algebra in the form of animated and $\\mathbb{E}_\\infty$ ultrasolid $k$-algebras, and their deformation theory. We focus on the subcategory of complete profinite $k$-algebras, which we prove is contravariantly equivalent to equal characteristic formal moduli problems with coconnective tangent complex.","sentences":["Solid modules over $\\mathbb{Q}$ or $\\mathbb{F}_p$, introduced by Clausen and Scholze, are a well-behaved variant of complete topological vector spaces that forms a symmetric monoidal Grothendieck abelian category.","For a discrete field $k$, we construct the category of ultrasolid $k$-modules, which specialises to solid modules over $\\mathbb{Q}$ or $\\mathbb{F}_p$. In this setting, we show some commutative algebra results like an ultrasolid variant of Nakayama's lemma.","We also explore higher algebra in the form of animated and $\\mathbb{E}_\\infty$ ultrasolid $k$-algebras, and their deformation theory.","We focus on the subcategory of complete profinite $k$-algebras, which we prove is contravariantly equivalent to equal characteristic formal moduli problems with coconnective tangent complex."],"url":"http://arxiv.org/abs/2406.04063v1","category":"math.AG"}
{"created":"2024-06-06 13:20:47","title":"Floquet Theory in an Irradiated Nodal Surface Semimetal","abstract":"A nodal surface semimetal (NSSM) features symmetry enforced band crossings along a surface within the three-dimensional (3D) Brillouin zone. The topological robustness of the same does not always come with nonzero Berry fluxes around nodal surfaces. Irrespective of that, however, light irradiation on such system can result in interesting dynamic behavior. We find that depending on the state of polarization, one can obtain additional Weyl points/ nodal surfaces in the Floquet Hamiltonian that the time periodic system gives rise to. For simplicity we only consider two band spinless models without any spin orbit coupling, the main emphasis being to understand the low energy behavior close to the band crossings and its evolution in a Floquet system in the high frequency limit. There we find the nodal surfaces to perish or get duplicated or triplicated for different polarization scenario or different NSSM Hamiltonians. One findings open up important directions on what out of equilibrium NSSM systems has to offer in many active fields including quantum computations.","sentences":["A nodal surface semimetal (NSSM) features symmetry enforced band crossings along a surface within the three-dimensional (3D) Brillouin zone.","The topological robustness of the same does not always come with nonzero Berry fluxes around nodal surfaces.","Irrespective of that, however, light irradiation on such system can result in interesting dynamic behavior.","We find that depending on the state of polarization, one can obtain additional Weyl points/ nodal surfaces in the Floquet Hamiltonian that the time periodic system gives rise to.","For simplicity we only consider two band spinless models without any spin orbit coupling, the main emphasis being to understand the low energy behavior close to the band crossings and its evolution in a Floquet system in the high frequency limit.","There we find the nodal surfaces to perish or get duplicated or triplicated for different polarization scenario or different NSSM Hamiltonians.","One findings open up important directions on what out of equilibrium NSSM systems has to offer in many active fields including quantum computations."],"url":"http://arxiv.org/abs/2406.04053v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-06 13:16:08","title":"Self-tunable approximated explicit MPC: Heat exchanger implementation and analysis","abstract":"The tunable approximated explicit model predictive control (MPC) comes with the benefits of real-time tunability without the necessity of solving the optimization problem online. This paper provides a novel self-tunable control policy that does not require any interventions of the control engineer during operation in order to retune the controller subject to the changed working conditions. Based on the current operating conditions, the autonomous tuning parameter scales the control input using linear interpolation between the boundary optimal control actions. The adjustment of the tuning parameter depends on the current reference value, which makes this strategy suitable for reference tracking problems. Furthermore, a novel technique for scaling the tuning parameter is proposed. This extension provides to exploit different ranges of the tuning parameter assigned to specified operating conditions. The self-tunable explicit MPC was implemented on a laboratory heat exchanger with nonlinear and asymmetric behavior. The asymmetric behavior of the plant was compensated by tuning the controller's aggressiveness, as the negative or positive sign of reference change was considered in the tuning procedure. The designed self-tunable controller improved control performance by decreasing sum-of-squared control error, maximal overshoots/ undershoots, and settling time compared to the conventional control strategy based on a single (non-tunable) controller.","sentences":["The tunable approximated explicit model predictive control (MPC) comes with the benefits of real-time tunability without the necessity of solving the optimization problem online.","This paper provides a novel self-tunable control policy that does not require any interventions of the control engineer during operation in order to retune the controller subject to the changed working conditions.","Based on the current operating conditions, the autonomous tuning parameter scales the control input using linear interpolation between the boundary optimal control actions.","The adjustment of the tuning parameter depends on the current reference value, which makes this strategy suitable for reference tracking problems.","Furthermore, a novel technique for scaling the tuning parameter is proposed.","This extension provides to exploit different ranges of the tuning parameter assigned to specified operating conditions.","The self-tunable explicit MPC was implemented on a laboratory heat exchanger with nonlinear and asymmetric behavior.","The asymmetric behavior of the plant was compensated by tuning the controller's aggressiveness, as the negative or positive sign of reference change was considered in the tuning procedure.","The designed self-tunable controller improved control performance by decreasing sum-of-squared control error, maximal overshoots/ undershoots, and settling time compared to the conventional control strategy based on a single (non-tunable) controller."],"url":"http://arxiv.org/abs/2406.04048v1","category":"eess.SY"}
{"created":"2024-06-06 13:14:37","title":"Some differential inequalities and criteria for univalency in the unit disk","abstract":"In this paper, Jack lemma is used for obtaining several differential inequalities over analytic functions that later on, lead to new criteria for univalency in the unit disk.","sentences":["In this paper, Jack lemma is used for obtaining several differential inequalities over analytic functions that later on, lead to new criteria for univalency in the unit disk."],"url":"http://arxiv.org/abs/2406.04044v1","category":"math.CV"}
{"created":"2024-06-06 13:00:11","title":"Topological Phases in Half-Integer Higher Spin $J_1$-$J_2$ Heisenberg Chains","abstract":"We study the ground state properties of antiferromagnetic $J_1$-$J_2$ chains with half-integer spins ranging from $S=\\frac{3}{2}$ to $S=\\frac{11}{2}$ using the density-matrix renormalization group method. We map out the ground-state phase diagrams as a function of $\\frac{J_2}{J_1}$ containing topological phases with alternating $\\frac{2S-1}{2}$ and $\\frac{2S+1}{2}$ valence bonds. We identify these topological phases and their boundaries by calculating the string order parameter, the dimer order parameter, and the spin gap for those high-$S$ systems in thermodynamic limit (finite size scaling). We find that these topological regions narrow down inversely with $S$ and converge to a single point at $\\frac{J_2}{J_1}=\\frac{1}{4}$ in the classical limit -- a critical threshold between commensurate and incommensurate orders. In addition, we extend the discussion of the Majumder-Ghosh state, previously noted only for $S=\\frac{1}{2}$, and speculate its possible presence as a ground state in half-integer high spin systems over a substantial range of $\\frac{J_2}{J_1}$ values.","sentences":["We study the ground state properties of antiferromagnetic $J_1$-$J_2$ chains with half-integer spins ranging from $S=\\frac{3}{2}$ to $S=\\frac{11}{2}$ using the density-matrix renormalization group method.","We map out the ground-state phase diagrams as a function of $\\frac{J_2}{J_1}$ containing topological phases with alternating $\\frac{2S-1}{2}$ and $\\frac{2S+1}{2}$ valence bonds.","We identify these topological phases and their boundaries by calculating the string order parameter, the dimer order parameter, and the spin gap for those high-$S$ systems in thermodynamic limit (finite size scaling).","We find that these topological regions narrow down inversely with $S$ and converge to a single point at $\\frac{J_2}{J_1}=\\frac{1}{4}$ in the classical limit -- a critical threshold between commensurate and incommensurate orders.","In addition, we extend the discussion of the Majumder-Ghosh state, previously noted only for $S=\\frac{1}{2}$, and speculate its possible presence as a ground state in half-integer high spin systems over a substantial range of $\\frac{J_2}{J_1}$ values."],"url":"http://arxiv.org/abs/2406.04030v1","category":"cond-mat.str-el"}
{"created":"2024-06-06 12:45:35","title":"A Rydberg atom based system for benchmarking mmWave automotive radar chips","abstract":"Rydberg atomic sensors and receivers have enabled sensitive and traceable measurements of RF fields at a wide range of frequencies. Here we demonstrate the detection of electric field amplitude in the extremely high frequency (EHF) band, at $131\\ \\mathrm{GHz}$. In our approach we propagate the EHF field in a beam, with control over its direction and polarization at the detector using photonic waveplates. This way, we take advantage of the highest detection sensitivity, registered for collinear propagation and circular polarization. To exhibit the potential for applications in this kind of Rydberg-atom based detection, we perform test measurements on the EHF field emitted from an on-chip radar, planned to be used in automotive industry as a vital sign detector. Our work elucidates practical applications of Rydberg-atom media as well as photonic metamaterial elements.","sentences":["Rydberg atomic sensors and receivers have enabled sensitive and traceable measurements of RF fields at a wide range of frequencies.","Here we demonstrate the detection of electric field amplitude in the extremely high frequency (EHF) band, at $131\\ \\mathrm{GHz}$. In our approach we propagate the EHF field in a beam, with control over its direction and polarization at the detector using photonic waveplates.","This way, we take advantage of the highest detection sensitivity, registered for collinear propagation and circular polarization.","To exhibit the potential for applications in this kind of Rydberg-atom based detection, we perform test measurements on the EHF field emitted from an on-chip radar, planned to be used in automotive industry as a vital sign detector.","Our work elucidates practical applications of Rydberg-atom media as well as photonic metamaterial elements."],"url":"http://arxiv.org/abs/2406.04021v1","category":"physics.app-ph"}
{"created":"2024-06-06 12:41:22","title":"Identification of highly-forbidden optical transitions in highly charged ions","abstract":"Optical clocks represent the most precise experimental devices, finding application in fields spanning from frequency metrology to fundamental physics. Recently, the first highly charged ions (HCI) based optical clock was demonstrated using Ar$^{13+}$, opening up a plethora of novel systems with advantageous atomic properties for high accuracy clocks. While numerous candidate systems have been explored theoretically, the considerable uncertainty of the clock transition frequency for most species poses experimental challenges. Here, we close this gap by exploring quantum logic-inspired experimental search techniques for sub-Hertz clock transitions in HCI confined to a linear Paul trap. These techniques encompass Rabi excitation, an optical dipole force (ODF) approach, and linear continuous sweeping (LCS) and their applicability for different types of HCI. Through our investigation, we provide tools to pave the way for the development of exceptionally precise HCI-based optical clocks.","sentences":["Optical clocks represent the most precise experimental devices, finding application in fields spanning from frequency metrology to fundamental physics.","Recently, the first highly charged ions (HCI) based optical clock was demonstrated using Ar$^{13+}$, opening up a plethora of novel systems with advantageous atomic properties for high accuracy clocks.","While numerous candidate systems have been explored theoretically, the considerable uncertainty of the clock transition frequency for most species poses experimental challenges.","Here, we close this gap by exploring quantum logic-inspired experimental search techniques for sub-Hertz clock transitions in HCI confined to a linear Paul trap.","These techniques encompass Rabi excitation, an optical dipole force (ODF) approach, and linear continuous sweeping (LCS) and their applicability for different types of HCI.","Through our investigation, we provide tools to pave the way for the development of exceptionally precise HCI-based optical clocks."],"url":"http://arxiv.org/abs/2406.04015v1","category":"physics.atom-ph"}
{"created":"2024-06-06 12:40:48","title":"Interactive zoom display in smartphone-based digital holographic microscope for 3D imaging","abstract":"Digital holography has applications in bio-imaging because it can simultaneously obtain the amplitude and phase information of a microscopic sample in a single shot, thus facilitating non-contact, noninvasive observation of the 3D shape of transparent objects (phase objects, which can be mapped with the phase information,) and moving objects. The combination of digital holography and microscopy is called digital holographic microscopy (DHM). In this study, we propose a compact and inexpensive smartphone-based DHM system for 3D imaging; this system includes an optical system comprising a 3D printer using commercially available image sensors and semiconductor lasers; further, an Android-based application is used to reconstruct the holograms acquired by this optical system, thus outlining the amplitude and phase information of the observed object. Also, by utilizing scalable diffraction calculation methods and touchscreen interaction, we implemented zoom functionality through pinch-in gestures. The study results showed that the DHM system successfully obtained the amplitude and phase information of the observed object via the acquired holograms in an almost real time manner. Thus, this study showed that it is possible to construct a low cost and compact DHM system that includes a 3D printer to construct the optical system and a smartphone application to reconstruct the holograms. This system is also expected to contribute to biology fieldwork and pathological diagnosis in remote areas.","sentences":["Digital holography has applications in bio-imaging because it can simultaneously obtain the amplitude and phase information of a microscopic sample in a single shot, thus facilitating non-contact, noninvasive observation of the 3D shape of transparent objects (phase objects, which can be mapped with the phase information,) and moving objects.","The combination of digital holography and microscopy is called digital holographic microscopy (DHM).","In this study, we propose a compact and inexpensive smartphone-based DHM system for 3D imaging; this system includes an optical system comprising a 3D printer using commercially available image sensors and semiconductor lasers; further, an Android-based application is used to reconstruct the holograms acquired by this optical system, thus outlining the amplitude and phase information of the observed object.","Also, by utilizing scalable diffraction calculation methods and touchscreen interaction, we implemented zoom functionality through pinch-in gestures.","The study results showed that the DHM system successfully obtained the amplitude and phase information of the observed object via the acquired holograms in an almost real time manner.","Thus, this study showed that it is possible to construct a low cost and compact DHM system that includes a 3D printer to construct the optical system and a smartphone application to reconstruct the holograms.","This system is also expected to contribute to biology fieldwork and pathological diagnosis in remote areas."],"url":"http://arxiv.org/abs/2406.04014v1","category":"cs.GR"}
{"created":"2024-06-06 12:38:59","title":"Variational inference, Mixture of Gaussians, Bayesian Machine Learning","abstract":"Variational inference (VI) is a popular approach in Bayesian inference, that looks for the best approximation of the posterior distribution within a parametric family, minimizing a loss that is typically the (reverse) Kullback-Leibler (KL) divergence. Despite its empirical success, the theoretical properties of VI have only received attention recently, and mostly when the parametric family is the one of Gaussians. This work aims to contribute to the theoretical study of VI in the non-Gaussian case by investigating the setting of Mixture of Gaussians with fixed covariance and constant weights. In this view, VI over this specific family can be casted as the minimization of a Mollified relative entropy, i.e. the KL between the convolution (with respect to a Gaussian kernel) of an atomic measure supported on Diracs, and the target distribution. The support of the atomic measure corresponds to the localization of the Gaussian components. Hence, solving variational inference becomes equivalent to optimizing the positions of the Diracs (the particles), which can be done through gradient descent and takes the form of an interacting particle system. We study two sources of error of variational inference in this context when optimizing the mollified relative entropy. The first one is an optimization result, that is a descent lemma establishing that the algorithm decreases the objective at each iteration. The second one is an approximation error, that upper bounds the objective between an optimal finite mixture and the target distribution.","sentences":["Variational inference (VI) is a popular approach in Bayesian inference, that looks for the best approximation of the posterior distribution within a parametric family, minimizing a loss that is typically the (reverse) Kullback-Leibler (KL) divergence.","Despite its empirical success, the theoretical properties of VI have only received attention recently, and mostly when the parametric family is the one of Gaussians.","This work aims to contribute to the theoretical study of VI in the non-Gaussian case by investigating the setting of Mixture of Gaussians with fixed covariance and constant weights.","In this view, VI over this specific family can be casted as the minimization of a Mollified relative entropy, i.e. the KL between the convolution (with respect to a Gaussian kernel) of an atomic measure supported on Diracs, and the target distribution.","The support of the atomic measure corresponds to the localization of the Gaussian components.","Hence, solving variational inference becomes equivalent to optimizing the positions of the Diracs (the particles), which can be done through gradient descent and takes the form of an interacting particle system.","We study two sources of error of variational inference in this context when optimizing the mollified relative entropy.","The first one is an optimization result, that is a descent lemma establishing that the algorithm decreases the objective at each iteration.","The second one is an approximation error, that upper bounds the objective between an optimal finite mixture and the target distribution."],"url":"http://arxiv.org/abs/2406.04012v1","category":"stat.ML"}
{"created":"2024-06-06 12:33:13","title":"Modeling the spatial resolution of magnetic solitons in Magnetic Force Microscopy and the effect on their sizes","abstract":"In this work, we explored theoretically the spatial resolution of magnetic solitons and the variations of their sizes when subjected to a Magnetic Force Microscopy (MFM) measurement. Next to tip-sample separation, we considered reversal in the magnetization direction of the tip, showing that the magnetic soliton size measurement can be strongly affected by the magnetization direction of the tip. In addition to previous studies that only consider thermal fluctuations, we developed a theoretical method to obtain the minimum observable length of a magnetic soliton and its length variation due to the influence of the MFM tip by minimizing the soliton's magnetic energy. Our model uses analytical and numerical calculations and prevents overestimating the characteristic length scales from MFM images. We compared our method with available data from MFM measurements of domain wall widths, and we performed micromagnetic simulations of a skyrmion-tip system, finding a good agreement for both attractive and repulsive domain wall profile signals and for the skyrmion diameter in the presence of the magnetic tip. Our results provide significant insights for a better interpretation of MFM measurements of different magnetic solitons and will be helpful in the design of potential reading devices based on magnetic solitons as information carriers.","sentences":["In this work, we explored theoretically the spatial resolution of magnetic solitons and the variations of their sizes when subjected to a Magnetic Force Microscopy (MFM) measurement.","Next to tip-sample separation, we considered reversal in the magnetization direction of the tip, showing that the magnetic soliton size measurement can be strongly affected by the magnetization direction of the tip.","In addition to previous studies that only consider thermal fluctuations, we developed a theoretical method to obtain the minimum observable length of a magnetic soliton and its length variation due to the influence of the MFM tip by minimizing the soliton's magnetic energy.","Our model uses analytical and numerical calculations and prevents overestimating the characteristic length scales from MFM images.","We compared our method with available data from MFM measurements of domain wall widths, and we performed micromagnetic simulations of a skyrmion-tip system, finding a good agreement for both attractive and repulsive domain wall profile signals and for the skyrmion diameter in the presence of the magnetic tip.","Our results provide significant insights for a better interpretation of MFM measurements of different magnetic solitons and will be helpful in the design of potential reading devices based on magnetic solitons as information carriers."],"url":"http://arxiv.org/abs/2406.04007v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-06 12:20:54","title":"Benign Nonconvex Landscapes in Optimal and Robust Control, Part II: Extended Convex Lifting","abstract":"Many optimal and robust control problems are nonconvex and potentially nonsmooth in their policy optimization forms. In Part II of this paper, we introduce a new and unified Extended Convex Lifting (ECL) framework to reveal hidden convexity in classical optimal and robust control problems from a modern optimization perspective. Our ECL offers a bridge between nonconvex policy optimization and convex reformulations, enabling convex analysis for nonconvex problems. Despite non-convexity and non-smoothness, the existence of an ECL not only reveals that minimizing the original function is equivalent to a convex problem but also certifies a class of first-order non-degenerate stationary points to be globally optimal. Therefore, no spurious stationarity exists in the set of non-degenerate policies. This ECL framework can cover many benchmark control problems, including state feedback linear quadratic regulator (LQR), dynamic output feedback linear quadratic Gaussian (LQG) control, and $\\mathcal{H}_\\infty$ robust control. ECL can also handle a class of distributed control problems when the notion of quadratic invariance (QI) holds. We further show that all static stabilizing policies are non-degenerate for state feedback LQR and $\\mathcal{H}_\\infty$ control under standard assumptions. We believe that the new ECL framework may be of independent interest for analyzing nonconvex problems beyond control.","sentences":["Many optimal and robust control problems are nonconvex and potentially nonsmooth in their policy optimization forms.","In Part II of this paper, we introduce a new and unified Extended Convex Lifting (ECL) framework to reveal hidden convexity in classical optimal and robust control problems from a modern optimization perspective.","Our ECL offers a bridge between nonconvex policy optimization and convex reformulations, enabling convex analysis for nonconvex problems.","Despite non-convexity and non-smoothness, the existence of an ECL not only reveals that minimizing the original function is equivalent to a convex problem but also certifies a class of first-order non-degenerate stationary points to be globally optimal.","Therefore, no spurious stationarity exists in the set of non-degenerate policies.","This ECL framework can cover many benchmark control problems, including state feedback linear quadratic regulator (LQR), dynamic output feedback linear quadratic Gaussian (LQG) control, and $\\mathcal{H}_\\infty$ robust control.","ECL can also handle a class of distributed control problems when the notion of quadratic invariance (QI) holds.","We further show that all static stabilizing policies are non-degenerate for state feedback LQR and $\\mathcal{H}_\\infty$ control under standard assumptions.","We believe that the new ECL framework may be of independent interest for analyzing nonconvex problems beyond control."],"url":"http://arxiv.org/abs/2406.04001v1","category":"math.OC"}
{"created":"2024-06-06 11:59:13","title":"A variational approach to the quaternionic Hessian equation","abstract":"In this paper, we introduce finite energy classes of quaternionic $m$-plurisubharmonic functions of Cegrell type and define the quaternionic $m$-Hessian operator on some Cegrell's classes. We use the variational approach to solve the quaternionic $m$-Hessian equation when the right-hand side is a positive Radon measure.","sentences":["In this paper, we introduce finite energy classes of quaternionic $m$-plurisubharmonic functions of Cegrell type and define the quaternionic $m$-Hessian operator on some Cegrell's classes.","We use the variational approach to solve the quaternionic $m$-Hessian equation when the right-hand side is a positive Radon measure."],"url":"http://arxiv.org/abs/2406.03985v1","category":"math.CV"}
{"created":"2024-06-06 11:45:36","title":"Benchmarking AlphaFold3's protein-protein complex accuracy and machine learning prediction reliability for binding free energy changes upon mutation","abstract":"AlphaFold 3 (AF3), the latest version of protein structure prediction software, goes beyond its predecessors by predicting protein-protein complexes. It could revolutionize drug discovery and protein engineering, marking a major step towards comprehensive, automated protein structure prediction. However, independent validation of AF3's predictions is necessary. Evaluated using the SKEMPI 2.0 database which involves 317 protein-protein complexes and 8338 mutations, AF3 complex structures give rise to a very good Pearson correlation coefficient of 0.86 for predicting protein-protein binding free energy changes upon mutation, slightly less than the 0.88 achieved earlier with the Protein Data Bank (PDB) structures. Nonetheless, AF3 complex structures led to a 8.6% increase in the prediction RMSE compared to original PDB complex structures. Additionally, some of AF3's complex structures have large errors, which were not captured in its ipTM performance metric. Finally, it is found that AF3's complex structures are not reliable for intrinsically flexible regions or domains.","sentences":["AlphaFold 3 (AF3), the latest version of protein structure prediction software, goes beyond its predecessors by predicting protein-protein complexes.","It could revolutionize drug discovery and protein engineering, marking a major step towards comprehensive, automated protein structure prediction.","However, independent validation of AF3's predictions is necessary.","Evaluated using the SKEMPI 2.0 database which involves 317 protein-protein complexes and 8338 mutations, AF3 complex structures give rise to a very good Pearson correlation coefficient of 0.86 for predicting protein-protein binding free energy changes upon mutation, slightly less than the 0.88 achieved earlier with the Protein Data Bank (PDB) structures.","Nonetheless, AF3 complex structures led to a 8.6% increase in the prediction RMSE compared to original PDB complex structures.","Additionally, some of AF3's complex structures have large errors, which were not captured in its ipTM performance metric.","Finally, it is found that AF3's complex structures are not reliable for intrinsically flexible regions or domains."],"url":"http://arxiv.org/abs/2406.03979v1","category":"q-bio.BM"}
{"created":"2024-06-06 11:42:33","title":"Mini Honor of Kings: A Lightweight Environment for Multi-Agent Reinforcement Learning","abstract":"Games are widely used as research environments for multi-agent reinforcement learning (MARL), but they pose three significant challenges: limited customization, high computational demands, and oversimplification. To address these issues, we introduce the first publicly available map editor for the popular mobile game Honor of Kings and design a lightweight environment, Mini Honor of Kings (Mini HoK), for researchers to conduct experiments. Mini HoK is highly efficient, allowing experiments to be run on personal PCs or laptops while still presenting sufficient challenges for existing MARL algorithms. We have tested our environment on common MARL algorithms and demonstrated that these algorithms have yet to find optimal solutions within this environment. This facilitates the dissemination and advancement of MARL methods within the research community. Additionally, we hope that more researchers will leverage the Honor of Kings map editor to develop innovative and scientifically valuable new maps. Our code and user manual are available at: https://github.com/tencent-ailab/mini-hok.","sentences":["Games are widely used as research environments for multi-agent reinforcement learning (MARL), but they pose three significant challenges: limited customization, high computational demands, and oversimplification.","To address these issues, we introduce the first publicly available map editor for the popular mobile game Honor of Kings and design a lightweight environment, Mini Honor of Kings (Mini HoK), for researchers to conduct experiments.","Mini HoK is highly efficient, allowing experiments to be run on personal PCs or laptops while still presenting sufficient challenges for existing MARL algorithms.","We have tested our environment on common MARL algorithms and demonstrated that these algorithms have yet to find optimal solutions within this environment.","This facilitates the dissemination and advancement of MARL methods within the research community.","Additionally, we hope that more researchers will leverage the Honor of Kings map editor to develop innovative and scientifically valuable new maps.","Our code and user manual are available at: https://github.com/tencent-ailab/mini-hok."],"url":"http://arxiv.org/abs/2406.03978v1","category":"cs.MA"}
{"created":"2024-06-06 11:14:26","title":"3D Ultrasound Shear Wave Elastography for Musculoskeletal Tissue Assessment Under Compressive Load: A Feasibility Study","abstract":"Given its real-time capability to quantify mechanical tissue properties, ultrasound shear wave elastography holds significant promise in clinical musculoskeletal imaging. However, existing shear wave elastography methods fall short in enabling full-limb analysis of 3D anatomical structures under diverse loading conditions, and may introduce measurement bias due to sonographer-applied force on the transducer. These limitations pose numerous challenges, particularly for 3D computational biomechanical tissue modeling in areas like prosthetic socket design. In this feasibility study, a clinical linear ultrasound transducer system with integrated shear wave elastography capabilities was utilized to scan both a calibrated phantom and human limbs in a water tank imaging setup. By conducting 2D and 3D scans under varying compressive loads, this study demonstrates the feasibility of volumetric ultrasound shear wave elastography of human limbs. Our preliminary results showcase a potential method for evaluating 3D spatially varying tissue properties, offering future extensions to computational biomechanical modeling of tissue for various clinical scenarios.","sentences":["Given its real-time capability to quantify mechanical tissue properties, ultrasound shear wave elastography holds significant promise in clinical musculoskeletal imaging.","However, existing shear wave elastography methods fall short in enabling full-limb analysis of 3D anatomical structures under diverse loading conditions, and may introduce measurement bias due to sonographer-applied force on the transducer.","These limitations pose numerous challenges, particularly for 3D computational biomechanical tissue modeling in areas like prosthetic socket design.","In this feasibility study, a clinical linear ultrasound transducer system with integrated shear wave elastography capabilities was utilized to scan both a calibrated phantom and human limbs in a water tank imaging setup.","By conducting 2D and 3D scans under varying compressive loads, this study demonstrates the feasibility of volumetric ultrasound shear wave elastography of human limbs.","Our preliminary results showcase a potential method for evaluating 3D spatially varying tissue properties, offering future extensions to computational biomechanical modeling of tissue for various clinical scenarios."],"url":"http://arxiv.org/abs/2406.03962v1","category":"physics.med-ph"}
{"created":"2024-06-06 11:12:10","title":"Path Integral Monte Carlo Simulation of Superfluid Ring Lattices","abstract":"The goal of this work is to lay the groundwork to construct and characterize a quantum device; which we refer to as a superfluid ring lattice; that could serve as a multi-qubit system in the future. Accordingly, a mathematical framework, called the Integer Lattice Method (ILM), is exploited to construct a two-dimensional optical landscape which could facilitate a superfluid ring qubit. The Integer Lattice Method allows one to design and explore both periodic and quasi-periodic structured coherent wave interference patterns. Furthermore, the formalism allows for a direct link to experimental realization. The lattices obtained from ILM can be investigated using WA-PIMC. In particular, the spatial and superfluid density at equilibrium are observables of interest to obtain our objective.   In this thesis, the ab-initio Path Integral Monte Carlo Worm algorithm is implemented with the future aim of investigating the construction of superfluid ring lattices. It is written in independent and interchangeable modules in the novel programming language Julia. To ensure scalability, a custom nearest-neighbour algorithm is added. The simulation software is benchmarked by comparing it with exact analytical results from the non-interacting Bose gas model. Emerging issues with our implementation of the Worm algorithm are identified and thoroughly investigated. Finally, some preliminary results on the equilibrium density distribution of the found ILM lattices are presented and discussed.","sentences":["The goal of this work is to lay the groundwork to construct and characterize a quantum device; which we refer to as a superfluid ring lattice; that could serve as a multi-qubit system in the future.","Accordingly, a mathematical framework, called the Integer Lattice Method (ILM), is exploited to construct a two-dimensional optical landscape which could facilitate a superfluid ring qubit.","The Integer Lattice Method allows one to design and explore both periodic and quasi-periodic structured coherent wave interference patterns.","Furthermore, the formalism allows for a direct link to experimental realization.","The lattices obtained from ILM can be investigated using WA-PIMC.","In particular, the spatial and superfluid density at equilibrium are observables of interest to obtain our objective.   ","In this thesis, the ab-initio Path Integral Monte Carlo Worm algorithm is implemented with the future aim of investigating the construction of superfluid ring lattices.","It is written in independent and interchangeable modules in the novel programming language Julia.","To ensure scalability, a custom nearest-neighbour algorithm is added.","The simulation software is benchmarked by comparing it with exact analytical results from the non-interacting Bose gas model.","Emerging issues with our implementation of the Worm algorithm are identified and thoroughly investigated.","Finally, some preliminary results on the equilibrium density distribution of the found ILM lattices are presented and discussed."],"url":"http://arxiv.org/abs/2406.03959v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-06 10:49:12","title":"The Perils of Pdot","abstract":"Schaefer (2024) has recently published observations of binary period derivatives $\\dot P$ for 52 cataclysmic variables, and concluded that these strongly conflict with all proposed evolutionary pictures for these systems. We point out once again that using measurements of $\\dot P$ is likely in practice to produce misleading evolutionary constraints in almost every case. The one identified exception to this is probably the recently-born X-ray binary SN 2022jli, because of its extremely high mass transfer rate.","sentences":["Schaefer (2024) has recently published observations of binary period derivatives $\\dot P$ for 52 cataclysmic variables, and concluded that these strongly conflict with all proposed evolutionary pictures for these systems.","We point out once again that using measurements of $\\dot P$ is likely in practice to produce misleading evolutionary constraints in almost every case.","The one identified exception to this is probably the recently-born X-ray binary SN 2022jli, because of its extremely high mass transfer rate."],"url":"http://arxiv.org/abs/2406.03948v1","category":"astro-ph.HE"}
{"created":"2024-06-06 10:38:01","title":"Provably Neural Active Learning Succeeds via Prioritizing Perplexing Samples","abstract":"Neural Network-based active learning (NAL) is a cost-effective data selection technique that utilizes neural networks to select and train on a small subset of samples. While existing work successfully develops various effective or theory-justified NAL algorithms, the understanding of the two commonly used query criteria of NAL: uncertainty-based and diversity-based, remains in its infancy. In this work, we try to move one step forward by offering a unified explanation for the success of both query criteria-based NAL from a feature learning view. Specifically, we consider a feature-noise data model comprising easy-to-learn or hard-to-learn features disrupted by noise, and conduct analysis over 2-layer NN-based NALs in the pool-based scenario. We provably show that both uncertainty-based and diversity-based NAL are inherently amenable to one and the same principle, i.e., striving to prioritize samples that contain yet-to-be-learned features. We further prove that this shared principle is the key to their success-achieve small test error within a small labeled set. Contrastingly, the strategy-free passive learning exhibits a large test error due to the inadequate learning of yet-to-be-learned features, necessitating resort to a significantly larger label complexity for a sufficient test error reduction. Experimental results validate our findings.","sentences":["Neural Network-based active learning (NAL) is a cost-effective data selection technique that utilizes neural networks to select and train on a small subset of samples.","While existing work successfully develops various effective or theory-justified NAL algorithms, the understanding of the two commonly used query criteria of NAL: uncertainty-based and diversity-based, remains in its infancy.","In this work, we try to move one step forward by offering a unified explanation for the success of both query criteria-based NAL from a feature learning view.","Specifically, we consider a feature-noise data model comprising easy-to-learn or hard-to-learn features disrupted by noise, and conduct analysis over 2-layer NN-based NALs in the pool-based scenario.","We provably show that both uncertainty-based and diversity-based NAL are inherently amenable to one and the same principle, i.e., striving to prioritize samples that contain yet-to-be-learned features.","We further prove that this shared principle is the key to their success-achieve small test error within a small labeled set.","Contrastingly, the strategy-free passive learning exhibits a large test error due to the inadequate learning of yet-to-be-learned features, necessitating resort to a significantly larger label complexity for a sufficient test error reduction.","Experimental results validate our findings."],"url":"http://arxiv.org/abs/2406.03944v1","category":"cs.LG"}
{"created":"2024-06-06 10:21:23","title":"Revisiting gravitational angular momentum and mass dipole losses in the eikonal framework","abstract":"We review the description of classical gravitational scatterings of two compact objects by means of the eikonal framework. This encodes via scattering amplitudes both the motion of the bodies and the gravitational-wave signals that such systems produce. As an application, we combine the next-to-leading post-Minkowskian (PM) waveform derived in the post-Newtonian (PN) limit with the 4PM static loss due to the linear memory effect to reproduce known results for the total angular mometum loss in the center-of-mass frame up to $\\mathcal O(G^4)$ and 2.5PN order. We also provide similar expressions for the change in the system's mass dipole, discussing the subtleties related to its sensitivity to the Coulombic components of the field and to the nonlinear memory effect.","sentences":["We review the description of classical gravitational scatterings of two compact objects by means of the eikonal framework.","This encodes via scattering amplitudes both the motion of the bodies and the gravitational-wave signals that such systems produce.","As an application, we combine the next-to-leading post-Minkowskian (PM) waveform derived in the post-Newtonian (PN) limit with the 4PM static loss due to the linear memory effect to reproduce known results for the total angular mometum loss in the center-of-mass frame up to $\\mathcal O(G^4)$ and 2.5PN order.","We also provide similar expressions for the change in the system's mass dipole, discussing the subtleties related to its sensitivity to the Coulombic components of the field and to the nonlinear memory effect."],"url":"http://arxiv.org/abs/2406.03937v1","category":"gr-qc"}
{"created":"2024-06-06 10:12:02","title":"Ringing Thick Braneworld with Finite Extra Dimension","abstract":"In this work, we investigate the quasinormal modes of the Poincar\\'e thick brane with a finite extra dimension. Unlike the case with an infinite extra dimension, the gravitational effective potential exhibits three distinct shapes within different ranges of the parameter $n$ in the warp factor: harmonic oscillator potential, P\\\"oschl-Teller potential, and volcano-like potential. We then study various types of perturbations in this system. Utilizing a combination of analytical, semi-analytical, and numerical methods, we obtain the quasinormal modes of the perturbed fields. Our findings reveal a set of discrete quasinormal modes for the thick brane, similar to those of black holes. Interestingly, when $n=1$, the quasinormal modes exhibit purely imaginary behavior. This study may provide a new way to detect the existence of extra dimensions.","sentences":["In this work, we investigate the quasinormal modes of the Poincar\\'e thick brane with a finite extra dimension.","Unlike the case with an infinite extra dimension, the gravitational effective potential exhibits three distinct shapes within different ranges of the parameter $n$ in the warp factor: harmonic oscillator potential, P\\\"oschl-Teller potential, and volcano-like potential.","We then study various types of perturbations in this system.","Utilizing a combination of analytical, semi-analytical, and numerical methods, we obtain the quasinormal modes of the perturbed fields.","Our findings reveal a set of discrete quasinormal modes for the thick brane, similar to those of black holes.","Interestingly, when $n=1$, the quasinormal modes exhibit purely imaginary behavior.","This study may provide a new way to detect the existence of extra dimensions."],"url":"http://arxiv.org/abs/2406.03929v1","category":"gr-qc"}
{"created":"2024-06-06 10:08:59","title":"Engineering open-shell extended edge states in chiral graphene nanoribbons on MgO","abstract":"Graphene nanostructures are a promising platform for engineering electronic states with tailored magnetic and quantum properties. Synthesis strategies on metallic substrates have made it possible to manufacture atomically precise nanographenes with controlled size, shape and edge geometry. In these nanographenes, finite spin magnetic moment can arise as a result of many-body interactions in molecular orbitals with $\\pi$-conjugated character and subject to strong spatial confinement, for example at the zig-zag edges. However, owing to the mixing of the molecular orbitals and metallic states from the catalysing substrate, most of their expected quantum phenomenology is severely hindered. The use of in-situ ultra-thin decoupling layers can impede nanographene-metal hybridization and facilitate the expression of predicted properties. Here we show that the edges of narrow chiral graphene nanoribbons (GNRs) over MgO monolayers on Ag(001) can host integer charge and spin-1/2 frontier states. The electron occupation varies with the GNR length, which alternates even or odd number of electrons, thus resulting correspondingly in a non-magnetic closed-shell state or an open-shell paramagnetic system. For the latter, we found the spectral fingerprint of a narrow Coulomb correlation gap. Charged states, up to 19 additional electrons, were identified by comparing mean-field Hubbard (MFH) simulations of the density of states with experimental maps of the discretized molecular orbitals acquired with a scanning tunnelling microscope (STM). In consideration of the length-dependent magnetic moment and the discrete nature of the electronic structure, we envisage that GNRs supported by thin insulating films can be used as tailor-made active elements in quantum sensing and quantum information processing.","sentences":["Graphene nanostructures are a promising platform for engineering electronic states with tailored magnetic and quantum properties.","Synthesis strategies on metallic substrates have made it possible to manufacture atomically precise nanographenes with controlled size, shape and edge geometry.","In these nanographenes, finite spin magnetic moment can arise as a result of many-body interactions in molecular orbitals with $\\pi$-conjugated character and subject to strong spatial confinement, for example at the zig-zag edges.","However, owing to the mixing of the molecular orbitals and metallic states from the catalysing substrate, most of their expected quantum phenomenology is severely hindered.","The use of in-situ ultra-thin decoupling layers can impede nanographene-metal hybridization and facilitate the expression of predicted properties.","Here we show that the edges of narrow chiral graphene nanoribbons (GNRs) over MgO monolayers on Ag(001) can host integer charge and spin-1/2 frontier states.","The electron occupation varies with the GNR length, which alternates even or odd number of electrons, thus resulting correspondingly in a non-magnetic closed-shell state or an open-shell paramagnetic system.","For the latter, we found the spectral fingerprint of a narrow Coulomb correlation gap.","Charged states, up to 19 additional electrons, were identified by comparing mean-field Hubbard (MFH) simulations of the density of states with experimental maps of the discretized molecular orbitals acquired with a scanning tunnelling microscope (STM).","In consideration of the length-dependent magnetic moment and the discrete nature of the electronic structure, we envisage that GNRs supported by thin insulating films can be used as tailor-made active elements in quantum sensing and quantum information processing."],"url":"http://arxiv.org/abs/2406.03927v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-06 10:08:31","title":"Equivariant vector bundles over the complex projective line","abstract":"Let $G$ be a finite abelian group acting faithfully on ${\\mathbb C}{\\mathbb P}^1$ via holomorphic automorphisms. In \\cite{DF2} the $G$--equivariant algebraic vector bundles on $G$--invariant affine open subsets of ${\\mathbb C}{\\mathbb P}^1$ were classified. We classify the $G$--equivariant algebraic vector bundles on ${\\mathbb C}{\\mathbb P}^1$.","sentences":["Let $G$ be a finite abelian group acting faithfully on ${\\mathbb C}{\\mathbb P}^1$ via holomorphic automorphisms.","In \\cite{DF2} the $G$--equivariant algebraic vector bundles on $G$--invariant affine open subsets of ${\\mathbb C}{\\mathbb P}^1$ were classified.","We classify the $G$--equivariant algebraic vector bundles on ${\\mathbb C}{\\mathbb P}^1$."],"url":"http://arxiv.org/abs/2406.03926v1","category":"math.AG"}
{"created":"2024-06-06 10:07:29","title":"Topological phases in discrete stochastic systems","abstract":"Topological invariants have proved useful for analyzing emergent function as they characterize a property of the entire system, and are insensitive to local details, disorder, and noise. They support edge states, which reduce the system response to a lower dimensional space and offer a mechanism for the emergence of global cycles within a large phase space. Topological invariants have been heavily studied in quantum electronic systems and been observed in other classical platforms such as mechanical lattices. However, this framework largely describes equilibrium systems within an ordered crystalline lattice, whereas biological systems are often strongly non-equilibrium with stochastic components. We review recent developments in topological states in discrete stochastic models in 1d and 2d systems, and initial progress in identifying testable signature of topological states in molecular systems and ecology. These models further provide simple principles for targeted dynamics in synthetic systems or in the engineering of reconfigurable materials. Lastly, we describe novel theoretical properties of these systems such as the necessity for non-Hermiticity in permitting edge states, as well as new analytical tools to reveal these properties. The emerging developments shed light on fundamental principles for non-equilibrium systems and topological protection enabling robust biological function.","sentences":["Topological invariants have proved useful for analyzing emergent function as they characterize a property of the entire system, and are insensitive to local details, disorder, and noise.","They support edge states, which reduce the system response to a lower dimensional space and offer a mechanism for the emergence of global cycles within a large phase space.","Topological invariants have been heavily studied in quantum electronic systems and been observed in other classical platforms such as mechanical lattices.","However, this framework largely describes equilibrium systems within an ordered crystalline lattice, whereas biological systems are often strongly non-equilibrium with stochastic components.","We review recent developments in topological states in discrete stochastic models in 1d and 2d systems, and initial progress in identifying testable signature of topological states in molecular systems and ecology.","These models further provide simple principles for targeted dynamics in synthetic systems or in the engineering of reconfigurable materials.","Lastly, we describe novel theoretical properties of these systems such as the necessity for non-Hermiticity in permitting edge states, as well as new analytical tools to reveal these properties.","The emerging developments shed light on fundamental principles for non-equilibrium systems and topological protection enabling robust biological function."],"url":"http://arxiv.org/abs/2406.03925v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-06 10:04:10","title":"Engineering Semi-streaming DFS algorithms","abstract":"Depth first search is a fundamental graph problem having a wide range of applications. For a graph $G=(V,E)$ having $n$ vertices and $m$ edges, the DFS tree can be computed in $O(m+n)$ using $O(m)$ space where $m=O(n^2)$. In the streaming environment, most graph problems are studied in the semi-streaming model where several passes (preferably one) are allowed over the input, allowing $O(nk)$ local space for some $k=o(n)$. Trivially, using $O(m)$ space, DFS can be computed in one pass, and using $O(n)$ space, it can be computed in $O(n)$ passes.   Khan and Mehta [STACS19] presented several algorithms allowing trade-offs between space and passes, where $O(nk)$ space results in $O(n/k)$ passes. They also empirically analyzed their algorithm to require only a few passes in practice for even $O(n)$ space. Chang et al. [STACS20] presented an alternate proof for the same and also presented $O(\\sqrt{n})$ pass algorithm requiring $O(n~poly\\log n)$ space with a finer trade-off between space and passes. However, their algorithm uses complex black box algorithms, making it impractical.   We perform an experimental analysis of the practical semi-streaming DFS algorithms. Our analysis ranges from real graphs to random graphs (uniform and power-law). We also present several heuristics to improve the state-of-the-art algorithms and study their impact. Our heuristics improve state of the art by $40-90\\%$, achieving optimal one pass in almost $40-50\\%$ cases (improved from zero). In random graphs, they improve from $30-90\\%$, again requiring optimal one pass for even very small values of $k$. Overall, our heuristics improved the relatively complex state-of-the-art algorithm significantly, requiring merely two passes in the worst case for random graphs. Additionally, our heuristics made the relatively simpler algorithm practically usable even for very small space bounds, which was impractical earlier.","sentences":["Depth first search is a fundamental graph problem having a wide range of applications.","For a graph $G=(V,E)$ having $n$ vertices and $m$ edges, the DFS tree can be computed in $O(m+n)$ using $O(m)$ space where $m=O(n^2)$.","In the streaming environment, most graph problems are studied in the semi-streaming model where several passes (preferably one) are allowed over the input, allowing $O(nk)$ local space for some $k=o(n)$. Trivially, using $O(m)$ space, DFS can be computed in one pass, and using $O(n)$ space, it can be computed in $O(n)$ passes.   ","Khan and Mehta [STACS19] presented several algorithms allowing trade-offs between space and passes, where $O(nk)$ space results in $O(n/k)$ passes.","They also empirically analyzed their algorithm to require only a few passes in practice for even $O(n)$ space.","Chang et al.","[STACS20] presented an alternate proof for the same and also presented $O(\\sqrt{n})$ pass algorithm requiring $O(n~poly\\log n)$ space with a finer trade-off between space and passes.","However, their algorithm uses complex black box algorithms, making it impractical.   ","We perform an experimental analysis of the practical semi-streaming DFS algorithms.","Our analysis ranges from real graphs to random graphs (uniform and power-law).","We also present several heuristics to improve the state-of-the-art algorithms and study their impact.","Our heuristics improve state of the art by $40-90\\%$, achieving optimal one pass in almost $40-50\\%$ cases (improved from zero).","In random graphs, they improve from $30-90\\%$, again requiring optimal one pass for even very small values of $k$. Overall, our heuristics improved the relatively complex state-of-the-art algorithm significantly, requiring merely two passes in the worst case for random graphs.","Additionally, our heuristics made the relatively simpler algorithm practically usable even for very small space bounds, which was impractical earlier."],"url":"http://arxiv.org/abs/2406.03922v1","category":"cs.DS"}
{"created":"2024-06-06 10:02:49","title":"Towards Physically Consistent Deep Learning For Climate Model Parameterizations","abstract":"Climate models play a critical role in understanding and projecting climate change. Due to their complexity, their horizontal resolution of ~40-100 km remains too coarse to resolve processes such as clouds and convection, which need to be approximated via parameterizations. These parameterizations are a major source of systematic errors and large uncertainties in climate projections. Deep learning (DL)-based parameterizations, trained on computationally expensive, short high-resolution simulations, have shown great promise for improving climate models in that regard. However, their lack of interpretability and tendency to learn spurious non-physical correlations result in reduced trust in the climate simulation. We propose an efficient supervised learning framework for DL-based parameterizations that leads to physically consistent models with improved interpretability and negligible computational overhead compared to standard supervised training. First, key features determining the target physical processes are uncovered. Subsequently, the neural network is fine-tuned using only those relevant features. We show empirically that our method robustly identifies a small subset of the inputs as actual physical drivers, therefore, removing spurious non-physical relationships. This results in by design physically consistent and interpretable neural networks while maintaining the predictive performance of standard black-box DL-based parameterizations. Our framework represents a crucial step in addressing a major challenge in data-driven climate model parameterizations by respecting the underlying physical processes, and may also benefit physically consistent deep learning in other research fields.","sentences":["Climate models play a critical role in understanding and projecting climate change.","Due to their complexity, their horizontal resolution of ~40-100 km remains too coarse to resolve processes such as clouds and convection, which need to be approximated via parameterizations.","These parameterizations are a major source of systematic errors and large uncertainties in climate projections.","Deep learning (DL)-based parameterizations, trained on computationally expensive, short high-resolution simulations, have shown great promise for improving climate models in that regard.","However, their lack of interpretability and tendency to learn spurious non-physical correlations result in reduced trust in the climate simulation.","We propose an efficient supervised learning framework for DL-based parameterizations that leads to physically consistent models with improved interpretability and negligible computational overhead compared to standard supervised training.","First, key features determining the target physical processes are uncovered.","Subsequently, the neural network is fine-tuned using only those relevant features.","We show empirically that our method robustly identifies a small subset of the inputs as actual physical drivers, therefore, removing spurious non-physical relationships.","This results in by design physically consistent and interpretable neural networks while maintaining the predictive performance of standard black-box DL-based parameterizations.","Our framework represents a crucial step in addressing a major challenge in data-driven climate model parameterizations by respecting the underlying physical processes, and may also benefit physically consistent deep learning in other research fields."],"url":"http://arxiv.org/abs/2406.03920v1","category":"cs.LG"}
{"created":"2024-06-06 09:58:35","title":"The \u03b1-Lomax Distribution: A Compound Channel Model","abstract":"In this paper, we propose the {\\alpha}-Lomax distribution as a new compound fading channel model. This new distribution generalizes the recently introduced Lomax fading channel model. It is worth noting that the Lomax distribution is a decreasing function, while the {\\alpha}-Lomax is a unimodal function, offering greater flexibility in modeling wireless fading channels. In particular, we derive closed-form expressions for the probability density function and cumulative distribution function for the instantaneous signal-to-noise ratio (SNR). Additionally, we provide closed-form expressions for several fundamental performance metrics, including outage probability, average bit error rate, and channel capacity. Furthermore, we derive closed-form expression for the average block-length error rate in short-packet communications. Moreover, we fit the PDF of the proposed channel model to empirical data obtained from a device-to-device communication system. We also offer simple and accurate approximations for these expressions in the high SNR regime.","sentences":["In this paper, we propose the {\\alpha}-Lomax distribution as a new compound fading channel model.","This new distribution generalizes the recently introduced Lomax fading channel model.","It is worth noting that the Lomax distribution is a decreasing function, while the {\\alpha}-Lomax is a unimodal function, offering greater flexibility in modeling wireless fading channels.","In particular, we derive closed-form expressions for the probability density function and cumulative distribution function for the instantaneous signal-to-noise ratio (SNR).","Additionally, we provide closed-form expressions for several fundamental performance metrics, including outage probability, average bit error rate, and channel capacity.","Furthermore, we derive closed-form expression for the average block-length error rate in short-packet communications.","Moreover, we fit the PDF of the proposed channel model to empirical data obtained from a device-to-device communication system.","We also offer simple and accurate approximations for these expressions in the high SNR regime."],"url":"http://arxiv.org/abs/2406.03918v1","category":"cs.IT"}
{"created":"2024-06-06 09:57:56","title":"Frequency-based Matcher for Long-tailed Semantic Segmentation","abstract":"The successful application of semantic segmentation technology in the real world has been among the most exciting achievements in the computer vision community over the past decade. Although the long-tailed phenomenon has been investigated in many fields, e.g., classification and object detection, it has not received enough attention in semantic segmentation and has become a non-negligible obstacle to applying semantic segmentation technology in autonomous driving and virtual reality. Therefore, in this work, we focus on a relatively under-explored task setting, long-tailed semantic segmentation (LTSS). We first establish three representative datasets from different aspects, i.e., scene, object, and human. We further propose a dual-metric evaluation system and construct the LTSS benchmark to demonstrate the performance of semantic segmentation methods and long-tailed solutions. We also propose a transformer-based algorithm to improve LTSS, frequency-based matcher, which solves the oversuppression problem by one-to-many matching and automatically determines the number of matching queries for each class. Given the comprehensiveness of this work and the importance of the issues revealed, this work aims to promote the empirical study of semantic segmentation tasks. Our datasets, codes, and models will be publicly available.","sentences":["The successful application of semantic segmentation technology in the real world has been among the most exciting achievements in the computer vision community over the past decade.","Although the long-tailed phenomenon has been investigated in many fields, e.g., classification and object detection, it has not received enough attention in semantic segmentation and has become a non-negligible obstacle to applying semantic segmentation technology in autonomous driving and virtual reality.","Therefore, in this work, we focus on a relatively under-explored task setting, long-tailed semantic segmentation (LTSS).","We first establish three representative datasets from different aspects, i.e., scene, object, and human.","We further propose a dual-metric evaluation system and construct the LTSS benchmark to demonstrate the performance of semantic segmentation methods and long-tailed solutions.","We also propose a transformer-based algorithm to improve LTSS, frequency-based matcher, which solves the oversuppression problem by one-to-many matching and automatically determines the number of matching queries for each class.","Given the comprehensiveness of this work and the importance of the issues revealed, this work aims to promote the empirical study of semantic segmentation tasks.","Our datasets, codes, and models will be publicly available."],"url":"http://arxiv.org/abs/2406.03917v1","category":"cs.CV"}
{"created":"2024-06-06 09:52:22","title":"Recognizing weighted means in geodesic spaces","abstract":"Geodesic metric spaces support a variety of averaging constructions for given finite sets. Computing such averages has generated extensive interest in diverse disciplines. Here we consider the inverse problem of recognizing computationally whether or not a given point is such an average, exactly or approximately. In nonpositively curved spaces, several averaging notions, including the usual weighted barycenter, produce the same \"mean set\". In such spaces, at points where the tangent cone is a Euclidean space, the recognition problem reduces to Euclidean projection onto a polytope. Hadamard manifolds comprise one example. Another consists of CAT(0) cubical complexes, at relative-interior points: the recognition problem is harder for general points, but we present an efficient semidefinite-programming-based algorithm.","sentences":["Geodesic metric spaces support a variety of averaging constructions for given finite sets.","Computing such averages has generated extensive interest in diverse disciplines.","Here we consider the inverse problem of recognizing computationally whether or not a given point is such an average, exactly or approximately.","In nonpositively curved spaces, several averaging notions, including the usual weighted barycenter, produce the same \"mean set\".","In such spaces, at points where the tangent cone is a Euclidean space, the recognition problem reduces to Euclidean projection onto a polytope.","Hadamard manifolds comprise one example.","Another consists of CAT(0) cubical complexes, at relative-interior points: the recognition problem is harder for general points, but we present an efficient semidefinite-programming-based algorithm."],"url":"http://arxiv.org/abs/2406.03913v1","category":"math.OC"}
{"created":"2024-06-06 09:40:57","title":"Megastable quantization in self-excited systems","abstract":"A classical particle in a confining potential gives rise to a Hamiltonian conservative dynamical system with an uncountably infinite continuous energy spectra, whereas the corresponding quantum particle exhibits countably infinite discrete energy levels. We consider a class of nonlinear self-sustained oscillators describing a classical active particle in a harmonic potential. These nonlinear oscillators emerge in the low-memory regime of both state-dependent time-delay systems as well as in non-Markovian stroboscopic models of walking droplets. Using averaging techniques, we prove the existence of a countably infinite number of asymptotically stable quantized orbits, i.e. megastability, for this class of self-excited systems. The set of periodic orbits consists of a sequence of nested limit-cycle attractors with quasilinear increasing amplitude and alternating stability, yielding smooth basins of attraction. By using the Lyapunov energy function, we estimate the energy spectra of this megastable set of orbits, and perform numerical simulations to confirm the mathematical analysis. Our formalism can be extended to self-excited particles in general confining potentials, resulting in different energy-frequency relations for these dynamical analogs of quantization.","sentences":["A classical particle in a confining potential gives rise to a Hamiltonian conservative dynamical system with an uncountably infinite continuous energy spectra, whereas the corresponding quantum particle exhibits countably infinite discrete energy levels.","We consider a class of nonlinear self-sustained oscillators describing a classical active particle in a harmonic potential.","These nonlinear oscillators emerge in the low-memory regime of both state-dependent time-delay systems as well as in non-Markovian stroboscopic models of walking droplets.","Using averaging techniques, we prove the existence of a countably infinite number of asymptotically stable quantized orbits, i.e. megastability, for this class of self-excited systems.","The set of periodic orbits consists of a sequence of nested limit-cycle attractors with quasilinear increasing amplitude and alternating stability, yielding smooth basins of attraction.","By using the Lyapunov energy function, we estimate the energy spectra of this megastable set of orbits, and perform numerical simulations to confirm the mathematical analysis.","Our formalism can be extended to self-excited particles in general confining potentials, resulting in different energy-frequency relations for these dynamical analogs of quantization."],"url":"http://arxiv.org/abs/2406.03906v1","category":"quant-ph"}
{"created":"2024-06-06 09:37:22","title":"Enhanced variable selection for boosting sparser and less complex models in distributional copula regression","abstract":"Structured additive distributional copula regression allows to model the joint distribution of multivariate outcomes by relating all distribution parameters to covariates. Estimation via statistical boosting enables accounting for high-dimensional data and incorporating data-driven variable selection, both of which are useful given the complexity of the model class. However, as known from univariate (distributional) regression, the standard boosting algorithm tends to select too many variables with minor importance, particularly in settings with large sample sizes, leading to complex models with difficult interpretation. To counteract this behavior and to avoid selecting base-learners with only a negligible impact, we combined the ideas of probing, stability selection and a new deselection approach with statistical boosting for distributional copula regression. In a simulation study and an application to the joint modelling of weight and length of newborns, we found that all proposed methods enhance variable selection by reducing the number of false positives. However, only stability selection and the deselection approach yielded similar predictive performance to classical boosting. Finally, the deselection approach is better scalable to larger datasets and led to a competitive predictive performance, which we further illustrated in a genomic cohort study from the UK Biobank by modelling the joint genetic predisposition for two phenotypes.","sentences":["Structured additive distributional copula regression allows to model the joint distribution of multivariate outcomes by relating all distribution parameters to covariates.","Estimation via statistical boosting enables accounting for high-dimensional data and incorporating data-driven variable selection, both of which are useful given the complexity of the model class.","However, as known from univariate (distributional) regression, the standard boosting algorithm tends to select too many variables with minor importance, particularly in settings with large sample sizes, leading to complex models with difficult interpretation.","To counteract this behavior and to avoid selecting base-learners with only a negligible impact, we combined the ideas of probing, stability selection and a new deselection approach with statistical boosting for distributional copula regression.","In a simulation study and an application to the joint modelling of weight and length of newborns, we found that all proposed methods enhance variable selection by reducing the number of false positives.","However, only stability selection and the deselection approach yielded similar predictive performance to classical boosting.","Finally, the deselection approach is better scalable to larger datasets and led to a competitive predictive performance, which we further illustrated in a genomic cohort study from the UK Biobank by modelling the joint genetic predisposition for two phenotypes."],"url":"http://arxiv.org/abs/2406.03900v1","category":"stat.ME"}
{"created":"2024-06-06 09:36:38","title":"PLDNet: PLD-Guided Lightweight Deep Network Boosted by Efficient Attention for Handheld Dual-Microphone Speech Enhancement","abstract":"Low-complexity speech enhancement on mobile phones is crucial in the era of 5G. Thus, focusing on handheld mobile phone communication scenario, based on power level difference (PLD) algorithm and lightweight U-Net, we propose PLD-guided lightweight deep network (PLDNet), an extremely lightweight dual-microphone speech enhancement method that integrates the guidance of signal processing algorithm and lightweight attention-augmented U-Net. For the guidance information, we employ PLD algorithm to pre-process dual-microphone spectrum, and feed the output into subsequent deep neural network, which utilizes a lightweight U-Net with our proposed gated convolution augmented frequency attention (GCAFA) module to extract desired clean speech. Experimental results demonstrate that our proposed method achieves competitive performance with recent top-performing models while reducing computational cost by over 90%, highlighting the potential for low-complexity speech enhancement on mobile phones.","sentences":["Low-complexity speech enhancement on mobile phones is crucial in the era of 5G.","Thus, focusing on handheld mobile phone communication scenario, based on power level difference (PLD) algorithm and lightweight U-Net, we propose PLD-guided lightweight deep network (PLDNet), an extremely lightweight dual-microphone speech enhancement method that integrates the guidance of signal processing algorithm and lightweight attention-augmented U-Net.","For the guidance information, we employ PLD algorithm to pre-process dual-microphone spectrum, and feed the output into subsequent deep neural network, which utilizes a lightweight U-Net with our proposed gated convolution augmented frequency attention (GCAFA) module to extract desired clean speech.","Experimental results demonstrate that our proposed method achieves competitive performance with recent top-performing models while reducing computational cost by over 90%, highlighting the potential for low-complexity speech enhancement on mobile phones."],"url":"http://arxiv.org/abs/2406.03899v1","category":"eess.AS"}
{"created":"2024-06-06 09:36:33","title":"Informed Graph Learning By Domain Knowledge Injection and Smooth Graph Signal Representation","abstract":"Graph signal processing represents an important advancement in the field of data analysis, extending conventional signal processing methodologies to complex networks and thereby facilitating the exploration of informative patterns and structures across various domains. However, acquiring the underlying graphs for specific applications remains a challenging task. While graph inference based on smooth graph signal representation has become one of the state-of-the-art methods, these approaches usually overlook the unique properties of networks, which are generally derived from domain-specific knowledge. Overlooking this information could make the approaches less interpretable and less effective overall. In this study, we propose a new graph inference method that leverages available domain knowledge. The proposed methodology is evaluated on the task of denoising and imputing missing sensor data, utilizing graph signal reconstruction techniques. The results demonstrate that incorporating domain knowledge into the graph inference process can improve graph signal reconstruction in district heating networks. Our code is available at \\href{https://github.com/Keiv4n/IGL}{github.com/Keiv4n/IGL}.","sentences":["Graph signal processing represents an important advancement in the field of data analysis, extending conventional signal processing methodologies to complex networks and thereby facilitating the exploration of informative patterns and structures across various domains.","However, acquiring the underlying graphs for specific applications remains a challenging task.","While graph inference based on smooth graph signal representation has become one of the state-of-the-art methods, these approaches usually overlook the unique properties of networks, which are generally derived from domain-specific knowledge.","Overlooking this information could make the approaches less interpretable and less effective overall.","In this study, we propose a new graph inference method that leverages available domain knowledge.","The proposed methodology is evaluated on the task of denoising and imputing missing sensor data, utilizing graph signal reconstruction techniques.","The results demonstrate that incorporating domain knowledge into the graph inference process can improve graph signal reconstruction in district heating networks.","Our code is available at \\href{https://github.com/Keiv4n/IGL}{github.com/Keiv4n/IGL}."],"url":"http://arxiv.org/abs/2406.03898v1","category":"eess.SP"}
{"created":"2024-06-06 09:36:05","title":"Data-driven discovery of self-similarity using neural networks","abstract":"Finding self-similarity is a key step for understanding the governing law behind complex physical phenomena. Traditional methods for identifying self-similarity often rely on specific models, which can introduce significant bias. In this paper, we present a novel neural network-based approach that discovers self-similarity directly from observed data, without presupposing any models. The presence of self-similar solutions in a physical problem signals that the governing law contains a function whose arguments are given by power-law monomials of physical parameters, which are characterized by power-law exponents. The basic idea is to enforce such particular forms structurally in a neural network in a parametrized way. We train the neural network model using the observed data, and when the training is successful, we can extract the power exponents that characterize scale-transformation symmetries of the physical problem. We demonstrate the effectiveness of our method with both synthetic and experimental data, validating its potential as a robust, model-independent tool for exploring self-similarity in complex systems.","sentences":["Finding self-similarity is a key step for understanding the governing law behind complex physical phenomena.","Traditional methods for identifying self-similarity often rely on specific models, which can introduce significant bias.","In this paper, we present a novel neural network-based approach that discovers self-similarity directly from observed data, without presupposing any models.","The presence of self-similar solutions in a physical problem signals that the governing law contains a function whose arguments are given by power-law monomials of physical parameters, which are characterized by power-law exponents.","The basic idea is to enforce such particular forms structurally in a neural network in a parametrized way.","We train the neural network model using the observed data, and when the training is successful, we can extract the power exponents that characterize scale-transformation symmetries of the physical problem.","We demonstrate the effectiveness of our method with both synthetic and experimental data, validating its potential as a robust, model-independent tool for exploring self-similarity in complex systems."],"url":"http://arxiv.org/abs/2406.03896v1","category":"cond-mat.soft"}
{"created":"2024-06-06 09:26:48","title":"Polyhedral Conic Classifier for CTR Prediction","abstract":"This paper introduces a novel approach for click-through rate (CTR) prediction within industrial recommender systems, addressing the inherent challenges of numerical imbalance and geometric asymmetry. These challenges stem from imbalanced datasets, where positive (click) instances occur less frequently than negatives (non-clicks), and geometrically asymmetric distributions, where positive samples exhibit visually coherent patterns while negatives demonstrate greater diversity. To address these challenges, we have used a deep neural network classifier that uses the polyhedral conic functions. This classifier is similar to the one-class classifiers in spirit and it returns compact polyhedral acceptance regions to separate the positive class samples from the negative samples that have diverse distributions. Extensive experiments have been conducted to test the proposed approach using state-of-the-art (SOTA) CTR prediction models on four public datasets, namely Criteo, Avazu, MovieLens and Frappe. The experimental evaluations highlight the superiority of our proposed approach over Binary Cross Entropy (BCE) Loss, which is widely used in CTR prediction tasks.","sentences":["This paper introduces a novel approach for click-through rate (CTR) prediction within industrial recommender systems, addressing the inherent challenges of numerical imbalance and geometric asymmetry.","These challenges stem from imbalanced datasets, where positive (click) instances occur less frequently than negatives (non-clicks), and geometrically asymmetric distributions, where positive samples exhibit visually coherent patterns while negatives demonstrate greater diversity.","To address these challenges, we have used a deep neural network classifier that uses the polyhedral conic functions.","This classifier is similar to the one-class classifiers in spirit and it returns compact polyhedral acceptance regions to separate the positive class samples from the negative samples that have diverse distributions.","Extensive experiments have been conducted to test the proposed approach using state-of-the-art (SOTA) CTR prediction models on four public datasets, namely Criteo, Avazu, MovieLens and Frappe.","The experimental evaluations highlight the superiority of our proposed approach over Binary Cross Entropy (BCE) Loss, which is widely used in CTR prediction tasks."],"url":"http://arxiv.org/abs/2406.03892v1","category":"cs.IR"}
{"created":"2024-06-06 09:24:42","title":"MSE-Based Training and Transmission Optimization for MIMO ISAC Systems","abstract":"In this paper, we investigate a multiple-input multiple-output (MIMO) integrated sensing and communication (ISAC) system under typical block-fading channels. As a non-trivial extension to most existing works on ISAC, both the training and transmission signals sent by the ISAC transmitter are exploited for sensing. Specifically, we develop two training and transmission design schemes to minimize a weighted sum of the mean-squared errors (MSEs) of data transmission and radar target response matrix (TRM) estimation. For the former, we first optimize the training signal for simultaneous communication channel and radar TRM estimation. Then, based on the estimated instantaneous channel state information (CSI), we propose an efficient majorization-minimization (MM)-based robust ISAC transmission design, where a semi-closed form solution is obtained in each iteration. For the second scheme, the ISAC transmitter is assumed to have statistical CSI only for reducing the feedback overhead. With CSI statistics available, we integrate the training and transmission design into one single problem and propose an MM-based alternating algorithm to find a high-quality solution. In addition, we provide alternative structured and low-complexity solutions for both schemes under certain special cases. Finally, simulation results demonstrate that the radar performance is significantly improved compared to the existing scheme that integrates sensing into the transmission stage only. Moreover, it is verified that the investigated two schemes have advantages in terms of communication and sensing performances, respectively.","sentences":["In this paper, we investigate a multiple-input multiple-output (MIMO) integrated sensing and communication (ISAC) system under typical block-fading channels.","As a non-trivial extension to most existing works on ISAC, both the training and transmission signals sent by the ISAC transmitter are exploited for sensing.","Specifically, we develop two training and transmission design schemes to minimize a weighted sum of the mean-squared errors (MSEs) of data transmission and radar target response matrix (TRM) estimation.","For the former, we first optimize the training signal for simultaneous communication channel and radar TRM estimation.","Then, based on the estimated instantaneous channel state information (CSI), we propose an efficient majorization-minimization (MM)-based robust ISAC transmission design, where a semi-closed form solution is obtained in each iteration.","For the second scheme, the ISAC transmitter is assumed to have statistical CSI only for reducing the feedback overhead.","With CSI statistics available, we integrate the training and transmission design into one single problem and propose an MM-based alternating algorithm to find a high-quality solution.","In addition, we provide alternative structured and low-complexity solutions for both schemes under certain special cases.","Finally, simulation results demonstrate that the radar performance is significantly improved compared to the existing scheme that integrates sensing into the transmission stage only.","Moreover, it is verified that the investigated two schemes have advantages in terms of communication and sensing performances, respectively."],"url":"http://arxiv.org/abs/2406.03888v1","category":"cs.IT"}
{"created":"2024-06-06 09:24:37","title":"UOCS. XIV. Uncovering extremely low mass white dwarfs and blue lurkers in NGC 752","abstract":"Evolutionary pathways of binary systems are vastly different from single stellar evolution, and thus, there is a need to quantify their frequency and diversity. Open clusters are the best test-bed to unveil the secrets of binary populations due to their coeval nature. And the availability of multi-wavelength data in recent years has been critical in characterising the binary population. NGC 752 is a solar metallicity, intermediate-age open cluster located at 460 pc. In this work, we aim to identify the optically subluminous white dwarfs in NGC 752 and identify the illusive blue lurkers by association. We used multiwavelength photometry from Astrosat/UVIT, swift/UVOT, Gaia DR3 and other archival surveys to analyse the colour-magnitude diagrams and spectral energy distributions of 37 cluster members. We detected eight white dwarfs as companions to cluster members. Four of the systems are main sequence stars with extremely low mass white dwarfs as their companions. Two are these main sequence stars are also fast rotators. The presence of low mass white dwarfs and high rotation signals a past mass transfer, and we classified the four main sequence stars as blue lurkers. The binary fraction in NGC 752 was estimated to be 50--70%, and it shows that the contribution of optically undetected stars is crucial in quantifying the present-day binary fraction.","sentences":["Evolutionary pathways of binary systems are vastly different from single stellar evolution, and thus, there is a need to quantify their frequency and diversity.","Open clusters are the best test-bed to unveil the secrets of binary populations due to their coeval nature.","And the availability of multi-wavelength data in recent years has been critical in characterising the binary population.","NGC 752 is a solar metallicity, intermediate-age open cluster located at 460 pc.","In this work, we aim to identify the optically subluminous white dwarfs in NGC 752 and identify the illusive blue lurkers by association.","We used multiwavelength photometry from Astrosat/UVIT, swift/UVOT, Gaia DR3 and other archival surveys to analyse the colour-magnitude diagrams and spectral energy distributions of 37 cluster members.","We detected eight white dwarfs as companions to cluster members.","Four of the systems are main sequence stars with extremely low mass white dwarfs as their companions.","Two are these main sequence stars are also fast rotators.","The presence of low mass white dwarfs and high rotation signals a past mass transfer, and we classified the four main sequence stars as blue lurkers.","The binary fraction in NGC 752 was estimated to be 50--70%, and it shows that the contribution of optically undetected stars is crucial in quantifying the present-day binary fraction."],"url":"http://arxiv.org/abs/2406.03887v1","category":"astro-ph.SR"}
{"created":"2024-06-06 09:24:21","title":"BiomedBench: A benchmark suite of TinyML biomedical applications for low-power wearables","abstract":"The design of low-power wearables for the biomedical domain has received a lot of attention in recent decades, as technological advances in chip manufacturing have allowed real-time monitoring of patients using low-complexity ML within the mW range. Despite advances in application and hardware design research, the domain lacks a systematic approach to hardware evaluation. In this work, we propose BiomedBench, a new benchmark suite composed of complete end-to-end TinyML biomedical applications for real-time monitoring of patients using wearable devices. Each application presents different requirements during typical signal acquisition and processing phases, including varying computational workloads and relations between active and idle times. Furthermore, our evaluation of five state-of-the-art low-power platforms in terms of energy efficiency shows that modern platforms cannot effectively target all types of biomedical applications. BiomedBench will be released as an open-source suite to enable future improvements in the entire domain of bioengineering systems and TinyML application design.","sentences":["The design of low-power wearables for the biomedical domain has received a lot of attention in recent decades, as technological advances in chip manufacturing have allowed real-time monitoring of patients using low-complexity ML within the mW range.","Despite advances in application and hardware design research, the domain lacks a systematic approach to hardware evaluation.","In this work, we propose BiomedBench, a new benchmark suite composed of complete end-to-end TinyML biomedical applications for real-time monitoring of patients using wearable devices.","Each application presents different requirements during typical signal acquisition and processing phases, including varying computational workloads and relations between active and idle times.","Furthermore, our evaluation of five state-of-the-art low-power platforms in terms of energy efficiency shows that modern platforms cannot effectively target all types of biomedical applications.","BiomedBench will be released as an open-source suite to enable future improvements in the entire domain of bioengineering systems and TinyML application design."],"url":"http://arxiv.org/abs/2406.03886v1","category":"cs.LG"}
{"created":"2024-06-06 09:23:55","title":"Convergence of a Riemannian gradient method for the Gross-Pitaevskii energy functional in a rotating frame","abstract":"This paper investigates the numerical approximation of ground states of rotating Bose-Einstein condensates. This problem requires the minimization of the Gross-Pitaevskii energy $E$ on a Riemannian manifold $\\mathbb{S}$. To find a corresponding minimizer $u$, we use a generalized Riemannian gradient method that is based on the concept of Sobolev gradients in combination with an adaptively changing metric on the manifold. By a suitable choice of the metric, global energy dissipation for the arising gradient method can be proved. The energy dissipation property in turn implies global convergence to the density $|u|^2$ of a critical point $u$ of $E$ on $\\mathbb{S}$. Furthermore, we present a precise characterization of the local convergence rates in a neighborhood of each ground state $u$ and how these rates depend on the first spectral gap of $E^{\\prime\\prime}(u)$ restricted to the $L^2$-orthogonal complement of $u$. With this we establish the first convergence results for a Riemannian gradient method to minimize the Gross-Pitaevskii energy functional in a rotating frame. At the same, we refine previous results obtained in the case without rotation. The major complication in our new analysis is the missing isolation of minimizers, which are at most unique up to complex phase shifts. For that, we introduce an auxiliary iteration in the tangent space $T_{\\mathrm{i} u} \\mathbb{S}$ and apply the Ostrowski theorem to characterize the asymptotic convergence rates through a weighted eigenvalue problem. Afterwards, we link the auxiliary iteration to the original Riemannian gradient method and bound the spectrum of the weighted eigenvalue problem to obtain quantitative convergence rates. Our findings are validated in numerical experiments.","sentences":["This paper investigates the numerical approximation of ground states of rotating Bose-Einstein condensates.","This problem requires the minimization of the Gross-Pitaevskii energy $E$ on a Riemannian manifold $\\mathbb{S}$. To find a corresponding minimizer $u$, we use a generalized Riemannian gradient method that is based on the concept of Sobolev gradients in combination with an adaptively changing metric on the manifold.","By a suitable choice of the metric, global energy dissipation for the arising gradient method can be proved.","The energy dissipation property in turn implies global convergence to the density $|u|^2$ of a critical point $u$ of $E$ on $\\mathbb{S}$. Furthermore, we present a precise characterization of the local convergence rates in a neighborhood of each ground state $u$ and how these rates depend on the first spectral gap of $E^{\\prime\\prime}(u)$ restricted to the $L^2$-orthogonal complement of $u$. With this we establish the first convergence results for a Riemannian gradient method to minimize the Gross-Pitaevskii energy functional in a rotating frame.","At the same, we refine previous results obtained in the case without rotation.","The major complication in our new analysis is the missing isolation of minimizers, which are at most unique up to complex phase shifts.","For that, we introduce an auxiliary iteration in the tangent space $T_{\\mathrm{i} u} \\mathbb{S}$ and apply the Ostrowski theorem to characterize the asymptotic convergence rates through a weighted eigenvalue problem.","Afterwards, we link the auxiliary iteration to the original Riemannian gradient method and bound the spectrum of the weighted eigenvalue problem to obtain quantitative convergence rates.","Our findings are validated in numerical experiments."],"url":"http://arxiv.org/abs/2406.03885v1","category":"math.NA"}
{"created":"2024-06-06 09:23:20","title":"Steady supersonic combustion flows with a contact discontinuity in two-dimensional finitely long nozzles","abstract":"In this paper, we are concerned with the two-dimensional steady supersonic combustion flows with a contact discontinuity moving through a nozzle of finite length. Mathematically, it can be formulated as a free boundary value problem governed by the two -dimensional steady combustion Euler equations with a contact discontinuity as the free boundary. The main mathematical difficulties are that the contact discontinuity is a characteristic free boundary and the equations for all states are coupled with each other due to the combustion process. We first employ the Lagrangian coordinate transformation to fix the free boundary. Then by introducing the flow slope and Bernoulli function, we further reduce the fixed boundary value problem into an initial boundary value problem for a first order hyperbolic system coupled with several ordinary differential equations. A new iteration scheme is developed near the background states by employing the intrinsic structure of the equation for the mass fraction of the non-combustion gas. We show that there is a fixed point for the iteration by deriving some novel $C^{1,\\alpha}$-estimates of the solutions and applying the fixed point theorem, and then the uniqueness of the fixed point is proved by a contraction argument. On the other hand, a quasi-one-dimensional approximate system is often used to simplify the two-dimensional steady supersonic combustion model. The error between these two systems is estimated. Finally, given a piece-wise $C^{1,\\alpha}$-solution containing a contact discontinuity with piece-wise constant states on the entrance of the nozzle, we can show that the solution is the piece-wise constant states with a straight contact discontinuity.","sentences":["In this paper, we are concerned with the two-dimensional steady supersonic combustion flows with a contact discontinuity moving through a nozzle of finite length.","Mathematically, it can be formulated as a free boundary value problem governed by the two -dimensional steady combustion Euler equations with a contact discontinuity as the free boundary.","The main mathematical difficulties are that the contact discontinuity is a characteristic free boundary and the equations for all states are coupled with each other due to the combustion process.","We first employ the Lagrangian coordinate transformation to fix the free boundary.","Then by introducing the flow slope and Bernoulli function, we further reduce the fixed boundary value problem into an initial boundary value problem for a first order hyperbolic system coupled with several ordinary differential equations.","A new iteration scheme is developed near the background states by employing the intrinsic structure of the equation for the mass fraction of the non-combustion gas.","We show that there is a fixed point for the iteration by deriving some novel $C^{1,\\alpha}$-estimates of the solutions and applying the fixed point theorem, and then the uniqueness of the fixed point is proved by a contraction argument.","On the other hand, a quasi-one-dimensional approximate system is often used to simplify the two-dimensional steady supersonic combustion model.","The error between these two systems is estimated.","Finally, given a piece-wise $C^{1,\\alpha}$-solution containing a contact discontinuity with piece-wise constant states on the entrance of the nozzle, we can show that the solution is the piece-wise constant states with a straight contact discontinuity."],"url":"http://arxiv.org/abs/2406.03884v1","category":"math.AP"}
{"created":"2024-06-06 09:09:47","title":"Energy-storing analysis and fishtail stiffness optimization for a wire-driven elastic robotic fish","abstract":"The robotic fish with high propulsion efficiency and good maneuverability achieves underwater fishlike propulsion by commonly adopting the motor to drive the fishtail, causing the significant fluctuations of the motor power due to the uneven swing speed of the fishtail in one swing cycle. Hence, we propose a wire-driven robotic fish with a spring-steel-based active-segment elastic spine. This bionic spine can produce elastic deformation to store energy under the action of the wire driving and motor for responding to the fluctuations of the motor power. Further, we analyze the effects of the energy-storing of the active-segment elastic spine on the smoothness of motor power. Based on the developed Lagrangian dynamic model and cantilever beam model, the power-variance-based nonlinear optimization model for the stiffness of the active-segment elastic spine is established to respond to the sharp fluctuations of motor power during each fishtail swing cycle. Results validate that the energy-storing of the active-segment elastic spine plays a vital role in improving the power fluctuations and maximum frequency of the motor by adjusting its stiffness reasonably, which is beneficial to achieving high propulsion and high speed for robotic fish. Compared with the active-segment rigid spine that is incapable of storing energy, the energy-storing of the active-segment elastic spine is beneficial to increase the maximum frequency of the motor and the average thrust of the fishtail by 0.41 Hz, and 0.06 N, respectively.","sentences":["The robotic fish with high propulsion efficiency and good maneuverability achieves underwater fishlike propulsion by commonly adopting the motor to drive the fishtail, causing the significant fluctuations of the motor power due to the uneven swing speed of the fishtail in one swing cycle.","Hence, we propose a wire-driven robotic fish with a spring-steel-based active-segment elastic spine.","This bionic spine can produce elastic deformation to store energy under the action of the wire driving and motor for responding to the fluctuations of the motor power.","Further, we analyze the effects of the energy-storing of the active-segment elastic spine on the smoothness of motor power.","Based on the developed Lagrangian dynamic model and cantilever beam model, the power-variance-based nonlinear optimization model for the stiffness of the active-segment elastic spine is established to respond to the sharp fluctuations of motor power during each fishtail swing cycle.","Results validate that the energy-storing of the active-segment elastic spine plays a vital role in improving the power fluctuations and maximum frequency of the motor by adjusting its stiffness reasonably, which is beneficial to achieving high propulsion and high speed for robotic fish.","Compared with the active-segment rigid spine that is incapable of storing energy, the energy-storing of the active-segment elastic spine is beneficial to increase the maximum frequency of the motor and the average thrust of the fishtail by 0.41 Hz, and 0.06 N, respectively."],"url":"http://arxiv.org/abs/2406.03875v1","category":"eess.SY"}
{"created":"2024-06-06 08:55:01","title":"PALM: A Efficient Performance Simulator for Tiled Accelerators with Large-scale Model Training","abstract":"Deep learning (DL) models are piquing high interest and scaling at an unprecedented rate. To this end, a handful of tiled accelerators have been proposed to support such large-scale training tasks. However, these accelerators often incorporate numerous cores or tiles even extending to wafer-scale, substantial on-chip bandwidth, and distributed memory systems. This results in an exceedingly complex design space. Moreover, conducting actual training experiments to find optimal configurations is impractical due to time constraints. Hence, predicting the optimal mapping of various parallelisms to such tiled system architectures becomes crucial. In this study, leveraging an analysis of existing mainstream DL model training strategies, we introduce a performance simulator named PALM. PALM targets both the training and inference processes for tiled accelerators, aiming to inspire the design of current and future accelerators. Specifically, (i) we establish a scheduling mechanism among tiled accelerators based on an event-driven framework; (ii) we support user-configurable pipeline, tensor, and data parallelism on tiled accelerators, determining the absolute performance throughput under these parallelism strategies; (iii) we model the interaction of on-chip SRAM, NoC, and off-chip DRAM during operator execution. This work is available here: https://github.com/fangjh21/PALM.","sentences":["Deep learning (DL) models are piquing high interest and scaling at an unprecedented rate.","To this end, a handful of tiled accelerators have been proposed to support such large-scale training tasks.","However, these accelerators often incorporate numerous cores or tiles even extending to wafer-scale, substantial on-chip bandwidth, and distributed memory systems.","This results in an exceedingly complex design space.","Moreover, conducting actual training experiments to find optimal configurations is impractical due to time constraints.","Hence, predicting the optimal mapping of various parallelisms to such tiled system architectures becomes crucial.","In this study, leveraging an analysis of existing mainstream DL model training strategies, we introduce a performance simulator named PALM.","PALM targets both the training and inference processes for tiled accelerators, aiming to inspire the design of current and future accelerators.","Specifically, (i) we establish a scheduling mechanism among tiled accelerators based on an event-driven framework; (ii) we support user-configurable pipeline, tensor, and data parallelism on tiled accelerators, determining the absolute performance throughput under these parallelism strategies; (iii) we model the interaction of on-chip SRAM, NoC, and off-chip DRAM during operator execution.","This work is available here: https://github.com/fangjh21/PALM."],"url":"http://arxiv.org/abs/2406.03868v1","category":"cs.DC"}
{"created":"2024-06-06 08:49:57","title":"Topological Materials for Near-Field Radiative Heat Transfer","abstract":"Topological materials provide a platform that utilizes the geometric characteristics of structured materials to control the flow of waves, enabling unidirectional and protected transmission that is immune to defects or impurities. The topologically designed photonic materials can carry quantum states and electromagnetic energy, benefiting nanolasers or quantum photonic systems. This article reviews recent advances in the topological applications of photonic materials for radiative heat transfer, especially in the near field. When the separation distance between media is considerably smaller than the thermal wavelength, the heat transfer exhibits super-Planckian behavior that surpasses Planck's blackbody predictions. Near-field thermal radiation in subwavelength systems supporting surface modes has various applications, including nanoscale thermal management and energy conversion. Photonic materials and structures that support topological surface states show immense potential for enhancing or suppressing near-field thermal radiation. We present various topological effects, such as periodic and quasi-periodic nanoparticle arrays, Dirac and Weyl semimetal-based materials, structures with broken global symmetries, and other topological insulators, on near-field heat transfer. Also, the possibility of realizing near-field thermal radiation in such topological materials for alternative thermal management and heat flux guiding in nano-scale systems is discussed based on the existing technology.","sentences":["Topological materials provide a platform that utilizes the geometric characteristics of structured materials to control the flow of waves, enabling unidirectional and protected transmission that is immune to defects or impurities.","The topologically designed photonic materials can carry quantum states and electromagnetic energy, benefiting nanolasers or quantum photonic systems.","This article reviews recent advances in the topological applications of photonic materials for radiative heat transfer, especially in the near field.","When the separation distance between media is considerably smaller than the thermal wavelength, the heat transfer exhibits super-Planckian behavior that surpasses Planck's blackbody predictions.","Near-field thermal radiation in subwavelength systems supporting surface modes has various applications, including nanoscale thermal management and energy conversion.","Photonic materials and structures that support topological surface states show immense potential for enhancing or suppressing near-field thermal radiation.","We present various topological effects, such as periodic and quasi-periodic nanoparticle arrays, Dirac and Weyl semimetal-based materials, structures with broken global symmetries, and other topological insulators, on near-field heat transfer.","Also, the possibility of realizing near-field thermal radiation in such topological materials for alternative thermal management and heat flux guiding in nano-scale systems is discussed based on the existing technology."],"url":"http://arxiv.org/abs/2406.03863v1","category":"physics.optics"}
{"created":"2024-06-06 08:46:52","title":"On the downward L\u00f6wenheim-Skolem Theorem for elementary submodels","abstract":"We introduce a new definition of a model for a formal mathematical system. The definition is based upon the substitution in the formal systems, which allows a purely algebraic approach to model theory. This is very suitable for applications due to a general syntax used in the formal systems. For our models we present a new proof of the downward L\\\"owenheim-Skolem Theorem for elementary submodels.","sentences":["We introduce a new definition of a model for a formal mathematical system.","The definition is based upon the substitution in the formal systems, which allows a purely algebraic approach to model theory.","This is very suitable for applications due to a general syntax used in the formal systems.","For our models we present a new proof of the downward L\\\"owenheim-Skolem Theorem for elementary submodels."],"url":"http://arxiv.org/abs/2406.03860v1","category":"math.LO"}
{"created":"2024-06-06 08:42:36","title":"MuJo: Multimodal Joint Feature Space Learning for Human Activity Recognition","abstract":"Human Activity Recognition is a longstanding problem in AI with applications in a broad range of areas: from healthcare, sports and fitness, security, and human computer interaction to robotics. The performance of HAR in real-world settings is strongly dependent on the type and quality of the input signal that can be acquired. Given an unobstructed, high-quality camera view of a scene, computer vision systems, in particular in conjunction with foundational models (e.g., CLIP), can today fairly reliably distinguish complex activities. On the other hand, recognition using modalities such as wearable sensors (which are often more broadly available, e.g, in mobile phones and smartwatches) is a more difficult problem, as the signals often contain less information and labeled training data is more difficult to acquire. In this work, we show how we can improve HAR performance across different modalities using multimodal contrastive pretraining. Our approach MuJo (Multimodal Joint Feature Space Learning), learns a multimodal joint feature space with video, language, pose, and IMU sensor data. The proposed approach combines contrastive and multitask learning methods and analyzes different multitasking strategies for learning a compact shared representation. A large dataset with parallel video, language, pose, and sensor data points is also introduced to support the research, along with an analysis of the robustness of the multimodal joint space for modal-incomplete and low-resource data. On the MM-Fit dataset, our model achieves an impressive Macro F1-Score of up to 0.992 with only 2% of the train data and 0.999 when using all available training data for classification tasks. Moreover, in the scenario where the MM-Fit dataset is unseen, we demonstrate a generalization performance of up to 0.638.","sentences":["Human Activity Recognition is a longstanding problem in AI with applications in a broad range of areas: from healthcare, sports and fitness, security, and human computer interaction to robotics.","The performance of HAR in real-world settings is strongly dependent on the type and quality of the input signal that can be acquired.","Given an unobstructed, high-quality camera view of a scene, computer vision systems, in particular in conjunction with foundational models (e.g., CLIP), can today fairly reliably distinguish complex activities.","On the other hand, recognition using modalities such as wearable sensors (which are often more broadly available, e.g, in mobile phones and smartwatches) is a more difficult problem, as the signals often contain less information and labeled training data is more difficult to acquire.","In this work, we show how we can improve HAR performance across different modalities using multimodal contrastive pretraining.","Our approach MuJo (Multimodal Joint Feature Space Learning), learns a multimodal joint feature space with video, language, pose, and IMU sensor data.","The proposed approach combines contrastive and multitask learning methods and analyzes different multitasking strategies for learning a compact shared representation.","A large dataset with parallel video, language, pose, and sensor data points is also introduced to support the research, along with an analysis of the robustness of the multimodal joint space for modal-incomplete and low-resource data.","On the MM-Fit dataset, our model achieves an impressive Macro F1-Score of up to 0.992 with only 2% of the train data and 0.999 when using all available training data for classification tasks.","Moreover, in the scenario where the MM-Fit dataset is unseen, we demonstrate a generalization performance of up to 0.638."],"url":"http://arxiv.org/abs/2406.03857v1","category":"cs.LG"}
{"created":"2024-06-06 08:42:09","title":"Multidimensional Quantum Generative Modeling by Quantum Hartley Transform","abstract":"We develop an approach for building quantum models based on the exponentially growing orthonormal basis of Hartley kernel functions. First, we design a differentiable Hartley feature map parametrized by real-valued argument that enables quantum models suitable for solving stochastic differential equations and regression problems. Unlike the naturally complex Fourier encoding, the proposed Hartley feature map circuit leads to quantum states with real-valued amplitudes, introducing an inductive bias and natural regularization. Next, we propose a quantum Hartley transform circuit as a map between computational and Hartley basis. We apply the developed paradigm to generative modeling from solutions of stochastic differential equations, and utilize the quantum Hartley transform for fine sampling from parameterized distributions through an extended register. Finally, we present tools for implementing multivariate quantum generative modeling for both correlated and uncorrelated distributions. As a result, the developed quantum Hartley models offer a distinct quantum approach to generative AI at increasing scale.","sentences":["We develop an approach for building quantum models based on the exponentially growing orthonormal basis of Hartley kernel functions.","First, we design a differentiable Hartley feature map parametrized by real-valued argument that enables quantum models suitable for solving stochastic differential equations and regression problems.","Unlike the naturally complex Fourier encoding, the proposed Hartley feature map circuit leads to quantum states with real-valued amplitudes, introducing an inductive bias and natural regularization.","Next, we propose a quantum Hartley transform circuit as a map between computational and Hartley basis.","We apply the developed paradigm to generative modeling from solutions of stochastic differential equations, and utilize the quantum Hartley transform for fine sampling from parameterized distributions through an extended register.","Finally, we present tools for implementing multivariate quantum generative modeling for both correlated and uncorrelated distributions.","As a result, the developed quantum Hartley models offer a distinct quantum approach to generative AI at increasing scale."],"url":"http://arxiv.org/abs/2406.03856v1","category":"quant-ph"}
{"created":"2024-06-06 08:34:35","title":"Entanglement-assist cyclic weak-value-amplification metrology","abstract":"Weak measurement has garnered widespread interest for its ability to amplify small physical effects at the cost of low detection probabilities. Previous entanglement and recycling techniques enhance postselection efficiency and signal-to-noise ratio (SNR) of weak measurement from distinct perspectives. Here, we incorporate a power recycling cavity into the entanglement-assisted weak measurement system. We obtain an improvement of both detection efficiency and Fisher information, and find that the improvement from entanglement and recycling occur in different dimensions. Furthermore, we analyze two types of errors, walk-off errors and readout errors. The conclusions suggest that entanglement exacerbates the walk-off effect caused by recycling, but this detriment can be balanced by proper parameter selection. In addition, power-recycling can complement entanglement in suppressing readout noise, thus enhancing the accuracy in the measurement results and recovering the lost Fisher information. This work delves deeper into the metrological advantages of weak measurement.","sentences":["Weak measurement has garnered widespread interest for its ability to amplify small physical effects at the cost of low detection probabilities.","Previous entanglement and recycling techniques enhance postselection efficiency and signal-to-noise ratio (SNR) of weak measurement from distinct perspectives.","Here, we incorporate a power recycling cavity into the entanglement-assisted weak measurement system.","We obtain an improvement of both detection efficiency and Fisher information, and find that the improvement from entanglement and recycling occur in different dimensions.","Furthermore, we analyze two types of errors, walk-off errors and readout errors.","The conclusions suggest that entanglement exacerbates the walk-off effect caused by recycling, but this detriment can be balanced by proper parameter selection.","In addition, power-recycling can complement entanglement in suppressing readout noise, thus enhancing the accuracy in the measurement results and recovering the lost Fisher information.","This work delves deeper into the metrological advantages of weak measurement."],"url":"http://arxiv.org/abs/2406.03851v1","category":"quant-ph"}
{"created":"2024-06-06 08:32:46","title":"Strong-field effects in the photo-induced dissociation of the hydrogen molecule on a silver nanoshell","abstract":"Plasmonic catalysis is a rapidly growing field of research, both from experimental and computational perspectives. Experimental observations demonstrate an enhanced dissociation rate for molecules in the presence of plasmonic nanoparticles under low-intensity visible light. The hot-carrier transfer from the nanoparticle to the molecule is often claimed as the mechanism for dissociation. However, the charge transfer time scale is on the order of few femtoseconds and cannot be resolved experimentally. In this situation, ab initio non-adiabatic calculations can provide a solution. Such simulations, however, have their own limitations related to the computational cost. To accelerate plasmonic catalysis simulations, many researchers resort to applying high-intensity external fields to nanoparticle-molecule systems. Here, we show why such an approach can be problematic and emphasize the importance of considering strong-field effects when interpreting the results of time-dependent density functional theory simulations of plasmonic catalysis. By studying the hydrogen molecule dissociation on the surface of a silver nanoshell and analyzing the electron transfer at different field frequencies and intensities, we demonstrate that the molecule dissociates due to multiphoton absorption and subsequent ionization.","sentences":["Plasmonic catalysis is a rapidly growing field of research, both from experimental and computational perspectives.","Experimental observations demonstrate an enhanced dissociation rate for molecules in the presence of plasmonic nanoparticles under low-intensity visible light.","The hot-carrier transfer from the nanoparticle to the molecule is often claimed as the mechanism for dissociation.","However, the charge transfer time scale is on the order of few femtoseconds and cannot be resolved experimentally.","In this situation, ab initio non-adiabatic calculations can provide a solution.","Such simulations, however, have their own limitations related to the computational cost.","To accelerate plasmonic catalysis simulations, many researchers resort to applying high-intensity external fields to nanoparticle-molecule systems.","Here, we show why such an approach can be problematic and emphasize the importance of considering strong-field effects when interpreting the results of time-dependent density functional theory simulations of plasmonic catalysis.","By studying the hydrogen molecule dissociation on the surface of a silver nanoshell and analyzing the electron transfer at different field frequencies and intensities, we demonstrate that the molecule dissociates due to multiphoton absorption and subsequent ionization."],"url":"http://arxiv.org/abs/2406.03850v1","category":"physics.chem-ph"}
{"created":"2024-06-06 08:25:43","title":"Lean Workbook: A large-scale Lean problem set formalized from natural language math problems","abstract":"Large language models have demonstrated impressive capabilities across various natural language processing tasks, especially in solving mathematical problems. However, large language models are not good at math theorem proving using formal languages like Lean. A significant challenge in this area is the scarcity of training data available in these formal languages. To address this issue, we propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements, and vice versa. Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs in translating and understanding complex mathematical problems and proofs. Our final dataset contains about 57K formal-informal question pairs along with searched proof from the math contest forum and 21 new IMO questions. We open-source our code at https://github.com/InternLM/InternLM-Math and our data at https://huggingface.co/datasets/InternLM/Lean-Workbook.","sentences":["Large language models have demonstrated impressive capabilities across various natural language processing tasks, especially in solving mathematical problems.","However, large language models are not good at math theorem proving using formal languages like Lean.","A significant challenge in this area is the scarcity of training data available in these formal languages.","To address this issue, we propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements, and vice versa.","Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs in translating and understanding complex mathematical problems and proofs.","Our final dataset contains about 57K formal-informal question pairs along with searched proof from the math contest forum and 21 new IMO questions.","We open-source our code at https://github.com/InternLM/InternLM-Math and our data at https://huggingface.co/datasets/InternLM/Lean-Workbook."],"url":"http://arxiv.org/abs/2406.03847v1","category":"cs.CL"}
{"created":"2024-06-06 08:17:59","title":"Gamma-ray burst interaction with the circumburst medium: The CBM phase of GRBs","abstract":"Progenitor stars of long gamma-ray bursts (GRBs) could be surrounded by a significant and complex nebula structure lying at a parsec scale distance. After the initial release of energy from the GRB jet, the jet will interact with this nebula environment. We show here that for a large, plausible parameter space region, the interaction between the jet blastwave and the wind termination (reverse) shock is expected to be weak, and may be associated with a precursor emission. As the jet blast wave encounters the contact discontinuity separating the shocked wind and the shocked interstellar medium, we find that a bright flash of synchrotron emission from the newly-formed reverse shock is produced. This flash is expected to be observed at around ~100 s after the initial explosion and precursor. Such a delayed emission thus constitutes a circumburst medium (CBM) phase in a GRB, having a physically distinct origin from the preceding prompt phase and the succeeding afterglow phase. The CBM phase emission may thus provide a natural explanation to bursts observed to have a precursor followed by an intense, synchrotron-dominated main episode that is found in a substantial minority, ~10% of GRBs. A correct identification of the emission phase is thus required to infer the properties of the flow and of the immediate environment around GRB progenitors.","sentences":["Progenitor stars of long gamma-ray bursts (GRBs) could be surrounded by a significant and complex nebula structure lying at a parsec scale distance.","After the initial release of energy from the GRB jet, the jet will interact with this nebula environment.","We show here that for a large, plausible parameter space region, the interaction between the jet blastwave and the wind termination (reverse) shock is expected to be weak, and may be associated with a precursor emission.","As the jet blast wave encounters the contact discontinuity separating the shocked wind and the shocked interstellar medium, we find that a bright flash of synchrotron emission from the newly-formed reverse shock is produced.","This flash is expected to be observed at around ~100 s after the initial explosion and precursor.","Such a delayed emission thus constitutes a circumburst medium (CBM) phase in a GRB, having a physically distinct origin from the preceding prompt phase and the succeeding afterglow phase.","The CBM phase emission may thus provide a natural explanation to bursts observed to have a precursor followed by an intense, synchrotron-dominated main episode that is found in a substantial minority, ~10% of GRBs.","A correct identification of the emission phase is thus required to infer the properties of the flow and of the immediate environment around GRB progenitors."],"url":"http://arxiv.org/abs/2406.03841v1","category":"astro-ph.HE"}
{"created":"2024-06-06 08:17:45","title":"Global tensor polarization of spin $3/2$ hadrons and quark spin correlations in relativistic heavy ion collisions","abstract":"We study the global polarization of spin-$3/2$ hadrons in relativistic heavy ion collisions. We show in particular that the global tensor polarizations of rank two or three for spin-$3/2$ hadrons are sensitive to the local two or three quark spin correlations respectively in the quark gluon plasma produced in the collision processes. We present the relationships between these measurable tensor polarizations and quark spin correlations in the quark matter system.","sentences":["We study the global polarization of spin-$3/2$ hadrons in relativistic heavy ion collisions.","We show in particular that the global tensor polarizations of rank two or three for spin-$3/2$ hadrons are sensitive to the local two or three quark spin correlations respectively in the quark gluon plasma produced in the collision processes.","We present the relationships between these measurable tensor polarizations and quark spin correlations in the quark matter system."],"url":"http://arxiv.org/abs/2406.03840v1","category":"hep-ph"}
{"created":"2024-06-06 08:12:38","title":"Monocular Localization with Semantics Map for Autonomous Vehicles","abstract":"Accurate and robust localization remains a significant challenge for autonomous vehicles. The cost of sensors and limitations in local computational efficiency make it difficult to scale to large commercial applications. Traditional vision-based approaches focus on texture features that are susceptible to changes in lighting, season, perspective, and appearance. Additionally, the large storage size of maps with descriptors and complex optimization processes hinder system performance. To balance efficiency and accuracy, we propose a novel lightweight visual semantic localization algorithm that employs stable semantic features instead of low-level texture features. First, semantic maps are constructed offline by detecting semantic objects, such as ground markers, lane lines, and poles, using cameras or LiDAR sensors. Then, online visual localization is performed through data association of semantic features and map objects. We evaluated our proposed localization framework in the publicly available KAIST Urban dataset and in scenarios recorded by ourselves. The experimental results demonstrate that our method is a reliable and practical localization solution in various autonomous driving localization tasks.","sentences":["Accurate and robust localization remains a significant challenge for autonomous vehicles.","The cost of sensors and limitations in local computational efficiency make it difficult to scale to large commercial applications.","Traditional vision-based approaches focus on texture features that are susceptible to changes in lighting, season, perspective, and appearance.","Additionally, the large storage size of maps with descriptors and complex optimization processes hinder system performance.","To balance efficiency and accuracy, we propose a novel lightweight visual semantic localization algorithm that employs stable semantic features instead of low-level texture features.","First, semantic maps are constructed offline by detecting semantic objects, such as ground markers, lane lines, and poles, using cameras or LiDAR sensors.","Then, online visual localization is performed through data association of semantic features and map objects.","We evaluated our proposed localization framework in the publicly available KAIST Urban dataset and in scenarios recorded by ourselves.","The experimental results demonstrate that our method is a reliable and practical localization solution in various autonomous driving localization tasks."],"url":"http://arxiv.org/abs/2406.03835v1","category":"cs.CV"}
{"created":"2024-06-06 08:09:19","title":"The Broadband X-ray Spectral Properties during the Rising Phases of the Outburst of the New Black Hole X-ray Binary Candidate Swift J1727.8-1613","abstract":"We report data analysis results about the outburst evolution and spectral properties during the hard state of the recently discovered X-ray transient Swift J1727.8-163 as observed by \\emph{Insight}-HXMT and NuSTAR. We find that the broadband X-ray spectrum of Swift J1727.8-163 is more complex than the most typical spectral patterns of black hole X-ray binary systems, with not only a comparatively weaker reflection component but also an additional spectral continuum component, manifesting itself as a hard X-ray tail beyond the thermal Comptonization description detectable below 100 keV. This additional component can be phenomenologically well fitted by adding an extra power-law model with high energy exponential cutoff in the 2-120 keV energy band. We made an attempt to explain the broadband X-ray spectral continuum with a thermal/non-thermal hybrid plasma corona scenario , and find an ultra high compactness parameter ($l_{\\rm s}\\sim2000$) and a steep non-thermal electron distribution ($\\Gamma_{\\rm inj}>4$), suggesting the source was accreting with high Eddington rates and that the electron acceleration mechanism is not very efficient. We also present a detailed multi-epoch analysis of spectral properties using \\emph{Insight}-HXMT data to investigate the evolution of the key physical properties regarding the disk and corona during the hard states. No significant variation is found with the inner disk radius and the coronal temperature during this time period, and the weak reflection and hard X-ray tail features are persistent. We discuss the physical implications of our spectral analysis results in the context of disk-corona relation, particle acceleration, and jet contribution, during the rise of a black hole X-ray binary in outburst.","sentences":["We report data analysis results about the outburst evolution and spectral properties during the hard state of the recently discovered X-ray transient Swift J1727.8-163 as observed by \\emph{Insight}-HXMT and NuSTAR.","We find that the broadband X-ray spectrum of Swift J1727.8-163 is more complex than the most typical spectral patterns of black hole X-ray binary systems, with not only a comparatively weaker reflection component but also an additional spectral continuum component, manifesting itself as a hard X-ray tail beyond the thermal Comptonization description detectable below 100 keV.","This additional component can be phenomenologically well fitted by adding an extra power-law model with high energy exponential cutoff in the 2-120 keV energy band.","We made an attempt to explain the broadband X-ray spectral continuum with a thermal/non-thermal hybrid plasma corona scenario , and find an ultra high compactness parameter ($l_{\\rm s}\\sim2000$) and a steep non-thermal electron distribution ($\\Gamma_{\\rm inj}>4$), suggesting the source was accreting with high Eddington rates and that the electron acceleration mechanism is not very efficient.","We also present a detailed multi-epoch analysis of spectral properties using \\emph{Insight}-HXMT data to investigate the evolution of the key physical properties regarding the disk and corona during the hard states.","No significant variation is found with the inner disk radius and the coronal temperature during this time period, and the weak reflection and hard X-ray tail features are persistent.","We discuss the physical implications of our spectral analysis results in the context of disk-corona relation, particle acceleration, and jet contribution, during the rise of a black hole X-ray binary in outburst."],"url":"http://arxiv.org/abs/2406.03834v1","category":"astro-ph.HE"}
{"created":"2024-06-06 08:05:20","title":"Malware Classification Based on Image Segmentation","abstract":"Executable programs are highly structured files that can be recognized by operating systems and loaded into memory, analyzed for their dependencies, allocated resources, and ultimately executed. Each section of an executable program possesses distinct file and semantic boundaries, resembling puzzle pieces with varying shapes, textures, and sizes. These individualistic sections, when combined in diverse manners, constitute a complete executable program. This paper proposes a novel approach for the visualization and classification of malware. Specifically, we segment the grayscale images generated from malware binary files based on the section categories, resulting in multiple sub-images of different classes. These sub-images are then treated as multi-channel images and input into a deep convolutional neural network for malware classification. Experimental results demonstrate that images of different malware section classes exhibit favorable classification characteristics. Additionally, we discuss how the width alignment of malware grayscale images can influence the performance of the model.","sentences":["Executable programs are highly structured files that can be recognized by operating systems and loaded into memory, analyzed for their dependencies, allocated resources, and ultimately executed.","Each section of an executable program possesses distinct file and semantic boundaries, resembling puzzle pieces with varying shapes, textures, and sizes.","These individualistic sections, when combined in diverse manners, constitute a complete executable program.","This paper proposes a novel approach for the visualization and classification of malware.","Specifically, we segment the grayscale images generated from malware binary files based on the section categories, resulting in multiple sub-images of different classes.","These sub-images are then treated as multi-channel images and input into a deep convolutional neural network for malware classification.","Experimental results demonstrate that images of different malware section classes exhibit favorable classification characteristics.","Additionally, we discuss how the width alignment of malware grayscale images can influence the performance of the model."],"url":"http://arxiv.org/abs/2406.03831v1","category":"cs.CR"}
{"created":"2024-06-06 07:49:11","title":"Subspace Clustering in Wavelet Packets Domain","abstract":"Subspace clustering (SC) algorithms utilize the union of subspaces model to cluster data points according to the subspaces from which they are drawn. To better address separability of subspaces and robustness to noise we propose a wavelet packet (WP) based transform domain subspace clustering. Depending on the number of resolution levels, WP yields several representations instantiated in terms of subbands. The first approach combines original and subband data into one complementary multi-view representation. Afterward, we formulate joint representation learning as a low-rank MERA tensor network approximation problem. That is motivated by the strong representation power of the MERA network to capture complex intra/inter-view dependencies in corresponding self-representation tensor. In the second approach, we use a self-stopping computationally efficient method to select the subband with the smallest clustering error on the validation set. When existing SC algorithms are applied to the chosen subband, their performance is expected to improve. Consequently, both approaches enable the re-use of SC algorithms developed so far. Improved clustering performance is due to the dual nature of subbands as representations and filters, which is essential for noise suppression. We exemplify the proposed WP domain approach to SC on the MERA tensor network and eight other well-known linear SC algorithms using six well-known image datasets representing faces, digits, and objects. Although WP domain-based SC is a linear method, it achieved clustering performance comparable with some best deep SC algorithms and outperformed many other deep SC algorithms by a significant margin. That is in particular case for the WP MERA SC algorithm. On the COIL100 dataset, it achieves an accuracy of 87.45% and outperforms the best deep SC competitor in the amount of 14.75%.","sentences":["Subspace clustering (SC) algorithms utilize the union of subspaces model to cluster data points according to the subspaces from which they are drawn.","To better address separability of subspaces and robustness to noise we propose a wavelet packet (WP) based transform domain subspace clustering.","Depending on the number of resolution levels, WP yields several representations instantiated in terms of subbands.","The first approach combines original and subband data into one complementary multi-view representation.","Afterward, we formulate joint representation learning as a low-rank MERA tensor network approximation problem.","That is motivated by the strong representation power of the MERA network to capture complex intra/inter-view dependencies in corresponding self-representation tensor.","In the second approach, we use a self-stopping computationally efficient method to select the subband with the smallest clustering error on the validation set.","When existing SC algorithms are applied to the chosen subband, their performance is expected to improve.","Consequently, both approaches enable the re-use of SC algorithms developed so far.","Improved clustering performance is due to the dual nature of subbands as representations and filters, which is essential for noise suppression.","We exemplify the proposed WP domain approach to SC on the MERA tensor network and eight other well-known linear SC algorithms using six well-known image datasets representing faces, digits, and objects.","Although WP domain-based SC is a linear method, it achieved clustering performance comparable with some best deep SC algorithms and outperformed many other deep SC algorithms by a significant margin.","That is in particular case for the WP MERA SC algorithm.","On the COIL100 dataset, it achieves an accuracy of 87.45% and outperforms the best deep SC competitor in the amount of 14.75%."],"url":"http://arxiv.org/abs/2406.03819v1","category":"cs.LG"}
{"created":"2024-06-06 07:49:02","title":"Amortized Equation Discovery in Hybrid Dynamical Systems","abstract":"Hybrid dynamical systems are prevalent in science and engineering to express complex systems with continuous and discrete states. To learn the laws of systems, all previous methods for equation discovery in hybrid systems follow a two-stage paradigm, i.e. they first group time series into small cluster fragments and then discover equations in each fragment separately through methods in non-hybrid systems. Although effective, these methods do not fully take advantage of the commonalities in the shared dynamics of multiple fragments that are driven by the same equations. Besides, the two-stage paradigm breaks the interdependence between categorizing and representing dynamics that jointly form hybrid systems. In this paper, we reformulate the problem and propose an end-to-end learning framework, i.e. Amortized Equation Discovery (AMORE), to jointly categorize modes and discover equations characterizing the dynamics of each mode by all segments of the mode. Experiments on four hybrid and six non-hybrid systems show that our method outperforms previous methods on equation discovery, segmentation, and forecasting.","sentences":["Hybrid dynamical systems are prevalent in science and engineering to express complex systems with continuous and discrete states.","To learn the laws of systems, all previous methods for equation discovery in hybrid systems follow a two-stage paradigm, i.e. they first group time series into small cluster fragments and then discover equations in each fragment separately through methods in non-hybrid systems.","Although effective, these methods do not fully take advantage of the commonalities in the shared dynamics of multiple fragments that are driven by the same equations.","Besides, the two-stage paradigm breaks the interdependence between categorizing and representing dynamics that jointly form hybrid systems.","In this paper, we reformulate the problem and propose an end-to-end learning framework, i.e. Amortized Equation Discovery (AMORE), to jointly categorize modes and discover equations characterizing the dynamics of each mode by all segments of the mode.","Experiments on four hybrid and six non-hybrid systems show that our method outperforms previous methods on equation discovery, segmentation, and forecasting."],"url":"http://arxiv.org/abs/2406.03818v1","category":"cs.CV"}
{"created":"2024-06-06 07:39:29","title":"Triton and Pluto: same origin but separated at birth","abstract":"Assessing the origin of Pluto and Triton has profound implications for the bigger picture of Solar System formation and evolution. In such a context, this chapter reviews our current knowledge of the formation conditions of Pluto and Triton's constitutive building blocks in the protosolar nebula, which can be derived from their known or estimated volatile contents. Assuming that the ultravolatiles carbon monoxide and dinitrogen detected in Pluto and Triton are primordial, the presence of these molecules suggest that the two bodies accreted material originating from the vicinity of the carbon monoxide and dinitrogen icelines. Dinitrogen--rich and water--poor comets such as comet C/2016 R2 (PanSTARRS) obviously present a compositional link with Pluto and Triton, indicating that their building blocks formed in nearby regions of the protosolar nebula, despite of the variation of the water abundance among those bodies. Also, the assumption of Triton's growth in Neptune's circumplanetary disk requires that its building blocks formed at earlier epochs in the protosolar nebula, to remain consistent with its estimated composition.","sentences":["Assessing the origin of Pluto and Triton has profound implications for the bigger picture of Solar System formation and evolution.","In such a context, this chapter reviews our current knowledge of the formation conditions of Pluto and Triton's constitutive building blocks in the protosolar nebula, which can be derived from their known or estimated volatile contents.","Assuming that the ultravolatiles carbon monoxide and dinitrogen detected in Pluto and Triton are primordial, the presence of these molecules suggest that the two bodies accreted material originating from the vicinity of the carbon monoxide and dinitrogen icelines.","Dinitrogen--rich and water--poor comets such as comet C/2016 R2 (PanSTARRS) obviously present a compositional link with Pluto and Triton, indicating that their building blocks formed in nearby regions of the protosolar nebula, despite of the variation of the water abundance among those bodies.","Also, the assumption of Triton's growth in Neptune's circumplanetary disk requires that its building blocks formed at earlier epochs in the protosolar nebula, to remain consistent with its estimated composition."],"url":"http://arxiv.org/abs/2406.03815v1","category":"astro-ph.EP"}
{"created":"2024-06-06 07:39:17","title":"Improving Zero-Shot Chinese-English Code-Switching ASR with kNN-CTC and Gated Monolingual Datastores","abstract":"The kNN-CTC model has proven to be effective for monolingual automatic speech recognition (ASR). However, its direct application to multilingual scenarios like code-switching, presents challenges. Although there is potential for performance improvement, a kNN-CTC model utilizing a single bilingual datastore can inadvertently introduce undesirable noise from the alternative language. To address this, we propose a novel kNN-CTC-based code-switching ASR (CS-ASR) framework that employs dual monolingual datastores and a gated datastore selection mechanism to reduce noise interference. Our method selects the appropriate datastore for decoding each frame, ensuring the injection of language-specific information into the ASR process. We apply this framework to cutting-edge CTC-based models, developing an advanced CS-ASR system. Extensive experiments demonstrate the remarkable effectiveness of our gated datastore mechanism in enhancing the performance of zero-shot Chinese-English CS-ASR.","sentences":["The kNN-CTC model has proven to be effective for monolingual automatic speech recognition (ASR).","However, its direct application to multilingual scenarios like code-switching, presents challenges.","Although there is potential for performance improvement, a kNN-CTC model utilizing a single bilingual datastore can inadvertently introduce undesirable noise from the alternative language.","To address this, we propose a novel kNN-CTC-based code-switching ASR (CS-ASR) framework that employs dual monolingual datastores and a gated datastore selection mechanism to reduce noise interference.","Our method selects the appropriate datastore for decoding each frame, ensuring the injection of language-specific information into the ASR process.","We apply this framework to cutting-edge CTC-based models, developing an advanced CS-ASR system.","Extensive experiments demonstrate the remarkable effectiveness of our gated datastore mechanism in enhancing the performance of zero-shot Chinese-English CS-ASR."],"url":"http://arxiv.org/abs/2406.03814v1","category":"cs.CL"}
{"created":"2024-06-06 07:34:58","title":"Spherinator and HiPSter: Representation Learning for Unbiased Knowledge Discovery from Simulations","abstract":"Simulations are the best approximation to experimental laboratories in astrophysics and cosmology. However, the complexity, richness, and large size of their outputs severely limit the interpretability of their predictions. We describe a new, unbiased, and machine learning based approach to obtaining useful scientific insights from a broad range of simulations. The method can be used on today's largest simulations and will be essential to solve the extreme data exploration and analysis challenges posed by the Exascale era. Furthermore, this concept is so flexible, that it will also enable explorative access to observed data. Our concept is based on applying nonlinear dimensionality reduction to learn compact representations of the data in a low-dimensional space. The simulation data is projected onto this space for interactive inspection, visual interpretation, sample selection, and local analysis. We present a prototype using a rotational invariant hyperspherical variational convolutional autoencoder, utilizing a power distribution in the latent space, and trained on galaxies from IllustrisTNG simulation. Thereby, we obtain a natural Hubble tuning fork like similarity space that can be visualized interactively on the surface of a sphere by exploiting the power of HiPS tilings in Aladin Lite.","sentences":["Simulations are the best approximation to experimental laboratories in astrophysics and cosmology.","However, the complexity, richness, and large size of their outputs severely limit the interpretability of their predictions.","We describe a new, unbiased, and machine learning based approach to obtaining useful scientific insights from a broad range of simulations.","The method can be used on today's largest simulations and will be essential to solve the extreme data exploration and analysis challenges posed by the Exascale era.","Furthermore, this concept is so flexible, that it will also enable explorative access to observed data.","Our concept is based on applying nonlinear dimensionality reduction to learn compact representations of the data in a low-dimensional space.","The simulation data is projected onto this space for interactive inspection, visual interpretation, sample selection, and local analysis.","We present a prototype using a rotational invariant hyperspherical variational convolutional autoencoder, utilizing a power distribution in the latent space, and trained on galaxies from IllustrisTNG simulation.","Thereby, we obtain a natural Hubble tuning fork like similarity space that can be visualized interactively on the surface of a sphere by exploiting the power of HiPS tilings in Aladin Lite."],"url":"http://arxiv.org/abs/2406.03810v1","category":"astro-ph.IM"}
{"created":"2024-06-06 07:16:47","title":"The behavior of higher proof theory I: Case $\u03a3^1_2$","abstract":"The current ordinal analysis provides the proof-theoretic ordinal of a theory, which calibrates the robustness of the $\\Pi^1_1$-consequences of the theory. We can ask whether there is an ordinal characteristic capturing more complex consequences, and it turns out that we can define the $\\Sigma^1_2$-proof-theoretic ordinal capturing the robustness of the $\\Sigma^1_2$-consequences of a theory. In this paper, we study the behavior of $\\Sigma^1_2$-proof-theoretic ordinal, and it turns out that $\\Sigma^1_2$-proof-theoretic ordinal also follows an analogue of Walsh's characterization of proof-theoretic ordinal.","sentences":["The current ordinal analysis provides the proof-theoretic ordinal of a theory, which calibrates the robustness of the $\\Pi^1_1$-consequences of the theory.","We can ask whether there is an ordinal characteristic capturing more complex consequences, and it turns out that we can define the $\\Sigma^1_2$-proof-theoretic ordinal capturing the robustness of the $\\Sigma^1_2$-consequences of a theory.","In this paper, we study the behavior of $\\Sigma^1_2$-proof-theoretic ordinal, and it turns out that $\\Sigma^1_2$-proof-theoretic ordinal also follows an analogue of Walsh's characterization of proof-theoretic ordinal."],"url":"http://arxiv.org/abs/2406.03801v1","category":"math.LO"}
{"created":"2024-06-06 07:08:20","title":"Beyond a binary theorizing of prosociality","abstract":"A stylized experiment, the public goods game, has taught us the peculiar reproducible fact that humans tend to contribute more to shared resources than expected from economically rational assumptions. There have been two competing explanations for this phenomenon: either contributing to the public good is an innate human trait (the prosocial preference hypothesis) or a transitory effect while learning the game (the confused learner hypothesis). We use large-scale experimental data from a novel experimental design to distinguish between these two hypotheses. By monitoring the effects of zealots (persistently cooperating bots) and varying the participants' awareness of them, we find a considerably more complex scenario than previously reported. People indeed have a prosocial bias, but not to the degree that they always forego taking action to increase their profit. While our findings end the simplistic theorizing of prosociality in the public goods game, an observed positive, cooperative response to zealots has actionable policy implications.","sentences":["A stylized experiment, the public goods game, has taught us the peculiar reproducible fact that humans tend to contribute more to shared resources than expected from economically rational assumptions.","There have been two competing explanations for this phenomenon: either contributing to the public good is an innate human trait (the prosocial preference hypothesis) or a transitory effect while learning the game (the confused learner hypothesis).","We use large-scale experimental data from a novel experimental design to distinguish between these two hypotheses.","By monitoring the effects of zealots (persistently cooperating bots) and varying the participants' awareness of them, we find a considerably more complex scenario than previously reported.","People indeed have a prosocial bias, but not to the degree that they always forego taking action to increase their profit.","While our findings end the simplistic theorizing of prosociality in the public goods game, an observed positive, cooperative response to zealots has actionable policy implications."],"url":"http://arxiv.org/abs/2406.03796v1","category":"physics.soc-ph"}
{"created":"2024-06-06 07:05:58","title":"Infusing Self-Consistency into Density Functional Theory Hamiltonian Prediction via Deep Equilibrium Models","abstract":"In this study, we introduce a unified neural network architecture, the Deep Equilibrium Density Functional Theory Hamiltonian (DEQH) model, which incorporates Deep Equilibrium Models (DEQs) for predicting Density Functional Theory (DFT) Hamiltonians. The DEQH model inherently captures the self-consistency nature of Hamiltonian, a critical aspect often overlooked by traditional machine learning approaches for Hamiltonian prediction. By employing DEQ within our model architecture, we circumvent the need for DFT calculations during the training phase to introduce the Hamiltonian's self-consistency, thus addressing computational bottlenecks associated with large or complex systems. We propose a versatile framework that combines DEQ with off-the-shelf machine learning models for predicting Hamiltonians. When benchmarked on the MD17 and QH9 datasets, DEQHNet, an instantiation of the DEQH framework, has demonstrated a significant improvement in prediction accuracy. Beyond a predictor, the DEQH model is a Hamiltonian solver, in the sense that it uses the fixed-point solving capability of the deep equilibrium model to iteratively solve for the Hamiltonian. Ablation studies of DEQHNet further elucidate the network's effectiveness, offering insights into the potential of DEQ-integrated networks for Hamiltonian learning.","sentences":["In this study, we introduce a unified neural network architecture, the Deep Equilibrium Density Functional Theory Hamiltonian (DEQH) model, which incorporates Deep Equilibrium Models (DEQs) for predicting Density Functional Theory (DFT) Hamiltonians.","The DEQH model inherently captures the self-consistency nature of Hamiltonian, a critical aspect often overlooked by traditional machine learning approaches for Hamiltonian prediction.","By employing DEQ within our model architecture, we circumvent the need for DFT calculations during the training phase to introduce the Hamiltonian's self-consistency, thus addressing computational bottlenecks associated with large or complex systems.","We propose a versatile framework that combines DEQ with off-the-shelf machine learning models for predicting Hamiltonians.","When benchmarked on the MD17 and QH9 datasets, DEQHNet, an instantiation of the DEQH framework, has demonstrated a significant improvement in prediction accuracy.","Beyond a predictor, the DEQH model is a Hamiltonian solver, in the sense that it uses the fixed-point solving capability of the deep equilibrium model to iteratively solve for the Hamiltonian.","Ablation studies of DEQHNet further elucidate the network's effectiveness, offering insights into the potential of DEQ-integrated networks for Hamiltonian learning."],"url":"http://arxiv.org/abs/2406.03794v1","category":"cs.LG"}
{"created":"2024-06-06 06:56:56","title":"Projection-Free Variance Reduction Methods for Stochastic Constrained Multi-Level Compositional Optimization","abstract":"This paper investigates projection-free algorithms for stochastic constrained multi-level optimization. In this context, the objective function is a nested composition of several smooth functions, and the decision set is closed and convex. Existing projection-free algorithms for solving this problem suffer from two limitations: 1) they solely focus on the gradient mapping criterion and fail to match the optimal sample complexities in unconstrained settings; 2) their analysis is exclusively applicable to non-convex functions, without considering convex and strongly convex objectives. To address these issues, we introduce novel projection-free variance reduction algorithms and analyze their complexities under different criteria. For gradient mapping, our complexities improve existing results and match the optimal rates for unconstrained problems. For the widely-used Frank-Wolfe gap criterion, we provide theoretical guarantees that align with those for single-level problems. Additionally, by using a stage-wise adaptation, we further obtain complexities for convex and strongly convex functions. Finally, numerical experiments on different tasks demonstrate the effectiveness of our methods.","sentences":["This paper investigates projection-free algorithms for stochastic constrained multi-level optimization.","In this context, the objective function is a nested composition of several smooth functions, and the decision set is closed and convex.","Existing projection-free algorithms for solving this problem suffer from two limitations: 1) they solely focus on the gradient mapping criterion and fail to match the optimal sample complexities in unconstrained settings; 2) their analysis is exclusively applicable to non-convex functions, without considering convex and strongly convex objectives.","To address these issues, we introduce novel projection-free variance reduction algorithms and analyze their complexities under different criteria.","For gradient mapping, our complexities improve existing results and match the optimal rates for unconstrained problems.","For the widely-used Frank-Wolfe gap criterion, we provide theoretical guarantees that align with those for single-level problems.","Additionally, by using a stage-wise adaptation, we further obtain complexities for convex and strongly convex functions.","Finally, numerical experiments on different tasks demonstrate the effectiveness of our methods."],"url":"http://arxiv.org/abs/2406.03787v1","category":"math.OC"}
{"created":"2024-06-06 06:55:16","title":"Adaptive Lightweight Security for Performance Efficiency in Critical Healthcare Monitoring","abstract":"The healthcare infrastructure requires robust security procedures, technologies, and policies due to its critical nature. Since the Internet of Things (IoT) with its diverse technologies has become an integral component of future healthcare systems, its security requires a thorough analysis due to its inherent security limitations that arise from resource constraints. Existing communication technologies used for IoT connectivity, such as 5G, provide communications security with the underlying communication infrastructure to a certain level. However, the evolving healthcare paradigm requires adaptive security procedures and technologies that can adapt to the varying resource constraints of IoT devices. This need for adaptive security is particularly pronounced when considering components outside the security sandbox of 5G, such as IoT nodes and M2M connections, which introduce additional security challenges. This article brings forth the unique healthcare monitoring requirements and studies the existing encryption-based security approaches to provide the necessary security. Furthermore, this research introduces a novel approach to optimizing security and performance in IoT in healthcare, particularly in critical use cases such as remote patient monitoring. Finally, the results from the practical implementation demonstrate a marked improvement in the system performance.","sentences":["The healthcare infrastructure requires robust security procedures, technologies, and policies due to its critical nature.","Since the Internet of Things (IoT) with its diverse technologies has become an integral component of future healthcare systems, its security requires a thorough analysis due to its inherent security limitations that arise from resource constraints.","Existing communication technologies used for IoT connectivity, such as 5G, provide communications security with the underlying communication infrastructure to a certain level.","However, the evolving healthcare paradigm requires adaptive security procedures and technologies that can adapt to the varying resource constraints of IoT devices.","This need for adaptive security is particularly pronounced when considering components outside the security sandbox of 5G, such as IoT nodes and M2M connections, which introduce additional security challenges.","This article brings forth the unique healthcare monitoring requirements and studies the existing encryption-based security approaches to provide the necessary security.","Furthermore, this research introduces a novel approach to optimizing security and performance in IoT in healthcare, particularly in critical use cases such as remote patient monitoring.","Finally, the results from the practical implementation demonstrate a marked improvement in the system performance."],"url":"http://arxiv.org/abs/2406.03786v1","category":"cs.CR"}
{"created":"2024-06-06 06:48:43","title":"Operator dynamics and entanglement in space-time dual Hadamard lattices","abstract":"Many-body quantum dynamics defined on a spatial lattice and in discrete time -- either as stroboscopic Floquet systems or quantum circuits -- has been an active area of research for several years. Being discrete in space and time, a natural question arises: when can such a model be viewed as evolving unitarily in space as well as in time? Models with this property, which sometimes goes by the name space-time duality, have been shown to have a number of interesting features related to entanglement growth and correlations. One natural way in which the property arises in the context of (brickwork) quantum circuits is by choosing dual unitary gates: two site operators that are unitary in both the space and time directions. We introduce a class of models with $q$ states per site, defined on the square lattice by a complex partition function and paremeterized in terms of $q\\times q$ Hadamard matrices, that have the property of space-time duality. These may interpreted as particular dual unitary circuits or stroboscopically evolving systems, and generalize the well studied self-dual kicked Ising model. We explore the operator dynamics in the case of Clifford circuits, making connections to Clifford cellular automata [J. Math. Phys. 49, 112104 (2008)] and in the $q\\to\\infty$ limit to the classical spatiotemporal cat model of many body chaos [Nonlinearity 34, 2800 (2021)]. We establish integrability and the corresponding conserved charges for a large subfamily and show how the long-range entanglement protocol discussed in the recent paper [Phys. Rev. B 105, 144306 (2022)] can be reinterpreted in purely graphical terms and directly applied here.","sentences":["Many-body quantum dynamics defined on a spatial lattice and in discrete time -- either as stroboscopic Floquet systems or quantum circuits -- has been an active area of research for several years.","Being discrete in space and time, a natural question arises: when can such a model be viewed as evolving unitarily in space as well as in time?","Models with this property, which sometimes goes by the name space-time duality, have been shown to have a number of interesting features related to entanglement growth and correlations.","One natural way in which the property arises in the context of (brickwork) quantum circuits is by choosing dual unitary gates: two site operators that are unitary in both the space and time directions.","We introduce a class of models with $q$ states per site, defined on the square lattice by a complex partition function and paremeterized in terms of $q\\times q$ Hadamard matrices, that have the property of space-time duality.","These may interpreted as particular dual unitary circuits or stroboscopically evolving systems, and generalize the well studied self-dual kicked Ising model.","We explore the operator dynamics in the case of Clifford circuits, making connections to Clifford cellular automata [J. Math.","Phys. 49, 112104 (2008)] and in the $q\\to\\infty$ limit to the classical spatiotemporal cat model of many body chaos [Nonlinearity 34, 2800 (2021)].","We establish integrability and the corresponding conserved charges for a large subfamily and show how the long-range entanglement protocol discussed in the recent paper [Phys. Rev. B 105, 144306 (2022)] can be reinterpreted in purely graphical terms and directly applied here."],"url":"http://arxiv.org/abs/2406.03781v1","category":"quant-ph"}
{"created":"2024-06-06 06:45:33","title":"Iterative Sparse Identification of Nonlinear Dynamics","abstract":"In order to extract governing equations from time-series data, various approaches are proposed. Among those, sparse identification of nonlinear dynamics (SINDy) stands out as a successful method capable of modeling governing equations with a minimal number of terms, utilizing the principles of compressive sensing. This feature, which relies on a small number of terms, is crucial for interpretability. The effectiveness of SINDy hinges on the choice of candidate functions within its dictionary to extract governing equations of dynamical systems. A larger dictionary allows for more terms, enhancing the quality of approximations. However, the computational complexity scales with dictionary size, rendering SINDy less suitable for high-dimensional datasets, even though it has been successfully applied to low-dimensional datasets. To address this challenge, we introduce iterative SINDy in this paper, where the dictionary undergoes expansion and compression through iterations. We also conduct an analysis of the convergence properties of iterative SINDy. Simulation results validate that iterative SINDy can achieve nearly identical performance to SINDy, while significantly reducing computational complexity. Notably, iterative SINDy demonstrates effectiveness with high-dimensional time-series data without incurring the prohibitively high computational cost associated with SINDy.","sentences":["In order to extract governing equations from time-series data, various approaches are proposed.","Among those, sparse identification of nonlinear dynamics (SINDy) stands out as a successful method capable of modeling governing equations with a minimal number of terms, utilizing the principles of compressive sensing.","This feature, which relies on a small number of terms, is crucial for interpretability.","The effectiveness of SINDy hinges on the choice of candidate functions within its dictionary to extract governing equations of dynamical systems.","A larger dictionary allows for more terms, enhancing the quality of approximations.","However, the computational complexity scales with dictionary size, rendering SINDy less suitable for high-dimensional datasets, even though it has been successfully applied to low-dimensional datasets.","To address this challenge, we introduce iterative SINDy in this paper, where the dictionary undergoes expansion and compression through iterations.","We also conduct an analysis of the convergence properties of iterative SINDy.","Simulation results validate that iterative SINDy can achieve nearly identical performance to SINDy, while significantly reducing computational complexity.","Notably, iterative SINDy demonstrates effectiveness with high-dimensional time-series data without incurring the prohibitively high computational cost associated with SINDy."],"url":"http://arxiv.org/abs/2406.03779v1","category":"eess.SP"}
{"created":"2024-06-06 06:26:49","title":"Nonstandard derivation of the Gorini-Kossakowski-Sudarshan-Lindblad master equation of a quantum dynamical semigroup from the Kraus representation","abstract":"We give a new nonstandard proof of a well-known theorem that the generator $L$ of a quantum dynamical semigroup $\\exp(tL)$ on a finite-dimensional quantum system has a specific form called a Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) generator (also known as a Lindbladian) and vice versa. The proof starts from the Kraus representation of the quantum channel $\\exp (\\delta t L)$ for an infinitesimal hyperreal number $\\delta t>0$ and then estimates the orders of the traceless components of the Kraus operators. The jump operators then naturally arise as the standard parts of the traceless parts divided by $\\sqrt{\\delta t}$. We also give a nonstandard proof of a related fact that close completely positive maps have close Kraus operators.","sentences":["We give a new nonstandard proof of a well-known theorem that the generator $L$ of a quantum dynamical semigroup $\\exp(tL)$ on a finite-dimensional quantum system has a specific form called a Gorini-Kossakowski-Sudarshan-Lindblad (GKSL) generator (also known as a Lindbladian) and vice versa.","The proof starts from the Kraus representation of the quantum channel $\\exp (\\delta t L)$ for an infinitesimal hyperreal number $\\delta t>0$ and then estimates the orders of the traceless components of the Kraus operators.","The jump operators then naturally arise as the standard parts of the traceless parts divided by $\\sqrt{\\delta t}$. We also give a nonstandard proof of a related fact that close completely positive maps have close Kraus operators."],"url":"http://arxiv.org/abs/2406.03775v1","category":"quant-ph"}
{"created":"2024-06-06 06:18:55","title":"Global synchronization in generalized multilayer higher-order networks","abstract":"Networks incorporating higher-order interactions are increasingly recognized for their ability to introduce novel dynamics into various processes, including synchronization. Previous studies on synchronization within multilayer networks have often been limited to specific models, such as the Kuramoto model, or have focused solely on higher-order interactions within individual layers. Here, we present a comprehensive framework for investigating synchronization, particularly global synchronization, in multilayer networks with higher-order interactions. Our framework considers interactions beyond pairwise connections, both within and across layers. We demonstrate the existence of a stable global synchronous state, with a condition resembling the master stability function, contingent on the choice of coupling functions. Our theoretical findings are supported by simulations using Hindmarsh-Rose neuronal and R\\\"{o}ssler oscillators. These simulations illustrate how synchronization is facilitated by higher-order interactions, both within and across layers, highlighting the advantages over scenarios involving interactions within single layers.","sentences":["Networks incorporating higher-order interactions are increasingly recognized for their ability to introduce novel dynamics into various processes, including synchronization.","Previous studies on synchronization within multilayer networks have often been limited to specific models, such as the Kuramoto model, or have focused solely on higher-order interactions within individual layers.","Here, we present a comprehensive framework for investigating synchronization, particularly global synchronization, in multilayer networks with higher-order interactions.","Our framework considers interactions beyond pairwise connections, both within and across layers.","We demonstrate the existence of a stable global synchronous state, with a condition resembling the master stability function, contingent on the choice of coupling functions.","Our theoretical findings are supported by simulations using Hindmarsh-Rose neuronal and R\\\"{o}ssler oscillators.","These simulations illustrate how synchronization is facilitated by higher-order interactions, both within and across layers, highlighting the advantages over scenarios involving interactions within single layers."],"url":"http://arxiv.org/abs/2406.03771v1","category":"nlin.AO"}
{"created":"2024-06-06 06:12:15","title":"Privacy Preserving Semi-Decentralized Mean Estimation over Intermittently-Connected Networks","abstract":"We consider the problem of privately estimating the mean of vectors distributed across different nodes of an unreliable wireless network, where communications between nodes can fail intermittently. We adopt a semi-decentralized setup, wherein to mitigate the impact of intermittently connected links, nodes can collaborate with their neighbors to compute a local consensus, which they relay to a central server. In such a setting, the communications between any pair of nodes must ensure that the privacy of the nodes is rigorously maintained to prevent unauthorized information leakage. We study the tradeoff between collaborative relaying and privacy leakage due to the data sharing among nodes and, subsequently, propose PriCER: Private Collaborative Estimation via Relaying -- a differentially private collaborative algorithm for mean estimation to optimize this tradeoff. The privacy guarantees of PriCER arise (i) implicitly, by exploiting the inherent stochasticity of the flaky network connections, and (ii) explicitly, by adding Gaussian perturbations to the estimates exchanged by the nodes. Local and central privacy guarantees are provided against eavesdroppers who can observe different signals, such as the communications amongst nodes during local consensus and (possibly multiple) transmissions from the relays to the central server. We substantiate our theoretical findings with numerical simulations. Our implementation is available at https://github.com/rajarshisaha95/private-collaborative-relaying.","sentences":["We consider the problem of privately estimating the mean of vectors distributed across different nodes of an unreliable wireless network, where communications between nodes can fail intermittently.","We adopt a semi-decentralized setup, wherein to mitigate the impact of intermittently connected links, nodes can collaborate with their neighbors to compute a local consensus, which they relay to a central server.","In such a setting, the communications between any pair of nodes must ensure that the privacy of the nodes is rigorously maintained to prevent unauthorized information leakage.","We study the tradeoff between collaborative relaying and privacy leakage due to the data sharing among nodes and, subsequently, propose PriCER: Private Collaborative Estimation via Relaying -- a differentially private collaborative algorithm for mean estimation to optimize this tradeoff.","The privacy guarantees of PriCER arise (i) implicitly, by exploiting the inherent stochasticity of the flaky network connections, and (ii) explicitly, by adding Gaussian perturbations to the estimates exchanged by the nodes.","Local and central privacy guarantees are provided against eavesdroppers who can observe different signals, such as the communications amongst nodes during local consensus and (possibly multiple) transmissions from the relays to the central server.","We substantiate our theoretical findings with numerical simulations.","Our implementation is available at https://github.com/rajarshisaha95/private-collaborative-relaying."],"url":"http://arxiv.org/abs/2406.03766v1","category":"eess.SP"}
{"created":"2024-06-06 05:50:09","title":"A second-order accurate, original energy dissipative numerical scheme for chemotaxis and its convergence analysis","abstract":"This paper proposes a second-order accurate numerical scheme for the Patlak-Keller-Segel system with various mobilities for the description of chemotaxis. Formulated in a variational structure, the entropy part is novelly discretized by a modified Crank-Nicolson approach so that the solution to the proposed nonlinear scheme corresponds to a minimizer of a convex functional. A careful theoretical analysis reveals that the unique solvability and positivity-preserving property could be theoretically justified. More importantly, such a second order numerical scheme is able to preserve the dissipative property of the original energy functional, instead of a modified one. To the best of our knowledge, the proposed scheme is the first second-order accurate one in literature that could achieve both the numerical positivity and original energy dissipation. In addition, an optimal rate convergence estimate is provided for the proposed scheme, in which rough and refined error estimate techniques have to be included to accomplish such an analysis. Ample numerical results are presented to demonstrate robust performance of the proposed scheme in preserving positivity and original energy dissipation in blowup simulations.","sentences":["This paper proposes a second-order accurate numerical scheme for the Patlak-Keller-Segel system with various mobilities for the description of chemotaxis.","Formulated in a variational structure, the entropy part is novelly discretized by a modified Crank-Nicolson approach so that the solution to the proposed nonlinear scheme corresponds to a minimizer of a convex functional.","A careful theoretical analysis reveals that the unique solvability and positivity-preserving property could be theoretically justified.","More importantly, such a second order numerical scheme is able to preserve the dissipative property of the original energy functional, instead of a modified one.","To the best of our knowledge, the proposed scheme is the first second-order accurate one in literature that could achieve both the numerical positivity and original energy dissipation.","In addition, an optimal rate convergence estimate is provided for the proposed scheme, in which rough and refined error estimate techniques have to be included to accomplish such an analysis.","Ample numerical results are presented to demonstrate robust performance of the proposed scheme in preserving positivity and original energy dissipation in blowup simulations."],"url":"http://arxiv.org/abs/2406.03761v1","category":"math.NA"}
{"created":"2024-06-06 05:49:20","title":"Maximum Likelihood Identification of Uncontrollable Linear Time-Invariant Models for Offset-Free Control","abstract":"Maximum likelihood identification of linear time-invariant models is a difficult problem because it is, in general, a nonlinear semidefinite program, with semidefinite covariance matrix arguments and semidefinite filter stability constraints. To enforce filter stability, we establish a general theory of closed constraints on the system eigenvalues using LMI regions. To solve the identification problem, we employ a Cholesky factorization method that reduces the semidefinite program to a standard nonlinear program. Finally, we apply the identification algorithm to a class of linear plant and disturbance models commonly used in offset-free model predictive control applications. Specifically, we consider models that are structured with uncontrollable, integrating disturbance states. We solve this disturbance modeling problem, and validate the resulting controller and estimator performance, in two real-world case studies: first, a low-cost benchmark temperature control laboratory, and second, an industrial-scale chemical reactor at Eastman Chemical's Kingsport plant.","sentences":["Maximum likelihood identification of linear time-invariant models is a difficult problem because it is, in general, a nonlinear semidefinite program, with semidefinite covariance matrix arguments and semidefinite filter stability constraints.","To enforce filter stability, we establish a general theory of closed constraints on the system eigenvalues using LMI regions.","To solve the identification problem, we employ a Cholesky factorization method that reduces the semidefinite program to a standard nonlinear program.","Finally, we apply the identification algorithm to a class of linear plant and disturbance models commonly used in offset-free model predictive control applications.","Specifically, we consider models that are structured with uncontrollable, integrating disturbance states.","We solve this disturbance modeling problem, and validate the resulting controller and estimator performance, in two real-world case studies: first, a low-cost benchmark temperature control laboratory, and second, an industrial-scale chemical reactor at Eastman Chemical's Kingsport plant."],"url":"http://arxiv.org/abs/2406.03760v1","category":"eess.SY"}
{"created":"2024-06-06 05:41:47","title":"RoboCoder: Robotic Learning from Basic Skills to General Tasks with Large Language Models","abstract":"The emergence of Large Language Models (LLMs) has improved the prospects for robotic tasks. However, existing benchmarks are still limited to single tasks with limited generalization capabilities. In this work, we introduce a comprehensive benchmark and an autonomous learning framework, RoboCoder aimed at enhancing the generalization capabilities of robots in complex environments. Unlike traditional methods that focus on single-task learning, our research emphasizes the development of a general-purpose robotic coding algorithm that enables robots to leverage basic skills to tackle increasingly complex tasks. The newly proposed benchmark consists of 80 manually designed tasks across 7 distinct entities, testing the models' ability to learn from minimal initial mastery. Initial testing revealed that even advanced models like GPT-4 could only achieve a 47% pass rate in three-shot scenarios with humanoid entities. To address these limitations, the RoboCoder framework integrates Large Language Models (LLMs) with a dynamic learning system that uses real-time environmental feedback to continuously update and refine action codes. This adaptive method showed a remarkable improvement, achieving a 36% relative improvement. Our codes will be released.","sentences":["The emergence of Large Language Models (LLMs) has improved the prospects for robotic tasks.","However, existing benchmarks are still limited to single tasks with limited generalization capabilities.","In this work, we introduce a comprehensive benchmark and an autonomous learning framework, RoboCoder aimed at enhancing the generalization capabilities of robots in complex environments.","Unlike traditional methods that focus on single-task learning, our research emphasizes the development of a general-purpose robotic coding algorithm that enables robots to leverage basic skills to tackle increasingly complex tasks.","The newly proposed benchmark consists of 80 manually designed tasks across 7 distinct entities, testing the models' ability to learn from minimal initial mastery.","Initial testing revealed that even advanced models like GPT-4 could only achieve a 47% pass rate in three-shot scenarios with humanoid entities.","To address these limitations, the RoboCoder framework integrates Large Language Models (LLMs) with a dynamic learning system that uses real-time environmental feedback to continuously update and refine action codes.","This adaptive method showed a remarkable improvement, achieving a 36% relative improvement.","Our codes will be released."],"url":"http://arxiv.org/abs/2406.03757v1","category":"cs.RO"}
{"created":"2024-06-06 05:28:09","title":"Model fusion for efficient learning of nonlinear dynamical systems","abstract":"In the context of model-based control of industrial processes, it is a common practice to develop a data-driven linear dynamical model around a specified operating point. However, in applications involving wider operating conditions, representation of the dynamics using a single linear dynamic model is often inadequate, requiring either a nonlinear model or multiple linear models to accommodate the nonlinear behaviour. While the development of the former suffers from the requirements of extensive experiments spanning multiple levels, significant compromise in the nominal product quality and dealing with unmeasured disturbances over wider operating conditions, the latter faces the challenge of model switch scheduling and inadequate description of dynamics for the operating regions in-between. To overcome these challenges, we propose an efficient approach to obtain a parsimonious nonlinear dynamic model by developing multiple linear models from data at multiple operating points, lifting the data features obtained from individual model simulations to adequately accommodate the underlying nonlinear behaviour and finally, sparse optimization techniques to obtain a parsimonious model. The performance and effectiveness of the proposed algorithm is demonstrated through simulation case studies.","sentences":["In the context of model-based control of industrial processes, it is a common practice to develop a data-driven linear dynamical model around a specified operating point.","However, in applications involving wider operating conditions, representation of the dynamics using a single linear dynamic model is often inadequate, requiring either a nonlinear model or multiple linear models to accommodate the nonlinear behaviour.","While the development of the former suffers from the requirements of extensive experiments spanning multiple levels, significant compromise in the nominal product quality and dealing with unmeasured disturbances over wider operating conditions, the latter faces the challenge of model switch scheduling and inadequate description of dynamics for the operating regions in-between.","To overcome these challenges, we propose an efficient approach to obtain a parsimonious nonlinear dynamic model by developing multiple linear models from data at multiple operating points, lifting the data features obtained from individual model simulations to adequately accommodate the underlying nonlinear behaviour and finally, sparse optimization techniques to obtain a parsimonious model.","The performance and effectiveness of the proposed algorithm is demonstrated through simulation case studies."],"url":"http://arxiv.org/abs/2406.03752v1","category":"eess.SY"}
{"created":"2024-06-06 05:27:33","title":"Adaptive Multi-Scale Decomposition Framework for Time Series Forecasting","abstract":"Transformer-based and MLP-based methods have emerged as leading approaches in time series forecasting (TSF). While Transformer-based methods excel in capturing long-range dependencies, they suffer from high computational complexities and tend to overfit. Conversely, MLP-based methods offer computational efficiency and adeptness in modeling temporal dynamics, but they struggle with capturing complex temporal patterns effectively. To address these challenges, we propose a novel MLP-based Adaptive Multi-Scale Decomposition (AMD) framework for TSF. Our framework decomposes time series into distinct temporal patterns at multiple scales, leveraging the Multi-Scale Decomposable Mixing (MDM) block to dissect and aggregate these patterns in a residual manner. Complemented by the Dual Dependency Interaction (DDI) block and the Adaptive Multi-predictor Synthesis (AMS) block, our approach effectively models both temporal and channel dependencies and utilizes autocorrelation to refine multi-scale data integration. Comprehensive experiments demonstrate that our AMD framework not only overcomes the limitations of existing methods but also consistently achieves state-of-the-art performance in both long-term and short-term forecasting tasks across various datasets, showcasing superior efficiency. Code is available at \\url{https://github.com/TROUBADOUR000/AMD}","sentences":["Transformer-based and MLP-based methods have emerged as leading approaches in time series forecasting (TSF).","While Transformer-based methods excel in capturing long-range dependencies, they suffer from high computational complexities and tend to overfit.","Conversely, MLP-based methods offer computational efficiency and adeptness in modeling temporal dynamics, but they struggle with capturing complex temporal patterns effectively.","To address these challenges, we propose a novel MLP-based Adaptive Multi-Scale Decomposition (AMD) framework for TSF.","Our framework decomposes time series into distinct temporal patterns at multiple scales, leveraging the Multi-Scale Decomposable Mixing (MDM) block to dissect and aggregate these patterns in a residual manner.","Complemented by the Dual Dependency Interaction (DDI) block and the Adaptive Multi-predictor Synthesis (AMS) block, our approach effectively models both temporal and channel dependencies and utilizes autocorrelation to refine multi-scale data integration.","Comprehensive experiments demonstrate that our AMD framework not only overcomes the limitations of existing methods but also consistently achieves state-of-the-art performance in both long-term and short-term forecasting tasks across various datasets, showcasing superior efficiency.","Code is available at \\url{https://github.com/TROUBADOUR000/AMD}"],"url":"http://arxiv.org/abs/2406.03751v1","category":"cs.LG"}
{"created":"2024-06-06 05:21:03","title":"Stochastic Dynamic Network Utility Maximization with Application to Disaster Response","abstract":"In this paper, we are interested in solving Network Utility Maximization (NUM) problems whose underlying local utilities and constraints depend on a complex stochastic dynamic environment. While the general model applies broadly, this work is motivated by resource sharing during disasters concurrently occurring in multiple areas. In such situations, hierarchical layers of Incident Command Systems (ICS) are engaged; specifically, a central entity (e.g., the federal government) typically coordinates the incident response allocating resources to different sites, which then get distributed to the affected by local entities. The benefits of an allocation decision to the different sites are generally not expressed explicitly as a closed-form utility function because of the complexity of the response and the random nature of the underlying phenomenon we try to contain. We use the classic approach of decomposing the NUM formulation and applying a primal-dual algorithm to achieve optimal higher-level decisions under coupled constraints while modeling the optimized response to the local dynamics with deep reinforcement learning algorithms.   The decomposition we propose has several benefits: 1) the entities respond to their local utilities based on a congestion signal conveyed by the ICS upper layers; 2) the complexity of capturing the utility of local responses and their diversity is addressed effectively without sharing local parameters and priorities with the ICS layers above; 3) utilities, known as explicit functions, are approximated as convex functions of the resources allocated; 4) decisions rely on up-to-date data from the ground along with future forecasts.","sentences":["In this paper, we are interested in solving Network Utility Maximization (NUM) problems whose underlying local utilities and constraints depend on a complex stochastic dynamic environment.","While the general model applies broadly, this work is motivated by resource sharing during disasters concurrently occurring in multiple areas.","In such situations, hierarchical layers of Incident Command Systems (ICS) are engaged; specifically, a central entity (e.g., the federal government) typically coordinates the incident response allocating resources to different sites, which then get distributed to the affected by local entities.","The benefits of an allocation decision to the different sites are generally not expressed explicitly as a closed-form utility function because of the complexity of the response and the random nature of the underlying phenomenon we try to contain.","We use the classic approach of decomposing the NUM formulation and applying a primal-dual algorithm to achieve optimal higher-level decisions under coupled constraints while modeling the optimized response to the local dynamics with deep reinforcement learning algorithms.   ","The decomposition we propose has several benefits: 1) the entities respond to their local utilities based on a congestion signal conveyed by the ICS upper layers; 2) the complexity of capturing the utility of local responses and their diversity is addressed effectively without sharing local parameters and priorities with the ICS layers above; 3) utilities, known as explicit functions, are approximated as convex functions of the resources allocated; 4) decisions rely on up-to-date data from the ground along with future forecasts."],"url":"http://arxiv.org/abs/2406.03750v1","category":"eess.SY"}
{"created":"2024-06-06 17:59:09","title":"Causal Estimation of Memorisation Profiles","abstract":"Understanding memorisation in language models has practical and societal implications, e.g., studying models' training dynamics or preventing copyright infringements. Prior work defines memorisation as the causal effect of training with an instance on the model's ability to predict that instance. This definition relies on a counterfactual: the ability to observe what would have happened had the model not seen that instance. Existing methods struggle to provide computationally efficient and accurate estimates of this counterfactual. Further, they often estimate memorisation for a model architecture rather than for a specific model instance. This paper fills an important gap in the literature, proposing a new, principled, and efficient method to estimate memorisation based on the difference-in-differences design from econometrics. Using this method, we characterise a model's memorisation profile--its memorisation trends across training--by only observing its behaviour on a small set of instances throughout training. In experiments with the Pythia model suite, we find that memorisation (i) is stronger and more persistent in larger models, (ii) is determined by data order and learning rate, and (iii) has stable trends across model sizes, thus making memorisation in larger models predictable from smaller ones.","sentences":["Understanding memorisation in language models has practical and societal implications, e.g., studying models' training dynamics or preventing copyright infringements.","Prior work defines memorisation as the causal effect of training with an instance on the model's ability to predict that instance.","This definition relies on a counterfactual: the ability to observe what would have happened had the model not seen that instance.","Existing methods struggle to provide computationally efficient and accurate estimates of this counterfactual.","Further, they often estimate memorisation for a model architecture rather than for a specific model instance.","This paper fills an important gap in the literature, proposing a new, principled, and efficient method to estimate memorisation based on the difference-in-differences design from econometrics.","Using this method, we characterise a model's memorisation profile--its memorisation trends across training--by only observing its behaviour on a small set of instances throughout training.","In experiments with the Pythia model suite, we find that memorisation (i) is stronger and more persistent in larger models, (ii) is determined by data order and learning rate, and (iii) has stable trends across model sizes, thus making memorisation in larger models predictable from smaller ones."],"url":"http://arxiv.org/abs/2406.04327v1","category":"cs.LG"}
{"created":"2024-06-06 17:59:09","title":"The Brain's Bitter Lesson: Scaling Speech Decoding With Self-Supervised Learning","abstract":"The past few years have produced a series of spectacular advances in the decoding of speech from brain activity. The engine of these advances has been the acquisition of labelled data, with increasingly large datasets acquired from single subjects. However, participants exhibit anatomical and other individual differences, and datasets use varied scanners and task designs. As a result, prior work has struggled to leverage data from multiple subjects, multiple datasets, multiple tasks, and unlabelled datasets. In turn, the field has not benefited from the rapidly growing number of open neural data repositories to exploit large-scale data and deep learning. To address this, we develop an initial set of neuroscience-inspired self-supervised objectives, together with a neural architecture, for representation learning from heterogeneous and unlabelled neural recordings. Experimental results show that representations learned with these objectives generalise across subjects, datasets, and tasks, and are also learned faster than using only labelled data. In addition, we set new benchmarks for two foundational speech decoding tasks. Taken together, these methods now unlock the potential for training speech decoding models with orders of magnitude more existing data.","sentences":["The past few years have produced a series of spectacular advances in the decoding of speech from brain activity.","The engine of these advances has been the acquisition of labelled data, with increasingly large datasets acquired from single subjects.","However, participants exhibit anatomical and other individual differences, and datasets use varied scanners and task designs.","As a result, prior work has struggled to leverage data from multiple subjects, multiple datasets, multiple tasks, and unlabelled datasets.","In turn, the field has not benefited from the rapidly growing number of open neural data repositories to exploit large-scale data and deep learning.","To address this, we develop an initial set of neuroscience-inspired self-supervised objectives, together with a neural architecture, for representation learning from heterogeneous and unlabelled neural recordings.","Experimental results show that representations learned with these objectives generalise across subjects, datasets, and tasks, and are also learned faster than using only labelled data.","In addition, we set new benchmarks for two foundational speech decoding tasks.","Taken together, these methods now unlock the potential for training speech decoding models with orders of magnitude more existing data."],"url":"http://arxiv.org/abs/2406.04328v1","category":"cs.LG"}
{"created":"2024-06-06 17:55:02","title":"Approximation-Aware Bayesian Optimization","abstract":"High-dimensional Bayesian optimization (BO) tasks such as molecular design often require 10,000 function evaluations before obtaining meaningful results. While methods like sparse variational Gaussian processes (SVGPs) reduce computational requirements in these settings, the underlying approximations result in suboptimal data acquisitions that slow the progress of optimization. In this paper we modify SVGPs to better align with the goals of BO: targeting informed data acquisition rather than global posterior fidelity. Using the framework of utility-calibrated variational inference, we unify GP approximation and data acquisition into a joint optimization problem, thereby ensuring optimal decisions under a limited computational budget. Our approach can be used with any decision-theoretic acquisition function and is compatible with trust region methods like TuRBO. We derive efficient joint objectives for the expected improvement and knowledge gradient acquisition functions in both the standard and batch BO settings. Our approach outperforms standard SVGPs on high-dimensional benchmark tasks in control and molecular design.","sentences":["High-dimensional Bayesian optimization (BO) tasks such as molecular design often require 10,000 function evaluations before obtaining meaningful results.","While methods like sparse variational Gaussian processes (SVGPs) reduce computational requirements in these settings, the underlying approximations result in suboptimal data acquisitions that slow the progress of optimization.","In this paper we modify SVGPs to better align with the goals of BO: targeting informed data acquisition rather than global posterior fidelity.","Using the framework of utility-calibrated variational inference, we unify GP approximation and data acquisition into a joint optimization problem, thereby ensuring optimal decisions under a limited computational budget.","Our approach can be used with any decision-theoretic acquisition function and is compatible with trust region methods like TuRBO.","We derive efficient joint objectives for the expected improvement and knowledge gradient acquisition functions in both the standard and batch BO settings.","Our approach outperforms standard SVGPs on high-dimensional benchmark tasks in control and molecular design."],"url":"http://arxiv.org/abs/2406.04308v1","category":"cs.LG"}
{"created":"2024-06-06 17:51:49","title":"Information Benchmark for Biological Sensors Beyond Steady States -- Mpemba-like sensory withdrawal effect","abstract":"Biological sensors rely on the temporal dynamics of ligand concentration for signaling. The sensory performance is bounded by the distinguishability between the sensory state transition dynamics under different environmental protocols. This work presents a comprehensive theory to characterize arbitrary transient sensory dynamics of biological sensors. Here the sensory performance is quantified by the Kullback-Leibler (KL) divergence between the probability distributions of the sensor's stochastic paths. We introduce a novel benchmark to assess a sensor's transient sensory performance arbitrarily far from equilibrium. We identify a counter-intuitive phenomenon in multi-state sensors: while an initial exposure to high ligand concentration may hinder a sensor's sensitivity towards a future concentration up-shift, certain sensors may show a boost in sensitivity if the initial high concentration exposure is followed by a transient resetting at a low concentration environment. The boosted performance exceeds that of a sensor starting from an initially low concentration environment. This effect, reminiscent of a drug withdrawal effect, can be explained by the Markovian dynamics of the multi-state sensor, similar to the Markovian Mpemba effect. Moreover, an exhaustive machine learning study of 4-state sensors reveals a tight connection between the sensor's performance and the structure of the Markovian graph of its states.","sentences":["Biological sensors rely on the temporal dynamics of ligand concentration for signaling.","The sensory performance is bounded by the distinguishability between the sensory state transition dynamics under different environmental protocols.","This work presents a comprehensive theory to characterize arbitrary transient sensory dynamics of biological sensors.","Here the sensory performance is quantified by the Kullback-Leibler (KL) divergence between the probability distributions of the sensor's stochastic paths.","We introduce a novel benchmark to assess a sensor's transient sensory performance arbitrarily far from equilibrium.","We identify a counter-intuitive phenomenon in multi-state sensors: while an initial exposure to high ligand concentration may hinder a sensor's sensitivity towards a future concentration up-shift, certain sensors may show a boost in sensitivity if the initial high concentration exposure is followed by a transient resetting at a low concentration environment.","The boosted performance exceeds that of a sensor starting from an initially low concentration environment.","This effect, reminiscent of a drug withdrawal effect, can be explained by the Markovian dynamics of the multi-state sensor, similar to the Markovian Mpemba effect.","Moreover, an exhaustive machine learning study of 4-state sensors reveals a tight connection between the sensor's performance and the structure of the Markovian graph of its states."],"url":"http://arxiv.org/abs/2406.04304v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-06 17:29:12","title":"Robust preparation of ground state phases under noisy imaginary time evolution","abstract":"Non-unitary state preparation protocols such as imaginary time evolution (ITE) offer substantial advantages relative to unitary ones, including the ability to prepare certain long-range correlated states more efficiently. Here, we ask whether such protocols are also robust to noise arising due to coupling to the environment. We consider a non-unitary ITE \"circuit\" subjected to a variety of noise models and investigate whether the resulting steady state remains in the same phase as the target state of the ITE at finite noise strength. Taking the one-dimensional quantum Ising model as a concrete example, we find that the ground state order and associated phase transition persist in the presence of noise, provided the noise does not explicitly break the symmetry that protects the phase transition. That is, the noise must possess the protecting symmetry in a weak (or average) form. Our analysis is facilitated by a mapping to an effective Hamiltonian picture in a doubled Hilbert space. We discuss possible implications of these findings for quantum simulation on noisy quantum hardware.","sentences":["Non-unitary state preparation protocols such as imaginary time evolution (ITE) offer substantial advantages relative to unitary ones, including the ability to prepare certain long-range correlated states more efficiently.","Here, we ask whether such protocols are also robust to noise arising due to coupling to the environment.","We consider a non-unitary ITE \"circuit\" subjected to a variety of noise models and investigate whether the resulting steady state remains in the same phase as the target state of the ITE at finite noise strength.","Taking the one-dimensional quantum Ising model as a concrete example, we find that the ground state order and associated phase transition persist in the presence of noise, provided the noise does not explicitly break the symmetry that protects the phase transition.","That is, the noise must possess the protecting symmetry in a weak (or average) form.","Our analysis is facilitated by a mapping to an effective Hamiltonian picture in a doubled Hilbert space.","We discuss possible implications of these findings for quantum simulation on noisy quantum hardware."],"url":"http://arxiv.org/abs/2406.04285v1","category":"quant-ph"}
{"created":"2024-06-06 17:22:11","title":"Entangling Quantum Memories at Channel Capacity","abstract":"Entangling quantum memories, mediated by optical-frequency or microwave channels, at high rates and fidelities is key for linking qubits across short and long ranges. All well-known protocols encode up to one qubit per optical mode, hence entangling one pair of memory qubits per transmitted mode over the channel, with probability $\\eta$, the channel's transmissivity. The rate is proportional to $\\eta$ ideal Bell states (ebits) per mode. The quantum capacity, $C(\\eta) = -\\log_2(1-{\\eta})$ ebits per mode, which $\\approx 1.44\\eta$ for high loss, i.e., $\\eta \\ll 1$, thereby making these schemes near rate-optimal. However, $C(\\eta) \\to \\infty$ as $\\eta \\to 1$, making the known schemes highly rate-suboptimal for shorter ranges. We show that a cavity-assisted memory-photon interface can be used to entangle matter memories with Gottesman-Kitaev-Preskill (GKP) photonic qudits, which along with dual-homodyne entanglement swaps that retain analog information, enables entangling memories at capacity-approaching rates at low loss. We benefit from loss resilience of GKP qudits, and their ability to encode multiple qubits in one mode. Our memory-photon interface further supports the preparation of needed ancilla GKP qudits. We expect our result to spur research in low-loss high-cooperativity cavity-coupled qubits with high-efficiency optical coupling, and demonstrations of high-rate short-range quantum links.","sentences":["Entangling quantum memories, mediated by optical-frequency or microwave channels, at high rates and fidelities is key for linking qubits across short and long ranges.","All well-known protocols encode up to one qubit per optical mode, hence entangling one pair of memory qubits per transmitted mode over the channel, with probability $\\eta$, the channel's transmissivity.","The rate is proportional to $\\eta$ ideal Bell states (ebits) per mode.","The quantum capacity, $C(\\eta) = -\\log_2(1-{\\eta})$ ebits per mode, which $\\approx 1.44\\eta$ for high loss, i.e., $\\eta \\ll 1$, thereby making these schemes near rate-optimal.","However, $C(\\eta) \\to \\infty$ as $\\eta \\to 1$, making the known schemes highly rate-suboptimal for shorter ranges.","We show that a cavity-assisted memory-photon interface can be used to entangle matter memories with Gottesman-Kitaev-Preskill (GKP) photonic qudits, which along with dual-homodyne entanglement swaps that retain analog information, enables entangling memories at capacity-approaching rates at low loss.","We benefit from loss resilience of GKP qudits, and their ability to encode multiple qubits in one mode.","Our memory-photon interface further supports the preparation of needed ancilla GKP qudits.","We expect our result to spur research in low-loss high-cooperativity cavity-coupled qubits with high-efficiency optical coupling, and demonstrations of high-rate short-range quantum links."],"url":"http://arxiv.org/abs/2406.04272v1","category":"quant-ph"}
{"created":"2024-06-06 17:21:47","title":"Optimization of state parameters in displacement assisted photon subtracted measurement-device-independent quantum key distribution","abstract":"Non-Gaussian operations, in particular, photon subtraction (PS), have been shown to enhance the performance of various quantum information processing tasks including continuous variable measurement device independent quantum key distribution (CV-MDI-QKD). This work investigates the role of non-Gaussian resource states, namely, the photon subtracted two-mode squeezed coherent (PSTMSC) (which include photon subtracted two-mode squeezed vacuum (PSTMSV) as a special case) states in CV-MDI-QKD. To this end, we derive the Wigner characteristic function for the resource states, from which the covariance matrix and, finally, the secret key rate expressions are extracted. The optimization of the state parameters is undertaken to find the most suitable resource states in this family of states. There have been previous studies on the PSTMSV and PSTMSC states in CV-MDI-QKD that make use of PS operation. We evaluate such proposals and find to our surprise that both PSTMSC and PSTMSV resource states underperform as compared to the TMSV state rendering PS operation and displacement undesirable.","sentences":["Non-Gaussian operations, in particular, photon subtraction (PS), have been shown to enhance the performance of various quantum information processing tasks including continuous variable measurement device independent quantum key distribution (CV-MDI-QKD).","This work investigates the role of non-Gaussian resource states, namely, the photon subtracted two-mode squeezed coherent (PSTMSC) (which include photon subtracted two-mode squeezed vacuum (PSTMSV) as a special case) states in CV-MDI-QKD.","To this end, we derive the Wigner characteristic function for the resource states, from which the covariance matrix and, finally, the secret key rate expressions are extracted.","The optimization of the state parameters is undertaken to find the most suitable resource states in this family of states.","There have been previous studies on the PSTMSV and PSTMSC states in CV-MDI-QKD that make use of PS operation.","We evaluate such proposals and find to our surprise that both PSTMSC and PSTMSV resource states underperform as compared to the TMSV state rendering PS operation and displacement undesirable."],"url":"http://arxiv.org/abs/2406.04270v1","category":"quant-ph"}
{"created":"2024-06-06 17:20:21","title":"Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement","abstract":"Deep learning-based speech enhancement (SE) models have achieved impressive performance in the past decade. Numerous advanced architectures have been designed to deliver state-of-the-art performance; however, their scalability potential remains unrevealed. Meanwhile, the majority of research focuses on small-sized datasets with restricted diversity, leading to a plateau in performance improvement. In this paper, we aim to provide new insights for addressing the above issues by exploring the scalability of SE models in terms of architectures, model sizes, compute budgets, and dataset sizes. Our investigation involves several popular SE architectures and speech data from different domains. Experiments reveal both similarities and distinctions between the scaling effects in SE and other tasks such as speech recognition. These findings further provide insights into the under-explored SE directions, e.g., larger-scale multi-domain corpora and efficiently scalable architectures.","sentences":["Deep learning-based speech enhancement (SE) models have achieved impressive performance in the past decade.","Numerous advanced architectures have been designed to deliver state-of-the-art performance; however, their scalability potential remains unrevealed.","Meanwhile, the majority of research focuses on small-sized datasets with restricted diversity, leading to a plateau in performance improvement.","In this paper, we aim to provide new insights for addressing the above issues by exploring the scalability of SE models in terms of architectures, model sizes, compute budgets, and dataset sizes.","Our investigation involves several popular SE architectures and speech data from different domains.","Experiments reveal both similarities and distinctions between the scaling effects in SE and other tasks such as speech recognition.","These findings further provide insights into the under-explored SE directions, e.g., larger-scale multi-domain corpora and efficiently scalable architectures."],"url":"http://arxiv.org/abs/2406.04269v1","category":"eess.AS"}
{"created":"2024-06-06 17:03:51","title":"Data Measurements for Decentralized Data Markets","abstract":"Decentralized data markets can provide more equitable forms of data acquisition for machine learning. However, to realize practical marketplaces, efficient techniques for seller selection need to be developed. We propose and benchmark federated data measurements to allow a data buyer to find sellers with relevant and diverse datasets. Diversity and relevance measures enable a buyer to make relative comparisons between sellers without requiring intermediate brokers and training task-dependent models.","sentences":["Decentralized data markets can provide more equitable forms of data acquisition for machine learning.","However, to realize practical marketplaces, efficient techniques for seller selection need to be developed.","We propose and benchmark federated data measurements to allow a data buyer to find sellers with relevant and diverse datasets.","Diversity and relevance measures enable a buyer to make relative comparisons between sellers without requiring intermediate brokers and training task-dependent models."],"url":"http://arxiv.org/abs/2406.04257v1","category":"cs.LG"}
{"created":"2024-06-06 16:50:33","title":"An adaptive parameter estimator for poor-quality spectral data of white dwarfs","abstract":"White dwarfs represent the end stage for 97% of stars, making precise parameter measurement crucial for understanding stellar evolution. Traditional estimation methods involve fitting spectra or photometry, which require high-quality data. In recent years, machine learning has played a crucial role in processing spectral data due to its speed, automation, and accuracy. However, two common issues have been identified. First, most studies rely on data with high signal-to-noise ratios (SNR > 10), leaving many poor-quality datasets underutilized. Second, existing machine learning models, primarily based on convolutional networks, recurrent networks, and their variants, cannot simultaneously capture both the spatial and sequential information of spectra. To address these challenges, we designed the Estimator Network (EstNet), an advanced algorithm integrating multiple techniques, including Residual Networks, Squeeze and Excitation Attention, Gated Recurrent Units, Adaptive Loss, and Monte-Carlo Dropout Layers. We conducted parameter estimation on 5,965 poor-quality white dwarf spectra (R~1800, SNR~1.17), achieving average percentage errors of 14.86% for effective temperature and 3.97% for surface gravity. These results are significantly superior to other mainstream algorithms and consistent with the outcomes of traditional theoretical spectrum fitting methods. In the future, our algorithms will be applied for large-scale parameter estimation on the Chinese Space Station Telescope and the Large Synoptic Survey Telescope.","sentences":["White dwarfs represent the end stage for 97% of stars, making precise parameter measurement crucial for understanding stellar evolution.","Traditional estimation methods involve fitting spectra or photometry, which require high-quality data.","In recent years, machine learning has played a crucial role in processing spectral data due to its speed, automation, and accuracy.","However, two common issues have been identified.","First, most studies rely on data with high signal-to-noise ratios (SNR > 10), leaving many poor-quality datasets underutilized.","Second, existing machine learning models, primarily based on convolutional networks, recurrent networks, and their variants, cannot simultaneously capture both the spatial and sequential information of spectra.","To address these challenges, we designed the Estimator Network (EstNet), an advanced algorithm integrating multiple techniques, including Residual Networks, Squeeze and Excitation Attention, Gated Recurrent Units, Adaptive Loss, and Monte-Carlo Dropout Layers.","We conducted parameter estimation on 5,965 poor-quality white dwarf spectra (R~1800, SNR~1.17), achieving average percentage errors of 14.86% for effective temperature and 3.97% for surface gravity.","These results are significantly superior to other mainstream algorithms and consistent with the outcomes of traditional theoretical spectrum fitting methods.","In the future, our algorithms will be applied for large-scale parameter estimation on the Chinese Space Station Telescope and the Large Synoptic Survey Telescope."],"url":"http://arxiv.org/abs/2406.04248v1","category":"astro-ph.SR"}
{"created":"2024-06-06 16:34:02","title":"Hidden zeros = secret BCFW scaling, and a new path to uniqueness","abstract":"We explore several features related to the newly discovered hidden amplitude zeros arXiv:2312.16282, which describes the vanishing of scattering amplitudes on special external kinematics. We prove the conjecture that amplitude zeros uniquely fix Tr $\\phi^3$, by showing that any amplitude can be divided into special subsets, which independently satisfy the zero condition. We further prove that the same subsets satisfy a previously unknown secret enhancement under Britto-Cachazo-Feng-Witten shifts, which turns out to be equivalent to the amplitude zero condition. Similar but weaker statements also hold for non-linear sigma model, while for Yang-Mills, when imposed together with color-kinematic duality, amplitude zeros uniquely fix the different tensor components of gluon amplitudes. This direction suggests a straightforward avenue for understanding previous uniqueness results, as well as the connections between naively independent properties of scattering amplitudes.","sentences":["We explore several features related to the newly discovered hidden amplitude zeros arXiv:2312.16282, which describes the vanishing of scattering amplitudes on special external kinematics.","We prove the conjecture that amplitude zeros uniquely fix Tr $\\phi^3$, by showing that any amplitude can be divided into special subsets, which independently satisfy the zero condition.","We further prove that the same subsets satisfy a previously unknown secret enhancement under Britto-Cachazo-Feng-Witten shifts, which turns out to be equivalent to the amplitude zero condition.","Similar but weaker statements also hold for non-linear sigma model, while for Yang-Mills, when imposed together with color-kinematic duality, amplitude zeros uniquely fix the different tensor components of gluon amplitudes.","This direction suggests a straightforward avenue for understanding previous uniqueness results, as well as the connections between naively independent properties of scattering amplitudes."],"url":"http://arxiv.org/abs/2406.04234v1","category":"hep-th"}
{"created":"2024-06-06 16:23:11","title":"C*-framework for higher-order bulk-boundary correspondences","abstract":"A typical crystal is a finite piece of a material which may be invariant under some point symmetry group. If it is a so-called intrinsic higher-order topological insulator or superconductor then it displays boundary modes at hinges or corners protected by the crystalline symmetry and the bulk topology. We explain the mechanism behind that using operator K-theory. Specifically, we derive a groupoid $C^\\ast$-algebra that 1) encodes the dynamics of the electrons in the infinite size limit of a crystal; 2) remembers the boundary conditions at the crystal's boundaries, and 3) accepts a natural action by the point symmetries of the atomic lattice. The filtrations of the groupoid's unit space by closed subsets that are invariant under the groupoid and point group actions supply equivariant co-filtrations of the groupoid $C^\\ast$-algebra. We show that specific derivations of the induced spectral sequences in twisted equivariant K-theories enumerate all non-trivial higher-order bulk-boundary correspondences.","sentences":["A typical crystal is a finite piece of a material which may be invariant under some point symmetry group.","If it is a so-called intrinsic higher-order topological insulator or superconductor then it displays boundary modes at hinges or corners protected by the crystalline symmetry and the bulk topology.","We explain the mechanism behind that using operator K-theory.","Specifically, we derive a groupoid $C^\\ast$-algebra that 1) encodes the dynamics of the electrons in the infinite size limit of a crystal; 2) remembers the boundary conditions at the crystal's boundaries, and 3) accepts a natural action by the point symmetries of the atomic lattice.","The filtrations of the groupoid's unit space by closed subsets that are invariant under the groupoid and point group actions supply equivariant co-filtrations of the groupoid $C^\\ast$-algebra.","We show that specific derivations of the induced spectral sequences in twisted equivariant K-theories enumerate all non-trivial higher-order bulk-boundary correspondences."],"url":"http://arxiv.org/abs/2406.04226v1","category":"math-ph"}
{"created":"2024-06-06 16:18:20","title":"Multi-Agent Imitation Learning: Value is Easy, Regret is Hard","abstract":"We study a multi-agent imitation learning (MAIL) problem where we take the perspective of a learner attempting to coordinate a group of agents based on demonstrations of an expert doing so. Most prior work in MAIL essentially reduces the problem to matching the behavior of the expert within the support of the demonstrations. While doing so is sufficient to drive the value gap between the learner and the expert to zero under the assumption that agents are non-strategic, it does not guarantee robustness to deviations by strategic agents. Intuitively, this is because strategic deviations can depend on a counterfactual quantity: the coordinator's recommendations outside of the state distribution their recommendations induce. In response, we initiate the study of an alternative objective for MAIL in Markov Games we term the regret gap that explicitly accounts for potential deviations by agents in the group. We first perform an in-depth exploration of the relationship between the value and regret gaps. First, we show that while the value gap can be efficiently minimized via a direct extension of single-agent IL algorithms, even value equivalence can lead to an arbitrarily large regret gap. This implies that achieving regret equivalence is harder than achieving value equivalence in MAIL. We then provide a pair of efficient reductions to no-regret online convex optimization that are capable of minimizing the regret gap (a) under a coverage assumption on the expert (MALICE) or (b) with access to a queryable expert (BLADES).","sentences":["We study a multi-agent imitation learning (MAIL) problem where we take the perspective of a learner attempting to coordinate a group of agents based on demonstrations of an expert doing so.","Most prior work in MAIL essentially reduces the problem to matching the behavior of the expert within the support of the demonstrations.","While doing so is sufficient to drive the value gap between the learner and the expert to zero under the assumption that agents are non-strategic, it does not guarantee robustness to deviations by strategic agents.","Intuitively, this is because strategic deviations can depend on a counterfactual quantity: the coordinator's recommendations outside of the state distribution their recommendations induce.","In response, we initiate the study of an alternative objective for MAIL in Markov Games we term the regret gap that explicitly accounts for potential deviations by agents in the group.","We first perform an in-depth exploration of the relationship between the value and regret gaps.","First, we show that while the value gap can be efficiently minimized via a direct extension of single-agent IL algorithms, even value equivalence can lead to an arbitrarily large regret gap.","This implies that achieving regret equivalence is harder than achieving value equivalence in MAIL.","We then provide a pair of efficient reductions to no-regret online convex optimization that are capable of minimizing the regret gap (a) under a coverage assumption on the expert (MALICE) or (b) with access to a queryable expert (BLADES)."],"url":"http://arxiv.org/abs/2406.04219v1","category":"cs.LG"}
{"created":"2024-06-06 15:58:36","title":"High-Fidelity Electron Spin Gates in a Scalable Diamond Quantum Register","abstract":"Diamond is a promising platform for quantum information processing as it can host highly coherent qubits that might allow for the construction of large quantum registers. A prerequisite for such devices is a coherent interaction between electron nitrogen vacancy (NV) spins. Entanglement between dipolar-coupled NV spin pairs has been demonstrated, but with a limited entanglement fidelity and its error sources have not been characterized. Here, we design a robust, easy to implement entangling gate between NV spins in diamond and quantify the influence of multiple error sources on the gate performance. Experimentally, we demonstrate a record gate fidelity of $F=(96.0 \\pm 2.5)$ % under ambient conditions. Our identification of the dominant errors paves the way towards NV-NV gates beyond the error correction threshold.","sentences":["Diamond is a promising platform for quantum information processing as it can host highly coherent qubits that might allow for the construction of large quantum registers.","A prerequisite for such devices is a coherent interaction between electron nitrogen vacancy (NV) spins.","Entanglement between dipolar-coupled NV spin pairs has been demonstrated, but with a limited entanglement fidelity and its error sources have not been characterized.","Here, we design a robust, easy to implement entangling gate between NV spins in diamond and quantify the influence of multiple error sources on the gate performance.","Experimentally, we demonstrate a record gate fidelity of $F=(96.0 \\pm 2.5)$ % under ambient conditions.","Our identification of the dominant errors paves the way towards NV-NV gates beyond the error correction threshold."],"url":"http://arxiv.org/abs/2406.04199v1","category":"quant-ph"}
{"created":"2024-06-06 15:26:51","title":"Parametric Intrusive Reduced Order Models enhanced with Machine Learning Correction Terms","abstract":"In this paper, we propose an equation-based parametric Reduced Order Model (ROM), whose accuracy is improved with data-driven terms added into the reduced equations. These additions have the aim of reintroducing contributions that in standard ROMs are not taken into account. In particular, in this work we consider two types of contributions: the turbulence modeling, added through a reduced-order approximation of the eddy viscosity field, and the correction model, aimed to re-introduce the contribution of the discarded modes. Both approaches have been investigated in previous works and the goal of this paper is to extend the model to a parametric setting making use of ad-hoc machine learning procedures. More in detail, we investigate different neural networks' architectures, from simple dense feed-forward to Long-Short Term Memory neural networks, in order to find the most suitable model for the re-introduced contributions. We tested the methods on two test cases with different behaviors: the periodic turbulent flow past a circular cylinder and the unsteady turbulent flow in a channel-driven cavity. In both cases, the parameter considered is the Reynolds number and the machine learning-enhanced ROM considerably improved the pressure and velocity accuracy with respect to the standard ROM.","sentences":["In this paper, we propose an equation-based parametric Reduced Order Model (ROM), whose accuracy is improved with data-driven terms added into the reduced equations.","These additions have the aim of reintroducing contributions that in standard ROMs are not taken into account.","In particular, in this work we consider two types of contributions: the turbulence modeling, added through a reduced-order approximation of the eddy viscosity field, and the correction model, aimed to re-introduce the contribution of the discarded modes.","Both approaches have been investigated in previous works and the goal of this paper is to extend the model to a parametric setting making use of ad-hoc machine learning procedures.","More in detail, we investigate different neural networks' architectures, from simple dense feed-forward to Long-Short Term Memory neural networks, in order to find the most suitable model for the re-introduced contributions.","We tested the methods on two test cases with different behaviors: the periodic turbulent flow past a circular cylinder and the unsteady turbulent flow in a channel-driven cavity.","In both cases, the parameter considered is the Reynolds number and the machine learning-enhanced ROM considerably improved the pressure and velocity accuracy with respect to the standard ROM."],"url":"http://arxiv.org/abs/2406.04169v1","category":"math.NA"}
{"created":"2024-06-06 15:26:23","title":"Majorana zero modes under electron correlation","abstract":"In this work, we perform a systematic investigation of the correlated topological superconductors (TSCs), especially their non-trivial Majorana zero modes (MZMs). Compared to the non-interacting MZMs, the emerged correlated MZMs become projected MZMs. To prove this, we study the topological superconducting nanowire under the Hubbard and Hatsugai-Kohmoto interactions. Both of them become correlated TSCs under a magnetic field. Their topological properties are numerically computed by the Wilson loop and entanglement spectrum. We successfully extract the projected MZMs connecting the ground state and excited state through exact diagonalization. We also extend our results to the spinful Kitaev chain. Our results can provide a new perspective and understanding of the correlated MZMs.","sentences":["In this work, we perform a systematic investigation of the correlated topological superconductors (TSCs), especially their non-trivial Majorana zero modes (MZMs).","Compared to the non-interacting MZMs, the emerged correlated MZMs become projected MZMs.","To prove this, we study the topological superconducting nanowire under the Hubbard and Hatsugai-Kohmoto interactions.","Both of them become correlated TSCs under a magnetic field.","Their topological properties are numerically computed by the Wilson loop and entanglement spectrum.","We successfully extract the projected MZMs connecting the ground state and excited state through exact diagonalization.","We also extend our results to the spinful Kitaev chain.","Our results can provide a new perspective and understanding of the correlated MZMs."],"url":"http://arxiv.org/abs/2406.04168v1","category":"cond-mat.supr-con"}
{"created":"2024-06-06 15:22:33","title":"Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe","abstract":"Text embeddings are essential for many tasks, such as document retrieval, clustering, and semantic similarity assessment. In this paper, we study how to contrastively train text embedding models in a compute-optimal fashion, given a suite of pre-trained decoder-only language models. Our innovation is an algorithm that produces optimal configurations of model sizes, data quantities, and fine-tuning methods for text-embedding models at different computational budget levels. The resulting recipe, which we obtain through extensive experiments, can be used by practitioners to make informed design choices for their embedding models. Specifically, our findings suggest that full fine-tuning and low-rank adaptation fine-tuning produce optimal models at lower and higher computational budgets respectively.","sentences":["Text embeddings are essential for many tasks, such as document retrieval, clustering, and semantic similarity assessment.","In this paper, we study how to contrastively train text embedding models in a compute-optimal fashion, given a suite of pre-trained decoder-only language models.","Our innovation is an algorithm that produces optimal configurations of model sizes, data quantities, and fine-tuning methods for text-embedding models at different computational budget levels.","The resulting recipe, which we obtain through extensive experiments, can be used by practitioners to make informed design choices for their embedding models.","Specifically, our findings suggest that full fine-tuning and low-rank adaptation fine-tuning produce optimal models at lower and higher computational budgets respectively."],"url":"http://arxiv.org/abs/2406.04165v1","category":"cs.LG"}
{"created":"2024-06-06 15:15:39","title":"A novel robust meta-analysis model using the $t$ distribution for outlier accommodation and detection","abstract":"Random effects meta-analysis model is an important tool for integrating results from multiple independent studies. However, the standard model is based on the assumption of normal distributions for both random effects and within-study errors, making it susceptible to outlying studies. Although robust modeling using the $t$ distribution is an appealing idea, the existing work, that explores the use of the $t$ distribution only for random effects, involves complicated numerical integration and numerical optimization. In this paper, a novel robust meta-analysis model using the $t$ distribution is proposed ($t$Meta). The novelty is that the marginal distribution of the effect size in $t$Meta follows the $t$ distribution, enabling that $t$Meta can simultaneously accommodate and detect outlying studies in a simple and adaptive manner. A simple and fast EM-type algorithm is developed for maximum likelihood estimation. Due to the mathematical tractability of the $t$ distribution, $t$Meta frees from numerical integration and allows for efficient optimization. Experiments on real data demonstrate that $t$Meta is compared favorably with related competitors in situations involving mild outliers. Moreover, in the presence of gross outliers, while related competitors may fail, $t$Meta continues to perform consistently and robustly.","sentences":["Random effects meta-analysis model is an important tool for integrating results from multiple independent studies.","However, the standard model is based on the assumption of normal distributions for both random effects and within-study errors, making it susceptible to outlying studies.","Although robust modeling using the $t$ distribution is an appealing idea, the existing work, that explores the use of the $t$ distribution only for random effects, involves complicated numerical integration and numerical optimization.","In this paper, a novel robust meta-analysis model using the $t$ distribution is proposed ($t$Meta).","The novelty is that the marginal distribution of the effect size in $t$Meta follows the $t$ distribution, enabling that $t$Meta can simultaneously accommodate and detect outlying studies in a simple and adaptive manner.","A simple and fast EM-type algorithm is developed for maximum likelihood estimation.","Due to the mathematical tractability of the $t$ distribution, $t$Meta frees from numerical integration and allows for efficient optimization.","Experiments on real data demonstrate that $t$Meta is compared favorably with related competitors in situations involving mild outliers.","Moreover, in the presence of gross outliers, while related competitors may fail, $t$Meta continues to perform consistently and robustly."],"url":"http://arxiv.org/abs/2406.04150v1","category":"stat.ME"}
{"created":"2024-06-06 15:13:15","title":"Direct optimization of neoclassical ion transport in stellarator reactors","abstract":"We directly optimize stellarator neoclassical ion transport while holding neoclassical electron transport at a moderate level, creating a scenario favorable for impurity expulsion and retaining good ion confinement. Traditional neoclassical stellarator optimization has focused on minimizing $\\epsilon_\\mathrm{eff}$, the geometric factor that characterizes the amount of radial transport due to particles in the $1/\\nu$ regime. Under expected reactor-relevant conditions, core electrons will be in the $1/\\nu$ regime and core fuel ions will be in the $\\sqrt{\\nu}$ regime. Traditional optimizations thus minimize electron transport and rely on the radial electric field $\\left(E_r\\right)$ that develops to confine the ions. This often results in an inward-pointing $E_r$ that drives high-$Z$ impurities into the core, which may be troublesome in future reactors. In our optimizations, we increase the ratio of the thermal transport coefficients $L_{1 1}^{e}/L_{1 1}^{i}$, which previous work has shown can create an outward-pointing $E_r$. This effect is very beneficial for impurity expulsion. We obtain self-consistent density, temperature, and $E_r$ profiles at reactor-relevant conditions for optimized equilibria. These equilibria are expected to enjoy significantly improved impurity transport properties. We conclude by providing several directions of future research that may help further improve the presented optimization algorithm.","sentences":["We directly optimize stellarator neoclassical ion transport while holding neoclassical electron transport at a moderate level, creating a scenario favorable for impurity expulsion and retaining good ion confinement.","Traditional neoclassical stellarator optimization has focused on minimizing $\\epsilon_\\mathrm{eff}$, the geometric factor that characterizes the amount of radial transport due to particles in the $1/\\nu$ regime.","Under expected reactor-relevant conditions, core electrons will be in the $1/\\nu$ regime and core fuel ions will be in the $\\sqrt{\\nu}$ regime.","Traditional optimizations thus minimize electron transport and rely on the radial electric field $\\left(E_r\\right)$ that develops to confine the ions.","This often results in an inward-pointing $E_r$ that drives high-$Z$ impurities into the core, which may be troublesome in future reactors.","In our optimizations, we increase the ratio of the thermal transport coefficients $L_{1 1}^{e}/L_{1 1}^{i}$, which previous work has shown can create an outward-pointing $E_r$. This effect is very beneficial for impurity expulsion.","We obtain self-consistent density, temperature, and $E_r$ profiles at reactor-relevant conditions for optimized equilibria.","These equilibria are expected to enjoy significantly improved impurity transport properties.","We conclude by providing several directions of future research that may help further improve the presented optimization algorithm."],"url":"http://arxiv.org/abs/2406.04147v1","category":"physics.plasm-ph"}
{"created":"2024-06-06 14:50:13","title":"Realization of higher coordinated Er in high-pressure cotunnite phase of Er$_2$Ti$_2$O$_7$","abstract":"In this article we report the structural stability of Er$_2$Ti$_2$O$_7$ cubic pyrochlore with pressure using x-ray diffraction, Raman spectroscopy, photoluminescence, x-ray absorption and ab-initio calculations. Our studies establish a phase transformation in Er$_2$Ti$_2$O$_7$ from ambient cubic phase to high-pressure orthorhombic (cotunnite) phase, initiated at ~40 GPa. The transformation is sluggish and it does not complete even at the highest measured pressure in our study i.e. ~60.0 GPa. This is further supported by the first principle calculations which reveal that cotunnite phase is energetically more stable than the ambient phase above ~53 GPa. After complete release of pressure, the high-pressure cotunnite phase is retained while the fraction of untransformed pyrochlore phase becomes amorphous. Furthermore, the EXAFS data of the recovered sample at L3 edge of Er3+ ion show an increase in the coordination number of cations from eight at ambient to nine in the high-pressure phase. The mechanism of structural transformation is explained in terms of accumulation of cation antisite defects and subsequent disordering of cations and anions in their respective sublattice. The amorphization of the pyrochlore phase upon release is interpreted as the inability of accommodating the point defects at ambient conditions, which are formed in the pyrochlore lattice under compression.","sentences":["In this article we report the structural stability of Er$_2$Ti$_2$O$_7$ cubic pyrochlore with pressure using x-ray diffraction, Raman spectroscopy, photoluminescence, x-ray absorption and ab-initio calculations.","Our studies establish a phase transformation in Er$_2$Ti$_2$O$_7$ from ambient cubic phase to high-pressure orthorhombic (cotunnite) phase, initiated at ~40 GPa.","The transformation is sluggish and it does not complete even at the highest measured pressure in our study i.e. ~60.0 GPa.","This is further supported by the first principle calculations which reveal that cotunnite phase is energetically more stable than the ambient phase above ~53 GPa.","After complete release of pressure, the high-pressure cotunnite phase is retained while the fraction of untransformed pyrochlore phase becomes amorphous.","Furthermore, the EXAFS data of the recovered sample at L3 edge of Er3+ ion show an increase in the coordination number of cations from eight at ambient to nine in the high-pressure phase.","The mechanism of structural transformation is explained in terms of accumulation of cation antisite defects and subsequent disordering of cations and anions in their respective sublattice.","The amorphization of the pyrochlore phase upon release is interpreted as the inability of accommodating the point defects at ambient conditions, which are formed in the pyrochlore lattice under compression."],"url":"http://arxiv.org/abs/2406.04128v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-06 14:45:49","title":"Fully spin-polarized two dimensional polar semi-metallic phase in Eu substituted GdI$_2$ monolayer","abstract":"The coexistence of seemingly mutually exclusive properties such as ferromagnetism, ferroelectricity and metallicity in atomically thin materials is the requirement of the hour in electronics as the Moore's law faces an impending end. Only a few 2D multiferroic materials have been predicted/realized so far. The polar metals with simultaneous presence of polarity and conductivity are also equally rare. Here, we predict, based on first-principles calculations that an Eu-substituted rare-earth halide GdI$_2$ monolayer showcases ferromagnetism, ferroelasticity while being polar and a fully spin-polarized semi-metal at the same time. The ferroelasticity and polarity are shown to be coupled making it possible to switch the polar direction using external mechanical stress. Further, it is observed that, an application of biaxial tensile strain of $5\\%$ causes the spin easy-axis to shift from out-of-plane to in-plane direction. Thus spin easy axis gets coupled with the direction of polarization in the strained monolayer making the switching of magnetization also possible using external strain. Simultaneous coexistence and coupling of the ferroic orders in a metallic 2D material makes the Eu substituted GdI$_2$ monolayer an incredibly rare material for nano-electronics and spintronics applications.","sentences":["The coexistence of seemingly mutually exclusive properties such as ferromagnetism, ferroelectricity and metallicity in atomically thin materials is the requirement of the hour in electronics as the Moore's law faces an impending end.","Only a few 2D multiferroic materials have been predicted/realized so far.","The polar metals with simultaneous presence of polarity and conductivity are also equally rare.","Here, we predict, based on first-principles calculations that an Eu-substituted rare-earth halide GdI$_2$ monolayer showcases ferromagnetism, ferroelasticity while being polar and a fully spin-polarized semi-metal at the same time.","The ferroelasticity and polarity are shown to be coupled making it possible to switch the polar direction using external mechanical stress.","Further, it is observed that, an application of biaxial tensile strain of $5\\%$ causes the spin easy-axis to shift from out-of-plane to in-plane direction.","Thus spin easy axis gets coupled with the direction of polarization in the strained monolayer making the switching of magnetization also possible using external strain.","Simultaneous coexistence and coupling of the ferroic orders in a metallic 2D material makes the Eu substituted GdI$_2$ monolayer an incredibly rare material for nano-electronics and spintronics applications."],"url":"http://arxiv.org/abs/2406.04122v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-06 14:36:36","title":"All crepant resolutions of hyperpolygon spaces via their Cox rings","abstract":"We construct and enumerate all crepant resolutions of hyperpolygon spaces, a family of conical symplectic singularities arising as Nakajima quiver varieties associated to a star-shaped quiver. We provide an explicit presentation of the Cox ring of any such crepant resolution. Using techniques developed by Arzhantsev-Derenthal-Hausen-Laface we construct all crepant resolutions of the hyperpolygon spaces, including those which are not projective over the singularity. We find that the number of crepant resolutions equals the Ho\\c{s}ten-Morris numbers. In proving these results, we obtain a description of all complete geometric quotients associated to the classical GIT problem constructing moduli spaces of ordered points on the projective line. These moduli spaces appear as the Lagrangian subvarieties of crepant resolutions of hyperpolygon spaces fixed under the conical action.","sentences":["We construct and enumerate all crepant resolutions of hyperpolygon spaces, a family of conical symplectic singularities arising as Nakajima quiver varieties associated to a star-shaped quiver.","We provide an explicit presentation of the Cox ring of any such crepant resolution.","Using techniques developed by Arzhantsev-Derenthal-Hausen-Laface we construct all crepant resolutions of the hyperpolygon spaces, including those which are not projective over the singularity.","We find that the number of crepant resolutions equals the Ho\\c{s}ten-Morris numbers.","In proving these results, we obtain a description of all complete geometric quotients associated to the classical GIT problem constructing moduli spaces of ordered points on the projective line.","These moduli spaces appear as the Lagrangian subvarieties of crepant resolutions of hyperpolygon spaces fixed under the conical action."],"url":"http://arxiv.org/abs/2406.04117v1","category":"math.AG"}
{"created":"2024-06-06 14:21:15","title":"From Tissue Plane to Organ World: A Benchmark Dataset for Multimodal Biomedical Image Registration using Deep Co-Attention Networks","abstract":"Correlating neuropathology with neuroimaging findings provides a multiscale view of pathologic changes in the human organ spanning the meso- to micro-scales, and is an emerging methodology expected to shed light on numerous disease states. To gain the most information from this multimodal, multiscale approach, it is desirable to identify precisely where a histologic tissue section was taken from within the organ in order to correlate with the tissue features in exactly the same organ region. Histology-to-organ registration poses an extra challenge, as any given histologic section can capture only a small portion of a human organ. Making use of the capabilities of state-of-the-art deep learning models, we unlock the potential to address and solve such intricate challenges. Therefore, we create the ATOM benchmark dataset, sourced from diverse institutions, with the primary objective of transforming this challenge into a machine learning problem and delivering outstanding outcomes that enlighten the biomedical community. The performance of our RegisMCAN model demonstrates the potential of deep learning to accurately predict where a subregion extracted from an organ image was obtained from within the overall 3D volume. The code and dataset can be found at: https://github.com/haizailache999/Image-Registration/tree/main","sentences":["Correlating neuropathology with neuroimaging findings provides a multiscale view of pathologic changes in the human organ spanning the meso- to micro-scales, and is an emerging methodology expected to shed light on numerous disease states.","To gain the most information from this multimodal, multiscale approach, it is desirable to identify precisely where a histologic tissue section was taken from within the organ in order to correlate with the tissue features in exactly the same organ region.","Histology-to-organ registration poses an extra challenge, as any given histologic section can capture only a small portion of a human organ.","Making use of the capabilities of state-of-the-art deep learning models, we unlock the potential to address and solve such intricate challenges.","Therefore, we create the ATOM benchmark dataset, sourced from diverse institutions, with the primary objective of transforming this challenge into a machine learning problem and delivering outstanding outcomes that enlighten the biomedical community.","The performance of our RegisMCAN model demonstrates the potential of deep learning to accurately predict where a subregion extracted from an organ image was obtained from within the overall 3D volume.","The code and dataset can be found at: https://github.com/haizailache999/Image-Registration/tree/main"],"url":"http://arxiv.org/abs/2406.04105v1","category":"cs.LG"}
{"created":"2024-06-06 14:13:38","title":"A Large-Scale Neutral Comparison Study of Survival Models on Low-Dimensional Data","abstract":"This work presents the first large-scale neutral benchmark experiment focused on single-event, right-censored, low-dimensional survival data. Benchmark experiments are essential in methodological research to scientifically compare new and existing model classes through proper empirical evaluation. Existing benchmarks in the survival literature are often narrow in scope, focusing, for example, on high-dimensional data. Additionally, they may lack appropriate tuning or evaluation procedures, or are qualitative reviews, rather than quantitative comparisons. This comprehensive study aims to fill the gap by neutrally evaluating a broad range of methods and providing generalizable conclusions. We benchmark 18 models, ranging from classical statistical approaches to many common machine learning methods, on 32 publicly available datasets. The benchmark tunes for both a discrimination measure and a proper scoring rule to assess performance in different settings. Evaluating on 8 survival metrics, we assess discrimination, calibration, and overall predictive performance of the tested models. Using discrimination measures, we find that no method significantly outperforms the Cox model. However, (tuned) Accelerated Failure Time models were able to achieve significantly better results with respect to overall predictive performance as measured by the right-censored log-likelihood. Machine learning methods that performed comparably well include Oblique Random Survival Forests under discrimination, and Cox-based likelihood-boosting under overall predictive performance. We conclude that for predictive purposes in the standard survival analysis setting of low-dimensional, right-censored data, the Cox Proportional Hazards model remains a simple and robust method, sufficient for practitioners.","sentences":["This work presents the first large-scale neutral benchmark experiment focused on single-event, right-censored, low-dimensional survival data.","Benchmark experiments are essential in methodological research to scientifically compare new and existing model classes through proper empirical evaluation.","Existing benchmarks in the survival literature are often narrow in scope, focusing, for example, on high-dimensional data.","Additionally, they may lack appropriate tuning or evaluation procedures, or are qualitative reviews, rather than quantitative comparisons.","This comprehensive study aims to fill the gap by neutrally evaluating a broad range of methods and providing generalizable conclusions.","We benchmark 18 models, ranging from classical statistical approaches to many common machine learning methods, on 32 publicly available datasets.","The benchmark tunes for both a discrimination measure and a proper scoring rule to assess performance in different settings.","Evaluating on 8 survival metrics, we assess discrimination, calibration, and overall predictive performance of the tested models.","Using discrimination measures, we find that no method significantly outperforms the Cox model.","However, (tuned) Accelerated Failure Time models were able to achieve significantly better results with respect to overall predictive performance as measured by the right-censored log-likelihood.","Machine learning methods that performed comparably well include Oblique Random Survival Forests under discrimination, and Cox-based likelihood-boosting under overall predictive performance.","We conclude that for predictive purposes in the standard survival analysis setting of low-dimensional, right-censored data, the Cox Proportional Hazards model remains a simple and robust method, sufficient for practitioners."],"url":"http://arxiv.org/abs/2406.04098v1","category":"stat.ML"}
{"created":"2024-06-06 14:13:36","title":"Resonant phonons: Localization in a structurally ordered crystal","abstract":"Phonon localization is a phenomenon that influences numerous material properties in condensed matter physics. Anderson localization brings rise to localized atomic-scale phonon interferences in disordered lattices with an influence limited to high-frequency phonons having wavelengths comparable to the size of a randomly perturbed unit cell. Here we theoretically reveal a new form of phonon localization induced by augmenting a crystalline material with intrinsic phonon nanoresonators with feature sizes that can be smaller or larger than the phonon wavelengths but must be relatively small compared to the phonon mean free paths. This mechanism is deterministic and takes place within numerous discrete narrow-frequency bands spread throughout the full spectrum with central frequencies controlled by design. For demonstration, we run molecular dynamics simulations of all-silicon nanopillared membranes at room temperature, and apply to the underlying thermalized environment narrowband wave packets as an excitation at precisely the frequencies where resonant hybridizations are evident in the anharmonic phonon band structure. Upon comparison to other frequency ranges where the nanostructure does not exhibit local resonances, significant intrinsic spatial phonon localization along the direction of transport is explicitly observed. Furthermore, the energy exchange with external sources is minimized at the resonant frequencies. We conclude by making a direct comparison with Anderson localization highlighting the superiority of the resonant phonons across both sides of the interference frequency limit.","sentences":["Phonon localization is a phenomenon that influences numerous material properties in condensed matter physics.","Anderson localization brings rise to localized atomic-scale phonon interferences in disordered lattices with an influence limited to high-frequency phonons having wavelengths comparable to the size of a randomly perturbed unit cell.","Here we theoretically reveal a new form of phonon localization induced by augmenting a crystalline material with intrinsic phonon nanoresonators with feature sizes that can be smaller or larger than the phonon wavelengths but must be relatively small compared to the phonon mean free paths.","This mechanism is deterministic and takes place within numerous discrete narrow-frequency bands spread throughout the full spectrum with central frequencies controlled by design.","For demonstration, we run molecular dynamics simulations of all-silicon nanopillared membranes at room temperature, and apply to the underlying thermalized environment narrowband wave packets as an excitation at precisely the frequencies where resonant hybridizations are evident in the anharmonic phonon band structure.","Upon comparison to other frequency ranges where the nanostructure does not exhibit local resonances, significant intrinsic spatial phonon localization along the direction of transport is explicitly observed.","Furthermore, the energy exchange with external sources is minimized at the resonant frequencies.","We conclude by making a direct comparison with Anderson localization highlighting the superiority of the resonant phonons across both sides of the interference frequency limit."],"url":"http://arxiv.org/abs/2406.04097v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-06 13:58:41","title":"Deterministic Uncertainty Propagation for Improved Model-Based Offline Reinforcement Learning","abstract":"Current approaches to model-based offline Reinforcement Learning (RL) often incorporate uncertainty-based reward penalization to address the distributional shift problem. While these approaches have achieved some success, we argue that this penalization introduces excessive conservatism, potentially resulting in suboptimal policies through underestimation. We identify as an important cause of over-penalization the lack of a reliable uncertainty estimator capable of propagating uncertainties in the Bellman operator. The common approach to calculating the penalty term relies on sampling-based uncertainty estimation, resulting in high variance. To address this challenge, we propose a novel method termed Moment Matching Offline Model-Based Policy Optimization (MOMBO). MOMBO learns a Q-function using moment matching, which allows us to deterministically propagate uncertainties through the Q-function. We evaluate MOMBO's performance across various environments and demonstrate empirically that MOMBO is a more stable and sample-efficient approach.","sentences":["Current approaches to model-based offline Reinforcement Learning (RL) often incorporate uncertainty-based reward penalization to address the distributional shift problem.","While these approaches have achieved some success, we argue that this penalization introduces excessive conservatism, potentially resulting in suboptimal policies through underestimation.","We identify as an important cause of over-penalization the lack of a reliable uncertainty estimator capable of propagating uncertainties in the Bellman operator.","The common approach to calculating the penalty term relies on sampling-based uncertainty estimation, resulting in high variance.","To address this challenge, we propose a novel method termed Moment Matching Offline Model-Based Policy Optimization (MOMBO).","MOMBO learns a Q-function using moment matching, which allows us to deterministically propagate uncertainties through the Q-function.","We evaluate MOMBO's performance across various environments and demonstrate empirically that MOMBO is a more stable and sample-efficient approach."],"url":"http://arxiv.org/abs/2406.04088v1","category":"cs.LG"}
{"created":"2024-06-06 13:30:42","title":"Online Learning in Betting Markets: Profit versus Prediction","abstract":"We examine two types of binary betting markets, whose primary goal is for profit (such as sports gambling) or to gain information (such as prediction markets). We articulate the interplay between belief and price-setting to analyse both types of markets, and show that the goals of maximising bookmaker profit and eliciting information are fundamentally incompatible. A key insight is that profit hinges on the deviation between (the distribution of) bettor and true beliefs, and that heavier tails in bettor belief distribution imply higher profit. Our algorithmic contribution is to introduce online learning methods for price-setting. Traditionally bookmakers update their prices rather infrequently, we present two algorithms that guide price updates upon seeing each bet, assuming very little of bettor belief distributions. The online pricing algorithm achieves stochastic regret of $\\mathcal{O}(\\sqrt{T})$ against the worst local maximum, or $ \\mathcal{O}(\\sqrt{T \\log T}) $ with high probability against the global maximum under fair odds. More broadly, the inherent trade-off between profit and information-seeking in binary betting may inspire new understandings of large-scale multi-agent behaviour.","sentences":["We examine two types of binary betting markets, whose primary goal is for profit (such as sports gambling) or to gain information (such as prediction markets).","We articulate the interplay between belief and price-setting to analyse both types of markets, and show that the goals of maximising bookmaker profit and eliciting information are fundamentally incompatible.","A key insight is that profit hinges on the deviation between (the distribution of) bettor and true beliefs, and that heavier tails in bettor belief distribution imply higher profit.","Our algorithmic contribution is to introduce online learning methods for price-setting.","Traditionally bookmakers update their prices rather infrequently, we present two algorithms that guide price updates upon seeing each bet, assuming very little of bettor belief distributions.","The online pricing algorithm achieves stochastic regret of $\\mathcal{O}(\\sqrt{T})$ against the worst local maximum, or $ \\mathcal{O}(\\sqrt{T \\log T}) $ with high probability against the global maximum under fair odds.","More broadly, the inherent trade-off between profit and information-seeking in binary betting may inspire new understandings of large-scale multi-agent behaviour."],"url":"http://arxiv.org/abs/2406.04062v1","category":"cs.GT"}
{"created":"2024-06-06 13:25:14","title":"Bisimulation Metrics are Optimal Transport Distances, and Can be Computed Efficiently","abstract":"We propose a new framework for formulating optimal transport distances between Markov chains. Previously known formulations studied couplings between the entire joint distribution induced by the chains, and derived solutions via a reduction to dynamic programming (DP) in an appropriately defined Markov decision process. This formulation has, however, not led to particularly efficient algorithms so far, since computing the associated DP operators requires fully solving a static optimal transport problem, and these operators need to be applied numerous times during the overall optimization process. In this work, we develop an alternative perspective by considering couplings between a flattened version of the joint distributions that we call discounted occupancy couplings, and show that calculating optimal transport distances in the full space of joint distributions can be equivalently formulated as solving a linear program (LP) in this reduced space. This LP formulation allows us to port several algorithmic ideas from other areas of optimal transport theory. In particular, our formulation makes it possible to introduce an appropriate notion of entropy regularization into the optimization problem, which in turn enables us to directly calculate optimal transport distances via a Sinkhorn-like method we call Sinkhorn Value Iteration (SVI). We show both theoretically and empirically that this method converges quickly to an optimal coupling, essentially at the same computational cost of running vanilla Sinkhorn in each pair of states. Along the way, we point out that our optimal transport distance exactly matches the common notion of bisimulation metrics between Markov chains, and thus our results also apply to computing such metrics, and in fact our algorithm turns out to be significantly more efficient than the best known methods developed so far for this purpose.","sentences":["We propose a new framework for formulating optimal transport distances between Markov chains.","Previously known formulations studied couplings between the entire joint distribution induced by the chains, and derived solutions via a reduction to dynamic programming (DP) in an appropriately defined Markov decision process.","This formulation has, however, not led to particularly efficient algorithms so far, since computing the associated DP operators requires fully solving a static optimal transport problem, and these operators need to be applied numerous times during the overall optimization process.","In this work, we develop an alternative perspective by considering couplings between a flattened version of the joint distributions that we call discounted occupancy couplings, and show that calculating optimal transport distances in the full space of joint distributions can be equivalently formulated as solving a linear program (LP) in this reduced space.","This LP formulation allows us to port several algorithmic ideas from other areas of optimal transport theory.","In particular, our formulation makes it possible to introduce an appropriate notion of entropy regularization into the optimization problem, which in turn enables us to directly calculate optimal transport distances via a Sinkhorn-like method we call Sinkhorn Value Iteration (SVI).","We show both theoretically and empirically that this method converges quickly to an optimal coupling, essentially at the same computational cost of running vanilla Sinkhorn in each pair of states.","Along the way, we point out that our optimal transport distance exactly matches the common notion of bisimulation metrics between Markov chains, and thus our results also apply to computing such metrics, and in fact our algorithm turns out to be significantly more efficient than the best known methods developed so far for this purpose."],"url":"http://arxiv.org/abs/2406.04056v1","category":"cs.LG"}
{"created":"2024-06-06 13:13:29","title":"Energy-based Epistemic Uncertainty for Graph Neural Networks","abstract":"In domains with interdependent data, such as graphs, quantifying the epistemic uncertainty of a Graph Neural Network (GNN) is challenging as uncertainty can arise at different structural scales. Existing techniques neglect this issue or only distinguish between structure-aware and structure-agnostic uncertainty without combining them into a single measure. We propose GEBM, an energy-based model (EBM) that provides high-quality uncertainty estimates by aggregating energy at different structural levels that naturally arise from graph diffusion. In contrast to logit-based EBMs, we provably induce an integrable density in the data space by regularizing the energy function. We introduce an evidential interpretation of our EBM that significantly improves the predictive robustness of the GNN. Our framework is a simple and effective post hoc method applicable to any pre-trained GNN that is sensitive to various distribution shifts. It consistently achieves the best separation of in-distribution and out-of-distribution data on 6 out of 7 anomaly types while having the best average rank over shifts on \\emph{all} datasets.","sentences":["In domains with interdependent data, such as graphs, quantifying the epistemic uncertainty of a Graph Neural Network (GNN) is challenging as uncertainty can arise at different structural scales.","Existing techniques neglect this issue or only distinguish between structure-aware and structure-agnostic uncertainty without combining them into a single measure.","We propose GEBM, an energy-based model (EBM) that provides high-quality uncertainty estimates by aggregating energy at different structural levels that naturally arise from graph diffusion.","In contrast to logit-based EBMs, we provably induce an integrable density in the data space by regularizing the energy function.","We introduce an evidential interpretation of our EBM that significantly improves the predictive robustness of the GNN.","Our framework is a simple and effective post hoc method applicable to any pre-trained GNN that is sensitive to various distribution shifts.","It consistently achieves the best separation of in-distribution and out-of-distribution data on 6 out of 7 anomaly types while having the best average rank over shifts on \\emph{all} datasets."],"url":"http://arxiv.org/abs/2406.04043v1","category":"cs.LG"}
{"created":"2024-06-06 12:45:45","title":"The SKA Galactic Centre Survey -- A White Paper","abstract":"With its extreme density of stars and stellar remnants, dense young massive clusters, high specific star formation rate, intense radiation field, high magnetic field strength, and properties of the interstellar medium that resemble those in high redshift galaxies and starbursts, the Galactic Centre is the most extreme environment that we can observe in detail. It is also the only nucleus of a galaxy that we can observe with a resolution of just a few milli parsecs. This makes it a crucial target to understand the physics of galactic nuclei and star formation, as well as the connection between them. It enables studies of a large number of otherwise rare objects, such as extremely massive stars and stellar remnants, at a well-defined distance, thus facilitating the interpretation of their properties. The Galactic Centre has been and is being studied intensively with the most advanced facilities. In this White Paper, we advocate for a large-area, multi-wavelength survey with the Square Kilometre Array of an area of about 1.25x0.3 deg**2 (180x40 pc**2), centered on the massive black hole Sagittarius A* and for repeated deep observations of the nuclear star cluster over a decade, which will allow the community to address multiple science problems with a single data set.","sentences":["With its extreme density of stars and stellar remnants, dense young massive clusters, high specific star formation rate, intense radiation field, high magnetic field strength, and properties of the interstellar medium that resemble those in high redshift galaxies and starbursts, the Galactic Centre is the most extreme environment that we can observe in detail.","It is also the only nucleus of a galaxy that we can observe with a resolution of just a few milli parsecs.","This makes it a crucial target to understand the physics of galactic nuclei and star formation, as well as the connection between them.","It enables studies of a large number of otherwise rare objects, such as extremely massive stars and stellar remnants, at a well-defined distance, thus facilitating the interpretation of their properties.","The Galactic Centre has been and is being studied intensively with the most advanced facilities.","In this White Paper, we advocate for a large-area, multi-wavelength survey with the Square Kilometre Array of an area of about 1.25x0.3 deg**2 (180x40 pc**2), centered on the massive black hole Sagittarius A* and for repeated deep observations of the nuclear star cluster over a decade, which will allow the community to address multiple science problems with a single data set."],"url":"http://arxiv.org/abs/2406.04022v1","category":"astro-ph.GA"}
{"created":"2024-06-06 12:42:16","title":"Geometric Martingale Benamou-Brenier transport and geometric Bass martingales","abstract":"We introduce and study geometric Bass martingales. Bass martingales were introduced in \\cite{Ba83} and studied recently in a series of works, including \\cite{BaBeHuKa20,BaBeScTs23}, where they appear as solutions to the martingale version of the Benamou-Brenier optimal transport formulation. These arithmetic, as well as our novel geometric, Bass martingales are continuous martingale on $[0,1]$ with prescribed initial and terminal distributions. An arithmetic Bass martingale is the one closest to Brownian motion: its quadratic variation is as close as possible to being linear in the averaged $L^2$ sense. Its geometric counterpart we develop here, is the one closest to a geometric Brownian motion: the quadratic variation of its logarithm is as close as possible to being linear. By analogy between Bachelier and Black-Scholes models in mathematical finance, the newly obtained geometric Bass martingales} have the potential to be of more practical importance in a number of applications.   Our main contribution is to exhibit an explicit bijection between geometric Bass martingales and their arithmetic counterparts. This allows us, in particular, to translate fine properties of the latter into the new geometric setting. We obtain an explicit representation for a geometric Bass martingale for given initial and terminal marginals, we characterise it as a solution to an SDE, and we show that geometric Brownian motion is the only process which is both an arithmetic and a geometric Bass martingale. Finally, we deduce a dual formulation for our geometric martingale Benamou-Brenier problem. Our main proof is probabilistic in nature and uses a suitable change of measure, but we also provide PDE arguments relying on the dual formulation of the problem, which offer a rigorous proof under suitable regularity assumptions.","sentences":["We introduce and study geometric Bass martingales.","Bass martingales were introduced in \\cite{Ba83} and studied recently in a series of works, including \\cite{BaBeHuKa20,BaBeScTs23}, where they appear as solutions to the martingale version of the Benamou-Brenier optimal transport formulation.","These arithmetic, as well as our novel geometric, Bass martingales are continuous martingale on $[0,1]$ with prescribed initial and terminal distributions.","An arithmetic Bass martingale is the one closest to Brownian motion: its quadratic variation is as close as possible to being linear in the averaged $L^2$ sense.","Its geometric counterpart we develop here, is the one closest to a geometric Brownian motion: the quadratic variation of its logarithm is as close as possible to being linear.","By analogy between Bachelier and Black-Scholes models in mathematical finance, the newly obtained geometric Bass martingales} have the potential to be of more practical importance in a number of applications.   ","Our main contribution is to exhibit an explicit bijection between geometric Bass martingales and their arithmetic counterparts.","This allows us, in particular, to translate fine properties of the latter into the new geometric setting.","We obtain an explicit representation for a geometric Bass martingale for given initial and terminal marginals, we characterise it as a solution to an SDE, and we show that geometric Brownian motion is the only process which is both an arithmetic and a geometric Bass martingale.","Finally, we deduce a dual formulation for our geometric martingale Benamou-Brenier problem.","Our main proof is probabilistic in nature and uses a suitable change of measure, but we also provide PDE arguments relying on the dual formulation of the problem, which offer a rigorous proof under suitable regularity assumptions."],"url":"http://arxiv.org/abs/2406.04016v1","category":"math.PR"}
{"created":"2024-06-06 12:05:44","title":"Numerical Simulation of Radiatively driven Transonic Relativistic Jets","abstract":"We perform the numerical simulations of axisymmetric, relativistic, optically thin jets under the influence of the radiation field of an accretion disk. We show that starting from a very low injection velocity at the base, jets can be accelerated to relativistic terminal speeds when traveling through the radiation field. The jet gains momentum through the interaction with the radiation field. We use a relativistic equation of state for multi-species plasma, which self-consistently calculates the adiabatic index for the jet material. All the jet solutions obtained are transonic in nature. In addition to the acceleration of the jet to relativistic speeds, our results show that the radiation field also acts as a collimating agent. The jets remain well collimated under the effect of radiation pressure. We also show that if the jet starts with a rotational velocity, the radiation field will reduce the angular momentum of the jet beam.","sentences":["We perform the numerical simulations of axisymmetric, relativistic, optically thin jets under the influence of the radiation field of an accretion disk.","We show that starting from a very low injection velocity at the base, jets can be accelerated to relativistic terminal speeds when traveling through the radiation field.","The jet gains momentum through the interaction with the radiation field.","We use a relativistic equation of state for multi-species plasma, which self-consistently calculates the adiabatic index for the jet material.","All the jet solutions obtained are transonic in nature.","In addition to the acceleration of the jet to relativistic speeds, our results show that the radiation field also acts as a collimating agent.","The jets remain well collimated under the effect of radiation pressure.","We also show that if the jet starts with a rotational velocity, the radiation field will reduce the angular momentum of the jet beam."],"url":"http://arxiv.org/abs/2406.03989v1","category":"astro-ph.HE"}
{"created":"2024-06-06 12:04:52","title":"On the Scalar Curvature Compactness Conjecture in the Conformal Case","abstract":"Is a sequence of Riemannian manifolds with positive scalar curvature, satisfying some conditions to keep the sequence reasonable, compact? What topology should one use for the convergence and what is the regularity of the limit space? In this paper we explore these questions by studying the case of a sequence of Riemannian manifolds which are conformal to the $n$-dimensional round sphere. We are able to show that the sequence of conformal factors are compact in several analytic senses and are able to establish $C^0$ convergence away from a singular set of small volume in a similar fashion as C. Dong. Under a bound on the total scalar curvature we are able to show that the limit conformal factor has weak positive scalar curvature in the sense of weakly solving the conformal positive scalar curvature equation.","sentences":["Is a sequence of Riemannian manifolds with positive scalar curvature, satisfying some conditions to keep the sequence reasonable, compact?","What topology should one use for the convergence and what is the regularity of the limit space?","In this paper we explore these questions by studying the case of a sequence of Riemannian manifolds which are conformal to the $n$-dimensional round sphere.","We are able to show that the sequence of conformal factors are compact in several analytic senses and are able to establish $C^0$ convergence away from a singular set of small volume in a similar fashion as C. Dong.","Under a bound on the total scalar curvature we are able to show that the limit conformal factor has weak positive scalar curvature in the sense of weakly solving the conformal positive scalar curvature equation."],"url":"http://arxiv.org/abs/2406.03988v1","category":"math.DG"}
{"created":"2024-06-06 11:57:25","title":"LNQ Challenge 2023: Learning Mediastinal Lymph Node Segmentation with a Probabilistic Lymph Node Atlas","abstract":"The evaluation of lymph node metastases plays a crucial role in achieving precise cancer staging, influencing subsequent decisions regarding treatment options. Lymph node detection poses challenges due to the presence of unclear boundaries and the diverse range of sizes and morphological characteristics, making it a resource-intensive process. As part of the LNQ 2023 MICCAI challenge, we propose the use of anatomical priors as a tool to address the challenges that persist in mediastinal lymph node segmentation in combination with the partial annotation of the challenge training data. The model ensemble using all suggested modifications yields a Dice score of 0.6033 and segments 57% of the ground truth lymph nodes, compared to 27% when training on CT only. Segmentation accuracy is improved significantly by incorporating a probabilistic lymph node atlas in loss weighting and post-processing. The largest performance gains are achieved by oversampling fully annotated data to account for the partial annotation of the challenge training data, as well as adding additional data augmentation to address the high heterogeneity of the CT images and lymph node appearance. Our code is available at https://github.com/MICAI-IMI-UzL/LNQ2023.","sentences":["The evaluation of lymph node metastases plays a crucial role in achieving precise cancer staging, influencing subsequent decisions regarding treatment options.","Lymph node detection poses challenges due to the presence of unclear boundaries and the diverse range of sizes and morphological characteristics, making it a resource-intensive process.","As part of the LNQ 2023 MICCAI challenge, we propose the use of anatomical priors as a tool to address the challenges that persist in mediastinal lymph node segmentation in combination with the partial annotation of the challenge training data.","The model ensemble using all suggested modifications yields a Dice score of 0.6033 and segments 57% of the ground truth lymph nodes, compared to 27% when training on CT only.","Segmentation accuracy is improved significantly by incorporating a probabilistic lymph node atlas in loss weighting and post-processing.","The largest performance gains are achieved by oversampling fully annotated data to account for the partial annotation of the challenge training data, as well as adding additional data augmentation to address the high heterogeneity of the CT images and lymph node appearance.","Our code is available at https://github.com/MICAI-IMI-UzL/LNQ2023."],"url":"http://arxiv.org/abs/2406.03984v1","category":"cs.CV"}
{"created":"2024-06-06 11:35:13","title":"Higher-Matter and Landau-Ginzburg Theory of Higher-Group Symmetries","abstract":"Higher-matter is defined by higher-representation of a symmetry algebra, such as the $p$-form symmetries, higher-group symmetries or higher-categorical symmetries. In this paper, we focus on the cases of higher-group symmetries, which are formulated in terms of the strictification of weak higher-groups. We systematically investigate higher-matter charged under 2-group symmetries, defined by automorphism 2-representations. Furthermore, we construct a Lagrangian formulation of such higher-matter fields coupled to 2-group gauge fields in the path space of the spacetime manifold. We interpret such model as the Landau-Ginzburg theory for 2-group symmetries, and discuss the spontaneous symmetry breaking (SSB) of 2-group symmetries under this framework. Examples of discrete and continuous 2-groups are discussed. Interestingly, we find that a non-split 2-group symmetry can admit an SSB to a split 2-group symmetry, where the Postnikov class is trivialized. We also briefly discuss the strictification of weak 3-groups, weak 3-group gauge fields and 3-representations in special cases.","sentences":["Higher-matter is defined by higher-representation of a symmetry algebra, such as the $p$-form symmetries, higher-group symmetries or higher-categorical symmetries.","In this paper, we focus on the cases of higher-group symmetries, which are formulated in terms of the strictification of weak higher-groups.","We systematically investigate higher-matter charged under 2-group symmetries, defined by automorphism 2-representations.","Furthermore, we construct a Lagrangian formulation of such higher-matter fields coupled to 2-group gauge fields in the path space of the spacetime manifold.","We interpret such model as the Landau-Ginzburg theory for 2-group symmetries, and discuss the spontaneous symmetry breaking (SSB) of 2-group symmetries under this framework.","Examples of discrete and continuous 2-groups are discussed.","Interestingly, we find that a non-split 2-group symmetry can admit an SSB to a split 2-group symmetry, where the Postnikov class is trivialized.","We also briefly discuss the strictification of weak 3-groups, weak 3-group gauge fields and 3-representations in special cases."],"url":"http://arxiv.org/abs/2406.03974v1","category":"hep-th"}
{"created":"2024-06-06 10:57:35","title":"Estimation of Out-of-Sample Sharpe Ratio for High Dimensional Portfolio Optimization","abstract":"Portfolio optimization aims at constructing a realistic portfolio with significant out-of-sample performance, which is typically measured by the out-of-sample Sharpe ratio. However, due to in-sample optimism, it is inappropriate to use the in-sample estimated covariance to evaluate the out-of-sample Sharpe, especially in the high dimensional settings. In this paper, we propose a novel method to estimate the out-of-sample Sharpe ratio using only in-sample data, based on random matrix theory. Furthermore, portfolio managers can use the estimated out-of-sample Sharpe as a criterion to decide the best tuning for constructing their portfolios. Specifically, we consider the classical framework of Markowits mean-variance portfolio optimization with known mean vector and the high dimensional regime of $p/n \\to c \\in (0,\\infty)$, where $p$ is the portfolio dimension and $n$ is the number of samples or time points. We propose to correct the sample covariance by a regularization matrix and provide a consistent estimator of its Sharpe ratio. The new estimator works well under either of three conditions: (1) bounded covariance spectrum, (2) arbitrary number of diverging spikes when $c < 1$, and (3) fixed number of diverging spikes when $c \\ge 1$. We can also extend the results to construct global minimum variance portfolio and correct out-of-sample efficient frontier. We demonstrate the effectiveness of our approach through comprehensive simulations and real data experiments. Our results highlight the potential of this methodology as a powerful tool for portfolio optimization in high dimensional settings.","sentences":["Portfolio optimization aims at constructing a realistic portfolio with significant out-of-sample performance, which is typically measured by the out-of-sample Sharpe ratio.","However, due to in-sample optimism, it is inappropriate to use the in-sample estimated covariance to evaluate the out-of-sample Sharpe, especially in the high dimensional settings.","In this paper, we propose a novel method to estimate the out-of-sample Sharpe ratio using only in-sample data, based on random matrix theory.","Furthermore, portfolio managers can use the estimated out-of-sample Sharpe as a criterion to decide the best tuning for constructing their portfolios.","Specifically, we consider the classical framework of Markowits mean-variance portfolio optimization with known mean vector and the high dimensional regime of $p/n \\to c \\in (0,\\infty)$, where $p$ is the portfolio dimension and $n$ is the number of samples or time points.","We propose to correct the sample covariance by a regularization matrix and provide a consistent estimator of its Sharpe ratio.","The new estimator works well under either of three conditions: (1) bounded covariance spectrum, (2) arbitrary number of diverging spikes when $c < 1$, and (3) fixed number of diverging spikes when $c","\\ge 1$.","We can also extend the results to construct global minimum variance portfolio and correct out-of-sample efficient frontier.","We demonstrate the effectiveness of our approach through comprehensive simulations and real data experiments.","Our results highlight the potential of this methodology as a powerful tool for portfolio optimization in high dimensional settings."],"url":"http://arxiv.org/abs/2406.03954v1","category":"math.ST"}
{"created":"2024-06-06 10:54:11","title":"Coexistence of Topological Dirac and Dirac Nodal line semimetal in SrCaP belonging to Nodal line semimetal family SrCaX(X= Bi, Sb, As, P)","abstract":"Nodal line semimetals represent precursor states for various topological phases, exhibiting intrinsic topological characteristics and intriguing properties. These materials host rare and distinctive topological features, which can give rise to exotic phenomena, thereby garnering significant attention in both fundamental research and technological applications. In this study, we conduct ab-initio calculations to explore the properties of SrCaX (X = Bi, Sb, As, P), identifying these as multiple Dirac nodal line semimetals protected by Z2 quantized Berry phases and manifesting multiple drum-head-like surface states. The nodal lines in these compounds are situated at the M point when kz = 0 and at the A point when kz = {\\pi}. Notably, SrCaX family exhibits a unique characteristic wherein they host both type II Dirac point and topological nodal line semimetal within a single crystal structure, hence providing an excellent platform for studying the interplay between different topological properties. Additionally, in SrCaP topological Dirac semimetal, Type II Dirac point and topological nodal line semimetal features coexist in a single crystal. These special features in this series of materials make them ideal candidates for further investigation by experimental means.","sentences":["Nodal line semimetals represent precursor states for various topological phases, exhibiting intrinsic topological characteristics and intriguing properties.","These materials host rare and distinctive topological features, which can give rise to exotic phenomena, thereby garnering significant attention in both fundamental research and technological applications.","In this study, we conduct ab-initio calculations to explore the properties of SrCaX (X = Bi, Sb, As, P), identifying these as multiple Dirac nodal line semimetals protected by Z2 quantized Berry phases and manifesting multiple drum-head-like surface states.","The nodal lines in these compounds are situated at the M point when kz = 0 and at the A point when kz = {\\pi}.","Notably, SrCaX family exhibits a unique characteristic wherein they host both type II Dirac point and topological nodal line semimetal within a single crystal structure, hence providing an excellent platform for studying the interplay between different topological properties.","Additionally, in SrCaP topological Dirac semimetal, Type II Dirac point and topological nodal line semimetal features coexist in a single crystal.","These special features in this series of materials make them ideal candidates for further investigation by experimental means."],"url":"http://arxiv.org/abs/2406.03952v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-06 10:28:35","title":"Magnetic Skyrmions: from lumps to supercompactons","abstract":"The magnetic Skyrmion is described by one control parameter and one length scale. We study the two extreme limits of the control parameter -- infinitely large and vanishing - and find that the magnetic Skyrmion becomes a \"restricted\" magnetic Skyrmion and an O(3) sigma model lump, respectively. Depending on the potential under consideration, the restricted limit manifests differently. In the case of the Zeeman term, the restricted magnetic Skyrmion becomes a \"supercompacton\" that develops a discontinuity, whereas for the Zeeman term to the power 3/2 it becomes a normal compacton. Finally, we observe that the case of the Zeeman term squared, which can also be understood as a special combination of the Zeeman term and the easy-plane potential - realizable in the laboratory, the lump solution is an analytically exact solution for all values of the control parameter.","sentences":["The magnetic Skyrmion is described by one control parameter and one length scale.","We study the two extreme limits of the control parameter -- infinitely large and vanishing - and find that the magnetic Skyrmion becomes a \"restricted\" magnetic Skyrmion and an O(3) sigma model lump, respectively.","Depending on the potential under consideration, the restricted limit manifests differently.","In the case of the Zeeman term, the restricted magnetic Skyrmion becomes a \"supercompacton\" that develops a discontinuity, whereas for the Zeeman term to the power 3/2 it becomes a normal compacton.","Finally, we observe that the case of the Zeeman term squared, which can also be understood as a special combination of the Zeeman term and the easy-plane potential - realizable in the laboratory, the lump solution is an analytically exact solution for all values of the control parameter."],"url":"http://arxiv.org/abs/2406.03941v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-06 10:28:05","title":"Scaling solutions for current-carrying cosmic string networks","abstract":"Cosmic string networks are the best motivated relics of cosmological phase transitions, being unavoidable in many physically plausible extensions of the Standard Model. Most studies, including those providing constraints from and forecasts of their observational signals, rely on assumptions of featureless networks, neglecting the additional degrees of freedom on the string worldsheet, e.g. charges and currents, which are all but unavoidable in physically realistic models. An extension of the canonical velocity-dependent one-scale model, accounting for all such possible degrees of freedom, has been recently developed. Here we improve its physical interpretation by studying and classifying its possible asymptotic scaling solutions, and in particular how they are affected by the expansion of the Universe and the available energy loss or transfer mechanisms. We find three classes of solutions. For sufficiently fast expansion rates the charges and currents decay and one asymptotes to the Nambu-Goto case, while for slower expansion rates they can dominate the network dynamics. In between the two there is a third regime in which the network, including its charge and current, reaches full scaling. Under specific but plausible assumptions, this intermediate regime corresponds to the matter-dominated era. Our results agree with, and significantly extend, those of previous studies.","sentences":["Cosmic string networks are the best motivated relics of cosmological phase transitions, being unavoidable in many physically plausible extensions of the Standard Model.","Most studies, including those providing constraints from and forecasts of their observational signals, rely on assumptions of featureless networks, neglecting the additional degrees of freedom on the string worldsheet, e.g. charges and currents, which are all but unavoidable in physically realistic models.","An extension of the canonical velocity-dependent one-scale model, accounting for all such possible degrees of freedom, has been recently developed.","Here we improve its physical interpretation by studying and classifying its possible asymptotic scaling solutions, and in particular how they are affected by the expansion of the Universe and the available energy loss or transfer mechanisms.","We find three classes of solutions.","For sufficiently fast expansion rates the charges and currents decay and one asymptotes to the Nambu-Goto case, while for slower expansion rates they can dominate the network dynamics.","In between the two there is a third regime in which the network, including its charge and current, reaches full scaling.","Under specific but plausible assumptions, this intermediate regime corresponds to the matter-dominated era.","Our results agree with, and significantly extend, those of previous studies."],"url":"http://arxiv.org/abs/2406.03940v1","category":"hep-ph"}
{"created":"2024-06-06 10:17:51","title":"Breeding Programs Optimization with Reinforcement Learning","abstract":"Crop breeding is crucial in improving agricultural productivity while potentially decreasing land usage, greenhouse gas emissions, and water consumption. However, breeding programs are challenging due to long turnover times, high-dimensional decision spaces, long-term objectives, and the need to adapt to rapid climate change. This paper introduces the use of Reinforcement Learning (RL) to optimize simulated crop breeding programs. RL agents are trained to make optimal crop selection and cross-breeding decisions based on genetic information. To benchmark RL-based breeding algorithms, we introduce a suite of Gym environments. The study demonstrates the superiority of RL techniques over standard practices in terms of genetic gain when simulated in silico using real-world genomic maize data.","sentences":["Crop breeding is crucial in improving agricultural productivity while potentially decreasing land usage, greenhouse gas emissions, and water consumption.","However, breeding programs are challenging due to long turnover times, high-dimensional decision spaces, long-term objectives, and the need to adapt to rapid climate change.","This paper introduces the use of Reinforcement Learning (RL) to optimize simulated crop breeding programs.","RL agents are trained to make optimal crop selection and cross-breeding decisions based on genetic information.","To benchmark RL-based breeding algorithms, we introduce a suite of Gym environments.","The study demonstrates the superiority of RL techniques over standard practices in terms of genetic gain when simulated in silico using real-world genomic maize data."],"url":"http://arxiv.org/abs/2406.03932v1","category":"cs.LG"}
{"created":"2024-06-06 10:09:37","title":"Balancing rationality and social influence: Alpha-rational Nash equilibrium in games with herding","abstract":"The classical game theory models rational players and proposes Nash equilibrium (NE) as the solution. However, real-world scenarios rarely feature rational players; instead, players make inconsistent and irrational decisions. Often, irrational players exhibit herding behaviour by simply following the majority.   In this paper, we consider the mean-field game with $\\alpha$-fraction of rational players and the rest being herding-irrational players. For such a game, we introduce a novel concept of equilibrium named $\\alpha$-Rational NE (in short, $\\alpha$-RNE). The $\\alpha$-RNEs and their implications are extensively analyzed in the game with two actions. Due to herding-irrational players, new equilibria may arise, and some classical NEs may be deleted.   The rational players are not harmed but benefit from the presence of irrational players. Notably, we demonstrate through examples that rational players leverage upon the herding behaviour of irrational players and may attain higher utility (under $\\alpha$-RNE) than social optimal utility (in the classical setting).   Interestingly, the irrational players may also benefit by not being rational. We observe that irrational players do not lose compared to some classical NEs for participation and bandwidth sharing games. More importantly, in bandwidth sharing game, irrational players receive utility that approaches the social optimal utility. Such examples indicate that it may sometimes be `rational' to be irrational.","sentences":["The classical game theory models rational players and proposes Nash equilibrium (NE) as the solution.","However, real-world scenarios rarely feature rational players; instead, players make inconsistent and irrational decisions.","Often, irrational players exhibit herding behaviour by simply following the majority.   ","In this paper, we consider the mean-field game with $\\alpha$-fraction of rational players and the rest being herding-irrational players.","For such a game, we introduce a novel concept of equilibrium named $\\alpha$-Rational NE (in short, $\\alpha$-RNE).","The $\\alpha$-RNEs and their implications are extensively analyzed in the game with two actions.","Due to herding-irrational players, new equilibria may arise, and some classical NEs may be deleted.   ","The rational players are not harmed but benefit from the presence of irrational players.","Notably, we demonstrate through examples that rational players leverage upon the herding behaviour of irrational players and may attain higher utility (under $\\alpha$-RNE) than social optimal utility (in the classical setting).   ","Interestingly, the irrational players may also benefit by not being rational.","We observe that irrational players do not lose compared to some classical NEs for participation and bandwidth sharing games.","More importantly, in bandwidth sharing game, irrational players receive utility that approaches the social optimal utility.","Such examples indicate that it may sometimes be `rational' to be irrational."],"url":"http://arxiv.org/abs/2406.03928v1","category":"cs.GT"}
{"created":"2024-06-06 10:06:27","title":"Statistical Multicriteria Benchmarking via the GSD-Front","abstract":"Given the vast number of classifiers that have been (and continue to be) proposed, reliable methods for comparing them are becoming increasingly important. The desire for reliability is broken down into three main aspects: (1) Comparisons should allow for different quality metrics simultaneously. (2) Comparisons should take into account the statistical uncertainty induced by the choice of benchmark suite. (3) The robustness of the comparisons under small deviations in the underlying assumptions should be verifiable. To address (1), we propose to compare classifiers using a generalized stochastic dominance ordering (GSD) and present the GSD-front as an information-efficient alternative to the classical Pareto-front. For (2), we propose a consistent statistical estimator for the GSD-front and construct a statistical test for whether a (potentially new) classifier lies in the GSD-front of a set of state-of-the-art classifiers. For (3), we relax our proposed test using techniques from robust statistics and imprecise probabilities. We illustrate our concepts on the benchmark suite PMLB and on the platform OpenML.","sentences":["Given the vast number of classifiers that have been (and continue to be) proposed, reliable methods for comparing them are becoming increasingly important.","The desire for reliability is broken down into three main aspects: (1) Comparisons should allow for different quality metrics simultaneously.","(2) Comparisons should take into account the statistical uncertainty induced by the choice of benchmark suite.","(3) The robustness of the comparisons under small deviations in the underlying assumptions should be verifiable.","To address (1), we propose to compare classifiers using a generalized stochastic dominance ordering (GSD) and present the GSD-front as an information-efficient alternative to the classical Pareto-front.","For (2), we propose a consistent statistical estimator for the GSD-front and construct a statistical test for whether a (potentially new) classifier lies in the GSD-front of a set of state-of-the-art classifiers.","For (3), we relax our proposed test using techniques from robust statistics and imprecise probabilities.","We illustrate our concepts on the benchmark suite PMLB and on the platform OpenML."],"url":"http://arxiv.org/abs/2406.03924v1","category":"stat.ML"}
{"created":"2024-06-06 09:52:56","title":"Neuro-Symbolic Temporal Point Processes","abstract":"Our goal is to $\\textit{efficiently}$ discover a compact set of temporal logic rules to explain irregular events of interest. We introduce a neural-symbolic rule induction framework within the temporal point process model. The negative log-likelihood is the loss that guides the learning, where the explanatory logic rules and their weights are learned end-to-end in a $\\textit{differentiable}$ way. Specifically, predicates and logic rules are represented as $\\textit{vector embeddings}$, where the predicate embeddings are fixed and the rule embeddings are trained via gradient descent to obtain the most appropriate compositional representations of the predicate embeddings. To make the rule learning process more efficient and flexible, we adopt a $\\textit{sequential covering algorithm}$, which progressively adds rules to the model and removes the event sequences that have been explained until all event sequences have been covered. All the found rules will be fed back to the models for a final rule embedding and weight refinement. Our approach showcases notable efficiency and accuracy across synthetic and real datasets, surpassing state-of-the-art baselines by a wide margin in terms of efficiency.","sentences":["Our goal is to $\\textit{efficiently}$ discover a compact set of temporal logic rules to explain irregular events of interest.","We introduce a neural-symbolic rule induction framework within the temporal point process model.","The negative log-likelihood is the loss that guides the learning, where the explanatory logic rules and their weights are learned end-to-end in a $\\textit{differentiable}$ way.","Specifically, predicates and logic rules are represented as $\\textit{vector embeddings}$, where the predicate embeddings are fixed and the rule embeddings are trained via gradient descent to obtain the most appropriate compositional representations of the predicate embeddings.","To make the rule learning process more efficient and flexible, we adopt a $\\textit{sequential covering algorithm}$, which progressively adds rules to the model and removes the event sequences that have been explained until all event sequences have been covered.","All the found rules will be fed back to the models for a final rule embedding and weight refinement.","Our approach showcases notable efficiency and accuracy across synthetic and real datasets, surpassing state-of-the-art baselines by a wide margin in terms of efficiency."],"url":"http://arxiv.org/abs/2406.03914v1","category":"cs.LG"}
{"created":"2024-06-06 09:44:00","title":"A generalized uncertainty-inspired quantum black hole","abstract":"We derive the full spacetime metric of a generalized uncertainty-inspired quantum black hole. We examine a previous model of the interior in this approach and show that extending its metric to the full spacetime leads to serious issues in the asymptotic region. To remedy this, we introduce an \"improved scheme\" mimicking a similar prescription used in loop quantum gravity, where the quantum parameters are made momentum-dependent. Under this scheme, we rework the interior of the black hole and extend it to the full spacetime. We find that the resulting metric is asymptotically flat and its associated Kretschmann scalar is regular everywhere. We also show that the null expansion and Raychaudhuri equation are regular everywhere in this spacetime, implying that the classical singularity is resolved.","sentences":["We derive the full spacetime metric of a generalized uncertainty-inspired quantum black hole.","We examine a previous model of the interior in this approach and show that extending its metric to the full spacetime leads to serious issues in the asymptotic region.","To remedy this, we introduce an \"improved scheme\" mimicking a similar prescription used in loop quantum gravity, where the quantum parameters are made momentum-dependent.","Under this scheme, we rework the interior of the black hole and extend it to the full spacetime.","We find that the resulting metric is asymptotically flat and its associated Kretschmann scalar is regular everywhere.","We also show that the null expansion and Raychaudhuri equation are regular everywhere in this spacetime, implying that the classical singularity is resolved."],"url":"http://arxiv.org/abs/2406.03909v1","category":"gr-qc"}
{"created":"2024-06-06 09:39:19","title":"Pressurized phase transition cascade in BaMn$_2$P$_2$ and BaMn$_2$As$_2$","abstract":"The structural analogue of iron-based superconductors the BaMn$_2$P$_2$ and BaMn$_2$As$_2$ compounds under hydrostatic pressure upto 140 GPa were studied within the framework of DFT+U. The transition from an antiferromagnetic (AFM) insulator to an antiferromagnetic metal is observed under pressure of 6.4 GPa for BaMn$_2$P$_2$ and 8.3 GPa for BaMn$_2$As$_2$. This second order phase transition to the AFM metallic state provides an appropriate normal state for possible superconductivity in these materials. Moreover, a further increase in pressure leads to a series of first order magnetostructural phase transitions between different antiferromagnetic phases, then to a ferromagnetic metal and finally to a nonmagnetic metal. In case of doping these compounds could potentially be a superconductors under pressure (above 6-8 GPa) with critical temperature growing under pressure.","sentences":["The structural analogue of iron-based superconductors the BaMn$_2$P$_2$ and BaMn$_2$As$_2$ compounds under hydrostatic pressure upto 140 GPa were studied within the framework of DFT+U.","The transition from an antiferromagnetic (AFM) insulator to an antiferromagnetic metal is observed under pressure of 6.4 GPa for BaMn$_2$P$_2$ and 8.3 GPa for BaMn$_2$As$_2$. This second order phase transition to the AFM metallic state provides an appropriate normal state for possible superconductivity in these materials.","Moreover, a further increase in pressure leads to a series of first order magnetostructural phase transitions between different antiferromagnetic phases, then to a ferromagnetic metal and finally to a nonmagnetic metal.","In case of doping these compounds could potentially be a superconductors under pressure (above 6-8 GPa) with critical temperature growing under pressure."],"url":"http://arxiv.org/abs/2406.03904v1","category":"cond-mat.supr-con"}
{"created":"2024-06-06 09:35:21","title":"Lattice Lipschitz superposition operators on Banach function spaces","abstract":"We analyse and characterise the notion of lattice Lipschitz operator (a class of superposition operators, diagonal Lipschitz maps) when defined between Banach function spaces. After showing some general results, we restrict our attention to the case of those Lipschitz operators which are representable by pointwise composition with a strongly measurable function. Mimicking the classical definition and characterizations of (linear) multiplication operators between Banach function spaces, we show that under certain conditions the requirement for a diagonal Lipschitz operator to be well-defined between two such spaces $X(\\mu)$ and $Y(\\mu)$ is that it can be represented by a strongly measurable function which belongs to the Bochner space $\\mathcal M(X,Y) \\big(\\mu, Lip_0(\\mathbb R) \\big). $ Here, $\\mathcal M(X,Y) $ is the space of multiplication operators between $X(\\mu)$ and $Y(\\mu),$ and $Lip_0(\\mathbb R)$ is the space of real-valued Lipschitz maps with real variable that are equal to $0$ in $0. $ This opens the door to a better understanding of these maps, as well as finding the relation of these operators to some normed tensor products and other classes of maps.","sentences":["We analyse and characterise the notion of lattice Lipschitz operator (a class of superposition operators, diagonal Lipschitz maps) when defined between Banach function spaces.","After showing some general results, we restrict our attention to the case of those Lipschitz operators which are representable by pointwise composition with a strongly measurable function.","Mimicking the classical definition and characterizations of (linear) multiplication operators between Banach function spaces, we show that under certain conditions the requirement for a diagonal Lipschitz operator to be well-defined between two such spaces $X(\\mu)$ and $Y(\\mu)$ is that it can be represented by a strongly measurable function which belongs to the Bochner space $\\mathcal M(X,Y) \\big(\\mu, Lip_0(\\mathbb R) \\big).","$ Here, $\\mathcal M(X,Y) $ is the space of multiplication operators between $X(\\mu)$ and $Y(\\mu),$ and $Lip_0(\\mathbb R)$ is the space of real-valued Lipschitz maps with real variable that are equal to $0$ in $0.","$","This opens the door to a better understanding of these maps, as well as finding the relation of these operators to some normed tensor products and other classes of maps."],"url":"http://arxiv.org/abs/2406.03895v1","category":"math.FA"}
{"created":"2024-06-06 09:26:02","title":"Exploring Pessimism and Optimism Dynamics in Deep Reinforcement Learning","abstract":"Off-policy actor-critic algorithms have shown promise in deep reinforcement learning for continuous control tasks. Their success largely stems from leveraging pessimistic state-action value function updates, which effectively address function approximation errors and improve performance. However, such pessimism can lead to under-exploration, constraining the agent's ability to explore/refine its policies. Conversely, optimism can counteract under-exploration, but it also carries the risk of excessive risk-taking and poor convergence if not properly balanced. Based on these insights, we introduce Utility Soft Actor-Critic (USAC), a novel framework within the actor-critic paradigm that enables independent control over the degree of pessimism/optimism for both the actor and the critic via interpretable parameters. USAC adapts its exploration strategy based on the uncertainty of critics through a utility function that allows us to balance between pessimism and optimism separately. By going beyond binary choices of optimism and pessimism, USAC represents a significant step towards achieving balance within off-policy actor-critic algorithms. Our experiments across various continuous control problems show that the degree of pessimism or optimism depends on the nature of the task. Furthermore, we demonstrate that USAC can outperform state-of-the-art algorithms for appropriately configured pessimism/optimism parameters.","sentences":["Off-policy actor-critic algorithms have shown promise in deep reinforcement learning for continuous control tasks.","Their success largely stems from leveraging pessimistic state-action value function updates, which effectively address function approximation errors and improve performance.","However, such pessimism can lead to under-exploration, constraining the agent's ability to explore/refine its policies.","Conversely, optimism can counteract under-exploration, but it also carries the risk of excessive risk-taking and poor convergence if not properly balanced.","Based on these insights, we introduce Utility Soft Actor-Critic (USAC), a novel framework within the actor-critic paradigm that enables independent control over the degree of pessimism/optimism for both the actor and the critic via interpretable parameters.","USAC adapts its exploration strategy based on the uncertainty of critics through a utility function that allows us to balance between pessimism and optimism separately.","By going beyond binary choices of optimism and pessimism, USAC represents a significant step towards achieving balance within off-policy actor-critic algorithms.","Our experiments across various continuous control problems show that the degree of pessimism or optimism depends on the nature of the task.","Furthermore, we demonstrate that USAC can outperform state-of-the-art algorithms for appropriately configured pessimism/optimism parameters."],"url":"http://arxiv.org/abs/2406.03890v1","category":"cs.LG"}
{"created":"2024-06-06 08:54:45","title":"A Comprehensive Study of Quantum Arithmetic Circuits","abstract":"In recent decades, the field of quantum computing has experienced remarkable progress. This progress is marked by the superior performance of many quantum algorithms compared to their classical counterparts, with Shor's algorithm serving as a prominent illustration. Quantum arithmetic circuits, which are the fundamental building blocks in numerous quantum algorithms, have attracted much attention. Despite extensive exploration of various designs in the existing literature, researchers remain keen on developing novel designs and improving existing ones.   In this review article, we aim to provide a systematically organized and easily comprehensible overview of the current state-of-the-art in quantum arithmetic circuits. Specifically, this study covers fundamental operations such as addition, subtraction, multiplication, division and modular exponentiation. We delve into the detailed quantum implementations of these prominent designs and evaluate their efficiency considering various objectives. We also discuss potential applications of presented arithmetic circuits and suggest future research directions.","sentences":["In recent decades, the field of quantum computing has experienced remarkable progress.","This progress is marked by the superior performance of many quantum algorithms compared to their classical counterparts, with Shor's algorithm serving as a prominent illustration.","Quantum arithmetic circuits, which are the fundamental building blocks in numerous quantum algorithms, have attracted much attention.","Despite extensive exploration of various designs in the existing literature, researchers remain keen on developing novel designs and improving existing ones.   ","In this review article, we aim to provide a systematically organized and easily comprehensible overview of the current state-of-the-art in quantum arithmetic circuits.","Specifically, this study covers fundamental operations such as addition, subtraction, multiplication, division and modular exponentiation.","We delve into the detailed quantum implementations of these prominent designs and evaluate their efficiency considering various objectives.","We also discuss potential applications of presented arithmetic circuits and suggest future research directions."],"url":"http://arxiv.org/abs/2406.03867v1","category":"quant-ph"}
{"created":"2024-06-06 08:50:16","title":"PairNet: Training with Observed Pairs to Estimate Individual Treatment Effect","abstract":"Given a dataset of individuals each described by a covariate vector, a treatment, and an observed outcome on the treatment, the goal of the individual treatment effect (ITE) estimation task is to predict outcome changes resulting from a change in treatment. A fundamental challenge is that in the observational data, a covariate's outcome is observed only under one treatment, whereas we need to infer the difference in outcomes under two different treatments. Several existing approaches address this issue through training with inferred pseudo-outcomes, but their success relies on the quality of these pseudo-outcomes. We propose PairNet, a novel ITE estimation training strategy that minimizes losses over pairs of examples based on their factual observed outcomes. Theoretical analysis for binary treatments reveals that PairNet is a consistent estimator of ITE risk, and achieves smaller generalization error than baseline models. Empirical comparison with thirteen existing methods across eight benchmarks, covering both discrete and continuous treatments, shows that PairNet achieves significantly lower ITE error compared to the baselines. Also, it is model-agnostic and easy to implement.","sentences":["Given a dataset of individuals each described by a covariate vector, a treatment, and an observed outcome on the treatment, the goal of the individual treatment effect (ITE) estimation task is to predict outcome changes resulting from a change in treatment.","A fundamental challenge is that in the observational data, a covariate's outcome is observed only under one treatment, whereas we need to infer the difference in outcomes under two different treatments.","Several existing approaches address this issue through training with inferred pseudo-outcomes, but their success relies on the quality of these pseudo-outcomes.","We propose PairNet, a novel ITE estimation training strategy that minimizes losses over pairs of examples based on their factual observed outcomes.","Theoretical analysis for binary treatments reveals that PairNet is a consistent estimator of ITE risk, and achieves smaller generalization error than baseline models.","Empirical comparison with thirteen existing methods across eight benchmarks, covering both discrete and continuous treatments, shows that PairNet achieves significantly lower ITE error compared to the baselines.","Also, it is model-agnostic and easy to implement."],"url":"http://arxiv.org/abs/2406.03864v1","category":"cs.LG"}
{"created":"2024-06-06 08:49:22","title":"Small area estimation with generalized random forests: Estimating poverty rates in Mexico","abstract":"Identifying and addressing poverty is challenging in administrative units with limited information on income distribution and well-being. To overcome this obstacle, small area estimation methods have been developed to provide reliable and efficient estimators at disaggregated levels, enabling informed decision-making by policymakers despite the data scarcity. From a theoretical perspective, we propose a robust and flexible approach for estimating poverty indicators based on binary response variables within the small area estimation context: the generalized mixed effects random forest. Our method employs machine learning techniques to identify predictive, non-linear relationships from data, while also modeling hierarchical structures. Mean squared error estimation is explored using a parametric bootstrap. From an applied perspective, we examine the impact of information loss due to converting continuous variables into binary variables on the performance of small area estimation methods. We evaluate the proposed point and uncertainty estimates in both model- and design-based simulations. Finally, we apply our method to a case study revealing spatial patterns of poverty in the Mexican state of Tlaxcala.","sentences":["Identifying and addressing poverty is challenging in administrative units with limited information on income distribution and well-being.","To overcome this obstacle, small area estimation methods have been developed to provide reliable and efficient estimators at disaggregated levels, enabling informed decision-making by policymakers despite the data scarcity.","From a theoretical perspective, we propose a robust and flexible approach for estimating poverty indicators based on binary response variables within the small area estimation context: the generalized mixed effects random forest.","Our method employs machine learning techniques to identify predictive, non-linear relationships from data, while also modeling hierarchical structures.","Mean squared error estimation is explored using a parametric bootstrap.","From an applied perspective, we examine the impact of information loss due to converting continuous variables into binary variables on the performance of small area estimation methods.","We evaluate the proposed point and uncertainty estimates in both model- and design-based simulations.","Finally, we apply our method to a case study revealing spatial patterns of poverty in the Mexican state of Tlaxcala."],"url":"http://arxiv.org/abs/2406.03861v1","category":"stat.ME"}
{"created":"2024-06-06 08:45:36","title":"Reducing the climate impact of data portals: a case study","abstract":"The carbon footprint share of the information and communication technology (ICT) sector has steadily increased in the past decade and is predicted to make up as much as 23 \\% of global emissions in 2030. This shows a pressing need for developers, including the information retrieval community, to make their code more energy-efficient. In this project proposal, we discuss techniques to reduce the energy footprint of the MaRDI (Mathematical Research Data Initiative) Portal, a MediaWiki-based knowledge base. In future work, we plan to implement these changes and provide concrete measurements on the gain in energy efficiency. Researchers developing similar knowledge bases can adapt our measures to reduce their environmental footprint. In this way, we are working on mitigating the climate impact of Information Retrieval research.","sentences":["The carbon footprint share of the information and communication technology (ICT) sector has steadily increased in the past decade and is predicted to make up as much as 23 \\% of global emissions in 2030.","This shows a pressing need for developers, including the information retrieval community, to make their code more energy-efficient.","In this project proposal, we discuss techniques to reduce the energy footprint of the MaRDI (Mathematical Research Data Initiative) Portal, a MediaWiki-based knowledge base.","In future work, we plan to implement these changes and provide concrete measurements on the gain in energy efficiency.","Researchers developing similar knowledge bases can adapt our measures to reduce their environmental footprint.","In this way, we are working on mitigating the climate impact of Information Retrieval research."],"url":"http://arxiv.org/abs/2406.03858v1","category":"cs.IR"}
{"created":"2024-06-06 08:41:46","title":"Performance of large language models in numerical vs. semantic medical knowledge: Benchmarking on evidence-based Q&As","abstract":"Clinical problem-solving requires processing of semantic medical knowledge such as illness scripts and numerical medical knowledge of diagnostic tests for evidence-based decision-making. As large language models (LLMs) show promising results in many aspects of language-based clinical practice, their ability to generate non-language evidence-based answers to clinical questions is inherently limited by tokenization. Therefore, we evaluated LLMs' performance on two question types: numeric (correlating findings) and semantic (differentiating entities) while examining differences within and between LLMs in medical aspects and comparing their performance to humans. To generate straightforward multi-choice questions and answers (QAs) based on evidence-based medicine (EBM), we used a comprehensive medical knowledge graph (encompassed data from more than 50,00 peer-reviewed articles) and created the \"EBMQA\". EBMQA contains 105,000 QAs labeled with medical and non-medical topics and classified into numerical or semantic questions. We benchmarked this dataset using more than 24,500 QAs on two state-of-the-art LLMs: Chat-GPT4 and Claude3-Opus. We evaluated the LLMs accuracy on semantic and numerical question types and according to sub-labeled topics. For validation, six medical experts were tested on 100 numerical EBMQA questions. We found that both LLMs excelled more in semantic than numerical QAs, with Claude3 surpassing GPT4 in numerical QAs. However, both LLMs showed inter and intra gaps in different medical aspects and remained inferior to humans. Thus, their medical advice should be addressed carefully.","sentences":["Clinical problem-solving requires processing of semantic medical knowledge such as illness scripts and numerical medical knowledge of diagnostic tests for evidence-based decision-making.","As large language models (LLMs) show promising results in many aspects of language-based clinical practice, their ability to generate non-language evidence-based answers to clinical questions is inherently limited by tokenization.","Therefore, we evaluated LLMs' performance on two question types: numeric (correlating findings) and semantic (differentiating entities) while examining differences within and between LLMs in medical aspects and comparing their performance to humans.","To generate straightforward multi-choice questions and answers (QAs) based on evidence-based medicine (EBM), we used a comprehensive medical knowledge graph (encompassed data from more than 50,00 peer-reviewed articles) and created the \"EBMQA\".","EBMQA contains 105,000 QAs labeled with medical and non-medical topics and classified into numerical or semantic questions.","We benchmarked this dataset using more than 24,500 QAs on two state-of-the-art LLMs: Chat-GPT4 and Claude3-Opus.","We evaluated the LLMs accuracy on semantic and numerical question types and according to sub-labeled topics.","For validation, six medical experts were tested on 100 numerical EBMQA questions.","We found that both LLMs excelled more in semantic than numerical QAs, with Claude3 surpassing GPT4 in numerical QAs.","However, both LLMs showed inter and intra gaps in different medical aspects and remained inferior to humans.","Thus, their medical advice should be addressed carefully."],"url":"http://arxiv.org/abs/2406.03855v1","category":"cs.CL"}
{"created":"2024-06-06 08:41:16","title":"Universal properties of locally generated terahertz waveforms from polarization-controlled two- and multi-color ionizing fields","abstract":"The polarization states of terahertz (THz) radiation generated in a photo-ionized gas driven by strong two- or multi-frequency fields with locally controlled polarization are studied. We reveal a universal property of the resulting THz waveforms: the ellipticity of their polarization state increases linearly with the frequency. This ``linear chirp of ellipticity'' makes plasma-based THz generation unique among other THz sources. However, it also puts some constraints on the polarization properties of the generated THz radiation. We derive a general expression for the THz ellipticity and demonstrate how the polarization states of the generated THz waveforms can be manipulated and controlled by the polarization of the pump pulses.","sentences":["The polarization states of terahertz (THz) radiation generated in a photo-ionized gas driven by strong two- or multi-frequency fields with locally controlled polarization are studied.","We reveal a universal property of the resulting THz waveforms: the ellipticity of their polarization state increases linearly with the frequency.","This ``linear chirp of ellipticity'' makes plasma-based THz generation unique among other THz sources.","However, it also puts some constraints on the polarization properties of the generated THz radiation.","We derive a general expression for the THz ellipticity and demonstrate how the polarization states of the generated THz waveforms can be manipulated and controlled by the polarization of the pump pulses."],"url":"http://arxiv.org/abs/2406.03854v1","category":"physics.optics"}
{"created":"2024-06-06 08:29:29","title":"WaveCastNet: A Deep Learning Ocean Wave Model with Energy Conservation","abstract":"Traditional wave forecasting models, although based on energy conservation equations, are computationally expensive. On the other hand, existing deep learning geophysical fluid models, while computationally efficient, often suffer from issues such as energy dissipation in long-term forecasts. This paper proposes a novel energy-balanced deep learning wave forecasting model called WaveCastNet (WCN). By incorporating wind fields at the current, previous, and future time steps, as well as wave fields at the current and previous time steps as input variables, WCN maintains energy balance within the model. Furthermore, the model employs adaptive Fourier operators as its core components and designs a masked loss function to better handle the impact of land-sea boundaries. A series of experiments on the ERA5 dataset demonstrate that WCN can achieve short-term forecast accuracy comparable to traditional models while exhibiting an understanding of the wave generation process. In comparative experiments under both normal and extreme conditions, WCN consistently outperforms the widely used WaveWatch III model in the industry. Even after long-term forecasting, WCN maintains a stable and energy-rich state. By further constructing a simple meteorological model, WCN-wind, which considers energy balance, this paper confirms the importance of energy constraints for improving the long-term forecast performance of deep learning meteorological models. This finding provides new ideas for future research on deep learning geophysical fluid models.","sentences":["Traditional wave forecasting models, although based on energy conservation equations, are computationally expensive.","On the other hand, existing deep learning geophysical fluid models, while computationally efficient, often suffer from issues such as energy dissipation in long-term forecasts.","This paper proposes a novel energy-balanced deep learning wave forecasting model called WaveCastNet (WCN).","By incorporating wind fields at the current, previous, and future time steps, as well as wave fields at the current and previous time steps as input variables, WCN maintains energy balance within the model.","Furthermore, the model employs adaptive Fourier operators as its core components and designs a masked loss function to better handle the impact of land-sea boundaries.","A series of experiments on the ERA5 dataset demonstrate that WCN can achieve short-term forecast accuracy comparable to traditional models while exhibiting an understanding of the wave generation process.","In comparative experiments under both normal and extreme conditions, WCN consistently outperforms the widely used WaveWatch III model in the industry.","Even after long-term forecasting, WCN maintains a stable and energy-rich state.","By further constructing a simple meteorological model, WCN-wind, which considers energy balance, this paper confirms the importance of energy constraints for improving the long-term forecast performance of deep learning meteorological models.","This finding provides new ideas for future research on deep learning geophysical fluid models."],"url":"http://arxiv.org/abs/2406.03848v1","category":"physics.ao-ph"}
{"created":"2024-06-06 08:13:24","title":"Measure solutions, smoothing effect, and deterministic particle approximation for a conservation law with nonlocal flux","abstract":"We consider a class of nonlocal conservation laws with an interaction kernel supported on the negative real half-line and featuring a decreasing jump at the origin. We provide, for the first time, an existence and uniqueness theory for said model with initial data in the space of probability measures. Our concept of solution allows to sort a lack of uniqueness problem which we exhibit in a specific example. Our approach uses the so-called \\emph{quantile}, or \\emph{pseudo-inverse} formulation of the PDE, which has been largely used for similar types of nonlocal transport equations in one-space dimension. Partly related to said approach, we then provide a deterministic particle approximation theorem for the equation under consideration, which works for general initial data in the space of probability measures with compact support. As a crucial step in both results, we use that our concept of solution (which we call \\emph{dissipative measure solution}) implies an instantaneous \\emph{measure-to-$L^\\infty$} smoothing effect, a property which is known to be featured as well by local conservation laws with genuinely nonlinear fluxes.","sentences":["We consider a class of nonlocal conservation laws with an interaction kernel supported on the negative real half-line and featuring a decreasing jump at the origin.","We provide, for the first time, an existence and uniqueness theory for said model with initial data in the space of probability measures.","Our concept of solution allows to sort a lack of uniqueness problem which we exhibit in a specific example.","Our approach uses the so-called \\emph{quantile}, or \\emph{pseudo-inverse} formulation of the PDE, which has been largely used for similar types of nonlocal transport equations in one-space dimension.","Partly related to said approach, we then provide a deterministic particle approximation theorem for the equation under consideration, which works for general initial data in the space of probability measures with compact support.","As a crucial step in both results, we use that our concept of solution (which we call \\emph{dissipative measure solution}) implies an instantaneous \\emph{measure-to-$L^\\infty$} smoothing effect, a property which is known to be featured as well by local conservation laws with genuinely nonlinear fluxes."],"url":"http://arxiv.org/abs/2406.03837v1","category":"math.AP"}
{"created":"2024-06-06 07:56:20","title":"Bayesian generalized method of moments applied to pseudo-observations in survival analysis","abstract":"Bayesian inference for survival regression modeling offers numerous advantages, especially for decision-making and external data borrowing, but demands the specification of the baseline hazard function, which may be a challenging task. We propose an alternative approach that does not need the specification of this function. Our approach combines pseudo-observations to convert censored data into longitudinal data with the Generalized Methods of Moments (GMM) to estimate the parameters of interest from the survival function directly. GMM may be viewed as an extension of the Generalized Estimating Equation (GEE) currently used for frequentist pseudo-observations analysis and can be extended to the Bayesian framework using a pseudo-likelihood function. We assessed the behavior of the frequentist and Bayesian GMM in the new context of analyzing pseudo-observations. We compared their performances to the Cox, GEE, and Bayesian piecewise exponential models through a simulation study of two-arm randomized clinical trials. Frequentist and Bayesian GMM gave valid inferences with similar performances compared to the three benchmark methods, except for small sample sizes and high censoring rates. For illustration, three post-hoc efficacy analyses were performed on randomized clinical trials involving patients with Ewing Sarcoma, producing results similar to those of the benchmark methods. Through a simple application of estimating hazard ratios, these findings confirm the effectiveness of this new Bayesian approach based on pseudo-observations and the generalized method of moments. This offers new insights on using pseudo-observations for Bayesian survival analysis.","sentences":["Bayesian inference for survival regression modeling offers numerous advantages, especially for decision-making and external data borrowing, but demands the specification of the baseline hazard function, which may be a challenging task.","We propose an alternative approach that does not need the specification of this function.","Our approach combines pseudo-observations to convert censored data into longitudinal data with the Generalized Methods of Moments (GMM) to estimate the parameters of interest from the survival function directly.","GMM may be viewed as an extension of the Generalized Estimating Equation (GEE) currently used for frequentist pseudo-observations analysis and can be extended to the Bayesian framework using a pseudo-likelihood function.","We assessed the behavior of the frequentist and Bayesian GMM in the new context of analyzing pseudo-observations.","We compared their performances to the Cox, GEE, and Bayesian piecewise exponential models through a simulation study of two-arm randomized clinical trials.","Frequentist and Bayesian GMM gave valid inferences with similar performances compared to the three benchmark methods, except for small sample sizes and high censoring rates.","For illustration, three post-hoc efficacy analyses were performed on randomized clinical trials involving patients with Ewing Sarcoma, producing results similar to those of the benchmark methods.","Through a simple application of estimating hazard ratios, these findings confirm the effectiveness of this new Bayesian approach based on pseudo-observations and the generalized method of moments.","This offers new insights on using pseudo-observations for Bayesian survival analysis."],"url":"http://arxiv.org/abs/2406.03821v1","category":"stat.AP"}
{"created":"2024-06-06 07:32:35","title":"Time delay of fast radio burst population with respect to the star formation history","abstract":"In spite of significant progress in the research of fast radio bursts (FRBs) in recent decade, their origin is still under extensive debate. Investigation on the population of FRBs can provide new insight into this interesting problem. In this paper, based on the first CHIME/FRB catalog, we construct a Bayesian framework to analyze the FRB population, with the selection effect of the CHIME telescope being properly taken into account. The energy function is modeled as the power-law with an exponential cutoff. Four redshift distribution models are considered, i.e., the star formation history (SFH) model, and three time-delayed models (Gaussian delay, log-normal delay, and power-law delay). The free parameters are simultaneously constrained using Bayesian inference method, and the Bayesian information criterion (BIC) is used in model comparison. According to BIC, the log-normal delay model fits the data best. The power-law delay model and Gaussian delay model can also give reasonable fits, although they are not as good as the log-normal delay model. However, the SFH model is strongly disfavored compared with the three time-delayed models. The energy function is tightly constrained and is almost independent of the redshift models, with the best-fitting power-law index $\\alpha\\approx 1.8$, and cut-off energy $\\log(E_c/{\\rm erg})\\approx 42$. The FRB population shows on average $3\\sim 5$ billion years time delay with respect to the SFH. Therefore, the hypothesis that the FRB population traces the SFH is conclusively ruled out.","sentences":["In spite of significant progress in the research of fast radio bursts (FRBs) in recent decade, their origin is still under extensive debate.","Investigation on the population of FRBs can provide new insight into this interesting problem.","In this paper, based on the first CHIME/FRB catalog, we construct a Bayesian framework to analyze the FRB population, with the selection effect of the CHIME telescope being properly taken into account.","The energy function is modeled as the power-law with an exponential cutoff.","Four redshift distribution models are considered, i.e., the star formation history (SFH) model, and three time-delayed models (Gaussian delay, log-normal delay, and power-law delay).","The free parameters are simultaneously constrained using Bayesian inference method, and the Bayesian information criterion (BIC) is used in model comparison.","According to BIC, the log-normal delay model fits the data best.","The power-law delay model and Gaussian delay model can also give reasonable fits, although they are not as good as the log-normal delay model.","However, the SFH model is strongly disfavored compared with the three time-delayed models.","The energy function is tightly constrained and is almost independent of the redshift models, with the best-fitting power-law index $\\alpha\\approx 1.8$, and cut-off energy $\\log(E_c/{\\rm erg})\\approx 42$.","The FRB population shows on average $3\\sim 5$ billion years time delay with respect to the SFH.","Therefore, the hypothesis that the FRB population traces the SFH is conclusively ruled out."],"url":"http://arxiv.org/abs/2406.03809v1","category":"astro-ph.HE"}
{"created":"2024-06-06 07:27:59","title":"Increasing the detectability of long-period and nulling pulsars in next-generation pulsar surveys","abstract":"Recent discoveries of multiple long-period pulsars (periods ${\\sim}10\\,$s or larger) are starting to challenge the conventional notion that coherent radio emission cannot be produced by objects that are below the many theorised death lines. Many of the past pulsar surveys and software have been prone to selection effects that restricted their sensitivities towards long-period and sporadically-emitting objects. Pulsar surveys using new-generation low-frequency facilities are starting to employ longer dwell times, which makes them significantly more sensitive in detecting long-period or nulling pulsars. There have also been software advancements to aid more sensitive searches towards long-period objects. Furthermore, recent discoveries suggest that nulling may be a key aspect of the long-period pulsar population. We simulate both long-period and nulling pulsar signals, using the Southern-sky MWA Rapid Two-meter (SMART) survey data as reference, and explore the detection efficacy of popular search methods such as the fast Fourier transform (FFT), fast-folding algorithm (FFA) and single pulse search (SPS). For FFT-based search and SPS, we make use of the PRESTO implementation, and for FFA we use RIPTIDE. We find RIPTIDE's FFA to be more sensitive; however, it is also the slowest algorithm. PRESTO's FFT, although faster than others, also shows some unexpected inaccuracies in detection properties. SPS is highly sensitive to long-period and nulling signals, but only for pulses with high intrinsic signal-to-noise ratios. We use these findings to inform current and future pulsar surveys that aim to uncover a large population of long-period or nulling objects and comment on how to make optimal use of these methods in unison.","sentences":["Recent discoveries of multiple long-period pulsars (periods ${\\sim}10\\,$s or larger) are starting to challenge the conventional notion that coherent radio emission cannot be produced by objects that are below the many theorised death lines.","Many of the past pulsar surveys and software have been prone to selection effects that restricted their sensitivities towards long-period and sporadically-emitting objects.","Pulsar surveys using new-generation low-frequency facilities are starting to employ longer dwell times, which makes them significantly more sensitive in detecting long-period or nulling pulsars.","There have also been software advancements to aid more sensitive searches towards long-period objects.","Furthermore, recent discoveries suggest that nulling may be a key aspect of the long-period pulsar population.","We simulate both long-period and nulling pulsar signals, using the Southern-sky MWA Rapid Two-meter (SMART) survey data as reference, and explore the detection efficacy of popular search methods such as the fast Fourier transform (FFT), fast-folding algorithm (FFA) and single pulse search (SPS).","For FFT-based search and SPS, we make use of the PRESTO implementation, and for FFA we use RIPTIDE.","We find RIPTIDE's FFA to be more sensitive; however, it is also the slowest algorithm.","PRESTO's FFT, although faster than others, also shows some unexpected inaccuracies in detection properties.","SPS is highly sensitive to long-period and nulling signals, but only for pulses with high intrinsic signal-to-noise ratios.","We use these findings to inform current and future pulsar surveys that aim to uncover a large population of long-period or nulling objects and comment on how to make optimal use of these methods in unison."],"url":"http://arxiv.org/abs/2406.03806v1","category":"astro-ph.HE"}
{"created":"2024-06-06 07:05:20","title":"Low-Rank Similarity Mining for Multimodal Dataset Distillation","abstract":"Though dataset distillation has witnessed rapid development in recent years, the distillation of multimodal data, e.g., image-text pairs, poses unique and under-explored challenges. Unlike unimodal data, image-text contrastive learning (ITC) data lack inherent categorization and should instead place greater emphasis on modality correspondence. In this work, we propose Low-Rank Similarity Mining (LoRS) for multimodal dataset distillation, that concurrently distills a ground truth similarity matrix with image-text pairs, and leverages low-rank factorization for efficiency and scalability. The proposed approach brings significant improvement to the existing algorithms, marking a significant contribution to the field of visual-language dataset distillation. We advocate adopting LoRS as a foundational synthetic data setup for image-text dataset distillation. Our code is available at https://github.com/silicx/LoRS_Distill.","sentences":["Though dataset distillation has witnessed rapid development in recent years, the distillation of multimodal data, e.g., image-text pairs, poses unique and under-explored challenges.","Unlike unimodal data, image-text contrastive learning (ITC) data lack inherent categorization and should instead place greater emphasis on modality correspondence.","In this work, we propose Low-Rank Similarity Mining (LoRS) for multimodal dataset distillation, that concurrently distills a ground truth similarity matrix with image-text pairs, and leverages low-rank factorization for efficiency and scalability.","The proposed approach brings significant improvement to the existing algorithms, marking a significant contribution to the field of visual-language dataset distillation.","We advocate adopting LoRS as a foundational synthetic data setup for image-text dataset distillation.","Our code is available at https://github.com/silicx/LoRS_Distill."],"url":"http://arxiv.org/abs/2406.03793v1","category":"cs.LG"}
{"created":"2024-06-06 05:41:30","title":"High-Order Continuous Geometrical Validity","abstract":"We propose a conservative algorithm to test the geometrical validity of simplicial (triangles, tetrahedra), tensor product (quadrilaterals, hexahedra), and mixed (prisms) elements of arbitrary polynomial order as they deform over a piecewise-linear trajectory.   Our algorithm uses a combination of adaptive B\\'ezier refinement and bisection search to determine if, when, and where the Jacobian determinant of an element's polynomial geometric map becomes negative in the transition from one configuration to another.   Unlike previous approaches, our method preserves its properties also when implemented using floating point arithmetic: This feature comes at a small additional runtime cost compared to existing inexact methods, making it a drop-in replacement for current validity tests, while providing superior robustness and generality.   To prove the practical effectiveness of our algorithm, we demonstrate its use in a high-order Incremental Potential Contact (IPC) elastodynamic simulator, and we experimentally show that it prevents invalid, simulation-breaking configurations that would otherwise occur using inexact methods, without the need for manual parameter tuning.","sentences":["We propose a conservative algorithm to test the geometrical validity of simplicial (triangles, tetrahedra), tensor product (quadrilaterals, hexahedra), and mixed (prisms) elements of arbitrary polynomial order as they deform over a piecewise-linear trajectory.   ","Our algorithm uses a combination of adaptive B\\'ezier refinement and bisection search to determine if, when, and where the Jacobian determinant of an element's polynomial geometric map becomes negative in the transition from one configuration to another.   ","Unlike previous approaches, our method preserves its properties also when implemented using floating point arithmetic: This feature comes at a small additional runtime cost compared to existing inexact methods, making it a drop-in replacement for current validity tests, while providing superior robustness and generality.   ","To prove the practical effectiveness of our algorithm, we demonstrate its use in a high-order Incremental Potential Contact (IPC) elastodynamic simulator, and we experimentally show that it prevents invalid, simulation-breaking configurations that would otherwise occur using inexact methods, without the need for manual parameter tuning."],"url":"http://arxiv.org/abs/2406.03756v1","category":"cs.CG"}
{"created":"2024-06-06 05:35:33","title":"Flavor-independent yield of high-$p_T$ hadrons from nuclear collisions","abstract":"Data on high-$p_T$ hadron production in heavy ion collisions at Feynman $x_F=0$ indicate at universality of the observed nuclear suppression. Our analysis of the production mechanisms demonstrates important role of the color transparency effects which make the survival probability of a quark-antiquark dipole independent of the quark flavor, provided that the hadron wave function is formed outside the medium. The latter condition imposes restrictions on the range of $p_T$, which should be sufficiently high to make the nuclear suppression universal. We also found that the in-medium broadening rate $\\hat q$ (frequently called transport coefficient) significantly depends on the quark flavor, diminishing for heavy quarks.","sentences":["Data on high-$p_T$ hadron production in heavy ion collisions at Feynman $x_F=0$ indicate at universality of the observed nuclear suppression.","Our analysis of the production mechanisms demonstrates important role of the color transparency effects which make the survival probability of a quark-antiquark dipole independent of the quark flavor, provided that the hadron wave function is formed outside the medium.","The latter condition imposes restrictions on the range of $p_T$, which should be sufficiently high to make the nuclear suppression universal.","We also found that the in-medium broadening rate $\\hat q$ (frequently called transport coefficient) significantly depends on the quark flavor, diminishing for heavy quarks."],"url":"http://arxiv.org/abs/2406.03755v1","category":"hep-ph"}
{"created":"2024-06-06 04:31:59","title":"Correlated Electronic Structure and Incipient Flat Bands of the Kagome Superconductor CsCr3Sb5","abstract":"Kagome materials exhibit many novel phenomena emerging from the interplay between lattice geometry, electronic structure, and topology. A prime example is the vanadium-based kagome materials AV3Sb5 (A = K, Rb, and Cs) with superconductivity and unconventional charge-density wave (CDW). More interestingly, the substitution of vanadium by chromium further introduces magnetism and enhances the correlation effect in CsCr3Sb5 which likewise exhibits superconductivity under pressure and competing density-wave state. Here we systematically investigate the electronic structure of CsCr3Sb5 using high-resolution angle-resolved photoemission spectroscopy (APRES) and ab-initio calculations. Overall, the measured electronic structure agrees with the theoretical calculation. Remarkably, Cr 3d orbitals exhibit incoherent electronic states and contribute to incipient flat bands close to the Fermi level. The electronic structure shows a minor change across the magnetic transition at 55 K, suggesting a weak interplay between the local magnetic moment and itinerant electrons. Furthermore, we reveal a drastic enhancement of the electron scattering rate across the magnetic transition, which is relevant to the semiconducting-like transport property of the system at high temperatures. Our results suggest that CsCr3Sb5 is a strongly correlated Hund's metal with incipient flat bands near the Fermi level, which provides an electronic basis for understanding its novel properties in comparison to the non-magnetic and weakly correlated AV3Sb5.","sentences":["Kagome materials exhibit many novel phenomena emerging from the interplay between lattice geometry, electronic structure, and topology.","A prime example is the vanadium-based kagome materials AV3Sb5","(A = K, Rb, and Cs) with superconductivity and unconventional charge-density wave (CDW).","More interestingly, the substitution of vanadium by chromium further introduces magnetism and enhances the correlation effect in CsCr3Sb5 which likewise exhibits superconductivity under pressure and competing density-wave state.","Here we systematically investigate the electronic structure of CsCr3Sb5 using high-resolution angle-resolved photoemission spectroscopy (APRES) and ab-initio calculations.","Overall, the measured electronic structure agrees with the theoretical calculation.","Remarkably, Cr 3d orbitals exhibit incoherent electronic states and contribute to incipient flat bands close to the Fermi level.","The electronic structure shows a minor change across the magnetic transition at 55 K, suggesting a weak interplay between the local magnetic moment and itinerant electrons.","Furthermore, we reveal a drastic enhancement of the electron scattering rate across the magnetic transition, which is relevant to the semiconducting-like transport property of the system at high temperatures.","Our results suggest that CsCr3Sb5 is a strongly correlated Hund's metal with incipient flat bands near the Fermi level, which provides an electronic basis for understanding its novel properties in comparison to the non-magnetic and weakly correlated AV3Sb5."],"url":"http://arxiv.org/abs/2406.03740v1","category":"cond-mat.str-el"}
{"created":"2024-06-06 04:24:12","title":"Energy-Efficient Hybrid Beamforming for Integrated Sensing and Communication Enabled mmWave MIMO Systems","abstract":"This paper conceives a hybrid beamforming design (HBF) that maximizes the energy efficiency (EE) of an integrated sensing and communication (ISAC)-enabled millimeter wave (mmWave) multiple-input multiple-output (MIMO) system. In the system under consideration, an ISAC base station (BS) with the hybrid MIMO architecture communicates with multiple users and simultaneously detects multiple targets. The proposed scheme seeks to maximize the EE of the system, considering the signal-to-interference and noise ratio (SINR) as the user's quality of service (QoS) and the sensing beampattern gain of the targets as constraints. To solve this non-convex problem, we initially adopt Dinkelbach's method to convert the fractional objective function to subtractive form and subsequently obtain the sub-optimal fully-digital transmit beamformer by leveraging the principle of semi-definite relaxation. Subsequently, we propose a penalty-based manifold optimization scheme in conjunction with an alternating minimization method to determine the baseband (BB) and analog beamformers based on the designed fully-digital transmit beamformer. Finally, simulation results are given to demonstrate the efficacy of our proposed algorithm with respect to the benchmarks.","sentences":["This paper conceives a hybrid beamforming design (HBF) that maximizes the energy efficiency (EE) of an integrated sensing and communication (ISAC)-enabled millimeter wave (mmWave) multiple-input multiple-output (MIMO) system.","In the system under consideration, an ISAC base station (BS) with the hybrid MIMO architecture communicates with multiple users and simultaneously detects multiple targets.","The proposed scheme seeks to maximize the EE of the system, considering the signal-to-interference and noise ratio (SINR) as the user's quality of service (QoS) and the sensing beampattern gain of the targets as constraints.","To solve this non-convex problem, we initially adopt Dinkelbach's method to convert the fractional objective function to subtractive form and subsequently obtain the sub-optimal fully-digital transmit beamformer by leveraging the principle of semi-definite relaxation.","Subsequently, we propose a penalty-based manifold optimization scheme in conjunction with an alternating minimization method to determine the baseband (BB) and analog beamformers based on the designed fully-digital transmit beamformer.","Finally, simulation results are given to demonstrate the efficacy of our proposed algorithm with respect to the benchmarks."],"url":"http://arxiv.org/abs/2406.03737v1","category":"eess.SP"}
{"created":"2024-06-06 04:08:09","title":"On the stability of singular Hopf bifurcation and its application","abstract":"Recently, research on the complex periodic behavior of multi-scale systems has become increasingly popular. Krupa et al. \\cite{krupa2} provided a way to obtain relaxation oscillations in slow-fast systems through singular Hopf bifurcations and canard explosion. The authors derived a $O(1)$ expression $A$ for the first Lyapunov coefficient (under the condition $A \\neq 0$), and deduced the bifurcation curves of singular Hopf and canard explosions.   This paper employs Blow-up technique, normal form theory, and Lyapunov coefficient formula to present higher-order approximate expressions for the first Lyapunov coefficient when $A=0$ for slow-fast systems. As an application, we investigate the bifurcation phenomena of a predator-prey model with Allee effects. Utilizing the formulas obtained in this paper, we identify both supercritical and subcritical Hopf bifurcations that may occur simultaneously in the system. Numerical simulations validate the results. Finally, by normal form and slow divergence integral theory, we prove the cyclicity of the system is 1.","sentences":["Recently, research on the complex periodic behavior of multi-scale systems has become increasingly popular.","Krupa et al. \\cite{krupa2} provided a way to obtain relaxation oscillations in slow-fast systems through singular Hopf bifurcations and canard explosion.","The authors derived a $O(1)$ expression $A$ for the first Lyapunov coefficient (under the condition $A \\neq 0$), and deduced the bifurcation curves of singular Hopf and canard explosions.   ","This paper employs Blow-up technique, normal form theory, and Lyapunov coefficient formula to present higher-order approximate expressions for the first Lyapunov coefficient when $A=0$ for slow-fast systems.","As an application, we investigate the bifurcation phenomena of a predator-prey model with Allee effects.","Utilizing the formulas obtained in this paper, we identify both supercritical and subcritical Hopf bifurcations that may occur simultaneously in the system.","Numerical simulations validate the results.","Finally, by normal form and slow divergence integral theory, we prove the cyclicity of the system is 1."],"url":"http://arxiv.org/abs/2406.03732v1","category":"math.DS"}
{"created":"2024-06-06 04:06:00","title":"Quality-Diversity with Limited Resources","abstract":"Quality-Diversity (QD) algorithms have emerged as a powerful optimization paradigm with the aim of generating a set of high-quality and diverse solutions. To achieve such a challenging goal, QD algorithms require maintaining a large archive and a large population in each iteration, which brings two main issues, sample and resource efficiency. Most advanced QD algorithms focus on improving the sample efficiency, while the resource efficiency is overlooked to some extent. Particularly, the resource overhead during the training process has not been touched yet, hindering the wider application of QD algorithms. In this paper, we highlight this important research question, i.e., how to efficiently train QD algorithms with limited resources, and propose a novel and effective method called RefQD to address it. RefQD decomposes a neural network into representation and decision parts, and shares the representation part with all decision parts in the archive to reduce the resource overhead. It also employs a series of strategies to address the mismatch issue between the old decision parts and the newly updated representation part. Experiments on different types of tasks from small to large resource consumption demonstrate the excellent performance of RefQD: it not only uses significantly fewer resources (e.g., 16\\% GPU memories on QDax and 3.7\\% on Atari) but also achieves comparable or better performance compared to sample-efficient QD algorithms. Our code is available at \\url{https://github.com/lamda-bbo/RefQD}.","sentences":["Quality-Diversity (QD) algorithms have emerged as a powerful optimization paradigm with the aim of generating a set of high-quality and diverse solutions.","To achieve such a challenging goal, QD algorithms require maintaining a large archive and a large population in each iteration, which brings two main issues, sample and resource efficiency.","Most advanced QD algorithms focus on improving the sample efficiency, while the resource efficiency is overlooked to some extent.","Particularly, the resource overhead during the training process has not been touched yet, hindering the wider application of QD algorithms.","In this paper, we highlight this important research question, i.e., how to efficiently train QD algorithms with limited resources, and propose a novel and effective method called RefQD to address it.","RefQD decomposes a neural network into representation and decision parts, and shares the representation part with all decision parts in the archive to reduce the resource overhead.","It also employs a series of strategies to address the mismatch issue between the old decision parts and the newly updated representation part.","Experiments on different types of tasks from small to large resource consumption demonstrate the excellent performance of RefQD: it not only uses significantly fewer resources (e.g., 16\\% GPU memories on QDax and 3.7\\% on Atari) but also achieves comparable or better performance compared to sample-efficient QD algorithms.","Our code is available at \\url{https://github.com/lamda-bbo/RefQD}."],"url":"http://arxiv.org/abs/2406.03731v1","category":"cs.LG"}
{"created":"2024-06-06 04:05:12","title":"Enhancing Sign Language Detection through Mediapipe and Convolutional Neural Networks (CNN)","abstract":"This research combines MediaPipe and CNNs for the efficient and accurate interpretation of ASL dataset for the real-time detection of sign language. The system presented here captures and processes hands' gestures in real time. the intended purpose was to create a very easy, accurate, and fast way of entering commands without the necessity of touching something.MediaPipe supports one of the powerful frameworks in real-time hand tracking capabilities for the ability to capture and preprocess hand movements, which increases the accuracy of the gesture recognition system. Actually, the integration of CNN with the MediaPipe results in higher efficiency in using the model of real-time processing.The accuracy achieved by the model on ASL datasets is 99.12\\%.The model was tested using American Sign Language (ASL) datasets. The results were then compared to those of existing methods to evaluate how well it performed, using established evaluation techniques. The system will have applications in the communication, education, and accessibility domains. Making systems such as described in this paper even better will assist people with hearing impairment and make things accessible to them. We tested the recognition and translation performance on an ASL dataset and achieved better accuracy over previous models.It is meant to the research is to identify the characters that American signs recognize using hand images taken from a web camera by based on mediapipe and CNNs","sentences":["This research combines MediaPipe and CNNs for the efficient and accurate interpretation of ASL dataset for the real-time detection of sign language.","The system presented here captures and processes hands' gestures in real time.","the intended purpose was to create a very easy, accurate, and fast way of entering commands without the necessity of touching something.","MediaPipe supports one of the powerful frameworks in real-time hand tracking capabilities for the ability to capture and preprocess hand movements, which increases the accuracy of the gesture recognition system.","Actually, the integration of CNN with the MediaPipe results in higher efficiency in using the model of real-time processing.","The accuracy achieved by the model on ASL datasets is 99.12\\%.The model was tested using American Sign Language (ASL) datasets.","The results were then compared to those of existing methods to evaluate how well it performed, using established evaluation techniques.","The system will have applications in the communication, education, and accessibility domains.","Making systems such as described in this paper even better will assist people with hearing impairment and make things accessible to them.","We tested the recognition and translation performance on an ASL dataset and achieved better accuracy over previous models.","It is meant to the research is to identify the characters that American signs recognize using hand images taken from a web camera by based on mediapipe and CNNs"],"url":"http://arxiv.org/abs/2406.03729v1","category":"cs.LG"}
{"created":"2024-06-06 03:57:08","title":"Evaluating Durability: Benchmark Insights into Multimodal Watermarking","abstract":"With the development of large models, watermarks are increasingly employed to assert copyright, verify authenticity, or monitor content distribution. As applications become more multimodal, the utility of watermarking techniques becomes even more critical. The effectiveness and reliability of these watermarks largely depend on their robustness to various disturbances. However, the robustness of these watermarks in real-world scenarios, particularly under perturbations and corruption, is not well understood. To highlight the significance of robustness in watermarking techniques, our study evaluated the robustness of watermarked content generated by image and text generation models against common real-world image corruptions and text perturbations. Our results could pave the way for the development of more robust watermarking techniques in the future. Our project website can be found at \\url{https://mmwatermark-robustness.github.io/}.","sentences":["With the development of large models, watermarks are increasingly employed to assert copyright, verify authenticity, or monitor content distribution.","As applications become more multimodal, the utility of watermarking techniques becomes even more critical.","The effectiveness and reliability of these watermarks largely depend on their robustness to various disturbances.","However, the robustness of these watermarks in real-world scenarios, particularly under perturbations and corruption, is not well understood.","To highlight the significance of robustness in watermarking techniques, our study evaluated the robustness of watermarked content generated by image and text generation models against common real-world image corruptions and text perturbations.","Our results could pave the way for the development of more robust watermarking techniques in the future.","Our project website can be found at \\url{https://mmwatermark-robustness.github.io/}."],"url":"http://arxiv.org/abs/2406.03728v1","category":"cs.CV"}
{"created":"2024-06-06 03:15:13","title":"A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions","abstract":"Large language models (LLMs), such as GPT series models, have received substantial attention due to their impressive capabilities for generating and understanding human-level language. More recently, LLMs have emerged as an innovative and powerful adjunct in the medical field, transforming traditional practices and heralding a new era of enhanced healthcare services. This survey provides a comprehensive overview of Medical Large Language Models (Med-LLMs), outlining their evolution from general to the medical-specific domain (i.e, Technology and Application), as well as their transformative impact on healthcare (e.g., Trustworthiness and Safety). Concretely, starting from the fundamental history and technology of LLMs, we first delve into the progressive adaptation and refinements of general LLM models in the medical domain, especially emphasizing the advanced algorithms that boost the LLMs' performance in handling complicated medical environments, including clinical reasoning, knowledge graph, retrieval-augmented generation, human alignment, and multi-modal learning. Secondly, we explore the extensive applications of Med-LLMs across domains such as clinical decision support, report generation, and medical education, illustrating their potential to streamline healthcare services and augment patient outcomes. Finally, recognizing the imperative and responsible innovation, we discuss the challenges of ensuring fairness, accountability, privacy, and robustness in Med-LLMs applications. Finally, we conduct a concise discussion for anticipating possible future trajectories of Med-LLMs, identifying avenues for the prudent expansion of Med-LLMs. By consolidating above-mentioned insights, this review seeks to provide a comprehensive investigation of the potential strengths and limitations of Med-LLMs for professionals and researchers, ensuring a responsible landscape in the healthcare setting.","sentences":["Large language models (LLMs), such as GPT series models, have received substantial attention due to their impressive capabilities for generating and understanding human-level language.","More recently, LLMs have emerged as an innovative and powerful adjunct in the medical field, transforming traditional practices and heralding a new era of enhanced healthcare services.","This survey provides a comprehensive overview of Medical Large Language Models (Med-LLMs), outlining their evolution from general to the medical-specific domain (i.e, Technology and Application), as well as their transformative impact on healthcare (e.g., Trustworthiness and Safety).","Concretely, starting from the fundamental history and technology of LLMs, we first delve into the progressive adaptation and refinements of general LLM models in the medical domain, especially emphasizing the advanced algorithms that boost the LLMs' performance in handling complicated medical environments, including clinical reasoning, knowledge graph, retrieval-augmented generation, human alignment, and multi-modal learning.","Secondly, we explore the extensive applications of Med-LLMs across domains such as clinical decision support, report generation, and medical education, illustrating their potential to streamline healthcare services and augment patient outcomes.","Finally, recognizing the imperative and responsible innovation, we discuss the challenges of ensuring fairness, accountability, privacy, and robustness in Med-LLMs applications.","Finally, we conduct a concise discussion for anticipating possible future trajectories of Med-LLMs, identifying avenues for the prudent expansion of Med-LLMs.","By consolidating above-mentioned insights, this review seeks to provide a comprehensive investigation of the potential strengths and limitations of Med-LLMs for professionals and researchers, ensuring a responsible landscape in the healthcare setting."],"url":"http://arxiv.org/abs/2406.03712v1","category":"cs.CL"}
{"created":"2024-06-06 03:11:49","title":"Mean-variance portfolio selection in jump-diffusion model under no-shorting constraint: A viscosity solution approach","abstract":"This paper concerns a continuous time mean-variance (MV) portfolio selection problem in a jump-diffusion financial model with no-shorting trading constraint. The problem is reduced to two subproblems: solving a stochastic linear-quadratic (LQ) control problem under control constraint, and finding a maximal point of a real function. Based on a two-dimensional fully coupled ordinary differential equation (ODE), we construct an explicit viscosity solution to the Hamilton-Jacobi-Bellman equation of the constrained LQ problem. Together with the Meyer-It\\^o formula and a verification procedure, we obtain the optimal feedback controls of the constrained LQ problem and the original MV problem, which corrects the flawed results in some existing literatures. In addition, closed-form efficient portfolio and efficient frontier are derived. In the end, we present several examples where the two-dimensional ODE is decoupled.","sentences":["This paper concerns a continuous time mean-variance (MV) portfolio selection problem in a jump-diffusion financial model with no-shorting trading constraint.","The problem is reduced to two subproblems: solving a stochastic linear-quadratic (LQ) control problem under control constraint, and finding a maximal point of a real function.","Based on a two-dimensional fully coupled ordinary differential equation (ODE), we construct an explicit viscosity solution to the Hamilton-Jacobi-Bellman equation of the constrained LQ problem.","Together with the Meyer-It\\^o formula and a verification procedure, we obtain the optimal feedback controls of the constrained LQ problem and the original MV problem, which corrects the flawed results in some existing literatures.","In addition, closed-form efficient portfolio and efficient frontier are derived.","In the end, we present several examples where the two-dimensional ODE is decoupled."],"url":"http://arxiv.org/abs/2406.03709v1","category":"math.OC"}
{"created":"2024-06-06 03:10:15","title":"Enhanced Model-Free Dynamic State Estimation for a Soft Robot Finger Using an Embedded Optical Waveguide Sensor","abstract":"In this letter, an advanced stretchable optical waveguide sensor is implemented into a multidirectional PneuNet soft actuator to enhance dynamic state estimation through a NARX neural network. The stretchable waveguide featuring a semidivided core design from previous work is sensitive to multiple strain modes. It is integrated into a soft finger actuator with two pressure chambers that replicates human finger motions. The soft finger, designed for applications in soft robotic grippers or hands, is viewed in isolation under pneumatic actuation controlled by motorized linear stages. The research first characterizes the soft finger's workspace and sensor response. Subsequently, three dynamic state estimators are developed using NARX architecture, differing in the degree of incorporating the optical waveguide sensor response. Evaluation on a testing path reveals that the full sensor response significantly improves end effector position estimation, reducing mean error by 51\\% from 5.70 mm to 2.80 mm, compared to only 21\\% improvement to 4.53 mm using the estimator representing a single core waveguide design. The letter concludes by discussing the application of these estimators for (open-loop) model-predictive control and recommends future focus on advanced, structured soft (optical) sensors for model-free state estimation and control of soft robots.","sentences":["In this letter, an advanced stretchable optical waveguide sensor is implemented into a multidirectional PneuNet soft actuator to enhance dynamic state estimation through a NARX neural network.","The stretchable waveguide featuring a semidivided core design from previous work is sensitive to multiple strain modes.","It is integrated into a soft finger actuator with two pressure chambers that replicates human finger motions.","The soft finger, designed for applications in soft robotic grippers or hands, is viewed in isolation under pneumatic actuation controlled by motorized linear stages.","The research first characterizes the soft finger's workspace and sensor response.","Subsequently, three dynamic state estimators are developed using NARX architecture, differing in the degree of incorporating the optical waveguide sensor response.","Evaluation on a testing path reveals that the full sensor response significantly improves end effector position estimation, reducing mean error by 51\\% from 5.70 mm to 2.80 mm, compared to only 21\\% improvement to 4.53 mm using the estimator representing a single core waveguide design.","The letter concludes by discussing the application of these estimators for (open-loop) model-predictive control and recommends future focus on advanced, structured soft (optical) sensors for model-free state estimation and control of soft robots."],"url":"http://arxiv.org/abs/2406.03708v1","category":"cs.RO"}
{"created":"2024-06-06 02:32:41","title":"Superpoint Gaussian Splatting for Real-Time High-Fidelity Dynamic Scene Reconstruction","abstract":"Rendering novel view images in dynamic scenes is a crucial yet challenging task. Current methods mainly utilize NeRF-based methods to represent the static scene and an additional time-variant MLP to model scene deformations, resulting in relatively low rendering quality as well as slow inference speed. To tackle these challenges, we propose a novel framework named Superpoint Gaussian Splatting (SP-GS). Specifically, our framework first employs explicit 3D Gaussians to reconstruct the scene and then clusters Gaussians with similar properties (e.g., rotation, translation, and location) into superpoints. Empowered by these superpoints, our method manages to extend 3D Gaussian splatting to dynamic scenes with only a slight increase in computational expense. Apart from achieving state-of-the-art visual quality and real-time rendering under high resolutions, the superpoint representation provides a stronger manipulation capability. Extensive experiments demonstrate the practicality and effectiveness of our approach on both synthetic and real-world datasets. Please see our project page at https://dnvtmf.github.io/SP_GS.github.io.","sentences":["Rendering novel view images in dynamic scenes is a crucial yet challenging task.","Current methods mainly utilize NeRF-based methods to represent the static scene and an additional time-variant MLP to model scene deformations, resulting in relatively low rendering quality as well as slow inference speed.","To tackle these challenges, we propose a novel framework named Superpoint Gaussian Splatting (SP-GS).","Specifically, our framework first employs explicit 3D Gaussians to reconstruct the scene and then clusters Gaussians with similar properties (e.g., rotation, translation, and location) into superpoints.","Empowered by these superpoints, our method manages to extend 3D Gaussian splatting to dynamic scenes with only a slight increase in computational expense.","Apart from achieving state-of-the-art visual quality and real-time rendering under high resolutions, the superpoint representation provides a stronger manipulation capability.","Extensive experiments demonstrate the practicality and effectiveness of our approach on both synthetic and real-world datasets.","Please see our project page at https://dnvtmf.github.io/SP_GS.github.io."],"url":"http://arxiv.org/abs/2406.03697v1","category":"cs.CV"}
{"created":"2024-06-06 02:21:03","title":"New Taub-NUT Black Holes with Massive Spin-2 Hair","abstract":"We consider Einstein gravity extended with quadratic curvature invariants, where the well-known Ricci-flat Taub-NUT black hole remains a solution. An analysis of the unstable Lichnerowicz modes in the Taub-NUT background enables us to identify the mass and NUT parameters (m,n) where new Taub-NUT black holes can emerge. We then adopt numerical technique to construct these new Taub-NUT black holes that bifurcate away from the Ricci-flat ones. Unlike the Ricci-flat Taub-NUT, there can exist two new black holes for an appropriate given temperature, making it a total of three if we include the Ricci-flat one.","sentences":["We consider Einstein gravity extended with quadratic curvature invariants, where the well-known Ricci-flat Taub-NUT black hole remains a solution.","An analysis of the unstable Lichnerowicz modes in the Taub-NUT background enables us to identify the mass and NUT parameters (m,n) where new Taub-NUT black holes can emerge.","We then adopt numerical technique to construct these new Taub-NUT black holes that bifurcate away from the Ricci-flat ones.","Unlike the Ricci-flat Taub-NUT, there can exist two new black holes for an appropriate given temperature, making it a total of three if we include the Ricci-flat one."],"url":"http://arxiv.org/abs/2406.03692v1","category":"gr-qc"}
{"created":"2024-06-06 02:20:42","title":"Precise measurement of pion-bump structure using future MeV gamma-ray detectors","abstract":"The pion-bump structure in the gamma-ray spectrum is a direct proof for the hadronic origin of the gamma rays, and thus the decisive evidence for the acceleration of hadronic cosmic rays in astrophysical objects. However, the identification of such a spectral feature is limited by the resolution and energy coverage of current gamma-ray instruments. Furthermore, there are unavoidable bremsstrahlung emissions from secondary and primary electrons, which may dominate the gamma-ray emission below the pion-bump. Thus, the study of this gamma-ray emission component can provide unique information on the acceleration and confinement of high-energy particles. In this paper, we studied the predicted gamma-ray spectrum assuming both hadronic or leptonic origin in mid-aged supernova remnants W44, we discuss the detection potential of future MeV missions on these emissions and possible implications.","sentences":["The pion-bump structure in the gamma-ray spectrum is a direct proof for the hadronic origin of the gamma rays, and thus the decisive evidence for the acceleration of hadronic cosmic rays in astrophysical objects.","However, the identification of such a spectral feature is limited by the resolution and energy coverage of current gamma-ray instruments.","Furthermore, there are unavoidable bremsstrahlung emissions from secondary and primary electrons, which may dominate the gamma-ray emission below the pion-bump.","Thus, the study of this gamma-ray emission component can provide unique information on the acceleration and confinement of high-energy particles.","In this paper, we studied the predicted gamma-ray spectrum assuming both hadronic or leptonic origin in mid-aged supernova remnants W44, we discuss the detection potential of future MeV missions on these emissions and possible implications."],"url":"http://arxiv.org/abs/2406.03691v1","category":"astro-ph.HE"}
{"created":"2024-06-06 02:10:50","title":"BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning","abstract":"Generating novel active molecules for a given protein is an extremely challenging task for generative models that requires an understanding of the complex physical interactions between the molecule and its environment. In this paper, we present a novel generative model, BindGPT which uses a conceptually simple but powerful approach to create 3D molecules within the protein's binding site. Our model produces molecular graphs and conformations jointly, eliminating the need for an extra graph reconstruction step. We pretrain BindGPT on a large-scale dataset and fine-tune it with reinforcement learning using scores from external simulation software. We demonstrate how a single pretrained language model can serve at the same time as a 3D molecular generative model, conformer generator conditioned on the molecular graph, and a pocket-conditioned 3D molecule generator. Notably, the model does not make any representational equivariance assumptions about the domain of generation. We show how such simple conceptual approach combined with pretraining and scaling can perform on par or better than the current best specialized diffusion models, language models, and graph neural networks while being two orders of magnitude cheaper to sample.","sentences":["Generating novel active molecules for a given protein is an extremely challenging task for generative models that requires an understanding of the complex physical interactions between the molecule and its environment.","In this paper, we present a novel generative model, BindGPT which uses a conceptually simple but powerful approach to create 3D molecules within the protein's binding site.","Our model produces molecular graphs and conformations jointly, eliminating the need for an extra graph reconstruction step.","We pretrain BindGPT on a large-scale dataset and fine-tune it with reinforcement learning using scores from external simulation software.","We demonstrate how a single pretrained language model can serve at the same time as a 3D molecular generative model, conformer generator conditioned on the molecular graph, and a pocket-conditioned 3D molecule generator.","Notably, the model does not make any representational equivariance assumptions about the domain of generation.","We show how such simple conceptual approach combined with pretraining and scaling can perform on par or better than the current best specialized diffusion models, language models, and graph neural networks while being two orders of magnitude cheaper to sample."],"url":"http://arxiv.org/abs/2406.03686v1","category":"cs.LG"}
{"created":"2024-06-06 02:05:35","title":"Principles of Designing Robust Remote Face Anti-Spoofing Systems","abstract":"Protecting digital identities of human face from various attack vectors is paramount, and face anti-spoofing plays a crucial role in this endeavor. Current approaches primarily focus on detecting spoofing attempts within individual frames to detect presentation attacks. However, the emergence of hyper-realistic generative models capable of real-time operation has heightened the risk of digitally generated attacks. In light of these evolving threats, this paper aims to address two key aspects. First, it sheds light on the vulnerabilities of state-of-the-art face anti-spoofing methods against digital attacks. Second, it presents a comprehensive taxonomy of common threats encountered in face anti-spoofing systems. Through a series of experiments, we demonstrate the limitations of current face anti-spoofing detection techniques and their failure to generalize to novel digital attack scenarios. Notably, the existing models struggle with digital injection attacks including adversarial noise, realistic deepfake attacks, and digital replay attacks. To aid in the design and implementation of robust face anti-spoofing systems resilient to these emerging vulnerabilities, the paper proposes key design principles from model accuracy and robustness to pipeline robustness and even platform robustness. Especially, we suggest to implement the proactive face anti-spoofing system using active sensors to significant reduce the risks for unseen attack vectors and improve the user experience.","sentences":["Protecting digital identities of human face from various attack vectors is paramount, and face anti-spoofing plays a crucial role in this endeavor.","Current approaches primarily focus on detecting spoofing attempts within individual frames to detect presentation attacks.","However, the emergence of hyper-realistic generative models capable of real-time operation has heightened the risk of digitally generated attacks.","In light of these evolving threats, this paper aims to address two key aspects.","First, it sheds light on the vulnerabilities of state-of-the-art face anti-spoofing methods against digital attacks.","Second, it presents a comprehensive taxonomy of common threats encountered in face anti-spoofing systems.","Through a series of experiments, we demonstrate the limitations of current face anti-spoofing detection techniques and their failure to generalize to novel digital attack scenarios.","Notably, the existing models struggle with digital injection attacks including adversarial noise, realistic deepfake attacks, and digital replay attacks.","To aid in the design and implementation of robust face anti-spoofing systems resilient to these emerging vulnerabilities, the paper proposes key design principles from model accuracy and robustness to pipeline robustness and even platform robustness.","Especially, we suggest to implement the proactive face anti-spoofing system using active sensors to significant reduce the risks for unseen attack vectors and improve the user experience."],"url":"http://arxiv.org/abs/2406.03684v1","category":"cs.CV"}
{"created":"2024-06-06 01:52:28","title":"Bayesian Power Steering: An Effective Approach for Domain Adaptation of Diffusion Models","abstract":"We propose a Bayesian framework for fine-tuning large diffusion models with a novel network structure called Bayesian Power Steering (BPS). We clarify the meaning behind adaptation from a \\textit{large probability space} to a \\textit{small probability space} and explore the task of fine-tuning pre-trained models using learnable modules from a Bayesian perspective. BPS extracts task-specific knowledge from a pre-trained model's learned prior distribution. It efficiently leverages large diffusion models, differentially intervening different hidden features with a head-heavy and foot-light configuration. Experiments highlight the superiority of BPS over contemporary methods across a range of tasks even with limited amount of data. Notably, BPS attains an FID score of 10.49 under the sketch condition on the COCO17 dataset.","sentences":["We propose a Bayesian framework for fine-tuning large diffusion models with a novel network structure called Bayesian Power Steering (BPS).","We clarify the meaning behind adaptation from a \\textit{large probability space} to a \\textit{small probability space} and explore the task of fine-tuning pre-trained models using learnable modules from a Bayesian perspective.","BPS extracts task-specific knowledge from a pre-trained model's learned prior distribution.","It efficiently leverages large diffusion models, differentially intervening different hidden features with a head-heavy and foot-light configuration.","Experiments highlight the superiority of BPS over contemporary methods across a range of tasks even with limited amount of data.","Notably, BPS attains an FID score of 10.49 under the sketch condition on the COCO17 dataset."],"url":"http://arxiv.org/abs/2406.03683v1","category":"cs.LG"}
{"created":"2024-06-06 01:52:09","title":"A Universal Class of Sharpness-Aware Minimization Algorithms","abstract":"Recently, there has been a surge in interest in developing optimization algorithms for overparameterized models as achieving generalization is believed to require algorithms with suitable biases. This interest centers on minimizing sharpness of the original loss function; the Sharpness-Aware Minimization (SAM) algorithm has proven effective. However, most literature only considers a few sharpness measures, such as the maximum eigenvalue or trace of the training loss Hessian, which may not yield meaningful insights for non-convex optimization scenarios like neural networks. Additionally, many sharpness measures are sensitive to parameter invariances in neural networks, magnifying significantly under rescaling parameters. Motivated by these challenges, we introduce a new class of sharpness measures in this paper, leading to new sharpness-aware objective functions. We prove that these measures are \\textit{universally expressive}, allowing any function of the training loss Hessian matrix to be represented by appropriate hyperparameters. Furthermore, we show that the proposed objective functions explicitly bias towards minimizing their corresponding sharpness measures, and how they allow meaningful applications to models with parameter invariances (such as scale-invariances). Finally, as instances of our proposed general framework, we present \\textit{Frob-SAM} and \\textit{Det-SAM}, which are specifically designed to minimize the Frobenius norm and the determinant of the Hessian of the training loss, respectively. We also demonstrate the advantages of our general framework through extensive experiments.","sentences":["Recently, there has been a surge in interest in developing optimization algorithms for overparameterized models as achieving generalization is believed to require algorithms with suitable biases.","This interest centers on minimizing sharpness of the original loss function; the Sharpness-Aware Minimization (SAM) algorithm has proven effective.","However, most literature only considers a few sharpness measures, such as the maximum eigenvalue or trace of the training loss Hessian, which may not yield meaningful insights for non-convex optimization scenarios like neural networks.","Additionally, many sharpness measures are sensitive to parameter invariances in neural networks, magnifying significantly under rescaling parameters.","Motivated by these challenges, we introduce a new class of sharpness measures in this paper, leading to new sharpness-aware objective functions.","We prove that these measures are \\textit{universally expressive}, allowing any function of the training loss Hessian matrix to be represented by appropriate hyperparameters.","Furthermore, we show that the proposed objective functions explicitly bias towards minimizing their corresponding sharpness measures, and how they allow meaningful applications to models with parameter invariances (such as scale-invariances).","Finally, as instances of our proposed general framework, we present \\textit{Frob-SAM} and \\textit{Det-SAM}, which are specifically designed to minimize the Frobenius norm and the determinant of the Hessian of the training loss, respectively.","We also demonstrate the advantages of our general framework through extensive experiments."],"url":"http://arxiv.org/abs/2406.03682v1","category":"cs.LG"}
{"created":"2024-06-06 01:29:47","title":"Bidding in Uniform Price Auctions for Value Maximizing Buyers","abstract":"We study the problem of bidding in uniform price auctions widely used in practice. Although these auctions are non-truthful for bidders with quasilinear utility functions, several empirical findings suggest that the auction format induces truthful bidding from the bidders. We attribute this difference in theory and practice to the assumption of the behavioral model of the bidders. In this pursuit, we study uniform price auctions in a repeated setting from the perspective of a value-maximizing buyer who aims to maximize their acquired cumulative value across $T$ rounds, subject to per-round return-on-investment (RoI) constraints. For a RoI-constrained, value-maximizing buyer, we study a generalized version of the uniform bidding format, commonly used in practice, which we term as $m$-uniform bidding. To characterize the optimal $m$-uniform bid, we introduce and study the notion of universally feasible (UF) bidding policies, which are robust, meaning that RoI feasibility is obtained regardless of the competitors' bids. We show that the optimal class of UF bidding policies is essentially a generalization of truthful bidding policies, which depends only on the valuation curve of the bidder and target RoI. To measure the performance of UF bidding policies against the optimal bidding policy that is not necessarily UF, we introduce a metric called the Price of Universal Feasibility (PoUF) and establish that PoUF is at most 2, irrespective of $m$ and the upper bound is tight. We further compare the generalized $m$-uniform bidding interface against the classical uniform bidding format under which $m=1$, showing the total value under $m$-uniform bidding increases at most by a factor of $m$. Numerical simulations on semi-synthetic data demonstrate that UF bidding policies perform significantly better than the derived theoretical bounds.","sentences":["We study the problem of bidding in uniform price auctions widely used in practice.","Although these auctions are non-truthful for bidders with quasilinear utility functions, several empirical findings suggest that the auction format induces truthful bidding from the bidders.","We attribute this difference in theory and practice to the assumption of the behavioral model of the bidders.","In this pursuit, we study uniform price auctions in a repeated setting from the perspective of a value-maximizing buyer who aims to maximize their acquired cumulative value across $T$ rounds, subject to per-round return-on-investment (RoI) constraints.","For a RoI-constrained, value-maximizing buyer, we study a generalized version of the uniform bidding format, commonly used in practice, which we term as $m$-uniform bidding.","To characterize the optimal $m$-uniform bid, we introduce and study the notion of universally feasible (UF) bidding policies, which are robust, meaning that RoI feasibility is obtained regardless of the competitors' bids.","We show that the optimal class of UF bidding policies is essentially a generalization of truthful bidding policies, which depends only on the valuation curve of the bidder and target RoI. To measure the performance of UF bidding policies against the optimal bidding policy that is not necessarily UF, we introduce a metric called the Price of Universal Feasibility (PoUF) and establish that PoUF is at most 2, irrespective of $m$ and the upper bound is tight.","We further compare the generalized $m$-uniform bidding interface against the classical uniform bidding format under which $m=1$, showing the total value under $m$-uniform bidding increases at most by a factor of $m$. Numerical simulations on semi-synthetic data demonstrate that UF bidding policies perform significantly better than the derived theoretical bounds."],"url":"http://arxiv.org/abs/2406.03674v1","category":"cs.DS"}
{"created":"2024-06-06 01:11:55","title":"POAM: Probabilistic Online Attentive Mapping for Efficient Robotic Information Gathering","abstract":"Gaussian Process (GP) models are widely used for Robotic Information Gathering (RIG) in exploring unknown environments due to their ability to model complex phenomena with non-parametric flexibility and accurately quantify prediction uncertainty. Previous work has developed informative planners and adaptive GP models to enhance the data efficiency of RIG by improving the robot's sampling strategy to focus on informative regions in non-stationary environments. However, computational efficiency becomes a bottleneck when using GP models in large-scale environments with limited computational resources. We propose a framework -- Probabilistic Online Attentive Mapping (POAM) -- that leverages the modeling strengths of the non-stationary Attentive Kernel while achieving constant-time computational complexity for online decision-making. POAM guides the optimization process via variational Expectation Maximization, providing constant-time update rules for inducing inputs, variational parameters, and hyperparameters. Extensive experiments in active bathymetric mapping tasks demonstrate that POAM significantly improves computational efficiency, model accuracy, and uncertainty quantification capability compared to existing online sparse GP models.","sentences":["Gaussian Process (GP) models are widely used for Robotic Information Gathering (RIG) in exploring unknown environments due to their ability to model complex phenomena with non-parametric flexibility and accurately quantify prediction uncertainty.","Previous work has developed informative planners and adaptive GP models to enhance the data efficiency of RIG by improving the robot's sampling strategy to focus on informative regions in non-stationary environments.","However, computational efficiency becomes a bottleneck when using GP models in large-scale environments with limited computational resources.","We propose a framework -- Probabilistic Online Attentive Mapping (POAM) -- that leverages the modeling strengths of the non-stationary Attentive Kernel while achieving constant-time computational complexity for online decision-making.","POAM guides the optimization process via variational Expectation Maximization, providing constant-time update rules for inducing inputs, variational parameters, and hyperparameters.","Extensive experiments in active bathymetric mapping tasks demonstrate that POAM significantly improves computational efficiency, model accuracy, and uncertainty quantification capability compared to existing online sparse GP models."],"url":"http://arxiv.org/abs/2406.03669v1","category":"cs.RO"}
{"created":"2024-06-06 00:51:28","title":"What Makes Language Models Good-enough?","abstract":"Psycholinguistic research suggests that humans may build a representation of linguistic input that is 'good-enough' for the task at hand. This study examines what architectural features make language models learn human-like good-enough language processing. We focus on the number of layers and self-attention heads in Transformers. We create a good-enough language processing (GELP) evaluation dataset (7,680 examples), which is designed to test the effects of two plausibility types, eight construction types, and three degrees of memory cost on language processing. To annotate GELP, we first conduct a crowdsourcing experiment whose design follows prior psycholinguistic studies. Our model evaluation against the annotated GELP then reveals that the full model as well as models with fewer layers and/or self-attention heads exhibit a good-enough performance. This result suggests that models with shallower depth and fewer heads can learn good-enough language processing.","sentences":["Psycholinguistic research suggests that humans may build a representation of linguistic input that is 'good-enough' for the task at hand.","This study examines what architectural features make language models learn human-like good-enough language processing.","We focus on the number of layers and self-attention heads in Transformers.","We create a good-enough language processing (GELP) evaluation dataset (7,680 examples), which is designed to test the effects of two plausibility types, eight construction types, and three degrees of memory cost on language processing.","To annotate GELP, we first conduct a crowdsourcing experiment whose design follows prior psycholinguistic studies.","Our model evaluation against the annotated GELP then reveals that the full model as well as models with fewer layers and/or self-attention heads exhibit a good-enough performance.","This result suggests that models with shallower depth and fewer heads can learn good-enough language processing."],"url":"http://arxiv.org/abs/2406.03666v1","category":"cs.CL"}
{"created":"2024-06-05 23:38:49","title":"Collision Avoidance Maneuvers Optimization in the Presence of Multiple Encounters","abstract":"The optimization of fuel-optimal low-thrust collision avoidance maneuvers (CAMs) in scenarios involving multiple encounters between spacecraft is addressed. The optimization's objective is the minimization of the total fuel consumption while respecting constraints on the total probability of collision. The solution methodology combines sequential convex programming, second-order cone programming, and differential algebra to approximate the non-convex optimal control problem progressively. A Gaussian mixture model method is used to propagate the initial covariance matrix of the secondary spacecraft, allowing us to split it into multiple mixands that can be treated as different objects. This leads to an accurate propagation of the uncertainties. No theoretical guarantee is given for the convergence of the method to the global optimum of the original optimal control problem. Nonetheless, good performance is demonstrated through case studies involving multiple short- and long-term encounters, showcasing the generation of fuel-efficient CAMs while respecting operational constraints.","sentences":["The optimization of fuel-optimal low-thrust collision avoidance maneuvers (CAMs) in scenarios involving multiple encounters between spacecraft is addressed.","The optimization's objective is the minimization of the total fuel consumption while respecting constraints on the total probability of collision.","The solution methodology combines sequential convex programming, second-order cone programming, and differential algebra to approximate the non-convex optimal control problem progressively.","A Gaussian mixture model method is used to propagate the initial covariance matrix of the secondary spacecraft, allowing us to split it into multiple mixands that can be treated as different objects.","This leads to an accurate propagation of the uncertainties.","No theoretical guarantee is given for the convergence of the method to the global optimum of the original optimal control problem.","Nonetheless, good performance is demonstrated through case studies involving multiple short- and long-term encounters, showcasing the generation of fuel-efficient CAMs while respecting operational constraints."],"url":"http://arxiv.org/abs/2406.03654v1","category":"math.OC"}
{"created":"2024-06-05 22:49:41","title":"CLASSY X: Highlighting Differences Between Partial Covering and Semi-Analytic Modeling in the Estimate of Galactic Outflow Properties","abstract":"Feedback driven massive outflows play a crucial role in galaxy evolution by regulating star formation and influencing the dynamics of surrounding media. Extracting outflow properties from spectral lines is a notoriously difficult process for a number of reasons, including the possibility that a substantial fraction of the outflow is carried by dense gas in a very narrow range in velocity. This gas can hide in spectra with insufficient resolution. Empirically motivated analysis based on the Apparent Optical Depth method, commonly used in the literature, neglects the contribution of this gas, and may therefore underestimate the true gas column density. More complex semi-analytical line transfer (e.g., SALT) models, on the other hand, allow for the presence of this gas by modeling the radial density and velocity of the outflows as power laws. Here we compare the two approaches to quantify the uncertainties in the inferences of outflow properties based on 1-D \"down-the-barrel\" using the UV spectra of the CLASSY galaxy sample. We find that empirical modeling may significantly underestimate the column densities relative to SALT analysis, particularly in the optically thick regime. We use simulations to show that the main reason for this discrepancy is the presence of large amount of dense material at low velocities, which can be hidden by the finite spectral resolution of the data. The SALT models in turn could over-estimate the column densities if the assumed power laws of the density profiles strong are not a property of actual outflows.","sentences":["Feedback driven massive outflows play a crucial role in galaxy evolution by regulating star formation and influencing the dynamics of surrounding media.","Extracting outflow properties from spectral lines is a notoriously difficult process for a number of reasons, including the possibility that a substantial fraction of the outflow is carried by dense gas in a very narrow range in velocity.","This gas can hide in spectra with insufficient resolution.","Empirically motivated analysis based on the Apparent Optical Depth method, commonly used in the literature, neglects the contribution of this gas, and may therefore underestimate the true gas column density.","More complex semi-analytical line transfer (e.g., SALT) models, on the other hand, allow for the presence of this gas by modeling the radial density and velocity of the outflows as power laws.","Here we compare the two approaches to quantify the uncertainties in the inferences of outflow properties based on 1-D \"down-the-barrel\" using the UV spectra of the CLASSY galaxy sample.","We find that empirical modeling may significantly underestimate the column densities relative to SALT analysis, particularly in the optically thick regime.","We use simulations to show that the main reason for this discrepancy is the presence of large amount of dense material at low velocities, which can be hidden by the finite spectral resolution of the data.","The SALT models in turn could over-estimate the column densities if the assumed power laws of the density profiles strong are not a property of actual outflows."],"url":"http://arxiv.org/abs/2406.03646v1","category":"astro-ph.GA"}
{"created":"2024-06-05 22:35:17","title":"Is Free Self-Alignment Possible?","abstract":"Aligning pretrained language models (LMs) is a complex and resource-intensive process, often requiring access to large amounts of ground-truth preference data and substantial compute. Are these costs necessary? That is, it is possible to align using only inherent model knowledge and without additional training? We tackle this challenge with AlignEZ, a novel approach that uses (1) self-generated preference data and (2) representation editing to provide nearly cost-free alignment. During inference, AlignEZ modifies LM representations to reduce undesirable and boost desirable components using subspaces identified via self-generated preference pairs. Our experiments reveal that this nearly cost-free procedure significantly narrows the gap between base pretrained and tuned models by an average of 31.6%, observed across six datasets and three model architectures. Additionally, we explore the potential of using AlignEZ as a means of expediting more expensive alignment procedures. Our experiments show that AlignEZ improves DPO models tuned only using a small subset of ground-truth preference data. Lastly, we study the conditions under which improvement using AlignEZ is feasible, providing valuable insights into its effectiveness.","sentences":["Aligning pretrained language models (LMs) is a complex and resource-intensive process, often requiring access to large amounts of ground-truth preference data and substantial compute.","Are these costs necessary?","That is, it is possible to align using only inherent model knowledge and without additional training?","We tackle this challenge with AlignEZ, a novel approach that uses (1) self-generated preference data and (2) representation editing to provide nearly cost-free alignment.","During inference, AlignEZ modifies LM representations to reduce undesirable and boost desirable components using subspaces identified via self-generated preference pairs.","Our experiments reveal that this nearly cost-free procedure significantly narrows the gap between base pretrained and tuned models by an average of 31.6%, observed across six datasets and three model architectures.","Additionally, we explore the potential of using AlignEZ as a means of expediting more expensive alignment procedures.","Our experiments show that AlignEZ improves DPO models tuned only using a small subset of ground-truth preference data.","Lastly, we study the conditions under which improvement using AlignEZ is feasible, providing valuable insights into its effectiveness."],"url":"http://arxiv.org/abs/2406.03642v1","category":"cs.CL"}
{"created":"2024-06-05 20:48:57","title":"Generalized two-point visual control model of human steering for accurate state estimation","abstract":"We derive and validate a generalization of the two-point visual control model, an accepted cognitive science model for human steering behavior. The generalized model is needed as current steering models are either insufficiently accurate or too complex for online state estimation. We demonstrate that the generalized model replicates specific human steering behavior with high precision (85\\% reduction in modeling error) and integrate this model into a human-as-advisor framework where human steering inputs are used for state estimation. As a benchmark study, we use this framework to decipher ambiguous lane markings represented by biased lateral position measurements. We demonstrate that, with the generalized model, the state estimator can accurately estimate the true vehicle state, providing lateral state estimates with under 0.25 m error on average across participants. However, without the generalized model, the estimator cannot accurately estimate the vehicle's lateral state.","sentences":["We derive and validate a generalization of the two-point visual control model, an accepted cognitive science model for human steering behavior.","The generalized model is needed as current steering models are either insufficiently accurate or too complex for online state estimation.","We demonstrate that the generalized model replicates specific human steering behavior with high precision (85\\% reduction in modeling error) and integrate this model into a human-as-advisor framework where human steering inputs are used for state estimation.","As a benchmark study, we use this framework to decipher ambiguous lane markings represented by biased lateral position measurements.","We demonstrate that, with the generalized model, the state estimator can accurately estimate the true vehicle state, providing lateral state estimates with under 0.25 m error on average across participants.","However, without the generalized model, the estimator cannot accurately estimate the vehicle's lateral state."],"url":"http://arxiv.org/abs/2406.03622v1","category":"eess.SY"}
{"created":"2024-06-05 20:38:30","title":"Symmetry Discovery Beyond Affine Transformations","abstract":"Symmetry detection has been shown to improve various machine learning tasks. In the context of continuous symmetry detection, current state of the art experiments are limited to the detection of affine transformations. Under the manifold assumption, we outline a framework for discovering continuous symmetry in data beyond the affine transformation group. We also provide a similar framework for discovering discrete symmetry. We experimentally compare our method to an existing method known as LieGAN and show that our method is competitive at detecting affine symmetries for large sample sizes and superior than LieGAN for small sample sizes. We also show our method is able to detect continuous symmetries beyond the affine group and is generally more computationally efficient than LieGAN.","sentences":["Symmetry detection has been shown to improve various machine learning tasks.","In the context of continuous symmetry detection, current state of the art experiments are limited to the detection of affine transformations.","Under the manifold assumption, we outline a framework for discovering continuous symmetry in data beyond the affine transformation group.","We also provide a similar framework for discovering discrete symmetry.","We experimentally compare our method to an existing method known as LieGAN and show that our method is competitive at detecting affine symmetries for large sample sizes and superior than LieGAN for small sample sizes.","We also show our method is able to detect continuous symmetries beyond the affine group and is generally more computationally efficient than LieGAN."],"url":"http://arxiv.org/abs/2406.03619v1","category":"cs.LG"}
{"created":"2024-06-05 20:26:30","title":"Five-dimensional compatible systems and the Tate conjecture for elliptic surfaces","abstract":"Let $(\\rho_\\lambda\\colon G_{\\mathbb Q}\\to \\operatorname{GL}_5(\\overline{E}_\\lambda))_\\lambda$ be a strictly compatible system of Galois representations such that no Hodge--Tate weight has multiplicity $5$. We show that if $\\rho_{\\lambda_0}$ is irreducible for some $\\lambda_0$, then $\\rho_\\lambda$ is irreducible for all but finitely many $\\lambda$. More generally, if $(\\rho_\\lambda)_\\lambda$ is essentially self-dual, we show that either $\\rho_\\lambda$ is irreducible for all but finitely many $\\lambda$, or the compatible system $(\\rho_\\lambda)_\\lambda$ decomposes as a direct sum of lower-dimensional compatible systems.   We apply our results to study the Tate conjecture for elliptic surfaces. For example, if $X_0\\colon y^2 + (t+3)xy + y= x^3$, we prove the codimension one $\\ell$-adic Tate conjecture for all but finitely many $\\ell$, for all but finitely many general, degree $3$, genus $2$ branched multiplicative covers of $X_0$.   To prove this result, we classify the elliptic surfaces into six families, and prove, using perverse sheaf theory and a result of Cadoret--Tamagawa, that if one surface in a family satisfies the Tate conjecture, then all but finitely many do. We then verify the Tate conjecture for one representative of each family by making our irreducibility result explicit: for the compatible system arising from the transcendental part of $H^2_{\\mathrm{et}}(X_{\\overline{\\mathbb Q}}, \\mathbb{Q}_\\ell(1))$ for a representative $X$, we formulate an algorithm that takes as input the characteristic polynomials of Frobenius, and terminates if and only if the compatible system is irreducible.","sentences":["Let $(\\rho_\\lambda\\colon G_{\\mathbb Q}\\to \\operatorname{GL}_5(\\overline{E}_\\lambda))_\\lambda$ be a strictly compatible system of Galois representations such that no Hodge--Tate weight has multiplicity $5$. We show that if $\\rho_{\\lambda_0}$ is irreducible for some $\\lambda_0$, then $\\rho_\\lambda$ is irreducible for all but finitely many $\\lambda$. More generally, if $(\\rho_\\lambda)_\\lambda$ is essentially self-dual, we show that either $\\rho_\\lambda$ is irreducible for all but finitely many $\\lambda$, or the compatible system $(\\rho_\\lambda)_\\lambda$ decomposes as a direct sum of lower-dimensional compatible systems.   ","We apply our results to study the Tate conjecture for elliptic surfaces.","For example, if $X_0\\colon y^2 + (t+3)xy + y= x^3$, we prove the codimension one $\\ell$-adic Tate conjecture for all but finitely many $\\ell$, for all but finitely many general, degree $3$, genus $2$ branched multiplicative covers of $X_0$.   To prove this result, we classify the elliptic surfaces into six families, and prove, using perverse sheaf theory and a result of Cadoret--Tamagawa, that if one surface in a family satisfies the Tate conjecture, then all but finitely many do.","We then verify the Tate conjecture for one representative of each family by making our irreducibility result explicit: for the compatible system arising from the transcendental part of $H^2_{\\mathrm{et}}(X_{\\overline{\\mathbb Q}}, \\mathbb{Q}_\\ell(1))$ for a representative $X$, we formulate an algorithm that takes as input the characteristic polynomials of Frobenius, and terminates if and only if the compatible system is irreducible."],"url":"http://arxiv.org/abs/2406.03617v1","category":"math.NT"}
{"created":"2024-06-05 20:23:52","title":"BEACON: A Bayesian Optimization Strategy for Novelty Search in Expensive Black-Box Systems","abstract":"Novelty search (NS) refers to a class of exploration algorithms that automatically uncover diverse system behaviors through simulations or experiments. Systematically obtaining diverse outcomes is a key component in many real-world design problems such as material and drug discovery, neural architecture search, reinforcement learning, and robot navigation. Since the relationship between the inputs and outputs (i.e., behaviors) of these complex systems is typically not available in closed form, NS requires a black-box perspective. Consequently, popular NS algorithms rely on evolutionary optimization and other meta-heuristics that require intensive sampling of the input space, which is impractical when the system is expensive to evaluate. We propose a Bayesian optimization inspired algorithm for sample-efficient NS that is specifically designed for such expensive black-box systems. Our approach models the input-to-behavior mapping with multi-output Gaussian processes (MOGP) and selects the next point to evaluate by maximizing a novelty metric that depends on a posterior sample drawn from the MOGP that promotes both exploration and exploitation. By leveraging advances in efficient posterior sampling and high-dimensional Gaussian process modeling, we discuss how our approach can be made scalable with respect to both amount of data and number of inputs. We test our approach on ten synthetic benchmark problems and eight real-world problems (with up to 2133 inputs) including new applications such as discovery of diverse metal organic frameworks for use in clean energy technology. We show that our approach greatly outperforms existing NS algorithms by finding substantially larger sets of diverse behaviors under limited sample budgets.","sentences":["Novelty search (NS) refers to a class of exploration algorithms that automatically uncover diverse system behaviors through simulations or experiments.","Systematically obtaining diverse outcomes is a key component in many real-world design problems such as material and drug discovery, neural architecture search, reinforcement learning, and robot navigation.","Since the relationship between the inputs and outputs (i.e., behaviors) of these complex systems is typically not available in closed form, NS requires a black-box perspective.","Consequently, popular NS algorithms rely on evolutionary optimization and other meta-heuristics that require intensive sampling of the input space, which is impractical when the system is expensive to evaluate.","We propose a Bayesian optimization inspired algorithm for sample-efficient NS that is specifically designed for such expensive black-box systems.","Our approach models the input-to-behavior mapping with multi-output Gaussian processes (MOGP) and selects the next point to evaluate by maximizing a novelty metric that depends on a posterior sample drawn from the MOGP that promotes both exploration and exploitation.","By leveraging advances in efficient posterior sampling and high-dimensional Gaussian process modeling, we discuss how our approach can be made scalable with respect to both amount of data and number of inputs.","We test our approach on ten synthetic benchmark problems and eight real-world problems (with up to 2133 inputs) including new applications such as discovery of diverse metal organic frameworks for use in clean energy technology.","We show that our approach greatly outperforms existing NS algorithms by finding substantially larger sets of diverse behaviors under limited sample budgets."],"url":"http://arxiv.org/abs/2406.03616v1","category":"stat.ML"}
{"created":"2024-06-05 20:01:49","title":"Fantastyc: Blockchain-based Federated Learning Made Secure and Practical","abstract":"Federated Learning is a decentralized framework that enables multiple clients to collaboratively train a machine learning model under the orchestration of a central server without sharing their local data. The centrality of this framework represents a point of failure which is addressed in literature by blockchain-based federated learning approaches. While ensuring a fully-decentralized solution with traceability, such approaches still face several challenges about integrity, confidentiality and scalability to be practically deployed. In this paper, we propose Fantastyc, a solution designed to address these challenges that have been never met together in the state of the art.","sentences":["Federated Learning is a decentralized framework that enables multiple clients to collaboratively train a machine learning model under the orchestration of a central server without sharing their local data.","The centrality of this framework represents a point of failure which is addressed in literature by blockchain-based federated learning approaches.","While ensuring a fully-decentralized solution with traceability, such approaches still face several challenges about integrity, confidentiality and scalability to be practically deployed.","In this paper, we propose Fantastyc, a solution designed to address these challenges that have been never met together in the state of the art."],"url":"http://arxiv.org/abs/2406.03608v1","category":"cs.CR"}
{"created":"2024-06-05 19:57:23","title":"Neural force functional for non-equilibrium many-body colloidal systems","abstract":"We combine power functional theory and machine learning to study non-equilibrium overdamped many-body systems of colloidal particles at the level of one-body fields. We first sample in steady state the one-body fields relevant for the dynamics from computer simulations of Brownian particles under the influence of randomly generated external fields. A neural network is then trained with this data to represent locally in space the formally exact functional mapping from the one-body density and velocity profiles to the one-body internal force field. The trained network is used to analyse the non-equilibrium superadiabatic force field and the transport coefficients such as shear and bulk viscosities. Due to the local learning approach, the network can be applied to systems much larger than the original simulation box in which the one-body fields are sampled. Complemented with the exact non-equilibrium one-body force balance equation and a continuity equation, the network yields viable predictions of the dynamics in time-dependent situations. Even though training is based on steady states only, the predicted dynamics is in good agreement with simulation results. A neural dynamical density functional theory can be straightforwardly implemented as a limiting case in which the internal force field is that of an equilibrium system. The framework is general and directly applicable to other many-body systems of interacting particles following Brownian dynamics.","sentences":["We combine power functional theory and machine learning to study non-equilibrium overdamped many-body systems of colloidal particles at the level of one-body fields.","We first sample in steady state the one-body fields relevant for the dynamics from computer simulations of Brownian particles under the influence of randomly generated external fields.","A neural network is then trained with this data to represent locally in space the formally exact functional mapping from the one-body density and velocity profiles to the one-body internal force field.","The trained network is used to analyse the non-equilibrium superadiabatic force field and the transport coefficients such as shear and bulk viscosities.","Due to the local learning approach, the network can be applied to systems much larger than the original simulation box in which the one-body fields are sampled.","Complemented with the exact non-equilibrium one-body force balance equation and a continuity equation, the network yields viable predictions of the dynamics in time-dependent situations.","Even though training is based on steady states only, the predicted dynamics is in good agreement with simulation results.","A neural dynamical density functional theory can be straightforwardly implemented as a limiting case in which the internal force field is that of an equilibrium system.","The framework is general and directly applicable to other many-body systems of interacting particles following Brownian dynamics."],"url":"http://arxiv.org/abs/2406.03606v1","category":"cond-mat.soft"}
{"created":"2024-06-05 19:55:45","title":"Alignment Calibration: Machine Unlearning for Contrastive Learning under Auditing","abstract":"Machine unlearning provides viable solutions to revoke the effect of certain training data on pre-trained model parameters. Existing approaches provide unlearning recipes for classification and generative models. However, a category of important machine learning models, i.e., contrastive learning (CL) methods, is overlooked. In this paper, we fill this gap by first proposing the framework of Machine Unlearning for Contrastive learning (MUC) and adapting existing methods. Furthermore, we observe that several methods are mediocre unlearners and existing auditing tools may not be sufficient for data owners to validate the unlearning effects in contrastive learning. We thus propose a novel method called Alignment Calibration (AC) by explicitly considering the properties of contrastive learning and optimizing towards novel auditing metrics to easily verify unlearning. We empirically compare AC with baseline methods on SimCLR, MoCo and CLIP. We observe that AC addresses drawbacks of existing methods: (1) achieving state-of-the-art performance and approximating exact unlearning (retraining); (2) allowing data owners to clearly visualize the effect caused by unlearning through black-box auditing.","sentences":["Machine unlearning provides viable solutions to revoke the effect of certain training data on pre-trained model parameters.","Existing approaches provide unlearning recipes for classification and generative models.","However, a category of important machine learning models, i.e., contrastive learning (CL) methods, is overlooked.","In this paper, we fill this gap by first proposing the framework of Machine Unlearning for Contrastive learning (MUC) and adapting existing methods.","Furthermore, we observe that several methods are mediocre unlearners and existing auditing tools may not be sufficient for data owners to validate the unlearning effects in contrastive learning.","We thus propose a novel method called Alignment Calibration (AC) by explicitly considering the properties of contrastive learning and optimizing towards novel auditing metrics to easily verify unlearning.","We empirically compare AC with baseline methods on SimCLR, MoCo and CLIP.","We observe that AC addresses drawbacks of existing methods: (1) achieving state-of-the-art performance and approximating exact unlearning (retraining); (2) allowing data owners to clearly visualize the effect caused by unlearning through black-box auditing."],"url":"http://arxiv.org/abs/2406.03603v1","category":"cs.LG"}
{"created":"2024-06-05 19:20:34","title":"BVE + EKF: A viewpoint estimator for the estimation of the object's position in the 3D task space using Extended Kalman Filters","abstract":"RGB-D sensors face multiple challenges operating under open-field environments because of their sensitivity to external perturbations such as radiation or rain. Multiple works are approaching the challenge of perceiving the 3D position of objects using monocular cameras. However, most of these works focus mainly on deep learning-based solutions, which are complex, data-driven, and difficult to predict. So, we aim to approach the problem of predicting the 3D objects' position using a Gaussian viewpoint estimator named best viewpoint estimator (BVE) powered by an extended Kalman filter (EKF). The algorithm proved efficient on the tasks and reached a maximum average Euclidean error of about 32 mm. The experiments were deployed and evaluated in MATLAB using artificial Gaussian noise. Future work aims to implement the system in a robotic system.","sentences":["RGB-D sensors face multiple challenges operating under open-field environments because of their sensitivity to external perturbations such as radiation or rain.","Multiple works are approaching the challenge of perceiving the 3D position of objects using monocular cameras.","However, most of these works focus mainly on deep learning-based solutions, which are complex, data-driven, and difficult to predict.","So, we aim to approach the problem of predicting the 3D objects' position using a Gaussian viewpoint estimator named best viewpoint estimator (BVE) powered by an extended Kalman filter (EKF).","The algorithm proved efficient on the tasks and reached a maximum average Euclidean error of about 32 mm.","The experiments were deployed and evaluated in MATLAB using artificial Gaussian noise.","Future work aims to implement the system in a robotic system."],"url":"http://arxiv.org/abs/2406.03591v1","category":"cs.RO"}
{"created":"2024-06-05 18:39:28","title":"A Simple Learning-Augmented Algorithm for Online Packing with Concave Objectives","abstract":"Learning-augmented algorithms has been extensively studied recently in the computer-science community, due to the potential of using machine learning predictions in order to improve the performance of algorithms. Predictions are especially useful for online algorithms making irrevocable decisions without knowledge of the future. Such learning-augmented algorithms aim to overcome the limitations of classical online algorithms when the predictions are accurate, and still perform comparably when the predictions are inaccurate.   A common approach is to adapt existing online algorithms to the particular advice notion employed, which often involves understanding previous sophisticated algorithms and their analyses. However, ideally, one would simply use previous online solutions in a black-box fashion, without much loss in the approximation guarantees. Such clean solutions that avoid opening up black-boxes are often rare, and may be even missed the first time around. For example, Grigorescu et al. (NeurIPS 22) proposed a learning-augmented algorithms for online covering linear programs, but it later turned out that their results can be subsumed by a natural approach that switches between the advice and an online algorithm given as a black-box, as noted in their paper.   In this work, we introduce and analyze a simple learning-augmented algorithm for online packing problems with linear constraints and concave objectives. We exhibit several direct applications of our framework including online packing linear programming, knapsack, resource management benefit, throughput maximization, and network utility maximization. We further raise the problem of understanding necessary and sufficient conditions for when such simple black-box solutions may be optimal. We believe this is an important direction of research that would unify many ad-hoc approaches from the literature.","sentences":["Learning-augmented algorithms has been extensively studied recently in the computer-science community, due to the potential of using machine learning predictions in order to improve the performance of algorithms.","Predictions are especially useful for online algorithms making irrevocable decisions without knowledge of the future.","Such learning-augmented algorithms aim to overcome the limitations of classical online algorithms when the predictions are accurate, and still perform comparably when the predictions are inaccurate.   ","A common approach is to adapt existing online algorithms to the particular advice notion employed, which often involves understanding previous sophisticated algorithms and their analyses.","However, ideally, one would simply use previous online solutions in a black-box fashion, without much loss in the approximation guarantees.","Such clean solutions that avoid opening up black-boxes are often rare, and may be even missed the first time around.","For example, Grigorescu et al. (NeurIPS 22) proposed a learning-augmented algorithms for online covering linear programs, but it later turned out that their results can be subsumed by a natural approach that switches between the advice and an online algorithm given as a black-box, as noted in their paper.   ","In this work, we introduce and analyze a simple learning-augmented algorithm for online packing problems with linear constraints and concave objectives.","We exhibit several direct applications of our framework including online packing linear programming, knapsack, resource management benefit, throughput maximization, and network utility maximization.","We further raise the problem of understanding necessary and sufficient conditions for when such simple black-box solutions may be optimal.","We believe this is an important direction of research that would unify many ad-hoc approaches from the literature."],"url":"http://arxiv.org/abs/2406.03574v1","category":"cs.DS"}
{"created":"2024-06-05 18:08:52","title":"Superbubbles as Galactic PeVatrons: The Potential Role of Rapid Second-Order Fermi Acceleration","abstract":"The origin of Galactic cosmic rays is still a mystery, in particular the sources and acceleration mechanism for cosmic rays with energies up to or beyond a PeV. Recently LHAASO has and H.E.S.S have shown that two gamma-ray sources associated with superbubbles created by young massive stellar clusters are likely PeVatrons. This has renewed the interest in the cosmic-ray acceleration processes in superbubbles.   To study the possibility and conditions under which second-order Fermi acceleration can accelerate particles beyond PeV energies in superbubbles. An analytical equation is derived for the maximum energy a cosmic-ray particle can obtain as a function of acceleration duration. The maximum energy depends critically on the diffusion coefficient D and the Alfv\\'en velocity, $V_A$. The analytical solutions for the acceleration time scale shows that second-order Fermi acceleration can be just as efficient as diffusive shock acceleration, when comparable relevant velocities are used-i.e. the Alfv\\'en velocity or shock velocity. The probable values for the diffusion coefficient and Alv\\'en speed are studied for two likely PeVatron regions, HESS J1646-458 associated with Westerlund 1, and the Cygnus Cocoon, associated Cyg OB2.   It is shown that within a typical stellar cluster time scale of 1-5 Myr cosmic-rays can be accelerated to $> 10^{15}$ eV, provided that $V_{\\rm A} > 300$ km/s, and the diffusion coefficient is $D \\sim 10^{26}$ cm$^2$/s at 100 TeV. This suggests that second-order Fermi acceleration in superbubbles should be considered as a possible source of Galactic cosmic rays up to, or beyond a PeV.","sentences":["The origin of Galactic cosmic rays is still a mystery, in particular the sources and acceleration mechanism for cosmic rays with energies up to or beyond a PeV. Recently LHAASO has and H.E.S.S have shown that two gamma-ray sources associated with superbubbles created by young massive stellar clusters are likely PeVatrons.","This has renewed the interest in the cosmic-ray acceleration processes in superbubbles.   ","To study the possibility and conditions under which second-order Fermi acceleration can accelerate particles beyond PeV energies in superbubbles.","An analytical equation is derived for the maximum energy a cosmic-ray particle can obtain as a function of acceleration duration.","The maximum energy depends critically on the diffusion coefficient D and the Alfv\\'en velocity, $V_A$. The analytical solutions for the acceleration time scale shows that second-order Fermi acceleration can be just as efficient as diffusive shock acceleration, when comparable relevant velocities are used-i.e. the Alfv\\'en velocity or shock velocity.","The probable values for the diffusion coefficient and Alv\\'en speed are studied for two likely PeVatron regions, HESS J1646-458 associated with Westerlund 1, and the Cygnus Cocoon, associated Cyg OB2.   ","It is shown that within a typical stellar cluster time scale of 1-5 Myr cosmic-rays can be accelerated to $> 10^{15}$ eV, provided that $V_{\\rm A} > 300$ km/s, and the diffusion coefficient is $D \\sim 10^{26}$ cm$^2$/s at 100 TeV. This suggests that second-order Fermi acceleration in superbubbles should be considered as a possible source of Galactic cosmic rays up to, or beyond a PeV."],"url":"http://arxiv.org/abs/2406.03555v1","category":"astro-ph.HE"}
{"created":"2024-06-05 18:08:25","title":"Magnetic ground state and strain-mediated chiral-like atomic distortions behavior in two-dimensional rectangular spin lattice","abstract":"Due to the large perpendicular magnetic anisotropy originating from spin-orbit coupling, magnetoelastic coupling is generally reported in easy-plane magnets with rectangular lattice where the easy magnetization is coupled with the lattice direction, while the acquisition of a novel coupling, beyond the easy-plane ferromagnets, in two-dimensional (2D) materials remains unknown. Here, by employing the density functional theory calculations, we demonstrate this feasibility with the discovery of long-range ferromagnetic ordering and elastic strain-mediated chiral-like atomic distortions behavior in a newly tetragonal As-Fe-As trilayer (t-FeAs monolayer), which shows large perpendicular magnetic anisotropy, robust ferromagnetic ordering, and in-plane ferroelasticity. We firstly point out that obvious limits exist when using the four magnetic configurations to determine the magnetic ground state for a rectangular spin lattice even if more exchange interaction parameters are included. A four-state mapping analysis is carefully examined for t-FeAs, where the calculated Curie temperature, Tc, is 435 K, which is higher than most reported 2D magnets, and can be further tuned by appropriate strains. Intriguingly, the chiral-like atomic distortion behavior of the Fe sub-layer is scanning tunneling microscopy characterizable, which can switch the magnetization axis between the out-of-plane and in-plane direction. This unusual finding of ferroelastic manipulation of both the atomic displacement and spin properties makes t-FeAs a promising candidate for future spintronics and also provides the possibility for exploring unprecedented coupling physics.","sentences":["Due to the large perpendicular magnetic anisotropy originating from spin-orbit coupling, magnetoelastic coupling is generally reported in easy-plane magnets with rectangular lattice where the easy magnetization is coupled with the lattice direction, while the acquisition of a novel coupling, beyond the easy-plane ferromagnets, in two-dimensional (2D) materials remains unknown.","Here, by employing the density functional theory calculations, we demonstrate this feasibility with the discovery of long-range ferromagnetic ordering and elastic strain-mediated chiral-like atomic distortions behavior in a newly tetragonal As-Fe-As trilayer (t-FeAs monolayer), which shows large perpendicular magnetic anisotropy, robust ferromagnetic ordering, and in-plane ferroelasticity.","We firstly point out that obvious limits exist when using the four magnetic configurations to determine the magnetic ground state for a rectangular spin lattice even if more exchange interaction parameters are included.","A four-state mapping analysis is carefully examined for t-FeAs, where the calculated Curie temperature, Tc, is 435 K, which is higher than most reported 2D magnets, and can be further tuned by appropriate strains.","Intriguingly, the chiral-like atomic distortion behavior of the Fe sub-layer is scanning tunneling microscopy characterizable, which can switch the magnetization axis between the out-of-plane and in-plane direction.","This unusual finding of ferroelastic manipulation of both the atomic displacement and spin properties makes t-FeAs a promising candidate for future spintronics and also provides the possibility for exploring unprecedented coupling physics."],"url":"http://arxiv.org/abs/2406.03554v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-05 18:06:27","title":"The signatures of density fluctuations and mixing gas in circumgalactic absorption systems","abstract":"We investigate the prospects for detecting and constraining density and temperature inhomogeneities in the circumgalactic medium (CGM) using absorption measurements of metal ions. Distributions in the gas thermal properties could arise from turbulence, gas cooling from the hot phase, and mixing between the cool and hot phases. Focusing on these physically motivated models, we parameterize each with a single parameter for simplicity and provide empirical and theoretical estimates for reasonable parameter values. We then construct the probability distribution functions for each of these scenarios, calculate the effective ion fractions, and fit our models to the COS-Halos absorption measurements to infer the gas densities and metallicities. We find that the models we consider (i) produce similarly good fits to the observations with or without distributions in the gas thermal properties, and (ii) result in detectable changes in the column densities only at the boundaries of reasonable parameter values. We show that He II self-shielding can have a larger effect on the ion fractions than density and temperature fluctuations. As a result, uncertainties in cloud geometry and their spatial distribution, affecting the details of radiation transfer, may obscure the effect of inhomogeneities.","sentences":["We investigate the prospects for detecting and constraining density and temperature inhomogeneities in the circumgalactic medium (CGM) using absorption measurements of metal ions.","Distributions in the gas thermal properties could arise from turbulence, gas cooling from the hot phase, and mixing between the cool and hot phases.","Focusing on these physically motivated models, we parameterize each with a single parameter for simplicity and provide empirical and theoretical estimates for reasonable parameter values.","We then construct the probability distribution functions for each of these scenarios, calculate the effective ion fractions, and fit our models to the COS-Halos absorption measurements to infer the gas densities and metallicities.","We find that the models we consider (i) produce similarly good fits to the observations with or without distributions in the gas thermal properties, and (ii) result in detectable changes in the column densities only at the boundaries of reasonable parameter values.","We show that He II self-shielding can have a larger effect on the ion fractions than density and temperature fluctuations.","As a result, uncertainties in cloud geometry and their spatial distribution, affecting the details of radiation transfer, may obscure the effect of inhomogeneities."],"url":"http://arxiv.org/abs/2406.03553v1","category":"astro-ph.GA"}
{"created":"2024-06-05 18:01:13","title":"Dynamical phase transitions in two-dimensional Brownian Matter","abstract":"We investigate a system of two-dimensional interacting Brownian particles at finite density. In the continuum limit, we uncover a hidden symmetry under area-preserving diffeomorphism transformations. This symmetry leads to the conservation of local vorticity. By calculating the generating functional within the saddle-point plus Gaussian fluctuations approximation, we reveal the emergence of a $U(1)$ gauge symmetry. This emergent symmetry enables us to describe the dynamics of density fluctuations as a gauge theory. We solve the corresponding equations of motion for various local and non-local two-body potentials, demonstrating the presence of multiple dynamical regimes and associated dynamical phase transitions.","sentences":["We investigate a system of two-dimensional interacting Brownian particles at finite density.","In the continuum limit, we uncover a hidden symmetry under area-preserving diffeomorphism transformations.","This symmetry leads to the conservation of local vorticity.","By calculating the generating functional within the saddle-point plus Gaussian fluctuations approximation, we reveal the emergence of a $U(1)$ gauge symmetry.","This emergent symmetry enables us to describe the dynamics of density fluctuations as a gauge theory.","We solve the corresponding equations of motion for various local and non-local two-body potentials, demonstrating the presence of multiple dynamical regimes and associated dynamical phase transitions."],"url":"http://arxiv.org/abs/2406.03551v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-05 18:00:01","title":"Mixed-Dimensional Qudit State Preparation Using Edge-Weighted Decision Diagrams","abstract":"Quantum computers have the potential to solve important problems which are fundamentally intractable on a classical computer. The underlying physics of quantum computing platforms supports using multi-valued logic, which promises a boost in performance over the prevailing two-level logic. One key element to exploiting this potential is the capability to efficiently prepare quantum states for multi-valued, or qudit, systems. Due to the time sensitivity of quantum computers, the circuits to prepare the required states have to be as short as possible. In this paper, we investigate quantum state preparation with a focus on mixed-dimensional systems, where the individual qudits may have different dimensionalities. The proposed approach automatically realizes quantum circuits constructing a corresponding mixed-dimensional quantum state. To this end, decision diagrams are used as a compact representation of the quantum state to be realized. We further incorporate the ability to approximate the quantum state to enable a finely controlled trade-off between accuracy, memory complexity, and number of operations in the circuit. Empirical evaluations demonstrate the effectiveness of the proposed approach in facilitating fast and scalable quantum state preparation, with performance directly linked to the size of the decision diagram. The implementation is freely available as part of Munich Quantum Toolkit~(MQT), under the framework MQT Qudits at github.com/cda-tum/mqt-qudits.","sentences":["Quantum computers have the potential to solve important problems which are fundamentally intractable on a classical computer.","The underlying physics of quantum computing platforms supports using multi-valued logic, which promises a boost in performance over the prevailing two-level logic.","One key element to exploiting this potential is the capability to efficiently prepare quantum states for multi-valued, or qudit, systems.","Due to the time sensitivity of quantum computers, the circuits to prepare the required states have to be as short as possible.","In this paper, we investigate quantum state preparation with a focus on mixed-dimensional systems, where the individual qudits may have different dimensionalities.","The proposed approach automatically realizes quantum circuits constructing a corresponding mixed-dimensional quantum state.","To this end, decision diagrams are used as a compact representation of the quantum state to be realized.","We further incorporate the ability to approximate the quantum state to enable a finely controlled trade-off between accuracy, memory complexity, and number of operations in the circuit.","Empirical evaluations demonstrate the effectiveness of the proposed approach in facilitating fast and scalable quantum state preparation, with performance directly linked to the size of the decision diagram.","The implementation is freely available as part of Munich Quantum Toolkit~(MQT), under the framework MQT Qudits at github.com/cda-tum/mqt-qudits."],"url":"http://arxiv.org/abs/2406.03531v1","category":"quant-ph"}
{"created":"2024-06-05 18:00:01","title":"Atmospheric Response for MeV $\\mathrm\u03b3$ Rays Observed with Balloon-Borne Detectors","abstract":"The atmospheric response for MeV $\\gamma$ rays ($\\sim$ 0.1 $-$ 10 MeV) can be characterized in terms of two observed components. The first component is due to photons that reach the detector without scattering. The second component is due to photons that reach the detector after scattering one or more times. While the former can be determined in a straightforward manner, the latter is much more complex to quantify, as it requires tracking the transport of all source photons that are incident on Earth's atmosphere. The scattered component can cause a significant energy-dependent distortion in the measured spectrum, which is important to account for when making balloon-borne observations. In this work we simulate the full response for $\\gamma$-ray transport in the atmosphere. We find that the scattered component becomes increasingly more significant towards lower energies, and at 0.1 MeV it may increase the measured flux by as much as a factor of $\\sim2-4$, depending on the photon index and off-axis angle of the source. This is particularly important for diffuse sources, whereas the effect from scattering can be significantly reduced for point sources observed with an imaging telescope.","sentences":["The atmospheric response for MeV $\\gamma$ rays ($\\sim$ 0.1 $-$ 10 MeV) can be characterized in terms of two observed components.","The first component is due to photons that reach the detector without scattering.","The second component is due to photons that reach the detector after scattering one or more times.","While the former can be determined in a straightforward manner, the latter is much more complex to quantify, as it requires tracking the transport of all source photons that are incident on Earth's atmosphere.","The scattered component can cause a significant energy-dependent distortion in the measured spectrum, which is important to account for when making balloon-borne observations.","In this work we simulate the full response for $\\gamma$-ray transport in the atmosphere.","We find that the scattered component becomes increasingly more significant towards lower energies, and at 0.1 MeV it may increase the measured flux by as much as a factor of $\\sim2-4$, depending on the photon index and off-axis angle of the source.","This is particularly important for diffuse sources, whereas the effect from scattering can be significantly reduced for point sources observed with an imaging telescope."],"url":"http://arxiv.org/abs/2406.03534v1","category":"astro-ph.HE"}
{"created":"2024-06-06 17:58:56","title":"The Square Kilometer Array as a Cosmic Microwave Background Experiment","abstract":"Contemporary cosmic microwave background (CMB) experiments typically have observing bands covering the range 20 - 800 GHz. Certain science goals, including the detection of $\\mu$-type distortions to the CMB spectrum and the characterization of low-frequency foregrounds, benefit from extended low-frequency coverage, but the standard CMB detector technology is not trivially adaptable to radio wavelengths. We propose using the upcoming Square Kilometer Array (SKA) as a CMB experiment, exploiting the immense raw sensitivity of SKA, in particular in single-dish mode, to measure medium-to-large-angular-scale modes of the CMB at radio wavelengths. As a worked example, we forecast the power of SKA combined with the upcoming LiteBIRD CMB space mission to constrain primordial non-Gaussianity through measurements of the correlation between anisotropies in the CMB $\\mu$-distortion, temperature, and $E$-mode polarization fields. We find that adding SKA data significantly improves the constraints on $f_\\textrm{nl}$, even for spatially varying low-frequency foregrounds.","sentences":["Contemporary cosmic microwave background (CMB) experiments typically have observing bands covering the range 20 - 800 GHz.","Certain science goals, including the detection of $\\mu$-type distortions to the CMB spectrum and the characterization of low-frequency foregrounds, benefit from extended low-frequency coverage, but the standard CMB detector technology is not trivially adaptable to radio wavelengths.","We propose using the upcoming Square Kilometer Array (SKA) as a CMB experiment, exploiting the immense raw sensitivity of SKA, in particular in single-dish mode, to measure medium-to-large-angular-scale modes of the CMB at radio wavelengths.","As a worked example, we forecast the power of SKA combined with the upcoming LiteBIRD CMB space mission to constrain primordial non-Gaussianity through measurements of the correlation between anisotropies in the CMB $\\mu$-distortion, temperature, and $E$-mode polarization fields.","We find that adding SKA data significantly improves the constraints on $f_\\textrm{nl}$, even for spatially varying low-frequency foregrounds."],"url":"http://arxiv.org/abs/2406.04326v1","category":"astro-ph.CO"}
{"created":"2024-06-06 17:33:23","title":"SpectralZoom: Efficient Segmentation with an Adaptive Hyperspectral Camera","abstract":"Hyperspectral image segmentation is crucial for many fields such as agriculture, remote sensing, biomedical imaging, battlefield sensing and astronomy. However, the challenge of hyper and multi spectral imaging is its large data footprint. We propose both a novel camera design and a vision transformer-based (ViT) algorithm that alleviate both the captured data footprint and the computational load for hyperspectral segmentation. Our camera is able to adaptively sample image regions or patches at different resolutions, instead of capturing the entire hyperspectral cube at one high resolution. Our segmentation algorithm works in concert with the camera, applying ViT-based segmentation only to adaptively selected patches. We show results both in simulation and on a real hardware platform demonstrating both accurate segmentation results and reduced computational burden.","sentences":["Hyperspectral image segmentation is crucial for many fields such as agriculture, remote sensing, biomedical imaging, battlefield sensing and astronomy.","However, the challenge of hyper and multi spectral imaging is its large data footprint.","We propose both a novel camera design and a vision transformer-based (ViT) algorithm that alleviate both the captured data footprint and the computational load for hyperspectral segmentation.","Our camera is able to adaptively sample image regions or patches at different resolutions, instead of capturing the entire hyperspectral cube at one high resolution.","Our segmentation algorithm works in concert with the camera, applying ViT-based segmentation only to adaptively selected patches.","We show results both in simulation and on a real hardware platform demonstrating both accurate segmentation results and reduced computational burden."],"url":"http://arxiv.org/abs/2406.04287v1","category":"cs.CV"}
{"created":"2024-06-06 16:55:07","title":"Localized Gaussian Point Management","abstract":"Point management is a critical component in optimizing 3D Gaussian Splatting (3DGS) models, as the point initiation (e.g., via structure from motion) is distributionally inappropriate. Typically, the Adaptive Density Control (ADC) algorithm is applied, leveraging view-averaged gradient magnitude thresholding for point densification, opacity thresholding for pruning, and regular all-points opacity reset. However, we reveal that this strategy is limited in tackling intricate/special image regions (e.g., transparent) as it is unable to identify all the 3D zones that require point densification, and lacking an appropriate mechanism to handle the ill-conditioned points with negative impacts (occlusion due to false high opacity). To address these limitations, we propose a Localized Point Management (LPM) strategy, capable of identifying those error-contributing zones in the highest demand for both point addition and geometry calibration. Zone identification is achieved by leveraging the underlying multiview geometry constraints, with the guidance of image rendering errors. We apply point densification in the identified zone, whilst resetting the opacity of those points residing in front of these regions so that a new opportunity is created to correct ill-conditioned points. Serving as a versatile plugin, LPM can be seamlessly integrated into existing 3D Gaussian Splatting models. Experimental evaluation across both static 3D and dynamic 4D scenes validate the efficacy of our LPM strategy in boosting a variety of existing 3DGS models both quantitatively and qualitatively. Notably, LPM improves both vanilla 3DGS and SpaceTimeGS to achieve state-of-the-art rendering quality while retaining real-time speeds, outperforming on challenging datasets such as Tanks & Temples and the Neural 3D Video Dataset.","sentences":["Point management is a critical component in optimizing 3D Gaussian Splatting (3DGS) models, as the point initiation (e.g., via structure from motion) is distributionally inappropriate.","Typically, the Adaptive Density Control (ADC) algorithm is applied, leveraging view-averaged gradient magnitude thresholding for point densification, opacity thresholding for pruning, and regular all-points opacity reset.","However, we reveal that this strategy is limited in tackling intricate/special image regions (e.g., transparent) as it is unable to identify all the 3D zones that require point densification, and lacking an appropriate mechanism to handle the ill-conditioned points with negative impacts (occlusion due to false high opacity).","To address these limitations, we propose a Localized Point Management (LPM) strategy, capable of identifying those error-contributing zones in the highest demand for both point addition and geometry calibration.","Zone identification is achieved by leveraging the underlying multiview geometry constraints, with the guidance of image rendering errors.","We apply point densification in the identified zone, whilst resetting the opacity of those points residing in front of these regions so that a new opportunity is created to correct ill-conditioned points.","Serving as a versatile plugin, LPM can be seamlessly integrated into existing 3D Gaussian Splatting models.","Experimental evaluation across both static 3D and dynamic 4D scenes validate the efficacy of our LPM strategy in boosting a variety of existing 3DGS models both quantitatively and qualitatively.","Notably, LPM improves both vanilla 3DGS and SpaceTimeGS to achieve state-of-the-art rendering quality while retaining real-time speeds, outperforming on challenging datasets such as Tanks & Temples and the Neural 3D Video Dataset."],"url":"http://arxiv.org/abs/2406.04251v1","category":"cs.CV"}
{"created":"2024-06-06 16:04:30","title":"CDMamba: Remote Sensing Image Change Detection with Mamba","abstract":"Recently, the Mamba architecture based on state space models has demonstrated remarkable performance in a series of natural language processing tasks and has been rapidly applied to remote sensing change detection (CD) tasks. However, most methods enhance the global receptive field by directly modifying the scanning mode of Mamba, neglecting the crucial role that local information plays in dense prediction tasks (e.g., CD). In this article, we propose a model called CDMamba, which effectively combines global and local features for handling CD tasks. Specifically, the Scaled Residual ConvMamba (SRCM) block is proposed to utilize the ability of Mamba to extract global features and convolution to enhance the local details, to alleviate the issue that current Mamba-based methods lack detailed clues and are difficult to achieve fine detection in dense prediction tasks. Furthermore, considering the characteristics of bi-temporal feature interaction required for CD, the Adaptive Global Local Guided Fusion (AGLGF) block is proposed to dynamically facilitate the bi-temporal interaction guided by other temporal global/local features. Our intuition is that more discriminative change features can be acquired with the guidance of other temporal features. Extensive experiments on three datasets demonstrate that our proposed CDMamba outperforms the current state-of-the-art methods. Our code will be open-sourced at https://github.com/zmoka-zht/CDMamba.","sentences":["Recently, the Mamba architecture based on state space models has demonstrated remarkable performance in a series of natural language processing tasks and has been rapidly applied to remote sensing change detection (CD) tasks.","However, most methods enhance the global receptive field by directly modifying the scanning mode of Mamba, neglecting the crucial role that local information plays in dense prediction tasks (e.g., CD).","In this article, we propose a model called CDMamba, which effectively combines global and local features for handling CD tasks.","Specifically, the Scaled Residual ConvMamba (SRCM) block is proposed to utilize the ability of Mamba to extract global features and convolution to enhance the local details, to alleviate the issue that current Mamba-based methods lack detailed clues and are difficult to achieve fine detection in dense prediction tasks.","Furthermore, considering the characteristics of bi-temporal feature interaction required for CD, the Adaptive Global Local Guided Fusion (AGLGF) block is proposed to dynamically facilitate the bi-temporal interaction guided by other temporal global/local features.","Our intuition is that more discriminative change features can be acquired with the guidance of other temporal features.","Extensive experiments on three datasets demonstrate that our proposed CDMamba outperforms the current state-of-the-art methods.","Our code will be open-sourced at https://github.com/zmoka-zht/CDMamba."],"url":"http://arxiv.org/abs/2406.04207v1","category":"cs.CV"}
{"created":"2024-06-06 15:08:06","title":"Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance","abstract":"Stochastic gradient descent with momentum, also known as Stochastic Heavy Ball method (SHB), is one of the most popular algorithms for solving large-scale stochastic optimization problems in various machine learning tasks. In practical scenarios, tuning the step-size and momentum parameters of the method is a prohibitively expensive and time-consuming process. In this work, inspired by the recent advantages of stochastic Polyak step-size in the performance of stochastic gradient descent (SGD), we propose and explore new Polyak-type variants suitable for the update rule of the SHB method. In particular, using the Iterate Moving Average (IMA) viewpoint of SHB, we propose and analyze three novel step-size selections: MomSPS$_{\\max}$, MomDecSPS, and MomAdaSPS. For MomSPS$_{\\max}$, we provide convergence guarantees for SHB to a neighborhood of the solution for convex and smooth problems (without assuming interpolation). If interpolation is also satisfied, then using MomSPS$_{\\max}$, SHB converges to the true solution at a fast rate matching the deterministic HB. The other two variants, MomDecSPS and MomAdaSPS, are the first adaptive step-sizes for SHB that guarantee convergence to the exact minimizer without prior knowledge of the problem parameters and without assuming interpolation. The convergence analysis of SHB is tight and obtains the convergence guarantees of SGD with stochastic Polyak step-sizes as a special case. We supplement our analysis with experiments that validate the theory and demonstrate the effectiveness and robustness of the new algorithms.","sentences":["Stochastic gradient descent with momentum, also known as Stochastic Heavy Ball method (SHB), is one of the most popular algorithms for solving large-scale stochastic optimization problems in various machine learning tasks.","In practical scenarios, tuning the step-size and momentum parameters of the method is a prohibitively expensive and time-consuming process.","In this work, inspired by the recent advantages of stochastic Polyak step-size in the performance of stochastic gradient descent (SGD), we propose and explore new Polyak-type variants suitable for the update rule of the SHB method.","In particular, using the Iterate Moving Average (IMA) viewpoint of SHB, we propose and analyze three novel step-size selections: MomSPS$_{\\max}$, MomDecSPS, and MomAdaSPS.","For MomSPS$_{\\max}$, we provide convergence guarantees for SHB to a neighborhood of the solution for convex and smooth problems (without assuming interpolation).","If interpolation is also satisfied, then using MomSPS$_{\\max}$, SHB converges to the true solution at a fast rate matching the deterministic HB.","The other two variants, MomDecSPS and MomAdaSPS, are the first adaptive step-sizes for SHB that guarantee convergence to the exact minimizer without prior knowledge of the problem parameters and without assuming interpolation.","The convergence analysis of SHB is tight and obtains the convergence guarantees of SGD with stochastic Polyak step-sizes as a special case.","We supplement our analysis with experiments that validate the theory and demonstrate the effectiveness and robustness of the new algorithms."],"url":"http://arxiv.org/abs/2406.04142v1","category":"math.OC"}
{"created":"2024-06-06 13:44:44","title":"Federated TrustChain: Blockchain-Enhanced LLM Training and Unlearning","abstract":"The development of Large Language Models (LLMs) faces a significant challenge: the exhausting of publicly available fresh data. This is because training a LLM needs a large demanding of new data. Federated learning emerges as a promising solution, enabling collaborative model to contribute their private data to LLM global model. However, integrating federated learning with LLMs introduces new challenges, including the lack of transparency and the need for effective unlearning mechanisms. Transparency is essential to ensuring trust and fairness among participants, while accountability is crucial for deterring malicious behaviour and enabling corrective actions when necessary. To address these challenges, we propose a novel blockchain-based federated learning framework for LLMs that enhances transparency, accountability, and unlearning capabilities. Our framework leverages blockchain technology to create a tamper-proof record of each model's contributions and introduces an innovative unlearning function that seamlessly integrates with the federated learning mechanism. We investigate the impact of Low-Rank Adaptation (LoRA) hyperparameters on unlearning performance and integrate Hyperledger Fabric to ensure the security, transparency, and verifiability of the unlearning process. Through comprehensive experiments and analysis, we showcase the effectiveness of our proposed framework in achieving highly effective unlearning in LLMs trained using federated learning. Our findings highlight the feasibility of integrating blockchain technology into federated learning frameworks for LLMs.","sentences":["The development of Large Language Models (LLMs) faces a significant challenge: the exhausting of publicly available fresh data.","This is because training a LLM needs a large demanding of new data.","Federated learning emerges as a promising solution, enabling collaborative model to contribute their private data to LLM global model.","However, integrating federated learning with LLMs introduces new challenges, including the lack of transparency and the need for effective unlearning mechanisms.","Transparency is essential to ensuring trust and fairness among participants, while accountability is crucial for deterring malicious behaviour and enabling corrective actions when necessary.","To address these challenges, we propose a novel blockchain-based federated learning framework for LLMs that enhances transparency, accountability, and unlearning capabilities.","Our framework leverages blockchain technology to create a tamper-proof record of each model's contributions and introduces an innovative unlearning function that seamlessly integrates with the federated learning mechanism.","We investigate the impact of Low-Rank Adaptation (LoRA) hyperparameters on unlearning performance and integrate Hyperledger Fabric to ensure the security, transparency, and verifiability of the unlearning process.","Through comprehensive experiments and analysis, we showcase the effectiveness of our proposed framework in achieving highly effective unlearning in LLMs trained using federated learning.","Our findings highlight the feasibility of integrating blockchain technology into federated learning frameworks for LLMs."],"url":"http://arxiv.org/abs/2406.04076v1","category":"cs.CR"}
{"created":"2024-06-06 10:16:43","title":"Culturally Aware and Adapted NLP: A Taxonomy and a Survey of the State of the Art","abstract":"The surge of interest in culturally aware and adapted Natural Language Processing (NLP) has inspired much recent research. However, the lack of common understanding of the concept of \"culture\" has made it difficult to evaluate progress in this emerging area. Drawing on prior research in NLP and related fields, we propose an extensive taxonomy of elements of culture that can provide a systematic framework for analyzing and understanding research progress. Using the taxonomy, we survey existing resources and models for culturally aware and adapted NLP, providing an overview of the state of the art and the research gaps that still need to be filled.","sentences":["The surge of interest in culturally aware and adapted Natural Language Processing (NLP) has inspired much recent research.","However, the lack of common understanding of the concept of \"culture\" has made it difficult to evaluate progress in this emerging area.","Drawing on prior research in NLP and related fields, we propose an extensive taxonomy of elements of culture that can provide a systematic framework for analyzing and understanding research progress.","Using the taxonomy, we survey existing resources and models for culturally aware and adapted NLP, providing an overview of the state of the art and the research gaps that still need to be filled."],"url":"http://arxiv.org/abs/2406.03930v1","category":"cs.CL"}
{"created":"2024-06-06 09:42:58","title":"Noisy certification of continuous variables graph states","abstract":"Continuous variables (CV) offer a promising platform for the development of various applications, such as quantum communication, computing, and sensing, and CV graph states represent a family of powerful entangled resource states for all these areas. In many of these protocols, a crucial aspect is the certification of the quantum state subsequently used. While numerous protocols exist, most rely on assumptions unrealistic for physical continuous variable states, such as infinite precision in quadrature measurement or the use of states requiring infinite squeezing. In this work, we adapt existing protocols to deal with these unavoidable considerations, and use them to certify their application for different quantum information tasks. More specifically, we show how CV graph states can be efficiently verified and certified even in a noisy and imperfect setting. We then discuss how our findings impact the usability of states obtained after the protocol for different applications, including quantum teleportation, computing, and sensing.","sentences":["Continuous variables (CV) offer a promising platform for the development of various applications, such as quantum communication, computing, and sensing, and CV graph states represent a family of powerful entangled resource states for all these areas.","In many of these protocols, a crucial aspect is the certification of the quantum state subsequently used.","While numerous protocols exist, most rely on assumptions unrealistic for physical continuous variable states, such as infinite precision in quadrature measurement or the use of states requiring infinite squeezing.","In this work, we adapt existing protocols to deal with these unavoidable considerations, and use them to certify their application for different quantum information tasks.","More specifically, we show how CV graph states can be efficiently verified and certified even in a noisy and imperfect setting.","We then discuss how our findings impact the usability of states obtained after the protocol for different applications, including quantum teleportation, computing, and sensing."],"url":"http://arxiv.org/abs/2406.03908v1","category":"quant-ph"}
{"created":"2024-06-06 09:37:56","title":"C^2RV: Cross-Regional and Cross-View Learning for Sparse-View CBCT Reconstruction","abstract":"Cone beam computed tomography (CBCT) is an important imaging technology widely used in medical scenarios, such as diagnosis and preoperative planning. Using fewer projection views to reconstruct CT, also known as sparse-view reconstruction, can reduce ionizing radiation and further benefit interventional radiology. Compared with sparse-view reconstruction for traditional parallel/fan-beam CT, CBCT reconstruction is more challenging due to the increased dimensionality caused by the measurement process based on cone-shaped X-ray beams. As a 2D-to-3D reconstruction problem, although implicit neural representations have been introduced to enable efficient training, only local features are considered and different views are processed equally in previous works, resulting in spatial inconsistency and poor performance on complicated anatomies. To this end, we propose C^2RV by leveraging explicit multi-scale volumetric representations to enable cross-regional learning in the 3D space. Additionally, the scale-view cross-attention module is introduced to adaptively aggregate multi-scale and multi-view features. Extensive experiments demonstrate that our C^2RV achieves consistent and significant improvement over previous state-of-the-art methods on datasets with diverse anatomy.","sentences":["Cone beam computed tomography (CBCT) is an important imaging technology widely used in medical scenarios, such as diagnosis and preoperative planning.","Using fewer projection views to reconstruct CT, also known as sparse-view reconstruction, can reduce ionizing radiation and further benefit interventional radiology.","Compared with sparse-view reconstruction for traditional parallel/fan-beam CT, CBCT reconstruction is more challenging due to the increased dimensionality caused by the measurement process based on cone-shaped X-ray beams.","As a 2D-to-3D reconstruction problem, although implicit neural representations have been introduced to enable efficient training, only local features are considered and different views are processed equally in previous works, resulting in spatial inconsistency and poor performance on complicated anatomies.","To this end, we propose C^2RV by leveraging explicit multi-scale volumetric representations to enable cross-regional learning in the 3D space.","Additionally, the scale-view cross-attention module is introduced to adaptively aggregate multi-scale and multi-view features.","Extensive experiments demonstrate that our C^2RV achieves consistent and significant improvement over previous state-of-the-art methods on datasets with diverse anatomy."],"url":"http://arxiv.org/abs/2406.03902v1","category":"eess.IV"}
{"created":"2024-06-06 08:46:00","title":"From operculum and body tail movements to different coupling of physical activity and respiratory frequency in farmed gilthead sea bream and European sea bass. Insights on aquaculture biosensing","abstract":"The AEFishBIT tri-axial accelerometer was externally attached to the operculum to assess the divergent activity and respiratory patterns of two marine farmed fish, the gilthead sea bream (Sparus aurata) and European sea bass (Dicentrarchus labrax). Analysis of raw data from exercised fish highlighted the large amplitude of operculum aperture and body tail movements in European sea bass, which were overall more stable at low-medium exercise intensity levels. Cosinor analysis in free-swimming fish (on-board data processing) highlighted a pronounced daily rhythmicity of locomotor activity and respiratory frequency in both gilthead sea bream and European sea bass. Acrophases of activity and respiration were coupled in gilthead sea bream, acting feeding time (once daily at 11:00 h) as a main synchronizing factor. By contrast, locomotor activity and respiratory frequency were out of phase in European sea bass with activity acrophase on early morning and respiration acrophase on the afternoon. The daily range of activity and respiration variation was also higher in European sea bass, probably as part of the adaptation of this fish species to act as a fast swimming predator. In any case, lower locomotor activity and enhanced respiration were associated with larger body weight in both fish species. This agrees with the notion that selection for fast growth in farming conditions is accompanied by a lower activity profile, which may favor an efficient feed conversion for growth purposes. Therefore, the use of behavioral monitoring is becoming a reliable and large-scale promising tool for selecting more efficient farmed fish, allowing researchers and farmers to establish stricter criteria of welfare for more sustainable and ethical fish production.","sentences":["The AEFishBIT tri-axial accelerometer was externally attached to the operculum to assess the divergent activity and respiratory patterns of two marine farmed fish, the gilthead sea bream (Sparus aurata) and European sea bass (Dicentrarchus labrax).","Analysis of raw data from exercised fish highlighted the large amplitude of operculum aperture and body tail movements in European sea bass, which were overall more stable at low-medium exercise intensity levels.","Cosinor analysis in free-swimming fish (on-board data processing) highlighted a pronounced daily rhythmicity of locomotor activity and respiratory frequency in both gilthead sea bream and European sea bass.","Acrophases of activity and respiration were coupled in gilthead sea bream, acting feeding time (once daily at 11:00 h) as a main synchronizing factor.","By contrast, locomotor activity and respiratory frequency were out of phase in European sea bass with activity acrophase on early morning and respiration acrophase on the afternoon.","The daily range of activity and respiration variation was also higher in European sea bass, probably as part of the adaptation of this fish species to act as a fast swimming predator.","In any case, lower locomotor activity and enhanced respiration were associated with larger body weight in both fish species.","This agrees with the notion that selection for fast growth in farming conditions is accompanied by a lower activity profile, which may favor an efficient feed conversion for growth purposes.","Therefore, the use of behavioral monitoring is becoming a reliable and large-scale promising tool for selecting more efficient farmed fish, allowing researchers and farmers to establish stricter criteria of welfare for more sustainable and ethical fish production."],"url":"http://arxiv.org/abs/2406.03859v1","category":"cs.CV"}
{"created":"2024-06-06 03:46:59","title":"LLMEmbed: Rethinking Lightweight LLM's Genuine Function in Text Classification","abstract":"With the booming of Large Language Models (LLMs), prompt-learning has become a promising method mainly researched in various research areas. Recently, many attempts based on prompt-learning have been made to improve the performance of text classification. However, most of these methods are based on heuristic Chain-of-Thought (CoT), and tend to be more complex but less efficient. In this paper, we rethink the LLM-based text classification methodology, propose a simple and effective transfer learning strategy, namely LLMEmbed, to address this classical but challenging task. To illustrate, we first study how to properly extract and fuse the text embeddings via various lightweight LLMs at different network depths to improve their robustness and discrimination, then adapt such embeddings to train the classifier. We perform extensive experiments on publicly available datasets, and the results show that LLMEmbed achieves strong performance while enjoys low training overhead using lightweight LLM backbones compared to recent methods based on larger LLMs, i.e. GPT-3, and sophisticated prompt-based strategies. Our LLMEmbed achieves adequate accuracy on publicly available benchmarks without any fine-tuning while merely use 4% model parameters, 1.8% electricity consumption and 1.5% runtime compared to its counterparts. Code is available at: https://github.com/ChunLiu-cs/LLMEmbed-ACL2024.","sentences":["With the booming of Large Language Models (LLMs), prompt-learning has become a promising method mainly researched in various research areas.","Recently, many attempts based on prompt-learning have been made to improve the performance of text classification.","However, most of these methods are based on heuristic Chain-of-Thought (CoT), and tend to be more complex but less efficient.","In this paper, we rethink the LLM-based text classification methodology, propose a simple and effective transfer learning strategy, namely LLMEmbed, to address this classical but challenging task.","To illustrate, we first study how to properly extract and fuse the text embeddings via various lightweight LLMs at different network depths to improve their robustness and discrimination, then adapt such embeddings to train the classifier.","We perform extensive experiments on publicly available datasets, and the results show that LLMEmbed achieves strong performance while enjoys low training overhead using lightweight LLM backbones compared to recent methods based on larger LLMs, i.e. GPT-3, and sophisticated prompt-based strategies.","Our LLMEmbed achieves adequate accuracy on publicly available benchmarks without any fine-tuning while merely use 4% model parameters, 1.8% electricity consumption and 1.5% runtime compared to its counterparts.","Code is available at: https://github.com/ChunLiu-cs/LLMEmbed-ACL2024."],"url":"http://arxiv.org/abs/2406.03725v1","category":"cs.CL"}
{"created":"2024-06-06 03:17:44","title":"Retrieval Augmented Generation in Prompt-based Text-to-Speech Synthesis with Context-Aware Contrastive Language-Audio Pretraining","abstract":"Recent prompt-based text-to-speech (TTS) models can clone an unseen speaker using only a short speech prompt. They leverage a strong in-context ability to mimic the speech prompts, including speaker style, prosody, and emotion. Therefore, the selection of a speech prompt greatly influences the generated speech, akin to the importance of a prompt in large language models (LLMs). However, current prompt-based TTS models choose the speech prompt manually or simply at random. Hence, in this paper, we adapt retrieval augmented generation (RAG) from LLMs to prompt-based TTS. Unlike traditional RAG methods, we additionally consider contextual information during the retrieval process and present a Context-Aware Contrastive Language-Audio Pre-training (CA-CLAP) model to extract context-aware, style-related features. The objective and subjective evaluations demonstrate that our proposed RAG method outperforms baselines, and our CA-CLAP achieves better results than text-only retrieval methods.","sentences":["Recent prompt-based text-to-speech (TTS) models can clone an unseen speaker using only a short speech prompt.","They leverage a strong in-context ability to mimic the speech prompts, including speaker style, prosody, and emotion.","Therefore, the selection of a speech prompt greatly influences the generated speech, akin to the importance of a prompt in large language models (LLMs).","However, current prompt-based TTS models choose the speech prompt manually or simply at random.","Hence, in this paper, we adapt retrieval augmented generation (RAG) from LLMs to prompt-based TTS.","Unlike traditional RAG methods, we additionally consider contextual information during the retrieval process and present a Context-Aware Contrastive Language-Audio Pre-training (CA-CLAP) model to extract context-aware, style-related features.","The objective and subjective evaluations demonstrate that our proposed RAG method outperforms baselines, and our CA-CLAP achieves better results than text-only retrieval methods."],"url":"http://arxiv.org/abs/2406.03714v1","category":"cs.SD"}
{"created":"2024-06-06 03:17:00","title":"Gait-Adaptive Navigation and Human Searching in field with Cyborg Insect","abstract":"This study focuses on improving the ability of cyborg insects to navigate autonomously during search and rescue missions in outdoor environments. We propose an algorithm that leverages data from an IMU to calculate orientation and position based on the insect's walking gait. These computed factors serve as essential feedback channels across 3 phases of our exploration. Our method functions without relying on external systems. The results of our trials, carried out in both indoor (4.8 x 6.6 m^2) and outdoor (3.5 x 6.0 m^2) settings, show that the cyborg insect is capable of seeking a human without knowing the human's position. This exploration strategy would help to bring terrestrial cyborg insects closer to practical application in real-life search and rescue (SAR) missions.","sentences":["This study focuses on improving the ability of cyborg insects to navigate autonomously during search and rescue missions in outdoor environments.","We propose an algorithm that leverages data from an IMU to calculate orientation and position based on the insect's walking gait.","These computed factors serve as essential feedback channels across 3 phases of our exploration.","Our method functions without relying on external systems.","The results of our trials, carried out in both indoor (4.8 x 6.6 m^2) and outdoor (3.5 x 6.0 m^2) settings, show that the cyborg insect is capable of seeking a human without knowing the human's position.","This exploration strategy would help to bring terrestrial cyborg insects closer to practical application in real-life search and rescue (SAR) missions."],"url":"http://arxiv.org/abs/2406.03713v1","category":"cs.RO"}
{"created":"2024-06-06 03:06:45","title":"Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model","abstract":"Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.","sentences":["Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS.","They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt.","However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios.","In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements.","Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information.","Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity.","The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios."],"url":"http://arxiv.org/abs/2406.03706v1","category":"cs.SD"}
{"created":"2024-06-06 02:20:34","title":"AMPIC: Adaptive Model Predictive Ising Controller for large-scale urban traffic signals","abstract":"Realizing smooth traffic flow is important for achieving carbon neutrality. Adaptive traffic signal control, which considers traffic conditions, has thus attracted attention. However, it is difficult to ensure optimal vehicle flow throughout a large city using existing control methods because of their heavy computational load. Here, we propose a control method called AMPIC (Adaptive Model Predictive Ising Controller) that guarantees both scalability and optimality. The proposed method employs model predictive control to solve an optimal control problem at each control interval with explicit consideration of a predictive model of vehicle flow. This optimal control problem is transformed into a combinatorial optimization problem with binary variables that is equivalent to the so-called Ising problem. This transformation allows us to use an Ising solver, which has been widely studied and is expected to have fast and efficient optimization performance. We performed numerical experiments using a microscopic traffic simulator for a realistic city road network. The results show that AMPIC enables faster vehicle cruising speed with less waiting time than that achieved by classical control methods, resulting in lower CO2 emissions. The model predictive approach with a long prediction horizon thus effectively improves control performance. Systematic parametric studies on model cities indicate that the proposed method realizes smoother traffic flows for large city road networks. Among Ising solvers, D-Wave's quantum annealing is shown to find near-optimal solutions at a reasonable computational cost.","sentences":["Realizing smooth traffic flow is important for achieving carbon neutrality.","Adaptive traffic signal control, which considers traffic conditions, has thus attracted attention.","However, it is difficult to ensure optimal vehicle flow throughout a large city using existing control methods because of their heavy computational load.","Here, we propose a control method called AMPIC (Adaptive Model Predictive Ising Controller) that guarantees both scalability and optimality.","The proposed method employs model predictive control to solve an optimal control problem at each control interval with explicit consideration of a predictive model of vehicle flow.","This optimal control problem is transformed into a combinatorial optimization problem with binary variables that is equivalent to the so-called Ising problem.","This transformation allows us to use an Ising solver, which has been widely studied and is expected to have fast and efficient optimization performance.","We performed numerical experiments using a microscopic traffic simulator for a realistic city road network.","The results show that AMPIC enables faster vehicle cruising speed with less waiting time than that achieved by classical control methods, resulting in lower CO2 emissions.","The model predictive approach with a long prediction horizon thus effectively improves control performance.","Systematic parametric studies on model cities indicate that the proposed method realizes smoother traffic flows for large city road networks.","Among Ising solvers, D-Wave's quantum annealing is shown to find near-optimal solutions at a reasonable computational cost."],"url":"http://arxiv.org/abs/2406.03690v1","category":"math.OC"}
{"created":"2024-06-06 01:50:01","title":"Meta-learning for Positive-unlabeled Classification","abstract":"We propose a meta-learning method for positive and unlabeled (PU) classification, which improves the performance of binary classifiers obtained from only PU data in unseen target tasks. PU learning is an important problem since PU data naturally arise in real-world applications such as outlier detection and information retrieval. Existing PU learning methods require many PU data, but sufficient data are often unavailable in practice. The proposed method minimizes the test classification risk after the model is adapted to PU data by using related tasks that consist of positive, negative, and unlabeled data. We formulate the adaptation as an estimation problem of the Bayes optimal classifier, which is an optimal classifier to minimize the classification risk. The proposed method embeds each instance into a task-specific space using neural networks. With the embedded PU data, the Bayes optimal classifier is estimated through density-ratio estimation of PU densities, whose solution is obtained as a closed-form solution. The closed-form solution enables us to efficiently and effectively minimize the test classification risk. We empirically show that the proposed method outperforms existing methods with one synthetic and three real-world datasets.","sentences":["We propose a meta-learning method for positive and unlabeled (PU) classification, which improves the performance of binary classifiers obtained from only PU data in unseen target tasks.","PU learning is an important problem since PU data naturally arise in real-world applications such as outlier detection and information retrieval.","Existing PU learning methods require many PU data, but sufficient data are often unavailable in practice.","The proposed method minimizes the test classification risk after the model is adapted to PU data by using related tasks that consist of positive, negative, and unlabeled data.","We formulate the adaptation as an estimation problem of the Bayes optimal classifier, which is an optimal classifier to minimize the classification risk.","The proposed method embeds each instance into a task-specific space using neural networks.","With the embedded PU data, the Bayes optimal classifier is estimated through density-ratio estimation of PU densities, whose solution is obtained as a closed-form solution.","The closed-form solution enables us to efficiently and effectively minimize the test classification risk.","We empirically show that the proposed method outperforms existing methods with one synthetic and three real-world datasets."],"url":"http://arxiv.org/abs/2406.03680v1","category":"cs.LG"}
{"created":"2024-06-06 00:22:46","title":"Refactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models","abstract":"Pythonic idioms are highly valued and widely used in the Python programming community. However, many Python users find it challenging to use Pythonic idioms. Adopting a rule-based approach or LLM-only approach is not sufficient to overcome three persistent challenges of code idiomatization including code miss, wrong detection and wrong refactoring. Motivated by the determinism of rules and adaptability of LLMs, we propose a hybrid approach consisting of three modules. We not only write prompts to instruct LLMs to complete tasks, but we also invoke Analytic Rule Interfaces (ARIs) to accomplish tasks. The ARIs are Python code generated by prompting LLMs to generate code. We first construct a knowledge module with three elements including ASTscenario, ASTcomponent and Condition, and prompt LLMs to generate Python code for incorporation into an ARI library for subsequent use. After that, for any syntax-error-free Python code, we invoke ARIs from the ARI library to extract ASTcomponent from the ASTscenario, and then filter out ASTcomponent that does not meet the condition. Finally, we design prompts to instruct LLMs to abstract and idiomatize code, and then invoke ARIs from the ARI library to rewrite non-idiomatic code into the idiomatic code. Next, we conduct a comprehensive evaluation of our approach, RIdiom, and Prompt-LLM on nine established Pythonic idioms in RIdiom. Our approach exhibits superior accuracy, F1-score, and recall, while maintaining precision levels comparable to RIdiom, all of which consistently exceed or come close to 90% for each metric of each idiom. Lastly, we extend our evaluation to encompass four new Pythonic idioms. Our approach consistently outperforms Prompt-LLM, achieving metrics with values consistently exceeding 90% for accuracy, F1-score, precision, and recall.","sentences":["Pythonic idioms are highly valued and widely used in the Python programming community.","However, many Python users find it challenging to use Pythonic idioms.","Adopting a rule-based approach or LLM-only approach is not sufficient to overcome three persistent challenges of code idiomatization including code miss, wrong detection and wrong refactoring.","Motivated by the determinism of rules and adaptability of LLMs, we propose a hybrid approach consisting of three modules.","We not only write prompts to instruct LLMs to complete tasks, but we also invoke Analytic Rule Interfaces (ARIs) to accomplish tasks.","The ARIs are Python code generated by prompting LLMs to generate code.","We first construct a knowledge module with three elements including ASTscenario, ASTcomponent and Condition, and prompt LLMs to generate Python code for incorporation into an ARI library for subsequent use.","After that, for any syntax-error-free Python code, we invoke ARIs from the ARI library to extract ASTcomponent from the ASTscenario, and then filter out ASTcomponent that does not meet the condition.","Finally, we design prompts to instruct LLMs to abstract and idiomatize code, and then invoke ARIs from the ARI library to rewrite non-idiomatic code into the idiomatic code.","Next, we conduct a comprehensive evaluation of our approach, RIdiom, and Prompt-LLM on nine established Pythonic idioms in RIdiom.","Our approach exhibits superior accuracy, F1-score, and recall, while maintaining precision levels comparable to RIdiom, all of which consistently exceed or come close to 90% for each metric of each idiom.","Lastly, we extend our evaluation to encompass four new Pythonic idioms.","Our approach consistently outperforms Prompt-LLM, achieving metrics with values consistently exceeding 90% for accuracy, F1-score, precision, and recall."],"url":"http://arxiv.org/abs/2406.03660v1","category":"cs.SE"}
{"created":"2024-06-05 18:00:00","title":"Dissipative realization of Kondo models","abstract":"We propose a dissipative implementation of a variety of Kondo models by means of strong two-body losses localized on a few impurity sites of a fermionic lattice--a setup which is suited to experiments with ultracold atomic gases. We study in detail the simplest scenario of just one dissipated site, showing that it is effectively described by the Anderson impurity model with infinite repulsion, perturbed by a small residual dissipation. We compute a number of signatures of the Kondo effect in transport across the impurity, finding a competition between the Kondo resonance and the residual dissipation. Our dissipative setup can be generalized to two or more sites subject to losses--realizing an impurity with spin 1 or higher--and more reservoirs, opening up the possibility of simulating several kinds of Kondo-like models with ultracold atoms.","sentences":["We propose a dissipative implementation of a variety of Kondo models by means of strong two-body losses localized on a few impurity sites of a fermionic lattice--a setup which is suited to experiments with ultracold atomic gases.","We study in detail the simplest scenario of just one dissipated site, showing that it is effectively described by the Anderson impurity model with infinite repulsion, perturbed by a small residual dissipation.","We compute a number of signatures of the Kondo effect in transport across the impurity, finding a competition between the Kondo resonance and the residual dissipation.","Our dissipative setup can be generalized to two or more sites subject to losses--realizing an impurity with spin 1 or higher--and more reservoirs, opening up the possibility of simulating several kinds of Kondo-like models with ultracold atoms."],"url":"http://arxiv.org/abs/2406.03527v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-06 17:59:41","title":"On the Expressive Power of Spectral Invariant Graph Neural Networks","abstract":"Incorporating spectral information to enhance Graph Neural Networks (GNNs) has shown promising results but raises a fundamental challenge due to the inherent ambiguity of eigenvectors. Various architectures have been proposed to address this ambiguity, referred to as spectral invariant architectures. Notable examples include GNNs and Graph Transformers that use spectral distances, spectral projection matrices, or other invariant spectral features. However, the potential expressive power of these spectral invariant architectures remains largely unclear. The goal of this work is to gain a deep theoretical understanding of the expressive power obtainable when using spectral features. We first introduce a unified message-passing framework for designing spectral invariant GNNs, called Eigenspace Projection GNN (EPNN). A comprehensive analysis shows that EPNN essentially unifies all prior spectral invariant architectures, in that they are either strictly less expressive or equivalent to EPNN. A fine-grained expressiveness hierarchy among different architectures is also established. On the other hand, we prove that EPNN itself is bounded by a recently proposed class of Subgraph GNNs, implying that all these spectral invariant architectures are strictly less expressive than 3-WL. Finally, we discuss whether using spectral features can gain additional expressiveness when combined with more expressive GNNs.","sentences":["Incorporating spectral information to enhance Graph Neural Networks (GNNs) has shown promising results but raises a fundamental challenge due to the inherent ambiguity of eigenvectors.","Various architectures have been proposed to address this ambiguity, referred to as spectral invariant architectures.","Notable examples include GNNs and Graph Transformers that use spectral distances, spectral projection matrices, or other invariant spectral features.","However, the potential expressive power of these spectral invariant architectures remains largely unclear.","The goal of this work is to gain a deep theoretical understanding of the expressive power obtainable when using spectral features.","We first introduce a unified message-passing framework for designing spectral invariant GNNs, called Eigenspace Projection GNN (EPNN).","A comprehensive analysis shows that EPNN essentially unifies all prior spectral invariant architectures, in that they are either strictly less expressive or equivalent to EPNN.","A fine-grained expressiveness hierarchy among different architectures is also established.","On the other hand, we prove that EPNN itself is bounded by a recently proposed class of Subgraph GNNs, implying that all these spectral invariant architectures are strictly less expressive than 3-WL.","Finally, we discuss whether using spectral features can gain additional expressiveness when combined with more expressive GNNs."],"url":"http://arxiv.org/abs/2406.04336v1","category":"cs.LG"}
{"created":"2024-06-06 17:55:34","title":"ReFiNe: Recursive Field Networks for Cross-modal Multi-scene Representation","abstract":"The common trade-offs of state-of-the-art methods for multi-shape representation (a single model \"packing\" multiple objects) involve trading modeling accuracy against memory and storage. We show how to encode multiple shapes represented as continuous neural fields with a higher degree of precision than previously possible and with low memory usage. Key to our approach is a recursive hierarchical formulation that exploits object self-similarity, leading to a highly compressed and efficient shape latent space. Thanks to the recursive formulation, our method supports spatial and global-to-local latent feature fusion without needing to initialize and maintain auxiliary data structures, while still allowing for continuous field queries to enable applications such as raytracing. In experiments on a set of diverse datasets, we provide compelling qualitative results and demonstrate state-of-the-art multi-scene reconstruction and compression results with a single network per dataset.","sentences":["The common trade-offs of state-of-the-art methods for multi-shape representation (a single model \"packing\" multiple objects) involve trading modeling accuracy against memory and storage.","We show how to encode multiple shapes represented as continuous neural fields with a higher degree of precision than previously possible and with low memory usage.","Key to our approach is a recursive hierarchical formulation that exploits object self-similarity, leading to a highly compressed and efficient shape latent space.","Thanks to the recursive formulation, our method supports spatial and global-to-local latent feature fusion without needing to initialize and maintain auxiliary data structures, while still allowing for continuous field queries to enable applications such as raytracing.","In experiments on a set of diverse datasets, we provide compelling qualitative results and demonstrate state-of-the-art multi-scene reconstruction and compression results with a single network per dataset."],"url":"http://arxiv.org/abs/2406.04309v1","category":"cs.CV"}
{"created":"2024-06-06 17:38:38","title":"On the Legendrian realisation of parametric families of knots","abstract":"We study the natural inclusion of the space of Legendrian embeddings in $(\\mathbb{S}^3,\\xi_{\\operatorname{std}})$ into the space of smooth embeddings from a homotopical viewpoint.   T. K\\'alm\\'an posed in [Kal] the open question of whether for every fixed knot type $\\mathcal{K}$ and Legendrian representative $\\mathcal{L}$, the homomorphism $\\pi_1(\\mathcal{L})\\to\\pi_1(\\mathcal{K})$ is surjective. We positively answer this question for infinitely many knot types $\\mathcal{K}$ in the three main families (hyperbolic, torus and satellites) and every stabilised Legendrian representative in $(\\mathbb{S}^3,\\xi_{\\operatorname{std}})$.   We then show that for every $n\\geq 3$, the homomorphisms $\\pi_n(\\mathcal{L})\\to\\pi_n(\\mathcal{K})$ and $\\pi_n(\\mathcal{FL})\\to\\pi_n(\\mathcal{K})$ are never surjective for any knot type $\\mathcal K$, Legendrian representative $\\mathcal L$ or formal Legendrian representative $\\mathcal{FL}$. This shows the existence of rigidity at every higher-homotopy level beyond $\\pi_3$. For completeness, we also show that surjectivity at the $\\pi_2$-level depends on the smooth knot type.","sentences":["We study the natural inclusion of the space of Legendrian embeddings in $(\\mathbb{S}^3,\\xi_{\\operatorname{std}})$ into the space of smooth embeddings from a homotopical viewpoint.   ","T. K\\'alm\\'an posed in [Kal] the open question of whether for every fixed knot type $\\mathcal{K}$ and Legendrian representative $\\mathcal{L}$, the homomorphism $\\pi_1(\\mathcal{L})\\to\\pi_1(\\mathcal{K})$ is surjective.","We positively answer this question for infinitely many knot types $\\mathcal{K}$ in the three main families (hyperbolic, torus and satellites) and every stabilised Legendrian representative in $(\\mathbb{S}^3,\\xi_{\\operatorname{std}})$.   We then show that for every $n\\geq 3$, the homomorphisms $\\pi_n(\\mathcal{L})\\to\\pi_n(\\mathcal{K})$ and $\\pi_n(\\mathcal{FL})\\to\\pi_n(\\mathcal{K})$ are never surjective for any knot type $\\mathcal K$, Legendrian representative $\\mathcal L$ or formal Legendrian representative $\\mathcal{FL}$. This shows the existence of rigidity at every higher-homotopy level beyond $\\pi_3$. For completeness, we also show that surjectivity at the $\\pi_2$-level depends on the smooth knot type."],"url":"http://arxiv.org/abs/2406.04293v1","category":"math.GT"}
{"created":"2024-06-06 17:28:14","title":"Systolic inequalities and the Horowitz-Myers conjecture","abstract":"Let $3 \\leq n \\leq 7$, and let $g$ be a Riemannian metric on $B^2 \\times T^{n-2}$ with scalar curvature at least $-n(n-1)$. We establish an inequality relating the systole of the boundary to the infimum of the mean curvature on the boundary. As a consequence, we confirm a conjecture of Horowitz and Myers.","sentences":["Let $3 \\leq n \\leq 7$, and let $g$ be a Riemannian metric on $B^2 \\times T^{n-2}$ with scalar curvature at least $-n(n-1)$. We establish an inequality relating the systole of the boundary to the infimum of the mean curvature on the boundary.","As a consequence, we confirm a conjecture of Horowitz and Myers."],"url":"http://arxiv.org/abs/2406.04283v1","category":"math.DG"}
{"created":"2024-06-06 17:14:44","title":"Transformers need glasses! Information over-squashing in language tasks","abstract":"We study how information propagates in decoder-only Transformers, which are the architectural backbone of most existing frontier large language models (LLMs). We rely on a theoretical signal propagation analysis -- specifically, we analyse the representations of the last token in the final layer of the Transformer, as this is the representation used for next-token prediction. Our analysis reveals a representational collapse phenomenon: we prove that certain distinct sequences of inputs to the Transformer can yield arbitrarily close representations in the final token. This effect is exacerbated by the low-precision floating-point formats frequently used in modern LLMs. As a result, the model is provably unable to respond to these sequences in different ways -- leading to errors in, e.g., tasks involving counting or copying. Further, we show that decoder-only Transformer language models can lose sensitivity to specific tokens in the input, which relates to the well-known phenomenon of over-squashing in graph neural networks. We provide empirical evidence supporting our claims on contemporary LLMs. Our theory also points to simple solutions towards ameliorating these issues.","sentences":["We study how information propagates in decoder-only Transformers, which are the architectural backbone of most existing frontier large language models (LLMs).","We rely on a theoretical signal propagation analysis -- specifically, we analyse the representations of the last token in the final layer of the Transformer, as this is the representation used for next-token prediction.","Our analysis reveals a representational collapse phenomenon: we prove that certain distinct sequences of inputs to the Transformer can yield arbitrarily close representations in the final token.","This effect is exacerbated by the low-precision floating-point formats frequently used in modern LLMs.","As a result, the model is provably unable to respond to these sequences in different ways -- leading to errors in, e.g., tasks involving counting or copying.","Further, we show that decoder-only Transformer language models can lose sensitivity to specific tokens in the input, which relates to the well-known phenomenon of over-squashing in graph neural networks.","We provide empirical evidence supporting our claims on contemporary LLMs.","Our theory also points to simple solutions towards ameliorating these issues."],"url":"http://arxiv.org/abs/2406.04267v1","category":"cs.CL"}
{"created":"2024-06-06 17:05:09","title":"Simulating, Fast and Slow: Learning Policies for Black-Box Optimization","abstract":"In recent years, solving optimization problems involving black-box simulators has become a point of focus for the machine learning community due to their ubiquity in science and engineering. The simulators describe a forward process $f_{\\mathrm{sim}}: (\\psi, x) \\rightarrow y$ from simulation parameters $\\psi$ and input data $x$ to observations $y$, and the goal of the optimization problem is to find parameters $\\psi$ that minimize a desired loss function. Sophisticated optimization algorithms typically require gradient information regarding the forward process, $f_{\\mathrm{sim}}$, with respect to the parameters $\\psi$. However, obtaining gradients from black-box simulators can often be prohibitively expensive or, in some cases, impossible. Furthermore, in many applications, practitioners aim to solve a set of related problems. Thus, starting the optimization ``ab initio\", i.e. from scratch, each time might be inefficient if the forward model is expensive to evaluate. To address those challenges, this paper introduces a novel method for solving classes of similar black-box optimization problems by learning an active learning policy that guides a differentiable surrogate's training and uses the surrogate's gradients to optimize the simulation parameters with gradient descent. After training the policy, downstream optimization of problems involving black-box simulators requires up to $\\sim$90\\% fewer expensive simulator calls compared to baselines such as local surrogate-based approaches, numerical optimization, and Bayesian methods.","sentences":["In recent years, solving optimization problems involving black-box simulators has become a point of focus for the machine learning community due to their ubiquity in science and engineering.","The simulators describe a forward process $f_{\\mathrm{sim}}: (\\psi, x) \\rightarrow y$ from simulation parameters $\\psi$ and input data $x$ to observations $y$, and the goal of the optimization problem is to find parameters $\\psi$ that minimize a desired loss function.","Sophisticated optimization algorithms typically require gradient information regarding the forward process, $f_{\\mathrm{sim}}$, with respect to the parameters $\\psi$. However, obtaining gradients from black-box simulators can often be prohibitively expensive or, in some cases, impossible.","Furthermore, in many applications, practitioners aim to solve a set of related problems.","Thus, starting the optimization ``ab initio\", i.e. from scratch, each time might be inefficient if the forward model is expensive to evaluate.","To address those challenges, this paper introduces a novel method for solving classes of similar black-box optimization problems by learning an active learning policy that guides a differentiable surrogate's training and uses the surrogate's gradients to optimize the simulation parameters with gradient descent.","After training the policy, downstream optimization of problems involving black-box simulators requires up to $\\sim$90\\% fewer expensive simulator calls compared to baselines such as local surrogate-based approaches, numerical optimization, and Bayesian methods."],"url":"http://arxiv.org/abs/2406.04261v1","category":"cs.LG"}
{"created":"2024-06-06 16:36:17","title":"On the equational theory of finite modular lattices","abstract":"It is shown that the equational theory of modular lattices is undecidable","sentences":["It is shown that the equational theory of modular lattices is undecidable"],"url":"http://arxiv.org/abs/2406.04237v1","category":"math.LO"}
{"created":"2024-06-06 15:27:58","title":"Equivariant Connections and their applications to Yang-Mills equations","abstract":"We reduce Yang-Mills equations for $SO^+(p,q)$, $Spin^+(p,q)$ and $SU(n)$ bundles, with constant and isotropic metrics, by developing the concept of $SO^+(p,q)$-equivariance. This allows us to model the electroweak interaction and $SO^+(p,q)$ bundles with a non-linear second order differential equation as well as the weak and strong interaction with a non-linear wave equation.","sentences":["We reduce Yang-Mills equations for $SO^+(p,q)$, $Spin^+(p,q)$ and $SU(n)$ bundles, with constant and isotropic metrics, by developing the concept of $SO^+(p,q)$-equivariance.","This allows us to model the electroweak interaction and $SO^+(p,q)$ bundles with a non-linear second order differential equation as well as the weak and strong interaction with a non-linear wave equation."],"url":"http://arxiv.org/abs/2406.04171v1","category":"math-ph"}
{"created":"2024-06-06 15:22:56","title":"Photometric Classification of Stars Around the Milky Way's Central Black Hole: I. Central Parsec","abstract":"The presence of young massive stars in the Galactic Centre (GC) raises questions about star formation near the black hole Sagittarius A* (Sgr A*). Additionally, the initial mass function (IMF) in this region appears different from the standard Salpeter/Kroupa law. Extreme extinction and crowding limit our understanding of the stellar population, with spectroscopic data available only for selected bright sources. We aim to improve knowledge about the distribution and IMF of young, massive stars near Sgr A*. Using intermediate band (IB) photometry, we identify candidates for massive young stars through Bayesian inference, a neural network, and a gradient-boosted trees algorithm. We obtained spectral energy distributions for 6590 stars, 1181 of which have been previously classified spectroscopically. We identify 351 stars classified as early types by all three methods, including 155 newly identified candidates. The radial density profiles for late and early-type stars fit broken power laws, with a break radius of 9.2 +- 0.6'' for early-type stars. Late-type stars show a core-like distribution around Sgr A*, while early-type stars' density increases steeply towards the black hole. We infer a top-heavy IMF of young stars near Sgr A* (R < 9''), with a power-law of 1.6 +- 0.1. At greater distances, a standard Salpeter/Kroupa IMF fits the data. IB photometry also constrains the metallicities of late-type stars, estimating metallicities for over 600 stars. The IMF variation with radial distance suggests different star formation mechanisms, with a top-heavy IMF near Sgr A* consistent with disc star formation.","sentences":["The presence of young massive stars in the Galactic Centre (GC) raises questions about star formation near the black hole Sagittarius A* (Sgr A*).","Additionally, the initial mass function (IMF) in this region appears different from the standard Salpeter/Kroupa law.","Extreme extinction and crowding limit our understanding of the stellar population, with spectroscopic data available only for selected bright sources.","We aim to improve knowledge about the distribution and IMF of young, massive stars near Sgr A*.","Using intermediate band (IB) photometry, we identify candidates for massive young stars through Bayesian inference, a neural network, and a gradient-boosted trees algorithm.","We obtained spectral energy distributions for 6590 stars, 1181 of which have been previously classified spectroscopically.","We identify 351 stars classified as early types by all three methods, including 155 newly identified candidates.","The radial density profiles for late and early-type stars fit broken power laws, with a break radius of 9.2 +- 0.6'' for early-type stars.","Late-type stars show a core-like distribution around Sgr A*, while early-type stars' density increases steeply towards the black hole.","We infer a top-heavy IMF of young stars near Sgr A* (R < 9''), with a power-law of 1.6 +- 0.1.","At greater distances, a standard Salpeter/Kroupa IMF fits the data.","IB photometry also constrains the metallicities of late-type stars, estimating metallicities for over 600 stars.","The IMF variation with radial distance suggests different star formation mechanisms, with a top-heavy IMF near Sgr A* consistent with disc star formation."],"url":"http://arxiv.org/abs/2406.04166v1","category":"astro-ph.GA"}
{"created":"2024-06-06 14:47:45","title":"Particle creation using the classical stochastic method","abstract":"We compute the particle creation of a harmonic oscillator using the classical stochastic method. This method reproduces all the vacuum expectation values in quantum theory. We prepare the vacuum state at the initial time and evolve it over time using Langevin equations of motion. By averaging over the ensemble, we compute the energy of the state at the final time and determine the amount of particles created. We verify that the particle creation agrees with predictions from quantum theory.","sentences":["We compute the particle creation of a harmonic oscillator using the classical stochastic method.","This method reproduces all the vacuum expectation values in quantum theory.","We prepare the vacuum state at the initial time and evolve it over time using Langevin equations of motion.","By averaging over the ensemble, we compute the energy of the state at the final time and determine the amount of particles created.","We verify that the particle creation agrees with predictions from quantum theory."],"url":"http://arxiv.org/abs/2406.04125v1","category":"hep-th"}
{"created":"2024-06-06 14:28:43","title":"UrbanSARFloods: Sentinel-1 SLC-Based Benchmark Dataset for Urban and Open-Area Flood Mapping","abstract":"Due to its cloud-penetrating capability and independence from solar illumination, satellite Synthetic Aperture Radar (SAR) is the preferred data source for large-scale flood mapping, providing global coverage and including various land cover classes. However, most studies on large-scale SAR-derived flood mapping using deep learning algorithms have primarily focused on flooded open areas, utilizing available open-access datasets (e.g., Sen1Floods11) and with limited attention to urban floods. To address this gap, we introduce \\textbf{UrbanSARFloods}, a floodwater dataset featuring pre-processed Sentinel-1 intensity data and interferometric coherence imagery acquired before and during flood events. It contains 8,879 $512\\times 512$ chips covering 807,500 $km^2$ across 20 land cover classes and 5 continents, spanning 18 flood events. We used UrbanSARFloods to benchmark existing state-of-the-art convolutional neural networks (CNNs) for segmenting open and urban flood areas. Our findings indicate that prevalent approaches, including the Weighted Cross-Entropy (WCE) loss and the application of transfer learning with pretrained models, fall short in overcoming the obstacles posed by imbalanced data and the constraints of a small training dataset. Urban flood detection remains challenging. Future research should explore strategies for addressing imbalanced data challenges and investigate transfer learning's potential for SAR-based large-scale flood mapping. Besides, expanding this dataset to include additional flood events holds promise for enhancing its utility and contributing to advancements in flood mapping techniques.","sentences":["Due to its cloud-penetrating capability and independence from solar illumination, satellite Synthetic Aperture Radar (SAR) is the preferred data source for large-scale flood mapping, providing global coverage and including various land cover classes.","However, most studies on large-scale SAR-derived flood mapping using deep learning algorithms have primarily focused on flooded open areas, utilizing available open-access datasets (e.g., Sen1Floods11) and with limited attention to urban floods.","To address this gap, we introduce \\textbf{UrbanSARFloods}, a floodwater dataset featuring pre-processed Sentinel-1 intensity data and interferometric coherence imagery acquired before and during flood events.","It contains 8,879 $512\\times 512$ chips covering 807,500 $km^2$ across 20 land cover classes and 5 continents, spanning 18 flood events.","We used UrbanSARFloods to benchmark existing state-of-the-art convolutional neural networks (CNNs) for segmenting open and urban flood areas.","Our findings indicate that prevalent approaches, including the Weighted Cross-Entropy (WCE) loss and the application of transfer learning with pretrained models, fall short in overcoming the obstacles posed by imbalanced data and the constraints of a small training dataset.","Urban flood detection remains challenging.","Future research should explore strategies for addressing imbalanced data challenges and investigate transfer learning's potential for SAR-based large-scale flood mapping.","Besides, expanding this dataset to include additional flood events holds promise for enhancing its utility and contributing to advancements in flood mapping techniques."],"url":"http://arxiv.org/abs/2406.04111v1","category":"cs.CV"}
{"created":"2024-06-06 14:16:03","title":"How Far Can We Compress Instant-NGP-Based NeRF?","abstract":"In recent years, Neural Radiance Field (NeRF) has demonstrated remarkable capabilities in representing 3D scenes. To expedite the rendering process, learnable explicit representations have been introduced for combination with implicit NeRF representation, which however results in a large storage space requirement. In this paper, we introduce the Context-based NeRF Compression (CNC) framework, which leverages highly efficient context models to provide a storage-friendly NeRF representation. Specifically, we excavate both level-wise and dimension-wise context dependencies to enable probability prediction for information entropy reduction. Additionally, we exploit hash collision and occupancy grids as strong prior knowledge for better context modeling. To the best of our knowledge, we are the first to construct and exploit context models for NeRF compression. We achieve a size reduction of 100$\\times$ and 70$\\times$ with improved fidelity against the baseline Instant-NGP on Synthesic-NeRF and Tanks and Temples datasets, respectively. Additionally, we attain 86.7\\% and 82.3\\% storage size reduction against the SOTA NeRF compression method BiRF. Our code is available here: https://github.com/YihangChen-ee/CNC.","sentences":["In recent years, Neural Radiance Field (NeRF) has demonstrated remarkable capabilities in representing 3D scenes.","To expedite the rendering process, learnable explicit representations have been introduced for combination with implicit NeRF representation, which however results in a large storage space requirement.","In this paper, we introduce the Context-based NeRF Compression (CNC) framework, which leverages highly efficient context models to provide a storage-friendly NeRF representation.","Specifically, we excavate both level-wise and dimension-wise context dependencies to enable probability prediction for information entropy reduction.","Additionally, we exploit hash collision and occupancy grids as strong prior knowledge for better context modeling.","To the best of our knowledge, we are the first to construct and exploit context models for NeRF compression.","We achieve a size reduction of 100$\\times$ and 70$\\times$ with improved fidelity against the baseline Instant-NGP on Synthesic-NeRF and Tanks and Temples datasets, respectively.","Additionally, we attain 86.7\\% and 82.3\\% storage size reduction against the SOTA NeRF compression method BiRF.","Our code is available here: https://github.com/YihangChen-ee/CNC."],"url":"http://arxiv.org/abs/2406.04101v1","category":"cs.CV"}
{"created":"2024-06-06 14:11:09","title":"Data-driven Explainable Controller for Soft Robots based on Recurrent Neural Networks","abstract":"The nonlinearity and hysteresis of soft robot motions have posed challenges in accurate soft robot control. Neural networks, especially recurrent neural networks (RNNs), have been widely leveraged for this issue due to their nonlinear activation functions and recurrent structures. Although they have shown satisfying accuracy in most tasks, these black-box approaches are not explainable, and hence, they are unsuitable for areas with high safety requirements, like robot-assisted surgery. Based on the RNN controllers, we propose a data-driven explainable controller (DDEC) whose parameters can be updated online. We discuss the Jacobian controller and kinematics controller in theory and demonstrate that they are only special cases of DDEC. Moreover, we utilize RNN, the Jacobian controller, the kinematics controller, and DDECs for trajectory following tasks. Experimental results have shown that our approach outperforms the other controllers considering trajectory following errors while being explainable. We also conduct a study to explore and explain the functions of each DDEC component. This is the first interpretable soft robot controller that overcomes the shortcomings of both NN controllers and interpretable controllers. Future work may involve proposing different DDECs based on different RNN controllers and exploiting them for high-safety-required applications.","sentences":["The nonlinearity and hysteresis of soft robot motions have posed challenges in accurate soft robot control.","Neural networks, especially recurrent neural networks (RNNs), have been widely leveraged for this issue due to their nonlinear activation functions and recurrent structures.","Although they have shown satisfying accuracy in most tasks, these black-box approaches are not explainable, and hence, they are unsuitable for areas with high safety requirements, like robot-assisted surgery.","Based on the RNN controllers, we propose a data-driven explainable controller (DDEC) whose parameters can be updated online.","We discuss the Jacobian controller and kinematics controller in theory and demonstrate that they are only special cases of DDEC.","Moreover, we utilize RNN, the Jacobian controller, the kinematics controller, and DDECs for trajectory following tasks.","Experimental results have shown that our approach outperforms the other controllers considering trajectory following errors while being explainable.","We also conduct a study to explore and explain the functions of each DDEC component.","This is the first interpretable soft robot controller that overcomes the shortcomings of both NN controllers and interpretable controllers.","Future work may involve proposing different DDECs based on different RNN controllers and exploiting them for high-safety-required applications."],"url":"http://arxiv.org/abs/2406.04094v1","category":"cs.RO"}
{"created":"2024-06-06 14:01:28","title":"Interpretable Lightweight Transformer via Unrolling of Learned Graph Smoothness Priors","abstract":"We build interpretable and lightweight transformer-like neural networks by unrolling iterative optimization algorithms that minimize graph smoothness priors -- the quadratic graph Laplacian regularizer (GLR) and the $\\ell_1$-norm graph total variation (GTV) -- subject to an interpolation constraint. The crucial insight is that a normalized signal-dependent graph learning module amounts to a variant of the basic self-attention mechanism in conventional transformers. Unlike \"black-box\" transformers that require learning of large key, query and value matrices to compute scaled dot products as affinities and subsequent output embeddings, resulting in huge parameter sets, our unrolled networks employ shallow CNNs to learn low-dimensional features per node to establish pairwise Mahalanobis distances and construct sparse similarity graphs. At each layer, given a learned graph, the target interpolated signal is simply a low-pass filtered output derived from the minimization of an assumed graph smoothness prior, leading to a dramatic reduction in parameter count. Experiments for two image interpolation applications verify the restoration performance, parameter efficiency and robustness to covariate shift of our graph-based unrolled networks compared to conventional transformers.","sentences":["We build interpretable and lightweight transformer-like neural networks by unrolling iterative optimization algorithms that minimize graph smoothness priors -- the quadratic graph Laplacian regularizer (GLR) and the $\\ell_1$-norm graph total variation (GTV) -- subject to an interpolation constraint.","The crucial insight is that a normalized signal-dependent graph learning module amounts to a variant of the basic self-attention mechanism in conventional transformers.","Unlike \"black-box\" transformers that require learning of large key, query and value matrices to compute scaled dot products as affinities and subsequent output embeddings, resulting in huge parameter sets, our unrolled networks employ shallow CNNs to learn low-dimensional features per node to establish pairwise Mahalanobis distances and construct sparse similarity graphs.","At each layer, given a learned graph, the target interpolated signal is simply a low-pass filtered output derived from the minimization of an assumed graph smoothness prior, leading to a dramatic reduction in parameter count.","Experiments for two image interpolation applications verify the restoration performance, parameter efficiency and robustness to covariate shift of our graph-based unrolled networks compared to conventional transformers."],"url":"http://arxiv.org/abs/2406.04090v1","category":"cs.LG"}
{"created":"2024-06-06 11:05:49","title":"Haptic in-sensor computing device made of carbon nanotube-polydimethylsiloxane nanocomposites","abstract":"The importance of haptic in-sensor computing devices has been increasing. In this study, we successfully fabricated a haptic sensor with a hierarchical structure via the sacrificial template method, using carbon nanotubes-polydimethylsiloxane (CNTs-PDMS) nanocomposites for in-sensor computing applications. The CNTs-PDMS nanocomposite sensors, with different sensitivities, were obtained by varying the amount of CNTs. We transformed the input stimuli into higher-dimensional information, enabling a new path for the CNTs-PDMS nanocomposite application, which was implemented on a robotic hand as an in-sensor computing device by applying a reservoir computing paradigm. The nonlinear output data obtained from the sensors were trained using linear regression and used to classify nine different objects used in everyday life with an object recognition accuracy of >80 % for each object. This approach could enable tactile sensation in robots while reducing the computational cost.","sentences":["The importance of haptic in-sensor computing devices has been increasing.","In this study, we successfully fabricated a haptic sensor with a hierarchical structure via the sacrificial template method, using carbon nanotubes-polydimethylsiloxane (CNTs-PDMS) nanocomposites for in-sensor computing applications.","The CNTs-PDMS nanocomposite sensors, with different sensitivities, were obtained by varying the amount of CNTs.","We transformed the input stimuli into higher-dimensional information, enabling a new path for the CNTs-PDMS nanocomposite application, which was implemented on a robotic hand as an in-sensor computing device by applying a reservoir computing paradigm.","The nonlinear output data obtained from the sensors were trained using linear regression and used to classify nine different objects used in everyday life with an object recognition accuracy of >80 % for each object.","This approach could enable tactile sensation in robots while reducing the computational cost."],"url":"http://arxiv.org/abs/2406.03958v1","category":"cs.ET"}
{"created":"2024-06-06 10:45:19","title":"A Probabilistic Approach to Learning the Degree of Equivariance in Steerable CNNs","abstract":"Steerable convolutional neural networks (SCNNs) enhance task performance by modelling geometric symmetries through equivariance constraints on weights. Yet, unknown or varying symmetries can lead to overconstrained weights and decreased performance. To address this, this paper introduces a probabilistic method to learn the degree of equivariance in SCNNs. We parameterise the degree of equivariance as a likelihood distribution over the transformation group using Fourier coefficients, offering the option to model layer-wise and shared equivariance. These likelihood distributions are regularised to ensure an interpretable degree of equivariance across the network. Advantages include the applicability to many types of equivariant networks through the flexible framework of SCNNs and the ability to learn equivariance with respect to any subgroup of any compact group without requiring additional layers. Our experiments reveal competitive performance on datasets with mixed symmetries, with learnt likelihood distributions that are representative of the underlying degree of equivariance.","sentences":["Steerable convolutional neural networks (SCNNs) enhance task performance by modelling geometric symmetries through equivariance constraints on weights.","Yet, unknown or varying symmetries can lead to overconstrained weights and decreased performance.","To address this, this paper introduces a probabilistic method to learn the degree of equivariance in SCNNs.","We parameterise the degree of equivariance as a likelihood distribution over the transformation group using Fourier coefficients, offering the option to model layer-wise and shared equivariance.","These likelihood distributions are regularised to ensure an interpretable degree of equivariance across the network.","Advantages include the applicability to many types of equivariant networks through the flexible framework of SCNNs and the ability to learn equivariance with respect to any subgroup of any compact group without requiring additional layers.","Our experiments reveal competitive performance on datasets with mixed symmetries, with learnt likelihood distributions that are representative of the underlying degree of equivariance."],"url":"http://arxiv.org/abs/2406.03946v1","category":"cs.LG"}
{"created":"2024-06-06 10:20:59","title":"Observational constraints on phenomenological emergent dark energy and barotropic dark matter characterized by a constant equation of state parameter","abstract":"We propose a new cosmological model that considers dark matter as a barotropic fluid with a constant equation of state parameter and interprets dark energy as the phenomenological emergent dark energy rather than a cosmological constant. This proposal is based on extensive research on the extended properties of dark matter in the context of a cosmological constant and the intriguing findings that have emerged from our exploration of dark matter properties within the context of PEDE in our previous studies. We then place constraints on this model in light of the Planck 2018 Cosmic Microwave Background (CMB) anisotropies, baryon acoustic oscillation (BAO) measurements, the Pantheon compilation of Type Ia supernovae, a prior on $H_0$ that based on the latest local measurement by Riess et al., and the combination of KiDS and the VISTA Kilo-Degree Infrared Galaxy Survey (KiDS+VIKING-450). The results indicate a preference for a positive dark matter equation of state parameter at 68\\% confidence level for CMB+BAO, CMB+BAO+Pantheon and CMB+BAO+Pantheon+$H_0$ datasets. Furthermore, the Hubble tension between all of the datasets we used with R22 is very close to those of the PEDE, and the $S_8$ tension between Planck 2018 and KiDS+VIKING-450 is reduced from 2.3$\\sigma$ in the PEDE model to 0.4$\\sigma$ in the new model. However, Bayesian evidence indicates that PEDE favors our new model with very strong evidence from all the datasets considered in this study. Consequently, we conclude that the PEDE+$w_{\\rm dm}$ model is not a viable alternative to the PEDE model.","sentences":["We propose a new cosmological model that considers dark matter as a barotropic fluid with a constant equation of state parameter and interprets dark energy as the phenomenological emergent dark energy rather than a cosmological constant.","This proposal is based on extensive research on the extended properties of dark matter in the context of a cosmological constant and the intriguing findings that have emerged from our exploration of dark matter properties within the context of PEDE in our previous studies.","We then place constraints on this model in light of the Planck 2018 Cosmic Microwave Background (CMB) anisotropies, baryon acoustic oscillation (BAO) measurements, the Pantheon compilation of Type Ia supernovae, a prior on $H_0$ that based on the latest local measurement by Riess et al., and the combination of KiDS and the VISTA Kilo-Degree Infrared Galaxy Survey (KiDS+VIKING-450).","The results indicate a preference for a positive dark matter equation of state parameter at 68\\% confidence level for CMB+BAO, CMB+BAO+Pantheon and CMB+BAO+Pantheon+$H_0$ datasets.","Furthermore, the Hubble tension between all of the datasets we used with R22 is very close to those of the PEDE, and the $S_8$ tension between Planck 2018 and KiDS+VIKING-450 is reduced from 2.3$\\sigma$ in the PEDE model to 0.4$\\sigma$ in the new model.","However, Bayesian evidence indicates that PEDE favors our new model with very strong evidence from all the datasets considered in this study.","Consequently, we conclude that the PEDE+$w_{\\rm dm}$ model is not a viable alternative to the PEDE model."],"url":"http://arxiv.org/abs/2406.03936v1","category":"astro-ph.CO"}
{"created":"2024-06-06 10:17:35","title":"Evolution of current-carrying string networks","abstract":"Cosmic string networks are expected to form in early Universe phase transitions via the Kibble mechanism and are unavoidable in several Beyond the Standard Model theories. While most predictions of observational signals of string networks assume featureless Abelian-Higgs or Nambu-Goto string networks, in many such extensions the networks can carry additional degrees of freedom, including charges and currents; these are often generically known as superconducting strings. All such degrees of freedom can impact the evolution of the networks and therefore their observational signatures. We report on the results of $2048^3$ field theory simulations of the evolution of a current-carrying network of strings, highlighting the different scaling behaviours of the network in the radiation and matter eras. We also report the first numerical measurements of the coherence length scales for the charge and current and of the condensate equation of state, and show that the latter mainly depends on the expansion rate, with chirality occurring for the matter era. Qualitatively, the fact that the matter era is the optimal expansion rate for these networks to reach scaling is in agreement with recent analytic modeling.","sentences":["Cosmic string networks are expected to form in early Universe phase transitions via the Kibble mechanism and are unavoidable in several Beyond the Standard Model theories.","While most predictions of observational signals of string networks assume featureless Abelian-Higgs or Nambu-Goto string networks, in many such extensions the networks can carry additional degrees of freedom, including charges and currents; these are often generically known as superconducting strings.","All such degrees of freedom can impact the evolution of the networks and therefore their observational signatures.","We report on the results of $2048^3$ field theory simulations of the evolution of a current-carrying network of strings, highlighting the different scaling behaviours of the network in the radiation and matter eras.","We also report the first numerical measurements of the coherence length scales for the charge and current and of the condensate equation of state, and show that the latter mainly depends on the expansion rate, with chirality occurring for the matter era.","Qualitatively, the fact that the matter era is the optimal expansion rate for these networks to reach scaling is in agreement with recent analytic modeling."],"url":"http://arxiv.org/abs/2406.03931v1","category":"hep-ph"}
{"created":"2024-06-06 10:04:53","title":"Latent Neural Operator for Solving Forward and Inverse PDE Problems","abstract":"Neural operators effectively solve PDE problems from data without knowing the explicit equations, which learn the map from the input sequences of observed samples to the predicted values. Most existed works build the model in the original geometric space, leading to high computational costs when the number of sample points is large. We present the Latent Neural Operator (LNO) solving PDEs in the latent space. In particular, we first propose Physics-Cross-Attention (PhCA) transforming representation from the geometric space to the latent space, then learn the operator in the latent space, and finally recover the real-world geometric space via the inverse PhCA map. Our model retains flexibility that can decode values in any position not limited to locations defined in training set, and therefore can naturally perform interpolation and extrapolation tasks particularly useful for inverse problems. Moreover, the proposed LNO improves in both prediction accuracy and computational efficiency. Experiments show that LNO reduces the GPU memory by 50%, speeds up training 1.8 times, and reaches state-of-the-art accuracy on four out of six benchmarks for forward problems and a benchmark for inverse problem.","sentences":["Neural operators effectively solve PDE problems from data without knowing the explicit equations, which learn the map from the input sequences of observed samples to the predicted values.","Most existed works build the model in the original geometric space, leading to high computational costs when the number of sample points is large.","We present the Latent Neural Operator (LNO) solving PDEs in the latent space.","In particular, we first propose Physics-Cross-Attention (PhCA) transforming representation from the geometric space to the latent space, then learn the operator in the latent space, and finally recover the real-world geometric space via the inverse PhCA map.","Our model retains flexibility that can decode values in any position not limited to locations defined in training set, and therefore can naturally perform interpolation and extrapolation tasks particularly useful for inverse problems.","Moreover, the proposed LNO improves in both prediction accuracy and computational efficiency.","Experiments show that LNO reduces the GPU memory by 50%, speeds up training 1.8 times, and reaches state-of-the-art accuracy on four out of six benchmarks for forward problems and a benchmark for inverse problem."],"url":"http://arxiv.org/abs/2406.03923v1","category":"cs.LG"}
{"created":"2024-06-06 09:44:11","title":"Traveling waves for nonlinear Schr\u00f6dinger equations","abstract":"We look for traveling wave solutions to the nonlinear Schr\\\"odinger equation with a subsonic speed, covering several physical models with Sobolev subcritical nonlinear effects. Our approach is based on a variant of Sobolev-type inequality involving the momentum and we show the existence of its minimizers solving the nonlinear Schr\\\"odinger equation.","sentences":["We look for traveling wave solutions to the nonlinear Schr\\\"odinger equation with a subsonic speed, covering several physical models with Sobolev subcritical nonlinear effects.","Our approach is based on a variant of Sobolev-type inequality involving the momentum and we show the existence of its minimizers solving the nonlinear Schr\\\"odinger equation."],"url":"http://arxiv.org/abs/2406.03910v1","category":"math.AP"}
{"created":"2024-06-06 09:37:46","title":"Polyp and Surgical Instrument Segmentation with Double Encoder-Decoder Networks","abstract":"This paper describes a solution for the MedAI competition, in which participants were required to segment both polyps and surgical instruments from endoscopic images. Our approach relies on a double encoder-decoder neural network which we have previously applied for polyp segmentation, but with a series of enhancements: a more powerful encoder architecture, an improved optimization procedure, and the post-processing of segmentations based on tempered model ensembling. Experimental results show that our method produces segmentations that show a good agreement with manual delineations provided by medical experts.","sentences":["This paper describes a solution for the MedAI competition, in which participants were required to segment both polyps and surgical instruments from endoscopic images.","Our approach relies on a double encoder-decoder neural network which we have previously applied for polyp segmentation, but with a series of enhancements: a more powerful encoder architecture, an improved optimization procedure, and the post-processing of segmentations based on tempered model ensembling.","Experimental results show that our method produces segmentations that show a good agreement with manual delineations provided by medical experts."],"url":"http://arxiv.org/abs/2406.03901v1","category":"eess.IV"}
{"created":"2024-06-06 09:25:02","title":"Harnack inequality for doubly nonlinear mixed local and nonlocal parabolic equations","abstract":"In this paper, we establish the Harnack inequality of nonnegative weak solutions to the doubly nonlinear mixed local and nonlocal parabolic equations. This result is obtained by combining a related comparison principle, a local boundedness estimate, and an integral Harnack-type inequality. Our proof is based on the expansion of positivity together with a comparison argument.","sentences":["In this paper, we establish the Harnack inequality of nonnegative weak solutions to the doubly nonlinear mixed local and nonlocal parabolic equations.","This result is obtained by combining a related comparison principle, a local boundedness estimate, and an integral Harnack-type inequality.","Our proof is based on the expansion of positivity together with a comparison argument."],"url":"http://arxiv.org/abs/2406.03889v1","category":"math.AP"}
{"created":"2024-06-06 08:31:52","title":"A Noise-robust Multi-head Attention Mechanism for Formation Resistivity Prediction: Frequency Aware LSTM","abstract":"The prediction of formation resistivity plays a crucial role in the evaluation of oil and gas reservoirs, identification and assessment of geothermal energy resources, groundwater detection and monitoring, and carbon capture and storage. However, traditional well logging techniques fail to measure accurate resistivity in cased boreholes, and the transient electromagnetic method for cased borehole resistivity logging encounters challenges of high-frequency disaster (the problem of inadequate learning by neural networks in high-frequency features) and noise interference, badly affecting accuracy. To address these challenges, frequency-aware framework and temporal anti-noise block are proposed to build frequency aware LSTM (FAL). The frequency-aware framework implements a dual-stream structure through wavelet transformation, allowing the neural network to simultaneously handle high-frequency and low-frequency flows of time-series data, thus avoiding high-frequency disaster. The temporal anti-noise block integrates multiple attention mechanisms and soft-threshold attention mechanisms, enabling the model to better distinguish noise from redundant features. Ablation experiments demonstrate that the frequency-aware framework and temporal anti-noise block contribute significantly to performance improvement. FAL achieves a 24.3% improvement in R2 over LSTM, reaching the highest value of 0.91 among all models. In robustness experiments, the impact of noise on FAL is approximately 1/8 of the baseline, confirming the noise resistance of FAL. The proposed FAL effectively reduces noise interference in predicting formation resistivity from cased transient electromagnetic well logging curves, better learns high-frequency features, and thereby enhances the prediction accuracy and noise resistance of the neural network model.","sentences":["The prediction of formation resistivity plays a crucial role in the evaluation of oil and gas reservoirs, identification and assessment of geothermal energy resources, groundwater detection and monitoring, and carbon capture and storage.","However, traditional well logging techniques fail to measure accurate resistivity in cased boreholes, and the transient electromagnetic method for cased borehole resistivity logging encounters challenges of high-frequency disaster (the problem of inadequate learning by neural networks in high-frequency features) and noise interference, badly affecting accuracy.","To address these challenges, frequency-aware framework and temporal anti-noise block are proposed to build frequency aware LSTM (FAL).","The frequency-aware framework implements a dual-stream structure through wavelet transformation, allowing the neural network to simultaneously handle high-frequency and low-frequency flows of time-series data, thus avoiding high-frequency disaster.","The temporal anti-noise block integrates multiple attention mechanisms and soft-threshold attention mechanisms, enabling the model to better distinguish noise from redundant features.","Ablation experiments demonstrate that the frequency-aware framework and temporal anti-noise block contribute significantly to performance improvement.","FAL achieves a 24.3% improvement in R2 over LSTM, reaching the highest value of 0.91 among all models.","In robustness experiments, the impact of noise on FAL is approximately 1/8 of the baseline, confirming the noise resistance of FAL.","The proposed FAL effectively reduces noise interference in predicting formation resistivity from cased transient electromagnetic well logging curves, better learns high-frequency features, and thereby enhances the prediction accuracy and noise resistance of the neural network model."],"url":"http://arxiv.org/abs/2406.03849v1","category":"cs.LG"}
{"created":"2024-06-06 08:24:14","title":"Non-Kerr Constraints using Binary Black Hole inspirals considering phase modifications up to 4 PN order","abstract":"The gravitational field around an astrophysical black hole (BH) is thought to be described by the Kerr spacetime, which is a solution of the Einstein equation. Signatures of binary black hole (BBH) coalescence in gravitational waves (GW) follow the Kerr spacetime as the theoretical foundation. Hence, any possible deviations from the Kerr spacetime around BHs serve as a test of the nature of gravity in the strong-field regime and of the predictions of General Relativity. In our study, we perform a theory-agnostic test of the Kerr hypothesis using BBH inspirals from the third Gravitational-wave Transient Catalog (GWTC-3). Considering the Johannsen metric, we compute the leading-order deviation to the emitted GW in the frequency domain. Our results provide constraints on two deformation parameters ($\\alpha_{13}$ and $\\epsilon_3$) and demonstrate the degeneracy between these two non-Kerr parameters.","sentences":["The gravitational field around an astrophysical black hole (BH) is thought to be described by the Kerr spacetime, which is a solution of the Einstein equation.","Signatures of binary black hole (BBH) coalescence in gravitational waves (GW) follow the Kerr spacetime as the theoretical foundation.","Hence, any possible deviations from the Kerr spacetime around BHs serve as a test of the nature of gravity in the strong-field regime and of the predictions of General Relativity.","In our study, we perform a theory-agnostic test of the Kerr hypothesis using BBH inspirals from the third Gravitational-wave Transient Catalog (GWTC-3).","Considering the Johannsen metric, we compute the leading-order deviation to the emitted GW in the frequency domain.","Our results provide constraints on two deformation parameters ($\\alpha_{13}$ and $\\epsilon_3$) and demonstrate the degeneracy between these two non-Kerr parameters."],"url":"http://arxiv.org/abs/2406.03846v1","category":"gr-qc"}
{"created":"2024-06-06 08:15:01","title":"On universal splittings of tree-level particle and string scattering amplitudes","abstract":"In this paper, we study the newly discovered universal splitting behavior for tree-level scattering amplitudes of particles and strings~\\cite{Cao:2024gln}: when a set of Mandelstam variables (and Lorentz products involving polarizations for gluons/gravitons) vanish, the $n$-point amplitude factorizes as the product of two lower-point {\\it currents} with $n{+}3$ external legs in total. We refer to any such subspace of the kinematic space of $n$ massless momenta as ``2-split kinematics\", where the scattering potential for string amplitudes and the corresponding scattering equations for particle amplitudes nicely split into two parts. Based on these, we provide a systematic and detailed study of the splitting behavior for essentially all ingredients which appear as integrands for open- and closed-string amplitudes as well as Cachazo-He-Yuan (CHY) formulas, including Parke-Taylor factors, correlators in superstring and bosonic string theories, and CHY integrands for a variety of amplitudes of scalars, gluons and gravitons. These results then immediately lead to the splitting behavior of string and particle amplitudes in a wide range of theories, including bi-adjoint $\\phi^3$ (with string extension known as $Z$ and $J$ integrals), non-linear sigma model, Dirac-Born-Infeld, the special Galileon, \\textit{etc.}, as well as Yang-Mills and Einstein gravity (with bosonic and superstring extensions). Our results imply and extend some other factorization behavior of tree amplitudes considered recently, including smooth splittings~\\cite{Cachazo:2021wsz} and factorizations near zeros~\\cite{Arkani-Hamed:2023swr}, to all these theories. A special case of splitting also yields soft theorems for gluons/gravitons as well as analogous soft behavior for Goldstone particles near their Adler zeros.","sentences":["In this paper, we study the newly discovered universal splitting behavior for tree-level scattering amplitudes of particles and strings~\\cite{Cao:2024gln}: when a set of Mandelstam variables (and Lorentz products involving polarizations for gluons/gravitons) vanish, the $n$-point amplitude factorizes as the product of two lower-point {\\it currents} with $n{+}3$ external legs in total.","We refer to any such subspace of the kinematic space of $n$ massless momenta as ``2-split kinematics\", where the scattering potential for string amplitudes and the corresponding scattering equations for particle amplitudes nicely split into two parts.","Based on these, we provide a systematic and detailed study of the splitting behavior for essentially all ingredients which appear as integrands for open- and closed-string amplitudes as well as Cachazo-He-Yuan (CHY) formulas, including Parke-Taylor factors, correlators in superstring and bosonic string theories, and CHY integrands for a variety of amplitudes of scalars, gluons and gravitons.","These results then immediately lead to the splitting behavior of string and particle amplitudes in a wide range of theories, including bi-adjoint $\\phi^3$ (with string extension known as $Z$ and $J$ integrals), non-linear sigma model, Dirac-Born-Infeld, the special Galileon, \\textit{etc.}, as well as Yang-Mills and Einstein gravity (with bosonic and superstring extensions).","Our results imply and extend some other factorization behavior of tree amplitudes considered recently, including smooth splittings~\\cite{Cachazo:2021wsz} and factorizations near zeros~\\cite{Arkani-Hamed:2023swr}, to all these theories.","A special case of splitting also yields soft theorems for gluons/gravitons as well as analogous soft behavior for Goldstone particles near their Adler zeros."],"url":"http://arxiv.org/abs/2406.03838v1","category":"hep-th"}
{"created":"2024-06-06 08:08:01","title":"Exploiting Global Graph Homophily for Generalized Defense in Graph Neural Networks","abstract":"Graph neural network (GNN) models play a pivotal role in numerous tasks involving graph-related data analysis. Despite their efficacy, similar to other deep learning models, GNNs are susceptible to adversarial attacks. Even minor perturbations in graph data can induce substantial alterations in model predictions. While existing research has explored various adversarial defense techniques for GNNs, the challenge of defending against adversarial attacks on real-world scale graph data remains largely unresolved. On one hand, methods reliant on graph purification and preprocessing tend to excessively emphasize local graph information, leading to sub-optimal defensive outcomes. On the other hand, approaches rooted in graph structure learning entail significant time overheads, rendering them impractical for large-scale graphs. In this paper, we propose a new defense method named Talos, which enhances the global, rather than local, homophily of graphs as a defense. Experiments show that the proposed approach notably outperforms state-of-the-art defense approaches, while imposing little computational overhead.","sentences":["Graph neural network (GNN) models play a pivotal role in numerous tasks involving graph-related data analysis.","Despite their efficacy, similar to other deep learning models, GNNs are susceptible to adversarial attacks.","Even minor perturbations in graph data can induce substantial alterations in model predictions.","While existing research has explored various adversarial defense techniques for GNNs, the challenge of defending against adversarial attacks on real-world scale graph data remains largely unresolved.","On one hand, methods reliant on graph purification and preprocessing tend to excessively emphasize local graph information, leading to sub-optimal defensive outcomes.","On the other hand, approaches rooted in graph structure learning entail significant time overheads, rendering them impractical for large-scale graphs.","In this paper, we propose a new defense method named Talos, which enhances the global, rather than local, homophily of graphs as a defense.","Experiments show that the proposed approach notably outperforms state-of-the-art defense approaches, while imposing little computational overhead."],"url":"http://arxiv.org/abs/2406.03833v1","category":"cs.LG"}
{"created":"2024-06-06 08:04:56","title":"How much do we know the halo mass function? Predictions beyond resolution","abstract":"As a common gravitation virialized object in the standard $\\Lambda$CDM cosmology, dark matter halo connects from the large-scale structure all the way down to galaxy and star formation. However, as the nature of dark matter particles is still unclear, the smallest halo that can be formed in the universe is still unknown. Based on some simple assumptions, this paper uses the \\textsc{hmf} package to investigate different halo functions used to quantify its number and mass distributions -- the halo mass function and the integrated/differential mass function (IMF/DMF) respectively. The halo mass in this study extends from the galaxy cluster to the dark matter particle mass at the GeV scale. Surprisingly, different fitting functions for the HMF are in remarkable agreement, a scatter within 2 orders of magnitude, down to dark matter particle mass, of which the halo mass spans about 80 orders of magnitude and the HMF covers over 100 orders of magnitude. The DMF reveals an interesting and consistent peak at $\\sim 10^{13} \\hMsun$, which implies galaxy groups have the highest contribution to the total matter mass. Furthermore, the effects of cosmology parameters on these halo functions are also examined with the most massive halos, or these halo functions at the most massive halo mass end, more sensitive to them. Different behaviours of these halo functions due to the changes in cosmology parameters can be used to break the degeneracy between them.","sentences":["As a common gravitation virialized object in the standard $\\Lambda$CDM cosmology, dark matter halo connects from the large-scale structure all the way down to galaxy and star formation.","However, as the nature of dark matter particles is still unclear, the smallest halo that can be formed in the universe is still unknown.","Based on some simple assumptions, this paper uses the \\textsc{hmf} package to investigate different halo functions used to quantify its number and mass distributions -- the halo mass function and the integrated/differential mass function (IMF/DMF) respectively.","The halo mass in this study extends from the galaxy cluster to the dark matter particle mass at the GeV scale.","Surprisingly, different fitting functions for the HMF are in remarkable agreement, a scatter within 2 orders of magnitude, down to dark matter particle mass, of which the halo mass spans about 80 orders of magnitude and the HMF covers over 100 orders of magnitude.","The DMF reveals an interesting and consistent peak at $\\sim 10^{13} \\hMsun$, which implies galaxy groups have the highest contribution to the total matter mass.","Furthermore, the effects of cosmology parameters on these halo functions are also examined with the most massive halos, or these halo functions at the most massive halo mass end, more sensitive to them.","Different behaviours of these halo functions due to the changes in cosmology parameters can be used to break the degeneracy between them."],"url":"http://arxiv.org/abs/2406.03829v1","category":"astro-ph.CO"}
{"created":"2024-06-06 08:03:57","title":"The G\u00f6del Universe as a Lie group with left-invariant Lorentz metric and\\newline the Iwasawa decomposition","abstract":"We discuss models of the G\\\"odel Universe as Lie groups with left-invariant Lorentz metric for two simply connected four-dimensional Lie groups, the Iwasawa decomposition for semisimple Lie groups, and left-invariant Lorentz metric on $SL(2,\\mathbb{R})$, following K.-H.~Neeb. Also we show that the isometry between two non-isomorphic sub-Riemannian Lie group, constructed by A.~Agrachev and D.~Barilari, is induced by some Iwasawa decomposition of $SL(2,\\mathbb{R})$.","sentences":["We discuss models of the G\\\"odel Universe as Lie groups with left-invariant Lorentz metric for two simply connected four-dimensional Lie groups, the Iwasawa decomposition for semisimple Lie groups, and left-invariant Lorentz metric on $SL(2,\\mathbb{R})$, following K.-H.~Neeb.","Also we show that the isometry between two non-isomorphic sub-Riemannian Lie group, constructed by A.~Agrachev and D.~Barilari, is induced by some Iwasawa decomposition of $SL(2,\\mathbb{R})$."],"url":"http://arxiv.org/abs/2406.03828v1","category":"math.DG"}
{"created":"2024-06-06 07:58:31","title":"SilentCipher: Deep Audio Watermarking","abstract":"In the realm of audio watermarking, it is challenging to simultaneously encode imperceptible messages while enhancing the message capacity and robustness. Although recent advancements in deep learning-based methods bolster the message capacity and robustness over traditional methods, the encoded messages introduce audible artefacts that restricts their usage in professional settings. In this study, we introduce three key innovations. Firstly, our work is the first deep learning-based model to integrate psychoacoustic model based thresholding to achieve imperceptible watermarks. Secondly, we introduce psuedo-differentiable compression layers, enhancing the robustness of our watermarking algorithm. Lastly, we introduce a method to eliminate the need for perceptual losses, enabling us to achieve SOTA in both robustness as well as imperceptible watermarking. Our contributions lead us to SilentCipher, a model enabling users to encode messages within audio signals sampled at 44.1kHz.","sentences":["In the realm of audio watermarking, it is challenging to simultaneously encode imperceptible messages while enhancing the message capacity and robustness.","Although recent advancements in deep learning-based methods bolster the message capacity and robustness over traditional methods, the encoded messages introduce audible artefacts that restricts their usage in professional settings.","In this study, we introduce three key innovations.","Firstly, our work is the first deep learning-based model to integrate psychoacoustic model based thresholding to achieve imperceptible watermarks.","Secondly, we introduce psuedo-differentiable compression layers, enhancing the robustness of our watermarking algorithm.","Lastly, we introduce a method to eliminate the need for perceptual losses, enabling us to achieve SOTA in both robustness as well as imperceptible watermarking.","Our contributions lead us to SilentCipher, a model enabling users to encode messages within audio signals sampled at 44.1kHz."],"url":"http://arxiv.org/abs/2406.03822v1","category":"cs.SD"}
{"created":"2024-06-06 07:42:20","title":"Field Theory of Active Brownian Particles with Dry Friction","abstract":"We present a field theoretic approach to capture the motion of a particle with dry friction for one- and two-dimensional diffusive particles, and further expand the framework for two-dimensional active Brownian particles. Starting with the Fokker-Planck equation and introducing the Hermite polynomials as the corresponding eigen-functions, we obtain the actions and propagators. Using a perturbation expansion, we calculate the effective diffusion coefficient in the presence of both wet and dry frictions in a perturbative way via the Green-Kubo relation. We further compare the analytical result with the numerical simulation. Our result can be used to estimate the values of dry friction coefficient in experiments.","sentences":["We present a field theoretic approach to capture the motion of a particle with dry friction for one- and two-dimensional diffusive particles, and further expand the framework for two-dimensional active Brownian particles.","Starting with the Fokker-Planck equation and introducing the Hermite polynomials as the corresponding eigen-functions, we obtain the actions and propagators.","Using a perturbation expansion, we calculate the effective diffusion coefficient in the presence of both wet and dry frictions in a perturbative way via the Green-Kubo relation.","We further compare the analytical result with the numerical simulation.","Our result can be used to estimate the values of dry friction coefficient in experiments."],"url":"http://arxiv.org/abs/2406.03817v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-06 07:20:16","title":"Continual Counting with Gradual Privacy Expiration","abstract":"Differential privacy with gradual expiration models the setting where data items arrive in a stream and at a given time $t$ the privacy loss guaranteed for a data item seen at time $(t-d)$ is $\\epsilon g(d)$, where $g$ is a monotonically non-decreasing function. We study the fundamental $\\textit{continual (binary) counting}$ problem where each data item consists of a bit, and the algorithm needs to output at each time step the sum of all the bits streamed so far. For a stream of length $T$ and privacy $\\textit{without}$ expiration continual counting is possible with maximum (over all time steps) additive error $O(\\log^2(T)/\\varepsilon)$ and the best known lower bound is $\\Omega(\\log(T)/\\varepsilon)$; closing this gap is a challenging open problem.   We show that the situation is very different for privacy with gradual expiration by giving upper and lower bounds for a large set of expiration functions $g$. Specifically, our algorithm achieves an additive error of $ O(\\log(T)/\\epsilon)$ for a large set of privacy expiration functions. We also give a lower bound that shows that if $C$ is the additive error of any $\\epsilon$-DP algorithm for this problem, then the product of $C$ and the privacy expiration function after $2C$ steps must be $\\Omega(\\log(T)/\\epsilon)$. Our algorithm matches this lower bound as its additive error is $O(\\log(T)/\\epsilon)$, even when $g(2C) = O(1)$.   Our empirical evaluation shows that we achieve a slowly growing privacy loss with significantly smaller empirical privacy loss for large values of $d$ than a natural baseline algorithm.","sentences":["Differential privacy with gradual expiration models the setting where data items arrive in a stream and at a given time $t$ the privacy loss guaranteed for a data item seen at time $(t-d)$ is $\\epsilon g(d)$, where $g$ is a monotonically non-decreasing function.","We study the fundamental $\\textit{continual (binary) counting}$ problem where each data item consists of a bit, and the algorithm needs to output at each time step the sum of all the bits streamed so far.","For a stream of length $T$ and privacy $\\textit{without}$ expiration continual counting is possible with maximum (over all time steps) additive error $O(\\log^2(T)/\\varepsilon)$ and the best known lower bound is $\\Omega(\\log(T)/\\varepsilon)$; closing this gap is a challenging open problem.   ","We show that the situation is very different for privacy with gradual expiration by giving upper and lower bounds for a large set of expiration functions $g$. Specifically, our algorithm achieves an additive error of $ O(\\log(T)/\\epsilon)$ for a large set of privacy expiration functions.","We also give a lower bound that shows that if $C$ is the additive error of any $\\epsilon$-DP algorithm for this problem, then the product of $C$ and the privacy expiration function after $2C$ steps must be $\\Omega(\\log(T)/\\epsilon)$. Our algorithm matches this lower bound as its additive error is $O(\\log(T)/\\epsilon)$, even when $g(2C) = O(1)$.   Our empirical evaluation shows that we achieve a slowly growing privacy loss with significantly smaller empirical privacy loss for large values of $d$ than a natural baseline algorithm."],"url":"http://arxiv.org/abs/2406.03802v1","category":"cs.CR"}
{"created":"2024-06-06 07:08:52","title":"Morpho-Photometric Classification of KiDS DR5 Sources Based on Neural Networks: A Comprehensive Star-Quasar-Galaxy Catalog","abstract":"We present a novel multimodal neural network for classifying astronomical sources in multiband ground-based observations, from optical to near infrared, to separate sources in stars, galaxies and quasars. Our approach combines a convolutional neural network branch for learning morphological features from $r$-band images with an artificial neural network branch for extracting spectral energy distribution (SED) information. Specifically, we have used 9-band optical ($ugri$) and NIR ($ZYHJK_s$) data from the Kilo-Degree Survey (KiDS) Data Release 5. The two branches of the network are concatenated and feed into fully-connected layers for final classification. We train the network on a spectroscopically confirmed sample from the Sloan Digital Sky Survey cross-matched with KiDS. The trained model achieves 98.76\\% overall accuracy on an independent testing dataset, with F1 scores exceeding 95\\% for each class. Raising the output probability threshold, we obtain higher purity at the cost of a lower completeness. We have also validated the network using external catalogs cross-matched with KiDS, correctly classifying 99.74\\% of a pure star sample selected from Gaia parallaxes and proper motions, and 99.74\\% of an external galaxy sample from the Galaxy and Mass Assembly survey, adjusted for low-redshift contamination. We apply the trained network to 27,334,751 KiDS DR5 sources with $r \\leqslant 23$ mag to generate a new classification catalog. This multimodal neural network successfully leverages both morphological and SED information to enable efficient and robust classification of stars, quasars, and galaxies in large photometric surveys.","sentences":["We present a novel multimodal neural network for classifying astronomical sources in multiband ground-based observations, from optical to near infrared, to separate sources in stars, galaxies and quasars.","Our approach combines a convolutional neural network branch for learning morphological features from $r$-band images with an artificial neural network branch for extracting spectral energy distribution (SED) information.","Specifically, we have used 9-band optical ($ugri$) and NIR ($ZYHJK_s$) data from the Kilo-Degree Survey (KiDS) Data Release 5.","The two branches of the network are concatenated and feed into fully-connected layers for final classification.","We train the network on a spectroscopically confirmed sample from the Sloan Digital Sky Survey cross-matched with KiDS.","The trained model achieves 98.76\\% overall accuracy on an independent testing dataset, with F1 scores exceeding 95\\% for each class.","Raising the output probability threshold, we obtain higher purity at the cost of a lower completeness.","We have also validated the network using external catalogs cross-matched with KiDS, correctly classifying 99.74\\% of a pure star sample selected from Gaia parallaxes and proper motions, and 99.74\\% of an external galaxy sample from the Galaxy and Mass Assembly survey, adjusted for low-redshift contamination.","We apply the trained network to 27,334,751 KiDS DR5 sources with $r \\leqslant 23$ mag to generate a new classification catalog.","This multimodal neural network successfully leverages both morphological and SED information to enable efficient and robust classification of stars, quasars, and galaxies in large photometric surveys."],"url":"http://arxiv.org/abs/2406.03797v1","category":"astro-ph.GA"}
{"created":"2024-06-06 07:01:50","title":"End-to-End Trainable Soft Retriever for Low-resource Relation Extraction","abstract":"This study addresses a crucial challenge in instance-based relation extraction using text generation models: end-to-end training in target relation extraction task is not applicable to retrievers due to the non-differentiable nature of instance selection. We propose a novel End-to-end TRAinable Soft K-nearest neighbor retriever (ETRASK) by the neural prompting method that utilizes a soft, differentiable selection of the $k$ nearest instances. This approach enables the end-to-end training of retrievers in target tasks. On the TACRED benchmark dataset with a low-resource setting where the training data was reduced to 10\\%, our method achieved a state-of-the-art F1 score of 71.5\\%. Moreover, ETRASK consistently improved the baseline model by adding instances for all settings. These results highlight the efficacy of our approach in enhancing relation extraction performance, especially in resource-constrained environments. Our findings offer a promising direction for future research with extraction and the broader application of text generation in natural language processing.","sentences":["This study addresses a crucial challenge in instance-based relation extraction using text generation models: end-to-end training in target relation extraction task is not applicable to retrievers due to the non-differentiable nature of instance selection.","We propose a novel End-to-end TRAinable Soft K-nearest neighbor retriever (ETRASK) by the neural prompting method that utilizes a soft, differentiable selection of the $k$ nearest instances.","This approach enables the end-to-end training of retrievers in target tasks.","On the TACRED benchmark dataset with a low-resource setting where the training data was reduced to 10\\%, our method achieved a state-of-the-art F1 score of 71.5\\%.","Moreover, ETRASK consistently improved the baseline model by adding instances for all settings.","These results highlight the efficacy of our approach in enhancing relation extraction performance, especially in resource-constrained environments.","Our findings offer a promising direction for future research with extraction and the broader application of text generation in natural language processing."],"url":"http://arxiv.org/abs/2406.03790v1","category":"cs.CL"}
{"created":"2024-06-06 06:55:08","title":"Count-mean Sketch as an Optimized Framework for Frequency Estimation with Local Differential Privacy","abstract":"This paper identifies that a group of state-of-the-art locally-differentially-private (LDP) algorithms for frequency estimation are equivalent to the private Count-Mean Sketch (CMS) algorithm with different parameters. Therefore, we revisit the private CMS, correct errors in the original CMS paper regarding expectation and variance, modify the CMS implementation to eliminate existing bias, and explore optimized parameters for CMS to achieve optimality in reducing the worst-case mean squared error (MSE), $l_1$ loss, and $l_2$ loss. Additionally, we prove that pairwise-independent hashing is sufficient for CMS, reducing its communication cost to the logarithm of the cardinality of all possible values (i.e., a dictionary). As a result, the aforementioned optimized CMS is proven theoretically and empirically to be the only algorithm optimized for reducing the worst-case MSE, $l_1$ loss, and $l_2$ loss when dealing with a very large dictionary. Furthermore, we demonstrate that randomness is necessary to ensure the correctness of CMS, and the communication cost of CMS, though low, is unavoidable despite the randomness being public or private.","sentences":["This paper identifies that a group of state-of-the-art locally-differentially-private (LDP) algorithms for frequency estimation are equivalent to the private Count-Mean Sketch (CMS) algorithm with different parameters.","Therefore, we revisit the private CMS, correct errors in the original CMS paper regarding expectation and variance, modify the CMS implementation to eliminate existing bias, and explore optimized parameters for CMS to achieve optimality in reducing the worst-case mean squared error (MSE), $l_1$ loss, and $l_2$ loss.","Additionally, we prove that pairwise-independent hashing is sufficient for CMS, reducing its communication cost to the logarithm of the cardinality of all possible values (i.e., a dictionary).","As a result, the aforementioned optimized CMS is proven theoretically and empirically to be the only algorithm optimized for reducing the worst-case MSE, $l_1$ loss, and $l_2$ loss when dealing with a very large dictionary.","Furthermore, we demonstrate that randomness is necessary to ensure the correctness of CMS, and the communication cost of CMS, though low, is unavoidable despite the randomness being public or private."],"url":"http://arxiv.org/abs/2406.03785v1","category":"cs.CR"}
{"created":"2024-06-06 05:57:38","title":"CORTEX: Large-Scale Brain Simulator Utilizing Indegree Sub-Graph Decomposition on Fugaku Supercomputer","abstract":"We introduce CORTEX, an algorithmic framework designed for large-scale brain simulation. Leveraging the computational capacity of the Fugaku Supercomputer, CORTEX maximizes available problem size and processing performance. Our primary innovation, Indegree Sub-Graph Decomposition, along with a suite of parallel algorithms, facilitates efficient domain decomposition by segmenting the global graph structure into smaller, identically structured sub-graphs. This segmentation allows for parallel processing of synaptic interactions without inter-process dependencies, effectively eliminating data racing at the thread level without necessitating mutexes or atomic operations. Additionally, this strategy enhances the overlap of communication and computation. Benchmark tests conducted on spiking neural networks, characterized by biological parameters, have demonstrated significant enhancements in both problem size and simulation performance, surpassing the capabilities of the current leading open-source solution, the NEST Simulator. Our work offers a powerful new tool for the field of neuromorphic computing and understanding brain function.","sentences":["We introduce CORTEX, an algorithmic framework designed for large-scale brain simulation.","Leveraging the computational capacity of the Fugaku Supercomputer, CORTEX maximizes available problem size and processing performance.","Our primary innovation, Indegree Sub-Graph Decomposition, along with a suite of parallel algorithms, facilitates efficient domain decomposition by segmenting the global graph structure into smaller, identically structured sub-graphs.","This segmentation allows for parallel processing of synaptic interactions without inter-process dependencies, effectively eliminating data racing at the thread level without necessitating mutexes or atomic operations.","Additionally, this strategy enhances the overlap of communication and computation.","Benchmark tests conducted on spiking neural networks, characterized by biological parameters, have demonstrated significant enhancements in both problem size and simulation performance, surpassing the capabilities of the current leading open-source solution, the NEST Simulator.","Our work offers a powerful new tool for the field of neuromorphic computing and understanding brain function."],"url":"http://arxiv.org/abs/2406.03762v1","category":"cs.DC"}
{"created":"2024-06-06 05:07:44","title":"NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting by Learning from Human","abstract":"Increasing concerns about privacy leakage issues in academia and industry arise when employing NLP models from third-party providers to process sensitive texts. To protect privacy before sending sensitive data to those models, we suggest sanitizing sensitive text using two common strategies used by humans: i) deleting sensitive expressions, and ii) obscuring sensitive details by abstracting them. To explore the issues and develop a tool for text rewriting, we curate the first corpus, coined NAP^2, through both crowdsourcing and the use of large language models (LLMs). Compared to the prior works based on differential privacy, which lead to a sharp drop in information utility and unnatural texts, the human-inspired approaches result in more natural rewrites and offer an improved balance between privacy protection and data utility, as demonstrated by our extensive experiments.","sentences":["Increasing concerns about privacy leakage issues in academia and industry arise when employing NLP models from third-party providers to process sensitive texts.","To protect privacy before sending sensitive data to those models, we suggest sanitizing sensitive text using two common strategies used by humans: i) deleting sensitive expressions, and ii) obscuring sensitive details by abstracting them.","To explore the issues and develop a tool for text rewriting, we curate the first corpus, coined NAP^2, through both crowdsourcing and the use of large language models (LLMs).","Compared to the prior works based on differential privacy, which lead to a sharp drop in information utility and unnatural texts, the human-inspired approaches result in more natural rewrites and offer an improved balance between privacy protection and data utility, as demonstrated by our extensive experiments."],"url":"http://arxiv.org/abs/2406.03749v1","category":"cs.CL"}
{"created":"2024-06-06 04:44:10","title":"ReDistill: Residual Encoded Distillation for Peak Memory Reduction","abstract":"The expansion of neural network sizes and the enhancement of image resolution through modern camera sensors result in heightened memory and power demands for neural networks. Reducing peak memory, which is the maximum memory consumed during the execution of a neural network, is critical to deploy neural networks on edge devices with limited memory budget. A naive approach to reducing peak memory is aggressive down-sampling of feature maps via pooling with large stride, which often results in unacceptable degradation in network performance. To mitigate this problem, we propose residual encoded distillation (ReDistill) for peak memory reduction in a teacher-student framework, in which a student network with less memory is derived from the teacher network using aggressive pooling. We apply our distillation method to multiple problems in computer vision including image classification and diffusion based image generation. For image classification, our method yields 2x-3.2x measured peak memory on an edge GPU with negligible degradation in accuracy for most CNN based architectures. Additionally, our method yields improved test accuracy for tiny vision transformer (ViT) based models distilled from large CNN based teacher architectures. For diffusion-based image generation, our proposed distillation method yields a denoising network with 4x lower theoretical peak memory while maintaining decent diversity and fidelity for image generation. Experiments demonstrate our method's superior performance compared to other feature-based and response-based distillation methods.","sentences":["The expansion of neural network sizes and the enhancement of image resolution through modern camera sensors result in heightened memory and power demands for neural networks.","Reducing peak memory, which is the maximum memory consumed during the execution of a neural network, is critical to deploy neural networks on edge devices with limited memory budget.","A naive approach to reducing peak memory is aggressive down-sampling of feature maps via pooling with large stride, which often results in unacceptable degradation in network performance.","To mitigate this problem, we propose residual encoded distillation (ReDistill) for peak memory reduction in a teacher-student framework, in which a student network with less memory is derived from the teacher network using aggressive pooling.","We apply our distillation method to multiple problems in computer vision including image classification and diffusion based image generation.","For image classification, our method yields 2x-3.2x measured peak memory on an edge GPU with negligible degradation in accuracy for most CNN based architectures.","Additionally, our method yields improved test accuracy for tiny vision transformer (ViT) based models distilled from large CNN based teacher architectures.","For diffusion-based image generation, our proposed distillation method yields a denoising network with 4x lower theoretical peak memory while maintaining decent diversity and fidelity for image generation.","Experiments demonstrate our method's superior performance compared to other feature-based and response-based distillation methods."],"url":"http://arxiv.org/abs/2406.03744v1","category":"cs.CV"}
{"created":"2024-06-06 03:37:39","title":"Gear-NeRF: Free-Viewpoint Rendering and Tracking with Motion-aware Spatio-Temporal Sampling","abstract":"Extensions of Neural Radiance Fields (NeRFs) to model dynamic scenes have enabled their near photo-realistic, free-viewpoint rendering. Although these methods have shown some potential in creating immersive experiences, two drawbacks limit their ubiquity: (i) a significant reduction in reconstruction quality when the computing budget is limited, and (ii) a lack of semantic understanding of the underlying scenes. To address these issues, we introduce Gear-NeRF, which leverages semantic information from powerful image segmentation models. Our approach presents a principled way for learning a spatio-temporal (4D) semantic embedding, based on which we introduce the concept of gears to allow for stratified modeling of dynamic regions of the scene based on the extent of their motion. Such differentiation allows us to adjust the spatio-temporal sampling resolution for each region in proportion to its motion scale, achieving more photo-realistic dynamic novel view synthesis. At the same time, almost for free, our approach enables free-viewpoint tracking of objects of interest - a functionality not yet achieved by existing NeRF-based methods. Empirical studies validate the effectiveness of our method, where we achieve state-of-the-art rendering and tracking performance on multiple challenging datasets.","sentences":["Extensions of Neural Radiance Fields (NeRFs) to model dynamic scenes have enabled their near photo-realistic, free-viewpoint rendering.","Although these methods have shown some potential in creating immersive experiences, two drawbacks limit their ubiquity: (i) a significant reduction in reconstruction quality when the computing budget is limited, and (ii) a lack of semantic understanding of the underlying scenes.","To address these issues, we introduce Gear-NeRF, which leverages semantic information from powerful image segmentation models.","Our approach presents a principled way for learning a spatio-temporal (4D) semantic embedding, based on which we introduce the concept of gears to allow for stratified modeling of dynamic regions of the scene based on the extent of their motion.","Such differentiation allows us to adjust the spatio-temporal sampling resolution for each region in proportion to its motion scale, achieving more photo-realistic dynamic novel view synthesis.","At the same time, almost for free, our approach enables free-viewpoint tracking of objects of interest - a functionality not yet achieved by existing NeRF-based methods.","Empirical studies validate the effectiveness of our method, where we achieve state-of-the-art rendering and tracking performance on multiple challenging datasets."],"url":"http://arxiv.org/abs/2406.03723v1","category":"cs.CV"}
{"created":"2024-06-06 03:25:16","title":"Weighted Voronoi-Delaunay dual on polyhedral surfaces and its finiteness","abstract":"We aim to give a strict proof of the existence and uniqueness of the weighted Voronoi decomposition and the dual weighted Delaunay triangulation on Euclidean and hyperbolic polyhedral surface as well as hyperbolic surface with geodesic boundaries. Since the former definition of the Voronoi cell may not be simply connected, we slightly adjust the definition. Our proof is to construct an isotopic map instead of using the edge-flipping algorithm, which is a generalization of the one by Dyer et al. The main theorem of this paper is a lemma for proving the existence of the inversive distance circle packing.","sentences":["We aim to give a strict proof of the existence and uniqueness of the weighted Voronoi decomposition and the dual weighted Delaunay triangulation on Euclidean and hyperbolic polyhedral surface as well as hyperbolic surface with geodesic boundaries.","Since the former definition of the Voronoi cell may not be simply connected, we slightly adjust the definition.","Our proof is to construct an isotopic map instead of using the edge-flipping algorithm, which is a generalization of the one by Dyer et al.","The main theorem of this paper is a lemma for proving the existence of the inversive distance circle packing."],"url":"http://arxiv.org/abs/2406.03717v1","category":"math.DG"}
{"created":"2024-06-06 03:17:50","title":"Strong convergence rates for full-discrete approximations of the stochastic Allen-Cahn equations on 2D torus","abstract":"In this paper we construct space-time full discretizations of stochastic Allen-Cahn equations driven by space-time white noise on 2D torus. The approximations are implemented by tamed exponential Euler discretization in time and spectral Galerkin method in space. We finally obtain the convergence rates with the spatial order of $\\alpha-\\delta$ and the temporal order of ${\\alpha}/{6}-\\delta$ in $\\mathcal C^{-\\alpha}$ for $\\alpha\\in(0,1/3)$ and $\\delta>0$ arbitrarily small.","sentences":["In this paper we construct space-time full discretizations of stochastic Allen-Cahn equations driven by space-time white noise on 2D torus.","The approximations are implemented by tamed exponential Euler discretization in time and spectral Galerkin method in space.","We finally obtain the convergence rates with the spatial order of $\\alpha-\\delta$ and the temporal order of ${\\alpha}/{6}-\\delta$ in $\\mathcal C^{-\\alpha}$ for $\\alpha\\in(0,1/3)$ and $\\delta>0$ arbitrarily small."],"url":"http://arxiv.org/abs/2406.03715v1","category":"math.PR"}
{"created":"2024-06-06 02:51:57","title":"DSNet: A Novel Way to Use Atrous Convolutions in Semantic Segmentation","abstract":"Atrous convolutions are employed as a method to increase the receptive field in semantic segmentation tasks. However, in previous works of semantic segmentation, it was rarely employed in the shallow layers of the model. We revisit the design of atrous convolutions in modern convolutional neural networks (CNNs), and demonstrate that the concept of using large kernels to apply atrous convolutions could be a more powerful paradigm. We propose three guidelines to apply atrous convolutions more efficiently. Following these guidelines, we propose DSNet, a Dual-Branch CNN architecture, which incorporates atrous convolutions in the shallow layers of the model architecture, as well as pretraining the nearly entire encoder on ImageNet to achieve better performance. To demonstrate the effectiveness of our approach, our models achieve a new state-of-the-art trade-off between accuracy and speed on ADE20K, Cityscapes and BDD datasets. Specifically, DSNet achieves 40.0% mIOU with inference speed of 179.2 FPS on ADE20K, and 80.4% mIOU with speed of 81.9 FPS on Cityscapes. Source code and models are available at Github: https://github.com/takaniwa/DSNet.","sentences":["Atrous convolutions are employed as a method to increase the receptive field in semantic segmentation tasks.","However, in previous works of semantic segmentation, it was rarely employed in the shallow layers of the model.","We revisit the design of atrous convolutions in modern convolutional neural networks (CNNs), and demonstrate that the concept of using large kernels to apply atrous convolutions could be a more powerful paradigm.","We propose three guidelines to apply atrous convolutions more efficiently.","Following these guidelines, we propose DSNet, a Dual-Branch CNN architecture, which incorporates atrous convolutions in the shallow layers of the model architecture, as well as pretraining the nearly entire encoder on ImageNet to achieve better performance.","To demonstrate the effectiveness of our approach, our models achieve a new state-of-the-art trade-off between accuracy and speed on ADE20K, Cityscapes and BDD datasets.","Specifically, DSNet achieves 40.0% mIOU with inference speed of 179.2 FPS on ADE20K, and 80.4% mIOU with speed of 81.9 FPS on Cityscapes.","Source code and models are available at Github: https://github.com/takaniwa/DSNet."],"url":"http://arxiv.org/abs/2406.03702v1","category":"cs.CV"}
{"created":"2024-06-06 02:34:53","title":"Self-dual polyhedra and polarity","abstract":"Every polyhedron can be described by an H-representation H(P) consisting of half spaces or equivalently by a V-representation V(P) as the convex hull of a set of vertices and extreme rays. By a suitable encoding we can define a polyhedron Q by setting H(Q)=V(P). We call P self-dual if V(Q) is isomorphic to H(P). It is well known and often stated that polytopes that contain the origin in their interior and pointed polyhedral cones are self-dual. It seems to be less well known that, more generally, a polyhedron is self-dual if and only if it contains the origin. We show this by using Minkowski's bipolar equation and discuss its application to the H/V conversion problem.","sentences":["Every polyhedron can be described by an H-representation H(P) consisting of half spaces or equivalently by a V-representation V(P) as the convex hull of a set of vertices and extreme rays.","By a suitable encoding we can define a polyhedron Q by setting H(Q)=V(P).","We call P self-dual if V(Q) is isomorphic to H(P).","It is well known and often stated that polytopes that contain the origin in their interior and pointed polyhedral cones are self-dual.","It seems to be less well known that, more generally, a polyhedron is self-dual if and only if it contains the origin.","We show this by using Minkowski's bipolar equation and discuss its application to the H/V conversion problem."],"url":"http://arxiv.org/abs/2406.03698v1","category":"math.OC"}
{"created":"2024-06-06 02:22:43","title":"Untrained Neural Nets for Snapshot Compressive Imaging: Theory and Algorithms","abstract":"Snapshot compressive imaging (SCI) recovers high-dimensional (3D) data cubes from a single 2D measurement, enabling diverse applications like video and hyperspectral imaging to go beyond standard techniques in terms of acquisition speed and efficiency. In this paper, we focus on SCI recovery algorithms that employ untrained neural networks (UNNs), such as deep image prior (DIP), to model source structure. Such UNN-based methods are appealing as they have the potential of avoiding the computationally intensive retraining required for different source models and different measurement scenarios. We first develop a theoretical framework for characterizing the performance of such UNN-based methods. The theoretical framework, on the one hand, enables us to optimize the parameters of data-modulating masks, and on the other hand, provides a fundamental connection between the number of data frames that can be recovered from a single measurement to the parameters of the untrained NN. We also employ the recently proposed bagged-deep-image-prior (bagged-DIP) idea to develop SCI Bagged Deep Video Prior (SCI-BDVP) algorithms that address the common challenges faced by standard UNN solutions. Our experimental results show that in video SCI our proposed solution achieves state-of-the-art among UNN methods, and in the case of noisy measurements, it even outperforms supervised solutions.","sentences":["Snapshot compressive imaging (SCI) recovers high-dimensional (3D) data cubes from a single 2D measurement, enabling diverse applications like video and hyperspectral imaging to go beyond standard techniques in terms of acquisition speed and efficiency.","In this paper, we focus on SCI recovery algorithms that employ untrained neural networks (UNNs), such as deep image prior (DIP), to model source structure.","Such UNN-based methods are appealing as they have the potential of avoiding the computationally intensive retraining required for different source models and different measurement scenarios.","We first develop a theoretical framework for characterizing the performance of such UNN-based methods.","The theoretical framework, on the one hand, enables us to optimize the parameters of data-modulating masks, and on the other hand, provides a fundamental connection between the number of data frames that can be recovered from a single measurement to the parameters of the untrained NN.","We also employ the recently proposed bagged-deep-image-prior (bagged-DIP) idea to develop SCI Bagged Deep Video Prior (SCI-BDVP) algorithms that address the common challenges faced by standard UNN solutions.","Our experimental results show that in video SCI our proposed solution achieves state-of-the-art among UNN methods, and in the case of noisy measurements, it even outperforms supervised solutions."],"url":"http://arxiv.org/abs/2406.03694v1","category":"cs.CV"}
{"created":"2024-06-06 01:12:43","title":"Field Theory for Superconducting Branes and Generalized Particle-Vortex Duality","abstract":"We propose a field theory of closed $p$-brane $C_p^{}$ interacting with a $(p+1)$-form gauge field $A_{p+1}^{}$. This is a generalization of the Ginzburg-Landau theory (Abelian-Higgs model) for superconducting particles to higher-dimensional superconducting branes. A higher-form gauge invariant action is constructed by utilizing the Area derivative, which is a higher-dimensional generalization of the ordinary derivative. We find that the fundamental phenomena of superconductivity, such as the Meisser effect, topological defects, topological order, are naturally extended in the brane-field theory. We explicitly construct a topologically non-trivial static configuration that is characterized by the first homotopy group. Then, we calculate the low-energy effective theory in the presence of the topological defect and find that it is described by a BF-type topological field theory coupled with the world-volume of the topological defect. We also conjecture an infra-red duality between the superconducting brane-field model and dual brane-field model with a global $\\mathrm{U}(1)$ higher-form symmetry as a generalization of the Particle-Vortex duality.","sentences":["We propose a field theory of closed $p$-brane $C_p^{}$ interacting with a $(p+1)$-form gauge field $A_{p+1}^{}$. This is a generalization of the Ginzburg-Landau theory (Abelian-Higgs model) for superconducting particles to higher-dimensional superconducting branes.","A higher-form gauge invariant action is constructed by utilizing the Area derivative, which is a higher-dimensional generalization of the ordinary derivative.","We find that the fundamental phenomena of superconductivity, such as the Meisser effect, topological defects, topological order, are naturally extended in the brane-field theory.","We explicitly construct a topologically non-trivial static configuration that is characterized by the first homotopy group.","Then, we calculate the low-energy effective theory in the presence of the topological defect and find that it is described by a BF-type topological field theory coupled with the world-volume of the topological defect.","We also conjecture an infra-red duality between the superconducting brane-field model and dual brane-field model with a global $\\mathrm{U}(1)$ higher-form symmetry as a generalization of the Particle-Vortex duality."],"url":"http://arxiv.org/abs/2406.03670v1","category":"hep-th"}
{"created":"2024-06-06 00:30:51","title":"A Hybrid Deep Learning Classification of Perimetric Glaucoma Using Peripapillary Nerve Fiber Layer Reflectance and Other OCT Parameters from Three Anatomy Regions","abstract":"Precis: A hybrid deep-learning model combines NFL reflectance and other OCT parameters to improve glaucoma diagnosis. Objective: To investigate if a deep learning model could be used to combine nerve fiber layer (NFL) reflectance and other OCT parameters for glaucoma diagnosis. Patients and Methods: This is a prospective observational study where of 106 normal subjects and 164 perimetric glaucoma (PG) patients. Peripapillary NFL reflectance map, NFL thickness map, optic head analysis of disc, and macular ganglion cell complex thickness were obtained using spectral domain OCT. A hybrid deep learning model combined a fully connected network (FCN) and a convolution neural network (CNN) to develop and combine those OCT maps and parameters to distinguish normal and PG eyes. Two deep learning models were compared based on whether the NFL reflectance map was used as part of the input or not. Results: The hybrid deep learning model with reflectance achieved 0.909 sensitivity at 99% specificity and 0.926 at 95%. The overall accuracy was 0.948 with 0.893 sensitivity and 1.000 specificity, and the AROC was 0.979, which is significantly better than the logistic regression models (p < 0.001). The second best model is the hybrid deep learning model w/o reflectance, which also had significantly higher AROC than logistic regression models (p < 0.001). Logistic regression with reflectance model had slightly higher AROC or sensitivity than the other logistic regression model without reflectance (p = 0.024). Conclusions: Hybrid deep learning model significantly improved the diagnostic accuracy, without or without NFL reflectance. Hybrid deep learning model, combining reflectance/NFL thickness/GCC thickness/ONH parameter, may be a practical model for glaucoma screen purposes.","sentences":["Precis: A hybrid deep-learning model combines NFL reflectance and other OCT parameters to improve glaucoma diagnosis.","Objective: To investigate if a deep learning model could be used to combine nerve fiber layer (NFL) reflectance and other OCT parameters for glaucoma diagnosis.","Patients and Methods: This is a prospective observational study where of 106 normal subjects and 164 perimetric glaucoma (PG) patients.","Peripapillary NFL reflectance map, NFL thickness map, optic head analysis of disc, and macular ganglion cell complex thickness were obtained using spectral domain OCT.","A hybrid deep learning model combined a fully connected network (FCN) and a convolution neural network (CNN) to develop and combine those OCT maps and parameters to distinguish normal and PG eyes.","Two deep learning models were compared based on whether the NFL reflectance map was used as part of the input or not.","Results:","The hybrid deep learning model with reflectance achieved 0.909 sensitivity at 99% specificity and 0.926 at 95%.","The overall accuracy was 0.948 with 0.893 sensitivity and 1.000 specificity, and the AROC was 0.979, which is significantly better than the logistic regression models (p < 0.001).","The second best model is the hybrid deep learning model w/o reflectance, which also had significantly higher AROC than logistic regression models (p < 0.001).","Logistic regression with reflectance model had slightly higher AROC or sensitivity than the other logistic regression model without reflectance (p = 0.024).","Conclusions: Hybrid deep learning model significantly improved the diagnostic accuracy, without or without NFL reflectance.","Hybrid deep learning model, combining reflectance/NFL thickness/GCC thickness/ONH parameter, may be a practical model for glaucoma screen purposes."],"url":"http://arxiv.org/abs/2406.03663v1","category":"eess.IV"}
{"created":"2024-06-06 00:28:49","title":"The Missing Curve Detectors of InceptionV1: Applying Sparse Autoencoders to InceptionV1 Early Vision","abstract":"Recent work on sparse autoencoders (SAEs) has shown promise in extracting interpretable features from neural networks and addressing challenges with polysemantic neurons caused by superposition. In this paper, we apply SAEs to the early vision layers of InceptionV1, a well-studied convolutional neural network, with a focus on curve detectors. Our results demonstrate that SAEs can uncover new interpretable features not apparent from examining individual neurons, including additional curve detectors that fill in previous gaps. We also find that SAEs can decompose some polysemantic neurons into more monosemantic constituent features. These findings suggest SAEs are a valuable tool for understanding InceptionV1, and convolutional neural networks more generally.","sentences":["Recent work on sparse autoencoders (SAEs) has shown promise in extracting interpretable features from neural networks and addressing challenges with polysemantic neurons caused by superposition.","In this paper, we apply SAEs to the early vision layers of InceptionV1, a well-studied convolutional neural network, with a focus on curve detectors.","Our results demonstrate that SAEs can uncover new interpretable features not apparent from examining individual neurons, including additional curve detectors that fill in previous gaps.","We also find that SAEs can decompose some polysemantic neurons into more monosemantic constituent features.","These findings suggest SAEs are a valuable tool for understanding InceptionV1, and convolutional neural networks more generally."],"url":"http://arxiv.org/abs/2406.03662v1","category":"cs.LG"}
{"created":"2024-06-06 00:03:16","title":"Applications of Deep Learning parameterization of Ocean Momentum Forcing","abstract":"Mesoscale eddies are of utmost importance in understanding ocean dynamics and the transport of heat, salt, and nutrients. Accurate representation of these eddies in ocean models is essential for improving model predictions. However, accurately representing these mesoscale features in numerical models is challenging due to their relatively small size. In this study, we propose a convolutional neural network (CNN) that combines data-driven techniques with physical principles to develop a robust and interpretable parameterization scheme for mesoscale eddies in ocean modeling. We first analyze a high-resolution reanalysis dataset to extract subgrid eddy momentum and use machine learning algorithms to identify patterns and correlations. To ensure physical consistency, we have introduced conservation of momentum constraints in our CNN parameterization scheme through soft and hard constraints. The interpretability analysis illustrate that the pre-trained CNN parameterization shows promising results in accurately solving the resolved mean velocity at the local scale and effectively capturing the representation of unresolved subgrid turbulence processes at the global scale. Furthermore, to validate the CNN parameterization scheme offline, we conduct simulations using the MITgcm ocean model. A series of experiments is conducted to compare the performance of the model with the CNN parameterization scheme and high-resolution simulations. The offline validation using MITgcm simulations demonstrates the effectiveness of the CNN parameterization scheme in improving the representation of mesoscale eddies in the ocean model. Incorporating the CNN parameterization scheme leads to better agreement with high-resolution simulations and a more accurate representation of the kinetic energy spectra.","sentences":["Mesoscale eddies are of utmost importance in understanding ocean dynamics and the transport of heat, salt, and nutrients.","Accurate representation of these eddies in ocean models is essential for improving model predictions.","However, accurately representing these mesoscale features in numerical models is challenging due to their relatively small size.","In this study, we propose a convolutional neural network (CNN) that combines data-driven techniques with physical principles to develop a robust and interpretable parameterization scheme for mesoscale eddies in ocean modeling.","We first analyze a high-resolution reanalysis dataset to extract subgrid eddy momentum and use machine learning algorithms to identify patterns and correlations.","To ensure physical consistency, we have introduced conservation of momentum constraints in our CNN parameterization scheme through soft and hard constraints.","The interpretability analysis illustrate that the pre-trained CNN parameterization shows promising results in accurately solving the resolved mean velocity at the local scale and effectively capturing the representation of unresolved subgrid turbulence processes at the global scale.","Furthermore, to validate the CNN parameterization scheme offline, we conduct simulations using the MITgcm ocean model.","A series of experiments is conducted to compare the performance of the model with the CNN parameterization scheme and high-resolution simulations.","The offline validation using MITgcm simulations demonstrates the effectiveness of the CNN parameterization scheme in improving the representation of mesoscale eddies in the ocean model.","Incorporating the CNN parameterization scheme leads to better agreement with high-resolution simulations and a more accurate representation of the kinetic energy spectra."],"url":"http://arxiv.org/abs/2406.03659v1","category":"physics.flu-dyn"}
{"created":"2024-06-05 23:01:24","title":"Impact of nuclear matter properties on the nucleosynthesis and the kilonova from binary neutron star merger ejecta","abstract":"Material expelled from binary neutron star (BNS) mergers can harbor r-process nucleosynthesis and power a Kilonova (KN), both intimately related to the astrophysical conditions of the ejection. In turn such conditions indirectly depend on the equation of state (EOS) describing matter inside the neutron star. Therefore, in principle the above observables can hold valuable information on nuclear matter, as the merger gravitational wave signal already does. In this work, we consider the outcome of a set of BNS merger simulations employing different finite-temperature nuclear EOSs. The latter are obtained from a Skyrme-type interaction model where nuclear properties, such as the incompressibility and the nucleon effective mass at saturation density, are systematically varied. We post-process the ejecta using a reaction network coupled with a semi-analytic KN model, to asses the sensitivity on the input EOS of the final yields and the KN light curves. Both of them are found to be non-trivially influenced by the EOS, with the overall outcome being dominated by the heterogeneous outflows from the remnant disk, hosting a variable degree of neutron-rich material. The dynamical ejecta can be more directly related to the EOS parameters considered, however, we find its role in the yields production and the KN emission too entangled with the other ejecta components, in order to infer solid correlations. This result highlights the strong degeneracy that intervenes between the merger outcome and the behaviour of the intrinsic nuclear matter, and places itself as a limit to the employment of EOS-constraining approaches of such kind.","sentences":["Material expelled from binary neutron star (BNS) mergers can harbor r-process nucleosynthesis and power a Kilonova (KN), both intimately related to the astrophysical conditions of the ejection.","In turn such conditions indirectly depend on the equation of state (EOS) describing matter inside the neutron star.","Therefore, in principle the above observables can hold valuable information on nuclear matter, as the merger gravitational wave signal already does.","In this work, we consider the outcome of a set of BNS merger simulations employing different finite-temperature nuclear EOSs.","The latter are obtained from a Skyrme-type interaction model where nuclear properties, such as the incompressibility and the nucleon effective mass at saturation density, are systematically varied.","We post-process the ejecta using a reaction network coupled with a semi-analytic KN model, to asses the sensitivity on the input EOS of the final yields and the KN light curves.","Both of them are found to be non-trivially influenced by the EOS, with the overall outcome being dominated by the heterogeneous outflows from the remnant disk, hosting a variable degree of neutron-rich material.","The dynamical ejecta can be more directly related to the EOS parameters considered, however, we find its role in the yields production and the KN emission too entangled with the other ejecta components, in order to infer solid correlations.","This result highlights the strong degeneracy that intervenes between the merger outcome and the behaviour of the intrinsic nuclear matter, and places itself as a limit to the employment of EOS-constraining approaches of such kind."],"url":"http://arxiv.org/abs/2406.03649v1","category":"astro-ph.HE"}
{"created":"2024-06-05 22:49:30","title":"Partial Label Learning with Focal Loss for Sea Ice Classification Based on Ice Charts","abstract":"Sea ice, crucial to the Arctic and Earth's climate, requires consistent monitoring and high-resolution mapping. Manual sea ice mapping, however, is time-consuming and subjective, prompting the need for automated deep learning-based classification approaches. However, training these algorithms is challenging because expert-generated ice charts, commonly used as training data, do not map single ice types but instead map polygons with multiple ice types. Moreover, the distribution of various ice types in these charts is frequently imbalanced, resulting in a performance bias towards the dominant class. In this paper, we present a novel GeoAI approach to training sea ice classification by formalizing it as a partial label learning task with explicit confidence scores to address multiple labels and class imbalance. We treat the polygon-level labels as candidate partial labels, assign the corresponding ice concentrations as confidence scores to each candidate label, and integrate them with focal loss to train a Convolutional Neural Network (CNN). Our proposed approach leads to enhanced performance for sea ice classification in Sentinel-1 dual-polarized SAR images, improving classification accuracy (from 87% to 92%) and weighted average F-1 score (from 90% to 93%) compared to the conventional training approach of using one-hot encoded labels and Categorical Cross-Entropy loss. It also improves the F-1 score in 4 out of the 6 sea ice classes.","sentences":["Sea ice, crucial to the Arctic and Earth's climate, requires consistent monitoring and high-resolution mapping.","Manual sea ice mapping, however, is time-consuming and subjective, prompting the need for automated deep learning-based classification approaches.","However, training these algorithms is challenging because expert-generated ice charts, commonly used as training data, do not map single ice types but instead map polygons with multiple ice types.","Moreover, the distribution of various ice types in these charts is frequently imbalanced, resulting in a performance bias towards the dominant class.","In this paper, we present a novel GeoAI approach to training sea ice classification by formalizing it as a partial label learning task with explicit confidence scores to address multiple labels and class imbalance.","We treat the polygon-level labels as candidate partial labels, assign the corresponding ice concentrations as confidence scores to each candidate label, and integrate them with focal loss to train a Convolutional Neural Network (CNN).","Our proposed approach leads to enhanced performance for sea ice classification in Sentinel-1 dual-polarized SAR images, improving classification accuracy (from 87% to 92%) and weighted average F-1 score (from 90% to 93%) compared to the conventional training approach of using one-hot encoded labels and Categorical Cross-Entropy loss.","It also improves the F-1 score in 4 out of the 6 sea ice classes."],"url":"http://arxiv.org/abs/2406.03645v1","category":"cs.CV"}
{"created":"2024-06-05 22:23:32","title":"Gravitating vortices and Symplectic Reduction by Stages","abstract":"We undertake a novel approach to the existence problem for gravitating vortices on a Riemann surface based on symplectic reduction by stages, which seems to be new in the PDE as well as the gauge theory literature. The main technical tool for our study is the reduced $\\alpha$-K-energy, for which we establish convexity properties by means of finite-energy pluripotential theory, as recently applied to the study of constant scalar curvature K\\\"ahler metrics. Using these methods, we prove that the existence of solutions to the gravitating vortex equations on the sphere implies the polystability of the effective divisor defined by the zeroes of the Higgs field. This approach also enables us to establish the uniqueness of gravitating vortices in any admissible K\\\"ahler class, in the absence of automorphisms. Lastly, we also prove the existence of solutions for the gravitating vortex equations for genus $g\\geq 1$ for certain ranges of the coupling constant $\\alpha$ and the volume.","sentences":["We undertake a novel approach to the existence problem for gravitating vortices on a Riemann surface based on symplectic reduction by stages, which seems to be new in the PDE as well as the gauge theory literature.","The main technical tool for our study is the reduced $\\alpha$-K-energy, for which we establish convexity properties by means of finite-energy pluripotential theory, as recently applied to the study of constant scalar curvature K\\\"ahler metrics.","Using these methods, we prove that the existence of solutions to the gravitating vortex equations on the sphere implies the polystability of the effective divisor defined by the zeroes of the Higgs field.","This approach also enables us to establish the uniqueness of gravitating vortices in any admissible K\\\"ahler class, in the absence of automorphisms.","Lastly, we also prove the existence of solutions for the gravitating vortex equations for genus $g\\geq 1$ for certain ranges of the coupling constant $\\alpha$ and the volume."],"url":"http://arxiv.org/abs/2406.03639v1","category":"math.DG"}
{"created":"2024-06-05 22:10:54","title":"Approximating partial differential equations without boundary conditions","abstract":"We consider the problem of numerically approximating the solutions to an elliptic partial differential equation (PDE) for which the boundary conditions are lacking. To alleviate this missing information, we assume to be given measurement functionals of the solution. In this context, a near optimal recovery algorithm based on the approximation of the Riesz representers of these functionals in some intermediate Hilbert spaces is proposed and analyzed in [Binev et al. 2024]. Inherent to this algorithm is the computation of $H^s$, $s>1/2$, inner products on the boundary of the computational domain. We take advantage of techniques borrowed from the analysis of fractional diffusion problems to design and analyze a fully practical near optimal algorithm not relying on the challenging computation of $H^s$ inner products.","sentences":["We consider the problem of numerically approximating the solutions to an elliptic partial differential equation (PDE) for which the boundary conditions are lacking.","To alleviate this missing information, we assume to be given measurement functionals of the solution.","In this context, a near optimal recovery algorithm based on the approximation of the Riesz representers of these functionals in some intermediate Hilbert spaces is proposed and analyzed in [Binev et al. 2024].","Inherent to this algorithm is the computation of $H^s$, $s>1/2$, inner products on the boundary of the computational domain.","We take advantage of techniques borrowed from the analysis of fractional diffusion problems to design and analyze a fully practical near optimal algorithm not relying on the challenging computation of $H^s$ inner products."],"url":"http://arxiv.org/abs/2406.03634v1","category":"math.NA"}
{"created":"2024-06-05 20:50:00","title":"The fundamental functions of the canonical basis of Hardy spaces of Dirichlet series","abstract":"Given a frequency $\\lambda=(\\lambda_n)$, we consider the Hardy spaces $ \\mathcal{H}_p^\\lambda$ of $\\lambda$-Dirichlet series $ D = \\sum_n a_n e^{-\\lambda_n s}$ and study the asymptotic behavior of the upper and lower democracy functions of its canonical basis $\\mathcal B=\\{e^{-\\lambda_ns}\\}$. For the ordinary case, $\\mathcal B=\\{n^{-s}\\}$, we give the correct asymptotic behavior of all such functions, while in the general case we give sharp lower and upper bounds for all possible behaviors. Moreover, for $p>2$ we present examples showing that any intermediate behavior (between the extreme bounds) can occur. We also study how different properties of the frequency $\\lambda$ lead to particular behaviors of the corresponding fundamental functions. Finally, we apply our results to analyze greedy-type properties of $\\mathcal B=\\{e^{-\\lambda_ns}\\}$ for some particular $\\lambda$'s.","sentences":["Given a frequency $\\lambda=(\\lambda_n)$, we consider the Hardy spaces $ \\mathcal{H}_p^\\lambda$ of $\\lambda$-Dirichlet series $ D = \\sum_n a_n","e^{-\\lambda_n s}$ and study the asymptotic behavior of the upper and lower democracy functions of its canonical basis $\\mathcal B=\\{e^{-\\lambda_ns}\\}$. For the ordinary case, $\\mathcal B=\\{n^{-s}\\}$, we give the correct asymptotic behavior of all such functions, while in the general case we give sharp lower and upper bounds for all possible behaviors.","Moreover, for $p>2$ we present examples showing that any intermediate behavior (between the extreme bounds) can occur.","We also study how different properties of the frequency $\\lambda$ lead to particular behaviors of the corresponding fundamental functions.","Finally, we apply our results to analyze greedy-type properties of $\\mathcal B=\\{e^{-\\lambda_ns}\\}$ for some particular $\\lambda$'s."],"url":"http://arxiv.org/abs/2406.03623v1","category":"math.FA"}
{"created":"2024-06-05 20:43:05","title":"Private Online Learning via Lazy Algorithms","abstract":"We study the problem of private online learning, specifically, online prediction from experts (OPE) and online convex optimization (OCO). We propose a new transformation that transforms lazy online learning algorithms into private algorithms. We apply our transformation for differentially private OPE and OCO using existing lazy algorithms for these problems. Our final algorithms obtain regret, which significantly improves the regret in the high privacy regime $\\varepsilon \\ll 1$, obtaining $\\sqrt{T \\log d} + T^{1/3} \\log(d)/\\varepsilon^{2/3}$ for DP-OPE and $\\sqrt{T} + T^{1/3} \\sqrt{d}/\\varepsilon^{2/3}$ for DP-OCO. We also complement our results with a lower bound for DP-OPE, showing that these rates are optimal for a natural family of low-switching private algorithms.","sentences":["We study the problem of private online learning, specifically, online prediction from experts (OPE) and online convex optimization (OCO).","We propose a new transformation that transforms lazy online learning algorithms into private algorithms.","We apply our transformation for differentially private OPE and OCO using existing lazy algorithms for these problems.","Our final algorithms obtain regret, which significantly improves the regret in the high privacy regime $\\varepsilon \\ll 1$, obtaining $\\sqrt{T \\log d} + T^{1/3} \\log(d)/\\varepsilon^{2/3}$ for DP-OPE and $\\sqrt{T} + T^{1/3} \\sqrt{d}/\\varepsilon^{2/3}$ for DP-OCO.","We also complement our results with a lower bound for DP-OPE, showing that these rates are optimal for a natural family of low-switching private algorithms."],"url":"http://arxiv.org/abs/2406.03620v1","category":"cs.LG"}
{"created":"2024-06-05 20:19:09","title":"Advancing Anomaly Detection: Non-Semantic Financial Data Encoding with LLMs","abstract":"Detecting anomalies in general ledger data is of utmost importance to ensure trustworthiness of financial records. Financial audits increasingly rely on machine learning (ML) algorithms to identify irregular or potentially fraudulent journal entries, each characterized by a varying number of transactions. In machine learning, heterogeneity in feature dimensions adds significant complexity to data analysis. In this paper, we introduce a novel approach to anomaly detection in financial data using Large Language Models (LLMs) embeddings. To encode non-semantic categorical data from real-world financial records, we tested 3 pre-trained general purpose sentence-transformer models. For the downstream classification task, we implemented and evaluated 5 optimized ML models including Logistic Regression, Random Forest, Gradient Boosting Machines, Support Vector Machines, and Neural Networks. Our experiments demonstrate that LLMs contribute valuable information to anomaly detection as our models outperform the baselines, in selected settings even by a large margin. The findings further underscore the effectiveness of LLMs in enhancing anomaly detection in financial journal entries, particularly by tackling feature sparsity. We discuss a promising perspective on using LLM embeddings for non-semantic data in the financial context and beyond.","sentences":["Detecting anomalies in general ledger data is of utmost importance to ensure trustworthiness of financial records.","Financial audits increasingly rely on machine learning (ML) algorithms to identify irregular or potentially fraudulent journal entries, each characterized by a varying number of transactions.","In machine learning, heterogeneity in feature dimensions adds significant complexity to data analysis.","In this paper, we introduce a novel approach to anomaly detection in financial data using Large Language Models (LLMs) embeddings.","To encode non-semantic categorical data from real-world financial records, we tested 3 pre-trained general purpose sentence-transformer models.","For the downstream classification task, we implemented and evaluated 5 optimized ML models including Logistic Regression, Random Forest, Gradient Boosting Machines, Support Vector Machines, and Neural Networks.","Our experiments demonstrate that LLMs contribute valuable information to anomaly detection as our models outperform the baselines, in selected settings even by a large margin.","The findings further underscore the effectiveness of LLMs in enhancing anomaly detection in financial journal entries, particularly by tackling feature sparsity.","We discuss a promising perspective on using LLM embeddings for non-semantic data in the financial context and beyond."],"url":"http://arxiv.org/abs/2406.03614v1","category":"cs.LG"}
{"created":"2024-06-05 19:19:55","title":"Quantum Mechanics of Particles Constrained to Spiral Curves with Application to Polyene Chains","abstract":"Context: Due to advances in synthesizing lower dimensional materials there is the challenge of finding the wave equation that effectively describes quantum particles moving on 1D and 2D domains. Jensen and Koppe and Da Costa independently introduced a confining potential formalism showing that the effective constrained dynamics is subjected to a scalar geometry-induced potential; for the confinement to a curve, the potential depends on the curve's curvature function.   Method: To characterize the $\\pi$ electrons in polyenes, we follow two approaches. First, we utilize a weakened Coulomb potential associated with a spiral curve. The solution to the Schr\\\"{o}dinger equation with Dirichlet boundary conditions yields Bessel functions, and the spectrum is obtained analytically. We employ the particle-in-a-box model in the second approach, incorporating effective mass corrections. The $\\pi$-$\\pi^*$ transitions of polyenes were calculated in good experimental agreement with both approaches, although with different wave functions.","sentences":["Context: Due to advances in synthesizing lower dimensional materials there is the challenge of finding the wave equation that effectively describes quantum particles moving on 1D and 2D domains.","Jensen and Koppe and Da Costa independently introduced a confining potential formalism showing that the effective constrained dynamics is subjected to a scalar geometry-induced potential; for the confinement to a curve, the potential depends on the curve's curvature function.   ","Method: To characterize the $\\pi$ electrons in polyenes, we follow two approaches.","First, we utilize a weakened Coulomb potential associated with a spiral curve.","The solution to the Schr\\\"{o}dinger equation with Dirichlet boundary conditions yields Bessel functions, and the spectrum is obtained analytically.","We employ the particle-in-a-box model in the second approach, incorporating effective mass corrections.","The $\\pi$-$\\pi^*$ transitions of polyenes were calculated in good experimental agreement with both approaches, although with different wave functions."],"url":"http://arxiv.org/abs/2406.03590v1","category":"math-ph"}
{"created":"2024-06-05 19:12:16","title":"Novel Casimir wormholes in Einstein gravity","abstract":"In the context of General Relativity (GR), violation of the null energy condition (NEC) is necessary for existence of static spherically symmetric wormhole solutions. Also, it is a well-known fact that the energy conditions are violated by certain quantum fields, such as the Casimir effect. The magnitude and sign of the Casimir energy depend on Dirichlet or Neumann boundary conditions and geometrical configuration of the objects involved in a Casimir setup. The Casimir energy may act as an ideal candidate for the matter that supports the wormhole geometry. In the present work, we firstly find traversable wormhole solutions supported by a general form for the Casimir energy density assuming a constant redshift function. As well, in this framework, assuming that the radial pressure and energy density obey a linear equation of state, we derive for the first time Casimir traversable wormhole solutions admitting suitable shape function. Then, we consider three geometric configurations of the Casimir effect such as (i) two parallel plates, (ii) two parallel cylindrical shells, and (iii) two spheres. We study wormhole solutions for each case and their property in detail. We also check the weak and strong energy conditions in the spacetime for the obtained wormhole solutions. The stability of the Casimir traversable wormhole solutions are investigated using the Tolman-Oppenheimer-Volkoff (TOV) equation. Finally, we study trajectory of null as well as timelike particles in the wormhole spacetime.","sentences":["In the context of General Relativity (GR), violation of the null energy condition (NEC) is necessary for existence of static spherically symmetric wormhole solutions.","Also, it is a well-known fact that the energy conditions are violated by certain quantum fields, such as the Casimir effect.","The magnitude and sign of the Casimir energy depend on Dirichlet or Neumann boundary conditions and geometrical configuration of the objects involved in a Casimir setup.","The Casimir energy may act as an ideal candidate for the matter that supports the wormhole geometry.","In the present work, we firstly find traversable wormhole solutions supported by a general form for the Casimir energy density assuming a constant redshift function.","As well, in this framework, assuming that the radial pressure and energy density obey a linear equation of state, we derive for the first time Casimir traversable wormhole solutions admitting suitable shape function.","Then, we consider three geometric configurations of the Casimir effect such as (i) two parallel plates, (ii) two parallel cylindrical shells, and (iii) two spheres.","We study wormhole solutions for each case and their property in detail.","We also check the weak and strong energy conditions in the spacetime for the obtained wormhole solutions.","The stability of the Casimir traversable wormhole solutions are investigated using the Tolman-Oppenheimer-Volkoff (TOV) equation.","Finally, we study trajectory of null as well as timelike particles in the wormhole spacetime."],"url":"http://arxiv.org/abs/2406.03588v1","category":"gr-qc"}
{"created":"2024-06-05 19:01:07","title":"On the automorphism groups of smooth Fano threefolds","abstract":"Let $\\mathcal{X}$ be a smooth Fano threefold over the complex numbers of Picard rank $1$ with finite automorphism group. We give numerical restrictions on the order of the automorphism group $\\mathrm{Aut}(\\mathcal{X})$ provided the genus $g(\\mathcal{X})\\leq 10$ and $\\mathcal{X}$ is not an ordinary smooth Gushel-Mukai threefold. More precisely, we show that the order $|\\mathrm{Aut}(\\mathcal{X})|$ divides a certain explicit number depending on the genus of $\\mathcal{X}$. We use a classification of Fano threefolds in terms of complete intersections in homogeneous varieties and the previous paper of A. Gorinov and the author regarding the topology of spaces of regular sections.","sentences":["Let $\\mathcal{X}$ be a smooth Fano threefold over the complex numbers of Picard rank $1$ with finite automorphism group.","We give numerical restrictions on the order of the automorphism group $\\mathrm{Aut}(\\mathcal{X})$ provided the genus $g(\\mathcal{X})\\leq 10$ and $\\mathcal{X}$ is not an ordinary smooth Gushel-Mukai threefold.","More precisely, we show that the order $|\\mathrm{Aut}(\\mathcal{X})|$ divides a certain explicit number depending on the genus of $\\mathcal{X}$. We use a classification of Fano threefolds in terms of complete intersections in homogeneous varieties and the previous paper of A. Gorinov and the author regarding the topology of spaces of regular sections."],"url":"http://arxiv.org/abs/2406.03584v1","category":"math.AG"}
{"created":"2024-06-05 18:54:45","title":"Charm total cross sections with nonuniversal fragmentation treatment","abstract":"Total charm-pair cross sections in $pp$ collisions are interesting because they can be calculated to NNLO in QCD without any reference to fragmentation effects. On the other hand, the fiducial differential charm cross sections from which the total cross sections must be extrapolated are currently known to NLO+NLL at most (e.g. FONLL), and must be treated for known effects of nonuniversal charm fragmentation. A new procedure using the FONLL framework as input for an empirical parametrization of the data in both shape and normalization, with all its parameters actually fitted to data, is used to derive so-called data-driven FONLL (ddFONLL) parametrizations which can be used to extrapolate the differential cross sections to total cross sections with minimal bias. This includes an empirical treatment of all known non-universal charm fragmentation effects, in particular for the baryon-to-meson ratio as a function of transverse momentum. The total charm-pair cross sections obtained in this way, which supersede all previous ones obtained using the assumption of charm-fragmentation universality, are consistent with NNLO predictions, and allow first studies of their sensitivity e.g. to the charm-quark mass and/or the NNLO gluon PDF at very low transverse momentum fractions $x$.","sentences":["Total charm-pair cross sections in $pp$ collisions are interesting because they can be calculated to NNLO in QCD without any reference to fragmentation effects.","On the other hand, the fiducial differential charm cross sections from which the total cross sections must be extrapolated are currently known to NLO+NLL at most (e.g. FONLL), and must be treated for known effects of nonuniversal charm fragmentation.","A new procedure using the FONLL framework as input for an empirical parametrization of the data in both shape and normalization, with all its parameters actually fitted to data, is used to derive so-called data-driven FONLL (ddFONLL) parametrizations which can be used to extrapolate the differential cross sections to total cross sections with minimal bias.","This includes an empirical treatment of all known non-universal charm fragmentation effects, in particular for the baryon-to-meson ratio as a function of transverse momentum.","The total charm-pair cross sections obtained in this way, which supersede all previous ones obtained using the assumption of charm-fragmentation universality, are consistent with NNLO predictions, and allow first studies of their sensitivity e.g. to the charm-quark mass and/or the NNLO gluon PDF at very low transverse momentum fractions $x$."],"url":"http://arxiv.org/abs/2406.03581v1","category":"hep-ph"}
{"created":"2024-06-05 18:31:37","title":"GFN: A graph feedforward network for resolution-invariant reduced operator learning in multifidelity applications","abstract":"This work presents a novel resolution-invariant model order reduction strategy for multifidelity applications. We base our architecture on a novel neural network layer developed in this work, the graph feedforward network, which extends the concept of feedforward networks to graph-structured data by creating a direct link between the weights of a neural network and the nodes of a mesh, enhancing the interpretability of the network. We exploit the method's capability of training and testing on different mesh sizes in an autoencoder-based reduction strategy for parametrised partial differential equations. We show that this extension comes with provable guarantees on the performance via error bounds. The capabilities of the proposed methodology are tested on three challenging benchmarks, including advection-dominated phenomena and problems with a high-dimensional parameter space. The method results in a more lightweight and highly flexible strategy when compared to state-of-the-art models, while showing excellent generalisation performance in both single fidelity and multifidelity scenarios.","sentences":["This work presents a novel resolution-invariant model order reduction strategy for multifidelity applications.","We base our architecture on a novel neural network layer developed in this work, the graph feedforward network, which extends the concept of feedforward networks to graph-structured data by creating a direct link between the weights of a neural network and the nodes of a mesh, enhancing the interpretability of the network.","We exploit the method's capability of training and testing on different mesh sizes in an autoencoder-based reduction strategy for parametrised partial differential equations.","We show that this extension comes with provable guarantees on the performance via error bounds.","The capabilities of the proposed methodology are tested on three challenging benchmarks, including advection-dominated phenomena and problems with a high-dimensional parameter space.","The method results in a more lightweight and highly flexible strategy when compared to state-of-the-art models, while showing excellent generalisation performance in both single fidelity and multifidelity scenarios."],"url":"http://arxiv.org/abs/2406.03569v1","category":"math.NA"}
{"created":"2024-06-05 18:23:05","title":"Equivariant Graph Neural Networks for Prediction of Tensor Material Properties of Crystals","abstract":"Traditional machine learning methods applied to the material sciences have often predicted invariant, scalar properties of material systems to great effect. Newer, coordinate equivariant models promise to provide a coordinate system dependent output in a well defined manner, but recent applications often neglect a direct prediction of directional (i.e. coordinate system dependent) quantities and instead are used to predict still just invariant quantities. This component-wise prediction of tensorial properties is achieved by decomposing tensors into harmonic subspaces via a \\textit{tensor spherical harmonic decomposition}, by which we may also associate arbitrary tensors with the irreducible representations of the rotation group. This essentially allows us to read off tensors component-wise from the output representations of these equivariant models. In this work, we present results for the prediction of various material property tensors directly from crystalline structures. Namely, given some material's crystalline structure, we may predict tensor components of dielectric, piezoelectric, and elasticity tensors directly from the output of a $SE(3)$ equivariant model.","sentences":["Traditional machine learning methods applied to the material sciences have often predicted invariant, scalar properties of material systems to great effect.","Newer, coordinate equivariant models promise to provide a coordinate system dependent output in a well defined manner, but recent applications often neglect a direct prediction of directional (i.e. coordinate system dependent) quantities and instead are used to predict still just invariant quantities.","This component-wise prediction of tensorial properties is achieved by decomposing tensors into harmonic subspaces via a \\textit{tensor spherical harmonic decomposition}, by which we may also associate arbitrary tensors with the irreducible representations of the rotation group.","This essentially allows us to read off tensors component-wise from the output representations of these equivariant models.","In this work, we present results for the prediction of various material property tensors directly from crystalline structures.","Namely, given some material's crystalline structure, we may predict tensor components of dielectric, piezoelectric, and elasticity tensors directly from the output of a $SE(3)$ equivariant model."],"url":"http://arxiv.org/abs/2406.03563v1","category":"physics.comp-ph"}
{"created":"2024-06-05 18:17:33","title":"Neural empirical interpolation method for nonlinear model reduction","abstract":"In this paper, we introduce the neural empirical interpolation method (NEIM), a neural network-based alternative to the discrete empirical interpolation method for reducing the time complexity of computing the nonlinear term in a reduced order model (ROM) for a parameterized nonlinear partial differential equation. NEIM is a greedy algorithm which accomplishes this reduction by approximating an affine decomposition of the nonlinear term of the ROM, where the vector terms of the expansion are given by neural networks depending on the ROM solution, and the coefficients are given by an interpolation of some \"optimal\" coefficients. Because NEIM is based on a greedy strategy, we are able to provide a basic error analysis to investigate its performance. NEIM has the advantages of being easy to implement in models with automatic differentiation, of being a nonlinear projection of the ROM nonlinearity, of being efficient for both nonlocal and local nonlinearities, and of relying solely on data and not the explicit form of the ROM nonlinearity. We demonstrate the effectiveness of the methodology on solution-dependent and solution-independent nonlinearities, a nonlinear elliptic problem, and a nonlinear parabolic model of liquid crystals.","sentences":["In this paper, we introduce the neural empirical interpolation method (NEIM), a neural network-based alternative to the discrete empirical interpolation method for reducing the time complexity of computing the nonlinear term in a reduced order model (ROM) for a parameterized nonlinear partial differential equation.","NEIM is a greedy algorithm which accomplishes this reduction by approximating an affine decomposition of the nonlinear term of the ROM, where the vector terms of the expansion are given by neural networks depending on the ROM solution, and the coefficients are given by an interpolation of some \"optimal\" coefficients.","Because NEIM is based on a greedy strategy, we are able to provide a basic error analysis to investigate its performance.","NEIM has the advantages of being easy to implement in models with automatic differentiation, of being a nonlinear projection of the ROM nonlinearity, of being efficient for both nonlocal and local nonlinearities, and of relying solely on data and not the explicit form of the ROM nonlinearity.","We demonstrate the effectiveness of the methodology on solution-dependent and solution-independent nonlinearities, a nonlinear elliptic problem, and a nonlinear parabolic model of liquid crystals."],"url":"http://arxiv.org/abs/2406.03562v1","category":"math.NA"}
{"created":"2024-06-05 18:00:17","title":"BPS complexes and Chern--Simons theories from $G$-structures in gauge theory and gravity","abstract":"We consider a variety of physical systems in which one has states that can be thought of as generalised instantons. These include Yang--Mills theories on manifolds with a torsion-free $G$-structure, analogous gravitational instantons and certain supersymmetric solutions of ten-dimensional supergravity, using their formulation as generalised $G$-structures on Courant algebroids. We provide a universal algebraic construction of a complex, which we call the BPS complex, that computes the infinitesimal moduli space of the instanton as one of its cohomologies. We call a class of these spinor type complexes, which are closely connected to supersymmetric systems, and show how their Laplacians have nice properties. In the supergravity context, the BPS complex becomes a double complex, in a way that corresponds to the left- and right-moving sectors of the string, and becomes much like the double complex of $(p,q)$-forms on a K\\\"ahler manifold. If the BPS complex has a symplectic inner product, one can write down an associated linearised BV Chern--Simons theory, which reproduces several classic examples in gauge theory. We discuss applications to (quasi-)topological string theories and heterotic superpotential functionals, whose quadratic parts can also be constructed naturally from the BPS complex.","sentences":["We consider a variety of physical systems in which one has states that can be thought of as generalised instantons.","These include Yang--Mills theories on manifolds with a torsion-free $G$-structure, analogous gravitational instantons and certain supersymmetric solutions of ten-dimensional supergravity, using their formulation as generalised $G$-structures on Courant algebroids.","We provide a universal algebraic construction of a complex, which we call the BPS complex, that computes the infinitesimal moduli space of the instanton as one of its cohomologies.","We call a class of these spinor type complexes, which are closely connected to supersymmetric systems, and show how their Laplacians have nice properties.","In the supergravity context, the BPS complex becomes a double complex, in a way that corresponds to the left- and right-moving sectors of the string, and becomes much like the double complex of $(p,q)$-forms on a K\\\"ahler manifold.","If the BPS complex has a symplectic inner product, one can write down an associated linearised BV Chern--Simons theory, which reproduces several classic examples in gauge theory.","We discuss applications to (quasi-)topological string theories and heterotic superpotential functionals, whose quadratic parts can also be constructed naturally from the BPS complex."],"url":"http://arxiv.org/abs/2406.03550v1","category":"hep-th"}
{"created":"2024-06-05 17:41:42","title":"Noise-Aware Algorithm for Heterogeneous Differentially Private Federated Learning","abstract":"High utility and rigorous data privacy are of the main goals of a federated learning (FL) system, which learns a model from the data distributed among some clients. The latter has been tried to achieve by using differential privacy in FL (DPFL). There is often heterogeneity in clients privacy requirements, and existing DPFL works either assume uniform privacy requirements for clients or are not applicable when server is not fully trusted (our setting). Furthermore, there is often heterogeneity in batch and/or dataset size of clients, which as shown, results in extra variation in the DP noise level across clients model updates. With these sources of heterogeneity, straightforward aggregation strategies, e.g., assigning clients aggregation weights proportional to their privacy parameters will lead to lower utility. We propose Robust-HDP, which efficiently estimates the true noise level in clients model updates and reduces the noise-level in the aggregated model updates considerably. Robust-HDP improves utility and convergence speed, while being safe to the clients that may maliciously send falsified privacy parameter to server. Extensive experimental results on multiple datasets and our theoretical analysis confirm the effectiveness of Robust-HDP. Our code can be found here.","sentences":["High utility and rigorous data privacy are of the main goals of a federated learning (FL) system, which learns a model from the data distributed among some clients.","The latter has been tried to achieve by using differential privacy in FL (DPFL).","There is often heterogeneity in clients privacy requirements, and existing DPFL works either assume uniform privacy requirements for clients or are not applicable when server is not fully trusted (our setting).","Furthermore, there is often heterogeneity in batch and/or dataset size of clients, which as shown, results in extra variation in the DP noise level across clients model updates.","With these sources of heterogeneity, straightforward aggregation strategies, e.g., assigning clients aggregation weights proportional to their privacy parameters will lead to lower utility.","We propose Robust-HDP, which efficiently estimates the true noise level in clients model updates and reduces the noise-level in the aggregated model updates considerably.","Robust-HDP improves utility and convergence speed, while being safe to the clients that may maliciously send falsified privacy parameter to server.","Extensive experimental results on multiple datasets and our theoretical analysis confirm the effectiveness of Robust-HDP.","Our code can be found here."],"url":"http://arxiv.org/abs/2406.03519v1","category":"cs.LG"}
{"created":"2024-06-06 17:59:56","title":"Learning 1D Causal Visual Representation with De-focus Attention Networks","abstract":"Modality differences have led to the development of heterogeneous architectures for vision and language models. While images typically require 2D non-causal modeling, texts utilize 1D causal modeling. This distinction poses significant challenges in constructing unified multi-modal models. This paper explores the feasibility of representing images using 1D causal modeling. We identify an \"over-focus\" issue in existing 1D causal vision models, where attention overly concentrates on a small proportion of visual tokens. The issue of \"over-focus\" hinders the model's ability to extract diverse visual features and to receive effective gradients for optimization. To address this, we propose De-focus Attention Networks, which employ learnable bandpass filters to create varied attention patterns. During training, large and scheduled drop path rates, and an auxiliary loss on globally pooled features for global understanding tasks are introduced. These two strategies encourage the model to attend to a broader range of tokens and enhance network optimization. Extensive experiments validate the efficacy of our approach, demonstrating that 1D causal visual representation can perform comparably to 2D non-causal representation in tasks such as global perception, dense prediction, and multi-modal understanding. Code is released at https://github.com/OpenGVLab/De-focus-Attention-Networks.","sentences":["Modality differences have led to the development of heterogeneous architectures for vision and language models.","While images typically require 2D non-causal modeling, texts utilize 1D causal modeling.","This distinction poses significant challenges in constructing unified multi-modal models.","This paper explores the feasibility of representing images using 1D causal modeling.","We identify an \"over-focus\" issue in existing 1D causal vision models, where attention overly concentrates on a small proportion of visual tokens.","The issue of \"over-focus\" hinders the model's ability to extract diverse visual features and to receive effective gradients for optimization.","To address this, we propose De-focus Attention Networks, which employ learnable bandpass filters to create varied attention patterns.","During training, large and scheduled drop path rates, and an auxiliary loss on globally pooled features for global understanding tasks are introduced.","These two strategies encourage the model to attend to a broader range of tokens and enhance network optimization.","Extensive experiments validate the efficacy of our approach, demonstrating that 1D causal visual representation can perform comparably to 2D non-causal representation in tasks such as global perception, dense prediction, and multi-modal understanding.","Code is released at https://github.com/OpenGVLab/De-focus-Attention-Networks."],"url":"http://arxiv.org/abs/2406.04342v1","category":"cs.CV"}
{"created":"2024-06-06 17:59:56","title":"Verbalized Machine Learning: Revisiting Machine Learning with Language Models","abstract":"Motivated by the large progress made by large language models (LLMs), we introduce the framework of verbalized machine learning (VML). In contrast to conventional machine learning models that are typically optimized over a continuous parameter space, VML constrains the parameter space to be human-interpretable natural language. Such a constraint leads to a new perspective of function approximation, where an LLM with a text prompt can be viewed as a function parameterized by the text prompt. Guided by this perspective, we revisit classical machine learning problems, such as regression and classification, and find that these problems can be solved by an LLM-parameterized learner and optimizer. The major advantages of VML include (1) easy encoding of inductive bias: prior knowledge about the problem and hypothesis class can be encoded in natural language and fed into the LLM-parameterized learner; (2) automatic model class selection: the optimizer can automatically select a concrete model class based on data and verbalized prior knowledge, and it can update the model class during training; and (3) interpretable learner updates: the LLM-parameterized optimizer can provide explanations for why each learner update is performed. We conduct several studies to empirically evaluate the effectiveness of VML, and hope that VML can serve as a stepping stone to stronger interpretability and trustworthiness in ML.","sentences":["Motivated by the large progress made by large language models (LLMs), we introduce the framework of verbalized machine learning (VML).","In contrast to conventional machine learning models that are typically optimized over a continuous parameter space, VML constrains the parameter space to be human-interpretable natural language.","Such a constraint leads to a new perspective of function approximation, where an LLM with a text prompt can be viewed as a function parameterized by the text prompt.","Guided by this perspective, we revisit classical machine learning problems, such as regression and classification, and find that these problems can be solved by an LLM-parameterized learner and optimizer.","The major advantages of VML include (1) easy encoding of inductive bias: prior knowledge about the problem and hypothesis class can be encoded in natural language and fed into the LLM-parameterized learner; (2) automatic model class selection: the optimizer can automatically select a concrete model class based on data and verbalized prior knowledge, and it can update the model class during training; and (3) interpretable learner updates: the LLM-parameterized optimizer can provide explanations for why each learner update is performed.","We conduct several studies to empirically evaluate the effectiveness of VML, and hope that VML can serve as a stepping stone to stronger interpretability and trustworthiness in ML."],"url":"http://arxiv.org/abs/2406.04344v1","category":"cs.LG"}
{"created":"2024-06-06 17:59:23","title":"Coarse-To-Fine Tensor Trains for Compact Visual Representations","abstract":"The ability to learn compact, high-quality, and easy-to-optimize representations for visual data is paramount to many applications such as novel view synthesis and 3D reconstruction. Recent work has shown substantial success in using tensor networks to design such compact and high-quality representations. However, the ability to optimize tensor-based representations, and in particular, the highly compact tensor train representation, is still lacking. This has prevented practitioners from deploying the full potential of tensor networks for visual data. To this end, we propose 'Prolongation Upsampling Tensor Train (PuTT)', a novel method for learning tensor train representations in a coarse-to-fine manner. Our method involves the prolonging or `upsampling' of a learned tensor train representation, creating a sequence of 'coarse-to-fine' tensor trains that are incrementally refined. We evaluate our representation along three axes: (1). compression, (2). denoising capability, and (3). image completion capability. To assess these axes, we consider the tasks of image fitting, 3D fitting, and novel view synthesis, where our method shows an improved performance compared to state-of-the-art tensor-based methods. For full results see our project webpage: https://sebulo.github.io/PuTT_website/","sentences":["The ability to learn compact, high-quality, and easy-to-optimize representations for visual data is paramount to many applications such as novel view synthesis and 3D reconstruction.","Recent work has shown substantial success in using tensor networks to design such compact and high-quality representations.","However, the ability to optimize tensor-based representations, and in particular, the highly compact tensor train representation, is still lacking.","This has prevented practitioners from deploying the full potential of tensor networks for visual data.","To this end, we propose 'Prolongation Upsampling Tensor Train (PuTT)', a novel method for learning tensor train representations in a coarse-to-fine manner.","Our method involves the prolonging or `upsampling' of a learned tensor train representation, creating a sequence of 'coarse-to-fine' tensor trains that are incrementally refined.","We evaluate our representation along three axes: (1).","compression, (2).","denoising capability, and (3).","image completion capability.","To assess these axes, we consider the tasks of image fitting, 3D fitting, and novel view synthesis, where our method shows an improved performance compared to state-of-the-art tensor-based methods.","For full results see our project webpage: https://sebulo.github.io/PuTT_website/"],"url":"http://arxiv.org/abs/2406.04332v1","category":"cs.CV"}
{"created":"2024-06-06 17:48:24","title":"Representational Alignment Supports Effective Machine Teaching","abstract":"A good teacher should not only be knowledgeable; but should be able to communicate in a way that the student understands -- to share the student's representation of the world. In this work, we integrate insights from machine teaching and pragmatic communication with the burgeoning literature on representational alignment to characterize a utility curve defining a relationship between representational alignment and teacher capability for promoting student learning. To explore the characteristics of this utility curve, we design a supervised learning environment that disentangles representational alignment from teacher accuracy. We conduct extensive computational experiments with machines teaching machines, complemented by a series of experiments in which machines teach humans. Drawing on our findings that improved representational alignment with a student improves student learning outcomes (i.e., task accuracy), we design a classroom matching procedure that assigns students to teachers based on the utility curve. If we are to design effective machine teachers, it is not enough to build teachers that are accurate -- we want teachers that can align, representationally, to their students too.","sentences":["A good teacher should not only be knowledgeable; but should be able to communicate in a way that the student understands -- to share the student's representation of the world.","In this work, we integrate insights from machine teaching and pragmatic communication with the burgeoning literature on representational alignment to characterize a utility curve defining a relationship between representational alignment and teacher capability for promoting student learning.","To explore the characteristics of this utility curve, we design a supervised learning environment that disentangles representational alignment from teacher accuracy.","We conduct extensive computational experiments with machines teaching machines, complemented by a series of experiments in which machines teach humans.","Drawing on our findings that improved representational alignment with a student improves student learning outcomes (i.e., task accuracy), we design a classroom matching procedure that assigns students to teachers based on the utility curve.","If we are to design effective machine teachers, it is not enough to build teachers that are accurate -- we want teachers that can align, representationally, to their students too."],"url":"http://arxiv.org/abs/2406.04302v1","category":"cs.LG"}
{"created":"2024-06-06 17:28:56","title":"What is Dataset Distillation Learning?","abstract":"Dataset distillation has emerged as a strategy to overcome the hurdles associated with large datasets by learning a compact set of synthetic data that retains essential information from the original dataset. While distilled data can be used to train high performing models, little is understood about how the information is stored. In this study, we posit and answer three questions about the behavior, representativeness, and point-wise information content of distilled data. We reveal distilled data cannot serve as a substitute for real data during training outside the standard evaluation setting for dataset distillation. Additionally, the distillation process retains high task performance by compressing information related to the early training dynamics of real models. Finally, we provide an framework for interpreting distilled data and reveal that individual distilled data points contain meaningful semantic information. This investigation sheds light on the intricate nature of distilled data, providing a better understanding on how they can be effectively utilized.","sentences":["Dataset distillation has emerged as a strategy to overcome the hurdles associated with large datasets by learning a compact set of synthetic data that retains essential information from the original dataset.","While distilled data can be used to train high performing models, little is understood about how the information is stored.","In this study, we posit and answer three questions about the behavior, representativeness, and point-wise information content of distilled data.","We reveal distilled data cannot serve as a substitute for real data during training outside the standard evaluation setting for dataset distillation.","Additionally, the distillation process retains high task performance by compressing information related to the early training dynamics of real models.","Finally, we provide an framework for interpreting distilled data and reveal that individual distilled data points contain meaningful semantic information.","This investigation sheds light on the intricate nature of distilled data, providing a better understanding on how they can be effectively utilized."],"url":"http://arxiv.org/abs/2406.04284v1","category":"cs.LG"}
{"created":"2024-06-06 16:13:25","title":"Systematic analysis of jellyfish galaxy candidates in Fornax, Antlia, and Hydra from the S-PLUS survey: A self-supervised visual identification aid","abstract":"We study 51 jellyfish galaxy candidates in the Fornax, Antlia, and Hydra clusters. These candidates are identified using the JClass scheme based on the visual classification of wide-field, twelve-band optical images obtained from the Southern Photometric Local Universe Survey. A comprehensive astrophysical analysis of the jellyfish (JClass > 0), non-jellyfish (JClass = 0), and independently organized control samples is undertaken. We develop a semi-automated pipeline using self-supervised learning and similarity search to detect jellyfish galaxies. The proposed framework is designed to assist visual classifiers by providing more reliable JClasses for galaxies. We find that jellyfish candidates exhibit a lower Gini coefficient, higher entropy, and a lower 2D S\\'ersic index as the jellyfish features in these galaxies become more pronounced. Jellyfish candidates show elevated star formation rates (including contributions from the main body and tails) by $\\sim$1.75 dex, suggesting a significant increase in the SFR caused by the ram-pressure stripping phenomenon. Galaxies in the Antlia and Fornax clusters preferentially fall towards the cluster's centre, whereas only a mild preference is observed for Hydra galaxies. Our self-supervised pipeline, applied in visually challenging cases, offers two main advantages: it reduces human visual biases and scales effectively for large datasets. This versatile framework promises substantial enhancements in morphology studies for future galaxy image surveys.","sentences":["We study 51 jellyfish galaxy candidates in the Fornax, Antlia, and Hydra clusters.","These candidates are identified using the JClass scheme based on the visual classification of wide-field, twelve-band optical images obtained from the Southern Photometric Local Universe Survey.","A comprehensive astrophysical analysis of the jellyfish (JClass > 0), non-jellyfish (JClass = 0), and independently organized control samples is undertaken.","We develop a semi-automated pipeline using self-supervised learning and similarity search to detect jellyfish galaxies.","The proposed framework is designed to assist visual classifiers by providing more reliable JClasses for galaxies.","We find that jellyfish candidates exhibit a lower Gini coefficient, higher entropy, and a lower 2D S\\'ersic index as the jellyfish features in these galaxies become more pronounced.","Jellyfish candidates show elevated star formation rates (including contributions from the main body and tails) by $\\sim$1.75 dex, suggesting a significant increase in the SFR caused by the ram-pressure stripping phenomenon.","Galaxies in the Antlia and Fornax clusters preferentially fall towards the cluster's centre, whereas only a mild preference is observed for Hydra galaxies.","Our self-supervised pipeline, applied in visually challenging cases, offers two main advantages: it reduces human visual biases and scales effectively for large datasets.","This versatile framework promises substantial enhancements in morphology studies for future galaxy image surveys."],"url":"http://arxiv.org/abs/2406.04213v1","category":"astro-ph.GA"}
{"created":"2024-06-06 15:17:00","title":"Learned Feature Importance Scores for Automated Feature Engineering","abstract":"Feature engineering has demonstrated substantial utility for many machine learning workflows, such as in the small data regime or when distribution shifts are severe. Thus automating this capability can relieve much manual effort and improve model performance. Towards this, we propose AutoMAN, or Automated Mask-based Feature Engineering, an automated feature engineering framework that achieves high accuracy, low latency, and can be extended to heterogeneous and time-varying data. AutoMAN is based on effectively exploring the candidate transforms space, without explicitly manifesting transformed features. This is achieved by learning feature importance masks, which can be extended to support other modalities such as time series. AutoMAN learns feature transform importance end-to-end, incorporating a dataset's task target directly into feature engineering, resulting in state-of-the-art performance with significantly lower latency compared to alternatives.","sentences":["Feature engineering has demonstrated substantial utility for many machine learning workflows, such as in the small data regime or when distribution shifts are severe.","Thus automating this capability can relieve much manual effort and improve model performance.","Towards this, we propose AutoMAN, or Automated Mask-based Feature Engineering, an automated feature engineering framework that achieves high accuracy, low latency, and can be extended to heterogeneous and time-varying data.","AutoMAN is based on effectively exploring the candidate transforms space, without explicitly manifesting transformed features.","This is achieved by learning feature importance masks, which can be extended to support other modalities such as time series.","AutoMAN learns feature transform importance end-to-end, incorporating a dataset's task target directly into feature engineering, resulting in state-of-the-art performance with significantly lower latency compared to alternatives."],"url":"http://arxiv.org/abs/2406.04153v1","category":"cs.LG"}
{"created":"2024-06-06 15:13:48","title":"Fast Redescription Mining Using Locality-Sensitive Hashing","abstract":"Redescription mining is a data analysis technique that has found applications in diverse fields. The most used redescription mining approaches involve two phases: finding matching pairs among data attributes and extending the pairs. This process is relatively efficient when the number of attributes remains limited and when the attributes are Boolean, but becomes almost intractable when the data consist of many numerical attributes. In this paper, we present new algorithms that perform the matching and extension orders of magnitude faster than the existing approaches. Our algorithms are based on locality-sensitive hashing with a tailored approach to handle the discretisation of numerical attributes as used in redescription mining.","sentences":["Redescription mining is a data analysis technique that has found applications in diverse fields.","The most used redescription mining approaches involve two phases: finding matching pairs among data attributes and extending the pairs.","This process is relatively efficient when the number of attributes remains limited and when the attributes are Boolean, but becomes almost intractable when the data consist of many numerical attributes.","In this paper, we present new algorithms that perform the matching and extension orders of magnitude faster than the existing approaches.","Our algorithms are based on locality-sensitive hashing with a tailored approach to handle the discretisation of numerical attributes as used in redescription mining."],"url":"http://arxiv.org/abs/2406.04148v1","category":"cs.LG"}
{"created":"2024-06-06 13:17:24","title":"Semmeldetector: Application of Machine Learning in Commercial Bakeries","abstract":"The Semmeldetector, is a machine learning application that utilizes object detection models to detect, classify and count baked goods in images. Our application allows commercial bakers to track unsold baked goods, which allows them to optimize production and increase resource efficiency. We compiled a dataset comprising 1151 images that distinguishes between 18 different types of baked goods to train our detection models. To facilitate model training, we used a Copy-Paste augmentation pipeline to expand our dataset. We trained the state-of-the-art object detection model YOLOv8 on our detection task. We tested the impact of different training data, model scale, and online image augmentation pipelines on model performance. Our overall best performing model, achieved an AP@0.5 of 89.1% on our test set. Based on our results, we conclude that machine learning can be a valuable tool even for unforeseen industries like bakeries, even with very limited datasets.","sentences":["The Semmeldetector, is a machine learning application that utilizes object detection models to detect, classify and count baked goods in images.","Our application allows commercial bakers to track unsold baked goods, which allows them to optimize production and increase resource efficiency.","We compiled a dataset comprising 1151 images that distinguishes between 18 different types of baked goods to train our detection models.","To facilitate model training, we used a Copy-Paste augmentation pipeline to expand our dataset.","We trained the state-of-the-art object detection model YOLOv8 on our detection task.","We tested the impact of different training data, model scale, and online image augmentation pipelines on model performance.","Our overall best performing model, achieved an AP@0.5 of 89.1% on our test set.","Based on our results, we conclude that machine learning can be a valuable tool even for unforeseen industries like bakeries, even with very limited datasets."],"url":"http://arxiv.org/abs/2406.04050v1","category":"cs.CV"}
{"created":"2024-06-06 13:04:43","title":"Road Network Representation Learning with the Third Law of Geography","abstract":"Road network representation learning aims to learn compressed and effective vectorized representations for road segments that are applicable to numerous tasks. In this paper, we identify the limitations of existing methods, particularly their overemphasis on the distance effect as outlined in the First Law of Geography. In response, we propose to endow road network representation with the principles of the recent Third Law of Geography. To this end, we propose a novel graph contrastive learning framework that employs geographic configuration-aware graph augmentation and spectral negative sampling, ensuring that road segments with similar geographic configurations yield similar representations, and vice versa, aligning with the principles stated in the Third Law. The framework further fuses the Third Law with the First Law through a dual contrastive learning objective to effectively balance the implications of both laws. We evaluate our framework on two real-world datasets across three downstream tasks. The results show that the integration of the Third Law significantly improves the performance of road segment representations in downstream tasks.","sentences":["Road network representation learning aims to learn compressed and effective vectorized representations for road segments that are applicable to numerous tasks.","In this paper, we identify the limitations of existing methods, particularly their overemphasis on the distance effect as outlined in the First Law of Geography.","In response, we propose to endow road network representation with the principles of the recent Third Law of Geography.","To this end, we propose a novel graph contrastive learning framework that employs geographic configuration-aware graph augmentation and spectral negative sampling, ensuring that road segments with similar geographic configurations yield similar representations, and vice versa, aligning with the principles stated in the Third Law.","The framework further fuses the Third Law with the First Law through a dual contrastive learning objective to effectively balance the implications of both laws.","We evaluate our framework on two real-world datasets across three downstream tasks.","The results show that the integration of the Third Law significantly improves the performance of road segment representations in downstream tasks."],"url":"http://arxiv.org/abs/2406.04038v1","category":"cs.LG"}
{"created":"2024-06-06 12:06:14","title":"Galactic center GeV excess and classification of Fermi-LAT sources with machine learning","abstract":"Excess of gamma rays with a spherical morphology around the Galactic center (GC) observed in the Fermi large area telescope (LAT) data is one of the most intriguing features in the gamma-ray sky. The excess has been interpreted by annihilating dark matter as well as emission from a population of unresolved millisecond pulsars (MSPs). We use a multi-class classification of Fermi-LAT sources with machine learning to study the distribution of MSP-like sources among unassociated Fermi-LAT sources near the GC. We find that the source count distribution of MSP-like sources is comparable with the MSP explanation of the GC excess.","sentences":["Excess of gamma rays with a spherical morphology around the Galactic center (GC) observed in the Fermi large area telescope (LAT) data is one of the most intriguing features in the gamma-ray sky.","The excess has been interpreted by annihilating dark matter as well as emission from a population of unresolved millisecond pulsars (MSPs).","We use a multi-class classification of Fermi-LAT sources with machine learning to study the distribution of MSP-like sources among unassociated Fermi-LAT sources near the GC.","We find that the source count distribution of MSP-like sources is comparable with the MSP explanation of the GC excess."],"url":"http://arxiv.org/abs/2406.03990v1","category":"astro-ph.HE"}
{"created":"2024-06-06 11:51:12","title":"Position: Embracing Negative Results in Machine Learning","abstract":"Publications proposing novel machine learning methods are often primarily rated by exhibited predictive performance on selected problems. In this position paper we argue that predictive performance alone is not a good indicator for the worth of a publication. Using it as such even fosters problems like inefficiencies of the machine learning research community as a whole and setting wrong incentives for researchers. We therefore put out a call for the publication of \"negative\" results, which can help alleviate some of these problems and improve the scientific output of the machine learning research community. To substantiate our position, we present the advantages of publishing negative results and provide concrete measures for the community to move towards a paradigm where their publication is normalized.","sentences":["Publications proposing novel machine learning methods are often primarily rated by exhibited predictive performance on selected problems.","In this position paper we argue that predictive performance alone is not a good indicator for the worth of a publication.","Using it as such even fosters problems like inefficiencies of the machine learning research community as a whole and setting wrong incentives for researchers.","We therefore put out a call for the publication of \"negative\" results, which can help alleviate some of these problems and improve the scientific output of the machine learning research community.","To substantiate our position, we present the advantages of publishing negative results and provide concrete measures for the community to move towards a paradigm where their publication is normalized."],"url":"http://arxiv.org/abs/2406.03980v1","category":"cs.LG"}
{"created":"2024-06-06 10:46:51","title":"Weight-based Decomposition: A Case for Bilinear MLPs","abstract":"Gated Linear Units (GLUs) have become a common building block in modern foundation models. Bilinear layers drop the non-linearity in the \"gate\" but still have comparable performance to other GLUs. An attractive quality of bilinear layers is that they can be fully expressed in terms of a third-order tensor and linear operations. Leveraging this, we develop a method to decompose the bilinear tensor into a set of sparsely interacting eigenvectors that show promising interpretability properties in preliminary experiments for shallow image classifiers (MNIST) and small language models (Tiny Stories). Since the decomposition is fully equivalent to the model's original computations, bilinear layers may be an interpretability-friendly architecture that helps connect features to the model weights. Application of our method may not be limited to pretrained bilinear models since we find that language models such as TinyLlama-1.1B can be finetuned into bilinear variants.","sentences":["Gated Linear Units (GLUs) have become a common building block in modern foundation models.","Bilinear layers drop the non-linearity in the \"gate\" but still have comparable performance to other GLUs.","An attractive quality of bilinear layers is that they can be fully expressed in terms of a third-order tensor and linear operations.","Leveraging this, we develop a method to decompose the bilinear tensor into a set of sparsely interacting eigenvectors that show promising interpretability properties in preliminary experiments for shallow image classifiers (MNIST) and small language models (Tiny Stories).","Since the decomposition is fully equivalent to the model's original computations, bilinear layers may be an interpretability-friendly architecture that helps connect features to the model weights.","Application of our method may not be limited to pretrained bilinear models since we find that language models such as TinyLlama-1.1B can be finetuned into bilinear variants."],"url":"http://arxiv.org/abs/2406.03947v1","category":"cs.LG"}
{"created":"2024-06-06 09:41:39","title":"Exploring the Zero-Shot Capabilities of Vision-Language Models for Improving Gaze Following","abstract":"Contextual cues related to a person's pose and interactions with objects and other people in the scene can provide valuable information for gaze following. While existing methods have focused on dedicated cue extraction methods, in this work we investigate the zero-shot capabilities of Vision-Language Models (VLMs) for extracting a wide array of contextual cues to improve gaze following performance. We first evaluate various VLMs, prompting strategies, and in-context learning (ICL) techniques for zero-shot cue recognition performance. We then use these insights to extract contextual cues for gaze following, and investigate their impact when incorporated into a state of the art model for the task. Our analysis indicates that BLIP-2 is the overall top performing VLM and that ICL can improve performance. We also observe that VLMs are sensitive to the choice of the text prompt although ensembling over multiple text prompts can provide more robust performance. Additionally, we discover that using the entire image along with an ellipse drawn around the target person is the most effective strategy for visual prompting. For gaze following, incorporating the extracted cues results in better generalization performance, especially when considering a larger set of cues, highlighting the potential of this approach.","sentences":["Contextual cues related to a person's pose and interactions with objects and other people in the scene can provide valuable information for gaze following.","While existing methods have focused on dedicated cue extraction methods, in this work we investigate the zero-shot capabilities of Vision-Language Models (VLMs) for extracting a wide array of contextual cues to improve gaze following performance.","We first evaluate various VLMs, prompting strategies, and in-context learning (ICL) techniques for zero-shot cue recognition performance.","We then use these insights to extract contextual cues for gaze following, and investigate their impact when incorporated into a state of the art model for the task.","Our analysis indicates that BLIP-2 is the overall top performing VLM and that ICL can improve performance.","We also observe that VLMs are sensitive to the choice of the text prompt although ensembling over multiple text prompts can provide more robust performance.","Additionally, we discover that using the entire image along with an ellipse drawn around the target person is the most effective strategy for visual prompting.","For gaze following, incorporating the extracted cues results in better generalization performance, especially when considering a larger set of cues, highlighting the potential of this approach."],"url":"http://arxiv.org/abs/2406.03907v1","category":"cs.CV"}
{"created":"2024-06-06 09:29:40","title":"Transductive Off-policy Proximal Policy Optimization","abstract":"Proximal Policy Optimization (PPO) is a popular model-free reinforcement learning algorithm, esteemed for its simplicity and efficacy. However, due to its inherent on-policy nature, its proficiency in harnessing data from disparate policies is constrained. This paper introduces a novel off-policy extension to the original PPO method, christened Transductive Off-policy PPO (ToPPO). Herein, we provide theoretical justification for incorporating off-policy data in PPO training and prudent guidelines for its safe application. Our contribution includes a novel formulation of the policy improvement lower bound for prospective policies derived from off-policy data, accompanied by a computationally efficient mechanism to optimize this bound, underpinned by assurances of monotonic improvement. Comprehensive experimental results across six representative tasks underscore ToPPO's promising performance.","sentences":["Proximal Policy Optimization (PPO) is a popular model-free reinforcement learning algorithm, esteemed for its simplicity and efficacy.","However, due to its inherent on-policy nature, its proficiency in harnessing data from disparate policies is constrained.","This paper introduces a novel off-policy extension to the original PPO method, christened Transductive Off-policy PPO (ToPPO).","Herein, we provide theoretical justification for incorporating off-policy data in PPO training and prudent guidelines for its safe application.","Our contribution includes a novel formulation of the policy improvement lower bound for prospective policies derived from off-policy data, accompanied by a computationally efficient mechanism to optimize this bound, underpinned by assurances of monotonic improvement.","Comprehensive experimental results across six representative tasks underscore ToPPO's promising performance."],"url":"http://arxiv.org/abs/2406.03894v1","category":"cs.LG"}
{"created":"2024-06-06 09:14:32","title":"Decay Pruning Method: Smooth Pruning With a Self-Rectifying Procedure","abstract":"Current structured pruning methods often result in considerable accuracy drops due to abrupt network changes and loss of information from pruned structures. To address these issues, we introduce the Decay Pruning Method (DPM), a novel smooth pruning approach with a self-rectifying mechanism. DPM consists of two key components: (i) Smooth Pruning: It converts conventional single-step pruning into multi-step smooth pruning, gradually reducing redundant structures to zero over N steps with ongoing optimization. (ii) Self-Rectifying: This procedure further enhances the aforementioned process by rectifying sub-optimal pruning based on gradient information. Our approach demonstrates strong generalizability and can be easily integrated with various existing pruning methods. We validate the effectiveness of DPM by integrating it with three popular pruning methods: OTOv2, Depgraph, and Gate Decorator. Experimental results show consistent improvements in performance compared to the original pruning methods, along with further reductions of FLOPs in most scenarios.","sentences":["Current structured pruning methods often result in considerable accuracy drops due to abrupt network changes and loss of information from pruned structures.","To address these issues, we introduce the Decay Pruning Method (DPM), a novel smooth pruning approach with a self-rectifying mechanism.","DPM consists of two key components: (i) Smooth Pruning: It converts conventional single-step pruning into multi-step smooth pruning, gradually reducing redundant structures to zero over N steps with ongoing optimization.","(ii) Self-Rectifying:","This procedure further enhances the aforementioned process by rectifying sub-optimal pruning based on gradient information.","Our approach demonstrates strong generalizability and can be easily integrated with various existing pruning methods.","We validate the effectiveness of DPM by integrating it with three popular pruning methods: OTOv2, Depgraph, and Gate Decorator.","Experimental results show consistent improvements in performance compared to the original pruning methods, along with further reductions of FLOPs in most scenarios."],"url":"http://arxiv.org/abs/2406.03879v1","category":"cs.LG"}
{"created":"2024-06-06 08:36:21","title":"Why the Metric Backbone Preserves Community Structure","abstract":"The metric backbone of a weighted graph is the union of all-pairs shortest paths. It is obtained by removing all edges $(u,v)$ that are not the shortest path between $u$ and $v$. In networks with well-separated communities, the metric backbone tends to preserve many inter-community edges, because these edges serve as bridges connecting two communities, but tends to delete many intra-community edges because the communities are dense. This suggests that the metric backbone would dilute or destroy the community structure of the network. However, this is not borne out by prior empirical work, which instead showed that the metric backbone of real networks preserves the community structure of the original network well. In this work, we analyze the metric backbone of a broad class of weighted random graphs with communities, and we formally prove the robustness of the community structure with respect to the deletion of all the edges that are not in the metric backbone. An empirical comparison of several graph sparsification techniques confirms our theoretical finding and shows that the metric backbone is an efficient sparsifier in the presence of communities.","sentences":["The metric backbone of a weighted graph is the union of all-pairs shortest paths.","It is obtained by removing all edges $(u,v)$ that are not the shortest path between $u$ and $v$. In networks with well-separated communities, the metric backbone tends to preserve many inter-community edges, because these edges serve as bridges connecting two communities, but tends to delete many intra-community edges because the communities are dense.","This suggests that the metric backbone would dilute or destroy the community structure of the network.","However, this is not borne out by prior empirical work, which instead showed that the metric backbone of real networks preserves the community structure of the original network well.","In this work, we analyze the metric backbone of a broad class of weighted random graphs with communities, and we formally prove the robustness of the community structure with respect to the deletion of all the edges that are not in the metric backbone.","An empirical comparison of several graph sparsification techniques confirms our theoretical finding and shows that the metric backbone is an efficient sparsifier in the presence of communities."],"url":"http://arxiv.org/abs/2406.03852v1","category":"cs.SI"}
{"created":"2024-06-06 08:15:12","title":"PCART: Automated Repair of Python API Parameter Compatibility Issues","abstract":"In modern software development, Python third-party libraries have become crucial, particularly due to their widespread use in fields such as deep learning and scientific computing. However, the parameters of APIs in third-party libraries often change during evolution, causing compatibility issues for client applications that depend on specific versions. Due to Python's flexible parameter-passing mechanism, different methods of parameter passing can result in different API compatibility. Currently, no tool is capable of automatically detecting and repairing Python API parameter compatibility issues. To fill this gap, we propose PCART, the first to implement a fully automated process from API extraction, code instrumentation, and API mapping establishment, to compatibility assessment, and finally to repair and validation, for solving various types of Python API parameter compatibility issues, i.e., parameter addition, removal, renaming, reordering of parameters, as well as the conversion of positional parameters to keyword parameters. We construct a large-scale benchmark PCBENCH, including 47,478 test cases mutated from 844 parameter-changed APIs of 33 popular Python libraries, to evaluate PCART. The evaluation results show that PCART is effective yet efficient, significantly outperforming existing tools (MLCatchUp and Relancer) and the large language model ChatGPT-4, achieving an F-measure of 96.49% in detecting API parameter compatibility issues and a repair accuracy of 91.36%. The evaluation on 14 real-world Python projects from GitHub further demonstrates that PCART has good practicality. We believe PCART can help programmers reduce the time spent on maintaining Python API updates and facilitate automated Python API compatibility issue repair.","sentences":["In modern software development, Python third-party libraries have become crucial, particularly due to their widespread use in fields such as deep learning and scientific computing.","However, the parameters of APIs in third-party libraries often change during evolution, causing compatibility issues for client applications that depend on specific versions.","Due to Python's flexible parameter-passing mechanism, different methods of parameter passing can result in different API compatibility.","Currently, no tool is capable of automatically detecting and repairing Python API parameter compatibility issues.","To fill this gap, we propose PCART, the first to implement a fully automated process from API extraction, code instrumentation, and API mapping establishment, to compatibility assessment, and finally to repair and validation, for solving various types of Python API parameter compatibility issues, i.e., parameter addition, removal, renaming, reordering of parameters, as well as the conversion of positional parameters to keyword parameters.","We construct a large-scale benchmark PCBENCH, including 47,478 test cases mutated from 844 parameter-changed APIs of 33 popular Python libraries, to evaluate PCART.","The evaluation results show that PCART is effective yet efficient, significantly outperforming existing tools (MLCatchUp and Relancer) and the large language model ChatGPT-4, achieving an F-measure of 96.49% in detecting API parameter compatibility issues and a repair accuracy of 91.36%.","The evaluation on 14 real-world Python projects from GitHub further demonstrates that PCART has good practicality.","We believe PCART can help programmers reduce the time spent on maintaining Python API updates and facilitate automated Python API compatibility issue repair."],"url":"http://arxiv.org/abs/2406.03839v1","category":"cs.SE"}
{"created":"2024-06-06 08:05:34","title":"UltraPINK -- New possibilities to explore Self-Organizing Kohonen Maps","abstract":"Unsupervised learning algorithms like self-organizing Kohonen maps are a promising approach to gain an overview among massive datasets. With UltraPINK, researchers can train, inspect, and explore self-organizing maps, whereby the toolbox of interaction possibilities grows continually. Key feature of UltraPINK is the consideration of versality in astronomical data. By keeping the operations as abstract as possible and using design patterns meant for abstract usage, we ensure that data is compatible with UltraPINK, regardless of its type, formatting, or origin. Future work on the application will keep extending the catalogue of exploration tools and the interfaces towards other established applications to process astronomical data. Ultimatively, we aim towards a solid infrastructure for data analysis in astronomy.","sentences":["Unsupervised learning algorithms like self-organizing Kohonen maps are a promising approach to gain an overview among massive datasets.","With UltraPINK, researchers can train, inspect, and explore self-organizing maps, whereby the toolbox of interaction possibilities grows continually.","Key feature of UltraPINK is the consideration of versality in astronomical data.","By keeping the operations as abstract as possible and using design patterns meant for abstract usage, we ensure that data is compatible with UltraPINK, regardless of its type, formatting, or origin.","Future work on the application will keep extending the catalogue of exploration tools and the interfaces towards other established applications to process astronomical data.","Ultimatively, we aim towards a solid infrastructure for data analysis in astronomy."],"url":"http://arxiv.org/abs/2406.03832v1","category":"astro-ph.IM"}
{"created":"2024-06-06 07:59:19","title":"Predictability Analysis of Regression Problems via Conditional Entropy Estimations","abstract":"In the field of machine learning, regression problems are pivotal due to their ability to predict continuous outcomes. Traditional error metrics like mean squared error, mean absolute error, and coefficient of determination measure model accuracy. The model accuracy is the consequence of the selected model and the features, which blurs the analysis of contribution. Predictability, in the other hand, focus on the predictable level of a target variable given a set of features. This study introduces conditional entropy estimators to assess predictability in regression problems, bridging this gap. We enhance and develop reliable conditional entropy estimators, particularly the KNIFE-P estimator and LMC-P estimator, which offer under- and over-estimation, providing a practical framework for predictability analysis. Extensive experiments on synthesized and real-world datasets demonstrate the robustness and utility of these estimators. Additionally, we extend the analysis to the coefficient of determination \\(R^2 \\), enhancing the interpretability of predictability. The results highlight the effectiveness of KNIFE-P and LMC-P in capturing the achievable performance and limitations of feature sets, providing valuable tools in the development of regression models. These indicators offer a robust framework for assessing the predictability for regression problems.","sentences":["In the field of machine learning, regression problems are pivotal due to their ability to predict continuous outcomes.","Traditional error metrics like mean squared error, mean absolute error, and coefficient of determination measure model accuracy.","The model accuracy is the consequence of the selected model and the features, which blurs the analysis of contribution.","Predictability, in the other hand, focus on the predictable level of a target variable given a set of features.","This study introduces conditional entropy estimators to assess predictability in regression problems, bridging this gap.","We enhance and develop reliable conditional entropy estimators, particularly the KNIFE-P estimator and LMC-P estimator, which offer under- and over-estimation, providing a practical framework for predictability analysis.","Extensive experiments on synthesized and real-world datasets demonstrate the robustness and utility of these estimators.","Additionally, we extend the analysis to the coefficient of determination \\(R^2 \\), enhancing the interpretability of predictability.","The results highlight the effectiveness of KNIFE-P and LMC-P in capturing the achievable performance and limitations of feature sets, providing valuable tools in the development of regression models.","These indicators offer a robust framework for assessing the predictability for regression problems."],"url":"http://arxiv.org/abs/2406.03824v1","category":"cs.LG"}
{"created":"2024-06-06 07:37:57","title":"Touch100k: A Large-Scale Touch-Language-Vision Dataset for Touch-Centric Multimodal Representation","abstract":"Touch holds a pivotal position in enhancing the perceptual and interactive capabilities of both humans and robots. Despite its significance, current tactile research mainly focuses on visual and tactile modalities, overlooking the language domain. Inspired by this, we construct Touch100k, a paired touch-language-vision dataset at the scale of 100k, featuring tactile sensation descriptions in multiple granularities (i.e., sentence-level natural expressions with rich semantics, including contextual and dynamic relationships, and phrase-level descriptions capturing the key features of tactile sensations). Based on the dataset, we propose a pre-training method, Touch-Language-Vision Representation Learning through Curriculum Linking (TLV-Link, for short), inspired by the concept of curriculum learning. TLV-Link aims to learn a tactile representation for the GelSight sensor and capture the relationship between tactile, language, and visual modalities. We evaluate our representation's performance across two task categories (namely, material property identification and robot grasping prediction), focusing on tactile representation and zero-shot touch understanding. The experimental evaluation showcases the effectiveness of our representation. By enabling TLV-Link to achieve substantial improvements and establish a new state-of-the-art in touch-centric multimodal representation learning, Touch100k demonstrates its value as a valuable resource for research. Project page: https://cocacola-lab.github.io/Touch100k/.","sentences":["Touch holds a pivotal position in enhancing the perceptual and interactive capabilities of both humans and robots.","Despite its significance, current tactile research mainly focuses on visual and tactile modalities, overlooking the language domain.","Inspired by this, we construct Touch100k, a paired touch-language-vision dataset at the scale of 100k, featuring tactile sensation descriptions in multiple granularities (i.e., sentence-level natural expressions with rich semantics, including contextual and dynamic relationships, and phrase-level descriptions capturing the key features of tactile sensations).","Based on the dataset, we propose a pre-training method, Touch-Language-Vision Representation Learning through Curriculum Linking (TLV-Link, for short), inspired by the concept of curriculum learning.","TLV-Link aims to learn a tactile representation for the GelSight sensor and capture the relationship between tactile, language, and visual modalities.","We evaluate our representation's performance across two task categories (namely, material property identification and robot grasping prediction), focusing on tactile representation and zero-shot touch understanding.","The experimental evaluation showcases the effectiveness of our representation.","By enabling TLV-Link to achieve substantial improvements and establish a new state-of-the-art in touch-centric multimodal representation learning, Touch100k demonstrates its value as a valuable resource for research.","Project page: https://cocacola-lab.github.io/Touch100k/."],"url":"http://arxiv.org/abs/2406.03813v1","category":"cs.RO"}
{"created":"2024-06-06 07:03:28","title":"Speed of Light Exact Greedy Decoding for RNN-T Speech Recognition Models on GPU","abstract":"The vast majority of inference time for RNN Transducer (RNN-T) models today is spent on decoding. Current state-of-the-art RNN-T decoding implementations leave the GPU idle ~80% of the time. Leveraging a new CUDA 12.4 feature, CUDA graph conditional nodes, we present an exact GPU-based implementation of greedy decoding for RNN-T models that eliminates this idle time. Our optimizations speed up a 1.1 billion parameter RNN-T model end-to-end by a factor of 2.5x. This technique can applied to the \"label looping\" alternative greedy decoding algorithm as well, achieving 1.7x and 1.4x end-to-end speedups when applied to 1.1 billion parameter RNN-T and Token and Duration Transducer models respectively. This work enables a 1.1 billion parameter RNN-T model to run only 16% slower than a similarly sized CTC model, contradicting the common belief that RNN-T models are not suitable for high throughput inference. The implementation is available in NVIDIA NeMo.","sentences":["The vast majority of inference time for RNN Transducer (RNN-T) models today is spent on decoding.","Current state-of-the-art RNN-T decoding implementations leave the GPU idle ~80% of the time.","Leveraging a new CUDA 12.4 feature, CUDA graph conditional nodes, we present an exact GPU-based implementation of greedy decoding for RNN-T models that eliminates this idle time.","Our optimizations speed up a 1.1 billion parameter RNN-T model end-to-end by a factor of 2.5x.","This technique can applied to the \"label looping\" alternative greedy decoding algorithm as well, achieving 1.7x and 1.4x end-to-end speedups when applied to 1.1 billion parameter RNN-T and Token and Duration Transducer models respectively.","This work enables a 1.1 billion parameter RNN-T model to run only 16% slower than a similarly sized CTC model, contradicting the common belief that RNN-T models are not suitable for high throughput inference.","The implementation is available in NVIDIA NeMo."],"url":"http://arxiv.org/abs/2406.03791v1","category":"cs.LG"}
{"created":"2024-06-06 06:23:42","title":"Optimizing Multi-User Semantic Communication via Transfer Learning and Knowledge Distillation","abstract":"Semantic communication, notable for ensuring quality of service by jointly optimizing source and channel coding, effectively extracts data semantics, reduces transmission length, and mitigates channel noise. However, most studies overlook multi-user scenarios and resource availability, limiting real-world application. This paper addresses this gap by focusing on downlink communication from a base station to multiple users with varying computing capacities. Users employ variants of Swin transformer models for source decoding and a simple architecture for channel decoding. We propose a novel training regimen, incorporating transfer learning and knowledge distillation to improve low-computing users' performance. Extensive simulations validate the proposed methods.","sentences":["Semantic communication, notable for ensuring quality of service by jointly optimizing source and channel coding, effectively extracts data semantics, reduces transmission length, and mitigates channel noise.","However, most studies overlook multi-user scenarios and resource availability, limiting real-world application.","This paper addresses this gap by focusing on downlink communication from a base station to multiple users with varying computing capacities.","Users employ variants of Swin transformer models for source decoding and a simple architecture for channel decoding.","We propose a novel training regimen, incorporating transfer learning and knowledge distillation to improve low-computing users' performance.","Extensive simulations validate the proposed methods."],"url":"http://arxiv.org/abs/2406.03773v1","category":"cs.IT"}
{"created":"2024-06-06 06:17:27","title":"DeepRacer on Physical Track: Parameters Exploration and Performance Evaluation","abstract":"This paper focuses on the physical racetrack capabilities of AWS DeepRacer. Two separate experiments were conducted. The first experiment (Experiment I) focused on evaluating the impact of hyperparameters on the physical environment. Hyperparameters such as gradient descent batch size and loss type were changed systematically as well as training time settings. The second experiment (Experiment II) focused on exploring AWS DeepRacer object avoidance in the physical environment. It was uncovered that in the simulated environment, models with a higher gradient descent batch size had better performance than models with a lower gradient descent batch size. Alternatively, in the physical environment, a gradient descent batch size of 128 appears to be preferable. It was found that models using the loss type of Huber outperformed models that used the loss type of MSE in both the simulated and physical environments. Finally, object avoidance in the simulated environment appeared to be effective; however, when bringing these models to the physical environment, there was a pronounced challenge to avoid objects. Therefore, object avoidance in the physical environment remains an open challenge.","sentences":["This paper focuses on the physical racetrack capabilities of AWS DeepRacer.","Two separate experiments were conducted.","The first experiment (Experiment I) focused on evaluating the impact of hyperparameters on the physical environment.","Hyperparameters such as gradient descent batch size and loss type were changed systematically as well as training time settings.","The second experiment (Experiment II) focused on exploring AWS DeepRacer object avoidance in the physical environment.","It was uncovered that in the simulated environment, models with a higher gradient descent batch size had better performance than models with a lower gradient descent batch size.","Alternatively, in the physical environment, a gradient descent batch size of 128 appears to be preferable.","It was found that models using the loss type of Huber outperformed models that used the loss type of MSE in both the simulated and physical environments.","Finally, object avoidance in the simulated environment appeared to be effective; however, when bringing these models to the physical environment, there was a pronounced challenge to avoid objects.","Therefore, object avoidance in the physical environment remains an open challenge."],"url":"http://arxiv.org/abs/2406.03769v1","category":"cs.LG"}
{"created":"2024-06-06 05:06:05","title":"Avoiding Barren Plateaus with Entanglement","abstract":"In the search for quantum advantage with near-term quantum devices, navigating the optimization landscape is significantly hampered by the barren plateaus phenomenon. This study presents a strategy to overcome this obstacle without changing the quantum circuit architecture. We propose incorporating auxiliary control qubits to shift the circuit from a unitary $2$-design to a unitary $1$-design, mitigating the prevalence of barren plateaus. We then remove these auxiliary qubits to return to the original circuit structure while preserving the unitary $1$-design properties. Our experiment suggests that the proposed structure effectively mitigates the barren plateaus phenomenon. A significant experimental finding is that the gradient of $\\theta_{1,1}$, the first parameter in the quantum circuit, displays a broader distribution as the number of qubits and layers increases. This suggests a higher probability of obtaining effective gradients. This stability is critical for the efficient training of quantum circuits, especially for larger and more complex systems. The results of this study represent a significant advance in the optimization of quantum circuits and offer a promising avenue for the scalable and practical implementation of quantum computing technologies. This approach opens up new opportunities in quantum learning and other applications that require robust quantum computing power.","sentences":["In the search for quantum advantage with near-term quantum devices, navigating the optimization landscape is significantly hampered by the barren plateaus phenomenon.","This study presents a strategy to overcome this obstacle without changing the quantum circuit architecture.","We propose incorporating auxiliary control qubits to shift the circuit from a unitary $2$-design to a unitary $1$-design, mitigating the prevalence of barren plateaus.","We then remove these auxiliary qubits to return to the original circuit structure while preserving the unitary $1$-design properties.","Our experiment suggests that the proposed structure effectively mitigates the barren plateaus phenomenon.","A significant experimental finding is that the gradient of $\\theta_{1,1}$, the first parameter in the quantum circuit, displays a broader distribution as the number of qubits and layers increases.","This suggests a higher probability of obtaining effective gradients.","This stability is critical for the efficient training of quantum circuits, especially for larger and more complex systems.","The results of this study represent a significant advance in the optimization of quantum circuits and offer a promising avenue for the scalable and practical implementation of quantum computing technologies.","This approach opens up new opportunities in quantum learning and other applications that require robust quantum computing power."],"url":"http://arxiv.org/abs/2406.03748v1","category":"quant-ph"}
{"created":"2024-06-06 14:36:02","title":"Global Parameterization-based Texture Space Optimization","abstract":"Texture mapping is a common technology in the area of computer graphics, it maps the 3D surface space onto the 2D texture space. However, the loose texture space will reduce the efficiency of data storage and GPU memory addressing in the rendering process. Many of the existing methods focus on repacking given textures, but they still suffer from high computational cost and hardly produce a wholly tight texture space. In this paper, we propose a method to optimize the texture space and produce a new texture mapping which is compact based on global parameterization. The proposed method is computationally robust and efficient. Experiments show the effectiveness of the proposed method and the potency in improving the storage and rendering efficiency.","sentences":["Texture mapping is a common technology in the area of computer graphics, it maps the 3D surface space onto the 2D texture space.","However, the loose texture space will reduce the efficiency of data storage and GPU memory addressing in the rendering process.","Many of the existing methods focus on repacking given textures, but they still suffer from high computational cost and hardly produce a wholly tight texture space.","In this paper, we propose a method to optimize the texture space and produce a new texture mapping which is compact based on global parameterization.","The proposed method is computationally robust and efficient.","Experiments show the effectiveness of the proposed method and the potency in improving the storage and rendering efficiency."],"url":"http://arxiv.org/abs/2406.04115v1","category":"cs.CV"}
{"created":"2024-06-06 12:37:25","title":"Positive definiteness of fourth order three dimensional symmetric tensors","abstract":"For a 4th order 3-dimensional symmetric tensor with its entries $1$ or $-1$, we show the analytic sufficient and necessary conditions of its positive definiteness. By applying these conclusions, several strict inequalities is bulit for ternary quartic homogeneous polynomials.","sentences":["For a 4th order 3-dimensional symmetric tensor with its entries $1$ or $-1$, we show the analytic sufficient and necessary conditions of its positive definiteness.","By applying these conclusions, several strict inequalities is bulit for ternary quartic homogeneous polynomials."],"url":"http://arxiv.org/abs/2406.04010v1","category":"math.CA"}
{"created":"2024-06-06 10:51:30","title":"$\\cal{CP}$-sensitive simplified template cross-sections for $t\\bar t H$","abstract":"The $\\cal{CP}$ structure of the Higgs boson is a fundamental property which has not yet been constrained with high precision. $\\cal{CP}$ violation in the Yukawa coupling between the Higgs boson and top quark pair can be probed directly at the Large Hadron Collider by measuring top-quark-associated Higgs production. Multivariate analysis techniques commonly developed so far by the experiments are designed for a specific signal model and, therefore, complicate reinterpretations and statistical combinations. With this motivation in mind, we propose a $\\cal{CP}$-sensitive extension of the simplified template cross-section (STXS) framework. Considering multiple Higgs decay channels, we perform an in-depth comparison of $\\cal{CP}$-sensitive observables and combinations thereof. Our resulting proposal is to extend the existing binning in the transverse momentum of the Higgs boson $p_{T,H}$ by either the pseudorapidity difference of the two top-quarks $\\Delta \\eta_{t\\bar t}$, or a variable that is based on the top quark momenta, namely $b_2$ or the Collins-Soper angle $|\\cos\\theta^*|$. We demonstrate that this variable selection provides close to optimal sensitivity to the $\\cal{CP}$ mixture in the top Yukawa coupling for an integrated luminosity of $300\\mathrm{fb}^{-1}$, by comparing it to the results of a multivariate analysis. Our results also suggest a benefit of the two-dimensional STXS extension at 3000$\\mathrm{fb}^{-1}$.","sentences":["The $\\cal{CP}$ structure of the Higgs boson is a fundamental property which has not yet been constrained with high precision.","$\\cal{CP}$ violation in the Yukawa coupling between the Higgs boson and top quark pair can be probed directly at the Large Hadron Collider by measuring top-quark-associated Higgs production.","Multivariate analysis techniques commonly developed so far by the experiments are designed for a specific signal model and, therefore, complicate reinterpretations and statistical combinations.","With this motivation in mind, we propose a $\\cal{CP}$-sensitive extension of the simplified template cross-section (STXS) framework.","Considering multiple Higgs decay channels, we perform an in-depth comparison of $\\cal{CP}$-sensitive observables and combinations thereof.","Our resulting proposal is to extend the existing binning in the transverse momentum of the Higgs boson $p_{T,H}$ by either the pseudorapidity difference of the two top-quarks $\\Delta \\eta_{t\\bar t}$, or a variable that is based on the top quark momenta, namely $b_2$ or the Collins-Soper angle $|\\cos\\theta^*|$. We demonstrate that this variable selection provides close to optimal sensitivity to the $\\cal{CP}$ mixture in the top Yukawa coupling for an integrated luminosity of $300\\mathrm{fb}^{-1}$, by comparing it to the results of a multivariate analysis.","Our results also suggest a benefit of the two-dimensional STXS extension at 3000$\\mathrm{fb}^{-1}$."],"url":"http://arxiv.org/abs/2406.03950v1","category":"hep-ph"}
{"created":"2024-06-06 09:53:32","title":"Impact of ageostrophic dynamics on the predictability of Lagrangian trajectories in surface-ocean turbulence","abstract":"Turbulent flows at the surface of the ocean deviate from geostrophic equilibrium on scales smaller than about 10 km. These scales are associated with important vertical transport of active and passive tracers, and should play a prominent role in the heat transport at climatic scales and for plankton dynamics. Measuring velocity fields on such small scales is notoriously difficult but new, high-resolution satellite altimetry is starting to reveal them. However, the satellite-derived velocities essentially represent the geostrophic flow component, and the impact of unresolved ageostrophic motions on particle dispersion needs to be understood to properly characterize transport properties. Here, we investigate ocean fine-scale turbulence using a model that represents some of the processes due to ageostrophic dynamics. We take a Lagrangian approach and focus on the predictability of the particle dynamics, comparing trajectories advected by either the full flow or by its geostrophic component only. Our results indicate that, over long times, relative dispersion is marginally affected by the filtering of the ageostrophic component. Nevertheless, advection by the filtered flow leads to an overestimation of the typical pair-separation rate, and to a bias on trajectories (in terms of displacement from the actual ones), whose importance grows with the Rossby number. We further explore the intensity of the transient particle clustering induced by ageostrophic motions and find that it can be significant, even for small flow compressibility. Indeed, we show that clustering is here due to the interplay between compressibility and persistent flow structures that trap particles, enhancing their aggregation.","sentences":["Turbulent flows at the surface of the ocean deviate from geostrophic equilibrium on scales smaller than about 10 km.","These scales are associated with important vertical transport of active and passive tracers, and should play a prominent role in the heat transport at climatic scales and for plankton dynamics.","Measuring velocity fields on such small scales is notoriously difficult but new, high-resolution satellite altimetry is starting to reveal them.","However, the satellite-derived velocities essentially represent the geostrophic flow component, and the impact of unresolved ageostrophic motions on particle dispersion needs to be understood to properly characterize transport properties.","Here, we investigate ocean fine-scale turbulence using a model that represents some of the processes due to ageostrophic dynamics.","We take a Lagrangian approach and focus on the predictability of the particle dynamics, comparing trajectories advected by either the full flow or by its geostrophic component only.","Our results indicate that, over long times, relative dispersion is marginally affected by the filtering of the ageostrophic component.","Nevertheless, advection by the filtered flow leads to an overestimation of the typical pair-separation rate, and to a bias on trajectories (in terms of displacement from the actual ones), whose importance grows with the Rossby number.","We further explore the intensity of the transient particle clustering induced by ageostrophic motions and find that it can be significant, even for small flow compressibility.","Indeed, we show that clustering is here due to the interplay between compressibility and persistent flow structures that trap particles, enhancing their aggregation."],"url":"http://arxiv.org/abs/2406.03915v1","category":"physics.flu-dyn"}
{"created":"2024-06-06 09:50:04","title":"Analyzing the sensitivity of an atom interferometer with a phase modulation readout scheme","abstract":"The sensitivity of an interferometer depends on its readout scheme. However, little attention has been paid to the readout schemes of atom interferometers from the viewpoint of their sensitivity. The difference in sensitivity between readout schemes or their optimization has not been considered in the literature. Herein, we analytically calculate the sensitivities of an atom interferometer with typical readout schemes by applying the two-photon formalism, which was developed for optical interferometers to deal with quantum noise. Our calculations reveal that by using sinusoidal phase modulation, the sensitivity can surpass that obtained by the conventional phase sweeping scheme. The superiority of this phase modulation scheme for both cold and thermal atomic beams is demonstrated. In addition, we show that the phase modulation scheme is advantageous for atom-flux fluctuation and resists atom-flux drift. This study performs a general analysis of the sensitivity of atom interferometers and identifies an advantageous readout scheme.","sentences":["The sensitivity of an interferometer depends on its readout scheme.","However, little attention has been paid to the readout schemes of atom interferometers from the viewpoint of their sensitivity.","The difference in sensitivity between readout schemes or their optimization has not been considered in the literature.","Herein, we analytically calculate the sensitivities of an atom interferometer with typical readout schemes by applying the two-photon formalism, which was developed for optical interferometers to deal with quantum noise.","Our calculations reveal that by using sinusoidal phase modulation, the sensitivity can surpass that obtained by the conventional phase sweeping scheme.","The superiority of this phase modulation scheme for both cold and thermal atomic beams is demonstrated.","In addition, we show that the phase modulation scheme is advantageous for atom-flux fluctuation and resists atom-flux drift.","This study performs a general analysis of the sensitivity of atom interferometers and identifies an advantageous readout scheme."],"url":"http://arxiv.org/abs/2406.03911v1","category":"physics.atom-ph"}
{"created":"2024-06-06 08:59:35","title":"Development of high-level applications for High Energy Photon Source booster","abstract":"The High Energy Photon Source (HEPS), is the first fourth-generation storage ring light source being built in the suburb of Beijing, China. The storage ring was designed with the emittance lower than 60 pm.rad with a circumference of 1.36 km and beam energy of 6 GeV. Its injector contains a 500 MeV S-band Linac and a 454 m booster which was designed as an accumulator at the extraction energy. In the energy ramping control design of HEPS booster, the ramping process was programed to be able to stop and stay at any energy between the injection energy and the extraction energy. This feature enables us to conduct energy-dependent machine studies and ramping curve optimization. The beam commissioning of HEPS Linac finished in June, 2023. And the beam commissioning of booster started in the end of July, 2023. In November 17, main target values proposed in the preliminary design report has been reached. The high-level applications (HLAs) are essential tools for beam commissioning. The development of HLAs, which are based on the framework named Python accelerator physics application set (Pyapas), started in the end of 2021. The HEPS physics team spent more than one year to develop and test the HLAs to meet the requirements of beam commissioning of the booster. Thanks to the modular design, the principle based on physical quantities, and the ability of running simulation models online from the Pyapas, the development efficiency and reliability of the HLAs have been greatly improved. In particular, the principle based on physical quantities allows us to control the beam more intuitively.","sentences":["The High Energy Photon Source (HEPS), is the first fourth-generation storage ring light source being built in the suburb of Beijing, China.","The storage ring was designed with the emittance lower than 60 pm.rad with a circumference of 1.36 km and beam energy of 6 GeV. Its injector contains a 500 MeV S-band Linac and a 454 m booster which was designed as an accumulator at the extraction energy.","In the energy ramping control design of HEPS booster, the ramping process was programed to be able to stop and stay at any energy between the injection energy and the extraction energy.","This feature enables us to conduct energy-dependent machine studies and ramping curve optimization.","The beam commissioning of HEPS Linac finished in June, 2023.","And the beam commissioning of booster started in the end of July, 2023.","In November 17, main target values proposed in the preliminary design report has been reached.","The high-level applications (HLAs) are essential tools for beam commissioning.","The development of HLAs, which are based on the framework named Python accelerator physics application set (Pyapas), started in the end of 2021.","The HEPS physics team spent more than one year to develop and test the HLAs to meet the requirements of beam commissioning of the booster.","Thanks to the modular design, the principle based on physical quantities, and the ability of running simulation models online from the Pyapas, the development efficiency and reliability of the HLAs have been greatly improved.","In particular, the principle based on physical quantities allows us to control the beam more intuitively."],"url":"http://arxiv.org/abs/2406.03871v1","category":"physics.acc-ph"}
{"created":"2024-06-06 07:24:41","title":"AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens","abstract":"Jailbreak attacks in large language models (LLMs) entail inducing the models to generate content that breaches ethical and legal norm through the use of malicious prompts, posing a substantial threat to LLM security. Current strategies for jailbreak attack and defense often focus on optimizing locally within specific algorithmic frameworks, resulting in ineffective optimization and limited scalability. In this paper, we present a systematic analysis of the dependency relationships in jailbreak attack and defense techniques, generalizing them to all possible attack surfaces. We employ directed acyclic graphs (DAGs) to position and analyze existing jailbreak attacks, defenses, and evaluation methodologies, and propose three comprehensive, automated, and logical frameworks. \\texttt{AutoAttack} investigates dependencies in two lines of jailbreak optimization strategies: genetic algorithm (GA)-based attacks and adversarial-generation-based attacks, respectively. We then introduce an ensemble jailbreak attack to exploit these dependencies. \\texttt{AutoDefense} offers a mixture-of-defenders approach by leveraging the dependency relationships in pre-generative and post-generative defense strategies. \\texttt{AutoEvaluation} introduces a novel evaluation method that distinguishes hallucinations, which are often overlooked, from jailbreak attack and defense responses. Through extensive experiments, we demonstrate that the proposed ensemble jailbreak attack and defense framework significantly outperforms existing research.","sentences":["Jailbreak attacks in large language models (LLMs) entail inducing the models to generate content that breaches ethical and legal norm through the use of malicious prompts, posing a substantial threat to LLM security.","Current strategies for jailbreak attack and defense often focus on optimizing locally within specific algorithmic frameworks, resulting in ineffective optimization and limited scalability.","In this paper, we present a systematic analysis of the dependency relationships in jailbreak attack and defense techniques, generalizing them to all possible attack surfaces.","We employ directed acyclic graphs (DAGs) to position and analyze existing jailbreak attacks, defenses, and evaluation methodologies, and propose three comprehensive, automated, and logical frameworks.","\\texttt{AutoAttack} investigates dependencies in two lines of jailbreak optimization strategies: genetic algorithm (GA)-based attacks and adversarial-generation-based attacks, respectively.","We then introduce an ensemble jailbreak attack to exploit these dependencies.","\\texttt{AutoDefense} offers a mixture-of-defenders approach by leveraging the dependency relationships in pre-generative and post-generative defense strategies.","\\texttt{AutoEvaluation} introduces a novel evaluation method that distinguishes hallucinations, which are often overlooked, from jailbreak attack and defense responses.","Through extensive experiments, we demonstrate that the proposed ensemble jailbreak attack and defense framework significantly outperforms existing research."],"url":"http://arxiv.org/abs/2406.03805v1","category":"cs.CR"}
{"created":"2024-06-06 06:43:41","title":"A Nearly Optimal Deterministic Algorithm for Online Transportation Problem","abstract":"We propose a new deterministic algorithm called Subtree-Decomposition for the online transportation problem and show that the algorithm is $(8m-5)$-competitive, where $m$ is the number of server sites.   It has long been known that the competitive ratio of any deterministic algorithm is lower bounded by $2m-1$ for this problem. On the other hand, the conjecture proposed by Kalyanasundaram and Pruhs in 1998 asking whether a deterministic $(2m-1)$-competitive algorithm exists for the online transportation problem has remained open for over two decades.   The upper bound on the competitive ratio, $8m-5$, which is the result of this paper, is the first to come close to this conjecture, and is the best possible within a constant factor.","sentences":["We propose a new deterministic algorithm called Subtree-Decomposition for the online transportation problem and show that the algorithm is $(8m-5)$-competitive, where $m$ is the number of server sites.   ","It has long been known that the competitive ratio of any deterministic algorithm is lower bounded by $2m-1$ for this problem.","On the other hand, the conjecture proposed by Kalyanasundaram and Pruhs in 1998 asking whether a deterministic $(2m-1)$-competitive algorithm exists for the online transportation problem has remained open for over two decades.   ","The upper bound on the competitive ratio, $8m-5$, which is the result of this paper, is the first to come close to this conjecture, and is the best possible within a constant factor."],"url":"http://arxiv.org/abs/2406.03778v1","category":"cs.DS"}
{"created":"2024-06-06 06:17:46","title":"Wave packet dynamics of entangled q-deformed states","abstract":"This paper explores the wave packet dynamics of a math-type q- deformed field interacting with atoms in a Kerr-type nonlinear medium. The primary focus is on the generation and dynamics of entanglement using the q- deformed field, with the quantification of entanglement accomplished through the von Neumann entropy. Two distinct initial q-deformed states, the q-deformed Fock state, and the q-deformed coherent state, are investigated. The entanglement dynamics reveal characteristics of periodic, quasi-periodic, and chaotic behavior. Non-deformed initial states display wave packet near revivals and fractional revivals in entanglement dynamics while introducing q-deformation eliminates these features. The q-deformation significantly influences wave packet revivals and fractional revivals, with even a slight introduction causing their disappearance. For large values of q, the entanglement dynamics exhibit a chaotic nature. In the case of a beam splitter-type interaction applied to the initial deformed Fock state, an optimal deformation parameter q is identified, leading to maximum entanglement exceeding the non-deformed scenario.","sentences":["This paper explores the wave packet dynamics of a math-type q- deformed field interacting with atoms in a Kerr-type nonlinear medium.","The primary focus is on the generation and dynamics of entanglement using the q- deformed field, with the quantification of entanglement accomplished through the von Neumann entropy.","Two distinct initial q-deformed states, the q-deformed Fock state, and the q-deformed coherent state, are investigated.","The entanglement dynamics reveal characteristics of periodic, quasi-periodic, and chaotic behavior.","Non-deformed initial states display wave packet near revivals and fractional revivals in entanglement dynamics while introducing q-deformation eliminates these features.","The q-deformation significantly influences wave packet revivals and fractional revivals, with even a slight introduction causing their disappearance.","For large values of q, the entanglement dynamics exhibit a chaotic nature.","In the case of a beam splitter-type interaction applied to the initial deformed Fock state, an optimal deformation parameter q is identified, leading to maximum entanglement exceeding the non-deformed scenario."],"url":"http://arxiv.org/abs/2406.03770v1","category":"quant-ph"}
{"created":"2024-06-06 04:27:53","title":"Prioritized-MVBA: A New Approach to Design an Optimal Asynchronous Byzantine Agreement Protocol","abstract":"The multi-valued byzantine agreement protocol (MVBA) in the authenticated setting has been widely used as a core to design atomic broadcast and fault-tolerant state machine replication protocols in asynchronous networks. Originating from the seminal work of Cachin et al. \\cite{CACHIN01}, subsequent research endeavors have sought to optimize protocol efficiency in terms of communication complexity. Notable advancements following Cachin's contributions include: i) VABA \\cite{BYZ17}, requiring multiple protocol instances to achieve agreement on a party's request, and ii) Dumbo-MVBA \\cite{LU20}, employing a cryptographic asynchronous dispersal and recovery methods to manage communication complexity alongside additional computational and communication rounds overheads.   Our objective is to devise an MVBA protocol that achieves agreement in each instance without extra computation and communication rounds while maintaining the optimal metrics. Central to our design approach is the introduction of the committee in the classic MVBA protocol, wherein a randomly selected subset of ($f+1$, where $n=3f+1$) parties get selected and simultaneously broadcast their requests (transactions) to gather verifiable proofs. Successive distributions of these proofs afford us the necessary properties to employ the asynchronous binary Byzantine agreement (ABBA) protocol for reaching an agreement on a selected party's requests. By integrating the committee and ABBA protocols, we devise the optimal MVBA protocol, termed pMVBA (Prioritized-MVBA). This protocol exhibits resilience to tolerate up to $\\lfloor \\frac{n}{3}\\rfloor$ Byzantine failures, with an expected runtime of $O(1)$, optimal message complexity of $O(n^2)$, and optimal communication complexity $O((l+\\lambda)n^2)$ .","sentences":["The multi-valued byzantine agreement protocol (MVBA) in the authenticated setting has been widely used as a core to design atomic broadcast and fault-tolerant state machine replication protocols in asynchronous networks.","Originating from the seminal work of Cachin et al. \\cite{CACHIN01}, subsequent research endeavors have sought to optimize protocol efficiency in terms of communication complexity.","Notable advancements following Cachin's contributions include: i) VABA \\cite{BYZ17}, requiring multiple protocol instances to achieve agreement on a party's request, and ii) Dumbo-MVBA \\cite{LU20}, employing a cryptographic asynchronous dispersal and recovery methods to manage communication complexity alongside additional computational and communication rounds overheads.   ","Our objective is to devise an MVBA protocol that achieves agreement in each instance without extra computation and communication rounds while maintaining the optimal metrics.","Central to our design approach is the introduction of the committee in the classic MVBA protocol, wherein a randomly selected subset of ($f+1$, where $n=3f+1$) parties get selected and simultaneously broadcast their requests (transactions) to gather verifiable proofs.","Successive distributions of these proofs afford us the necessary properties to employ the asynchronous binary Byzantine agreement (ABBA) protocol for reaching an agreement on a selected party's requests.","By integrating the committee and ABBA protocols, we devise the optimal MVBA protocol, termed pMVBA (Prioritized-MVBA).","This protocol exhibits resilience to tolerate up to $\\lfloor \\frac{n}{3}\\rfloor$ Byzantine failures, with an expected runtime of $O(1)$, optimal message complexity of $O(n^2)$, and optimal communication complexity $O((l+\\lambda)n^2)$ ."],"url":"http://arxiv.org/abs/2406.03739v1","category":"cs.DC"}
{"created":"2024-06-06 04:19:55","title":"Phase-Amplitude Reduction-Based Imitation Learning","abstract":"In this study, we propose the use of the phase-amplitude reduction method to construct an imitation learning framework. Imitating human movement trajectories is recognized as a promising strategy for generating a range of human-like robot movements. Unlike previous dynamical system-based imitation learning approaches, our proposed method allows the robot not only to imitate a limit cycle trajectory but also to replicate the transient movement from the initial or disturbed state to the limit cycle. Consequently, our method offers a safer imitation learning approach that avoids generating unpredictable motions immediately after disturbances or from a specified initial state. We first validated our proposed method by reconstructing a simple limit-cycle attractor. We then compared the proposed approach with a conventional method on a lemniscate trajectory tracking task with a simulated robot arm. Our findings confirm that our proposed method can more accurately generate transient movements to converge on a target periodic attractor compared to the previous standard approach. Subsequently, we applied our method to a real robot arm to imitate periodic human movements.","sentences":["In this study, we propose the use of the phase-amplitude reduction method to construct an imitation learning framework.","Imitating human movement trajectories is recognized as a promising strategy for generating a range of human-like robot movements.","Unlike previous dynamical system-based imitation learning approaches, our proposed method allows the robot not only to imitate a limit cycle trajectory but also to replicate the transient movement from the initial or disturbed state to the limit cycle.","Consequently, our method offers a safer imitation learning approach that avoids generating unpredictable motions immediately after disturbances or from a specified initial state.","We first validated our proposed method by reconstructing a simple limit-cycle attractor.","We then compared the proposed approach with a conventional method on a lemniscate trajectory tracking task with a simulated robot arm.","Our findings confirm that our proposed method can more accurately generate transient movements to converge on a target periodic attractor compared to the previous standard approach.","Subsequently, we applied our method to a real robot arm to imitate periodic human movements."],"url":"http://arxiv.org/abs/2406.03735v1","category":"cs.RO"}
{"created":"2024-06-06 04:14:06","title":"Policy Gradient Methods for the Cost-Constrained LQR: Strong Duality and Global Convergence","abstract":"In safety-critical applications, reinforcement learning (RL) needs to consider safety constraints. However, theoretical understandings of constrained RL for continuous control are largely absent. As a case study, this paper presents a cost-constrained LQR formulation, where a number of LQR costs with user-defined penalty matrices are subject to constraints. To solve it, we propose a policy gradient primal-dual method to find an optimal state feedback gain. Despite the non-convexity of the cost-constrained LQR problem, we provide a constructive proof for strong duality and a geometric interpretation of an optimal multiplier set. By proving that the concave dual function is Lipschitz smooth, we further provide convergence guarantees for the PG primal-dual method. Finally, we perform simulations to validate our theoretical findings.","sentences":["In safety-critical applications, reinforcement learning (RL) needs to consider safety constraints.","However, theoretical understandings of constrained RL for continuous control are largely absent.","As a case study, this paper presents a cost-constrained LQR formulation, where a number of LQR costs with user-defined penalty matrices are subject to constraints.","To solve it, we propose a policy gradient primal-dual method to find an optimal state feedback gain.","Despite the non-convexity of the cost-constrained LQR problem, we provide a constructive proof for strong duality and a geometric interpretation of an optimal multiplier set.","By proving that the concave dual function is Lipschitz smooth, we further provide convergence guarantees for the PG primal-dual method.","Finally, we perform simulations to validate our theoretical findings."],"url":"http://arxiv.org/abs/2406.03734v1","category":"math.OC"}
{"created":"2024-06-06 03:49:34","title":"Efficient Graph Encoder Embedding for Large Sparse Graphs in Python","abstract":"Graph is a ubiquitous representation of data in various research fields, and graph embedding is a prevalent machine learning technique for capturing key features and generating fixed-sized attributes. However, most state-of-the-art graph embedding methods are computationally and spatially expensive. Recently, the Graph Encoder Embedding (GEE) has been shown as the fastest graph embedding technique and is suitable for a variety of network data applications. As real-world data often involves large and sparse graphs, the huge sparsity usually results in redundant computations and storage. To address this issue, we propose an improved version of GEE, sparse GEE, which optimizes the calculation and storage of zero entries in sparse matrices to enhance the running time further. Our experiments demonstrate that the sparse version achieves significant speedup compared to the original GEE with Python implementation for large sparse graphs, and sparse GEE is capable of processing millions of edges within minutes on a standard laptop.","sentences":["Graph is a ubiquitous representation of data in various research fields, and graph embedding is a prevalent machine learning technique for capturing key features and generating fixed-sized attributes.","However, most state-of-the-art graph embedding methods are computationally and spatially expensive.","Recently, the Graph Encoder Embedding (GEE) has been shown as the fastest graph embedding technique and is suitable for a variety of network data applications.","As real-world data often involves large and sparse graphs, the huge sparsity usually results in redundant computations and storage.","To address this issue, we propose an improved version of GEE, sparse GEE, which optimizes the calculation and storage of zero entries in sparse matrices to enhance the running time further.","Our experiments demonstrate that the sparse version achieves significant speedup compared to the original GEE with Python implementation for large sparse graphs, and sparse GEE is capable of processing millions of edges within minutes on a standard laptop."],"url":"http://arxiv.org/abs/2406.03726v1","category":"cs.LG"}
{"created":"2024-06-06 02:55:16","title":"Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking","abstract":"Continuous action spaces in reinforcement learning (RL) are commonly defined as interval sets. While intervals usually reflect the action boundaries for tasks well, they can be challenging for learning because the typically large global action space leads to frequent exploration of irrelevant actions. Yet, little task knowledge can be sufficient to identify significantly smaller state-specific sets of relevant actions. Focusing learning on these relevant actions can significantly improve training efficiency and effectiveness. In this paper, we propose to focus learning on the set of relevant actions and introduce three continuous action masking methods for exactly mapping the action space to the state-dependent set of relevant actions. Thus, our methods ensure that only relevant actions are executed, enhancing the predictability of the RL agent and enabling its use in safety-critical applications. We further derive the implications of the proposed methods on the policy gradient. Using Proximal Policy Optimization (PPO), we evaluate our methods on three control tasks, where the relevant action set is computed based on the system dynamics and a relevant state set. Our experiments show that the three action masking methods achieve higher final rewards and converge faster than the baseline without action masking.","sentences":["Continuous action spaces in reinforcement learning (RL) are commonly defined as interval sets.","While intervals usually reflect the action boundaries for tasks well, they can be challenging for learning because the typically large global action space leads to frequent exploration of irrelevant actions.","Yet, little task knowledge can be sufficient to identify significantly smaller state-specific sets of relevant actions.","Focusing learning on these relevant actions can significantly improve training efficiency and effectiveness.","In this paper, we propose to focus learning on the set of relevant actions and introduce three continuous action masking methods for exactly mapping the action space to the state-dependent set of relevant actions.","Thus, our methods ensure that only relevant actions are executed, enhancing the predictability of the RL agent and enabling its use in safety-critical applications.","We further derive the implications of the proposed methods on the policy gradient.","Using Proximal Policy Optimization (PPO), we evaluate our methods on three control tasks, where the relevant action set is computed based on the system dynamics and a relevant state set.","Our experiments show that the three action masking methods achieve higher final rewards and converge faster than the baseline without action masking."],"url":"http://arxiv.org/abs/2406.03704v1","category":"cs.LG"}
{"created":"2024-06-06 02:26:14","title":"Discrete error dynamics of mini-batch gradient descent for least squares regression","abstract":"We study the discrete dynamics of mini-batch gradient descent for least squares regression when sampling without replacement. We show that the dynamics and generalization error of mini-batch gradient descent depends on a sample cross-covariance matrix $Z$ between the original features $X$ and a set of new features $\\widetilde{X}$, in which each feature is modified by the mini-batches that appear before it during the learning process in an averaged way. Using this representation, we rigorously establish that the dynamics of mini-batch and full-batch gradient descent agree up to leading order with respect to the step size using the linear scaling rule. We also study discretization effects that a continuous-time gradient flow analysis cannot detect, and show that mini-batch gradient descent converges to a step-size dependent solution, in contrast with full-batch gradient descent. Finally, we investigate the effects of batching, assuming a random matrix model, by using tools from free probability theory to numerically compute the spectrum of $Z$.","sentences":["We study the discrete dynamics of mini-batch gradient descent for least squares regression when sampling without replacement.","We show that the dynamics and generalization error of mini-batch gradient descent depends on a sample cross-covariance matrix $Z$ between the original features $X$ and a set of new features $\\widetilde{X}$, in which each feature is modified by the mini-batches that appear before it during the learning process in an averaged way.","Using this representation, we rigorously establish that the dynamics of mini-batch and full-batch gradient descent agree up to leading order with respect to the step size using the linear scaling rule.","We also study discretization effects that a continuous-time gradient flow analysis cannot detect, and show that mini-batch gradient descent converges to a step-size dependent solution, in contrast with full-batch gradient descent.","Finally, we investigate the effects of batching, assuming a random matrix model, by using tools from free probability theory to numerically compute the spectrum of $Z$."],"url":"http://arxiv.org/abs/2406.03696v1","category":"stat.ML"}
{"created":"2024-06-05 22:59:35","title":"Maximum Flow by Augmenting Paths in $n^{2+o(1)}$ Time","abstract":"We present a combinatorial algorithm for computing exact maximum flows in directed graphs with $n$ vertices and edge capacities from $\\{1,\\dots,U\\}$ in $n^{2+o(1)}\\log U$ time, which is almost optimal in dense graphs. Our algorithm is a novel implementation of the classical augmenting-path framework; we list augmenting paths more efficiently using a new variant of the push-relabel algorithm that uses additional edge weights to guide the algorithm, and we derive the edge weights by constructing a directed expander hierarchy.   Even in unit-capacity graphs, this breaks the long-standing $O(m\\cdot\\min\\{\\sqrt{m},n^{2/3}\\})$ time bound of the previous combinatorial algorithms by Karzanov (1973) and Even and Tarjan (1975) when the graph has $m=\\omega(n^{4/3})$ edges. Notably, our approach does not rely on continuous optimization nor heavy dynamic graph data structures, both of which are crucial in the recent developments that led to the almost-linear time algorithm by Chen et al. (FOCS 2022). Our running time also matches the $n^{2+o(1)}$ time bound of the independent combinatorial algorithm by Chuzhoy and Khanna (STOC 2024) for computing the maximum bipartite matching, a special case of maximum flow.","sentences":["We present a combinatorial algorithm for computing exact maximum flows in directed graphs with $n$ vertices and edge capacities from $\\{1,\\dots,U\\}$ in $n^{2+o(1)}\\log U$ time, which is almost optimal in dense graphs.","Our algorithm is a novel implementation of the classical augmenting-path framework; we list augmenting paths more efficiently using a new variant of the push-relabel algorithm that uses additional edge weights to guide the algorithm, and we derive the edge weights by constructing a directed expander hierarchy.   ","Even in unit-capacity graphs, this breaks the long-standing $O(m\\cdot\\min\\{\\sqrt{m},n^{2/3}\\})$ time bound of the previous combinatorial algorithms by Karzanov (1973) and Even and Tarjan (1975) when the graph has $m=\\omega(n^{4/3})$ edges.","Notably, our approach does not rely on continuous optimization nor heavy dynamic graph data structures, both of which are crucial in the recent developments that led to the almost-linear time algorithm by Chen et al. (FOCS 2022).","Our running time also matches the $n^{2+o(1)}$ time bound of the independent combinatorial algorithm by Chuzhoy and Khanna (STOC 2024) for computing the maximum bipartite matching, a special case of maximum flow."],"url":"http://arxiv.org/abs/2406.03648v1","category":"cs.DS"}
{"created":"2024-06-05 22:47:53","title":"On Exponential Convergence of Random Variables","abstract":"Given the discrete time sequence of nonnegative random variables, general dependencies between the exponential convergence of the expectations, exponential convergence of the trajectories and the convergence of the corresponding expected hitting times are analysed . The applications are presented: the general results are applied to the areas of optimization, control and estimation.","sentences":["Given the discrete time sequence of nonnegative random variables, general dependencies between the exponential convergence of the expectations, exponential convergence of the trajectories and the convergence of the corresponding expected hitting times are analysed .","The applications are presented: the general results are applied to the areas of optimization, control and estimation."],"url":"http://arxiv.org/abs/2406.03644v1","category":"math.PR"}
{"created":"2024-06-05 22:24:12","title":"Lithium Borohydride (LiBH4): An Innovative Material for Neutron Radiation Shielding","abstract":"Radiation shielding plays a crucial role in various industries, including nuclear and space exploration. Among the most abundant elements and isotopes found in nature, 10B has one of the highest neutron absorption cross-sections, closely followed by 6Li. It is worth noting that hydrogen, with its light nucleus, serves as an excellent neutron reflector. Surprisingly, the potential of the lithium borohydride molecule (LiBH4), which consists exclusively of these elements, as a shield against neutron radiation has not yet been explored. This study investigates various materials that can potentially be used as shields. First, we assessed traditional shields and previous optimizations for shielding. The findings showed that concrete containing 10% B4C yielded the best results. High-performance concrete (HPC) replaced regular concrete. By gradually incorporating lithium borohydride into the shield, along with the appropriate level of boron carbide, further optimization was achieved. Calculations were performed using the MCNPX 2.7E code. The introduction of the new shield resulted in a significant 40% reduction in volume compared with the previous sample. The study findings showed that a 30 cm thick shield effectively blocked 95% of the total neutrons and 92% of the total gamma radiation. Additionally, it was noted that the shielding effects of lithium borohydride against fast neutrons are greater than those of boron carbide. Various parameters and data of the designed shield were calculated and compared with those of the previous sample.","sentences":["Radiation shielding plays a crucial role in various industries, including nuclear and space exploration.","Among the most abundant elements and isotopes found in nature, 10B has one of the highest neutron absorption cross-sections, closely followed by 6Li.","It is worth noting that hydrogen, with its light nucleus, serves as an excellent neutron reflector.","Surprisingly, the potential of the lithium borohydride molecule (LiBH4), which consists exclusively of these elements, as a shield against neutron radiation has not yet been explored.","This study investigates various materials that can potentially be used as shields.","First, we assessed traditional shields and previous optimizations for shielding.","The findings showed that concrete containing 10% B4C yielded the best results.","High-performance concrete (HPC) replaced regular concrete.","By gradually incorporating lithium borohydride into the shield, along with the appropriate level of boron carbide, further optimization was achieved.","Calculations were performed using the MCNPX 2.7E code.","The introduction of the new shield resulted in a significant 40% reduction in volume compared with the previous sample.","The study findings showed that a 30 cm thick shield effectively blocked 95% of the total neutrons and 92% of the total gamma radiation.","Additionally, it was noted that the shielding effects of lithium borohydride against fast neutrons are greater than those of boron carbide.","Various parameters and data of the designed shield were calculated and compared with those of the previous sample."],"url":"http://arxiv.org/abs/2406.03640v1","category":"physics.atm-clus"}
{"created":"2024-06-05 22:17:47","title":"Style Mixture of Experts for Expressive Text-To-Speech Synthesis","abstract":"Recent advances in style transfer text-to-speech (TTS) have improved the expressiveness of synthesized speech. Despite these advancements, encoding stylistic information from diverse and unseen reference speech remains challenging. This paper introduces StyleMoE, an approach that divides the embedding space, modeled by the style encoder, into tractable subsets handled by style experts. The proposed method replaces the style encoder in a TTS system with a Mixture of Experts (MoE) layer. By utilizing a gating network to route reference speeches to different style experts, each expert specializes in aspects of the style space during optimization. Our experiments objectively and subjectively demonstrate the effectiveness of our proposed method in increasing the coverage of the style space for diverse and unseen styles. This approach can enhance the performance of existing state-of-the-art style transfer TTS models, marking the first study of MoE in style transfer TTS to our knowledge.","sentences":["Recent advances in style transfer text-to-speech (TTS) have improved the expressiveness of synthesized speech.","Despite these advancements, encoding stylistic information from diverse and unseen reference speech remains challenging.","This paper introduces StyleMoE, an approach that divides the embedding space, modeled by the style encoder, into tractable subsets handled by style experts.","The proposed method replaces the style encoder in a TTS system with a Mixture of Experts (MoE) layer.","By utilizing a gating network to route reference speeches to different style experts, each expert specializes in aspects of the style space during optimization.","Our experiments objectively and subjectively demonstrate the effectiveness of our proposed method in increasing the coverage of the style space for diverse and unseen styles.","This approach can enhance the performance of existing state-of-the-art style transfer TTS models, marking the first study of MoE in style transfer TTS to our knowledge."],"url":"http://arxiv.org/abs/2406.03637v1","category":"eess.AS"}
{"created":"2024-06-05 21:18:22","title":"Optimal Control and Glassiness in Quantum Sensing","abstract":"Quantum systems are powerful detectors with wide-ranging applications from scanning probe microscopy of materials to biomedical imaging. Nitrogen vacancy (NV) centers in diamond, for instance, can be operated as qubits for sensing of magnetic field, temperature, or related signals. By well-designed application of pulse sequences, experiments can filter this signal from environmental noise, allowing extremely sensitive measurements with single NV centers. Recently, optimal control has been used to further improve sensitivity by modification of the pulse sequence, most notably by optimal placement of $\\pi$ pulses. Here we consider extending beyond $\\pi$ pulses, exploring optimization of a continuous, time-dependent control field. We show that the difficulty of optimizing these protocols can be mapped to the difficulty of finding minimum free energy in a classical frustrated spin system. While most optimizations we consider show autocorrelations of the sensing protocol that grow as a power law -- similar to an Ising spin glass -- the continuous control shows slower logarithmic growth, suggestive of a harder Heisenberg-like glassy landscape.","sentences":["Quantum systems are powerful detectors with wide-ranging applications from scanning probe microscopy of materials to biomedical imaging.","Nitrogen vacancy (NV) centers in diamond, for instance, can be operated as qubits for sensing of magnetic field, temperature, or related signals.","By well-designed application of pulse sequences, experiments can filter this signal from environmental noise, allowing extremely sensitive measurements with single NV centers.","Recently, optimal control has been used to further improve sensitivity by modification of the pulse sequence, most notably by optimal placement of $\\pi$ pulses.","Here we consider extending beyond $\\pi$ pulses, exploring optimization of a continuous, time-dependent control field.","We show that the difficulty of optimizing these protocols can be mapped to the difficulty of finding minimum free energy in a classical frustrated spin system.","While most optimizations we consider show autocorrelations of the sensing protocol that grow as a power law -- similar to an Ising spin glass -- the continuous control shows slower logarithmic growth, suggestive of a harder Heisenberg-like glassy landscape."],"url":"http://arxiv.org/abs/2406.03627v1","category":"quant-ph"}
{"created":"2024-06-05 21:13:22","title":"Learning in Spatial Branching: Limitations of Strong Branching Imitation","abstract":"Over the last few years, there has been a surge in the use of learning techniques to improve the performance of optimization algorithms. In particular, the learning of branching rules in mixed integer linear programming has received a lot of attention, with most methodologies based on strong branching imitation. Recently, some advances have been made as well in the context of nonlinear programming, with some methodologies focusing on learning to select the best branching rule among a predefined set of rules leading to promising results. In this paper we explore, in the nonlinear setting, the limits on the improvements that might be achieved by the above two approaches: learning to select the best variable (strong branching) and learning to select the best rule (rule selection).","sentences":["Over the last few years, there has been a surge in the use of learning techniques to improve the performance of optimization algorithms.","In particular, the learning of branching rules in mixed integer linear programming has received a lot of attention, with most methodologies based on strong branching imitation.","Recently, some advances have been made as well in the context of nonlinear programming, with some methodologies focusing on learning to select the best branching rule among a predefined set of rules leading to promising results.","In this paper we explore, in the nonlinear setting, the limits on the improvements that might be achieved by the above two approaches: learning to select the best variable (strong branching) and learning to select the best rule (rule selection)."],"url":"http://arxiv.org/abs/2406.03626v1","category":"math.OC"}
{"created":"2024-06-05 18:53:20","title":"Optimization of Energy Consumption in Delay-Tolerant Networks","abstract":"Delay tolerant network is a network architecture and protocol suite specifically designed to handle challenging communications environments, such as deep space communications, disaster response, and remote area communications. Although DTN [1]can provide efficient and reliable data transmission in environments with high latency, unstable connections, and high bit error rates, its energy consumption optimization problem is still a challenge, especially in scenarios with limited resources.To solve this problem, this study combines the Epidemic[2] and MaxProp[3] routing protocols with Machine Learning Models to optimize the energy consumption of DTNs. Hundreds of simulations were conducted in the ONE simulator, and an external real-world dataset from San Francisco taxi mobility traces [54] was imported. Random Forest[4] and Gradient Boosting Machine (GBM)[5] models were employed for data analysis. Through optimization involving Hyperparameter Tuning and Feature Selection, the Random Forest model achieved an R-squared value of 0.53, while the GBM model achieved an R-squared value of 0.65.","sentences":["Delay tolerant network is a network architecture and protocol suite specifically designed to handle challenging communications environments, such as deep space communications, disaster response, and remote area communications.","Although DTN [1]can provide efficient and reliable data transmission in environments with high latency, unstable connections, and high bit error rates, its energy consumption optimization problem is still a challenge, especially in scenarios with limited resources.","To solve this problem, this study combines the Epidemic[2] and MaxProp[3] routing protocols with Machine Learning Models to optimize the energy consumption of DTNs.","Hundreds of simulations were conducted in the ONE simulator, and an external real-world dataset from San Francisco taxi mobility traces","[54] was imported.","Random Forest[4] and Gradient Boosting Machine (GBM)[5] models were employed for data analysis.","Through optimization involving Hyperparameter Tuning and Feature Selection, the Random Forest model achieved an R-squared value of 0.53, while the GBM model achieved an R-squared value of 0.65."],"url":"http://arxiv.org/abs/2406.03580v1","category":"cs.NI"}
{"created":"2024-06-05 18:33:20","title":"Exceptional Fano varieties with small minimal log discrepancy","abstract":"We construct exceptional Fano varieties with the smallest known minimal log discrepancies in all dimensions. These varieties are well-formed hypersurfaces in weighted projective space. Their minimal log discrepancies decay doubly exponentially with dimension, and achieve the optimal value in dimension 2.","sentences":["We construct exceptional Fano varieties with the smallest known minimal log discrepancies in all dimensions.","These varieties are well-formed hypersurfaces in weighted projective space.","Their minimal log discrepancies decay doubly exponentially with dimension, and achieve the optimal value in dimension 2."],"url":"http://arxiv.org/abs/2406.03570v1","category":"math.AG"}
{"created":"2024-06-05 18:25:31","title":"Second-Order Algorithms for Finding Local Nash Equilibria in Zero-Sum Games","abstract":"Zero-sum games arise in a wide variety of problems, including robust optimization and adversarial learning. However, algorithms deployed for finding a local Nash equilibrium in these games often converge to non-Nash stationary points. This highlights a key challenge: for any algorithm, the stability properties of its underlying dynamical system can cause non-Nash points to be potential attractors. To overcome this challenge, algorithms must account for subtleties involving the curvatures of players' costs. To this end, we leverage dynamical system theory and develop a second-order algorithm for finding a local Nash equilibrium in the smooth, possibly nonconvex-nonconcave, zero-sum game setting. First, we prove that this novel method guarantees convergence to only local Nash equilibria with a local linear convergence rate. We then interpret a version of this method as a modified Gauss-Newton algorithm with local superlinear convergence to the neighborhood of a point that satisfies first-order local Nash equilibrium conditions. In comparison, current related state-of-the-art methods do not offer convergence rate guarantees. Furthermore, we show that this approach naturally generalizes to settings with convex and potentially coupled constraints while retaining earlier guarantees of convergence to only local (generalized) Nash equilibria.","sentences":["Zero-sum games arise in a wide variety of problems, including robust optimization and adversarial learning.","However, algorithms deployed for finding a local Nash equilibrium in these games often converge to non-Nash stationary points.","This highlights a key challenge: for any algorithm, the stability properties of its underlying dynamical system can cause non-Nash points to be potential attractors.","To overcome this challenge, algorithms must account for subtleties involving the curvatures of players' costs.","To this end, we leverage dynamical system theory and develop a second-order algorithm for finding a local Nash equilibrium in the smooth, possibly nonconvex-nonconcave, zero-sum game setting.","First, we prove that this novel method guarantees convergence to only local Nash equilibria with a local linear convergence rate.","We then interpret a version of this method as a modified Gauss-Newton algorithm with local superlinear convergence to the neighborhood of a point that satisfies first-order local Nash equilibrium conditions.","In comparison, current related state-of-the-art methods do not offer convergence rate guarantees.","Furthermore, we show that this approach naturally generalizes to settings with convex and potentially coupled constraints while retaining earlier guarantees of convergence to only local (generalized)","Nash equilibria."],"url":"http://arxiv.org/abs/2406.03565v1","category":"cs.GT"}
{"created":"2024-06-05 18:15:33","title":"Anatomy-based quality metric of diffusion-weighted MRI data for accurate derivation of muscle fiber orientation","abstract":"Diffusion-weighted MRI (DW-MRI) is used to quantitatively characterize the microscopic structure of soft tissue due to the anisotropic diffusion of water in muscle. Applications such as fiber tractography or modeling of tumor spread in soft tissue require precise detection of muscle fiber orientation, which is derived from the principal eigenvector of the diffusion tensor. For clinical applications, high image quality and high signal-to-noise ratio (SNR) of DW-MRI for fiber orientation must be balanced with an appropriate scan duration. Muscles with known structural heterogeneity, e.g. bipennate muscles such as the thigh rectus femoris, provide a natural quality benchmark to determine fiber orientation at different scan parameters. Here, we analyze DW-MR images of the thigh of a healthy volunteer at different SNRs and use PCA to identify subsets of voxels with different directions of diffusion tensor eigenvectors. We propose to use the mixing index of spatial co-localization of the clustered eigenvectors as a quality metric for fiber orientation detection. Comparing acquisitions at different SNRs, we find that high SNR results in a low mixing index, reflecting a clear separation of the two compartments of the bipennate muscle on either side of the central tendon. Because the mixing index allows joint estimation of spatial and directional noise in DW-MRI as a single parameter, it will allow future quantitative optimization of DW-MRI protocols for soft tissue.","sentences":["Diffusion-weighted MRI (DW-MRI) is used to quantitatively characterize the microscopic structure of soft tissue due to the anisotropic diffusion of water in muscle.","Applications such as fiber tractography or modeling of tumor spread in soft tissue require precise detection of muscle fiber orientation, which is derived from the principal eigenvector of the diffusion tensor.","For clinical applications, high image quality and high signal-to-noise ratio (SNR) of DW-MRI for fiber orientation must be balanced with an appropriate scan duration.","Muscles with known structural heterogeneity, e.g. bipennate muscles such as the thigh rectus femoris, provide a natural quality benchmark to determine fiber orientation at different scan parameters.","Here, we analyze DW-MR images of the thigh of a healthy volunteer at different SNRs and use PCA to identify subsets of voxels with different directions of diffusion tensor eigenvectors.","We propose to use the mixing index of spatial co-localization of the clustered eigenvectors as a quality metric for fiber orientation detection.","Comparing acquisitions at different SNRs, we find that high SNR results in a low mixing index, reflecting a clear separation of the two compartments of the bipennate muscle on either side of the central tendon.","Because the mixing index allows joint estimation of spatial and directional noise in DW-MRI as a single parameter, it will allow future quantitative optimization of DW-MRI protocols for soft tissue."],"url":"http://arxiv.org/abs/2406.03560v1","category":"physics.med-ph"}
{"created":"2024-06-05 18:00:06","title":"Impact of correlations on nuclear binding energies","abstract":"A strong effort will be dedicated in the coming years to extend the reach of ab initio nuclear-structure calculations to heavy doubly open-shell nuclei. In order to do so, the most efficient strategies to incorporate dominant many-body correlations at play in such nuclei must be identified. With this motivation in mind, the present work pedagogically analyses the inclusion of many-body correlations and their impact on binding energies of Calcium and Chromium isotopes. Employing an empirically-optimal Hamiltonian built from chiral effective field theory, binding energies along both isotopic chains are studied via a hierarchy of approximations based on polynomially-scaling expansion many-body methods. The corresponding results are compared to experimental data and to those obtained via valence-space in-medium similarity renormalization group calculations at the normal-ordered two-body level that act as a reference in the present study. The spherical mean-field approximation is shown to display specific shortcomings in Ca isotopes that can be understood analytically and that are efficiently corrected via the consistent addition of low-order dynamical correlations on top of it. While the same setting cannot appropriately reproduce binding energies in doubly open-shell Cr isotopes, allowing the unperturbed mean-field state to break rotational symmetry permits to efficiently capture the static correlations responsible for the phenomenological differences observed between the two isotopic chains. Eventually, the present work demonstrates in a pedagogical way that polynomially-scaling expansion methods based on unperturbed states that possibly break (and restore) symmetries constitute an optimal route to extend ab initio calculations to heavy closed- and open-shell nuclei.","sentences":["A strong effort will be dedicated in the coming years to extend the reach of ab initio nuclear-structure calculations to heavy doubly open-shell nuclei.","In order to do so, the most efficient strategies to incorporate dominant many-body correlations at play in such nuclei must be identified.","With this motivation in mind, the present work pedagogically analyses the inclusion of many-body correlations and their impact on binding energies of Calcium and Chromium isotopes.","Employing an empirically-optimal Hamiltonian built from chiral effective field theory, binding energies along both isotopic chains are studied via a hierarchy of approximations based on polynomially-scaling expansion many-body methods.","The corresponding results are compared to experimental data and to those obtained via valence-space in-medium similarity renormalization group calculations at the normal-ordered two-body level that act as a reference in the present study.","The spherical mean-field approximation is shown to display specific shortcomings in Ca isotopes that can be understood analytically and that are efficiently corrected via the consistent addition of low-order dynamical correlations on top of it.","While the same setting cannot appropriately reproduce binding energies in doubly open-shell Cr isotopes, allowing the unperturbed mean-field state to break rotational symmetry permits to efficiently capture the static correlations responsible for the phenomenological differences observed between the two isotopic chains.","Eventually, the present work demonstrates in a pedagogical way that polynomially-scaling expansion methods based on unperturbed states that possibly break (and restore) symmetries constitute an optimal route to extend ab initio calculations to heavy closed- and open-shell nuclei."],"url":"http://arxiv.org/abs/2406.03545v1","category":"nucl-th"}
{"created":"2024-06-05 18:00:03","title":"Corners and Islands in the S-matrix Bootstrap of the Open Superstring","abstract":"We bootstrap the Veneziano superstring amplitude in 10 dimensions from the bottom-up. Starting with the most general maximally supersymmetric Yang-Mills EFT, we input information about the lowest-lying massive states, which we assume contribute via tree-level exchanges to the 4-point amplitude. We show the following: (1) if there is only a single state at the lowest mass, it must be a scalar. (2) Assuming a string-inspired gap between the mass of this scalar and any other massive states, the allowed region of Wilson coefficients has a new sharp corner where the Veneziano amplitude is located. (3) Upon fixing the next massive state to be a vector, the EFT bounds have a one-parameter family of corners; these would correspond to models with linear Regge trajectories of varying slopes, one of which is the open superstring. (4) When the ratio between the massive scalar coupling and the $\\text{tr}\\, F^4$ coefficient is fixed to its string value, the spin and mass of the second massive state is determined by the bootstrap and the Veneziano amplitude is isolated on a small island in parameter space. Finally, we compare with other recent bootstraps approaches, both the pion model and imposing Regge-inspired maximal spin constraints.","sentences":["We bootstrap the Veneziano superstring amplitude in 10 dimensions from the bottom-up.","Starting with the most general maximally supersymmetric Yang-Mills EFT, we input information about the lowest-lying massive states, which we assume contribute via tree-level exchanges to the 4-point amplitude.","We show the following: (1) if there is only a single state at the lowest mass, it must be a scalar.","(2) Assuming a string-inspired gap between the mass of this scalar and any other massive states, the allowed region of Wilson coefficients has a new sharp corner where the Veneziano amplitude is located.","(3) Upon fixing the next massive state to be a vector, the EFT bounds have a one-parameter family of corners; these would correspond to models with linear Regge trajectories of varying slopes, one of which is the open superstring.","(4) When the ratio between the massive scalar coupling and the $\\text{tr}\\, F^4$ coefficient is fixed to its string value, the spin and mass of the second massive state is determined by the bootstrap and the Veneziano amplitude is isolated on a small island in parameter space.","Finally, we compare with other recent bootstraps approaches, both the pion model and imposing Regge-inspired maximal spin constraints."],"url":"http://arxiv.org/abs/2406.03543v1","category":"hep-th"}
{"created":"2024-06-05 17:18:17","title":"The SMILES Mid-Infrared Survey","abstract":"The Mid-Infrared Instrument (MIRI) for JWST is supplied with a suite of imaging bandpass filters optimized for full spectral coverage in eight intermediate-width bands from 5 to 26 microns and a narrower one at 11.3 microns. This contrasts with previous infrared space telescopes, which generally have provided only two broad bands, one near 10 microns and the other near 20 microns. The expanded MIRI spectral capability provides new possibilities for detailed interpretation of survey results. This is an important feature of the instrument, on top of its great increase in sensitivity and angular resolution over any previous mission. The Systematic Mid-infrared Instrument Legacy Extragalactic Survey (SMILES) was designed to take full advantage of this capability. This paper briefly describes the history of infrared surveys that paved the way for MIRI on JWST and for our approach to designng SMILES. It illustrates the use of the observations for a broad range of science programs, and concludes with a brief summary of the need for additional surveys with JWST/MIRI.","sentences":["The Mid-Infrared Instrument (MIRI) for JWST is supplied with a suite of imaging bandpass filters optimized for full spectral coverage in eight intermediate-width bands from 5 to 26 microns and a narrower one at 11.3 microns.","This contrasts with previous infrared space telescopes, which generally have provided only two broad bands, one near 10 microns and the other near 20 microns.","The expanded MIRI spectral capability provides new possibilities for detailed interpretation of survey results.","This is an important feature of the instrument, on top of its great increase in sensitivity and angular resolution over any previous mission.","The Systematic Mid-infrared Instrument Legacy Extragalactic Survey (SMILES) was designed to take full advantage of this capability.","This paper briefly describes the history of infrared surveys that paved the way for MIRI on JWST and for our approach to designng SMILES.","It illustrates the use of the observations for a broad range of science programs, and concludes with a brief summary of the need for additional surveys with JWST/MIRI."],"url":"http://arxiv.org/abs/2406.03518v1","category":"astro-ph.GA"}
{"created":"2024-06-06 17:38:38","title":"Wilson Loops with Lagrangians: large spin OPE and cusp anomalous dimension dictionary","abstract":"In the context of planar conformal gauge theory, we study five-point correlation functions between the interaction Lagrangian and four of the lightest single-trace, gauge-invariant scalar primaries. After performing two light-cone OPEs, we express this correlator in terms of the three-point functions between two leading-twist spinning operators and the Lagrangian. For finite values of spin, we compute these structure constants in perturbation theory up to two loops in $\\mathcal{N}=4$ Super Yang-Mills theory. Large values of spin are captured by null polygon kinematics, where we use dualities with null polygon Wilson loops as well as factorization properties to bootstrap the universal behavior of the structure constants at all loops. We find explicit maps that relate the Lagrangian structure constants with the leading-twist anomalous dimension. From the large-spin map, we recover the cusp anomalous dimension at strong and weak coupling, including genus-one terms.","sentences":["In the context of planar conformal gauge theory, we study five-point correlation functions between the interaction Lagrangian and four of the lightest single-trace, gauge-invariant scalar primaries.","After performing two light-cone OPEs, we express this correlator in terms of the three-point functions between two leading-twist spinning operators and the Lagrangian.","For finite values of spin, we compute these structure constants in perturbation theory up to two loops in $\\mathcal{N}=4$ Super Yang-Mills theory.","Large values of spin are captured by null polygon kinematics, where we use dualities with null polygon Wilson loops as well as factorization properties to bootstrap the universal behavior of the structure constants at all loops.","We find explicit maps that relate the Lagrangian structure constants with the leading-twist anomalous dimension.","From the large-spin map, we recover the cusp anomalous dimension at strong and weak coupling, including genus-one terms."],"url":"http://arxiv.org/abs/2406.04294v1","category":"hep-th"}
{"created":"2024-06-06 16:21:57","title":"Homology of spectral minimal partitions","abstract":"A spectral minimal partition of a manifold is its decomposition into disjoint open sets that minimizes a spectral energy functional. It is known that bipartite spectral minimal partitions coincide with nodal partitions of Courant-sharp Laplacian eigenfunctions. However, almost all minimal partitions are non-bipartite. To study those, we define a modified Laplacian operator and prove that the nodal partitions of its Courant-sharp eigenfunctions are minimal within a certain topological class of partitions. This yields new results in the non-bipartite case and recovers the above known result in the bipartite case. Our approach is based on tools from algebraic topology, which we illustrate by a number of examples where the topological types of partitions are characterized by relative homology.","sentences":["A spectral minimal partition of a manifold is its decomposition into disjoint open sets that minimizes a spectral energy functional.","It is known that bipartite spectral minimal partitions coincide with nodal partitions of Courant-sharp Laplacian eigenfunctions.","However, almost all minimal partitions are non-bipartite.","To study those, we define a modified Laplacian operator and prove that the nodal partitions of its Courant-sharp eigenfunctions are minimal within a certain topological class of partitions.","This yields new results in the non-bipartite case and recovers the above known result in the bipartite case.","Our approach is based on tools from algebraic topology, which we illustrate by a number of examples where the topological types of partitions are characterized by relative homology."],"url":"http://arxiv.org/abs/2406.04225v1","category":"math.AP"}
{"created":"2024-06-06 15:39:30","title":"Minimal W-algebras with non-admissible levels and intermediate Lie algebras","abstract":"In \\cite{Kawasetsu:2018irs}, Kawasetsu proved that the simple W-algebra associated with a minimal nilpotent element $W_{k}(\\mathfrak{g},f_\\theta)$ is rational and $C_2$-cofinite for $\\mathfrak{g}=D_4,E_6,E_7,E_8$ with non-admissible level $k=-h^\\vee/6$. In this paper, we study ${W}_{k}(\\mathfrak{g},f_\\theta)$ algebra for $\\mathfrak{g}=E_6,E_7,E_8$ with non-admissible level $k=-h^\\vee/6+1$. We determine all irreducible (Ramond twisted) modules, compute their characters and find coset constructions and Hecke operator interpretations. These W-algebras are closely related to intermediate Lie algebras and intermediate vertex subalgebras.","sentences":["In \\cite{Kawasetsu:2018irs}, Kawasetsu proved that the simple W-algebra associated with a minimal nilpotent element $W_{k}(\\mathfrak{g},f_\\theta)$ is rational and $C_2$-cofinite for $\\mathfrak{g}=D_4,E_6,E_7,E_8$ with non-admissible level $k=-h^\\vee/6$. In this paper, we study ${W}_{k}(\\mathfrak{g},f_\\theta)$ algebra for $\\mathfrak{g}=E_6,E_7,E_8$ with non-admissible level $k=-h^\\vee/6+1$.","We determine all irreducible (Ramond twisted) modules, compute their characters and find coset constructions and Hecke operator interpretations.","These W-algebras are closely related to intermediate Lie algebras and intermediate vertex subalgebras."],"url":"http://arxiv.org/abs/2406.04182v1","category":"math-ph"}
{"created":"2024-06-06 15:31:01","title":"An improved description of charm fragmentation data","abstract":"We consider the fragmentation of heavy quarks into heavy-flavoured hadrons, specifically the production of charmed mesons in $e^+e^-$ collisions, at different centre-of-mass energies. We focus our attention on the ratio of moments of the $D^{*+}$ energy spectrum measured by ALEPH and CLEO. This ratio is believed to provide us with a direct test of perturbative QCD evolution because hadronisation effects should cancel between the numerator and denominator. However, state-of-the-art calculations based on standard (final-state) collinear factorisation fail to describe the experimental data. We show that this discrepancy is considerably reduced if heavy-quark threshold effects are accounted for not only in DGLAP evolution, as it is usually done, but also in the resummed coefficient functions.","sentences":["We consider the fragmentation of heavy quarks into heavy-flavoured hadrons, specifically the production of charmed mesons in $e^+e^-$ collisions, at different centre-of-mass energies.","We focus our attention on the ratio of moments of the $D^{*+}$ energy spectrum measured by ALEPH and CLEO.","This ratio is believed to provide us with a direct test of perturbative QCD evolution because hadronisation effects should cancel between the numerator and denominator.","However, state-of-the-art calculations based on standard (final-state) collinear factorisation fail to describe the experimental data.","We show that this discrepancy is considerably reduced if heavy-quark threshold effects are accounted for not only in DGLAP evolution, as it is usually done, but also in the resummed coefficient functions."],"url":"http://arxiv.org/abs/2406.04173v1","category":"hep-ph"}
{"created":"2024-06-06 15:21:08","title":"Scattering of Vortices with Excited Normal Modes","abstract":"We consider head-on collisions at critical coupling of vortices modelled by the Abelian-Higgs model. We investigate the 2-vortex scattering, whereby the vortices are excited by the shape mode causing fluctuations in the gauge-invariant quantities. When the vortices are excited with a sufficiently large amplitude the moduli space approximation fails, and we observe an interesting behaviour in which the vortices can become trapped in a quasi-bound state with multiple bounces. We perform a detailed investigation on the behaviour of these excited vortices and sample a phase space of solutions. Interestingly, we find a fractal structure dependent on the initial phase of the mode and velocity of the vortices.","sentences":["We consider head-on collisions at critical coupling of vortices modelled by the Abelian-Higgs model.","We investigate the 2-vortex scattering, whereby the vortices are excited by the shape mode causing fluctuations in the gauge-invariant quantities.","When the vortices are excited with a sufficiently large amplitude the moduli space approximation fails, and we observe an interesting behaviour in which the vortices can become trapped in a quasi-bound state with multiple bounces.","We perform a detailed investigation on the behaviour of these excited vortices and sample a phase space of solutions.","Interestingly, we find a fractal structure dependent on the initial phase of the mode and velocity of the vortices."],"url":"http://arxiv.org/abs/2406.04164v1","category":"math-ph"}
{"created":"2024-06-06 14:51:39","title":"GLOBUS: Global building renovation potential by 2070","abstract":"Surpassing the two large emission sectors of transportation and industry, the building sector accounted for 34% and 37% of global energy consumption and carbon emissions in 2021, respectively. The building sector, the final piece to be addressed in the transition to net-zero carbon emissions, requires a comprehensive, multisectoral strategy for reducing emissions. Until now, the absence of data on global building floorspace has impeded the measurement of building carbon intensity (carbon emissions per floorspace) and the identification of ways to achieve carbon neutrality for buildings. For this study, we develop a global building stock model (GLOBUS) to fill that data gap. Our study's primary contribution lies in providing a dataset of global building stock turnover using scenarios that incorporate various levels of building renovation. By unifying the evaluation indicators, the dataset empowers building science researchers to perform comparative analyses based on floorspace. Specifically, the building stock dataset establishes a reference for measuring carbon emission intensity and decarbonization intensity of buildings within different countries. Further, we emphasize the sufficiency of existing buildings by incorporating building renovation into the model. Renovation can minimize the need to expand the building stock, thereby bolstering decarbonization of the building sector.","sentences":["Surpassing the two large emission sectors of transportation and industry, the building sector accounted for 34% and 37% of global energy consumption and carbon emissions in 2021, respectively.","The building sector, the final piece to be addressed in the transition to net-zero carbon emissions, requires a comprehensive, multisectoral strategy for reducing emissions.","Until now, the absence of data on global building floorspace has impeded the measurement of building carbon intensity (carbon emissions per floorspace) and the identification of ways to achieve carbon neutrality for buildings.","For this study, we develop a global building stock model (GLOBUS) to fill that data gap.","Our study's primary contribution lies in providing a dataset of global building stock turnover using scenarios that incorporate various levels of building renovation.","By unifying the evaluation indicators, the dataset empowers building science researchers to perform comparative analyses based on floorspace.","Specifically, the building stock dataset establishes a reference for measuring carbon emission intensity and decarbonization intensity of buildings within different countries.","Further, we emphasize the sufficiency of existing buildings by incorporating building renovation into the model.","Renovation can minimize the need to expand the building stock, thereby bolstering decarbonization of the building sector."],"url":"http://arxiv.org/abs/2406.04133v1","category":"econ.EM"}
{"created":"2024-06-06 14:47:44","title":"Light quark mass dependence of nucleon mass to two-loop order","abstract":"We investigate the nucleon self energy through the sixth chiral order in the covariant $SU(2)$ chiral perturbation theory ($\\chi$PT) in the single baryon sector. The validity of the extended on-mass-shell (EOMS) renormalization scheme is explicitly verified to two-loop order, manifested by the miraculous cancellation of all nonlocal divergences and power-counting-breaking (PCB) terms that are nonanalytic in pion mass. Using the $\\sigma_{\\pi N}$ term determined from the latest lattice simulation to constrain some unknown higher-order low energy constants (LECs), we predict the nucleon mass in the chiral limit to be $856.6\\pm 1.7$ MeV. It is found that the EOMS scheme exhibits quite satisfactory convergence behavior through ${\\cal O}(q^6)$ around physical point. We also predict the pion mass dependence of the nucleon mass to the accuracy of ${\\cal O}(q^6)$, which is in satisfactory agreement with the recent lattice results over a wide range of pion mass.","sentences":["We investigate the nucleon self energy through the sixth chiral order in the covariant $SU(2)$ chiral perturbation theory ($\\chi$PT) in the single baryon sector.","The validity of the extended on-mass-shell (EOMS) renormalization scheme is explicitly verified to two-loop order, manifested by the miraculous cancellation of all nonlocal divergences and power-counting-breaking (PCB) terms that are nonanalytic in pion mass.","Using the $\\sigma_{\\pi N}$ term determined from the latest lattice simulation to constrain some unknown higher-order low energy constants (LECs), we predict the nucleon mass in the chiral limit to be $856.6\\pm 1.7$ MeV.","It is found that the EOMS scheme exhibits quite satisfactory convergence behavior through ${\\cal O}(q^6)$ around physical point.","We also predict the pion mass dependence of the nucleon mass to the accuracy of ${\\cal O}(q^6)$, which is in satisfactory agreement with the recent lattice results over a wide range of pion mass."],"url":"http://arxiv.org/abs/2406.04124v1","category":"hep-ph"}
{"created":"2024-06-06 13:43:17","title":"The Vela Pulsar Progenitor Was Most Likely a Binary Merger","abstract":"Stellar evolution theory restricted to single stars predicts a minimum mass for core-collapse supernovae (CCSNe) of around eight solar masses; this minimum mass corresponds to a maximum age of around 45 million years for the progenitor and the coeval population of stars. Binary evolution complicates this prediction. For example, an older stellar population around 100 million years could contain stellar mergers that reach the minimum mass for core collapse. Despite this clear prediction by binary evolution, there are few, if any CCSNe associated with a distinctly older stellar population...until now. The stellar population within 150 pc of the Vela Pulsar is inconsistent with single-star evolution only; instead, the most likely solution is that the stellar population is $\\ge$80 Myr old, and the brightest stars are mass gainers and/or mergers, the result of binary evolution. The evidence is as follows. Even though the main sequence is clearly dominated by a $\\ge$80-Myr-old population, a large fraction of the corresponding red giants is missing. The best-fitting single-star model expects 51.5 red giants, yet there are only 22; the Poisson probability of this is $1.7 \\times 10^{-6}$. In addition, there is an overabundance of bright, young-looking stars (25-30 Myrs old), yet there is not a corresponding young main sequence (MS). Upon closer inspection, the vast majority of the young-looking stars show either past or current signs of binary evolution. These new results are possible due to exquisite Gaia parallaxes and a new age-dating software called {\\it Stellar Ages}.","sentences":["Stellar evolution theory restricted to single stars predicts a minimum mass for core-collapse supernovae (CCSNe) of around eight solar masses; this minimum mass corresponds to a maximum age of around 45 million years for the progenitor and the coeval population of stars.","Binary evolution complicates this prediction.","For example, an older stellar population around 100 million years could contain stellar mergers that reach the minimum mass for core collapse.","Despite this clear prediction by binary evolution, there are few, if any CCSNe associated with a distinctly older stellar population...until now.","The stellar population within 150 pc of the Vela Pulsar is inconsistent with single-star evolution only; instead, the most likely solution is that the stellar population is $\\ge$80 Myr old, and the brightest stars are mass gainers and/or mergers, the result of binary evolution.","The evidence is as follows.","Even though the main sequence is clearly dominated by a $\\ge$80-Myr-old population, a large fraction of the corresponding red giants is missing.","The best-fitting single-star model expects 51.5 red giants, yet there are only 22; the Poisson probability of this is $1.7 \\times 10^{-6}$.","In addition, there is an overabundance of bright, young-looking stars (25-30 Myrs old), yet there is not a corresponding young main sequence (MS).","Upon closer inspection, the vast majority of the young-looking stars show either past or current signs of binary evolution.","These new results are possible due to exquisite Gaia parallaxes and a new age-dating software called {\\it Stellar Ages}."],"url":"http://arxiv.org/abs/2406.04075v1","category":"astro-ph.SR"}
{"created":"2024-06-06 13:37:19","title":"Variational Prior Replacement in Bayesian Inference and Inversion","abstract":"Many scientific investigations require that the values of a set of model parameters are estimated using recorded data. In Bayesian inference, information from both observed data and prior knowledge is combined to update model parameters probabilistically. Prior information represents our belief about the range of values that the variables can take, and their relative probabilities when considered independently of recorded data. Situations arise in which we wish to change prior information: (i) the subjective nature of prior information, (ii) cases in which we wish to test different states of prior information as hypothesis tests, and (iii) information from new studies may emerge so prior information may evolve over time. Estimating the solution to any single inference problem is usually computationally costly, as it typically requires thousands of model samples and their forward simulations. Therefore, recalculating the Bayesian solution every time prior information changes can be extremely expensive. We develop a mathematical formulation that allows prior information to be changed in a solution using variational methods, without performing Bayesian inference on each occasion. In this method, existing prior information is removed from a previously obtained posterior distribution and is replaced by new prior information. We therefore call the methodology variational prior replacement (VPR). We demonstrate VPR using a 2D seismic full waveform inversion example, where VPR provides almost identical posterior solutions compared to those obtained by solving independent inference problems using different priors. The former can be completed within minutes even on a laptop whereas the latter requires days of computations using high-performance computing resources. We demonstrate the value of the method by comparing the posterior solutions obtained using three different types of prior information.","sentences":["Many scientific investigations require that the values of a set of model parameters are estimated using recorded data.","In Bayesian inference, information from both observed data and prior knowledge is combined to update model parameters probabilistically.","Prior information represents our belief about the range of values that the variables can take, and their relative probabilities when considered independently of recorded data.","Situations arise in which we wish to change prior information: (i) the subjective nature of prior information, (ii) cases in which we wish to test different states of prior information as hypothesis tests, and (iii) information from new studies may emerge so prior information may evolve over time.","Estimating the solution to any single inference problem is usually computationally costly, as it typically requires thousands of model samples and their forward simulations.","Therefore, recalculating the Bayesian solution every time prior information changes can be extremely expensive.","We develop a mathematical formulation that allows prior information to be changed in a solution using variational methods, without performing Bayesian inference on each occasion.","In this method, existing prior information is removed from a previously obtained posterior distribution and is replaced by new prior information.","We therefore call the methodology variational prior replacement (VPR).","We demonstrate VPR using a 2D seismic full waveform inversion example, where VPR provides almost identical posterior solutions compared to those obtained by solving independent inference problems using different priors.","The former can be completed within minutes even on a laptop whereas the latter requires days of computations using high-performance computing resources.","We demonstrate the value of the method by comparing the posterior solutions obtained using three different types of prior information."],"url":"http://arxiv.org/abs/2406.04072v1","category":"stat.ME"}
{"created":"2024-06-06 13:32:56","title":"Magnon sensing of NO, NO$_2$ and NH$_3$ gas capture on CrSBr monolayer","abstract":"Air pollution and greenhouse emissions are a significant problem across various sectors, urging the need for advanced technologies to detect and capture harmful gases. In recent years, two-dimensional (2D) materials have attracted an increasing attention due to their large surface-to-volume ratio and reactivity. Herein, we investigate the potential of single-layer CrSBr for gas sensing and capturing by means of first-principles calculations. We explore the adsorption behaviour of different pollutant gases (H$_2$S, NH$_3$, NO, NO$_2$, CO and CO$_2$) on this 2D ferromagnet and the impact of intrinsic defects on its magnetic properties. Interestingly, we find that Br vacancies enhance the adsorption of NH$_3$, NO and NO$_2$ and induces a selective frequency shift on the magnon dispersion. This work motivates the creation of novel magnonic gas sensing devices based on 2D van der Waals magnetic materials.","sentences":["Air pollution and greenhouse emissions are a significant problem across various sectors, urging the need for advanced technologies to detect and capture harmful gases.","In recent years, two-dimensional (2D) materials have attracted an increasing attention due to their large surface-to-volume ratio and reactivity.","Herein, we investigate the potential of single-layer CrSBr for gas sensing and capturing by means of first-principles calculations.","We explore the adsorption behaviour of different pollutant gases (H$_2$S, NH$_3$, NO, NO$_2$, CO and CO$_2$) on this 2D ferromagnet and the impact of intrinsic defects on its magnetic properties.","Interestingly, we find that Br vacancies enhance the adsorption of NH$_3$, NO and NO$_2$ and induces a selective frequency shift on the magnon dispersion.","This work motivates the creation of novel magnonic gas sensing devices based on 2D van der Waals magnetic materials."],"url":"http://arxiv.org/abs/2406.04067v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-06 12:51:14","title":"The syntax-semantics interface in a child's path: A study of 3- to 11-year-olds' elicited production of Mandarin recursive relative clauses","abstract":"There have been apparently conflicting claims over the syntax-semantics relationship in child acquisition. However, few of them have assessed the child's path toward the acquisition of recursive relative clauses (RRCs). The authors of the current paper did experiments to investigate 3- to 11-year-olds' most-structured elicited production of eight Mandarin RRCs in a 4 (syntactic types)*2 (semantic conditions) design. The four syntactic types were RRCs with a subject-gapped RC embedded in an object-gapped RC (SORRCs), RRCs with an object-gapped RC embedded in another object-gapped RC (OORRCs), RRCs with an object-gapped RC embedded in a subject-gapped RC (OSRRCs), and RRCs with a subject-gapped RC embedded in another subject-gapped RC (SSRRCs). Each syntactic type was put in two conditions differing in internal semantics: irreversible internal semantics (IIS) and reversible internal semantics (RIS). For example, \"the balloon that [the girl that _ eats the banana] holds _\" is SORRCs in the IIS condition; \"the monkey that [the dog that _ bites the pig] hits_\" is SORRCs in the RIS condition. For each target, the participants were provided with a speech-visual stimulus constructing a condition of irreversible external semantics (IES). The results showed that SSRRCs, OSRRCs and SORRCs in the IIS-IES condition were produced two years earlier than their counterparts in the RIS-IES condition. Thus, a 2-stage development path is proposed: the language acquisition device starts with the interface between (irreversible) syntax and IIS, and ends with the interface between syntax and IES, both abiding by the syntax-semantic interface principle.","sentences":["There have been apparently conflicting claims over the syntax-semantics relationship in child acquisition.","However, few of them have assessed the child's path toward the acquisition of recursive relative clauses (RRCs).","The authors of the current paper did experiments to investigate 3- to 11-year-olds' most-structured elicited production of eight Mandarin RRCs in a 4 (syntactic types)*2 (semantic conditions) design.","The four syntactic types were RRCs with a subject-gapped RC embedded in an object-gapped RC (SORRCs), RRCs with an object-gapped RC embedded in another object-gapped RC (OORRCs), RRCs with an object-gapped RC embedded in a subject-gapped RC (OSRRCs), and RRCs with a subject-gapped RC embedded in another subject-gapped RC (SSRRCs).","Each syntactic type was put in two conditions differing in internal semantics: irreversible internal semantics (IIS) and reversible internal semantics (RIS).","For example, \"the balloon that [the girl that _ eats the banana] holds _\" is SORRCs in the IIS condition; \"the monkey that [the dog that _ bites the pig] hits_\" is SORRCs in the RIS condition.","For each target, the participants were provided with a speech-visual stimulus constructing a condition of irreversible external semantics (IES).","The results showed that SSRRCs, OSRRCs and SORRCs in the IIS-IES condition were produced two years earlier than their counterparts in the RIS-IES condition.","Thus, a 2-stage development path is proposed: the language acquisition device starts with the interface between (irreversible) syntax and IIS, and ends with the interface between syntax and IES, both abiding by the syntax-semantic interface principle."],"url":"http://arxiv.org/abs/2406.04025v1","category":"cs.CL"}
{"created":"2024-06-06 12:29:14","title":"The Failed Migration of Academic Twitter","abstract":"Following change in Twitter's ownership and subsequent changes to content moderation policies, many in academia looked to move their discourse elsewhere and migration to Mastodon was pursued by some. Our study looks at the dynamics of this migration. Utilizing publicly available user account data, we track the posting activity of academics on Mastodon over a one year period. Our analyses reveal significant challenges sustaining user engagement on Mastodon due to its decentralized structure as well as competition from other platforms such as Bluesky and Threads. The movement lost momentum after an initial surge of enthusiasm as most users did not maintain their activity levels, and those who did faced lower levels of engagement compared to Twitter. Our findings highlight the challenges involved in transitioning professional communities to decentralized platforms, emphasizing the need for focusing on migrating social connections for long-term user engagement.","sentences":["Following change in Twitter's ownership and subsequent changes to content moderation policies, many in academia looked to move their discourse elsewhere and migration to Mastodon was pursued by some.","Our study looks at the dynamics of this migration.","Utilizing publicly available user account data, we track the posting activity of academics on Mastodon over a one year period.","Our analyses reveal significant challenges sustaining user engagement on Mastodon due to its decentralized structure as well as competition from other platforms such as Bluesky and Threads.","The movement lost momentum after an initial surge of enthusiasm as most users did not maintain their activity levels, and those who did faced lower levels of engagement compared to Twitter.","Our findings highlight the challenges involved in transitioning professional communities to decentralized platforms, emphasizing the need for focusing on migrating social connections for long-term user engagement."],"url":"http://arxiv.org/abs/2406.04005v1","category":"cs.SI"}
{"created":"2024-06-06 11:37:39","title":"Observation of quantum entanglement in top quark pair production in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"Entanglement is an intrinsic property of quantum mechanics and is predicted to be exhibited in the particles produced at the Large Hadron Collider. A measurement of the extent of entanglement in top quark-antiquark ($\\mathrm{t\\bar{t}}$) events produced in proton-proton collisions at a center-of-mass energy of 13 TeV is performed with the data recorded by the CMS experiment at the CERN LHC in 2016, and corresponding to an integrated luminosity of 36.3 fb$^{-1}$. The events are selected based on the presence of two leptons with opposite charges and high transverse momentum. An entanglement-sensitive observable $D$ is derived from the top quark spin-dependent parts of the $\\mathrm{t\\bar{t}}$ production density matrix and measured in the region of the $\\mathrm{t\\bar{t}}$ production threshold. Values of $D\\lt-$1/3 are evidence of entanglement and $D$ is observed (expected) to be $-$0.480$^{+0.026}_{-0.029}$ ($-$0.467$^{+0.026}_{-0.029}$) at the parton level. With an observed significance of 5.1 standard deviations with respect to the non-entangled hypothesis, this provides observation of quantum mechanical entanglement within $\\mathrm{t\\bar{t}}$ pairs in this phase space. This measurement provides a new probe of quantum mechanics at the highest energies ever produced.","sentences":["Entanglement is an intrinsic property of quantum mechanics and is predicted to be exhibited in the particles produced at the Large Hadron Collider.","A measurement of the extent of entanglement in top quark-antiquark ($\\mathrm{t\\bar{t}}$) events produced in proton-proton collisions at a center-of-mass energy of 13 TeV is performed with the data recorded by the CMS experiment at the CERN LHC in 2016, and corresponding to an integrated luminosity of 36.3 fb$^{-1}$. The events are selected based on the presence of two leptons with opposite charges and high transverse momentum.","An entanglement-sensitive observable $D$ is derived from the top quark spin-dependent parts of the $\\mathrm{t\\bar{t}}$ production density matrix and measured in the region of the $\\mathrm{t\\bar{t}}$ production threshold.","Values of $D\\lt-$1/3 are evidence of entanglement and $D$ is observed (expected) to be $-$0.480$^{+0.026}_{-0.029}$ ($-$0.467$^{+0.026}_{-0.029}$) at the parton level.","With an observed significance of 5.1 standard deviations with respect to the non-entangled hypothesis, this provides observation of quantum mechanical entanglement within $\\mathrm{t\\bar{t}}$ pairs in this phase space.","This measurement provides a new probe of quantum mechanics at the highest energies ever produced."],"url":"http://arxiv.org/abs/2406.03976v1","category":"hep-ex"}
{"created":"2024-06-06 11:36:11","title":"Observation of $\u03b3\u03b3\\to\u03c4\u03c4$ in proton-proton collisions and limits on the anomalous electromagnetic moments of the $\u03c4$ lepton","abstract":"The production of a pair of $\\tau$ leptons via photon-photon fusion, $\\gamma\\gamma\\to\\tau\\tau$, is observed for the first time in proton-proton collisions, with a significance of 5.3 standard deviations. This observation is based on a data set recorded with the CMS detector at the LHC at a center-of-mass energy of 13 TeV and corresponding to an integrated luminosity of 138 fb$^{-1}$. Events with a pair of $\\tau$ leptons produced via photon-photon fusion are selected by requiring them to be back-to-back in the azimuthal direction and to have a minimum number of charged hadrons associated with their production vertex. The $\\tau$ leptons are reconstructed in their leptonic and hadronic decay modes. The measured fiducial cross section of $\\gamma\\gamma\\to\\tau\\tau$ is $\\sigma^\\text{fid}_\\text{obs}$ = 12.4$^{+3.8}_{-3.1}$ fb. Constraints are set on the contributions to the anomalous magnetic moment ($a_\\tau$) and electric dipole moments ($d_\\tau$) of the $\\tau$ lepton originating from potential effects of new physics on the $\\gamma\\tau\\tau$ vertex: $a_\\tau$ = 0.0009$_{-0.0031}^{+0.0032}$ and $\\lvert d_\\tau \\rvert$ $\\lt$ 2.9 $\\times$ 10$^{-17}$ $e\\,$cm (95% confidence level), consistent with the standard model.","sentences":["The production of a pair of $\\tau$ leptons via photon-photon fusion, $\\gamma\\gamma\\to\\tau\\tau$, is observed for the first time in proton-proton collisions, with a significance of 5.3 standard deviations.","This observation is based on a data set recorded with the CMS detector at the LHC at a center-of-mass energy of 13 TeV and corresponding to an integrated luminosity of 138 fb$^{-1}$. Events with a pair of $\\tau$ leptons produced via photon-photon fusion are selected by requiring them to be back-to-back in the azimuthal direction and to have a minimum number of charged hadrons associated with their production vertex.","The $\\tau$ leptons are reconstructed in their leptonic and hadronic decay modes.","The measured fiducial cross section of $\\gamma\\gamma\\to\\tau\\tau$ is $\\sigma^\\text{fid}_\\text{obs}$ = 12.4$^{+3.8}_{-3.1}$ fb.","Constraints are set on the contributions to the anomalous magnetic moment ($a_\\tau$) and electric dipole moments ($d_\\tau$) of the $\\tau$ lepton originating from potential effects of new physics on the $\\gamma\\tau\\tau$ vertex: $a_\\tau$ = 0.0009$_{-0.0031}^{+0.0032}$ and $\\lvert d_\\tau \\rvert$ $\\lt$ 2.9 $\\times$ 10$^{-17}$ $e\\,$cm (95% confidence level), consistent with the standard model."],"url":"http://arxiv.org/abs/2406.03975v1","category":"hep-ex"}
{"created":"2024-06-06 10:35:41","title":"The asymmetric intrinsic charm in the nucleon and its implications for the $D^{0}$ production in the LHCb $p\\!+\\!\\!^{20}\\!N\\!e$ fixed-target experiment","abstract":"Recent results indicate that charm quarks are intrinsic components of the proton wave function, and that the charm and anticharm distributions for a given value of the Bjorken - $x$ variable can be different. In this paper, we will investigate the impact of this asymmetric intrinsic charm on the production of $D^0$ and ${\\bar D}^0$ mesons for fixed target $p \\!+ ^{20}\\!\\!Ne$ collisions at the LHCb. In our calculations, we include the contribution of the gluon-gluon fusion, gluon - charm and recombination processes and assume distinct models for the treatment of the intrinsic charm component. We demonstrate that the presence of an intrinsic charm improves the description of the current data for the rapidity and transverse momentum distributions of $D$ mesons. However, such models are not able to describe the LHCb data for the $D^0$-${\\bar D}^0$ asymmetry at large transverse momentum, which point out that the description of the intrinsic charm needs to be improved and/or new effects should be taken into account in the production of heavy mesons at forward rapidities in fixed - target collisions.","sentences":["Recent results indicate that charm quarks are intrinsic components of the proton wave function, and that the charm and anticharm distributions for a given value of the Bjorken - $x$ variable can be different.","In this paper, we will investigate the impact of this asymmetric intrinsic charm on the production of $D^0$ and ${\\bar D}^0$ mesons for fixed target $p \\!+ ^{20}\\!\\!Ne$ collisions at the LHCb.","In our calculations, we include the contribution of the gluon-gluon fusion, gluon - charm and recombination processes and assume distinct models for the treatment of the intrinsic charm component.","We demonstrate that the presence of an intrinsic charm improves the description of the current data for the rapidity and transverse momentum distributions of $D$ mesons.","However, such models are not able to describe the LHCb data for the $D^0$-${\\bar D}^0$ asymmetry at large transverse momentum, which point out that the description of the intrinsic charm needs to be improved and/or new effects should be taken into account in the production of heavy mesons at forward rapidities in fixed - target collisions."],"url":"http://arxiv.org/abs/2406.03943v1","category":"hep-ph"}
{"created":"2024-06-06 10:26:28","title":"Flux-density stability and temporal changes in spectra of millisecond pulsars using GMRT","abstract":"This paper presents an investigation of spectral properties of 10 millisecond pulsars (MSPs) discovered by the uGMRT, observed from 2017-2023 using band 3 (300-500 MHz) and 4 (550-750 MHz) of uGMRT. For these MSPs, we have reported a range of spectral indices from ~0 to -4.8, while averaging the full observing band and all the observing epochs. For every MSP, we calculated the mean flux densities across 7-8 sub-bands each with approximately 25 MHz bandwidth spanning band 3 and band 4. We computed their modulation indices as well as average and maximum-to-median flux densities within each subband. Using a temporal variation of flux density we calculated the refractive scintillation time scales and estimated structure function with time lag for 8 MSPs in the sample. We note a significant temporal evolution of the in-band spectra, classified into three categories based on the nature of the best-fit power-law spectra, having single positive spectral indices, multiple broken power law, and single negative spectral indices. Additionally, indications of low-frequency turnover and a temporal variation of the turnover frequency (to the extent that turnover was observed for some of the epochs while not seen for the rest) were noted for all the MSPs. To the best of our knowledge, this is the first systematic investigation probing temporal changes in the MSP spectra as well as in turnover frequency. Future exploration with dense monitoring combined with modeling of spectra can provide vital insight into the intrinsic emission properties of the MSPs and ISM properties.","sentences":["This paper presents an investigation of spectral properties of 10 millisecond pulsars (MSPs) discovered by the uGMRT, observed from 2017-2023 using band 3 (300-500 MHz) and 4 (550-750 MHz) of uGMRT.","For these MSPs, we have reported a range of spectral indices from ~0 to -4.8, while averaging the full observing band and all the observing epochs.","For every MSP, we calculated the mean flux densities across 7-8 sub-bands each with approximately 25 MHz bandwidth spanning band 3 and band 4.","We computed their modulation indices as well as average and maximum-to-median flux densities within each subband.","Using a temporal variation of flux density we calculated the refractive scintillation time scales and estimated structure function with time lag for 8 MSPs in the sample.","We note a significant temporal evolution of the in-band spectra, classified into three categories based on the nature of the best-fit power-law spectra, having single positive spectral indices, multiple broken power law, and single negative spectral indices.","Additionally, indications of low-frequency turnover and a temporal variation of the turnover frequency (to the extent that turnover was observed for some of the epochs while not seen for the rest) were noted for all the MSPs.","To the best of our knowledge, this is the first systematic investigation probing temporal changes in the MSP spectra as well as in turnover frequency.","Future exploration with dense monitoring combined with modeling of spectra can provide vital insight into the intrinsic emission properties of the MSPs and ISM properties."],"url":"http://arxiv.org/abs/2406.03939v1","category":"astro-ph.HE"}
{"created":"2024-06-06 10:20:03","title":"Simulation-based Inference for Gravitational-waves from Intermediate-Mass Binary Black Holes in Real Noise","abstract":"We present an exploratory investigation into using Simulation-based Inference techniques, specifically Flow-Matching Posterior Estimation, to construct a posterior density estimator trained using real gravitational-wave detector noise. We use this prototype estimator to investigate possible effects on parameter estimation for Intermediate-Mass Binary Black Holes, showing statistically significant reduced measurement bias. While the results do show potential for improved measurements, they also highlight the need for further work.","sentences":["We present an exploratory investigation into using Simulation-based Inference techniques, specifically Flow-Matching Posterior Estimation, to construct a posterior density estimator trained using real gravitational-wave detector noise.","We use this prototype estimator to investigate possible effects on parameter estimation for Intermediate-Mass Binary Black Holes, showing statistically significant reduced measurement bias.","While the results do show potential for improved measurements, they also highlight the need for further work."],"url":"http://arxiv.org/abs/2406.03935v1","category":"gr-qc"}
{"created":"2024-06-06 09:13:13","title":"Decoder-only Streaming Transformer for Simultaneous Translation","abstract":"Simultaneous Machine Translation (SiMT) generates translation while reading source tokens, essentially producing the target prefix based on the source prefix. To achieve good performance, it leverages the relationship between source and target prefixes to exact a policy to guide the generation of translations. Although existing SiMT methods primarily focus on the Encoder-Decoder architecture, we explore the potential of Decoder-only architecture, owing to its superior performance in various tasks and its inherent compatibility with SiMT. However, directly applying the Decoder-only architecture to SiMT poses challenges in terms of training and inference. To alleviate the above problems, we propose the first Decoder-only SiMT model, named Decoder-only Streaming Transformer (DST). Specifically, DST separately encodes the positions of the source and target prefixes, ensuring that the position of the target prefix remains unaffected by the expansion of the source prefix. Furthermore, we propose a Streaming Self-Attention (SSA) mechanism tailored for the Decoder-only architecture. It is capable of obtaining translation policy by assessing the sufficiency of input source information and integrating with the soft-attention mechanism to generate translations. Experiments demonstrate that our approach achieves state-of-the-art performance on three translation tasks.","sentences":["Simultaneous Machine Translation (SiMT) generates translation while reading source tokens, essentially producing the target prefix based on the source prefix.","To achieve good performance, it leverages the relationship between source and target prefixes to exact a policy to guide the generation of translations.","Although existing SiMT methods primarily focus on the Encoder-Decoder architecture, we explore the potential of Decoder-only architecture, owing to its superior performance in various tasks and its inherent compatibility with SiMT.","However, directly applying the Decoder-only architecture to SiMT poses challenges in terms of training and inference.","To alleviate the above problems, we propose the first Decoder-only SiMT model, named Decoder-only Streaming Transformer (DST).","Specifically, DST separately encodes the positions of the source and target prefixes, ensuring that the position of the target prefix remains unaffected by the expansion of the source prefix.","Furthermore, we propose a Streaming Self-Attention (SSA) mechanism tailored for the Decoder-only architecture.","It is capable of obtaining translation policy by assessing the sufficiency of input source information and integrating with the soft-attention mechanism to generate translations.","Experiments demonstrate that our approach achieves state-of-the-art performance on three translation tasks."],"url":"http://arxiv.org/abs/2406.03878v1","category":"cs.CL"}
{"created":"2024-06-06 09:11:26","title":"Time-resolved optical assessment of exciton formation in mixed two-dimensional perovskite films","abstract":"We report the observation of exciton formation from the cooled band-edge carriers in mixed two-dimensional hybrid organic-inorganic perovskites using femtosecond transient absorption spectroscopy. By monitoring the changes of bleach signal upon excitations with various photon energy, we are able to extract the values of exciton binding energy and the occupancies of carriers of free and bound states for each two-dimensional phase. We also confirm the existence of Mahan exciton when injected carrier density is above the Mott criterion.","sentences":["We report the observation of exciton formation from the cooled band-edge carriers in mixed two-dimensional hybrid organic-inorganic perovskites using femtosecond transient absorption spectroscopy.","By monitoring the changes of bleach signal upon excitations with various photon energy, we are able to extract the values of exciton binding energy and the occupancies of carriers of free and bound states for each two-dimensional phase.","We also confirm the existence of Mahan exciton when injected carrier density is above the Mott criterion."],"url":"http://arxiv.org/abs/2406.03876v1","category":"physics.optics"}
{"created":"2024-06-06 08:53:01","title":"LLplace: The 3D Indoor Scene Layout Generation and Editing via Large Language Model","abstract":"Designing 3D indoor layouts is a crucial task with significant applications in virtual reality, interior design, and automated space planning. Existing methods for 3D layout design either rely on diffusion models, which utilize spatial relationship priors, or heavily leverage the inferential capabilities of proprietary Large Language Models (LLMs), which require extensive prompt engineering and in-context exemplars via black-box trials. These methods often face limitations in generalization and dynamic scene editing. In this paper, we introduce LLplace, a novel 3D indoor scene layout designer based on lightweight fine-tuned open-source LLM Llama3. LLplace circumvents the need for spatial relationship priors and in-context exemplars, enabling efficient and credible room layout generation based solely on user inputs specifying the room type and desired objects. We curated a new dialogue dataset based on the 3D-Front dataset, expanding the original data volume and incorporating dialogue data for adding and removing objects. This dataset can enhance the LLM's spatial understanding. Furthermore, through dialogue, LLplace activates the LLM's capability to understand 3D layouts and perform dynamic scene editing, enabling the addition and removal of objects. Our approach demonstrates that LLplace can effectively generate and edit 3D indoor layouts interactively and outperform existing methods in delivering high-quality 3D design solutions. Code and dataset will be released.","sentences":["Designing 3D indoor layouts is a crucial task with significant applications in virtual reality, interior design, and automated space planning.","Existing methods for 3D layout design either rely on diffusion models, which utilize spatial relationship priors, or heavily leverage the inferential capabilities of proprietary Large Language Models (LLMs), which require extensive prompt engineering and in-context exemplars via black-box trials.","These methods often face limitations in generalization and dynamic scene editing.","In this paper, we introduce LLplace, a novel 3D indoor scene layout designer based on lightweight fine-tuned open-source LLM Llama3.","LLplace circumvents the need for spatial relationship priors and in-context exemplars, enabling efficient and credible room layout generation based solely on user inputs specifying the room type and desired objects.","We curated a new dialogue dataset based on the 3D-Front dataset, expanding the original data volume and incorporating dialogue data for adding and removing objects.","This dataset can enhance the LLM's spatial understanding.","Furthermore, through dialogue, LLplace activates the LLM's capability to understand 3D layouts and perform dynamic scene editing, enabling the addition and removal of objects.","Our approach demonstrates that LLplace can effectively generate and edit 3D indoor layouts interactively and outperform existing methods in delivering high-quality 3D design solutions.","Code and dataset will be released."],"url":"http://arxiv.org/abs/2406.03866v1","category":"cs.CV"}
{"created":"2024-06-06 08:40:28","title":"Speculative Decoding via Early-exiting for Faster LLM Inference with Thompson Sampling Control Mechanism","abstract":"The recent advancements in large language models (LLMs) have been extraordinary, yet the escalating inference costs associated with them present challenges in real-world applications. To address these challenges, we propose a novel approach called Early-exiting Speculative Decoding (EESD) with lossless acceleration. Specifically, EESD utilizes a segment of the LLM to generate draft tokens, incorporating Early-exiting structures after the first N layers. To enhance the quality of draft tokens, a self-distillation method is integrated. This early-exiting design not only reduces deployment and training costs but also significantly accelerates the token generation speed. Moreover, we introduce a novel sampling mechanism that leverages Thompson Sampling to regulate the generation processes, automatically determining the quantity of draft tokens in each round. The original LLM is then employed to validate these draft tokens through a single forward pass, and thus guarantees that the final output text maintains a distribution consistent with vanilla auto-regressive decoding. The experimental results on both 13B and 70B models demonstrate that our approach decodes tokens at a markedly accelerated rate compared to prior methods, showing the effectiveness of our approach.","sentences":["The recent advancements in large language models (LLMs) have been extraordinary, yet the escalating inference costs associated with them present challenges in real-world applications.","To address these challenges, we propose a novel approach called Early-exiting Speculative Decoding (EESD) with lossless acceleration.","Specifically, EESD utilizes a segment of the LLM to generate draft tokens, incorporating Early-exiting structures after the first N layers.","To enhance the quality of draft tokens, a self-distillation method is integrated.","This early-exiting design not only reduces deployment and training costs but also significantly accelerates the token generation speed.","Moreover, we introduce a novel sampling mechanism that leverages Thompson Sampling to regulate the generation processes, automatically determining the quantity of draft tokens in each round.","The original LLM is then employed to validate these draft tokens through a single forward pass, and thus guarantees that the final output text maintains a distribution consistent with vanilla auto-regressive decoding.","The experimental results on both 13B and 70B models demonstrate that our approach decodes tokens at a markedly accelerated rate compared to prior methods, showing the effectiveness of our approach."],"url":"http://arxiv.org/abs/2406.03853v1","category":"cs.CL"}
{"created":"2024-06-06 08:23:01","title":"PREX and CREX: Evidence for Strong Isovector Spin-Orbit Interaction","abstract":"The recent PREX-2 and CREX data on the model-independent extraction of the charge-weak form factor difference $\\Delta F_{\\rm CW}$ in $^{208}$Pb and $^{48}$Ca challenge modern nuclear energy density functionals (EDFs) as well as our present understanding on the neutron skin and nuclear symmetry energy. Within the Skyrme-like EDFs, we demonstrate that the isovector spin-orbit interaction can strongly change the $\\Delta F_{\\rm CW}$ in $^{48}$Ca while it has essentially no influence on the $\\Delta F_{\\rm CW}$ in $^{208}$Pb, mainly due to the eight spin-orbit unpaired $1f_{7/2}$ neutrons in $^{48}$Ca. To simultaneously describe PREX-2 and CREX data in $1\\sigma$ error, we find the strength of isovector spin-orbit interaction should be larger than about four times of that in the conventional Skyrme-like EDFs, implying the neutrons and protons have significantly different spin-orbit interaction. To further reconcile the data on electric dipole polarizability in $^{208}$Pb and $^{48}$Ca, we obtain $L \\approx 55$ MeV for the slope parameter of the symmetry energy, $\\Delta r_{\\rm np}(^{208}\\rm{Pb}) \\approx 0.19$ fm and $\\Delta r_{\\rm np}(^{48}\\rm{Ca}) \\approx 0.12$ fm for the neutron skin thickness. The implications of the strong isovector spin-orbit interaction are discussed.","sentences":["The recent PREX-2 and CREX data on the model-independent extraction of the charge-weak form factor difference $\\Delta F_{\\rm CW}$ in $^{208}$Pb and $^{48}$Ca challenge modern nuclear energy density functionals (EDFs) as well as our present understanding on the neutron skin and nuclear symmetry energy.","Within the Skyrme-like EDFs, we demonstrate that the isovector spin-orbit interaction can strongly change the $\\Delta F_{\\rm CW}$ in $^{48}$Ca while it has essentially no influence on the $\\Delta F_{\\rm CW}$ in $^{208}$Pb, mainly due to the eight spin-orbit unpaired $1f_{7/2}$ neutrons in $^{48}$Ca.","To simultaneously describe PREX-2 and CREX data in $1\\sigma$ error, we find the strength of isovector spin-orbit interaction should be larger than about four times of that in the conventional Skyrme-like EDFs, implying the neutrons and protons have significantly different spin-orbit interaction.","To further reconcile the data on electric dipole polarizability in $^{208}$Pb and $^{48}$Ca, we obtain $L \\approx 55$ MeV for the slope parameter of the symmetry energy, $\\Delta r_{\\rm np}(^{208}\\rm{Pb})","\\approx 0.19$ fm and $\\Delta r_{\\rm np}(^{48}\\rm{Ca})","\\approx 0.12$ fm for the neutron skin thickness.","The implications of the strong isovector spin-orbit interaction are discussed."],"url":"http://arxiv.org/abs/2406.03844v1","category":"nucl-th"}
{"created":"2024-06-06 08:01:58","title":"Ensemble Inequivalence with Competing Interactions","abstract":"We study the effect of competing interactions on ensemble inequivalence. We consider a one-dimensional Ising model with ferromagnetic mean-field interactions and short-range couplings which can be either ferromagnetic or antiferromagnetic. Despite the relative simplicity of the model, our calculations in the microcanonical ensemble reveal a rich phase diagram. The comparison with the corresponding phase diagram in the canonical ensemble shows the presence of phase transition points and lines which are different in the two ensembles. As an example, in a region of the phase diagram where the canonical ensemble shows a critical point and a critical end point, the microcanonical ensemble has an additional critical point and also a triple point. The regions of ensemble inequivalence typically occur at lower temperatures and at larger absolute values of the competing couplings. The presence of two free parameters in the model allows us to obtain a fourth-order critical point, which can be fully characterized by deriving its Landau normal form.","sentences":["We study the effect of competing interactions on ensemble inequivalence.","We consider a one-dimensional Ising model with ferromagnetic mean-field interactions and short-range couplings which can be either ferromagnetic or antiferromagnetic.","Despite the relative simplicity of the model, our calculations in the microcanonical ensemble reveal a rich phase diagram.","The comparison with the corresponding phase diagram in the canonical ensemble shows the presence of phase transition points and lines which are different in the two ensembles.","As an example, in a region of the phase diagram where the canonical ensemble shows a critical point and a critical end point, the microcanonical ensemble has an additional critical point and also a triple point.","The regions of ensemble inequivalence typically occur at lower temperatures and at larger absolute values of the competing couplings.","The presence of two free parameters in the model allows us to obtain a fourth-order critical point, which can be fully characterized by deriving its Landau normal form."],"url":"http://arxiv.org/abs/2406.03826v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-06 08:01:11","title":"Regions without zeros for the auxiliary function of Riemann","abstract":"We give explicit and extended versions of some of Siegel's results. We extend the validity of Siegel's asymptotic development in the second quadrant to most of the third quadrant. We also give precise bounds of the error; this allows us to give an explicit region free of zeros, or with only trivial zeros. The left limit of the zeros on the upper half plane is extended from $1-\\sigma\\ge a t^{3/7}$ in Siegel to $1-\\sigma\\ge A t^{2/5}\\log t$. Siegel claims that it can be proved that there are no zeros in the region $1-\\sigma\\ge t^\\varepsilon$ for any $\\varepsilon>0$. We show that Siegel's proof for the exponent $3/7$ does not extend to prove his claim.","sentences":["We give explicit and extended versions of some of Siegel's results.","We extend the validity of Siegel's asymptotic development in the second quadrant to most of the third quadrant.","We also give precise bounds of the error; this allows us to give an explicit region free of zeros, or with only trivial zeros.","The left limit of the zeros on the upper half plane is extended from $1-\\sigma\\ge a t^{3/7}$ in Siegel to $1-\\sigma\\ge A t^{2/5}\\log t$. Siegel claims that it can be proved that there are no zeros in the region $1-\\sigma\\ge t^\\varepsilon$ for any $\\varepsilon>0$. We show that Siegel's proof for the exponent $3/7$ does not extend to prove his claim."],"url":"http://arxiv.org/abs/2406.03825v1","category":"math.NT"}
{"created":"2024-06-06 07:23:56","title":"Exploring the interplay between mass-energy equivalence, interactions and entanglement in an optical lattice clock","abstract":"We propose protocols that probe manifestations of the mass-energy equivalence in an optical lattice clock (OLC) interrogated with spin coherent and entangled quantum states. To tune and uniquely distinguish the mass-energy equivalence effects (gravitational redshift and second order Doppler shift) in such setting, we devise a dressing protocol using an additional nuclear spin state. We then analyze the interplay between photon-mediated interactions and gravitational redshift and show that such interplay can lead to entanglement generation and frequency synchronization. In the regime where all atomic spins synchronize, we show the synchronization time depends on the initial entanglement of the state and can be used as a proxy of its metrological gain compared to a classical state. Our work opens new possibilities for exploring the effects of general relativity on quantum coherence and entanglement in OLC experiments.","sentences":["We propose protocols that probe manifestations of the mass-energy equivalence in an optical lattice clock (OLC) interrogated with spin coherent and entangled quantum states.","To tune and uniquely distinguish the mass-energy equivalence effects (gravitational redshift and second order Doppler shift) in such setting, we devise a dressing protocol using an additional nuclear spin state.","We then analyze the interplay between photon-mediated interactions and gravitational redshift and show that such interplay can lead to entanglement generation and frequency synchronization.","In the regime where all atomic spins synchronize, we show the synchronization time depends on the initial entanglement of the state and can be used as a proxy of its metrological gain compared to a classical state.","Our work opens new possibilities for exploring the effects of general relativity on quantum coherence and entanglement in OLC experiments."],"url":"http://arxiv.org/abs/2406.03804v1","category":"quant-ph"}
{"created":"2024-06-06 07:10:12","title":"Optical biomarker of metabolism for breast tumor diagnosis: Insights from subcellular dynamics","abstract":"Label-free metabolic dynamics contrast is highly appealing but difficult to achieve in biomedical imaging. Interference offers a highly sensitive mechanism for capturing the metabolic dynamics of the subcellular scatterers. However, traditional interference detection methods fail to isolate pure metabolic dynamics, as the dynamic signals are coupled with scatterer reflectivity and other uncontrollable imaging factors. Here, we demonstrate active phase modulation-assisted dynamic full-field optical coherence tomography (APMD-FFOCT) that decouples and quantifies the metabolic dynamics by adding a reference movement for all interferential scatterers. This novel technique enables imaging and dynamic analysis of subcellular structures along with their changes during the apoptotic process in tumor tissues. Furthermore, the nucleus-to-cytoplasm dynamic intensity ratio could serve as an optical biomarker for breast tumor grading, enhancing intraoperative diagnosis.","sentences":["Label-free metabolic dynamics contrast is highly appealing but difficult to achieve in biomedical imaging.","Interference offers a highly sensitive mechanism for capturing the metabolic dynamics of the subcellular scatterers.","However, traditional interference detection methods fail to isolate pure metabolic dynamics, as the dynamic signals are coupled with scatterer reflectivity and other uncontrollable imaging factors.","Here, we demonstrate active phase modulation-assisted dynamic full-field optical coherence tomography (APMD-FFOCT) that decouples and quantifies the metabolic dynamics by adding a reference movement for all interferential scatterers.","This novel technique enables imaging and dynamic analysis of subcellular structures along with their changes during the apoptotic process in tumor tissues.","Furthermore, the nucleus-to-cytoplasm dynamic intensity ratio could serve as an optical biomarker for breast tumor grading, enhancing intraoperative diagnosis."],"url":"http://arxiv.org/abs/2406.03798v1","category":"physics.med-ph"}
{"created":"2024-06-06 07:03:29","title":"Light-PEFT: Lightening Parameter-Efficient Fine-Tuning via Early Pruning","abstract":"Parameter-efficient fine-tuning (PEFT) has emerged as the predominant technique for fine-tuning in the era of large language models. However, existing PEFT methods still have inadequate training efficiency. Firstly, the utilization of large-scale foundation models during the training process is excessively redundant for certain fine-tuning tasks. Secondly, as the model size increases, the growth in trainable parameters of empirically added PEFT modules becomes non-negligible and redundant, leading to inefficiency. To achieve task-specific efficient fine-tuning, we propose the Light-PEFT framework, which includes two methods: Masked Early Pruning of the Foundation Model and Multi-Granularity Early Pruning of PEFT. The Light-PEFT framework allows for the simultaneous estimation of redundant parameters in both the foundation model and PEFT modules during the early stage of training. These parameters can then be pruned for more efficient fine-tuning. We validate our approach on GLUE, SuperGLUE, QA tasks, and various models. With Light-PEFT, parameters of the foundation model can be pruned by up to over 40%, while still controlling trainable parameters to be only 25% of the original PEFT method. Compared to utilizing the PEFT method directly, Light-PEFT achieves training and inference speedup, reduces memory usage, and maintains comparable performance and the plug-and-play feature of PEFT.","sentences":["Parameter-efficient fine-tuning (PEFT) has emerged as the predominant technique for fine-tuning in the era of large language models.","However, existing PEFT methods still have inadequate training efficiency.","Firstly, the utilization of large-scale foundation models during the training process is excessively redundant for certain fine-tuning tasks.","Secondly, as the model size increases, the growth in trainable parameters of empirically added PEFT modules becomes non-negligible and redundant, leading to inefficiency.","To achieve task-specific efficient fine-tuning, we propose the Light-PEFT framework, which includes two methods: Masked Early Pruning of the Foundation Model and Multi-Granularity Early Pruning of PEFT.","The Light-PEFT framework allows for the simultaneous estimation of redundant parameters in both the foundation model and PEFT modules during the early stage of training.","These parameters can then be pruned for more efficient fine-tuning.","We validate our approach on GLUE, SuperGLUE, QA tasks, and various models.","With Light-PEFT, parameters of the foundation model can be pruned by up to over 40%, while still controlling trainable parameters to be only 25% of the original PEFT method.","Compared to utilizing the PEFT method directly, Light-PEFT achieves training and inference speedup, reduces memory usage, and maintains comparable performance and the plug-and-play feature of PEFT."],"url":"http://arxiv.org/abs/2406.03792v1","category":"cs.CL"}
{"created":"2024-06-06 06:55:01","title":"Constructing tree amplitudes of scalar EFT from double soft theorem","abstract":"The well known Adler zero can fully determine tree amplitudes of non-linear sigma model (NLSM), but fails to fix tree pion amplitudes with higher-derivative interactions. To fill this gap, in this paper we propose a new method based on exploiting the double soft theorem for scalars, which can be applied to a wider range. A remarkable feature of this method is, we only assume the universality of soft behavior at the beginning, and determine the explicit form of double soft factor in the process of constructing amplitudes. To test the applicability, we use this method to construct tree NLSM amplitudes and tree amplitudes those pions in NLSM couple to bi-adjoint scalars. We also construct the simplest pion amplitudes which receive leading higher-derivative correction, with arbitrary number of external legs. All resulted amplitudes are formulated as universal expansions to appropriate basis.","sentences":["The well known Adler zero can fully determine tree amplitudes of non-linear sigma model (NLSM), but fails to fix tree pion amplitudes with higher-derivative interactions.","To fill this gap, in this paper we propose a new method based on exploiting the double soft theorem for scalars, which can be applied to a wider range.","A remarkable feature of this method is, we only assume the universality of soft behavior at the beginning, and determine the explicit form of double soft factor in the process of constructing amplitudes.","To test the applicability, we use this method to construct tree NLSM amplitudes and tree amplitudes those pions in NLSM couple to bi-adjoint scalars.","We also construct the simplest pion amplitudes which receive leading higher-derivative correction, with arbitrary number of external legs.","All resulted amplitudes are formulated as universal expansions to appropriate basis."],"url":"http://arxiv.org/abs/2406.03784v1","category":"hep-th"}
{"created":"2024-06-06 06:13:21","title":"Thermal Conductivity of Double Polymorph Ga2O3 Structures","abstract":"Recently discovered double gamma/beta ({\\gamma}/\\b{eta}) polymorph Ga2O3 structures constitute a class of novel materials providing an option to modulate functional properties across interfaces without changing chemical compositions of materials, in contrast to that in conventional heterostructures. In this work, for the first time, we investigate thermal transport in such homo-interface structures as an example of their physical properties. Specifically, the cross-plane thermal conductivity (k) was measured by femtosecond laser-based time-domain thermoreflectance with MHz modulation rates, effectively obtaining depth profiles of the thermal conductivity across the {\\gamma}/\\b{eta}-Ga2O3 structures. In this way, the thermal conductivity of {\\gamma}-Ga2O3 k=1.84{\\div}2.11 W m-1K-1 was found to be independent of the initial \\b{eta}-substrates orientations, in accordance with the cubic spinel structure of the {\\gamma}-phase and consistently with the molecular dynamics simulation data. In its turn, the thermal conductivity of monoclinic \\b{eta}-Ga2O3 showed a distinct anisotropy, with values ranging from 10 W m-1K-1 for [201] to 20 Wm-1K-1 for [010] orientations. Thus, for double {\\gamma}/\\b{eta} Ga2O3 polymorph structures formed on [010] \\b{eta}-substrates, there is an order of magnitude difference in thermal conductivity across the {\\gamma}/\\b{eta} interface, which potentially can be exploited in thermal energy conversion applications.","sentences":["Recently discovered double gamma/beta ({\\gamma}/\\b{eta}) polymorph Ga2O3 structures constitute a class of novel materials providing an option to modulate functional properties across interfaces without changing chemical compositions of materials, in contrast to that in conventional heterostructures.","In this work, for the first time, we investigate thermal transport in such homo-interface structures as an example of their physical properties.","Specifically, the cross-plane thermal conductivity (k) was measured by femtosecond laser-based time-domain thermoreflectance with MHz modulation rates, effectively obtaining depth profiles of the thermal conductivity across the {\\gamma}/\\b{eta}-Ga2O3 structures.","In this way, the thermal conductivity of {\\gamma}-Ga2O3 k=1.84{\\div}2.11 W m-1K-1 was found to be independent of the initial \\b{eta}-substrates orientations, in accordance with the cubic spinel structure of the {\\gamma}-phase and consistently with the molecular dynamics simulation data.","In its turn, the thermal conductivity of monoclinic \\b{eta}-Ga2O3 showed a distinct anisotropy, with values ranging from 10 W m-1K-1 for [201] to 20 Wm-1K-1 for [010] orientations.","Thus, for double {\\gamma}/\\b{eta} Ga2O3 polymorph structures formed on [010] \\b{eta}-substrates, there is an order of magnitude difference in thermal conductivity across the {\\gamma}/\\b{eta} interface, which potentially can be exploited in thermal energy conversion applications."],"url":"http://arxiv.org/abs/2406.03767v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-06 05:59:15","title":"More on spin-2 operators in holographic quantum mechanics","abstract":"We study the spectrum, unitarity bound and holographic central charge of spin-2 operators in a warped $\\text{AdS}_2 \\times \\text{S}^2 \\times \\text{T}^4 \\times \\mathcal{I}_\\psi \\times \\mathcal{I}_\\rho$ background in the Type IIB theory. We were able to identify a class of solutions that is completely independent of the functions that define the background solution. We comment on the relation of our results to the previous ones in the literature.","sentences":["We study the spectrum, unitarity bound and holographic central charge of spin-2 operators in a warped $\\text{AdS}_2 \\times \\text{S}^2 \\times \\text{T}^4 \\times \\mathcal{I}_\\psi \\times \\mathcal{I}_\\rho$ background in the Type IIB theory.","We were able to identify a class of solutions that is completely independent of the functions that define the background solution.","We comment on the relation of our results to the previous ones in the literature."],"url":"http://arxiv.org/abs/2406.03764v1","category":"hep-th"}
{"created":"2024-06-06 04:45:48","title":"Estimate for the neutrino magnetic moment from pulsar kick velocities induced at the birth of strange quark matter neutron stars","abstract":"We estimate the magnetic moment of electron neutrinos by computing the neutrino chirality flip rate that can occur in the core of a strange quark matter neutron star at birth. We show that this process allows neutrinos to anisotropically escape thus inducing the star kick velocity. The process is not subject to the no-go theorem since, although the flip from left- to right-handed neutrinos happens at equilibrium, the reverse process does not take place, given that right-handed neutrinos do not interact with matter and therefore detailed balance is lost. For simplicity, we model the star core as consisting of strange quark matter. We find that even when the energy released in right-handed neutrinos is a small fraction of the total energy released in left-handed neutrinos, the process describes kick velocities for natal conditions which are consistent with the observed ones and span the correct range of radii, temperatures and chemical potentials for typical magnetic field intensities. The neutrino magnetic moment is estimated to be $\\mu_\\nu \\sim 3.6 \\times 10^{-18}\\mu_B$, where $\\mu_B$ is the Bohr magneton. This value is more stringent than the bound found for massive neutrinos in a minimal extension of the standard model.","sentences":["We estimate the magnetic moment of electron neutrinos by computing the neutrino chirality flip rate that can occur in the core of a strange quark matter neutron star at birth.","We show that this process allows neutrinos to anisotropically escape thus inducing the star kick velocity.","The process is not subject to the no-go theorem since, although the flip from left- to right-handed neutrinos happens at equilibrium, the reverse process does not take place, given that right-handed neutrinos do not interact with matter and therefore detailed balance is lost.","For simplicity, we model the star core as consisting of strange quark matter.","We find that even when the energy released in right-handed neutrinos is a small fraction of the total energy released in left-handed neutrinos, the process describes kick velocities for natal conditions which are consistent with the observed ones and span the correct range of radii, temperatures and chemical potentials for typical magnetic field intensities.","The neutrino magnetic moment is estimated to be $\\mu_\\nu \\sim 3.6 \\times 10^{-18}\\mu_B$, where $\\mu_B$ is the Bohr magneton.","This value is more stringent than the bound found for massive neutrinos in a minimal extension of the standard model."],"url":"http://arxiv.org/abs/2406.03745v1","category":"hep-ph"}
{"created":"2024-06-06 04:27:09","title":"Magnetic geometry to quantum geometry nonlinear transports","abstract":"Nonlinear transports (NLTs) have garnered broad attention based on their topological origin in quantum geometry. When quantum geometry meets magnetic geometry in magnets, their crossover excites diverse phenomena particularly related to antiferromagnetic spintronics. However, very few material platforms have been predicted and experimentally verified to date, where spin-orbit coupling (SOC) plays an indispensable role in generating NLTs. Therefore, to boost antiferromagnetic spintronics affected by the dual effect of quantum geometry and magnetic geometry, a material database of antiferromagnets (AFMs) with magnetic geometry driven quantum geometry and more significant NLT effects is urgently needed. Here, we integrate the state-of-the-art spin space group theory into the symmetry analysis of NLT tensors. By completely disentangling SOC effects, we find that collinear and coplanar magnetic geometry can only induce NLT driven by Berry curvature dipole, and noncoplanar one may trigger NLT driven by dipoles of Berry curvature, inverse mass, and quantum metric. Remarkably, a materials database of 260 AFMs with SOC-free NLT effects is established. Several prototypical material candidates are presented by first-principle calculations, including collinear AFM VNb$_{3}$S$_{6}$ and coplanar AFM Ca$_{2}$Cr$_{2}$O$_{5}$ with NLT driven by Berry curvature dipole, and a room-temperature noncoplanar AFM CrSe with NLTs driven by inverse mass dipole and quantum metric dipole. Our work not only provides a universal theoretical framework for studying various magnetism-driven transport effects, but also predicts broad, experimentally accessible material platforms for antiferromagnetic spintronics.","sentences":["Nonlinear transports (NLTs) have garnered broad attention based on their topological origin in quantum geometry.","When quantum geometry meets magnetic geometry in magnets, their crossover excites diverse phenomena particularly related to antiferromagnetic spintronics.","However, very few material platforms have been predicted and experimentally verified to date, where spin-orbit coupling (SOC) plays an indispensable role in generating NLTs.","Therefore, to boost antiferromagnetic spintronics affected by the dual effect of quantum geometry and magnetic geometry, a material database of antiferromagnets (AFMs) with magnetic geometry driven quantum geometry and more significant NLT effects is urgently needed.","Here, we integrate the state-of-the-art spin space group theory into the symmetry analysis of NLT tensors.","By completely disentangling SOC effects, we find that collinear and coplanar magnetic geometry can only induce NLT driven by Berry curvature dipole, and noncoplanar one may trigger NLT driven by dipoles of Berry curvature, inverse mass, and quantum metric.","Remarkably, a materials database of 260 AFMs with SOC-free NLT effects is established.","Several prototypical material candidates are presented by first-principle calculations, including collinear AFM VNb$_{3}$S$_{6}$ and coplanar AFM Ca$_{2}$Cr$_{2}$O$_{5}$ with NLT driven by Berry curvature dipole, and a room-temperature noncoplanar AFM CrSe with NLTs driven by inverse mass dipole and quantum metric dipole.","Our work not only provides a universal theoretical framework for studying various magnetism-driven transport effects, but also predicts broad, experimentally accessible material platforms for antiferromagnetic spintronics."],"url":"http://arxiv.org/abs/2406.03738v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-06 03:51:49","title":"Flowing plasma rearrangement in the presence of static perturbing fields","abstract":"Charged particles interacting with electromagnetic waves have a portion of their energy tied up in wave-driven oscillations. When these waves are localized to the exhaust of linear magnetic confinement systems this ponderomotive effect can be utilized to enhance particle confinement. The same effect can be derived for particles moving via an $\\mathbf{E} \\times \\mathbf{B}$ drift into a region of a static perturbation to the electromagnetic fields which has a large wave vector component in the direction of the motion. In this work we use a simplified slab model to self-consistently solve for the electromagnetic fields within the fluid flowing plasma of a static flute-like ($k_\\parallel = 0)$ perturbation, and evaluate the resulting ponderomotive potential. We find that two types of perturbations can exist within the flowing plasma, which are an O wave and an X wave in the frame moving with the fluid. In the case of tenuous plasma, these perturbations are magnetostatic or electrostatic multipole-analog perpendicular to the guiding magnetic field in the lab frame, respectfully. For denser plasmas, the O wave-like perturbation is screened at the electron skin depth scale, and the X wave-like perturbation is a combination of a similar perpendicular electric perturbation and parallel magnetic perturbation. The ponderomotive potential generated in the X wave-like case is gyrofrequency-dependent, and can be used as either potential barriers or potential wells, depending on the direction of the flow velocity.","sentences":["Charged particles interacting with electromagnetic waves have a portion of their energy tied up in wave-driven oscillations.","When these waves are localized to the exhaust of linear magnetic confinement systems this ponderomotive effect can be utilized to enhance particle confinement.","The same effect can be derived for particles moving via an $\\mathbf{E} \\times \\mathbf{B}$ drift into a region of a static perturbation to the electromagnetic fields which has a large wave vector component in the direction of the motion.","In this work we use a simplified slab model to self-consistently solve for the electromagnetic fields within the fluid flowing plasma of a static flute-like ($k_\\parallel = 0)$ perturbation, and evaluate the resulting ponderomotive potential.","We find that two types of perturbations can exist within the flowing plasma, which are an O wave and an X wave in the frame moving with the fluid.","In the case of tenuous plasma, these perturbations are magnetostatic or electrostatic multipole-analog perpendicular to the guiding magnetic field in the lab frame, respectfully.","For denser plasmas, the O wave-like perturbation is screened at the electron skin depth scale, and the X wave-like perturbation is a combination of a similar perpendicular electric perturbation and parallel magnetic perturbation.","The ponderomotive potential generated in the X wave-like case is gyrofrequency-dependent, and can be used as either potential barriers or potential wells, depending on the direction of the flow velocity."],"url":"http://arxiv.org/abs/2406.03727v1","category":"physics.plasm-ph"}
{"created":"2024-06-06 03:24:32","title":"Demonstration of a Mobile Optical Clock Ensemble at Sea","abstract":"Atomic clocks have been at the leading edge of accuracy and precision since their inception in the 1950s. However, typically the most capable of these clocks have been confined to laboratories despite the fact that there are compelling reasons to apply them in the field and/or while in motion. These applications include synchronization of distributed critical infrastructure (e.g. data servers, communications, electricity grids), scientific applications (e.g. radio-astronomy) and to mitigate the effects of interruption to Global Navigation Satellite Systems.   Over the last 20 years, there has been a breakthrough in the performance of atomic clocks by transitioning from an atomic reference based on a microwave transition to an optical frequency transition. The $10^5$-fold increase in reference frequency confers the potential for significantly higher performance. However, this performance increase has come at the cost of size, complexity and fragility which has continued the confinement of these clocks to the laboratory.   Here we report on a recent international collaboration where three emerging optical clocks, each operating on different principles, were trialed at sea. These clocks incorporate optical frequency combs so that their stable frequency outputs can be used directly in electronic apparatus and were also automated so that they do not require expert supervision. We present the frequency stability and reliability of these three clocks over multiple weeks of unsupervised naval trials, both in harbour and on the ocean. The performance of all three devices was orders of magnitude superior to existing best-in-class commercial solutions over short and medium timescales. This demonstrates that optical clocks are ready to deliver advantages for real-world applications.","sentences":["Atomic clocks have been at the leading edge of accuracy and precision since their inception in the 1950s.","However, typically the most capable of these clocks have been confined to laboratories despite the fact that there are compelling reasons to apply them in the field and/or while in motion.","These applications include synchronization of distributed critical infrastructure (e.g. data servers, communications, electricity grids), scientific applications (e.g. radio-astronomy) and to mitigate the effects of interruption to Global Navigation Satellite Systems.   ","Over the last 20 years, there has been a breakthrough in the performance of atomic clocks by transitioning from an atomic reference based on a microwave transition to an optical frequency transition.","The $10^5$-fold increase in reference frequency confers the potential for significantly higher performance.","However, this performance increase has come at the cost of size, complexity and fragility which has continued the confinement of these clocks to the laboratory.   ","Here we report on a recent international collaboration where three emerging optical clocks, each operating on different principles, were trialed at sea.","These clocks incorporate optical frequency combs so that their stable frequency outputs can be used directly in electronic apparatus and were also automated so that they do not require expert supervision.","We present the frequency stability and reliability of these three clocks over multiple weeks of unsupervised naval trials, both in harbour and on the ocean.","The performance of all three devices was orders of magnitude superior to existing best-in-class commercial solutions over short and medium timescales.","This demonstrates that optical clocks are ready to deliver advantages for real-world applications."],"url":"http://arxiv.org/abs/2406.03716v1","category":"physics.atom-ph"}
{"created":"2024-06-06 02:50:41","title":"Ferroelectricity-tuned band topology and superconductivity in two-dimensional materials and related heterostructures","abstract":"Ferroelectricity, band topology, and superconductivity are respectively local, global, and macroscopic properties of quantum materials, and understanding their mutual couplings offers unique opportunities for exploring rich physics and enhanced functionalities. In this mini-review, we attempt to highlight some of the latest advances in this vibrant area, focusing in particular on ferroelectricity-tuned superconductivity and band topology in two-dimensional (2D) materials and related heterostructures. We will first present results from predictive studies of the delicate couplings between ferroelectricity and topology or superconductivity based on first-principles calculations and phenomenological modeling, with ferroelectricity-enabled topological superconductivity as an appealing objective. Next, we will cover the latest advances on experimental studies of ferroelectricity-tuned superconductivity based on different 2D materials or van der Waals heterostructures. Finally, as perspectives, we will outline schemes that may allow to materialize new types of 2D systems that simultaneously harbor ferroelectricity and superconductivity, or that may lead to enhanced ferroelectric superconductivity, ferroelectric topological superconductivity, and new types of superconducting devices such as superconducting diodes.","sentences":["Ferroelectricity, band topology, and superconductivity are respectively local, global, and macroscopic properties of quantum materials, and understanding their mutual couplings offers unique opportunities for exploring rich physics and enhanced functionalities.","In this mini-review, we attempt to highlight some of the latest advances in this vibrant area, focusing in particular on ferroelectricity-tuned superconductivity and band topology in two-dimensional (2D) materials and related heterostructures.","We will first present results from predictive studies of the delicate couplings between ferroelectricity and topology or superconductivity based on first-principles calculations and phenomenological modeling, with ferroelectricity-enabled topological superconductivity as an appealing objective.","Next, we will cover the latest advances on experimental studies of ferroelectricity-tuned superconductivity based on different 2D materials or van der Waals heterostructures.","Finally, as perspectives, we will outline schemes that may allow to materialize new types of 2D systems that simultaneously harbor ferroelectricity and superconductivity, or that may lead to enhanced ferroelectric superconductivity, ferroelectric topological superconductivity, and new types of superconducting devices such as superconducting diodes."],"url":"http://arxiv.org/abs/2406.03700v1","category":"cond-mat.supr-con"}
{"created":"2024-06-06 02:12:05","title":"A small cosmological constant from a large number of extra dimensions","abstract":"In this article, we consider the $4+n$ dimensional spacetimes among which one is the four dimensional physical Universe and the other is an n-dimensional sphere with constant radius in the framework of Lanczos-Lovelock gravity. We find that the curvature of extra dimensional sphere contributes a huge but negative energy density provided that its radius is sufficiently small, such as the scale of Planck length. Therefore, the huge positive vacuum energy, i.e. the large positive cosmological constant is exactly cancelled out by the curvature of extra sphere. In the mean time the higher order of Lanczos-Lovelock term contributes an observations-allowed small cosmological constant if the number of extra dimensions is sufficiently large, such as $n\\approx{69}$.","sentences":["In this article, we consider the $4+n$ dimensional spacetimes among which one is the four dimensional physical Universe and the other is an n-dimensional sphere with constant radius in the framework of Lanczos-Lovelock gravity.","We find that the curvature of extra dimensional sphere contributes a huge but negative energy density provided that its radius is sufficiently small, such as the scale of Planck length.","Therefore, the huge positive vacuum energy, i.e. the large positive cosmological constant is exactly cancelled out by the curvature of extra sphere.","In the mean time the higher order of Lanczos-Lovelock term contributes an observations-allowed small cosmological constant if the number of extra dimensions is sufficiently large, such as $n\\approx{69}$."],"url":"http://arxiv.org/abs/2406.03687v1","category":"gr-qc"}
{"created":"2024-06-06 02:07:40","title":"Shockingly Bright Warm Carbon Monoxide Molecular Features in the Supernova Remnant Cassiopeia A Revealed by JWST","abstract":"We present JWST NIRCam (F356W and F444W filters) and MIRI (F770W) images and NIRSpec- IFU spectroscopy of the young supernova remnant Cassiopeia A (Cas A). We obtained the data as part of a JWST survey of Cas A. The NIRCam and MIRI images map the spatial distributions of synchrotron radiation, Ar-rich ejecta, and CO on both large and small scales, revealing remarkably complex structures. The CO emission is stronger at the outer layers than the Ar ejecta, which indicates the reformation of CO molecules behind the reverse shock. NIRSpec-IFU spectra (3 - 5.5 microns) were obtained toward two representative knots in the NE and S fields. Both regions are dominated by the bright fundamental rovibrational band of CO in the two R and P branches, with strong [Ar VI] and relatively weaker, variable strength ejecta lines of [Si IX], [Ca IV], [Ca V] and [Mg IV]. The NIRSpec-IFU data resolve individual ejecta knots and filaments spatially and in velocity space. The fundamental CO band in the JWST spectra reveals unique shapes of CO, showing a few tens of sinusoidal patterns of rovibrational lines with pseudo-continuum underneath, which is attributed to the high-velocity widths of CO lines. The CO also shows high J lines at different vibrational transitions. Our results with LTE modeling of CO emission indicate a temperature of 1080 K and provide unique insight into the correlations between dust, molecules, and highly ionized ejecta in supernovae, and have strong ramifications for modeling dust formation that is led by CO cooling in the early Universe.","sentences":["We present JWST NIRCam (F356W and F444W filters) and MIRI (F770W) images and NIRSpec- IFU spectroscopy of the young supernova remnant Cassiopeia A (Cas A).","We obtained the data as part of a JWST survey of Cas A. The NIRCam and MIRI images map the spatial distributions of synchrotron radiation, Ar-rich ejecta, and CO on both large and small scales, revealing remarkably complex structures.","The CO emission is stronger at the outer layers than the Ar ejecta, which indicates the reformation of CO molecules behind the reverse shock.","NIRSpec-IFU spectra (3 - 5.5 microns) were obtained toward two representative knots in the NE and S fields.","Both regions are dominated by the bright fundamental rovibrational band of CO in the two R and P branches, with strong [Ar VI] and relatively weaker, variable strength ejecta lines of [Si IX],","[Ca IV], [Ca V] and [Mg IV].","The NIRSpec-IFU data resolve individual ejecta knots and filaments spatially and in velocity space.","The fundamental CO band in the JWST spectra reveals unique shapes of CO, showing a few tens of sinusoidal patterns of rovibrational lines with pseudo-continuum underneath, which is attributed to the high-velocity widths of CO lines.","The CO also shows high J lines at different vibrational transitions.","Our results with LTE modeling of CO emission indicate a temperature of 1080 K and provide unique insight into the correlations between dust, molecules, and highly ionized ejecta in supernovae, and have strong ramifications for modeling dust formation that is led by CO cooling in the early Universe."],"url":"http://arxiv.org/abs/2406.03685v1","category":"astro-ph.GA"}
