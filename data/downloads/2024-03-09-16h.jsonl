{"created":"2024-03-07 18:59:12","title":"Nonparametric Regression under Cluster Sampling","abstract":"This paper develops a general asymptotic theory for nonparametric kernel regression in the presence of cluster dependence. We examine nonparametric density estimation, Nadaraya-Watson kernel regression, and local linear estimation. Our theory accommodates growing and heterogeneous cluster sizes. We derive asymptotic conditional bias and variance, establish uniform consistency, and prove asymptotic normality. Our findings reveal that under heterogeneous cluster sizes, the asymptotic variance includes a new term reflecting within-cluster dependence, which is overlooked when cluster sizes are presumed to be bounded. We propose valid approaches for bandwidth selection and inference, introduce estimators of the asymptotic variance, and demonstrate their consistency. In simulations, we verify the effectiveness of the cluster-robust bandwidth selection and show that the derived cluster-robust confidence interval improves the coverage ratio. We illustrate the application of these methods using a policy-targeting dataset in development economics.","sentences":["This paper develops a general asymptotic theory for nonparametric kernel regression in the presence of cluster dependence.","We examine nonparametric density estimation, Nadaraya-Watson kernel regression, and local linear estimation.","Our theory accommodates growing and heterogeneous cluster sizes.","We derive asymptotic conditional bias and variance, establish uniform consistency, and prove asymptotic normality.","Our findings reveal that under heterogeneous cluster sizes, the asymptotic variance includes a new term reflecting within-cluster dependence, which is overlooked when cluster sizes are presumed to be bounded.","We propose valid approaches for bandwidth selection and inference, introduce estimators of the asymptotic variance, and demonstrate their consistency.","In simulations, we verify the effectiveness of the cluster-robust bandwidth selection and show that the derived cluster-robust confidence interval improves the coverage ratio.","We illustrate the application of these methods using a policy-targeting dataset in development economics."],"url":"http://arxiv.org/abs/2403.04766v1","category":"econ.EM"}
{"created":"2024-03-07 18:57:46","title":"BloomGML: Graph Machine Learning through the Lens of Bilevel Optimization","abstract":"Bilevel optimization refers to scenarios whereby the optimal solution of a lower-level energy function serves as input features to an upper-level objective of interest. These optimal features typically depend on tunable parameters of the lower-level energy in such a way that the entire bilevel pipeline can be trained end-to-end. Although not generally presented as such, this paper demonstrates how a variety of graph learning techniques can be recast as special cases of bilevel optimization or simplifications thereof. In brief, building on prior work we first derive a more flexible class of energy functions that, when paired with various descent steps (e.g., gradient descent, proximal methods, momentum, etc.), form graph neural network (GNN) message-passing layers; critically, we also carefully unpack where any residual approximation error lies with respect to the underlying constituent message-passing functions. We then probe several simplifications of this framework to derive close connections with non-GNN-based graph learning approaches, including knowledge graph embeddings, various forms of label propagation, and efficient graph-regularized MLP models. And finally, we present supporting empirical results that demonstrate the versatility of the proposed bilevel lens, which we refer to as BloomGML, referencing that BiLevel Optimization Offers More Graph Machine Learning. Our code is available at https://github.com/amberyzheng/BloomGML. Let graph ML bloom.","sentences":["Bilevel optimization refers to scenarios whereby the optimal solution of a lower-level energy function serves as input features to an upper-level objective of interest.","These optimal features typically depend on tunable parameters of the lower-level energy in such a way that the entire bilevel pipeline can be trained end-to-end.","Although not generally presented as such, this paper demonstrates how a variety of graph learning techniques can be recast as special cases of bilevel optimization or simplifications thereof.","In brief, building on prior work we first derive a more flexible class of energy functions that, when paired with various descent steps (e.g., gradient descent, proximal methods, momentum, etc.), form graph neural network (GNN) message-passing layers; critically, we also carefully unpack where any residual approximation error lies with respect to the underlying constituent message-passing functions.","We then probe several simplifications of this framework to derive close connections with non-GNN-based graph learning approaches, including knowledge graph embeddings, various forms of label propagation, and efficient graph-regularized MLP models.","And finally, we present supporting empirical results that demonstrate the versatility of the proposed bilevel lens, which we refer to as BloomGML, referencing that BiLevel Optimization Offers More Graph Machine Learning.","Our code is available at https://github.com/amberyzheng/BloomGML.","Let graph ML bloom."],"url":"http://arxiv.org/abs/2403.04763v1","category":"cs.LG"}
{"created":"2024-03-07 18:56:39","title":"iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries","abstract":"The recent explosion in popularity of large language models (LLMs) has inspired learning engineers to incorporate them into adaptive educational tools that automatically score summary writing. Understanding and evaluating LLMs is vital before deploying them in critical learning environments, yet their unprecedented size and expanding number of parameters inhibits transparency and impedes trust when they underperform. Through a collaborative user-centered design process with several learning engineers building and deploying summary scoring LLMs, we characterized fundamental design challenges and goals around interpreting their models, including aggregating large text inputs, tracking score provenance, and scaling LLM interpretability methods. To address their concerns, we developed iScore, an interactive visual analytics tool for learning engineers to upload, score, and compare multiple summaries simultaneously. Tightly integrated views allow users to iteratively revise the language in summaries, track changes in the resulting LLM scores, and visualize model weights at multiple levels of abstraction. To validate our approach, we deployed iScore with three learning engineers over the course of a month. We present a case study where interacting with iScore led a learning engineer to improve their LLM's score accuracy by three percentage points. Finally, we conducted qualitative interviews with the learning engineers that revealed how iScore enabled them to understand, evaluate, and build trust in their LLMs during deployment.","sentences":["The recent explosion in popularity of large language models (LLMs) has inspired learning engineers to incorporate them into adaptive educational tools that automatically score summary writing.","Understanding and evaluating LLMs is vital before deploying them in critical learning environments, yet their unprecedented size and expanding number of parameters inhibits transparency and impedes trust when they underperform.","Through a collaborative user-centered design process with several learning engineers building and deploying summary scoring LLMs, we characterized fundamental design challenges and goals around interpreting their models, including aggregating large text inputs, tracking score provenance, and scaling LLM interpretability methods.","To address their concerns, we developed iScore, an interactive visual analytics tool for learning engineers to upload, score, and compare multiple summaries simultaneously.","Tightly integrated views allow users to iteratively revise the language in summaries, track changes in the resulting LLM scores, and visualize model weights at multiple levels of abstraction.","To validate our approach, we deployed iScore with three learning engineers over the course of a month.","We present a case study where interacting with iScore led a learning engineer to improve their LLM's score accuracy by three percentage points.","Finally, we conducted qualitative interviews with the learning engineers that revealed how iScore enabled them to understand, evaluate, and build trust in their LLMs during deployment."],"url":"http://arxiv.org/abs/2403.04760v1","category":"cs.HC"}
{"created":"2024-03-07 18:56:33","title":"Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing","abstract":"On-device learning has emerged as a prevailing trend that avoids the slow response time and costly communication of cloud-based learning. The ability to learn continuously and indefinitely in a changing environment, and with resource constraints, is critical for real sensor deployments. However, existing designs are inadequate for practical scenarios with (i) streaming data input, (ii) lack of supervision and (iii) limited on-board resources. In this paper, we design and deploy the first on-device lifelong learning system called LifeHD for general IoT applications with limited supervision. LifeHD is designed based on a novel neurally-inspired and lightweight learning paradigm called Hyperdimensional Computing (HDC). We utilize a two-tier associative memory organization to intelligently store and manage high-dimensional, low-precision vectors, which represent the historical patterns as cluster centroids. We additionally propose two variants of LifeHD to cope with scarce labeled inputs and power constraints. We implement LifeHD on off-the-shelf edge platforms and perform extensive evaluations across three scenarios. Our measurements show that LifeHD improves the unsupervised clustering accuracy by up to 74.8% compared to the state-of-the-art NN-based unsupervised lifelong learning baselines with as much as 34.3x better energy efficiency. Our code is available at https://github.com/Orienfish/LifeHD.","sentences":["On-device learning has emerged as a prevailing trend that avoids the slow response time and costly communication of cloud-based learning.","The ability to learn continuously and indefinitely in a changing environment, and with resource constraints, is critical for real sensor deployments.","However, existing designs are inadequate for practical scenarios with (i) streaming data input, (ii) lack of supervision and (iii) limited on-board resources.","In this paper, we design and deploy the first on-device lifelong learning system called LifeHD for general IoT applications with limited supervision.","LifeHD is designed based on a novel neurally-inspired and lightweight learning paradigm called Hyperdimensional Computing (HDC).","We utilize a two-tier associative memory organization to intelligently store and manage high-dimensional, low-precision vectors, which represent the historical patterns as cluster centroids.","We additionally propose two variants of LifeHD to cope with scarce labeled inputs and power constraints.","We implement LifeHD on off-the-shelf edge platforms and perform extensive evaluations across three scenarios.","Our measurements show that LifeHD improves the unsupervised clustering accuracy by up to 74.8% compared to the state-of-the-art NN-based unsupervised lifelong learning baselines with as much as 34.3x better energy efficiency.","Our code is available at https://github.com/Orienfish/LifeHD."],"url":"http://arxiv.org/abs/2403.04759v1","category":"cs.LG"}
{"created":"2024-03-07 18:56:31","title":"KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts","abstract":"Recent growth in the popularity of large language models has led to their increased usage for summarizing, predicting, and generating text, making it vital to help researchers and engineers understand how and why they work. We present KnowledgeVis, a human-in-the-loop visual analytics system for interpreting language models using fill-in-the-blank sentences as prompts. By comparing predictions between sentences, KnowledgeVis reveals learned associations that intuitively connect what language models learn during training to natural language tasks downstream, helping users create and test multiple prompt variations, analyze predicted words using a novel semantic clustering technique, and discover insights using interactive visualizations. Collectively, these visualizations help users identify the likelihood and uniqueness of individual predictions, compare sets of predictions between prompts, and summarize patterns and relationships between predictions across all prompts. We demonstrate the capabilities of KnowledgeVis with feedback from six NLP experts as well as three different use cases: (1) probing biomedical knowledge in two domain-adapted models; and (2) evaluating harmful identity stereotypes and (3) discovering facts and relationships between three general-purpose models.","sentences":["Recent growth in the popularity of large language models has led to their increased usage for summarizing, predicting, and generating text, making it vital to help researchers and engineers understand how and why they work.","We present KnowledgeVis, a human-in-the-loop visual analytics system for interpreting language models using fill-in-the-blank sentences as prompts.","By comparing predictions between sentences, KnowledgeVis reveals learned associations that intuitively connect what language models learn during training to natural language tasks downstream, helping users create and test multiple prompt variations, analyze predicted words using a novel semantic clustering technique, and discover insights using interactive visualizations.","Collectively, these visualizations help users identify the likelihood and uniqueness of individual predictions, compare sets of predictions between prompts, and summarize patterns and relationships between predictions across all prompts.","We demonstrate the capabilities of KnowledgeVis with feedback from six NLP experts as well as three different use cases: (1) probing biomedical knowledge in two domain-adapted models; and (2) evaluating harmful identity stereotypes and (3) discovering facts and relationships between three general-purpose models."],"url":"http://arxiv.org/abs/2403.04758v1","category":"cs.HC"}
{"created":"2024-03-07 18:56:16","title":"Preliminary Guidelines For Combining Data Integration and Visual Data Analysis","abstract":"Data integration is often performed to consolidate information from multiple disparate data sources during visual data analysis. However, integration operations are usually separate from visual analytics operations such as encode and filter in both interface design and empirical research. We conducted a preliminary user study to investigate whether and how data integration should be incorporated directly into the visual analytics process. We used two interface alternatives featuring contrasting approaches to the data preparation and analysis workflow: manual file-based ex-situ integration as a separate step from visual analytics operations; and automatic UI-based in-situ integration merged with visual analytics operations. Participants were asked to complete specific and free-form tasks with each interface, browsing for patterns, generating insights, and summarizing relationships between attributes distributed across multiple files. Analyzing participants' interactions and feedback, we found both task completion time and total interactions to be similar across interfaces and tasks, as well as unique integration strategies between interfaces and emergent behaviors related to satisficing and cognitive bias. Participants' time spent and interactions revealed that in-situ integration enabled users to spend more time on analysis tasks compared with ex-situ integration. Participants' integration strategies and analytical behaviors revealed differences in interface usage for generating and tracking hypotheses and insights. With these results, we synthesized preliminary guidelines for designing future visual analytics interfaces that can support integrating attributes throughout an active analysis process.","sentences":["Data integration is often performed to consolidate information from multiple disparate data sources during visual data analysis.","However, integration operations are usually separate from visual analytics operations such as encode and filter in both interface design and empirical research.","We conducted a preliminary user study to investigate whether and how data integration should be incorporated directly into the visual analytics process.","We used two interface alternatives featuring contrasting approaches to the data preparation and analysis workflow: manual file-based ex-situ integration as a separate step from visual analytics operations; and automatic UI-based in-situ integration merged with visual analytics operations.","Participants were asked to complete specific and free-form tasks with each interface, browsing for patterns, generating insights, and summarizing relationships between attributes distributed across multiple files.","Analyzing participants' interactions and feedback, we found both task completion time and total interactions to be similar across interfaces and tasks, as well as unique integration strategies between interfaces and emergent behaviors related to satisficing and cognitive bias.","Participants' time spent and interactions revealed that in-situ integration enabled users to spend more time on analysis tasks compared with ex-situ integration.","Participants' integration strategies and analytical behaviors revealed differences in interface usage for generating and tracking hypotheses and insights.","With these results, we synthesized preliminary guidelines for designing future visual analytics interfaces that can support integrating attributes throughout an active analysis process."],"url":"http://arxiv.org/abs/2403.04757v1","category":"cs.HC"}
{"created":"2024-03-07 18:54:59","title":"Mechanism for Decision-aware Collaborative Federated Learning: A Pitfall of Shapley Values","abstract":"This paper investigates mechanism design for decision-aware collaboration via federated learning (FL) platforms. Our framework consists of a digital platform and multiple decision-aware agents, each endowed with proprietary data sets. The platform offers an infrastructure that enables access to the data, creates incentives for collaborative learning aimed at operational decision-making, and conducts FL to avoid direct raw data sharing. The computation and communication efficiency of the FL process is inherently influenced by the agent participation equilibrium induced by the mechanism. Therefore, assessing the system's efficiency involves two critical factors: the surplus created by coalition formation and the communication costs incurred across the coalition during FL. To evaluate the system efficiency under the intricate interplay between mechanism design, agent participation, operational decision-making, and the performance of FL algorithms, we introduce a multi-action collaborative federated learning (MCFL) framework for decision-aware agents. Under this framework, we further analyze the equilibrium for the renowned Shapley value based mechanisms. Specifically, we examine the issue of false-name manipulation, a form of dishonest behavior where participating agents create duplicate fake identities to split their original data among these identities. By solving the agent participation equilibrium, we demonstrate that while Shapley value effectively maximizes coalition-generated surplus by encouraging full participation, it inadvertently promotes false-name manipulation. This further significantly increases the communication costs when the platform conducts FL. Thus, we highlight a significant pitfall of Shapley value based mechanisms, which implicitly incentivizes data splitting and identity duplication, ultimately impairing the overall efficiency in FL systems.","sentences":["This paper investigates mechanism design for decision-aware collaboration via federated learning (FL) platforms.","Our framework consists of a digital platform and multiple decision-aware agents, each endowed with proprietary data sets.","The platform offers an infrastructure that enables access to the data, creates incentives for collaborative learning aimed at operational decision-making, and conducts FL to avoid direct raw data sharing.","The computation and communication efficiency of the FL process is inherently influenced by the agent participation equilibrium induced by the mechanism.","Therefore, assessing the system's efficiency involves two critical factors: the surplus created by coalition formation and the communication costs incurred across the coalition during FL.","To evaluate the system efficiency under the intricate interplay between mechanism design, agent participation, operational decision-making, and the performance of FL algorithms, we introduce a multi-action collaborative federated learning (MCFL) framework for decision-aware agents.","Under this framework, we further analyze the equilibrium for the renowned Shapley value based mechanisms.","Specifically, we examine the issue of false-name manipulation, a form of dishonest behavior where participating agents create duplicate fake identities to split their original data among these identities.","By solving the agent participation equilibrium, we demonstrate that while Shapley value effectively maximizes coalition-generated surplus by encouraging full participation, it inadvertently promotes false-name manipulation.","This further significantly increases the communication costs when the platform conducts FL.","Thus, we highlight a significant pitfall of Shapley value based mechanisms, which implicitly incentivizes data splitting and identity duplication, ultimately impairing the overall efficiency in FL systems."],"url":"http://arxiv.org/abs/2403.04753v1","category":"cs.GT"}
{"created":"2024-03-07 18:54:54","title":"On semidefinite descriptions for convex hulls of quadratic programs","abstract":"Quadratically constrained quadratic programs (QCQPs) are a highly expressive class of nonconvex optimization problems. While QCQPs are NP-hard in general, they admit a natural convex relaxation via the standard semidefinite program (SDP) relaxation. In this paper we study when the convex hull of the epigraph of a QCQP coincides with the projected epigraph of the SDP relaxation. We present a sufficient condition for convex hull exactness and show that this condition is further necessary under an additional geometric assumption. The sufficient condition is based on geometric properties of $\\Gamma$, the cone of convex Lagrange multipliers, and its relatives $\\Gamma_1$ and $\\Gamma^\\circ$.","sentences":["Quadratically constrained quadratic programs (QCQPs) are a highly expressive class of nonconvex optimization problems.","While QCQPs are NP-hard in general, they admit a natural convex relaxation via the standard semidefinite program (SDP) relaxation.","In this paper we study when the convex hull of the epigraph of a QCQP coincides with the projected epigraph of the SDP relaxation.","We present a sufficient condition for convex hull exactness and show that this condition is further necessary under an additional geometric assumption.","The sufficient condition is based on geometric properties of $\\Gamma$, the cone of convex Lagrange multipliers, and its relatives $\\Gamma_1$ and $\\Gamma^\\circ$."],"url":"http://arxiv.org/abs/2403.04752v1","category":"math.OC"}
{"created":"2024-03-07 18:53:53","title":"JAX-SPH: A Differentiable Smoothed Particle Hydrodynamics Framework","abstract":"Particle-based fluid simulations have emerged as a powerful tool for solving the Navier-Stokes equations, especially in cases that include intricate physics and free surfaces. The recent addition of machine learning methods to the toolbox for solving such problems is pushing the boundary of the quality vs. speed tradeoff of such numerical simulations. In this work, we lead the way to Lagrangian fluid simulators compatible with deep learning frameworks, and propose JAX-SPH - a Smoothed Particle Hydrodynamics (SPH) framework implemented in JAX. JAX-SPH builds on the code for dataset generation from the LagrangeBench project (Toshev et al., 2023) and extends this code in multiple ways: (a) integration of further key SPH algorithms, (b) restructuring the code toward a Python library, (c) verification of the gradients through the solver, and (d) demonstration of the utility of the gradients for solving inverse problems as well as a Solver-in-the-Loop application. Our code is available at https://github.com/tumaer/jax-sph.","sentences":["Particle-based fluid simulations have emerged as a powerful tool for solving the Navier-Stokes equations, especially in cases that include intricate physics and free surfaces.","The recent addition of machine learning methods to the toolbox for solving such problems is pushing the boundary of the quality vs. speed tradeoff of such numerical simulations.","In this work, we lead the way to Lagrangian fluid simulators compatible with deep learning frameworks, and propose JAX-SPH - a Smoothed Particle Hydrodynamics (SPH) framework implemented in JAX.","JAX-SPH builds on the code for dataset generation from the LagrangeBench project (Toshev et al., 2023) and extends this code in multiple ways: (a) integration of further key SPH algorithms, (b) restructuring the code toward a Python library, (c) verification of the gradients through the solver, and (d) demonstration of the utility of the gradients for solving inverse problems as well as a Solver-in-the-Loop application.","Our code is available at https://github.com/tumaer/jax-sph."],"url":"http://arxiv.org/abs/2403.04750v1","category":"physics.flu-dyn"}
{"created":"2024-03-07 18:53:33","title":"Iyer-Wald ambiguities and gauge covariance of Entropy current in Higher derivative theories of gravity","abstract":"In [arXiv:2105.06455, arXiv:2206.04538], the authors have been able to argue for an ultra-local version of the second law of black hole mechanics, for arbitrary diffeomorphism invariant theories of gravity non-minimally coupled to matter fields, by constructing an entropy current on the dynamical horizon with manifestly positive divergence. This has been achieved by working in the horizon-adapted coordinate system. In this work, we show that the local entropy production through the divergence of the entropy current is covariant under affine reparametrizations that leave the gauge of horizon-adapted coordinates invariant. We explicitly derive a formula for how the entropy current transforms under such coordinate transformations. This extends the analysis of [arXiv:2204.08447] for arbitrary diffeomorphism invariant theories of gravity non-minimally coupled to matter fields. We also study the Iyer-Wald ambiguities of the covariant phase formalism that generically plague the components of the entropy current.","sentences":["In [arXiv:2105.06455, arXiv:2206.04538], the authors have been able to argue for an ultra-local version of the second law of black hole mechanics, for arbitrary diffeomorphism invariant theories of gravity non-minimally coupled to matter fields, by constructing an entropy current on the dynamical horizon with manifestly positive divergence.","This has been achieved by working in the horizon-adapted coordinate system.","In this work, we show that the local entropy production through the divergence of the entropy current is covariant under affine reparametrizations that leave the gauge of horizon-adapted coordinates invariant.","We explicitly derive a formula for how the entropy current transforms under such coordinate transformations.","This extends the analysis of [arXiv:2204.08447] for arbitrary diffeomorphism invariant theories of gravity non-minimally coupled to matter fields.","We also study the Iyer-Wald ambiguities of the covariant phase formalism that generically plague the components of the entropy current."],"url":"http://arxiv.org/abs/2403.04749v1","category":"hep-th"}
{"created":"2024-03-07 18:52:27","title":"GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks","abstract":"Graph neural networks (GNNs), and especially message-passing neural networks, excel in various domains such as physics, drug discovery, and molecular modeling. The expressivity of GNNs with respect to their ability to discriminate non-isomorphic graphs critically depends on the functions employed for message aggregation and graph-level readout. By applying signal propagation theory, we propose a variance-preserving aggregation function (VPA) that maintains expressivity, but yields improved forward and backward dynamics. Experiments demonstrate that VPA leads to increased predictive performance for popular GNN architectures as well as improved learning dynamics. Our results could pave the way towards normalizer-free or self-normalizing GNNs.","sentences":["Graph neural networks (GNNs), and especially message-passing neural networks, excel in various domains such as physics, drug discovery, and molecular modeling.","The expressivity of GNNs with respect to their ability to discriminate non-isomorphic graphs critically depends on the functions employed for message aggregation and graph-level readout.","By applying signal propagation theory, we propose a variance-preserving aggregation function (VPA) that maintains expressivity, but yields improved forward and backward dynamics.","Experiments demonstrate that VPA leads to increased predictive performance for popular GNN architectures as well as improved learning dynamics.","Our results could pave the way towards normalizer-free or self-normalizing GNNs."],"url":"http://arxiv.org/abs/2403.04747v1","category":"cs.LG"}
{"created":"2024-03-07 18:50:51","title":"LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error","abstract":"Tools are essential for large language models (LLMs) to acquire up-to-date information and take consequential actions in external environments. Existing work on tool-augmented LLMs primarily focuses on the broad coverage of tools and the flexibility of adding new tools. However, a critical aspect that has surprisingly been understudied is simply how accurately an LLM uses tools for which it has been trained. We find that existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice. We propose a biologically inspired method for tool-augmented LLMs, simulated trial and error (STE), that orchestrates three key mechanisms for successful tool use behaviors in the biological system: trial and error, imagination, and memory. Specifically, STE leverages an LLM's 'imagination' to simulate plausible scenarios for using a tool, after which the LLM interacts with the tool to learn from its execution feedback. Both short-term and long-term memory are employed to improve the depth and breadth of the exploration, respectively. Comprehensive experiments on ToolBench show that STE substantially improves tool learning for LLMs under both in-context learning and fine-tuning settings, bringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform GPT-4. We also show effective continual learning of tools via a simple experience replay strategy.","sentences":["Tools are essential for large language models (LLMs) to acquire up-to-date information and take consequential actions in external environments.","Existing work on tool-augmented LLMs primarily focuses on the broad coverage of tools and the flexibility of adding new tools.","However, a critical aspect that has surprisingly been understudied is simply how accurately an LLM uses tools for which it has been trained.","We find that existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice.","We propose a biologically inspired method for tool-augmented LLMs, simulated trial and error (STE), that orchestrates three key mechanisms for successful tool use behaviors in the biological system: trial and error, imagination, and memory.","Specifically, STE leverages an LLM's 'imagination' to simulate plausible scenarios for using a tool, after which the LLM interacts with the tool to learn from its execution feedback.","Both short-term and long-term memory are employed to improve the depth and breadth of the exploration, respectively.","Comprehensive experiments on ToolBench show that STE substantially improves tool learning for LLMs under both in-context learning and fine-tuning settings, bringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform GPT-4.","We also show effective continual learning of tools via a simple experience replay strategy."],"url":"http://arxiv.org/abs/2403.04746v1","category":"cs.CL"}
{"created":"2024-03-07 18:49:36","title":"A General Calibrated Regret Metric for Detecting and Mitigating Human-Robot Interaction Failures","abstract":"Robot decision-making increasingly relies on expressive data-driven human prediction models when operating around people. While these models are known to suffer from prediction errors in out-of-distribution interactions, not all prediction errors equally impact downstream robot performance. We identify that the mathematical notion of regret precisely characterizes the degree to which incorrect predictions of future interaction outcomes degraded closed-loop robot performance. However, canonical regret measures are poorly calibrated across diverse deployment interactions. We extend the canonical notion of regret by deriving a calibrated regret metric that generalizes from absolute reward space to probability space. With this transformation, our metric removes the need for explicit reward functions to calculate the robot's regret, enables fairer comparison of interaction anomalies across disparate deployment contexts, and facilitates targetted dataset construction of \"system-level\" prediction failures. We experimentally quantify the value of this high-regret interaction data for aiding the robot in improving its downstream decision-making. In a suite of closed-loop autonomous driving simulations, we find that fine-tuning ego-conditioned behavior predictors exclusively on high-regret human-robot interaction data can improve the robot's overall re-deployment performance with significantly (77%) less data.","sentences":["Robot decision-making increasingly relies on expressive data-driven human prediction models when operating around people.","While these models are known to suffer from prediction errors in out-of-distribution interactions, not all prediction errors equally impact downstream robot performance.","We identify that the mathematical notion of regret precisely characterizes the degree to which incorrect predictions of future interaction outcomes degraded closed-loop robot performance.","However, canonical regret measures are poorly calibrated across diverse deployment interactions.","We extend the canonical notion of regret by deriving a calibrated regret metric that generalizes from absolute reward space to probability space.","With this transformation, our metric removes the need for explicit reward functions to calculate the robot's regret, enables fairer comparison of interaction anomalies across disparate deployment contexts, and facilitates targetted dataset construction of \"system-level\" prediction failures.","We experimentally quantify the value of this high-regret interaction data for aiding the robot in improving its downstream decision-making.","In a suite of closed-loop autonomous driving simulations, we find that fine-tuning ego-conditioned behavior predictors exclusively on high-regret human-robot interaction data can improve the robot's overall re-deployment performance with significantly (77%) less data."],"url":"http://arxiv.org/abs/2403.04745v1","category":"cs.RO"}
{"created":"2024-03-07 18:49:32","title":"SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker Assumptions","abstract":"We study the complexity of Non-Gaussian Component Analysis (NGCA) in the Statistical Query (SQ) model. Prior work developed a general methodology to prove SQ lower bounds for this task that have been applicable to a wide range of contexts. In particular, it was known that for any univariate distribution $A$ satisfying certain conditions, distinguishing between a standard multivariate Gaussian and a distribution that behaves like $A$ in a random hidden direction and like a standard Gaussian in the orthogonal complement, is SQ-hard. The required conditions were that (1) $A$ matches many low-order moments with the standard univariate Gaussian, and (2) the chi-squared norm of $A$ with respect to the standard Gaussian is finite. While the moment-matching condition is necessary for hardness, the chi-squared condition was only required for technical reasons. In this work, we establish that the latter condition is indeed not necessary. In particular, we prove near-optimal SQ lower bounds for NGCA under the moment-matching condition only. Our result naturally generalizes to the setting of a hidden subspace. Leveraging our general SQ lower bound, we obtain near-optimal SQ lower bounds for a range of concrete estimation tasks where existing techniques provide sub-optimal or even vacuous guarantees.","sentences":["We study the complexity of Non-Gaussian Component Analysis (NGCA) in the Statistical Query (SQ) model.","Prior work developed a general methodology to prove SQ lower bounds for this task that have been applicable to a wide range of contexts.","In particular, it was known that for any univariate distribution $A$ satisfying certain conditions, distinguishing between a standard multivariate Gaussian and a distribution that behaves like $A$ in a random hidden direction and like a standard Gaussian in the orthogonal complement, is SQ-hard.","The required conditions were that (1) $A$ matches many low-order moments with the standard univariate Gaussian, and (2) the chi-squared norm of $A$ with respect to the standard Gaussian is finite.","While the moment-matching condition is necessary for hardness, the chi-squared condition was only required for technical reasons.","In this work, we establish that the latter condition is indeed not necessary.","In particular, we prove near-optimal SQ lower bounds for NGCA under the moment-matching condition only.","Our result naturally generalizes to the setting of a hidden subspace.","Leveraging our general SQ lower bound, we obtain near-optimal SQ lower bounds for a range of concrete estimation tasks where existing techniques provide sub-optimal or even vacuous guarantees."],"url":"http://arxiv.org/abs/2403.04744v1","category":"cs.LG"}
{"created":"2024-03-07 18:49:01","title":"Bayesian Inference of Time-Varying Origin-Destination Matrices from Boarding/Alighting Counts for Transit Services","abstract":"Origin-destination (OD) demand matrices are crucial for transit agencies to design and operate transit systems. This paper presents a novel temporal Bayesian model designed to estimate transit OD matrices at the individual bus-journey level from boarding/alighting counts at bus stops. Our approach begins by modeling the number of alighting passengers at subsequent bus stops, given a boarding stop, through a multinomial distribution parameterized by alighting probabilities. Given the large scale of the problem, we generate alighting probabilities with a latent variable matrix and factorize it into a mapping matrix and a temporal matrix, thereby substantially reducing the number of parameters. To further encode a temporally-smooth structure in the parameters, we impose a Gaussian process prior on the columns of the temporal factor matrix. For model inference, we develop a two-stage algorithm with the Markov chain Monte Carlo (MCMC) method. In the first stage, latent OD matrices are sampled conditional on model parameters using a Metropolis-Hastings sampling algorithm with a Markov model-based proposal distribution. In the second stage, we sample model parameters conditional on latent OD matrices using slice and elliptical slice sampling algorithms. We assess the proposed model using real-world data collected from three bus routes with varying numbers of stops, and the results demonstrate that our model achieves accurate posterior mean estimation and outperforms the widely used iterative proportional fitting (IPF) method. Additionally, our model can provide uncertainty quantification for the OD demand matrices, thus benefiting many downstream planning/operational tasks that require robust decisions.","sentences":["Origin-destination (OD) demand matrices are crucial for transit agencies to design and operate transit systems.","This paper presents a novel temporal Bayesian model designed to estimate transit OD matrices at the individual bus-journey level from boarding/alighting counts at bus stops.","Our approach begins by modeling the number of alighting passengers at subsequent bus stops, given a boarding stop, through a multinomial distribution parameterized by alighting probabilities.","Given the large scale of the problem, we generate alighting probabilities with a latent variable matrix and factorize it into a mapping matrix and a temporal matrix, thereby substantially reducing the number of parameters.","To further encode a temporally-smooth structure in the parameters, we impose a Gaussian process prior on the columns of the temporal factor matrix.","For model inference, we develop a two-stage algorithm with the Markov chain Monte Carlo (MCMC) method.","In the first stage, latent OD matrices are sampled conditional on model parameters using a Metropolis-Hastings sampling algorithm with a Markov model-based proposal distribution.","In the second stage, we sample model parameters conditional on latent OD matrices using slice and elliptical slice sampling algorithms.","We assess the proposed model using real-world data collected from three bus routes with varying numbers of stops, and the results demonstrate that our model achieves accurate posterior mean estimation and outperforms the widely used iterative proportional fitting (IPF) method.","Additionally, our model can provide uncertainty quantification for the OD demand matrices, thus benefiting many downstream planning/operational tasks that require robust decisions."],"url":"http://arxiv.org/abs/2403.04742v1","category":"stat.AP"}
{"created":"2024-03-07 18:47:11","title":"Effects of mechanical stress, chemical potential, and coverage on hydrogen solubility during hydrogen enhanced decohesion of ferritic steel grain boundaries: A first-principles study","abstract":"Hydrogen enhanced decohesion (HEDE) is one of the many mechanisms of hydrogen embrittlement, a phenomenon which severely impacts structural materials such as iron and iron alloys. Grain boundaries (GBs) play a critical role in this mechanism, where they can provide trapping sites or act as hydrogen diffusion pathways. The interaction of H with GBs and other crystallographic defects, and thus the solubility and distribution of H in the microstructure, depends on the concentration, chemical potential and local stress. Therefore, for a quantitative assessment of HEDE, a generalized solution energy in conjunction with the cohesive strength as a function of hydrogen coverage is needed. In this work, we carry out density functional theory calculations to investigate the influence of H on the decohesion of the $\\Sigma$5(310)[001] and $\\Sigma$3(112)[1$\\bar{1}$0] symmetrical tilt GBs in bcc Fe, as examples for open and close-packed GB structures. A method to identify the segregation sites at the GB plane is proposed. The results indicate that at higher local concentrations, H leads to a significant reduction of the cohesive strength of the GB planes, significantly more pronounced at the $\\Sigma$5 than at the $\\Sigma$3 GB. Interestingly, at finite stress the $\\Sigma$3 GB becomes more favorable for H solution, as opposed to the case of zero stress, where the $\\Sigma$5 GB is more attractive. This suggests that under certain conditions stresses in the microstructure can lead to a re-distribution of H to the stronger grain boundary, which opens a new path to designing H-resistant microstructures. To round up our study, we investigate the effects of typical alloying elements in ferritic steel, C, V, Cr and Mn, on the solubility of H and the strength of the GBs.","sentences":["Hydrogen enhanced decohesion (HEDE) is one of the many mechanisms of hydrogen embrittlement, a phenomenon which severely impacts structural materials such as iron and iron alloys.","Grain boundaries (GBs) play a critical role in this mechanism, where they can provide trapping sites or act as hydrogen diffusion pathways.","The interaction of H with GBs and other crystallographic defects, and thus the solubility and distribution of H in the microstructure, depends on the concentration, chemical potential and local stress.","Therefore, for a quantitative assessment of HEDE, a generalized solution energy in conjunction with the cohesive strength as a function of hydrogen coverage is needed.","In this work, we carry out density functional theory calculations to investigate the influence of H on the decohesion of the $\\Sigma$5(310)[001] and $\\Sigma$3(112)[1$\\bar{1}$0] symmetrical tilt GBs in bcc Fe, as examples for open and close-packed GB structures.","A method to identify the segregation sites at the GB plane is proposed.","The results indicate that at higher local concentrations, H leads to a significant reduction of the cohesive strength of the GB planes, significantly more pronounced at the $\\Sigma$5 than at the $\\Sigma$3 GB.","Interestingly, at finite stress the $\\Sigma$3 GB becomes more favorable for H solution, as opposed to the case of zero stress, where the $\\Sigma$5 GB is more attractive.","This suggests that under certain conditions stresses in the microstructure can lead to a re-distribution of H to the stronger grain boundary, which opens a new path to designing H-resistant microstructures.","To round up our study, we investigate the effects of typical alloying elements in ferritic steel, C, V, Cr and Mn, on the solubility of H and the strength of the GBs."],"url":"http://arxiv.org/abs/2403.04741v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-07 18:46:58","title":"Quantum One-Wayness of the Single-Round Sponge with Invertible Permutations","abstract":"Sponge hashing is a novel class of cryptographic hash algorithms which underlies the current international hash function standard SHA-3. In a nutshell, a sponge function takes as input a bit-stream of any length and processes it via a simple iterative procedure: it repeatedly feeds each block of the input into a so-called block function, and then produces a short digest which consists of a subset of the final output bits. While much is known about the post-quantum security of the sponge construction in the case when the block function is modeled as a random function or permutation, the case of invertible permutations, which more accurately models the construction underlying SHA-3, has so far remained a fundamental open problem.   In this work, we make new progress towards overcoming this barrier and show several results. First, we prove the \"double-sided zero-search\" conjecture proposed by Unruh (eprint' 2021) and show that finding zero-pairs in a random $2n$-bit permutation requires at least $\\Omega(2^{n/2})$ many queries -- and this is tight due to Grover's algorithm. At the core of our proof lies a novel \"symmetrization argument\" which uses insights from the theory of Young subgroups. Second, we consider more general variants of the double-sided search problem and show similar query lower bounds for them. As an application, we prove the quantum one-wayness of the single-round sponge with invertible permutations in the quantum random oracle model.","sentences":["Sponge hashing is a novel class of cryptographic hash algorithms which underlies the current international hash function standard SHA-3.","In a nutshell, a sponge function takes as input a bit-stream of any length and processes it via a simple iterative procedure: it repeatedly feeds each block of the input into a so-called block function, and then produces a short digest which consists of a subset of the final output bits.","While much is known about the post-quantum security of the sponge construction in the case when the block function is modeled as a random function or permutation, the case of invertible permutations, which more accurately models the construction underlying SHA-3, has so far remained a fundamental open problem.   ","In this work, we make new progress towards overcoming this barrier and show several results.","First, we prove the \"double-sided zero-search\" conjecture proposed by Unruh (eprint' 2021) and show that finding zero-pairs in a random $2n$-bit permutation requires at least $\\Omega(2^{n/2})$ many queries -- and this is tight due to Grover's algorithm.","At the core of our proof lies a novel \"symmetrization argument\" which uses insights from the theory of Young subgroups.","Second, we consider more general variants of the double-sided search problem and show similar query lower bounds for them.","As an application, we prove the quantum one-wayness of the single-round sponge with invertible permutations in the quantum random oracle model."],"url":"http://arxiv.org/abs/2403.04740v1","category":"quant-ph"}
{"created":"2024-03-07 18:35:54","title":"How Far Are We from Intelligent Visual Deductive Reasoning?","abstract":"Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective when applied to LLMs do not seamlessly translate to the challenges presented by visual reasoning tasks. Moreover, a detailed analysis reveals that VLMs struggle to solve these tasks mainly because they are unable to perceive and comprehend multiple, confounding abstract patterns in RPM examples.","sentences":["Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated incredible strides on diverse vision language tasks.","We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs.","Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues.","We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN.","The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning.","We found that certain standard strategies that are effective when applied to LLMs do not seamlessly translate to the challenges presented by visual reasoning tasks.","Moreover, a detailed analysis reveals that VLMs struggle to solve these tasks mainly because they are unable to perceive and comprehend multiple, confounding abstract patterns in RPM examples."],"url":"http://arxiv.org/abs/2403.04732v1","category":"cs.AI"}
{"created":"2024-03-07 18:35:18","title":"Photonic probabilistic machine learning using quantum vacuum noise","abstract":"Probabilistic machine learning utilizes controllable sources of randomness to encode uncertainty and enable statistical modeling. Harnessing the pure randomness of quantum vacuum noise, which stems from fluctuating electromagnetic fields, has shown promise for high speed and energy-efficient stochastic photonic elements. Nevertheless, photonic computing hardware which can control these stochastic elements to program probabilistic machine learning algorithms has been limited. Here, we implement a photonic probabilistic computer consisting of a controllable stochastic photonic element - a photonic probabilistic neuron (PPN). Our PPN is implemented in a bistable optical parametric oscillator (OPO) with vacuum-level injected bias fields. We then program a measurement-and-feedback loop for time-multiplexed PPNs with electronic processors (FPGA or GPU) to solve certain probabilistic machine learning tasks. We showcase probabilistic inference and image generation of MNIST-handwritten digits, which are representative examples of discriminative and generative models. In both implementations, quantum vacuum noise is used as a random seed to encode classification uncertainty or probabilistic generation of samples. In addition, we propose a path towards an all-optical probabilistic computing platform, with an estimated sampling rate of ~ 1 Gbps and energy consumption of ~ 5 fJ/MAC. Our work paves the way for scalable, ultrafast, and energy-efficient probabilistic machine learning hardware.","sentences":["Probabilistic machine learning utilizes controllable sources of randomness to encode uncertainty and enable statistical modeling.","Harnessing the pure randomness of quantum vacuum noise, which stems from fluctuating electromagnetic fields, has shown promise for high speed and energy-efficient stochastic photonic elements.","Nevertheless, photonic computing hardware which can control these stochastic elements to program probabilistic machine learning algorithms has been limited.","Here, we implement a photonic probabilistic computer consisting of a controllable stochastic photonic element - a photonic probabilistic neuron (PPN).","Our PPN is implemented in a bistable optical parametric oscillator (OPO) with vacuum-level injected bias fields.","We then program a measurement-and-feedback loop for time-multiplexed PPNs with electronic processors (FPGA or GPU) to solve certain probabilistic machine learning tasks.","We showcase probabilistic inference and image generation of MNIST-handwritten digits, which are representative examples of discriminative and generative models.","In both implementations, quantum vacuum noise is used as a random seed to encode classification uncertainty or probabilistic generation of samples.","In addition, we propose a path towards an all-optical probabilistic computing platform, with an estimated sampling rate of ~ 1","Gbps and energy consumption of ~ 5 fJ/MAC.","Our work paves the way for scalable, ultrafast, and energy-efficient probabilistic machine learning hardware."],"url":"http://arxiv.org/abs/2403.04731v1","category":"physics.optics"}
{"created":"2024-03-07 18:33:50","title":"Fast, robust and laser-free universal entangling gates for trapped-ion quantum computing","abstract":"A novel two-qubit entangling gate for RF-controlled trapped-ion quantum processors is proposed theoretically and demonstrated experimentally. The speed of this gate is an order of magnitude higher than that of previously demonstrated two-qubit entangling gates in static magnetic field gradients. At the same time, the phase-modulated field driving the gate, dynamically decouples the qubits from amplitude and frequency noise, increasing the qubits' coherence time by two orders of magnitude. The gate requires only a single continuous RF field per qubit, making it well suited for scaling a quantum processor to large numbers of qubits. Implementing this entangling gate, we generate the Bell states $|\\Phi^+\\rangle$ and $|\\Psi^+\\rangle$ in $\\leq 313$ $\\mathrm{\\mathrm{\\mu}}$s with fidelities up to $98^{+2}_{-3}$ % in a static magnetic gradient of only 19.09 T/m. At higher magnetic field gradients, the entangling gate speed can be further improved to match that of laser-based counterparts.","sentences":["A novel two-qubit entangling gate for RF-controlled trapped-ion quantum processors is proposed theoretically and demonstrated experimentally.","The speed of this gate is an order of magnitude higher than that of previously demonstrated two-qubit entangling gates in static magnetic field gradients.","At the same time, the phase-modulated field driving the gate, dynamically decouples the qubits from amplitude and frequency noise, increasing the qubits' coherence time by two orders of magnitude.","The gate requires only a single continuous RF field per qubit, making it well suited for scaling a quantum processor to large numbers of qubits.","Implementing this entangling gate, we generate the Bell states $|\\Phi^+\\rangle$ and $|\\Psi^+\\rangle$ in $\\leq 313$ $\\mathrm{\\mathrm{\\mu}}$s with fidelities up to $98^{+2}_{-3}$ % in a static magnetic gradient of only 19.09 T/m. At higher magnetic field gradients, the entangling gate speed can be further improved to match that of laser-based counterparts."],"url":"http://arxiv.org/abs/2403.04730v1","category":"quant-ph"}
{"created":"2024-03-07 18:31:32","title":"Stretchable Pneumatic Sleeve for Adaptable, Low-Displacement Anchoring in Exosuits","abstract":"Despite recent advances in wearable technology, interfacing movement assistance devices with the human body remains challenging. We present a stretchable pneumatic sleeve that can anchor an exosuit actuator to the human arm with a low displacement of the actuator's mounting point relative to the body during operation. Our sleeve has the potential to serve as an adaptable attachment mechanism for exosuits, since it can adjust its pressure to only compress the arm as much as needed to transmit the applied exosuit forces without a large displacement. We discuss the design of our sleeve, which is made of fabric pneumatic artificial muscle (fPAM) actuators formed into bands. We quantify the performance of nine fPAM bands of various lengths and widths, as well as three sleeves (an fPAM sleeve, a series pouch motor (SPM) sleeve as in previous literature, and an off the shelf hook and loop sleeve), through the measurement of the compressing force as a function of pressure and the localized pulling force that can be resisted as a function of both pressure and mounting point displacement. Our experimental results show that fPAM bands with smaller resting length and/or larger resting width produce higher forces. Also, when inflated, an fPAM sleeve that has equivalent dimensions to the SPM sleeve while fully stretched has similar performance to the SPM sleeve. While inflated, both pneumatic sleeves decrease the mounting point displacement compared to the hook and loop sleeve. Compared to the SPM sleeve, the fPAM sleeve is able to hold larger internal pressure before bursting, increasing its possible force range. Also, when not inflated, the fPAM sleeve resists the pulling force well, indicating its ability to provide anchoring when not actuated.","sentences":["Despite recent advances in wearable technology, interfacing movement assistance devices with the human body remains challenging.","We present a stretchable pneumatic sleeve that can anchor an exosuit actuator to the human arm with a low displacement of the actuator's mounting point relative to the body during operation.","Our sleeve has the potential to serve as an adaptable attachment mechanism for exosuits, since it can adjust its pressure to only compress the arm as much as needed to transmit the applied exosuit forces without a large displacement.","We discuss the design of our sleeve, which is made of fabric pneumatic artificial muscle (fPAM) actuators formed into bands.","We quantify the performance of nine fPAM bands of various lengths and widths, as well as three sleeves (an fPAM sleeve, a series pouch motor (SPM) sleeve as in previous literature, and an off the shelf hook and loop sleeve), through the measurement of the compressing force as a function of pressure and the localized pulling force that can be resisted as a function of both pressure and mounting point displacement.","Our experimental results show that fPAM bands with smaller resting length and/or larger resting width produce higher forces.","Also, when inflated, an fPAM sleeve that has equivalent dimensions to the SPM sleeve while fully stretched has similar performance to the SPM sleeve.","While inflated, both pneumatic sleeves decrease the mounting point displacement compared to the hook and loop sleeve.","Compared to the SPM sleeve, the fPAM sleeve is able to hold larger internal pressure before bursting, increasing its possible force range.","Also, when not inflated, the fPAM sleeve resists the pulling force well, indicating its ability to provide anchoring when not actuated."],"url":"http://arxiv.org/abs/2403.04729v1","category":"cs.RO"}
{"created":"2024-03-07 18:31:26","title":"Tight general bounds for the extremal numbers of 0-1 matrices","abstract":"A zero-one matrix $M$ is said to contain another zero-one matrix $A$ if we can delete some rows and columns of $M$ and replace some $1$-entries with $0$-entries such that the resulting matrix is $A$. The extremal number of $A$, denoted $\\operatorname{ex}(n,A)$, is the maximum number of $1$-entries that an $n\\times n$ zero-one matrix can have without containing $A$. The systematic study of this function for various patterns $A$ goes back to the work of F\\\"uredi and Hajnal from 1992, and the field has many connections to other areas of mathematics and theoretical computer science. The problem has been particularly extensively studied for so-called acyclic matrices, but very little is known about the general case (that is, the case where $A$ is not necessarily acyclic). We prove the first asymptotically tight general result by showing that if $A$ has at most $t$ $1$-entries in every row, then $\\operatorname{ex}(n,A)\\leq n^{2-1/t+o(1)}$. This verifies a conjecture of Methuku and Tomon.   Our result also provides the first tight general bound for the extremal number of vertex-ordered graphs with interval chromatic number $2$, generalizing a celebrated result of F\\\"uredi, and Alon, Krivelevich and Sudakov about the (unordered) extremal number of bipartite graphs with maximum degree $t$ in one of the vertex classes.","sentences":["A zero-one matrix $M$ is said to contain another zero-one matrix $A$ if we can delete some rows and columns of $M$ and replace some $1$-entries with $0$-entries such that the resulting matrix is $A$.","The extremal number of $A$, denoted $\\operatorname{ex}(n,A)$, is the maximum number of $1$-entries that an $n\\times n$ zero-one matrix can have without containing $A$.","The systematic study of this function for various patterns $A$ goes back to the work of F\\\"uredi and Hajnal from 1992, and the field has many connections to other areas of mathematics and theoretical computer science.","The problem has been particularly extensively studied for so-called acyclic matrices, but very little is known about the general case (that is, the case where $A$ is not necessarily acyclic).","We prove the first asymptotically tight general result by showing that if $A$ has at most $t$ $1$-entries in every row, then $\\operatorname{ex}(n,A)\\leq n^{2-1/t+o(1)}$. This verifies a conjecture of Methuku and Tomon.   ","Our result also provides the first tight general bound for the extremal number of vertex-ordered graphs with interval chromatic number $2$, generalizing a celebrated result of F\\\"uredi, and Alon, Krivelevich and Sudakov about the (unordered) extremal number of bipartite graphs with maximum degree $t$ in one of the vertex classes."],"url":"http://arxiv.org/abs/2403.04728v1","category":"math.CO"}
{"created":"2024-03-07 18:30:02","title":"On the evaluations of multiple $S$ and $T$ values of the form $S(\\overset{{}_{(-)}}{2}, 1, \\ldots, 1, \\overset{{}_{(-)}}{1})$ and $T(\\overset{{}_{(-)}}{2}, 1, \\ldots, 1, \\overset{{}_{(-)}}{1})$","abstract":"Xu, Yan and Zhao showed that in even weight, the multiple $T$ value $T(2, 1, \\ldots, 1, \\overline{1})$ is a polynomial in $\\log(2)$, $\\pi$, Riemann zeta values, and Dirichlet beta values. Based on low-weight examples, they conjectured that $\\log(2)$ does not appear in the evaluation. We show that their conjecture is correct, and in fact follows largely from various earlier results of theirs. More precisely, we derive explicit formulae for $T(2, 1, \\ldots, 1, \\overline{1})$ in even weight and $S(2, 1, \\ldots, 1, \\overline{1})$ in odd weight via generating series calculations. We also resolve another conjecture of theirs on the evaluations of $T(\\overline{2}, 1, \\ldots, 1, \\overline{1})$, $S(\\overline{2}, 1, \\ldots, 1, 1)$, and $S(\\overline{2}, 1, \\ldots, 1, \\overline{1})$ in even weight, by way of calculations involving Goncharov's theory of iterated integrals and multiple polylogarithms.","sentences":["Xu, Yan and Zhao showed that in even weight, the multiple $T$ value $T(2, 1, \\ldots, 1, \\overline{1})$ is a polynomial in $\\log(2)$, $\\pi$, Riemann zeta values, and Dirichlet beta values.","Based on low-weight examples, they conjectured that $\\log(2)$ does not appear in the evaluation.","We show that their conjecture is correct, and in fact follows largely from various earlier results of theirs.","More precisely, we derive explicit formulae for $T(2, 1, \\ldots, 1, \\overline{1})$ in even weight and $S(2, 1, \\ldots, 1, \\overline{1})$ in odd weight via generating series calculations.","We also resolve another conjecture of theirs on the evaluations of $T(\\overline{2}, 1, \\ldots, 1, \\overline{1})$, $S(\\overline{2}, 1, \\ldots, 1, 1)$, and $S(\\overline{2}, 1, \\ldots, 1, \\overline{1})$ in even weight, by way of calculations involving Goncharov's theory of iterated integrals and multiple polylogarithms."],"url":"http://arxiv.org/abs/2403.04727v1","category":"math.NT"}
{"created":"2024-03-07 18:23:51","title":"A Sub-Quadratic Time Algorithm for Robust Sparse Mean Estimation","abstract":"We study the algorithmic problem of sparse mean estimation in the presence of adversarial outliers. Specifically, the algorithm observes a \\emph{corrupted} set of samples from $\\mathcal{N}(\\mu,\\mathbf{I}_d)$, where the unknown mean $\\mu \\in \\mathbb{R}^d$ is constrained to be $k$-sparse. A series of prior works has developed efficient algorithms for robust sparse mean estimation with sample complexity $\\mathrm{poly}(k,\\log d, 1/\\epsilon)$ and runtime $d^2 \\mathrm{poly}(k,\\log d,1/\\epsilon)$, where $\\epsilon$ is the fraction of contamination. In particular, the fastest runtime of existing algorithms is quadratic ($\\Omega(d^2)$), which can be prohibitive in high dimensions. This quadratic barrier in the runtime stems from the reliance of these algorithms on the sample covariance matrix, which is of size $d^2$. Our main contribution is an algorithm for robust sparse mean estimation which runs in \\emph{subquadratic} time using $\\mathrm{poly}(k,\\log d,1/\\epsilon)$ samples. We also provide analogous results for robust sparse PCA. Our results build on algorithmic advances in detecting weak correlations, a generalized version of the light-bulb problem by Valiant.","sentences":["We study the algorithmic problem of sparse mean estimation in the presence of adversarial outliers.","Specifically, the algorithm observes a \\emph{corrupted} set of samples from $\\mathcal{N}(\\mu,\\mathbf{I}_d)$, where the unknown mean $\\mu \\in \\mathbb{R}^d$ is constrained to be $k$-sparse.","A series of prior works has developed efficient algorithms for robust sparse mean estimation with sample complexity $\\mathrm{poly}(k,\\log d, 1/\\epsilon)$ and runtime $d^2 \\mathrm{poly}(k,\\log d,1/\\epsilon)$, where $\\epsilon$ is the fraction of contamination.","In particular, the fastest runtime of existing algorithms is quadratic ($\\Omega(d^2)$), which can be prohibitive in high dimensions.","This quadratic barrier in the runtime stems from the reliance of these algorithms on the sample covariance matrix, which is of size $d^2$. Our main contribution is an algorithm for robust sparse mean estimation which runs in \\emph{subquadratic} time using $\\mathrm{poly}(k,\\log d,1/\\epsilon)$ samples.","We also provide analogous results for robust sparse PCA.","Our results build on algorithmic advances in detecting weak correlations, a generalized version of the light-bulb problem by Valiant."],"url":"http://arxiv.org/abs/2403.04726v1","category":"cs.DS"}
{"created":"2024-03-07 18:18:34","title":"Quantum-enhanced joint estimation of phase and phase diffusion","abstract":"Accurate phase estimation in the presence of unknown phase diffusive noise is a crucial yet challenging task in noisy quantum metrology. This problem is particularly interesting due to the detrimental impact of the associated noise. Here, we investigate the joint estimation of phase and phase diffusion using generalized Holland-Burnett states, known for their experimental accessibility. These states provide performance close to the optimal state in single-parameter phase estimation, even in the presence of photon losses. We adopt a twofold approach by analyzing the joint information extraction through the double homodyne measurement and the joint information availability across all probe states. Through our analysis, we find that the highest sensitivities are obtained by using states created by directing all input photons into one port of a balanced beam splitter. Furthermore, we infer that good levels of sensitivity persist even in the presence of moderate photon losses, illustrating the remarkable resilience of our probe states under lossy conditions.","sentences":["Accurate phase estimation in the presence of unknown phase diffusive noise is a crucial yet challenging task in noisy quantum metrology.","This problem is particularly interesting due to the detrimental impact of the associated noise.","Here, we investigate the joint estimation of phase and phase diffusion using generalized Holland-Burnett states, known for their experimental accessibility.","These states provide performance close to the optimal state in single-parameter phase estimation, even in the presence of photon losses.","We adopt a twofold approach by analyzing the joint information extraction through the double homodyne measurement and the joint information availability across all probe states.","Through our analysis, we find that the highest sensitivities are obtained by using states created by directing all input photons into one port of a balanced beam splitter.","Furthermore, we infer that good levels of sensitivity persist even in the presence of moderate photon losses, illustrating the remarkable resilience of our probe states under lossy conditions."],"url":"http://arxiv.org/abs/2403.04722v1","category":"quant-ph"}
{"created":"2024-03-07 18:18:10","title":"A sharp H\u00f6rmander condition for bilinear Fourier multipliers with Lipschitz singularities","abstract":"This paper studies the $L^{p}$ boundedness of bilinear Fourier multipliers in the local $L^{2}$ range. We assume a H\\\"{o}rmander condition relative to a singular set that is a finite union of Lipschitz curves. The H\\\"{o}rmander condition is sharp with respect to the Sobolev exponent. Our setup generalizes the non-degenerate bilinear Hilbert transform but avoids issues of uniform bounds near degeneracy.","sentences":["This paper studies the $L^{p}$ boundedness of bilinear Fourier multipliers in the local $L^{2}$ range.","We assume a H\\\"{o}rmander condition relative to a singular set that is a finite union of Lipschitz curves.","The H\\\"{o}rmander condition is sharp with respect to the Sobolev exponent.","Our setup generalizes the non-degenerate bilinear Hilbert transform but avoids issues of uniform bounds near degeneracy."],"url":"http://arxiv.org/abs/2403.04721v1","category":"math.CA"}
{"created":"2024-03-07 18:16:29","title":"Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization","abstract":"Effectively representing heterogeneous tabular datasets for meta-learning remains an open problem. Previous approaches rely on predefined meta-features, for example, statistical measures or landmarkers. Encoder-based models, such as Dataset2Vec, allow us to extract significant meta-features automatically without human intervention. This research introduces a novel encoder-based representation of tabular datasets implemented within the liltab package available on GitHub https://github.com/azoz01/liltab. Our package is based on an established model for heterogeneous tabular data proposed in [Iwata and Kumagai, 2020]. The proposed approach employs a different model for encoding feature relationships, generating alternative representations compared to existing methods like Dataset2Vec. Both of them leverage the fundamental assumption of dataset similarity learning. In this work, we evaluate Dataset2Vec and liltab on two common meta-tasks - representing entire datasets and hyperparameter optimization warm-start. However, validation on an independent metaMIMIC dataset highlights the nuanced challenges in representation learning. We show that general representations may not suffice for some meta-tasks where requirements are not explicitly considered during extraction.   [Iwata and Kumagai, 2020] Tomoharu Iwata and Atsutoshi Kumagai. Meta-learning from Tasks with Heterogeneous Attribute Spaces. In Advances in Neural Information Processing Systems, 2020.","sentences":["Effectively representing heterogeneous tabular datasets for meta-learning remains an open problem.","Previous approaches rely on predefined meta-features, for example, statistical measures or landmarkers.","Encoder-based models, such as Dataset2Vec, allow us to extract significant meta-features automatically without human intervention.","This research introduces a novel encoder-based representation of tabular datasets implemented within the liltab package available on GitHub https://github.com/azoz01/liltab.","Our package is based on an established model for heterogeneous tabular data proposed in [Iwata and Kumagai, 2020].","The proposed approach employs a different model for encoding feature relationships, generating alternative representations compared to existing methods like Dataset2Vec.","Both of them leverage the fundamental assumption of dataset similarity learning.","In this work, we evaluate Dataset2Vec and liltab on two common meta-tasks - representing entire datasets and hyperparameter optimization warm-start.","However, validation on an independent metaMIMIC dataset highlights the nuanced challenges in representation learning.","We show that general representations may not suffice for some meta-tasks where requirements are not explicitly considered during extraction.   ","[Iwata and Kumagai, 2020]","Tomoharu Iwata and Atsutoshi Kumagai.","Meta-learning from Tasks with Heterogeneous Attribute Spaces.","In Advances in Neural Information Processing Systems, 2020."],"url":"http://arxiv.org/abs/2403.04720v1","category":"cs.LG"}
{"created":"2024-03-07 18:13:39","title":"Molecular Gas Tracers in Young and Old Protoplanetary Disks","abstract":"Molecular emission is used to investigate both the physical and chemical properties of protoplanetary disks. Therefore, to accurately derive disk properties, we need a thorough understanding of the behavior of the molecular probes we rely on. Here we investigate how the molecular line emission of N$_2$H$^+$, HCO$^+$, HCN, and C$^{18}$O compare to other measured quantities in a set of 20 protoplanetary disks. Overall, we find positive correlations between multiple line fluxes and the disk dust mass and radius. We also generally find strong positive correlations between the line fluxes of different molecular species. However, some disks do show noticeable differences in the relative fluxes of N$_2$H$^+$, HCO$^+$, HCN, and C$^{18}$O. These differences occur even within a single star-forming region. This results in a potentially large range of different disk masses and chemical compositions for systems of similar age and birth environment. While we make preliminary comparisons of molecular fluxes across different star-forming regions, more complete and uniform samples are needed in the future to search for trends with birth environment or age.","sentences":["Molecular emission is used to investigate both the physical and chemical properties of protoplanetary disks.","Therefore, to accurately derive disk properties, we need a thorough understanding of the behavior of the molecular probes we rely on.","Here we investigate how the molecular line emission of N$_2$H$^+$, HCO$^+$, HCN, and C$^{18}$O compare to other measured quantities in a set of 20 protoplanetary disks.","Overall, we find positive correlations between multiple line fluxes and the disk dust mass and radius.","We also generally find strong positive correlations between the line fluxes of different molecular species.","However, some disks do show noticeable differences in the relative fluxes of N$_2$H$^+$, HCO$^+$, HCN, and C$^{18}$O. These differences occur even within a single star-forming region.","This results in a potentially large range of different disk masses and chemical compositions for systems of similar age and birth environment.","While we make preliminary comparisons of molecular fluxes across different star-forming regions, more complete and uniform samples are needed in the future to search for trends with birth environment or age."],"url":"http://arxiv.org/abs/2403.04715v1","category":"astro-ph.EP"}
{"created":"2024-03-07 18:07:52","title":"Seedless Extractors for Device-Independent Quantum Cryptography","abstract":"Device-independent (DI) quantum cryptography aims at providing secure cryptography with minimal trust in, or characterisation of, the underlying quantum devices. An essential step in DI protocols is randomness extraction (or privacy amplification) which requires the honest parties to have a seed of additional bits with sufficient entropy and statistical independence of any bits generated during the protocol. In this work we introduce a method for extraction in DI protocols which does not require a seed and is secure against computationally unbounded quantum adversary. The key idea is to use the Bell violation of the raw data, instead of its min-entropy, as the extractor promise.","sentences":["Device-independent (DI) quantum cryptography aims at providing secure cryptography with minimal trust in, or characterisation of, the underlying quantum devices.","An essential step in DI protocols is randomness extraction (or privacy amplification) which requires the honest parties to have a seed of additional bits with sufficient entropy and statistical independence of any bits generated during the protocol.","In this work we introduce a method for extraction in DI protocols which does not require a seed and is secure against computationally unbounded quantum adversary.","The key idea is to use the Bell violation of the raw data, instead of its min-entropy, as the extractor promise."],"url":"http://arxiv.org/abs/2403.04713v1","category":"quant-ph"}
{"created":"2024-03-07 18:07:41","title":"GMKF: Generalized Moment Kalman Filter for Polynomial Systems with Arbitrary Noise","abstract":"This paper develops a new filtering approach for state estimation in polynomial systems corrupted by arbitrary noise, which commonly arise in robotics. We first consider a batch setup where we perform state estimation using all data collected from the initial to the current time. We formulate the batch state estimation problem as a Polynomial Optimization Problem (POP) and relax the assumption of Gaussian noise by specifying a finite number of moments of the noise. We solve the resulting POP using a moment relaxation and prove that under suitable conditions on the rank of the relaxation, (i) we can extract a provably optimal estimate from the moment relaxation, and (ii) we can obtain a belief representation from the dual (sum-of-squares) relaxation. We then turn our attention to the filtering setup and apply similar insights to develop a GMKF for recursive state estimation in polynomial systems with arbitrary noise. The GMKF formulates the prediction and update steps as POPs and solves them using moment relaxations, carrying over a possibly non-Gaussian belief. In the linear-Gaussian case, GMKF reduces to the standard Kalman Filter. We demonstrate that GMKF performs well under highly non-Gaussian noise and outperforms common alternatives, including the Extended and Unscented Kalman Filter, and their variants on matrix Lie group.","sentences":["This paper develops a new filtering approach for state estimation in polynomial systems corrupted by arbitrary noise, which commonly arise in robotics.","We first consider a batch setup where we perform state estimation using all data collected from the initial to the current time.","We formulate the batch state estimation problem as a Polynomial Optimization Problem (POP) and relax the assumption of Gaussian noise by specifying a finite number of moments of the noise.","We solve the resulting POP using a moment relaxation and prove that under suitable conditions on the rank of the relaxation, (i) we can extract a provably optimal estimate from the moment relaxation, and (ii) we can obtain a belief representation from the dual (sum-of-squares) relaxation.","We then turn our attention to the filtering setup and apply similar insights to develop a GMKF for recursive state estimation in polynomial systems with arbitrary noise.","The GMKF formulates the prediction and update steps as POPs and solves them using moment relaxations, carrying over a possibly non-Gaussian belief.","In the linear-Gaussian case, GMKF reduces to the standard Kalman Filter.","We demonstrate that GMKF performs well under highly non-Gaussian noise and outperforms common alternatives, including the Extended and Unscented Kalman Filter, and their variants on matrix Lie group."],"url":"http://arxiv.org/abs/2403.04712v1","category":"cs.RO"}
{"created":"2024-03-07 18:07:28","title":"Stability of low-momentum rest frame modes in first- and second-order spin hydrodynamics","abstract":"The low-momentum rest frame stability of the second-order spin hydrodynamics is analyzed in detail. A truncation scheme of the Israel-Stewart formalism derived in our earlier work is proposed that extends the minimal causal formulation. It consists of altogether 40 interconnected relaxation-type dynamical equations -- 16 (24) of them correspond to the independent components of the energy-momentum (spin) tensor. Similarly to previous studies, we find that the stability of the perturbations depends on the form of the spin equation of state. The stability is achieved if the equation of state satisfies the generalized Frenkel condition demanding that the \"electric\" and \"magnetic\" components of the spin density tensor have opposite signs. We then re-examine the study of the first-order (Navier-Stokes) dissipative spin hydrodynamics and find a similar type of behavior, which is consistent with the overall stability of low-momentum rest frame modes in conventional relativistic hydrodynamics.","sentences":["The low-momentum rest frame stability of the second-order spin hydrodynamics is analyzed in detail.","A truncation scheme of the Israel-Stewart formalism derived in our earlier work is proposed that extends the minimal causal formulation.","It consists of altogether 40 interconnected relaxation-type dynamical equations -- 16 (24) of them correspond to the independent components of the energy-momentum (spin) tensor.","Similarly to previous studies, we find that the stability of the perturbations depends on the form of the spin equation of state.","The stability is achieved if the equation of state satisfies the generalized Frenkel condition demanding that the \"electric\" and \"magnetic\" components of the spin density tensor have opposite signs.","We then re-examine the study of the first-order (Navier-Stokes) dissipative spin hydrodynamics and find a similar type of behavior, which is consistent with the overall stability of low-momentum rest frame modes in conventional relativistic hydrodynamics."],"url":"http://arxiv.org/abs/2403.04711v1","category":"hep-ph"}
{"created":"2024-03-07 18:01:11","title":"The Variable Radius Form of the Extended Exterior Sphere Condition","abstract":"We introduce a variable radius form of the extended exterior sphere condition of [16], and then, we prove that the complement of a closed set satisfying this new property is nothing but the union of closed balls with lower semicontinous radius function. This generalizes, to the variable radius case, the main result of [16], namely, [16, Theorem 1.2]. On the other hand, as it is shown in [14,15] for prox-regularity, the exterior sphere condition, and the union of closed balls property, we prove that the constant and the variable radius forms of the extended exterior sphere condition belong to the S-convexity regularity class.","sentences":["We introduce a variable radius form of the extended exterior sphere condition of [16], and then, we prove that the complement of a closed set satisfying this new property is nothing but the union of closed balls with lower semicontinous radius function.","This generalizes, to the variable radius case, the main result of [16], namely, [16, Theorem 1.2].","On the other hand, as it is shown in [14,15] for prox-regularity, the exterior sphere condition, and the union of closed balls property, we prove that the constant and the variable radius forms of the extended exterior sphere condition belong to the S-convexity regularity class."],"url":"http://arxiv.org/abs/2403.04707v1","category":"math.MG"}
{"created":"2024-03-07 18:00:40","title":"Common 7B Language Models Already Possess Strong Math Capabilities","abstract":"Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks, respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitation, we employ synthetic data, which proves to be nearly as effective as real data and shows no clear saturation when scaled up to approximately one million samples. This straightforward approach achieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B models, surpassing previous models by 14.2% and 20.8%, respectively. We also provide insights into scaling behaviors across different reasoning complexities and error types.","sentences":["Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training.","This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations.","The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities.","Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks, respectively.","We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers.","However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions.","To overcome this limitation, we employ synthetic data, which proves to be nearly as effective as real data and shows no clear saturation when scaled up to approximately one million samples.","This straightforward approach achieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B models, surpassing previous models by 14.2% and 20.8%, respectively.","We also provide insights into scaling behaviors across different reasoning complexities and error types."],"url":"http://arxiv.org/abs/2403.04706v1","category":"cs.CL"}
{"created":"2024-03-07 17:51:45","title":"Convergence of a Ramshaw-Mesina Iteration","abstract":"In 1991 Ramshaw and Mesina introduced a clever synthesis of penalty methods and artificial compression methods. Its form makes it an interesting option to replace the pressure update in the Uzawa iteration. The result, for the Stokes problem, is \\begin{equation} \\left\\{ \\begin{array} [c]{cc} Step\\ 1: & -\\triangle u^{n+1}+\\nabla p^{n}=f(x),\\ {\\rm in}\\ \\Omega,\\ u^{n+1}|_{\\partial\\Omega}=0,\\\\ Step\\ 2: & p^{n+1}-p^{n}+\\beta\\nabla\\cdot(u^{n+1}-u^{n})+\\alpha ^{2}\\nabla\\cdot u^{n+1}=0. \\end{array} \\right. \\end{equation} For saddle point problems, including Stokes, this iteration converges under a condition similar to the one required for Uzawa iteration.","sentences":["In 1991 Ramshaw and Mesina introduced a clever synthesis of penalty methods and artificial compression methods.","Its form makes it an interesting option to replace the pressure update in the Uzawa iteration.","The result, for the Stokes problem, is \\begin{equation} \\left\\{ \\begin{array}","[c]{cc} Step\\ 1: & -\\triangle u^{n+1}+\\nabla p^{n}=f(x),\\ {\\rm in}\\ \\Omega,\\ u^{n+1}|_{\\partial\\Omega}=0,\\\\ Step\\ 2: & p^{n+1}-p^{n}+\\beta\\nabla\\cdot(u^{n+1}-u^{n})+\\alpha ^{2}\\nabla\\cdot u^{n+1}=0.","\\end{array} \\right.","\\end{equation} For saddle point problems, including Stokes, this iteration converges under a condition similar to the one required for Uzawa iteration."],"url":"http://arxiv.org/abs/2403.04702v1","category":"math.NA"}
{"created":"2024-03-07 17:48:48","title":"ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes","abstract":"Given the large-scale multi-modal training of recent vision-based models and their generalization capabilities, understanding the extent of their robustness is critical for their real-world deployment. In this work, we evaluate the resilience of current vision-based models against diverse object-to-background context variations. The majority of robustness evaluation methods have introduced synthetic datasets to induce changes to object characteristics (viewpoints, scale, color) or utilized image transformation techniques (adversarial changes, common corruptions) on real images to simulate shifts in distributions. Recent works have explored leveraging large language models and diffusion models to generate changes in the background. However, these methods either lack in offering control over the changes to be made or distort the object semantics, making them unsuitable for the task. Our method, on the other hand, can induce diverse object-to-background changes while preserving the original semantics and appearance of the object. To achieve this goal, we harness the generative capabilities of text-to-image, image-to-text, and image-to-segment models to automatically generate a broad spectrum of object-to-background changes. We induce both natural and adversarial background changes by either modifying the textual prompts or optimizing the latents and textual embedding of text-to-image models. This allows us to quantify the role of background context in understanding the robustness and generalization of deep neural networks. We produce various versions of standard vision datasets (ImageNet, COCO), incorporating either diverse and realistic backgrounds into the images or introducing color, texture, and adversarial changes in the background. We conduct extensive experiment to analyze the robustness of vision-based models against object-to-background context variations across diverse tasks.","sentences":["Given the large-scale multi-modal training of recent vision-based models and their generalization capabilities, understanding the extent of their robustness is critical for their real-world deployment.","In this work, we evaluate the resilience of current vision-based models against diverse object-to-background context variations.","The majority of robustness evaluation methods have introduced synthetic datasets to induce changes to object characteristics (viewpoints, scale, color) or utilized image transformation techniques (adversarial changes, common corruptions) on real images to simulate shifts in distributions.","Recent works have explored leveraging large language models and diffusion models to generate changes in the background.","However, these methods either lack in offering control over the changes to be made or distort the object semantics, making them unsuitable for the task.","Our method, on the other hand, can induce diverse object-to-background changes while preserving the original semantics and appearance of the object.","To achieve this goal, we harness the generative capabilities of text-to-image, image-to-text, and image-to-segment models to automatically generate a broad spectrum of object-to-background changes.","We induce both natural and adversarial background changes by either modifying the textual prompts or optimizing the latents and textual embedding of text-to-image models.","This allows us to quantify the role of background context in understanding the robustness and generalization of deep neural networks.","We produce various versions of standard vision datasets (ImageNet, COCO), incorporating either diverse and realistic backgrounds into the images or introducing color, texture, and adversarial changes in the background.","We conduct extensive experiment to analyze the robustness of vision-based models against object-to-background context variations across diverse tasks."],"url":"http://arxiv.org/abs/2403.04701v1","category":"cs.CV"}
{"created":"2024-03-07 17:47:53","title":"Discrete hypocoercivity for a nonlinear kinetic reaction model","abstract":"In this article, we propose a finite volume discretization of a one dimensional nonlinear reaction kinetic model proposed in [Neumann, Schmeiser, Kint. Rel. Mod. 2016], which describes a 2-species recombination-generation process. Specifically, we establish the long-time convergence of approximate solutions towards equilibrium, at exponential rate. The study is based on an adaptation for a discretization of the linearized problem of the $L^2$ hypocoercivity method introduced in [Dolbeault, Mouhot, Schmeiser, 2015]. From this, we can deduce a local result for the discrete nonlinear problem. As in the continuous framework, this result requires the establishment of a maximum principle, which necessitates the use of monotone numerical fluxes.","sentences":["In this article, we propose a finite volume discretization of a one dimensional nonlinear reaction kinetic model proposed in [Neumann, Schmeiser, Kint.","Rel.","Mod. 2016], which describes a 2-species recombination-generation process.","Specifically, we establish the long-time convergence of approximate solutions towards equilibrium, at exponential rate.","The study is based on an adaptation for a discretization of the linearized problem of the $L^2$ hypocoercivity method introduced in [Dolbeault, Mouhot, Schmeiser, 2015].","From this, we can deduce a local result for the discrete nonlinear problem.","As in the continuous framework, this result requires the establishment of a maximum principle, which necessitates the use of monotone numerical fluxes."],"url":"http://arxiv.org/abs/2403.04699v1","category":"math.NA"}
{"created":"2024-03-07 17:46:50","title":"AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors","abstract":"Facial Action Units (AU) is a vital concept in the realm of affective computing, and AU detection has always been a hot research topic. Existing methods suffer from overfitting issues due to the utilization of a large number of learnable parameters on scarce AU-annotated datasets or heavy reliance on substantial additional relevant data. Parameter-Efficient Transfer Learning (PETL) provides a promising paradigm to address these challenges, whereas its existing methods lack design for AU characteristics. Therefore, we innovatively investigate PETL paradigm to AU detection, introducing AUFormer and proposing a novel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individual MoKE specific to a certain AU with minimal learnable parameters first integrates personalized multi-scale and correlation knowledge. Then the MoKE collaborates with other MoKEs in the expert group to obtain aggregated information and inject it into the frozen Vision Transformer (ViT) to achieve parameter-efficient AU detection. Additionally, we design a Margin-truncated Difficulty-aware Weighted Asymmetric Loss (MDWA-Loss), which can encourage the model to focus more on activated AUs, differentiate the difficulty of unactivated AUs, and discard potential mislabeled samples. Extensive experiments from various perspectives, including within-domain, cross-domain, data efficiency, and micro-expression domain, demonstrate AUFormer's state-of-the-art performance and robust generalization abilities without relying on additional relevant data. The code for AUFormer is available at https://github.com/yuankaishen2001/AUFormer.","sentences":["Facial Action Units (AU) is a vital concept in the realm of affective computing, and AU detection has always been a hot research topic.","Existing methods suffer from overfitting issues due to the utilization of a large number of learnable parameters on scarce AU-annotated datasets or heavy reliance on substantial additional relevant data.","Parameter-Efficient Transfer Learning (PETL) provides a promising paradigm to address these challenges, whereas its existing methods lack design for AU characteristics.","Therefore, we innovatively investigate PETL paradigm to AU detection, introducing AUFormer and proposing a novel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism.","An individual MoKE specific to a certain AU with minimal learnable parameters first integrates personalized multi-scale and correlation knowledge.","Then the MoKE collaborates with other MoKEs","in the expert group to obtain aggregated information and inject it into the frozen Vision Transformer (ViT) to achieve parameter-efficient AU detection.","Additionally, we design a Margin-truncated Difficulty-aware Weighted Asymmetric Loss (MDWA-Loss), which can encourage the model to focus more on activated AUs, differentiate the difficulty of unactivated AUs, and discard potential mislabeled samples.","Extensive experiments from various perspectives, including within-domain, cross-domain, data efficiency, and micro-expression domain, demonstrate AUFormer's state-of-the-art performance and robust generalization abilities without relying on additional relevant data.","The code for AUFormer is available at https://github.com/yuankaishen2001/AUFormer."],"url":"http://arxiv.org/abs/2403.04697v1","category":"cs.CV"}
{"created":"2024-03-07 17:44:17","title":"Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification","abstract":"Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for six different LLMs and three languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge.","sentences":["Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output.","Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them.","Current services that leverage LLMs usually do not provide any means for detecting unreliable generations.","Here, we aim to bridge this gap.","In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification.","Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output.","Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use.","Our method Claim Conditioned Probability (CCP) measures only the uncertainty of particular claim value expressed by the model.","Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for six different LLMs and three languages.","Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge."],"url":"http://arxiv.org/abs/2403.04696v1","category":"cs.CL"}
{"created":"2024-03-07 17:44:07","title":"On phase transitions during collisions near the horizon of black holes","abstract":"During particle collisions in the vicinity of the horizon of black holes, it is possible to achieve energies and temperatures corresponding to phase transitions in particle physics. It is shown that the sizes of the regions of the new phase are of the order of the Compton length for the corresponding mass scale. The lifetime is also on the order of the Compton time. It is shown that the inverse influence of the energy density in the electro-weak phase transition in collisions on the space-time metric can be neglected.","sentences":["During particle collisions in the vicinity of the horizon of black holes, it is possible to achieve energies and temperatures corresponding to phase transitions in particle physics.","It is shown that the sizes of the regions of the new phase are of the order of the Compton length for the corresponding mass scale.","The lifetime is also on the order of the Compton time.","It is shown that the inverse influence of the energy density in the electro-weak phase transition in collisions on the space-time metric can be neglected."],"url":"http://arxiv.org/abs/2403.04695v1","category":"gr-qc"}
{"created":"2024-03-07 17:41:37","title":"PixArt-\u03a3: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation","abstract":"In this paper, we introduce PixArt-\\Sigma, a Diffusion Transformer model~(DiT) capable of directly generating images at 4K resolution. PixArt-\\Sigma represents a significant advancement over its predecessor, PixArt-\\alpha, offering images of markedly higher fidelity and improved alignment with text prompts. A key feature of PixArt-\\Sigma is its training efficiency. Leveraging the foundational pre-training of PixArt-\\alpha, it evolves from the `weaker' baseline to a `stronger' model via incorporating higher quality data, a process we term \"weak-to-strong training\". The advancements in PixArt-\\Sigma are twofold: (1) High-Quality Training Data: PixArt-\\Sigma incorporates superior-quality image data, paired with more precise and detailed image captions. (2) Efficient Token Compression: we propose a novel attention module within the DiT framework that compresses both keys and values, significantly improving efficiency and facilitating ultra-high-resolution image generation. Thanks to these improvements, PixArt-\\Sigma achieves superior image quality and user prompt adherence capabilities with significantly smaller model size (0.6B parameters) than existing text-to-image diffusion models, such as SDXL (2.6B parameters) and SD Cascade (5.1B parameters). Moreover, PixArt-\\Sigma's capability to generate 4K images supports the creation of high-resolution posters and wallpapers, efficiently bolstering the production of high-quality visual content in industries such as film and gaming.","sentences":["In this paper, we introduce PixArt-\\Sigma, a Diffusion Transformer model~(DiT) capable of directly generating images at 4K resolution.","PixArt-\\Sigma represents a significant advancement over its predecessor, PixArt-\\alpha, offering images of markedly higher fidelity and improved alignment with text prompts.","A key feature of PixArt-\\Sigma is its training efficiency.","Leveraging the foundational pre-training of PixArt-\\alpha, it evolves from the `weaker' baseline to a `stronger' model via incorporating higher quality data, a process we term \"weak-to-strong training\".","The advancements in PixArt-\\Sigma are twofold: (1) High-Quality Training Data: PixArt-\\Sigma incorporates superior-quality image data, paired with more precise and detailed image captions.","(2) Efficient Token Compression: we propose a novel attention module within the DiT framework that compresses both keys and values, significantly improving efficiency and facilitating ultra-high-resolution image generation.","Thanks to these improvements, PixArt-\\Sigma achieves superior image quality and user prompt adherence capabilities with significantly smaller model size (0.6B parameters) than existing text-to-image diffusion models, such as SDXL (2.6B parameters) and SD Cascade (5.1B parameters).","Moreover, PixArt-\\Sigma's capability to generate 4K images supports the creation of high-resolution posters and wallpapers, efficiently bolstering the production of high-quality visual content in industries such as film and gaming."],"url":"http://arxiv.org/abs/2403.04692v1","category":"cs.CV"}
{"created":"2024-03-07 17:35:58","title":"Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level","abstract":"Neighborhood attention reduces the cost of self attention by restricting each token's attention span to its nearest neighbors. This restriction, parameterized by a window size and dilation factor, draws a spectrum of possible attention patterns between linear projection and self attention. Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both. In this work, we first show that neighborhood attention can be represented as a batched GEMM problem, similar to standard attention, and implement it for 1-D and 2-D neighborhood attention. These kernels on average provide 895% and 272% improvement in full precision latency compared to existing naive kernels for 1-D and 2-D neighborhood attention respectively. We find certain inherent inefficiencies in all unfused neighborhood attention kernels that bound their performance and lower-precision scalability. We also developed fused neighborhood attention; an adaptation of fused dot-product attention kernels that allow fine-grained control over attention across different spatial axes. Known for reducing the quadratic time complexity of self attention to a linear complexity, neighborhood attention can now enjoy a reduced and constant memory footprint, and record-breaking half precision latency. We observe that our fused kernels successfully circumvent some of the unavoidable inefficiencies in unfused implementations. While our unfused GEMM-based kernels only improve half precision performance compared to naive kernels by an average of 496% and 113% in 1-D and 2-D problems respectively, our fused kernels improve naive kernels by an average of 1607% and 581% in 1-D and 2-D problems respectively.","sentences":["Neighborhood attention reduces the cost of self attention by restricting each token's attention span to its nearest neighbors.","This restriction, parameterized by a window size and dilation factor, draws a spectrum of possible attention patterns between linear projection and self attention.","Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both.","In this work, we first show that neighborhood attention can be represented as a batched GEMM problem, similar to standard attention, and implement it for 1-D and 2-D neighborhood attention.","These kernels on average provide 895% and 272% improvement in full precision latency compared to existing naive kernels for 1-D and 2-D neighborhood attention respectively.","We find certain inherent inefficiencies in all unfused neighborhood attention kernels that bound their performance and lower-precision scalability.","We also developed fused neighborhood attention; an adaptation of fused dot-product attention kernels that allow fine-grained control over attention across different spatial axes.","Known for reducing the quadratic time complexity of self attention to a linear complexity, neighborhood attention can now enjoy a reduced and constant memory footprint, and record-breaking half precision latency.","We observe that our fused kernels successfully circumvent some of the unavoidable inefficiencies in unfused implementations.","While our unfused GEMM-based kernels only improve half precision performance compared to naive kernels by an average of 496% and 113% in 1-D and 2-D problems respectively, our fused kernels improve naive kernels by an average of 1607% and 581% in 1-D and 2-D problems respectively."],"url":"http://arxiv.org/abs/2403.04690v1","category":"cs.CV"}
{"created":"2024-03-07 17:29:31","title":"Fermionic vacuum stresses in models with toroidal compact dimensions","abstract":"We investigate vacuum expectation value of the energy-momentum tensor for a massive Dirac field in flat spacetime with a toroidal subspace of a general dimension. Quasiperiodicity conditions with arbitrary phases are imposed on the field operator along compact dimensions. These phases are interpreted in terms of magnetic fluxes enclosed by compact dimensions. The equation of state in the uncompact subspace is of the cosmological constant type. It is shown that, in addition to the diagonal components, the vacuum energy-momentum tensor has nonzero off-diagonal components. In special cases of twisted (antiperiodic) and untwisted (periodic) fields the off diagonal components vanish. For untwisted fields the vacuum energy density is positive and the energy-momentum tensor obeys the strong energy condition. For general values of the phases in the periodicity conditions the energy density and stresses can be either positive or negative. The numerical results are given for a Kaluza-Klein type model with two extra dimensions.","sentences":["We investigate vacuum expectation value of the energy-momentum tensor for a massive Dirac field in flat spacetime with a toroidal subspace of a general dimension.","Quasiperiodicity conditions with arbitrary phases are imposed on the field operator along compact dimensions.","These phases are interpreted in terms of magnetic fluxes enclosed by compact dimensions.","The equation of state in the uncompact subspace is of the cosmological constant type.","It is shown that, in addition to the diagonal components, the vacuum energy-momentum tensor has nonzero off-diagonal components.","In special cases of twisted (antiperiodic) and untwisted (periodic) fields the off diagonal components vanish.","For untwisted fields the vacuum energy density is positive and the energy-momentum tensor obeys the strong energy condition.","For general values of the phases in the periodicity conditions the energy density and stresses can be either positive or negative.","The numerical results are given for a Kaluza-Klein type model with two extra dimensions."],"url":"http://arxiv.org/abs/2403.04684v1","category":"hep-th"}
{"created":"2024-03-07 17:27:39","title":"Propagation and emission of gravitational waves in the weak-field limit within the Palatini formalism","abstract":"In the era of gravitational waves physics, when detections of wave fronts are increasing in number, sensibility, frequencies and distances, gravitational physics has entered a period of maximum activity and brilliance. This has open a new window where General Relativity can be challenged in both weak as strong-field regimes. In this paper, we focus on the analysis of gravitational waves propagation and emission in the weak-field regime for gravitational theories within the Palatini formalism. Our results suggest that while propagation in vacuum match General Relativity predictions, emission of gravitational waves from weak sources shows a clear deviation that might be constrained by future detections.","sentences":["In the era of gravitational waves physics, when detections of wave fronts are increasing in number, sensibility, frequencies and distances, gravitational physics has entered a period of maximum activity and brilliance.","This has open a new window where General Relativity can be challenged in both weak as strong-field regimes.","In this paper, we focus on the analysis of gravitational waves propagation and emission in the weak-field regime for gravitational theories within the Palatini formalism.","Our results suggest that while propagation in vacuum match General Relativity predictions, emission of gravitational waves from weak sources shows a clear deviation that might be constrained by future detections."],"url":"http://arxiv.org/abs/2403.04683v1","category":"gr-qc"}
{"created":"2024-03-07 17:26:51","title":"Curvature Perturbations Protected Against One Loop","abstract":"We examine one-loop corrections from small-scale curvature perturbations to the superhorizon-limit ones in single-field inflation models, which have recently caused controversy. We consider the case where the Universe experiences transitions of slow-roll (SR) $\\to$ intermediate period $\\to$ SR. The intermediate period can be an ultra-slow-roll period or a resonant amplification period, either of which enhances small-scale curvature perturbations. We assume that the superhorizon curvature perturbations are conserved at least during each of the SR periods. Within this framework, we show that the superhorizon curvature perturbations during the first and the second SR periods coincide at one-loop level in the slow-roll limit.","sentences":["We examine one-loop corrections from small-scale curvature perturbations to the superhorizon-limit ones in single-field inflation models, which have recently caused controversy.","We consider the case where the Universe experiences transitions of slow-roll (SR) $\\to$ intermediate period $\\to$ SR.","The intermediate period can be an ultra-slow-roll period or a resonant amplification period, either of which enhances small-scale curvature perturbations.","We assume that the superhorizon curvature perturbations are conserved at least during each of the SR periods.","Within this framework, we show that the superhorizon curvature perturbations during the first and the second SR periods coincide at one-loop level in the slow-roll limit."],"url":"http://arxiv.org/abs/2403.04682v1","category":"astro-ph.CO"}
{"created":"2024-03-07 17:23:00","title":"On finite group global and gauged $q$-form symmetries in TQFT","abstract":"We describe a method to implement finite group global and gauged $q$-form symmetries into the axiomatic structure of $d$-dimensional Topological Quantum Field Theory (TQFT) in terms of bordisms decorated by cohomology classes. Namely, on a manifold with a boundary, the gauge field is considered as a class in an appropriate relative cohomology group. It is defined in a way that allows self-consistent cutting and gluing of the manifolds and involves a choice of a $(d-q-2)$-skeleton in the boundary. The method, in a sense, generalizes to arbitrary $d$ and $q$ a method that has been considered in the literature in the case of $d=3,\\;q=0,1$.","sentences":["We describe a method to implement finite group global and gauged $q$-form symmetries into the axiomatic structure of $d$-dimensional Topological Quantum Field Theory (TQFT) in terms of bordisms decorated by cohomology classes.","Namely, on a manifold with a boundary, the gauge field is considered as a class in an appropriate relative cohomology group.","It is defined in a way that allows self-consistent cutting and gluing of the manifolds and involves a choice of a $(d-q-2)$-skeleton in the boundary.","The method, in a sense, generalizes to arbitrary $d$ and $q$ a method that has been considered in the literature in the case of $d=3,\\;q=0,1$."],"url":"http://arxiv.org/abs/2403.04677v1","category":"math-ph"}
{"created":"2024-03-07 17:17:20","title":"Greater than the sum of its parts: The role of minority and majority status in collaborative problem-solving communication","abstract":"Collaborative problem-solving (CPS) is a vital skill used both in the workplace and in educational environments. CPS is useful in tackling increasingly complex global, economic, and political issues and is considered a central 21st century skill. The increasingly connected global community presents a fruitful opportunity for creative and collaborative problem-solving interactions and solutions that involve diverse perspectives. Unfortunately, women and underrepresented minorities (URMs) often face obstacles during collaborative interactions that hinder their key participation in these problem-solving conversations. Here, we explored the communication patterns of minority and non-minority individuals working together in a CPS task. Group Communication Analysis (GCA), a temporally-sensitive computational linguistic tool, was used to examine how URM status impacts individuals' sociocognitive linguistic patterns. Results show differences across racial/ethnic groups in key sociocognitive features that indicate fruitful collaborative interactions. We also investigated how the groups' racial/ethnic composition impacts both individual and group communication patterns. In general, individuals in more demographically diverse groups displayed more productive communication behaviors than individuals who were in majority-dominated groups. We discuss the implications of individual and group diversity on communication patterns that emerge during CPS and how these patterns can impact collaborative outcomes.","sentences":["Collaborative problem-solving (CPS) is a vital skill used both in the workplace and in educational environments.","CPS is useful in tackling increasingly complex global, economic, and political issues and is considered a central 21st century skill.","The increasingly connected global community presents a fruitful opportunity for creative and collaborative problem-solving interactions and solutions that involve diverse perspectives.","Unfortunately, women and underrepresented minorities (URMs) often face obstacles during collaborative interactions that hinder their key participation in these problem-solving conversations.","Here, we explored the communication patterns of minority and non-minority individuals working together in a CPS task.","Group Communication Analysis (GCA), a temporally-sensitive computational linguistic tool, was used to examine how URM status impacts individuals' sociocognitive linguistic patterns.","Results show differences across racial/ethnic groups in key sociocognitive features that indicate fruitful collaborative interactions.","We also investigated how the groups' racial/ethnic composition impacts both individual and group communication patterns.","In general, individuals in more demographically diverse groups displayed more productive communication behaviors than individuals who were in majority-dominated groups.","We discuss the implications of individual and group diversity on communication patterns that emerge during CPS and how these patterns can impact collaborative outcomes."],"url":"http://arxiv.org/abs/2403.04671v1","category":"cs.CL"}
{"created":"2024-03-07 17:14:22","title":"The Social Impact of Generative AI: An Analysis on ChatGPT","abstract":"In recent months, the social impact of Artificial Intelligence (AI) has gained considerable public interest, driven by the emergence of Generative AI models, ChatGPT in particular. The rapid development of these models has sparked heated discussions regarding their benefits, limitations, and associated risks. Generative models hold immense promise across multiple domains, such as healthcare, finance, and education, to cite a few, presenting diverse practical applications. Nevertheless, concerns about potential adverse effects have elicited divergent perspectives, ranging from privacy risks to escalating social inequality. This paper adopts a methodology to delve into the societal implications of Generative AI tools, focusing primarily on the case of ChatGPT. It evaluates the potential impact on several social sectors and illustrates the findings of a comprehensive literature review of both positive and negative effects, emerging trends, and areas of opportunity of Generative AI models. This analysis aims to facilitate an in-depth discussion by providing insights that can inspire policy, regulation, and responsible development practices to foster a human-centered AI.","sentences":["In recent months, the social impact of Artificial Intelligence (AI) has gained considerable public interest, driven by the emergence of Generative AI models, ChatGPT in particular.","The rapid development of these models has sparked heated discussions regarding their benefits, limitations, and associated risks.","Generative models hold immense promise across multiple domains, such as healthcare, finance, and education, to cite a few, presenting diverse practical applications.","Nevertheless, concerns about potential adverse effects have elicited divergent perspectives, ranging from privacy risks to escalating social inequality.","This paper adopts a methodology to delve into the societal implications of Generative AI tools, focusing primarily on the case of ChatGPT.","It evaluates the potential impact on several social sectors and illustrates the findings of a comprehensive literature review of both positive and negative effects, emerging trends, and areas of opportunity of Generative AI models.","This analysis aims to facilitate an in-depth discussion by providing insights that can inspire policy, regulation, and responsible development practices to foster a human-centered AI."],"url":"http://arxiv.org/abs/2403.04667v1","category":"cs.AI"}
{"created":"2024-03-07 17:13:12","title":"Telecom Language Models: Must They Be Large?","abstract":"The increasing interest in Large Language Models (LLMs) within the telecommunications sector underscores their potential to revolutionize operational efficiency. However, the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments. Addressing this challenge, recent advancements have seen the emergence of small language models that surprisingly exhibit performance comparable to their larger counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a compact yet powerful model, exemplifies this new wave of efficient small language models. This paper conducts a comprehensive evaluation of Phi-2's intrinsic understanding of the telecommunications domain. Recognizing the scale-related limitations, we enhance Phi-2's capabilities through a Retrieval-Augmented Generation approach, meticulously integrating an extensive knowledge base specifically curated with telecom standard specifications. The enhanced Phi-2 model demonstrates a profound improvement in accuracy, answering questions about telecom standards with a precision that closely rivals the more resource-intensive GPT-3.5. The paper further explores the refined capabilities of Phi-2 in addressing problem-solving scenarios within the telecom sector, highlighting its potential and limitations.","sentences":["The increasing interest in Large Language Models (LLMs) within the telecommunications sector underscores their potential to revolutionize operational efficiency.","However, the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments.","Addressing this challenge, recent advancements have seen the emergence of small language models that surprisingly exhibit performance comparable to their larger counterparts in many tasks, such as coding and common-sense reasoning.","Phi-2, a compact yet powerful model, exemplifies this new wave of efficient small language models.","This paper conducts a comprehensive evaluation of Phi-2's intrinsic understanding of the telecommunications domain.","Recognizing the scale-related limitations, we enhance Phi-2's capabilities through a Retrieval-Augmented Generation approach, meticulously integrating an extensive knowledge base specifically curated with telecom standard specifications.","The enhanced Phi-2 model demonstrates a profound improvement in accuracy, answering questions about telecom standards with a precision that closely rivals the more resource-intensive GPT-3.5.","The paper further explores the refined capabilities of Phi-2 in addressing problem-solving scenarios within the telecom sector, highlighting its potential and limitations."],"url":"http://arxiv.org/abs/2403.04666v1","category":"cs.CL"}
{"created":"2024-03-07 17:12:12","title":"GreenBytes: Intelligent Energy Estimation for Edge-Cloud","abstract":"This study investigates the application of advanced machine learning models, specifically Long Short-Term Memory (LSTM) networks and Gradient Booster models, for accurate energy consumption estimation within a Kubernetes cluster environment. It aims to enhance sustainable computing practices by providing precise predictions of energy usage across various computing nodes. Through meticulous analysis of model performance on both master and worker nodes, the research reveals the strengths and potential applications of these models in promoting energy efficiency. The LSTM model demonstrates remarkable predictive accuracy, particularly in capturing dynamic computing workloads over time, evidenced by low mean squared error (MSE) rates and the ability to closely track actual energy consumption trends. Conversely, the Gradient Booster model showcases robustness and adaptability across different computational environments, despite slightly higher MSE values. The study underscores the complementary nature of these models in advancing sustainable computing practices, suggesting their integration into energy management systems could significantly enhance environmental sustainability in technology operations.","sentences":["This study investigates the application of advanced machine learning models, specifically Long Short-Term Memory (LSTM) networks and Gradient Booster models, for accurate energy consumption estimation within a Kubernetes cluster environment.","It aims to enhance sustainable computing practices by providing precise predictions of energy usage across various computing nodes.","Through meticulous analysis of model performance on both master and worker nodes, the research reveals the strengths and potential applications of these models in promoting energy efficiency.","The LSTM model demonstrates remarkable predictive accuracy, particularly in capturing dynamic computing workloads over time, evidenced by low mean squared error (MSE) rates and the ability to closely track actual energy consumption trends.","Conversely, the Gradient Booster model showcases robustness and adaptability across different computational environments, despite slightly higher MSE values.","The study underscores the complementary nature of these models in advancing sustainable computing practices, suggesting their integration into energy management systems could significantly enhance environmental sustainability in technology operations."],"url":"http://arxiv.org/abs/2403.04665v1","category":"cs.DC"}
{"created":"2024-03-07 17:06:15","title":"Exploring the Design Space of Optical See-through AR Head-Mounted Displays to Support First Responders in the Field","abstract":"First responders (FRs) navigate hazardous, unfamiliar environments in the field (e.g., mass-casualty incidents), making life-changing decisions in a split second. AR head-mounted displays (HMDs) have shown promise in supporting them due to its capability of recognizing and augmenting the challenging environments in a hands-free manner. However, the design space have not been thoroughly explored by involving various FRs who serve different roles (e.g., firefighters, law enforcement) but collaborate closely in the field. We interviewed 26 first responders in the field who experienced a state-of-the-art optical-see-through AR HMD, as well as its interaction techniques and four types of AR cues (i.e., overview cues, directional cues, highlighting cues, and labeling cues), soliciting their first-hand experiences, design ideas, and concerns. Our study revealed both generic and role-specific preferences and needs for AR hardware, interactions, and feedback, as well as identifying desired AR designs tailored to urgent, risky scenarios (e.g., affordance augmentation to facilitate fast and safe action). While acknowledging the value of AR HMDs, concerns were also raised around trust, privacy, and proper integration with other equipment. Finally, we derived comprehensive and actionable design guidelines to inform future AR systems for in-field FRs.","sentences":["First responders (FRs) navigate hazardous, unfamiliar environments in the field (e.g., mass-casualty incidents), making life-changing decisions in a split second.","AR head-mounted displays (HMDs) have shown promise in supporting them due to its capability of recognizing and augmenting the challenging environments in a hands-free manner.","However, the design space have not been thoroughly explored by involving various FRs who serve different roles (e.g., firefighters, law enforcement) but collaborate closely in the field.","We interviewed 26 first responders in the field who experienced a state-of-the-art optical-see-through AR HMD, as well as its interaction techniques and four types of AR cues (i.e., overview cues, directional cues, highlighting cues, and labeling cues), soliciting their first-hand experiences, design ideas, and concerns.","Our study revealed both generic and role-specific preferences and needs for AR hardware, interactions, and feedback, as well as identifying desired AR designs tailored to urgent, risky scenarios (e.g., affordance augmentation to facilitate fast and safe action).","While acknowledging the value of AR HMDs, concerns were also raised around trust, privacy, and proper integration with other equipment.","Finally, we derived comprehensive and actionable design guidelines to inform future AR systems for in-field FRs."],"url":"http://arxiv.org/abs/2403.04660v1","category":"cs.HC"}
{"created":"2024-03-07 17:04:33","title":"Natural geometric Fourier transforms and the associated fractional Laplacian","abstract":"To each arbitrary given general geometric structure on $\\mathbb{R}^{n}$, we associate a pair of compatible Fourier transforms, that prove to appear naturally in the framework of Poisson's summation formula for full lattices. We study their properties and the compatibility with the classical $n-$dimensional Fourier transform. In the case of a positive definite geometric structure, we show that these geometric Fourier transforms induce a geometric fractional Laplacian, with properties similar to those of the classical fractional Laplacian.","sentences":["To each arbitrary given general geometric structure on $\\mathbb{R}^{n}$, we associate a pair of compatible Fourier transforms, that prove to appear naturally in the framework of Poisson's summation formula for full lattices.","We study their properties and the compatibility with the classical $n-$dimensional Fourier transform.","In the case of a positive definite geometric structure, we show that these geometric Fourier transforms induce a geometric fractional Laplacian, with properties similar to those of the classical fractional Laplacian."],"url":"http://arxiv.org/abs/2403.04658v1","category":"math.CA"}
{"created":"2024-03-07 16:59:55","title":"Chain of Thought Explanation for Dialogue State Tracking","abstract":"Dialogue state tracking (DST) aims to record user queries and goals during a conversational interaction achieved by maintaining a predefined set of slots and their corresponding values. Current approaches decide slot values opaquely, while humans usually adopt a more deliberate approach by collecting information from relevant dialogue turns and then reasoning the appropriate values. In this work, we focus on the steps needed to figure out slot values by proposing a model named Chain-of-Thought-Explanation (CoTE) for the DST task. CoTE, which is built on the generative DST framework, is designed to create detailed explanations step by step after determining the slot values. This process leads to more accurate and reliable slot values. More-over, to improve the reasoning ability of the CoTE, we further construct more fluent and high-quality explanations with automatic paraphrasing, leading the method CoTE-refined. Experimental results on three widely recognized DST benchmarks-MultiWOZ 2.2, WoZ 2.0, and M2M-demonstrate the remarkable effectiveness of the CoTE. Furthermore, through a meticulous fine-grained analysis, we observe significant benefits of our CoTE on samples characterized by longer dialogue turns, user responses, and reasoning steps.","sentences":["Dialogue state tracking (DST) aims to record user queries and goals during a conversational interaction achieved by maintaining a predefined set of slots and their corresponding values.","Current approaches decide slot values opaquely, while humans usually adopt a more deliberate approach by collecting information from relevant dialogue turns and then reasoning the appropriate values.","In this work, we focus on the steps needed to figure out slot values by proposing a model named Chain-of-Thought-Explanation (CoTE) for the DST task.","CoTE, which is built on the generative DST framework, is designed to create detailed explanations step by step after determining the slot values.","This process leads to more accurate and reliable slot values.","More-over, to improve the reasoning ability of the CoTE, we further construct more fluent and high-quality explanations with automatic paraphrasing, leading the method CoTE-refined.","Experimental results on three widely recognized DST benchmarks-MultiWOZ 2.2, WoZ 2.0, and M2M-demonstrate the remarkable effectiveness of the CoTE.","Furthermore, through a meticulous fine-grained analysis, we observe significant benefits of our CoTE on samples characterized by longer dialogue turns, user responses, and reasoning steps."],"url":"http://arxiv.org/abs/2403.04656v1","category":"cs.CL"}
{"created":"2024-03-07 16:52:49","title":"Yi: Open Foundation Models by 01.AI","abstract":"We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model. We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance. We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance. We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models.","sentences":["We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities.","The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models.","Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena.","Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts.","For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline.","For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers.","For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model.","We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance.","We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance.","We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models."],"url":"http://arxiv.org/abs/2403.04652v1","category":"cs.CL"}
{"created":"2024-03-07 16:50:25","title":"Context-Based Multimodal Fusion","abstract":"The fusion models, which effectively combine information from different sources, are widely used in solving multimodal tasks. However, they have significant limitations related to aligning data distributions across different modalities. This challenge can lead to inconsistencies and difficulties in learning robust representations. Alignment models, while specifically addressing this issue, often require training \"from scratch\" with large datasets to achieve optimal results, which can be costly in terms of resources and time. To overcome these limitations, we propose an innovative model called Context-Based Multimodal Fusion (CBMF), which combines both modality fusion and data distribution alignment. In CBMF, each modality is represented by a specific context vector, fused with the embedding of each modality. This enables the use of large pre-trained models that can be frozen, reducing the computational and training data requirements. Additionally, the network learns to differentiate embeddings of different modalities through fusion with context and aligns data distributions using a contrastive approach for self-supervised learning. Thus, CBMF offers an effective and economical solution for solving complex multimodal tasks.","sentences":["The fusion models, which effectively combine information from different sources, are widely used in solving multimodal tasks.","However, they have significant limitations related to aligning data distributions across different modalities.","This challenge can lead to inconsistencies and difficulties in learning robust representations.","Alignment models, while specifically addressing this issue, often require training \"from scratch\" with large datasets to achieve optimal results, which can be costly in terms of resources and time.","To overcome these limitations, we propose an innovative model called Context-Based Multimodal Fusion (CBMF), which combines both modality fusion and data distribution alignment.","In CBMF, each modality is represented by a specific context vector, fused with the embedding of each modality.","This enables the use of large pre-trained models that can be frozen, reducing the computational and training data requirements.","Additionally, the network learns to differentiate embeddings of different modalities through fusion with context and aligns data distributions using a contrastive approach for self-supervised learning.","Thus, CBMF offers an effective and economical solution for solving complex multimodal tasks."],"url":"http://arxiv.org/abs/2403.04650v1","category":"cs.LG"}
{"created":"2024-03-07 16:46:56","title":"Constructing equilibrium states for Smale spaces","abstract":"There are several known constructions of equilibrium states for H\\\"older continuous potentials in the context of both subshifts of finite type and uniformly hyperbolic systems. In this article we present another method of building such measures, formulated in the unified and more general setting of Smale spaces. This simultaneously extends the authors' previous work for hyperbolic attractors (modelled after Sinai's classical approach for SRB-measures) and gives a new and original construction of equilibrium states for subshifts of finite type.","sentences":["There are several known constructions of equilibrium states for H\\\"older continuous potentials in the context of both subshifts of finite type and uniformly hyperbolic systems.","In this article we present another method of building such measures, formulated in the unified and more general setting of Smale spaces.","This simultaneously extends the authors' previous work for hyperbolic attractors (modelled after Sinai's classical approach for SRB-measures) and gives a new and original construction of equilibrium states for subshifts of finite type."],"url":"http://arxiv.org/abs/2403.04646v1","category":"math.DS"}
{"created":"2024-03-07 16:45:47","title":"Accelerating the Convergence of Coupled Cluster Calculations of the Homogeneous Electron Gas Using Bayesian Ridge Regression","abstract":"The homogeneous electron gas is a system which has many applications in chemistry and physics. However, its infinite nature makes studies at the many-body level complicated due to long computational run times. Because it is size extensive, coupled cluster theory is capable of studying the homogeneous electron gas, but it still poses a large computational challenge as the time needed for precise calculations increases in a polynomial manner with the number of particles and single-particle states. Consequently, achieving convergence in energy calculations becomes challenging, if not prohibited, due to long computational run times and high computational resource requirements. This paper develops the sequential regression extrapolation (SRE) to predict the coupled cluster energies of the homogeneous electron gas in the complete basis limit using Bayesian ridge regression to make predictions from calculations at truncated basis sizes. Using the SRE method we were able to predict the coupled cluster doubles energies for the electron gas across a variety of values of N and $r_s$, for a total of 70 predictions, with an average error of 4.28x10$^{-4}$ Hartrees while saving 88.9 hours of computational time. The SRE method can accurately extrapolate electron gas energies to the complete basis limit, saving both computational time and resources. Additionally, the SRE is a general method that can be applied to a variety of systems, many-body methods, and extrapolations.","sentences":["The homogeneous electron gas is a system which has many applications in chemistry and physics.","However, its infinite nature makes studies at the many-body level complicated due to long computational run times.","Because it is size extensive, coupled cluster theory is capable of studying the homogeneous electron gas, but it still poses a large computational challenge as the time needed for precise calculations increases in a polynomial manner with the number of particles and single-particle states.","Consequently, achieving convergence in energy calculations becomes challenging, if not prohibited, due to long computational run times and high computational resource requirements.","This paper develops the sequential regression extrapolation (SRE) to predict the coupled cluster energies of the homogeneous electron gas in the complete basis limit using Bayesian ridge regression to make predictions from calculations at truncated basis sizes.","Using the SRE method we were able to predict the coupled cluster doubles energies for the electron gas across a variety of values of N and $r_s$, for a total of 70 predictions, with an average error of 4.28x10$^{-4}$ Hartrees while saving 88.9 hours of computational time.","The SRE method can accurately extrapolate electron gas energies to the complete basis limit, saving both computational time and resources.","Additionally, the SRE is a general method that can be applied to a variety of systems, many-body methods, and extrapolations."],"url":"http://arxiv.org/abs/2403.04645v1","category":"physics.comp-ph"}
{"created":"2024-03-07 16:43:15","title":"Leptoquark-Mediated Two-Loop Neutrino Mass in Unified Theory","abstract":"Scalar leptoquarks naturally arise within unified theories, offering a promising avenue for addressing one of the most significant challenges of the Standard Model--the existence of non-zero neutrino masses. In this work, we present a unified theory based on the SU(5) gauge group, where neutrino mass appears at the two-loop level via the propagation of scalar leptoquarks. Due to the unified framework, the charged fermion and neutrino masses and mixings are entangled and determined by a common set of Yukawa couplings. These exotic particles not only shed light on the neutrino mass generation mechanism but also help achieve the unification of gauge couplings and are expected to lead to substantial lepton flavor violating rates, offering tangible opportunities for experimental verification. Reproducing the observed neutrino mass scale necessitates that leptoquarks reside a few orders of magnitude below the unification scale--a specific feature of the proposed scenario. Moreover, maximizing the unification scale implies TeV scale new physics states, making them accessible at colliders. The diverse roles that leptoquarks play highlight the elegance and predictive ability of the proposed unified model.","sentences":["Scalar leptoquarks naturally arise within unified theories, offering a promising avenue for addressing one of the most significant challenges of the Standard Model--the existence of non-zero neutrino masses.","In this work, we present a unified theory based on the SU(5) gauge group, where neutrino mass appears at the two-loop level via the propagation of scalar leptoquarks.","Due to the unified framework, the charged fermion and neutrino masses and mixings are entangled and determined by a common set of Yukawa couplings.","These exotic particles not only shed light on the neutrino mass generation mechanism but also help achieve the unification of gauge couplings and are expected to lead to substantial lepton flavor violating rates, offering tangible opportunities for experimental verification.","Reproducing the observed neutrino mass scale necessitates that leptoquarks reside a few orders of magnitude below the unification scale--a specific feature of the proposed scenario.","Moreover, maximizing the unification scale implies TeV scale new physics states, making them accessible at colliders.","The diverse roles that leptoquarks play highlight the elegance and predictive ability of the proposed unified model."],"url":"http://arxiv.org/abs/2403.04644v1","category":"hep-ph"}
{"created":"2024-03-07 16:42:37","title":"QAQ: Quality Adaptive Quantization for LLM KV Cache","abstract":"The emergence of LLMs has ignited a fresh surge of breakthroughs in NLP applications, particularly in domains such as question-answering systems and text generation. As the need for longer context grows, a significant bottleneck in model deployment emerges due to the linear expansion of the Key-Value (KV) cache with the context length. Existing methods primarily rely on various hypotheses, such as sorting the KV cache based on attention scores for replacement or eviction, to compress the KV cache and improve model throughput. However, heuristics used by these strategies may wrongly evict essential KV cache, which can significantly degrade model performance. In this paper, we propose QAQ, a Quality Adaptive Quantization scheme for the KV cache. We theoretically demonstrate that key cache and value cache exhibit distinct sensitivities to quantization, leading to the formulation of separate quantization strategies for their non-uniform quantization. Through the integration of dedicated outlier handling, as well as an improved attention-aware approach, QAQ achieves up to 10x the compression ratio of the KV cache size with a neglectable impact on model performance. QAQ significantly reduces the practical hurdles of deploying LLMs, opening up new possibilities for longer-context applications. The code is available at github.com/ClubieDong/KVCacheQuantization.","sentences":["The emergence of LLMs has ignited a fresh surge of breakthroughs in NLP applications, particularly in domains such as question-answering systems and text generation.","As the need for longer context grows, a significant bottleneck in model deployment emerges due to the linear expansion of the Key-Value (KV) cache with the context length.","Existing methods primarily rely on various hypotheses, such as sorting the KV cache based on attention scores for replacement or eviction, to compress the KV cache and improve model throughput.","However, heuristics used by these strategies may wrongly evict essential KV cache, which can significantly degrade model performance.","In this paper, we propose QAQ, a Quality Adaptive Quantization scheme for the KV cache.","We theoretically demonstrate that key cache and value cache exhibit distinct sensitivities to quantization, leading to the formulation of separate quantization strategies for their non-uniform quantization.","Through the integration of dedicated outlier handling, as well as an improved attention-aware approach, QAQ achieves up to 10x the compression ratio of the KV cache size with a neglectable impact on model performance.","QAQ significantly reduces the practical hurdles of deploying LLMs, opening up new possibilities for longer-context applications.","The code is available at github.com/ClubieDong/KVCacheQuantization."],"url":"http://arxiv.org/abs/2403.04643v1","category":"cs.CL"}
{"created":"2024-03-07 16:34:02","title":"The interdefinability of expansions of Belnap-Dunn logic","abstract":"Belnap-Dunn logic, also knows as the logic of First-Degree Entailment, is a logic that can serve as the underlying logic of theories that are inconsistent or incomplete. For various reasons, different expansions of Belnap-Dunn logic with non-classical connectives have been studied. This paper investigates the question whether those expansions are interdefinable with an expansion whose connectives include only classical connectives. This is worth knowing because it is difficult to say how close a logic with non-classical connectives is related to classical logic. The notion of interdefinability of logics used is based on a general notion of definability of a connective in a logic that seems to have been forgotten.","sentences":["Belnap-Dunn logic, also knows as the logic of First-Degree Entailment, is a logic that can serve as the underlying logic of theories that are inconsistent or incomplete.","For various reasons, different expansions of Belnap-Dunn logic with non-classical connectives have been studied.","This paper investigates the question whether those expansions are interdefinable with an expansion whose connectives include only classical connectives.","This is worth knowing because it is difficult to say how close a logic with non-classical connectives is related to classical logic.","The notion of interdefinability of logics used is based on a general notion of definability of a connective in a logic that seems to have been forgotten."],"url":"http://arxiv.org/abs/2403.04641v1","category":"cs.LO"}
{"created":"2024-03-07 16:29:12","title":"Scalable, Simulation-Guided Compliant Tactile Finger Design","abstract":"Compliant grippers enable robots to work with humans in unstructured environments. In general, these grippers can improve with tactile sensing to estimate the state of objects around them to precisely manipulate objects. However, co-designing compliant structures with high-resolution tactile sensing is a challenging task. We propose a simulation framework for the end-to-end forward design of GelSight Fin Ray sensors. Our simulation framework consists of mechanical simulation using the finite element method (FEM) and optical simulation including physically based rendering (PBR). To simulate the fluorescent paint used in these GelSight Fin Rays, we propose an efficient method that can be directly integrated in PBR. Using the simulation framework, we investigate design choices available in the compliant grippers, namely gel pad shapes, illumination conditions, Fin Ray gripper sizes, and Fin Ray stiffness. This infrastructure enables faster design and prototype time frames of new Fin Ray sensors that have various sensing areas, ranging from 48 mm $\\times$ \\18 mm to 70 mm $\\times$ 35 mm. Given the parameters we choose, we can thus optimize different Fin Ray designs and show their utility in grasping day-to-day objects.","sentences":["Compliant grippers enable robots to work with humans in unstructured environments.","In general, these grippers can improve with tactile sensing to estimate the state of objects around them to precisely manipulate objects.","However, co-designing compliant structures with high-resolution tactile sensing is a challenging task.","We propose a simulation framework for the end-to-end forward design of GelSight Fin Ray sensors.","Our simulation framework consists of mechanical simulation using the finite element method (FEM) and optical simulation including physically based rendering (PBR).","To simulate the fluorescent paint used in these GelSight Fin Rays, we propose an efficient method that can be directly integrated in PBR.","Using the simulation framework, we investigate design choices available in the compliant grippers, namely gel pad shapes, illumination conditions, Fin Ray gripper sizes, and Fin Ray stiffness.","This infrastructure enables faster design and prototype time frames of new Fin Ray sensors that have various sensing areas, ranging from 48 mm $\\times$ \\18","mm to 70 mm $\\times$ 35 mm.","Given the parameters we choose, we can thus optimize different Fin Ray designs and show their utility in grasping day-to-day objects."],"url":"http://arxiv.org/abs/2403.04638v1","category":"cs.RO"}
{"created":"2024-03-07 16:18:28","title":"Pix2Gif: Motion-Guided Diffusion for GIF Generation","abstract":"We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence. In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. After pretraining, we apply our model in a zero-shot manner to a number of video datasets. Extensive qualitative and quantitative experiments demonstrate the effectiveness of our model -- it not only captures the semantic prompt from text but also the spatial ones from motion guidance. We train all our models using a single node of 16xV100 GPUs. Code, dataset and models are made public at: https://hiteshk03.github.io/Pix2Gif/.","sentences":["We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation.","We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig.","To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts.","Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence.","In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects.","After pretraining, we apply our model in a zero-shot manner to a number of video datasets.","Extensive qualitative and quantitative experiments demonstrate the effectiveness of our model -- it not only captures the semantic prompt from text but also the spatial ones from motion guidance.","We train all our models using a single node of 16xV100 GPUs.","Code, dataset and models are made public at: https://hiteshk03.github.io/Pix2Gif/."],"url":"http://arxiv.org/abs/2403.04634v1","category":"cs.CV"}
{"created":"2024-03-07 16:16:39","title":"Message-Observing Sessions","abstract":"We present Most, a process language with message-observing session types. Message-observing session types extend binary session types with type-level computation to specify communication protocols that vary based on messages observed on other channels. Hence, Most allows us to express global invariants about processes, rather than just local invariants, in a bottom-up, compositional way. We give Most a semantic foundation using traces with binding, a semantic approach for compositionally reasoning about traces in the presence of name generation. We use this semantics to prove type soundness and compositionality for Most processes. We see this as a significant step towards capturing message-dependencies and providing more precise guarantees about processes.","sentences":["We present Most, a process language with message-observing session types.","Message-observing session types extend binary session types with type-level computation to specify communication protocols that vary based on messages observed on other channels.","Hence, Most allows us to express global invariants about processes, rather than just local invariants, in a bottom-up, compositional way.","We give Most a semantic foundation using traces with binding, a semantic approach for compositionally reasoning about traces in the presence of name generation.","We use this semantics to prove type soundness and compositionality for Most processes.","We see this as a significant step towards capturing message-dependencies and providing more precise guarantees about processes."],"url":"http://arxiv.org/abs/2403.04633v1","category":"cs.PL"}
{"created":"2024-03-07 16:14:26","title":"Galilean symmetry of the KdV hierarchy","abstract":"By solving the infinitesimal Galilean symmetry for the KdV hierarchy, we obtain an explicit expression for the corresponding one-parameter Lie group, which we call the Galilean symmetry of the KdV hierarchy. As an application, we establish an explicit relationship between the non-abelian Born--Infeld partition function and the generalized Br\\'ezin--Gross--Witten partition function.","sentences":["By solving the infinitesimal Galilean symmetry for the KdV hierarchy, we obtain an explicit expression for the corresponding one-parameter Lie group, which we call the Galilean symmetry of the KdV hierarchy.","As an application, we establish an explicit relationship between the non-abelian Born--Infeld partition function and the generalized Br\\'ezin--Gross--Witten partition function."],"url":"http://arxiv.org/abs/2403.04631v1","category":"math-ph"}
{"created":"2024-03-07 16:14:08","title":"Time-Aware Projections: Truly Node-Private Graph Statistics under Continual Observation","abstract":"We describe the first algorithms that satisfy the standard notion of node-differential privacy in the continual release setting (i.e., without an assumed promise on input streams). Previous work addresses node-private continual release by assuming an unenforced promise on the maximum degree in a graph; indeed, the algorithms from these works exhibit blatant privacy violations when the degree bound is not met. Our algorithms are accurate on sparse graphs, for several fundamental graph problems: counting edges, triangles, other subgraphs, and connected components; and releasing degree histograms. Our unconditionally private algorithms generally have optimal error, up to polylogarithmic factors and lower-order terms.   We provide general transformations that take a base algorithm for the continual release setting, which need only be private for streams satisfying a promised degree bound, and produce an algorithm that is unconditionally private yet mimics the base algorithm when the stream meets the degree bound (and adds only linear overhead to the time and space complexity of the base algorithm). To do so, we design new projection algorithms for graph streams, based on the batch-model techniques of Day et al. 2016 and Blocki et al. 2013, which modify the stream to limit its degree. Our main technical innovation is to show that the projections are stable -- meaning that similar input graphs have similar projections -- when the input stream satisfies a privately testable safety condition. Our transformation then follows a novel online variant of the Propose-Test-Release framework (Dwork and Lei, 2009), privately testing the safety condition before releasing output at each step.","sentences":["We describe the first algorithms that satisfy the standard notion of node-differential privacy in the continual release setting (i.e., without an assumed promise on input streams).","Previous work addresses node-private continual release by assuming an unenforced promise on the maximum degree in a graph; indeed, the algorithms from these works exhibit blatant privacy violations when the degree bound is not met.","Our algorithms are accurate on sparse graphs, for several fundamental graph problems: counting edges, triangles, other subgraphs, and connected components; and releasing degree histograms.","Our unconditionally private algorithms generally have optimal error, up to polylogarithmic factors and lower-order terms.   ","We provide general transformations that take a base algorithm for the continual release setting, which need only be private for streams satisfying a promised degree bound, and produce an algorithm that is unconditionally private yet mimics the base algorithm when the stream meets the degree bound (and adds only linear overhead to the time and space complexity of the base algorithm).","To do so, we design new projection algorithms for graph streams, based on the batch-model techniques of Day et al. 2016 and Blocki et al. 2013, which modify the stream to limit its degree.","Our main technical innovation is to show that the projections are stable -- meaning that similar input graphs have similar projections -- when the input stream satisfies a privately testable safety condition.","Our transformation then follows a novel online variant of the Propose-Test-Release framework (Dwork and Lei, 2009), privately testing the safety condition before releasing output at each step."],"url":"http://arxiv.org/abs/2403.04630v1","category":"cs.DS"}
{"created":"2024-03-07 16:13:32","title":"Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration","abstract":"Bayesian optimization (BO) with Gaussian processes (GP) has become an indispensable algorithm for black box optimization problems. Not without a dash of irony, BO is often considered a black box itself, lacking ways to provide reasons as to why certain parameters are proposed to be evaluated. This is particularly relevant in human-in-the-loop applications of BO, such as in robotics. We address this issue by proposing ShapleyBO, a framework for interpreting BO's proposals by game-theoretic Shapley values.They quantify each parameter's contribution to BO's acquisition function. Exploiting the linearity of Shapley values, we are further able to identify how strongly each parameter drives BO's exploration and exploitation for additive acquisition functions like the confidence bound. We also show that ShapleyBO can disentangle the contributions to exploration into those that explore aleatoric and epistemic uncertainty. Moreover, our method gives rise to a ShapleyBO-assisted human machine interface (HMI), allowing users to interfere with BO in case proposals do not align with human reasoning. We demonstrate this HMI's benefits for the use case of personalizing wearable robotic devices (assistive back exosuits) by human-in-the-loop BO. Results suggest human-BO teams with access to ShapleyBO can achieve lower regret than teams without.","sentences":["Bayesian optimization (BO) with Gaussian processes (GP) has become an indispensable algorithm for black box optimization problems.","Not without a dash of irony, BO is often considered a black box itself, lacking ways to provide reasons as to why certain parameters are proposed to be evaluated.","This is particularly relevant in human-in-the-loop applications of BO, such as in robotics.","We address this issue by proposing ShapleyBO, a framework for interpreting BO's proposals by game-theoretic Shapley values.","They quantify each parameter's contribution to BO's acquisition function.","Exploiting the linearity of Shapley values, we are further able to identify how strongly each parameter drives BO's exploration and exploitation for additive acquisition functions like the confidence bound.","We also show that ShapleyBO can disentangle the contributions to exploration into those that explore aleatoric and epistemic uncertainty.","Moreover, our method gives rise to a ShapleyBO-assisted human machine interface (HMI), allowing users to interfere with BO in case proposals do not align with human reasoning.","We demonstrate this HMI's benefits for the use case of personalizing wearable robotic devices (assistive back exosuits) by human-in-the-loop BO.","Results suggest human-BO teams with access to ShapleyBO can achieve lower regret than teams without."],"url":"http://arxiv.org/abs/2403.04629v1","category":"cs.LG"}
{"created":"2024-03-07 16:12:59","title":"On the extinction of multiple shocks in scalar viscous conservation laws","abstract":"We are interested in the dynamics of interfaces, or zeros, of shock waves in general scalar viscous conservation laws with a locally Lipschitz continuous flux function, such as the modular Burgers' equation. We prove that all interfaces coalesce within finite time, leaving behind either a single interface or no interface at all. Our proof relies on mass and energy estimates, regularization of the flux function, and an application of the Sturm theorems on the number of zeros of solutions of parabolic problems. Our analysis yields an explicit upper bound on the time of extinction in terms of the initial condition and the flux function. Moreover, in the case of a smooth flux function, we characterize the generic bifurcations arising at a coalescence event with and without the presence of odd symmetry. We identify associated scaling laws describing the local interface dynamics near collision. Finally, we present an extension of these results to the case of anti-shock waves converging to asymptotic limits of opposite signs. Our analysis is corroborated by numerical simulations in the modular Burgers' equation and its regularizations.","sentences":["We are interested in the dynamics of interfaces, or zeros, of shock waves in general scalar viscous conservation laws with a locally Lipschitz continuous flux function, such as the modular Burgers' equation.","We prove that all interfaces coalesce within finite time, leaving behind either a single interface or no interface at all.","Our proof relies on mass and energy estimates, regularization of the flux function, and an application of the Sturm theorems on the number of zeros of solutions of parabolic problems.","Our analysis yields an explicit upper bound on the time of extinction in terms of the initial condition and the flux function.","Moreover, in the case of a smooth flux function, we characterize the generic bifurcations arising at a coalescence event with and without the presence of odd symmetry.","We identify associated scaling laws describing the local interface dynamics near collision.","Finally, we present an extension of these results to the case of anti-shock waves converging to asymptotic limits of opposite signs.","Our analysis is corroborated by numerical simulations in the modular Burgers' equation and its regularizations."],"url":"http://arxiv.org/abs/2403.04628v1","category":"math.AP"}
{"created":"2024-03-07 16:12:54","title":"Distributed Multi-objective Optimization in Cyber-Physical Energy Systems","abstract":"Managing complex Cyber-Physical Energy Systems (CPES) requires solving various optimization problems with multiple objectives and constraints. As distributed control architectures are becoming more popular in CPES for certain tasks due to their flexibility, robustness, and privacy protection, multi-objective optimization must also be distributed. For this purpose, we present MO-COHDA, a fully distributed, agent-based algorithm, for solving multi-objective optimization problems of CPES. MO-COHDA allows an easy and flexible adaptation to different use cases and integration of custom functionality. To evaluate the effectiveness of MO-COHDA, we compare it to a central NSGA-2 algorithm using multi-objective benchmark functions from the ZDT problem suite. The results show that MO-COHDA can approximate the reference front of the benchmark problems well and is suitable for solving multi-objective optimization problems. In addition, an example use case of scheduling a group of generation units while optimizing three different objectives was evaluated to show how MO-COHDA can be easily applied to real-world optimization problems in CPES.","sentences":["Managing complex Cyber-Physical Energy Systems (CPES) requires solving various optimization problems with multiple objectives and constraints.","As distributed control architectures are becoming more popular in CPES for certain tasks due to their flexibility, robustness, and privacy protection, multi-objective optimization must also be distributed.","For this purpose, we present MO-COHDA, a fully distributed, agent-based algorithm, for solving multi-objective optimization problems of CPES.","MO-COHDA allows an easy and flexible adaptation to different use cases and integration of custom functionality.","To evaluate the effectiveness of MO-COHDA, we compare it to a central NSGA-2 algorithm using multi-objective benchmark functions from the ZDT problem suite.","The results show that MO-COHDA can approximate the reference front of the benchmark problems well and is suitable for solving multi-objective optimization problems.","In addition, an example use case of scheduling a group of generation units while optimizing three different objectives was evaluated to show how MO-COHDA can be easily applied to real-world optimization problems in CPES."],"url":"http://arxiv.org/abs/2403.04627v1","category":"cs.MA"}
{"created":"2024-03-07 16:02:08","title":"Tensor bispectrum mediated by an excited scalar field during inflation","abstract":"We calculate the tensor bispectrum mediated by an excited scalar field during inflation and find that the bispectrum peaks in the squeezed configuration, which is different from that of gravitational waves induced by enhanced curvature perturbations re-entering the horizon in the radiation-dominated era. Measuring the bispectrum provides a promising way to distinguish the stochastic gravitational-wave background generated during inflation from that generated after inflation.","sentences":["We calculate the tensor bispectrum mediated by an excited scalar field during inflation and find that the bispectrum peaks in the squeezed configuration, which is different from that of gravitational waves induced by enhanced curvature perturbations re-entering the horizon in the radiation-dominated era.","Measuring the bispectrum provides a promising way to distinguish the stochastic gravitational-wave background generated during inflation from that generated after inflation."],"url":"http://arxiv.org/abs/2403.04617v1","category":"astro-ph.CO"}
{"created":"2024-03-07 15:59:35","title":"Rectangular Rotational Invariant Estimator for High-Rank Matrix Estimation","abstract":"We consider estimating a matrix from noisy observations coming from an arbitrary additive bi-rotational invariant perturbation. We propose an estimator which is optimal among the class of rectangular rotational invariant estimators and can be applied irrespective of the prior on the signal. For the particular case of Gaussian noise, we prove the optimality of the proposed estimator, and we find an explicit expression for the MMSE in terms of the limiting singular value distribution of the observation matrix. Moreover, we prove a formula linking the asymptotic mutual information and the limit of a log-spherical integral of rectangular matrices. We also provide numerical checks for our results for general bi-rotational invariant noise, as well as Gaussian noise, which match our theoretical predictions.","sentences":["We consider estimating a matrix from noisy observations coming from an arbitrary additive bi-rotational invariant perturbation.","We propose an estimator which is optimal among the class of rectangular rotational invariant estimators and can be applied irrespective of the prior on the signal.","For the particular case of Gaussian noise, we prove the optimality of the proposed estimator, and we find an explicit expression for the MMSE in terms of the limiting singular value distribution of the observation matrix.","Moreover, we prove a formula linking the asymptotic mutual information and the limit of a log-spherical integral of rectangular matrices.","We also provide numerical checks for our results for general bi-rotational invariant noise, as well as Gaussian noise, which match our theoretical predictions."],"url":"http://arxiv.org/abs/2403.04615v1","category":"cs.IT"}
{"created":"2024-03-07 15:58:25","title":"Simultaneous Conformal Prediction of Missing Outcomes with Propensity Score $\u03b5$-Discretization","abstract":"We study the problem of simultaneous predictive inference on multiple outcomes missing at random. We consider a suite of possible simultaneous coverage properties, conditionally on the missingness pattern and on the -- possibly discretized/binned -- feature values. For data with discrete feature distributions, we develop a procedure which attains feature- and missingness-conditional coverage; and further improve it via pooling its results after partitioning the unobserved outcomes. To handle general continuous feature distributions, we introduce methods based on discretized feature values. To mitigate the issue that feature-discretized data may fail to remain missing at random, we propose propensity score $\\epsilon$-discretization. This approach is inspired by the balancing property of the propensity score, namely that the missing data mechanism is independent of the outcome conditional on the propensity [Rosenbaum and Rubin (1983)]. We show that the resulting pro-CP method achieves propensity score discretized feature- and missingness-conditional coverage, when the propensity score is known exactly or is estimated sufficiently accurately. Furthermore, we consider a stronger inferential target, the squared-coverage guarantee, which penalizes the spread of the coverage proportion. We propose methods -- termed pro-CP2 -- to achieve it with similar conditional properties as we have shown for usual coverage. A key novel technical contribution in our results is that propensity score discretization leads to a notion of approximate balancing, which we formalize and characterize precisely. In extensive empirical experiments on simulated data and on a job search intervention dataset, we illustrate that our procedures provide informative prediction sets with valid conditional coverage.","sentences":["We study the problem of simultaneous predictive inference on multiple outcomes missing at random.","We consider a suite of possible simultaneous coverage properties, conditionally on the missingness pattern and on the -- possibly discretized/binned -- feature values.","For data with discrete feature distributions, we develop a procedure which attains feature- and missingness-conditional coverage; and further improve it via pooling its results after partitioning the unobserved outcomes.","To handle general continuous feature distributions, we introduce methods based on discretized feature values.","To mitigate the issue that feature-discretized data may fail to remain missing at random, we propose propensity score $\\epsilon$-discretization.","This approach is inspired by the balancing property of the propensity score, namely that the missing data mechanism is independent of the outcome conditional on the propensity","[Rosenbaum and Rubin (1983)].","We show that the resulting pro-CP method achieves propensity score discretized feature- and missingness-conditional coverage, when the propensity score is known exactly or is estimated sufficiently accurately.","Furthermore, we consider a stronger inferential target, the squared-coverage guarantee, which penalizes the spread of the coverage proportion.","We propose methods -- termed pro-CP2 -- to achieve it with similar conditional properties as we have shown for usual coverage.","A key novel technical contribution in our results is that propensity score discretization leads to a notion of approximate balancing, which we formalize and characterize precisely.","In extensive empirical experiments on simulated data and on a job search intervention dataset, we illustrate that our procedures provide informative prediction sets with valid conditional coverage."],"url":"http://arxiv.org/abs/2403.04613v1","category":"stat.ME"}
{"created":"2024-03-07 15:58:03","title":"A Domain Translation Framework with an Adversarial Denoising Diffusion Model to Generate Synthetic Datasets of Echocardiography Images","abstract":"Currently, medical image domain translation operations show a high demand from researchers and clinicians. Amongst other capabilities, this task allows the generation of new medical images with sufficiently high image quality, making them clinically relevant. Deep Learning (DL) architectures, most specifically deep generative models, are widely used to generate and translate images from one domain to another. The proposed framework relies on an adversarial Denoising Diffusion Model (DDM) to synthesize echocardiography images and perform domain translation. Contrary to Generative Adversarial Networks (GANs), DDMs are able to generate high quality image samples with a large diversity. If a DDM is combined with a GAN, this ability to generate new data is completed at an even faster sampling time. In this work we trained an adversarial DDM combined with a GAN to learn the reverse denoising process, relying on a guide image, making sure relevant anatomical structures of each echocardiography image were kept and represented on the generated image samples. For several domain translation operations, the results verified that such generative model was able to synthesize high quality image samples: MSE: 11.50 +/- 3.69, PSNR (dB): 30.48 +/- 0.09, SSIM: 0.47 +/- 0.03. The proposed method showed high generalization ability, introducing a framework to create echocardiography images suitable to be used for clinical research purposes.","sentences":["Currently, medical image domain translation operations show a high demand from researchers and clinicians.","Amongst other capabilities, this task allows the generation of new medical images with sufficiently high image quality, making them clinically relevant.","Deep Learning (DL) architectures, most specifically deep generative models, are widely used to generate and translate images from one domain to another.","The proposed framework relies on an adversarial Denoising Diffusion Model (DDM) to synthesize echocardiography images and perform domain translation.","Contrary to Generative Adversarial Networks (GANs), DDMs are able to generate high quality image samples with a large diversity.","If a DDM is combined with a GAN, this ability to generate new data is completed at an even faster sampling time.","In this work we trained an adversarial DDM combined with a GAN to learn the reverse denoising process, relying on a guide image, making sure relevant anatomical structures of each echocardiography image were kept and represented on the generated image samples.","For several domain translation operations, the results verified that such generative model was able to synthesize high quality image samples: MSE: 11.50 +/- 3.69, PSNR (dB): 30.48 +/- 0.09, SSIM: 0.47 +/- 0.03.","The proposed method showed high generalization ability, introducing a framework to create echocardiography images suitable to be used for clinical research purposes."],"url":"http://arxiv.org/abs/2403.04612v1","category":"eess.IV"}
{"created":"2024-03-07 15:57:57","title":"Cavity-assisted resonance fluorescence from a nitrogen-vacancy center in diamond","abstract":"The nitrogen-vacancy center in diamond, owing to its optically addressable and long-lived electronic spin, is an attractive resource for the generation of remote entangled states. However, the center's low native fraction of coherent photon emission, $\\sim$3\\%, strongly reduces the achievable spin-photon entanglement rates. Here, we couple a nitrogen-vacancy center with a narrow extrinsically broadened linewidth (\\unit[159]{MHz}), hosted in a micron-thin membrane, to the mode of an open optical microcavity. The resulting Purcell factor of $\\sim$1.8 increases the fraction of zero-phonon line photons to above 44\\%, leading to coherent photon emission rates exceeding four times the state of the art under non-resonant excitation. Bolstered by the enhancement provided by the cavity, we for the first time measure resonance fluorescence without any temporal filtering with $>$10 signal-to-laser background ratio. Our microcavity platform would increase spin-spin entanglement success probabilities by more than an order of magnitude compared to existing implementations. Selective enhancement of the center's zero-phonon transitions could furthermore unlock efficient application of quantum optics techniques such as wave-packet shaping or all-optical spin manipulation.","sentences":["The nitrogen-vacancy center in diamond, owing to its optically addressable and long-lived electronic spin, is an attractive resource for the generation of remote entangled states.","However, the center's low native fraction of coherent photon emission, $\\sim$3\\%, strongly reduces the achievable spin-photon entanglement rates.","Here, we couple a nitrogen-vacancy center with a narrow extrinsically broadened linewidth (\\unit[159]{MHz}), hosted in a micron-thin membrane, to the mode of an open optical microcavity.","The resulting Purcell factor of $\\sim$1.8 increases the fraction of zero-phonon line photons to above 44\\%, leading to coherent photon emission rates exceeding four times the state of the art under non-resonant excitation.","Bolstered by the enhancement provided by the cavity, we for the first time measure resonance fluorescence without any temporal filtering with $>$10 signal-to-laser background ratio.","Our microcavity platform would increase spin-spin entanglement success probabilities by more than an order of magnitude compared to existing implementations.","Selective enhancement of the center's zero-phonon transitions could furthermore unlock efficient application of quantum optics techniques such as wave-packet shaping or all-optical spin manipulation."],"url":"http://arxiv.org/abs/2403.04611v1","category":"quant-ph"}
{"created":"2024-03-07 15:57:45","title":"Wondertopes","abstract":"Positive geometries were introduced by Arkani-Hamed--Bai--Lam as a method of computing scattering amplitudes in theoretical physics. We show that a positive geometry from a polytope admits a log resolution of singularities to another positive geometry. Our result states that the regions in a wonderful compactification of a hyperplane arrangement complement, which we call wondertopes, are positive geometries. A familiar wondertope is the curvy associahedron, which tiles the moduli space of pointed stable rational curves. Thus our work generalizes the known positive geometry structure on this moduli space.","sentences":["Positive geometries were introduced by Arkani-Hamed--Bai--Lam as a method of computing scattering amplitudes in theoretical physics.","We show that a positive geometry from a polytope admits a log resolution of singularities to another positive geometry.","Our result states that the regions in a wonderful compactification of a hyperplane arrangement complement, which we call wondertopes, are positive geometries.","A familiar wondertope is the curvy associahedron, which tiles the moduli space of pointed stable rational curves.","Thus our work generalizes the known positive geometry structure on this moduli space."],"url":"http://arxiv.org/abs/2403.04610v1","category":"math.AG"}
{"created":"2024-03-07 15:55:19","title":"A Market Mechanism for a Two-stage Settlement Electricity Market with Energy Storage","abstract":"Electricity markets typically clear in two stages: a day-ahead market and a real-time market. In this paper, we propose market mechanisms for a two-stage multi-interval electricity market with energy storage, generators, and demand uncertainties. We consider two possible mixed bidding strategies: storage first bids cycle depths in the day ahead and then charge-discharge power bids in the real-time market for any last-minute adjustments. While the first strategy only considers day-ahead decisions from an individual participant's perspective as part of their individual optimization formulation, the second strategy accounts for both the market operator's and participants' perspectives. We demonstrate that the competitive equilibrium exists uniquely for both mechanisms. However, accounting for the day-ahead decisions in the bidding function has several advantages. Numerical experiments using New York ISO data provide bounds on the proposed market mechanism.","sentences":["Electricity markets typically clear in two stages: a day-ahead market and a real-time market.","In this paper, we propose market mechanisms for a two-stage multi-interval electricity market with energy storage, generators, and demand uncertainties.","We consider two possible mixed bidding strategies: storage first bids cycle depths in the day ahead and then charge-discharge power bids in the real-time market for any last-minute adjustments.","While the first strategy only considers day-ahead decisions from an individual participant's perspective as part of their individual optimization formulation, the second strategy accounts for both the market operator's and participants' perspectives.","We demonstrate that the competitive equilibrium exists uniquely for both mechanisms.","However, accounting for the day-ahead decisions in the bidding function has several advantages.","Numerical experiments using New York ISO data provide bounds on the proposed market mechanism."],"url":"http://arxiv.org/abs/2403.04609v1","category":"math.OC"}
{"created":"2024-03-07 15:55:05","title":"Standardization of Cloth Objects and its Relevance in Robotic Manipulation","abstract":"The field of robotics faces inherent challenges in manipulating deformable objects, particularly in understanding and standardising fabric properties like elasticity, stiffness, and friction. While the significance of these properties is evident in the realm of cloth manipulation, accurately categorising and comprehending them in real-world applications remains elusive. This study sets out to address two primary objectives: (1) to provide a framework suitable for robotics applications to characterise cloth objects, and (2) to study how these properties influence robotic manipulation tasks. Our preliminary results validate the framework's ability to characterise cloth properties and compare cloth sets, and reveal the influence that different properties have on the outcome of five manipulation primitives. We believe that, in general, results on the manipulation of clothes should be reported along with a better description of the garments used in the evaluation. This paper proposes a set of these measures.","sentences":["The field of robotics faces inherent challenges in manipulating deformable objects, particularly in understanding and standardising fabric properties like elasticity, stiffness, and friction.","While the significance of these properties is evident in the realm of cloth manipulation, accurately categorising and comprehending them in real-world applications remains elusive.","This study sets out to address two primary objectives: (1) to provide a framework suitable for robotics applications to characterise cloth objects, and (2) to study how these properties influence robotic manipulation tasks.","Our preliminary results validate the framework's ability to characterise cloth properties and compare cloth sets, and reveal the influence that different properties have on the outcome of five manipulation primitives.","We believe that, in general, results on the manipulation of clothes should be reported along with a better description of the garments used in the evaluation.","This paper proposes a set of these measures."],"url":"http://arxiv.org/abs/2403.04608v1","category":"cs.RO"}
{"created":"2024-03-07 15:54:55","title":"Repelling-Attracting Hamiltonian Monte Carlo","abstract":"We propose a variant of Hamiltonian Monte Carlo (HMC), called the Repelling-Attracting Hamiltonian Monte Carlo (RAHMC), for sampling from multimodal distributions. The key idea that underpins RAHMC is a departure from the conservative dynamics of Hamiltonian systems, which form the basis of traditional HMC, and turning instead to the dissipative dynamics of conformal Hamiltonian systems. In particular, RAHMC involves two stages: a mode-repelling stage to encourage the sampler to move away from regions of high probability density; and, a mode-attracting stage, which facilitates the sampler to find and settle near alternative modes. We achieve this by introducing just one additional tuning parameter -- the coefficient of friction. The proposed method adapts to the geometry of the target distribution, e.g., modes and density ridges, and can generate proposals that cross low-probability barriers with little to no computational overhead in comparison to traditional HMC. Notably, RAHMC requires no additional information about the target distribution or memory of previously visited modes. We establish the theoretical basis for RAHMC, and we discuss repelling-attracting extensions to several variants of HMC in literature. Finally, we provide a tuning-free implementation via dual-averaging, and we demonstrate its effectiveness in sampling from, both, multimodal and unimodal distributions in high dimensions.","sentences":["We propose a variant of Hamiltonian Monte Carlo (HMC), called the Repelling-Attracting Hamiltonian Monte Carlo (RAHMC), for sampling from multimodal distributions.","The key idea that underpins RAHMC is a departure from the conservative dynamics of Hamiltonian systems, which form the basis of traditional HMC, and turning instead to the dissipative dynamics of conformal Hamiltonian systems.","In particular, RAHMC involves two stages: a mode-repelling stage to encourage the sampler to move away from regions of high probability density; and, a mode-attracting stage, which facilitates the sampler to find and settle near alternative modes.","We achieve this by introducing just one additional tuning parameter -- the coefficient of friction.","The proposed method adapts to the geometry of the target distribution, e.g., modes and density ridges, and can generate proposals that cross low-probability barriers with little to no computational overhead in comparison to traditional HMC.","Notably, RAHMC requires no additional information about the target distribution or memory of previously visited modes.","We establish the theoretical basis for RAHMC, and we discuss repelling-attracting extensions to several variants of HMC in literature.","Finally, we provide a tuning-free implementation via dual-averaging, and we demonstrate its effectiveness in sampling from, both, multimodal and unimodal distributions in high dimensions."],"url":"http://arxiv.org/abs/2403.04607v1","category":"math.ST"}
{"created":"2024-03-07 15:53:58","title":"Foundation for the \u0394SCF Approach in Density Functional Theory","abstract":"We extend ground-state density-functional theory to excited states and provide the theoretical formulation for the widely used $\\Delta SCF$ method for calculating excited-state energies and densities. As the electron density alone is insufficient to characterize excited states, we formulate excited-state theory using the defining variables of a noninteracting reference system, namely (1) the excitation quantum number $n_{s}$ and the potential $w_{s}(\\mathbf{r})$ (excited-state potential-functional theory, $n$PFT), (2) the noninteracting wavefunction $\\Phi$ ($\\Phi$-functional theory, $\\Phi$FT), or (3) the noninteracting one-electron reduced density matrix $\\gamma_{s}(\\mathbf{r},\\mathbf{r}')$ (density-matrix-functional theory, $\\gamma_{s}$FT). We show the equivalence of these three sets of variables and their corresponding energy functionals. Importantly, the ground and excited-state exchange-correlation energy use the \\textit{same} universal functional, regardless of whether $\\left(n_{s},w_{s}(\\boldsymbol{r})\\right)$, $\\Phi$, or $\\gamma_{s}(\\mathbf{r},\\mathbf{r}')$ is selected as the fundamental descriptor of the system. We derive the excited-state (generalized) Kohn-Sham equations. The minimum of all three functionals is the ground-state energy and, for ground states, they are all equivalent to the Hohenberg-Kohn-Sham method. The other stationary points of the functionals provide the excited-state energies and electron densities, establishing the foundation for the $\\Delta SCF$ method.","sentences":["We extend ground-state density-functional theory to excited states and provide the theoretical formulation for the widely used $\\Delta SCF$ method for calculating excited-state energies and densities.","As the electron density alone is insufficient to characterize excited states, we formulate excited-state theory using the defining variables of a noninteracting reference system, namely (1) the excitation quantum number $n_{s}$ and the potential $w_{s}(\\mathbf{r})$ (excited-state potential-functional theory, $n$PFT), (2) the noninteracting wavefunction $\\Phi$ ($\\Phi$-functional theory, $\\Phi$FT), or (3) the noninteracting one-electron reduced density matrix $\\gamma_{s}(\\mathbf{r},\\mathbf{r}')$ (density-matrix-functional theory, $\\gamma_{s}$FT).","We show the equivalence of these three sets of variables and their corresponding energy functionals.","Importantly, the ground and excited-state exchange-correlation energy use the \\textit{same} universal functional, regardless of whether $\\left(n_{s},w_{s}(\\boldsymbol{r})\\right)$, $\\Phi$, or $\\gamma_{s}(\\mathbf{r},\\mathbf{r}')$ is selected as the fundamental descriptor of the system.","We derive the excited-state (generalized) Kohn-Sham equations.","The minimum of all three functionals is the ground-state energy and, for ground states, they are all equivalent to the Hohenberg-Kohn-Sham method.","The other stationary points of the functionals provide the excited-state energies and electron densities, establishing the foundation for the $\\Delta SCF$ method."],"url":"http://arxiv.org/abs/2403.04604v1","category":"physics.chem-ph"}
{"created":"2024-03-07 15:36:29","title":"On the zeros of polyanalytic polynomials","abstract":"We give sufficient conditions under which a polyanalytic polynomial of degree $n$ has (i) at least one zero, and (ii) finitely many zeros. In the latter case, we prove that the number of zeros is bounded by $n^2$. We then show that for all $k \\in \\{0,\\dots, n^2, \\infty\\}$ there exists a polyanalytic polynomial of degree $n$ with exactly $k$ distinct zeros. Moreover, we generalize the Lagrange and Cauchy bounds from analytic to polyanalytic polynomials and obtain inclusion disks for the zeros. Finally, we construct a harmonic and thus polyanalytic polynomial of degree $n$ with $n$ nonzero coefficients and the maximum number of $n^2$ zeros.","sentences":["We give sufficient conditions under which a polyanalytic polynomial of degree $n$ has (i) at least one zero, and (ii) finitely many zeros.","In the latter case, we prove that the number of zeros is bounded by $n^2$. We then show that for all $k \\in \\{0,\\dots, n^2, \\infty\\}$ there exists a polyanalytic polynomial of degree $n$ with exactly $k$ distinct zeros.","Moreover, we generalize the Lagrange and Cauchy bounds from analytic to polyanalytic polynomials and obtain inclusion disks for the zeros.","Finally, we construct a harmonic and thus polyanalytic polynomial of degree $n$ with $n$ nonzero coefficients and the maximum number of $n^2$ zeros."],"url":"http://arxiv.org/abs/2403.04591v1","category":"math.CV"}
{"created":"2024-03-07 15:35:29","title":"Zero-shot cross-modal transfer of Reinforcement Learning policies through a Global Workspace","abstract":"Humans perceive the world through multiple senses, enabling them to create a comprehensive representation of their surroundings and to generalize information across domains. For instance, when a textual description of a scene is given, humans can mentally visualize it. In fields like robotics and Reinforcement Learning (RL), agents can also access information about the environment through multiple sensors; yet redundancy and complementarity between sensors is difficult to exploit as a source of robustness (e.g. against sensor failure) or generalization (e.g. transfer across domains). Prior research demonstrated that a robust and flexible multimodal representation can be efficiently constructed based on the cognitive science notion of a 'Global Workspace': a unique representation trained to combine information across modalities, and to broadcast its signal back to each modality. Here, we explore whether such a brain-inspired multimodal representation could be advantageous for RL agents. First, we train a 'Global Workspace' to exploit information collected about the environment via two input modalities (a visual input, or an attribute vector representing the state of the agent and/or its environment). Then, we train a RL agent policy using this frozen Global Workspace. In two distinct environments and tasks, our results reveal the model's ability to perform zero-shot cross-modal transfer between input modalities, i.e. to apply to image inputs a policy previously trained on attribute vectors (and vice-versa), without additional training or fine-tuning. Variants and ablations of the full Global Workspace (including a CLIP-like multimodal representation trained via contrastive learning) did not display the same generalization abilities.","sentences":["Humans perceive the world through multiple senses, enabling them to create a comprehensive representation of their surroundings and to generalize information across domains.","For instance, when a textual description of a scene is given, humans can mentally visualize it.","In fields like robotics and Reinforcement Learning (RL), agents can also access information about the environment through multiple sensors; yet redundancy and complementarity between sensors is difficult to exploit as a source of robustness (e.g. against sensor failure) or generalization (e.g. transfer across domains).","Prior research demonstrated that a robust and flexible multimodal representation can be efficiently constructed based on the cognitive science notion of a 'Global Workspace': a unique representation trained to combine information across modalities, and to broadcast its signal back to each modality.","Here, we explore whether such a brain-inspired multimodal representation could be advantageous for RL agents.","First, we train a 'Global Workspace' to exploit information collected about the environment via two input modalities (a visual input, or an attribute vector representing the state of the agent and/or its environment).","Then, we train a RL agent policy using this frozen Global Workspace.","In two distinct environments and tasks, our results reveal the model's ability to perform zero-shot cross-modal transfer between input modalities, i.e. to apply to image inputs a policy previously trained on attribute vectors (and vice-versa), without additional training or fine-tuning.","Variants and ablations of the full Global Workspace (including a CLIP-like multimodal representation trained via contrastive learning) did not display the same generalization abilities."],"url":"http://arxiv.org/abs/2403.04588v1","category":"cs.AI"}
{"created":"2024-03-07 15:33:41","title":"Direct observation of electronic band gap and hot carrier dynamics in GeAs semiconductor","abstract":"Germanium arsenide (GeAs) is a layered semiconductor with remarkably anisotropic physical, thermoelectric and optical properties, and a promising candidate for multifunctional devices based on in-plane polarization dependent response. Understanding the underlying mechanism of such devices requires the knowledge of GeAs electronic band structure and of the hot carrier dynamics in its conduction band, whose details are still unclear. In this work, we investigated the properties of occupied and photoexcited states of GeAs in energy-momentum space, by combining scanning tunneling spectroscopy (STS), angle-resolved photoemission spectroscopy (ARPES) and time-resolved ARPES. We found that, GeAs is an indirect gap semiconductor having an electronic gap of 0.8 eV, for which the conduction band minimum (CBM) is located at the Gamma point while the valence band maximum (VBM) is out of Gamma. A Stark broadening of the valence band is observed immediately after photoexcitation, which can be attributed to the effects of the electrical field at the surface induced by inhomogeneous screening. Moreover, the hot electrons relaxation time of 1.56 ps down to the CBM which is dominated from both inter-valley and intra-valley coupling. Besides their relevance for our understanding of GeAs, these findings present general interest for the design on high performance thermoelectric and optoelectronic devices based on 2D semiconductors.","sentences":["Germanium arsenide (GeAs) is a layered semiconductor with remarkably anisotropic physical, thermoelectric and optical properties, and a promising candidate for multifunctional devices based on in-plane polarization dependent response.","Understanding the underlying mechanism of such devices requires the knowledge of GeAs electronic band structure and of the hot carrier dynamics in its conduction band, whose details are still unclear.","In this work, we investigated the properties of occupied and photoexcited states of GeAs in energy-momentum space, by combining scanning tunneling spectroscopy (STS), angle-resolved photoemission spectroscopy (ARPES) and time-resolved ARPES.","We found that, GeAs is an indirect gap semiconductor having an electronic gap of 0.8 eV, for which the conduction band minimum (CBM) is located at the Gamma point while the valence band maximum (VBM) is out of Gamma.","A Stark broadening of the valence band is observed immediately after photoexcitation, which can be attributed to the effects of the electrical field at the surface induced by inhomogeneous screening.","Moreover, the hot electrons relaxation time of 1.56 ps down to the CBM which is dominated from both inter-valley and intra-valley coupling.","Besides their relevance for our understanding of GeAs, these findings present general interest for the design on high performance thermoelectric and optoelectronic devices based on 2D semiconductors."],"url":"http://arxiv.org/abs/2403.04587v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-07 15:30:54","title":"Learning Agility Adaptation for Flight in Clutter","abstract":"Animals learn to adapt agility of their movements to their capabilities and the environment they operate in. Mobile robots should also demonstrate this ability to combine agility and safety. The aim of this work is to endow flight vehicles with the ability of agility adaptation in prior unknown and partially observable cluttered environments. We propose a hierarchical learning and planning framework where we utilize both trial and error to comprehensively learn an agility policy with the vehicle's observation as the input, and well-established methods of model-based trajectory generation. Technically, we use online model-free reinforcement learning and a pre-training-fine-tuning reward scheme to obtain the deployable policy. The statistical results in simulation demonstrate the advantages of our method over the constant agility baselines and an alternative method in terms of flight efficiency and safety. In particular, the policy leads to intelligent behaviors, such as perception awareness, which distinguish it from other approaches. By deploying the policy to hardware, we verify that these advantages can be brought to the real world.","sentences":["Animals learn to adapt agility of their movements to their capabilities and the environment they operate in.","Mobile robots should also demonstrate this ability to combine agility and safety.","The aim of this work is to endow flight vehicles with the ability of agility adaptation in prior unknown and partially observable cluttered environments.","We propose a hierarchical learning and planning framework where we utilize both trial and error to comprehensively learn an agility policy with the vehicle's observation as the input, and well-established methods of model-based trajectory generation.","Technically, we use online model-free reinforcement learning and a pre-training-fine-tuning reward scheme to obtain the deployable policy.","The statistical results in simulation demonstrate the advantages of our method over the constant agility baselines and an alternative method in terms of flight efficiency and safety.","In particular, the policy leads to intelligent behaviors, such as perception awareness, which distinguish it from other approaches.","By deploying the policy to hardware, we verify that these advantages can be brought to the real world."],"url":"http://arxiv.org/abs/2403.04586v1","category":"cs.RO"}
{"created":"2024-03-07 15:29:48","title":"Heisenberg-Limited Quantum Metrology without Ancilla","abstract":"The asymptotic theory of quantum channel estimation has been well established, but in general noiseless and controllable ancilla is required for attaining the ultimate limit in the asymptotic regime. Little is known about the metrological performance without noiseless ancilla, which is more relevant in practical circumstances. In this work, we present a novel theoretical framework to address this problem, bridging quantum metrology and the asymptotic theory of quantum channels. Leveraging this framework, we prove sufficient conditions for achieving the Heisenberg limit with repeated application of the channel to estimate, both with and without applying interleaved unitary control operations. For the latter case, we design an algorithm to identify the control operation. Finally, we analyze several intriguing examples by our approach.","sentences":["The asymptotic theory of quantum channel estimation has been well established, but in general noiseless and controllable ancilla is required for attaining the ultimate limit in the asymptotic regime.","Little is known about the metrological performance without noiseless ancilla, which is more relevant in practical circumstances.","In this work, we present a novel theoretical framework to address this problem, bridging quantum metrology and the asymptotic theory of quantum channels.","Leveraging this framework, we prove sufficient conditions for achieving the Heisenberg limit with repeated application of the channel to estimate, both with and without applying interleaved unitary control operations.","For the latter case, we design an algorithm to identify the control operation.","Finally, we analyze several intriguing examples by our approach."],"url":"http://arxiv.org/abs/2403.04585v1","category":"quant-ph"}
{"created":"2024-03-07 15:27:31","title":"Memristive control of plasmon-mediated nonlinear photoluminescence in Au nanowires","abstract":"Nonlinear photoluminescence (N-PL) is a broadband photon emission arising from non-equilibrium electron distribution generated at the surface of metallic nanostructures by an ultrafast pulsed laser illumination. N-PL is sensitive to surface morphology, local electromagnetic field strength, and electronic band structure making it relevant to probe optically excited nanoscale plasmonic systems. It also has been key to access the complex multiscale time dynamics ruling electron thermalization. Here, we show that the surface plasmons mediated N-PL emitted by a gold nanowire can be modified by an electrical architecture featuring a nanogap. Upon voltage activation, we observe that N-PL becomes dependent to the electrical transport dynamics and can thus be locally modulated. This finding brings an electrical leverage to externally control the photoluminescence generated from metal nanostructures, and constitutes an asset for the development of emerging nanoscale interface devices managing photons and electrons.","sentences":["Nonlinear photoluminescence (N-PL) is a broadband photon emission arising from non-equilibrium electron distribution generated at the surface of metallic nanostructures by an ultrafast pulsed laser illumination.","N-PL is sensitive to surface morphology, local electromagnetic field strength, and electronic band structure making it relevant to probe optically excited nanoscale plasmonic systems.","It also has been key to access the complex multiscale time dynamics ruling electron thermalization.","Here, we show that the surface plasmons mediated N-PL emitted by a gold nanowire can be modified by an electrical architecture featuring a nanogap.","Upon voltage activation, we observe that N-PL becomes dependent to the electrical transport dynamics and can thus be locally modulated.","This finding brings an electrical leverage to externally control the photoluminescence generated from metal nanostructures, and constitutes an asset for the development of emerging nanoscale interface devices managing photons and electrons."],"url":"http://arxiv.org/abs/2403.04581v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-07 15:22:07","title":"Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition","abstract":"Web tables contain a large amount of valuable knowledge and have inspired tabular language models aimed at tackling table interpretation (TI) tasks. In this paper, we analyse a widely used benchmark dataset for evaluation of TI tasks, particularly focusing on the entity linking task. Our analysis reveals that this dataset is overly simplified, potentially reducing its effectiveness for thorough evaluation and failing to accurately represent tables as they appear in the real-world. To overcome this drawback, we construct and annotate a new more challenging dataset. In addition to introducing the new dataset, we also introduce a novel problem aimed at addressing the entity linking task: named entity recognition within cells. Finally, we propose a prompting framework for evaluating the newly developed large language models (LLMs) on this novel TI task. We conduct experiments on prompting LLMs under various settings, where we use both random and similarity-based selection to choose the examples presented to the models. Our ablation study helps us gain insights into the impact of the few-shot examples. Additionally, we perform qualitative analysis to gain insights into the challenges encountered by the models and to understand the limitations of the proposed dataset.","sentences":["Web tables contain a large amount of valuable knowledge and have inspired tabular language models aimed at tackling table interpretation (TI) tasks.","In this paper, we analyse a widely used benchmark dataset for evaluation of TI tasks, particularly focusing on the entity linking task.","Our analysis reveals that this dataset is overly simplified, potentially reducing its effectiveness for thorough evaluation and failing to accurately represent tables as they appear in the real-world.","To overcome this drawback, we construct and annotate a new more challenging dataset.","In addition to introducing the new dataset, we also introduce a novel problem aimed at addressing the entity linking task: named entity recognition within cells.","Finally, we propose a prompting framework for evaluating the newly developed large language models (LLMs) on this novel TI task.","We conduct experiments on prompting LLMs under various settings, where we use both random and similarity-based selection to choose the examples presented to the models.","Our ablation study helps us gain insights into the impact of the few-shot examples.","Additionally, we perform qualitative analysis to gain insights into the challenges encountered by the models and to understand the limitations of the proposed dataset."],"url":"http://arxiv.org/abs/2403.04577v1","category":"cs.AI"}
{"created":"2024-03-07 15:21:53","title":"Bijections between colored compositions, Dyck paths, and polygon partitions","abstract":"In this paper, we give part-preserving bijections between three fundamental families of objects that serve as natural framework for many problems in enumerative combinatorics. Specifically, we consider compositions, Dyck paths, and partitions of a convex polygon, and identify suitable building blocks that are then appropriately decorated to achieve matching cardinalities. Our bijections are constructive and apply for the general case where the building blocks are allowed to come in different colors.","sentences":["In this paper, we give part-preserving bijections between three fundamental families of objects that serve as natural framework for many problems in enumerative combinatorics.","Specifically, we consider compositions, Dyck paths, and partitions of a convex polygon, and identify suitable building blocks that are then appropriately decorated to achieve matching cardinalities.","Our bijections are constructive and apply for the general case where the building blocks are allowed to come in different colors."],"url":"http://arxiv.org/abs/2403.04575v1","category":"math.CO"}
{"created":"2024-03-07 15:21:40","title":"Children Age Group Detection based on Human-Computer Interaction and Time Series Analysis","abstract":"This article proposes a novel Children-Computer Interaction (CCI) approach for the task of age group detection. This approach focuses on the automatic analysis of the time series generated from the interaction of the children with mobile devices. In particular, we extract a set of 25 time series related to spatial, pressure, and kinematic information of the children interaction while colouring a tree through a pen stylus tablet, a specific test from the large-scale public ChildCIdb database.   A complete analysis of the proposed approach is carried out using different time series selection techniques to choose the most discriminative ones for the age group detection task: i) a statistical analysis, and ii) an automatic algorithm called Sequential Forward Search (SFS). In addition, different classification algorithms such as Dynamic Time Warping Barycenter Averaging (DBA) and Hidden Markov Models (HMM) are studied. Accuracy results over 85% are achieved, outperforming previous approaches in the literature and in more challenging age group conditions. Finally, the approach presented in this study can benefit many children-related applications, for example, towards an age-appropriate environment with the technology.","sentences":["This article proposes a novel Children-Computer Interaction (CCI) approach for the task of age group detection.","This approach focuses on the automatic analysis of the time series generated from the interaction of the children with mobile devices.","In particular, we extract a set of 25 time series related to spatial, pressure, and kinematic information of the children interaction while colouring a tree through a pen stylus tablet, a specific test from the large-scale public ChildCIdb database.   ","A complete analysis of the proposed approach is carried out using different time series selection techniques to choose the most discriminative ones for the age group detection task: i) a statistical analysis, and ii) an automatic algorithm called Sequential Forward Search (SFS).","In addition, different classification algorithms such as Dynamic Time Warping Barycenter Averaging (DBA) and Hidden Markov Models (HMM) are studied.","Accuracy results over 85% are achieved, outperforming previous approaches in the literature and in more challenging age group conditions.","Finally, the approach presented in this study can benefit many children-related applications, for example, towards an age-appropriate environment with the technology."],"url":"http://arxiv.org/abs/2403.04574v1","category":"cs.HC"}
{"created":"2024-03-07 15:13:32","title":"Topology and entanglement of molecular phase space","abstract":"We formulate a quantum phase space for molecular rotational and nuclear-spin states. Taking in molecular geometry and nuclear-spin data, our framework yields admissible position and momentum states, inter-convertible via a generalized Fourier transform. We classify molecules into three types -- asymmetric, rotationally symmetric, and perrotationally symmetric -- with the last type having no macroscopic analogue due to nuclear-spin statistics constraints. We identify two features in perrotationally symmetric state spaces that are Hamiltonian-independent and induced solely by symmetry and spin statistics. First, many molecular species are intrinsically rotation-spin entangled in a way that cannot be broken without transitioning to another species or breaking symmetry. Second, each molecular position state houses an internal pseudo-spin or \"fiber\" degree of freedom, and the fiber's Berry phase or matrix after adiabatic changes in position yields naturally robust operations, akin to braiding anyonic quasiparticles or realizing fault-tolerant quantum gates. We outline scenarios where these features can be experimentally probed.","sentences":["We formulate a quantum phase space for molecular rotational and nuclear-spin states.","Taking in molecular geometry and nuclear-spin data, our framework yields admissible position and momentum states, inter-convertible via a generalized Fourier transform.","We classify molecules into three types -- asymmetric, rotationally symmetric, and perrotationally symmetric -- with the last type having no macroscopic analogue due to nuclear-spin statistics constraints.","We identify two features in perrotationally symmetric state spaces that are Hamiltonian-independent and induced solely by symmetry and spin statistics.","First, many molecular species are intrinsically rotation-spin entangled in a way that cannot be broken without transitioning to another species or breaking symmetry.","Second, each molecular position state houses an internal pseudo-spin or \"fiber\" degree of freedom, and the fiber's Berry phase or matrix after adiabatic changes in position yields naturally robust operations, akin to braiding anyonic quasiparticles or realizing fault-tolerant quantum gates.","We outline scenarios where these features can be experimentally probed."],"url":"http://arxiv.org/abs/2403.04572v1","category":"quant-ph"}
{"created":"2024-03-07 15:12:06","title":"Machine learning and information theory concepts towards an AI Mathematician","abstract":"The current state-of-the-art in artificial intelligence is impressive, especially in terms of mastery of language, but not so much in terms of mathematical reasoning. What could be missing? Can we learn something useful about that gap from how the brains of mathematicians go about their craft? This essay builds on the idea that current deep learning mostly succeeds at system 1 abilities -- which correspond to our intuition and habitual behaviors -- but still lacks something important regarding system 2 abilities -- which include reasoning and robust uncertainty estimation. It takes an information-theoretical posture to ask questions about what constitutes an interesting mathematical statement, which could guide future work in crafting an AI mathematician. The focus is not on proving a given theorem but on discovering new and interesting conjectures. The central hypothesis is that a desirable body of theorems better summarizes the set of all provable statements, for example by having a small description length while at the same time being close (in terms of number of derivation steps) to many provable statements.","sentences":["The current state-of-the-art in artificial intelligence is impressive, especially in terms of mastery of language, but not so much in terms of mathematical reasoning.","What could be missing?","Can we learn something useful about that gap from how the brains of mathematicians go about their craft?","This essay builds on the idea that current deep learning mostly succeeds at system 1 abilities -- which correspond to our intuition and habitual behaviors -- but still lacks something important regarding system 2 abilities -- which include reasoning and robust uncertainty estimation.","It takes an information-theoretical posture to ask questions about what constitutes an interesting mathematical statement, which could guide future work in crafting an AI mathematician.","The focus is not on proving a given theorem but on discovering new and interesting conjectures.","The central hypothesis is that a desirable body of theorems better summarizes the set of all provable statements, for example by having a small description length while at the same time being close (in terms of number of derivation steps) to many provable statements."],"url":"http://arxiv.org/abs/2403.04571v1","category":"cs.AI"}
{"created":"2024-03-07 15:02:20","title":"Pullback of arithmetic theta series and its modularity for unitary Shimura curves","abstract":"This paper is a complement of the modularity result of Bruinier, Howard, Kudla, Rapoport and Yang (BHKRY) for the special case $U(1,1)$ not considered there. The main idea to embed a $U(1, 1)$ Shimura curve to many $U(n-1, 1)$ Shimura varieties for big $n$, and prove a precise pullback formula of the generating series of arithmetic divisors. Afterwards, we use the modularity result of BHKRY together with existence of non-vanishing of classical theta series at any given point in the upper half plane to prove the modulartiy result on $U(1, 1)$ Shimura curves.","sentences":["This paper is a complement of the modularity result of Bruinier, Howard, Kudla, Rapoport and Yang (BHKRY) for the special case $U(1,1)$ not considered there.","The main idea to embed a $U(1, 1)$ Shimura curve to many $U(n-1, 1)$ Shimura varieties for big $n$, and prove a precise pullback formula of the generating series of arithmetic divisors.","Afterwards, we use the modularity result of BHKRY together with existence of non-vanishing of classical theta series at any given point in the upper half plane to prove the modulartiy result on $U(1, 1)$ Shimura curves."],"url":"http://arxiv.org/abs/2403.04566v1","category":"math.NT"}
{"created":"2024-03-07 15:01:31","title":"On the Unipotent $p$-adic Simpson Correspondence","abstract":"The goal of this paper is to show a (derived) $p$-adic Simpson correspondence for (locally) unipotent coefficients on smooth rigid-analytic varieties. Our results depend on a deformation to $\\mathbf{B}_\\mathtt{dr}^+/\\xi^2$, and not on a choice of exponential (as required for more general coefficients). Our methods are inherently higher categorical, hinging on the theory of modules over $\\mathbf{E}_\\infty$-algebras.","sentences":["The goal of this paper is to show a (derived) $p$-adic Simpson correspondence for (locally) unipotent coefficients on smooth rigid-analytic varieties.","Our results depend on a deformation to $\\mathbf{B}_\\mathtt{dr}^+/\\xi^2$, and not on a choice of exponential (as required for more general coefficients).","Our methods are inherently higher categorical, hinging on the theory of modules over $\\mathbf{E}_\\infty$-algebras."],"url":"http://arxiv.org/abs/2403.04565v1","category":"math.AG"}
{"created":"2024-03-07 14:59:34","title":"Out of the Room: Generalizing Event-Based Dynamic Motion Segmentation for Complex Scenes","abstract":"Rapid and reliable identification of dynamic scene parts, also known as motion segmentation, is a key challenge for mobile sensors. Contemporary RGB camera-based methods rely on modeling camera and scene properties however, are often under-constrained and fall short in unknown categories. Event cameras have the potential to overcome these limitations, but corresponding methods have only been demonstrated in smaller-scale indoor environments with simplified dynamic objects. This work presents an event-based method for class-agnostic motion segmentation that can successfully be deployed across complex large-scale outdoor environments too. To this end, we introduce a novel divide-and-conquer pipeline that combines: (a) ego-motion compensated events, computed via a scene understanding module that predicts monocular depth and camera pose as auxiliary tasks, and (b) optical flow from a dedicated optical flow module. These intermediate representations are then fed into a segmentation module that predicts motion segmentation masks. A novel transformer-based temporal attention module in the segmentation module builds correlations across adjacent 'frames' to get temporally consistent segmentation masks. Our method sets the new state-of-the-art on the classic EV-IMO benchmark (indoors), where we achieve improvements of 2.19 moving object IoU (2.22 mIoU) and 4.52 point IoU respectively, as well as on a newly-generated motion segmentation and tracking benchmark (outdoors) based on the DSEC event dataset, termed DSEC-MOTS, where we show improvement of 12.91 moving object IoU.","sentences":["Rapid and reliable identification of dynamic scene parts, also known as motion segmentation, is a key challenge for mobile sensors.","Contemporary RGB camera-based methods rely on modeling camera and scene properties however, are often under-constrained and fall short in unknown categories.","Event cameras have the potential to overcome these limitations, but corresponding methods have only been demonstrated in smaller-scale indoor environments with simplified dynamic objects.","This work presents an event-based method for class-agnostic motion segmentation that can successfully be deployed across complex large-scale outdoor environments too.","To this end, we introduce a novel divide-and-conquer pipeline that combines: (a) ego-motion compensated events, computed via a scene understanding module that predicts monocular depth and camera pose as auxiliary tasks, and (b) optical flow from a dedicated optical flow module.","These intermediate representations are then fed into a segmentation module that predicts motion segmentation masks.","A novel transformer-based temporal attention module in the segmentation module builds correlations across adjacent 'frames' to get temporally consistent segmentation masks.","Our method sets the new state-of-the-art on the classic EV-IMO benchmark (indoors), where we achieve improvements of 2.19 moving object IoU (2.22 mIoU) and 4.52 point IoU respectively, as well as on a newly-generated motion segmentation and tracking benchmark (outdoors) based on the DSEC event dataset, termed DSEC-MOTS, where we show improvement of 12.91 moving object IoU."],"url":"http://arxiv.org/abs/2403.04562v1","category":"cs.CV"}
{"created":"2024-03-07 14:58:38","title":"Einstein-Podolsky-Rosen correlations in spontaneous parametric down-conversion: Beyond the Gaussian approximation","abstract":"We present analytic expressions for the coincidence detection probability amplitudes of photon pairs generated by spontaneous parametric down-conversion in both momentum and position spaces, without making use of the Gaussian approximation, and taking into account the effects of birefringence in the nonlinear crystal. We also present experimental data supporting our theoretical predictions, using Einstein-Podolsky-Rosen correlations as benchmarks, for 8 different pump beam configurations.","sentences":["We present analytic expressions for the coincidence detection probability amplitudes of photon pairs generated by spontaneous parametric down-conversion in both momentum and position spaces, without making use of the Gaussian approximation, and taking into account the effects of birefringence in the nonlinear crystal.","We also present experimental data supporting our theoretical predictions, using Einstein-Podolsky-Rosen correlations as benchmarks, for 8 different pump beam configurations."],"url":"http://arxiv.org/abs/2403.04561v1","category":"quant-ph"}
{"created":"2024-03-07 14:56:18","title":"A generalization of quantum Lakshmibai-Seshadri paths for arbitrary weights","abstract":"We construct an injective weight-preserving map (called the forgetful map) from the set of all admissible subsets in the quantum alcove model associated to an arbitrary weight. The image of this forgetful map can be explicitly described by introducing the notion of \"interpolated quantum Lakshmibai-Seshadri (QLS for short) paths\", which can be thought of as a generalization of quantum Lakshmibai-Seshadri paths. As an application, we reformulate, in terms of interpolated QLS paths, an identity of Chevalley type for the graded characters of Demazure submodules of a level-zero extremal weight module over a quantum affine algebra, which is a representation-theoretic analog of the Chevalley formula for the torus-equivariant $K$-group of a semi-infinite flag manifold.","sentences":["We construct an injective weight-preserving map (called the forgetful map) from the set of all admissible subsets in the quantum alcove model associated to an arbitrary weight.","The image of this forgetful map can be explicitly described by introducing the notion of \"interpolated quantum Lakshmibai-Seshadri (QLS for short) paths\", which can be thought of as a generalization of quantum Lakshmibai-Seshadri paths.","As an application, we reformulate, in terms of interpolated QLS paths, an identity of Chevalley type for the graded characters of Demazure submodules of a level-zero extremal weight module over a quantum affine algebra, which is a representation-theoretic analog of the Chevalley formula for the torus-equivariant $K$-group of a semi-infinite flag manifold."],"url":"http://arxiv.org/abs/2403.04560v1","category":"math.CO"}
{"created":"2024-03-07 14:56:06","title":"Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology","abstract":"Deep Learning models have been successfully utilized to extract clinically actionable insights from routinely available histology data. Generally, these models require annotations performed by clinicians, which are scarce and costly to generate. The emergence of self-supervised learning (SSL) methods remove this barrier, allowing for large-scale analyses on non-annotated data. However, recent SSL approaches apply increasingly expansive model architectures and larger datasets, causing the rapid escalation of data volumes, hardware prerequisites, and overall expenses, limiting access to these resources to few institutions. Therefore, we investigated the complexity of contrastive SSL in computational pathology in relation to classification performance with the utilization of consumer-grade hardware. Specifically, we analyzed the effects of adaptations in data volume, architecture, and algorithms on downstream classification tasks, emphasizing their impact on computational resources. We trained breast cancer foundation models on a large public patient cohort and validated them on various downstream classification tasks in a weakly supervised manner on two external public patient cohorts. Our experiments demonstrate that we can improve downstream classification performance whilst reducing SSL training duration by 90%. In summary, we propose a set of adaptations which enable the utilization of SSL in computational pathology in non-resource abundant environments.","sentences":["Deep Learning models have been successfully utilized to extract clinically actionable insights from routinely available histology data.","Generally, these models require annotations performed by clinicians, which are scarce and costly to generate.","The emergence of self-supervised learning (SSL) methods remove this barrier, allowing for large-scale analyses on non-annotated data.","However, recent SSL approaches apply increasingly expansive model architectures and larger datasets, causing the rapid escalation of data volumes, hardware prerequisites, and overall expenses, limiting access to these resources to few institutions.","Therefore, we investigated the complexity of contrastive SSL in computational pathology in relation to classification performance with the utilization of consumer-grade hardware.","Specifically, we analyzed the effects of adaptations in data volume, architecture, and algorithms on downstream classification tasks, emphasizing their impact on computational resources.","We trained breast cancer foundation models on a large public patient cohort and validated them on various downstream classification tasks in a weakly supervised manner on two external public patient cohorts.","Our experiments demonstrate that we can improve downstream classification performance whilst reducing SSL training duration by 90%.","In summary, we propose a set of adaptations which enable the utilization of SSL in computational pathology in non-resource abundant environments."],"url":"http://arxiv.org/abs/2403.04558v1","category":"cs.LG"}
{"created":"2024-03-07 14:43:17","title":"CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?","abstract":"We study the effectiveness of data-balancing for mitigating biases in contrastive language-image pretraining (CLIP), identifying areas of strength and limitation. First, we reaffirm prior conclusions that CLIP models can inadvertently absorb societal stereotypes. To counter this, we present a novel algorithm, called Multi-Modal Moment Matching (M4), designed to reduce both representation and association biases (i.e. in first- and second-order statistics) in multimodal data. We use M4 to conduct an in-depth analysis taking into account various factors, such as the model, representation, and data size. Our study also explores the dynamic nature of how CLIP learns and unlearns biases. In particular, we find that fine-tuning is effective in countering representation biases, though its impact diminishes for association biases. Also, data balancing has a mixed impact on quality: it tends to improve classification but can hurt retrieval. Interestingly, data and architectural improvements seem to mitigate the negative impact of data balancing on performance; e.g. applying M4 to SigLIP-B/16 with data quality filters improves COCO image-to-text retrieval @5 from 86% (without data balancing) to 87% and ImageNet 0-shot classification from 77% to 77.5%! Finally, we conclude with recommendations for improving the efficacy of data balancing in multimodal systems.","sentences":["We study the effectiveness of data-balancing for mitigating biases in contrastive language-image pretraining (CLIP), identifying areas of strength and limitation.","First, we reaffirm prior conclusions that CLIP models can inadvertently absorb societal stereotypes.","To counter this, we present a novel algorithm, called Multi-Modal Moment Matching (M4), designed to reduce both representation and association biases (i.e. in first- and second-order statistics) in multimodal data.","We use M4 to conduct an in-depth analysis taking into account various factors, such as the model, representation, and data size.","Our study also explores the dynamic nature of how CLIP learns and unlearns biases.","In particular, we find that fine-tuning is effective in countering representation biases, though its impact diminishes for association biases.","Also, data balancing has a mixed impact on quality: it tends to improve classification but can hurt retrieval.","Interestingly, data and architectural improvements seem to mitigate the negative impact of data balancing on performance; e.g. applying M4 to SigLIP-B/16 with data quality filters improves COCO image-to-text retrieval @5 from 86% (without data balancing) to 87% and ImageNet 0-shot classification from 77% to 77.5%!","Finally, we conclude with recommendations for improving the efficacy of data balancing in multimodal systems."],"url":"http://arxiv.org/abs/2403.04547v1","category":"cs.LG"}
{"created":"2024-03-07 14:40:53","title":"Improve Generalization Ability of Deep Wide Residual Network with A Suitable Scaling Factor","abstract":"Deep Residual Neural Networks (ResNets) have demonstrated remarkable success across a wide range of real-world applications. In this paper, we identify a suitable scaling factor (denoted by $\\alpha$) on the residual branch of deep wide ResNets to achieve good generalization ability. We show that if $\\alpha$ is a constant, the class of functions induced by Residual Neural Tangent Kernel (RNTK) is asymptotically not learnable, as the depth goes to infinity. We also highlight a surprising phenomenon: even if we allow $\\alpha$ to decrease with increasing depth $L$, the degeneration phenomenon may still occur. However, when $\\alpha$ decreases rapidly with $L$, the kernel regression with deep RNTK with early stopping can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space associated with the infinite-depth RNTK. Our simulation studies on synthetic data and real classification tasks such as MNIST, CIFAR10 and CIFAR100 support our theoretical criteria for choosing $\\alpha$.","sentences":["Deep Residual Neural Networks (ResNets) have demonstrated remarkable success across a wide range of real-world applications.","In this paper, we identify a suitable scaling factor (denoted by $\\alpha$) on the residual branch of deep wide ResNets to achieve good generalization ability.","We show that if $\\alpha$ is a constant, the class of functions induced by Residual Neural Tangent Kernel (RNTK) is asymptotically not learnable, as the depth goes to infinity.","We also highlight a surprising phenomenon: even if we allow $\\alpha$ to decrease with increasing depth $L$, the degeneration phenomenon may still occur.","However, when $\\alpha$ decreases rapidly with $L$, the kernel regression with deep RNTK with early stopping can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space associated with the infinite-depth RNTK.","Our simulation studies on synthetic data and real classification tasks such as MNIST, CIFAR10 and CIFAR100 support our theoretical criteria for choosing $\\alpha$."],"url":"http://arxiv.org/abs/2403.04545v1","category":"cs.LG"}
{"created":"2024-03-07 14:40:24","title":"On products of K-moduli spaces","abstract":"We study the K-moduli space of products of Fano varieties in relation to the product of K-moduli spaces of the product components. We show that there exists a well-defined morphism from the product of K-moduli stacks of Fano varieties to the K-moduli stack of their product. We further show that this morphism is an isomorphism if the two varieties are non-isomorphic, and a torsor if they are. Our results rely on the theory of stacks, and we extend them to general Artin stacks admitting good moduli spaces with some conditions we specify, and to the K-moduli spaces of log Fano pairs. We use our main result to obtain an explicit description of the K-moduli stack/ space of Fano threefolds with Picard rank greater than 6, along with a wall-crossing description, and a detailed polyhedral wall-crossing description for K-moduli of log Fano pairs.","sentences":["We study the K-moduli space of products of Fano varieties in relation to the product of K-moduli spaces of the product components.","We show that there exists a well-defined morphism from the product of K-moduli stacks of Fano varieties to the K-moduli stack of their product.","We further show that this morphism is an isomorphism if the two varieties are non-isomorphic, and a torsor if they are.","Our results rely on the theory of stacks, and we extend them to general Artin stacks admitting good moduli spaces with some conditions we specify, and to the K-moduli spaces of log Fano pairs.","We use our main result to obtain an explicit description of the K-moduli stack/ space of Fano threefolds with Picard rank greater than 6, along with a wall-crossing description, and a detailed polyhedral wall-crossing description for K-moduli of log Fano pairs."],"url":"http://arxiv.org/abs/2403.04544v1","category":"math.AG"}
{"created":"2024-03-07 14:40:16","title":"Poisson equation with measure data, reconstruction formula and Doob classes of processes","abstract":"We consider the Dirichlet problem for equation involving a general operator associated with a symmetric transient regular Dirichlet form and bounded Borel measure on the right-hand side of the equation. We introduce a new function space (depending on the form) which allows us to distinguish between solutions with diffuse measure and with general Borel measure. This new space can by characterized analytically in terms of the Poisson kernel associated with the underlying operator or probabilistically by using the notion of Doob class (D) of processes naturally associated with the operator. We also prove a reconstruction formula describing, in terms of the carr\\'e du champ operator and jump measure associated with the underlying form, the behaviour of the solution on the set where it is very large.","sentences":["We consider the Dirichlet problem for equation involving a general operator associated with a symmetric transient regular Dirichlet form and bounded Borel measure on the right-hand side of the equation.","We introduce a new function space (depending on the form) which allows us to distinguish between solutions with diffuse measure and with general Borel measure.","This new space can by characterized analytically in terms of the Poisson kernel associated with the underlying operator or probabilistically by using the notion of Doob class (D) of processes naturally associated with the operator.","We also prove a reconstruction formula describing, in terms of the carr\\'e du champ operator and jump measure associated with the underlying form, the behaviour of the solution on the set where it is very large."],"url":"http://arxiv.org/abs/2403.04543v1","category":"math.AP"}
{"created":"2024-03-07 14:40:07","title":"A Simple and Near-Optimal Algorithm for Directed Expander Decompositions","abstract":"In this work, we present the first algorithm to compute expander decompositions in an $m$-edge directed graph with near-optimal time $\\tilde{O}(m)$. Further, our algorithm can maintain such a decomposition in a dynamic graph and again obtains near-optimal update times. Our result improves over previous algorithms of Bernstein-Probst Gutenberg-Saranurak (FOCS 2020), Hua-Kyng-Probst Gutenberg-Wu (SODA 2023) that only obtained algorithms optimal up to subpolynomial factors. At the same time, our algorithm is much simpler and more accessible than previous work. In order to obtain our new algorithm, we present a new push-pull-relabel flow framework that generalizes the classic push-relabel flow algorithm of Goldberg-Tarjan (JACM 1988), which was later dynamized for computing expander decompositions in undirected graphs by Henzinger-Rao-Wang (SIAM J. Comput. 2020), Saranurak-Wang (SODA 2019). We then show that the flow problems formulated in recent work of Hua-Kyng-Probst Gutenberg-Wu (SODA 2023) to decompose directed graphs can be solved much more efficiently in the push-pull-relabel flow framework.","sentences":["In this work, we present the first algorithm to compute expander decompositions in an $m$-edge directed graph with near-optimal time $\\tilde{O}(m)$. Further, our algorithm can maintain such a decomposition in a dynamic graph and again obtains near-optimal update times.","Our result improves over previous algorithms of Bernstein-Probst Gutenberg-Saranurak (FOCS 2020), Hua-Kyng-Probst Gutenberg-Wu (SODA 2023) that only obtained algorithms optimal up to subpolynomial factors.","At the same time, our algorithm is much simpler and more accessible than previous work.","In order to obtain our new algorithm, we present a new push-pull-relabel flow framework that generalizes the classic push-relabel flow algorithm of Goldberg-Tarjan (JACM 1988), which was later dynamized for computing expander decompositions in undirected graphs by Henzinger-Rao-Wang (SIAM J. Comput. 2020), Saranurak-Wang (SODA 2019).","We then show that the flow problems formulated in recent work of Hua-Kyng-Probst Gutenberg-Wu (SODA 2023) to decompose directed graphs can be solved much more efficiently in the push-pull-relabel flow framework."],"url":"http://arxiv.org/abs/2403.04542v1","category":"cs.DS"}
{"created":"2024-03-07 14:36:52","title":"Towards Automatic Composition of ASP Programs from Natural Language Specifications","abstract":"This paper moves the first step towards automating the composition of Answer Set Programming (ASP) specifications. In particular, the following contributions are provided: (i) A dataset focused on graph-related problem specifications, designed to develop and assess tools for ASP automatic coding; (ii) A two-step architecture, implemented in the NL2ASP tool, for generating ASP programs from natural language specifications. NL2ASP uses neural machine translation to transform natural language into Controlled Natural Language (CNL) statements. Subsequently, CNL statements are converted into ASP code using the CNL2ASP tool. An experiment confirms the viability of the approach.","sentences":["This paper moves the first step towards automating the composition of Answer Set Programming (ASP) specifications.","In particular, the following contributions are provided: (i) A dataset focused on graph-related problem specifications, designed to develop and assess tools for ASP automatic coding; (ii) A two-step architecture, implemented in the NL2ASP tool, for generating ASP programs from natural language specifications.","NL2ASP uses neural machine translation to transform natural language into Controlled Natural Language (CNL) statements.","Subsequently, CNL statements are converted into ASP code using the CNL2ASP tool.","An experiment confirms the viability of the approach."],"url":"http://arxiv.org/abs/2403.04541v1","category":"cs.AI"}
{"created":"2024-03-07 14:35:03","title":"A stochastic optimisation unadjusted Langevin method for empirical Bayesian estimation in semi-blind image deblurring problems","abstract":"This paper presents a novel stochastic optimisation methodology to perform empirical Bayesian inference in semi-blind image deconvolution problems. Given a blurred image and a parametric class of possible operators, the proposed optimisation approach automatically calibrates the parameters of the blur model by maximum marginal likelihood estimation, followed by (non-blind) image deconvolution by maximum-a-posteriori estimation conditionally to the estimated model parameters. In addition to the blur model, the proposed approach also automatically calibrates the noise variance as well as any regularisation parameters. The marginal likelihood of the blur, noise variance, and regularisation parameters is generally computationally intractable, as it requires calculating several integrals over the entire solution space. Our approach addresses this difficulty by using a stochastic approximation proximal gradient optimisation scheme, which iteratively solves such integrals by using a Moreau-Yosida regularised unadjusted Langevin Markov chain Monte Carlo algorithm. This optimisation strategy can be easily and efficiently applied to any model that is log-concave, and by using the same gradient and proximal operators that are required to compute the maximum-a-posteriori solution by convex optimisation. We provide convergence guarantees for the proposed optimisation scheme under realistic and easily verifiable conditions and subsequently demonstrate the effectiveness of the approach with a series of deconvolution experiments and comparisons with alternative strategies from the state of the art.","sentences":["This paper presents a novel stochastic optimisation methodology to perform empirical Bayesian inference in semi-blind image deconvolution problems.","Given a blurred image and a parametric class of possible operators, the proposed optimisation approach automatically calibrates the parameters of the blur model by maximum marginal likelihood estimation, followed by (non-blind) image deconvolution by maximum-a-posteriori estimation conditionally to the estimated model parameters.","In addition to the blur model, the proposed approach also automatically calibrates the noise variance as well as any regularisation parameters.","The marginal likelihood of the blur, noise variance, and regularisation parameters is generally computationally intractable, as it requires calculating several integrals over the entire solution space.","Our approach addresses this difficulty by using a stochastic approximation proximal gradient optimisation scheme, which iteratively solves such integrals by using a Moreau-Yosida regularised unadjusted Langevin Markov chain Monte Carlo algorithm.","This optimisation strategy can be easily and efficiently applied to any model that is log-concave, and by using the same gradient and proximal operators that are required to compute the maximum-a-posteriori solution by convex optimisation.","We provide convergence guarantees for the proposed optimisation scheme under realistic and easily verifiable conditions and subsequently demonstrate the effectiveness of the approach with a series of deconvolution experiments and comparisons with alternative strategies from the state of the art."],"url":"http://arxiv.org/abs/2403.04536v1","category":"stat.AP"}
{"created":"2024-03-07 14:34:00","title":"Quandle Coloring Quivers of general Torus links by dihedral quandles","abstract":"We completely characterize the coloring quivers of general torus links by dihedral quandles by first exhausting all possible numbers of colorings, followed by determining the interconnections between colorings in each case. The quiver is obtained as function of the number of colorings. The quiver always contains complete subgraphs, in particular a complete subgraph corresponding to the trivial colorings, but the total number of subgraphs in the quiver and the weights of their edges varies depending on the number of colorings.","sentences":["We completely characterize the coloring quivers of general torus links by dihedral quandles by first exhausting all possible numbers of colorings, followed by determining the interconnections between colorings in each case.","The quiver is obtained as function of the number of colorings.","The quiver always contains complete subgraphs, in particular a complete subgraph corresponding to the trivial colorings, but the total number of subgraphs in the quiver and the weights of their edges varies depending on the number of colorings."],"url":"http://arxiv.org/abs/2403.04534v1","category":"math.GT"}
{"created":"2024-03-07 14:33:34","title":"Anisotropic Hall effects in Bi$_2$Se$_3$/EuS interfaces","abstract":"Proximity coupling of ferromagnetic insulator EuS to the topological insulator Bi$_2$Se$_3$ has been proposed to break time-reversal symmetry near the surface of Bi$_2$Se$_3$, introducing an energy gap or a tilt in the surface Dirac cone. As an inverse proximity effect, strong spin-orbit coupling available in the topological surface states can enhance the Curie temperature of ferromagnetism in EuS largely beyond its bulk value, and also generate a magnetic anisotropy. This can result in a canting of the magnetic moment of Eu ions in a plane perpendicular to the interface. Here, we investigate theoretically electronic transport properties arising from the Bi$_2$Se$_3$/EuS interfaces in the planar Hall geometry. Our analysis, based on a realistic model Hamiltonian and a semi-classical formalism for the Boltzmann transport equation, reveals distinct intriguing features of anisotropic planar Hall conductivity, depending on different scenarios for the canting of the Eu moments: fixed Eu moment canting, and freely-orientable Eu moment in response to the external in-plane magnetic field. The anisotropy in the planar Hall conductivity arises from the asymmetric Berry curvature of the gapped topological surface states. We also explore topological Hall effect of the Dirac surface states, coupled to a skyrmion crystal which can emerge in the EuS due to the interplay of ferromagnetic Heisenberg exchange, interfacial Dzyaloshinskii-Moriya interaction, and perpendicular alignment of the Eu moment. Our study provides new impetus for probing complex interplay between magnetic exchange interactions and topological surface states via anisotropic planar Hall effects.","sentences":["Proximity coupling of ferromagnetic insulator EuS to the topological insulator Bi$_2$Se$_3$ has been proposed to break time-reversal symmetry near the surface of Bi$_2$Se$_3$, introducing an energy gap or a tilt in the surface Dirac cone.","As an inverse proximity effect, strong spin-orbit coupling available in the topological surface states can enhance the Curie temperature of ferromagnetism in EuS largely beyond its bulk value, and also generate a magnetic anisotropy.","This can result in a canting of the magnetic moment of Eu ions in a plane perpendicular to the interface.","Here, we investigate theoretically electronic transport properties arising from the Bi$_2$Se$_3$/EuS interfaces in the planar Hall geometry.","Our analysis, based on a realistic model Hamiltonian and a semi-classical formalism for the Boltzmann transport equation, reveals distinct intriguing features of anisotropic planar Hall conductivity, depending on different scenarios for the canting of the Eu moments: fixed Eu moment canting, and freely-orientable Eu moment in response to the external in-plane magnetic field.","The anisotropy in the planar Hall conductivity arises from the asymmetric Berry curvature of the gapped topological surface states.","We also explore topological Hall effect of the Dirac surface states, coupled to a skyrmion crystal which can emerge in the EuS due to the interplay of ferromagnetic Heisenberg exchange, interfacial Dzyaloshinskii-Moriya interaction, and perpendicular alignment of the Eu moment.","Our study provides new impetus for probing complex interplay between magnetic exchange interactions and topological surface states via anisotropic planar Hall effects."],"url":"http://arxiv.org/abs/2403.04533v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-07 14:29:29","title":"Anatomy-Guided Surface Diffusion Model for Alzheimer's Disease Normative Modeling","abstract":"Normative modeling has emerged as a pivotal approach for characterizing heterogeneity and individual variance in neurodegenerative diseases, notably Alzheimer's disease(AD). One of the challenges of cortical normative modeling is the anatomical structure mismatch due to folding pattern variability. Traditionally, registration is applied to address this issue and recently many studies have utilized deep generative models to generate anatomically align samples for analyzing disease progression; however, these models are predominantly applied to volume-based data, which often falls short in capturing intricate morphological changes on the brain cortex. As an alternative, surface-based analysis has been proven to be more sensitive in disease modeling such as AD, yet, like volume-based data, it also suffers from the mismatch problem. To address these limitations, we proposed a novel generative normative modeling framework by transferring the conditional diffusion generative model to the spherical non-Euclidean domain. Additionally, this approach generates normal feature map distributions by explicitly conditioning on individual anatomical segmentation to ensure better geometrical alignment which helps to reduce anatomical variance between subjects in analysis. We find that our model can generate samples that are better anatomically aligned than registered reference data and through ablation study and normative assessment experiments, the samples are able to better measure individual differences from the normal distribution and increase sensitivity in differentiating cognitively normal (CN), mild cognitive impairment (MCI), and Alzheimer's disease (AD) patients.","sentences":["Normative modeling has emerged as a pivotal approach for characterizing heterogeneity and individual variance in neurodegenerative diseases, notably Alzheimer's disease(AD).","One of the challenges of cortical normative modeling is the anatomical structure mismatch due to folding pattern variability.","Traditionally, registration is applied to address this issue and recently many studies have utilized deep generative models to generate anatomically align samples for analyzing disease progression; however, these models are predominantly applied to volume-based data, which often falls short in capturing intricate morphological changes on the brain cortex.","As an alternative, surface-based analysis has been proven to be more sensitive in disease modeling such as AD, yet, like volume-based data, it also suffers from the mismatch problem.","To address these limitations, we proposed a novel generative normative modeling framework by transferring the conditional diffusion generative model to the spherical non-Euclidean domain.","Additionally, this approach generates normal feature map distributions by explicitly conditioning on individual anatomical segmentation to ensure better geometrical alignment which helps to reduce anatomical variance between subjects in analysis.","We find that our model can generate samples that are better anatomically aligned than registered reference data and through ablation study and normative assessment experiments, the samples are able to better measure individual differences from the normal distribution and increase sensitivity in differentiating cognitively normal (CN), mild cognitive impairment (MCI), and Alzheimer's disease (AD) patients."],"url":"http://arxiv.org/abs/2403.04531v1","category":"eess.IV"}
{"created":"2024-03-07 14:28:04","title":"Enhancing Data Quality in Federated Fine-Tuning of Foundation Models","abstract":"In the current landscape of foundation model training, there is a significant reliance on public domain data, which is nearing exhaustion according to recent research. To further scale up, it is crucial to incorporate collaboration among multiple specialized and high-quality private domain data sources. However, the challenge of training models locally without sharing private data presents numerous obstacles in data quality control. To tackle this issue, we propose a data quality control pipeline for federated fine-tuning of foundation models. This pipeline computes scores reflecting the quality of training data and determines a global threshold for a unified standard, aiming for improved global performance. Our experiments show that the proposed quality control pipeline facilitates the effectiveness and reliability of the model training, leading to better performance.","sentences":["In the current landscape of foundation model training, there is a significant reliance on public domain data, which is nearing exhaustion according to recent research.","To further scale up, it is crucial to incorporate collaboration among multiple specialized and high-quality private domain data sources.","However, the challenge of training models locally without sharing private data presents numerous obstacles in data quality control.","To tackle this issue, we propose a data quality control pipeline for federated fine-tuning of foundation models.","This pipeline computes scores reflecting the quality of training data and determines a global threshold for a unified standard, aiming for improved global performance.","Our experiments show that the proposed quality control pipeline facilitates the effectiveness and reliability of the model training, leading to better performance."],"url":"http://arxiv.org/abs/2403.04529v1","category":"cs.LG"}
{"created":"2024-03-07 14:27:08","title":"Hyperspectral unmixing for Raman spectroscopy via physics-constrained autoencoders","abstract":"Raman spectroscopy is widely used across scientific domains to characterize the chemical composition of samples in a non-destructive, label-free manner. Many applications entail the unmixing of signals from mixtures of molecular species to identify the individual components present and their proportions, yet conventional methods for chemometrics often struggle with complex mixture scenarios encountered in practice. Here, we develop hyperspectral unmixing algorithms based on autoencoder neural networks, and we systematically validate them using both synthetic and experimental benchmark datasets created in-house. Our results demonstrate that unmixing autoencoders provide improved accuracy, robustness and efficiency compared to standard unmixing methods. We also showcase the applicability of autoencoders to complex biological settings by showing improved biochemical characterization of volumetric Raman imaging data from a monocytic cell.","sentences":["Raman spectroscopy is widely used across scientific domains to characterize the chemical composition of samples in a non-destructive, label-free manner.","Many applications entail the unmixing of signals from mixtures of molecular species to identify the individual components present and their proportions, yet conventional methods for chemometrics often struggle with complex mixture scenarios encountered in practice.","Here, we develop hyperspectral unmixing algorithms based on autoencoder neural networks, and we systematically validate them using both synthetic and experimental benchmark datasets created in-house.","Our results demonstrate that unmixing autoencoders provide improved accuracy, robustness and efficiency compared to standard unmixing methods.","We also showcase the applicability of autoencoders to complex biological settings by showing improved biochemical characterization of volumetric Raman imaging data from a monocytic cell."],"url":"http://arxiv.org/abs/2403.04526v1","category":"cs.LG"}
{"created":"2024-03-07 14:25:59","title":"Local limits of random bipartite maps in high genus: the general case","abstract":"We prove the local convergence of uniform bipartite maps with prescribed face degrees in the high genus regime. Unlike in the previous work arxiv:2012.05813 on the subject, we do not make any assumption on the tail of the face degrees, except that they remain finite in the limit.","sentences":["We prove the local convergence of uniform bipartite maps with prescribed face degrees in the high genus regime.","Unlike in the previous work arxiv:2012.05813 on the subject, we do not make any assumption on the tail of the face degrees, except that they remain finite in the limit."],"url":"http://arxiv.org/abs/2403.04524v1","category":"math.PR"}
{"created":"2024-03-07 14:25:03","title":"T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers","abstract":"The development and adoption of Vision Transformers and other deep-learning architectures for image classification tasks has been rapid. However, the \"black box\" nature of neural networks is a barrier to adoption in applications where explainability is essential. While some techniques for generating explanations have been proposed, primarily for Convolutional Neural Networks, adapting such techniques to the new paradigm of Vision Transformers is non-trivial. This paper presents T-TAME, Transformer-compatible Trainable Attention Mechanism for Explanations, a general methodology for explaining deep neural networks used in image classification tasks. The proposed architecture and training technique can be easily applied to any convolutional or Vision Transformer-like neural network, using a streamlined training approach. After training, explanation maps can be computed in a single forward pass; these explanation maps are comparable to or outperform the outputs of computationally expensive perturbation-based explainability techniques, achieving SOTA performance. We apply T-TAME to three popular deep learning classifier architectures, VGG-16, ResNet-50, and ViT-B-16, trained on the ImageNet dataset, and we demonstrate improvements over existing state-of-the-art explainability methods. A detailed analysis of the results and an ablation study provide insights into how the T-TAME design choices affect the quality of the generated explanation maps.","sentences":["The development and adoption of Vision Transformers and other deep-learning architectures for image classification tasks has been rapid.","However, the \"black box\" nature of neural networks is a barrier to adoption in applications where explainability is essential.","While some techniques for generating explanations have been proposed, primarily for Convolutional Neural Networks, adapting such techniques to the new paradigm of Vision Transformers is non-trivial.","This paper presents T-TAME, Transformer-compatible Trainable Attention Mechanism for Explanations, a general methodology for explaining deep neural networks used in image classification tasks.","The proposed architecture and training technique can be easily applied to any convolutional or Vision Transformer-like neural network, using a streamlined training approach.","After training, explanation maps can be computed in a single forward pass; these explanation maps are comparable to or outperform the outputs of computationally expensive perturbation-based explainability techniques, achieving SOTA performance.","We apply T-TAME to three popular deep learning classifier architectures, VGG-16, ResNet-50, and ViT-B-16, trained on the ImageNet dataset, and we demonstrate improvements over existing state-of-the-art explainability methods.","A detailed analysis of the results and an ablation study provide insights into how the T-TAME design choices affect the quality of the generated explanation maps."],"url":"http://arxiv.org/abs/2403.04523v1","category":"cs.CV"}
{"created":"2024-03-07 14:23:25","title":"Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge Graph Completion","abstract":"Few-shot knowledge graph completion (FKGC) aims to query the unseen facts of a relation given its few-shot reference entity pairs. The side effect of noises due to the uncertainty of entities and triples may limit the few-shot learning, but existing FKGC works neglect such uncertainty, which leads them more susceptible to limited reference samples with noises. In this paper, we propose a novel uncertainty-aware few-shot KG completion framework (UFKGC) to model uncertainty for a better understanding of the limited data by learning representations under Gaussian distribution. Uncertainty representation is first designed for estimating the uncertainty scope of the entity pairs after transferring feature representations into a Gaussian distribution. Further, to better integrate the neighbors with uncertainty characteristics for entity features, we design an uncertainty-aware relational graph neural network (UR-GNN) to conduct convolution operations between the Gaussian distributions. Then, multiple random samplings are conducted for reference triples within the Gaussian distribution to generate smooth reference representations during the optimization. The final completion score for each query instance is measured by the designed uncertainty optimization to make our approach more robust to the noises in few-shot scenarios. Experimental results show that our approach achieves excellent performance on two benchmark datasets compared to its competitors.","sentences":["Few-shot knowledge graph completion (FKGC) aims to query the unseen facts of a relation given its few-shot reference entity pairs.","The side effect of noises due to the uncertainty of entities and triples may limit the few-shot learning, but existing FKGC works neglect such uncertainty, which leads them more susceptible to limited reference samples with noises.","In this paper, we propose a novel uncertainty-aware few-shot KG completion framework (UFKGC) to model uncertainty for a better understanding of the limited data by learning representations under Gaussian distribution.","Uncertainty representation is first designed for estimating the uncertainty scope of the entity pairs after transferring feature representations into a Gaussian distribution.","Further, to better integrate the neighbors with uncertainty characteristics for entity features, we design an uncertainty-aware relational graph neural network (UR-GNN) to conduct convolution operations between the Gaussian distributions.","Then, multiple random samplings are conducted for reference triples within the Gaussian distribution to generate smooth reference representations during the optimization.","The final completion score for each query instance is measured by the designed uncertainty optimization to make our approach more robust to the noises in few-shot scenarios.","Experimental results show that our approach achieves excellent performance on two benchmark datasets compared to its competitors."],"url":"http://arxiv.org/abs/2403.04521v1","category":"cs.CL"}
{"created":"2024-03-07 14:18:52","title":"On a mathematical model for tissue regeneration","abstract":"We propose a PDE-ODE model for tissue regeneration, obtained by parabolic upscaling from kinetic transport equations written for the mesoscopic densities of mesenchymal stem cells and chondrocytes which evolve in an artificial scaffold impregnated with hyaluron. Due to the simple chosen turning kernels, the effective equations obtained on the macroscopic level are of the usual reaction-diffusion-taxis type. We prove global existence of solutions to the coupled macroscopic system and perform a stability and bifurcation analysis, which shows that the observed patterns are driven by taxis. Numerical simulations illustrate the model behavior for various tactic sensitivities and initial conditions.","sentences":["We propose a PDE-ODE model for tissue regeneration, obtained by parabolic upscaling from kinetic transport equations written for the mesoscopic densities of mesenchymal stem cells and chondrocytes which evolve in an artificial scaffold impregnated with hyaluron.","Due to the simple chosen turning kernels, the effective equations obtained on the macroscopic level are of the usual reaction-diffusion-taxis type.","We prove global existence of solutions to the coupled macroscopic system and perform a stability and bifurcation analysis, which shows that the observed patterns are driven by taxis.","Numerical simulations illustrate the model behavior for various tactic sensitivities and initial conditions."],"url":"http://arxiv.org/abs/2403.04516v1","category":"math.AP"}
{"created":"2024-03-07 14:15:09","title":"A topological characterization of the existence of w-stable sets","abstract":"The theory of optimal choice sets is a solution theory that has a long and well-established tradition in social choice and game theories. Some of important general solution concepts of choice problems when the set of best alternatives does not exist (this problem occurs when the preferences yielded by an economic process are cyclic) is the Stable Set (Von Neumann-Morgenstern set) and its variants (Generalized Stable set, Extended Stable set, m-Stable set and w-Stable set). The theory of w-stable sets solution is more realistic because: (1) It solves the existence problem of solution; (2) It expands the notions of maximal alternative set and (3) The concept of stability is defined in such a way as to prevent a chosen alternative from being dominated by another alternative and sets this stability within the solution. In this paper, we present a topological characterization of the existence of w-Stable sets solution of arbitrary binary relations over non-finite sets of alternatives.","sentences":["The theory of optimal choice sets is a solution theory that has a long and well-established tradition in social choice and game theories.","Some of important general solution concepts of choice problems when the set of best alternatives does not exist (this problem occurs when the preferences yielded by an economic process are cyclic) is the Stable Set (Von Neumann-Morgenstern set) and its variants (Generalized Stable set, Extended Stable set, m-Stable set and w-Stable set).","The theory of w-stable sets solution is more realistic because: (1) It solves the existence problem of solution; (2) It expands the notions of maximal alternative set and (3) The concept of stability is defined in such a way as to prevent a chosen alternative from being dominated by another alternative and sets this stability within the solution.","In this paper, we present a topological characterization of the existence of w-Stable sets solution of arbitrary binary relations over non-finite sets of alternatives."],"url":"http://arxiv.org/abs/2403.04512v1","category":"econ.TH"}
{"created":"2024-03-07 14:14:40","title":"Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video Recommendation","abstract":"Filter bubbles have been studied extensively within the context of online content platforms due to their potential to cause undesirable outcomes such as user dissatisfaction or polarization. With the rise of short-video platforms, the filter bubble has been given extra attention because these platforms rely on an unprecedented use of the recommender system to provide relevant content. In our work, we investigate the deep filter bubble, which refers to the user being exposed to narrow content within their broad interests. We accomplish this using one-year interaction data from a top short-video platform in China, which includes hierarchical data with three levels of categories for each video. We formalize our definition of a \"deep\" filter bubble within this context, and then explore various correlations within the data: first understanding the evolution of the deep filter bubble over time, and later revealing some of the factors that give rise to this phenomenon, such as specific categories, user demographics, and feedback type. We observe that while the overall proportion of users in a filter bubble remains largely constant over time, the depth composition of their filter bubble changes. In addition, we find that some demographic groups that have a higher likelihood of seeing narrower content and implicit feedback signals can lead to less bubble formation. Finally, we propose some ways in which recommender systems can be designed to reduce the risk of a user getting caught in a bubble.","sentences":["Filter bubbles have been studied extensively within the context of online content platforms due to their potential to cause undesirable outcomes such as user dissatisfaction or polarization.","With the rise of short-video platforms, the filter bubble has been given extra attention because these platforms rely on an unprecedented use of the recommender system to provide relevant content.","In our work, we investigate the deep filter bubble, which refers to the user being exposed to narrow content within their broad interests.","We accomplish this using one-year interaction data from a top short-video platform in China, which includes hierarchical data with three levels of categories for each video.","We formalize our definition of a \"deep\" filter bubble within this context, and then explore various correlations within the data: first understanding the evolution of the deep filter bubble over time, and later revealing some of the factors that give rise to this phenomenon, such as specific categories, user demographics, and feedback type.","We observe that while the overall proportion of users in a filter bubble remains largely constant over time, the depth composition of their filter bubble changes.","In addition, we find that some demographic groups that have a higher likelihood of seeing narrower content and implicit feedback signals can lead to less bubble formation.","Finally, we propose some ways in which recommender systems can be designed to reduce the risk of a user getting caught in a bubble."],"url":"http://arxiv.org/abs/2403.04511v1","category":"cs.AI"}
{"created":"2024-03-07 14:12:41","title":"Where does In-context Translation Happen in Large Language Models","abstract":"Self-supervised large language models have demonstrated the ability to perform Machine Translation (MT) via in-context learning, but little is known about where the model performs the task with respect to prompt instructions and demonstration examples. In this work, we attempt to characterize the region where large language models transition from in-context learners to translation models. Through a series of layer-wise context-masking experiments on \\textsc{GPTNeo2.7B}, \\textsc{Bloom3B}, \\textsc{Llama7b} and \\textsc{Llama7b-chat}, we demonstrate evidence of a \"task recognition\" point where the translation task is encoded into the input representations and attention to context is no longer necessary. We further observe correspondence between the low performance when masking out entire layers, and the task recognition layers. Taking advantage of this redundancy results in 45\\% computational savings when prompting with 5 examples, and task recognition achieved at layer 14 / 32. Our layer-wise fine-tuning experiments indicate that the most effective layers for MT fine-tuning are the layers critical to task recognition.","sentences":["Self-supervised large language models have demonstrated the ability to perform Machine Translation (MT) via in-context learning, but little is known about where the model performs the task with respect to prompt instructions and demonstration examples.","In this work, we attempt to characterize the region where large language models transition from in-context learners to translation models.","Through a series of layer-wise context-masking experiments on \\textsc{GPTNeo2.7B}, \\textsc{Bloom3B}, \\textsc{Llama7b} and \\textsc{Llama7b-chat}, we demonstrate evidence of a \"task recognition\" point where the translation task is encoded into the input representations and attention to context is no longer necessary.","We further observe correspondence between the low performance when masking out entire layers, and the task recognition layers.","Taking advantage of this redundancy results in 45\\% computational savings when prompting with 5 examples, and task recognition achieved at layer 14 / 32.","Our layer-wise fine-tuning experiments indicate that the most effective layers for MT fine-tuning are the layers critical to task recognition."],"url":"http://arxiv.org/abs/2403.04510v1","category":"cs.CL"}
{"created":"2024-03-07 14:04:33","title":"Improving Matrix Completion by Exploiting Rating Ordinality in Graph Neural Networks","abstract":"Matrix completion is an important area of research in recommender systems. Recent methods view a rating matrix as a user-item bi-partite graph with labeled edges denoting observed ratings and predict the edges between the user and item nodes by using the graph neural network (GNN). Despite their effectiveness, they treat each rating type as an independent relation type and thus cannot sufficiently consider the ordinal nature of the ratings. In this paper, we explore a new approach to exploit rating ordinality for GNN, which has not been studied well in the literature. We introduce a new method, called ROGMC, to leverage Rating Ordinality in GNN-based Matrix Completion. It uses cumulative preference propagation to directly incorporate rating ordinality in GNN's message passing, allowing for users' stronger preferences to be more emphasized based on inherent orders of rating types. This process is complemented by interest regularization which facilitates preference learning using the underlying interest information. Our extensive experiments show that ROGMC consistently outperforms the existing strategies of using rating types for GNN. We expect that our attempt to explore the feasibility of utilizing rating ordinality for GNN may stimulate further research in this direction.","sentences":["Matrix completion is an important area of research in recommender systems.","Recent methods view a rating matrix as a user-item bi-partite graph with labeled edges denoting observed ratings and predict the edges between the user and item nodes by using the graph neural network (GNN).","Despite their effectiveness, they treat each rating type as an independent relation type and thus cannot sufficiently consider the ordinal nature of the ratings.","In this paper, we explore a new approach to exploit rating ordinality for GNN, which has not been studied well in the literature.","We introduce a new method, called ROGMC, to leverage Rating Ordinality in GNN-based Matrix Completion.","It uses cumulative preference propagation to directly incorporate rating ordinality in GNN's message passing, allowing for users' stronger preferences to be more emphasized based on inherent orders of rating types.","This process is complemented by interest regularization which facilitates preference learning using the underlying interest information.","Our extensive experiments show that ROGMC consistently outperforms the existing strategies of using rating types for GNN.","We expect that our attempt to explore the feasibility of utilizing rating ordinality for GNN may stimulate further research in this direction."],"url":"http://arxiv.org/abs/2403.04504v1","category":"cs.AI"}
{"created":"2024-03-07 13:59:34","title":"A Learnable Prior Improves Inverse Tumor Growth Modeling","abstract":"Biophysical modeling, particularly involving partial differential equations (PDEs), offers significant potential for tailoring disease treatment protocols to individual patients. However, the inverse problem-solving aspect of these models presents a substantial challenge, either due to the high computational requirements of model-based approaches or the limited robustness of deep learning (DL) methods. We propose a novel framework that leverages the unique strengths of both approaches in a synergistic manner. Our method incorporates a DL ensemble for initial parameter estimation, facilitating efficient downstream evolutionary sampling initialized with this DL-based prior. We showcase the effectiveness of integrating a rapid deep-learning algorithm with a high-precision evolution strategy in estimating brain tumor cell concentrations from magnetic resonance images. The DL-Prior plays a pivotal role, significantly constraining the effective sampling-parameter space. This reduction results in a fivefold convergence acceleration and a Dice-score of 95%","sentences":["Biophysical modeling, particularly involving partial differential equations (PDEs), offers significant potential for tailoring disease treatment protocols to individual patients.","However, the inverse problem-solving aspect of these models presents a substantial challenge, either due to the high computational requirements of model-based approaches or the limited robustness of deep learning (DL) methods.","We propose a novel framework that leverages the unique strengths of both approaches in a synergistic manner.","Our method incorporates a DL ensemble for initial parameter estimation, facilitating efficient downstream evolutionary sampling initialized with this DL-based prior.","We showcase the effectiveness of integrating a rapid deep-learning algorithm with a high-precision evolution strategy in estimating brain tumor cell concentrations from magnetic resonance images.","The DL-Prior plays a pivotal role, significantly constraining the effective sampling-parameter space.","This reduction results in a fivefold convergence acceleration and a Dice-score of 95%"],"url":"http://arxiv.org/abs/2403.04500v1","category":"physics.med-ph"}
{"created":"2024-03-07 13:58:10","title":"PCH-EM: A solution to information loss in the photon transfer method","abstract":"Working from a Poisson-Gaussian noise model, a multi-sample extension of the Photon Counting Histogram Expectation Maximization (PCH-EM) algorithm is derived as a general-purpose alternative to the Photon Transfer (PT) method. This algorithm is derived from the same model, requires the same experimental data, and estimates the same sensor performance parameters as the time-tested PT method, all while obtaining lower uncertainty estimates. It is shown that as read noise becomes large, multiple data samples are necessary to capture enough information about the parameters of a device under test, justifying the need for a multi-sample extension. An estimation procedure is devised consisting of initial PT characterization followed by repeated iteration of PCH-EM to demonstrate the improvement in estimate uncertainty achievable with PCH-EM; particularly in the regime of Deep Sub-Electron Read Noise (DSERN). A statistical argument based on the information theoretic concept of sufficiency is formulated to explain how PT data reduction procedures discard information contained in raw sensor data, thus explaining why the proposed algorithm is able to obtain lower uncertainty estimates of key sensor performance parameters such as read noise and conversion gain. Experimental data captured from a CMOS quanta image sensor with DSERN is then used to demonstrate the algorithm's usage and validate the underlying theory and statistical model. In support of the reproducible research effort, the code associated with this work can be obtained on the MathWorks File Exchange (Hendrickson et al., 2024).","sentences":["Working from a Poisson-Gaussian noise model, a multi-sample extension of the Photon Counting Histogram Expectation Maximization (PCH-EM) algorithm is derived as a general-purpose alternative to the Photon Transfer (PT) method.","This algorithm is derived from the same model, requires the same experimental data, and estimates the same sensor performance parameters as the time-tested PT method, all while obtaining lower uncertainty estimates.","It is shown that as read noise becomes large, multiple data samples are necessary to capture enough information about the parameters of a device under test, justifying the need for a multi-sample extension.","An estimation procedure is devised consisting of initial PT characterization followed by repeated iteration of PCH-EM to demonstrate the improvement in estimate uncertainty achievable with PCH-EM; particularly in the regime of Deep Sub-Electron Read Noise (DSERN).","A statistical argument based on the information theoretic concept of sufficiency is formulated to explain how PT data reduction procedures discard information contained in raw sensor data, thus explaining why the proposed algorithm is able to obtain lower uncertainty estimates of key sensor performance parameters such as read noise and conversion gain.","Experimental data captured from a CMOS quanta image sensor with DSERN is then used to demonstrate the algorithm's usage and validate the underlying theory and statistical model.","In support of the reproducible research effort, the code associated with this work can be obtained on the MathWorks File Exchange (Hendrickson et al., 2024)."],"url":"http://arxiv.org/abs/2403.04498v1","category":"physics.ins-det"}
{"created":"2024-03-07 13:49:43","title":"What makes an image realistic?","abstract":"The last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video. Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data. This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI. Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like. In particular, we introduce the notion of a universal critic, which unlike adversarial critics does not require adversarial training. While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool for analyzing existing attempts to capture realism.","sentences":["The last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video.","Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data.","This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI.","Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like.","In particular, we introduce the notion of a universal critic, which unlike adversarial critics does not require adversarial training.","While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool for analyzing existing attempts to capture realism."],"url":"http://arxiv.org/abs/2403.04493v1","category":"cs.LG"}
{"created":"2024-03-07 13:36:15","title":"Source Matters: Source Dataset Impact on Model Robustness in Medical Imaging","abstract":"Transfer learning has become an essential part of medical imaging classification algorithms, often leveraging ImageNet weights. However, the domain shift from natural to medical images has prompted alternatives such as RadImageNet, often demonstrating comparable classification performance. However, it remains unclear whether the performance gains from transfer learning stem from improved generalization or shortcut learning. To address this, we investigate potential confounders -- whether synthetic or sampled from the data -- across two publicly available chest X-ray and CT datasets. We show that ImageNet and RadImageNet achieve comparable classification performance, yet ImageNet is much more prone to overfitting to confounders. We recommend that researchers using ImageNet-pretrained models reexamine their model robustness by conducting similar experiments. Our code and experiments are available at https://github.com/DovileDo/source-matters.","sentences":["Transfer learning has become an essential part of medical imaging classification algorithms, often leveraging ImageNet weights.","However, the domain shift from natural to medical images has prompted alternatives such as RadImageNet, often demonstrating comparable classification performance.","However, it remains unclear whether the performance gains from transfer learning stem from improved generalization or shortcut learning.","To address this, we investigate potential confounders -- whether synthetic or sampled from the data -- across two publicly available chest X-ray and CT datasets.","We show that ImageNet and RadImageNet achieve comparable classification performance, yet ImageNet is much more prone to overfitting to confounders.","We recommend that researchers using ImageNet-pretrained models reexamine their model robustness by conducting similar experiments.","Our code and experiments are available at https://github.com/DovileDo/source-matters."],"url":"http://arxiv.org/abs/2403.04484v1","category":"cs.CV"}
{"created":"2024-03-07 13:36:08","title":"GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability","abstract":"Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic. Graph is a common data structure in the real world, and understanding graph data is a crucial part for advancing general intelligence. To evaluate and enhance the graph understanding abilities of LLMs, in this paper, we propose a benchmark named GraphInstruct, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed reasoning steps. Based on GraphInstruct, we further construct GraphLM through efficient instruction-tuning, which shows prominent graph understanding capability. In order to enhance the LLM with graph reasoning capability as well, we propose a step mask training strategy, and construct a model named GraphLM+. As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demonstrated the superiority of GraphLM and GraphLM+ over other LLMs. We look forward to more researchers exploring the potential of LLMs in the graph data mining domain through GraphInstruct. Our code for generating GraphInstruct is released publicly at: https://github.com/CGCL-codes/GraphInstruct.","sentences":["Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic.","Graph is a common data structure in the real world, and understanding graph data is a crucial part for advancing general intelligence.","To evaluate and enhance the graph understanding abilities of LLMs, in this paper, we propose a benchmark named GraphInstruct, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed reasoning steps.","Based on GraphInstruct, we further construct GraphLM through efficient instruction-tuning, which shows prominent graph understanding capability.","In order to enhance the LLM with graph reasoning capability as well, we propose a step mask training strategy, and construct a model named GraphLM+.","As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demonstrated the superiority of GraphLM and GraphLM+ over other LLMs.","We look forward to more researchers exploring the potential of LLMs in the graph data mining domain through GraphInstruct.","Our code for generating GraphInstruct is released publicly at: https://github.com/CGCL-codes/GraphInstruct."],"url":"http://arxiv.org/abs/2403.04483v1","category":"cs.AI"}
{"created":"2024-03-07 13:33:30","title":"On the Topology Awareness and Generalization Performance of Graph Neural Networks","abstract":"Many computer vision and machine learning problems are modelled as learning tasks on graphs, where graph neural networks (GNNs) have emerged as a dominant tool for learning representations of graph-structured data. A key feature of GNNs is their use of graph structures as input, enabling them to exploit the graphs' inherent topological properties-known as the topology awareness of GNNs. Despite the empirical successes of GNNs, the influence of topology awareness on generalization performance remains unexplored, particularly for node-level tasks that diverge from the assumption of data being independent and identically distributed (I.I.D.). The precise definition and characterization of the topology awareness of GNNs, especially concerning different topological features, are still unclear. This paper introduces a comprehensive framework to characterize the topology awareness of GNNs across any topological feature. Using this framework, we investigate the effects of topology awareness on GNN generalization performance. Contrary to the prevailing belief that enhancing the topology awareness of GNNs is always advantageous, our analysis reveals a critical insight: improving the topology awareness of GNNs may inadvertently lead to unfair generalization across structural groups, which might not be desired in some scenarios. Additionally, we conduct a case study using the intrinsic graph metric, the shortest path distance, on various benchmark datasets. The empirical results of this case study confirm our theoretical insights. Moreover, we demonstrate the practical applicability of our framework by using it to tackle the cold start problem in graph active learning.","sentences":["Many computer vision and machine learning problems are modelled as learning tasks on graphs, where graph neural networks (GNNs) have emerged as a dominant tool for learning representations of graph-structured data.","A key feature of GNNs is their use of graph structures as input, enabling them to exploit the graphs' inherent topological properties-known as the topology awareness of GNNs.","Despite the empirical successes of GNNs, the influence of topology awareness on generalization performance remains unexplored, particularly for node-level tasks that diverge from the assumption of data being independent and identically distributed (I.I.D.).","The precise definition and characterization of the topology awareness of GNNs, especially concerning different topological features, are still unclear.","This paper introduces a comprehensive framework to characterize the topology awareness of GNNs across any topological feature.","Using this framework, we investigate the effects of topology awareness on GNN generalization performance.","Contrary to the prevailing belief that enhancing the topology awareness of GNNs is always advantageous, our analysis reveals a critical insight: improving the topology awareness of GNNs may inadvertently lead to unfair generalization across structural groups, which might not be desired in some scenarios.","Additionally, we conduct a case study using the intrinsic graph metric, the shortest path distance, on various benchmark datasets.","The empirical results of this case study confirm our theoretical insights.","Moreover, we demonstrate the practical applicability of our framework by using it to tackle the cold start problem in graph active learning."],"url":"http://arxiv.org/abs/2403.04482v1","category":"cs.LG"}
{"created":"2024-03-07 13:30:52","title":"Do Large Language Model Understand Multi-Intent Spoken Language ?","abstract":"This study marks a significant advancement by harnessing Large Language Models (LLMs) for multi-intent spoken language understanding (SLU), proposing a unique methodology that capitalizes on the generative power of LLMs within an SLU context. Our innovative technique reconfigures entity slots specifically for LLM application in multi-intent SLU environments and introduces the concept of Sub-Intent Instruction (SII), enhancing the dissection and interpretation of intricate, multi-intent communication within varied domains. The resultant datasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing benchmarks. Our research illustrates that LLMs can match and potentially excel beyond the capabilities of current state-of-the-art multi-intent SLU models. It further explores LLM efficacy across various intent configurations and dataset proportions. Moreover, we introduce two pioneering metrics, Entity Slot Accuracy (ESA) and Combined Semantic Accuracy (CSA), to provide an in-depth analysis of LLM proficiency in this complex field.","sentences":["This study marks a significant advancement by harnessing Large Language Models (LLMs) for multi-intent spoken language understanding (SLU), proposing a unique methodology that capitalizes on the generative power of LLMs within an SLU context.","Our innovative technique reconfigures entity slots specifically for LLM application in multi-intent SLU environments and introduces the concept of Sub-Intent Instruction (SII), enhancing the dissection and interpretation of intricate, multi-intent communication within varied domains.","The resultant datasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing benchmarks.","Our research illustrates that LLMs can match and potentially excel beyond the capabilities of current state-of-the-art multi-intent SLU models.","It further explores LLM efficacy across various intent configurations and dataset proportions.","Moreover, we introduce two pioneering metrics, Entity Slot Accuracy (ESA) and Combined Semantic Accuracy (CSA), to provide an in-depth analysis of LLM proficiency in this complex field."],"url":"http://arxiv.org/abs/2403.04481v1","category":"cs.CL"}
{"created":"2024-03-07 13:18:00","title":"On the $(k+2,k)$-problem of Brown, Erd\u0151s and S\u00f3s for $k=5,6,7$","abstract":"Let $f^{(r)}(n;s,k)$ denote the maximum number of edges in an $n$-vertex $r$-uniform hypergraph containing no subgraph with $k$ edges and at most $s$ vertices. Brown, Erd\\H{o}s and S\\'os [New directions in the theory of graphs (Proc. Third Ann Arbor Conf., Univ. Michigan 1971), pp. 53--63, Academic Press 1973] conjectured that the limit $\\lim_{n\\rightarrow \\infty}n^{-2}f^{(3)}(n;k+2,k)$ exists for all $k$. The value of the limit was previously determined for $k=2$ in the original paper of Brown, Erd\\H{o}s and S\\'os, for $k=3$ by Glock [Bull. Lond. Math. Soc. 51 (2019) 230--236] and for $k=4$ by Glock, Joos, Kim, K\\\"uhn, Lichev and Pikhurko [arXiv:2209.14177, accepted by Proc. Amer. Math. Soc.] while Delcourt and Postle [arXiv:2210.01105, accepted by Proc. Amer. Math. Soc.] proved the conjecture (without determining the limiting value).   In this paper, we determine the value of the limit in the Brown-Erd\\H{o}s-S\\'os Problem for $k\\in \\{5,6,7\\}$. More generally, we obtain the value of $\\lim_{n\\rightarrow \\infty}n^{-2}f^{(r)}(n;rk-2k+2,k)$ for all $r\\geq 3$ and $k\\in \\{5,6,7\\}$. In addition, by combining these new values with recent results of Bennett, Cushman and Dudek [arXiv:2309.00182] we obtain new asymptotic values for several generalised Ramsey numbers.","sentences":["Let $f^{(r)}(n;s,k)$ denote the maximum number of edges in an $n$-vertex $r$-uniform hypergraph containing no subgraph with $k$ edges and at most $s$ vertices.","Brown, Erd\\H{o}s and S\\'os [New directions in the theory of graphs (Proc.","Third Ann Arbor Conf., Univ.","Michigan 1971), pp.","53--63, Academic Press 1973] conjectured that the limit $\\lim_{n\\rightarrow \\infty}n^{-2}f^{(3)}(n;k+2,k)$ exists for all $k$. The value of the limit was previously determined for $k=2$ in the original paper of Brown, Erd\\H{o}s and S\\'os, for $k=3$ by Glock [Bull. Lond.","Math.","Soc.","51 (2019) 230--236] and for $k=4$ by Glock, Joos, Kim, K\\\"uhn, Lichev and Pikhurko [arXiv:2209.14177, accepted by Proc.","Amer.","Math.","Soc.]","while Delcourt and Postle [arXiv:2210.01105, accepted by Proc.","Amer.","Math.","Soc.] proved the conjecture (without determining the limiting value).   ","In this paper, we determine the value of the limit in the Brown-Erd\\H{o}s-S\\'os Problem for $k\\in \\{5,6,7\\}$. More generally, we obtain the value of $\\lim_{n\\rightarrow \\infty}n^{-2}f^{(r)}(n;rk-2k+2,k)$ for all $r\\geq 3$ and $k\\in \\{5,6,7\\}$.","In addition, by combining these new values with recent results of Bennett, Cushman and Dudek","[arXiv:2309.00182] we obtain new asymptotic values for several generalised Ramsey numbers."],"url":"http://arxiv.org/abs/2403.04474v1","category":"math.CO"}
{"created":"2024-03-07 13:16:24","title":"TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document","abstract":"We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks, including document question answering (DocVQA) and scene text analysis. Our approach introduces enhancement across several dimensions: by adopting Shifted Window Attention with zero-initialization, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model's performance. Moreover, by expanding our model's capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability and minimize hallucinations. Additionally, TextMonkey can be finetuned to gain the ability to comprehend commands for clicking screenshots. Overall, our method notably boosts performance across various benchmark datasets, achieving increases of 5.2%, 6.9%, and 2.8% in Scene Text-Centric VQA, Document Oriented VQA, and KIE, respectively, especially with a score of 561 on OCRBench, surpassing prior open-sourced large multimodal models for document understanding. Code will be released at https://github.com/Yuliang-Liu/Monkey.","sentences":["We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks, including document question answering (DocVQA) and scene text analysis.","Our approach introduces enhancement across several dimensions: by adopting Shifted Window Attention with zero-initialization, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model's performance.","Moreover, by expanding our model's capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability and minimize hallucinations.","Additionally, TextMonkey can be finetuned to gain the ability to comprehend commands for clicking screenshots.","Overall, our method notably boosts performance across various benchmark datasets, achieving increases of 5.2%, 6.9%, and 2.8% in Scene Text-Centric VQA, Document Oriented VQA, and KIE, respectively, especially with a score of 561 on OCRBench, surpassing prior open-sourced large multimodal models for document understanding.","Code will be released at https://github.com/Yuliang-Liu/Monkey."],"url":"http://arxiv.org/abs/2403.04473v1","category":"cs.CV"}
{"created":"2024-03-07 13:16:07","title":"The Shutdown Problem: Three Theorems","abstract":"I explain the shutdown problem: the problem of designing artificial agents that (1) shut down when a shutdown button is pressed, (2) don't try to prevent or cause the pressing of the shutdown button, and (3) otherwise pursue goals competently. I prove three theorems that make the difficulty precise. These theorems show that agents satisfying some innocuous-seeming conditions will often try to prevent or cause the pressing of the shutdown button, even in cases where it's costly to do so. And patience trades off against shutdownability: the more patient an agent, the greater the costs that agent is willing to incur to manipulate the shutdown button. I end by noting that these theorems can guide our search for solutions.","sentences":["I explain the shutdown problem: the problem of designing artificial agents that (1) shut down when a shutdown button is pressed, (2) don't try to prevent or cause the pressing of the shutdown button, and (3) otherwise pursue goals competently.","I prove three theorems that make the difficulty precise.","These theorems show that agents satisfying some innocuous-seeming conditions will often try to prevent or cause the pressing of the shutdown button, even in cases where it's costly to do so.","And patience trades off against shutdownability: the more patient an agent, the greater the costs that agent is willing to incur to manipulate the shutdown button.","I end by noting that these theorems can guide our search for solutions."],"url":"http://arxiv.org/abs/2403.04471v1","category":"cs.AI"}
{"created":"2024-03-07 13:10:37","title":"A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges","abstract":"Graph-structured data exhibits universality and widespread applicability across diverse domains, such as social network analysis, biochemistry, financial fraud detection, and network security. Significant strides have been made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success in these areas. However, in real-world scenarios, the training environment for models is often far from ideal, leading to substantial performance degradation of GNN models due to various unfavorable factors, including imbalance in data distribution, the presence of noise in erroneous data, privacy protection of sensitive information, and generalization capability for out-of-distribution (OOD) scenarios. To tackle these issues, substantial efforts have been devoted to improving the performance of GNN models in practical real-world scenarios, as well as enhancing their reliability and robustness. In this paper, we present a comprehensive survey that systematically reviews existing GNN models, focusing on solutions to the four mentioned real-world challenges including imbalance, noise, privacy, and OOD in practical scenarios that many existing reviews have not considered. Specifically, we first highlight the four key challenges faced by existing GNNs, paving the way for our exploration of real-world GNN models. Subsequently, we provide detailed discussions on these four aspects, dissecting how these solutions contribute to enhancing the reliability and robustness of GNN models. Last but not least, we outline promising directions and offer future perspectives in the field.","sentences":["Graph-structured data exhibits universality and widespread applicability across diverse domains, such as social network analysis, biochemistry, financial fraud detection, and network security.","Significant strides have been made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success in these areas.","However, in real-world scenarios, the training environment for models is often far from ideal, leading to substantial performance degradation of GNN models due to various unfavorable factors, including imbalance in data distribution, the presence of noise in erroneous data, privacy protection of sensitive information, and generalization capability for out-of-distribution (OOD) scenarios.","To tackle these issues, substantial efforts have been devoted to improving the performance of GNN models in practical real-world scenarios, as well as enhancing their reliability and robustness.","In this paper, we present a comprehensive survey that systematically reviews existing GNN models, focusing on solutions to the four mentioned real-world challenges including imbalance, noise, privacy, and OOD in practical scenarios that many existing reviews have not considered.","Specifically, we first highlight the four key challenges faced by existing GNNs, paving the way for our exploration of real-world GNN models.","Subsequently, we provide detailed discussions on these four aspects, dissecting how these solutions contribute to enhancing the reliability and robustness of GNN models.","Last but not least, we outline promising directions and offer future perspectives in the field."],"url":"http://arxiv.org/abs/2403.04468v1","category":"cs.LG"}
{"created":"2024-03-07 12:54:16","title":"Extended Time-Dependent Density Functional Theory for Multi-Body Densities","abstract":"Time-dependent density functional theory (TDDFT) is widely used for understanding and predicting properties and behaviors of matter. As one of the fundamental theorems in TDDFT, van Leeuwen's theorem [Phys. Rev. Lett. 82, 3863 (1999)] guarantees how to construct a unique potential with the same one-body density evolution. Here we extend van Leeuwen's theorem by exploring truncation criteria in BBGKY-hierarchy. Our generalized theorem demonstrates the existence of a unique non-local potential to accurately reconstruct the multi-body density evolution in binary interacting systems. Under non-stringent conditions, truncation of the BBGKY-hierarchy equations aligns with the behavior of multi-body density evolution, and maintains consistency in the reduced equations. As one of applications within the extended TDDFT supported by our theorem, multiple excitation energy can be typically solved as the eigenvalue of a generalized Casida's equation. The extended TDDFT provides an accurate and first-principle framework capable of describing the kinetic processes of correlated system, including strongly coupled particle transport, multiple excitation and ionization processes.","sentences":["Time-dependent density functional theory (TDDFT) is widely used for understanding and predicting properties and behaviors of matter.","As one of the fundamental theorems in TDDFT, van Leeuwen's theorem [Phys. Rev. Lett.","82, 3863 (1999)] guarantees how to construct a unique potential with the same one-body density evolution.","Here we extend van Leeuwen's theorem by exploring truncation criteria in BBGKY-hierarchy.","Our generalized theorem demonstrates the existence of a unique non-local potential to accurately reconstruct the multi-body density evolution in binary interacting systems.","Under non-stringent conditions, truncation of the BBGKY-hierarchy equations aligns with the behavior of multi-body density evolution, and maintains consistency in the reduced equations.","As one of applications within the extended TDDFT supported by our theorem, multiple excitation energy can be typically solved as the eigenvalue of a generalized Casida's equation.","The extended TDDFT provides an accurate and first-principle framework capable of describing the kinetic processes of correlated system, including strongly coupled particle transport, multiple excitation and ionization processes."],"url":"http://arxiv.org/abs/2403.04458v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-07 12:48:42","title":"On the Classification of Finite Quasi-Quantum Groups over Abelian Groups","abstract":"Using a variety of methods developed in the theory of finite-dimensional quasi-Hopf algebras, we classify all finite-dimensional coradically graded pointed coquasi-Hopf algebras over abelian groups. As a consequence, we partially confirm the generation conjecture of pointed finite tensor categories due to Etingof, Gelaki, Nikshych and Ostrik.","sentences":["Using a variety of methods developed in the theory of finite-dimensional quasi-Hopf algebras, we classify all finite-dimensional coradically graded pointed coquasi-Hopf algebras over abelian groups.","As a consequence, we partially confirm the generation conjecture of pointed finite tensor categories due to Etingof, Gelaki, Nikshych and Ostrik."],"url":"http://arxiv.org/abs/2403.04455v1","category":"math.QA"}
{"created":"2024-03-07 12:47:42","title":"Low-Resource Court Judgment Summarization for Common Law Systems","abstract":"Common law courts need to refer to similar precedents' judgments to inform their current decisions. Generating high-quality summaries of court judgment documents can facilitate legal practitioners to efficiently review previous cases and assist the general public in accessing how the courts operate and how the law is applied. Previous court judgment summarization research focuses on civil law or a particular jurisdiction's judgments. However, judges can refer to the judgments from all common law jurisdictions. Current summarization datasets are insufficient to satisfy the demands of summarizing precedents across multiple jurisdictions, especially when labeled data are scarce for many jurisdictions. To address the lack of datasets, we present CLSum, the first dataset for summarizing multi-jurisdictional common law court judgment documents. Besides, this is the first court judgment summarization work adopting large language models (LLMs) in data augmentation, summary generation, and evaluation. Specifically, we design an LLM-based data augmentation method incorporating legal knowledge. We also propose a legal knowledge enhanced evaluation metric based on LLM to assess the quality of generated judgment summaries. Our experimental results verify that the LLM-based summarization methods can perform well in the few-shot and zero-shot settings. Our LLM-based data augmentation method can mitigate the impact of low data resources. Furthermore, we carry out comprehensive comparative experiments to find essential model components and settings that are capable of enhancing summarization performance.","sentences":["Common law courts need to refer to similar precedents' judgments to inform their current decisions.","Generating high-quality summaries of court judgment documents can facilitate legal practitioners to efficiently review previous cases and assist the general public in accessing how the courts operate and how the law is applied.","Previous court judgment summarization research focuses on civil law or a particular jurisdiction's judgments.","However, judges can refer to the judgments from all common law jurisdictions.","Current summarization datasets are insufficient to satisfy the demands of summarizing precedents across multiple jurisdictions, especially when labeled data are scarce for many jurisdictions.","To address the lack of datasets, we present CLSum, the first dataset for summarizing multi-jurisdictional common law court judgment documents.","Besides, this is the first court judgment summarization work adopting large language models (LLMs) in data augmentation, summary generation, and evaluation.","Specifically, we design an LLM-based data augmentation method incorporating legal knowledge.","We also propose a legal knowledge enhanced evaluation metric based on LLM to assess the quality of generated judgment summaries.","Our experimental results verify that the LLM-based summarization methods can perform well in the few-shot and zero-shot settings.","Our LLM-based data augmentation method can mitigate the impact of low data resources.","Furthermore, we carry out comprehensive comparative experiments to find essential model components and settings that are capable of enhancing summarization performance."],"url":"http://arxiv.org/abs/2403.04454v1","category":"cs.CL"}
{"created":"2024-03-07 12:45:17","title":"Homotopical Minimal Measures for Geodesic flows on Surfaces of Higher Genus","abstract":"We study the homotopical minimal measures for positive definite autonomous Lagrangian systems. Homotopical minimal measures are action-minimizers in their homotopy classes, while the classical minimal measures (Mather measures) are action-minimizers in homology classes. Homotopical minimal measures are much more general, they are not necessarily homological action-minimizers. However, some of them can be obtained from the classical ones by lifting them to finite-fold covering spaces. We apply this idea of finite covering to the geodesic flows on surfaces of higher genus. Let $(M,G)$ be a compact closed surface with genus $g>1$, where $G$ is a complete Riemannian metric on $M$. Consider the positive definite autonomous Lagrangian $L(x,v)=G_x(v,v)$, whose Lagrangian system $\\phi_t: TM\\rightarrow TM$ is exactly the complete geodesic flow on $TM$. We show that for each homotopical minimal ergodic measure $\\mu$ that is supported on a nontrivial simple closed periodic trajectory, there is a finite-fold covering space $M'$ such that each ergodic preimage of $\\mu$ on $TM'$ is a minimal measure in the classic Mather theory for the Lagrangian system on $TM'$.","sentences":["We study the homotopical minimal measures for positive definite autonomous Lagrangian systems.","Homotopical minimal measures are action-minimizers in their homotopy classes, while the classical minimal measures (Mather measures) are action-minimizers in homology classes.","Homotopical minimal measures are much more general, they are not necessarily homological action-minimizers.","However, some of them can be obtained from the classical ones by lifting them to finite-fold covering spaces.","We apply this idea of finite covering to the geodesic flows on surfaces of higher genus.","Let $(M,G)$ be a compact closed surface with genus $g>1$, where $G$ is a complete Riemannian metric on $M$. Consider the positive definite autonomous Lagrangian $L(x,v)=G_x(v,v)$, whose Lagrangian system $\\phi_t: TM\\rightarrow TM$ is exactly the complete geodesic flow on $TM$. We show that for each homotopical minimal ergodic measure $\\mu$ that is supported on a nontrivial simple closed periodic trajectory, there is a finite-fold covering space $M'$ such that each ergodic preimage of $\\mu$ on $TM'$ is a minimal measure in the classic Mather theory for the Lagrangian system on $TM'$."],"url":"http://arxiv.org/abs/2403.04452v1","category":"math.DS"}
{"created":"2024-03-07 12:43:42","title":"Membership Inference Attacks and Privacy in Topic Modeling","abstract":"Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data. However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities. In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation. Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models. Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling. We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility.","sentences":["Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data.","However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities.","In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation.","Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models.","Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling.","We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility."],"url":"http://arxiv.org/abs/2403.04451v1","category":"cs.CR"}
{"created":"2024-03-07 12:37:52","title":"Feedback-Generation for Programming Exercises With GPT-4","abstract":"Ever since Large Language Models (LLMs) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. LLMs such as Codex, GPT-3.5, and GPT 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale. This paper explores the quality of GPT-4 Turbo's generated output for prompts containing both the programming task specification and a student's submission as input. Two assignments from an introductory programming course were selected, and GPT-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions. The output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. Compared to prior work and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For example, the output is more structured and consistent. GPT-4 Turbo can also accurately identify invalid casing in student programs' output. In some cases, the feedback also includes the output of the student program. At the same time, inconsistent feedback was noted such as stating that the submission is correct but an error needs to be fixed. The present work increases our understanding of LLMs' potential, limitations, and how to integrate them into e-assessment systems, pedagogical scenarios, and instructing students who are using applications based on GPT-4.","sentences":["Ever since Large Language Models (LLMs) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education.","LLMs such as Codex, GPT-3.5, and GPT 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale.","This paper explores the quality of GPT-4 Turbo's generated output for prompts containing both the programming task specification and a student's submission as input.","Two assignments from an introductory programming course were selected, and GPT-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions.","The output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material.","Compared to prior work and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements.","For example, the output is more structured and consistent.","GPT-4 Turbo can also accurately identify invalid casing in student programs' output.","In some cases, the feedback also includes the output of the student program.","At the same time, inconsistent feedback was noted such as stating that the submission is correct but an error needs to be fixed.","The present work increases our understanding of LLMs' potential, limitations, and how to integrate them into e-assessment systems, pedagogical scenarios, and instructing students who are using applications based on GPT-4."],"url":"http://arxiv.org/abs/2403.04449v1","category":"cs.AI"}
{"created":"2024-03-07 12:34:03","title":"FRRI: a novel algorithm for fuzzy-rough rule induction","abstract":"Interpretability is the next frontier in machine learning research. In the search for white box models - as opposed to black box models, like random forests or neural networks - rule induction algorithms are a logical and promising option, since the rules can easily be understood by humans. Fuzzy and rough set theory have been successfully applied to this archetype, almost always separately. As both approaches to rule induction involve granular computing based on the concept of equivalence classes, it is natural to combine them. The QuickRules\\cite{JensenCornelis2009} algorithm was a first attempt at using fuzzy rough set theory for rule induction. It is based on QuickReduct, a greedy algorithm for building decision reducts. QuickRules already showed an improvement over other rule induction methods. However, to evaluate the full potential of a fuzzy rough rule induction algorithm, one needs to start from the foundations. In this paper, we introduce a novel rule induction algorithm called Fuzzy Rough Rule Induction (FRRI). We provide background and explain the workings of our algorithm. Furthermore, we perform a computational experiment to evaluate the performance of our algorithm and compare it to other state-of-the-art rule induction approaches. We find that our algorithm is more accurate while creating small rulesets consisting of relatively short rules. We end the paper by outlining some directions for future work.","sentences":["Interpretability is the next frontier in machine learning research.","In the search for white box models - as opposed to black box models, like random forests or neural networks - rule induction algorithms are a logical and promising option, since the rules can easily be understood by humans.","Fuzzy and rough set theory have been successfully applied to this archetype, almost always separately.","As both approaches to rule induction involve granular computing based on the concept of equivalence classes, it is natural to combine them.","The QuickRules\\cite{JensenCornelis2009} algorithm was a first attempt at using fuzzy rough set theory for rule induction.","It is based on QuickReduct, a greedy algorithm for building decision reducts.","QuickRules already showed an improvement over other rule induction methods.","However, to evaluate the full potential of a fuzzy rough rule induction algorithm, one needs to start from the foundations.","In this paper, we introduce a novel rule induction algorithm called Fuzzy Rough Rule Induction (FRRI).","We provide background and explain the workings of our algorithm.","Furthermore, we perform a computational experiment to evaluate the performance of our algorithm and compare it to other state-of-the-art rule induction approaches.","We find that our algorithm is more accurate while creating small rulesets consisting of relatively short rules.","We end the paper by outlining some directions for future work."],"url":"http://arxiv.org/abs/2403.04447v1","category":"cs.LG"}
{"created":"2024-03-07 12:27:43","title":"Weak Hopf symmetry and tube algebra of the generalized multifusion string-net model","abstract":"We investigate the multifusion generalization of string-net ground states and lattice Hamiltonians, delving into its associated weak Hopf symmetry. For the multifusion string-net, the gauge symmetry manifests as a general weak Hopf algebra, leading to a reducible vacuum string label; the charge symmetry, serving as a quantum double of gauge symmetry, constitutes a connected weak Hopf algebra. This implies that the associated topological phase retains its characterization by a unitary modular tensor category (UMTC). The bulk charge symmetry can also be captured by a weak Hopf tube algebra. We offer an explicit construction of the weak Hopf tube algebra structure and thoroughly discuss its properties. The gapped boundary and domain wall models are extensively discussed, with these $1d$ phases characterized by unitary multifusion categories (UMFCs). We delve into the gauge and charge symmetries of these $1d$ phases, as well as the construction of the boundary and domain wall tube algebras. Additionally, we illustrate that the domain wall tube algebra can be regarded as a cross product of two boundary tube algebras. We establish the anyon condensation theory to elucidate the bulk-to-boundary and bulk-to-wall condensation phenomena from UMTCs to a UMFCs. As an application of our model, we elucidate how to interpret the defective string-net as a restricted multifusion string-net.","sentences":["We investigate the multifusion generalization of string-net ground states and lattice Hamiltonians, delving into its associated weak Hopf symmetry.","For the multifusion string-net, the gauge symmetry manifests as a general weak Hopf algebra, leading to a reducible vacuum string label; the charge symmetry, serving as a quantum double of gauge symmetry, constitutes a connected weak Hopf algebra.","This implies that the associated topological phase retains its characterization by a unitary modular tensor category (UMTC).","The bulk charge symmetry can also be captured by a weak Hopf tube algebra.","We offer an explicit construction of the weak Hopf tube algebra structure and thoroughly discuss its properties.","The gapped boundary and domain wall models are extensively discussed, with these $1d$ phases characterized by unitary multifusion categories (UMFCs).","We delve into the gauge and charge symmetries of these $1d$ phases, as well as the construction of the boundary and domain wall tube algebras.","Additionally, we illustrate that the domain wall tube algebra can be regarded as a cross product of two boundary tube algebras.","We establish the anyon condensation theory to elucidate the bulk-to-boundary and bulk-to-wall condensation phenomena from UMTCs to a UMFCs.","As an application of our model, we elucidate how to interpret the defective string-net as a restricted multifusion string-net."],"url":"http://arxiv.org/abs/2403.04446v1","category":"hep-th"}
{"created":"2024-03-07 12:19:04","title":"FriendNet: Detection-Friendly Dehazing Network","abstract":"Adverse weather conditions often impair the quality of captured images, inevitably inducing cutting-edge object detection models for advanced driver assistance systems (ADAS) and autonomous driving. In this paper, we raise an intriguing question: can the combination of image restoration and object detection enhance detection performance in adverse weather conditions? To answer it, we propose an effective architecture that bridges image dehazing and object detection together via guidance information and task-driven learning to achieve detection-friendly dehazing, termed FriendNet. FriendNet aims to deliver both high-quality perception and high detection capacity. Different from existing efforts that intuitively treat image dehazing as pre-processing, FriendNet establishes a positive correlation between these two tasks. Clean features generated by the dehazing network potentially contribute to improvements in object detection performance. Conversely, object detection crucially guides the learning process of the image dehazing network under the task-driven learning scheme. We shed light on how downstream tasks can guide upstream dehazing processes, considering both network architecture and learning objectives. We design Guidance Fusion Block (GFB) and Guidance Attention Block (GAB) to facilitate the integration of detection information into the network. Furthermore, the incorporation of the detection task loss aids in refining the optimization process. Additionally, we introduce a new Physics-aware Feature Enhancement Block (PFEB), which integrates physics-based priors to enhance the feature extraction and representation capabilities. Extensive experiments on synthetic and real-world datasets demonstrate the superiority of our method over state-of-the-art methods on both image quality and detection precision. Our source code is available at https://github.com/fanyihua0309/FriendNet.","sentences":["Adverse weather conditions often impair the quality of captured images, inevitably inducing cutting-edge object detection models for advanced driver assistance systems (ADAS) and autonomous driving.","In this paper, we raise an intriguing question: can the combination of image restoration and object detection enhance detection performance in adverse weather conditions?","To answer it, we propose an effective architecture that bridges image dehazing and object detection together via guidance information and task-driven learning to achieve detection-friendly dehazing, termed FriendNet.","FriendNet aims to deliver both high-quality perception and high detection capacity.","Different from existing efforts that intuitively treat image dehazing as pre-processing, FriendNet establishes a positive correlation between these two tasks.","Clean features generated by the dehazing network potentially contribute to improvements in object detection performance.","Conversely, object detection crucially guides the learning process of the image dehazing network under the task-driven learning scheme.","We shed light on how downstream tasks can guide upstream dehazing processes, considering both network architecture and learning objectives.","We design Guidance Fusion Block (GFB) and Guidance Attention Block (GAB) to facilitate the integration of detection information into the network.","Furthermore, the incorporation of the detection task loss aids in refining the optimization process.","Additionally, we introduce a new Physics-aware Feature Enhancement Block (PFEB), which integrates physics-based priors to enhance the feature extraction and representation capabilities.","Extensive experiments on synthetic and real-world datasets demonstrate the superiority of our method over state-of-the-art methods on both image quality and detection precision.","Our source code is available at https://github.com/fanyihua0309/FriendNet."],"url":"http://arxiv.org/abs/2403.04443v1","category":"cs.CV"}
{"created":"2024-03-07 12:16:51","title":"Cooperative Bayesian Optimization for Imperfect Agents","abstract":"We introduce a cooperative Bayesian optimization problem for optimizing black-box functions of two variables where two agents choose together at which points to query the function but have only control over one variable each. This setting is inspired by human-AI teamwork, where an AI-assistant helps its human user solve a problem, in this simplest case, collaborative optimization. We formulate the solution as sequential decision-making, where the agent we control models the user as a computationally rational agent with prior knowledge about the function. We show that strategic planning of the queries enables better identification of the global maximum of the function as long as the user avoids excessive exploration. This planning is made possible by using Bayes Adaptive Monte Carlo planning and by endowing the agent with a user model that accounts for conservative belief updates and exploratory sampling of the points to query.","sentences":["We introduce a cooperative Bayesian optimization problem for optimizing black-box functions of two variables where two agents choose together at which points to query the function but have only control over one variable each.","This setting is inspired by human-AI teamwork, where an AI-assistant helps its human user solve a problem, in this simplest case, collaborative optimization.","We formulate the solution as sequential decision-making, where the agent we control models the user as a computationally rational agent with prior knowledge about the function.","We show that strategic planning of the queries enables better identification of the global maximum of the function as long as the user avoids excessive exploration.","This planning is made possible by using Bayes Adaptive Monte Carlo planning and by endowing the agent with a user model that accounts for conservative belief updates and exploratory sampling of the points to query."],"url":"http://arxiv.org/abs/2403.04442v1","category":"cs.LG"}
{"created":"2024-03-07 12:11:02","title":"StableDrag: Stable Dragging for Point-based Image Editing","abstract":"Point-based image editing has attracted remarkable attention since the emergence of DragGAN. Recently, DragDiffusion further pushes forward the generative quality via adapting this dragging technique to diffusion models. Despite these great success, this dragging scheme exhibits two major drawbacks, namely inaccurate point tracking and incomplete motion supervision, which may result in unsatisfactory dragging outcomes. To tackle these issues, we build a stable and precise drag-based editing framework, coined as StableDrag, by designing a discirminative point tracking method and a confidence-based latent enhancement strategy for motion supervision. The former allows us to precisely locate the updated handle points, thereby boosting the stability of long-range manipulation, while the latter is responsible for guaranteeing the optimized latent as high-quality as possible across all the manipulation steps. Thanks to these unique designs, we instantiate two types of image editing models including StableDrag-GAN and StableDrag-Diff, which attains more stable dragging performance, through extensive qualitative experiments and quantitative assessment on DragBench.","sentences":["Point-based image editing has attracted remarkable attention since the emergence of DragGAN.","Recently, DragDiffusion further pushes forward the generative quality via adapting this dragging technique to diffusion models.","Despite these great success, this dragging scheme exhibits two major drawbacks, namely inaccurate point tracking and incomplete motion supervision, which may result in unsatisfactory dragging outcomes.","To tackle these issues, we build a stable and precise drag-based editing framework, coined as StableDrag, by designing a discirminative point tracking method and a confidence-based latent enhancement strategy for motion supervision.","The former allows us to precisely locate the updated handle points, thereby boosting the stability of long-range manipulation, while the latter is responsible for guaranteeing the optimized latent as high-quality as possible across all the manipulation steps.","Thanks to these unique designs, we instantiate two types of image editing models including StableDrag-GAN and StableDrag-Diff, which attains more stable dragging performance, through extensive qualitative experiments and quantitative assessment on DragBench."],"url":"http://arxiv.org/abs/2403.04437v1","category":"cs.CV"}
{"created":"2024-03-07 12:10:41","title":"Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation","abstract":"We present Human to Humanoid (H2O), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable \"sim-to-data\" process to filter and pick feasible motions using a privileged motion imitator. Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner. We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation.","sentences":["We present Human to Humanoid (H2O), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera.","To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable \"sim-to-data\" process to filter and pick feasible motions using a privileged motion imitator.","Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner.","We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc.","To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation."],"url":"http://arxiv.org/abs/2403.04436v1","category":"cs.RO"}
{"created":"2024-03-07 12:04:13","title":"Wavepacket interference of two photons: from temporal entanglement to wavepacket shaping","abstract":"Quantum interference based on beam splitting can be used for entanglement generations and has applications in quantum information. However, interference among photons with different temporal shapes has received little attention. Here we analytically study the interference of two photons with different temporal shapes through a beam splitter (BS), and propose its application in temporal entanglement and shaping of photons. The temporal entanglement is determined by the splitting ratio of BS and the temporal indistinguishability of input photons. Maximum entanglement can be achieved with a 50/50 BS configuration. Then, detecting one of the entangled photons at a specific time enables the probabilistic shaping of the other photon. This process can shape the exponentially decaying (ED) wavepacket into the ED sine shapes, which can be further shaped into Gaussian shapes with a fidelity exceeding 99\\%. The temporal entanglement and shaping of photons based on interference may solve the shape mismatch issues in complex large-scale optical quantum networks.","sentences":["Quantum interference based on beam splitting can be used for entanglement generations and has applications in quantum information.","However, interference among photons with different temporal shapes has received little attention.","Here we analytically study the interference of two photons with different temporal shapes through a beam splitter (BS), and propose its application in temporal entanglement and shaping of photons.","The temporal entanglement is determined by the splitting ratio of BS and the temporal indistinguishability of input photons.","Maximum entanglement can be achieved with a 50/50 BS configuration.","Then, detecting one of the entangled photons at a specific time enables the probabilistic shaping of the other photon.","This process can shape the exponentially decaying (ED) wavepacket into the ED sine shapes, which can be further shaped into Gaussian shapes with a fidelity exceeding 99\\%.","The temporal entanglement and shaping of photons based on interference may solve the shape mismatch issues in complex large-scale optical quantum networks."],"url":"http://arxiv.org/abs/2403.04432v1","category":"quant-ph"}
{"created":"2024-03-07 12:00:33","title":"On-demand Quantization for Green Federated Generative Diffusion in Mobile Edge Networks","abstract":"Generative Artificial Intelligence (GAI) shows remarkable productivity and creativity in Mobile Edge Networks, such as the metaverse and the Industrial Internet of Things. Federated learning is a promising technique for effectively training GAI models in mobile edge networks due to its data distribution. However, there is a notable issue with communication consumption when training large GAI models like generative diffusion models in mobile edge networks. Additionally, the substantial energy consumption associated with training diffusion-based models, along with the limited resources of edge devices and complexities of network environments, pose challenges for improving the training efficiency of GAI models. To address this challenge, we propose an on-demand quantized energy-efficient federated diffusion approach for mobile edge networks. Specifically, we first design a dynamic quantized federated diffusion training scheme considering various demands from the edge devices. Then, we study an energy efficiency problem based on specific quantization requirements. Numerical results show that our proposed method significantly reduces system energy consumption and transmitted model size compared to both baseline federated diffusion and fixed quantized federated diffusion methods while effectively maintaining reasonable quality and diversity of generated data.","sentences":["Generative Artificial Intelligence (GAI) shows remarkable productivity and creativity in Mobile Edge Networks, such as the metaverse and the Industrial Internet of Things.","Federated learning is a promising technique for effectively training GAI models in mobile edge networks due to its data distribution.","However, there is a notable issue with communication consumption when training large GAI models like generative diffusion models in mobile edge networks.","Additionally, the substantial energy consumption associated with training diffusion-based models, along with the limited resources of edge devices and complexities of network environments, pose challenges for improving the training efficiency of GAI models.","To address this challenge, we propose an on-demand quantized energy-efficient federated diffusion approach for mobile edge networks.","Specifically, we first design a dynamic quantized federated diffusion training scheme considering various demands from the edge devices.","Then, we study an energy efficiency problem based on specific quantization requirements.","Numerical results show that our proposed method significantly reduces system energy consumption and transmitted model size compared to both baseline federated diffusion and fixed quantized federated diffusion methods while effectively maintaining reasonable quality and diversity of generated data."],"url":"http://arxiv.org/abs/2403.04430v1","category":"cs.LG"}
{"created":"2024-03-07 11:56:36","title":"Sentiment-driven prediction of financial returns: a Bayesian-enhanced FinBERT approach","abstract":"Predicting financial returns accurately poses a significant challenge due to the inherent uncertainty in financial time series data. Enhancing prediction models' performance hinges on effectively capturing both social and financial sentiment. In this study, we showcase the efficacy of leveraging sentiment information extracted from tweets using the FinBERT large language model. By meticulously curating an optimal feature set through correlation analysis and employing Bayesian-optimized Recursive Feature Elimination for automatic feature selection, we surpass existing methodologies, achieving an F1-score exceeding 70% on the test set. This success translates into demonstrably higher cumulative profits during backtested trading. Our investigation focuses on real-world SPY ETF data alongside corresponding tweets sourced from the StockTwits platform.","sentences":["Predicting financial returns accurately poses a significant challenge due to the inherent uncertainty in financial time series data.","Enhancing prediction models' performance hinges on effectively capturing both social and financial sentiment.","In this study, we showcase the efficacy of leveraging sentiment information extracted from tweets using the FinBERT large language model.","By meticulously curating an optimal feature set through correlation analysis and employing Bayesian-optimized Recursive Feature Elimination for automatic feature selection, we surpass existing methodologies, achieving an F1-score exceeding 70% on the test set.","This success translates into demonstrably higher cumulative profits during backtested trading.","Our investigation focuses on real-world SPY ETF data alongside corresponding tweets sourced from the StockTwits platform."],"url":"http://arxiv.org/abs/2403.04427v1","category":"cs.CE"}
{"created":"2024-03-07 11:55:08","title":"Spin-Phonon interaction in quasi 2D- Cr$_2Te_3$","abstract":"Spin-phonon interaction plays an important role in 2D magnetic materials and motivates the development of next-generation spin- and charge-dependent microelectronic devices. Understanding the spin-phonon interaction by tuning the growth parameter of single crystal Cr$_2Te_3$, a robust quasi-2D room temperature magnetic material, is crucial for spintronic devices. The synthesis of single crystal 2D Cr$_2Te_3$ flakes on a Si substrate from co-deposited thin film by plasma annealing techniques is a significant achievement. The temperature dependence and polarization-resolved Raman spectroscopy with support of density functional theory classified lattice symmetry operations were used to identify the phonon modes to investigate the spin/electron-phonon interactions in Cr$_2Te_3$. The mean-field theory model in single crystal Cr$_2Te_3$ is employed to quantify the spin-phonon interaction and correlate with in-plane and out-of-plane magnetic behavior. The observation of a positive correlation between phonon mode frequency and spin-phonon interaction strength in single crystal Cr$_2Te_3$ can be a potential candidate for spintronic applications.","sentences":["Spin-phonon interaction plays an important role in 2D magnetic materials and motivates the development of next-generation spin- and charge-dependent microelectronic devices.","Understanding the spin-phonon interaction by tuning the growth parameter of single crystal Cr$_2Te_3$, a robust quasi-2D room temperature magnetic material, is crucial for spintronic devices.","The synthesis of single crystal 2D Cr$_2Te_3$ flakes on a Si substrate from co-deposited thin film by plasma annealing techniques is a significant achievement.","The temperature dependence and polarization-resolved Raman spectroscopy with support of density functional theory classified lattice symmetry operations were used to identify the phonon modes to investigate the spin/electron-phonon interactions in Cr$_2Te_3$.","The mean-field theory model in single crystal Cr$_2Te_3$ is employed to quantify the spin-phonon interaction and correlate with in-plane and out-of-plane magnetic behavior.","The observation of a positive correlation between phonon mode frequency and spin-phonon interaction strength in single crystal Cr$_2Te_3$ can be a potential candidate for spintronic applications."],"url":"http://arxiv.org/abs/2403.04426v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-07 11:43:12","title":"Design of supercurrent diode by vortex phase texture","abstract":"We investigate supercurrent nonreciprocal effects in a superconducting weak-link hosting distinct types of vortices. We demonstrate how the winding number of the vortex, its spatial configuration and the shape of the superconducting lead can steer the sign and amplitude of the supercurrent rectification. We find a general criterion for the vortex pattern to maximize the rectification amplitude of the supercurrent. The underlying strategy is the search of specific vortex core position yielding a vanishing amplitude of the supercurrent first harmonic. We also prove that supercurrent nonreciprocal effects can be used to diagnose high-winding vortex and to distinguish between different types of vorticity. Our results thus provide a toolkit to control the supercurrent rectification by means of vortex phase textures and nonreciprocal signatures to detect vortex states with nonstandard phase patterns.","sentences":["We investigate supercurrent nonreciprocal effects in a superconducting weak-link hosting distinct types of vortices.","We demonstrate how the winding number of the vortex, its spatial configuration and the shape of the superconducting lead can steer the sign and amplitude of the supercurrent rectification.","We find a general criterion for the vortex pattern to maximize the rectification amplitude of the supercurrent.","The underlying strategy is the search of specific vortex core position yielding a vanishing amplitude of the supercurrent first harmonic.","We also prove that supercurrent nonreciprocal effects can be used to diagnose high-winding vortex and to distinguish between different types of vorticity.","Our results thus provide a toolkit to control the supercurrent rectification by means of vortex phase textures and nonreciprocal signatures to detect vortex states with nonstandard phase patterns."],"url":"http://arxiv.org/abs/2403.04421v1","category":"cond-mat.supr-con"}
{"created":"2024-03-07 11:31:46","title":"Scaling relations for heat and momentum transport in sheared Rayleigh-B\u00e9nard convection","abstract":"We provide scaling relations for the Nusselt number $Nu$ and the friction coefficient $C_{S}$ in sheared Rayleigh-B\\'enard convection, i.e., in Rayleigh-B\\'enard flow with Couette or Poiseuille type shear forcing, by extending the Grossmann & Lohse (2000,2001,2002,2004) theory to sheared thermal convection. The control parameters for these systems are the Rayleigh number $Ra$, the Prandtl number $Pr$, and the Reynolds number $Re_S$ that characterises the strength of the imposed shear. By direct numerical simulations and theoretical considerations, we show that in turbulent Rayleigh-B\\'enard convection, the friction coefficients associated with the applied shear and the shear generated by the large-scale convection rolls are both well described by Prandtl's (1932) logarithmic friction law, suggesting some kind of universality between purely shear driven flows and thermal convection. These scaling relations hold well for $10^6 \\leq Ra \\leq 10^8$, $0.5 \\leq Pr \\leq 5.0$, and $0 \\leq Re_S \\leq 10^4$.","sentences":["We provide scaling relations for the Nusselt number $Nu$ and the friction coefficient $C_{S}$ in sheared Rayleigh-B\\'enard convection, i.e., in Rayleigh-B\\'enard flow with Couette or Poiseuille type shear forcing, by extending the Grossmann & Lohse (2000,2001,2002,2004) theory to sheared thermal convection.","The control parameters for these systems are the Rayleigh number $Ra$, the Prandtl number $Pr$, and the Reynolds number $Re_S$ that characterises the strength of the imposed shear.","By direct numerical simulations and theoretical considerations, we show that in turbulent Rayleigh-B\\'enard convection, the friction coefficients associated with the applied shear and the shear generated by the large-scale convection rolls are both well described by Prandtl's (1932) logarithmic friction law, suggesting some kind of universality between purely shear driven flows and thermal convection.","These scaling relations hold well for $10^6 \\leq Ra \\leq 10^8$, $0.5 \\leq Pr \\leq 5.0$, and $0 \\leq Re_S","\\leq 10^4$."],"url":"http://arxiv.org/abs/2403.04418v1","category":"physics.flu-dyn"}
{"created":"2024-03-07 11:30:56","title":"Promising and worth-to-try future directions for advancing state-of-the-art surrogates methods of agent-based models in social and health computational sciences","abstract":"The execution and runtime performance of model-based analysis tools for realistic large-scale ABMs (Agent-Based Models) can be excessively long. This due to the computational demand exponentially proportional to the model size (e.g. Population size) and the number of model parameters. Even the runtime of a single simulation of a realistic ABM may demand huge computational resources when attempting to employ realistic population size. The main aim of this ad-hoc brief report is to highlight some of surrogate models that were adequate and computationally less demanding for nonlinear dynamical models in various modeling application areas.To the author knowledge, these methods have been not, at least extensively, employed for ABMs within the field of (SHCS) Social Health Computational Sciences, yet. Thus, they might be, but not necessarily, useful in progressing state of the art for establishing surrogate models for ABMs in the field of SHCS.","sentences":["The execution and runtime performance of model-based analysis tools for realistic large-scale ABMs (Agent-Based Models) can be excessively long.","This due to the computational demand exponentially proportional to the model size (e.g. Population size) and the number of model parameters.","Even the runtime of a single simulation of a realistic ABM may demand huge computational resources when attempting to employ realistic population size.","The main aim of this ad-hoc brief report is to highlight some of surrogate models that were adequate and computationally less demanding for nonlinear dynamical models in various modeling application areas.","To the author knowledge, these methods have been not, at least extensively, employed for ABMs within the field of (SHCS)","Social Health Computational Sciences, yet.","Thus, they might be, but not necessarily, useful in progressing state of the art for establishing surrogate models for ABMs in the field of SHCS."],"url":"http://arxiv.org/abs/2403.04417v1","category":"cs.CL"}
{"created":"2024-03-07 11:28:26","title":"iTRPL: An Intelligent and Trusted RPL Protocol based on Multi-Agent Reinforcement Learning","abstract":"Routing Protocol for Low Power and Lossy Networks (RPL) is the de-facto routing standard in IoT networks. It enables nodes to collaborate and autonomously build ad-hoc networks modeled by tree-like destination-oriented direct acyclic graphs (DODAG). Despite its widespread usage in industry and healthcare domains, RPL is susceptible to insider attacks. Although the state-of-the-art RPL ensures that only authenticated nodes participate in DODAG, such hard security measures are still inadequate to prevent insider threats. This entails a need to integrate soft security mechanisms to support decision-making. This paper proposes iTRPL, an intelligent and behavior-based framework that incorporates trust to segregate honest and malicious nodes within a DODAG. It also leverages multi-agent reinforcement learning (MARL) to make autonomous decisions concerning the DODAG. The framework enables a parent node to compute the trust for its child and decide if the latter can join the DODAG. It tracks the behavior of the child node, updates the trust, computes the rewards (or penalties), and shares with the root. The root aggregates the rewards/penalties of all nodes, computes the overall return, and decides via its $\\epsilon$-Greedy MARL module if the DODAG will be retained or modified for the future. A simulation-based performance evaluation demonstrates that iTRPL learns to make optimal decisions with time.","sentences":["Routing Protocol for Low Power and Lossy Networks (RPL) is the de-facto routing standard in IoT networks.","It enables nodes to collaborate and autonomously build ad-hoc networks modeled by tree-like destination-oriented direct acyclic graphs (DODAG).","Despite its widespread usage in industry and healthcare domains, RPL is susceptible to insider attacks.","Although the state-of-the-art RPL ensures that only authenticated nodes participate in DODAG, such hard security measures are still inadequate to prevent insider threats.","This entails a need to integrate soft security mechanisms to support decision-making.","This paper proposes iTRPL, an intelligent and behavior-based framework that incorporates trust to segregate honest and malicious nodes within a DODAG.","It also leverages multi-agent reinforcement learning (MARL) to make autonomous decisions concerning the DODAG.","The framework enables a parent node to compute the trust for its child and decide if the latter can join the DODAG.","It tracks the behavior of the child node, updates the trust, computes the rewards (or penalties), and shares with the root.","The root aggregates the rewards/penalties of all nodes, computes the overall return, and decides via its $\\epsilon$-Greedy MARL module if the DODAG will be retained or modified for the future.","A simulation-based performance evaluation demonstrates that iTRPL learns to make optimal decisions with time."],"url":"http://arxiv.org/abs/2403.04416v1","category":"cs.NI"}
{"created":"2024-03-07 11:25:07","title":"A methodology to automatically optimize dynamic memory managers applying grammatical evolution","abstract":"Modern consumer devices must execute multimedia applications that exhibit high resource utilization. In order to efficiently execute these applications, the dynamic memory subsystem needs to be optimized. This complex task can be tackled in two complementary ways: optimizing the application source code or designing custom dynamic memory management mechanisms. Currently, the first approach has been well established, and several automatic methodologies have been proposed. Regarding the second approach, software engineers often write custom dynamic memory managers from scratch, which is a difficult and error-prone work. This paper presents a novel way to automatically generate custom dynamic memory managers optimizing both performance and memory usage of the target application. The design space is pruned using grammatical evolution converging to the best dynamic memory manager implementation for the target application. Our methodology achieves important improvements (62.55\\% and 30.62\\% better on average in performance and memory usage, respectively) when its results are compared to five different general-purpose dynamic memory managers.","sentences":["Modern consumer devices must execute multimedia applications that exhibit high resource utilization.","In order to efficiently execute these applications, the dynamic memory subsystem needs to be optimized.","This complex task can be tackled in two complementary ways: optimizing the application source code or designing custom dynamic memory management mechanisms.","Currently, the first approach has been well established, and several automatic methodologies have been proposed.","Regarding the second approach, software engineers often write custom dynamic memory managers from scratch, which is a difficult and error-prone work.","This paper presents a novel way to automatically generate custom dynamic memory managers optimizing both performance and memory usage of the target application.","The design space is pruned using grammatical evolution converging to the best dynamic memory manager implementation for the target application.","Our methodology achieves important improvements (62.55\\% and 30.62\\% better on average in performance and memory usage, respectively) when its results are compared to five different general-purpose dynamic memory managers."],"url":"http://arxiv.org/abs/2403.04414v1","category":"cs.AR"}
{"created":"2024-03-07 11:17:25","title":"Model-free $H_{\\infty}$ control of It\u00f4 stochastic system via off-policy reinforcement learning","abstract":"The stochastic $H_{\\infty}$ control is studied for a linear stochastic It\\^o system with an unknown system model. The linear stochastic $H_{\\infty}$ control issue is known to be transformable into the problem of solving a so-called generalized algebraic Riccati equation (GARE), which is a nonlinear equation that is typically difficult to solve analytically. Worse, model-based techniques cannot be utilized to approximately solve a GARE when an accurate system model is unavailable or prohibitively expensive to construct in reality. To address these issues, an off-policy reinforcement learning (RL) approach is presented to learn the solution of a GARE from real system data rather than a system model; its convergence is demonstrated, and the robustness of RL to errors in the learning process is investigated. In the off-policy RL approach, the system data may be created with behavior policies rather than the target policies, which is highly significant and promising for use in actual systems. Finally, the proposed off-policy RL approach is validated on a stochastic linear F-16 aircraft system.","sentences":["The stochastic $H_{\\infty}$ control is studied for a linear stochastic It\\^o system with an unknown system model.","The linear stochastic $H_{\\infty}$ control issue is known to be transformable into the problem of solving a so-called generalized algebraic Riccati equation (GARE), which is a nonlinear equation that is typically difficult to solve analytically.","Worse, model-based techniques cannot be utilized to approximately solve a GARE when an accurate system model is unavailable or prohibitively expensive to construct in reality.","To address these issues, an off-policy reinforcement learning (RL) approach is presented to learn the solution of a GARE from real system data rather than a system model; its convergence is demonstrated, and the robustness of RL to errors in the learning process is investigated.","In the off-policy RL approach, the system data may be created with behavior policies rather than the target policies, which is highly significant and promising for use in actual systems.","Finally, the proposed off-policy RL approach is validated on a stochastic linear F-16 aircraft system."],"url":"http://arxiv.org/abs/2403.04412v1","category":"math.OC"}
{"created":"2024-03-07 11:11:42","title":"Cometary ion drift energy and temperature at comet 67P-Churyumov/Gerasimeko","abstract":"The Ion Composition Analyzer (ICA) on the Rosetta spacecraft observed both the solar wind and the cometary ionosphere around comet 67P/Churyumov-Gerasimenko for nearly two years. However, observations of low energy cometary ions were affected by a highly negative spacecraft potential, and the ICA ion density estimates were often much lower than plasma densities found by other instruments. Since the low energy cometary ions are often the highest density population in the plasma environment, it is nonetheless desirable to understand their properties. To do so, we select ICA data with densities comparable to those of Rosetta's Langmuir Probe (LAP)/Mutual Impedance Probe throughout the mission. We then correct the cometary ion energy distribution of each energy-angle scan for spacecraft potential and fit a drifting Maxwell-Boltzmann distribution, which gives an estimate of the drift energy and temperature for 3521 scans. The resulting drift energy is generally between 11--18 eV and the temperature between 0.5--1 eV. The drift energy shows good agreement with published ion flow speeds from LAP during the same time period and is much higher than the cometary neutral speed. We see additional higher energy cometary ions in the spectra closest to perihelion, which can either be a second Maxwellian or a kappa distribution. The energy and temperature are negatively correlated with heliocentric distance, but the slope of the change is small. It cannot be quantitatively determined whether this trend is primarily due to heliocentric distance or spacecraft distance to the comet, which increased with decreasing heliocentric distance.","sentences":["The Ion Composition Analyzer (ICA) on the Rosetta spacecraft observed both the solar wind and the cometary ionosphere around comet 67P/Churyumov-Gerasimenko for nearly two years.","However, observations of low energy cometary ions were affected by a highly negative spacecraft potential, and the ICA ion density estimates were often much lower than plasma densities found by other instruments.","Since the low energy cometary ions are often the highest density population in the plasma environment, it is nonetheless desirable to understand their properties.","To do so, we select ICA data with densities comparable to those of Rosetta's Langmuir Probe (LAP)/Mutual Impedance Probe throughout the mission.","We then correct the cometary ion energy distribution of each energy-angle scan for spacecraft potential and fit a drifting Maxwell-Boltzmann distribution, which gives an estimate of the drift energy and temperature for 3521 scans.","The resulting drift energy is generally between 11--18 eV and the temperature between 0.5--1 eV. The drift energy shows good agreement with published ion flow speeds from LAP during the same time period and is much higher than the cometary neutral speed.","We see additional higher energy cometary ions in the spectra closest to perihelion, which can either be a second Maxwellian or a kappa distribution.","The energy and temperature are negatively correlated with heliocentric distance, but the slope of the change is small.","It cannot be quantitatively determined whether this trend is primarily due to heliocentric distance or spacecraft distance to the comet, which increased with decreasing heliocentric distance."],"url":"http://arxiv.org/abs/2403.04409v1","category":"astro-ph.EP"}
{"created":"2024-03-07 11:10:54","title":"Fractionation in young cores: Direct determinations of nitrogen and carbon fractionation in HCN","abstract":"We aim to determine the $^{14}$N/$^{15}$N and $^{12}$C/$^{13}$C ratios for HCN in six starless and prestellar cores and compare the results between the direct method using radiative transfer modeling and the indirect double isotope method assuming a fixed $^{12}$C/$^{13}$C ratio. We present IRAM 30m observations of the HCN 1-0, HCN 3-2, HC15N 1-0 and H13CN 1-0 transitions toward six embedded cores. The ${}^{14}$N/${}^{15}$N ratio was derived using both the indirect double isotope method and directly through non-local thermodynamic equilibrium (NLTE) 1D radiative transfer modeling of the HCN emission. The latter also provides the ${}^{12}$C/${}^{13}$C ratio, which we compared to the local interstellar value. The derived ${}^{14}$N/${}^{15}$N ratios using the indirect method are generally in the range of 300-550. This result could suggest an evolutionary trend in the nitrogen fractionation of HCN between starless cores and later stages of the star formation process. However, the direct method reveals lower fractionation ratios of around $\\sim$250, mainly resulting from a lower ${}^{12}$C/${}^{13}$C ratio in the range $\\sim$20-40, as compared to the local interstellar medium value of 68. This study reveals a significant difference between the nitrogen fractionation ratio in HCN derived using direct and indirect methods. This can influence the interpretation of the chemical evolution and reveal the pitfalls of the indirect double isotope method for fractionation studies. However, the direct method is challenging, as it requires well-constrained source models to produce accurate results. No trend in the nitrogen fractionation of HCN between earlier and later stages of the star formation process is evident when the results of the direct method are considered.","sentences":["We aim to determine the $^{14}$N/$^{15}$N and $^{12}$C/$^{13}$C ratios for HCN in six starless and prestellar cores and compare the results between the direct method using radiative transfer modeling and the indirect double isotope method assuming a fixed $^{12}$C/$^{13}$C ratio.","We present IRAM 30m observations of the HCN 1-0, HCN 3-2, HC15N 1-0 and H13CN 1-0 transitions toward six embedded cores.","The ${}^{14}$N/${}^{15}$N ratio was derived using both the indirect double isotope method and directly through non-local thermodynamic equilibrium (NLTE) 1D radiative transfer modeling of the HCN emission.","The latter also provides the ${}^{12}$C/${}^{13}$C ratio, which we compared to the local interstellar value.","The derived ${}^{14}$N/${}^{15}$N ratios using the indirect method are generally in the range of 300-550.","This result could suggest an evolutionary trend in the nitrogen fractionation of HCN between starless cores and later stages of the star formation process.","However, the direct method reveals lower fractionation ratios of around $\\sim$250, mainly resulting from a lower ${}^{12}$C/${}^{13}$C ratio in the range $\\sim$20-40, as compared to the local interstellar medium value of 68.","This study reveals a significant difference between the nitrogen fractionation ratio in HCN derived using direct and indirect methods.","This can influence the interpretation of the chemical evolution and reveal the pitfalls of the indirect double isotope method for fractionation studies.","However, the direct method is challenging, as it requires well-constrained source models to produce accurate results.","No trend in the nitrogen fractionation of HCN between earlier and later stages of the star formation process is evident when the results of the direct method are considered."],"url":"http://arxiv.org/abs/2403.04408v1","category":"astro-ph.GA"}
{"created":"2024-03-07 10:54:27","title":"Exploring Continual Learning of Compositional Generalization in NLI","abstract":"Compositional Natural Language Inference has been explored to assess the true abilities of neural models to perform NLI. Yet, current evaluations assume models to have full access to all primitive inferences in advance, in contrast to humans that continuously acquire inference knowledge. In this paper, we introduce the Continual Compositional Generalization in Inference (C2Gen NLI) challenge, where a model continuously acquires knowledge of constituting primitive inference tasks as a basis for compositional inferences. We explore how continual learning affects compositional generalization in NLI, by designing a continual learning setup for compositional NLI inference tasks. Our experiments demonstrate that models fail to compositionally generalize in a continual scenario. To address this problem, we first benchmark various continual learning algorithms and verify their efficacy. We then further analyze C2Gen, focusing on how to order primitives and compositional inference types and examining correlations between subtasks. Our analyses show that by learning subtasks continuously while observing their dependencies and increasing degrees of difficulty, continual learning can enhance composition generalization ability.","sentences":["Compositional Natural Language Inference has been explored to assess the true abilities of neural models to perform NLI.","Yet, current evaluations assume models to have full access to all primitive inferences in advance, in contrast to humans that continuously acquire inference knowledge.","In this paper, we introduce the Continual Compositional Generalization in Inference (C2Gen NLI) challenge, where a model continuously acquires knowledge of constituting primitive inference tasks as a basis for compositional inferences.","We explore how continual learning affects compositional generalization in NLI, by designing a continual learning setup for compositional NLI inference tasks.","Our experiments demonstrate that models fail to compositionally generalize in a continual scenario.","To address this problem, we first benchmark various continual learning algorithms and verify their efficacy.","We then further analyze C2Gen, focusing on how to order primitives and compositional inference types and examining correlations between subtasks.","Our analyses show that by learning subtasks continuously while observing their dependencies and increasing degrees of difficulty, continual learning can enhance composition generalization ability."],"url":"http://arxiv.org/abs/2403.04400v1","category":"cs.CL"}
{"created":"2024-03-07 10:48:19","title":"The 2nd Workshop on Recommendation with Generative Models","abstract":"The rise of generative models has driven significant advancements in recommender systems, leaving unique opportunities for enhancing users' personalized recommendations. This workshop serves as a platform for researchers to explore and exchange innovative concepts related to the integration of generative models into recommender systems. It primarily focuses on five key perspectives: (i) improving recommender algorithms, (ii) generating personalized content, (iii) evolving the user-system interaction paradigm, (iv) enhancing trustworthiness checks, and (v) refining evaluation methodologies for generative recommendations. With generative models advancing rapidly, an increasing body of research is emerging in these domains, underscoring the timeliness and critical importance of this workshop. The related research will introduce innovative technologies to recommender systems and contribute to fresh challenges in both academia and industry. In the long term, this research direction has the potential to revolutionize the traditional recommender paradigms and foster the development of next-generation recommender systems.","sentences":["The rise of generative models has driven significant advancements in recommender systems, leaving unique opportunities for enhancing users' personalized recommendations.","This workshop serves as a platform for researchers to explore and exchange innovative concepts related to the integration of generative models into recommender systems.","It primarily focuses on five key perspectives: (i) improving recommender algorithms, (ii) generating personalized content, (iii) evolving the user-system interaction paradigm, (iv) enhancing trustworthiness checks, and (v) refining evaluation methodologies for generative recommendations.","With generative models advancing rapidly, an increasing body of research is emerging in these domains, underscoring the timeliness and critical importance of this workshop.","The related research will introduce innovative technologies to recommender systems and contribute to fresh challenges in both academia and industry.","In the long term, this research direction has the potential to revolutionize the traditional recommender paradigms and foster the development of next-generation recommender systems."],"url":"http://arxiv.org/abs/2403.04399v1","category":"cs.IR"}
{"created":"2024-03-07 10:39:48","title":"SGNet: Folding Symmetrical Protein Complex with Deep Learning","abstract":"Deep learning has made significant progress in protein structure prediction, advancing the development of computational biology. However, despite the high accuracy achieved in predicting single-chain structures, a significant number of large homo-oligomeric assemblies exhibit internal symmetry, posing a major challenge in structure determination. The performances of existing deep learning methods are limited since the symmetrical protein assembly usually has a long sequence, making structural computation infeasible. In addition, multiple identical subunits in symmetrical protein complex cause the issue of supervision ambiguity in label assignment, requiring a consistent structure modeling for the training. To tackle these problems, we propose a protein folding framework called SGNet to model protein-protein interactions in symmetrical assemblies. SGNet conducts feature extraction on a single subunit and generates the whole assembly using our proposed symmetry module, which largely mitigates computational problems caused by sequence length. Thanks to the elaborate design of modeling symmetry consistently, we can model all global symmetry types in quaternary protein structure prediction. Extensive experimental results on a benchmark of symmetrical protein complexes further demonstrate the effectiveness of our method.","sentences":["Deep learning has made significant progress in protein structure prediction, advancing the development of computational biology.","However, despite the high accuracy achieved in predicting single-chain structures, a significant number of large homo-oligomeric assemblies exhibit internal symmetry, posing a major challenge in structure determination.","The performances of existing deep learning methods are limited since the symmetrical protein assembly usually has a long sequence, making structural computation infeasible.","In addition, multiple identical subunits in symmetrical protein complex cause the issue of supervision ambiguity in label assignment, requiring a consistent structure modeling for the training.","To tackle these problems, we propose a protein folding framework called SGNet to model protein-protein interactions in symmetrical assemblies.","SGNet conducts feature extraction on a single subunit and generates the whole assembly using our proposed symmetry module, which largely mitigates computational problems caused by sequence length.","Thanks to the elaborate design of modeling symmetry consistently, we can model all global symmetry types in quaternary protein structure prediction.","Extensive experimental results on a benchmark of symmetrical protein complexes further demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2403.04395v1","category":"q-bio.BM"}
{"created":"2024-03-07 10:38:01","title":"Derivation of a Biot-Plate-System for a thin poroelastic layer","abstract":"We study incompressible fluid flow through a thin poroelastic layer and rigorously derive a macroscopic model when the thickness of the layer tends to zero. Within the layer we assume a periodic structure and both, the periodicity and the thickness of the layer, are of order $\\varepsilon$ which is small compared to the length of the layer. The fluid flow is described by quasistatic Stokes-equations and for the elastic solid we consider linear elasticity equations, and both are coupled via continuity of the velocities and the normal stresses. The aim is to pass to the limit $\\varepsilon \\to 0$ in the weak microscopic formulation by using multi-scale techniques adapted to the simultaneous homogenization and dimension reduction in continuum mechanics. The macroscopic limit model is given by a coupled Biot-Plate-system consisting of a generalized Darcy-law coupled to a Kirchhoff-Love-type plate equation including the Darcy pressure.","sentences":["We study incompressible fluid flow through a thin poroelastic layer and rigorously derive a macroscopic model when the thickness of the layer tends to zero.","Within the layer we assume a periodic structure and both, the periodicity and the thickness of the layer, are of order $\\varepsilon$ which is small compared to the length of the layer.","The fluid flow is described by quasistatic Stokes-equations and for the elastic solid we consider linear elasticity equations, and both are coupled via continuity of the velocities and the normal stresses.","The aim is to pass to the limit $\\varepsilon \\to 0$ in the weak microscopic formulation by using multi-scale techniques adapted to the simultaneous homogenization and dimension reduction in continuum mechanics.","The macroscopic limit model is given by a coupled Biot-Plate-system consisting of a generalized Darcy-law coupled to a Kirchhoff-Love-type plate equation including the Darcy pressure."],"url":"http://arxiv.org/abs/2403.04392v1","category":"math.AP"}
{"created":"2024-03-07 10:29:05","title":"Comparison of Deep Learning Techniques on Human Activity Recognition using Ankle Inertial Signals","abstract":"Human Activity Recognition (HAR) is one of the fundamental building blocks of human assistive devices like orthoses and exoskeletons. There are different approaches to HAR depending on the application. Numerous studies have been focused on improving them by optimising input data or classification algorithms. However, most of these studies have been focused on applications like security and monitoring, smart devices, the internet of things, etc. On the other hand, HAR can help adjust and control wearable assistive devices, yet there has not been enough research facilitating its implementation. In this study, we propose several models to predict four activities from inertial sensors located in the ankle area of a lower-leg assistive device user. This choice is because they do not need to be attached to the user's skin and can be directly implemented inside the control unit of the device. The proposed models are based on Artificial Neural Networks and could achieve up to 92.8% average classification accuracy","sentences":["Human Activity Recognition (HAR) is one of the fundamental building blocks of human assistive devices like orthoses and exoskeletons.","There are different approaches to HAR depending on the application.","Numerous studies have been focused on improving them by optimising input data or classification algorithms.","However, most of these studies have been focused on applications like security and monitoring, smart devices, the internet of things, etc.","On the other hand, HAR can help adjust and control wearable assistive devices, yet there has not been enough research facilitating its implementation.","In this study, we propose several models to predict four activities from inertial sensors located in the ankle area of a lower-leg assistive device user.","This choice is because they do not need to be attached to the user's skin and can be directly implemented inside the control unit of the device.","The proposed models are based on Artificial Neural Networks and could achieve up to 92.8% average classification accuracy"],"url":"http://arxiv.org/abs/2403.04387v1","category":"cs.HC"}
{"created":"2024-03-07 10:25:23","title":"Impacts of Color and Texture Distortions on Earth Observation Data in Deep Learning","abstract":"Land cover classification and change detection are two important applications of remote sensing and Earth observation (EO) that have benefited greatly from the advances of deep learning. Convolutional and transformer-based U-net models are the state-of-the-art architectures for these tasks, and their performances have been boosted by an increased availability of large-scale annotated EO datasets. However, the influence of different visual characteristics of the input EO data on a model's predictions is not well understood. In this work we systematically examine model sensitivities with respect to several color- and texture-based distortions on the input EO data during inference, given models that have been trained without such distortions. We conduct experiments with multiple state-of-the-art segmentation networks for land cover classification and show that they are in general more sensitive to texture than to color distortions. Beyond revealing intriguing characteristics of widely used land cover classification models, our results can also be used to guide the development of more robust models within the EO domain.","sentences":["Land cover classification and change detection are two important applications of remote sensing and Earth observation (EO) that have benefited greatly from the advances of deep learning.","Convolutional and transformer-based U-net models are the state-of-the-art architectures for these tasks, and their performances have been boosted by an increased availability of large-scale annotated EO datasets.","However, the influence of different visual characteristics of the input EO data on a model's predictions is not well understood.","In this work we systematically examine model sensitivities with respect to several color- and texture-based distortions on the input EO data during inference, given models that have been trained without such distortions.","We conduct experiments with multiple state-of-the-art segmentation networks for land cover classification and show that they are in general more sensitive to texture than to color distortions.","Beyond revealing intriguing characteristics of widely used land cover classification models, our results can also be used to guide the development of more robust models within the EO domain."],"url":"http://arxiv.org/abs/2403.04385v1","category":"cs.CV"}
{"created":"2024-03-07 10:20:06","title":"Acceleron: A Tool to Accelerate Research Ideation","abstract":"Several tools have recently been proposed for assisting researchers during various stages of the research life-cycle. However, these primarily concentrate on tasks such as retrieving and recommending relevant literature, reviewing and critiquing the draft, and writing of research manuscripts. Our investigation reveals a significant gap in availability of tools specifically designed to assist researchers during the challenging ideation phase of the research life-cycle. To aid with research ideation, we propose `Acceleron', a research accelerator for different phases of the research life cycle, and which is specially designed to aid the ideation process. Acceleron guides researchers through the formulation of a comprehensive research proposal, encompassing a novel research problem. The proposals motivation is validated for novelty by identifying gaps in the existing literature and suggesting a plausible list of techniques to solve the proposed problem. We leverage the reasoning and domain-specific skills of Large Language Models (LLMs) to create an agent-based architecture incorporating colleague and mentor personas for LLMs. The LLM agents emulate the ideation process undertaken by researchers, engaging researchers in an interactive fashion to aid in the development of the research proposal. Notably, our tool addresses challenges inherent in LLMs, such as hallucinations, implements a two-stage aspect-based retrieval to manage precision-recall trade-offs, and tackles issues of unanswerability. As evaluation, we illustrate the execution of our motivation validation and method synthesis workflows on proposals from the ML and NLP domain, given by 3 distinct researchers. Our observations and evaluations provided by the researchers illustrate the efficacy of the tool in terms of assisting researchers with appropriate inputs at distinct stages and thus leading to improved time efficiency.","sentences":["Several tools have recently been proposed for assisting researchers during various stages of the research life-cycle.","However, these primarily concentrate on tasks such as retrieving and recommending relevant literature, reviewing and critiquing the draft, and writing of research manuscripts.","Our investigation reveals a significant gap in availability of tools specifically designed to assist researchers during the challenging ideation phase of the research life-cycle.","To aid with research ideation, we propose `Acceleron', a research accelerator for different phases of the research life cycle, and which is specially designed to aid the ideation process.","Acceleron guides researchers through the formulation of a comprehensive research proposal, encompassing a novel research problem.","The proposals motivation is validated for novelty by identifying gaps in the existing literature and suggesting a plausible list of techniques to solve the proposed problem.","We leverage the reasoning and domain-specific skills of Large Language Models (LLMs) to create an agent-based architecture incorporating colleague and mentor personas for LLMs.","The LLM agents emulate the ideation process undertaken by researchers, engaging researchers in an interactive fashion to aid in the development of the research proposal.","Notably, our tool addresses challenges inherent in LLMs, such as hallucinations, implements a two-stage aspect-based retrieval to manage precision-recall trade-offs, and tackles issues of unanswerability.","As evaluation, we illustrate the execution of our motivation validation and method synthesis workflows on proposals from the ML and NLP domain, given by 3 distinct researchers.","Our observations and evaluations provided by the researchers illustrate the efficacy of the tool in terms of assisting researchers with appropriate inputs at distinct stages and thus leading to improved time efficiency."],"url":"http://arxiv.org/abs/2403.04382v1","category":"cs.CL"}
{"created":"2024-03-07 10:14:23","title":"Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation","abstract":"The pursuit of accurate 3D hand pose estimation stands as a keystone for understanding human activity in the realm of egocentric vision. The majority of existing estimation methods still rely on single-view images as input, leading to potential limitations, e.g., limited field-of-view and ambiguity in depth. To address these problems, adding another camera to better capture the shape of hands is a practical direction. However, existing multi-view hand pose estimation methods suffer from two main drawbacks: 1) Requiring multi-view annotations for training, which are expensive. 2) During testing, the model becomes inapplicable if camera parameters/layout are not the same as those used in training. In this paper, we propose a novel Single-to-Dual-view adaptation (S2DHand) solution that adapts a pre-trained single-view estimator to dual views. Compared with existing multi-view training methods, 1) our adaptation process is unsupervised, eliminating the need for multi-view annotation. 2) Moreover, our method can handle arbitrary dual-view pairs with unknown camera parameters, making the model applicable to diverse camera settings. Specifically, S2DHand is built on certain stereo constraints, including pair-wise cross-view consensus and invariance of transformation between both views. These two stereo constraints are used in a complementary manner to generate pseudo-labels, allowing reliable adaptation. Evaluation results reveal that S2DHand achieves significant improvements on arbitrary camera pairs under both in-dataset and cross-dataset settings, and outperforms existing adaptation methods with leading performance. Project page: https://github.com/MickeyLLG/S2DHand.","sentences":["The pursuit of accurate 3D hand pose estimation stands as a keystone for understanding human activity in the realm of egocentric vision.","The majority of existing estimation methods still rely on single-view images as input, leading to potential limitations, e.g., limited field-of-view and ambiguity in depth.","To address these problems, adding another camera to better capture the shape of hands is a practical direction.","However, existing multi-view hand pose estimation methods suffer from two main drawbacks: 1) Requiring multi-view annotations for training, which are expensive.","2) During testing, the model becomes inapplicable if camera parameters/layout are not the same as those used in training.","In this paper, we propose a novel Single-to-Dual-view adaptation (S2DHand) solution that adapts a pre-trained single-view estimator to dual views.","Compared with existing multi-view training methods, 1) our adaptation process is unsupervised, eliminating the need for multi-view annotation.","2) Moreover, our method can handle arbitrary dual-view pairs with unknown camera parameters, making the model applicable to diverse camera settings.","Specifically, S2DHand is built on certain stereo constraints, including pair-wise cross-view consensus and invariance of transformation between both views.","These two stereo constraints are used in a complementary manner to generate pseudo-labels, allowing reliable adaptation.","Evaluation results reveal that S2DHand achieves significant improvements on arbitrary camera pairs under both in-dataset and cross-dataset settings, and outperforms existing adaptation methods with leading performance.","Project page: https://github.com/MickeyLLG/S2DHand."],"url":"http://arxiv.org/abs/2403.04381v1","category":"cs.CV"}
{"created":"2024-03-07 10:13:48","title":"Video-Driven Animation of Neural Head Avatars","abstract":"We present a new approach for video-driven animation of high-quality neural 3D head models, addressing the challenge of person-independent animation from video input. Typically, high-quality generative models are learned for specific individuals from multi-view video footage, resulting in person-specific latent representations that drive the generation process. In order to achieve person-independent animation from video input, we introduce an LSTM-based animation network capable of translating person-independent expression features into personalized animation parameters of person-specific 3D head models. Our approach combines the advantages of personalized head models (high quality and realism) with the convenience of video-driven animation employing multi-person facial performance capture. We demonstrate the effectiveness of our approach on synthesized animations with high quality based on different source videos as well as an ablation study.","sentences":["We present a new approach for video-driven animation of high-quality neural 3D head models, addressing the challenge of person-independent animation from video input.","Typically, high-quality generative models are learned for specific individuals from multi-view video footage, resulting in person-specific latent representations that drive the generation process.","In order to achieve person-independent animation from video input, we introduce an LSTM-based animation network capable of translating person-independent expression features into personalized animation parameters of person-specific 3D head models.","Our approach combines the advantages of personalized head models (high quality and realism) with the convenience of video-driven animation employing multi-person facial performance capture.","We demonstrate the effectiveness of our approach on synthesized animations with high quality based on different source videos as well as an ablation study."],"url":"http://arxiv.org/abs/2403.04380v1","category":"cs.CV"}
{"created":"2024-03-07 10:11:07","title":"Performance evaluation of conditional handover in 5G systems under fading scenario","abstract":"To enhance the handover performance in fifth generation (5G) cellular systems, conditional handover (CHO) has been evolved as a promising solution. Unlike A3 based handover where handover execution is certain after receiving handover command from the serving access network, in CHO, handover execution is conditional on the RSRP measurements from both current and target access networks, as well as on mobility parameters such as preparation and execution offsets. Analytic evaluation of conditional handover performance is unprecedented in literature. In this work, handover performance of CHO has been carried out in terms of handover latency, handover packet loss and handover failure probability. A Markov model accounting the effect of different mobility parameters (e.g., execution offset, preparation offset, time-to-preparation and time-to-execution), UE velocity and channel fading characteristics; has been proposed to characterize handover failure. Results obtained from the analytic model has been validated against extensive simulation results. Our study reveal that optimal configuration of $O_{exec}$, $O_{prep}$, $T_{exec}$ and $T_{prep}$ is actually conditional on underlying UE velocity and fading characteristics. This study will be helpful for the mobile operators to choose appropriate thresholds of the mobility parameters under different channel condition and UE velocities.","sentences":["To enhance the handover performance in fifth generation (5G) cellular systems, conditional handover (CHO) has been evolved as a promising solution.","Unlike A3 based handover where handover execution is certain after receiving handover command from the serving access network, in CHO, handover execution is conditional on the RSRP measurements from both current and target access networks, as well as on mobility parameters such as preparation and execution offsets.","Analytic evaluation of conditional handover performance is unprecedented in literature.","In this work, handover performance of CHO has been carried out in terms of handover latency, handover packet loss and handover failure probability.","A Markov model accounting the effect of different mobility parameters (e.g., execution offset, preparation offset, time-to-preparation and time-to-execution), UE velocity and channel fading characteristics; has been proposed to characterize handover failure.","Results obtained from the analytic model has been validated against extensive simulation results.","Our study reveal that optimal configuration of $O_{exec}$, $O_{prep}$, $T_{exec}$ and $T_{prep}$ is actually conditional on underlying UE velocity and fading characteristics.","This study will be helpful for the mobile operators to choose appropriate thresholds of the mobility parameters under different channel condition and UE velocities."],"url":"http://arxiv.org/abs/2403.04379v1","category":"cs.NI"}
{"created":"2024-03-07 10:06:46","title":"Model-Free Load Frequency Control of Nonlinear Power Systems Based on Deep Reinforcement Learning","abstract":"Load frequency control (LFC) is widely employed in power systems to stabilize frequency fluctuation and guarantee power quality. However, most existing LFC methods rely on accurate power system modeling and usually ignore the nonlinear characteristics of the system, limiting controllers' performance. To solve these problems, this paper proposes a model-free LFC method for nonlinear power systems based on deep deterministic policy gradient (DDPG) framework. The proposed method establishes an emulator network to emulate power system dynamics. After defining the action-value function, the emulator network is applied for control actions evaluation instead of the critic network. Then the actor network controller is effectively optimized by estimating the policy gradient based on zeroth-order optimization (ZOO) and backpropagation algorithm. Simulation results and corresponding comparisons demonstrate the designed controller can generate appropriate control actions and has strong adaptability for nonlinear power systems.","sentences":["Load frequency control (LFC) is widely employed in power systems to stabilize frequency fluctuation and guarantee power quality.","However, most existing LFC methods rely on accurate power system modeling and usually ignore the nonlinear characteristics of the system, limiting controllers' performance.","To solve these problems, this paper proposes a model-free LFC method for nonlinear power systems based on deep deterministic policy gradient (DDPG) framework.","The proposed method establishes an emulator network to emulate power system dynamics.","After defining the action-value function, the emulator network is applied for control actions evaluation instead of the critic network.","Then the actor network controller is effectively optimized by estimating the policy gradient based on zeroth-order optimization (ZOO) and backpropagation algorithm.","Simulation results and corresponding comparisons demonstrate the designed controller can generate appropriate control actions and has strong adaptability for nonlinear power systems."],"url":"http://arxiv.org/abs/2403.04374v1","category":"eess.SY"}
{"created":"2024-03-07 10:04:38","title":"Second-Order Nonlinear Circular Dichroism in Square Lattice Array of Germanium Nanohelices","abstract":"Second harmonic generation (SHG) is prohibited in centrosymmetric crystals such as silicon or germanium due to the presence of inversion symmetry. However, the structuring of such materials makes it possible to break the inversion symmetry, thus achieving generation of second-harmonic. Moreover, various symmetry properties of the resulting structure, such as chirality, also influence the SHG. In this work we investigate second harmonic generation from an array of nanohelices made of germanium. The intensity of the second harmonic displayed a remarkable enhancement of over 100 times compared to a non-structured Ge thin film, revealing the influence of interaction between nanohelices. In particular, nonlinear circular dichroism, characterized through the SHG anisotropy factor g, changed its sign not only with the helix handedness, but with its density as well. We believe that our discoveries will open up new paths for the development of nonlinear photonics based on metamaterials and metasurfaces made of centrosymmetric materials.","sentences":["Second harmonic generation (SHG) is prohibited in centrosymmetric crystals such as silicon or germanium due to the presence of inversion symmetry.","However, the structuring of such materials makes it possible to break the inversion symmetry, thus achieving generation of second-harmonic.","Moreover, various symmetry properties of the resulting structure, such as chirality, also influence the SHG.","In this work we investigate second harmonic generation from an array of nanohelices made of germanium.","The intensity of the second harmonic displayed a remarkable enhancement of over 100 times compared to a non-structured Ge thin film, revealing the influence of interaction between nanohelices.","In particular, nonlinear circular dichroism, characterized through the SHG anisotropy factor g, changed its sign not only with the helix handedness, but with its density as well.","We believe that our discoveries will open up new paths for the development of nonlinear photonics based on metamaterials and metasurfaces made of centrosymmetric materials."],"url":"http://arxiv.org/abs/2403.04372v1","category":"physics.optics"}
{"created":"2024-03-07 09:57:42","title":"From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction","abstract":"Confusing charge prediction is a challenging task in legal AI, which involves predicting confusing charges based on fact descriptions. While existing charge prediction methods have shown impressive performance, they face significant challenges when dealing with confusing charges, such as Snatch and Robbery. In the legal domain, constituent elements play a pivotal role in distinguishing confusing charges. Constituent elements are fundamental behaviors underlying criminal punishment and have subtle distinctions among charges. In this paper, we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces domain knowledge regarding constituent elements to guide the model in making judgments on confusing charges, much like a judge's reasoning process. Specifically, we first construct a legal knowledge graph containing constituent elements to help select keywords for each charge, forming a word bag. Subsequently, to guide the model's attention towards the differentiating information for each charge within the context, we expand the attention mechanism and introduce a new loss function with attention supervision through words in the word bag. We construct the confusing charges dataset from real-world judicial documents. Experiments demonstrate the effectiveness of our method, especially in maintaining exceptional performance in imbalanced label distributions.","sentences":["Confusing charge prediction is a challenging task in legal AI, which involves predicting confusing charges based on fact descriptions.","While existing charge prediction methods have shown impressive performance, they face significant challenges when dealing with confusing charges, such as Snatch and Robbery.","In the legal domain, constituent elements play a pivotal role in distinguishing confusing charges.","Constituent elements are fundamental behaviors underlying criminal punishment and have subtle distinctions among charges.","In this paper, we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces domain knowledge regarding constituent elements to guide the model in making judgments on confusing charges, much like a judge's reasoning process.","Specifically, we first construct a legal knowledge graph containing constituent elements to help select keywords for each charge, forming a word bag.","Subsequently, to guide the model's attention towards the differentiating information for each charge within the context, we expand the attention mechanism and introduce a new loss function with attention supervision through words in the word bag.","We construct the confusing charges dataset from real-world judicial documents.","Experiments demonstrate the effectiveness of our method, especially in maintaining exceptional performance in imbalanced label distributions."],"url":"http://arxiv.org/abs/2403.04369v1","category":"cs.AI"}
{"created":"2024-03-07 09:51:11","title":"Enhancing Court View Generation with Knowledge Injection and Guidance","abstract":"Court View Generation (CVG) is a challenging task in the field of Legal Artificial Intelligence (LegalAI), which aims to generate court views based on the plaintiff claims and the fact descriptions. While Pretrained Language Models (PLMs) have showcased their prowess in natural language generation, their application to the complex, knowledge-intensive domain of CVG often reveals inherent limitations. In this paper, we present a novel approach, named Knowledge Injection and Guidance (KIG), designed to bolster CVG using PLMs. To efficiently incorporate domain knowledge during the training stage, we introduce a knowledge-injected prompt encoder for prompt tuning, thereby reducing computational overhead. Moreover, to further enhance the model's ability to utilize domain knowledge, we employ a generating navigator, which dynamically guides the text generation process in the inference stage without altering the model's architecture, making it readily transferable. Comprehensive experiments on real-world data demonstrate the effectiveness of our approach compared to several established baselines, especially in the responsivity of claims, where it outperforms the best baseline by 11.87%.","sentences":["Court View Generation (CVG) is a challenging task in the field of Legal Artificial Intelligence (LegalAI), which aims to generate court views based on the plaintiff claims and the fact descriptions.","While Pretrained Language Models (PLMs) have showcased their prowess in natural language generation, their application to the complex, knowledge-intensive domain of CVG often reveals inherent limitations.","In this paper, we present a novel approach, named Knowledge Injection and Guidance (KIG), designed to bolster CVG using PLMs.","To efficiently incorporate domain knowledge during the training stage, we introduce a knowledge-injected prompt encoder for prompt tuning, thereby reducing computational overhead.","Moreover, to further enhance the model's ability to utilize domain knowledge, we employ a generating navigator, which dynamically guides the text generation process in the inference stage without altering the model's architecture, making it readily transferable.","Comprehensive experiments on real-world data demonstrate the effectiveness of our approach compared to several established baselines, especially in the responsivity of claims, where it outperforms the best baseline by 11.87%."],"url":"http://arxiv.org/abs/2403.04366v1","category":"cs.AI"}
{"created":"2024-03-07 09:49:37","title":"Promising Stabs in the Dark: Theory Virtues and Pursuit-Worthiness in the Dark Energy Problem","abstract":"The paper argues that we ought to conceive of the Dark Energy problem -- the question of how to account for observational data, naturally interpreted as accelerated expansion of the universe -- as a crisis of underdetermined pursuit-worthiness. Not only are the various approaches to the Dark Energy problem evidentially underdetermined; at present, no compelling reasons single out any of them as more likely to be true than the other. More vexingly for working scientists, none of the approaches stands out as uncontroversially preferable over its rivals in terms of its rationally warranted promise, i.e. the reasons to further work on, explore and develop it. We demonstrate this claim by applying a Peircean economic model of pursuit-worthiness in terms of a cognitive cost/benefit estimate -- with the instantiation of theory virtues as key indicators of cognitive gains -- to the four main Dark Energy proposals (the cosmological constant approach, modified gravity, quintessence, and inhomogeneous cosmologies). Our analysis yields that these approaches do not admit of an unambiguous, or uncontroversial, ranking with respect to which ansatz deserves distinguished attention and research efforts. The overall methodological counsel that our analysis underwrites recommends a pragmatic double research strategy forward: to encourage and foster theory pluralism and the search for tests -- with the goal of enhancing the testability of the $\\Lambda$CDM model and \"testing it to destruction\".","sentences":["The paper argues that we ought to conceive of the Dark Energy problem -- the question of how to account for observational data, naturally interpreted as accelerated expansion of the universe -- as a crisis of underdetermined pursuit-worthiness.","Not only are the various approaches to the Dark Energy problem evidentially underdetermined; at present, no compelling reasons single out any of them as more likely to be true than the other.","More vexingly for working scientists, none of the approaches stands out as uncontroversially preferable over its rivals in terms of its rationally warranted promise, i.e. the reasons to further work on, explore and develop it.","We demonstrate this claim by applying a Peircean economic model of pursuit-worthiness in terms of a cognitive cost/benefit estimate -- with the instantiation of theory virtues as key indicators of cognitive gains -- to the four main Dark Energy proposals (the cosmological constant approach, modified gravity, quintessence, and inhomogeneous cosmologies).","Our analysis yields that these approaches do not admit of an unambiguous, or uncontroversial, ranking with respect to which ansatz deserves distinguished attention and research efforts.","The overall methodological counsel that our analysis underwrites recommends a pragmatic double research strategy forward: to encourage and foster theory pluralism and the search for tests -- with the goal of enhancing the testability of the $\\Lambda$CDM model and \"testing it to destruction\"."],"url":"http://arxiv.org/abs/2403.04364v1","category":"physics.hist-ph"}
{"created":"2024-03-07 09:48:13","title":"Multi-step Temporal Modeling for UAV Tracking","abstract":"In the realm of unmanned aerial vehicle (UAV) tracking, Siamese-based approaches have gained traction due to their optimal balance between efficiency and precision. However, UAV scenarios often present challenges such as insufficient sampling resolution, fast motion and small objects with limited feature information. As a result, temporal context in UAV tracking tasks plays a pivotal role in target location, overshadowing the target's precise features. In this paper, we introduce MT-Track, a streamlined and efficient multi-step temporal modeling framework designed to harness the temporal context from historical frames for enhanced UAV tracking. This temporal integration occurs in two steps: correlation map generation and correlation map refinement. Specifically, we unveil a unique temporal correlation module that dynamically assesses the interplay between the template and search region features. This module leverages temporal information to refresh the template feature, yielding a more precise correlation map. Subsequently, we propose a mutual transformer module to refine the correlation maps of historical and current frames by modeling the temporal knowledge in the tracking sequence. This method significantly trims computational demands compared to the raw transformer. The compact yet potent nature of our tracking framework ensures commendable tracking outcomes, particularly in extended tracking scenarios.","sentences":["In the realm of unmanned aerial vehicle (UAV) tracking, Siamese-based approaches have gained traction due to their optimal balance between efficiency and precision.","However, UAV scenarios often present challenges such as insufficient sampling resolution, fast motion and small objects with limited feature information.","As a result, temporal context in UAV tracking tasks plays a pivotal role in target location, overshadowing the target's precise features.","In this paper, we introduce MT-Track, a streamlined and efficient multi-step temporal modeling framework designed to harness the temporal context from historical frames for enhanced UAV tracking.","This temporal integration occurs in two steps: correlation map generation and correlation map refinement.","Specifically, we unveil a unique temporal correlation module that dynamically assesses the interplay between the template and search region features.","This module leverages temporal information to refresh the template feature, yielding a more precise correlation map.","Subsequently, we propose a mutual transformer module to refine the correlation maps of historical and current frames by modeling the temporal knowledge in the tracking sequence.","This method significantly trims computational demands compared to the raw transformer.","The compact yet potent nature of our tracking framework ensures commendable tracking outcomes, particularly in extended tracking scenarios."],"url":"http://arxiv.org/abs/2403.04363v1","category":"cs.CV"}
{"created":"2024-03-07 09:41:53","title":"Eigenvalues and the stabilized automorphism group","abstract":"We study the stabilized automorphism group of minimal and, more generally, certain transitive dynamical systems. Our approach involves developing new algebraic tools to extract information about the rational eigenvalues of these systems from their stabilized automorphism groups. In particular, we prove that if two minimal system have isomorphic stabilized automorphism groups and each has at least one non-trivial rational eigenvalue, then the systems have the same rational eigenvalues. Using these tools, we also extend Schmieding's result on the recovery of entropy from the stabilized automorphism group to include irreducible shifts of finite type.","sentences":["We study the stabilized automorphism group of minimal and, more generally, certain transitive dynamical systems.","Our approach involves developing new algebraic tools to extract information about the rational eigenvalues of these systems from their stabilized automorphism groups.","In particular, we prove that if two minimal system have isomorphic stabilized automorphism groups and each has at least one non-trivial rational eigenvalue, then the systems have the same rational eigenvalues.","Using these tools, we also extend Schmieding's result on the recovery of entropy from the stabilized automorphism group to include irreducible shifts of finite type."],"url":"http://arxiv.org/abs/2403.04360v1","category":"math.DS"}
{"created":"2024-03-07 09:41:11","title":"Symmetry Considerations for Learning Task Symmetric Robot Policies","abstract":"Symmetry is a fundamental aspect of many real-world robotic tasks. However, current deep reinforcement learning (DRL) approaches can seldom harness and exploit symmetry effectively. Often, the learned behaviors fail to achieve the desired transformation invariances and suffer from motion artifacts. For instance, a quadruped may exhibit different gaits when commanded to move forward or backward, even though it is symmetrical about its torso. This issue becomes further pronounced in high-dimensional or complex environments, where DRL methods are prone to local optima and fail to explore regions of the state space equally. Past methods on encouraging symmetry for robotic tasks have studied this topic mainly in a single-task setting, where symmetry usually refers to symmetry in the motion, such as the gait patterns. In this paper, we revisit this topic for goal-conditioned tasks in robotics, where symmetry lies mainly in task execution and not necessarily in the learned motions themselves. In particular, we investigate two approaches to incorporate symmetry invariance into DRL -- data augmentation and mirror loss function. We provide a theoretical foundation for using augmented samples in an on-policy setting. Based on this, we show that the corresponding approach achieves faster convergence and improves the learned behaviors in various challenging robotic tasks, from climbing boxes with a quadruped to dexterous manipulation.","sentences":["Symmetry is a fundamental aspect of many real-world robotic tasks.","However, current deep reinforcement learning (DRL) approaches can seldom harness and exploit symmetry effectively.","Often, the learned behaviors fail to achieve the desired transformation invariances and suffer from motion artifacts.","For instance, a quadruped may exhibit different gaits when commanded to move forward or backward, even though it is symmetrical about its torso.","This issue becomes further pronounced in high-dimensional or complex environments, where DRL methods are prone to local optima and fail to explore regions of the state space equally.","Past methods on encouraging symmetry for robotic tasks have studied this topic mainly in a single-task setting, where symmetry usually refers to symmetry in the motion, such as the gait patterns.","In this paper, we revisit this topic for goal-conditioned tasks in robotics, where symmetry lies mainly in task execution and not necessarily in the learned motions themselves.","In particular, we investigate two approaches to incorporate symmetry invariance into DRL -- data augmentation and mirror loss function.","We provide a theoretical foundation for using augmented samples in an on-policy setting.","Based on this, we show that the corresponding approach achieves faster convergence and improves the learned behaviors in various challenging robotic tasks, from climbing boxes with a quadruped to dexterous manipulation."],"url":"http://arxiv.org/abs/2403.04359v1","category":"cs.RO"}
{"created":"2024-03-07 09:37:31","title":"A Logarithmic Mean Divisia Index Decomposition of CO$_2$ Emissions from Energy Use in Romania","abstract":"Carbon emissions have become a specific alarming indicators and intricate challenges that lead an extended argue about climate change. The growing trend in the utilization of fossil fuels for the economic progress and simultaneously reducing the carbon quantity has turn into a substantial and global challenge. The aim of this paper is to examine the driving factors of CO$_2$ emissions from energy sector in Romania during the period 2008-2022 emissions using the log mean Divisia index (LMDI) method and takes into account five items: CO$_2$ emissions, primary energy resources, energy consumption, gross domestic product and population, the driving forces of CO$_2$ emissions, based on which it was calculated the contribution of carbon intensity, energy mixes, generating efficiency, economy, and population. The results indicate that generating efficiency effect -90968.57 is the largest inhibiting index while economic effect is the largest positive index 69084.04 having the role of increasing CO$_2$ emissions.","sentences":["Carbon emissions have become a specific alarming indicators and intricate challenges that lead an extended argue about climate change.","The growing trend in the utilization of fossil fuels for the economic progress and simultaneously reducing the carbon quantity has turn into a substantial and global challenge.","The aim of this paper is to examine the driving factors of CO$_2$ emissions from energy sector in Romania during the period 2008-2022 emissions using the log mean Divisia index (LMDI) method and takes into account five items: CO$_2$ emissions, primary energy resources, energy consumption, gross domestic product and population, the driving forces of CO$_2$ emissions, based on which it was calculated the contribution of carbon intensity, energy mixes, generating efficiency, economy, and population.","The results indicate that generating efficiency effect -90968.57 is the largest inhibiting index while economic effect is the largest positive index 69084.04 having the role of increasing CO$_2$ emissions."],"url":"http://arxiv.org/abs/2403.04354v1","category":"econ.EM"}
{"created":"2024-03-07 09:35:49","title":"Spatiotemporal Pooling on Appropriate Topological Maps Represented as Two-Dimensional Images for EEG Classification","abstract":"Motor imagery classification based on electroencephalography (EEG) signals is one of the most important brain-computer interface applications, although it needs further improvement. Several methods have attempted to obtain useful information from EEG signals by using recent deep learning techniques such as transformers. To improve the classification accuracy, this study proposes a novel EEG-based motor imagery classification method with three key features: generation of a topological map represented as a two-dimensional image from EEG signals with coordinate transformation based on t-SNE, use of the InternImage to extract spatial features, and use of spatiotemporal pooling inspired by PoolFormer to exploit spatiotemporal information concealed in a sequence of EEG images. Experimental results using the PhysioNet EEG Motor Movement/Imagery dataset showed that the proposed method achieved the best classification accuracy of 88.57%, 80.65%, and 70.17% on two-, three-, and four-class motor imagery tasks in cross-individual validation.","sentences":["Motor imagery classification based on electroencephalography (EEG) signals is one of the most important brain-computer interface applications, although it needs further improvement.","Several methods have attempted to obtain useful information from EEG signals by using recent deep learning techniques such as transformers.","To improve the classification accuracy, this study proposes a novel EEG-based motor imagery classification method with three key features: generation of a topological map represented as a two-dimensional image from EEG signals with coordinate transformation based on t-SNE, use of the InternImage to extract spatial features, and use of spatiotemporal pooling inspired by PoolFormer to exploit spatiotemporal information concealed in a sequence of EEG images.","Experimental results using the PhysioNet EEG Motor Movement/Imagery dataset showed that the proposed method achieved the best classification accuracy of 88.57%, 80.65%, and 70.17% on two-, three-, and four-class motor imagery tasks in cross-individual validation."],"url":"http://arxiv.org/abs/2403.04353v1","category":"cs.CV"}
{"created":"2024-03-07 09:35:43","title":"Fast fitting of spectral lines with Gaussian and hyperfine structure models","abstract":"The fitting of spectral lines is a common step in the analysis of line observations and simulations. However, the observational noise, the presence of multiple velocity components, and potentially large data sets make it a non-trivial task. We present a new computer program Spectrum Iterative Fitter (SPIF) for the fitting of spectra with Gaussians or with hyperfine line profiles. The aim is to show the computational efficiency of the program and to use it to examine the general accuracy of approximating spectra with simple models. We describe the implementation of the program. To characterise its performance, we examined spectra with isolated Gaussian components or a hyperfine structure, also using synthetic observations from numerical simulations of interstellar clouds. We examined the search for the globally optimal fit and the accuracy to which single-velocity-component and multi-component fits recover true values for parameters such as line areas, velocity dispersion, and optical depth. The program is shown to be fast, with fits of single Gaussian components reaching on graphics processing units speeds approaching one million spectra per second. This also makes it feasible to use Monte Carlo simulations or Markov chain Monte Carlo calculations for the error estimation. However, in the case of hyperfine structure lines, degeneracies affect the parameter estimation and can complicate the derivation of the error estimates. The use of many random initial values makes the fits more robust, both for locating the global $\\chi^2$ minimum and for the selection of the optimal number of velocity components.","sentences":["The fitting of spectral lines is a common step in the analysis of line observations and simulations.","However, the observational noise, the presence of multiple velocity components, and potentially large data sets make it a non-trivial task.","We present a new computer program Spectrum Iterative Fitter (SPIF) for the fitting of spectra with Gaussians or with hyperfine line profiles.","The aim is to show the computational efficiency of the program and to use it to examine the general accuracy of approximating spectra with simple models.","We describe the implementation of the program.","To characterise its performance, we examined spectra with isolated Gaussian components or a hyperfine structure, also using synthetic observations from numerical simulations of interstellar clouds.","We examined the search for the globally optimal fit and the accuracy to which single-velocity-component and multi-component fits recover true values for parameters such as line areas, velocity dispersion, and optical depth.","The program is shown to be fast, with fits of single Gaussian components reaching on graphics processing units speeds approaching one million spectra per second.","This also makes it feasible to use Monte Carlo simulations or Markov chain Monte Carlo calculations for the error estimation.","However, in the case of hyperfine structure lines, degeneracies affect the parameter estimation and can complicate the derivation of the error estimates.","The use of many random initial values makes the fits more robust, both for locating the global $\\chi^2$ minimum and for the selection of the optimal number of velocity components."],"url":"http://arxiv.org/abs/2403.04352v1","category":"astro-ph.GA"}
{"created":"2024-03-07 09:28:13","title":"Extract non-Gaussian Features in Gravitational Wave Observation Data Using Self-Supervised Learning","abstract":"We propose a self-supervised learning model to denoise gravitational wave (GW) signals in the time series strain data without relying on waveform information. Denoising GW data is a crucial intermediate process for machine-learning-based data analysis techniques, as it can simplify the model for downstream tasks such as detections and parameter estimations. We use the blind-spot neural network and train it with whitened strain data with GW signals injected as both input data and target. Under the assumption of a Gaussian noise model, our model successfully denoises 38% of GW signals from binary black hole mergers in H1 data and 49% of signals in L1 data detected in the O1, O2, and O3 observation runs with an overlap greater than 0.5. We also test the model's potential to extract glitch features and loud inspiral compact binary coalescence signals a few seconds before the merger.","sentences":["We propose a self-supervised learning model to denoise gravitational wave (GW) signals in the time series strain data without relying on waveform information.","Denoising GW data is a crucial intermediate process for machine-learning-based data analysis techniques, as it can simplify the model for downstream tasks such as detections and parameter estimations.","We use the blind-spot neural network and train it with whitened strain data with GW signals injected as both input data and target.","Under the assumption of a Gaussian noise model, our model successfully denoises 38% of GW signals from binary black hole mergers in H1 data and 49% of signals in L1 data detected in the O1, O2, and O3 observation runs with an overlap greater than 0.5.","We also test the model's potential to extract glitch features and loud inspiral compact binary coalescence signals a few seconds before the merger."],"url":"http://arxiv.org/abs/2403.04350v1","category":"gr-qc"}
{"created":"2024-03-07 09:22:50","title":"LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression","abstract":"In Distributed optimization and Learning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of Local training, which reduces the communication frequency, and Compression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogenous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms.","sentences":["In Distributed optimization and Learning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical.","We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of Local training, which reduces the communication frequency, and Compression, in which short bitstreams are sent instead of full-dimensional vectors of floats.","LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods.","LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogenous regime with strongly convex functions.","This is confirmed in practice, with LoCoDL outperforming existing algorithms."],"url":"http://arxiv.org/abs/2403.04348v1","category":"math.OC"}
{"created":"2024-03-07 09:16:05","title":"A Novel Theoretical Framework for Exponential Smoothing","abstract":"Simple Exponential Smoothing is a classical technique used for smoothing time series data by assigning exponentially decreasing weights to past observations through a recursive equation; it is sometimes presented as a rule of thumb procedure. We introduce a novel theoretical perspective where the recursive equation that defines simple exponential smoothing occurs naturally as a stochastic gradient ascent scheme to optimize a sequence of Gaussian log-likelihood functions. Under this lens of analysis, our main theorem shows that -- in a general setting -- simple exponential smoothing converges to a neighborhood of the trend of a trend-stationary stochastic process. This offers a novel theoretical assurance that the exponential smoothing procedure yields reliable estimators of the underlying trend shedding light on long-standing observations in the literature regarding the robustness of simple exponential smoothing.","sentences":["Simple Exponential Smoothing is a classical technique used for smoothing time series data by assigning exponentially decreasing weights to past observations through a recursive equation; it is sometimes presented as a rule of thumb procedure.","We introduce a novel theoretical perspective where the recursive equation that defines simple exponential smoothing occurs naturally as a stochastic gradient ascent scheme to optimize a sequence of Gaussian log-likelihood functions.","Under this lens of analysis, our main theorem shows that -- in a general setting -- simple exponential smoothing converges to a neighborhood of the trend of a trend-stationary stochastic process.","This offers a novel theoretical assurance that the exponential smoothing procedure yields reliable estimators of the underlying trend shedding light on long-standing observations in the literature regarding the robustness of simple exponential smoothing."],"url":"http://arxiv.org/abs/2403.04345v1","category":"stat.ME"}
{"created":"2024-03-07 09:11:16","title":"CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning","abstract":"Visual instruction tuning is a key training stage of large multimodal models (LMMs). Nevertheless, the common practice of indiscriminately mixing instruction-following data from various tasks may result in suboptimal overall performance due to different instruction formats and knowledge domains across tasks. To mitigate this issue, we propose a novel Comprehensive Task Balancing (CoTBal) algorithm for multi-task visual instruction tuning of LMMs. To our knowledge, this is the first work that explores multi-task optimization in visual instruction tuning. Specifically, we consider two key dimensions for task balancing: (1) Inter-Task Contribution, the phenomenon where learning one task potentially enhances the performance in other tasks, attributable to the overlapping knowledge domains, and (2) Intra-Task Difficulty, which refers to the learning difficulty within a single task. By quantifying these two dimensions with performance-based metrics, task balancing is thus enabled by assigning more weights to tasks that offer substantial contributions to others, receive minimal contributions from others, and also have great intra-task difficulties. Experiments show that our CoTBal leads to superior overall performance in multi-task visual instruction tuning.","sentences":["Visual instruction tuning is a key training stage of large multimodal models (LMMs).","Nevertheless, the common practice of indiscriminately mixing instruction-following data from various tasks may result in suboptimal overall performance due to different instruction formats and knowledge domains across tasks.","To mitigate this issue, we propose a novel Comprehensive Task Balancing (CoTBal) algorithm for multi-task visual instruction tuning of LMMs.","To our knowledge, this is the first work that explores multi-task optimization in visual instruction tuning.","Specifically, we consider two key dimensions for task balancing: (1) Inter-Task Contribution, the phenomenon where learning one task potentially enhances the performance in other tasks, attributable to the overlapping knowledge domains, and (2) Intra-Task Difficulty, which refers to the learning difficulty within a single task.","By quantifying these two dimensions with performance-based metrics, task balancing is thus enabled by assigning more weights to tasks that offer substantial contributions to others, receive minimal contributions from others, and also have great intra-task difficulties.","Experiments show that our CoTBal leads to superior overall performance in multi-task visual instruction tuning."],"url":"http://arxiv.org/abs/2403.04343v1","category":"cs.AI"}
{"created":"2024-03-07 09:02:11","title":"Explainable AI for Embedded Systems Design: A Case Study of Static Redundant NVM Memory Write Prediction","abstract":"This paper investigates the application of eXplainable Artificial Intelligence (XAI) in the design of embedded systems using machine learning (ML). As a case study, it addresses the challenging problem of static silent store prediction. This involves identifying redundant memory writes based only on static program features. Eliminating such stores enhances performance and energy efficiency by reducing memory access and bus traffic, especially in the presence of emerging non-volatile memory technologies. To achieve this, we propose a methodology consisting of: 1) the development of relevant ML models for explaining silent store prediction, and 2) the application of XAI to explain these models. We employ two state-of-the-art model-agnostic XAI methods to analyze the causes of silent stores. Through the case study, we evaluate the effectiveness of the methods. We find that these methods provide explanations for silent store predictions, which are consistent with known causes of silent store occurrences from previous studies. Typically, this allows us to confirm the prevalence of silent stores in operations that write the zero constant into memory, or the absence of silent stores in operations involving loop induction variables. This suggests the potential relevance of XAI in analyzing ML models' decision in embedded system design. From the case study, we share some valuable insights and pitfalls we encountered. More generally, this study aims to lay the groundwork for future research in the emerging field of XAI for embedded system design.","sentences":["This paper investigates the application of eXplainable Artificial Intelligence (XAI) in the design of embedded systems using machine learning (ML).","As a case study, it addresses the challenging problem of static silent store prediction.","This involves identifying redundant memory writes based only on static program features.","Eliminating such stores enhances performance and energy efficiency by reducing memory access and bus traffic, especially in the presence of emerging non-volatile memory technologies.","To achieve this, we propose a methodology consisting of: 1) the development of relevant ML models for explaining silent store prediction, and 2) the application of XAI to explain these models.","We employ two state-of-the-art model-agnostic XAI methods to analyze the causes of silent stores.","Through the case study, we evaluate the effectiveness of the methods.","We find that these methods provide explanations for silent store predictions, which are consistent with known causes of silent store occurrences from previous studies.","Typically, this allows us to confirm the prevalence of silent stores in operations that write the zero constant into memory, or the absence of silent stores in operations involving loop induction variables.","This suggests the potential relevance of XAI in analyzing ML models' decision in embedded system design.","From the case study, we share some valuable insights and pitfalls we encountered.","More generally, this study aims to lay the groundwork for future research in the emerging field of XAI for embedded system design."],"url":"http://arxiv.org/abs/2403.04337v1","category":"cs.LG"}
{"created":"2024-03-07 09:01:58","title":"Periodicity in Hedge-myopic system and an asymmetric NE-solving paradigm for two-player zero-sum games","abstract":"In this paper, we consider the $n \\times n$ two-payer zero-sum repeated game in which one player (player X) employs the popular Hedge (also called multiplicative weights update) learning algorithm while the other player (player Y) adopts the myopic best response. We investigate the dynamics of such Hedge-myopic system by defining a metric $Q(\\textbf{x}_t)$, which measures the distance between the stage strategy $\\textbf{x}_t$ and Nash Equilibrium (NE) strategy of player X. We analyze the trend of $Q(\\textbf{x}_t)$ and prove that it is bounded and can only take finite values on the evolutionary path when the payoff matrix is rational and the game has an interior NE. Based on this, we prove that the stage strategy sequence of both players are periodic after finite stages and the time-averaged strategy of player Y within one period is an exact NE strategy. Accordingly, we propose an asymmetric paradigm for solving two-player zero-sum games. For the special game with rational payoff matrix and an interior NE, the paradigm can output the precise NE strategy; for any general games we prove that the time-averaged strategy can converge to an approximate NE. In comparison to the NE-solving method via Hedge self-play, this HBR paradigm exhibits faster computation/convergence, better stability and can attain precise NE convergence in most real cases.","sentences":["In this paper, we consider the $n \\times n$ two-payer zero-sum repeated game in which one player (player X) employs the popular Hedge (also called multiplicative weights update) learning algorithm while the other player (player Y) adopts the myopic best response.","We investigate the dynamics of such Hedge-myopic system by defining a metric $Q(\\textbf{x}_t)$, which measures the distance between the stage strategy $\\textbf{x}_t$ and Nash Equilibrium (NE) strategy of player X.","We analyze the trend of $Q(\\textbf{x}_t)$ and prove that it is bounded and can only take finite values on the evolutionary path when the payoff matrix is rational and the game has an interior NE.","Based on this, we prove that the stage strategy sequence of both players are periodic after finite stages and the time-averaged strategy of player Y within one period is an exact NE strategy.","Accordingly, we propose an asymmetric paradigm for solving two-player zero-sum games.","For the special game with rational payoff matrix and an interior NE, the paradigm can output the precise NE strategy; for any general games we prove that the time-averaged strategy can converge to an approximate NE.","In comparison to the NE-solving method via Hedge self-play, this HBR paradigm exhibits faster computation/convergence, better stability and can attain precise NE convergence in most real cases."],"url":"http://arxiv.org/abs/2403.04336v1","category":"math.DS"}
{"created":"2024-03-07 08:48:04","title":"ProMoAI: Process Modeling with Generative AI","abstract":"ProMoAI is a novel tool that leverages Large Language Models (LLMs) to automatically generate process models from textual descriptions, incorporating advanced prompt engineering, error handling, and code generation techniques. Beyond automating the generation of complex process models, ProMoAI also supports process model optimization. Users can interact with the tool by providing feedback on the generated model, which is then used for refining the process model. ProMoAI utilizes the capabilities LLMs to offer a novel, AI-driven approach to process modeling, significantly reducing the barrier to entry for users without deep technical knowledge in process modeling.","sentences":["ProMoAI is a novel tool that leverages Large Language Models (LLMs) to automatically generate process models from textual descriptions, incorporating advanced prompt engineering, error handling, and code generation techniques.","Beyond automating the generation of complex process models, ProMoAI also supports process model optimization.","Users can interact with the tool by providing feedback on the generated model, which is then used for refining the process model.","ProMoAI utilizes the capabilities LLMs to offer a novel, AI-driven approach to process modeling, significantly reducing the barrier to entry for users without deep technical knowledge in process modeling."],"url":"http://arxiv.org/abs/2403.04327v1","category":"cs.DB"}
{"created":"2024-03-07 08:45:31","title":"Edge-based Parametric Digital Twins for Intelligent Building Indoor Climate Modeling","abstract":"Digital transformation in the built environment generates vast data for developing data-driven models to optimize building operations. This study presents an integrated solution utilizing edge computing, digital twins, and deep learning to enhance the understanding of climate in buildings. Parametric digital twins, created using an ontology, ensure consistent data representation across diverse service systems equipped by different buildings. Based on created digital twins and collected data, deep learning methods are employed to develop predictive models for identifying patterns in indoor climate and providing insights. Both the parametric digital twin and deep learning models are deployed on edge for low latency and privacy compliance. As a demonstration, a case study was conducted in a historic building in \\\"Osterg\\\"otland, Sweden, to compare the performance of five deep learning architectures. The results indicate that the time-series dense encoder model exhibited strong competitiveness in performing multi-horizon forecasts of indoor temperature and relative humidity with low computational costs.","sentences":["Digital transformation in the built environment generates vast data for developing data-driven models to optimize building operations.","This study presents an integrated solution utilizing edge computing, digital twins, and deep learning to enhance the understanding of climate in buildings.","Parametric digital twins, created using an ontology, ensure consistent data representation across diverse service systems equipped by different buildings.","Based on created digital twins and collected data, deep learning methods are employed to develop predictive models for identifying patterns in indoor climate and providing insights.","Both the parametric digital twin and deep learning models are deployed on edge for low latency and privacy compliance.","As a demonstration, a case study was conducted in a historic building in \\\"Osterg\\\"otland, Sweden, to compare the performance of five deep learning architectures.","The results indicate that the time-series dense encoder model exhibited strong competitiveness in performing multi-horizon forecasts of indoor temperature and relative humidity with low computational costs."],"url":"http://arxiv.org/abs/2403.04326v1","category":"eess.SY"}
{"created":"2024-03-07 08:44:42","title":"Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models","abstract":"The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension.","sentences":["The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension.","Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking.","Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension.","Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension."],"url":"http://arxiv.org/abs/2403.04325v1","category":"cs.CL"}
{"created":"2024-03-07 08:37:36","title":"Memetic Differential Evolution Methods for Semi-Supervised Clustering","abstract":"In this paper, we deal with semi-supervised Minimum Sum-of-Squares Clustering (MSSC) problems where background knowledge is given in the form of instance-level constraints. In particular, we take into account \"must-link\" and \"cannot-link\" constraints, each of which indicates if two dataset points should be associated to the same or to a different cluster. The presence of such constraints makes the problem at least as hard as its unsupervised version: it is no more true that each point is associated to its nearest cluster center, thus requiring some modifications in crucial operations, such as the assignment step. In this scenario, we propose a novel memetic strategy based on the Differential Evolution paradigm, directly extending a state-of-the-art framework recently proposed in the unsupervised clustering literature. As far as we know, our contribution represents the first attempt to define a memetic methodology designed to generate a (hopefully) optimal feasible solution for the semi-supervised MSSC problem. The proposal is compared with some state-of-the-art algorithms from the literature on a set of well-known datasets, highlighting its effectiveness and efficiency in finding good quality clustering solutions.","sentences":["In this paper, we deal with semi-supervised Minimum Sum-of-Squares Clustering (MSSC) problems where background knowledge is given in the form of instance-level constraints.","In particular, we take into account \"must-link\" and \"cannot-link\" constraints, each of which indicates if two dataset points should be associated to the same or to a different cluster.","The presence of such constraints makes the problem at least as hard as its unsupervised version: it is no more true that each point is associated to its nearest cluster center, thus requiring some modifications in crucial operations, such as the assignment step.","In this scenario, we propose a novel memetic strategy based on the Differential Evolution paradigm, directly extending a state-of-the-art framework recently proposed in the unsupervised clustering literature.","As far as we know, our contribution represents the first attempt to define a memetic methodology designed to generate a (hopefully) optimal feasible solution for the semi-supervised MSSC problem.","The proposal is compared with some state-of-the-art algorithms from the literature on a set of well-known datasets, highlighting its effectiveness and efficiency in finding good quality clustering solutions."],"url":"http://arxiv.org/abs/2403.04322v1","category":"math.OC"}
{"created":"2024-03-07 08:37:33","title":"Discriminative Probing and Tuning for Text-to-Image Generation","abstract":"Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a bonus of the discriminative adapter, a self-correction mechanism can leverage discriminative gradients to better align generated images to text prompts during inference. Comprehensive evaluations across three benchmark datasets, including both in-distribution and out-of-distribution scenarios, demonstrate our method's superior generation performance. Meanwhile, it achieves state-of-the-art discriminative performance on the two discriminative tasks compared to other generative models.","sentences":["Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images.","Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning.","However, the inherent alignment capabilities of T2I models are still inadequate.","By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation.","In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation.","We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment.","As a bonus of the discriminative adapter, a self-correction mechanism can leverage discriminative gradients to better align generated images to text prompts during inference.","Comprehensive evaluations across three benchmark datasets, including both in-distribution and out-of-distribution scenarios, demonstrate our method's superior generation performance.","Meanwhile, it achieves state-of-the-art discriminative performance on the two discriminative tasks compared to other generative models."],"url":"http://arxiv.org/abs/2403.04321v1","category":"cs.CV"}
{"created":"2024-03-07 08:34:57","title":"Online Adaptation of Language Models with a Memory of Amortized Contexts","abstract":"Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs. Due to this crucial need to keep models updated, online learning has emerged as a critical necessity when utilizing LLMs for real-world applications. However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention. We propose an amortized feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient manner, we utilize amortization-based meta-learning, which substitutes the optimization process with a single forward pass of the encoder. Subsequently, we learn to choose from and aggregate selected documents into a single modulation by conditioning on the question, allowing us to adapt a frozen language model during test time without requiring further gradient updates. Our experiment demonstrates the superiority of MAC in multiple aspects, including online adaptation performance, time, and memory efficiency. Code is available at: https://github.com/jihoontack/MAC.","sentences":["Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs.","Due to this crucial need to keep models updated, online learning has emerged as a critical necessity when utilizing LLMs for real-world applications.","However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential.","To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention.","We propose an amortized feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank.","When answering questions, our model attends to and extracts relevant knowledge from this memory bank.","To learn informative modulations in an efficient manner, we utilize amortization-based meta-learning, which substitutes the optimization process with a single forward pass of the encoder.","Subsequently, we learn to choose from and aggregate selected documents into a single modulation by conditioning on the question, allowing us to adapt a frozen language model during test time without requiring further gradient updates.","Our experiment demonstrates the superiority of MAC in multiple aspects, including online adaptation performance, time, and memory efficiency.","Code is available at: https://github.com/jihoontack/MAC."],"url":"http://arxiv.org/abs/2403.04317v1","category":"cs.LG"}
{"created":"2024-03-07 08:34:12","title":"Testing scale-invariant inflation against cosmological data","abstract":"There is solid theoretical and observational motivation behind the idea of scale-invariance as a fundamental symmetry of Nature. We consider a recently proposed classically scale-invariant inflationary model, quadratic in curvature and featuring a scalar field non-minimally coupled to gravity. We go beyond earlier analytical studies, which showed that the model predicts inflationary observables in qualitative agreement with data, by solving the full two-field dynamics of the system -- this allows us to corroborate previous analytical findings and set robust constraints on the model's parameters using the latest Cosmic Microwave Background (CMB) data from Planck and BICEP/Keck. We demonstrate that scale-invariance constrains the two-field trajectory such that the effective dynamics are that of a single field, resulting in vanishing entropy perturbations and protecting the model from destabilization effects. We derive tight upper limits on the non-minimal coupling strength, excluding conformal coupling at high significance. By explicitly sampling over them, we demonstrate an overall insensitivity to initial conditions. We argue that the model \\textit{predicts} a minimal level of primordial tensor modes set by $r \\gtrsim 0.003$, well within the reach of next-generation CMB experiments. These will therefore provide a litmus test of scale-invariant inflation, and we comment on the possibility of distinguishing the model from Starobinsky and $\\alpha$-attractor inflation. Overall, we argue that scale-invariant inflation is in excellent health, and possesses features which make it an interesting benchmark for tests of inflation from future CMB data.","sentences":["There is solid theoretical and observational motivation behind the idea of scale-invariance as a fundamental symmetry of Nature.","We consider a recently proposed classically scale-invariant inflationary model, quadratic in curvature and featuring a scalar field non-minimally coupled to gravity.","We go beyond earlier analytical studies, which showed that the model predicts inflationary observables in qualitative agreement with data, by solving the full two-field dynamics of the system -- this allows us to corroborate previous analytical findings and set robust constraints on the model's parameters using the latest Cosmic Microwave Background (CMB) data from Planck and BICEP/Keck.","We demonstrate that scale-invariance constrains the two-field trajectory such that the effective dynamics are that of a single field, resulting in vanishing entropy perturbations and protecting the model from destabilization effects.","We derive tight upper limits on the non-minimal coupling strength, excluding conformal coupling at high significance.","By explicitly sampling over them, we demonstrate an overall insensitivity to initial conditions.","We argue that the model \\textit{predicts} a minimal level of primordial tensor modes set by $r \\gtrsim 0.003$, well within the reach of next-generation CMB experiments.","These will therefore provide a litmus test of scale-invariant inflation, and we comment on the possibility of distinguishing the model from Starobinsky and $\\alpha$-attractor inflation.","Overall, we argue that scale-invariant inflation is in excellent health, and possesses features which make it an interesting benchmark for tests of inflation from future CMB data."],"url":"http://arxiv.org/abs/2403.04316v1","category":"astro-ph.CO"}
{"created":"2024-03-07 08:32:17","title":"Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders","abstract":"Conversational systems often rely on embedding models for intent classification and intent clustering tasks. The advent of Large Language Models (LLMs), which enable instructional embeddings allowing one to adjust semantics over the embedding space using prompts, are being viewed as a panacea for these downstream conversational tasks. However, traditional evaluation benchmarks rely solely on task metrics that don't particularly measure gaps related to semantic understanding. Thus, we propose an intent semantic toolkit that gives a more holistic view of intent embedding models by considering three tasks -- (1) intent classification, (2) intent clustering, and (3) a novel triplet task. The triplet task gauges the model's understanding of two semantic concepts paramount in real-world conversational systems -- negation and implicature. We observe that current embedding models fare poorly in semantic understanding of these concepts. To address this, we propose a pre-training approach to improve the embedding model by leveraging augmentation with data generated by an auto-regressive model and a contrastive loss term. Our approach improves the semantic understanding of the intent embedding model on the aforementioned linguistic dimensions while slightly effecting their performance on downstream task metrics.","sentences":["Conversational systems often rely on embedding models for intent classification and intent clustering tasks.","The advent of Large Language Models (LLMs), which enable instructional embeddings allowing one to adjust semantics over the embedding space using prompts, are being viewed as a panacea for these downstream conversational tasks.","However, traditional evaluation benchmarks rely solely on task metrics that don't particularly measure gaps related to semantic understanding.","Thus, we propose an intent semantic toolkit that gives a more holistic view of intent embedding models by considering three tasks -- (1) intent classification, (2) intent clustering, and (3) a novel triplet task.","The triplet task gauges the model's understanding of two semantic concepts paramount in real-world conversational systems -- negation and implicature.","We observe that current embedding models fare poorly in semantic understanding of these concepts.","To address this, we propose a pre-training approach to improve the embedding model by leveraging augmentation with data generated by an auto-regressive model and a contrastive loss term.","Our approach improves the semantic understanding of the intent embedding model on the aforementioned linguistic dimensions while slightly effecting their performance on downstream task metrics."],"url":"http://arxiv.org/abs/2403.04314v1","category":"cs.CL"}
{"created":"2024-03-07 08:31:50","title":"Distribution of power residues over shifted subfields and maximal cliques in generalized Paley graphs","abstract":"We derive an asymptotic formula for the number of solutions in a given subfield to certain system of equations over finite fields. As an application, we construct new families of maximal cliques in generalized Paley graphs. Given integers $d\\ge2$ and $q \\equiv 1 \\pmod d$, we show that for each positive integer $m$ such that $\\operatorname{rad}(m) \\mid \\operatorname{rad}(d)$, there are maximal cliques of size approximately $q/m$ in the $d$-Paley graph defined on $\\mathbb{F}_{q^d}$. We also confirm a conjecture of Goryainov, Shalaginov, and the second author on the maximality of certain cliques in generalized Paley graphs, as well as an analogous conjecture of Goryainov for Peisert graphs.","sentences":["We derive an asymptotic formula for the number of solutions in a given subfield to certain system of equations over finite fields.","As an application, we construct new families of maximal cliques in generalized Paley graphs.","Given integers $d\\ge2$ and $q \\equiv 1 \\pmod d$, we show that for each positive integer $m$ such that $\\operatorname{rad}(m) \\mid \\operatorname{rad}(d)$, there are maximal cliques of size approximately $q/m$ in the $d$-Paley graph defined on $\\mathbb{F}_{q^d}$. We also confirm a conjecture of Goryainov, Shalaginov, and the second author on the maximality of certain cliques in generalized Paley graphs, as well as an analogous conjecture of Goryainov for Peisert graphs."],"url":"http://arxiv.org/abs/2403.04312v1","category":"math.NT"}
{"created":"2024-03-07 08:30:26","title":"ALTO: An Efficient Network Orchestrator for Compound AI Systems","abstract":"We present ALTO, a network orchestrator for efficiently serving compound AI systems such as pipelines of language models. ALTO achieves high throughput and low latency by taking advantage of an optimization opportunity specific to generative language models: streaming intermediate outputs. As language models produce outputs token by token, ALTO exposes opportunities to stream intermediate outputs between stages when possible. We highlight two new challenges of correctness and load balancing which emerge when streaming intermediate data across distributed pipeline stage instances. We also motivate the need for an aggregation-aware routing interface and distributed prompt-aware scheduling to address these challenges. We demonstrate the impact of ALTO's partial output streaming on a complex chatbot verification pipeline, increasing throughput by up to 3x for a fixed latency target of 4 seconds / request while also reducing tail latency by 1.8x compared to a baseline serving approach.","sentences":["We present ALTO, a network orchestrator for efficiently serving compound AI systems such as pipelines of language models.","ALTO achieves high throughput and low latency by taking advantage of an optimization opportunity specific to generative language models: streaming intermediate outputs.","As language models produce outputs token by token, ALTO exposes opportunities to stream intermediate outputs between stages when possible.","We highlight two new challenges of correctness and load balancing which emerge when streaming intermediate data across distributed pipeline stage instances.","We also motivate the need for an aggregation-aware routing interface and distributed prompt-aware scheduling to address these challenges.","We demonstrate the impact of ALTO's partial output streaming on a complex chatbot verification pipeline, increasing throughput by up to 3x for a fixed latency target of 4 seconds / request while also reducing tail latency by 1.8x compared to a baseline serving approach."],"url":"http://arxiv.org/abs/2403.04311v1","category":"cs.AI"}
{"created":"2024-03-07 08:30:17","title":"AO-DETR: Anti-Overlapping DETR for X-Ray Prohibited Items Detection","abstract":"Prohibited item detection in X-ray images is one of the most essential and highly effective methods widely employed in various security inspection scenarios. Considering the significant overlapping phenomenon in X-ray prohibited item images, we propose an Anti-Overlapping DETR (AO-DETR) based on one of the state-of-the-art general object detectors, DINO. Specifically, to address the feature coupling issue caused by overlapping phenomena, we introduce the Category-Specific One-to-One Assignment (CSA) strategy to constrain category-specific object queries in predicting prohibited items of fixed categories, which can enhance their ability to extract features specific to prohibited items of a particular category from the overlapping foreground-background features. To address the edge blurring problem caused by overlapping phenomena, we propose the Look Forward Densely (LFD) scheme, which improves the localization accuracy of reference boxes in mid-to-high-level decoder layers and enhances the ability to locate blurry edges of the final layer. Similar to DINO, our AO-DETR provides two different versions with distinct backbones, tailored to meet diverse application requirements. Extensive experiments on the PIXray and OPIXray datasets demonstrate that the proposed method surpasses the state-of-the-art object detectors, indicating its potential applications in the field of prohibited item detection. The source code will be released at https://github.com/Limingyuan001/AO-DETR-test.","sentences":["Prohibited item detection in X-ray images is one of the most essential and highly effective methods widely employed in various security inspection scenarios.","Considering the significant overlapping phenomenon in X-ray prohibited item images, we propose an Anti-Overlapping DETR (AO-DETR) based on one of the state-of-the-art general object detectors, DINO.","Specifically, to address the feature coupling issue caused by overlapping phenomena, we introduce the Category-Specific One-to-One Assignment (CSA) strategy to constrain category-specific object queries in predicting prohibited items of fixed categories, which can enhance their ability to extract features specific to prohibited items of a particular category from the overlapping foreground-background features.","To address the edge blurring problem caused by overlapping phenomena, we propose the Look Forward Densely (LFD) scheme, which improves the localization accuracy of reference boxes in mid-to-high-level decoder layers and enhances the ability to locate blurry edges of the final layer.","Similar to DINO, our AO-DETR provides two different versions with distinct backbones, tailored to meet diverse application requirements.","Extensive experiments on the PIXray and OPIXray datasets demonstrate that the proposed method surpasses the state-of-the-art object detectors, indicating its potential applications in the field of prohibited item detection.","The source code will be released at https://github.com/Limingyuan001/AO-DETR-test."],"url":"http://arxiv.org/abs/2403.04309v1","category":"cs.CV"}
{"created":"2024-03-07 08:26:00","title":"Generating insights about financial asks from Reddit posts and user interactions","abstract":"As an increasingly large number of people turn to platforms like Reddit, YouTube, Twitter, Instagram, etc. for financial advice, generating insights about the content generated and interactions taking place within these platforms have become a key research question. This study proposes content and interaction analysis techniques for a large repository created from social media content, where people interactions are centered around financial information exchange. We propose methods for content analysis that can generate human-interpretable insights using topic-centered clustering and multi-document abstractive summarization. We share details of insights generated from our experiments with a large repository of data gathered from subreddit for personal finance. We have also explored the use of ChatGPT and Vicuna for generating responses to queries and compared them with human responses. The methods proposed in this work are generic and applicable to all large social media platforms.","sentences":["As an increasingly large number of people turn to platforms like Reddit, YouTube, Twitter, Instagram, etc. for financial advice, generating insights about the content generated and interactions taking place within these platforms have become a key research question.","This study proposes content and interaction analysis techniques for a large repository created from social media content, where people interactions are centered around financial information exchange.","We propose methods for content analysis that can generate human-interpretable insights using topic-centered clustering and multi-document abstractive summarization.","We share details of insights generated from our experiments with a large repository of data gathered from subreddit for personal finance.","We have also explored the use of ChatGPT and Vicuna for generating responses to queries and compared them with human responses.","The methods proposed in this work are generic and applicable to all large social media platforms."],"url":"http://arxiv.org/abs/2403.04308v1","category":"cs.SI"}
{"created":"2024-03-07 08:25:46","title":"HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild","abstract":"Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains. Recent benchmarks designed to assess LLM hallucinations within conventional NLP tasks, such as knowledge-intensive question answering (QA) and summarization, are insufficient for capturing the complexities of user-LLM interactions in dynamic, real-world settings. To address this gap, we introduce HaluEval-Wild, the first benchmark specifically designed to evaluate LLM hallucinations in the wild. We meticulously collect challenging (adversarially filtered by Alpaca) user queries from existing real-world user-LLM interaction datasets, including ShareGPT, to evaluate the hallucination rates of various LLMs. Upon analyzing the collected queries, we categorize them into five distinct types, which enables a fine-grained analysis of the types of hallucinations LLMs exhibit, and synthesize the reference answers with the powerful GPT-4 model and retrieval-augmented generation (RAG). Our benchmark offers a novel approach towards enhancing our comprehension and improvement of LLM reliability in scenarios reflective of real-world interactions.","sentences":["Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains.","Recent benchmarks designed to assess LLM hallucinations within conventional NLP tasks, such as knowledge-intensive question answering (QA) and summarization, are insufficient for capturing the complexities of user-LLM interactions in dynamic, real-world settings.","To address this gap, we introduce HaluEval-Wild, the first benchmark specifically designed to evaluate LLM hallucinations in the wild.","We meticulously collect challenging (adversarially filtered by Alpaca) user queries from existing real-world user-LLM interaction datasets, including ShareGPT, to evaluate the hallucination rates of various LLMs.","Upon analyzing the collected queries, we categorize them into five distinct types, which enables a fine-grained analysis of the types of hallucinations LLMs exhibit, and synthesize the reference answers with the powerful GPT-4 model and retrieval-augmented generation (RAG).","Our benchmark offers a novel approach towards enhancing our comprehension and improvement of LLM reliability in scenarios reflective of real-world interactions."],"url":"http://arxiv.org/abs/2403.04307v1","category":"cs.CL"}
{"created":"2024-03-07 08:25:27","title":"Effectiveness Assessment of Recent Large Vision-Language Models","abstract":"The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localization. Moreover, we conduct empirical investigations utilizing the aforementioned models alongside GPT-4V, assessing their multi-modal understanding capacities in general tasks such as object counting, absurd question answering, affordance reasoning, attribute recognition, and spatial relation reasoning. Our investigations reveal that these models demonstrate limited proficiency not only in specialized tasks but also in general tasks. We delve deeper into this inadequacy and suggest several potential factors, including limited cognition in specialized tasks, object hallucination, text-to-image interference, and decreased robustness in complex problems. We hope this study would provide valuable insights for the future development of LVLMs, augmenting their power in coping with both general and specialized applications.","sentences":["The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence.","However, the extent of their efficacy across both specialized and general tasks warrants further investigation.","This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies.","To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks.","These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection.","We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localization.","Moreover, we conduct empirical investigations utilizing the aforementioned models alongside GPT-4V, assessing their multi-modal understanding capacities in general tasks such as object counting, absurd question answering, affordance reasoning, attribute recognition, and spatial relation reasoning.","Our investigations reveal that these models demonstrate limited proficiency not only in specialized tasks but also in general tasks.","We delve deeper into this inadequacy and suggest several potential factors, including limited cognition in specialized tasks, object hallucination, text-to-image interference, and decreased robustness in complex problems.","We hope this study would provide valuable insights for the future development of LVLMs, augmenting their power in coping with both general and specialized applications."],"url":"http://arxiv.org/abs/2403.04306v1","category":"cs.CV"}
{"created":"2024-03-07 08:06:38","title":"Characterizations of Controlled Generation of Right Linear Grammars with Unknown Behaviors","abstract":"This paper deals with the control generation of right linear grammars with unknown behaviors (RLUBs, for short) in which derivation behavior is not determined completely. In particular, we consider a physical property of control devices used in control systems and formulate it as a partial order over control alphabet of the control system. We give necessary and sufficient conditions for given finite language classes to be generated by RLUBs and their control systems using a given partial order over control alphabet.","sentences":["This paper deals with the control generation of right linear grammars with unknown behaviors (RLUBs, for short) in which derivation behavior is not determined completely.","In particular, we consider a physical property of control devices used in control systems and formulate it as a partial order over control alphabet of the control system.","We give necessary and sufficient conditions for given finite language classes to be generated by RLUBs and their control systems using a given partial order over control alphabet."],"url":"http://arxiv.org/abs/2403.04301v1","category":"cs.FL"}
{"created":"2024-03-07 08:04:18","title":"Engineering Entangled Schrodinger Cat States of Separated Cavity Modes in Cavity-QED","abstract":"We provide a scheme by utilizing a two-cavity setup to generate useful quantum mechanically entangled states of two cavity fields, which themselves are prepared in Schrodinger cat states. The underlying atom-field interaction is considered off-resonant and three atoms are successively sent through the cavities, initially fed with coherent fields. Analytical solution of the protocol, followed by conditional measurements on the atoms, produce a family of eight such entangled states. Entanglement properties of the obtained states are characterized by the Von Neumann entropy. We reveal the parameter domain for tuning the entanglement, the prime tuning parameters being the atom-field interaction time and the field amplitudes. The parameter domains for both quasi-Bell and non quasi-Bell states are discussed. We also present a Wigner phase space representation of the reduced state of the cavity, showing negative values and interference patterns similar to those of a compass state, used in quantum precision measurements, and despite its large entropy.","sentences":["We provide a scheme by utilizing a two-cavity setup to generate useful quantum mechanically entangled states of two cavity fields, which themselves are prepared in Schrodinger cat states.","The underlying atom-field interaction is considered off-resonant and three atoms are successively sent through the cavities, initially fed with coherent fields.","Analytical solution of the protocol, followed by conditional measurements on the atoms, produce a family of eight such entangled states.","Entanglement properties of the obtained states are characterized by the Von Neumann entropy.","We reveal the parameter domain for tuning the entanglement, the prime tuning parameters being the atom-field interaction time and the field amplitudes.","The parameter domains for both quasi-Bell and non quasi-Bell states are discussed.","We also present a Wigner phase space representation of the reduced state of the cavity, showing negative values and interference patterns similar to those of a compass state, used in quantum precision measurements, and despite its large entropy."],"url":"http://arxiv.org/abs/2403.04300v1","category":"quant-ph"}
{"created":"2024-03-07 07:58:58","title":"LitSim: Conflict-aware Policy for Long-term Interactive Traffic Simulation","abstract":"Simulation is pivotal in evaluating the performance of autonomous driving systems due to the advantages in efficiency and cost compared to on-road testing. Realistic multi-agent behavior~(e.g., interactive and long-term) is needed to narrow the gap between the simulation and the reality. The existing work has the following shortcomings in achieving this goal:~(1) log replay offers realistic scenarios but leads to unrealistic collisions due to lacking dynamic interactions, and~(2) model-based and learning-based solutions encourage interactions but often deviate from real-world data in long horizons. In this work, we propose LitSim, a long-term interactive simulation approach that maximizes realism while avoiding unrealistic collisions. Specifically, we replay the log for most scenarios and intervene only when LitSim predicts unrealistic conflicts. We then encourage interactions among the agents and resolve the conflicts, thereby reducing the likelihood of unrealistic collisions. We train and validate our model on the real-world dataset NGSIM, and the experimental results demonstrate that LitSim outperforms the current popular approaches in realism and reactivity.","sentences":["Simulation is pivotal in evaluating the performance of autonomous driving systems due to the advantages in efficiency and cost compared to on-road testing.","Realistic multi-agent behavior~(e.g., interactive and long-term) is needed to narrow the gap between the simulation and the reality.","The existing work has the following shortcomings in achieving this goal:~(1) log replay offers realistic scenarios but leads to unrealistic collisions due to lacking dynamic interactions, and~(2) model-based and learning-based solutions encourage interactions but often deviate from real-world data in long horizons.","In this work, we propose LitSim, a long-term interactive simulation approach that maximizes realism while avoiding unrealistic collisions.","Specifically, we replay the log for most scenarios and intervene only when LitSim predicts unrealistic conflicts.","We then encourage interactions among the agents and resolve the conflicts, thereby reducing the likelihood of unrealistic collisions.","We train and validate our model on the real-world dataset NGSIM, and the experimental results demonstrate that LitSim outperforms the current popular approaches in realism and reactivity."],"url":"http://arxiv.org/abs/2403.04299v1","category":"cs.RO"}
{"created":"2024-03-07 07:47:09","title":"Understanding how social discussion platforms like Reddit are influencing financial behavior","abstract":"This study proposes content and interaction analysis techniques for a large repository created from social media content. Though we have presented our study for a large platform dedicated to discussions around financial topics, the proposed methods are generic and applicable to all platforms. Along with an extension of topic extraction method using Latent Dirichlet Allocation, we propose a few measures to assess user participation, influence and topic affinities specifically. Our study also maps user-generated content to components of behavioral finance. While these types of information are usually gathered through surveys, it is obvious that large scale data analysis from social media can reveal many potentially unknown or rare insights. Characterising users based on their platform behavior to provide critical insights about how communities are formed and trust is established in these platforms using graphical analysis is also studied.","sentences":["This study proposes content and interaction analysis techniques for a large repository created from social media content.","Though we have presented our study for a large platform dedicated to discussions around financial topics, the proposed methods are generic and applicable to all platforms.","Along with an extension of topic extraction method using Latent Dirichlet Allocation, we propose a few measures to assess user participation, influence and topic affinities specifically.","Our study also maps user-generated content to components of behavioral finance.","While these types of information are usually gathered through surveys, it is obvious that large scale data analysis from social media can reveal many potentially unknown or rare insights.","Characterising users based on their platform behavior to provide critical insights about how communities are formed and trust is established in these platforms using graphical analysis is also studied."],"url":"http://arxiv.org/abs/2403.04298v1","category":"cs.SI"}
{"created":"2024-03-07 07:43:39","title":"Local well-posedness for a generalized sixth-order Boussinesq equation","abstract":"A formally second order correct Boussinesq-type equation that describes unidirectional shallow water waves is derived,   $$u_{tt} - u_{xx} - u_{xxxx} - u_{xxxxxx} - (u^2)_{xx} - (u^2)_{xxxx} - (uu_{xx})_{xx} - (u^3)_{xx} = 0.$$   Such equation is analogous to original Boussinesq equation but with higher order approximation which may ensure a more accuracy description on a long time scale. Moreover, through a rigorous derivation from Boussiensq systems, it has redeemed all the non-linear terms neglected in the sixth order Boussinesq equation (SOBE),   $$u_{tt} - u_{xx} - u_{xxxx} - u_{xxxxxx} - (u^2)_{xx} = 0.$$   The Cauchy problem for this generalized SOBE is then considered under the Bourgain space, $X^{s,b}$, framework. The multi-linear estimates for $(u^2)_{xx}$, $(u^2)_{xxxx}$, $(uu_{xx})_{xx}$ and $(u^3)_{xx}$ are given, the local wellposedness of the gSOBE is established for $s>\\frac{1}{2}$.","sentences":["A formally second order correct Boussinesq-type equation that describes unidirectional shallow water waves is derived,   $$u_{tt} - u_{xx} - u_{xxxx} - u_{xxxxxx} - (u^2)_{xx} - (u^2)_{xxxx} - (uu_{xx})_{xx} - (u^3)_{xx} = 0.$$   Such equation is analogous to original Boussinesq equation but with higher order approximation which may ensure a more accuracy description on a long time scale.","Moreover, through a rigorous derivation from Boussiensq systems, it has redeemed all the non-linear terms neglected in the sixth order Boussinesq equation (SOBE),   $$u_{tt} - u_{xx} - u_{xxxx} - u_{xxxxxx} - (u^2)_{xx} = 0.$$   The Cauchy problem for this generalized SOBE is then considered under the Bourgain space, $X^{s,b}$, framework.","The multi-linear estimates for $(u^2)_{xx}$, $(u^2)_{xxxx}$, $(uu_{xx})_{xx}$ and $(u^3)_{xx}$ are given, the local wellposedness of the gSOBE is established for $s>\\frac{1}{2}$."],"url":"http://arxiv.org/abs/2403.04295v1","category":"math.AP"}
{"created":"2024-03-07 07:40:53","title":"MKF-ADS: A Multi-Knowledge Fused Anomaly Detection System for Automotive","abstract":"With the requirements of Intelligent Transport Systems (ITSs) for extensive connectivity of Electronic Control Units (ECUs) to the outside world, safety and security have become stringent problems. Intrusion detection systems (IDSs) are a crucial safety component in remediating Controller Area Network (CAN) bus vulnerabilities. However, supervised-based IDSs fail to identify complexity attacks and anomaly-based IDSs have higher false alarms owing to capability bottleneck. In this paper, we propose a novel multi-knowledge fused anomaly detection model, called MKF-IDS. Specifically, the method designs an integration framework, including spatial-temporal correlation with an attention mechanism (STcAM) module and patch sparse-transformer module (PatchST). The STcAM with fine-pruning uses one-dimensional convolution (Conv1D) to extract spatial features and subsequently utilizes the Bidirectional Long Short Term Memory (Bi-LSTM) to extract the temporal features, where the attention mechanism will focus on the important time steps. Meanwhile, the PatchST captures the combined long-time historical features from independent univariate time series. Finally, the proposed method is based on knowledge distillation to STcAM as a student model for learning intrinsic knowledge and cross the ability to mimic PatchST. In the detection phase, the MKF-ADS only deploys STcAM to maintain efficiency in a resource-limited IVN environment. Moreover, the redundant noisy signal is reduced with bit flip rate and boundary decision estimation. We conduct extensive experiments on six simulation attack scenarios across various CAN IDs and time steps, and two real attack scenarios, which present a competitive prediction and detection performance. Compared with the baseline in the same paradigm, the error rate and FAR are 2.62% and 2.41% and achieve a promising F1-score of 97.3%.","sentences":["With the requirements of Intelligent Transport Systems (ITSs) for extensive connectivity of Electronic Control Units (ECUs) to the outside world, safety and security have become stringent problems.","Intrusion detection systems (IDSs) are a crucial safety component in remediating Controller Area Network (CAN) bus vulnerabilities.","However, supervised-based IDSs fail to identify complexity attacks and anomaly-based IDSs have higher false alarms owing to capability bottleneck.","In this paper, we propose a novel multi-knowledge fused anomaly detection model, called MKF-IDS.","Specifically, the method designs an integration framework, including spatial-temporal correlation with an attention mechanism (STcAM) module and patch sparse-transformer module (PatchST).","The STcAM with fine-pruning uses one-dimensional convolution (Conv1D) to extract spatial features and subsequently utilizes the Bidirectional Long Short Term Memory (Bi-LSTM) to extract the temporal features, where the attention mechanism will focus on the important time steps.","Meanwhile, the PatchST captures the combined long-time historical features from independent univariate time series.","Finally, the proposed method is based on knowledge distillation to STcAM as a student model for learning intrinsic knowledge and cross the ability to mimic PatchST.","In the detection phase, the MKF-ADS only deploys STcAM to maintain efficiency in a resource-limited IVN environment.","Moreover, the redundant noisy signal is reduced with bit flip rate and boundary decision estimation.","We conduct extensive experiments on six simulation attack scenarios across various CAN IDs and time steps, and two real attack scenarios, which present a competitive prediction and detection performance.","Compared with the baseline in the same paradigm, the error rate and FAR are 2.62% and 2.41% and achieve a promising F1-score of 97.3%."],"url":"http://arxiv.org/abs/2403.04293v1","category":"cs.AI"}
{"created":"2024-03-07 07:39:54","title":"A challenge in A(G)I, cybernetics revived in the Ouroboros Model as one algorithm for all thinking","abstract":"A topical challenge for algorithms in general and for automatic image categorization and generation in particular is presented in the form of a drawing for AI to understand. In a second vein, AI is challenged to produce something similar from verbal description. The aim of the paper is to highlight strengths and deficiencies of current Artificial Intelligence approaches while coarsely sketching a way forward. A general lack of encompassing symbol-embedding and (not only) -grounding in some bodily basis is made responsible for current deficiencies. A concomitant dearth of hierarchical organization of concepts follows suite. As a remedy for these shortcomings, it is proposed to take a wide step back and to newly incorporate aspects of cybernetics and analog control processes. It is claimed that a promising overarching perspective is provided by the Ouroboros Model with a valid and versatile algorithmic backbone for general cognition at all accessible levels of abstraction and capabilities. Reality, rules, truth, and Free Will are all useful abstractions according to the Ouroboros Model. Logic deduction as well as intuitive guesses are claimed as produced on the basis of one compartmentalized memory for schemata and a pattern-matching, i.e., monitoring process termed consumption analysis. The latter directs attention on short (attention proper) and also on long times scales (emotional biases). In this cybernetic approach, discrepancies between expectations and actual activations (e.g., sensory precepts) drive the general process of cognition and at the same time steer the storage of new and adapted memory entries. Dedicated structures in the human brain work in concert according to this scheme.","sentences":["A topical challenge for algorithms in general and for automatic image categorization and generation in particular is presented in the form of a drawing for AI to understand.","In a second vein, AI is challenged to produce something similar from verbal description.","The aim of the paper is to highlight strengths and deficiencies of current Artificial Intelligence approaches while coarsely sketching a way forward.","A general lack of encompassing symbol-embedding and (not only) -grounding in some bodily basis is made responsible for current deficiencies.","A concomitant dearth of hierarchical organization of concepts follows suite.","As a remedy for these shortcomings, it is proposed to take a wide step back and to newly incorporate aspects of cybernetics and analog control processes.","It is claimed that a promising overarching perspective is provided by the Ouroboros Model with a valid and versatile algorithmic backbone for general cognition at all accessible levels of abstraction and capabilities.","Reality, rules, truth, and Free Will are all useful abstractions according to the Ouroboros Model.","Logic deduction as well as intuitive guesses are claimed as produced on the basis of one compartmentalized memory for schemata and a pattern-matching, i.e., monitoring process termed consumption analysis.","The latter directs attention on short (attention proper) and also on long times scales (emotional biases).","In this cybernetic approach, discrepancies between expectations and actual activations (e.g., sensory precepts) drive the general process of cognition and at the same time steer the storage of new and adapted memory entries.","Dedicated structures in the human brain work in concert according to this scheme."],"url":"http://arxiv.org/abs/2403.04292v1","category":"cs.AI"}
{"created":"2024-03-07 07:39:00","title":"MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant","abstract":"Medical generative models, acknowledged for their high-quality sample generation ability, have accelerated the fast growth of medical applications. However, recent works concentrate on separate medical generation models for distinct medical tasks and are restricted to inadequate medical multi-modal knowledge, constraining medical comprehensive diagnosis. In this paper, we propose MedM2G, a Medical Multi-Modal Generative framework, with the key innovation to align, extract, and generate medical multi-modal within a unified model. Extending beyond single or two medical modalities, we efficiently align medical multi-modal through the central alignment approach in the unified space. Significantly, our framework extracts valuable clinical knowledge by preserving the medical visual invariant of each imaging modal, thereby enhancing specific medical information for multi-modal generation. By conditioning the adaptive cross-guided parameters into the multi-flow diffusion framework, our model promotes flexible interactions among medical multi-modal for generation. MedM2G is the first medical generative model that unifies medical generation tasks of text-to-image, image-to-text, and unified generation of medical modalities (CT, MRI, X-ray). It performs 5 medical generation tasks across 10 datasets, consistently outperforming various state-of-the-art works.","sentences":["Medical generative models, acknowledged for their high-quality sample generation ability, have accelerated the fast growth of medical applications.","However, recent works concentrate on separate medical generation models for distinct medical tasks and are restricted to inadequate medical multi-modal knowledge, constraining medical comprehensive diagnosis.","In this paper, we propose MedM2G, a Medical Multi-Modal Generative framework, with the key innovation to align, extract, and generate medical multi-modal within a unified model.","Extending beyond single or two medical modalities, we efficiently align medical multi-modal through the central alignment approach in the unified space.","Significantly, our framework extracts valuable clinical knowledge by preserving the medical visual invariant of each imaging modal, thereby enhancing specific medical information for multi-modal generation.","By conditioning the adaptive cross-guided parameters into the multi-flow diffusion framework, our model promotes flexible interactions among medical multi-modal for generation.","MedM2G is the first medical generative model that unifies medical generation tasks of text-to-image, image-to-text, and unified generation of medical modalities (CT, MRI, X-ray).","It performs 5 medical generation tasks across 10 datasets, consistently outperforming various state-of-the-art works."],"url":"http://arxiv.org/abs/2403.04290v1","category":"eess.IV"}
{"created":"2024-03-07 07:36:50","title":"$\\mathcal{L}$-intersecting or Configuration Forbidden Families on Set Systems and Vector Spaces over Finite Fields","abstract":"In this paper, we derive a tight upper bound for the size of an intersecting $k$-Sperner family of subspaces of the $n$-dimensional vector space $\\mathbb{F}_{q}^{n}$ over finite field $\\mathbb{F}_{q}$ which gives a $q$-analogue of the Erd\\H{o}s' $k$-Sperner Theorem, and we then establish a general relationship between upper bounds for the sizes of families of subsets of $[n] = \\{1, 2, \\dots, n\\}$ with property $P$ and upper bounds for the sizes of families of subspaces of $\\mathbb{F}_{q}^{n}$ with property $P$, where $P$ is either $\\mathcal{L}$-intersecting or forbidding certain configuration. Applying this relationship, we derive generalizations of the well known results about the famous Erd\\H{o}s matching conjecture and Erd\\H{o}s-Chv\\'atal simplex conjecture to linear lattices. As a consequence, we disprove a related conjecture on families of subspaces of $\\mathbb{F}_{q}^{n}$ by Ihringer [Europ. J. Combin., 94 (2021), 103306].","sentences":["In this paper, we derive a tight upper bound for the size of an intersecting $k$-Sperner family of subspaces of the $n$-dimensional vector space $\\mathbb{F}_{q}^{n}$ over finite field $\\mathbb{F}_{q}$ which gives a $q$-analogue of the Erd\\H{o}s' $k$-Sperner Theorem, and we then establish a general relationship between upper bounds for the sizes of families of subsets of $[n] = \\{1, 2, \\dots, n\\}$ with property $P$ and upper bounds for the sizes of families of subspaces of $\\mathbb{F}_{q}^{n}$ with property $P$, where $P$ is either $\\mathcal{L}$-intersecting or forbidding certain configuration.","Applying this relationship, we derive generalizations of the well known results about the famous Erd\\H{o}s matching conjecture and Erd\\H{o}s-Chv\\'atal simplex conjecture to linear lattices.","As a consequence, we disprove a related conjecture on families of subspaces of $\\mathbb{F}_{q}^{n}$ by Ihringer [Europ.","J. Combin., 94 (2021), 103306]."],"url":"http://arxiv.org/abs/2403.04289v1","category":"math.CO"}
{"created":"2024-03-07 07:36:15","title":"Monochromatic high-harmonic generation by Bessel-Gauss beam in periodically modulated media","abstract":"High harmonic generation (HHG) has become a multipurpose source of coherent XUV radiation used in various applications. One of the notable aspects of HHG is its wide spectrum consisting of many harmonic orders. This might represent a bottleneck in HHG utility for applications requiring a single wavelength. We propose a method to generate radiation consisting of a single high-order harmonic frequency employing Bessel-Gauss driving beam and a periodically modulated gaseous medium. We validate it by analytical calculations and numerical simulations. Our method provides a way to generate monochromatic harmonic radiation directly from the source without the need for additional monochromatizing optics. Thus, it represents a substantial enhancement of the flux and simplification of the setup for numerous applications requiring monochromatic short-wavelength radiation.","sentences":["High harmonic generation (HHG) has become a multipurpose source of coherent XUV radiation used in various applications.","One of the notable aspects of HHG is its wide spectrum consisting of many harmonic orders.","This might represent a bottleneck in HHG utility for applications requiring a single wavelength.","We propose a method to generate radiation consisting of a single high-order harmonic frequency employing Bessel-Gauss driving beam and a periodically modulated gaseous medium.","We validate it by analytical calculations and numerical simulations.","Our method provides a way to generate monochromatic harmonic radiation directly from the source without the need for additional monochromatizing optics.","Thus, it represents a substantial enhancement of the flux and simplification of the setup for numerous applications requiring monochromatic short-wavelength radiation."],"url":"http://arxiv.org/abs/2403.04288v1","category":"physics.optics"}
{"created":"2024-03-07 18:56:47","title":"DeepSee: Multidimensional Visualizations of Seabed Ecosystems","abstract":"Scientists studying deep ocean microbial ecosystems use limited numbers of sediment samples collected from the seafloor to characterize important life-sustaining biogeochemical cycles in the environment. Yet conducting fieldwork to sample these extreme remote environments is both expensive and time consuming, requiring tools that enable scientists to explore the sampling history of field sites and predict where taking new samples is likely to maximize scientific return. We conducted a collaborative, user-centered design study with a team of scientific researchers to develop DeepSee, an interactive data workspace that visualizes 2D and 3D interpolations of biogeochemical and microbial processes in context together with sediment sampling history overlaid on 2D seafloor maps. Based on a field deployment and qualitative interviews, we found that DeepSee increased the scientific return from limited sample sizes, catalyzed new research workflows, reduced long-term costs of sharing data, and supported teamwork and communication between team members with diverse research goals.","sentences":["Scientists studying deep ocean microbial ecosystems use limited numbers of sediment samples collected from the seafloor to characterize important life-sustaining biogeochemical cycles in the environment.","Yet conducting fieldwork to sample these extreme remote environments is both expensive and time consuming, requiring tools that enable scientists to explore the sampling history of field sites and predict where taking new samples is likely to maximize scientific return.","We conducted a collaborative, user-centered design study with a team of scientific researchers to develop DeepSee, an interactive data workspace that visualizes 2D and 3D interpolations of biogeochemical and microbial processes in context together with sediment sampling history overlaid on 2D seafloor maps.","Based on a field deployment and qualitative interviews, we found that DeepSee increased the scientific return from limited sample sizes, catalyzed new research workflows, reduced long-term costs of sharing data, and supported teamwork and communication between team members with diverse research goals."],"url":"http://arxiv.org/abs/2403.04761v1","category":"cs.HC"}
{"created":"2024-03-07 18:55:30","title":"That's My Point: Compact Object-centric LiDAR Pose Estimation for Large-scale Outdoor Localisation","abstract":"This paper is about 3D pose estimation on LiDAR scans with extremely minimal storage requirements to enable scalable mapping and localisation. We achieve this by clustering all points of segmented scans into semantic objects and representing them only with their respective centroid and semantic class. In this way, each LiDAR scan is reduced to a compact collection of four-number vectors. This abstracts away important structural information from the scenes, which is crucial for traditional registration approaches. To mitigate this, we introduce an object-matching network based on self- and cross-correlation that captures geometric and semantic relationships between entities. The respective matches allow us to recover the relative transformation between scans through weighted Singular Value Decomposition (SVD) and RANdom SAmple Consensus (RANSAC). We demonstrate that such representation is sufficient for metric localisation by registering point clouds taken under different viewpoints on the KITTI dataset, and at different periods of time localising between KITTI and KITTI-360. We achieve accurate metric estimates comparable with state-of-the-art methods with almost half the representation size, specifically 1.33 kB on average.","sentences":["This paper is about 3D pose estimation on LiDAR scans with extremely minimal storage requirements to enable scalable mapping and localisation.","We achieve this by clustering all points of segmented scans into semantic objects and representing them only with their respective centroid and semantic class.","In this way, each LiDAR scan is reduced to a compact collection of four-number vectors.","This abstracts away important structural information from the scenes, which is crucial for traditional registration approaches.","To mitigate this, we introduce an object-matching network based on self- and cross-correlation that captures geometric and semantic relationships between entities.","The respective matches allow us to recover the relative transformation between scans through weighted Singular Value Decomposition (SVD) and RANdom SAmple Consensus (RANSAC).","We demonstrate that such representation is sufficient for metric localisation by registering point clouds taken under different viewpoints on the KITTI dataset, and at different periods of time localising between KITTI and KITTI-360.","We achieve accurate metric estimates comparable with state-of-the-art methods with almost half the representation size, specifically 1.33 kB on average."],"url":"http://arxiv.org/abs/2403.04755v1","category":"cs.CV"}
{"created":"2024-03-07 18:37:52","title":"Coherent multidimensional spectroscopy in polariton systems","abstract":"The fast dynamics of molecular polaritonics is scrutinized theoretically through the implementation of two-dimensional spectroscopy protocols. We derive conceptually simple and computationally efficient formulas to calculate two-dimensional spectra for molecules, each of them modeled as a system of two electronic states including vibrational relaxation, immersed in an optical cavity, thus coupled to quantized radiation. Cavity photon losses and molecular relaxation are incorporated into the Hamiltonian dynamics to form an open quantum system that is solved through a master equation. In the collective case, the relaxation dynamics into dark states reveals to be the crucial factor to explain the asymmetries in both the diagonal and cross peaks of two-dimensional spectra for long waiting times between excitation and detection, a feature shown by recent experiments. Our theoretical method provides a deeper insight in those processes that yield relevant signals in multidimensional molecular spectroscopy.","sentences":["The fast dynamics of molecular polaritonics is scrutinized theoretically through the implementation of two-dimensional spectroscopy protocols.","We derive conceptually simple and computationally efficient formulas to calculate two-dimensional spectra for molecules, each of them modeled as a system of two electronic states including vibrational relaxation, immersed in an optical cavity, thus coupled to quantized radiation.","Cavity photon losses and molecular relaxation are incorporated into the Hamiltonian dynamics to form an open quantum system that is solved through a master equation.","In the collective case, the relaxation dynamics into dark states reveals to be the crucial factor to explain the asymmetries in both the diagonal and cross peaks of two-dimensional spectra for long waiting times between excitation and detection, a feature shown by recent experiments.","Our theoretical method provides a deeper insight in those processes that yield relevant signals in multidimensional molecular spectroscopy."],"url":"http://arxiv.org/abs/2403.04734v1","category":"quant-ph"}
{"created":"2024-03-07 17:53:37","title":"mmPlace: Robust Place Recognition with Intermediate Frequency Signal of Low-cost Single-chip Millimeter Wave Radar","abstract":"Place recognition is crucial for tasks like loop-closure detection and re-localization. Single-chip millimeter wave radar (single-chip radar in short) emerges as a low-cost sensor option for place recognition, with the advantage of insensitivity to degraded visual environments. However, it encounters two challenges. Firstly, sparse point cloud from single-chip radar leads to poor performance when using current place recognition methods, which assume much denser data. Secondly, its performance significantly declines in scenarios involving rotational and lateral variations, due to limited overlap in its field of view (FOV). We propose mmPlace, a robust place recognition system to address these challenges. Specifically, mmPlace transforms intermediate frequency (IF) signal into range azimuth heatmap and employs a spatial encoder to extract features. Additionally, to improve the performance in scenarios involving rotational and lateral variations, mmPlace employs a rotating platform and concatenates heatmaps in a rotation cycle, effectively expanding the system's FOV. We evaluate mmPlace's performance on the milliSonic dataset, which is collected on the University of Science and Technology of China (USTC) campus, the city roads surrounding the campus, and an underground parking garage. The results demonstrate that mmPlace outperforms point cloud-based methods and achieves 87.37% recall@1 in scenarios involving rotational and lateral variations.","sentences":["Place recognition is crucial for tasks like loop-closure detection and re-localization.","Single-chip millimeter wave radar (single-chip radar in short) emerges as a low-cost sensor option for place recognition, with the advantage of insensitivity to degraded visual environments.","However, it encounters two challenges.","Firstly, sparse point cloud from single-chip radar leads to poor performance when using current place recognition methods, which assume much denser data.","Secondly, its performance significantly declines in scenarios involving rotational and lateral variations, due to limited overlap in its field of view (FOV).","We propose mmPlace, a robust place recognition system to address these challenges.","Specifically, mmPlace transforms intermediate frequency (IF) signal into range azimuth heatmap and employs a spatial encoder to extract features.","Additionally, to improve the performance in scenarios involving rotational and lateral variations, mmPlace employs a rotating platform and concatenates heatmaps in a rotation cycle, effectively expanding the system's FOV.","We evaluate mmPlace's performance on the milliSonic dataset, which is collected on the University of Science and Technology of China (USTC) campus, the city roads surrounding the campus, and an underground parking garage.","The results demonstrate that mmPlace outperforms point cloud-based methods and achieves 87.37% recall@1 in scenarios involving rotational and lateral variations."],"url":"http://arxiv.org/abs/2403.04703v1","category":"cs.RO"}
{"created":"2024-03-07 16:31:02","title":"CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios","abstract":"This paper focuses on the challenge of answering questions in scenarios that are composed of rich and complex dynamic audio-visual components. Although existing Multimodal Large Language Models (MLLMs) can respond to audio-visual content, these responses are sometimes ambiguous and fail to describe specific audio-visual events. To overcome this limitation, we introduce the CAT, which enhances MLLM in three ways: 1) besides straightforwardly bridging audio and video, we design a clue aggregator that aggregates question-related clues in dynamic audio-visual scenarios to enrich the detailed knowledge required for large language models. 2) CAT is trained on a mixed multimodal dataset, allowing direct application in audio-visual scenarios. Notably, we collect an audio-visual joint instruction dataset named AVinstruct, to further enhance the capacity of CAT to model cross-semantic correlations. 3) we propose AI-assisted ambiguity-aware direct preference optimization, a strategy specialized in retraining the model to favor the non-ambiguity response and improve the ability to localize specific audio-visual objects. Extensive experimental results demonstrate that CAT outperforms existing methods on multimodal tasks, especially in Audio-Visual Question Answering (AVQA) tasks. The codes and the collected instructions are released at https://github.com/rikeilong/Bay-CAT.","sentences":["This paper focuses on the challenge of answering questions in scenarios that are composed of rich and complex dynamic audio-visual components.","Although existing Multimodal Large Language Models (MLLMs) can respond to audio-visual content, these responses are sometimes ambiguous and fail to describe specific audio-visual events.","To overcome this limitation, we introduce the CAT, which enhances MLLM in three ways: 1) besides straightforwardly bridging audio and video, we design a clue aggregator that aggregates question-related clues in dynamic audio-visual scenarios to enrich the detailed knowledge required for large language models.","2) CAT is trained on a mixed multimodal dataset, allowing direct application in audio-visual scenarios.","Notably, we collect an audio-visual joint instruction dataset named AVinstruct, to further enhance the capacity of CAT to model cross-semantic correlations.","3) we propose AI-assisted ambiguity-aware direct preference optimization, a strategy specialized in retraining the model to favor the non-ambiguity response and improve the ability to localize specific audio-visual objects.","Extensive experimental results demonstrate that CAT outperforms existing methods on multimodal tasks, especially in Audio-Visual Question Answering (AVQA) tasks.","The codes and the collected instructions are released at https://github.com/rikeilong/Bay-CAT."],"url":"http://arxiv.org/abs/2403.04640v1","category":"cs.CV"}
{"created":"2024-03-07 15:35:36","title":"Algorithms and complexity for path covers of temporal DAGs: when is Dilworth dynamic?","abstract":"In this paper, we study a dynamic analogue of the Path Cover problem, which can be solved in polynomial-time in directed acyclic graphs. A temporal digraph has an arc set that changes over discrete time-steps, if the underlying digraph (the union of all the arc sets) is acyclic, then we have a temporal DAG. A temporal path is a directed path in the underlying digraph, such that the time-steps of arcs are strictly increasing along the path. Two temporal paths are temporally disjoint if they do not occupy any vertex at the same time. A temporal (resp. temporally disjoint) path cover is a collection of (resp. temporally disjoint) temporal paths that covers all vertices. In this paper, we study the computational complexities of the problems of finding a temporal (disjoint) path cover with minimum cardinality, denoted as Temporal Path Cover (TPC) and Temporally Disjoint Path Cover (TD-PC). We show that both problems are NP-hard even when the underlying DAG is planar, bipartite, subcubic, and there are only two arc-disjoint time-steps. Moreover, TD-PC remains NP-hard even on temporal oriented trees. In contrast, we show that TPC is polynomial-time solvable on temporal oriented trees by a reduction to Clique Cover for (static undirected) weakly chordal graphs (a subclass of perfect graphs for which Clique Cover admits an efficient algorithm). This highlights an interesting algorithmic difference between the two problems. Although it is NP-hard on temporal oriented trees, TD-PC becomes polynomial-time solvable on temporal oriented lines and temporal rooted directed trees. We also show that TPC (resp. TD-PC) admits an XP (resp. FPT) time algorithm with respect to parameter tmax + tw, where tmax is the maximum time-step, and tw is the treewidth of the underlying static undirected graph.","sentences":["In this paper, we study a dynamic analogue of the Path Cover problem, which can be solved in polynomial-time in directed acyclic graphs.","A temporal digraph has an arc set that changes over discrete time-steps, if the underlying digraph (the union of all the arc sets) is acyclic, then we have a temporal DAG.","A temporal path is a directed path in the underlying digraph, such that the time-steps of arcs are strictly increasing along the path.","Two temporal paths are temporally disjoint if they do not occupy any vertex at the same time.","A temporal (resp.","temporally disjoint) path cover is a collection of (resp.","temporally disjoint) temporal paths that covers all vertices.","In this paper, we study the computational complexities of the problems of finding a temporal (disjoint) path cover with minimum cardinality, denoted as Temporal Path Cover (TPC) and Temporally Disjoint Path Cover (TD-PC).","We show that both problems are NP-hard even when the underlying DAG is planar, bipartite, subcubic, and there are only two arc-disjoint time-steps.","Moreover, TD-PC remains NP-hard even on temporal oriented trees.","In contrast, we show that TPC is polynomial-time solvable on temporal oriented trees by a reduction to Clique Cover for (static undirected) weakly chordal graphs (a subclass of perfect graphs for which Clique Cover admits an efficient algorithm).","This highlights an interesting algorithmic difference between the two problems.","Although it is NP-hard on temporal oriented trees, TD-PC becomes polynomial-time solvable on temporal oriented lines and temporal rooted directed trees.","We also show that TPC (resp.","TD-PC) admits an XP (resp.","FPT) time algorithm with respect to parameter tmax + tw, where tmax is the maximum time-step, and tw is the treewidth of the underlying static undirected graph."],"url":"http://arxiv.org/abs/2403.04589v1","category":"cs.DS"}
{"created":"2024-03-07 15:29:24","title":"Search for long-lived heavy neutrinos in the decays of B mesons produced in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search for long-lived heavy neutrinos (N) in the decays of \\PB mesons produced in proton-proton collisions at $\\sqrt{s}$ = 13 TeV is presented. The data sample corresponds to an integrated luminosity of 41.6 fb$^{-1}$ collected in 2018 by the CMS experiment at the CERN LHC, using a dedicated data stream that enhances the number of recorded events containing B mesons. The search probes heavy neutrinos with masses in the range 1 $\\lt$ $m_\\mathrm{N}$ $\\lt$ 3 GeV and decay lengths in the range 10$^{-2}$ $\\lt$ $c\\tau$ $\\lt$ 10$^{4}$ mm, where $\\tau_\\mathrm{N}$ is the N proper mean lifetime. Signal events are defined by the signature B $\\to$ $\\ell_\\mathrm{B}$NX; N $\\to$ $\\ell^{\\pm} \\pi^{\\mp}$, where the leptons $\\ell_\\mathrm{B}$ and $\\ell$ can be either a muon or an electron, provided that at least one of them is a muon. The hadronic recoil system, X, is treated inclusively and is not reconstructed. No significant excess of events over the standard model background is observed in any of the $\\ell^{\\pm}\\pi^{\\mp}$ invariant mass distributions. Limits at 95% confidence level on the sum of the squares of the mixing amplitudes between heavy and light neutrinos, $\\vert V_\\mathrm{N}\\vert^2$, and on $c\\tau$ are obtained in different mixing scenarios for both Majorana and Dirac-like N particles. The most stringent upper limit $\\vert V_\\mathrm{N}\\vert^2$ $\\lt$ 2.0$\\times$10$^{-5}$ is obtained at $m_\\mathrm{N}$ = 1.95 GeV for the Majorana case where N mixes exclusively with muon neutrinos. The limits on $\\vert V_\\mathrm{N}\\vert^2$ for masses 1 $\\lt$ $m_\\mathrm{N}$ $\\lt$ 1.7 GeV are the most stringent from a collider experiment to date.","sentences":["A search for long-lived heavy neutrinos (N) in the decays of \\PB mesons produced in proton-proton collisions at $\\sqrt{s}$ = 13 TeV is presented.","The data sample corresponds to an integrated luminosity of 41.6 fb$^{-1}$ collected in 2018 by the CMS experiment at the CERN LHC, using a dedicated data stream that enhances the number of recorded events containing B mesons.","The search probes heavy neutrinos with masses in the range 1 $\\lt$ $m_\\mathrm{N}$ $\\lt$ 3 GeV and decay lengths in the range 10$^{-2}$","$\\lt$ $c\\tau$ $\\lt$ 10$^{4}$ mm, where $\\tau_\\mathrm{N}$ is the N proper mean lifetime.","Signal events are defined by the signature B $\\to$ $\\ell_\\mathrm{B}$NX; N $\\to$ $\\ell^{\\pm} \\pi^{\\mp}$, where the leptons $\\ell_\\mathrm{B}$ and $\\ell$ can be either a muon or an electron, provided that at least one of them is a muon.","The hadronic recoil system, X, is treated inclusively and is not reconstructed.","No significant excess of events over the standard model background is observed in any of the $\\ell^{\\pm}\\pi^{\\mp}$ invariant mass distributions.","Limits at 95% confidence level on the sum of the squares of the mixing amplitudes between heavy and light neutrinos, $\\vert V_\\mathrm{N}\\vert^2$, and on $c\\tau$ are obtained in different mixing scenarios for both Majorana and Dirac-like N particles.","The most stringent upper limit $\\vert V_\\mathrm{N}\\vert^2$ $\\lt$ 2.0$\\times$10$^{-5}$ is obtained at $m_\\mathrm{N}$ = 1.95 GeV for the Majorana case where N mixes exclusively with muon neutrinos.","The limits on $\\vert V_\\mathrm{N}\\vert^2$ for masses 1 $\\lt$ $m_\\mathrm{N}$ $\\lt$ 1.7 GeV are the most stringent from a collider experiment to date."],"url":"http://arxiv.org/abs/2403.04584v1","category":"hep-ex"}
{"created":"2024-03-07 14:54:43","title":"Rescaled Mode-Coupling Scheme for the Quantitative Description of Experimentally Observed Colloid Dynamics","abstract":"We describe experimentally observed collective dynamics in colloidal suspensions of model hard-sphere particles using a modified mode coupling theory (MCT). This rescaled MCT is capable to describe quantitatively the wave-vector and time-dependent diffusion in these systems. Intermediate scattering functions of liquid-like structured dispersions are determined by means of static and dynamic light scattering experiments. The structure and short-time dynamics of the systems can be described quantitatively employing a multi-component Percus-Yevick ansatz for the partial structure factors and an effective, one-component description of hydrodynamic interactions based on the semi-analytical $\\delta\\gamma$-expansion. Combined with a recently proposed empirical modification of MCT in which memory functions are calculated using effective structure factors at rescaled number densities, the scheme is able to model the collective dynamics over the entire accessible time and wave-vector range and predicts the volume-fraction-dependence of long-time self-diffusion coefficients and the zero-shear viscosity quantitatively. This highlights the potential of MCT as a practical tool for the quantitative analysis and prediction of experimental observations.","sentences":["We describe experimentally observed collective dynamics in colloidal suspensions of model hard-sphere particles using a modified mode coupling theory (MCT).","This rescaled MCT is capable to describe quantitatively the wave-vector and time-dependent diffusion in these systems.","Intermediate scattering functions of liquid-like structured dispersions are determined by means of static and dynamic light scattering experiments.","The structure and short-time dynamics of the systems can be described quantitatively employing a multi-component Percus-Yevick ansatz for the partial structure factors and an effective, one-component description of hydrodynamic interactions based on the semi-analytical $\\delta\\gamma$-expansion.","Combined with a recently proposed empirical modification of MCT in which memory functions are calculated using effective structure factors at rescaled number densities, the scheme is able to model the collective dynamics over the entire accessible time and wave-vector range and predicts the volume-fraction-dependence of long-time self-diffusion coefficients and the zero-shear viscosity quantitatively.","This highlights the potential of MCT as a practical tool for the quantitative analysis and prediction of experimental observations."],"url":"http://arxiv.org/abs/2403.04556v1","category":"cond-mat.soft"}
{"created":"2024-03-07 13:49:56","title":"Trigonometry of partially truncated triangles and tetrahedra","abstract":"The first main results of this note establish forms of the hyperbolic laws of cosines and sines for certain classes of quadrilaterals and pentagons in the hyperbolic plane, having at least one ideal vertex and right angles at non-ideal vertices, in which the length of a horocyclic cross-section at an ideal vertex plays the role filled by the dihedral angle in the usual versions of these laws. The final main result bounds the transversal length of a truncated tetrahedron in three-dimensional hyperbolic space, meaning the distance from a designated internal edge to its opposite, in terms of the tetrahedron's entire collection of internal edge lengths. All main results are established using the unifying perspective of the hyperboloid model and Lorentzian geometry. A thorough introduction to this perspective is provided, with references as appropriate.","sentences":["The first main results of this note establish forms of the hyperbolic laws of cosines and sines for certain classes of quadrilaterals and pentagons in the hyperbolic plane, having at least one ideal vertex and right angles at non-ideal vertices, in which the length of a horocyclic cross-section at an ideal vertex plays the role filled by the dihedral angle in the usual versions of these laws.","The final main result bounds the transversal length of a truncated tetrahedron in three-dimensional hyperbolic space, meaning the distance from a designated internal edge to its opposite, in terms of the tetrahedron's entire collection of internal edge lengths.","All main results are established using the unifying perspective of the hyperboloid model and Lorentzian geometry.","A thorough introduction to this perspective is provided, with references as appropriate."],"url":"http://arxiv.org/abs/2403.04494v1","category":"math.GT"}
{"created":"2024-03-07 13:22:00","title":"Star-spot activity, orbital obliquity, transmission spectrum, physical properties, and TTVs of the HATS-2 planetary system","abstract":"Our aim in this paper is to refine the orbital and physical parameters of the HATS-2 planetary system and study transit timing variations and atmospheric composition thanks to transit observations that span more than ten years and that were collected using different instruments and pass-band filters. We also investigate the orbital alignment of the system by studying the anomalies in the transit light curves induced by starspots on the photosphere of the parent star. We analysed new transit events from both ground-based telescopes and NASA's TESS mission. Anomalies were detected in most of the light curves and modelled as starspots occulted by the planet during transit events. We fitted the clean and symmetric light curves with the JKTEBOP code and those affected by anomalies with the PRISM+GEMC codes to simultaneously model the photometric parameters of the transits and the position, size, and contrast of each starspot. We found consistency between the values we found for the physical and orbital parameters and those from the discovery paper and ATLAS9 stellar atmospherical models. We identified different sets of consecutive starspot-crossing events that temporally occurred in less than five days. Under the hypothesis that we are dealing with the same starspots, occulted twice by the planet during two consecutive transits, we estimated the rotational period of the parent star and, in turn the projected and the true orbital obliquity of the planet. We find that the system is well aligned. We identified the possible presence of transit timing variations in the system, which can be caused by tidal orbital decay, and we derived a low-resolution transmission spectrum.","sentences":["Our aim in this paper is to refine the orbital and physical parameters of the HATS-2 planetary system and study transit timing variations and atmospheric composition thanks to transit observations that span more than ten years and that were collected using different instruments and pass-band filters.","We also investigate the orbital alignment of the system by studying the anomalies in the transit light curves induced by starspots on the photosphere of the parent star.","We analysed new transit events from both ground-based telescopes and NASA's TESS mission.","Anomalies were detected in most of the light curves and modelled as starspots occulted by the planet during transit events.","We fitted the clean and symmetric light curves with the JKTEBOP code and those affected by anomalies with the PRISM+GEMC codes to simultaneously model the photometric parameters of the transits and the position, size, and contrast of each starspot.","We found consistency between the values we found for the physical and orbital parameters and those from the discovery paper and ATLAS9 stellar atmospherical models.","We identified different sets of consecutive starspot-crossing events that temporally occurred in less than five days.","Under the hypothesis that we are dealing with the same starspots, occulted twice by the planet during two consecutive transits, we estimated the rotational period of the parent star and, in turn the projected and the true orbital obliquity of the planet.","We find that the system is well aligned.","We identified the possible presence of transit timing variations in the system, which can be caused by tidal orbital decay, and we derived a low-resolution transmission spectrum."],"url":"http://arxiv.org/abs/2403.04476v1","category":"astro-ph.EP"}
{"created":"2024-03-07 11:49:33","title":"Student behaviour and engagement with adaptive exercises on a thermodynamics course","abstract":"A teaching experiment was carried out in a university-level thermodynamics course using adaptive and interactive e-learning material, created in the new Moodle question type Stateful extending the original e-learning platform STACK. The system collects data about the students that is used to algorithmically classify them according to their behaviour in solving problems. It is observed that the classification of this data predicts students' success in the other parts of the course for a majority of students. Also, the classification is statistically consistent with Thermodynamic Concept Survey and Maryland Physics Expectation Survey.","sentences":["A teaching experiment was carried out in a university-level thermodynamics course using adaptive and interactive e-learning material, created in the new Moodle question type Stateful extending the original e-learning platform STACK.","The system collects data about the students that is used to algorithmically classify them according to their behaviour in solving problems.","It is observed that the classification of this data predicts students' success in the other parts of the course for a majority of students.","Also, the classification is statistically consistent with Thermodynamic Concept Survey and Maryland Physics Expectation Survey."],"url":"http://arxiv.org/abs/2403.04424v1","category":"physics.ed-ph"}
{"created":"2024-03-07 11:36:09","title":"Unveiling A Hidden Risk: Exposing Educational but Malicious Repositories in GitHub","abstract":"Are malicious repositories hiding under the educational label in GitHub? Recent studies have identified collections of GitHub repositories hosting malware source code with notable collaboration among the developers. Thus, analyzing GitHub repositories deserves inevitable attention due to its open-source nature providing easy access to malicious software code and artifacts. Here we leverage the capabilities of ChatGPT in a qualitative study to annotate an educational GitHub repository based on maliciousness of its metadata contents. Our contribution is twofold. First, we demonstrate the employment of ChatGPT to understand and annotate the content published in software repositories. Second, we provide evidence of hidden risk in educational repositories contributing to the opportunities of potential threats and malicious intents. We carry out a systematic study on a collection of 35.2K GitHub repositories claimed to be created for educational purposes only. First, our study finds an increasing trend in the number of such repositories published every year. Second, 9294 of them are labeled by ChatGPT as malicious, and further categorization of the malicious ones detects 14 different malware families including DDoS, keylogger, ransomware and so on. Overall, this exploratory study flags a wake-up call for the community for better understanding and analysis of software platforms.","sentences":["Are malicious repositories hiding under the educational label in GitHub?","Recent studies have identified collections of GitHub repositories hosting malware source code with notable collaboration among the developers.","Thus, analyzing GitHub repositories deserves inevitable attention due to its open-source nature providing easy access to malicious software code and artifacts.","Here we leverage the capabilities of ChatGPT in a qualitative study to annotate an educational GitHub repository based on maliciousness of its metadata contents.","Our contribution is twofold.","First, we demonstrate the employment of ChatGPT to understand and annotate the content published in software repositories.","Second, we provide evidence of hidden risk in educational repositories contributing to the opportunities of potential threats and malicious intents.","We carry out a systematic study on a collection of 35.2K GitHub repositories claimed to be created for educational purposes only.","First, our study finds an increasing trend in the number of such repositories published every year.","Second, 9294 of them are labeled by ChatGPT as malicious, and further categorization of the malicious ones detects 14 different malware families including DDoS, keylogger, ransomware and so on.","Overall, this exploratory study flags a wake-up call for the community for better understanding and analysis of software platforms."],"url":"http://arxiv.org/abs/2403.04419v1","category":"cs.SE"}
{"created":"2024-03-07 11:15:33","title":"Direct visualization of domain wall pinning in sub-100nm 3D magnetic nanowires with cross-sectional curvature","abstract":"The study of 3D magnetic nanostructures has uncovered a range of rich phenomena including the stabilization and control of topological spin textures using nanoscale curvature, dynamic effects allowing controlled spin-wave emission, and novel ground states enabled by collective 3D frustrated interactions. From a technological perspective, 3D nanostructures offer routes to ultrahigh density data storage, massive interconnectivity within neuromorphic devices, as well as applications within health technologies, such as mechanical induction of stem cell differentiation. However, the fabrication of 3D nanomagnetic systems with feature sizes down to 10 nm poses a significant challenge. In this work we present a means of fabricating sub-100 nm 3D ferromagnetic nanowires, with both cross-sectional and longitudinal curvature, using two-photon lithography at a wavelength of 405 nm, combined with conventional deposition. Physical characterization illustrates that nanostructures with lateral features as low as 70 nm can be rapidly and reproducibly fabricated. A range of novel domain walls, with anti-vortex textures, coupled transverse textures, and hybrid vortex/anti-vortex textures are found to be enabled by the cross-sectional curvature of the system, as demonstrated by finite-element micromagnetic simulations. Magnetic force microscopy experiments in an externally applied magnetic field are used to image the injection and pinning of domain walls in the 3D magnetic nanowire. At specific field values, domain walls are observed to hop from trap to trap, providing a direct means to probe the local energy landscape. A simple model is presented demonstrating that thickness gradients and local roughness dictate the variation of pinning probability across the wire.","sentences":["The study of 3D magnetic nanostructures has uncovered a range of rich phenomena including the stabilization and control of topological spin textures using nanoscale curvature, dynamic effects allowing controlled spin-wave emission, and novel ground states enabled by collective 3D frustrated interactions.","From a technological perspective, 3D nanostructures offer routes to ultrahigh density data storage, massive interconnectivity within neuromorphic devices, as well as applications within health technologies, such as mechanical induction of stem cell differentiation.","However, the fabrication of 3D nanomagnetic systems with feature sizes down to 10 nm poses a significant challenge.","In this work we present a means of fabricating sub-100 nm 3D ferromagnetic nanowires, with both cross-sectional and longitudinal curvature, using two-photon lithography at a wavelength of 405 nm, combined with conventional deposition.","Physical characterization illustrates that nanostructures with lateral features as low as 70 nm can be rapidly and reproducibly fabricated.","A range of novel domain walls, with anti-vortex textures, coupled transverse textures, and hybrid vortex/anti-vortex textures are found to be enabled by the cross-sectional curvature of the system, as demonstrated by finite-element micromagnetic simulations.","Magnetic force microscopy experiments in an externally applied magnetic field are used to image the injection and pinning of domain walls in the 3D magnetic nanowire.","At specific field values, domain walls are observed to hop from trap to trap, providing a direct means to probe the local energy landscape.","A simple model is presented demonstrating that thickness gradients and local roughness dictate the variation of pinning probability across the wire."],"url":"http://arxiv.org/abs/2403.04411v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-07 09:06:19","title":"Momentum correlation of light nuclei in Au + Au collisions at $\\sqrt{s_{NN}}$ = 2.0 $\\sim$ 7.7 GeV","abstract":"Within the Ultra-relativistic Quantum Molecular Dynamics model (UrQMD) coupled with nucleon coalescence model and Mini-Spanning-Tree model, the yields of light nuclei have been stimulated in Au + Au collisions over an energy range of \\(\\sqrt{s_{NN}}=2.0\\sim7.7\\ \\rm{GeV}\\) and the momentum correlation functions of light nuclei pairs have been calculated by both the Lednick\\'{y}-Lyuboshitz and the Correlation After Burner methods. We compare the yields of light nuclei and their momentum correlation functions at midrapidity in this energy region with experimental data. It is found that there are differences between the results of the two models, and the coalescence method seems less valid at low collision energy. Furthermore, both the peak values of proton-proton correlation functions and the transition point of elliptic flows from out-of-plane to in-plane emission show a turning point around 3-4 GeV, which suggests that there is a relation between momentum correlation function and collective flow of particles.","sentences":["Within the Ultra-relativistic Quantum Molecular Dynamics model (UrQMD) coupled with nucleon coalescence model and Mini-Spanning-Tree model, the yields of light nuclei have been stimulated in Au + Au collisions over an energy range of \\(\\sqrt{s_{NN}}=2.0\\sim7.7\\ \\rm{GeV}\\) and the momentum correlation functions of light nuclei pairs have been calculated by both the Lednick\\'{y}-Lyuboshitz and the Correlation After Burner methods.","We compare the yields of light nuclei and their momentum correlation functions at midrapidity in this energy region with experimental data.","It is found that there are differences between the results of the two models, and the coalescence method seems less valid at low collision energy.","Furthermore, both the peak values of proton-proton correlation functions and the transition point of elliptic flows from out-of-plane to in-plane emission show a turning point around 3-4 GeV, which suggests that there is a relation between momentum correlation function and collective flow of particles."],"url":"http://arxiv.org/abs/2403.04341v1","category":"nucl-th"}
{"created":"2024-03-07 09:05:25","title":"Search for a pentaquark state decaying into $pJ/\u03c8$ in $\u03a5(1,2S)$ inclusive decays at Belle","abstract":"Using the data samples of 102 million $\\Upsilon(1S)$ and 158 million $\\Upsilon(2S)$ events collected by the Belle detector, we search for a pentaquark state in the $pJ/\\psi$ final state from $\\Upsilon(1,2S)$ inclusive decays. Here, the charge-conjugate $\\bar{p}J/\\psi$ is included. We observe clear $pJ/\\psi$ production in $\\Upsilon(1,2S)$ decays and measure the branching fractions to be $\\mathcal{B}[\\Upsilon(1S) \\to pJ/\\psi + anything] = [4.27 \\pm 0.16(stat.) \\pm 0.20(syst.)] \\times 10^{-5}$ and $\\mathcal{B}[\\Upsilon(2S) \\to pJ/\\psi + anything] = [3.59 \\pm 0.14(stat.) \\pm 0.16(syst.)] \\times 10^{-5}$. We also measure the cross section of inclusive $pJ/\\psi$ production in $e^+e^-$ annihilation to be $\\sigma(e^+e^- \\to pJ/\\psi + anything) = [57.5 \\pm 2.1 (stat.) \\pm 2.5(syst.)]$~fb at $\\sqrt{s} = 10.52~\\hbox{GeV}$ using an 89.5~fb$^{-1}$ continuum data sample. There is no significant $P_c(4312)^+$, $P_c(4440)^+$ or $P_c(4457)^+$ signal found in the $pJ/\\psi$ final states in $\\Upsilon(1,2S)$ inclusive decays. We determine the upper limits of $\\mathcal{B}[\\Upsilon(1,2S)\\to P_c^{+} + anything] \\cdot \\mathcal{B}(P_c^{+}\\to pJ/\\psi)$ to be at the $10^{-6}$ level.","sentences":["Using the data samples of 102 million $\\Upsilon(1S)$ and 158 million $\\Upsilon(2S)$ events collected by the Belle detector, we search for a pentaquark state in the $pJ/\\psi$ final state from $\\Upsilon(1,2S)$ inclusive decays.","Here, the charge-conjugate $\\bar{p}J/\\psi$ is included.","We observe clear $pJ/\\psi$ production in $\\Upsilon(1,2S)$ decays and measure the branching fractions to be $\\mathcal{B}[\\Upsilon(1S) \\to pJ/\\psi + anything]","= [4.27 \\pm 0.16(stat.)","\\pm 0.20(syst.)]","\\times 10^{-5}$ and $\\mathcal{B}[\\Upsilon(2S)","\\to pJ/\\psi + anything] =","[3.59 \\pm 0.14(stat.)","\\pm 0.16(syst.)]","\\times 10^{-5}$. We also measure the cross section of inclusive $pJ/\\psi$ production in $e^+e^-$ annihilation to be $\\sigma(e^+e^- \\to pJ/\\psi + anything) =","[57.5 \\pm 2.1 (stat.)","\\pm 2.5(syst.)]$~fb at $\\sqrt{s} = 10.52~\\hbox{GeV}$ using an 89.5~fb$^{-1}$","continuum data sample.","There is no significant $P_c(4312)^+$, $P_c(4440)^+$ or $P_c(4457)^+$ signal found in the $pJ/\\psi$ final states in $\\Upsilon(1,2S)$ inclusive decays.","We determine the upper limits of $\\mathcal{B}[\\Upsilon(1,2S)\\to P_c^{+} + anything] \\cdot \\mathcal{B}(P_c^{+}\\to pJ/\\psi)$ to be at the $10^{-6}$ level."],"url":"http://arxiv.org/abs/2403.04340v1","category":"hep-ex"}
{"created":"2024-03-07 07:31:00","title":"Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy","abstract":"Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach to ensure Large Language Models (LLMs) align with human values. However, existing RLHF methods require a high computational cost, one main reason being that RLHF assigns both the generation and alignment tasks to the LLM simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the generation and alignment processes of LLMs, achieving alignment with human values at a much lower computational cost. We start with a novel Markov Decision Process (MDP) designed for the alignment process and employ Reinforcement Learning (RL) to train a streamlined proxy model that oversees the token generation of the LLM, without altering the LLM itself. Experiments show that our method achieves a comparable level of alignment with only 1\\% of the training parameters of other methods.","sentences":["Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach to ensure Large Language Models (LLMs) align with human values.","However, existing RLHF methods require a high computational cost, one main reason being that RLHF assigns both the generation and alignment tasks to the LLM simultaneously.","In this paper, we introduce Proxy-RLHF, which decouples the generation and alignment processes of LLMs, achieving alignment with human values at a much lower computational cost.","We start with a novel Markov Decision Process (MDP) designed for the alignment process and employ Reinforcement Learning (RL) to train a streamlined proxy model that oversees the token generation of the LLM, without altering the LLM itself.","Experiments show that our method achieves a comparable level of alignment with only 1\\% of the training parameters of other methods."],"url":"http://arxiv.org/abs/2403.04283v1","category":"cs.CL"}
{"created":"2024-03-07 07:24:32","title":"A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain","abstract":"This work is an attempt to introduce a comprehensive benchmark for Arabic speech recognition, specifically tailored to address the challenges of telephone conversations in Arabic language. Arabic, characterized by its rich dialectal diversity and phonetic complexity, presents a number of unique challenges for automatic speech recognition (ASR) systems. These challenges are further amplified in the domain of telephone calls, where audio quality, background noise, and conversational speech styles negatively affect recognition accuracy. Our work aims to establish a robust benchmark that not only encompasses the broad spectrum of Arabic dialects but also emulates the real-world conditions of call-based communications. By incorporating diverse dialectical expressions and accounting for the variable quality of call recordings, this benchmark seeks to provide a rigorous testing ground for the development and evaluation of ASR systems capable of navigating the complexities of Arabic speech in telephonic contexts. This work also attempts to establish a baseline performance evaluation using state-of-the-art ASR technologies.","sentences":["This work is an attempt to introduce a comprehensive benchmark for Arabic speech recognition, specifically tailored to address the challenges of telephone conversations in Arabic language.","Arabic, characterized by its rich dialectal diversity and phonetic complexity, presents a number of unique challenges for automatic speech recognition (ASR) systems.","These challenges are further amplified in the domain of telephone calls, where audio quality, background noise, and conversational speech styles negatively affect recognition accuracy.","Our work aims to establish a robust benchmark that not only encompasses the broad spectrum of Arabic dialects but also emulates the real-world conditions of call-based communications.","By incorporating diverse dialectical expressions and accounting for the variable quality of call recordings, this benchmark seeks to provide a rigorous testing ground for the development and evaluation of ASR systems capable of navigating the complexities of Arabic speech in telephonic contexts.","This work also attempts to establish a baseline performance evaluation using state-of-the-art ASR technologies."],"url":"http://arxiv.org/abs/2403.04280v1","category":"cs.AI"}
{"created":"2024-03-07 07:24:18","title":"Controllable Generation with Text-to-Image Diffusion Models: A Survey","abstract":"In the rapidly advancing realm of visual generation, diffusion models have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions. However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios. Acknowledging this shortfall, a variety of studies aim to control pre-trained text-to-image (T2I) models to support novel conditions. In this survey, we undertake a thorough review of the literature on controllable generation with T2I diffusion models, covering both the theoretical foundations and practical advancements in this domain. Our review begins with a brief introduction to the basics of denoising diffusion probabilistic models (DDPMs) and widely used T2I diffusion models. We then reveal the controlling mechanisms of diffusion models, theoretically analyzing how novel conditions are introduced into the denoising process for conditional generation. Additionally, we offer a detailed overview of research in this area, organizing it into distinct categories from the condition perspective: generation with specific conditions, generation with multiple conditions, and universal controllable generation. For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at \\url{https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models}.","sentences":["In the rapidly advancing realm of visual generation, diffusion models have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions.","However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios.","Acknowledging this shortfall, a variety of studies aim to control pre-trained text-to-image (T2I) models to support novel conditions.","In this survey, we undertake a thorough review of the literature on controllable generation with T2I diffusion models, covering both the theoretical foundations and practical advancements in this domain.","Our review begins with a brief introduction to the basics of denoising diffusion probabilistic models (DDPMs) and widely used T2I diffusion models.","We then reveal the controlling mechanisms of diffusion models, theoretically analyzing how novel conditions are introduced into the denoising process for conditional generation.","Additionally, we offer a detailed overview of research in this area, organizing it into distinct categories from the condition perspective: generation with specific conditions, generation with multiple conditions, and universal controllable generation.","For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at \\url{https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models}."],"url":"http://arxiv.org/abs/2403.04279v1","category":"cs.CV"}
{"created":"2024-03-07 06:56:24","title":"Competitive Facility Location under Random Utilities and Routing Constraints","abstract":"In this paper, we study a facility location problem within a competitive market context, where customer demand is predicted by a random utility choice model. Unlike prior research, which primarily focuses on simple constraints such as a cardinality constraint on the number of selected locations, we introduce routing constraints that necessitate the selection of locations in a manner that guarantees the existence of a tour visiting all chosen locations while adhering to a specified tour length upper bound. Such routing constraints find crucial applications in various real-world scenarios. The problem at hand features a non-linear objective function, resulting from the utilization of random utilities, together with complex routing constraints, making it computationally challenging. To tackle this problem, we explore three types of valid cuts, namely, outer-approximation and submodular cuts to handle the nonlinear objective function, as well as sub-tour elimination cuts to address the complex routing constraints. These lead to the development of two exact solution methods: a nested cutting plane and nested branch-and-cut algorithms, where these valid cuts are iteratively added to a master problem through two nested loops. We also prove that our nested cutting plane method always converges to optimality after a finite number of iterations. Furthermore, we develop a local search-based metaheuristic tailored for solving large-scale instances and show its pros and cons compared to exact methods. Extensive experiments are conducted on problem instances of varying sizes, demonstrating that our approach excels in terms of solution quality and computation time when compared to other baseline approaches.","sentences":["In this paper, we study a facility location problem within a competitive market context, where customer demand is predicted by a random utility choice model.","Unlike prior research, which primarily focuses on simple constraints such as a cardinality constraint on the number of selected locations, we introduce routing constraints that necessitate the selection of locations in a manner that guarantees the existence of a tour visiting all chosen locations while adhering to a specified tour length upper bound.","Such routing constraints find crucial applications in various real-world scenarios.","The problem at hand features a non-linear objective function, resulting from the utilization of random utilities, together with complex routing constraints, making it computationally challenging.","To tackle this problem, we explore three types of valid cuts, namely, outer-approximation and submodular cuts to handle the nonlinear objective function, as well as sub-tour elimination cuts to address the complex routing constraints.","These lead to the development of two exact solution methods: a nested cutting plane and nested branch-and-cut algorithms, where these valid cuts are iteratively added to a master problem through two nested loops.","We also prove that our nested cutting plane method always converges to optimality after a finite number of iterations.","Furthermore, we develop a local search-based metaheuristic tailored for solving large-scale instances and show its pros and cons compared to exact methods.","Extensive experiments are conducted on problem instances of varying sizes, demonstrating that our approach excels in terms of solution quality and computation time when compared to other baseline approaches."],"url":"http://arxiv.org/abs/2403.04264v1","category":"cs.AI"}
{"created":"2024-03-07 06:52:51","title":"Advancing Biomedical Text Mining with Community Challenges","abstract":"The field of biomedical research has witnessed a significant increase in the accumulation of vast amounts of textual data from various sources such as scientific literatures, electronic health records, clinical trial reports, and social media. However, manually processing and analyzing these extensive and complex resources is time-consuming and inefficient. To address this challenge, biomedical text mining, also known as biomedical natural language processing, has garnered great attention. Community challenge evaluation competitions have played an important role in promoting technology innovation and interdisciplinary collaboration in biomedical text mining research. These challenges provide platforms for researchers to develop state-of-the-art solutions for data mining and information processing in biomedical research. In this article, we review the recent advances in community challenges specific to Chinese biomedical text mining. Firstly, we collect the information of these evaluation tasks, such as data sources and task types. Secondly, we conduct systematic summary and comparative analysis, including named entity recognition, entity normalization, attribute extraction, relation extraction, event extraction, text classification, text similarity, knowledge graph construction, question answering, text generation, and large language model evaluation. Then, we summarize the potential clinical applications of these community challenge tasks from translational informatics perspective. Finally, we discuss the contributions and limitations of these community challenges, while highlighting future directions in the era of large language models.","sentences":["The field of biomedical research has witnessed a significant increase in the accumulation of vast amounts of textual data from various sources such as scientific literatures, electronic health records, clinical trial reports, and social media.","However, manually processing and analyzing these extensive and complex resources is time-consuming and inefficient.","To address this challenge, biomedical text mining, also known as biomedical natural language processing, has garnered great attention.","Community challenge evaluation competitions have played an important role in promoting technology innovation and interdisciplinary collaboration in biomedical text mining research.","These challenges provide platforms for researchers to develop state-of-the-art solutions for data mining and information processing in biomedical research.","In this article, we review the recent advances in community challenges specific to Chinese biomedical text mining.","Firstly, we collect the information of these evaluation tasks, such as data sources and task types.","Secondly, we conduct systematic summary and comparative analysis, including named entity recognition, entity normalization, attribute extraction, relation extraction, event extraction, text classification, text similarity, knowledge graph construction, question answering, text generation, and large language model evaluation.","Then, we summarize the potential clinical applications of these community challenge tasks from translational informatics perspective.","Finally, we discuss the contributions and limitations of these community challenges, while highlighting future directions in the era of large language models."],"url":"http://arxiv.org/abs/2403.04261v1","category":"cs.AI"}
{"created":"2024-03-07 06:38:41","title":"Federated Recommendation via Hybrid Retrieval Augmented Generation","abstract":"Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations. However, traditional FR systems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to the data sparsity and heterogeneity in FR. On the other hand, Large Language Models (LLMs) as recommenders have proven effective across various recommendation scenarios. Yet, LLM-based recommenders encounter challenges such as low inference efficiency and potential hallucination, compromising their performance in real-world scenarios. To this end, we propose GPT-FedRec, a federated recommendation framework leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism. GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval process, mining ID-based user patterns and text-based item features. Next, the retrieved results are converted into text prompts and fed into GPT for re-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims to extract generalized features from data and exploit pretrained knowledge within LLM, overcoming data sparsity and heterogeneity in FR. In addition, the RAG approach also prevents LLM hallucination, improving the recommendation performance for real-world users. Experimental results on diverse benchmark datasets demonstrate the superior performance of GPT-FedRec against state-of-the-art baseline methods.","sentences":["Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations.","However, traditional FR systems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to the data sparsity and heterogeneity in FR.","On the other hand, Large Language Models (LLMs) as recommenders have proven effective across various recommendation scenarios.","Yet, LLM-based recommenders encounter challenges such as low inference efficiency and potential hallucination, compromising their performance in real-world scenarios.","To this end, we propose GPT-FedRec, a federated recommendation framework leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.","GPT-FedRec is a two-stage solution.","The first stage is a hybrid retrieval process, mining ID-based user patterns and text-based item features.","Next, the retrieved results are converted into text prompts and fed into GPT for re-ranking.","Our proposed hybrid retrieval mechanism and LLM-based re-rank aims to extract generalized features from data and exploit pretrained knowledge within LLM, overcoming data sparsity and heterogeneity in FR.","In addition, the RAG approach also prevents LLM hallucination, improving the recommendation performance for real-world users.","Experimental results on diverse benchmark datasets demonstrate the superior performance of GPT-FedRec against state-of-the-art baseline methods."],"url":"http://arxiv.org/abs/2403.04256v1","category":"cs.IR"}
{"created":"2024-03-07 06:23:42","title":"Low Complexity Radio Frequency Interference Mitigation for Radio Astronomy Using Large Antenna Array","abstract":"With the ongoing growth in radio communications, there is an increased contamination of radio astronomical source data, which hinders the study of celestial radio sources. In many cases, fast mitigation of strong radio frequency interference (RFI) is valuable for studying short lived radio transients so that the astronomers can perform detailed observations of celestial radio sources. The standard method to manually excise contaminated blocks in time and frequency makes the removed data useless for radio astronomy analyses. This motivates the need for better radio frequency interference (RFI) mitigation techniques for array of size M antennas. Although many solutions for mitigating strong RFI improves the quality of the final celestial source signal, many standard approaches require all the eigenvalues of the spatial covariance matrix ($\\textbf{R} \\in \\mathbb{C}^{M \\times M}$) of the received signal, which has $O(M^3)$ computation complexity for removing RFI of size $d$ where $\\textit{d} \\ll M$. In this work, we investigate two approaches for RFI mitigation, 1) the computationally efficient Lanczos method based on the Quadratic Mean to Arithmetic Mean (QMAM) approach using information from previously-collected data under similar radio-sky-conditions, and 2) an approach using a celestial source as a reference for RFI mitigation. QMAM uses the Lanczos method for finding the Rayleigh-Ritz values of the covariance matrix $\\textbf{R}$, thus, reducing the computational complexity of the overall approach to $O(\\textit{d}M^2)$. Our numerical results, using data from the radio observatory Long Wavelength Array (LWA-1), demonstrate the effectiveness of both proposed approaches to remove strong RFI, with the QMAM-based approach still being computationally efficient.","sentences":["With the ongoing growth in radio communications, there is an increased contamination of radio astronomical source data, which hinders the study of celestial radio sources.","In many cases, fast mitigation of strong radio frequency interference (RFI) is valuable for studying short lived radio transients so that the astronomers can perform detailed observations of celestial radio sources.","The standard method to manually excise contaminated blocks in time and frequency makes the removed data useless for radio astronomy analyses.","This motivates the need for better radio frequency interference (RFI) mitigation techniques for array of size M antennas.","Although many solutions for mitigating strong RFI improves the quality of the final celestial source signal, many standard approaches require all the eigenvalues of the spatial covariance matrix ($\\textbf{R} \\in \\mathbb{C}^{M \\times M}$) of the received signal, which has $O(M^3)$ computation complexity for removing RFI of size $d$ where $\\textit{d} \\ll M$. In this work, we investigate two approaches for RFI mitigation, 1) the computationally efficient Lanczos method based on the Quadratic Mean to Arithmetic Mean (QMAM) approach using information from previously-collected data under similar radio-sky-conditions, and 2) an approach using a celestial source as a reference for RFI mitigation.","QMAM uses the Lanczos method for finding the Rayleigh-Ritz values of the covariance matrix $\\textbf{R}$, thus, reducing the computational complexity of the overall approach to $O(\\textit{d}M^2)$. Our numerical results, using data from the radio observatory Long Wavelength Array (LWA-1), demonstrate the effectiveness of both proposed approaches to remove strong RFI, with the QMAM-based approach still being computationally efficient."],"url":"http://arxiv.org/abs/2403.04250v1","category":"eess.SP"}
{"created":"2024-03-07 06:07:31","title":"Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic Differential Equations","abstract":"This study addresses the challenges in parameter estimation of stochastic differential equations driven by non-Gaussian noises, which are critical in understanding dynamic phenomena such as price fluctuations and the spread of infectious diseases. Previous research highlighted the potential of LSTM networks in estimating parameters of alpha stable Levy driven SDEs but faced limitations including high time complexity and constraints of the LSTM chaining property. To mitigate these issues, we introduce the PEnet, a novel CNN-LSTM-based three-stage model that offers an end to end approach with superior accuracy and adaptability to varying data structures, enhanced inference speed for long sequence observations through initial data feature condensation by CNN, and high generalization capability, allowing its application to various complex SDE scenarios. Experiments on synthetic datasets confirm PEnet significant advantage in estimating SDE parameters associated with noise characteristics, establishing it as a competitive method for SDE parameter estimation in the presence of Levy noise.","sentences":["This study addresses the challenges in parameter estimation of stochastic differential equations driven by non-Gaussian noises, which are critical in understanding dynamic phenomena such as price fluctuations and the spread of infectious diseases.","Previous research highlighted the potential of LSTM networks in estimating parameters of alpha stable Levy driven SDEs but faced limitations including high time complexity and constraints of the LSTM chaining property.","To mitigate these issues, we introduce the PEnet, a novel CNN-LSTM-based three-stage model that offers an end to end approach with superior accuracy and adaptability to varying data structures, enhanced inference speed for long sequence observations through initial data feature condensation by CNN, and high generalization capability, allowing its application to various complex SDE scenarios.","Experiments on synthetic datasets confirm PEnet significant advantage in estimating SDE parameters associated with noise characteristics, establishing it as a competitive method for SDE parameter estimation in the presence of Levy noise."],"url":"http://arxiv.org/abs/2403.04246v1","category":"stat.ML"}
{"created":"2024-03-07 05:52:08","title":"Signal Response Model in PandaX-4T","abstract":"PandaX-4T experiment is a deep-underground dark matter direct search experiment that employs a dual-phase time projection chamber with a sensitive volume containing 3.7 tonne of liquid xenon. The detector of PandaX-4T is capable of simultaneously collecting the primary scintillation and ionization signals, utilizing their ratio to discriminate dark matter signals from background sources such as gamma rays and beta particles. The signal response model plays a crucial role in interpreting the data obtained by PandaX-4T. It describes the conversion from the deposited energy by dark matter interactions to the detectable signals within the detector. The signal response model is utilized in various PandaX-4T results. This work provides a comprehensive description of the procedures involved in constructing and parameter-fitting the signal response model for the energy range of approximately 1 keV to 25 keV for electronic recoils and 6 keV to 90 keV for nuclear recoils. It also covers the signal reconstruction, selection, and correction methods, which are crucial components integrated into the signal response model.","sentences":["PandaX-4T experiment is a deep-underground dark matter direct search experiment that employs a dual-phase time projection chamber with a sensitive volume containing 3.7 tonne of liquid xenon.","The detector of PandaX-4T is capable of simultaneously collecting the primary scintillation and ionization signals, utilizing their ratio to discriminate dark matter signals from background sources such as gamma rays and beta particles.","The signal response model plays a crucial role in interpreting the data obtained by PandaX-4T. It describes the conversion from the deposited energy by dark matter interactions to the detectable signals within the detector.","The signal response model is utilized in various PandaX-4T results.","This work provides a comprehensive description of the procedures involved in constructing and parameter-fitting the signal response model for the energy range of approximately 1 keV to 25 keV for electronic recoils and 6 keV to 90 keV for nuclear recoils.","It also covers the signal reconstruction, selection, and correction methods, which are crucial components integrated into the signal response model."],"url":"http://arxiv.org/abs/2403.04239v1","category":"physics.ins-det"}
{"created":"2024-03-07 05:26:41","title":"DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning","abstract":"It has long been assumed that the sheer number of parameters in large language models (LLMs) drives in-context learning (ICL) capabilities, enabling remarkable performance improvements by leveraging task-specific demonstrations. Challenging this hypothesis, we introduce DEEP-ICL, a novel task Definition Enriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts task definitions from given demonstrations and generates responses through learning task-specific examples. We argue that improvement from ICL does not directly rely on model size, but essentially stems from understanding task definitions and task-guided learning. Inspired by this, DEEP-ICL combines two 3B models with distinct roles (one for concluding task definitions and the other for learning task demonstrations) and achieves comparable performance to LLaMA2-13B. Furthermore, our framework outperforms conventional ICL by overcoming pretraining sequence length limitations, by supporting unlimited demonstrations. We contend that DEEP-ICL presents a novel alternative for achieving efficient few-shot learning, extending beyond the conventional ICL.","sentences":["It has long been assumed that the sheer number of parameters in large language models (LLMs) drives in-context learning (ICL) capabilities, enabling remarkable performance improvements by leveraging task-specific demonstrations.","Challenging this hypothesis, we introduce DEEP-ICL, a novel task Definition Enriched ExPert Ensembling methodology for ICL.","DEEP-ICL explicitly extracts task definitions from given demonstrations and generates responses through learning task-specific examples.","We argue that improvement from ICL does not directly rely on model size, but essentially stems from understanding task definitions and task-guided learning.","Inspired by this, DEEP-ICL combines two 3B models with distinct roles (one for concluding task definitions and the other for learning task demonstrations) and achieves comparable performance to LLaMA2-13B. Furthermore, our framework outperforms conventional ICL by overcoming pretraining sequence length limitations, by supporting unlimited demonstrations.","We contend that DEEP-ICL presents a novel alternative for achieving efficient few-shot learning, extending beyond the conventional ICL."],"url":"http://arxiv.org/abs/2403.04233v1","category":"cs.CL"}
{"created":"2024-03-07 05:25:34","title":"Generalizing Cooperative Eco-driving via Multi-residual Task Learning","abstract":"Conventional control, such as model-based control, is commonly utilized in autonomous driving due to its efficiency and reliability. However, real-world autonomous driving contends with a multitude of diverse traffic scenarios that are challenging for these planning algorithms. Model-free Deep Reinforcement Learning (DRL) presents a promising avenue in this direction, but learning DRL control policies that generalize to multiple traffic scenarios is still a challenge. To address this, we introduce Multi-residual Task Learning (MRTL), a generic learning framework based on multi-task learning that, for a set of task scenarios, decomposes the control into nominal components that are effectively solved by conventional control methods and residual terms which are solved using learning. We employ MRTL for fleet-level emission reduction in mixed traffic using autonomous vehicles as a means of system control. By analyzing the performance of MRTL across nearly 600 signalized intersections and 1200 traffic scenarios, we demonstrate that it emerges as a promising approach to synergize the strengths of DRL and conventional methods in generalizable control.","sentences":["Conventional control, such as model-based control, is commonly utilized in autonomous driving due to its efficiency and reliability.","However, real-world autonomous driving contends with a multitude of diverse traffic scenarios that are challenging for these planning algorithms.","Model-free Deep Reinforcement Learning (DRL) presents a promising avenue in this direction, but learning DRL control policies that generalize to multiple traffic scenarios is still a challenge.","To address this, we introduce Multi-residual Task Learning (MRTL), a generic learning framework based on multi-task learning that, for a set of task scenarios, decomposes the control into nominal components that are effectively solved by conventional control methods and residual terms which are solved using learning.","We employ MRTL for fleet-level emission reduction in mixed traffic using autonomous vehicles as a means of system control.","By analyzing the performance of MRTL across nearly 600 signalized intersections and 1200 traffic scenarios, we demonstrate that it emerges as a promising approach to synergize the strengths of DRL and conventional methods in generalizable control."],"url":"http://arxiv.org/abs/2403.04232v1","category":"cs.RO"}
{"created":"2024-03-07 05:15:40","title":"Photon Absorption Remote Sensing (PARS): A Comprehensive Approach to Label-free Absorption Microscopy Across Biological Scales","abstract":"Label-free optical absorption microscopy techniques have evolved as effective tools for non-invasive chemical specific structural, and functional imaging. Yet most modern label-free microscopy modalities target only a fraction of the contrast afforded by an optical absorption interaction. We introduce a comprehensive optical absorption microscopy technique, Photon Absorption Remote Sensing (PARS), which simultaneously captures the dominant light matter interactions which occur as a pulse of light is absorbed by a molecule. In PARS, the optical scattering, attenuation, and the transient radiative and non-radiative relaxation processes are collected at each optical absorption event. This provides a complete representation of the absorption event, providing unique contrast presented here as the total absorption (TA) and quantum efficiency ratio (QER) measurements. By capturing a complete view of each absorption interaction, PARS bridges many of the specificity challenges associated with label-free imaging, facilitating recovery of a wider range of biomolecules than independent radiative or non-radiative modalities. To show the versatility of PARS, we explore imaging across a wide range of biological specimens, from single cells to in-vivo imaging of living subjects. These examples of label-free histopathological imaging, and vascular imaging illustrate some of the numerous fields where PARS may have profound impacts. Overall PARS may provide comprehensive label-free contrast in a wide variety of biological specimens, providing otherwise inaccessible visualizations, and representing a new a source of rich data to develop new AI and machine learning methods for diagnostics and visualization.","sentences":["Label-free optical absorption microscopy techniques have evolved as effective tools for non-invasive chemical specific structural, and functional imaging.","Yet most modern label-free microscopy modalities target only a fraction of the contrast afforded by an optical absorption interaction.","We introduce a comprehensive optical absorption microscopy technique, Photon Absorption Remote Sensing (PARS), which simultaneously captures the dominant light matter interactions which occur as a pulse of light is absorbed by a molecule.","In PARS, the optical scattering, attenuation, and the transient radiative and non-radiative relaxation processes are collected at each optical absorption event.","This provides a complete representation of the absorption event, providing unique contrast presented here as the total absorption (TA) and quantum efficiency ratio (QER) measurements.","By capturing a complete view of each absorption interaction, PARS bridges many of the specificity challenges associated with label-free imaging, facilitating recovery of a wider range of biomolecules than independent radiative or non-radiative modalities.","To show the versatility of PARS, we explore imaging across a wide range of biological specimens, from single cells to in-vivo imaging of living subjects.","These examples of label-free histopathological imaging, and vascular imaging illustrate some of the numerous fields where PARS may have profound impacts.","Overall PARS may provide comprehensive label-free contrast in a wide variety of biological specimens, providing otherwise inaccessible visualizations, and representing a new a source of rich data to develop new AI and machine learning methods for diagnostics and visualization."],"url":"http://arxiv.org/abs/2403.04229v1","category":"physics.optics"}
{"created":"2024-03-07 05:03:18","title":"Disciplining deliberation: a sociotechnical perspective on machine learning trade-offs","abstract":"This paper focuses on two highly publicized formal trade-offs in the field of responsible artificial intelligence (AI) -- between predictive accuracy and fairness and between predictive accuracy and interpretability. These formal trade-offs are often taken by researchers, practitioners, and policy-makers to directly imply corresponding tensions between underlying values. Thus interpreted, the trade-offs have formed a core focus of normative engagement in AI governance, accompanied by a particular division of labor along disciplinary lines. This paper argues against this prevalent interpretation by drawing attention to three sets of considerations that are critical for bridging the gap between these formal trade-offs and their practical impacts on relevant values. I show how neglecting these considerations can distort our normative deliberations, and result in costly and misaligned interventions and justifications. Taken together, these considerations form a sociotechnical framework that could guide those involved in AI governance to assess how, in many cases, we can and should have higher aspirations than the prevalent interpretation of the trade-offs would suggest. I end by drawing out the normative opportunities and challenges that emerge out of these considerations, and highlighting the imperative of interdisciplinary collaboration in fostering responsible AI.","sentences":["This paper focuses on two highly publicized formal trade-offs in the field of responsible artificial intelligence (AI) -- between predictive accuracy and fairness and between predictive accuracy and interpretability.","These formal trade-offs are often taken by researchers, practitioners, and policy-makers to directly imply corresponding tensions between underlying values.","Thus interpreted, the trade-offs have formed a core focus of normative engagement in AI governance, accompanied by a particular division of labor along disciplinary lines.","This paper argues against this prevalent interpretation by drawing attention to three sets of considerations that are critical for bridging the gap between these formal trade-offs and their practical impacts on relevant values.","I show how neglecting these considerations can distort our normative deliberations, and result in costly and misaligned interventions and justifications.","Taken together, these considerations form a sociotechnical framework that could guide those involved in AI governance to assess how, in many cases, we can and should have higher aspirations than the prevalent interpretation of the trade-offs would suggest.","I end by drawing out the normative opportunities and challenges that emerge out of these considerations, and highlighting the imperative of interdisciplinary collaboration in fostering responsible AI."],"url":"http://arxiv.org/abs/2403.04226v1","category":"cs.CY"}
{"created":"2024-03-07 05:01:07","title":"3DTextureTransformer: Geometry Aware Texture Generation for Arbitrary Mesh Topology","abstract":"Learning to generate textures for a novel 3D mesh given a collection of 3D meshes and real-world 2D images is an important problem with applications in various domains such as 3D simulation, augmented and virtual reality, gaming, architecture, and design. Existing solutions either do not produce high-quality textures or deform the original high-resolution input mesh topology into a regular grid to make this generation easier but also lose the original mesh topology. In this paper, we present a novel framework called the 3DTextureTransformer that enables us to generate high-quality textures without deforming the original, high-resolution input mesh. Our solution, a hybrid of geometric deep learning and StyleGAN-like architecture, is flexible enough to work on arbitrary mesh topologies and also easily extensible to texture generation for point cloud representations. Our solution employs a message-passing framework in 3D in conjunction with a StyleGAN-like architecture for 3D texture generation. The architecture achieves state-of-the-art performance among a class of solutions that can learn from a collection of 3D geometry and real-world 2D images while working with any arbitrary mesh topology.","sentences":["Learning to generate textures for a novel 3D mesh given a collection of 3D meshes and real-world 2D images is an important problem with applications in various domains such as 3D simulation, augmented and virtual reality, gaming, architecture, and design.","Existing solutions either do not produce high-quality textures or deform the original high-resolution input mesh topology into a regular grid to make this generation easier but also lose the original mesh topology.","In this paper, we present a novel framework called the 3DTextureTransformer that enables us to generate high-quality textures without deforming the original, high-resolution input mesh.","Our solution, a hybrid of geometric deep learning and StyleGAN-like architecture, is flexible enough to work on arbitrary mesh topologies and also easily extensible to texture generation for point cloud representations.","Our solution employs a message-passing framework in 3D in conjunction with a StyleGAN-like architecture for 3D texture generation.","The architecture achieves state-of-the-art performance among a class of solutions that can learn from a collection of 3D geometry and real-world 2D images while working with any arbitrary mesh topology."],"url":"http://arxiv.org/abs/2403.04225v1","category":"cs.CV"}
{"created":"2024-03-07 04:54:56","title":"Aligners: Decoupling LLMs and Alignment","abstract":"Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an \"ethical\" aligner and verify its efficacy empirically.","sentences":["Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications.","Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion.","We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance.","Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria.","We illustrate our method by training an \"ethical\" aligner and verify its efficacy empirically."],"url":"http://arxiv.org/abs/2403.04224v1","category":"cs.CL"}
{"created":"2024-03-07 04:49:48","title":"Why Online Reinforcement Learning is Causal","abstract":"Reinforcement learning (RL) and causal modelling naturally complement each other. The goal of causal modelling is to predict the effects of interventions in an environment, while the goal of reinforcement learning is to select interventions that maximize the rewards the agent receives from the environment. Reinforcement learning includes the two most powerful sources of information for estimating causal relationships: temporal ordering and the ability to act on an environment. This paper examines which reinforcement learning settings we can expect to benefit from causal modelling, and how. In online learning, the agent has the ability to interact directly with their environment, and learn from exploring it. Our main argument is that in online learning, conditional probabilities are causal, and therefore offline RL is the setting where causal learning has the most potential to make a difference. Essentially, the reason is that when an agent learns from their {\\em own} experience, there are no unobserved confounders that influence both the agent's own exploratory actions and the rewards they receive. Our paper formalizes this argument. For offline RL, where an agent may and typically does learn from the experience of {\\em others}, we describe previous and new methods for leveraging a causal model, including support for counterfactual queries.","sentences":["Reinforcement learning (RL) and causal modelling naturally complement each other.","The goal of causal modelling is to predict the effects of interventions in an environment, while the goal of reinforcement learning is to select interventions that maximize the rewards the agent receives from the environment.","Reinforcement learning includes the two most powerful sources of information for estimating causal relationships: temporal ordering and the ability to act on an environment.","This paper examines which reinforcement learning settings we can expect to benefit from causal modelling, and how.","In online learning, the agent has the ability to interact directly with their environment, and learn from exploring it.","Our main argument is that in online learning, conditional probabilities are causal, and therefore offline RL is the setting where causal learning has the most potential to make a difference.","Essentially, the reason is that when an agent learns from their {\\em own} experience, there are no unobserved confounders that influence both the agent's own exploratory actions and the rewards they receive.","Our paper formalizes this argument.","For offline RL, where an agent may and typically does learn from the experience of {\\em others}, we describe previous and new methods for leveraging a causal model, including support for counterfactual queries."],"url":"http://arxiv.org/abs/2403.04221v1","category":"cs.LG"}
{"created":"2024-03-07 04:39:32","title":"Scalable On-Chip Optical Linear Processing Unit Using a Single Thin-Film Lithium Niobate Ring Modulator","abstract":"Advancements in artificial intelligence (AI) and neuromorphic computing increasingly rely on the integration of photonics to achieve breakthroughs in processing capabilities. Our pioneering work introduces a photonic linear processing unit (LPU) that utilizes a cascading modulator structure incorporating micro-rings. This device, elegantly designed with only two thin-film lithium niobate (TFLN) modulators coupled with a micro-ring, stands as a paradigm of innovation that merges low-power consumption with formidable computational throughput, achieving 36.7 billion operations per second (GOPs).The crux of our design lies in the ring-modulator's flexible architecture, engineered as a compact and singular unit, which markedly streamlines system complexity while bolstering energy efficiency. It adeptly facilitates large-scale dot-product operations, supporting vector dimensions up to 5832, an impressive feat by current standards. Furthermore, this ring-modulator-based LPU exhibits proficiency in image classification, processing 28*28-pixel resolution imagery post hardware training. As data volume demands surge, our architecture offers a scalable solution through parallel matrix multiplications, which hinge solely on increased modulation rates. This development paves the way for a new class of photonic processors that promise to handle escalating data workloads with unprecedented flexibility and efficiency.","sentences":["Advancements in artificial intelligence (AI) and neuromorphic computing increasingly rely on the integration of photonics to achieve breakthroughs in processing capabilities.","Our pioneering work introduces a photonic linear processing unit (LPU) that utilizes a cascading modulator structure incorporating micro-rings.","This device, elegantly designed with only two thin-film lithium niobate (TFLN) modulators coupled with a micro-ring, stands as a paradigm of innovation that merges low-power consumption with formidable computational throughput, achieving 36.7 billion operations per second (GOPs).The crux of our design lies in the ring-modulator's flexible architecture, engineered as a compact and singular unit, which markedly streamlines system complexity while bolstering energy efficiency.","It adeptly facilitates large-scale dot-product operations, supporting vector dimensions up to 5832, an impressive feat by current standards.","Furthermore, this ring-modulator-based LPU exhibits proficiency in image classification, processing 28*28-pixel resolution imagery post hardware training.","As data volume demands surge, our architecture offers a scalable solution through parallel matrix multiplications, which hinge solely on increased modulation rates.","This development paves the way for a new class of photonic processors that promise to handle escalating data workloads with unprecedented flexibility and efficiency."],"url":"http://arxiv.org/abs/2403.04216v1","category":"physics.optics"}
{"created":"2024-03-07 04:23:07","title":"HeteroSwitch: Characterizing and Taming System-Induced Data Heterogeneity in Federated Learning","abstract":"Federated Learning (FL) is a practical approach to train deep learning models collaboratively across user-end devices, protecting user privacy by retaining raw data on-device. In FL, participating user-end devices are highly fragmented in terms of hardware and software configurations. Such fragmentation introduces a new type of data heterogeneity in FL, namely \\textit{system-induced data heterogeneity}, as each device generates distinct data depending on its hardware and software configurations. In this paper, we first characterize the impact of system-induced data heterogeneity on FL model performance. We collect a dataset using heterogeneous devices with variations across vendors and performance tiers. By using this dataset, we demonstrate that \\textit{system-induced data heterogeneity} negatively impacts accuracy, and deteriorates fairness and domain generalization problems in FL. To address these challenges, we propose HeteroSwitch, which adaptively adopts generalization techniques (i.e., ISP transformation and SWAD) depending on the level of bias caused by varying HW and SW configurations. In our evaluation with a realistic FL dataset (FLAIR), HeteroSwitch reduces the variance of averaged precision by 6.3\\% across device types.","sentences":["Federated Learning (FL) is a practical approach to train deep learning models collaboratively across user-end devices, protecting user privacy by retaining raw data on-device.","In FL, participating user-end devices are highly fragmented in terms of hardware and software configurations.","Such fragmentation introduces a new type of data heterogeneity in FL, namely \\textit{system-induced data heterogeneity}, as each device generates distinct data depending on its hardware and software configurations.","In this paper, we first characterize the impact of system-induced data heterogeneity on FL model performance.","We collect a dataset using heterogeneous devices with variations across vendors and performance tiers.","By using this dataset, we demonstrate that \\textit{system-induced data heterogeneity} negatively impacts accuracy, and deteriorates fairness and domain generalization problems in FL.","To address these challenges, we propose HeteroSwitch, which adaptively adopts generalization techniques (i.e., ISP transformation and SWAD) depending on the level of bias caused by varying HW and SW configurations.","In our evaluation with a realistic FL dataset (FLAIR), HeteroSwitch reduces the variance of averaged precision by 6.3\\% across device types."],"url":"http://arxiv.org/abs/2403.04207v1","category":"cs.LG"}
{"created":"2024-03-07 04:19:13","title":"On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models","abstract":"Big models have achieved revolutionary breakthroughs in the field of AI, but they might also pose potential concerns. Addressing such concerns, alignment technologies were introduced to make these models conform to human preferences and values. Despite considerable advancements in the past year, various challenges lie in establishing the optimal alignment strategy, such as data cost and scalable oversight, and how to align remains an open question. In this survey paper, we comprehensively investigate value alignment approaches. We first unpack the historical context of alignment tracing back to the 1920s (where it comes from), then delve into the mathematical essence of alignment (what it is), shedding light on the inherent challenges. Following this foundation, we provide a detailed examination of existing alignment methods, which fall into three categories: Reinforcement Learning, Supervised Fine-Tuning, and In-context Learning, and demonstrate their intrinsic connections, strengths, and limitations, helping readers better understand this research area. In addition, two emerging topics, personal alignment, and multimodal alignment, are also discussed as novel frontiers in this field. Looking forward, we discuss potential alignment paradigms and how they could handle remaining challenges, prospecting where future alignment will go.","sentences":["Big models have achieved revolutionary breakthroughs in the field of AI, but they might also pose potential concerns.","Addressing such concerns, alignment technologies were introduced to make these models conform to human preferences and values.","Despite considerable advancements in the past year, various challenges lie in establishing the optimal alignment strategy, such as data cost and scalable oversight, and how to align remains an open question.","In this survey paper, we comprehensively investigate value alignment approaches.","We first unpack the historical context of alignment tracing back to the 1920s (where it comes from), then delve into the mathematical essence of alignment (what it is), shedding light on the inherent challenges.","Following this foundation, we provide a detailed examination of existing alignment methods, which fall into three categories: Reinforcement Learning, Supervised Fine-Tuning, and In-context Learning, and demonstrate their intrinsic connections, strengths, and limitations, helping readers better understand this research area.","In addition, two emerging topics, personal alignment, and multimodal alignment, are also discussed as novel frontiers in this field.","Looking forward, we discuss potential alignment paradigms and how they could handle remaining challenges, prospecting where future alignment will go."],"url":"http://arxiv.org/abs/2403.04204v1","category":"cs.AI"}
{"created":"2024-03-07 04:12:24","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents","abstract":"Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in populations is not well understood. In this paper, we present a study of the learning dynamics of morally heterogeneous populations interacting in a social dilemma setting. Using a Prisoner's Dilemma environment with a partner selection mechanism, we investigate the extent to which the prevalence of diverse moral agents in populations affects individual agents' learning behaviors and emergent population-level outcomes. We observe several types of non-trivial interactions between pro-social and anti-social agents, and find that certain classes of moral agents are able to steer selfish agents towards more cooperative behavior.","sentences":["Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents.","A promising solution is the use of learning from experience, i.e., Reinforcement Learning.","In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents.","Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents.","However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice.","For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now).","The extent to which agents' co-development may be impacted by such moral heterogeneity in populations is not well understood.","In this paper, we present a study of the learning dynamics of morally heterogeneous populations interacting in a social dilemma setting.","Using a Prisoner's Dilemma environment with a partner selection mechanism, we investigate the extent to which the prevalence of diverse moral agents in populations affects individual agents' learning behaviors and emergent population-level outcomes.","We observe several types of non-trivial interactions between pro-social and anti-social agents, and find that certain classes of moral agents are able to steer selfish agents towards more cooperative behavior."],"url":"http://arxiv.org/abs/2403.04202v1","category":"cs.MA"}
{"created":"2024-03-07 03:58:28","title":"Large Language Models are In-Context Molecule Learners","abstract":"Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Cross-modal Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Cross-modal Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. Additionally, we also propose Post-retrieval Re-ranking with Sequence Reversal and Random Walk to further improve the quality of retrieval results. Finally, In-Context Molecule Tuning unlocks the in-context molecule learning capability of LLMs with retrieved examples and adapts the parameters of LLMs for the molecule-caption translation task. Experimental results demonstrate that ICMT can empower LLMs to achieve state-of-the-art or comparable performance without extra training corpora and intricate structures, showing that LLMs are inherently in-context molecule learners.","sentences":["Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts.","However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs.","To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning.","Specifically, ICMA incorporates the following three stages: Cross-modal Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning.","Initially, Cross-modal Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples.","Additionally, we also propose Post-retrieval Re-ranking with Sequence Reversal and Random Walk to further improve the quality of retrieval results.","Finally, In-Context Molecule Tuning unlocks the in-context molecule learning capability of LLMs with retrieved examples and adapts the parameters of LLMs for the molecule-caption translation task.","Experimental results demonstrate that ICMT can empower LLMs to achieve state-of-the-art or comparable performance without extra training corpora and intricate structures, showing that LLMs are inherently in-context molecule learners."],"url":"http://arxiv.org/abs/2403.04197v1","category":"cs.CL"}
{"created":"2024-03-07 03:55:56","title":"Fill-and-Spill: Deep Reinforcement Learning Policy Gradient Methods for Reservoir Operation Decision and Control","abstract":"Changes in demand, various hydrological inputs, and environmental stressors are among the issues that water managers and policymakers face on a regular basis. These concerns have sparked interest in applying different techniques to determine reservoir operation policy decisions. As the resolution of the analysis increases, it becomes more difficult to effectively represent a real-world system using traditional methods such as Dynamic Programming (DP) and Stochastic Dynamic Programming (SDP) for determining the best reservoir operation policy. One of the challenges is the \"curse of dimensionality,\" which means the number of samples needed to estimate an arbitrary function with a given level of accuracy grows exponentially with respect to the number of input variables (i.e., dimensionality) of the function. Deep Reinforcement Learning (DRL) is an intelligent approach to overcome the curses of stochastic optimization problems for reservoir operation policy decisions. To our knowledge, this study is the first attempt that examine various novel DRL continuous-action policy gradient methods (PGMs), including Deep Deterministic Policy Gradients (DDPG), Twin Delayed DDPG (TD3), and two different versions of Soft Actor-Critic (SAC18 and SAC19) for optimizing reservoir operation policy. In this study, multiple DRL techniques were implemented in order to find the optimal operation policy of Folsom Reservoir in California, USA. The reservoir system supplies agricultural, municipal, hydropower, and environmental flow demands and flood control operations to the City of Sacramento. Analysis suggests that the TD3 and SAC are robust to meet the Folsom Reservoir's demands and optimize reservoir operation policies.","sentences":["Changes in demand, various hydrological inputs, and environmental stressors are among the issues that water managers and policymakers face on a regular basis.","These concerns have sparked interest in applying different techniques to determine reservoir operation policy decisions.","As the resolution of the analysis increases, it becomes more difficult to effectively represent a real-world system using traditional methods such as Dynamic Programming (DP) and Stochastic Dynamic Programming (SDP) for determining the best reservoir operation policy.","One of the challenges is the \"curse of dimensionality,\" which means the number of samples needed to estimate an arbitrary function with a given level of accuracy grows exponentially with respect to the number of input variables (i.e., dimensionality) of the function.","Deep Reinforcement Learning (DRL) is an intelligent approach to overcome the curses of stochastic optimization problems for reservoir operation policy decisions.","To our knowledge, this study is the first attempt that examine various novel DRL continuous-action policy gradient methods (PGMs), including Deep Deterministic Policy Gradients (DDPG), Twin Delayed DDPG (TD3), and two different versions of Soft Actor-Critic (SAC18 and SAC19) for optimizing reservoir operation policy.","In this study, multiple DRL techniques were implemented in order to find the optimal operation policy of Folsom Reservoir in California, USA.","The reservoir system supplies agricultural, municipal, hydropower, and environmental flow demands and flood control operations to the City of Sacramento.","Analysis suggests that the TD3 and SAC are robust to meet the Folsom Reservoir's demands and optimize reservoir operation policies."],"url":"http://arxiv.org/abs/2403.04195v1","category":"cs.LG"}
{"created":"2024-03-07 03:38:44","title":"Generative AI for Synthetic Data Generation: Methods, Challenges and the Future","abstract":"The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI). Their ability to perform comparably to real-world data positions this approach as a compelling solution to low-resource challenges. This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task-specific training data. We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research.","sentences":["The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI).","Their ability to perform comparably to real-world data positions this approach as a compelling solution to low-resource challenges.","This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task-specific training data.","We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research."],"url":"http://arxiv.org/abs/2403.04190v1","category":"cs.LG"}
{"created":"2024-03-07 03:36:03","title":"Preference optimization of protein language models as a multi-objective binder design paradigm","abstract":"We present a multi-objective binder design paradigm based on instruction fine-tuning and direct preference optimization (DPO) of autoregressive protein language models (pLMs). Multiple design objectives are encoded in the language model through direct optimization on expert curated preference sequence datasets comprising preferred and dispreferred distributions. We show the proposed alignment strategy enables ProtGPT2 to effectively design binders conditioned on specified receptors and a drug developability criterion. Generated binder samples demonstrate median isoelectric point (pI) improvements by $17\\%-60\\%$.","sentences":["We present a multi-objective binder design paradigm based on instruction fine-tuning and direct preference optimization (DPO) of autoregressive protein language models (pLMs).","Multiple design objectives are encoded in the language model through direct optimization on expert curated preference sequence datasets comprising preferred and dispreferred distributions.","We show the proposed alignment strategy enables ProtGPT2 to effectively design binders conditioned on specified receptors and a drug developability criterion.","Generated binder samples demonstrate median isoelectric point (pI) improvements by $17\\%-60\\%$."],"url":"http://arxiv.org/abs/2403.04187v1","category":"physics.bio-ph"}
{"created":"2024-03-07 03:24:34","title":"Metric-aware LLM inference","abstract":"Large language models (LLMs) have demonstrated strong results on a range of NLP tasks. Typically, outputs are obtained via autoregressive sampling from the LLM's underlying distribution. We show that this inference strategy can be suboptimal for a range of tasks and associated evaluation metrics. As a remedy, we propose metric aware LLM inference: a decision theoretic approach optimizing for custom metrics at inference time. We report improvements over baselines on academic benchmarks and publicly available models.","sentences":["Large language models (LLMs) have demonstrated strong results on a range of NLP tasks.","Typically, outputs are obtained via autoregressive sampling from the LLM's underlying distribution.","We show that this inference strategy can be suboptimal for a range of tasks and associated evaluation metrics.","As a remedy, we propose metric aware LLM inference: a decision theoretic approach optimizing for custom metrics at inference time.","We report improvements over baselines on academic benchmarks and publicly available models."],"url":"http://arxiv.org/abs/2403.04182v1","category":"cs.CL"}
{"created":"2024-03-07 03:23:13","title":"RATSF: Empowering Customer Service Volume Management through Retrieval-Augmented Time-Series Forecasting","abstract":"An efficient customer service management system hinges on precise forecasting of service volume. In this scenario, where data non-stationarity is pronounced, successful forecasting heavily relies on identifying and leveraging similar historical data rather than merely summarizing periodic patterns. Existing models based on RNN or Transformer architectures often struggle with this flexible and effective utilization. To address this challenge, we propose an efficient and adaptable cross-attention module termed RACA, which effectively leverages historical segments in forecasting task, and we devised a precise representation scheme for querying historical sequences, coupled with the design of a knowledge repository. These critical components collectively form our Retrieval-Augmented Temporal Sequence Forecasting framework (RATSF). RATSF not only significantly enhances performance in the context of Fliggy hotel service volume forecasting but, more crucially, can be seamlessly integrated into other Transformer-based time-series forecasting models across various application scenarios. Extensive experimentation has validated the effectiveness and generalizability of this system design across multiple diverse contexts.","sentences":["An efficient customer service management system hinges on precise forecasting of service volume.","In this scenario, where data non-stationarity is pronounced, successful forecasting heavily relies on identifying and leveraging similar historical data rather than merely summarizing periodic patterns.","Existing models based on RNN or Transformer architectures often struggle with this flexible and effective utilization.","To address this challenge, we propose an efficient and adaptable cross-attention module termed RACA, which effectively leverages historical segments in forecasting task, and we devised a precise representation scheme for querying historical sequences, coupled with the design of a knowledge repository.","These critical components collectively form our Retrieval-Augmented Temporal Sequence Forecasting framework (RATSF).","RATSF not only significantly enhances performance in the context of Fliggy hotel service volume forecasting but, more crucially, can be seamlessly integrated into other Transformer-based time-series forecasting models across various application scenarios.","Extensive experimentation has validated the effectiveness and generalizability of this system design across multiple diverse contexts."],"url":"http://arxiv.org/abs/2403.04180v1","category":"cs.LG"}
{"created":"2024-03-07 03:12:31","title":"Understanding the PULSAR Effect in Combined Radiotherapy and Immunotherapy through Attention Mechanisms with a Transformer Model","abstract":"PULSAR (personalized, ultra-fractionated stereotactic adaptive radiotherapy) is the adaptation of stereotactic ablative radiotherapy towards personalized cancer management. For the first time, we applied a transformer-based attention mechanism to investigate the underlying interactions between combined PULSAR and PD-L1 blockade immunotherapy based on a murine cancer model (Lewis Lung Carcinoma, LLC). The proposed approach is able to predict the trend of tumor volume change semi-quantitatively, and excels in identifying the potential causal relationships through both self-attention and cross-attention scores.","sentences":["PULSAR (personalized, ultra-fractionated stereotactic adaptive radiotherapy) is the adaptation of stereotactic ablative radiotherapy towards personalized cancer management.","For the first time, we applied a transformer-based attention mechanism to investigate the underlying interactions between combined PULSAR and PD-L1 blockade immunotherapy based on a murine cancer model (Lewis Lung Carcinoma, LLC).","The proposed approach is able to predict the trend of tumor volume change semi-quantitatively, and excels in identifying the potential causal relationships through both self-attention and cross-attention scores."],"url":"http://arxiv.org/abs/2403.04175v1","category":"physics.med-ph"}
{"created":"2024-03-07 02:54:51","title":"Super-resolution on network telemetry time series","abstract":"Fine-grained monitoring is crucial for multiple data-driven tasks such as debugging, provisioning, and securing networks. Yet, practical constraints in collecting, extracting, and storing data often force operators to use coarse-grained sampled monitoring, degrading the performance of the various tasks. In this work, we explore the feasibility of leveraging the correlations among coarse-grained time series to impute their fine-grained counterparts in software. We present Zoom2Net, a transformer-based model for network imputation that incorporates domain knowledge through operational and measurement constraints, ensuring that the imputed network telemetry time series are not only realistic but also align with existing measurements and are plausible. This approach enhances the capabilities of current monitoring infrastructures, allowing operators to gain more insights into system behaviors without the need for hardware upgrades. We evaluate Zoom2Net on four diverse datasets (e.g. cloud telemetry and Internet data transfer) and use cases (such as bursts analysis and traffic classification). We demonstrate that Zoom2Net consistently achieves high imputation accuracy with a zoom-in factor of up to 100 and performs better on downstream tasks compared to baselines by an average of 38%.","sentences":["Fine-grained monitoring is crucial for multiple data-driven tasks such as debugging, provisioning, and securing networks.","Yet, practical constraints in collecting, extracting, and storing data often force operators to use coarse-grained sampled monitoring, degrading the performance of the various tasks.","In this work, we explore the feasibility of leveraging the correlations among coarse-grained time series to impute their fine-grained counterparts in software.","We present Zoom2Net, a transformer-based model for network imputation that incorporates domain knowledge through operational and measurement constraints, ensuring that the imputed network telemetry time series are not only realistic but also align with existing measurements and are plausible.","This approach enhances the capabilities of current monitoring infrastructures, allowing operators to gain more insights into system behaviors without the need for hardware upgrades.","We evaluate Zoom2Net on four diverse datasets (e.g. cloud telemetry and Internet data transfer) and use cases (such as bursts analysis and traffic classification).","We demonstrate that Zoom2Net consistently achieves high imputation accuracy with a zoom-in factor of up to 100 and performs better on downstream tasks compared to baselines by an average of 38%."],"url":"http://arxiv.org/abs/2403.04165v1","category":"cs.NI"}
{"created":"2024-03-07 02:48:42","title":"ProMISe: Promptable Medical Image Segmentation using SAM","abstract":"With the proposal of the Segment Anything Model (SAM), fine-tuning SAM for medical image segmentation (MIS) has become popular. However, due to the large size of the SAM model and the significant domain gap between natural and medical images, fine-tuning-based strategies are costly with potential risk of instability, feature damage and catastrophic forgetting. Furthermore, some methods of transferring SAM to a domain-specific MIS through fine-tuning strategies disable the model's prompting capability, severely limiting its utilization scenarios. In this paper, we propose an Auto-Prompting Module (APM), which provides SAM-based foundation model with Euclidean adaptive prompts in the target domain. Our experiments demonstrate that such adaptive prompts significantly improve SAM's non-fine-tuned performance in MIS. In addition, we propose a novel non-invasive method called Incremental Pattern Shifting (IPS) to adapt SAM to specific medical domains. Experimental results show that the IPS enables SAM to achieve state-of-the-art or competitive performance in MIS without the need for fine-tuning. By coupling these two methods, we propose ProMISe, an end-to-end non-fine-tuned framework for Promptable Medical Image Segmentation. Our experiments demonstrate that both using our methods individually or in combination achieves satisfactory performance in low-cost pattern shifting, with all of SAM's parameters frozen.","sentences":["With the proposal of the Segment Anything Model (SAM), fine-tuning SAM for medical image segmentation (MIS) has become popular.","However, due to the large size of the SAM model and the significant domain gap between natural and medical images, fine-tuning-based strategies are costly with potential risk of instability, feature damage and catastrophic forgetting.","Furthermore, some methods of transferring SAM to a domain-specific MIS through fine-tuning strategies disable the model's prompting capability, severely limiting its utilization scenarios.","In this paper, we propose an Auto-Prompting Module (APM), which provides SAM-based foundation model with Euclidean adaptive prompts in the target domain.","Our experiments demonstrate that such adaptive prompts significantly improve SAM's non-fine-tuned performance in MIS.","In addition, we propose a novel non-invasive method called Incremental Pattern Shifting (IPS) to adapt SAM to specific medical domains.","Experimental results show that the IPS enables SAM to achieve state-of-the-art or competitive performance in MIS without the need for fine-tuning.","By coupling these two methods, we propose ProMISe, an end-to-end non-fine-tuned framework for Promptable Medical Image Segmentation.","Our experiments demonstrate that both using our methods individually or in combination achieves satisfactory performance in low-cost pattern shifting, with all of SAM's parameters frozen."],"url":"http://arxiv.org/abs/2403.04164v1","category":"cs.CV"}
{"created":"2024-03-07 02:34:54","title":"Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy","abstract":"Document retrieval has greatly benefited from the advancements of large-scale pre-trained language models (PLMs). However, their effectiveness is often limited in theme-specific applications for specialized areas or industries, due to unique terminologies, incomplete contexts of user queries, and specialized search intents. To capture the theme-specific information and improve retrieval, we propose to use a corpus topical taxonomy, which outlines the latent topic structure of the corpus while reflecting user-interested aspects. We introduce ToTER (Topical Taxonomy Enhanced Retrieval) framework, which identifies the central topics of queries and documents with the guidance of the taxonomy, and exploits their topical relatedness to supplement missing contexts. As a plug-and-play framework, ToTER can be flexibly employed to enhance various PLM-based retrievers. Through extensive quantitative, ablative, and exploratory experiments on two real-world datasets, we ascertain the benefits of using topical taxonomy for retrieval in theme-specific applications and demonstrate the effectiveness of ToTER.","sentences":["Document retrieval has greatly benefited from the advancements of large-scale pre-trained language models (PLMs).","However, their effectiveness is often limited in theme-specific applications for specialized areas or industries, due to unique terminologies, incomplete contexts of user queries, and specialized search intents.","To capture the theme-specific information and improve retrieval, we propose to use a corpus topical taxonomy, which outlines the latent topic structure of the corpus while reflecting user-interested aspects.","We introduce ToTER (Topical Taxonomy Enhanced Retrieval) framework, which identifies the central topics of queries and documents with the guidance of the taxonomy, and exploits their topical relatedness to supplement missing contexts.","As a plug-and-play framework, ToTER can be flexibly employed to enhance various PLM-based retrievers.","Through extensive quantitative, ablative, and exploratory experiments on two real-world datasets, we ascertain the benefits of using topical taxonomy for retrieval in theme-specific applications and demonstrate the effectiveness of ToTER."],"url":"http://arxiv.org/abs/2403.04160v1","category":"cs.IR"}
{"created":"2024-03-07 02:30:46","title":"DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning","abstract":"Multi-Source cross-lingual transfer learning deals with the transfer of task knowledge from multiple labelled source languages to an unlabeled target language under the language shift. Existing methods typically focus on weighting the predictions produced by language-specific classifiers of different sources that follow a shared encoder. However, all source languages share the same encoder, which is updated by all these languages. The extracted representations inevitably contain different source languages' information, which may disturb the learning of the language-specific classifiers. Additionally, due to the language gap, language-specific classifiers trained with source labels are unable to make accurate predictions for the target language. Both facts impair the model's performance. To address these challenges, we propose a Disentangled and Adaptive Network (DA-Net). Firstly, we devise a feedback-guided collaborative disentanglement method that seeks to purify input representations of classifiers, thereby mitigating mutual interference from multiple sources. Secondly, we propose a class-aware parallel adaptation method that aligns class-level distributions for each source-target language pair, thereby alleviating the language pairs' language gap. Experimental results on three different tasks involving 38 languages validate the effectiveness of our approach.","sentences":["Multi-Source cross-lingual transfer learning deals with the transfer of task knowledge from multiple labelled source languages to an unlabeled target language under the language shift.","Existing methods typically focus on weighting the predictions produced by language-specific classifiers of different sources that follow a shared encoder.","However, all source languages share the same encoder, which is updated by all these languages.","The extracted representations inevitably contain different source languages' information, which may disturb the learning of the language-specific classifiers.","Additionally, due to the language gap, language-specific classifiers trained with source labels are unable to make accurate predictions for the target language.","Both facts impair the model's performance.","To address these challenges, we propose a Disentangled and Adaptive Network (DA-Net).","Firstly, we devise a feedback-guided collaborative disentanglement method that seeks to purify input representations of classifiers, thereby mitigating mutual interference from multiple sources.","Secondly, we propose a class-aware parallel adaptation method that aligns class-level distributions for each source-target language pair, thereby alleviating the language pairs' language gap.","Experimental results on three different tasks involving 38 languages validate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.04158v1","category":"cs.CL"}
{"created":"2024-03-07 01:52:05","title":"FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning","abstract":"Federated learning (FL) is a promising approach for learning a model from data distributed on massive clients without exposing data privacy. It works effectively in the ideal federation where clients share homogeneous data distribution and learning behavior. However, FL may fail to function appropriately when the federation is not ideal, amid an unhealthy state called Negative Federated Learning (NFL), in which most clients gain no benefit from participating in FL. Many studies have tried to address NFL. However, their solutions either (1) predetermine to prevent NFL in the entire learning life-cycle or (2) tackle NFL in the aftermath of numerous learning rounds. Thus, they either (1) indiscriminately incur extra costs even if FL can perform well without such costs or (2) waste numerous learning rounds. Additionally, none of the previous work takes into account the clients who may be unwilling/unable to follow the proposed NFL solutions when using those solutions to upgrade an FL system in use. This paper introduces FL-GUARD, a holistic framework that can be employed on any FL system for tackling NFL in a run-time paradigm. That is, to dynamically detect NFL at the early stage (tens of rounds) of learning and then to activate recovery measures when necessary. Specifically, we devise a cost-effective NFL detection mechanism, which relies on an estimation of performance gain on clients. Only when NFL is detected, we activate the NFL recovery process, in which each client learns in parallel an adapted model when training the global model. Extensive experiment results confirm the effectiveness of FL-GUARD in detecting NFL and recovering from NFL to a healthy learning state. We also show that FL-GUARD is compatible with previous NFL solutions and robust against clients unwilling/unable to take any recovery measures.","sentences":["Federated learning (FL) is a promising approach for learning a model from data distributed on massive clients without exposing data privacy.","It works effectively in the ideal federation where clients share homogeneous data distribution and learning behavior.","However, FL may fail to function appropriately when the federation is not ideal, amid an unhealthy state called Negative Federated Learning (NFL), in which most clients gain no benefit from participating in FL.","Many studies have tried to address NFL.","However, their solutions either (1) predetermine to prevent NFL in the entire learning life-cycle or (2) tackle NFL in the aftermath of numerous learning rounds.","Thus, they either (1) indiscriminately incur extra costs even if FL can perform well without such costs or (2) waste numerous learning rounds.","Additionally, none of the previous work takes into account the clients who may be unwilling/unable to follow the proposed NFL solutions when using those solutions to upgrade an FL system in use.","This paper introduces FL-GUARD, a holistic framework that can be employed on any FL system for tackling NFL in a run-time paradigm.","That is, to dynamically detect NFL at the early stage (tens of rounds) of learning and then to activate recovery measures when necessary.","Specifically, we devise a cost-effective NFL detection mechanism, which relies on an estimation of performance gain on clients.","Only when NFL is detected, we activate the NFL recovery process, in which each client learns in parallel an adapted model when training the global model.","Extensive experiment results confirm the effectiveness of FL-GUARD in detecting NFL and recovering from NFL to a healthy learning state.","We also show that FL-GUARD is compatible with previous NFL solutions and robust against clients unwilling/unable to take any recovery measures."],"url":"http://arxiv.org/abs/2403.04146v1","category":"cs.LG"}
{"created":"2024-03-07 01:41:12","title":"Contrastive Augmented Graph2Graph Memory Interaction for Few Shot Continual Learning","abstract":"Few-Shot Class-Incremental Learning (FSCIL) has gained considerable attention in recent years for its pivotal role in addressing continuously arriving classes. However, it encounters additional challenges. The scarcity of samples in new sessions intensifies overfitting, causing incompatibility between the output features of new and old classes, thereby escalating catastrophic forgetting. A prevalent strategy involves mitigating catastrophic forgetting through the Explicit Memory (EM), which comprise of class prototypes. However, current EM-based methods retrieves memory globally by performing Vector-to-Vector (V2V) interaction between features corresponding to the input and prototypes stored in EM, neglecting the geometric structure of local features. This hinders the accurate modeling of their positional relationships. To incorporate information of local geometric structure, we extend the V2V interaction to Graph-to-Graph (G2G) interaction. For enhancing local structures for better G2G alignment and the prevention of local feature collapse, we propose the Local Graph Preservation (LGP) mechanism. Additionally, to address sample scarcity in classes from new sessions, the Contrast-Augmented G2G (CAG2G) is introduced to promote the aggregation of same class features thus helps few-shot learning. Extensive comparisons on CIFAR100, CUB200, and the challenging ImageNet-R dataset demonstrate the superiority of our method over existing methods.","sentences":["Few-Shot Class-Incremental Learning (FSCIL) has gained considerable attention in recent years for its pivotal role in addressing continuously arriving classes.","However, it encounters additional challenges.","The scarcity of samples in new sessions intensifies overfitting, causing incompatibility between the output features of new and old classes, thereby escalating catastrophic forgetting.","A prevalent strategy involves mitigating catastrophic forgetting through the Explicit Memory (EM), which comprise of class prototypes.","However, current EM-based methods retrieves memory globally by performing Vector-to-Vector (V2V) interaction between features corresponding to the input and prototypes stored in EM, neglecting the geometric structure of local features.","This hinders the accurate modeling of their positional relationships.","To incorporate information of local geometric structure, we extend the V2V interaction to Graph-to-Graph (G2G) interaction.","For enhancing local structures for better G2G alignment and the prevention of local feature collapse, we propose the Local Graph Preservation (LGP) mechanism.","Additionally, to address sample scarcity in classes from new sessions, the Contrast-Augmented G2G (CAG2G) is introduced to promote the aggregation of same class features thus helps few-shot learning.","Extensive comparisons on CIFAR100, CUB200, and the challenging ImageNet-R dataset demonstrate the superiority of our method over existing methods."],"url":"http://arxiv.org/abs/2403.04140v1","category":"cs.AI"}
{"created":"2024-03-07 01:29:48","title":"Unsupervised Learning of Harmonic Analysis Based on Neural HSMM with Code Quality Templates","abstract":"This paper presents a method of unsupervised learning of harmonic analysis based on a hidden semi-Markov model (HSMM). We introduce the chord quality templates, which specify the probability of pitch class emissions given a root note and a chord quality. Other probability distributions that comprise the HSMM are automatically learned via unsupervised learning, which has been a challenge in existing research. The results of the harmonic analysis of the proposed model were evaluated using existing labeled data. While our proposed method has yet to perform as well as existing models that used supervised learning and complex rule design, it has the advantage of not requiring expensive labeled data or rule elaboration. Furthermore, we also show how to recognize the tonic without prior knowledge, based on the transition probabilities of the Markov model.","sentences":["This paper presents a method of unsupervised learning of harmonic analysis based on a hidden semi-Markov model (HSMM).","We introduce the chord quality templates, which specify the probability of pitch class emissions given a root note and a chord quality.","Other probability distributions that comprise the HSMM are automatically learned via unsupervised learning, which has been a challenge in existing research.","The results of the harmonic analysis of the proposed model were evaluated using existing labeled data.","While our proposed method has yet to perform as well as existing models that used supervised learning and complex rule design, it has the advantage of not requiring expensive labeled data or rule elaboration.","Furthermore, we also show how to recognize the tonic without prior knowledge, based on the transition probabilities of the Markov model."],"url":"http://arxiv.org/abs/2403.04135v1","category":"cs.AI"}
{"created":"2024-03-07 01:22:38","title":"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference","abstract":"Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \\url{https://chat.lmsys.org}.","sentences":["Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges.","To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences.","Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing.","The platform has been operational for several months, amassing over 240K votes.","This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models.","We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters.","These analyses collectively establish a robust foundation for the credibility of Chatbot Arena.","Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies.","Our demo is publicly available at \\url{https://chat.lmsys.org}."],"url":"http://arxiv.org/abs/2403.04132v1","category":"cs.AI"}
{"created":"2024-03-07 01:08:41","title":"An Explainable AI Framework for Artificial Intelligence of Medical Things","abstract":"The healthcare industry has been revolutionized by the convergence of Artificial Intelligence of Medical Things (AIoMT), allowing advanced data-driven solutions to improve healthcare systems. With the increasing complexity of Artificial Intelligence (AI) models, the need for Explainable Artificial Intelligence (XAI) techniques become paramount, particularly in the medical domain, where transparent and interpretable decision-making becomes crucial. Therefore, in this work, we leverage a custom XAI framework, incorporating techniques such as Local Interpretable Model-Agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), and Gradient-weighted Class Activation Mapping (Grad-Cam), explicitly designed for the domain of AIoMT. The proposed framework enhances the effectiveness of strategic healthcare methods and aims to instill trust and promote understanding in AI-driven medical applications. Moreover, we utilize a majority voting technique that aggregates predictions from multiple convolutional neural networks (CNNs) and leverages their collective intelligence to make robust and accurate decisions in the healthcare system. Building upon this decision-making process, we apply the XAI framework to brain tumor detection as a use case demonstrating accurate and transparent diagnosis. Evaluation results underscore the exceptional performance of the XAI framework, achieving high precision, recall, and F1 scores with a training accuracy of 99% and a validation accuracy of 98%. Combining advanced XAI techniques with ensemble-based deep-learning (DL) methodologies allows for precise and reliable brain tumor diagnoses as an application of AIoMT.","sentences":["The healthcare industry has been revolutionized by the convergence of Artificial Intelligence of Medical Things (AIoMT), allowing advanced data-driven solutions to improve healthcare systems.","With the increasing complexity of Artificial Intelligence (AI) models, the need for Explainable Artificial Intelligence (XAI) techniques become paramount, particularly in the medical domain, where transparent and interpretable decision-making becomes crucial.","Therefore, in this work, we leverage a custom XAI framework, incorporating techniques such as Local Interpretable Model-Agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), and Gradient-weighted Class Activation Mapping (Grad-Cam), explicitly designed for the domain of AIoMT.","The proposed framework enhances the effectiveness of strategic healthcare methods and aims to instill trust and promote understanding in AI-driven medical applications.","Moreover, we utilize a majority voting technique that aggregates predictions from multiple convolutional neural networks (CNNs) and leverages their collective intelligence to make robust and accurate decisions in the healthcare system.","Building upon this decision-making process, we apply the XAI framework to brain tumor detection as a use case demonstrating accurate and transparent diagnosis.","Evaluation results underscore the exceptional performance of the XAI framework, achieving high precision, recall, and F1 scores with a training accuracy of 99% and a validation accuracy of 98%.","Combining advanced XAI techniques with ensemble-based deep-learning (DL) methodologies allows for precise and reliable brain tumor diagnoses as an application of AIoMT."],"url":"http://arxiv.org/abs/2403.04130v1","category":"cs.CV"}
{"created":"2024-03-07 00:44:11","title":"Privacy-preserving Fine-tuning of Large Language Models through Flatness","abstract":"The privacy concerns associated with the use of Large Language Models (LLMs) have grown recently with the development of LLMs such as ChatGPT. Differential Privacy (DP) techniques are explored in existing work to mitigate their privacy risks at the cost of generalization degradation. Our paper reveals that the flatness of DP-trained models' loss landscape plays an essential role in the trade-off between their privacy and generalization. We further propose a holistic framework to enforce appropriate weight flatness, which substantially improves model generalization with competitive privacy preservation. It innovates from three coarse-to-grained levels, including perturbation-aware min-max optimization on model weights within a layer, flatness-guided sparse prefix-tuning on weights across layers, and weight knowledge distillation between DP \\& non-DP weights copies. Comprehensive experiments of both black-box and white-box scenarios are conducted to demonstrate the effectiveness of our proposal in enhancing generalization and maintaining DP characteristics. For instance, on text classification dataset QNLI, DP-Flat achieves similar performance with non-private full fine-tuning but with DP guarantee under privacy budget $\\epsilon=3$, and even better performance given higher privacy budgets. Codes are provided in the supplement.","sentences":["The privacy concerns associated with the use of Large Language Models (LLMs) have grown recently with the development of LLMs such as ChatGPT.","Differential Privacy (DP) techniques are explored in existing work to mitigate their privacy risks at the cost of generalization degradation.","Our paper reveals that the flatness of DP-trained models' loss landscape plays an essential role in the trade-off between their privacy and generalization.","We further propose a holistic framework to enforce appropriate weight flatness, which substantially improves model generalization with competitive privacy preservation.","It innovates from three coarse-to-grained levels, including perturbation-aware min-max optimization on model weights within a layer, flatness-guided sparse prefix-tuning on weights across layers, and weight knowledge distillation between DP \\& non-DP weights copies.","Comprehensive experiments of both black-box and white-box scenarios are conducted to demonstrate the effectiveness of our proposal in enhancing generalization and maintaining DP characteristics.","For instance, on text classification dataset QNLI, DP-Flat achieves similar performance with non-private full fine-tuning but with DP guarantee under privacy budget $\\epsilon=3$, and even better performance given higher privacy budgets.","Codes are provided in the supplement."],"url":"http://arxiv.org/abs/2403.04124v1","category":"cs.AI"}
{"created":"2024-03-07 00:44:01","title":"Exploring LLM-based Agents for Root Cause Analysis","abstract":"The growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle. Root cause analysis (RCA), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team's specific services. Automation of RCA can result in significant savings of time, and ease the burden of incident management on on-call engineers. Recently, researchers have utilized Large Language Models (LLMs) to perform RCA, and have demonstrated promising results. However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes. In this work, we explore the use of LLM based agents for RCA to address this limitation. We present a thorough empirical evaluation of a ReAct agent equipped with retrieval tools, on an out-of-distribution dataset of production incidents collected at Microsoft. Results show that ReAct performs competitively with strong retrieval and reasoning baselines, but with highly increased factual accuracy. We then extend this evaluation by incorporating discussions associated with incident reports as additional inputs for the models, which surprisingly does not yield significant performance improvements. Lastly, we conduct a case study with a team at Microsoft to equip the ReAct agent with tools that give it access to external diagnostic services that are used by the team for manual RCA. Our results show how agents can overcome the limitations of prior work, and practical considerations for implementing such a system in practice.","sentences":["The growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle.","Root cause analysis (RCA), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team's specific services.","Automation of RCA can result in significant savings of time, and ease the burden of incident management on on-call engineers.","Recently, researchers have utilized Large Language Models (LLMs) to perform RCA, and have demonstrated promising results.","However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes.","In this work, we explore the use of LLM based agents for RCA to address this limitation.","We present a thorough empirical evaluation of a ReAct agent equipped with retrieval tools, on an out-of-distribution dataset of production incidents collected at Microsoft.","Results show that ReAct performs competitively with strong retrieval and reasoning baselines, but with highly increased factual accuracy.","We then extend this evaluation by incorporating discussions associated with incident reports as additional inputs for the models, which surprisingly does not yield significant performance improvements.","Lastly, we conduct a case study with a team at Microsoft to equip the ReAct agent with tools that give it access to external diagnostic services that are used by the team for manual RCA.","Our results show how agents can overcome the limitations of prior work, and practical considerations for implementing such a system in practice."],"url":"http://arxiv.org/abs/2403.04123v1","category":"cs.SE"}
{"created":"2024-03-07 00:36:32","title":"Can Large Language Models Reason and Plan?","abstract":"While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.","sentences":["While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs."],"url":"http://arxiv.org/abs/2403.04121v1","category":"cs.AI"}
{"created":"2024-03-07 00:09:07","title":"DNAct: Diffusion Guided Multi-Task 3D Policy Learning","abstract":"This paper presents DNAct, a language-conditioned multi-task policy framework that integrates neural rendering pre-training and diffusion training to enforce multi-modality learning in action sequence spaces. To learn a generalizable multi-task policy with few demonstrations, the pre-training phase of DNAct leverages neural rendering to distill 2D semantic features from foundation models such as Stable Diffusion to a 3D space, which provides a comprehensive semantic understanding regarding the scene. Consequently, it allows various applications to challenging robotic tasks requiring rich 3D semantics and accurate geometry. Furthermore, we introduce a novel approach utilizing diffusion training to learn a vision and language feature that encapsulates the inherent multi-modality in the multi-task demonstrations. By reconstructing the action sequences from different tasks via the diffusion process, the model is capable of distinguishing different modalities and thus improving the robustness and the generalizability of the learned representation. DNAct significantly surpasses SOTA NeRF-based multi-task manipulation approaches with over 30% improvement in success rate. Project website: dnact.github.io.","sentences":["This paper presents DNAct, a language-conditioned multi-task policy framework that integrates neural rendering pre-training and diffusion training to enforce multi-modality learning in action sequence spaces.","To learn a generalizable multi-task policy with few demonstrations, the pre-training phase of DNAct leverages neural rendering to distill 2D semantic features from foundation models such as Stable Diffusion to a 3D space, which provides a comprehensive semantic understanding regarding the scene.","Consequently, it allows various applications to challenging robotic tasks requiring rich 3D semantics and accurate geometry.","Furthermore, we introduce a novel approach utilizing diffusion training to learn a vision and language feature that encapsulates the inherent multi-modality in the multi-task demonstrations.","By reconstructing the action sequences from different tasks via the diffusion process, the model is capable of distinguishing different modalities and thus improving the robustness and the generalizability of the learned representation.","DNAct significantly surpasses SOTA NeRF-based multi-task manipulation approaches with over 30% improvement in success rate.","Project website: dnact.github.io."],"url":"http://arxiv.org/abs/2403.04115v1","category":"cs.RO"}
{"created":"2024-03-06 23:57:16","title":"ZTRAN: Prototyping Zero Trust Security xApps for Open Radio Access Network Deployments","abstract":"The open radio access network (O-RAN) offers new degrees of freedom for building and operating advanced cellular networks. Emphasizing on RAN disaggregation, open interfaces, multi-vendor support, and RAN intelligent controllers (RICs), O-RAN facilitates adaptation to new applications and technology trends. Yet, this architecture introduces new security challenges. This paper proposes leveraging zero trust principles for O-RAN security. We introduce zero trust RAN (ZTRAN), which embeds service authentication, intrusion detection, and secure slicing subsystems that are encapsulated as xApps. We implement ZTRAN on the open artificial intelligence cellular (OAIC) research platform and demonstrate its feasibility and effectiveness in terms of legitimate user throughput and latency figures. Our experimental analysis illustrates how ZTRAN's intrusion detection and secure slicing microservices operate effectively and in concert as part of O-RAN Alliance's containerized near-real time RIC. Research directions include exploring machine learning and additional threat intelligence feeds for improving the performance and extending the scope of ZTRAN.","sentences":["The open radio access network (O-RAN) offers new degrees of freedom for building and operating advanced cellular networks.","Emphasizing on RAN disaggregation, open interfaces, multi-vendor support, and RAN intelligent controllers (RICs), O-RAN facilitates adaptation to new applications and technology trends.","Yet, this architecture introduces new security challenges.","This paper proposes leveraging zero trust principles for O-RAN security.","We introduce zero trust RAN (ZTRAN), which embeds service authentication, intrusion detection, and secure slicing subsystems that are encapsulated as xApps.","We implement ZTRAN on the open artificial intelligence cellular (OAIC) research platform and demonstrate its feasibility and effectiveness in terms of legitimate user throughput and latency figures.","Our experimental analysis illustrates how ZTRAN's intrusion detection and secure slicing microservices operate effectively and in concert as part of O-RAN Alliance's containerized near-real time RIC.","Research directions include exploring machine learning and additional threat intelligence feeds for improving the performance and extending the scope of ZTRAN."],"url":"http://arxiv.org/abs/2403.04113v1","category":"cs.CR"}
{"created":"2024-03-06 23:20:34","title":"Understanding Biology in the Age of Artificial Intelligence","abstract":"Modern life sciences research is increasingly relying on artificial intelligence approaches to model biological systems, primarily centered around the use of machine learning (ML) models. Although ML is undeniably useful for identifying patterns in large, complex data sets, its widespread application in biological sciences represents a significant deviation from traditional methods of scientific inquiry. As such, the interplay between these models and scientific understanding in biology is a topic with important implications for the future of scientific research, yet it is a subject that has received little attention. Here, we draw from an epistemological toolkit to contextualize recent applications of ML in biological sciences under modern philosophical theories of understanding, identifying general principles that can guide the design and application of ML systems to model biological phenomena and advance scientific knowledge. We propose that conceptions of scientific understanding as information compression, qualitative intelligibility, and dependency relation modelling provide a useful framework for interpreting ML-mediated understanding of biological systems. Through a detailed analysis of two key application areas of ML in modern biological research - protein structure prediction and single cell RNA-sequencing - we explore how these features have thus far enabled ML systems to advance scientific understanding of their target phenomena, how they may guide the development of future ML models, and the key obstacles that remain in preventing ML from achieving its potential as a tool for biological discovery. Consideration of the epistemological features of ML applications in biology will improve the prospects of these methods to solve important problems and advance scientific understanding of living systems.","sentences":["Modern life sciences research is increasingly relying on artificial intelligence approaches to model biological systems, primarily centered around the use of machine learning (ML) models.","Although ML is undeniably useful for identifying patterns in large, complex data sets, its widespread application in biological sciences represents a significant deviation from traditional methods of scientific inquiry.","As such, the interplay between these models and scientific understanding in biology is a topic with important implications for the future of scientific research, yet it is a subject that has received little attention.","Here, we draw from an epistemological toolkit to contextualize recent applications of ML in biological sciences under modern philosophical theories of understanding, identifying general principles that can guide the design and application of ML systems to model biological phenomena and advance scientific knowledge.","We propose that conceptions of scientific understanding as information compression, qualitative intelligibility, and dependency relation modelling provide a useful framework for interpreting ML-mediated understanding of biological systems.","Through a detailed analysis of two key application areas of ML in modern biological research - protein structure prediction and single cell RNA-sequencing - we explore how these features have thus far enabled ML systems to advance scientific understanding of their target phenomena, how they may guide the development of future ML models, and the key obstacles that remain in preventing ML from achieving its potential as a tool for biological discovery.","Consideration of the epistemological features of ML applications in biology will improve the prospects of these methods to solve important problems and advance scientific understanding of living systems."],"url":"http://arxiv.org/abs/2403.04106v1","category":"cs.AI"}
{"created":"2024-03-06 23:17:16","title":"Artificial Intelligence Exploring the Patent Field","abstract":"Advanced language-processing and machine-learning techniques promise massive efficiency improvements in the previously widely manual field of patent and technical knowledge management. This field presents large-scale and complex data with very precise contents and language representation of those contents. Particularly, patent texts can differ from mundane texts in various aspects, which entails significant opportunities and challenges. This paper presents a systematic overview of patent-related tasks and popular methodologies with a special focus on evolving and promising techniques. Language processing and particularly large language models as well as the recent boost of general generative methods promise to become game changers in the patent field. The patent literature and the fact-based argumentative procedures around patents appear almost as an ideal use case. However, patents entail a number of difficulties with which existing models struggle. The paper introduces fundamental aspects of patents and patent-related data that affect technology that wants to explore or manage them. It further reviews existing methods and approaches and points out how important reliable and unbiased evaluation metrics become. Although research has made substantial progress on certain tasks, the performance across many others remains suboptimal, sometimes because of either the special nature of patents and their language or inconsistencies between legal terms and the everyday meaning of terms. Moreover, yet few methods have demonstrated the ability to produce satisfactory text for specific sections of patents. By pointing out key developments, opportunities, and gaps, we aim to encourage further research and accelerate the advancement of this field.","sentences":["Advanced language-processing and machine-learning techniques promise massive efficiency improvements in the previously widely manual field of patent and technical knowledge management.","This field presents large-scale and complex data with very precise contents and language representation of those contents.","Particularly, patent texts can differ from mundane texts in various aspects, which entails significant opportunities and challenges.","This paper presents a systematic overview of patent-related tasks and popular methodologies with a special focus on evolving and promising techniques.","Language processing and particularly large language models as well as the recent boost of general generative methods promise to become game changers in the patent field.","The patent literature and the fact-based argumentative procedures around patents appear almost as an ideal use case.","However, patents entail a number of difficulties with which existing models struggle.","The paper introduces fundamental aspects of patents and patent-related data that affect technology that wants to explore or manage them.","It further reviews existing methods and approaches and points out how important reliable and unbiased evaluation metrics become.","Although research has made substantial progress on certain tasks, the performance across many others remains suboptimal, sometimes because of either the special nature of patents and their language or inconsistencies between legal terms and the everyday meaning of terms.","Moreover, yet few methods have demonstrated the ability to produce satisfactory text for specific sections of patents.","By pointing out key developments, opportunities, and gaps, we aim to encourage further research and accelerate the advancement of this field."],"url":"http://arxiv.org/abs/2403.04105v1","category":"cs.AI"}
{"created":"2024-03-06 23:03:12","title":"Many-Objective Multi-Solution Transport","abstract":"Optimizing the performance of many objectives (instantiated by tasks or clients) jointly with a few Pareto stationary solutions (models) is critical in machine learning. However, previous multi-objective optimization methods often focus on a few number of objectives and cannot scale to many objectives that outnumber the solutions, leading to either subpar performance or ignored objectives. We introduce Many-objective multi-solution Transport (MosT), a framework that finds multiple diverse solutions in the Pareto front of many objectives. Our insight is to seek multiple solutions, each performing as a domain expert and focusing on a specific subset of objectives while collectively covering all of them. MosT formulates the problem as a bi-level optimization of weighted objectives for each solution, where the weights are defined by an optimal transport between the objectives and solutions. Our algorithm ensures convergence to Pareto stationary solutions for complementary subsets of objectives. On a range of applications in federated learning, multi-task learning, and mixture-of-prompt learning for LLMs, MosT distinctly outperforms strong baselines, delivering high-quality, diverse solutions that profile the entire Pareto frontier, thus ensuring balanced trade-offs across many objectives.","sentences":["Optimizing the performance of many objectives (instantiated by tasks or clients) jointly with a few Pareto stationary solutions (models) is critical in machine learning.","However, previous multi-objective optimization methods often focus on a few number of objectives and cannot scale to many objectives that outnumber the solutions, leading to either subpar performance or ignored objectives.","We introduce Many-objective multi-solution Transport (MosT), a framework that finds multiple diverse solutions in the Pareto front of many objectives.","Our insight is to seek multiple solutions, each performing as a domain expert and focusing on a specific subset of objectives while collectively covering all of them.","MosT formulates the problem as a bi-level optimization of weighted objectives for each solution, where the weights are defined by an optimal transport between the objectives and solutions.","Our algorithm ensures convergence to Pareto stationary solutions for complementary subsets of objectives.","On a range of applications in federated learning, multi-task learning, and mixture-of-prompt learning for LLMs, MosT distinctly outperforms strong baselines, delivering high-quality, diverse solutions that profile the entire Pareto frontier, thus ensuring balanced trade-offs across many objectives."],"url":"http://arxiv.org/abs/2403.04099v1","category":"cs.LG"}
{"created":"2024-03-06 22:32:49","title":"The Cognitive Type Project -- Mapping Typography to Cognition","abstract":"The Cognitive Type Project is focused on developing computational tools to enable the design of typefaces with varying cognitive properties. This initiative aims to empower typographers to craft fonts that enhance click-through rates for online ads, improve reading levels in children's books, enable dyslexics to create personalized type, or provide insights into customer reactions to textual content in media. A significant challenge in research related to mapping typography to cognition is the creation of thousands of typefaces with minor variations, a process that is both labor-intensive and requires the expertise of skilled typographers. Cognitive science research highlights that the design and form of letters, along with the text's overall layout, are crucial in determining the ease of reading and other cognitive properties of type such as perceived beauty and memorability. These factors affect not only the legibility and clarity of information presentation but also the likability of a typeface.","sentences":["The Cognitive Type Project is focused on developing computational tools to enable the design of typefaces with varying cognitive properties.","This initiative aims to empower typographers to craft fonts that enhance click-through rates for online ads, improve reading levels in children's books, enable dyslexics to create personalized type, or provide insights into customer reactions to textual content in media.","A significant challenge in research related to mapping typography to cognition is the creation of thousands of typefaces with minor variations, a process that is both labor-intensive and requires the expertise of skilled typographers.","Cognitive science research highlights that the design and form of letters, along with the text's overall layout, are crucial in determining the ease of reading and other cognitive properties of type such as perceived beauty and memorability.","These factors affect not only the legibility and clarity of information presentation but also the likability of a typeface."],"url":"http://arxiv.org/abs/2403.04087v1","category":"cs.AI"}
{"created":"2024-03-06 22:06:23","title":"Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection","abstract":"Semi-supervised dialogue summarization (SSDS) leverages model-generated summaries to reduce reliance on human-labeled data and improve the performance of summarization models. While addressing label noise, previous works on semi-supervised learning primarily focus on natural language understanding tasks, assuming each sample has a unique label. However, these methods are not directly applicable to SSDS, as it is a generative task, and each dialogue can be summarized in different ways. In this work, we propose a novel scoring approach, SiCF, which encapsulates three primary dimensions of summarization model quality: Semantic invariance (indicative of model confidence), Coverage (factual recall), and Faithfulness (factual precision). Using the SiCF score, we select unlabeled dialogues with high-quality generated summaries to train summarization models. Comprehensive experiments on three public datasets demonstrate the effectiveness of SiCF scores in uncertainty estimation and semi-supervised learning for dialogue summarization tasks. Our code is available at \\url{https://github.com/amazon-science/summarization-sicf-score}.","sentences":["Semi-supervised dialogue summarization (SSDS) leverages model-generated summaries to reduce reliance on human-labeled data and improve the performance of summarization models.","While addressing label noise, previous works on semi-supervised learning primarily focus on natural language understanding tasks, assuming each sample has a unique label.","However, these methods are not directly applicable to SSDS, as it is a generative task, and each dialogue can be summarized in different ways.","In this work, we propose a novel scoring approach, SiCF, which encapsulates three primary dimensions of summarization model quality: Semantic invariance (indicative of model confidence), Coverage (factual recall), and Faithfulness (factual precision).","Using the SiCF score, we select unlabeled dialogues with high-quality generated summaries to train summarization models.","Comprehensive experiments on three public datasets demonstrate the effectiveness of SiCF scores in uncertainty estimation and semi-supervised learning for dialogue summarization tasks.","Our code is available at \\url{https://github.com/amazon-science/summarization-sicf-score}."],"url":"http://arxiv.org/abs/2403.04073v1","category":"cs.CL"}
{"created":"2024-03-06 22:06:21","title":"Forecasting and Mitigating Disruptions in Public Bus Transit Services","abstract":"Public transportation systems often suffer from unexpected fluctuations in demand and disruptions, such as mechanical failures and medical emergencies. These fluctuations and disruptions lead to delays and overcrowding, which are detrimental to the passengers' experience and to the overall performance of the transit service. To proactively mitigate such events, many transit agencies station substitute (reserve) vehicles throughout their service areas, which they can dispatch to augment or replace vehicles on routes that suffer overcrowding or disruption. However, determining the optimal locations where substitute vehicles should be stationed is a challenging problem due to the inherent randomness of disruptions and due to the combinatorial nature of selecting locations across a city. In collaboration with the transit agency of Nashville, TN, we address this problem by introducing data-driven statistical and machine-learning models for forecasting disruptions and an effective randomized local-search algorithm for selecting locations where substitute vehicles are to be stationed. Our research demonstrates promising results in proactive disruption management, offering a practical and easily implementable solution for transit agencies to enhance the reliability of their services. Our results resonate beyond mere operational efficiency: by advancing proactive strategies, our approach fosters more resilient and accessible public transportation, contributing to equitable urban mobility and ultimately benefiting the communities that rely on public transportation the most.","sentences":["Public transportation systems often suffer from unexpected fluctuations in demand and disruptions, such as mechanical failures and medical emergencies.","These fluctuations and disruptions lead to delays and overcrowding, which are detrimental to the passengers' experience and to the overall performance of the transit service.","To proactively mitigate such events, many transit agencies station substitute (reserve) vehicles throughout their service areas, which they can dispatch to augment or replace vehicles on routes that suffer overcrowding or disruption.","However, determining the optimal locations where substitute vehicles should be stationed is a challenging problem due to the inherent randomness of disruptions and due to the combinatorial nature of selecting locations across a city.","In collaboration with the transit agency of Nashville, TN, we address this problem by introducing data-driven statistical and machine-learning models for forecasting disruptions and an effective randomized local-search algorithm for selecting locations where substitute vehicles are to be stationed.","Our research demonstrates promising results in proactive disruption management, offering a practical and easily implementable solution for transit agencies to enhance the reliability of their services.","Our results resonate beyond mere operational efficiency: by advancing proactive strategies, our approach fosters more resilient and accessible public transportation, contributing to equitable urban mobility and ultimately benefiting the communities that rely on public transportation the most."],"url":"http://arxiv.org/abs/2403.04072v1","category":"cs.AI"}
{"created":"2024-03-06 22:04:14","title":"On-device Self-supervised Learning of Visual Perception Tasks aboard Hardware-limited Nano-quadrotors","abstract":"Sub-\\SI{50}{\\gram} nano-drones are gaining momentum in both academia and industry. Their most compelling applications rely on onboard deep learning models for perception despite severe hardware constraints (\\ie sub-\\SI{100}{\\milli\\watt} processor). When deployed in unknown environments not represented in the training data, these models often underperform due to domain shift. To cope with this fundamental problem, we propose, for the first time, on-device learning aboard nano-drones, where the first part of the in-field mission is dedicated to self-supervised fine-tuning of a pre-trained convolutional neural network (CNN). Leveraging a real-world vision-based regression task, we thoroughly explore performance-cost trade-offs of the fine-tuning phase along three axes: \\textit{i}) dataset size (more data increases the regression performance but requires more memory and longer computation); \\textit{ii}) methodologies (\\eg fine-tuning all model parameters vs. only a subset); and \\textit{iii}) self-supervision strategy. Our approach demonstrates an improvement in mean absolute error up to 30\\% compared to the pre-trained baseline, requiring only \\SI{22}{\\second} fine-tuning on an ultra-low-power GWT GAP9 System-on-Chip. Addressing the domain shift problem via on-device learning aboard nano-drones not only marks a novel result for hardware-limited robots but lays the ground for more general advancements for the entire robotics community.","sentences":["Sub-\\SI{50}{\\gram} nano-drones are gaining momentum in both academia and industry.","Their most compelling applications rely on onboard deep learning models for perception despite severe hardware constraints (\\ie sub-\\SI{100}{\\milli\\watt} processor).","When deployed in unknown environments not represented in the training data, these models often underperform due to domain shift.","To cope with this fundamental problem, we propose, for the first time, on-device learning aboard nano-drones, where the first part of the in-field mission is dedicated to self-supervised fine-tuning of a pre-trained convolutional neural network (CNN).","Leveraging a real-world vision-based regression task, we thoroughly explore performance-cost trade-offs of the fine-tuning phase along three axes: \\textit{i}) dataset size (more data increases the regression performance but requires more memory and longer computation); \\textit{ii}) methodologies (\\eg fine-tuning all model parameters vs. only a subset); and \\textit{iii}) self-supervision strategy.","Our approach demonstrates an improvement in mean absolute error up to 30\\% compared to the pre-trained baseline, requiring only \\SI{22}{\\second} fine-tuning on an ultra-low-power GWT GAP9 System-on-Chip.","Addressing the domain shift problem via on-device learning aboard nano-drones not only marks a novel result for hardware-limited robots but lays the ground for more general advancements for the entire robotics community."],"url":"http://arxiv.org/abs/2403.04071v1","category":"cs.RO"}
{"created":"2024-03-06 21:50:52","title":"Improving Adversarial Training using Vulnerability-Aware Perturbation Budget","abstract":"Adversarial Training (AT) effectively improves the robustness of Deep Neural Networks (DNNs) to adversarial attacks. Generally, AT involves training DNN models with adversarial examples obtained within a pre-defined, fixed perturbation bound. Notably, individual natural examples from which these adversarial examples are crafted exhibit varying degrees of intrinsic vulnerabilities, and as such, crafting adversarial examples with fixed perturbation radius for all instances may not sufficiently unleash the potency of AT. Motivated by this observation, we propose two simple, computationally cheap vulnerability-aware reweighting functions for assigning perturbation bounds to adversarial examples used for AT, named Margin-Weighted Perturbation Budget (MWPB) and Standard-Deviation-Weighted Perturbation Budget (SDWPB). The proposed methods assign perturbation radii to individual adversarial samples based on the vulnerability of their corresponding natural examples. Experimental results show that the proposed methods yield genuine improvements in the robustness of AT algorithms against various adversarial attacks.","sentences":["Adversarial Training (AT) effectively improves the robustness of Deep Neural Networks (DNNs) to adversarial attacks.","Generally, AT involves training DNN models with adversarial examples obtained within a pre-defined, fixed perturbation bound.","Notably, individual natural examples from which these adversarial examples are crafted exhibit varying degrees of intrinsic vulnerabilities, and as such, crafting adversarial examples with fixed perturbation radius for all instances may not sufficiently unleash the potency of AT.","Motivated by this observation, we propose two simple, computationally cheap vulnerability-aware reweighting functions for assigning perturbation bounds to adversarial examples used for AT, named Margin-Weighted Perturbation Budget (MWPB) and Standard-Deviation-Weighted Perturbation Budget (SDWPB).","The proposed methods assign perturbation radii to individual adversarial samples based on the vulnerability of their corresponding natural examples.","Experimental results show that the proposed methods yield genuine improvements in the robustness of AT algorithms against various adversarial attacks."],"url":"http://arxiv.org/abs/2403.04070v1","category":"cs.LG"}
{"created":"2024-03-06 20:53:49","title":"Exotic Hadrons at LHCb","abstract":"It has been five years since the data sample from the LHCb detector, the first experiment optimized for heavy-flavor physics studies at a hadronic collider, was completed. These data led to many major discoveries in exotic hadron spectroscopy, which we review in this article. We supplement the experimental results with a selection of phenomenological interpretations. As the upgraded LHCb detector is expected to collect a larger data sample starting in 2024, the near- and further-future potential of the LHCb program in exotic hadron physics is also discussed.","sentences":["It has been five years since the data sample from the LHCb detector, the first experiment optimized for heavy-flavor physics studies at a hadronic collider, was completed.","These data led to many major discoveries in exotic hadron spectroscopy, which we review in this article.","We supplement the experimental results with a selection of phenomenological interpretations.","As the upgraded LHCb detector is expected to collect a larger data sample starting in 2024, the near- and further-future potential of the LHCb program in exotic hadron physics is also discussed."],"url":"http://arxiv.org/abs/2403.04051v1","category":"hep-ex"}
{"created":"2024-03-06 20:44:51","title":"Continuous Randomness via Transformations of 2-Random Sequences","abstract":"Reimann and Slaman initiated the study of sequences that are Martin-L\\\"of random with respect to a continuous measure, establishing fundamental facts about NCR, the collection of sequences that are not Martin-L\\\"of random with respect to any continuous measure. In the case of sequences that are random with respect to a computable, continuous measure, the picture is fairly well-understood: such sequences are truth-table equivalent to a Martin-L\\\"of random sequence. However, given a sequence that is random with respect to a continuous measure but not with respect to any computable measure, we can ask: how close to effective is the measure with respect to which it is continuously random?   In this study, we take up this question by examining various transformations of 2-random sequences (sequences that are Martin-L\\\"of random relative to the halting set $\\emptyset'$) to establish several results on sequences that are continuously random with respect to a measure that is computable in $\\emptyset'$. In particular, we show that (i) every noncomputable sequence that is computable from a 2-random sequence is Martin-L\\\"of random with respect to a continuous, $\\emptyset'$-computable measure and (ii) the Turing jump of every 2-random sequence is Martin-L\\\"of random with respect to a continuous, $\\emptyset'$-computable measure. From these results, we obtain examples of sequences that are not proper, i.e., not random with respect to any computable measure, but are random with respect to a continuous, $\\emptyset'$-computable measure. Lastly, we consider the behavior of 2-randomness under a wider class of effective operators (c.e. operators, pseudojump operators, and operators defined in terms of pseudojump inversion), showing that these too yield sequences that are Martin-L\\\"of random with respect to a continuous, $\\emptyset'$-computable measure.","sentences":["Reimann and Slaman initiated the study of sequences that are Martin-L\\\"of random with respect to a continuous measure, establishing fundamental facts about NCR, the collection of sequences that are not Martin-L\\\"of random with respect to any continuous measure.","In the case of sequences that are random with respect to a computable, continuous measure, the picture is fairly well-understood: such sequences are truth-table equivalent to a Martin-L\\\"of random sequence.","However, given a sequence that is random with respect to a continuous measure but not with respect to any computable measure, we can ask: how close to effective is the measure with respect to which it is continuously random?   ","In this study, we take up this question by examining various transformations of 2-random sequences (sequences that are Martin-L\\\"of random relative to the halting set $\\emptyset'$) to establish several results on sequences that are continuously random with respect to a measure that is computable in $\\emptyset'$. In particular, we show that (i) every noncomputable sequence that is computable from a 2-random sequence is Martin-L\\\"of random with respect to a continuous, $\\emptyset'$-computable measure and (ii) the Turing jump of every 2-random sequence is Martin-L\\\"of random with respect to a continuous, $\\emptyset'$-computable measure.","From these results, we obtain examples of sequences that are not proper, i.e., not random with respect to any computable measure, but are random with respect to a continuous, $\\emptyset'$-computable measure.","Lastly, we consider the behavior of 2-randomness under a wider class of effective operators (c.e. operators, pseudojump operators, and","operators defined in terms of pseudojump inversion), showing that these too yield sequences that are Martin-L\\\"of random with respect to a continuous, $\\emptyset'$-computable measure."],"url":"http://arxiv.org/abs/2403.04047v1","category":"math.LO"}
{"created":"2024-03-06 20:44:12","title":"Bridging Computational Notions of Depth","abstract":"In this article, we study the relationship between notions of depth for sequences, namely, Bennett's notions of strong and weak depth, and deep $\\Pi^0_1$ classes, introduced by the authors and motivated by previous work of Levin. For the first main result of the study, we show that every member of a $\\Pi^0_1$ class is order-deep, a property that implies strong depth. From this result, we obtain new examples of strongly deep sequences based on properties studied in computability theory and algorithmic randomness. We further show that not every strongly deep sequence is a member of a deep $\\Pi^0_1$ class. For the second main result, we show that the collection of strongly deep sequences is negligible, which is equivalent to the statement that the probability of computing a strongly deep sequence with some random oracle is 0, a property also shared by every deep $\\Pi^0_1$ class. Finally, we show that variants of strong depth, given in terms of a priori complexity and monotone complexity, are equivalent to weak depth.","sentences":["In this article, we study the relationship between notions of depth for sequences, namely, Bennett's notions of strong and weak depth, and deep $\\Pi^0_1$ classes, introduced by the authors and motivated by previous work of Levin.","For the first main result of the study, we show that every member of a $\\Pi^0_1$ class is order-deep, a property that implies strong depth.","From this result, we obtain new examples of strongly deep sequences based on properties studied in computability theory and algorithmic randomness.","We further show that not every strongly deep sequence is a member of a deep $\\Pi^0_1$ class.","For the second main result, we show that the collection of strongly deep sequences is negligible, which is equivalent to the statement that the probability of computing a strongly deep sequence with some random oracle is 0, a property also shared by every deep $\\Pi^0_1$ class.","Finally, we show that variants of strong depth, given in terms of a priori complexity and monotone complexity, are equivalent to weak depth."],"url":"http://arxiv.org/abs/2403.04045v1","category":"cs.LO"}
{"created":"2024-03-06 20:34:08","title":"OCD-FL: A Novel Communication-Efficient Peer Selection-based Decentralized Federated Learning","abstract":"The conjunction of edge intelligence and the ever-growing Internet-of-Things (IoT) network heralds a new era of collaborative machine learning, with federated learning (FL) emerging as the most prominent paradigm. With the growing interest in these learning schemes, researchers started addressing some of their most fundamental limitations. Indeed, conventional FL with a central aggregator presents a single point of failure and a network bottleneck. To bypass this issue, decentralized FL where nodes collaborate in a peer-to-peer network has been proposed. Despite the latter's efficiency, communication costs and data heterogeneity remain key challenges in decentralized FL. In this context, we propose a novel scheme, called opportunistic communication-efficient decentralized federated learning, a.k.a., OCD-FL, consisting of a systematic FL peer selection for collaboration, aiming to achieve maximum FL knowledge gain while reducing energy consumption. Experimental results demonstrate the capability of OCD-FL to achieve similar or better performances than the fully collaborative FL, while significantly reducing consumed energy by at least 30% and up to 80%.","sentences":["The conjunction of edge intelligence and the ever-growing Internet-of-Things (IoT) network heralds a new era of collaborative machine learning, with federated learning (FL) emerging as the most prominent paradigm.","With the growing interest in these learning schemes, researchers started addressing some of their most fundamental limitations.","Indeed, conventional FL with a central aggregator presents a single point of failure and a network bottleneck.","To bypass this issue, decentralized FL where nodes collaborate in a peer-to-peer network has been proposed.","Despite the latter's efficiency, communication costs and data heterogeneity remain key challenges in decentralized FL.","In this context, we propose a novel scheme, called opportunistic communication-efficient decentralized federated learning, a.k.a., OCD-FL, consisting of a systematic FL peer selection for collaboration, aiming to achieve maximum FL knowledge gain while reducing energy consumption.","Experimental results demonstrate the capability of OCD-FL to achieve similar or better performances than the fully collaborative FL, while significantly reducing consumed energy by at least 30% and up to 80%."],"url":"http://arxiv.org/abs/2403.04037v1","category":"cs.LG"}
{"created":"2024-03-06 20:33:55","title":"Unsupervised Contrastive Learning for Robust RF Device Fingerprinting Under Time-Domain Shift","abstract":"Radio Frequency (RF) device fingerprinting has been recognized as a potential technology for enabling automated wireless device identification and classification. However, it faces a key challenge due to the domain shift that could arise from variations in the channel conditions and environmental settings, potentially degrading the accuracy of RF-based device classification when testing and training data is collected in different domains. This paper introduces a novel solution that leverages contrastive learning to mitigate this domain shift problem. Contrastive learning, a state-of-the-art self-supervised learning approach from deep learning, learns a distance metric such that positive pairs are closer (i.e. more similar) in the learned metric space than negative pairs. When applied to RF fingerprinting, our model treats RF signals from the same transmission as positive pairs and those from different transmissions as negative pairs. Through experiments on wireless and wired RF datasets collected over several days, we demonstrate that our contrastive learning approach captures domain-invariant features, diminishing the effects of domain-specific variations. Our results show large and consistent improvements in accuracy (10.8\\% to 27.8\\%) over baseline models, thus underscoring the effectiveness of contrastive learning in improving device classification under domain shift.","sentences":["Radio Frequency (RF) device fingerprinting has been recognized as a potential technology for enabling automated wireless device identification and classification.","However, it faces a key challenge due to the domain shift that could arise from variations in the channel conditions and environmental settings, potentially degrading the accuracy of RF-based device classification when testing and training data is collected in different domains.","This paper introduces a novel solution that leverages contrastive learning to mitigate this domain shift problem.","Contrastive learning, a state-of-the-art self-supervised learning approach from deep learning, learns a distance metric such that positive pairs are closer (i.e. more similar) in the learned metric space than negative pairs.","When applied to RF fingerprinting, our model treats RF signals from the same transmission as positive pairs and those from different transmissions as negative pairs.","Through experiments on wireless and wired RF datasets collected over several days, we demonstrate that our contrastive learning approach captures domain-invariant features, diminishing the effects of domain-specific variations.","Our results show large and consistent improvements in accuracy (10.8\\% to 27.8\\%) over baseline models, thus underscoring the effectiveness of contrastive learning in improving device classification under domain shift."],"url":"http://arxiv.org/abs/2403.04036v1","category":"cs.LG"}
{"created":"2024-03-06 20:25:04","title":"Personalizing explanations of AI-driven hints to users cognitive abilities: an empirical evaluation","abstract":"We investigate personalizing the explanations that an Intelligent Tutoring System generates to justify the hints it provides to students to foster their learning. The personalization targets students with low levels of two traits, Need for Cognition and Conscientiousness, and aims to enhance these students' engagement with the explanations, based on prior findings that these students do not naturally engage with the explanations but they would benefit from them if they do. To evaluate the effectiveness of the personalization, we conducted a user study where we found that our proposed personalization significantly increases our target users' interaction with the hint explanations, their understanding of the hints and their learning. Hence, this work provides valuable insights into effectively personalizing AI-driven explanations for cognitively demanding tasks such as learning.","sentences":["We investigate personalizing the explanations that an Intelligent Tutoring System generates to justify the hints it provides to students to foster their learning.","The personalization targets students with low levels of two traits, Need for Cognition and Conscientiousness, and aims to enhance these students' engagement with the explanations, based on prior findings that these students do not naturally engage with the explanations but they would benefit from them if they do.","To evaluate the effectiveness of the personalization, we conducted a user study where we found that our proposed personalization significantly increases our target users' interaction with the hint explanations, their understanding of the hints and their learning.","Hence, this work provides valuable insights into effectively personalizing AI-driven explanations for cognitively demanding tasks such as learning."],"url":"http://arxiv.org/abs/2403.04035v1","category":"cs.AI"}
{"created":"2024-03-06 20:23:59","title":"Online Learning with Unknown Constraints","abstract":"We consider the problem of online learning where the sequence of actions played by the learner must adhere to an unknown safety constraint at every round. The goal is to minimize regret with respect to the best safe action in hindsight while simultaneously satisfying the safety constraint with high probability on each round. We provide a general meta-algorithm that leverages an online regression oracle to estimate the unknown safety constraint, and converts the predictions of an online learning oracle to predictions that adhere to the unknown safety constraint. On the theoretical side, our algorithm's regret can be bounded by the regret of the online regression and online learning oracles, the eluder dimension of the model class containing the unknown safety constraint, and a novel complexity measure that captures the difficulty of safe learning. We complement our result with an asymptotic lower bound that shows that the aforementioned complexity measure is necessary. When the constraints are linear, we instantiate our result to provide a concrete algorithm with $\\sqrt{T}$ regret using a scaling transformation that balances optimistic exploration with pessimistic constraint satisfaction.","sentences":["We consider the problem of online learning where the sequence of actions played by the learner must adhere to an unknown safety constraint at every round.","The goal is to minimize regret with respect to the best safe action in hindsight while simultaneously satisfying the safety constraint with high probability on each round.","We provide a general meta-algorithm that leverages an online regression oracle to estimate the unknown safety constraint, and converts the predictions of an online learning oracle to predictions that adhere to the unknown safety constraint.","On the theoretical side, our algorithm's regret can be bounded by the regret of the online regression and online learning oracles, the eluder dimension of the model class containing the unknown safety constraint, and a novel complexity measure that captures the difficulty of safe learning.","We complement our result with an asymptotic lower bound that shows that the aforementioned complexity measure is necessary.","When the constraints are linear, we instantiate our result to provide a concrete algorithm with $\\sqrt{T}$ regret using a scaling transformation that balances optimistic exploration with pessimistic constraint satisfaction."],"url":"http://arxiv.org/abs/2403.04033v1","category":"cs.LG"}
{"created":"2024-03-06 20:22:08","title":"Can Large Language Models do Analytical Reasoning?","abstract":"This paper explores the cutting-edge Large Language Model with analytical reasoning on sports. Our analytical reasoning embodies the tasks of letting large language models count how many points each team scores in a quarter in the NBA and NFL games. Our major discoveries are in two folds. Firstly, we find among all the models we employed, GPT-4 stands out in effectiveness, followed by Claude-2.1, with GPT-3.5, Gemini-Pro, and Llama-2-70b lagging behind. Specifically, we compare three different prompting techniques and a divide-and-conquer approach, we find that the latter was the most effective. Our divide-and-conquer approach breaks down play-by-play data into smaller, more manageable segments, solves each piece individually, and then aggregates them together. Besides the divide-and-conquer approach, we also explore the Chain of Thought (CoT) strategy, which markedly improves outcomes for certain models, notably GPT-4 and Claude-2.1, with their accuracy rates increasing significantly. However, the CoT strategy has negligible or even detrimental effects on the performance of other models like GPT-3.5 and Gemini-Pro. Secondly, to our surprise, we observe that most models, including GPT-4, struggle to accurately count the total scores for NBA quarters despite showing strong performance in counting NFL quarter scores. This leads us to further investigate the factors that impact the complexity of analytical reasoning tasks with extensive experiments, through which we conclude that task complexity depends on the length of context, the information density, and the presence of related information. Our research provides valuable insights into the complexity of analytical reasoning tasks and potential directions for developing future large language models.","sentences":["This paper explores the cutting-edge Large Language Model with analytical reasoning on sports.","Our analytical reasoning embodies the tasks of letting large language models count how many points each team scores in a quarter in the NBA and NFL games.","Our major discoveries are in two folds.","Firstly, we find among all the models we employed, GPT-4 stands out in effectiveness, followed by Claude-2.1, with GPT-3.5, Gemini-Pro, and Llama-2-70b lagging behind.","Specifically, we compare three different prompting techniques and a divide-and-conquer approach, we find that the latter was the most effective.","Our divide-and-conquer approach breaks down play-by-play data into smaller, more manageable segments, solves each piece individually, and then aggregates them together.","Besides the divide-and-conquer approach, we also explore the Chain of Thought (CoT) strategy, which markedly improves outcomes for certain models, notably GPT-4 and Claude-2.1, with their accuracy rates increasing significantly.","However, the CoT strategy has negligible or even detrimental effects on the performance of other models like GPT-3.5 and Gemini-Pro.","Secondly, to our surprise, we observe that most models, including GPT-4, struggle to accurately count the total scores for NBA quarters despite showing strong performance in counting NFL quarter scores.","This leads us to further investigate the factors that impact the complexity of analytical reasoning tasks with extensive experiments, through which we conclude that task complexity depends on the length of context, the information density, and the presence of related information.","Our research provides valuable insights into the complexity of analytical reasoning tasks and potential directions for developing future large language models."],"url":"http://arxiv.org/abs/2403.04031v1","category":"cs.CL"}
{"created":"2024-03-06 20:19:46","title":"RISnet: A Domain-Knowledge Driven Neural Network Architecture for RIS Optimization with Mutual Coupling and Partial CSI","abstract":"Multiple access techniques are cornerstones of wireless communications. Their performance depends on the channel properties, which can be improved by reconfigurable intelligent surfaces (RISs). In this work, we jointly optimize MA precoding at the base station (BS) and RIS configuration. We tackle difficulties of mutual coupling between RIS elements, scalability to more than 1000 RIS elements, and channel estimation. We first derive an RIS-assisted channel model considering mutual coupling, then propose an unsupervised machine learning (ML) approach to optimize the RIS. In particular, we design a dedicated neural network (NN) architecture RISnet with good scalability and desired symmetry. Moreover, we combine ML-enabled RIS configuration and analytical precoding at BS since there exist analytical precoding schemes. Furthermore, we propose another variant of RISnet, which requires the channel state information (CSI) of a small portion of RIS elements (in this work, 16 out of 1296 elements) if the channel comprises a few specular propagation paths. More generally, this work is an early contribution to combine ML technique and domain knowledge in communication for NN architecture design. Compared to generic ML, the problem-specific ML can achieve higher performance, lower complexity and symmetry.","sentences":["Multiple access techniques are cornerstones of wireless communications.","Their performance depends on the channel properties, which can be improved by reconfigurable intelligent surfaces (RISs).","In this work, we jointly optimize MA precoding at the base station (BS) and RIS configuration.","We tackle difficulties of mutual coupling between RIS elements, scalability to more than 1000 RIS elements, and channel estimation.","We first derive an RIS-assisted channel model considering mutual coupling, then propose an unsupervised machine learning (ML) approach to optimize the RIS.","In particular, we design a dedicated neural network (NN) architecture RISnet with good scalability and desired symmetry.","Moreover, we combine ML-enabled RIS configuration and analytical precoding at BS since there exist analytical precoding schemes.","Furthermore, we propose another variant of RISnet, which requires the channel state information (CSI) of a small portion of RIS elements (in this work, 16 out of 1296 elements) if the channel comprises a few specular propagation paths.","More generally, this work is an early contribution to combine ML technique and domain knowledge in communication for NN architecture design.","Compared to generic ML, the problem-specific ML can achieve higher performance, lower complexity and symmetry."],"url":"http://arxiv.org/abs/2403.04028v1","category":"cs.IT"}
{"created":"2024-03-06 20:02:51","title":"Real Time Charged Track Reconstruction for CLAS12","abstract":"This paper presents the results of charged particle track reconstruction in CLAS12 using artificial intelligence. In our approach, we use machine learning algorithms to reconstruct tracks, including their momentum and direction, with high accuracy from raw hits of the CLAS12 drift chambers. The reconstruction is performed in real-time, with the rate of data acquisition, and allows for the identification of event topologies in real-time. This approach revolutionizes the Nuclear Physics experiments' data processing, allowing us to identify and categorize the experimental data on the fly, and will lead to a significant reduction in experiment data processing. It can also be used in streaming readout applications leading to more efficient data acquisition and post-processing.","sentences":["This paper presents the results of charged particle track reconstruction in CLAS12 using artificial intelligence.","In our approach, we use machine learning algorithms to reconstruct tracks, including their momentum and direction, with high accuracy from raw hits of the CLAS12 drift chambers.","The reconstruction is performed in real-time, with the rate of data acquisition, and allows for the identification of event topologies in real-time.","This approach revolutionizes the Nuclear Physics experiments' data processing, allowing us to identify and categorize the experimental data on the fly, and will lead to a significant reduction in experiment data processing.","It can also be used in streaming readout applications leading to more efficient data acquisition and post-processing."],"url":"http://arxiv.org/abs/2403.04020v1","category":"physics.ins-det"}
{"created":"2024-03-06 19:59:17","title":"Learning Guided Automated Reasoning: A Brief Survey","abstract":"Automated theorem provers and formal proof assistants are general reasoning systems that are in theory capable of proving arbitrarily hard theorems, thus solving arbitrary problems reducible to mathematics and logical reasoning. In practice, such systems however face large combinatorial explosion, and therefore include many heuristics and choice points that considerably influence their performance. This is an opportunity for trained machine learning predictors, which can guide the work of such reasoning systems. Conversely, deductive search supported by the notion of logically valid proof allows one to train machine learning systems on large reasoning corpora. Such bodies of proof are usually correct by construction and when combined with more and more precise trained guidance they can be boostrapped into very large corpora, with increasingly long reasoning chains and possibly novel proof ideas. In this paper we provide an overview of several automated reasoning and theorem proving domains and the learning and AI methods that have been so far developed for them. These include premise selection, proof guidance in several settings, AI systems and feedback loops iterating between reasoning and learning, and symbolic classification problems.","sentences":["Automated theorem provers and formal proof assistants are general reasoning systems that are in theory capable of proving arbitrarily hard theorems, thus solving arbitrary problems reducible to mathematics and logical reasoning.","In practice, such systems however face large combinatorial explosion, and therefore include many heuristics and choice points that considerably influence their performance.","This is an opportunity for trained machine learning predictors, which can guide the work of such reasoning systems.","Conversely, deductive search supported by the notion of logically valid proof allows one to train machine learning systems on large reasoning corpora.","Such bodies of proof are usually correct by construction and when combined with more and more precise trained guidance they can be boostrapped into very large corpora, with increasingly long reasoning chains and possibly novel proof ideas.","In this paper we provide an overview of several automated reasoning and theorem proving domains and the learning and AI methods that have been so far developed for them.","These include premise selection, proof guidance in several settings, AI systems and feedback loops iterating between reasoning and learning, and symbolic classification problems."],"url":"http://arxiv.org/abs/2403.04017v1","category":"cs.AI"}
{"created":"2024-03-06 19:58:19","title":"Knockoff-Guided Feature Selection via A Single Pre-trained Reinforced Agent","abstract":"Feature selection prepares the AI-readiness of data by eliminating redundant features. Prior research falls into two primary categories: i) Supervised Feature Selection, which identifies the optimal feature subset based on their relevance to the target variable; ii) Unsupervised Feature Selection, which reduces the feature space dimensionality by capturing the essential information within the feature set instead of using target variable. However, SFS approaches suffer from time-consuming processes and limited generalizability due to the dependence on the target variable and downstream ML tasks. UFS methods are constrained by the deducted feature space is latent and untraceable. To address these challenges, we introduce an innovative framework for feature selection, which is guided by knockoff features and optimized through reinforcement learning, to identify the optimal and effective feature subset. In detail, our method involves generating \"knockoff\" features that replicate the distribution and characteristics of the original features but are independent of the target variable. Each feature is then assigned a pseudo label based on its correlation with all the knockoff features, serving as a novel metric for feature evaluation. Our approach utilizes these pseudo labels to guide the feature selection process in 3 novel ways, optimized by a single reinforced agent: 1). A deep Q-network, pre-trained with the original features and their corresponding pseudo labels, is employed to improve the efficacy of the exploration process in feature selection. 2). We introduce unsupervised rewards to evaluate the feature subset quality based on the pseudo labels and the feature space reconstruction loss to reduce dependencies on the target variable. 3). A new {\\epsilon}-greedy strategy is used, incorporating insights from the pseudo labels to make the feature selection process more effective.","sentences":["Feature selection prepares the AI-readiness of data by eliminating redundant features.","Prior research falls into two primary categories: i) Supervised Feature Selection, which identifies the optimal feature subset based on their relevance to the target variable; ii) Unsupervised Feature Selection, which reduces the feature space dimensionality by capturing the essential information within the feature set instead of using target variable.","However, SFS approaches suffer from time-consuming processes and limited generalizability due to the dependence on the target variable and downstream ML tasks.","UFS methods are constrained by the deducted feature space is latent and untraceable.","To address these challenges, we introduce an innovative framework for feature selection, which is guided by knockoff features and optimized through reinforcement learning, to identify the optimal and effective feature subset.","In detail, our method involves generating \"knockoff\" features that replicate the distribution and characteristics of the original features but are independent of the target variable.","Each feature is then assigned a pseudo label based on its correlation with all the knockoff features, serving as a novel metric for feature evaluation.","Our approach utilizes these pseudo labels to guide the feature selection process in 3 novel ways, optimized by a single reinforced agent: 1).","A deep Q-network, pre-trained with the original features and their corresponding pseudo labels, is employed to improve the efficacy of the exploration process in feature selection.","2).","We introduce unsupervised rewards to evaluate the feature subset quality based on the pseudo labels and the feature space reconstruction loss to reduce dependencies on the target variable.","3).","A new {\\epsilon}-greedy strategy is used, incorporating insights from the pseudo labels to make the feature selection process more effective."],"url":"http://arxiv.org/abs/2403.04015v1","category":"cs.LG"}
{"created":"2024-03-06 19:55:01","title":"PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement","abstract":"The recent advancements in Generative AI have significantly advanced the field of text-to-image generation. The state-of-the-art text-to-image model, Stable Diffusion, is now capable of synthesizing high-quality images with a strong sense of aesthetics. Crafting text prompts that align with the model's interpretation and the user's intent thus becomes crucial. However, prompting remains challenging for novice users due to the complexity of the stable diffusion model and the non-trivial efforts required for iteratively editing and refining the text prompts. To address these challenges, we propose PromptCharm, a mixed-initiative system that facilitates text-to-image creation through multi-modal prompt engineering and refinement. To assist novice users in prompting, PromptCharm first automatically refines and optimizes the user's initial prompt. Furthermore, PromptCharm supports the user in exploring and selecting different image styles within a large database. To assist users in effectively refining their prompts and images, PromptCharm renders model explanations by visualizing the model's attention values. If the user notices any unsatisfactory areas in the generated images, they can further refine the images through model attention adjustment or image inpainting within the rich feedback loop of PromptCharm. To evaluate the effectiveness and usability of PromptCharm, we conducted a controlled user study with 12 participants and an exploratory user study with another 12 participants. These two studies show that participants using PromptCharm were able to create images with higher quality and better aligned with the user's expectations compared with using two variants of PromptCharm that lacked interaction or visualization support.","sentences":["The recent advancements in Generative AI have significantly advanced the field of text-to-image generation.","The state-of-the-art text-to-image model, Stable Diffusion, is now capable of synthesizing high-quality images with a strong sense of aesthetics.","Crafting text prompts that align with the model's interpretation and the user's intent thus becomes crucial.","However, prompting remains challenging for novice users due to the complexity of the stable diffusion model and the non-trivial efforts required for iteratively editing and refining the text prompts.","To address these challenges, we propose PromptCharm, a mixed-initiative system that facilitates text-to-image creation through multi-modal prompt engineering and refinement.","To assist novice users in prompting, PromptCharm first automatically refines and optimizes the user's initial prompt.","Furthermore, PromptCharm supports the user in exploring and selecting different image styles within a large database.","To assist users in effectively refining their prompts and images, PromptCharm renders model explanations by visualizing the model's attention values.","If the user notices any unsatisfactory areas in the generated images, they can further refine the images through model attention adjustment or image inpainting within the rich feedback loop of PromptCharm.","To evaluate the effectiveness and usability of PromptCharm, we conducted a controlled user study with 12 participants and an exploratory user study with another 12 participants.","These two studies show that participants using PromptCharm were able to create images with higher quality and better aligned with the user's expectations compared with using two variants of PromptCharm that lacked interaction or visualization support."],"url":"http://arxiv.org/abs/2403.04014v1","category":"cs.HC"}
{"created":"2024-03-06 19:51:26","title":"Whodunit: Classifying Code as Human Authored or GPT-4 Generated -- A case study on CodeChef problems","abstract":"Artificial intelligence (AI) assistants such as GitHub Copilot and ChatGPT, built on large language models like GPT-4, are revolutionizing how programming tasks are performed, raising questions about whether code is authored by generative AI models. Such questions are of particular interest to educators, who worry that these tools enable a new form of academic dishonesty, in which students submit AI generated code as their own work. Our research explores the viability of using code stylometry and machine learning to distinguish between GPT-4 generated and human-authored code. Our dataset comprises human-authored solutions from CodeChef and AI-authored solutions generated by GPT-4. Our classifier outperforms baselines, with an F1-score and AUC-ROC score of 0.91. A variant of our classifier that excludes gameable features (e.g., empty lines, whitespace) still performs well with an F1-score and AUC-ROC score of 0.89. We also evaluated our classifier with respect to the difficulty of the programming problem and found that there was almost no difference between easier and intermediate problems, and the classifier performed only slightly worse on harder problems. Our study shows that code stylometry is a promising approach for distinguishing between GPT-4 generated code and human-authored code.","sentences":["Artificial intelligence (AI) assistants such as GitHub Copilot and ChatGPT, built on large language models like GPT-4, are revolutionizing how programming tasks are performed, raising questions about whether code is authored by generative AI models.","Such questions are of particular interest to educators, who worry that these tools enable a new form of academic dishonesty, in which students submit AI generated code as their own work.","Our research explores the viability of using code stylometry and machine learning to distinguish between GPT-4 generated and human-authored code.","Our dataset comprises human-authored solutions from CodeChef and AI-authored solutions generated by GPT-4.","Our classifier outperforms baselines, with an F1-score and AUC-ROC score of 0.91.","A variant of our classifier that excludes gameable features (e.g., empty lines, whitespace) still performs well with an F1-score and AUC-ROC score of 0.89.","We also evaluated our classifier with respect to the difficulty of the programming problem and found that there was almost no difference between easier and intermediate problems, and the classifier performed only slightly worse on harder problems.","Our study shows that code stylometry is a promising approach for distinguishing between GPT-4 generated code and human-authored code."],"url":"http://arxiv.org/abs/2403.04013v1","category":"cs.SE"}
{"created":"2024-03-06 19:17:49","title":"Bidirectional Progressive Neural Networks with Episodic Return Progress for Emergent Task Sequencing and Robotic Skill Transfer","abstract":"Human brain and behavior provide a rich venue that can inspire novel control and learning methods for robotics. In an attempt to exemplify such a development by inspiring how humans acquire knowledge and transfer skills among tasks, we introduce a novel multi-task reinforcement learning framework named Episodic Return Progress with Bidirectional Progressive Neural Networks (ERP-BPNN). The proposed ERP-BPNN model (1) learns in a human-like interleaved manner by (2) autonomous task switching based on a novel intrinsic motivation signal and, in contrast to existing methods, (3) allows bidirectional skill transfer among tasks. ERP-BPNN is a general architecture applicable to several multi-task learning settings; in this paper, we present the details of its neural architecture and show its ability to enable effective learning and skill transfer among morphologically different robots in a reaching task. The developed Bidirectional Progressive Neural Network (BPNN) architecture enables bidirectional skill transfer without requiring incremental training and seamlessly integrates with online task arbitration. The task arbitration mechanism developed is based on soft Episodic Return progress (ERP), a novel intrinsic motivation (IM) signal. To evaluate our method, we use quantifiable robotics metrics such as 'expected distance to goal' and 'path straightness' in addition to the usual reward-based measure of episodic return common in reinforcement learning. With simulation experiments, we show that ERP-BPNN achieves faster cumulative convergence and improves performance in all metrics considered among morphologically different robots compared to the baselines.","sentences":["Human brain and behavior provide a rich venue that can inspire novel control and learning methods for robotics.","In an attempt to exemplify such a development by inspiring how humans acquire knowledge and transfer skills among tasks, we introduce a novel multi-task reinforcement learning framework named Episodic Return Progress with Bidirectional Progressive Neural Networks (ERP-BPNN).","The proposed ERP-BPNN model (1) learns in a human-like interleaved manner by (2) autonomous task switching based on a novel intrinsic motivation signal and, in contrast to existing methods, (3) allows bidirectional skill transfer among tasks.","ERP-BPNN is a general architecture applicable to several multi-task learning settings; in this paper, we present the details of its neural architecture and show its ability to enable effective learning and skill transfer among morphologically different robots in a reaching task.","The developed Bidirectional Progressive Neural Network (BPNN) architecture enables bidirectional skill transfer without requiring incremental training and seamlessly integrates with online task arbitration.","The task arbitration mechanism developed is based on soft Episodic Return progress (ERP), a novel intrinsic motivation (IM) signal.","To evaluate our method, we use quantifiable robotics metrics such as 'expected distance to goal' and 'path straightness' in addition to the usual reward-based measure of episodic return common in reinforcement learning.","With simulation experiments, we show that ERP-BPNN achieves faster cumulative convergence and improves performance in all metrics considered among morphologically different robots compared to the baselines."],"url":"http://arxiv.org/abs/2403.04001v1","category":"cs.RO"}
{"created":"2024-03-06 19:13:53","title":"Guiding Enumerative Program Synthesis with Large Language Models","abstract":"Pre-trained Large Language Models (LLMs) are beginning to dominate the discourse around automatic code generation with natural language specifications. In contrast, the best-performing synthesizers in the domain of formal synthesis with precise logical specifications are still based on enumerative algorithms. In this paper, we evaluate the abilities of LLMs to solve formal synthesis benchmarks by carefully crafting a library of prompts for the domain. When one-shot synthesis fails, we propose a novel enumerative synthesis algorithm, which integrates calls to an LLM into a weighted probabilistic search. This allows the synthesizer to provide the LLM with information about the progress of the enumerator, and the LLM to provide the enumerator with syntactic guidance in an iterative loop. We evaluate our techniques on benchmarks from the Syntax-Guided Synthesis (SyGuS) competition. We find that GPT-3.5 as a stand-alone tool for formal synthesis is easily outperformed by state-of-the-art formal synthesis algorithms, but our approach integrating the LLM into an enumerative synthesis algorithm shows significant performance gains over both the LLM and the enumerative synthesizer alone and the winning SyGuS competition tool.","sentences":["Pre-trained Large Language Models (LLMs) are beginning to dominate the discourse around automatic code generation with natural language specifications.","In contrast, the best-performing synthesizers in the domain of formal synthesis with precise logical specifications are still based on enumerative algorithms.","In this paper, we evaluate the abilities of LLMs to solve formal synthesis benchmarks by carefully crafting a library of prompts for the domain.","When one-shot synthesis fails, we propose a novel enumerative synthesis algorithm, which integrates calls to an LLM into a weighted probabilistic search.","This allows the synthesizer to provide the LLM with information about the progress of the enumerator, and the LLM to provide the enumerator with syntactic guidance in an iterative loop.","We evaluate our techniques on benchmarks from the Syntax-Guided Synthesis (SyGuS) competition.","We find that GPT-3.5 as a stand-alone tool for formal synthesis is easily outperformed by state-of-the-art formal synthesis algorithms, but our approach integrating the LLM into an enumerative synthesis algorithm shows significant performance gains over both the LLM and the enumerative synthesizer alone and the winning SyGuS competition tool."],"url":"http://arxiv.org/abs/2403.03997v1","category":"cs.AI"}
{"created":"2024-03-06 19:12:41","title":"Rethinking Urban Flood Risk Assessment By Adapting Health Domain Perspective","abstract":"Inspired by ideas from health risk assessment, this paper presents a new perspective for flood risk assessment. The proposed perspective focuses on three pillars for examining flood risk: (1) inherent susceptibility, (2) mitigation strategies, and (3) external stressors. These pillars collectively encompass the physical and environmental characteristics of urban areas, the effectiveness of human-intervention measures, and the influence of uncontrollable external factors, offering a fresh point of view for decoding flood risks. For each pillar, we delineate its individual contributions to flood risk and illustrate their interactive and overall impact. The three-pillars model embodies a shift in focus from the quest to precisely model and quantify flood risk to evaluating pathways to high flood risk. The shift in perspective is intended to alleviate the quest for quantifying and predicting flood risk at fine resolutions as a panacea for enhanced flood risk management. The decomposition of flood risk pathways into the three intertwined pillars (i.e., inherent factors, mitigation factors, and external factors) enables evaluation of changes in factors within each pillar enhance and exacerbate flood risk, creating a platform from which to inform plans, decisions, and actions. Building on this foundation, we argue that a flood risk pathway analysis approach, which examines the individual and collective impacts of inherent factors, mitigation strategies, and external stressors, is essential for a nuanced evaluation of flood risk. Accordingly, the proposed perspective could complement the existing frameworks and approaches for flood risk assessment.","sentences":["Inspired by ideas from health risk assessment, this paper presents a new perspective for flood risk assessment.","The proposed perspective focuses on three pillars for examining flood risk: (1) inherent susceptibility, (2) mitigation strategies, and (3) external stressors.","These pillars collectively encompass the physical and environmental characteristics of urban areas, the effectiveness of human-intervention measures, and the influence of uncontrollable external factors, offering a fresh point of view for decoding flood risks.","For each pillar, we delineate its individual contributions to flood risk and illustrate their interactive and overall impact.","The three-pillars model embodies a shift in focus from the quest to precisely model and quantify flood risk to evaluating pathways to high flood risk.","The shift in perspective is intended to alleviate the quest for quantifying and predicting flood risk at fine resolutions as a panacea for enhanced flood risk management.","The decomposition of flood risk pathways into the three intertwined pillars (i.e., inherent factors, mitigation factors, and external factors) enables evaluation of changes in factors within each pillar enhance and exacerbate flood risk, creating a platform from which to inform plans, decisions, and actions.","Building on this foundation, we argue that a flood risk pathway analysis approach, which examines the individual and collective impacts of inherent factors, mitigation strategies, and external stressors, is essential for a nuanced evaluation of flood risk.","Accordingly, the proposed perspective could complement the existing frameworks and approaches for flood risk assessment."],"url":"http://arxiv.org/abs/2403.03996v1","category":"cs.AI"}
{"created":"2024-03-06 19:08:28","title":"Personalized Negative Reservoir for Incremental Learning in Recommender Systems","abstract":"Recommender systems have become an integral part of online platforms. Every day the volume of training data is expanding and the number of user interactions is constantly increasing. The exploration of larger and more expressive models has become a necessary pursuit to improve user experience. However, this progression carries with it an increased computational burden. In commercial settings, once a recommendation system model has been trained and deployed it typically needs to be updated frequently as new client data arrive. Cumulatively, the mounting volume of data is guaranteed to eventually make full batch retraining of the model from scratch computationally infeasible. Naively fine-tuning solely on the new data runs into the well-documented problem of catastrophic forgetting. Despite the fact that negative sampling is a crucial part of training with implicit feedback, no specialized technique exists that is tailored to the incremental learning framework. In this work, we take the first step to propose, a personalized negative reservoir strategy which is used to obtain negative samples for the standard triplet loss. This technique balances alleviation of forgetting with plasticity by encouraging the model to remember stable user preferences and selectively forget when user interests change. We derive the mathematical formulation of a negative sampler to populate and update the reservoir. We integrate our design in three SOTA and commonly used incremental recommendation models. We show that these concrete realizations of our negative reservoir framework achieve state-of-the-art results in standard benchmarks, on multiple standard top-k evaluation metrics.","sentences":["Recommender systems have become an integral part of online platforms.","Every day the volume of training data is expanding and the number of user interactions is constantly increasing.","The exploration of larger and more expressive models has become a necessary pursuit to improve user experience.","However, this progression carries with it an increased computational burden.","In commercial settings, once a recommendation system model has been trained and deployed it typically needs to be updated frequently as new client data arrive.","Cumulatively, the mounting volume of data is guaranteed to eventually make full batch retraining of the model from scratch computationally infeasible.","Naively fine-tuning solely on the new data runs into the well-documented problem of catastrophic forgetting.","Despite the fact that negative sampling is a crucial part of training with implicit feedback, no specialized technique exists that is tailored to the incremental learning framework.","In this work, we take the first step to propose, a personalized negative reservoir strategy which is used to obtain negative samples for the standard triplet loss.","This technique balances alleviation of forgetting with plasticity by encouraging the model to remember stable user preferences and selectively forget when user interests change.","We derive the mathematical formulation of a negative sampler to populate and update the reservoir.","We integrate our design in three SOTA and commonly used incremental recommendation models.","We show that these concrete realizations of our negative reservoir framework achieve state-of-the-art results in standard benchmarks, on multiple standard top-k evaluation metrics."],"url":"http://arxiv.org/abs/2403.03993v1","category":"cs.IR"}
{"created":"2024-03-06 19:00:03","title":"Collective modes in terahertz field response of superconductors with paramagnetic impurities","abstract":"We consider a problem of nonlinear response to an external electromagnetic radiation of conventional disordered superconductors which contain a small amount of weak magnetic impurities. We focus on the diffusive limit and use Usadel equation to analyze the collective excitations and obtain the dispersion relations for the collective modes. We determine the resonant frequency and dispersion of both amplitude and phase (Carlson-Goldman) modes for moderate strength of magnetic scattering. We find that the Carlson-Goldman and superconducting plasmon modes can only be excited at some finite value of the threshold momentum which increases with an increase in spin-flip scattering rate while the amplitude mode is diffusive and becomes strongly suppressed with the increase in spin-flip scattering. The value of the threshold momentum is determined by the distance between the two consecutive spin-flip scattering events. Furthermore, we also find that the superconducting plasmon mode becomes gapless in the presence of the pair breaking processes. Possible ways towards experimental verification of our results are also discussed.","sentences":["We consider a problem of nonlinear response to an external electromagnetic radiation of conventional disordered superconductors which contain a small amount of weak magnetic impurities.","We focus on the diffusive limit and use Usadel equation to analyze the collective excitations and obtain the dispersion relations for the collective modes.","We determine the resonant frequency and dispersion of both amplitude and phase (Carlson-Goldman) modes for moderate strength of magnetic scattering.","We find that the Carlson-Goldman and superconducting plasmon modes can only be excited at some finite value of the threshold momentum which increases with an increase in spin-flip scattering rate while the amplitude mode is diffusive and becomes strongly suppressed with the increase in spin-flip scattering.","The value of the threshold momentum is determined by the distance between the two consecutive spin-flip scattering events.","Furthermore, we also find that the superconducting plasmon mode becomes gapless in the presence of the pair breaking processes.","Possible ways towards experimental verification of our results are also discussed."],"url":"http://arxiv.org/abs/2403.03980v1","category":"cond-mat.supr-con"}
{"created":"2024-03-07 18:59:56","title":"Robust teleportation of a surface code and cascade of topological quantum phase transitions","abstract":"Teleportation is a facet where quantum measurements can act as a powerful resource in quantum physics, as local measurements allow to steer quantum information in a non-local way. While this has long been established for a single Bell pair, the teleportation of a fault-tolerant logical qubit presents a fundamentally different challenge as it requires the teleportation of a many-qubit state. Here we investigate a tangible protocol for teleporting a long-range entangled surface code state using elementary Bell measurements and its stability in the presence of tunable coherent errors. We relate the underlying threshold problem to the physics of anyon condensation under weak measurements and map it to a variant of the Ashkin-Teller model of statistical mechanics with Nishimori type disorder, which gives rise to a cascade of phase transitions. Tuning the angle of the local Bell measurements, we find a continuously varying threshold. Notably, the threshold moves to infinity for the $X+Z$ angle along the self-dual line -- indicating an optimal protocol that is fault-tolerant even in the presence of coherent noise. Our teleportation protocol, which can be readily implemented in dynamically configurable Rydberg atom arrays, thereby gives guidance for a practical demonstration of the power of quantum measurements.","sentences":["Teleportation is a facet where quantum measurements can act as a powerful resource in quantum physics, as local measurements allow to steer quantum information in a non-local way.","While this has long been established for a single Bell pair, the teleportation of a fault-tolerant logical qubit presents a fundamentally different challenge as it requires the teleportation of a many-qubit state.","Here we investigate a tangible protocol for teleporting a long-range entangled surface code state using elementary Bell measurements and its stability in the presence of tunable coherent errors.","We relate the underlying threshold problem to the physics of anyon condensation under weak measurements and map it to a variant of the Ashkin-Teller model of statistical mechanics with Nishimori type disorder, which gives rise to a cascade of phase transitions.","Tuning the angle of the local Bell measurements, we find a continuously varying threshold.","Notably, the threshold moves to infinity for the $X+Z$ angle along the self-dual line -- indicating an optimal protocol that is fault-tolerant even in the presence of coherent noise.","Our teleportation protocol, which can be readily implemented in dynamically configurable Rydberg atom arrays, thereby gives guidance for a practical demonstration of the power of quantum measurements."],"url":"http://arxiv.org/abs/2403.04767v1","category":"quant-ph"}
{"created":"2024-03-07 18:55:39","title":"The $\\bar\\partial$-problem on $Z(q)$-domains","abstract":"Given a complex manifold containing a relatively compact $Z(q)$ domain, we give sufficient geometric conditions on the domain so that its $L^2$-cohomology in degree $(p,q)$ (known to be finite dimensional) vanishes. The condition consists of the existence of a smooth weight function in a neighborhood of the closure of the domain, where the complex Hessian of the weight has a prescribed number of eigenvalues of a particular sign, along with good interaction at the boundary of the Levi form with the complex Hessian, encoded in a subbundle of common positive directions for the two Hermitian forms.","sentences":["Given a complex manifold containing a relatively compact $Z(q)$ domain, we give sufficient geometric conditions on the domain so that its $L^2$-cohomology in degree $(p,q)$ (known to be finite dimensional) vanishes.","The condition consists of the existence of a smooth weight function in a neighborhood of the closure of the domain, where the complex Hessian of the weight has a prescribed number of eigenvalues of a particular sign, along with good interaction at the boundary of the Levi form with the complex Hessian, encoded in a subbundle of common positive directions for the two Hermitian forms."],"url":"http://arxiv.org/abs/2403.04756v1","category":"math.CV"}
{"created":"2024-03-07 18:53:56","title":"Noise-mitigated randomized measurements and self-calibrating shadow estimation","abstract":"Randomized measurements are increasingly appreciated as powerful tools to estimate properties of quantum systems, e.g., in the characterization of hybrid classical-quantum computation. On many platforms they constitute natively accessible measurements, serving as the building block of prominent schemes like shadow estimation. In the real world, however, the implementation of the random gates at the core of these schemes is susceptible to various sources of noise and imperfections, strongly limiting the applicability of protocols. To attenuate the impact of this shortcoming, in this work we introduce an error-mitigated method of randomized measurements, giving rise to a robust shadow estimation procedure. On the practical side, we show that error mitigation and shadow estimation can be carried out using the same session of quantum experiments, hence ensuring that we can address and mitigate the noise affecting the randomization measurements. Mathematically, we develop a picture derived from Fourier-transforms to connect randomized benchmarking and shadow estimation. We prove rigorous performance guarantees and show the functioning using comprehensive numerics. More conceptually, we demonstrate that, if properly used, easily accessible data from randomized benchmarking schemes already provide such valuable diagnostic information to inform about the noise dynamics and to assist in quantum learning procedures.","sentences":["Randomized measurements are increasingly appreciated as powerful tools to estimate properties of quantum systems, e.g., in the characterization of hybrid classical-quantum computation.","On many platforms they constitute natively accessible measurements, serving as the building block of prominent schemes like shadow estimation.","In the real world, however, the implementation of the random gates at the core of these schemes is susceptible to various sources of noise and imperfections, strongly limiting the applicability of protocols.","To attenuate the impact of this shortcoming, in this work we introduce an error-mitigated method of randomized measurements, giving rise to a robust shadow estimation procedure.","On the practical side, we show that error mitigation and shadow estimation can be carried out using the same session of quantum experiments, hence ensuring that we can address and mitigate the noise affecting the randomization measurements.","Mathematically, we develop a picture derived from Fourier-transforms to connect randomized benchmarking and shadow estimation.","We prove rigorous performance guarantees and show the functioning using comprehensive numerics.","More conceptually, we demonstrate that, if properly used, easily accessible data from randomized benchmarking schemes already provide such valuable diagnostic information to inform about the noise dynamics and to assist in quantum learning procedures."],"url":"http://arxiv.org/abs/2403.04751v1","category":"quant-ph"}
{"created":"2024-03-07 18:53:07","title":"A Stabilizing NMPC Strategy for a Class of Nonholonomic Systems with Drift","abstract":"In this paper, we present a stabilizing Nonlinear Model Predictive Control (NMPC) scheme tailored for a class of nonholonomic systems with drift, where the acceleration is laterally restrained. Examples include a mobile robot with drifting wheels on a planar surface or a spacecraft maneuvering in a vacuum. The novelty lies in the formulation of the terminal set, reachable from a significant distance from the equilibrium, and the terminal cost, represented as the integration of the stage cost. The proposed approach establishes essential steps for ensuring stability and feasibility guarantees. Simulation results substantiate the viability and effectiveness of the NMPC scheme.","sentences":["In this paper, we present a stabilizing Nonlinear Model Predictive Control (NMPC) scheme tailored for a class of nonholonomic systems with drift, where the acceleration is laterally restrained.","Examples include a mobile robot with drifting wheels on a planar surface or a spacecraft maneuvering in a vacuum.","The novelty lies in the formulation of the terminal set, reachable from a significant distance from the equilibrium, and the terminal cost, represented as the integration of the stage cost.","The proposed approach establishes essential steps for ensuring stability and feasibility guarantees.","Simulation results substantiate the viability and effectiveness of the NMPC scheme."],"url":"http://arxiv.org/abs/2403.04748v1","category":"math.OC"}
{"created":"2024-03-07 18:40:43","title":"Control Theorems for Hilbert Modular Varieties","abstract":"We prove an exact control theorem, in the sense of Hida theory, for the ordinary part of the middle degree \\'etale cohomology of certain Hilbert modular varieties, after localizing at a suitable maximal ideal of the Hecke algebra. Our method of proof builds upon the techniques introduced by Loeffler-Rockwood-Zerbes; another important ingredient in our proof is the recent work of Caraiani-Tamiozzo on the vanishing of the \\'etale cohomology of Hilbert modular varieties with torsion coefficients outside the middle degree. This work will be used in forthcoming work of the author to show that the Asai-Flach Euler system corresponding to a quadratic Hilbert modular form varies in Hida families.","sentences":["We prove an exact control theorem, in the sense of Hida theory, for the ordinary part of the middle degree \\'etale cohomology of certain Hilbert modular varieties, after localizing at a suitable maximal ideal of the Hecke algebra.","Our method of proof builds upon the techniques introduced by Loeffler-Rockwood-Zerbes; another important ingredient in our proof is the recent work of Caraiani-Tamiozzo on the vanishing of the \\'etale cohomology of Hilbert modular varieties with torsion coefficients outside the middle degree.","This work will be used in forthcoming work of the author to show that the Asai-Flach Euler system corresponding to a quadratic Hilbert modular form varies in Hida families."],"url":"http://arxiv.org/abs/2403.04738v1","category":"math.NT"}
{"created":"2024-03-07 18:38:49","title":"Symmetry-aware spectral bounds for the electronic structure Hamiltonian","abstract":"We present symmetry-aware spectral bounds to assess the query complexity of Hamiltonian oracles in quantum algorithms. Our numerical estimates indicate that these bounds are smaller than traditional ones for a variety of electronic structure systems, including exhibiting unique scaling behavior in thermodynamic and complete basis set limits. Our work highlights potential room for improvement in reducing the $\\ell_1$ norm through tensor factorization and block-encoding methods, while also offering a critical evaluation of query complexity limits for quantum algorithms in physics and chemistry.","sentences":["We present symmetry-aware spectral bounds to assess the query complexity of Hamiltonian oracles in quantum algorithms.","Our numerical estimates indicate that these bounds are smaller than traditional ones for a variety of electronic structure systems, including exhibiting unique scaling behavior in thermodynamic and complete basis set limits.","Our work highlights potential room for improvement in reducing the $\\ell_1$ norm through tensor factorization and block-encoding methods, while also offering a critical evaluation of query complexity limits for quantum algorithms in physics and chemistry."],"url":"http://arxiv.org/abs/2403.04737v1","category":"quant-ph"}
{"created":"2024-03-07 18:38:47","title":"Benchmarking News Recommendation in the Era of Green AI","abstract":"Over recent years, news recommender systems have gained significant attention in both academia and industry, emphasizing the need for a standardized benchmark to evaluate and compare the performance of these systems. Concurrently, Green AI advocates for reducing the energy consumption and environmental impact of machine learning. To address these concerns, we introduce the first Green AI benchmarking framework for news recommendation, known as GreenRec, and propose a metric for assessing the tradeoff between recommendation accuracy and efficiency. Our benchmark encompasses 30 base models and their variants, covering traditional end-to-end training paradigms as well as our proposed efficient only-encode-once (OLEO) paradigm. Through experiments consuming 2000 GPU hours, we observe that the OLEO paradigm achieves competitive accuracy compared to state-of-the-art end-to-end paradigms and delivers up to a 2992\\% improvement in sustainability metrics.","sentences":["Over recent years, news recommender systems have gained significant attention in both academia and industry, emphasizing the need for a standardized benchmark to evaluate and compare the performance of these systems.","Concurrently, Green AI advocates for reducing the energy consumption and environmental impact of machine learning.","To address these concerns, we introduce the first Green AI benchmarking framework for news recommendation, known as GreenRec, and propose a metric for assessing the tradeoff between recommendation accuracy and efficiency.","Our benchmark encompasses 30 base models and their variants, covering traditional end-to-end training paradigms as well as our proposed efficient only-encode-once (OLEO) paradigm.","Through experiments consuming 2000 GPU hours, we observe that the OLEO paradigm achieves competitive accuracy compared to state-of-the-art end-to-end paradigms and delivers up to a 2992\\% improvement in sustainability metrics."],"url":"http://arxiv.org/abs/2403.04736v1","category":"cs.IR"}
{"created":"2024-03-07 18:36:21","title":"Enumerating stably trivial vector bundles with higher real $K$-theory","abstract":"Given positive integers $r$ and $c$, let $\\phi(r,c)$ denote the number of isomorphism classes of complex rank $r$ topological vector bundles on $\\mathbb{CP}^{r+c}$ that are stably trivial. We compute the $p$-adic valuation of the number $\\phi(r,c)$ for all pairs $r$ and $c$ such that $c \\leq \\operatorname{min}\\{r,2p-3\\}$. We also give some systematic lower bounds for $p$-divisibility of $\\phi(r,c)$ when $c<2p^2-p-2$, and detect some nontrivial $p$-divisibility for larger $c$. As an additional application of our methods, we find new $p$-torsion in unstable homotopy groups of unitary groups.","sentences":["Given positive integers $r$ and $c$, let $\\phi(r,c)$ denote the number of isomorphism classes of complex rank $r$ topological vector bundles on $\\mathbb{CP}^{r+c}$ that are stably trivial.","We compute the $p$-adic valuation of the number $\\phi(r,c)$ for all pairs $r$ and $c$ such that","$c \\leq \\operatorname{min}\\{r,2p-3\\}$. We also give some systematic lower bounds for $p$-divisibility of $\\phi(r,c)$ when $c<2p^2-p-2$, and detect some nontrivial $p$-divisibility for larger $c$. As an additional application of our methods, we find new $p$-torsion in unstable homotopy groups of unitary groups."],"url":"http://arxiv.org/abs/2403.04733v1","category":"math.AT"}
{"created":"2024-03-07 18:22:03","title":"Masked Capsule Autoencoders","abstract":"We propose Masked Capsule Autoencoders (MCAE), the first Capsule Network that utilises pretraining in a self-supervised manner. Capsule Networks have emerged as a powerful alternative to Convolutional Neural Networks (CNNs), and have shown favourable properties when compared to Vision Transformers (ViT), but have struggled to effectively learn when presented with more complex data, leading to Capsule Network models that do not scale to modern tasks. Our proposed MCAE model alleviates this issue by reformulating the Capsule Network to use masked image modelling as a pretraining stage before finetuning in a supervised manner. Across several experiments and ablations studies we demonstrate that similarly to CNNs and ViTs, Capsule Networks can also benefit from self-supervised pretraining, paving the way for further advancements in this neural network domain. For instance, pretraining on the Imagenette dataset, a dataset of 10 classes of Imagenet-sized images, we achieve not only state-of-the-art results for Capsule Networks but also a 9% improvement compared to purely supervised training. Thus we propose that Capsule Networks benefit from and should be trained within a masked image modelling framework, with a novel capsule decoder, to improve a Capsule Network's performance on realistic-sized images.","sentences":["We propose Masked Capsule Autoencoders (MCAE), the first Capsule Network that utilises pretraining in a self-supervised manner.","Capsule Networks have emerged as a powerful alternative to Convolutional Neural Networks (CNNs), and have shown favourable properties when compared to Vision Transformers (ViT), but have struggled to effectively learn when presented with more complex data, leading to Capsule Network models that do not scale to modern tasks.","Our proposed MCAE model alleviates this issue by reformulating the Capsule Network to use masked image modelling as a pretraining stage before finetuning in a supervised manner.","Across several experiments and ablations studies we demonstrate that similarly to CNNs and ViTs, Capsule Networks can also benefit from self-supervised pretraining, paving the way for further advancements in this neural network domain.","For instance, pretraining on the Imagenette dataset, a dataset of 10 classes of Imagenet-sized images, we achieve not only state-of-the-art results for Capsule Networks but also a 9% improvement compared to purely supervised training.","Thus we propose that Capsule Networks benefit from and should be trained within a masked image modelling framework, with a novel capsule decoder, to improve a Capsule Network's performance on realistic-sized images."],"url":"http://arxiv.org/abs/2403.04724v1","category":"cs.CV"}
{"created":"2024-03-07 18:20:24","title":"Testing an entropy estimator related to the dynamical state of galaxy clusters","abstract":"We propose the entropy estimator $H_Z$, calculated from global dynamical parameters, in an attempt to capture the degree of evolution of galaxy systems. We assume that the observed (spatial and velocity) distributions of member galaxies in these systems evolve over time towards states of higher dynamical relaxation (higher entropy), becoming more random and homogeneous in virial equilibrium. Thus, the $H_Z$-entropy should correspond to the gravitacional assembly state of the systems. This was tested in a sample of 70 well sampled clusters in the Local Universe whose gravitational assembly state, classified from optical and X-ray analysis of substructures, shows clear statistical correlation with $H_Z$. This estimator was also tested on a sample of clusters (halos) from the IllustrisTNG simulations, obtaining results in agreement with the observational ones.","sentences":["We propose the entropy estimator $H_Z$, calculated from global dynamical parameters, in an attempt to capture the degree of evolution of galaxy systems.","We assume that the observed (spatial and velocity) distributions of member galaxies in these systems evolve over time towards states of higher dynamical relaxation (higher entropy), becoming more random and homogeneous in virial equilibrium.","Thus, the $H_Z$-entropy should correspond to the gravitacional assembly state of the systems.","This was tested in a sample of 70 well sampled clusters in the Local Universe whose gravitational assembly state, classified from optical and X-ray analysis of substructures, shows clear statistical correlation with $H_Z$. This estimator was also tested on a sample of clusters (halos) from the IllustrisTNG simulations, obtaining results in agreement with the observational ones."],"url":"http://arxiv.org/abs/2403.04723v1","category":"astro-ph.CO"}
{"created":"2024-03-07 18:15:07","title":"On the controllability of nonlinear systems with a periodic drift","abstract":"Sufficient and necessary conditions are established for controllability of affine control systems, motivated by these with a drift all of whose solutions are periodic. In contrast with previously known results, these conditions encompass the case of a control set whose convex hull contains the origin but is not a neighborhood of the origin. The condition is expressed by means of pushforwards along the flow of the drift, rather than in terms of Lie brackets. It turns out that this also amounts to local controllability of a time-varying linear approximation with constrained controls. Global and local results are given, as well as a few illustrative examples.","sentences":["Sufficient and necessary conditions are established for controllability of affine control systems, motivated by these with a drift all of whose solutions are periodic.","In contrast with previously known results, these conditions encompass the case of a control set whose convex hull contains the origin but is not a neighborhood of the origin.","The condition is expressed by means of pushforwards along the flow of the drift, rather than in terms of Lie brackets.","It turns out that this also amounts to local controllability of a time-varying linear approximation with constrained controls.","Global and local results are given, as well as a few illustrative examples."],"url":"http://arxiv.org/abs/2403.04718v1","category":"math.OC"}
{"created":"2024-03-07 18:14:52","title":"Literature Review of Current Sustainability Assessment Frameworks and Approaches for Organizations","abstract":"This systematic literature review explores sustainability assessment frameworks (SAFs) across diverse industries. The review focuses on SAF design approaches including the methods used for Sustainability Indicator (SI) selection, relative importance assessment, and interdependency analysis. Various methods, including literature reviews, stakeholder interviews, questionnaires, Pareto analysis, SMART approach, and adherence to sustainability standards, contribute to the complex SI selection process. Fuzzy-AHP stands out as a robust technique for assessing relative SI importance. While dynamic sustainability and performance indices are essential, methods like DEMATEL, VIKOR, correlation analysis, and causal models for interdependency assessment exhibit static limitations. The review presents strengths and limitations of SAFs, addressing gaps in design approaches and contributing to a comprehensive understanding. The insights of this review aim to benefit policymakers, administrators, leaders, and researchers, fostering sustainability practices. Future research recommendations include exploring multi-criteria decision-making models and hybrid approaches, extending sustainability evaluation across organizational levels and supply chains. Emphasizing adaptability to industry specifics and dynamic global adjustments is proposed for holistic sustainability practices, further enhancing organizational sustainability.","sentences":["This systematic literature review explores sustainability assessment frameworks (SAFs) across diverse industries.","The review focuses on SAF design approaches including the methods used for Sustainability Indicator (SI) selection, relative importance assessment, and interdependency analysis.","Various methods, including literature reviews, stakeholder interviews, questionnaires, Pareto analysis, SMART approach, and adherence to sustainability standards, contribute to the complex SI selection process.","Fuzzy-AHP stands out as a robust technique for assessing relative SI importance.","While dynamic sustainability and performance indices are essential, methods like DEMATEL, VIKOR, correlation analysis, and causal models for interdependency assessment exhibit static limitations.","The review presents strengths and limitations of SAFs, addressing gaps in design approaches and contributing to a comprehensive understanding.","The insights of this review aim to benefit policymakers, administrators, leaders, and researchers, fostering sustainability practices.","Future research recommendations include exploring multi-criteria decision-making models and hybrid approaches, extending sustainability evaluation across organizational levels and supply chains.","Emphasizing adaptability to industry specifics and dynamic global adjustments is proposed for holistic sustainability practices, further enhancing organizational sustainability."],"url":"http://arxiv.org/abs/2403.04717v1","category":"cs.CY"}
{"created":"2024-03-07 18:09:59","title":"Parendi: Thousand-Way Parallel RTL Simulation","abstract":"Hardware development relies on simulations, particularly cycle-accurate RTL (Register Transfer Level) simulations, which consume significant time. As single-processor performance grows only slowly, conventional, single-threaded RTL simulation is becoming less practical for increasingly complex chips and systems. A solution is parallel RTL simulation, where ideally, simulators could run on thousands of parallel cores. However, existing simulators can only exploit tens of cores.   This paper studies the challenges inherent in running parallel RTL simulation on a multi-thousand-core machine (the Graphcore IPU, a 1472-core machine). Simulation performance requires balancing three factors: synchronization, communication, and computation. We experimentally evaluate each metric and analyze how it affects parallel simulation speed, drawing on contrasts between the large-scale IPU and smaller but faster x86 systems.   Using this analysis, we build Parendi, an RTL simulator for the IPU. It distributes RTL simulation across 5888 cores on 4 IPU sockets. Parendi runs large RTL designs up to 4x faster than a powerful, state-of-the-art x86 multicore system.","sentences":["Hardware development relies on simulations, particularly cycle-accurate RTL (Register Transfer Level) simulations, which consume significant time.","As single-processor performance grows only slowly, conventional, single-threaded RTL simulation is becoming less practical for increasingly complex chips and systems.","A solution is parallel RTL simulation, where ideally, simulators could run on thousands of parallel cores.","However, existing simulators can only exploit tens of cores.   ","This paper studies the challenges inherent in running parallel RTL simulation on a multi-thousand-core machine (the Graphcore IPU, a 1472-core machine).","Simulation performance requires balancing three factors: synchronization, communication, and computation.","We experimentally evaluate each metric and analyze how it affects parallel simulation speed, drawing on contrasts between the large-scale IPU and smaller but faster x86 systems.   ","Using this analysis, we build Parendi, an RTL simulator for the IPU.","It distributes RTL simulation across 5888 cores on 4 IPU sockets.","Parendi runs large RTL designs up to 4x faster than a powerful, state-of-the-art x86 multicore system."],"url":"http://arxiv.org/abs/2403.04714v1","category":"cs.DC"}
{"created":"2024-03-07 18:04:24","title":"Tuning transduction from hidden observables to optimize information harvesting","abstract":"Biological and living organisms sense and process information from their surroundings, typically having access only to a subset of external observables for a limited amount of time. In this work, we uncover how biological systems can exploit these accessible degrees of freedom (DOFs) to transduce information from the inaccessible ones with a limited energy budget. We find that optimal transduction strategies may boost information harvesting over the ideal case in which all DOFs are known, even when only finite-time trajectories are observed, at the price of higher dissipation. We apply our results to red blood cells, inferring the implemented transduction strategy from membrane flickering data and shedding light on the connection between mechanical stress and transduction efficiency. Our framework offers novel insights into the adaptive strategies of biological systems under non-equilibrium conditions.","sentences":["Biological and living organisms sense and process information from their surroundings, typically having access only to a subset of external observables for a limited amount of time.","In this work, we uncover how biological systems can exploit these accessible degrees of freedom (DOFs) to transduce information from the inaccessible ones with a limited energy budget.","We find that optimal transduction strategies may boost information harvesting over the ideal case in which all DOFs are known, even when only finite-time trajectories are observed, at the price of higher dissipation.","We apply our results to red blood cells, inferring the implemented transduction strategy from membrane flickering data and shedding light on the connection between mechanical stress and transduction efficiency.","Our framework offers novel insights into the adaptive strategies of biological systems under non-equilibrium conditions."],"url":"http://arxiv.org/abs/2403.04709v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-07 17:59:58","title":"Observation of Nonlinear Response and Onsager Regression in a Photon Bose-Einstein Condensate","abstract":"The quantum regression theorem states that the correlations of a system at two different times are governed by the same equations of motion as the temporal response of the average values. Such a relation provides a powerful framework for the investigation of physical systems by establishing a formal connection between intrinsic microscopic behaviour and a macroscopic 'effect' due to an external 'cause'. Measuring the response to a controlled perturbation in this way allows to determine, for example, structure factors in condensed matter systems as well as other correlation functions of material systems. Here we experimentally demonstrate that the two-time particle number correlations in a photon Bose-Einstein condensate inside a dye-filled microcavity exhibit the same dynamics as the response of the condensate to a sudden perturbation of the dye molecule bath. This confirms the regression theorem for a quantum gas and, moreover, establishes a test of this relation in an unconventional form where the perturbation acts on the bath and only the condensate response is monitored. For strong perturbations, we observe nonlinear relaxation dynamics which our microscopic theory relates to the equilibrium fluctuations, thereby extending the regression theorem beyond the regime of linear response. The demonstrated nonlinearity of the condensate-bath system paves the way for studies of novel elementary excitations in lattices of driven-dissipative photon condensates.","sentences":["The quantum regression theorem states that the correlations of a system at two different times are governed by the same equations of motion as the temporal response of the average values.","Such a relation provides a powerful framework for the investigation of physical systems by establishing a formal connection between intrinsic microscopic behaviour and a macroscopic 'effect' due to an external 'cause'.","Measuring the response to a controlled perturbation in this way allows to determine, for example, structure factors in condensed matter systems as well as other correlation functions of material systems.","Here we experimentally demonstrate that the two-time particle number correlations in a photon Bose-Einstein condensate inside a dye-filled microcavity exhibit the same dynamics as the response of the condensate to a sudden perturbation of the dye molecule bath.","This confirms the regression theorem for a quantum gas and, moreover, establishes a test of this relation in an unconventional form where the perturbation acts on the bath and only the condensate response is monitored.","For strong perturbations, we observe nonlinear relaxation dynamics which our microscopic theory relates to the equilibrium fluctuations, thereby extending the regression theorem beyond the regime of linear response.","The demonstrated nonlinearity of the condensate-bath system paves the way for studies of novel elementary excitations in lattices of driven-dissipative photon condensates."],"url":"http://arxiv.org/abs/2403.04705v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-07 17:59:11","title":"Quantum Advantage in Reversing Unknown Unitary Evolutions","abstract":"We introduce the Quantum Unitary Reversal Algorithm (QURA), a deterministic and exact approach to universally reverse arbitrary unknown unitary transformations using $\\mathcal{O}(d^2)$ calls of the unitary, where $d$ is the system dimension. Our construction resolves a fundamental problem of time-reversal simulations for closed quantum systems by affirming the feasibility of reversing any unitary evolution without knowing the exact process. The algorithm also provides the construction of a key oracle for unitary inversion in quantum algorithm frameworks such as quantum singular value transformation. Notably, our work demonstrates that compared with classical methods relying on process tomography, reversing an unknown unitary on a quantum computer holds a quadratic quantum advantage in computation complexity. QURA ensures an exact unitary inversion while the classical counterpart can never achieve exact inversion using a finite number of unitary calls.","sentences":["We introduce the Quantum Unitary Reversal Algorithm (QURA), a deterministic and exact approach to universally reverse arbitrary unknown unitary transformations using $\\mathcal{O}(d^2)$ calls of the unitary, where $d$ is the system dimension.","Our construction resolves a fundamental problem of time-reversal simulations for closed quantum systems by affirming the feasibility of reversing any unitary evolution without knowing the exact process.","The algorithm also provides the construction of a key oracle for unitary inversion in quantum algorithm frameworks such as quantum singular value transformation.","Notably, our work demonstrates that compared with classical methods relying on process tomography, reversing an unknown unitary on a quantum computer holds a quadratic quantum advantage in computation complexity.","QURA ensures an exact unitary inversion while the classical counterpart can never achieve exact inversion using a finite number of unitary calls."],"url":"http://arxiv.org/abs/2403.04704v1","category":"quant-ph"}
{"created":"2024-03-07 17:48:47","title":"Delving into the Trajectory Long-tail Distribution for Muti-object Tracking","abstract":"Multiple Object Tracking (MOT) is a critical area within computer vision, with a broad spectrum of practical implementations. Current research has primarily focused on the development of tracking algorithms and enhancement of post-processing techniques. Yet, there has been a lack of thorough examination concerning the nature of tracking data it self. In this study, we pioneer an exploration into the distribution patterns of tracking data and identify a pronounced long-tail distribution issue within existing MOT datasets. We note a significant imbalance in the distribution of trajectory lengths across different pedestrians, a phenomenon we refer to as \"pedestrians trajectory long-tail distribution\". Addressing this challenge, we introduce a bespoke strategy designed to mitigate the effects of this skewed distribution. Specifically, we propose two data augmentation strategies, including Stationary Camera View Data Augmentation (SVA) and Dynamic Camera View Data Augmentation (DVA) , designed for viewpoint states and the Group Softmax (GS) module for Re-ID. SVA is to backtrack and predict the pedestrian trajectory of tail classes, and DVA is to use diffusion model to change the background of the scene. GS divides the pedestrians into unrelated groups and performs softmax operation on each group individually. Our proposed strategies can be integrated into numerous existing tracking systems, and extensive experimentation validates the efficacy of our method in reducing the influence of long-tail distribution on multi-object tracking performance. The code is available at https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT.","sentences":["Multiple Object Tracking (MOT) is a critical area within computer vision, with a broad spectrum of practical implementations.","Current research has primarily focused on the development of tracking algorithms and enhancement of post-processing techniques.","Yet, there has been a lack of thorough examination concerning the nature of tracking data it self.","In this study, we pioneer an exploration into the distribution patterns of tracking data and identify a pronounced long-tail distribution issue within existing MOT datasets.","We note a significant imbalance in the distribution of trajectory lengths across different pedestrians, a phenomenon we refer to as \"pedestrians trajectory long-tail distribution\".","Addressing this challenge, we introduce a bespoke strategy designed to mitigate the effects of this skewed distribution.","Specifically, we propose two data augmentation strategies, including Stationary Camera View Data Augmentation (SVA) and Dynamic Camera View Data Augmentation (DVA) , designed for viewpoint states and the Group Softmax (GS) module for Re-ID.","SVA is to backtrack and predict the pedestrian trajectory of tail classes, and DVA is to use diffusion model to change the background of the scene.","GS divides the pedestrians into unrelated groups and performs softmax operation on each group individually.","Our proposed strategies can be integrated into numerous existing tracking systems, and extensive experimentation validates the efficacy of our method in reducing the influence of long-tail distribution on multi-object tracking performance.","The code is available at https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT."],"url":"http://arxiv.org/abs/2403.04700v1","category":"cs.CV"}
{"created":"2024-03-07 17:47:03","title":"Dynamics of ergotropy and environment-induced work","abstract":"We investigate the dynamics of ergotropy in open systems under Markovian and non-Markovian evolutions. In this scenario, we begin by formulating the ergotropy of an arbitrary qubit state in terms of energy and coherence. Thus, we determine the conditions for ergotropy freezing and ergotropy sudden death as a consequence of the system-bath interaction. In order to use ergotropy as a resource for energy extraction in the form of work in an open-system scenario, we adopt the entropy-based formulation of quantum thermodynamics. In this approach, the work gains an additional environment-induced component, which may be present even for constant Hamiltonians. We then establish an analytical relationship between the environment-induced work and ergotropy, providing an interpretation of environment-induced work in terms of variation of ergotropy. In particular, energy transfer by environment-induced work can be performed up to a limit, which is governed by the energy cost to transit between the initial and final passive states of the quantum dynamics. We illustrate these results for qubit states evolving under non-dissipative and dissipative quantum processes.","sentences":["We investigate the dynamics of ergotropy in open systems under Markovian and non-Markovian evolutions.","In this scenario, we begin by formulating the ergotropy of an arbitrary qubit state in terms of energy and coherence.","Thus, we determine the conditions for ergotropy freezing and ergotropy sudden death as a consequence of the system-bath interaction.","In order to use ergotropy as a resource for energy extraction in the form of work in an open-system scenario, we adopt the entropy-based formulation of quantum thermodynamics.","In this approach, the work gains an additional environment-induced component, which may be present even for constant Hamiltonians.","We then establish an analytical relationship between the environment-induced work and ergotropy, providing an interpretation of environment-induced work in terms of variation of ergotropy.","In particular, energy transfer by environment-induced work can be performed up to a limit, which is governed by the energy cost to transit between the initial and final passive states of the quantum dynamics.","We illustrate these results for qubit states evolving under non-dissipative and dissipative quantum processes."],"url":"http://arxiv.org/abs/2403.04698v1","category":"quant-ph"}
{"created":"2024-03-07 17:43:21","title":"On $[1,2]$-Domination in Interval and Circle Graphs","abstract":"A subset $S$ of vertices in a graph $G=(V, E)$ is Dominating Set if each vertex in $V(G)\\setminus S$ is adjacent to at least one vertex in $S$. Chellali et al. in 2013, by restricting the number of neighbors in $S$ of a vertex outside $S$, introduced the concept of $[1,j]$-dominating set. A set $D \\subseteq V$ of a graph $G = (V, E)$ is called $[1,j]$-Dominating Set of $G$ if every vertex not in $D$ has at least one neighbor and at most $j$ neighbors in $D$. The Minimum $[1,j]$-Domination problem is the problem of finding the minimum set $D$. Given a positive integer $k$ and a graph $G = (V, E)$, the $[1,j]$-Domination Decision problem is to decide whether $G$ has $[1,j]$-dominating set of cardinality at most $k$. A polynomial-time algorithm was obtained in split graphs for a constant $j$ in contrast to the classic Dominating Set problem which is NP-hard in split graphs. This result motivates us to investigate the effect of restriction $j$ on the complexity of $[1,j]$-domination problem on various classes of graphs. Although for $j\\geq 3$, it has been proved that the minimum of classical domination is equal to minimum $[1,j]$-domination in interval graphs, the complexity of finding the minimum $[1,2]$-domination in interval graphs is still outstanding. In this paper, we propose a polynomial-time algorithm for computing a minimum $[1,2]$ on non-proper interval graphs by a dynamic programming technique. Next, on the negative side, we show that the minimum $[1,2]$-dominating set problem on circle graphs is $NP$-complete.","sentences":["A subset $S$ of vertices in a graph $G=(V, E)$ is Dominating Set if each vertex in $V(G)\\setminus S$ is adjacent to at least one vertex in $S$. Chellali et al.","in 2013, by restricting the number of neighbors in $S$ of a vertex outside $S$, introduced the concept of $[1,j]$-dominating set.","A set $D \\subseteq V$ of a graph $G = (V, E)$ is called $[1,j]$-Dominating Set of $G$ if every vertex not in $D$ has at least one neighbor and at most $j$ neighbors in $D$. The Minimum $[1,j]$-Domination problem is the problem of finding the minimum set $D$. Given a positive integer $k$ and a graph $G = (V, E)$, the $[1,j]$-Domination Decision problem is to decide whether $G$ has $[1,j]$-dominating set of cardinality at most $k$. A polynomial-time algorithm was obtained in split graphs for a constant $j$ in contrast to the classic Dominating Set problem which is NP-hard in split graphs.","This result motivates us to investigate the effect of restriction $j$ on the complexity of $[1,j]$-domination problem on various classes of graphs.","Although for $j\\geq 3$, it has been proved that the minimum of classical domination is equal to minimum $[1,j]$-domination in interval graphs, the complexity of finding the minimum $[1,2]$-domination in interval graphs is still outstanding.","In this paper, we propose a polynomial-time algorithm for computing a minimum $[1,2]$ on non-proper interval graphs by a dynamic programming technique.","Next, on the negative side, we show that the minimum $[1,2]$-dominating set problem on circle graphs is $NP$-complete."],"url":"http://arxiv.org/abs/2403.04694v1","category":"cs.CC"}
{"created":"2024-03-07 17:42:40","title":"Analysis of Systems' Performance in Natural Language Processing Competitions","abstract":"Collaborative competitions have gained popularity in the scientific and technological fields. These competitions involve defining tasks, selecting evaluation scores, and devising result verification methods. In the standard scenario, participants receive a training set and are expected to provide a solution for a held-out dataset kept by organizers. An essential challenge for organizers arises when comparing algorithms' performance, assessing multiple participants, and ranking them. Statistical tools are often used for this purpose; however, traditional statistical methods often fail to capture decisive differences between systems' performance. This manuscript describes an evaluation methodology for statistically analyzing competition results and competition. The methodology is designed to be universally applicable; however, it is illustrated using eight natural language competitions as case studies involving classification and regression problems. The proposed methodology offers several advantages, including off-the-shell comparisons with correction mechanisms and the inclusion of confidence intervals. Furthermore, we introduce metrics that allow organizers to assess the difficulty of competitions. Our analysis shows the potential usefulness of our methodology for effectively evaluating competition results.","sentences":["Collaborative competitions have gained popularity in the scientific and technological fields.","These competitions involve defining tasks, selecting evaluation scores, and devising result verification methods.","In the standard scenario, participants receive a training set and are expected to provide a solution for a held-out dataset kept by organizers.","An essential challenge for organizers arises when comparing algorithms' performance, assessing multiple participants, and ranking them.","Statistical tools are often used for this purpose; however, traditional statistical methods often fail to capture decisive differences between systems' performance.","This manuscript describes an evaluation methodology for statistically analyzing competition results and competition.","The methodology is designed to be universally applicable; however, it is illustrated using eight natural language competitions as case studies involving classification and regression problems.","The proposed methodology offers several advantages, including off-the-shell comparisons with correction mechanisms and the inclusion of confidence intervals.","Furthermore, we introduce metrics that allow organizers to assess the difficulty of competitions.","Our analysis shows the potential usefulness of our methodology for effectively evaluating competition results."],"url":"http://arxiv.org/abs/2403.04693v1","category":"cs.LG"}
{"created":"2024-03-07 17:34:53","title":"Dramatic Drop in the X-Ray Polarization of Swift J1727.8$-$1613 in the Soft Spectral State","abstract":"Black-hole X-ray binaries exhibit different spectral and timing properties in different accretion states. The X-ray outburst of a recently discovered and extraordinarily bright source, Swift$~$J1727.8$-$1613, has enabled the first investigation of how the X-ray polarization properties of a source evolve with spectral state. The 2$-$8 keV polarization degree was previously measured by the Imaging X-ray Polarimetry Explorer (IXPE) to be $\\approx$ 4% in the hard and hard intermediate states. Here we present new IXPE results taken in the soft state, with the X-ray flux dominated by the thermal accretion-disk emission. We find that the polarization degree has dropped dramatically to $\\lesssim$ 1%. This result indicates that the measured X-ray polarization is largely sensitive to the accretion state and the polarization fraction is significantly higher in the hard state when the X-ray emission is dominated by up-scattered radiation in the X-ray corona. The combined polarization measurements in the soft and hard states disfavor a very high or low inclination of the system.","sentences":["Black-hole X-ray binaries exhibit different spectral and timing properties in different accretion states.","The X-ray outburst of a recently discovered and extraordinarily bright source, Swift$~$J1727.8$-$1613, has enabled the first investigation of how the X-ray polarization properties of a source evolve with spectral state.","The 2$-$8 keV polarization degree was previously measured by the Imaging X-ray Polarimetry Explorer (IXPE) to be $\\approx$ 4% in the hard and hard intermediate states.","Here we present new IXPE results taken in the soft state, with the X-ray flux dominated by the thermal accretion-disk emission.","We find that the polarization degree has dropped dramatically to $\\lesssim$ 1%.","This result indicates that the measured X-ray polarization is largely sensitive to the accretion state and the polarization fraction is significantly higher in the hard state when the X-ray emission is dominated by up-scattered radiation in the X-ray corona.","The combined polarization measurements in the soft and hard states disfavor a very high or low inclination of the system."],"url":"http://arxiv.org/abs/2403.04689v1","category":"astro-ph.HE"}
{"created":"2024-03-07 17:34:04","title":"A divide-and-conquer approach for sparse recovery of high dimensional signals","abstract":"Compressed sensing (CS) techniques demand significant storage and computational resources, when recovering high-dimensional sparse signals. Block CS (BCS), a special class of CS, addresses both the storage and complexity issues by partitioning the sparse recovery problem into several sub-problems. In this paper, we derive a Welch bound-based guarantee on the reconstruction error with BCS. Our guarantee reveals that the reconstruction quality with BCS monotonically reduces with an increasing number of partitions. To alleviate this performance loss, we propose a sparse recovery technique that exploits correlation across the partitions of the sparse signal. Our method outperforms BCS in the moderate SNR regime, for a modest increase in the storage and computational complexities.","sentences":["Compressed sensing (CS) techniques demand significant storage and computational resources, when recovering high-dimensional sparse signals.","Block CS (BCS), a special class of CS, addresses both the storage and complexity issues by partitioning the sparse recovery problem into several sub-problems.","In this paper, we derive a Welch bound-based guarantee on the reconstruction error with BCS.","Our guarantee reveals that the reconstruction quality with BCS monotonically reduces with an increasing number of partitions.","To alleviate this performance loss, we propose a sparse recovery technique that exploits correlation across the partitions of the sparse signal.","Our method outperforms BCS in the moderate SNR regime, for a modest increase in the storage and computational complexities."],"url":"http://arxiv.org/abs/2403.04688v1","category":"eess.SP"}
{"created":"2024-03-07 17:32:42","title":"Alternative Method for Estimating Betti Numbers","abstract":"Topological data analysis (TDA) is a fast-growing field that utilizes advanced tools from topology to analyze large-scale data. A central problem in topological data analysis is estimating the so-called Betti numbers of the underlying simplicial complex. While the difficulty of this problem has been established as NP-hard, previous works have showcased appealing quantum speedup. In this article, we provide an alternative method for estimating Betti numbers of given simplicial complex, based on some recent results on quantum algorithm. Our method can be faster than the best-known classical method for finding Betti numbers, and interestingly, it can also find the Betti numbers of the complement graph to our original one.","sentences":["Topological data analysis (TDA) is a fast-growing field that utilizes advanced tools from topology to analyze large-scale data.","A central problem in topological data analysis is estimating the so-called Betti numbers of the underlying simplicial complex.","While the difficulty of this problem has been established as NP-hard, previous works have showcased appealing quantum speedup.","In this article, we provide an alternative method for estimating Betti numbers of given simplicial complex, based on some recent results on quantum algorithm.","Our method can be faster than the best-known classical method for finding Betti numbers, and interestingly, it can also find the Betti numbers of the complement graph to our original one."],"url":"http://arxiv.org/abs/2403.04686v1","category":"quant-ph"}
{"created":"2024-03-07 17:25:45","title":"On the rigidity of the complex Grassmannians","abstract":"We study the integrability to second order of the infinitesimal Einstein deformations of the symmetric metric $g$ on the complex Grassmannian of $k$-planes inside $\\mathbb{C}^n$. By showing the nonvanishing of Koiso's obstruction polynomial, we characterize the infinitesimal deformations that are integrable to second order as an explicit variety inside $\\mathfrak{su}(n)$. In particular we show that $g$ is isolated in the moduli space of Einstein metrics if $n$ is odd.","sentences":["We study the integrability to second order of the infinitesimal Einstein deformations of the symmetric metric $g$ on the complex Grassmannian of $k$-planes inside $\\mathbb{C}^n$. By showing the nonvanishing of Koiso's obstruction polynomial, we characterize the infinitesimal deformations that are integrable to second order as an explicit variety inside $\\mathfrak{su}(n)$. In particular we show that $g$ is isolated in the moduli space of Einstein metrics if $n$ is odd."],"url":"http://arxiv.org/abs/2403.04681v1","category":"math.DG"}
{"created":"2024-03-07 17:23:56","title":"Differentiable master equation solver for quantum device characterisation","abstract":"Differentiable models of physical systems provide a powerful platform for gradient-based algorithms, with particular impact on parameter estimation and optimal control. Quantum systems present a particular challenge for such characterisation and control, owing to their inherently stochastic nature and sensitivity to environmental parameters. To address this challenge, we present a versatile differentiable quantum master equation solver, and incorporate this solver into a framework for device characterisation. Our approach utilises gradient-based optimisation and Bayesian inference to provide estimates and uncertainties in quantum device parameters. To showcase our approach, we consider steady state charge transport through electrostatically defined quantum dots. Using simulated data, we demonstrate efficient estimation of parameters for a single quantum dot, and model selection as well as the capability of our solver to compute time evolution for a double quantum dot system. Our differentiable solver stands to widen the impact of physics-aware machine learning algorithms on quantum devices for characterisation and control.","sentences":["Differentiable models of physical systems provide a powerful platform for gradient-based algorithms, with particular impact on parameter estimation and optimal control.","Quantum systems present a particular challenge for such characterisation and control, owing to their inherently stochastic nature and sensitivity to environmental parameters.","To address this challenge, we present a versatile differentiable quantum master equation solver, and incorporate this solver into a framework for device characterisation.","Our approach utilises gradient-based optimisation and Bayesian inference to provide estimates and uncertainties in quantum device parameters.","To showcase our approach, we consider steady state charge transport through electrostatically defined quantum dots.","Using simulated data, we demonstrate efficient estimation of parameters for a single quantum dot, and model selection as well as the capability of our solver to compute time evolution for a double quantum dot system.","Our differentiable solver stands to widen the impact of physics-aware machine learning algorithms on quantum devices for characterisation and control."],"url":"http://arxiv.org/abs/2403.04678v1","category":"quant-ph"}
{"created":"2024-03-07 17:22:19","title":"Real-time Regulation of Detention Ponds via Feedback Control: Balancing Flood Mitigation and Water Quality","abstract":"Floods in urban areas are becoming more intense due to unplanned urbanization and more frequent due to climate change. One of the most effective strategies to alleviate the effects of flooding is the use of flood control reservoirs such as detention ponds, which attenuate flood waves by storing water and slowing the release after the storm. Detention ponds can also improve water quality by allowing the settlement of pollutants inside the reservoir. The operation of most detention ponds occurs passively, where the outflows are governed by fixed hydraulic structures such as fully open orifices and weirs. The operation of detention ponds can be enhanced with active controls: orifices can be retrofitted with controlled valves, and spillways can have controllable gates such that their control schedule can be defined in real-time with a model predictive control (MPC) approach. In this paper, we develop a distributed quasi-2D hydrologic-hydrodynamic coupled with a reservoir flood routing model and an optimization approach (MPC) to identify the opening or closing of valves and movable gates working as spillways. We adapt the optimization problem to switch from a flood-related cost function to a heuristic function that aims to increase the detention time when no inflow hydrographs are predicted within a prediction horizon. The numerical case studies show the potential results of applying the methods herein developed in a real-world watershed in Sao Paulo, Brazil. We test the performance of MPC compared to static (i.e., fixed hydraulic device opening) alternatives with valves either fully or partially opened. The results indicate that the control algorithm presented in this paper can achieve greater flood and proxy water quality performance compared to passive scenarios.","sentences":["Floods in urban areas are becoming more intense due to unplanned urbanization and more frequent due to climate change.","One of the most effective strategies to alleviate the effects of flooding is the use of flood control reservoirs such as detention ponds, which attenuate flood waves by storing water and slowing the release after the storm.","Detention ponds can also improve water quality by allowing the settlement of pollutants inside the reservoir.","The operation of most detention ponds occurs passively, where the outflows are governed by fixed hydraulic structures such as fully open orifices and weirs.","The operation of detention ponds can be enhanced with active controls: orifices can be retrofitted with controlled valves, and spillways can have controllable gates such that their control schedule can be defined in real-time with a model predictive control (MPC) approach.","In this paper, we develop a distributed quasi-2D hydrologic-hydrodynamic coupled with a reservoir flood routing model and an optimization approach (MPC) to identify the opening or closing of valves and movable gates working as spillways.","We adapt the optimization problem to switch from a flood-related cost function to a heuristic function that aims to increase the detention time when no inflow hydrographs are predicted within a prediction horizon.","The numerical case studies show the potential results of applying the methods herein developed in a real-world watershed in Sao Paulo, Brazil.","We test the performance of MPC compared to static (i.e., fixed hydraulic device opening) alternatives with valves either fully or partially opened.","The results indicate that the control algorithm presented in this paper can achieve greater flood and proxy water quality performance compared to passive scenarios."],"url":"http://arxiv.org/abs/2403.04675v1","category":"eess.SY"}
{"created":"2024-03-07 17:19:43","title":"Notes on complex $q=2$ SYK","abstract":"This note clarifies and extends results on the complex SYK model to the solvable q = 2 case. We calculate the four point function OPE of fermions in the low energy CFT, implying the existence of a tower of integer-weight operators in the IR. We comment on the lack of a mode breaking conformal symmetry in this special case of SYK and the consequences for deformations of the theory near the conformal fixed point. We use the nearly-free structure of the model to provide a closed form expression for OPE coefficients of the integer-weight operators. We also discuss analytic and numerical results relevant to the thermodynamics of q = 2 SYK in both the complex and real case. The tower of operators transform in the discrete series of representations of SL(2,R), the representations shared by dS2 and AdS2. In this work we continue discussion of holographic models including these representations.","sentences":["This note clarifies and extends results on the complex SYK model to the solvable q = 2 case.","We calculate the four point function OPE of fermions in the low energy CFT, implying the existence of a tower of integer-weight operators in the IR.","We comment on the lack of a mode breaking conformal symmetry in this special case of SYK and the consequences for deformations of the theory near the conformal fixed point.","We use the nearly-free structure of the model to provide a closed form expression for OPE coefficients of the integer-weight operators.","We also discuss analytic and numerical results relevant to the thermodynamics of q = 2 SYK in both the complex and real case.","The tower of operators transform in the discrete series of representations of SL(2,R), the representations shared by dS2 and AdS2.","In this work we continue discussion of holographic models including these representations."],"url":"http://arxiv.org/abs/2403.04673v1","category":"hep-th"}
{"created":"2024-03-07 16:58:00","title":"Closed-loop Performance Optimization of Model Predictive Control with Robustness Guarantees","abstract":"Model mismatch and process noise are two frequently occurring phenomena that can drastically affect the performance of model predictive control (MPC) in practical applications. We propose a principled way to tune the cost function and the constraints of linear MPC schemes to achieve good performance and robust constraint satisfaction on uncertain nonlinear dynamics with additive noise. The tuning is performed using a novel MPC tuning algorithm based on backpropagation developed in our earlier work. Using the scenario approach, we provide probabilistic bounds on the likelihood of closed-loop constraint violation over a finite horizon. We showcase the effectiveness of the proposed method on linear and nonlinear simulation examples.","sentences":["Model mismatch and process noise are two frequently occurring phenomena that can drastically affect the performance of model predictive control (MPC) in practical applications.","We propose a principled way to tune the cost function and the constraints of linear MPC schemes to achieve good performance and robust constraint satisfaction on uncertain nonlinear dynamics with additive noise.","The tuning is performed using a novel MPC tuning algorithm based on backpropagation developed in our earlier work.","Using the scenario approach, we provide probabilistic bounds on the likelihood of closed-loop constraint violation over a finite horizon.","We showcase the effectiveness of the proposed method on linear and nonlinear simulation examples."],"url":"http://arxiv.org/abs/2403.04655v1","category":"eess.SY"}
{"created":"2024-03-07 16:57:45","title":"Audio-Visual Person Verification based on Recursive Fusion of Joint Cross-Attention","abstract":"Person or identity verification has been recently gaining a lot of attention using audio-visual fusion as faces and voices share close associations with each other. Conventional approaches based on audio-visual fusion rely on score-level or early feature-level fusion techniques. Though existing approaches showed improvement over unimodal systems, the potential of audio-visual fusion for person verification is not fully exploited. In this paper, we have investigated the prospect of effectively capturing both the intra- and inter-modal relationships across audio and visual modalities, which can play a crucial role in significantly improving the fusion performance over unimodal systems. In particular, we introduce a recursive fusion of a joint cross-attentional model, where a joint audio-visual feature representation is employed in the cross-attention framework in a recursive fashion to progressively refine the feature representations that can efficiently capture the intra-and inter-modal relationships. To further enhance the audio-visual feature representations, we have also explored BLSTMs to improve the temporal modeling of audio-visual feature representations. Extensive experiments are conducted on the Voxceleb1 dataset to evaluate the proposed model. Results indicate that the proposed model shows promising improvement in fusion performance by adeptly capturing the intra-and inter-modal relationships across audio and visual modalities.","sentences":["Person or identity verification has been recently gaining a lot of attention using audio-visual fusion as faces and voices share close associations with each other.","Conventional approaches based on audio-visual fusion rely on score-level or early feature-level fusion techniques.","Though existing approaches showed improvement over unimodal systems, the potential of audio-visual fusion for person verification is not fully exploited.","In this paper, we have investigated the prospect of effectively capturing both the intra- and inter-modal relationships across audio and visual modalities, which can play a crucial role in significantly improving the fusion performance over unimodal systems.","In particular, we introduce a recursive fusion of a joint cross-attentional model, where a joint audio-visual feature representation is employed in the cross-attention framework in a recursive fashion to progressively refine the feature representations that can efficiently capture the intra-and inter-modal relationships.","To further enhance the audio-visual feature representations, we have also explored BLSTMs to improve the temporal modeling of audio-visual feature representations.","Extensive experiments are conducted on the Voxceleb1 dataset to evaluate the proposed model.","Results indicate that the proposed model shows promising improvement in fusion performance by adeptly capturing the intra-and inter-modal relationships across audio and visual modalities."],"url":"http://arxiv.org/abs/2403.04654v1","category":"cs.CV"}
{"created":"2024-03-07 16:48:51","title":"Online Maximum Likelihood Parameter Estimation for Continuously-Monitored Quantum Systems","abstract":"In this work, we consider the problem of online (real-time, single-shot) estimation of static or slow-varying parameters along quantum trajectories in quantum dynamical systems. Based on the measurement signal of a continuously-monitored quantum system, we propose a recursive algorithm for computing the maximum likelihood estimate of unknown parameters using an approach based on stochastic gradient ascent on the log-likelihood function. We formulate the algorithm in both discrete-time and continuous-time and illustrate the performance of the algorithm through simulations of a simple two-level system undergoing homodyne measurement from which we are able to track multiple parameters simultaneously.","sentences":["In this work, we consider the problem of online (real-time, single-shot) estimation of static or slow-varying parameters along quantum trajectories in quantum dynamical systems.","Based on the measurement signal of a continuously-monitored quantum system, we propose a recursive algorithm for computing the maximum likelihood estimate of unknown parameters using an approach based on stochastic gradient ascent on the log-likelihood function.","We formulate the algorithm in both discrete-time and continuous-time and illustrate the performance of the algorithm through simulations of a simple two-level system undergoing homodyne measurement from which we are able to track multiple parameters simultaneously."],"url":"http://arxiv.org/abs/2403.04648v1","category":"quant-ph"}
{"created":"2024-03-07 16:36:29","title":"Teaching Large Language Models to Reason with Reinforcement Learning","abstract":"Reinforcement Learning from Human Feedback (\\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple model sizes and initializations both with and without supervised fine-tuning (\\textbf{SFT}) data. Overall, we find all algorithms perform comparably, with Expert Iteration performing best in most cases. Surprisingly, we find the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of $10^6$ samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning.","sentences":["Reinforcement Learning from Human Feedback (\\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences.","Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities.","We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model.","We additionally start from multiple model sizes and initializations both with and without supervised fine-tuning (\\textbf{SFT}) data.","Overall, we find all algorithms perform comparably, with Expert Iteration performing best in most cases.","Surprisingly, we find the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of $10^6","$ samples to converge from a pretrained checkpoint.","We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models.","Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously.","We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning."],"url":"http://arxiv.org/abs/2403.04642v1","category":"cs.LG"}
{"created":"2024-03-07 16:21:02","title":"Virtuoso: An Open-Source, Comprehensive and Modular Simulation Framework for Virtual Memory Research","abstract":"Virtual memory is a cornerstone of modern computing systems.Introduced as one of the earliest instances of hardware-software co-design, VM facilitates programmer-transparent memory man agement, data sharing, process isolation and memory protection. Evaluating the efficiency of various virtual memory (VM) designs is crucial (i) given their significant impact on the system, including the CPU caches, the main memory, and the storage device and (ii) given that different system architectures might benefit from various VM techniques. Such an evaluation is not straightforward, as it heavily hinges on modeling the interplay between different VM techniques and the interactions of VM with the system architecture. Modern simulators, however, struggle to keep up with the rapid VM research developments, lacking the capability to model a wide range of contemporary VM techniques and their interactions. To this end, we present Virtuoso, an open-source, comprehensive and modular simulation framework that models various VM designs to establish a common ground for virtual memory research. We demonstrate the versatility and the potential of Virtuoso with four new case studies. Virtuoso is freely open-source and can be found at https://github.com/CMU-SAFARI/Virtuoso.","sentences":["Virtual memory is a cornerstone of modern computing systems.","Introduced as one of the earliest instances of hardware-software co-design, VM facilitates programmer-transparent memory man agement, data sharing, process isolation and memory protection.","Evaluating the efficiency of various virtual memory (VM) designs is crucial (i) given their significant impact on the system, including the CPU caches, the main memory, and the storage device and (ii) given that different system architectures might benefit from various VM techniques.","Such an evaluation is not straightforward, as it heavily hinges on modeling the interplay between different VM techniques and the interactions of VM with the system architecture.","Modern simulators, however, struggle to keep up with the rapid VM research developments, lacking the capability to model a wide range of contemporary VM techniques and their interactions.","To this end, we present Virtuoso, an open-source, comprehensive and modular simulation framework that models various VM designs to establish a common ground for virtual memory research.","We demonstrate the versatility and the potential of Virtuoso with four new case studies.","Virtuoso is freely open-source and can be found at https://github.com/CMU-SAFARI/Virtuoso."],"url":"http://arxiv.org/abs/2403.04635v1","category":"cs.AR"}
{"created":"2024-03-07 16:09:22","title":"Solitary waves in a stochastic parametrically forced nonlinear Schr\u00f6dinger equation","abstract":"We study a parametrically forced nonlinear Schr\\\"odinger (PFNLS) equation, driven by multiplicative translation-invariant noise. We show that a solitary wave in the stochastic equation is orbitally stable on a timescale which is exponential in the inverse square of the noise strength. We give explicit expressions for the phase shift and fluctuations around the shifted wave which are accurate to second order in the noise strength. This is done by developing a new perspective on the phase-lag method introduced by Kr\\\"uger and Stannat. Additionally, we show well-posedness of the equation in the fractional Bessel space $H^{s}$ for any $s \\in [0,\\infty)$, demonstrating persistence of regularity.","sentences":["We study a parametrically forced nonlinear Schr\\\"odinger (PFNLS) equation, driven by multiplicative translation-invariant noise.","We show that a solitary wave in the stochastic equation is orbitally stable on a timescale which is exponential in the inverse square of the noise strength.","We give explicit expressions for the phase shift and fluctuations around the shifted wave which are accurate to second order in the noise strength.","This is done by developing a new perspective on the phase-lag method introduced by Kr\\\"uger and Stannat.","Additionally, we show well-posedness of the equation in the fractional Bessel space $H^{s}$ for any $s \\in","[0,\\infty)$, demonstrating persistence of regularity."],"url":"http://arxiv.org/abs/2403.04625v1","category":"math.DS"}
{"created":"2024-03-07 16:08:55","title":"Bloch oscillations in a transmon embedded in a resonant electromagnetic environment","abstract":"Recently developed Josephson junction array transmission lines implement strong-coupling circuit electrodynamics compatible with a range of superconducting quantum devices. They provide both the high impedance which allows for strong quantum fluctuations, and photon modes with which to probe a quantum device, such as a small Josephson junction. In this high-impedance environment, current through the junction is accompanied by charge Bloch oscillations analogous to those in crystalline systems. However, the interplay between Bloch oscillations and environmental photon resonances remains largely unexplored. Here we describe the Bloch oscillations in a transmon-type qubit attached to high-impedance transmission lines with discrete photon spectra. Transmons are characterized by well-separated charge bands, favoring Bloch oscillations over Landau-Zener tunneling. We find resonances in the voltage--current relation and the spectrum of photons emitted by the Bloch oscillations. The transmon also scatters photons inelastically; we find the cross-section for a novel anti-Stokes-like process whereby photons gain a Bloch oscillation quantum. Our results outline how Bloch oscillations leave fingerprints for experiments across the DC, MHz, and GHz ranges.","sentences":["Recently developed Josephson junction array transmission lines implement strong-coupling circuit electrodynamics compatible with a range of superconducting quantum devices.","They provide both the high impedance which allows for strong quantum fluctuations, and photon modes with which to probe a quantum device, such as a small Josephson junction.","In this high-impedance environment, current through the junction is accompanied by charge Bloch oscillations analogous to those in crystalline systems.","However, the interplay between Bloch oscillations and environmental photon resonances remains largely unexplored.","Here we describe the Bloch oscillations in a transmon-type qubit attached to high-impedance transmission lines with discrete photon spectra.","Transmons are characterized by well-separated charge bands, favoring Bloch oscillations over Landau-Zener tunneling.","We find resonances in the voltage--current relation and the spectrum of photons emitted by the Bloch oscillations.","The transmon also scatters photons inelastically; we find the cross-section for a novel anti-Stokes-like process whereby photons gain a Bloch oscillation quantum.","Our results outline how Bloch oscillations leave fingerprints for experiments across the DC, MHz, and GHz ranges."],"url":"http://arxiv.org/abs/2403.04623v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-07 15:50:09","title":"Implementation of soft-constrained MPC for Tracking using its semi-banded problem structure","abstract":"Model Predictive Control (MPC) is a popular control approach due to its ability to consider constraints, including input and state restrictions, while minimizing a cost function. However, in practice, said constraints can result in feasibility issues, either because the system model is not accurate or due to the existence of external disturbances. To mitigate this problem, a solution adopted by the MPC community is the use of soft constraints. In this article, we consider a not-so-typical methodology to encode soft constraints in a particular MPC formulation known as MPC for Tracking (MPCT), which has several advantages when compared to standard MPC formulations. The motivation behind the proposed encoding is to maintain the semi-banded structure of the ingredients of a recently proposed solver for the considered MPCT formulation, thus providing an efficient and fast solver when compared to alternative approaches from the literature. We show numerical results highlighting the benefits of the formulation and the computational efficiency of the solver.","sentences":["Model Predictive Control (MPC) is a popular control approach due to its ability to consider constraints, including input and state restrictions, while minimizing a cost function.","However, in practice, said constraints can result in feasibility issues, either because the system model is not accurate or due to the existence of external disturbances.","To mitigate this problem, a solution adopted by the MPC community is the use of soft constraints.","In this article, we consider a not-so-typical methodology to encode soft constraints in a particular MPC formulation known as MPC for Tracking (MPCT), which has several advantages when compared to standard MPC formulations.","The motivation behind the proposed encoding is to maintain the semi-banded structure of the ingredients of a recently proposed solver for the considered MPCT formulation, thus providing an efficient and fast solver when compared to alternative approaches from the literature.","We show numerical results highlighting the benefits of the formulation and the computational efficiency of the solver."],"url":"http://arxiv.org/abs/2403.04601v1","category":"eess.SY"}
{"created":"2024-03-07 15:39:04","title":"89 New Ultracool Dwarf Co-Moving Companions Identified With The Backyard Worlds: Planet 9 Citizen Science Project","abstract":"We report the identification of 89 new systems containing ultracool dwarf companions to main sequence stars and white dwarfs, using the citizen science project Backyard Worlds: Planet 9 and cross-reference between Gaia and CatWISE2020. Thirty-two of these companions and thirty-three host stars were followed up with spectroscopic observations, with companion spectral types ranging from M7-T9 and host spectral types ranging from G2-M9. These systems exhibit diverse characteristics, from young to old ages, blue to very red spectral morphologies, potential membership to known young moving groups, and evidence of spectral binarity in 9 companions. Twenty of the host stars in our sample show evidence for higher order multiplicity, with an additional 11 host stars being resolved binaries themselves. We compare this sample's characteristics with those of the known stellar binary and exoplanet populations, and find our sample begins to fill in the gap between directly imaged exoplanets and stellary binaries on mass ratio-binding energy plots. With this study, we increase the population of ultracool dwarf companions to FGK stars by $\\sim$42\\%, and more than triple the known population of ultracool dwarf companions with separations larger than 1,000 au, providing excellent targets for future atmospheric retrievals.","sentences":["We report the identification of 89 new systems containing ultracool dwarf companions to main sequence stars and white dwarfs, using the citizen science project Backyard Worlds: Planet 9 and cross-reference between Gaia and CatWISE2020.","Thirty-two of these companions and thirty-three host stars were followed up with spectroscopic observations, with companion spectral types ranging from M7-T9 and host spectral types ranging from G2-M9.","These systems exhibit diverse characteristics, from young to old ages, blue to very red spectral morphologies, potential membership to known young moving groups, and evidence of spectral binarity in 9 companions.","Twenty of the host stars in our sample show evidence for higher order multiplicity, with an additional 11 host stars being resolved binaries themselves.","We compare this sample's characteristics with those of the known stellar binary and exoplanet populations, and find our sample begins to fill in the gap between directly imaged exoplanets and stellary binaries on mass ratio-binding energy plots.","With this study, we increase the population of ultracool dwarf companions to FGK stars by $\\sim$42\\%, and more than triple the known population of ultracool dwarf companions with separations larger than 1,000 au, providing excellent targets for future atmospheric retrievals."],"url":"http://arxiv.org/abs/2403.04592v1","category":"astro-ph.SR"}
{"created":"2024-03-07 15:29:04","title":"What Cannot be Skipped About the Skiplist: A Survey of Skiplists and Their Applications in Big Data Systems","abstract":"Skiplists have become prevalent in systems. The main advantages of skiplists are their simplicity and ease of implementation, and the ability to support operations in the same asymptotic complexities as their tree-based counterparts. In this survey, we explore skiplists and their many variants. We highlight many scenarios of how skiplists are useful and fit well in these usage scenarios. We study several extensions to skiplists to make them fit for more applications, e.g., their use in the multi-dimensional space, network overlaying algorithms, as well as serving as indexes in database systems. Besides, we also discuss systems that adopt the idea of skiplists and apply the probabilistic skip pattern into their designs.","sentences":["Skiplists have become prevalent in systems.","The main advantages of skiplists are their simplicity and ease of implementation, and the ability to support operations in the same asymptotic complexities as their tree-based counterparts.","In this survey, we explore skiplists and their many variants.","We highlight many scenarios of how skiplists are useful and fit well in these usage scenarios.","We study several extensions to skiplists to make them fit for more applications, e.g., their use in the multi-dimensional space, network overlaying algorithms, as well as serving as indexes in database systems.","Besides, we also discuss systems that adopt the idea of skiplists and apply the probabilistic skip pattern into their designs."],"url":"http://arxiv.org/abs/2403.04582v1","category":"cs.DB"}
{"created":"2024-03-07 15:22:19","title":"Tensor Power Flow Formulations for Multidimensional Analyses in Distribution Systems","abstract":"In this paper, we present two multidimensional power flow formulations based on a fixed-point iteration (FPI) algorithm to efficiently solve hundreds of thousands of power flows in distribution systems. The presented algorithms are the base for a new TensorPowerFlow (TPF) tool and shine for their simplicity, benefiting from multicore \\gls{cpu} and \\gls{gpu} parallelization. We also focus on the mathematical convergence properties of the algorithm, showing that its unique solution is at the practical operational point, which is the solution of high-voltage and low-current. The proof is validated using numerical simulations showing the robustness of the FPI algorithm compared to the classical \\gls{nr} approach. In the case study, a benchmark with different PF solution methods is performed, showing that for applications requiring a yearly simulation at 1-minute resolution the computation time is decreased by a factor of 164, compared to the NR in its sparse formulation.","sentences":["In this paper, we present two multidimensional power flow formulations based on a fixed-point iteration (FPI) algorithm to efficiently solve hundreds of thousands of power flows in distribution systems.","The presented algorithms are the base for a new TensorPowerFlow (TPF) tool and shine for their simplicity, benefiting from multicore \\gls{cpu} and \\gls{gpu} parallelization.","We also focus on the mathematical convergence properties of the algorithm, showing that its unique solution is at the practical operational point, which is the solution of high-voltage and low-current.","The proof is validated using numerical simulations showing the robustness of the FPI algorithm compared to the classical \\gls{nr} approach.","In the case study, a benchmark with different PF solution methods is performed, showing that for applications requiring a yearly simulation at 1-minute resolution the computation time is decreased by a factor of 164, compared to the NR in its sparse formulation."],"url":"http://arxiv.org/abs/2403.04578v1","category":"eess.SY"}
{"created":"2024-03-07 15:21:53","title":"A Model Hierarchy for Predicting the Flow in Stirred Tanks with Physics-Informed Neural Networks","abstract":"This paper explores the potential of Physics-Informed Neural Networks (PINNs) to serve as Reduced Order Models (ROMs) for simulating the flow field within stirred tank reactors (STRs). We solve the two-dimensional stationary Navier-Stokes equations within a geometrically intricate domain and explore methodologies that allow us to integrate additional physical insights into the model. These approaches include imposing the Dirichlet boundary conditions (BCs) strongly and employing domain decomposition (DD), with both overlapping and non-overlapping subdomains. We adapt the Extended Physics-Informed Neural Network (XPINN) approach to solve different sets of equations in distinct subdomains based on the diverse flow characteristics present in each region. Our exploration results in a hierarchy of models spanning various levels of complexity, where the best models exhibit l1 prediction errors of less than 1% for both pressure and velocity. To illustrate the reproducibility of our approach, we track the errors over repeated independent training runs of the best identified model and show its reliability. Subsequently, by incorporating the stirring rate as a parametric input, we develop a fast-to-evaluate model of the flow capable of interpolating across a wide range of Reynolds numbers. Although we exclusively restrict ourselves to STRs in this work, we conclude that the steps taken to obtain the presented model hierarchy can be transferred to other applications.","sentences":["This paper explores the potential of Physics-Informed Neural Networks (PINNs) to serve as Reduced Order Models (ROMs) for simulating the flow field within stirred tank reactors (STRs).","We solve the two-dimensional stationary Navier-Stokes equations within a geometrically intricate domain and explore methodologies that allow us to integrate additional physical insights into the model.","These approaches include imposing the Dirichlet boundary conditions (BCs) strongly and employing domain decomposition (DD), with both overlapping and non-overlapping subdomains.","We adapt the Extended Physics-Informed Neural Network (XPINN) approach to solve different sets of equations in distinct subdomains based on the diverse flow characteristics present in each region.","Our exploration results in a hierarchy of models spanning various levels of complexity, where the best models exhibit l1 prediction errors of less than 1% for both pressure and velocity.","To illustrate the reproducibility of our approach, we track the errors over repeated independent training runs of the best identified model and show its reliability.","Subsequently, by incorporating the stirring rate as a parametric input, we develop a fast-to-evaluate model of the flow capable of interpolating across a wide range of Reynolds numbers.","Although we exclusively restrict ourselves to STRs in this work, we conclude that the steps taken to obtain the presented model hierarchy can be transferred to other applications."],"url":"http://arxiv.org/abs/2403.04576v1","category":"cs.CE"}
{"created":"2024-03-07 15:15:29","title":"Dynamic critical behavior of the chiral phase transition from the real-time functional renormalization group","abstract":"In the chiral limit the complicated many-body dynamics around the second-order chiral phase transition of two-flavor QCD can be understood by appealing to universality. We present a novel formulation of the real-time functional renormalization group that describes the stochastic hydrodynamic equations of motion for systems in the same dynamic universality class, which corresponds to Model G in the Halperin-Hohenberg classification. Our approach preserves all relevant symmetries of such systems with reversible mode couplings. We show that the calculations indeed produce the non-trivial value $z=d/2$ for the dynamic critical exponent, where $d$ is the number of spatial dimensions. From the momentum and temperature dependence of the diffusion coefficient of the conserved charge densities, we extract the dimensionless universal scaling function.","sentences":["In the chiral limit the complicated many-body dynamics around the second-order chiral phase transition of two-flavor QCD can be understood by appealing to universality.","We present a novel formulation of the real-time functional renormalization group that describes the stochastic hydrodynamic equations of motion for systems in the same dynamic universality class, which corresponds to Model G in the Halperin-Hohenberg classification.","Our approach preserves all relevant symmetries of such systems with reversible mode couplings.","We show that the calculations indeed produce the non-trivial value $z=d/2$ for the dynamic critical exponent, where $d$ is the number of spatial dimensions.","From the momentum and temperature dependence of the diffusion coefficient of the conserved charge densities, we extract the dimensionless universal scaling function."],"url":"http://arxiv.org/abs/2403.04573v1","category":"hep-ph"}
{"created":"2024-03-07 15:05:27","title":"Injective cochain map between the simplicial de Rham complex and the \u010cech-de Rham complex","abstract":"For a class of simplicial geometries, we construct an open cover where each lower-dimensional simplex of codimension $p$ is covered by an intersection of $p+1$ open sets in the open cover. We construct an injective cochain map from the simplicial de Rham complex to the \\v{C}ech-de Rham complex of the open cover. Both the double complexes have coefficients in $L^2$, and the cochain map we construct is between the respective domain complexes. The image of the cochain map will be an embedding of the simplicial de Rham complex, realizing it as a subcomplex of the \\v{C}ech-de Rham complex. The simplicial de Rham complex and the \\v{C}ech-de Rham complex represent mixed-dimensional and equidimensional coupled problems, respectively.","sentences":["For a class of simplicial geometries, we construct an open cover where each lower-dimensional simplex of codimension $p$ is covered by an intersection of $p+1$ open sets in the open cover.","We construct an injective cochain map from the simplicial de Rham complex to the \\v{C}ech-de Rham complex of the open cover.","Both the double complexes have coefficients in $L^2$, and the cochain map we construct is between the respective domain complexes.","The image of the cochain map will be an embedding of the simplicial de Rham complex, realizing it as a subcomplex of the \\v{C}ech-de Rham complex.","The simplicial de Rham complex and the \\v{C}ech-de Rham complex represent mixed-dimensional and equidimensional coupled problems, respectively."],"url":"http://arxiv.org/abs/2403.04569v1","category":"math.AT"}
{"created":"2024-03-07 14:56:10","title":"Fourth-order suboptimality of nominal model predictive control in the presence of uncertainty","abstract":"We investigate the suboptimality resulting from the application of nominal model predictive control (MPC) to a nonlinear discrete time stochastic system. The suboptimality is defined with respect to the corresponding stochastic optimal control problem (OCP) that minimizes the expected cost of the closed loop system. In this context, nominal MPC corresponds to a form of certainty-equivalent control (CEC). We prove that, in a smooth and unconstrained setting, the suboptimality growth is of fourth order with respect to the level of uncertainty, a parameter which we can think of as a standard deviation. This implies that the suboptimality does not grow very quickly as the level of uncertainty is increased, providing further insight into the practical success of nominal MPC. Similarly, the difference between the optimal and suboptimal control inputs is of second order. We illustrate the result on a simple numerical example, which we also use to show how the proven relationship may cease to hold in the presence of state constraints.","sentences":["We investigate the suboptimality resulting from the application of nominal model predictive control (MPC) to a nonlinear discrete time stochastic system.","The suboptimality is defined with respect to the corresponding stochastic optimal control problem (OCP) that minimizes the expected cost of the closed loop system.","In this context, nominal MPC corresponds to a form of certainty-equivalent control (CEC).","We prove that, in a smooth and unconstrained setting, the suboptimality growth is of fourth order with respect to the level of uncertainty, a parameter which we can think of as a standard deviation.","This implies that the suboptimality does not grow very quickly as the level of uncertainty is increased, providing further insight into the practical success of nominal MPC.","Similarly, the difference between the optimal and suboptimal control inputs is of second order.","We illustrate the result on a simple numerical example, which we also use to show how the proven relationship may cease to hold in the presence of state constraints."],"url":"http://arxiv.org/abs/2403.04559v1","category":"math.OC"}
{"created":"2024-03-07 14:48:48","title":"Improvements & Evaluations on the MLCommons CloudMask Benchmark","abstract":"In this paper, we report the performance benchmarking results of deep learning models on MLCommons' Science cloud-masking benchmark using a high-performance computing cluster at New York University (NYU): NYU Greene. MLCommons is a consortium that develops and maintains several scientific benchmarks that can benefit from developments in AI. We provide a description of the cloud-masking benchmark task, updated code, and the best model for this benchmark when using our selected hyperparameter settings. Our benchmarking results include the highest accuracy achieved on the NYU system as well as the average time taken for both training and inference on the benchmark across several runs/seeds. Our code can be found on GitHub. MLCommons team has been kept informed about our progress and may use the developed code for their future work.","sentences":["In this paper, we report the performance benchmarking results of deep learning models on MLCommons' Science cloud-masking benchmark using a high-performance computing cluster at New York University (NYU): NYU Greene.","MLCommons is a consortium that develops and maintains several scientific benchmarks that can benefit from developments in AI.","We provide a description of the cloud-masking benchmark task, updated code, and the best model for this benchmark when using our selected hyperparameter settings.","Our benchmarking results include the highest accuracy achieved on the NYU system as well as the average time taken for both training and inference on the benchmark across several runs/seeds.","Our code can be found on GitHub.","MLCommons team has been kept informed about our progress and may use the developed code for their future work."],"url":"http://arxiv.org/abs/2403.04553v1","category":"cs.DC"}
{"created":"2024-03-07 14:45:35","title":"Analysis of a Leslie-Gower model with Alle effects, cooperative hunting, and constant placement rates","abstract":"This paper investigates the dynamical properties of the Leslie-Gower model with Alle effects, cooperative hunting, and constant placement rates. The conditions for the existence of the triple equilibrium point of the model are first analyzed. Subsequently, the canonical type theory and the qualitative theory of planar systems are applied to obtain that the triple equilibrium point can be a node with a residual dimension of 2 and an equilibrium point with a residual dimension of 3 under different parameter conditions. Finally, it is proved that the system bifurcates with a residual dimension of 2 in the vicinity of the node with cooperative hunting and placement rate as branching parameters.","sentences":["This paper investigates the dynamical properties of the Leslie-Gower model with Alle effects, cooperative hunting, and constant placement rates.","The conditions for the existence of the triple equilibrium point of the model are first analyzed.","Subsequently, the canonical type theory and the qualitative theory of planar systems are applied to obtain that the triple equilibrium point can be a node with a residual dimension of 2 and an equilibrium point with a residual dimension of 3 under different parameter conditions.","Finally, it is proved that the system bifurcates with a residual dimension of 2 in the vicinity of the node with cooperative hunting and placement rate as branching parameters."],"url":"http://arxiv.org/abs/2403.04552v1","category":"math.DS"}
{"created":"2024-03-07 14:43:40","title":"Explainable Face Verification via Feature-Guided Gradient Backpropagation","abstract":"Recent years have witnessed significant advancement in face recognition (FR) techniques, with their applications widely spread in people's lives and security-sensitive areas. There is a growing need for reliable interpretations of decisions of such systems. Existing studies relying on various mechanisms have investigated the usage of saliency maps as an explanation approach, but suffer from different limitations. This paper first explores the spatial relationship between face image and its deep representation via gradient backpropagation. Then a new explanation approach FGGB has been conceived, which provides precise and insightful similarity and dissimilarity saliency maps to explain the \"Accept\" and \"Reject\" decision of an FR system. Extensive visual presentation and quantitative measurement have shown that FGGB achieves superior performance in both similarity and dissimilarity maps when compared to current state-of-the-art explainable face verification approaches.","sentences":["Recent years have witnessed significant advancement in face recognition (FR) techniques, with their applications widely spread in people's lives and security-sensitive areas.","There is a growing need for reliable interpretations of decisions of such systems.","Existing studies relying on various mechanisms have investigated the usage of saliency maps as an explanation approach, but suffer from different limitations.","This paper first explores the spatial relationship between face image and its deep representation via gradient backpropagation.","Then a new explanation approach FGGB has been conceived, which provides precise and insightful similarity and dissimilarity saliency maps to explain the \"Accept\" and \"Reject\" decision of an FR system.","Extensive visual presentation and quantitative measurement have shown that FGGB achieves superior performance in both similarity and dissimilarity maps when compared to current state-of-the-art explainable face verification approaches."],"url":"http://arxiv.org/abs/2403.04549v1","category":"cs.CV"}
{"created":"2024-03-07 14:36:26","title":"PUMA: Efficient and Low-Cost Memory Allocation and Alignment Support for Processing-Using-Memory Architectures","abstract":"Processing-using-DRAM (PUD) architectures impose a restrictive data layout and alignment for their operands, where source and destination operands (i) must reside in the same DRAM subarray (i.e., a group of DRAM rows sharing the same row buffer and row decoder) and (ii) are aligned to the boundaries of a DRAM row. However, standard memory allocation routines (i.e., malloc, posix_memalign, and huge pages-based memory allocation) fail to meet the data layout and alignment requirements for PUD architectures to operate successfully. To allow the memory allocation API to influence the OS memory allocator and ensure that memory objects are placed within specific DRAM subarrays, we propose a new lazy data allocation routine (in the kernel) for PUD memory objects called PUMA. The key idea of PUMA is to use the internal DRAM mapping information together with huge pages and then split huge pages into finer-grained allocation units that are (i) aligned to the page address and size and (ii) virtually contiguous.   We implement PUMA as a kernel module using QEMU and emulate a RISC-V machine running Fedora 33 with v5.9.0 Linux Kernel. We emulate the implementation of a PUD system capable of executing row copy operations (as in RowClone) and Boolean AND/OR/NOT operations (as in Ambit). In our experiments, such an operation is performed in the host CPU if a given operation cannot be executed in our PUD substrate (due to data misalignment). PUMA significantly outperforms the baseline memory allocators for all evaluated microbenchmarks and allocation sizes.","sentences":["Processing-using-DRAM (PUD) architectures impose a restrictive data layout and alignment for their operands, where source and destination operands (i) must reside in the same DRAM subarray (i.e., a group of DRAM rows sharing the same row buffer and row decoder) and (ii) are aligned to the boundaries of a DRAM row.","However, standard memory allocation routines (i.e., malloc, posix_memalign, and huge pages-based memory allocation) fail to meet the data layout and alignment requirements for PUD architectures to operate successfully.","To allow the memory allocation API to influence the OS memory allocator and ensure that memory objects are placed within specific DRAM subarrays, we propose a new lazy data allocation routine (in the kernel) for PUD memory objects called PUMA.","The key idea of PUMA is to use the internal DRAM mapping information together with huge pages and then split huge pages into finer-grained allocation units that are (i) aligned to the page address and size and (ii) virtually contiguous.   ","We implement PUMA as a kernel module using QEMU and emulate a RISC-V machine running Fedora 33 with v5.9.0 Linux Kernel.","We emulate the implementation of a PUD system capable of executing row copy operations (as in RowClone) and Boolean AND/OR/NOT operations (as in Ambit).","In our experiments, such an operation is performed in the host CPU if a given operation cannot be executed in our PUD substrate (due to data misalignment).","PUMA significantly outperforms the baseline memory allocators for all evaluated microbenchmarks and allocation sizes."],"url":"http://arxiv.org/abs/2403.04539v1","category":"cs.AR"}
{"created":"2024-03-07 14:34:09","title":"Thermal structure of circumbinary discs: Circumbinary planets should be icy not rocky","abstract":"The process of forming a circumbinary planet is thought to be intimately related to the structure of the nascent circumbinary disc. It has been shown that the structure of a circumbinary disc depends strongly on 3-dimensional effects and on the detailed modelling of the thermodynamics. Here, we employ 3-dimensional hydrodynamical simulations, combined with a proper treatment of the thermal physics using the RADMC-3D radiation transport code, to examine the location of the snow line in circumbinary discs. The models have application to the circumbinary planets that have been discovered in recent years by the Kepler and TESS transit surveys. We find that the snow line is located in a narrow region of the circumbinary disc, close to the inner cavity that is carved out by the central binary, at typical orbital distances of $\\sim 1.5-2$ AU for the system parameters considered. In this region, previous work has shown that both grain growth and pebble accretion are likely to be inefficient because of the presence of hydrodynamical turbulence. Hence, in situ planet formation interior to the snow line is unlikely to occur and circumbinary planets should preferentially be icy, not rocky.","sentences":["The process of forming a circumbinary planet is thought to be intimately related to the structure of the nascent circumbinary disc.","It has been shown that the structure of a circumbinary disc depends strongly on 3-dimensional effects and on the detailed modelling of the thermodynamics.","Here, we employ 3-dimensional hydrodynamical simulations, combined with a proper treatment of the thermal physics using the RADMC-3D radiation transport code, to examine the location of the snow line in circumbinary discs.","The models have application to the circumbinary planets that have been discovered in recent years by the Kepler and TESS transit surveys.","We find that the snow line is located in a narrow region of the circumbinary disc, close to the inner cavity that is carved out by the central binary, at typical orbital distances of $\\sim 1.5-2$ AU for the system parameters considered.","In this region, previous work has shown that both grain growth and pebble accretion are likely to be inefficient because of the presence of hydrodynamical turbulence.","Hence, in situ planet formation interior to the snow line is unlikely to occur and circumbinary planets should preferentially be icy, not rocky."],"url":"http://arxiv.org/abs/2403.04535v1","category":"astro-ph.EP"}
{"created":"2024-03-07 14:26:13","title":"Examination Minutes Measurement of Single-Antenna-Element Beamforming","abstract":"This document shall provide all knowledge gained in conjunction and preparation with the conducted measurements in the antenna measurement chamber of the Institute of Microwave and Wireless Systems (IMW) of the Leibniz University of Hannover (LUH). The measurements have been prepared and conducted by Lukas Grundmann, IMW, and Nils L. Johannsen, Chair of Information and Coding Theory (ICT) of the Christian- Albrechts-University (CAU) of Kiel. This minute shall allow a simpler understanding and quicker reapplication of the required calibrations and system setup for the measurements of further antennas.","sentences":["This document shall provide all knowledge gained in conjunction and preparation with the conducted measurements in the antenna measurement chamber of the Institute of Microwave and Wireless Systems (IMW) of the Leibniz University of Hannover (LUH).","The measurements have been prepared and conducted by Lukas Grundmann, IMW, and Nils L. Johannsen, Chair of Information and Coding Theory (ICT) of the Christian- Albrechts-University (CAU) of Kiel.","This minute shall allow a simpler understanding and quicker reapplication of the required calibrations and system setup for the measurements of further antennas."],"url":"http://arxiv.org/abs/2403.04525v1","category":"eess.SP"}
{"created":"2024-03-07 14:24:40","title":"Absence of local conserved quantity in the Heisenberg model with next-nearest-neighbor interaction","abstract":"We rigorously prove that the Heisenberg chain with next-nearest-neighbor interaction, which is anticipated to be non-integrable, is indeed non-integrable in the sense that this system has no nontrivial local conserved quantity. Our result covers two important models, the Majundhar-Ghosh model and the Shastry-Sutherland model, as special cases. These models are shown to be non-integrable while have some solvable energy eigenstates.","sentences":["We rigorously prove that the Heisenberg chain with next-nearest-neighbor interaction, which is anticipated to be non-integrable, is indeed non-integrable in the sense that this system has no nontrivial local conserved quantity.","Our result covers two important models, the Majundhar-Ghosh model and the Shastry-Sutherland model, as special cases.","These models are shown to be non-integrable while have some solvable energy eigenstates."],"url":"http://arxiv.org/abs/2403.04522v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-07 14:21:47","title":"Qualitative analysis of a class of SIRS infectious disease models with nonlinear infection rate","abstract":"The existence and local stability of some non-negative equilibrium points of a class of SIRS infectious disease models with non-linear infection and treatment rates are investigated under the condition that the total population is a constant. The qualitative theory of differential equations was used to demonstrate that the endemic equilibrium point of the system is either a stable equilibrium, an unstable equilibrium or a degenerate equilibrium under different circumstances. Subsequently, the local stability of the non-negative equilibrium point of the system is analyzed. Finally, the bifurcation theory is used to prove that the system takes the natural recovery growth rate as the parameter of the saddle-node branching, and the conditions for the existence of the model saddle-node branching are given.","sentences":["The existence and local stability of some non-negative equilibrium points of a class of SIRS infectious disease models with non-linear infection and treatment rates are investigated under the condition that the total population is a constant.","The qualitative theory of differential equations was used to demonstrate that the endemic equilibrium point of the system is either a stable equilibrium, an unstable equilibrium or a degenerate equilibrium under different circumstances.","Subsequently, the local stability of the non-negative equilibrium point of the system is analyzed.","Finally, the bifurcation theory is used to prove that the system takes the natural recovery growth rate as the parameter of the saddle-node branching, and the conditions for the existence of the model saddle-node branching are given."],"url":"http://arxiv.org/abs/2403.04519v1","category":"math.DS"}
{"created":"2024-03-07 14:19:42","title":"A structure-preserving semi-implicit IMEX finite volume scheme for ideal magnetohydrodynamics at all Mach and Alfv\u00e9n numbers","abstract":"We present a divergence-free semi-implicit finite volume scheme for the simulation of the ideal magnetohydrodynamics (MHD) equations which is stable for large time steps controlled by the local transport speed at all Mach and Alfv\\'en numbers. An operator splitting technique allows to treat the convective terms explicitly while the hydrodynamic pressure and the magnetic field contributions are integrated implicitly, yielding two decoupled linear implicit systems. The linearity of the implicit part is achieved by means of a semi-implicit time linearization. This structure is favorable as second-order accuracy in time can be achieved relying on the class of semi-implicit IMplicit-EXplicit Runge-Kutta (IMEX-RK) methods. In space, implicit cell-centered finite difference operators are designed to discretely preserve the divergence-free property of the magnetic field on three-dimensional Cartesian meshes. The new scheme is also particularly well suited for low Mach number flows and for the incompressible limit of the MHD equations, since no explicit numerical dissipation is added to the implicit contribution and the time step is scale independent. Likewise, highly magnetized flows can benefit from the implicit treatment of the magnetic fluxes, hence improving the computational efficiency of the novel method. The convective terms undergo a shock-capturing second order finite volume discretization to guarantee the effectiveness of the proposed method even for high Mach number flows. The new scheme is benchmarked against a series of test cases for the ideal MHD equations addressing different acoustic and Alfv\\'en Mach number regimes where the performance and the stability of the new scheme is assessed.","sentences":["We present a divergence-free semi-implicit finite volume scheme for the simulation of the ideal magnetohydrodynamics (MHD) equations which is stable for large time steps controlled by the local transport speed at all Mach and Alfv\\'en numbers.","An operator splitting technique allows to treat the convective terms explicitly while the hydrodynamic pressure and the magnetic field contributions are integrated implicitly, yielding two decoupled linear implicit systems.","The linearity of the implicit part is achieved by means of a semi-implicit time linearization.","This structure is favorable as second-order accuracy in time can be achieved relying on the class of semi-implicit IMplicit-EXplicit Runge-Kutta (IMEX-RK) methods.","In space, implicit cell-centered finite difference operators are designed to discretely preserve the divergence-free property of the magnetic field on three-dimensional Cartesian meshes.","The new scheme is also particularly well suited for low Mach number flows and for the incompressible limit of the MHD equations, since no explicit numerical dissipation is added to the implicit contribution and the time step is scale independent.","Likewise, highly magnetized flows can benefit from the implicit treatment of the magnetic fluxes, hence improving the computational efficiency of the novel method.","The convective terms undergo a shock-capturing second order finite volume discretization to guarantee the effectiveness of the proposed method even for high Mach number flows.","The new scheme is benchmarked against a series of test cases for the ideal MHD equations addressing different acoustic and Alfv\\'en","Mach number regimes where the performance and the stability of the new scheme is assessed."],"url":"http://arxiv.org/abs/2403.04517v1","category":"math.NA"}
{"created":"2024-03-07 14:18:02","title":"Light-induced giant enhancement of nonreciprocal transport at KTaO3-based interfaces","abstract":"Nonlinear transport is a unique functionality of noncentrosymmetric systems, which reflects profound physics, such as spin-orbit interaction, superconductivity and band geometry. However, it remains highly challenging to enhance the nonreciprocal transport for promising rectification devices. Here, we observe a light-induced giant enhancement of nonreciprocal transport at the superconducting and epitaxial CaZrO3/KTaO3 (111) interfaces. The nonreciprocal transport coefficient undergoes a giant increase with three orders of magnitude up to 105 A-1T-1. Furthermore, a strong Rashba spin-orbit coupling effective field of 14.7 T is achieved with abundant high-mobility photocarriers under ultraviolet illumination, which accounts for the giant enhancement of nonreciprocal transport coefficient. Our first-principles calculations further disclose the stronger Rashba spin-orbit coupling strength and the longer relaxation time in the photocarrier excitation process, bridging the light-property quantitative relationship. Our work provides an alternative pathway to boost nonreciprocal transport in noncentrosymmetric systems and facilitates the promising applications in opto-rectification devices and spin-orbitronic devices.","sentences":["Nonlinear transport is a unique functionality of noncentrosymmetric systems, which reflects profound physics, such as spin-orbit interaction, superconductivity and band geometry.","However, it remains highly challenging to enhance the nonreciprocal transport for promising rectification devices.","Here, we observe a light-induced giant enhancement of nonreciprocal transport at the superconducting and epitaxial CaZrO3/KTaO3 (111) interfaces.","The nonreciprocal transport coefficient undergoes a giant increase with three orders of magnitude up to 105 A-1T-1.","Furthermore, a strong Rashba spin-orbit coupling effective field of 14.7 T is achieved with abundant high-mobility photocarriers under ultraviolet illumination, which accounts for the giant enhancement of nonreciprocal transport coefficient.","Our first-principles calculations further disclose the stronger Rashba spin-orbit coupling strength and the longer relaxation time in the photocarrier excitation process, bridging the light-property quantitative relationship.","Our work provides an alternative pathway to boost nonreciprocal transport in noncentrosymmetric systems and facilitates the promising applications in opto-rectification devices and spin-orbitronic devices."],"url":"http://arxiv.org/abs/2403.04515v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-07 14:07:00","title":"NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems","abstract":"With the advancements of transformer-based architectures, we observe the rise of natural language preprocessing (NLPre) tools capable of solving preliminary NLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or morphological analysis) without any external linguistic guidance. It is arduous to compare novel solutions to well-entrenched preprocessing toolkits, relying on rule-based morphological analysers or dictionaries. Aware of the shortcomings of existing NLPre evaluation approaches, we investigate a novel method of reliable and fair evaluation and performance reporting. Inspired by the GLUE benchmark, the proposed language-centric benchmarking system enables comprehensive ongoing evaluation of multiple NLPre tools, while credibly tracking their performance. The prototype application is configured for Polish and integrated with the thoroughly assembled NLPre-PL benchmark. Based on this benchmark, we conduct an extensive evaluation of a variety of Polish NLPre systems. To facilitate the construction of benchmarking environments for other languages, e.g. NLPre-GA for Irish or NLPre-ZH for Chinese, we ensure full customization of the publicly released source code of the benchmarking system. The links to all the resources (deployed platforms, source code, trained models, datasets etc.) can be found on the project website: https://sites.google.com/view/nlpre-benchmark.","sentences":["With the advancements of transformer-based architectures, we observe the rise of natural language preprocessing (NLPre) tools capable of solving preliminary NLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or morphological analysis) without any external linguistic guidance.","It is arduous to compare novel solutions to well-entrenched preprocessing toolkits, relying on rule-based morphological analysers or dictionaries.","Aware of the shortcomings of existing NLPre evaluation approaches, we investigate a novel method of reliable and fair evaluation and performance reporting.","Inspired by the GLUE benchmark, the proposed language-centric benchmarking system enables comprehensive ongoing evaluation of multiple NLPre tools, while credibly tracking their performance.","The prototype application is configured for Polish and integrated with the thoroughly assembled NLPre-PL benchmark.","Based on this benchmark, we conduct an extensive evaluation of a variety of Polish NLPre systems.","To facilitate the construction of benchmarking environments for other languages, e.g. NLPre-GA for Irish or NLPre-ZH for Chinese, we ensure full customization of the publicly released source code of the benchmarking system.","The links to all the resources (deployed platforms, source code, trained models, datasets etc.) can be found on the project website: https://sites.google.com/view/nlpre-benchmark."],"url":"http://arxiv.org/abs/2403.04507v1","category":"cs.CL"}
{"created":"2024-03-07 14:05:53","title":"Random Evolutionary Dynamics in Predator-Prey Systems Yields Large, Clustered Ecosystems","abstract":"We study the effect of speciation, i.e. the introduction of new species through evolution into communities, in the setting of predator-prey systems. Predator-prey dynamics is classically well modeled by Lotka-Volterra equations, also when multiple predator and prey species co-exist. The consequences of the emergence of new species in such systems are much less well understood. We find that introducing random evolving species leads to robust ecosystems in which large numbers of species coexist. Crucially, in these large ecosystems an emergent clustering of species is observed, tying functional differences to phylogenetic history.","sentences":["We study the effect of speciation, i.e. the introduction of new species through evolution into communities, in the setting of predator-prey systems.","Predator-prey dynamics is classically well modeled by Lotka-Volterra equations, also when multiple predator and prey species co-exist.","The consequences of the emergence of new species in such systems are much less well understood.","We find that introducing random evolving species leads to robust ecosystems in which large numbers of species coexist.","Crucially, in these large ecosystems an emergent clustering of species is observed, tying functional differences to phylogenetic history."],"url":"http://arxiv.org/abs/2403.04506v1","category":"q-bio.PE"}
{"created":"2024-03-07 14:04:38","title":"The importance of the incubation time distribution in compartmental epidemiological models","abstract":"This study investigates the utilization of various mathematical models for comprehending and managing outbreaks of infectious diseases, with a specific focus on how different distributions of incubation times influence predictions regarding epidemics. Two methodologies are examined: a compartmental SEnIR ODE model, which represents an enhanced version of the mean-field SEIR model, and a stochastic agent-based complex network model. Our findings demonstrate that the selection of diverse incubation time distributions can result in noteworthy discrepancies in critical epidemic forecasts, highlighting the crucial role of precise modeling in shaping effective public health interventions. The research underscores the necessity of integrating authentic distribution patterns into epidemic modeling to increase its reliability and applicability","sentences":["This study investigates the utilization of various mathematical models for comprehending and managing outbreaks of infectious diseases, with a specific focus on how different distributions of incubation times influence predictions regarding epidemics.","Two methodologies are examined: a compartmental SEnIR ODE model, which represents an enhanced version of the mean-field SEIR model, and a stochastic agent-based complex network model.","Our findings demonstrate that the selection of diverse incubation time distributions can result in noteworthy discrepancies in critical epidemic forecasts, highlighting the crucial role of precise modeling in shaping effective public health interventions.","The research underscores the necessity of integrating authentic distribution patterns into epidemic modeling to increase its reliability and applicability"],"url":"http://arxiv.org/abs/2403.04505v1","category":"nlin.AO"}
{"created":"2024-03-07 14:03:05","title":"Matched-filter Precoded Rate Splitting Multiple Access: A Simple and Energy-efficient Design","abstract":"We introduce an energy-efficient downlink rate splitting multiple access (RSMA) scheme, employing a simple matched filter (MF) for precoding. We consider a transmitter equipped with multiple antennas, serving several single-antenna users at the same frequency-time resource, each with distinct message requests. Within the conventional 1-layer RSMA framework, requested messages undergo splitting into common and private streams, which are then precoded separately before transmission. In contrast, we propose a novel strategy where only an MF is employed to precode both the common and private streams in RSMA, promising significantly improved energy efficiency and reduced complexity. We demonstrate that this MF-precoded RSMA achieves the same delivery performance as conventional RSMA, where the common stream is beamformed using maximal ratio transmission (MRT) and the private streams are precoded by MF. Taking into account imperfect channel state information at the transmitter, we proceed to analyze the delivery performance of the MF-precoded RSMA. We derive the ergodic rates for decoding the common and private streams at a target user respectively in the massive MIMO regime. Finally, numerical simulations validate the accuracy of our analytical models, as well as demonstrate the advantages over conventional RSMA.","sentences":["We introduce an energy-efficient downlink rate splitting multiple access (RSMA) scheme, employing a simple matched filter (MF) for precoding.","We consider a transmitter equipped with multiple antennas, serving several single-antenna users at the same frequency-time resource, each with distinct message requests.","Within the conventional 1-layer RSMA framework, requested messages undergo splitting into common and private streams, which are then precoded separately before transmission.","In contrast, we propose a novel strategy where only an MF is employed to precode both the common and private streams in RSMA, promising significantly improved energy efficiency and reduced complexity.","We demonstrate that this MF-precoded RSMA achieves the same delivery performance as conventional RSMA, where the common stream is beamformed using maximal ratio transmission (MRT) and the private streams are precoded by MF.","Taking into account imperfect channel state information at the transmitter, we proceed to analyze the delivery performance of the MF-precoded RSMA.","We derive the ergodic rates for decoding the common and private streams at a target user respectively in the massive MIMO regime.","Finally, numerical simulations validate the accuracy of our analytical models, as well as demonstrate the advantages over conventional RSMA."],"url":"http://arxiv.org/abs/2403.04502v1","category":"cs.IT"}
{"created":"2024-03-07 13:48:36","title":"Scalable approximation and solvers for ionic electrodiffusion in cellular geometries","abstract":"The activity and dynamics of excitable cells are fundamentally regulated and moderated by extracellular and intracellular ion concentrations and their electric potentials. The increasing availability of dense reconstructions of excitable tissue at extreme geometric detail pose a new and clear scientific computing challenge for computational modelling of ion dynamics and transport. In this paper, we design, develop and evaluate a scalable numerical algorithm for solving the time-dependent and nonlinear KNP-EMI equations describing ionic electrodiffusion for excitable cells with an explicit geometric representation of intracellular and extracellular compartments and interior interfaces. We also introduce and specify a set of model scenarios of increasing complexity suitable for benchmarking. Our solution strategy is based on an implicit-explicit discretization and linearization in time, a mixed finite element discretization of ion concentrations and electric potentials in intracellular and extracellular domains, and an algebraic multigrid-based, inexact block-diagonal preconditioner for GMRES. Numerical experiments with up to $10^8$ unknowns per time step and up to 256 cores demonstrate that this solution strategy is robust and scalable with respect to the problem size, time discretization and number of cores.","sentences":["The activity and dynamics of excitable cells are fundamentally regulated and moderated by extracellular and intracellular ion concentrations and their electric potentials.","The increasing availability of dense reconstructions of excitable tissue at extreme geometric detail pose a new and clear scientific computing challenge for computational modelling of ion dynamics and transport.","In this paper, we design, develop and evaluate a scalable numerical algorithm for solving the time-dependent and nonlinear KNP-EMI equations describing ionic electrodiffusion for excitable cells with an explicit geometric representation of intracellular and extracellular compartments and interior interfaces.","We also introduce and specify a set of model scenarios of increasing complexity suitable for benchmarking.","Our solution strategy is based on an implicit-explicit discretization and linearization in time, a mixed finite element discretization of ion concentrations and electric potentials in intracellular and extracellular domains, and an algebraic multigrid-based, inexact block-diagonal preconditioner for GMRES.","Numerical experiments with up to $10^8$ unknowns per time step and up to 256 cores demonstrate that this solution strategy is robust and scalable with respect to the problem size, time discretization and number of cores."],"url":"http://arxiv.org/abs/2403.04491v1","category":"math.NA"}
{"created":"2024-03-07 13:47:02","title":"Are prime numbers and quadratic residues random?","abstract":"Appeals to randomness of various number-theoretic constructions are regularly found in scientific publications. It is enough to mention such famous names as V.I.~Arnold, M.~Kac, Ya.G.~Sinai, and T.~Tao. Unfortunately, all this comes down to various, although often quite non-trivial, heuristics. I will describe a novel analytical approach to address this question. As an application, the expected positive answer to the question about the randomness of quadratic residues and the unexpected negative answer in the case of prime numbers will be given. Technically, the proposed approach is based on a fundamentally new construction of the entropy of the trajectory of a dynamical system, which is in some way intermediate between the classical Kolmogorov-Sinai metric entropy and topological entropy.","sentences":["Appeals to randomness of various number-theoretic constructions are regularly found in scientific publications.","It is enough to mention such famous names as V.I.~Arnold, M.~Kac, Ya.G.~Sinai, and T.~Tao.","Unfortunately, all this comes down to various, although often quite non-trivial, heuristics.","I will describe a novel analytical approach to address this question.","As an application, the expected positive answer to the question about the randomness of quadratic residues and the unexpected negative answer in the case of prime numbers will be given.","Technically, the proposed approach is based on a fundamentally new construction of the entropy of the trajectory of a dynamical system, which is in some way intermediate between the classical Kolmogorov-Sinai metric entropy and topological entropy."],"url":"http://arxiv.org/abs/2403.04490v1","category":"math.DS"}
{"created":"2024-03-07 13:44:01","title":"Optimal Denial-of-Service Attacks Against Status Updating","abstract":"In this paper, we investigate denial-of-service attacks against status updating. The target system is modeled by a Markov chain and an unreliable wireless channel, and the performance of status updating in the target system is measured based on two metrics: age of information and age of incorrect information. Our objective is to devise optimal attack policies that strike a balance between the deterioration of the system's performance and the adversary's energy. We model the optimal problem as a Markov decision process and prove rigorously that the optimal jamming policy is a threshold-based policy under both metrics. In addition, we provide a low-complexity algorithm to obtain the optimal threshold value of the jamming policy. Our numerical results show that the networked system with the age-of-incorrect-information metric is less sensitive to jamming attacks than with the age-of-information metric. Index Terms-age of incorrect information, age of information, cyber-physical systems, status updating, remote monitoring.","sentences":["In this paper, we investigate denial-of-service attacks against status updating.","The target system is modeled by a Markov chain and an unreliable wireless channel, and the performance of status updating in the target system is measured based on two metrics: age of information and age of incorrect information.","Our objective is to devise optimal attack policies that strike a balance between the deterioration of the system's performance and the adversary's energy.","We model the optimal problem as a Markov decision process and prove rigorously that the optimal jamming policy is a threshold-based policy under both metrics.","In addition, we provide a low-complexity algorithm to obtain the optimal threshold value of the jamming policy.","Our numerical results show that the networked system with the age-of-incorrect-information metric is less sensitive to jamming attacks than with the age-of-information metric.","Index Terms-age of incorrect information, age of information, cyber-physical systems, status updating, remote monitoring."],"url":"http://arxiv.org/abs/2403.04489v1","category":"cs.IT"}
{"created":"2024-03-07 13:43:34","title":"Dynamics of the Non-equilibrium spin Boson Model: A Benchmark of master equations and their validity","abstract":"In recent years, there has been tremendous focus on identifying whether effective descriptions of open quantum systems such as master equations, provide the correct steady state in the long time limit. The correct steady state is usually not known, however it can be approximated by means of the Mean Force Hamiltonian up to some fixed order, the reaction coordinate mapping or other pseudo-mode like approaches. A few years ago a controversy arose concerning the thermalization of master equations, namely, whether steady state coherences actually arise in systems with composite interactions or if they are some artifact caused by nonpositivity of the effective description, in this manuscript we confirm the existence of such steady state coherences in numerically exact dynamics, and see reminiscences of it in a CPTP map, while the steady state coherence seems to be real its oscillatory characyer seems to be an artifact of second order approximations, indicating that rather than being steady state coherences as one may think, the equilibrium state of the system is not diagonal in the basis of the system Hamiltonian. This paper also shows evidence than comparing Hamiltonian corrections calculated from the mean force approach and the quasi-steady states of dynamical equations may be deceiving, as dynamics may not tend to the state with the correction as shown in this paper.","sentences":["In recent years, there has been tremendous focus on identifying whether effective descriptions of open quantum systems such as master equations, provide the correct steady state in the long time limit.","The correct steady state is usually not known, however it can be approximated by means of the Mean Force Hamiltonian up to some fixed order, the reaction coordinate mapping or other pseudo-mode like approaches.","A few years ago a controversy arose concerning the thermalization of master equations, namely, whether steady state coherences actually arise in systems with composite interactions or if they are some artifact caused by nonpositivity of the effective description, in this manuscript we confirm the existence of such steady state coherences in numerically exact dynamics, and see reminiscences of it in a CPTP map, while the steady state coherence seems to be real its oscillatory characyer seems to be an artifact of second order approximations, indicating that rather than being steady state coherences as one may think, the equilibrium state of the system is not diagonal in the basis of the system Hamiltonian.","This paper also shows evidence than comparing Hamiltonian corrections calculated from the mean force approach and the quasi-steady states of dynamical equations may be deceiving, as dynamics may not tend to the state with the correction as shown in this paper."],"url":"http://arxiv.org/abs/2403.04488v1","category":"quant-ph"}
{"created":"2024-03-07 13:38:40","title":"Entanglement asymmetry and quantum Mpemba effect in two-dimensional free-fermion systems","abstract":"The quantum Mpemba effect is the counter-intuitive non-equilibrium phenomenon wherein the dynamic restoration of a broken symmetry occurs more rapidly when the initial state exhibits a higher degree of symmetry breaking. The effect has been recently discovered theoretically and observed experimentally in the framework of global quantum quenches, but so far it has only been investigated in one-dimensional systems. Here we focus on a two-dimensional free-fermion lattice employing the entanglement asymmetry as a measure of symmetry breaking. Our investigation begins with the ground state analysis of a system featuring nearest-neighbor hoppings and superconducting pairings, the latter breaking explicitly the $U(1)$ particle number symmetry. We compute analytically the entanglement asymmetry of a periodic strip using dimensional reduction, an approach that allows us to adjust the extent of the transverse size, achieving a smooth crossover between one and two dimensions. Further applying the same method, we study the time evolution of the entanglement asymmetry after a quench to a Hamiltonian with only nearest-neighbor hoppings, preserving the particle number symmetry which is restored in the stationary state. We find that the quantum Mpemba effect is strongly affected by the size of the system in the transverse dimension, with the potential to either enhance or spoil the phenomenon depending on the initial states. We establish the conditions for its occurrence based on the properties of the initial configurations, extending the criteria found in the one-dimensional case.","sentences":["The quantum Mpemba effect is the counter-intuitive non-equilibrium phenomenon wherein the dynamic restoration of a broken symmetry occurs more rapidly when the initial state exhibits a higher degree of symmetry breaking.","The effect has been recently discovered theoretically and observed experimentally in the framework of global quantum quenches, but so far it has only been investigated in one-dimensional systems.","Here we focus on a two-dimensional free-fermion lattice employing the entanglement asymmetry as a measure of symmetry breaking.","Our investigation begins with the ground state analysis of a system featuring nearest-neighbor hoppings and superconducting pairings, the latter breaking explicitly the $U(1)$ particle number symmetry.","We compute analytically the entanglement asymmetry of a periodic strip using dimensional reduction, an approach that allows us to adjust the extent of the transverse size, achieving a smooth crossover between one and two dimensions.","Further applying the same method, we study the time evolution of the entanglement asymmetry after a quench to a Hamiltonian with only nearest-neighbor hoppings, preserving the particle number symmetry which is restored in the stationary state.","We find that the quantum Mpemba effect is strongly affected by the size of the system in the transverse dimension, with the potential to either enhance or spoil the phenomenon depending on the initial states.","We establish the conditions for its occurrence based on the properties of the initial configurations, extending the criteria found in the one-dimensional case."],"url":"http://arxiv.org/abs/2403.04486v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-07 13:38:18","title":"Privacy in Cloud Computing through Immersion-based Coding","abstract":"Cloud computing enables users to process and store data remotely on high-performance computers and servers by sharing data over the Internet. However, transferring data to clouds causes unavoidable privacy concerns. Here, we present a synthesis framework to design coding mechanisms that allow sharing and processing data in a privacy-preserving manner without sacrificing data utility and algorithmic performance. We consider the setup where the user aims to run an algorithm in the cloud using private data. The cloud then returns some data utility back to the user (utility refers to the service that the algorithm provides, e.g., classification, prediction, AI models, etc.). To avoid privacy concerns, the proposed scheme provides tools to co-design: 1) coding mechanisms to distort the original data and guarantee a prescribed differential privacy level; 2) an equivalent-but-different algorithm (referred here to as the target algorithm) that runs on distorted data and produces distorted utility; and 3) a decoding function that extracts the true utility from the distorted one with a negligible error. Then, instead of sharing the original data and algorithm with the cloud, only the distorted data and target algorithm are disclosed, thereby avoiding privacy concerns. The proposed scheme is built on the synergy of differential privacy and system immersion tools from control theory. The key underlying idea is to design a higher-dimensional target algorithm that embeds all trajectories of the original algorithm and works on randomly encoded data to produce randomly encoded utility. We show that the proposed scheme can be designed to offer any level of differential privacy without degrading the algorithm's utility. We present two use cases to illustrate the performance of the developed tools: privacy in optimization/learning algorithms and a nonlinear networked control system.","sentences":["Cloud computing enables users to process and store data remotely on high-performance computers and servers by sharing data over the Internet.","However, transferring data to clouds causes unavoidable privacy concerns.","Here, we present a synthesis framework to design coding mechanisms that allow sharing and processing data in a privacy-preserving manner without sacrificing data utility and algorithmic performance.","We consider the setup where the user aims to run an algorithm in the cloud using private data.","The cloud then returns some data utility back to the user (utility refers to the service that the algorithm provides, e.g., classification, prediction, AI models, etc.).","To avoid privacy concerns, the proposed scheme provides tools to co-design: 1) coding mechanisms to distort the original data and guarantee a prescribed differential privacy level; 2) an equivalent-but-different algorithm (referred here to as the target algorithm) that runs on distorted data and produces distorted utility; and 3) a decoding function that extracts the true utility from the distorted one with a negligible error.","Then, instead of sharing the original data and algorithm with the cloud, only the distorted data and target algorithm are disclosed, thereby avoiding privacy concerns.","The proposed scheme is built on the synergy of differential privacy and system immersion tools from control theory.","The key underlying idea is to design a higher-dimensional target algorithm that embeds all trajectories of the original algorithm and works on randomly encoded data to produce randomly encoded utility.","We show that the proposed scheme can be designed to offer any level of differential privacy without degrading the algorithm's utility.","We present two use cases to illustrate the performance of the developed tools: privacy in optimization/learning algorithms and a nonlinear networked control system."],"url":"http://arxiv.org/abs/2403.04485v1","category":"cs.CR"}
{"created":"2024-03-07 13:19:41","title":"Critical quantum metrology robust against dissipation and non-adiabaticity","abstract":"Critical systems near quantum phase transitions were predicted to be useful for improvement of metrological precision, thanks to their ultra-sensitive response to a tiny variation of the control Hamiltonian. Despite the promising perspective, realization of criticality-enhanced quantum metrology is an experimentally challenging task, mainly owing to the extremely long time needed to encode the signal to some physical quantity of a critical system. We here circumvent this problem by making use of the critical behaviors in the Jaynes-Cummings model, comprising a single qubit and a photonic resonator, to which the signal field is coupled. The information about the field amplitude is encoded in the qubit's excitation number in the dark state, which displays a divergent changing rate at the critical point. The most remarkable feature of this critical sensor is that the performance is insensitive to the leakage to bright eigenstates, caused by decoherence and non-adiabatic effects. We demonstrate such a metrological protocol in a superconducting circuit, where an Xmon qubit, interacting with a resonator, is used as a probe for estimating the amplitude of a microwave field coupled to the resonator. The measured quantum Fisher information exhibits a critical quantum enhancement, confirming the potential of this system for quantum metrology.","sentences":["Critical systems near quantum phase transitions were predicted to be useful for improvement of metrological precision, thanks to their ultra-sensitive response to a tiny variation of the control Hamiltonian.","Despite the promising perspective, realization of criticality-enhanced quantum metrology is an experimentally challenging task, mainly owing to the extremely long time needed to encode the signal to some physical quantity of a critical system.","We here circumvent this problem by making use of the critical behaviors in the Jaynes-Cummings model, comprising a single qubit and a photonic resonator, to which the signal field is coupled.","The information about the field amplitude is encoded in the qubit's excitation number in the dark state, which displays a divergent changing rate at the critical point.","The most remarkable feature of this critical sensor is that the performance is insensitive to the leakage to bright eigenstates, caused by decoherence and non-adiabatic effects.","We demonstrate such a metrological protocol in a superconducting circuit, where an Xmon qubit, interacting with a resonator, is used as a probe for estimating the amplitude of a microwave field coupled to the resonator.","The measured quantum Fisher information exhibits a critical quantum enhancement, confirming the potential of this system for quantum metrology."],"url":"http://arxiv.org/abs/2403.04475v1","category":"quant-ph"}
{"created":"2024-03-07 13:11:54","title":"THz-assisted microscopy of silica matrix for biological materials encapsulation: a theoretical and experimental study","abstract":"In this study, we use THz-assisted atom probe tomography (APT) to analyse silica matrices used to encapsulate biomolecules. This technique provides the chemical composition and 3D structure without significantly heating the biosample, which is crucial for studying soft organic molecules such as proteins. Our results show that THz pulses and a positive static field trigger controlled evaporation of silica matrices, enabling 4D imaging with chemical sensitivity comparable to UV laser-assisted APT. To support the interpretation of these experimental results, we devise a computational model based on time-dependent density functional theory to describe the interaction between silica matrices and THz radiation. This model captures the nonlinear dynamics driven by THz-pulses and the interplay between the THz source and the static electric field in real time. This interdisciplinary approach expands the capabilities of APT and holds promise for other THz-based analyses offering new insights into material dynamics in complex biological environments.","sentences":["In this study, we use THz-assisted atom probe tomography (APT) to analyse silica matrices used to encapsulate biomolecules.","This technique provides the chemical composition and 3D structure without significantly heating the biosample, which is crucial for studying soft organic molecules such as proteins.","Our results show that THz pulses and a positive static field trigger controlled evaporation of silica matrices, enabling 4D imaging with chemical sensitivity comparable to UV laser-assisted APT.","To support the interpretation of these experimental results, we devise a computational model based on time-dependent density functional theory to describe the interaction between silica matrices and THz radiation.","This model captures the nonlinear dynamics driven by THz-pulses and the interplay between the THz source and the static electric field in real time.","This interdisciplinary approach expands the capabilities of APT and holds promise for other THz-based analyses offering new insights into material dynamics in complex biological environments."],"url":"http://arxiv.org/abs/2403.04470v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-07 13:09:44","title":"A Magnetic Millirobot Walks on Slippery Biological Surfaces for Targeted Cargo Delivery","abstract":"Small-scale robots hold great potential for targeted cargo delivery in minimally-inv asive medicine. However, current robots often face challenges to locomote efficiently on slip pery biological tissue surfaces, especially when loaded with heavy cargos. Here, we report a magnetic millirobot that can walk on rough and slippery biological tissues by anchoring itself on the soft tissue surface alternatingly with two feet and reciprocally rotating the body to mov e forward. We experimentally studied the locomotion, validated it with numerical simulations and optimized the actuation parameters to fit various terrains and loading conditions. Further more, we developed a permanent magnet set-up to enable wireless actuation within a huma n-scale volume which allows precise control of the millirobot to follow complex trajectories, cl imb vertical walls, and carry cargo up to four times of its own weight. Upon reaching the targ et location, it performs a deployment sequence to release the liquid drug into tissues. The ro bust gait of our millirobot on rough biological terrains, combined with its heavy load capacity, make it a versatile and effective miniaturized vehicle for targeted cargo delivery.","sentences":["Small-scale robots hold great potential for targeted cargo delivery in minimally-inv asive medicine.","However, current robots often face challenges to locomote efficiently on slip pery biological tissue surfaces, especially when loaded with heavy cargos.","Here, we report a magnetic millirobot that can walk on rough and slippery biological tissues by anchoring itself on the soft tissue surface alternatingly with two feet and reciprocally rotating the body to mov e forward.","We experimentally studied the locomotion, validated it with numerical simulations and optimized the actuation parameters to fit various terrains and loading conditions.","Further more, we developed a permanent magnet set-up to enable wireless actuation within a huma n-scale volume which allows precise control of the millirobot to follow complex trajectories, cl imb vertical walls, and carry cargo up to four times of its own weight.","Upon reaching the targ et location, it performs a deployment sequence to release the liquid drug into tissues.","The ro bust gait of our millirobot on rough biological terrains, combined with its heavy load capacity, make it a versatile and effective miniaturized vehicle for targeted cargo delivery."],"url":"http://arxiv.org/abs/2403.04467v1","category":"cs.RO"}
{"created":"2024-03-07 13:03:28","title":"Confronting compositional confusion through the characterisation of the sub-Neptune orbiting HD 77946","abstract":"We report on the detailed characterization of the HD 77946 planetary system. HD 77946 is an F5 ($M_*$ = 1.17 M$_{\\odot}$, $R_*$ = 1.31 R$_{\\odot}$) star, which hosts a transiting planet recently discovered by NASA's Transiting Exoplanet Survey Satellite (TESS), classified as TOI-1778 b. Using TESS photometry, high-resolution spectroscopic data from HARPS-N, and photometry from CHEOPS, we measure the radius and mass from the transit and RV observations, and find that the planet, HD 77946 b, orbits with period $P_{\\rm b}$ = $6.527282_{-0.000020}^{+0.000015}$ d, has a mass of $M_{\\rm b} = 8.38\\pm{1.32}$M$_\\oplus$, and a radius of $R_{\\rm b} = 2.705_{-0.081}^{+0.086}$R$_\\oplus$. From the combination of mass and radius measurements, and the stellar chemical composition, the planet properties suggest that HD 77946 b is a sub-Neptune with a $\\sim$1\\% H/He atmosphere. However, a degeneracy still exists between water-world and silicate/iron-hydrogen models, and even though interior structure modelling of this planet favours a sub-Neptune with a H/He layer that makes up a significant fraction of its radius, a water-world composition cannot be ruled out, as with $T_{\\rm eq} = 1248^{+40}_{-38}~$K, water may be in a supercritical state. The characterisation of HD 77946 b, adding to the small sample of well-characterised sub-Neptunes, is an important step forwards on our journey to understanding planetary formation and evolution pathways. Furthermore, HD 77946 b has one of the highest transmission spectroscopic metrics for small planets orbiting hot stars, thus transmission spectroscopy of this key planet could prove vital for constraining the compositional confusion that currently surrounds small exoplanets.","sentences":["We report on the detailed characterization of the HD 77946 planetary system.","HD 77946 is an F5 ($M_*$ = 1.17 M$_{\\odot}$, $R_*$ = 1.31 R$_{\\odot}$) star, which hosts a transiting planet recently discovered by NASA's Transiting Exoplanet Survey Satellite (TESS), classified as TOI-1778 b. Using TESS photometry, high-resolution spectroscopic data from HARPS-N, and photometry from CHEOPS, we measure the radius and mass from the transit and RV observations, and find that the planet, HD 77946 b, orbits with period $P_{\\rm b}$ = $6.527282_{-0.000020}^{+0.000015}$ d, has a mass of $M_{\\rm b} = 8.38\\pm{1.32}$M$_\\oplus$, and a radius of $R_{\\rm b} = 2.705_{-0.081}^{+0.086}$R$_\\oplus$. From the combination of mass and radius measurements, and the stellar chemical composition, the planet properties suggest that HD 77946 b is a sub-Neptune with a $\\sim$1\\% H/He atmosphere.","However, a degeneracy still exists between water-world and silicate/iron-hydrogen models, and even though interior structure modelling of this planet favours a sub-Neptune with a H/He layer that makes up a significant fraction of its radius, a water-world composition cannot be ruled out, as with $T_{\\rm eq} = 1248^{+40}_{-38}~$K, water may be in a supercritical state.","The characterisation of HD 77946 b, adding to the small sample of well-characterised sub-Neptunes, is an important step forwards on our journey to understanding planetary formation and evolution pathways.","Furthermore, HD 77946 b has one of the highest transmission spectroscopic metrics for small planets orbiting hot stars, thus transmission spectroscopy of this key planet could prove vital for constraining the compositional confusion that currently surrounds small exoplanets."],"url":"http://arxiv.org/abs/2403.04464v1","category":"astro-ph.EP"}
{"created":"2024-03-07 12:57:16","title":"Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset","abstract":"Conversational recommender system is an emerging area that has garnered an increasing interest in the community, especially with the advancements in large language models (LLMs) that enable diverse reasoning over conversational input. Despite the progress, the field has many aspects left to explore. The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations. To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators. We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues. Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide recommendations more relevant to the dialogue context than those in prior datasets.","sentences":["Conversational recommender system is an emerging area that has garnered an increasing interest in the community, especially with the advancements in large language models (LLMs) that enable diverse reasoning over conversational input.","Despite the progress, the field has many aspects left to explore.","The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations.","To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators.","We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues.","Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide recommendations more relevant to the dialogue context than those in prior datasets."],"url":"http://arxiv.org/abs/2403.04460v1","category":"cs.CL"}
{"created":"2024-03-07 12:51:39","title":"On the stability and shadowing of tree-shifts of finite type","abstract":"We investigate relations between the pseudo-orbit-tracing property, topological stability and openness for tree-shifts. We prove that a tree-shift is of finite type if and only if it has the pseudo-orbit-tracing property which implies that the tree-shift is topologically stable and all shift maps are open. We also present an example of a tree-shift for which all shift maps are open but which is not of finite type. It also turns out, that if a topologically stable tree-shift does not have isolated points then it is of finite type.","sentences":["We investigate relations between the pseudo-orbit-tracing property, topological stability and openness for tree-shifts.","We prove that a tree-shift is of finite type if and only if it has the pseudo-orbit-tracing property which implies that the tree-shift is topologically stable and all shift maps are open.","We also present an example of a tree-shift for which all shift maps are open but which is not of finite type.","It also turns out, that if a topologically stable tree-shift does not have isolated points then it is of finite type."],"url":"http://arxiv.org/abs/2403.04456v1","category":"math.DS"}
{"created":"2024-03-07 12:45:51","title":"Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation","abstract":"Existing off-policy reinforcement learning algorithms typically necessitate an explicit state-action-value function representation, which becomes problematic in high-dimensional action spaces. These algorithms often encounter challenges where they struggle with the curse of dimensionality, as maintaining a state-action-value function in such spaces becomes data-inefficient. In this work, we propose a novel off-policy trust region optimization approach, called Vlearn, that eliminates the requirement for an explicit state-action-value function. Instead, we demonstrate how to efficiently leverage just a state-value function as the critic, thus overcoming several limitations of existing methods. By doing so, Vlearn addresses the computational challenges posed by high-dimensional action spaces. Furthermore, Vlearn introduces an efficient approach to address the challenges associated with pure state-value function learning in the off-policy setting. This approach not only simplifies the implementation of off-policy policy gradient algorithms but also leads to consistent and robust performance across various benchmark tasks. Specifically, by removing the need for a state-action-value function Vlearn simplifies the learning process and allows for more efficient exploration and exploitation in complex environments","sentences":["Existing off-policy reinforcement learning algorithms typically necessitate an explicit state-action-value function representation, which becomes problematic in high-dimensional action spaces.","These algorithms often encounter challenges where they struggle with the curse of dimensionality, as maintaining a state-action-value function in such spaces becomes data-inefficient.","In this work, we propose a novel off-policy trust region optimization approach, called Vlearn, that eliminates the requirement for an explicit state-action-value function.","Instead, we demonstrate how to efficiently leverage just a state-value function as the critic, thus overcoming several limitations of existing methods.","By doing so, Vlearn addresses the computational challenges posed by high-dimensional action spaces.","Furthermore, Vlearn introduces an efficient approach to address the challenges associated with pure state-value function learning in the off-policy setting.","This approach not only simplifies the implementation of off-policy policy gradient algorithms but also leads to consistent and robust performance across various benchmark tasks.","Specifically, by removing the need for a state-action-value function Vlearn simplifies the learning process and allows for more efficient exploration and exploitation in complex environments"],"url":"http://arxiv.org/abs/2403.04453v1","category":"cs.LG"}
{"created":"2024-03-07 12:36:24","title":"Continuous-discrete derivative-free extended Kalman filter based on Euler-Maruyama and It\u00f4-Taylor discretizations: Conventional and square-root implementations","abstract":"In this paper, we continue to study the derivative-free extended Kalman filtering (DF-EKF) framework for state estimation of continuous-discrete nonlinear stochastic systems. Having considered the Euler-Maruyama and It\\^{o}-Taylor discretization schemes for solving stochastic differential equations, we derive the related filters' moment equations based on the derivative-free EKF principal. In contrast to the recently derived MATLAB-based continuous-discrete DF-EKF techniques, the novel DF-EKF methods preserve an information about the underlying stochastic process and provide the estimation procedure for a fixed number of iterates at the propagation steps. Additionally, the DF-EKF approach is particularly effective for working with stochastic systems with highly nonlinear and/or nondifferentiable drift and observation functions, but the price to be paid is its degraded numerical stability (to roundoff) compared to the standard EKF framework. To eliminate the mentioned pitfall of the derivative-free EKF methodology, we develop the conventional algorithms together with their stable square-root implementation methods. In contrast to the published DF-EKF results, the new square-root techniques are derived within both the Cholesky and singular value decompositions. A performance of the novel filters is demonstrated on a number of numerical tests including well- and ill-conditioned scenarios.","sentences":["In this paper, we continue to study the derivative-free extended Kalman filtering (DF-EKF) framework for state estimation of continuous-discrete nonlinear stochastic systems.","Having considered the Euler-Maruyama and It\\^{o}-Taylor discretization schemes for solving stochastic differential equations, we derive the related filters' moment equations based on the derivative-free EKF principal.","In contrast to the recently derived MATLAB-based continuous-discrete DF-EKF techniques, the novel DF-EKF methods preserve an information about the underlying stochastic process and provide the estimation procedure for a fixed number of iterates at the propagation steps.","Additionally, the DF-EKF approach is particularly effective for working with stochastic systems with highly nonlinear and/or nondifferentiable drift and observation functions, but the price to be paid is its degraded numerical stability (to roundoff) compared to the standard EKF framework.","To eliminate the mentioned pitfall of the derivative-free EKF methodology, we develop the conventional algorithms together with their stable square-root implementation methods.","In contrast to the published DF-EKF results, the new square-root techniques are derived within both the Cholesky and singular value decompositions.","A performance of the novel filters is demonstrated on a number of numerical tests including well- and ill-conditioned scenarios."],"url":"http://arxiv.org/abs/2403.04448v1","category":"math.NA"}
{"created":"2024-03-07 12:27:08","title":"Classist Tools: Social Class Correlates with Performance in NLP","abstract":"Since the foundational work of William Labov on the social stratification of language (Labov, 1964), linguistics has made concentrated efforts to explore the links between sociodemographic characteristics and language production and perception. But while there is strong evidence for socio-demographic characteristics in language, they are infrequently used in Natural Language Processing (NLP). Age and gender are somewhat well represented, but Labov's original target, socioeconomic status, is noticeably absent. And yet it matters. We show empirically that NLP disadvantages less-privileged socioeconomic groups. We annotate a corpus of 95K utterances from movies with social class, ethnicity and geographical language variety and measure the performance of NLP systems on three tasks: language modelling, automatic speech recognition, and grammar error correction. We find significant performance disparities that can be attributed to socioeconomic status as well as ethnicity and geographical differences. With NLP technologies becoming ever more ubiquitous and quotidian, they must accommodate all language varieties to avoid disadvantaging already marginalised groups. We argue for the inclusion of socioeconomic class in future language technologies.","sentences":["Since the foundational work of William Labov on the social stratification of language (Labov, 1964), linguistics has made concentrated efforts to explore the links between sociodemographic characteristics and language production and perception.","But while there is strong evidence for socio-demographic characteristics in language, they are infrequently used in Natural Language Processing (NLP).","Age and gender are somewhat well represented, but Labov's original target, socioeconomic status, is noticeably absent.","And yet it matters.","We show empirically that NLP disadvantages less-privileged socioeconomic groups.","We annotate a corpus of 95K utterances from movies with social class, ethnicity and geographical language variety and measure the performance of NLP systems on three tasks: language modelling, automatic speech recognition, and grammar error correction.","We find significant performance disparities that can be attributed to socioeconomic status as well as ethnicity and geographical differences.","With NLP technologies becoming ever more ubiquitous and quotidian, they must accommodate all language varieties to avoid disadvantaging already marginalised groups.","We argue for the inclusion of socioeconomic class in future language technologies."],"url":"http://arxiv.org/abs/2403.04445v1","category":"cs.CL"}
{"created":"2024-03-07 12:16:08","title":"Lindbladian approach to $P(E)$ theory","abstract":"The $P(E)$ theory describes inelastic tunneling of particles through junctions integrated into an electric circuit. The inelasticity stems from the interaction between the photonic field of the circuit and the tunneling particles possessing electric charge. In the conventional approach to the $P(E)$ theory, the tunneling rate and the electric current through the junction are derived using Fermi's golden rule. The derivation is carried out to leading order in the tunnel coupling between the leads and includes an averaging over the environmental photonic degrees of freedom. Tracing out the environmental degrees of freedom is also commonly used for open quantum systems when deriving dynamical equations such as the Lindbladian. In this work, we reveal the relations between the $P(E)$ theory and the Lindbladian dynamics. We demonstrate that the assumptions of the Fermi's golden rule are essentially equivalent with the Born-Markovian approximation used in the Lindbladian formalism. The resulting quantum master equation enables us to obtain not only the electric current but various other quantities, including for instance the heat current parametrized by the same $P(E)$ function, in a systematic and convenient way.","sentences":["The $P(E)$ theory describes inelastic tunneling of particles through junctions integrated into an electric circuit.","The inelasticity stems from the interaction between the photonic field of the circuit and the tunneling particles possessing electric charge.","In the conventional approach to the $P(E)$ theory, the tunneling rate and the electric current through the junction are derived using Fermi's golden rule.","The derivation is carried out to leading order in the tunnel coupling between the leads and includes an averaging over the environmental photonic degrees of freedom.","Tracing out the environmental degrees of freedom is also commonly used for open quantum systems when deriving dynamical equations such as the Lindbladian.","In this work, we reveal the relations between the $P(E)$ theory and the Lindbladian dynamics.","We demonstrate that the assumptions of the Fermi's golden rule are essentially equivalent with the Born-Markovian approximation used in the Lindbladian formalism.","The resulting quantum master equation enables us to obtain not only the electric current but various other quantities, including for instance the heat current parametrized by the same $P(E)$ function, in a systematic and convenient way."],"url":"http://arxiv.org/abs/2403.04441v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-07 12:15:29","title":"RoboKube: Establishing a New Foundation for the Cloud Native Evolution in Robotics","abstract":"Cloud native technologies have been observed to expand into the realm of Internet of Things (IoT) and Cyber-physical Systems, of which an important application domain is robotics. In this paper, we review the cloudification practice in the robotics domain from both literature and industrial perspectives. We propose RoboKube, an adaptive framework that is based on the Kubernetes (K8s) ecosystem to set up a common platform across the device-cloud continuum for the deployment of cloudified Robotic Operating System (ROS) powered applications, to facilitate the cloud native evolution in robotics. We examine the process of modernizing ROS applications using cloud-native technologies, focusing on both the platform and application perspectives. In addition, we address the challenges of networking setups for heterogeneous environments. This paper intends to serves as a guide for developers and researchers, offering insights into containerization strategies, ROS node distribution and clustering, and deployment options. To demonstrate the feasibility of our approach, we present a case study involving the cloudification of a teleoperation testbed.","sentences":["Cloud native technologies have been observed to expand into the realm of Internet of Things (IoT) and Cyber-physical Systems, of which an important application domain is robotics.","In this paper, we review the cloudification practice in the robotics domain from both literature and industrial perspectives.","We propose RoboKube, an adaptive framework that is based on the Kubernetes (K8s) ecosystem to set up a common platform across the device-cloud continuum for the deployment of cloudified Robotic Operating System (ROS) powered applications, to facilitate the cloud native evolution in robotics.","We examine the process of modernizing ROS applications using cloud-native technologies, focusing on both the platform and application perspectives.","In addition, we address the challenges of networking setups for heterogeneous environments.","This paper intends to serves as a guide for developers and researchers, offering insights into containerization strategies, ROS node distribution and clustering, and deployment options.","To demonstrate the feasibility of our approach, we present a case study involving the cloudification of a teleoperation testbed."],"url":"http://arxiv.org/abs/2403.04440v1","category":"cs.RO"}
{"created":"2024-03-07 12:13:22","title":"Maximizing Slice-Volumes of Semialgebraic Sets using Sum-of-Squares Programming","abstract":"This paper presents an algorithm to maximize the volume of an affine slice through a given semialgebraic set. This slice-volume task is formulated as an infinite-dimensional linear program in continuous functions, inspired by prior work in volume computation of semialgebraic sets. A convergent sequence of upper-bounds to the maximal slice volume are computed using the moment-Sum-of-Squares hierarchy of semidefinite programs in increasing size. The computational complexity of this scheme can be reduced by utilizing topological structure (in dimensions 2, 3, 4, 8) and symmetry. This numerical convergence can be accelerated through the introduction of redundant Stokes-based constraints. Demonstrations of slice-volume calculation are performed on example sets.","sentences":["This paper presents an algorithm to maximize the volume of an affine slice through a given semialgebraic set.","This slice-volume task is formulated as an infinite-dimensional linear program in continuous functions, inspired by prior work in volume computation of semialgebraic sets.","A convergent sequence of upper-bounds to the maximal slice volume are computed using the moment-Sum-of-Squares hierarchy of semidefinite programs in increasing size.","The computational complexity of this scheme can be reduced by utilizing topological structure (in dimensions 2, 3, 4, 8) and symmetry.","This numerical convergence can be accelerated through the introduction of redundant Stokes-based constraints.","Demonstrations of slice-volume calculation are performed on example sets."],"url":"http://arxiv.org/abs/2403.04438v1","category":"math.OC"}
{"created":"2024-03-07 12:09:36","title":"Pilot Spoofing Attack on the Downlink of Cell-Free Massive MIMO: From the Perspective of Adversaries","abstract":"The channel hardening effect is less pronounced in the cell-free massive multiple-input multiple-output (mMIMO) system compared to its cellular counterpart, making it necessary to estimate the downlink effective channel gains to ensure decent performance. However, the downlink training inadvertently creates an opportunity for adversarial nodes to launch pilot spoofing attacks (PSAs). First, we demonstrate that adversarial distributed access points (APs) can severely degrade the achievable downlink rate. They achieve this by estimating their channels to users in the uplink training phase and then precoding and sending the same pilot sequences as those used by legitimate APs during the downlink training phase. Then, the impact of the downlink PSA is investigated by rigorously deriving a closed-form expression of the per-user achievable downlink rate. By employing the min-max criterion to optimize the power allocation coefficients, the maximum per-user achievable rate of downlink transmission is minimized from the perspective of adversarial APs. As an alternative to the downlink PSA, adversarial APs may opt to precode random interference during the downlink data transmission phase in order to disrupt legitimate communications. In this scenario, the achievable downlink rate is derived, and then power optimization algorithms are also developed. We present numerical results to showcase the detrimental impact of the downlink PSA and compare the effects of these two types of attacks.","sentences":["The channel hardening effect is less pronounced in the cell-free massive multiple-input multiple-output (mMIMO) system compared to its cellular counterpart, making it necessary to estimate the downlink effective channel gains to ensure decent performance.","However, the downlink training inadvertently creates an opportunity for adversarial nodes to launch pilot spoofing attacks (PSAs).","First, we demonstrate that adversarial distributed access points (APs) can severely degrade the achievable downlink rate.","They achieve this by estimating their channels to users in the uplink training phase and then precoding and sending the same pilot sequences as those used by legitimate APs during the downlink training phase.","Then, the impact of the downlink PSA is investigated by rigorously deriving a closed-form expression of the per-user achievable downlink rate.","By employing the min-max criterion to optimize the power allocation coefficients, the maximum per-user achievable rate of downlink transmission is minimized from the perspective of adversarial APs.","As an alternative to the downlink PSA, adversarial APs may opt to precode random interference during the downlink data transmission phase in order to disrupt legitimate communications.","In this scenario, the achievable downlink rate is derived, and then power optimization algorithms are also developed.","We present numerical results to showcase the detrimental impact of the downlink PSA and compare the effects of these two types of attacks."],"url":"http://arxiv.org/abs/2403.04435v1","category":"cs.IT"}
{"created":"2024-03-07 12:03:04","title":"Boosting Fairness and Robustness in Over-the-Air Federated Learning","abstract":"Over-the-Air Computation is a beyond-5G communication strategy that has recently been shown to be useful for the decentralized training of machine learning models due to its efficiency. In this paper, we propose an Over-the-Air federated learning algorithm that aims to provide fairness and robustness through minmax optimization. By using the epigraph form of the problem at hand, we show that the proposed algorithm converges to the optimal solution of the minmax problem. Moreover, the proposed approach does not require reconstructing channel coefficients by complex encoding-decoding schemes as opposed to state-of-the-art approaches. This improves both efficiency and privacy.","sentences":["Over-the-Air Computation is a beyond-5G communication strategy that has recently been shown to be useful for the decentralized training of machine learning models due to its efficiency.","In this paper, we propose an Over-the-Air federated learning algorithm that aims to provide fairness and robustness through minmax optimization.","By using the epigraph form of the problem at hand, we show that the proposed algorithm converges to the optimal solution of the minmax problem.","Moreover, the proposed approach does not require reconstructing channel coefficients by complex encoding-decoding schemes as opposed to state-of-the-art approaches.","This improves both efficiency and privacy."],"url":"http://arxiv.org/abs/2403.04431v1","category":"cs.LG"}
{"created":"2024-03-07 11:59:00","title":"Exploring the Influence of Dimensionality Reduction on Anomaly Detection Performance in Multivariate Time Series","abstract":"This paper presents an extensive empirical study on the integration of dimensionality reduction techniques with advanced unsupervised time series anomaly detection models, focusing on the MUTANT and Anomaly-Transformer models. The study involves a comprehensive evaluation across three different datasets: MSL, SMAP, and SWaT. Each dataset poses unique challenges, allowing for a robust assessment of the models' capabilities in varied contexts. The dimensionality reduction techniques examined include PCA, UMAP, Random Projection, and t-SNE, each offering distinct advantages in simplifying high-dimensional data. Our findings reveal that dimensionality reduction not only aids in reducing computational complexity but also significantly enhances anomaly detection performance in certain scenarios. Moreover, a remarkable reduction in training times was observed, with reductions by approximately 300\\% and 650\\% when dimensionality was halved and minimized to the lowest dimensions, respectively. This efficiency gain underscores the dual benefit of dimensionality reduction in both performance enhancement and operational efficiency. The MUTANT model exhibits notable adaptability, especially with UMAP reduction, while the Anomaly-Transformer demonstrates versatility across various reduction techniques. These insights provide a deeper understanding of the synergistic effects of dimensionality reduction and anomaly detection, contributing valuable perspectives to the field of time series analysis. The study underscores the importance of selecting appropriate dimensionality reduction strategies based on specific model requirements and dataset characteristics, paving the way for more efficient, accurate, and scalable solutions in anomaly detection.","sentences":["This paper presents an extensive empirical study on the integration of dimensionality reduction techniques with advanced unsupervised time series anomaly detection models, focusing on the MUTANT and Anomaly-Transformer models.","The study involves a comprehensive evaluation across three different datasets: MSL, SMAP, and SWaT. Each dataset poses unique challenges, allowing for a robust assessment of the models' capabilities in varied contexts.","The dimensionality reduction techniques examined include PCA, UMAP, Random Projection, and t-SNE, each offering distinct advantages in simplifying high-dimensional data.","Our findings reveal that dimensionality reduction not only aids in reducing computational complexity but also significantly enhances anomaly detection performance in certain scenarios.","Moreover, a remarkable reduction in training times was observed, with reductions by approximately 300\\% and 650\\% when dimensionality was halved and minimized to the lowest dimensions, respectively.","This efficiency gain underscores the dual benefit of dimensionality reduction in both performance enhancement and operational efficiency.","The MUTANT model exhibits notable adaptability, especially with UMAP reduction, while the Anomaly-Transformer demonstrates versatility across various reduction techniques.","These insights provide a deeper understanding of the synergistic effects of dimensionality reduction and anomaly detection, contributing valuable perspectives to the field of time series analysis.","The study underscores the importance of selecting appropriate dimensionality reduction strategies based on specific model requirements and dataset characteristics, paving the way for more efficient, accurate, and scalable solutions in anomaly detection."],"url":"http://arxiv.org/abs/2403.04429v1","category":"cs.LG"}
{"created":"2024-03-07 11:52:59","title":"Broadband transparent Huygens' spaceplates","abstract":"Spaceplates have emerged in the context of nonlocal metasurfaces, enabling the compression of optical systems by minimizing the required empty space between their components. In this work, we design and analyze spaceplates that support resonances with opposite symmetries, operating under the so-called Huygens' condition. Using the temporal coupled-mode theory, we demonstrate that the spatial compression provided by Huygens' spaceplates is twice that of conventional single-resonance counterparts. Additionally, they can support broader operational bandwidths and numerical apertures, facilitating the reduction of chromatic aberrations. Moreover, Huygens' spaceplates maintain nearly full transparency over a wide frequency and angular range, allowing their straightforward cascading for multi-frequency broadband operation. Finally, we propose a physical implementation of a Huygens' spaceplate for optical frequencies based on a photonic crystal slab geometry.","sentences":["Spaceplates have emerged in the context of nonlocal metasurfaces, enabling the compression of optical systems by minimizing the required empty space between their components.","In this work, we design and analyze spaceplates that support resonances with opposite symmetries, operating under the so-called Huygens' condition.","Using the temporal coupled-mode theory, we demonstrate that the spatial compression provided by Huygens' spaceplates is twice that of conventional single-resonance counterparts.","Additionally, they can support broader operational bandwidths and numerical apertures, facilitating the reduction of chromatic aberrations.","Moreover, Huygens' spaceplates maintain nearly full transparency over a wide frequency and angular range, allowing their straightforward cascading for multi-frequency broadband operation.","Finally, we propose a physical implementation of a Huygens' spaceplate for optical frequencies based on a photonic crystal slab geometry."],"url":"http://arxiv.org/abs/2403.04425v1","category":"physics.optics"}
{"created":"2024-03-07 11:37:47","title":"Adaptive Memory Procedure for Solving Real-world Vehicle Routing Problem","abstract":"Logistics and transport are core of many industrial and business processes. One of the most promising segments in the field is optimisation of vehicle routes. Scientific effort is focused primarily on algorithms developed in simplified environment and cover a fraction of real industrial application due to complex combinatorial algorithms required to be promptly executed. In this paper, a real-world case study in all its complexity is observed and formulated as a real-world vehicle routing problem (VRP). To be able to computationally cope with the complexity, we propose a new procedure based on adaptive memory metaheuristic combined with local search. The initial solution is obtained with Clarke-Wright algorithm extended here by introducing a dropout factor to include a required stochastic attribute. The procedure and corresponding algorithms are tested on the existing benchmarks and further on the real industrial case study, which considers capacities, time windows, soft time windows, heterogeneous vehicles, dynamic fuel consumption, multi-trip delivery, crew skills, split delivery and, finally, time-dependent routes as the most significant factor. In comparison with the current state-of-the-art algorithms for vehicle routing problem with a large number of constraints, we obtain an average savings of 2.03% in delivery time and 20.98% in total delivery costs.","sentences":["Logistics and transport are core of many industrial and business processes.","One of the most promising segments in the field is optimisation of vehicle routes.","Scientific effort is focused primarily on algorithms developed in simplified environment and cover a fraction of real industrial application due to complex combinatorial algorithms required to be promptly executed.","In this paper, a real-world case study in all its complexity is observed and formulated as a real-world vehicle routing problem (VRP).","To be able to computationally cope with the complexity, we propose a new procedure based on adaptive memory metaheuristic combined with local search.","The initial solution is obtained with Clarke-Wright algorithm extended here by introducing a dropout factor to include a required stochastic attribute.","The procedure and corresponding algorithms are tested on the existing benchmarks and further on the real industrial case study, which considers capacities, time windows, soft time windows, heterogeneous vehicles, dynamic fuel consumption, multi-trip delivery, crew skills, split delivery and, finally, time-dependent routes as the most significant factor.","In comparison with the current state-of-the-art algorithms for vehicle routing problem with a large number of constraints, we obtain an average savings of 2.03% in delivery time and 20.98% in total delivery costs."],"url":"http://arxiv.org/abs/2403.04420v1","category":"math.OC"}
{"created":"2024-03-07 11:18:43","title":"Sharp estimates for convolution operators associated to hypersurfaces in $\\mathbb{R}^3$ with height $h\\le2$","abstract":"In this article, we study the convolution operator $M_k$ with oscillatory kernel, which is related with solutions to the Cauchy problem for the strictly hyperbolic equations. The operator $M_k$ is associated to the characteristic hypersurface $\\Sigma\\subset \\mathbb{R}^3$ of the equation and the smooth amplitude function, which is homogeneous of order $-k$ for large values of the argument. We study the convolution operators assuming that the support of the corresponding amplitude function is contained in a sufficiently small conic neighborhood of a given point $v\\in \\Sigma$ at which the height of the surface is less or equal to two. Such class contains surfaces related to simple and the $X_9, \\, J_{10}$ type singularities in the sense of Arnol'd's classification. Denoting by $k_p$ the minimal exponent such that $M_k$ is $L^p\\mapsto L^{p'}$-bounded for $k>k_p,$ we show that the number $k_p$ depends on some discrete characteristics of the Newton polygon of a smooth function constructed in an appropriate coordinate system.","sentences":["In this article, we study the convolution operator $M_k$ with oscillatory kernel, which is related with solutions to the Cauchy problem for the strictly hyperbolic equations.","The operator $M_k$ is associated to the characteristic hypersurface $\\Sigma\\subset \\mathbb{R}^3$ of the equation and the smooth amplitude function, which is homogeneous of order $-k$ for large values of the argument.","We study the convolution operators assuming that the support of the corresponding amplitude function is contained in a sufficiently small conic neighborhood of a given point $v\\in \\Sigma$ at which the height of the surface is less or equal to two.","Such class contains surfaces related to simple and the $X_9, \\, J_{10}$ type singularities in the sense of Arnol'd's classification.","Denoting by $k_p$ the minimal exponent such that $M_k$ is $L^p\\mapsto L^{p'}$-bounded for $k>k_p,$ we show that the number $k_p$ depends on some discrete characteristics of the Newton polygon of a smooth function constructed in an appropriate coordinate system."],"url":"http://arxiv.org/abs/2403.04413v1","category":"math.AP"}
{"created":"2024-03-07 11:03:53","title":"Regularity near the fixed boundary for transmission systems","abstract":"Given $\\Omega\\subset \\mathbb{R}^n$ with $n\\geq 2$, $D\\subset \\Omega$ open, and $u:\\Omega \\to \\mathbb{R}^m$, we study elliptic systems of the type $$ {\\rm div} \\big( ( A + (B- A)\\chi_D)\\nabla u\\big) = 0 \\quad \\text{in $\\Omega\\cap B_1$,} $$ for some uniformly elliptic tensors $A$ and $B$ with H\\\"{o}lder continuous entries. We show that, given appropriate boundary data, the Lipschitz regularity of $u$ inside $B_1 \\cap D$ is transmitted to $B_{1/2}\\cap \\Omega$ up to the boundary of $\\Omega$. This corresponds to the boundary counterpart of the interior regularity results in Figalli-Kim-Shahgholian, Nonlinear Anal. 2022.","sentences":["Given $\\Omega\\subset \\mathbb{R}^n$ with $n\\geq 2$, $D\\subset \\Omega$ open, and $u:\\Omega \\to \\mathbb{R}^m$, we study elliptic systems of the type $$ {\\rm div} \\big( ( A + (B- A)\\chi_D)\\nabla u\\big) = 0","\\quad \\text{in $\\Omega\\cap B_1$,} $$ for some uniformly elliptic tensors $A$ and $B$ with H\\\"{o}lder continuous entries.","We show that, given appropriate boundary data, the Lipschitz regularity of $u$ inside $B_1 \\cap D$ is transmitted to $B_{1/2}\\cap \\Omega$ up to the boundary of $\\Omega$. This corresponds to the boundary counterpart of the interior regularity results in Figalli-Kim-Shahgholian, Nonlinear Anal. 2022."],"url":"http://arxiv.org/abs/2403.04406v1","category":"math.AP"}
{"created":"2024-03-07 11:00:35","title":"Signature Isolation Forest","abstract":"Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly Detection (AD) algorithm designed for functional data. It relies on a tree partition procedure where an abnormality score is computed by projecting each curve observation on a drawn dictionary through a linear inner product. Such linear inner product and the dictionary are a priori choices that highly influence the algorithm's performances and might lead to unreliable results, particularly with complex datasets. This work addresses these challenges by introducing \\textit{Signature Isolation Forest}, a novel AD algorithm class leveraging the rough path theory's signature transform. Our objective is to remove the constraints imposed by FIF through the proposition of two algorithms which specifically target the linearity of the FIF inner product and the choice of the dictionary. We provide several numerical experiments, including a real-world applications benchmark showing the relevance of our methods.","sentences":["Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly Detection (AD) algorithm designed for functional data.","It relies on a tree partition procedure where an abnormality score is computed by projecting each curve observation on a drawn dictionary through a linear inner product.","Such linear inner product and the dictionary are a priori choices that highly influence the algorithm's performances and might lead to unreliable results, particularly with complex datasets.","This work addresses these challenges by introducing \\textit{Signature Isolation Forest}, a novel AD algorithm class leveraging the rough path theory's signature transform.","Our objective is to remove the constraints imposed by FIF through the proposition of two algorithms which specifically target the linearity of the FIF inner product and the choice of the dictionary.","We provide several numerical experiments, including a real-world applications benchmark showing the relevance of our methods."],"url":"http://arxiv.org/abs/2403.04405v1","category":"stat.ML"}
{"created":"2024-03-07 10:30:15","title":"Model-based pressure tracking using a feedback linearisation technique in thermoplastic injection moulding","abstract":"Injection moulding is a well-established automated process for manufacturing a wide variety of plastic components in large volumes and with high precision. There are, however, process control challenges associated with each stage of injection moulding, which should be monitored and controlled precisely to prevent defects in the injection moulded component. One of the process variables is the pressure profile during the injection and packing phases, which has a direct impact on the quality of the manufactured part. This research proposes a model-based controller design for the injection and cavity pressure during the moulding cycle, with a feedback linearisation controller. First, the injection and packing phases were mathematically modelled and converted to a state-space model. The procedure of designing the controller for the process was outlined. A pressure profile was defined as the target trajectory in the proposed controller and the ability of the designed controller to follow the set profile was explored.","sentences":["Injection moulding is a well-established automated process for manufacturing a wide variety of plastic components in large volumes and with high precision.","There are, however, process control challenges associated with each stage of injection moulding, which should be monitored and controlled precisely to prevent defects in the injection moulded component.","One of the process variables is the pressure profile during the injection and packing phases, which has a direct impact on the quality of the manufactured part.","This research proposes a model-based controller design for the injection and cavity pressure during the moulding cycle, with a feedback linearisation controller.","First, the injection and packing phases were mathematically modelled and converted to a state-space model.","The procedure of designing the controller for the process was outlined.","A pressure profile was defined as the target trajectory in the proposed controller and the ability of the designed controller to follow the set profile was explored."],"url":"http://arxiv.org/abs/2403.04388v1","category":"eess.SY"}
{"created":"2024-03-07 10:23:39","title":"HeROS: a miniaturised platform for research and development on Heterogeneous RObotic Systems","abstract":"Tests and prototyping are vital in the research and development of robotic systems. Work with target hardware is problematic. Hence, in the article, a low-cost, miniaturised physical platform is presented to deal with experiments on heterogeneous robotic systems. The platform comprises a physical board with tiles of the standardised base, diverse mobile robots, and manipulation robots. The number of exemplary applications validates the usefulness of the solution.","sentences":["Tests and prototyping are vital in the research and development of robotic systems.","Work with target hardware is problematic.","Hence, in the article, a low-cost, miniaturised physical platform is presented to deal with experiments on heterogeneous robotic systems.","The platform comprises a physical board with tiles of the standardised base, diverse mobile robots, and manipulation robots.","The number of exemplary applications validates the usefulness of the solution."],"url":"http://arxiv.org/abs/2403.04384v1","category":"cs.RO"}
{"created":"2024-03-07 10:22:42","title":"Jaynes-Cummings interaction with a traveling light pulse","abstract":"The Jaynes-Cummings model provides a simple and accurate description of the interaction between a two-level quantum emitter and a single mode of quantum radiation. Due to the multimode continuum of eigenmodes in free space and in waveguides, the Jaynes-Cummings model should not be expected to properly describe the interaction between an emitter and a traveling pulse of quantum radiation. In this article, we review a cascaded quantum system approach that accurately describes the interaction of a quantum system with an incident quantum pulse of radiation. This approach leads to different formulations of the theory, each of a similar structure as the Jaynes-Cummings model but with important modifications.","sentences":["The Jaynes-Cummings model provides a simple and accurate description of the interaction between a two-level quantum emitter and a single mode of quantum radiation.","Due to the multimode continuum of eigenmodes in free space and in waveguides, the Jaynes-Cummings model should not be expected to properly describe the interaction between an emitter and a traveling pulse of quantum radiation.","In this article, we review a cascaded quantum system approach that accurately describes the interaction of a quantum system with an incident quantum pulse of radiation.","This approach leads to different formulations of the theory, each of a similar structure as the Jaynes-Cummings model but with important modifications."],"url":"http://arxiv.org/abs/2403.04383v1","category":"quant-ph"}
{"created":"2024-03-07 10:10:34","title":"CARISMA: CAR-Integrated Service Mesh Architecture","abstract":"The amount of software in modern cars is increasing continuously with traditional electric/electronic (E/E) architectures reaching their limit when deploying complex applications, e.g., regarding bandwidth or computational power. To mitigate this situation, more powerful computing platforms are being employed and applications are developed as distributed applications, e.g., involving microservices. Microservices received widespread adoption and changed the way modern applications are developed. However, they also introduce additional complexity regarding inter-service communication. This has led to the emergence of service meshes, a promising approach to cope with this complexity. In this paper, we present an architecture applying the service mesh approach to automotive E/E platforms comprising multiple interlinked High-Performance Computers (HPCs). We validate the feasibility of our approach through a prototypical implementation.","sentences":["The amount of software in modern cars is increasing continuously with traditional electric/electronic (E/E) architectures reaching their limit when deploying complex applications, e.g., regarding bandwidth or computational power.","To mitigate this situation, more powerful computing platforms are being employed and applications are developed as distributed applications, e.g., involving microservices.","Microservices received widespread adoption and changed the way modern applications are developed.","However, they also introduce additional complexity regarding inter-service communication.","This has led to the emergence of service meshes, a promising approach to cope with this complexity.","In this paper, we present an architecture applying the service mesh approach to automotive E/E platforms comprising multiple interlinked High-Performance Computers (HPCs).","We validate the feasibility of our approach through a prototypical implementation."],"url":"http://arxiv.org/abs/2403.04378v1","category":"cs.DC"}
{"created":"2024-03-07 09:58:59","title":"Cooperative Task Execution in Multi-Agent Systems","abstract":"We propose a multi-agent system that enables groups of agents to collaborate and work autonomously to execute tasks. Groups can work in a decentralized manner and can adapt to dynamic changes in the environment. Groups of agents solve assigned tasks by exploring the solution space cooperatively based on the highest reward first. The tasks have a dependency structure associated with them. We rigorously evaluated the performance of the system and the individual group performance using centralized and decentralized control approaches for task distribution. Based on the results, the centralized approach is more efficient for systems with a less-dependent system $G_{18}$, while the decentralized approach performs better for systems with a highly-dependent system $G_{40}$. We also evaluated task allocation to groups that do not have interdependence. Our findings reveal that there was significantly less difference in the number of tasks allocated to each group in a less-dependent system than in a highly-dependent one. The experimental results showed that a large number of small-size cooperative groups of agents unequivocally improved the system's performance compared to a small number of large-size cooperative groups of agents. Therefore, it is essential to identify the optimal group size for a system to enhance its performance.","sentences":["We propose a multi-agent system that enables groups of agents to collaborate and work autonomously to execute tasks.","Groups can work in a decentralized manner and can adapt to dynamic changes in the environment.","Groups of agents solve assigned tasks by exploring the solution space cooperatively based on the highest reward first.","The tasks have a dependency structure associated with them.","We rigorously evaluated the performance of the system and the individual group performance using centralized and decentralized control approaches for task distribution.","Based on the results, the centralized approach is more efficient for systems with a less-dependent system $G_{18}$, while the decentralized approach performs better for systems with a highly-dependent system $G_{40}$. We also evaluated task allocation to groups that do not have interdependence.","Our findings reveal that there was significantly less difference in the number of tasks allocated to each group in a less-dependent system than in a highly-dependent one.","The experimental results showed that a large number of small-size cooperative groups of agents unequivocally improved the system's performance compared to a small number of large-size cooperative groups of agents.","Therefore, it is essential to identify the optimal group size for a system to enhance its performance."],"url":"http://arxiv.org/abs/2403.04370v1","category":"cs.MA"}
{"created":"2024-03-07 09:56:56","title":"Learning to Remove Wrinkled Transparent Film with Polarized Prior","abstract":"In this paper, we study a new problem, Film Removal (FR), which attempts to remove the interference of wrinkled transparent films and reconstruct the original information under films for industrial recognition systems. We first physically model the imaging of industrial materials covered by the film. Considering the specular highlight from the film can be effectively recorded by the polarized camera, we build a practical dataset with polarization information containing paired data with and without transparent film. We aim to remove interference from the film (specular highlights and other degradations) with an end-to-end framework. To locate the specular highlight, we use an angle estimation network to optimize the polarization angle with the minimized specular highlight. The image with minimized specular highlight is set as a prior for supporting the reconstruction network. Based on the prior and the polarized images, the reconstruction network can decouple all degradations from the film. Extensive experiments show that our framework achieves SOTA performance in both image reconstruction and industrial downstream tasks. Our code will be released at \\url{https://github.com/jqtangust/FilmRemoval}.","sentences":["In this paper, we study a new problem, Film Removal (FR), which attempts to remove the interference of wrinkled transparent films and reconstruct the original information under films for industrial recognition systems.","We first physically model the imaging of industrial materials covered by the film.","Considering the specular highlight from the film can be effectively recorded by the polarized camera, we build a practical dataset with polarization information containing paired data with and without transparent film.","We aim to remove interference from the film (specular highlights and other degradations) with an end-to-end framework.","To locate the specular highlight, we use an angle estimation network to optimize the polarization angle with the minimized specular highlight.","The image with minimized specular highlight is set as a prior for supporting the reconstruction network.","Based on the prior and the polarized images, the reconstruction network can decouple all degradations from the film.","Extensive experiments show that our framework achieves SOTA performance in both image reconstruction and industrial downstream tasks.","Our code will be released at \\url{https://github.com/jqtangust/FilmRemoval}."],"url":"http://arxiv.org/abs/2403.04368v1","category":"cs.CV"}
{"created":"2024-03-07 09:44:26","title":"Signatures of an $\u03b1$ + core structure in $^{44}$Ti + $^{44}$Ti collisions at $\\sqrt{s_{NN}}=5.02$ TeV by a multiphase transport model","abstract":"It is important to understand whether $\\alpha$-clustering structures can leave traces in ultra-relativistic heavy ion collisions. Using the modified AMPT model, we simulate three $\\alpha$ + core configurations of $^{44}$Ti in $^{44}$Ti+$^{44}$Ti collisions at $\\sqrt{s_{NN}}=5.02$ TeV as well as other systems with Woods-Saxon structures. One of these configurations has no additional constraint, but the other two have the Mott density edge $r_{\\mathrm{Mott}}$ set as either a lower or upper bound on the cluster position $r_{\\alpha}$ to check the influence of $\\alpha$ dissolution. This is the first time that the initial stage of the geometric properties in heavy-ion collisions has been configured using the traditional treatment of the nuclear structure. We compare the radial nucleon density, multiplicity distribution, transverse momentum spectra, eccentricity, triangularity, elliptic flow and triangular flow of these six systems. $\\alpha$ + core structures can alter all these observations especially in the most-central collisions, among which elliptic flow is the most hopeful as a probe of such structures.","sentences":["It is important to understand whether $\\alpha$-clustering structures can leave traces in ultra-relativistic heavy ion collisions.","Using the modified AMPT model, we simulate three $\\alpha$ + core configurations of $^{44}$Ti in $^{44}$Ti+$^{44}$Ti collisions at $\\sqrt{s_{NN}}=5.02$ TeV as well as other systems with Woods-Saxon structures.","One of these configurations has no additional constraint, but the other two have the Mott density edge $r_{\\mathrm{Mott}}$ set as either a lower or upper bound on the cluster position $r_{\\alpha}$ to check the influence of $\\alpha$ dissolution.","This is the first time that the initial stage of the geometric properties in heavy-ion collisions has been configured using the traditional treatment of the nuclear structure.","We compare the radial nucleon density, multiplicity distribution, transverse momentum spectra, eccentricity, triangularity, elliptic flow and triangular flow of these six systems.","$\\alpha$ + core structures can alter all these observations especially in the most-central collisions, among which elliptic flow is the most hopeful as a probe of such structures."],"url":"http://arxiv.org/abs/2403.04362v1","category":"nucl-th"}
{"created":"2024-03-07 09:39:15","title":"IMU Tracking of Kinematic Chains in the Absence of Gravitational and Magnetic Fields","abstract":"Tracking kinematic chains has many uses from healthcare to virtual reality. Inertial measurement units, IMUs, are well-recognised for their body tracking capabilities, however, existing solutions rely on gravity and often magnetic fields for drift correction. As humanity's presence in space increases, systems that don't rely on gravity or magnetism are required. We aim to demonstrate the viability of IMU body tracking in a microgravity environment by showing that gravity and magnetism are not necessary for correcting gyroscope-based dead-reckoning drift. We aim to build and evaluate an end-to-end solution accomplishing this. A novel algorithm is developed that compensates for drift using local accelerations alone, without needing gravity or magnetism. Custom PCB sensor, IMU, nodes are created and combined into a body-sensor-network to implement the algorithm and the system is evaluated to determine its strengths and weaknesses. Dead-reckoning alone is accurate to within 1 degree for 30s. The drift correction solution can correct large drifts in yaw within 4 seconds of lateral accelerations to within 3.3 degrees RMSE. Correction accuracy when drift-free and under motion is 1.1 degrees RSME. We demonstrate that gyroscopic drift can be compensated for in a kinematic chain by making use of local acceleration information and often-discarded centripetal and tangential acceleration information, even in the absence of gravitational and magnetic fields. Therefore, IMU body tracking is a viable technology for use in microgravity environments.","sentences":["Tracking kinematic chains has many uses from healthcare to virtual reality.","Inertial measurement units, IMUs, are well-recognised for their body tracking capabilities, however, existing solutions rely on gravity and often magnetic fields for drift correction.","As humanity's presence in space increases, systems that don't rely on gravity or magnetism are required.","We aim to demonstrate the viability of IMU body tracking in a microgravity environment by showing that gravity and magnetism are not necessary for correcting gyroscope-based dead-reckoning drift.","We aim to build and evaluate an end-to-end solution accomplishing this.","A novel algorithm is developed that compensates for drift using local accelerations alone, without needing gravity or magnetism.","Custom PCB sensor, IMU, nodes are created and combined into a body-sensor-network to implement the algorithm and the system is evaluated to determine its strengths and weaknesses.","Dead-reckoning alone is accurate to within 1 degree for 30s.","The drift correction solution can correct large drifts in yaw within 4 seconds of lateral accelerations to within 3.3 degrees RMSE.","Correction accuracy when drift-free and under motion is 1.1 degrees RSME.","We demonstrate that gyroscopic drift can be compensated for in a kinematic chain by making use of local acceleration information and often-discarded centripetal and tangential acceleration information, even in the absence of gravitational and magnetic fields.","Therefore, IMU body tracking is a viable technology for use in microgravity environments."],"url":"http://arxiv.org/abs/2403.04357v1","category":"cs.HC"}
{"created":"2024-03-07 09:38:16","title":"Fine-Grained Complexity of Earth Mover's Distance under Translation","abstract":"The Earth Mover's Distance is a popular similarity measure in several branches of computer science. It measures the minimum total edge length of a perfect matching between two point sets. The Earth Mover's Distance under Translation ($\\mathrm{EMDuT}$) is a translation-invariant version thereof. It minimizes the Earth Mover's Distance over all translations of one point set.   For $\\mathrm{EMDuT}$ in $\\mathbb{R}^1$, we present an $\\widetilde{\\mathcal{O}}(n^2)$-time algorithm. We also show that this algorithm is nearly optimal by presenting a matching conditional lower bound based on the Orthogonal Vectors Hypothesis. For $\\mathrm{EMDuT}$ in $\\mathbb{R}^d$, we present an $\\widetilde{\\mathcal{O}}(n^{2d+2})$-time algorithm for the $L_1$ and $L_\\infty$ metric. We show that this dependence on $d$ is asymptotically tight, as an $n^{o(d)}$-time algorithm for $L_1$ or $L_\\infty$ would contradict the Exponential Time Hypothesis (ETH). Prior to our work, only approximation algorithms were known for these problems.","sentences":["The Earth Mover's Distance is a popular similarity measure in several branches of computer science.","It measures the minimum total edge length of a perfect matching between two point sets.","The Earth Mover's Distance under Translation ($\\mathrm{EMDuT}$) is a translation-invariant version thereof.","It minimizes the Earth Mover's Distance over all translations of one point set.   ","For $\\mathrm{EMDuT}$ in $\\mathbb{R}^1$, we present an $\\widetilde{\\mathcal{O}}(n^2)$-time algorithm.","We also show that this algorithm is nearly optimal by presenting a matching conditional lower bound based on the Orthogonal Vectors Hypothesis.","For $\\mathrm{EMDuT}$ in $\\mathbb{R}^d$, we present an $\\widetilde{\\mathcal{O}}(n^{2d+2})$-time algorithm for the $L_1$ and $L_\\infty$ metric.","We show that this dependence on $d$ is asymptotically tight, as an $n^{o(d)}$-time algorithm for $L_1$ or $L_\\infty$ would contradict the Exponential Time Hypothesis (ETH).","Prior to our work, only approximation algorithms were known for these problems."],"url":"http://arxiv.org/abs/2403.04356v1","category":"cs.CG"}
{"created":"2024-03-07 09:34:55","title":"Speeding up all-electron real-time TDDFT demonstrated by the exciting package","abstract":"Currently, many ab initio codes are being prepared for exascale computing. A first and important step is to significantly improve the efficiency of existing implementations by devising better algorithms that can accomplish the same tasks with enhanced scalability. This manuscript addresses this challenge for real-time time-dependent density functional theory in the full-potential all-electron code exciting, with a focus on systems with reduced dimensionality. Following the strategy described here, calculations can run orders of magnitude faster than before. We demonstrate this with the molecules H$_2$ and CO, achieving speedups between 98 to over 50,000. We also present an example where conventional calculations would be particularly costly, namely the inorganic/organic heterostructure of pyridine physisorbed on monolayer MoS$_2$.","sentences":["Currently, many ab initio codes are being prepared for exascale computing.","A first and important step is to significantly improve the efficiency of existing implementations by devising better algorithms that can accomplish the same tasks with enhanced scalability.","This manuscript addresses this challenge for real-time time-dependent density functional theory in the full-potential all-electron code exciting, with a focus on systems with reduced dimensionality.","Following the strategy described here, calculations can run orders of magnitude faster than before.","We demonstrate this with the molecules H$_2$ and CO, achieving speedups between 98 to over 50,000.","We also present an example where conventional calculations would be particularly costly, namely the inorganic/organic heterostructure of pyridine physisorbed on monolayer MoS$_2$."],"url":"http://arxiv.org/abs/2403.04351v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-07 09:24:03","title":"Impacts of source morphology on the detectability of subhalos in strong lenses","abstract":"We provide an analysis of a convolutional neural network's ability to identify the lensing signal of single dark matter subhalos in strong galaxy-galaxy lenses in the presence of increasingly complex source light morphology. We simulate a balanced dataset of 800,000 strong lens images both perturbed and unperturbed by a single subhalo ranging in virial mass between $10^{7.5} M_{\\odot} - 10^{11}M_{\\odot}$ and characterise the source complexity by the number of Sersic clumps present in the source plane ranging from 1 to 5. Using the ResNet50 architecture we train the network to classify images as either perturbed or unperturbed. We find that the network is able to detect subhalos at low enough masses to distinguish between dark matter models even with complex sources and that source complexity has little impact on the accuracy beyond 3 clumps. The model was more confident in its classification when the clumps in the source were compact, but cared little about their spatial distribution. We also tested for the resolution of the data, finding that even in conditions akin to natural seeing the model was still able to achieve an accuracy of 74% in our highest peak signal-to-noise datasets, though this is heavily dominated by the high mass subhalos. It's robustness against resolution is attributed to the model learning the flux ratio anomalies in the perturbed lenses which are conserved in the lower resolution data.","sentences":["We provide an analysis of a convolutional neural network's ability to identify the lensing signal of single dark matter subhalos in strong galaxy-galaxy lenses in the presence of increasingly complex source light morphology.","We simulate a balanced dataset of 800,000 strong lens images both perturbed and unperturbed by a single subhalo ranging in virial mass between $10^{7.5} M_{\\odot} - 10^{11}M_{\\odot}$ and characterise the source complexity by the number of Sersic clumps present in the source plane ranging from 1 to 5.","Using the ResNet50 architecture we train the network to classify images as either perturbed or unperturbed.","We find that the network is able to detect subhalos at low enough masses to distinguish between dark matter models even with complex sources and that source complexity has little impact on the accuracy beyond 3 clumps.","The model was more confident in its classification when the clumps in the source were compact, but cared little about their spatial distribution.","We also tested for the resolution of the data, finding that even in conditions akin to natural seeing the model was still able to achieve an accuracy of 74% in our highest peak signal-to-noise datasets, though this is heavily dominated by the high mass subhalos.","It's robustness against resolution is attributed to the model learning the flux ratio anomalies in the perturbed lenses which are conserved in the lower resolution data."],"url":"http://arxiv.org/abs/2403.04349v1","category":"astro-ph.CO"}
{"created":"2024-03-07 09:21:39","title":"On a variational problem related to the Cwikel-Lieb-Rozenblum and Lieb-Thirring inequalities","abstract":"We explicitly solve a variational problem related to upper bounds on the optimal constants in the Cwikel--Lieb--Rozenblum (CLR) and Lieb--Thirring (LT) inequalities, which has recently been derived in [Invent. Math. 231 (2023), no.1, 111-167. https://doi.org/10.1007/s00222-022-01144-7 ] and [J. Eur. Math. Soc. (JEMS) 23 (2021), no.8, 2583-2600. https://doi.org/10.4171/jems/1062 ]. We achieve this through a variational characterization of the $L^1$ norm of the Fourier transform of a function and duality, from which we obtain a reformulation in terms of a variant of the Hadamard three lines lemma. By studying Hardy-like spaces of holomorphic functions in a strip in the complex plane, we are able to provide an analytic formula for the minimizers, and use it to get the best possible upper bounds for the optimal constants in the CLR and LT inequalities achievable by the method of [Invent. Math. 231 (2023), no.1, 111-167. https://doi.org/10.1007/s00222-022-01144-7 ] and [J. Eur. Math. Soc. (JEMS) 23 (2021), no.8, 2583-2600. https://doi.org/10.4171/jems/1062 ].","sentences":["We explicitly solve a variational problem related to upper bounds on the optimal constants in the Cwikel--Lieb--Rozenblum (CLR) and Lieb--Thirring (LT) inequalities, which has recently been derived in [Invent.","Math. 231 (2023), no.1, 111-167. https://doi.org/10.1007/s00222-022-01144-7 ]","and [J. Eur.","Math.","Soc.","(JEMS) 23 (2021), no.8, 2583-2600. https://doi.org/10.4171/jems/1062 ].","We achieve this through a variational characterization of the $L^1$ norm of the Fourier transform of a function and duality, from which we obtain a reformulation in terms of a variant of the Hadamard three lines lemma.","By studying Hardy-like spaces of holomorphic functions in a strip in the complex plane, we are able to provide an analytic formula for the minimizers, and use it to get the best possible upper bounds for the optimal constants in the CLR and LT inequalities achievable by the method of [Invent.","Math. 231 (2023), no.1, 111-167. https://doi.org/10.1007/s00222-022-01144-7 ]","and [J. Eur.","Math.","Soc.","(JEMS) 23 (2021), no.8, 2583-2600. https://doi.org/10.4171/jems/1062 ]."],"url":"http://arxiv.org/abs/2403.04347v1","category":"math-ph"}
{"created":"2024-03-07 09:12:23","title":"RL-CFR: Improving Action Abstraction for Imperfect Information Extensive-Form Games with Reinforcement Learning","abstract":"Effective action abstraction is crucial in tackling challenges associated with large action spaces in Imperfect Information Extensive-Form Games (IIEFGs). However, due to the vast state space and computational complexity in IIEFGs, existing methods often rely on fixed abstractions, resulting in sub-optimal performance. In response, we introduce RL-CFR, a novel reinforcement learning (RL) approach for dynamic action abstraction. RL-CFR builds upon our innovative Markov Decision Process (MDP) formulation, with states corresponding to public information and actions represented as feature vectors indicating specific action abstractions. The reward is defined as the expected payoff difference between the selected and default action abstractions. RL-CFR constructs a game tree with RL-guided action abstractions and utilizes counterfactual regret minimization (CFR) for strategy derivation. Impressively, it can be trained from scratch, achieving higher expected payoff without increased CFR solving time. In experiments on Heads-up No-limit Texas Hold'em, RL-CFR outperforms ReBeL's replication and Slumbot, demonstrating significant win-rate margins of $64\\pm 11$ and $84\\pm 17$ mbb/hand, respectively.","sentences":["Effective action abstraction is crucial in tackling challenges associated with large action spaces in Imperfect Information Extensive-Form Games (IIEFGs).","However, due to the vast state space and computational complexity in IIEFGs, existing methods often rely on fixed abstractions, resulting in sub-optimal performance.","In response, we introduce RL-CFR, a novel reinforcement learning (RL) approach for dynamic action abstraction.","RL-CFR builds upon our innovative Markov Decision Process (MDP) formulation, with states corresponding to public information and actions represented as feature vectors indicating specific action abstractions.","The reward is defined as the expected payoff difference between the selected and default action abstractions.","RL-CFR constructs a game tree with RL-guided action abstractions and utilizes counterfactual regret minimization (CFR) for strategy derivation.","Impressively, it can be trained from scratch, achieving higher expected payoff without increased CFR solving time.","In experiments on Heads-up No-limit Texas Hold'em, RL-CFR outperforms ReBeL's replication and Slumbot, demonstrating significant win-rate margins of $64\\pm 11$ and $84\\pm 17$ mbb/hand, respectively."],"url":"http://arxiv.org/abs/2403.04344v1","category":"cs.GT"}
{"created":"2024-03-07 09:03:38","title":"Structural disorder-induced topological phase transitions in quasicrystals","abstract":"Recently, the structural disorder-induced topological phase transitions in periodic systems have attracted much attention. However, in aperiodic systems such as quasicrystalline systems, the interplay between structural disorder and band topology is still unclear. In this work, we investigate the effects of structural disorder on a quantum spin Hall insulator phase and a higher-order topological phase in a two-dimensional Amman-Beenker tiling quasicrystalline lattice, respectively. We demonstrate that the structural disorder can induce a topological phase transition from a quasicrystalline normal insulator phase to an amorphous quantum spin Hall insulator phase, which is confirmed by bulk gap closing and reopening, robust edge states, quantized spin Bott index and conductance. Furthermore, the structural disorder-induced higher-order topological phase transition from a quasicrystalline normal insulator phase to an amorphous higher-order topological phase characterized by quantized quadrupole moment and topological corner states is also found. More strikingly, the disorder-induced higher-order topological insulator with eight corner states represents a distinctive topological state that eludes realization in conventional crystalline systems. Our work extends the study of the interplay between disorder effects and topologies to quasicrystalline and amorphous systems.","sentences":["Recently, the structural disorder-induced topological phase transitions in periodic systems have attracted much attention.","However, in aperiodic systems such as quasicrystalline systems, the interplay between structural disorder and band topology is still unclear.","In this work, we investigate the effects of structural disorder on a quantum spin Hall insulator phase and a higher-order topological phase in a two-dimensional Amman-Beenker tiling quasicrystalline lattice, respectively.","We demonstrate that the structural disorder can induce a topological phase transition from a quasicrystalline normal insulator phase to an amorphous quantum spin Hall insulator phase, which is confirmed by bulk gap closing and reopening, robust edge states, quantized spin Bott index and conductance.","Furthermore, the structural disorder-induced higher-order topological phase transition from a quasicrystalline normal insulator phase to an amorphous higher-order topological phase characterized by quantized quadrupole moment and topological corner states is also found.","More strikingly, the disorder-induced higher-order topological insulator with eight corner states represents a distinctive topological state that eludes realization in conventional crystalline systems.","Our work extends the study of the interplay between disorder effects and topologies to quasicrystalline and amorphous systems."],"url":"http://arxiv.org/abs/2403.04338v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-07 09:00:56","title":"Cyclicity of the shift operator and a related completeness problem in de Branges-Rovnyak spaces","abstract":"In this paper, we study the cyclic vectors of the shift operator $S_b$ acting on de Branges-Rovnyak space $\\mathcal H(b)$ associated to a non-extreme point of the closed unit ball of $H^\\infty$. We highlight an interesting link with a completeness problem that we study using the Cauchy transform. This enables us to obtain some nice consequences on cyclicity.","sentences":["In this paper, we study the cyclic vectors of the shift operator $S_b$ acting on de Branges-Rovnyak space $\\mathcal H(b)$ associated to a non-extreme point of the closed unit ball of $H^\\infty$. We highlight an interesting link with a completeness problem that we study using the Cauchy transform.","This enables us to obtain some nice consequences on cyclicity."],"url":"http://arxiv.org/abs/2403.04335v1","category":"math.CV"}
{"created":"2024-03-07 08:57:34","title":"A Survey of Application of Machine Learning in Wireless Indoor Positioning Systems","abstract":"Indoor human positioning has become increasingly important for applications such as health monitoring, breath monitoring, human identification, safety and rescue operations, and security surveillance. However, achieving robust indoor human positioning remains challenging due to various constraints. Numerous attempts have been made in the literature to develop efficient indoor positioning systems (IPSs), with a growing focus on machine learning (ML) based techniques. This paper aims to compare and analyze current ML-based wireless techniques and approaches for indoor positioning, providing a comprehensive review of enabling technologies for human detection, positioning, and activity recognition. The study explores different input measurement data, including RSSI, TDOA, etc., for various IPSs. Key positioning techniques such as RSSI-based fingerprinting, Angle-based, and Time-based approaches are examined in conjunction with various ML methods. The survey compares the positioning accuracy, scalability, and algorithm complexity, with the goal of determining the suitable technology in various services. Finally, the paper compares distinct datasets focused on indoor localization, which have been published using diverse technologies. Overall, the paper presents a comprehensive comparison of existing techniques and localization models.","sentences":["Indoor human positioning has become increasingly important for applications such as health monitoring, breath monitoring, human identification, safety and rescue operations, and security surveillance.","However, achieving robust indoor human positioning remains challenging due to various constraints.","Numerous attempts have been made in the literature to develop efficient indoor positioning systems (IPSs), with a growing focus on machine learning (ML) based techniques.","This paper aims to compare and analyze current ML-based wireless techniques and approaches for indoor positioning, providing a comprehensive review of enabling technologies for human detection, positioning, and activity recognition.","The study explores different input measurement data, including RSSI, TDOA, etc., for various IPSs.","Key positioning techniques such as RSSI-based fingerprinting, Angle-based, and Time-based approaches are examined in conjunction with various ML methods.","The survey compares the positioning accuracy, scalability, and algorithm complexity, with the goal of determining the suitable technology in various services.","Finally, the paper compares distinct datasets focused on indoor localization, which have been published using diverse technologies.","Overall, the paper presents a comprehensive comparison of existing techniques and localization models."],"url":"http://arxiv.org/abs/2403.04333v1","category":"eess.SP"}
{"created":"2024-03-07 08:55:35","title":"A new metric for the comparison of permittivity models in terahertz time-domain spectroscopy","abstract":"We present a robust method, as well as a new metric, for the comparison of permittivity models in terahertz timedomain spectroscopy (THz-TDS). In this work, we perform an extensive noise analysis of a THz-TDS system, we remove and model the unwanted deterministic noises and implement them into our fitting process. This is done using our open-source software, Fit@TDS, available at : https://github.com/THzbiophotonics/Fit-TDS. This work is the first step towards the derivation of uncertainties, and therefore the use of error bars. We hope that this will lead to performing analytical analysis with THz-TDS, as results obtained from different setups will be comparable. Finally, we apply this protocol to the study of a $\\alpha$-lactose monohydrate pellet in order to give more insight on the molecular dynamics behind the absorption peaks. The comparison with simulation results is made easier thanks to the probabilities derived from the metric.","sentences":["We present a robust method, as well as a new metric, for the comparison of permittivity models in terahertz timedomain spectroscopy (THz-TDS).","In this work, we perform an extensive noise analysis of a THz-TDS system, we remove and model the unwanted deterministic noises and implement them into our fitting process.","This is done using our open-source software, Fit@TDS, available at : https://github.com/THzbiophotonics/Fit-TDS.","This work is the first step towards the derivation of uncertainties, and therefore the use of error bars.","We hope that this will lead to performing analytical analysis with THz-TDS, as results obtained from different setups will be comparable.","Finally, we apply this protocol to the study of a $\\alpha$-lactose monohydrate pellet in order to give more insight on the molecular dynamics behind the absorption peaks.","The comparison with simulation results is made easier thanks to the probabilities derived from the metric."],"url":"http://arxiv.org/abs/2403.04332v1","category":"physics.app-ph"}
{"created":"2024-03-07 08:52:59","title":"Control-Barrier-Aided Teleoperation with Visual-Inertial SLAM for Safe MAV Navigation in Complex Environments","abstract":"In this paper, we consider a Micro Aerial Vehicle (MAV) system teleoperated by a non-expert and introduce a perceptive safety filter that leverages Control Barrier Functions (CBFs) in conjunction with Visual-Inertial Simultaneous Localization and Mapping (VI-SLAM) and dense 3D occupancy mapping to guarantee safe navigation in complex and unstructured environments. Our system relies solely on onboard IMU measurements, stereo infrared images, and depth images and autonomously corrects teleoperated inputs when they are deemed unsafe. We define a point in 3D space as unsafe if it satisfies either of two conditions: (i) it is occupied by an obstacle, or (ii) it remains unmapped. At each time step, an occupancy map of the environment is updated by the VI-SLAM by fusing the onboard measurements, and a CBF is constructed to parameterize the (un)safe region in the 3D space. Given the CBF and state feedback from the VI-SLAM module, a safety filter computes a certified reference that best matches the teleoperation input while satisfying the safety constraint encoded by the CBF. In contrast to existing perception-based safe control frameworks, we directly close the perception-action loop and demonstrate the full capability of safe control in combination with real-time VI-SLAM without any external infrastructure or prior knowledge of the environment. We verify the efficacy of the perceptive safety filter in real-time MAV experiments using exclusively onboard sensing and computation and show that the teleoperated MAV is able to safely navigate through unknown environments despite arbitrary inputs sent by the teleoperator.","sentences":["In this paper, we consider a Micro Aerial Vehicle (MAV) system teleoperated by a non-expert and introduce a perceptive safety filter that leverages Control Barrier Functions (CBFs) in conjunction with Visual-Inertial Simultaneous Localization and Mapping (VI-SLAM) and dense 3D occupancy mapping to guarantee safe navigation in complex and unstructured environments.","Our system relies solely on onboard IMU measurements, stereo infrared images, and depth images and autonomously corrects teleoperated inputs when they are deemed unsafe.","We define a point in 3D space as unsafe if it satisfies either of two conditions: (i) it is occupied by an obstacle, or (ii) it remains unmapped.","At each time step, an occupancy map of the environment is updated by the VI-SLAM by fusing the onboard measurements, and a CBF is constructed to parameterize the (un)safe region in the 3D space.","Given the CBF and state feedback from the VI-SLAM module, a safety filter computes a certified reference that best matches the teleoperation input while satisfying the safety constraint encoded by the CBF.","In contrast to existing perception-based safe control frameworks, we directly close the perception-action loop and demonstrate the full capability of safe control in combination with real-time VI-SLAM without any external infrastructure or prior knowledge of the environment.","We verify the efficacy of the perceptive safety filter in real-time MAV experiments using exclusively onboard sensing and computation and show that the teleoperated MAV is able to safely navigate through unknown environments despite arbitrary inputs sent by the teleoperator."],"url":"http://arxiv.org/abs/2403.04331v1","category":"cs.RO"}
{"created":"2024-03-07 08:48:42","title":"A mechanism-informed reinforcement learning framework for shape optimization of airfoils","abstract":"In this study, we present the mechanism-informed reinforcement learning framework for airfoil shape optimization. By leveraging the twin delayed deep deterministic policy gradient algorithm for its notable stability, our approach addresses the complexities of optimizing shapes governed by fluid dynamics. The PDEs-based solver is adopted for its accuracy even when the configurations and geometries are extraordinarily changed during the exploration. Dual-weighted residual-based mesh refinement strategy is applied to ensure the accurate calculation of target functionals. To streamline the iterative optimization process and handle geometric deformations, our approach integrates Laplacian smoothing, adaptive refinement, and a B\\'ezier fitting strategy. This combination not only remits mesh tangling but also guarantees a precise manipulation of the airfoil geometry. Our neural network architecture leverages B\\'ezier curves for efficient dimensionality reduction, thereby enhancing the learning process and ensuring the geometric accuracy of the airfoil shapes. An attention mechanism is embedded within the network to calculate potential action on the state as well. Furthermore, we have introduced different reward and penalty mechanisms tailored to the specific challenges of airfoil optimization. This algorithm is designed to support the optimization task, facilitating a more targeted and effective approach for airfoil shape optimization.","sentences":["In this study, we present the mechanism-informed reinforcement learning framework for airfoil shape optimization.","By leveraging the twin delayed deep deterministic policy gradient algorithm for its notable stability, our approach addresses the complexities of optimizing shapes governed by fluid dynamics.","The PDEs-based solver is adopted for its accuracy even when the configurations and geometries are extraordinarily changed during the exploration.","Dual-weighted residual-based mesh refinement strategy is applied to ensure the accurate calculation of target functionals.","To streamline the iterative optimization process and handle geometric deformations, our approach integrates Laplacian smoothing, adaptive refinement, and a B\\'ezier fitting strategy.","This combination not only remits mesh tangling but also guarantees a precise manipulation of the airfoil geometry.","Our neural network architecture leverages B\\'ezier curves for efficient dimensionality reduction, thereby enhancing the learning process and ensuring the geometric accuracy of the airfoil shapes.","An attention mechanism is embedded within the network to calculate potential action on the state as well.","Furthermore, we have introduced different reward and penalty mechanisms tailored to the specific challenges of airfoil optimization.","This algorithm is designed to support the optimization task, facilitating a more targeted and effective approach for airfoil shape optimization."],"url":"http://arxiv.org/abs/2403.04329v1","category":"math.NA"}
{"created":"2024-03-07 08:48:17","title":"A dual approach to nonparametric characterization for random utility models","abstract":"This paper develops a novel characterization for random utility models (RUM), which turns out to be a dual representation of the characterization by Kitamura and Stoye (2018, ECMA). For a given family of budgets and its \"patch\" representation \\'a la Kitamura and Stoye, we construct a matrix $\\Xi$ of which each row vector indicates the structure of possible revealed preference relations in each subfamily of budgets. Then, it is shown that a stochastic demand system on the patches of budget lines, say $\\pi$, is consistent with a RUM, if and only if $\\Xi\\pi \\geq \\mathbb{1}$. In addition to providing a concise closed form characterization, especially when $\\pi$ is inconsistent with RUMs, the vector $\\Xi\\pi$ also contains information concerning (1) sub-families of budgets in which cyclical choices must occur with positive probabilities, and (2) the maximal possible weights on rational choice patterns in a population. The notion of Chv\\'atal rank of polytopes and the duality theorem in linear programming play key roles to obtain these results.","sentences":["This paper develops a novel characterization for random utility models (RUM), which turns out to be a dual representation of the characterization by Kitamura and Stoye (2018, ECMA).","For a given family of budgets and its \"patch\" representation \\'a la Kitamura and Stoye, we construct a matrix $\\Xi$ of which each row vector indicates the structure of possible revealed preference relations in each subfamily of budgets.","Then, it is shown that a stochastic demand system on the patches of budget lines, say $\\pi$, is consistent with a RUM, if and only if $\\Xi\\pi \\geq \\mathbb{1}$.","In addition to providing a concise closed form characterization, especially when $\\pi$ is inconsistent with RUMs, the vector $\\Xi\\pi$ also contains information concerning (1) sub-families of budgets in which cyclical choices must occur with positive probabilities, and (2) the maximal possible weights on rational choice patterns in a population.","The notion of Chv\\'atal rank of polytopes and the duality theorem in linear programming play key roles to obtain these results."],"url":"http://arxiv.org/abs/2403.04328v1","category":"econ.TH"}
{"created":"2024-03-07 08:35:38","title":"Anisotropy-driven magnetic phase transitions in SU(4)-symmetric Fermi gas in three-dimensional optical lattices","abstract":"We study SU(4)-symmetric ultracold fermionic mixture in the cubic optical lattice with the variable tunneling amplitude along one particular crystallographic axis in the crossover region from the two- to three-dimensional spatial geometry. To theoretically analyze emerging magnetic phases and physical observables, we describe the system in the framework of the Fermi-Hubbard model and apply dynamical mean-field theory. We show that in two limiting cases of anisotropy there are two phases with different antiferromagnetic orderings in the zero temperature limit and determine a region of their coexistence. We also study the stability regions of different magnetically-ordered states and density profiles of the gas in the harmonic optical trap.","sentences":["We study SU(4)-symmetric ultracold fermionic mixture in the cubic optical lattice with the variable tunneling amplitude along one particular crystallographic axis in the crossover region from the two- to three-dimensional spatial geometry.","To theoretically analyze emerging magnetic phases and physical observables, we describe the system in the framework of the Fermi-Hubbard model and apply dynamical mean-field theory.","We show that in two limiting cases of anisotropy there are two phases with different antiferromagnetic orderings in the zero temperature limit and determine a region of their coexistence.","We also study the stability regions of different magnetically-ordered states and density profiles of the gas in the harmonic optical trap."],"url":"http://arxiv.org/abs/2403.04319v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-07 08:32:32","title":"Asymptotic teleportation schemes bridging between standard and port-based teleportation","abstract":"Various modified quantum teleportation schemes are proposed to overcome experimental constraints or to meet specific application requirements for quantum communication. Hence, most schemes are developed and studied with unique methodologies, each with its inherent challenges. Our research focuses on interconnecting these schemes appearing to be unrelated to each other, based on the idea that the unique advantages of one scheme can compensate for the limitations of another. In this paper, we introduce an asymptotic teleportation scheme requiring the receiver to perform a classical selection task followed by a quantum correction. This scheme bridges standard teleportation with port-based teleportation through the transformation of joint measurements. Specifically, we categorize and analytically investigate protocols within this scheme for qubit systems. Given that linear optics teleportation protocol without ancilla qubits is contained in the two non-trivial groups, we provide a novel perspective on its expansion. Furthermore, we discuss the potential application of a protocol from one of these groups as a universal programmable processor and extend these protocols to higher-dimensional systems while maintaining the same properties and potential, providing the analytic form of the joint measurement and its performance. These results thereby propose new avenues for developing a quantum network in higher-dimensional systems.","sentences":["Various modified quantum teleportation schemes are proposed to overcome experimental constraints or to meet specific application requirements for quantum communication.","Hence, most schemes are developed and studied with unique methodologies, each with its inherent challenges.","Our research focuses on interconnecting these schemes appearing to be unrelated to each other, based on the idea that the unique advantages of one scheme can compensate for the limitations of another.","In this paper, we introduce an asymptotic teleportation scheme requiring the receiver to perform a classical selection task followed by a quantum correction.","This scheme bridges standard teleportation with port-based teleportation through the transformation of joint measurements.","Specifically, we categorize and analytically investigate protocols within this scheme for qubit systems.","Given that linear optics teleportation protocol without ancilla qubits is contained in the two non-trivial groups, we provide a novel perspective on its expansion.","Furthermore, we discuss the potential application of a protocol from one of these groups as a universal programmable processor and extend these protocols to higher-dimensional systems while maintaining the same properties and potential, providing the analytic form of the joint measurement and its performance.","These results thereby propose new avenues for developing a quantum network in higher-dimensional systems."],"url":"http://arxiv.org/abs/2403.04315v1","category":"quant-ph"}
{"created":"2024-03-07 08:23:48","title":"The dynamics of fibers dispersed in viscoelastic turbulent flows","abstract":"This study explores the dynamics of finite-size fibers suspended freely in a viscoelastic turbulent flow. For a fiber suspended in Newtonian flows, two different flapping regimes were identified previously by Rosti et al (2018). Here we explore, how the fiber dynamics is modified by the elasticity of the carrier fluid by performing Direct Numerical Simulations of a two-way coupled fiber-fluid system in a parametric space spanning different Deborah numbers, fiber bending stiffness and the linear density difference between fiber and fluid. We examine how these parameters influence various fiber characteristics such as the frequency of flapping, curvature, and alignment with the fluid strain and polymer stretching directions. Results reveal that the neutrally-bouyant fibers, depending on their flexibility, oscillate with large and small time scales transpiring from the flow, but the smaller time-scales are suppressed as the polymer elasticity increases. Polymer stretching is uncommunicative to denser-than-fluid fibers, which flap with large time scales from the flow when flexible and with their natural frequency when rigid. Thus, the characteristic elastic time scale has a subdominant effect when the fibers are neutrally-bouyant, while its effect is absent when the fibers become more inertial. Additionally, we see that the inertial fibers have larger curvatures and are less responsive to the polymer presence, whereas the neutrally-bouyant fibers show quantitative changes. Also, the neutrally-bouyant fibers show a higher alignment with the polymer stretching directions compared to the denser ones. In a nutshell, the polymers exert a larger influence on neutrally-bouyant fibers compared to the denser ones. The study comprehensively addresses the interplay between polymer elasticity and the fiber structural properties in determining its response behaviour in an elasto-inertial turbulent flow.","sentences":["This study explores the dynamics of finite-size fibers suspended freely in a viscoelastic turbulent flow.","For a fiber suspended in Newtonian flows, two different flapping regimes were identified previously by Rosti et al (2018).","Here we explore, how the fiber dynamics is modified by the elasticity of the carrier fluid by performing Direct Numerical Simulations of a two-way coupled fiber-fluid system in a parametric space spanning different Deborah numbers, fiber bending stiffness and the linear density difference between fiber and fluid.","We examine how these parameters influence various fiber characteristics such as the frequency of flapping, curvature, and alignment with the fluid strain and polymer stretching directions.","Results reveal that the neutrally-bouyant fibers, depending on their flexibility, oscillate with large and small time scales transpiring from the flow, but the smaller time-scales are suppressed as the polymer elasticity increases.","Polymer stretching is uncommunicative to denser-than-fluid fibers, which flap with large time scales from the flow when flexible and with their natural frequency when rigid.","Thus, the characteristic elastic time scale has a subdominant effect when the fibers are neutrally-bouyant, while its effect is absent when the fibers become more inertial.","Additionally, we see that the inertial fibers have larger curvatures and are less responsive to the polymer presence, whereas the neutrally-bouyant fibers show quantitative changes.","Also, the neutrally-bouyant fibers show a higher alignment with the polymer stretching directions compared to the denser ones.","In a nutshell, the polymers exert a larger influence on neutrally-bouyant fibers compared to the denser ones.","The study comprehensively addresses the interplay between polymer elasticity and the fiber structural properties in determining its response behaviour in an elasto-inertial turbulent flow."],"url":"http://arxiv.org/abs/2403.04305v1","category":"physics.flu-dyn"}
{"created":"2024-03-07 08:10:30","title":"Experimental amplification and squeezing of a motional state of an optically levitated nanoparticle","abstract":"A contactless control of fluctuations of phase space variables of a nanoobject belongs among the key methods needed for ultra-precise nanotechnology and the upcoming quantum technology of macroscopic systems. Here we utilize the experimental platform of a single levitating nanoparticle (NP) to demonstrate essential protocols providing linear amplification of the mechanical phase space variables together with squeezing of phase space probability distribution. The protocol combines a controlled fast switching between the parabolic trapping potential and either weak parabolic or inverted parabolic amplifying potential leading to amplification of mean value and variance (fluctuations) along an arbitrary phase space variable and squeezing along the complementary one. The protocol is completed with cold damping scheme to control the initial fluctuations of the NP phase space variables. We reached the amplification gain $|G|>2$, the squeezing coefficient above 4 dB, and the second-order energy correlation function approaching 3 which corresponds to a maximum for a stochastic non-equilibrium classical state. These experimental results will already allow pre-amplification and manipulation of nanomechanical NP motion for all quantum protocols if the NP cooling towards the ground state is applied.","sentences":["A contactless control of fluctuations of phase space variables of a nanoobject belongs among the key methods needed for ultra-precise nanotechnology and the upcoming quantum technology of macroscopic systems.","Here we utilize the experimental platform of a single levitating nanoparticle (NP) to demonstrate essential protocols providing linear amplification of the mechanical phase space variables together with squeezing of phase space probability distribution.","The protocol combines a controlled fast switching between the parabolic trapping potential and either weak parabolic or inverted parabolic amplifying potential leading to amplification of mean value and variance (fluctuations) along an arbitrary phase space variable and squeezing along the complementary one.","The protocol is completed with cold damping scheme to control the initial fluctuations of the NP phase space variables.","We reached the amplification gain $|G|>2$, the squeezing coefficient above 4 dB, and the second-order energy correlation function approaching 3 which corresponds to a maximum for a stochastic non-equilibrium classical state.","These experimental results will already allow pre-amplification and manipulation of nanomechanical NP motion for all quantum protocols if the NP cooling towards the ground state is applied."],"url":"http://arxiv.org/abs/2403.04302v1","category":"quant-ph"}
{"created":"2024-03-07 07:46:03","title":"Analysis of Maximum Threshold and Quantum Security for Fault-Tolerant Encoding and Decoding Scheme Base on Steane Code","abstract":"Steane code is one of the most widely studied quantum error-correction codes, which is a natural choice for fault-tolerant quantum computation (FTQC). However, the original Steane code is not fault-tolerant because the CNOT gates in an encoded block may cause error propagation. In this paper, we first propose a fault-tolerant encoding and decoding scheme, which analyzes all possible errors caused by each quantum gate in an error-correction period. In this scheme, we combine the results of measuring redundant qubits with those of syndrome measurements to identify specific errors for different types of errors. But due to the error propagation, there may be cases where different errors produce the same measurement results. Therefore, we introduce the \"flag qubits\" scheme (providing its usage conditions) to reduce error interference as much as possible, and we consider the errors caused by the introduced quantum gates, realizing the truly fault-tolerant Steane code. Afterwards, we provide the fault-tolerant scheme of the universal quantum gate set, including fault-tolerant preparation and verification of ancillary states. This is the first time that fault tolerance has been considered for every process of FTQC. Finally, We propose an algorithm for a more accurate estimation of thresholds and optimal error-correction period selection. Our simulation results based on this entire scheme demonstrate the effectiveness of this algorithm, satisfying the threshold theorem and the currently widely recognized threshold. We analyze the relationship among the maximum threshold, concatenated levels, and quantum logical depth, showing that quantum operations play a crucial role in increasing the threshold. Furthermore, we analyze the computational theoretical limits of quantum computers from the perspectives of attack and active defense based on our FTQC scheme, thereby assessing the security of a system.","sentences":["Steane code is one of the most widely studied quantum error-correction codes, which is a natural choice for fault-tolerant quantum computation (FTQC).","However, the original Steane code is not fault-tolerant because the CNOT gates in an encoded block may cause error propagation.","In this paper, we first propose a fault-tolerant encoding and decoding scheme, which analyzes all possible errors caused by each quantum gate in an error-correction period.","In this scheme, we combine the results of measuring redundant qubits with those of syndrome measurements to identify specific errors for different types of errors.","But due to the error propagation, there may be cases where different errors produce the same measurement results.","Therefore, we introduce the \"flag qubits\" scheme (providing its usage conditions) to reduce error interference as much as possible, and we consider the errors caused by the introduced quantum gates, realizing the truly fault-tolerant Steane code.","Afterwards, we provide the fault-tolerant scheme of the universal quantum gate set, including fault-tolerant preparation and verification of ancillary states.","This is the first time that fault tolerance has been considered for every process of FTQC.","Finally, We propose an algorithm for a more accurate estimation of thresholds and optimal error-correction period selection.","Our simulation results based on this entire scheme demonstrate the effectiveness of this algorithm, satisfying the threshold theorem and the currently widely recognized threshold.","We analyze the relationship among the maximum threshold, concatenated levels, and quantum logical depth, showing that quantum operations play a crucial role in increasing the threshold.","Furthermore, we analyze the computational theoretical limits of quantum computers from the perspectives of attack and active defense based on our FTQC scheme, thereby assessing the security of a system."],"url":"http://arxiv.org/abs/2403.04297v1","category":"quant-ph"}
{"created":"2024-03-07 07:35:25","title":"DGR: A General Graph Desmoothing Framework for Recommendation via Global and Local Perspectives","abstract":"Graph Convolutional Networks (GCNs) have become pivotal in recommendation systems for learning user and item embeddings by leveraging the user-item interaction graph's node information and topology. However, these models often face the famous over-smoothing issue, leading to indistinct user and item embeddings and reduced personalization. Traditional desmoothing methods in GCN-based systems are model-specific, lacking a universal solution. This paper introduces a novel, model-agnostic approach named \\textbf{D}esmoothing Framework for \\textbf{G}CN-based \\textbf{R}ecommendation Systems (\\textbf{DGR}). It effectively addresses over-smoothing on general GCN-based recommendation models by considering both global and local perspectives. Specifically, we first introduce vector perturbations during each message passing layer to penalize the tendency of node embeddings approximating overly to be similar with the guidance of the global topological structure. Meanwhile, we further develop a tailored-design loss term for the readout embeddings to preserve the local collaborative relations between users and their neighboring items. In particular, items that exhibit a high correlation with neighboring items are also incorporated to enhance the local topological information. To validate our approach, we conduct extensive experiments on 5 benchmark datasets based on 5 well-known GCN-based recommendation models, demonstrating the effectiveness and generalization of our proposed framework.","sentences":["Graph Convolutional Networks (GCNs) have become pivotal in recommendation systems for learning user and item embeddings by leveraging the user-item interaction graph's node information and topology.","However, these models often face the famous over-smoothing issue, leading to indistinct user and item embeddings and reduced personalization.","Traditional desmoothing methods in GCN-based systems are model-specific, lacking a universal solution.","This paper introduces a novel, model-agnostic approach named \\textbf{D}esmoothing Framework for \\textbf{G}CN-based \\textbf{R}ecommendation Systems (\\textbf{DGR}).","It effectively addresses over-smoothing on general GCN-based recommendation models by considering both global and local perspectives.","Specifically, we first introduce vector perturbations during each message passing layer to penalize the tendency of node embeddings approximating overly to be similar with the guidance of the global topological structure.","Meanwhile, we further develop a tailored-design loss term for the readout embeddings to preserve the local collaborative relations between users and their neighboring items.","In particular, items that exhibit a high correlation with neighboring items are also incorporated to enhance the local topological information.","To validate our approach, we conduct extensive experiments on 5 benchmark datasets based on 5 well-known GCN-based recommendation models, demonstrating the effectiveness and generalization of our proposed framework."],"url":"http://arxiv.org/abs/2403.04287v1","category":"cs.IR"}
{"created":"2024-03-07 07:33:17","title":"Highly stable power control for chip-based continuous-variable quantum key distribution system","abstract":"Quantum key distribution allows secret key generation with information theoretical security. It can be realized with photonic integrated circuits to benefit the tiny footprints and the large-scale manufacturing capacity. Continuous-variable quantum key distribution is suitable for chip-based integration due to its compatibility with mature optical communication devices. However, the quantum signal power control compatible with the mature photonic integration process faces difficulties on stability, which limits the system performance and causes the overestimation of secret key rate that opens practical security loopholes. Here, a highly stable chip-based quantum signal power control scheme based on a biased Mach-Zehnder interferometer structure is proposed, theoretically analyzed and experimentally implemented with standard silicon photonic techniques. Simulations and experimental results show that the proposed scheme significantly improves the system stability, where the standard deviation of the secret key rate is suppressed by an order of magnitude compared with the system using traditional designs, showing a promising and practicable way to realize highly stable continuous-variable quantum key distribution system on chip.","sentences":["Quantum key distribution allows secret key generation with information theoretical security.","It can be realized with photonic integrated circuits to benefit the tiny footprints and the large-scale manufacturing capacity.","Continuous-variable quantum key distribution is suitable for chip-based integration due to its compatibility with mature optical communication devices.","However, the quantum signal power control compatible with the mature photonic integration process faces difficulties on stability, which limits the system performance and causes the overestimation of secret key rate that opens practical security loopholes.","Here, a highly stable chip-based quantum signal power control scheme based on a biased Mach-Zehnder interferometer structure is proposed, theoretically analyzed and experimentally implemented with standard silicon photonic techniques.","Simulations and experimental results show that the proposed scheme significantly improves the system stability, where the standard deviation of the secret key rate is suppressed by an order of magnitude compared with the system using traditional designs, showing a promising and practicable way to realize highly stable continuous-variable quantum key distribution system on chip."],"url":"http://arxiv.org/abs/2403.04284v1","category":"quant-ph"}
{"created":"2024-03-07 07:28:35","title":"Improving link prediction accuracy of network embedding algorithms via rich node attribute information","abstract":"Complex networks are widely used to represent an abundance of real-world relations ranging from social networks to brain networks. Inferring missing links or predicting future ones based on the currently observed network is known as the link prediction task.Recent network embedding based link prediction algorithms have demonstrated ground-breaking performance on link prediction accuracy. Those algorithms usually apply node attributes as the initial feature input to accelerate the convergence speed during the training process. However, they do not take full advantage of node feature information. In this paper,besides applying feature attributes as the initial input, we make better utilization of node attribute information by building attributable networks and plugging attributable networks into some typical link prediction algorithms and naming this algorithm Attributive Graph Enhanced Embedding (AGEE). AGEE is able to automatically learn the weighting trades-off between the structure and the attributive networks. Numerical experiments show that AGEE can improve the link prediction accuracy by around 3% compared with link prediction framework SEAL, Variational Graph AutoEncoder (VGAE), and Node2vec.","sentences":["Complex networks are widely used to represent an abundance of real-world relations ranging from social networks to brain networks.","Inferring missing links or predicting future ones based on the currently observed network is known as the link prediction task.","Recent network embedding based link prediction algorithms have demonstrated ground-breaking performance on link prediction accuracy.","Those algorithms usually apply node attributes as the initial feature input to accelerate the convergence speed during the training process.","However, they do not take full advantage of node feature information.","In this paper,besides applying feature attributes as the initial input, we make better utilization of node attribute information by building attributable networks and plugging attributable networks into some typical link prediction algorithms and naming this algorithm Attributive Graph Enhanced Embedding (AGEE).","AGEE is able to automatically learn the weighting trades-off between the structure and the attributive networks.","Numerical experiments show that AGEE can improve the link prediction accuracy by around 3% compared with link prediction framework SEAL, Variational Graph AutoEncoder (VGAE), and Node2vec."],"url":"http://arxiv.org/abs/2403.04282v1","category":"cs.SI"}
{"created":"2024-03-07 07:17:56","title":"The small mass limit for a McKean-Vlasov equation with state-dependent friction","abstract":"The small mass limit is derived for a McKean-Vlasov equation with state-dependent friction in $d$-dimensional space. By applying the averaging approach to a non-autonomous slow-fast system with the microscopic and macroscopic scales, the convergence in distribution is obtained.","sentences":["The small mass limit is derived for a McKean-Vlasov equation with state-dependent friction in $d$-dimensional space.","By applying the averaging approach to a non-autonomous slow-fast system with the microscopic and macroscopic scales, the convergence in distribution is obtained."],"url":"http://arxiv.org/abs/2403.04275v1","category":"math.AP"}
{"created":"2024-03-07 18:58:26","title":"Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization","abstract":"This paper presents a new approach for batch Bayesian Optimization (BO), where the sampling takes place by minimizing a Thompson Sampling approximation of a regret to uncertainty ratio. Our objective is able to coordinate the actions chosen in each batch in a way that minimizes redundancy between points whilst focusing on points with high predictive means or high uncertainty. We provide high-probability theoretical guarantees on the regret of our algorithm. Finally, numerically, we demonstrate that our method attains state-of-the-art performance on a range of nonconvex test functions, where it outperforms several competitive benchmark batch BO algorithms by an order of magnitude on average.","sentences":["This paper presents a new approach for batch Bayesian Optimization (BO), where the sampling takes place by minimizing a Thompson Sampling approximation of a regret to uncertainty ratio.","Our objective is able to coordinate the actions chosen in each batch in a way that minimizes redundancy between points whilst focusing on points with high predictive means or high uncertainty.","We provide high-probability theoretical guarantees on the regret of our algorithm.","Finally, numerically, we demonstrate that our method attains state-of-the-art performance on a range of nonconvex test functions, where it outperforms several competitive benchmark batch BO algorithms by an order of magnitude on average."],"url":"http://arxiv.org/abs/2403.04764v1","category":"cs.LG"}
{"created":"2024-03-07 18:46:01","title":"I Can't Believe It's Not Scene Flow!","abstract":"Current scene flow methods broadly fail to describe motion on small objects, and current scene flow evaluation protocols hide this failure by averaging over many points, with most drawn larger objects. To fix this evaluation failure, we propose a new evaluation protocol, Bucket Normalized EPE, which is class-aware and speed-normalized, enabling contextualized error comparisons between object types that move at vastly different speeds. To highlight current method failures, we propose a frustratingly simple supervised scene flow baseline, TrackFlow, built by bolting a high-quality pretrained detector (trained using many class rebalancing techniques) onto a simple tracker, that produces state-of-the-art performance on current standard evaluations and large improvements over prior art on our new evaluation. Our results make it clear that all scene flow evaluations must be class and speed aware, and supervised scene flow methods must address point class imbalances. We release the evaluation code publicly at https://github.com/kylevedder/BucketedSceneFlowEval.","sentences":["Current scene flow methods broadly fail to describe motion on small objects, and current scene flow evaluation protocols hide this failure by averaging over many points, with most drawn larger objects.","To fix this evaluation failure, we propose a new evaluation protocol, Bucket Normalized EPE, which is class-aware and speed-normalized, enabling contextualized error comparisons between object types that move at vastly different speeds.","To highlight current method failures, we propose a frustratingly simple supervised scene flow baseline, TrackFlow, built by bolting a high-quality pretrained detector (trained using many class rebalancing techniques) onto a simple tracker, that produces state-of-the-art performance on current standard evaluations and large improvements over prior art on our new evaluation.","Our results make it clear that all scene flow evaluations must be class and speed aware, and supervised scene flow methods must address point class imbalances.","We release the evaluation code publicly at https://github.com/kylevedder/BucketedSceneFlowEval."],"url":"http://arxiv.org/abs/2403.04739v1","category":"cs.CV"}
{"created":"2024-03-07 18:38:17","title":"SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM","abstract":"Vision-extended LLMs have made significant strides in Visual Question Answering (VQA). Despite these advancements, VLLMs still encounter substantial difficulties in handling queries involving long-tail entities, with a tendency to produce erroneous or hallucinated responses. In this work, we introduce a novel evaluative benchmark named \\textbf{SnapNTell}, specifically tailored for entity-centric VQA. This task aims to test the models' capabilities in identifying entities and providing detailed, entity-specific knowledge. We have developed the \\textbf{SnapNTell Dataset}, distinct from traditional VQA datasets: (1) It encompasses a wide range of categorized entities, each represented by images and explicitly named in the answers; (2) It features QA pairs that require extensive knowledge for accurate responses. The dataset is organized into 22 major categories, containing 7,568 unique entities in total. For each entity, we curated 10 illustrative images and crafted 10 knowledge-intensive QA pairs. To address this novel task, we devised a scalable, efficient, and transparent retrieval-augmented multimodal LLM. Our approach markedly outperforms existing methods on the SnapNTell dataset, achieving a 66.5\\% improvement in the BELURT score. We will soon make the dataset and the source code publicly accessible.","sentences":["Vision-extended LLMs have made significant strides in Visual Question Answering (VQA).","Despite these advancements, VLLMs still encounter substantial difficulties in handling queries involving long-tail entities, with a tendency to produce erroneous or hallucinated responses.","In this work, we introduce a novel evaluative benchmark named \\textbf{SnapNTell}, specifically tailored for entity-centric VQA.","This task aims to test the models' capabilities in identifying entities and providing detailed, entity-specific knowledge.","We have developed the \\textbf{SnapNTell Dataset}, distinct from traditional VQA datasets: (1) It encompasses a wide range of categorized entities, each represented by images and explicitly named in the answers; (2) It features QA pairs that require extensive knowledge for accurate responses.","The dataset is organized into 22 major categories, containing 7,568 unique entities in total.","For each entity, we curated 10 illustrative images and crafted 10 knowledge-intensive QA pairs.","To address this novel task, we devised a scalable, efficient, and transparent retrieval-augmented multimodal LLM.","Our approach markedly outperforms existing methods on the SnapNTell dataset, achieving a 66.5\\% improvement in the BELURT score.","We will soon make the dataset and the source code publicly accessible."],"url":"http://arxiv.org/abs/2403.04735v1","category":"cs.CV"}
{"created":"2024-03-07 17:32:18","title":"Birdtracks of Exotic SU(N) Color Structures","abstract":"I introduce a systematic procedure for constructing complete and independent sets of interactions of fields transforming under exotic representations of SU(N), in particular the SU(3) gauge group of QCD. It uncovers errors in previous results, starting with interactions of four fields including a single sextet.","sentences":["I introduce a systematic procedure for constructing complete and independent sets of interactions of fields transforming under exotic representations of SU(N), in particular the SU(3) gauge group of QCD.","It uncovers errors in previous results, starting with interactions of four fields including a single sextet."],"url":"http://arxiv.org/abs/2403.04685v1","category":"hep-ph"}
{"created":"2024-03-07 17:16:59","title":"End-to-end Conditional Robust Optimization","abstract":"The field of Contextual Optimization (CO) integrates machine learning and optimization to solve decision making problems under uncertainty. Recently, a risk sensitive variant of CO, known as Conditional Robust Optimization (CRO), combines uncertainty quantification with robust optimization in order to promote safety and reliability in high stake applications. Exploiting modern differentiable optimization methods, we propose a novel end-to-end approach to train a CRO model in a way that accounts for both the empirical risk of the prescribed decisions and the quality of conditional coverage of the contextual uncertainty set that supports them. While guarantees of success for the latter objective are impossible to obtain from the point of view of conformal prediction theory, high quality conditional coverage is achieved empirically by ingeniously employing a logistic regression differentiable layer within the calculation of coverage quality in our training loss. We show that the proposed training algorithms produce decisions that outperform the traditional estimate then optimize approaches.","sentences":["The field of Contextual Optimization (CO) integrates machine learning and optimization to solve decision making problems under uncertainty.","Recently, a risk sensitive variant of CO, known as Conditional Robust Optimization (CRO), combines uncertainty quantification with robust optimization in order to promote safety and reliability in high stake applications.","Exploiting modern differentiable optimization methods, we propose a novel end-to-end approach to train a CRO model in a way that accounts for both the empirical risk of the prescribed decisions and the quality of conditional coverage of the contextual uncertainty set that supports them.","While guarantees of success for the latter objective are impossible to obtain from the point of view of conformal prediction theory, high quality conditional coverage is achieved empirically by ingeniously employing a logistic regression differentiable layer within the calculation of coverage quality in our training loss.","We show that the proposed training algorithms produce decisions that outperform the traditional estimate then optimize approaches."],"url":"http://arxiv.org/abs/2403.04670v1","category":"cs.LG"}
{"created":"2024-03-07 17:12:01","title":"kSZ Pairwise Velocity Reconstruction with Machine Learning","abstract":"We demonstrate that pairwise peculiar velocity correlations for galaxy clusters can be directly reconstructed from the kinematic Sunyaev-Zel'dovich (kSZ) signature imprinted in the CMB using a machine learning model with a gradient boosting algorithm trained on high-fidelity kSZ simulations. The machine learning model is trained using six to eight cluster features that are directly related to observables from CMB and large-scale structure surveys. We validate the capabilities of the approach in light of the presence of primary CMB, detector noise, and potential uncertainties in the cluster mass estimate and cluster center location. The pairwise velocity statistics extracted using the techniques developed here have the potential to elicit valuable cosmological constraints on dark energy, modified gravity models, and massive neutrinos with kSZ measurements from upcoming CMB surveys, including the Simons Observatory, CMB-S4 and CCAT, and the DESI and SDSS galaxy surveys.","sentences":["We demonstrate that pairwise peculiar velocity correlations for galaxy clusters can be directly reconstructed from the kinematic Sunyaev-Zel'dovich (kSZ) signature imprinted in the CMB using a machine learning model with a gradient boosting algorithm trained on high-fidelity kSZ simulations.","The machine learning model is trained using six to eight cluster features that are directly related to observables from CMB and large-scale structure surveys.","We validate the capabilities of the approach in light of the presence of primary CMB, detector noise, and potential uncertainties in the cluster mass estimate and cluster center location.","The pairwise velocity statistics extracted using the techniques developed here have the potential to elicit valuable cosmological constraints on dark energy, modified gravity models, and massive neutrinos with kSZ measurements from upcoming CMB surveys, including the Simons Observatory, CMB-S4 and CCAT, and the DESI and SDSS galaxy surveys."],"url":"http://arxiv.org/abs/2403.04664v1","category":"astro-ph.CO"}
{"created":"2024-03-07 16:51:15","title":"Cedar: A New Language for Expressive, Fast, Safe, and Analyzable Authorization (Extended Version)","abstract":"Cedar is a new authorization policy language designed to be ergonomic, fast, safe, and analyzable. Rather than embed authorization logic in an application's code, developers can write that logic as Cedar policies and delegate access decisions to Cedar's evaluation engine. Cedar's simple and intuitive syntax supports common authorization use-cases with readable policies, naturally leveraging concepts from role-based, attribute-based, and relation-based access control models. Cedar's policy structure enables access requests to be decided quickly. Cedar's policy validator leverages optional typing to help policy writers avoid mistakes, but not get in their way. Cedar's design has been finely balanced to allow for a sound and complete logical encoding, which enables precise policy analysis, e.g., to ensure that when refactoring a set of policies, the authorized permissions do not change. We have modeled Cedar in the Lean programming language, and used Lean's proof assistant to prove important properties of Cedar's design. We have implemented Cedar in Rust, and released it open-source. Comparing Cedar to two open-source languages, OpenFGA and Rego, we find (subjectively) that Cedar has equally or more readable policies, but (objectively) performs far better.","sentences":["Cedar is a new authorization policy language designed to be ergonomic, fast, safe, and analyzable.","Rather than embed authorization logic in an application's code, developers can write that logic as Cedar policies and delegate access decisions to Cedar's evaluation engine.","Cedar's simple and intuitive syntax supports common authorization use-cases with readable policies, naturally leveraging concepts from role-based, attribute-based, and relation-based access control models.","Cedar's policy structure enables access requests to be decided quickly.","Cedar's policy validator leverages optional typing to help policy writers avoid mistakes, but not get in their way.","Cedar's design has been finely balanced to allow for a sound and complete logical encoding, which enables precise policy analysis, e.g., to ensure that when refactoring a set of policies, the authorized permissions do not change.","We have modeled Cedar in the Lean programming language, and used Lean's proof assistant to prove important properties of Cedar's design.","We have implemented Cedar in Rust, and released it open-source.","Comparing Cedar to two open-source languages, OpenFGA and Rego, we find (subjectively) that Cedar has equally or more readable policies, but (objectively) performs far better."],"url":"http://arxiv.org/abs/2403.04651v1","category":"cs.PL"}
{"created":"2024-03-07 16:08:55","title":"Quantum theory of Bloch oscillations in a resistively shunted transmon","abstract":"A transmon qubit embedded in a high-impedance environment acts in a way dual to a conventional Josephson junction. In analogy to the AC Josephson effect, biasing of the transmon by a direct current leads to the oscillations of voltage across it. These oscillations are known as the Bloch oscillations. We find the Bloch oscillations spectrum, and show that the zero-point fluctuations of charge make it broad-band. Despite having a broad-band spectrum, Bloch oscillations can be brought in resonance with an external microwave radiation. The resonances lead to steps in the voltage-current relation, which are dual to the conventional Shapiro steps. We find how the shape of the steps depends on the environment impedance $R$, parameters of the transmon, and the microwave amplitude. The Bloch oscillations rely on the insulating state of the transmon which is realized at impedances exceeding the Schmid transition point, $R > R_Q = h / (2e)^2$.","sentences":["A transmon qubit embedded in a high-impedance environment acts in a way dual to a conventional Josephson junction.","In analogy to the AC Josephson effect, biasing of the transmon by a direct current leads to the oscillations of voltage across it.","These oscillations are known as the Bloch oscillations.","We find the Bloch oscillations spectrum, and show that the zero-point fluctuations of charge make it broad-band.","Despite having a broad-band spectrum, Bloch oscillations can be brought in resonance with an external microwave radiation.","The resonances lead to steps in the voltage-current relation, which are dual to the conventional Shapiro steps.","We find how the shape of the steps depends on the environment impedance $R$, parameters of the transmon, and the microwave amplitude.","The Bloch oscillations rely on the insulating state of the transmon which is realized at impedances exceeding the Schmid transition point, $R > R_Q = h / (2e)^2$."],"url":"http://arxiv.org/abs/2403.04624v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-07 16:03:13","title":"Stationary switching random walks","abstract":"A switching random walk, commonly known under the misnomer `oscillating random walk', is a real-valued Markov chain such that the distribution of its increments depends only on the sign of the current position. In this note we find invariant measures for such chains. In the particular case where the chain is an actual random walk, our proof naturally relates its stationarity relative to the Lebesgue measure to stationarity of the renewal processes of its ascending and descending ladder heights.","sentences":["A switching random walk, commonly known under the misnomer `oscillating random walk', is a real-valued Markov chain such that the distribution of its increments depends only on the sign of the current position.","In this note we find invariant measures for such chains.","In the particular case where the chain is an actual random walk, our proof naturally relates its stationarity relative to the Lebesgue measure to stationarity of the renewal processes of its ascending and descending ladder heights."],"url":"http://arxiv.org/abs/2403.04620v1","category":"math.PR"}
{"created":"2024-03-07 16:02:31","title":"Strong Priority and Determinacy in Timed CCS","abstract":"Building on the classical theory of process algebra with priorities, we identify a new scheduling mechanism, called \"sequentially constructive reduction\" which is designed to capture the essence of synchronous programming. The distinctive property of this evaluation strategy is to achieve determinism-by-construction for multi-cast concurrent communication. In particular, it permits us to model shared memory multi-threading with reaction to absence as it lies at the core of the programming language Esterel. In the technical setting of CCS extended by clocks and priorities, we prove for a large class of processes, which we call \"structurally coherent\" the confluence property for constructive reductions. We further show that under some syntactic restrictions, called \"pivotable\" the operators of prefix, summation, parallel composition, restriction and hiding preserve structural coherence. This covers a strictly larger class of processes compared to those that are confluent in Milner's classical theory of CCS without priorities.","sentences":["Building on the classical theory of process algebra with priorities, we identify a new scheduling mechanism, called \"sequentially constructive reduction\" which is designed to capture the essence of synchronous programming.","The distinctive property of this evaluation strategy is to achieve determinism-by-construction for multi-cast concurrent communication.","In particular, it permits us to model shared memory multi-threading with reaction to absence as it lies at the core of the programming language Esterel.","In the technical setting of CCS extended by clocks and priorities, we prove for a large class of processes, which we call \"structurally coherent\" the confluence property for constructive reductions.","We further show that under some syntactic restrictions, called \"pivotable\" the operators of prefix, summation, parallel composition, restriction and hiding preserve structural coherence.","This covers a strictly larger class of processes compared to those that are confluent in Milner's classical theory of CCS without priorities."],"url":"http://arxiv.org/abs/2403.04618v1","category":"cs.PL"}
{"created":"2024-03-07 16:01:08","title":"Modeling reputation-based behavioral biases in school choice","abstract":"A fundamental component in the theoretical school choice literature is the problem a student faces in deciding which schools to apply to. Recent models have considered a set of schools of different selectiveness and a student who is unsure of their strength and can apply to at most $k$ schools. Such models assume that the student cares solely about maximizing the quality of the school that they attend, but experience suggests that students' decisions are also influenced by a set of behavioral biases based on reputational effects: a subjective reputational benefit when admitted to a selective school, whether or not they attend; and a subjective loss based on disappointment when rejected. Guided by these observations, and inspired by recent behavioral economics work on loss aversion relative to expectations, we propose a behavioral model by which a student chooses schools to balance these behavioral effects with the quality of the school they attend.   Our main results show that a student's choices change in dramatic ways when these reputation-based behavioral biases are taken into account. In particular, where a rational applicant spreads their applications evenly, a biased student applies very sparsely to highly selective schools, such that above a certain threshold they apply to only an absolute constant number of schools even as their budget of applications grows to infinity. Consequently, a biased student underperforms a rational student even when the rational student is restricted to a sufficiently large upper bound on applications and the biased student can apply to arbitrarily many. Our analysis shows that the reputation-based model is rich enough to cover a range of different ways that biased students cope with fear of rejection, including not just targeting less selective schools, but also occasionally applying to schools that are too selective, compared to rational students.","sentences":["A fundamental component in the theoretical school choice literature is the problem a student faces in deciding which schools to apply to.","Recent models have considered a set of schools of different selectiveness and a student who is unsure of their strength and can apply to at most $k$ schools.","Such models assume that the student cares solely about maximizing the quality of the school that they attend, but experience suggests that students' decisions are also influenced by a set of behavioral biases based on reputational effects: a subjective reputational benefit when admitted to a selective school, whether or not they attend; and a subjective loss based on disappointment when rejected.","Guided by these observations, and inspired by recent behavioral economics work on loss aversion relative to expectations, we propose a behavioral model by which a student chooses schools to balance these behavioral effects with the quality of the school they attend.   ","Our main results show that a student's choices change in dramatic ways when these reputation-based behavioral biases are taken into account.","In particular, where a rational applicant spreads their applications evenly, a biased student applies very sparsely to highly selective schools, such that above a certain threshold they apply to only an absolute constant number of schools even as their budget of applications grows to infinity.","Consequently, a biased student underperforms a rational student even when the rational student is restricted to a sufficiently large upper bound on applications and the biased student can apply to arbitrarily many.","Our analysis shows that the reputation-based model is rich enough to cover a range of different ways that biased students cope with fear of rejection, including not just targeting less selective schools, but also occasionally applying to schools that are too selective, compared to rational students."],"url":"http://arxiv.org/abs/2403.04616v1","category":"cs.GT"}
{"created":"2024-03-07 15:54:53","title":"New catalog of distances to planetary nebulae based on Gaia parallaxes and statistical distances","abstract":"We have developed a method to determine the most reliable distances for a large group of planetary nebulae. For this purpose, we analyze the distances obtained from \\textit{Gaia} parallaxes and three determinations of statistical distances. The most reliable distance is derived for 2211 objects, and uncertainties for these distances are calculated in a homogeneous way. Using our most reliable distances, we compare the distributions of Galactic heights of hydrogen-poor and hydrogen-rich central stars of planetary nebulae. We find that [WR] central stars are closer to the Galactic plane than hydrogen-rich central stars and than other hydrogen-poor central stars. The latter have a similar distribution to hydrogen-rich central stars, which is significantly different from the one of [WR] central stars. This result disagrees with the proposed evolutionary sequence for hydrogen-poor central stars.","sentences":["We have developed a method to determine the most reliable distances for a large group of planetary nebulae.","For this purpose, we analyze the distances obtained from \\textit{Gaia} parallaxes and three determinations of statistical distances.","The most reliable distance is derived for 2211 objects, and uncertainties for these distances are calculated in a homogeneous way.","Using our most reliable distances, we compare the distributions of Galactic heights of hydrogen-poor and hydrogen-rich central stars of planetary nebulae.","We find that [WR] central stars are closer to the Galactic plane than hydrogen-rich central stars and than other hydrogen-poor central stars.","The latter have a similar distribution to hydrogen-rich central stars, which is significantly different from the one of [WR] central stars.","This result disagrees with the proposed evolutionary sequence for hydrogen-poor central stars."],"url":"http://arxiv.org/abs/2403.04606v1","category":"astro-ph.SR"}
{"created":"2024-03-07 15:46:19","title":"Optimizing Inventory Placement for a Downstream Online Matching Problem","abstract":"We study the inventory placement problem of splitting $Q$ units of a single item across warehouses, in advance of a downstream online matching problem that represents the dynamic fulfillment decisions of an e-commerce retailer. This is a challenging problem both in theory, because the downstream matching problem itself is computationally hard, and in practice, because the fulfillment team is constantly updating its algorithm and the placement team cannot directly evaluate how a placement decision would perform.   We compare the performance of three placement procedures based on optimizing surrogate functions that have been studied and applied: Offline, Myopic, and Fluid placement. On the theory side, we show that optimizing inventory placement for the Offline surrogate leads to a $(1-(1-1/d)^d)/2$-approximation for the joint placement and fulfillment problem. We assume $d$ is an upper bound on how many warehouses can serve any demand location and that stochastic arrivals satisfy either temporal or spatial independence. The crux of our theoretical contribution is to use randomized rounding to derive a tight $(1-(1-1/d)^d)$-approximation for the integer programming problem of optimizing the Offline surrogate. We use statistical learning to show that rounding after optimizing a sample-average Offline surrogate, which is necessary due to the exponentially-sized support, does indeed have vanishing loss.   On the experimental side, we extract real-world sequences of customer orders from publicly-available JD.com data and evaluate different combinations of placement and fulfillment procedures. Optimizing the Offline surrogate performs best overall, even compared to simulation procedures, corroborating our theory.","sentences":["We study the inventory placement problem of splitting $Q$ units of a single item across warehouses, in advance of a downstream online matching problem that represents the dynamic fulfillment decisions of an e-commerce retailer.","This is a challenging problem both in theory, because the downstream matching problem itself is computationally hard, and in practice, because the fulfillment team is constantly updating its algorithm and the placement team cannot directly evaluate how a placement decision would perform.   ","We compare the performance of three placement procedures based on optimizing surrogate functions that have been studied and applied: Offline, Myopic, and Fluid placement.","On the theory side, we show that optimizing inventory placement for the Offline surrogate leads to a $(1-(1-1/d)^d)/2$-approximation for the joint placement and fulfillment problem.","We assume $d$ is an upper bound on how many warehouses can serve any demand location and that stochastic arrivals satisfy either temporal or spatial independence.","The crux of our theoretical contribution is to use randomized rounding to derive a tight $(1-(1-1/d)^d)$-approximation for the integer programming problem of optimizing the Offline surrogate.","We use statistical learning to show that rounding after optimizing a sample-average Offline surrogate, which is necessary due to the exponentially-sized support, does indeed have vanishing loss.   ","On the experimental side, we extract real-world sequences of customer orders from publicly-available JD.com data and evaluate different combinations of placement and fulfillment procedures.","Optimizing the Offline surrogate performs best overall, even compared to simulation procedures, corroborating our theory."],"url":"http://arxiv.org/abs/2403.04598v1","category":"cs.DS"}
{"created":"2024-03-07 15:29:11","title":"Unbiased Estimator for Distorted Conics in Camera Calibration","abstract":"In the literature, points and conics have been major features for camera geometric calibration. Although conics are more informative features than points, the loss of the conic property under distortion has critically limited the utility of conic features in camera calibration. Many existing approaches addressed conic-based calibration by ignoring distortion or introducing 3D spherical targets to circumvent this limitation. In this paper, we present a novel formulation for conic-based calibration using moments. Our derivation is based on the mathematical finding that the first moment can be estimated without bias even under distortion. This allows us to track moment changes during projection and distortion, ensuring the preservation of the first moment of the distorted conic. With an unbiased estimator, the circular patterns can be accurately detected at the sub-pixel level and can now be fully exploited for an entire calibration pipeline, resulting in significantly improved calibration. The entire code is readily available from github.com/ChaehyeonSong/discocal.","sentences":["In the literature, points and conics have been major features for camera geometric calibration.","Although conics are more informative features than points, the loss of the conic property under distortion has critically limited the utility of conic features in camera calibration.","Many existing approaches addressed conic-based calibration by ignoring distortion or introducing 3D spherical targets to circumvent this limitation.","In this paper, we present a novel formulation for conic-based calibration using moments.","Our derivation is based on the mathematical finding that the first moment can be estimated without bias even under distortion.","This allows us to track moment changes during projection and distortion, ensuring the preservation of the first moment of the distorted conic.","With an unbiased estimator, the circular patterns can be accurately detected at the sub-pixel level and can now be fully exploited for an entire calibration pipeline, resulting in significantly improved calibration.","The entire code is readily available from github.com/ChaehyeonSong/discocal."],"url":"http://arxiv.org/abs/2403.04583v1","category":"cs.CV"}
{"created":"2024-03-07 15:01:14","title":"Estimating hidden population size from a single respondent-driven sampling survey","abstract":"This work is concerned with the estimation of hard-to-reach population sizes using a single respondent-driven sampling (RDS) survey, a variant of chain-referral sampling that leverages social relationships to reach members of a hidden population. The popularity of RDS as a standard approach for surveying hidden populations brings theoretical and methodological challenges regarding the estimation of population sizes, mainly for public health purposes. This paper proposes a frequentist, model-based framework for estimating the size of a hidden population using a network-based approach. An optimization algorithm is proposed for obtaining the identification region of the target parameter when model assumptions are violated. We characterize the asymptotic behavior of our proposed methodology and assess its finite sample performance under departures from model assumptions.","sentences":["This work is concerned with the estimation of hard-to-reach population sizes using a single respondent-driven sampling (RDS) survey, a variant of chain-referral sampling that leverages social relationships to reach members of a hidden population.","The popularity of RDS as a standard approach for surveying hidden populations brings theoretical and methodological challenges regarding the estimation of population sizes, mainly for public health purposes.","This paper proposes a frequentist, model-based framework for estimating the size of a hidden population using a network-based approach.","An optimization algorithm is proposed for obtaining the identification region of the target parameter when model assumptions are violated.","We characterize the asymptotic behavior of our proposed methodology and assess its finite sample performance under departures from model assumptions."],"url":"http://arxiv.org/abs/2403.04564v1","category":"stat.ME"}
{"created":"2024-03-07 14:50:34","title":"Dissociative recombination of NS+ in collisions with slow electrons","abstract":"Cross sections and rate coefficients for the Dissociative Recombination (DR) of the NS+ ion induced by collisions with low-energy electrons are reported for temperatures between 10 and 1000 K, relevant to a large range of interstellar cloud temperatures. Uncertainties are discussed for these rates. Comparisons are made with DR rates for the isovalent NO+ molecular ion which are found to be much faster. The present findings lead to a moderate dissociative reaction rate coefficient, smaller by a factor of 2 than the current estimates reported in the different kinetic databases for a temperature of 10 K. We consider that our rate coefficients obtained through multichannel quantum defect theory for NS+ are likely to be better than those displayed in the different kinetic databases.","sentences":["Cross sections and rate coefficients for the Dissociative Recombination (DR) of the NS+ ion induced by collisions with low-energy electrons are reported for temperatures between 10 and 1000 K, relevant to a large range of interstellar cloud temperatures.","Uncertainties are discussed for these rates.","Comparisons are made with DR rates for the isovalent NO+","molecular ion which are found to be much faster.","The present findings lead to a moderate dissociative reaction rate coefficient, smaller by a factor of 2 than the current estimates reported in the different kinetic databases for a temperature of 10 K. We consider that our rate coefficients obtained through multichannel quantum defect theory for NS+ are likely to be better than those displayed in the different kinetic databases."],"url":"http://arxiv.org/abs/2403.04554v1","category":"astro-ph.IM"}
{"created":"2024-03-07 14:10:43","title":"Low-amplitude solar-like oscillations in the K5 V star $\\varepsilon$ Indi A","abstract":"We have detected solar-like oscillations in the mid K-dwarf $\\varepsilon$ Indi A, making it the coolest dwarf to have measured oscillations. The star is noteworthy for harboring a pair of brown dwarf companions and a Jupiter-type planet. We observed $\\varepsilon$ Indi A during two radial velocity campaigns, using the high-resolution spectrographs HARPS (2011) and UVES (2021). Weighting the time series, we computed the power spectra and established the detection of solar-like oscillations with a power excess located at $5265 \\pm 110 \\ \\mu$Hz -- the highest frequency solar-like oscillations so far measured in any star. The measurement of the center of the power excess allows us to compute a stellar mass of $0.782 \\pm 0.023 \\ M_\\odot$ based on scaling relations and a known radius from interferometry. We also determine the amplitude of the peak power and note that there is a slight difference between the two observing campaigns, indicating a varying activity level. Overall, this work confirms that low-amplitude solar-like oscillations can be detected in mid-K type stars in radial velocity measurements obtained with high-precision spectrographs.","sentences":["We have detected solar-like oscillations in the mid K-dwarf $\\varepsilon$ Indi A, making it the coolest dwarf to have measured oscillations.","The star is noteworthy for harboring a pair of brown dwarf companions and a Jupiter-type planet.","We observed $\\varepsilon$ Indi A during two radial velocity campaigns, using the high-resolution spectrographs HARPS (2011) and UVES (2021).","Weighting the time series, we computed the power spectra and established the detection of solar-like oscillations with a power excess located at $5265 \\pm 110 \\ \\mu$Hz -- the highest frequency solar-like oscillations so far measured in any star.","The measurement of the center of the power excess allows us to compute a stellar mass of $0.782 \\pm 0.023 \\ M_\\odot$ based on scaling relations and a known radius from interferometry.","We also determine the amplitude of the peak power and note that there is a slight difference between the two observing campaigns, indicating a varying activity level.","Overall, this work confirms that low-amplitude solar-like oscillations can be detected in mid-K type stars in radial velocity measurements obtained with high-precision spectrographs."],"url":"http://arxiv.org/abs/2403.04509v1","category":"astro-ph.SR"}
{"created":"2024-03-07 11:47:00","title":"Modulation of 2-Point Discrimination Threshold Under Chemically Induced Global Stimulation","abstract":"The two-point discrimination threshold(2PDT) serves as a critical indicator in the study of tactile acuity, representing the minimal distance at which an individual can differentiate two distinct points of contact on the skin. This measurement is instrumental in exploring the neural mechanisms underlying tactile perception. On the other hand, tactile acuity can be modulated by global stimulation. Prior research indicates that experimental inflammation induced by an application of capsaicin cream increases2PDT. In our study, we applied chemicals(oregano, menthol, and Sichuan pepper) to selectively activate receptors that usually respond to mild physical stimuli to investigate their influence on2PDT without inducing inflammation. The results unveiled a pronounced augmentation of2PDT following any form of global stimulation. Intriguingly, the cumulative effect of the chemical mix on2PDT appeared to be additive. These observations suggest that Wide Dynamic Range(WDR) neurons, functioning as relay nuclei with projections for touch, warmth, and cold sensations, play a pivotal role in this process. In lateral connection structures parallel to afferent nerve transmission pathways with WDR neurons as relay nuclei, global stimulation amplifies excitatory connections over inhibitory ones, thereby elevating the two-point discrimination threshold.","sentences":["The two-point discrimination threshold(2PDT) serves as a critical indicator in the study of tactile acuity, representing the minimal distance at which an individual can differentiate two distinct points of contact on the skin.","This measurement is instrumental in exploring the neural mechanisms underlying tactile perception.","On the other hand, tactile acuity can be modulated by global stimulation.","Prior research indicates that experimental inflammation induced by an application of capsaicin cream increases2PDT.","In our study, we applied chemicals(oregano, menthol, and Sichuan pepper) to selectively activate receptors that usually respond to mild physical stimuli to investigate their influence on2PDT without inducing inflammation.","The results unveiled a pronounced augmentation of2PDT following any form of global stimulation.","Intriguingly, the cumulative effect of the chemical mix on2PDT appeared to be additive.","These observations suggest that Wide Dynamic Range(WDR) neurons, functioning as relay nuclei with projections for touch, warmth, and cold sensations, play a pivotal role in this process.","In lateral connection structures parallel to afferent nerve transmission pathways with WDR neurons as relay nuclei, global stimulation amplifies excitatory connections over inhibitory ones, thereby elevating the two-point discrimination threshold."],"url":"http://arxiv.org/abs/2403.04423v1","category":"q-bio.NC"}
{"created":"2024-03-07 10:55:39","title":"Conjugate operators for transparent, explorable research outputs","abstract":"Charts, figures, and text derived from data play an important role in decision making, from data-driven policy development to day-to-day choices informed by online articles. Making sense of, or fact-checking, outputs means understanding how they relate to the underlying data. Even for domain experts with access to the source code and data sets, this poses a significant challenge. In this paper we introduce a new program analysis framework which supports interactive exploration of fine-grained I/O relationships directly through computed outputs, making use of dynamic dependence graphs. Our main contribution is a novel notion in data provenance which we call related inputs, a relation of mutual relevance or \"cognacy\" which arises between inputs when they contribute to common features of the output. Queries of this form allow readers to ask questions like \"What outputs use this data element, and what other data elements are used along with it?\". We show how Jonsson and Tarski's concept of conjugate operators on Boolean algebras appropriately characterises the notion of cognacy in a dependence graph, and give a procedure for computing related inputs over such a graph.","sentences":["Charts, figures, and text derived from data play an important role in decision making, from data-driven policy development to day-to-day choices informed by online articles.","Making sense of, or fact-checking, outputs means understanding how they relate to the underlying data.","Even for domain experts with access to the source code and data sets, this poses a significant challenge.","In this paper we introduce a new program analysis framework which supports interactive exploration of fine-grained I/O relationships directly through computed outputs, making use of dynamic dependence graphs.","Our main contribution is a novel notion in data provenance which we call related inputs, a relation of mutual relevance or \"cognacy\" which arises between inputs when they contribute to common features of the output.","Queries of this form allow readers to ask questions like \"What outputs use this data element, and what other data elements are used along with it?\".","We show how Jonsson and Tarski's concept of conjugate operators on Boolean algebras appropriately characterises the notion of cognacy in a dependence graph, and give a procedure for computing related inputs over such a graph."],"url":"http://arxiv.org/abs/2403.04403v1","category":"cs.PL"}
{"created":"2024-03-07 10:55:25","title":"Analytic torsion for fibred boundary metrics and conic degeneration","abstract":"We study the renormalized analytic torsion of complete manifolds with fibred boundary metrics, also referred to as $\\phi$-metrics. We establish invariance of the torsion under suitable deformations of the metric, and establish a gluing formula. As an application, we relate the analytic torsions for complete $\\phi$- and incomplete wedge-metrics. As a simple consequence we recover a result by Sher and Guillarmou about analytic torsion under conic degeneration.","sentences":["We study the renormalized analytic torsion of complete manifolds with fibred boundary metrics, also referred to as $\\phi$-metrics.","We establish invariance of the torsion under suitable deformations of the metric, and establish a gluing formula.","As an application, we relate the analytic torsions for complete $\\phi$- and incomplete wedge-metrics.","As a simple consequence we recover a result by Sher and Guillarmou about analytic torsion under conic degeneration."],"url":"http://arxiv.org/abs/2403.04402v1","category":"math.DG"}
{"created":"2024-03-07 10:40:10","title":"Non-negative solutions of a sublinear elliptic problem","abstract":"In this paper the existence of solutions, $(\\lambda,u)$, of the problem $$-\\Delta u=\\lambda u -a(x)|u|^{p-1}u \\quad \\hbox{in }\\Omega, \\qquad   u=0 \\quad \\hbox{on}\\;\\;\\partial\\Omega,$$ is explored for $0 < p < 1$. When $p>1$, it is known that there is an unbounded component of such solutions bifurcating from $(\\sigma_1, 0)$, where $\\sigma_1$ is the smallest eigenvalue of $-\\Delta$ in $\\Omega$ under Dirichlet boundary conditions on $\\partial\\Omega$. These solutions have $u \\in P$, the interior of the positive cone. The continuation argument used when $p>1$ to keep $u \\in P$ fails if $0 < p < 1$. Nevertheless when $0 < p < 1$, we are still able to show that there is a component of solutions bifurcating from $(\\sigma_1, \\infty)$, unbounded outside of a neighborhood of $(\\sigma_1, \\infty)$, and having $u \\gneq 0$. This non-negativity for $u$ cannot be improved as is shown via a detailed analysis of the simplest autonomous one-dimensional version of the problem: its set of non-negative solutions possesses a countable set of components, each of them consisting of positive solutions with a fixed (arbitrary) number of bumps. Finally, the structure of these components is fully described.","sentences":["In this paper the existence of solutions, $(\\lambda,u)$, of the problem $$-\\Delta u=\\lambda u -a(x)|u|^{p-1}u \\quad \\hbox{in }\\Omega, \\qquad   u=0","\\quad \\hbox{on}\\;\\;\\partial\\Omega,$$ is explored for $0 < p < 1$.","When $p>1$, it is known that there is an unbounded component of such solutions bifurcating from $(\\sigma_1, 0)$, where $\\sigma_1$ is the smallest eigenvalue of $-\\Delta$ in $\\Omega$ under Dirichlet boundary conditions on $\\partial\\Omega$. These solutions have $u \\in P$, the interior of the positive cone.","The continuation argument used when $p>1$ to keep $u \\in P$ fails if $0 < p <","1$.","Nevertheless when $0 < p < 1$, we are still able to show that there is a component of solutions bifurcating from $(\\sigma_1, \\infty)$, unbounded outside of a neighborhood of $(\\sigma_1, \\infty)$, and having $u \\gneq 0$.","This non-negativity for $u$ cannot be improved as is shown via a detailed analysis of the simplest autonomous one-dimensional version of the problem: its set of non-negative solutions possesses a countable set of components, each of them consisting of positive solutions with a fixed (arbitrary) number of bumps.","Finally, the structure of these components is fully described."],"url":"http://arxiv.org/abs/2403.04396v1","category":"math.AP"}
{"created":"2024-03-07 10:31:46","title":"Contribution of Population III Stars to Merging Binary Black Holes","abstract":"A large number of mergers of binary black holes (BHs) have been discovered by gravitational wave observations since the first detection of gravitational waves 2015. Binary BH mergers are the loudest events in the universe, however their origin(s) have been under debate. There have been many suggestions for merging binary BHs. Isolated binary stars are one of the most promising origins. We have investigated the evolution of isolated binary stars ranging from zero metallicity (Population III stars or Pop III stars) to the solar metallicity by means of so-called rapid binary population synthesis simulation. We have found that binary BHs formed from isolated binary stars reproduce the redshift evolution of the merger rate density and the distribution of primary BH masses and mass ratios inferred by Gravitational-Wave Transient Catalog 3 (GWTC-3). Pop III stars have a crucial role in forming merging binary BHs in so-called the pair instability mass gap. Note that we choose the conventional prescription of pair instability mass loss, based on the standard $^{12}$C($\\alpha$,$\\gamma$)$^{16}$O reaction rate. Finally, we have shown the redshift evolution of the rate density of pair instability supernovae, and have predicted that a few pair instability supernovae would be discovered in the next few years. The discoveries would validate our results of merging binary BHs.","sentences":["A large number of mergers of binary black holes (BHs) have been discovered by gravitational wave observations since the first detection of gravitational waves 2015.","Binary BH mergers are the loudest events in the universe, however their origin(s) have been under debate.","There have been many suggestions for merging binary BHs.","Isolated binary stars are one of the most promising origins.","We have investigated the evolution of isolated binary stars ranging from zero metallicity (Population III stars or Pop III stars) to the solar metallicity by means of so-called rapid binary population synthesis simulation.","We have found that binary BHs formed from isolated binary stars reproduce the redshift evolution of the merger rate density and the distribution of primary BH masses and mass ratios inferred by Gravitational-Wave Transient Catalog 3 (GWTC-3).","Pop III stars have a crucial role in forming merging binary BHs in so-called the pair instability mass gap.","Note that we choose the conventional prescription of pair instability mass loss, based on the standard $^{12}$C($\\alpha$,$\\gamma$)$^{16}$O reaction rate.","Finally, we have shown the redshift evolution of the rate density of pair instability supernovae, and have predicted that a few pair instability supernovae would be discovered in the next few years.","The discoveries would validate our results of merging binary BHs."],"url":"http://arxiv.org/abs/2403.04389v1","category":"astro-ph.HE"}
{"created":"2024-03-07 10:04:44","title":"Radial dependence of ionization clustering around a gold nanoparticle","abstract":"This work explores the enhancement of ionization clusters around a gold nanoparticle (NP), indicative of the induction of DNA lesions, a potential trigger for cell-death. Monte Carlo track structure simulations were performed to determine (a) the fluence of incident photons and electrons in water around a gold NP under charged particle equilibrium conditions and (b) the density of ionization clusters produced on average as well as conditional on the occurrence of at least one interaction in the nanoparticle using Associated Volume Clustering. Absorbed dose was determined for comparison with a recent benchmark intercomparison. Reported quantities are normalized to primary fluence, allowing to establish a connection to macroscopic dosimetric quantities. The modification of the electron fluence spectrum by the gold NP is minor and mainly occurs at low energies. The net fluence of electrons emitted from the NP is dominated by electrons resulting from photon interactions. Smaller NPs cause noticeable peaks in the conditional frequency of clusters at distances around 50 nm to 100 nm from the NP surface. The number of clusters per energy imparted is increased at distances of up to 150 nm, and accordingly the enhancement in clustering notably surpasses that of dose enhancement. This work highlights the necessity of nanodosimetric analysis and suggests increased ionization clustering near the nanoparticles due to the emission of low energy Auger electrons. Whereas the electron component of the radiation field plays an important role in determining the background contribution to ionization clustering and energy imparted, the dosimetric effects of nanoparticles are governed by the interplay of secondary electron production by photon interaction (including low energy Auger electrons) and their ability to leave the nanoparticle.","sentences":["This work explores the enhancement of ionization clusters around a gold nanoparticle (NP), indicative of the induction of DNA lesions, a potential trigger for cell-death.","Monte Carlo track structure simulations were performed to determine (a) the fluence of incident photons and electrons in water around a gold NP under charged particle equilibrium conditions and (b) the density of ionization clusters produced on average as well as conditional on the occurrence of at least one interaction in the nanoparticle using Associated Volume Clustering.","Absorbed dose was determined for comparison with a recent benchmark intercomparison.","Reported quantities are normalized to primary fluence, allowing to establish a connection to macroscopic dosimetric quantities.","The modification of the electron fluence spectrum by the gold NP is minor and mainly occurs at low energies.","The net fluence of electrons emitted from the NP is dominated by electrons resulting from photon interactions.","Smaller NPs cause noticeable peaks in the conditional frequency of clusters at distances around 50 nm to 100 nm from the NP surface.","The number of clusters per energy imparted is increased at distances of up to 150 nm, and accordingly the enhancement in clustering notably surpasses that of dose enhancement.","This work highlights the necessity of nanodosimetric analysis and suggests increased ionization clustering near the nanoparticles due to the emission of low energy Auger electrons.","Whereas the electron component of the radiation field plays an important role in determining the background contribution to ionization clustering and energy imparted, the dosimetric effects of nanoparticles are governed by the interplay of secondary electron production by photon interaction (including low energy Auger electrons) and their ability to leave the nanoparticle."],"url":"http://arxiv.org/abs/2403.04373v1","category":"physics.med-ph"}
{"created":"2024-03-07 08:41:04","title":"Sublinear expectation structure under finite states space","abstract":"In this study, we propose the sublinear expectation structure under finite states space. To describe an interesting \"nonlinear randomized\" trial, based on a convex closed domain, we introduce a family of probability measures under finite states space. Corresponding the sublinear expectation operator introduced by S. Peng, we consider the related notation under finite states space. Within the finite states framework, the sublinear expectation can be explicitly calculated by a novel repeated summation formula, and some interesting examples are given. Furthermore, we establish Monotone convergence theorem, Fatou's lemma and Dominated convergence theorem under finite states space. Afterwards, we consider the independence under each probability measure, upon which we establish the nonlinear law of large numbers and obtain the maximal distribution under sublinear expectation.","sentences":["In this study, we propose the sublinear expectation structure under finite states space.","To describe an interesting \"nonlinear randomized\" trial, based on a convex closed domain, we introduce a family of probability measures under finite states space.","Corresponding the sublinear expectation operator introduced by S. Peng, we consider the related notation under finite states space.","Within the finite states framework, the sublinear expectation can be explicitly calculated by a novel repeated summation formula, and some interesting examples are given.","Furthermore, we establish Monotone convergence theorem, Fatou's lemma and Dominated convergence theorem under finite states space.","Afterwards, we consider the independence under each probability measure, upon which we establish the nonlinear law of large numbers and obtain the maximal distribution under sublinear expectation."],"url":"http://arxiv.org/abs/2403.04324v1","category":"math.PR"}
{"created":"2024-03-07 07:43:04","title":"A$^{3}$lign-DFER: Pioneering Comprehensive Dynamic Affective Alignment for Dynamic Facial Expression Recognition with CLIP","abstract":"The performance of CLIP in dynamic facial expression recognition (DFER) task doesn't yield exceptional results as observed in other CLIP-based classification tasks. While CLIP's primary objective is to achieve alignment between images and text in the feature space, DFER poses challenges due to the abstract nature of text and the dynamic nature of video, making label representation limited and perfect alignment difficult. To address this issue, we have designed A$^{3}$lign-DFER, which introduces a new DFER labeling paradigm to comprehensively achieve alignment, thus enhancing CLIP's suitability for the DFER task. Specifically, our A$^{3}$lign-DFER method is designed with multiple modules that work together to obtain the most suitable expanded-dimensional embeddings for classification and to achieve alignment in three key aspects: affective, dynamic, and bidirectional. We replace the input label text with a learnable Multi-Dimensional Alignment Token (MAT), enabling alignment of text to facial expression video samples in both affective and dynamic dimensions. After CLIP feature extraction, we introduce the Joint Dynamic Alignment Synchronizer (JAS), further facilitating synchronization and alignment in the temporal dimension. Additionally, we implement a Bidirectional Alignment Training Paradigm (BAP) to ensure gradual and steady training of parameters for both modalities. Our insightful and concise A$^{3}$lign-DFER method achieves state-of-the-art results on multiple DFER datasets, including DFEW, FERV39k, and MAFW. Extensive ablation experiments and visualization studies demonstrate the effectiveness of A$^{3}$lign-DFER. The code will be available in the future.","sentences":["The performance of CLIP in dynamic facial expression recognition (DFER) task doesn't yield exceptional results as observed in other CLIP-based classification tasks.","While CLIP's primary objective is to achieve alignment between images and text in the feature space, DFER poses challenges due to the abstract nature of text and the dynamic nature of video, making label representation limited and perfect alignment difficult.","To address this issue, we have designed A$^{3}$lign-DFER, which introduces a new DFER labeling paradigm to comprehensively achieve alignment, thus enhancing CLIP's suitability for the DFER task.","Specifically, our A$^{3}$lign-DFER method is designed with multiple modules that work together to obtain the most suitable expanded-dimensional embeddings for classification and to achieve alignment in three key aspects: affective, dynamic, and bidirectional.","We replace the input label text with a learnable Multi-Dimensional Alignment Token (MAT), enabling alignment of text to facial expression video samples in both affective and dynamic dimensions.","After CLIP feature extraction, we introduce the Joint Dynamic Alignment Synchronizer (JAS), further facilitating synchronization and alignment in the temporal dimension.","Additionally, we implement a Bidirectional Alignment Training Paradigm (BAP) to ensure gradual and steady training of parameters for both modalities.","Our insightful and concise A$^{3}$lign-DFER method achieves state-of-the-art results on multiple DFER datasets, including DFEW, FERV39k, and MAFW.","Extensive ablation experiments and visualization studies demonstrate the effectiveness of A$^{3}$lign-DFER.","The code will be available in the future."],"url":"http://arxiv.org/abs/2403.04294v1","category":"cs.CV"}
{"created":"2024-03-07 07:24:11","title":"SSDRec: Self-Augmented Sequence Denoising for Sequential Recommendation","abstract":"Traditional sequential recommendation methods assume that users' sequence data is clean enough to learn accurate sequence representations to reflect user preferences. In practice, users' sequences inevitably contain noise (e.g., accidental interactions), leading to incorrect reflections of user preferences. Consequently, some pioneer studies have explored modeling sequentiality and correlations in sequences to implicitly or explicitly reduce noise's influence. However, relying on only available intra-sequence information (i.e., sequentiality and correlations in a sequence) is insufficient and may result in over-denoising and under-denoising problems (OUPs), especially for short sequences. To improve reliability, we propose to augment sequences by inserting items before denoising. However, due to the data sparsity issue and computational costs, it is challenging to select proper items from the entire item universe to insert into proper positions in a target sequence. Motivated by the above observation, we propose a novel framework--Self-augmented Sequence Denoising for sequential Recommendation (SSDRec) with a three-stage learning paradigm to solve the above challenges. In the first stage, we empower SSDRec by a global relation encoder to learn multi-faceted inter-sequence relations in a data-driven manner. These relations serve as prior knowledge to guide subsequent stages. In the second stage, we devise a self-augmentation module to augment sequences to alleviate OUPs. Finally, we employ a hierarchical denoising module in the third stage to reduce the risk of false augmentations and pinpoint all noise in raw sequences. Extensive experiments on five real-world datasets demonstrate the superiority of \\model over state-of-the-art denoising methods and its flexible applications to mainstream sequential recommendation models. The source code is available at https://github.com/zc-97/SSDRec.","sentences":["Traditional sequential recommendation methods assume that users' sequence data is clean enough to learn accurate sequence representations to reflect user preferences.","In practice, users' sequences inevitably contain noise (e.g., accidental interactions), leading to incorrect reflections of user preferences.","Consequently, some pioneer studies have explored modeling sequentiality and correlations in sequences to implicitly or explicitly reduce noise's influence.","However, relying on only available intra-sequence information (i.e., sequentiality and correlations in a sequence) is insufficient and may result in over-denoising and under-denoising problems (OUPs), especially for short sequences.","To improve reliability, we propose to augment sequences by inserting items before denoising.","However, due to the data sparsity issue and computational costs, it is challenging to select proper items from the entire item universe to insert into proper positions in a target sequence.","Motivated by the above observation, we propose a novel framework--Self-augmented Sequence Denoising for sequential Recommendation (SSDRec) with a three-stage learning paradigm to solve the above challenges.","In the first stage, we empower SSDRec by a global relation encoder to learn multi-faceted inter-sequence relations in a data-driven manner.","These relations serve as prior knowledge to guide subsequent stages.","In the second stage, we devise a self-augmentation module to augment sequences to alleviate OUPs.","Finally, we employ a hierarchical denoising module in the third stage to reduce the risk of false augmentations and pinpoint all noise in raw sequences.","Extensive experiments on five real-world datasets demonstrate the superiority of \\model over state-of-the-art denoising methods and its flexible applications to mainstream sequential recommendation models.","The source code is available at https://github.com/zc-97/SSDRec."],"url":"http://arxiv.org/abs/2403.04278v1","category":"cs.IR"}
{"created":"2024-03-07 07:19:55","title":"Certain observations on selection principles related to bornological covers using ideals","abstract":"We study selection principles related to bornological covers using the notion of ideals. We consider ideals $\\mathcal I$ and $\\mathcal J$ on $\\omega$ and standard ideal orderings $KB, K$. Relations between cardinality of a base of a bornology with certain selection principles related to bornological covers are established using cardinal invariants such as modified pseudointersection number, the unbounding number and slaloms numbers. When $\\mathcal I \\leq_\\square \\mathcal J$ for ideals $\\mathcal I, \\mathcal J$ and $\\square\\in \\{1\\text{-}1,KB,K\\}$, implications among various selection principles related to bornological covers are established. Under the assumption that ideal $\\mathcal I$ has a pseudounion we show equivalences among certain selection principles related to bornological covers. Finally, the $\\mathcal I\\text{-}\\mathfrak B^s$-Hurewicz property of $X$ is investigated. We prove that $\\mathcal I\\text{-}\\mathfrak B^s$-Hurewicz property of $X$ coincides with the $\\mathfrak B^s$-Hurewicz property of $X$ if $\\mathcal I$ has a pseudounion. Implications or equivalences among selection principles, games and $\\mathcal I\\text{-}\\mathfrak B^s$-Hurewicz property which are obtained from our investigations are described in diagrams.","sentences":["We study selection principles related to bornological covers using the notion of ideals.","We consider ideals $\\mathcal I$ and $\\mathcal J$ on $\\omega$ and standard ideal orderings $KB, K$. Relations between cardinality of a base of a bornology with certain selection principles related to bornological covers are established using cardinal invariants such as modified pseudointersection number, the unbounding number and slaloms numbers.","When $\\mathcal I \\leq_\\square \\mathcal J$ for ideals $\\mathcal I, \\mathcal J$ and $\\square\\in \\{1\\text{-}1,KB,K\\}$, implications among various selection principles related to bornological covers are established.","Under the assumption that ideal $\\mathcal I$ has a pseudounion we show equivalences among certain selection principles related to bornological covers.","Finally, the $\\mathcal I\\text{-}\\mathfrak B^s$-Hurewicz property of $X$ is investigated.","We prove that $\\mathcal I\\text{-}\\mathfrak B^s$-Hurewicz property of $X$ coincides with the $\\mathfrak B^s$-Hurewicz property of $X$ if $\\mathcal I$ has a pseudounion.","Implications or equivalences among selection principles, games and $\\mathcal I\\text{-}\\mathfrak B^s$-Hurewicz property which are obtained from our investigations are described in diagrams."],"url":"http://arxiv.org/abs/2403.04276v1","category":"math.GN"}
{"created":"2024-03-07 07:12:24","title":"Active Generalized Category Discovery","abstract":"Generalized Category Discovery (GCD) is a pragmatic and challenging open-world task, which endeavors to cluster unlabeled samples from both novel and old classes, leveraging some labeled data of old classes. Given that knowledge learned from old classes is not fully transferable to new classes, and that novel categories are fully unlabeled, GCD inherently faces intractable problems, including imbalanced classification performance and inconsistent confidence between old and new classes, especially in the low-labeling regime. Hence, some annotations of new classes are deemed necessary. However, labeling new classes is extremely costly. To address this issue, we take the spirit of active learning and propose a new setting called Active Generalized Category Discovery (AGCD). The goal is to improve the performance of GCD by actively selecting a limited amount of valuable samples for labeling from the oracle. To solve this problem, we devise an adaptive sampling strategy, which jointly considers novelty, informativeness and diversity to adaptively select novel samples with proper uncertainty. However, owing to the varied orderings of label indices caused by the clustering of novel classes, the queried labels are not directly applicable to subsequent training. To overcome this issue, we further propose a stable label mapping algorithm that transforms ground truth labels to the label space of the classifier, thereby ensuring consistent training across different active selection stages. Our method achieves state-of-the-art performance on both generic and fine-grained datasets. Our code is available at https://github.com/mashijie1028/ActiveGCD","sentences":["Generalized Category Discovery (GCD) is a pragmatic and challenging open-world task, which endeavors to cluster unlabeled samples from both novel and old classes, leveraging some labeled data of old classes.","Given that knowledge learned from old classes is not fully transferable to new classes, and that novel categories are fully unlabeled, GCD inherently faces intractable problems, including imbalanced classification performance and inconsistent confidence between old and new classes, especially in the low-labeling regime.","Hence, some annotations of new classes are deemed necessary.","However, labeling new classes is extremely costly.","To address this issue, we take the spirit of active learning and propose a new setting called Active Generalized Category Discovery (AGCD).","The goal is to improve the performance of GCD by actively selecting a limited amount of valuable samples for labeling from the oracle.","To solve this problem, we devise an adaptive sampling strategy, which jointly considers novelty, informativeness and diversity to adaptively select novel samples with proper uncertainty.","However, owing to the varied orderings of label indices caused by the clustering of novel classes, the queried labels are not directly applicable to subsequent training.","To overcome this issue, we further propose a stable label mapping algorithm that transforms ground truth labels to the label space of the classifier, thereby ensuring consistent training across different active selection stages.","Our method achieves state-of-the-art performance on both generic and fine-grained datasets.","Our code is available at https://github.com/mashijie1028/ActiveGCD"],"url":"http://arxiv.org/abs/2403.04272v1","category":"cs.CV"}
{"created":"2024-03-07 07:09:23","title":"Secure MIMO Communication Relying on Movable Antennas","abstract":"This paper considers a movable antenna (MA)-aided secure multiple-input multiple-output (MIMO) communication system consisting of a base station (BS), a legitimate information receiver (IR) and an eavesdropper (Eve), where the BS is equipped with MAs to enhance the system's physical layer security (PLS). Specifically, we aim to maximize the secrecy rate (SR) by jointly optimizing the transmit precoding (TPC) matrix, the artificial noise (AN) covariance matrix and the MAs' positions under the constraints of the maximum transmit power and the minimum distance between MAs. To solve this non-convex problem with highly coupled optimization variables, the block coordinate descent (BCD) method is applied to alternately update the variables. Specifically, we first reformulate the SR into a tractable form by utilizing the minimum mean square error (MMSE) method, and derive the optimal TPC matrix and the AN covariance matrix with fixed MAs' positions by applying the Lagrangian multiplier method in semi-closed forms. Then, the majorization-minimization (MM) algorithm is employed to iteratively optimize each MA's position while keeping others fixed. Finally, simulation results are provided to demonstrate the effectiveness of the proposed algorithms and the significant advantages of the MA-aided system over conventional fixed position antenna (FPA)-based system in enhancing system's security.","sentences":["This paper considers a movable antenna (MA)-aided secure multiple-input multiple-output (MIMO) communication system consisting of a base station (BS), a legitimate information receiver (IR) and an eavesdropper (Eve), where the BS is equipped with MAs to enhance the system's physical layer security (PLS).","Specifically, we aim to maximize the secrecy rate (SR) by jointly optimizing the transmit precoding (TPC) matrix, the artificial noise (AN) covariance matrix and the MAs' positions under the constraints of the maximum transmit power and the minimum distance between MAs.","To solve this non-convex problem with highly coupled optimization variables, the block coordinate descent (BCD) method is applied to alternately update the variables.","Specifically, we first reformulate the SR into a tractable form by utilizing the minimum mean square error (MMSE) method, and derive the optimal TPC matrix and the AN covariance matrix with fixed MAs' positions by applying the Lagrangian multiplier method in semi-closed forms.","Then, the majorization-minimization (MM) algorithm is employed to iteratively optimize each MA's position while keeping others fixed.","Finally, simulation results are provided to demonstrate the effectiveness of the proposed algorithms and the significant advantages of the MA-aided system over conventional fixed position antenna (FPA)-based system in enhancing system's security."],"url":"http://arxiv.org/abs/2403.04269v1","category":"cs.IT"}
{"created":"2024-03-07 06:55:42","title":"Switching Classes: Characterization and Computation","abstract":"In a graph, the switching operation reverses adjacencies between a subset of vertices and the others. For a hereditary graph class $\\mathcal{G}$, we are concerned with the maximum subclass and the minimum superclass of $\\mathcal{G}$ that are closed under switching. We characterize the maximum subclass for many important classes $\\mathcal{G}$, and prove that it is finite when $\\mathcal{G}$ is minor-closed and omits at least one graph. For several graph classes, we develop polynomial-time algorithms to recognize the minimum superclass. We also show that the recognition of the superclass is NP-complete for $H$-free graphs when $H$ is a sufficiently long path or cycle, and it cannot be solved in subexponential time assuming the Exponential Time Hypothesis.","sentences":["In a graph, the switching operation reverses adjacencies between a subset of vertices and the others.","For a hereditary graph class $\\mathcal{G}$, we are concerned with the maximum subclass and the minimum superclass of $\\mathcal{G}$ that are closed under switching.","We characterize the maximum subclass for many important classes $\\mathcal{G}$, and prove that it is finite when $\\mathcal{G}$ is minor-closed and omits at least one graph.","For several graph classes, we develop polynomial-time algorithms to recognize the minimum superclass.","We also show that the recognition of the superclass is NP-complete for $H$-free graphs when $H$ is a sufficiently long path or cycle, and it cannot be solved in subexponential time assuming the Exponential Time Hypothesis."],"url":"http://arxiv.org/abs/2403.04263v1","category":"cs.DS"}
{"created":"2024-03-07 06:40:53","title":"Depth-aware Test-Time Training for Zero-shot Video Object Segmentation","abstract":"Zero-shot Video Object Segmentation (ZSVOS) aims at segmenting the primary moving object without any human annotations. Mainstream solutions mainly focus on learning a single model on large-scale video datasets, which struggle to generalize to unseen videos. In this work, we introduce a test-time training (TTT) strategy to address the problem. Our key insight is to enforce the model to predict consistent depth during the TTT process. In detail, we first train a single network to perform both segmentation and depth prediction tasks. This can be effectively learned with our specifically designed depth modulation layer. Then, for the TTT process, the model is updated by predicting consistent depth maps for the same frame under different data augmentations. In addition, we explore different TTT weight updating strategies. Our empirical results suggest that the momentum-based weight initialization and looping-based training scheme lead to more stable improvements. Experiments show that the proposed method achieves clear improvements on ZSVOS. Our proposed video TTT strategy provides significant superiority over state-of-the-art TTT methods. Our code is available at: https://nifangbaage.github.io/DATTT.","sentences":["Zero-shot Video Object Segmentation (ZSVOS) aims at segmenting the primary moving object without any human annotations.","Mainstream solutions mainly focus on learning a single model on large-scale video datasets, which struggle to generalize to unseen videos.","In this work, we introduce a test-time training (TTT) strategy to address the problem.","Our key insight is to enforce the model to predict consistent depth during the TTT process.","In detail, we first train a single network to perform both segmentation and depth prediction tasks.","This can be effectively learned with our specifically designed depth modulation layer.","Then, for the TTT process, the model is updated by predicting consistent depth maps for the same frame under different data augmentations.","In addition, we explore different TTT weight updating strategies.","Our empirical results suggest that the momentum-based weight initialization and looping-based training scheme lead to more stable improvements.","Experiments show that the proposed method achieves clear improvements on ZSVOS.","Our proposed video TTT strategy provides significant superiority over state-of-the-art TTT methods.","Our code is available at: https://nifangbaage.github.io/DATTT."],"url":"http://arxiv.org/abs/2403.04258v1","category":"cs.CV"}
{"created":"2024-03-07 06:16:28","title":"Asymptotic Theory for Linear Functionals of Kernel Ridge Regression","abstract":"An asymptotic theory is established for linear functionals of the predictive function given by kernel ridge regression, when the reproducing kernel Hilbert space is equivalent to a Sobolev space. The theory covers a wide variety of linear functionals, including point evaluations, evaluation of derivatives, $L_2$ inner products, etc. We establish the upper and lower bounds of the estimates and their asymptotic normality. It is shown that $\\lambda\\sim n^{-1}$ is the universal optimal order of magnitude for the smoothing parameter to balance the variance and the worst-case bias. The theory also implies that the optimal $L_\\infty$ error of kernel ridge regression can be attained under the optimal smoothing parameter $\\lambda\\sim n^{-1}\\log n$. These optimal rates for the smoothing parameter differ from the known optimal rate $\\lambda\\sim n^{-\\frac{2m}{2m+d}}$ that minimizes the $L_2$ error of the kernel ridge regression.","sentences":["An asymptotic theory is established for linear functionals of the predictive function given by kernel ridge regression, when the reproducing kernel Hilbert space is equivalent to a Sobolev space.","The theory covers a wide variety of linear functionals, including point evaluations, evaluation of derivatives, $L_2$ inner products, etc.","We establish the upper and lower bounds of the estimates and their asymptotic normality.","It is shown that $\\lambda\\sim n^{-1}$ is the universal optimal order of magnitude for the smoothing parameter to balance the variance and the worst-case bias.","The theory also implies that the optimal $L_\\infty$ error of kernel ridge regression can be attained under the optimal smoothing parameter $\\lambda\\sim n^{-1}\\log n$. These optimal rates for the smoothing parameter differ from the known optimal rate $\\lambda\\sim n^{-\\frac{2m}{2m+d}}$ that minimizes the $L_2$ error of the kernel ridge regression."],"url":"http://arxiv.org/abs/2403.04248v1","category":"math.ST"}
{"created":"2024-03-07 06:06:55","title":"A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition","abstract":"Advanced Audio-Visual Speech Recognition (AVSR) systems have been observed to be sensitive to missing video frames, performing even worse than single-modality models. While applying the dropout technique to the video modality enhances robustness to missing frames, it simultaneously results in a performance loss when dealing with complete data input. In this paper, we investigate this contrasting phenomenon from the perspective of modality bias and reveal that an excessive modality bias on the audio caused by dropout is the underlying reason. Moreover, we present the Modality Bias Hypothesis (MBH) to systematically describe the relationship between modality bias and robustness against missing modality in multimodal systems. Building on these findings, we propose a novel Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on the audio modality and to maintain performance and robustness simultaneously. Finally, to address an entirely missing modality, we adopt adapters to dynamically switch decision strategies. The effectiveness of our proposed approach is evaluated and validated through a series of comprehensive experiments using the MISP2021 and MISP2022 datasets. Our code is available at https://github.com/dalision/ModalBiasAVSR","sentences":["Advanced Audio-Visual Speech Recognition (AVSR) systems have been observed to be sensitive to missing video frames, performing even worse than single-modality models.","While applying the dropout technique to the video modality enhances robustness to missing frames, it simultaneously results in a performance loss when dealing with complete data input.","In this paper, we investigate this contrasting phenomenon from the perspective of modality bias and reveal that an excessive modality bias on the audio caused by dropout is the underlying reason.","Moreover, we present the Modality Bias Hypothesis (MBH) to systematically describe the relationship between modality bias and robustness against missing modality in multimodal systems.","Building on these findings, we propose a novel Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on the audio modality and to maintain performance and robustness simultaneously.","Finally, to address an entirely missing modality, we adopt adapters to dynamically switch decision strategies.","The effectiveness of our proposed approach is evaluated and validated through a series of comprehensive experiments using the MISP2021 and MISP2022 datasets.","Our code is available at https://github.com/dalision/ModalBiasAVSR"],"url":"http://arxiv.org/abs/2403.04245v1","category":"cs.SD"}
{"created":"2024-03-07 06:04:51","title":"Resonant Quantum Magnetodielectric Effect in Multiferroic Metal-Organic Framework [CH3NH3]Co(HCOO)3","abstract":"We report the observation of both resonant quantum tunneling of magnetization (RQTM) and resonant quantum magnetodielectric (RQMD) effect in the perovskite multiferroic metal-organic framework [CH3NH3]Co(HCOO)3. An intrinsic magnetic phase separation emerges at low temperatures due to hydrogen-bond-modified long range super-exchange interaction, leading to the coexistence of canted antiferromagnetic order and single-ion magnet. Subsequently, a stair-shaped magnetic hysteresis loop along the [101] direction characterizing the RQTM appears below the magnetic blocking temperature. More interestingly, the magnetic field dependence of dielectric permittivity exhibits pronounced negative peaks at the critical fields corresponding to the RQTM, a phenomenon termed the RQMD effect which enables electrical detection of the RQTM. These intriguing properties make the multiferroic metal-organic framework a promising candidate for solid-state quantum computing.","sentences":["We report the observation of both resonant quantum tunneling of magnetization (RQTM) and resonant quantum magnetodielectric (RQMD) effect in the perovskite multiferroic metal-organic framework [CH3NH3]Co(HCOO)3.","An intrinsic magnetic phase separation emerges at low temperatures due to hydrogen-bond-modified long range super-exchange interaction, leading to the coexistence of canted antiferromagnetic order and single-ion magnet.","Subsequently, a stair-shaped magnetic hysteresis loop along the [101] direction characterizing the RQTM appears below the magnetic blocking temperature.","More interestingly, the magnetic field dependence of dielectric permittivity exhibits pronounced negative peaks at the critical fields corresponding to the RQTM, a phenomenon termed the RQMD effect which enables electrical detection of the RQTM.","These intriguing properties make the multiferroic metal-organic framework a promising candidate for solid-state quantum computing."],"url":"http://arxiv.org/abs/2403.04244v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-07 05:57:45","title":"Pressure tuning of hydrogen bond ordering in the metal-organic framework [(CH3)2NH2]Mn(HCOO)3","abstract":"The influence of pressure on the hydrogen bond ordering in the perovskite metal-organic framework [(CH3)2NH2]Mn(HCOO)3 has been investigated by dielectric, pyroelectric adn magnetic measurements in a piston-cylinder cell. Under ambient pressure the ordering of hydrogen bonds takes place at TC = 188 K and induces a first-order ferroelectric phase transition. With increasing pressure to p = 3.92 kbar, the order-disorder transition shifts to a lower temperature and retains the first-order ferroelectric nature. However, under higher pressures, the ordering process of hydrogen bonds is split into two transitions: a broad antiferroelectric transition at high temperature and a first-order ferroelectric transition at low temperature. With increasing pressure, the antiferroelectric phase is enhanced whereas the ferroelectric phase is greatly suppressed, which implies that compression of the perovskite framework favors antiparallel arrangement of the hydrogen bonds. The canted anti-ferromagnetic transition was almost unchanged when pressure up to 10.85 kbar. Our study demonstrated that the perovskite metal-organic frameworks are more sensitive to external pressure than conventional perovskite oxides so that their electric properties can be easily tuned by pressure.","sentences":["The influence of pressure on the hydrogen bond ordering in the perovskite metal-organic framework [(CH3)2NH2]Mn(HCOO)3 has been investigated by dielectric, pyroelectric adn magnetic measurements in a piston-cylinder cell.","Under ambient pressure the ordering of hydrogen bonds takes place at TC = 188 K and induces a first-order ferroelectric phase transition.","With increasing pressure to p = 3.92 kbar, the order-disorder transition shifts to a lower temperature and retains the first-order ferroelectric nature.","However, under higher pressures, the ordering process of hydrogen bonds is split into two transitions: a broad antiferroelectric transition at high temperature and a first-order ferroelectric transition at low temperature.","With increasing pressure, the antiferroelectric phase is enhanced whereas the ferroelectric phase is greatly suppressed, which implies that compression of the perovskite framework favors antiparallel arrangement of the hydrogen bonds.","The canted anti-ferromagnetic transition was almost unchanged when pressure up to 10.85 kbar.","Our study demonstrated that the perovskite metal-organic frameworks are more sensitive to external pressure than conventional perovskite oxides so that their electric properties can be easily tuned by pressure."],"url":"http://arxiv.org/abs/2403.04241v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-07 05:55:31","title":"Image enhancement algorithm for absorption imaging","abstract":"The noise in absorption imaging of cold atoms significantly impacts measurement accuracy across a range of applications with ultracold atoms. It is crucial to adopt an approach that offers effective denoising capabilities without compromising the unique structure of the atoms. Here we introduce a novel image enhancement algorithm for cold atomic absorption imaging. The algorithm successfully suppresses background noise, enhancing image contrast significantly. Experimental results showcase that this approach can enhance the accuracy of cold atom particle number measurements by approximately tenfold, all while preserving essential information. Moreover, the method exhibits exceptional performance and robustness when confronted with fringe noise and multi-component imaging scenarios, offering high stability. Importantly, the optimization process is entirely automated, eliminating the need for manual parameter selection. The method is both compatible and practical, making it applicable across various absorption imaging fields.","sentences":["The noise in absorption imaging of cold atoms significantly impacts measurement accuracy across a range of applications with ultracold atoms.","It is crucial to adopt an approach that offers effective denoising capabilities without compromising the unique structure of the atoms.","Here we introduce a novel image enhancement algorithm for cold atomic absorption imaging.","The algorithm successfully suppresses background noise, enhancing image contrast significantly.","Experimental results showcase that this approach can enhance the accuracy of cold atom particle number measurements by approximately tenfold, all while preserving essential information.","Moreover, the method exhibits exceptional performance and robustness when confronted with fringe noise and multi-component imaging scenarios, offering high stability.","Importantly, the optimization process is entirely automated, eliminating the need for manual parameter selection.","The method is both compatible and practical, making it applicable across various absorption imaging fields."],"url":"http://arxiv.org/abs/2403.04240v1","category":"quant-ph"}
{"created":"2024-03-07 05:26:52","title":"Fundamental limits of Non-Linear Low-Rank Matrix Estimation","abstract":"We consider the task of estimating a low-rank matrix from non-linear and noisy observations. We prove a strong universality result showing that Bayes-optimal performances are characterized by an equivalent Gaussian model with an effective prior, whose parameters are entirely determined by an expansion of the non-linear function. In particular, we show that to reconstruct the signal accurately, one requires a signal-to-noise ratio growing as $N^{\\frac 12 (1-1/k_F)}$, where $k_F$ is the first non-zero Fisher information coefficient of the function. We provide asymptotic characterization for the minimal achievable mean squared error (MMSE) and an approximate message-passing algorithm that reaches the MMSE under conditions analogous to the linear version of the problem. We also provide asymptotic errors achieved by methods such as principal component analysis combined with Bayesian denoising, and compare them with Bayes-optimal MMSE.","sentences":["We consider the task of estimating a low-rank matrix from non-linear and noisy observations.","We prove a strong universality result showing that Bayes-optimal performances are characterized by an equivalent Gaussian model with an effective prior, whose parameters are entirely determined by an expansion of the non-linear function.","In particular, we show that to reconstruct the signal accurately, one requires a signal-to-noise ratio growing as $N^{\\frac 12 (1-1/k_F)}$, where $k_F$ is the first non-zero Fisher information coefficient of the function.","We provide asymptotic characterization for the minimal achievable mean squared error (MMSE) and an approximate message-passing algorithm that reaches the MMSE under conditions analogous to the linear version of the problem.","We also provide asymptotic errors achieved by methods such as principal component analysis combined with Bayesian denoising, and compare them with Bayes-optimal MMSE."],"url":"http://arxiv.org/abs/2403.04234v1","category":"stat.ML"}
{"created":"2024-03-07 05:24:11","title":"Identification of socioeconomic factors influencing global food price security using machine learning","abstract":"Global concern over food prices and security has been exacerbated by the impacts of armed conflicts such as the Russia Ukraine War, pandemic diseases, and climate change. Traditionally, analyzing global food prices and their associations with socioeconomic factors has relied on static linear regression models. However, the complexity of socioeconomic factors and their implications extend beyond simple linear relationships. By incorporating determinants, critical characteristics identification, and comparative model analysis, this study aimed to identify the critical socioeconomic characteristics and multidimensional relationships associated with the underlying factors of food prices and security. Machine learning tools were used to uncover the socioeconomic factors influencing global food prices from 2000 to 2022. A total of 105 key variables from the World Development Indicators and the Food and Agriculture Organization of the United Nations were selected. Machine learning identified four key dimensions of food price security: economic and population metrics, military spending, health spending, and environmental factors. The top 30 determinants were selected for feature extraction using data mining. The efficiency of the support vector regression model allowed for precise prediction making and correlation analysis.   Keywords: environment and growth, global economics, price fluctuation, support vector regression","sentences":["Global concern over food prices and security has been exacerbated by the impacts of armed conflicts such as the Russia Ukraine War, pandemic diseases, and climate change.","Traditionally, analyzing global food prices and their associations with socioeconomic factors has relied on static linear regression models.","However, the complexity of socioeconomic factors and their implications extend beyond simple linear relationships.","By incorporating determinants, critical characteristics identification, and comparative model analysis, this study aimed to identify the critical socioeconomic characteristics and multidimensional relationships associated with the underlying factors of food prices and security.","Machine learning tools were used to uncover the socioeconomic factors influencing global food prices from 2000 to 2022.","A total of 105 key variables from the World Development Indicators and the Food and Agriculture Organization of the United Nations were selected.","Machine learning identified four key dimensions of food price security: economic and population metrics, military spending, health spending, and environmental factors.","The top 30 determinants were selected for feature extraction using data mining.","The efficiency of the support vector regression model allowed for precise prediction making and correlation analysis.   ","Keywords: environment and growth, global economics, price fluctuation, support vector regression"],"url":"http://arxiv.org/abs/2403.04231v1","category":"stat.AP"}
{"created":"2024-03-07 05:04:27","title":"Accreting Primordial Black Holes: Dark Matter Constituents","abstract":"This paper shows that accretion of positronium plasma between 0.01 to 14s after the Big Bang could have created small black holes contributing at least 1 percent of the dark matter present today, with uncertainties ranging from 10 percent or more. General relativistic magnetohydrodynamic (GRMHD) simulations newly adapted to the early Universe confirm that accretion is due to magneto-rotational instability (MRI) in a rotating plasma. By contrast with Bondi accretion producing primordial masses bigger than the Sun, MRI could produce masses 10^{15-18} g observable by their Hawking radiation contributing to background gamma rays.","sentences":["This paper shows that accretion of positronium plasma between 0.01 to 14s after the Big Bang could have created small black holes contributing at least 1 percent of the dark matter present today, with uncertainties ranging from 10 percent or more.","General relativistic magnetohydrodynamic (GRMHD) simulations newly adapted to the early Universe confirm that accretion is due to magneto-rotational instability (MRI) in a rotating plasma.","By contrast with Bondi accretion producing primordial masses bigger than the Sun, MRI could produce masses 10^{15-18} g observable by their Hawking radiation contributing to background gamma rays."],"url":"http://arxiv.org/abs/2403.04227v1","category":"astro-ph.HE"}
{"created":"2024-03-07 04:50:38","title":"Self-Evaluation of Large Language Model based on Glass-box Features","abstract":"The proliferation of open-source Large Language Models (LLMs) underscores the pressing need for evaluation methods. Existing works primarily rely on external evaluators, focusing on training and prompting strategies. However, a crucial aspect - model-aware glass-box features - is overlooked. In this study, we explore the utility of glass-box features under the scenario of self-evaluation, namely applying an LLM to evaluate its own output. We investigate various glass-box feature groups and discovered that the softmax distribution serves as a reliable indicator for quality evaluation. Furthermore, we propose two strategies to enhance the evaluation by incorporating features derived from references. Experimental results on public benchmarks validate the feasibility of self-evaluation of LLMs using glass-box features.","sentences":["The proliferation of open-source Large Language Models (LLMs) underscores the pressing need for evaluation methods.","Existing works primarily rely on external evaluators, focusing on training and prompting strategies.","However, a crucial aspect - model-aware glass-box features - is overlooked.","In this study, we explore the utility of glass-box features under the scenario of self-evaluation, namely applying an LLM to evaluate its own output.","We investigate various glass-box feature groups and discovered that the softmax distribution serves as a reliable indicator for quality evaluation.","Furthermore, we propose two strategies to enhance the evaluation by incorporating features derived from references.","Experimental results on public benchmarks validate the feasibility of self-evaluation of LLMs using glass-box features."],"url":"http://arxiv.org/abs/2403.04222v1","category":"cs.CL"}
{"created":"2024-03-07 04:49:14","title":"Recollements and $n$-cotorsion pairs","abstract":"In the present paper, we study the relationships of $n$-cotorsion pairs among three abelian categories in a recollement. Under certain conditions, we present an explicit construction of gluing of $n$-cotorsion pairs in an abelian category $\\mathcal{D}$ with respect to $n$-cotorsion pairs in abelian categories $\\mathcal{D}^{'}$, $\\mathcal{D}^{''}$ respectively. On the other hand, we study the construction of $n$-cotorsion pairs in abelian categories $\\mathcal{D}^{'}$, $\\mathcal{D}^{''}$ obtained from $n$-cotorsion pairs in an abelian category $\\mathcal{D}$.","sentences":["In the present paper, we study the relationships of $n$-cotorsion pairs among three abelian categories in a recollement.","Under certain conditions, we present an explicit construction of gluing of $n$-cotorsion pairs in an abelian category $\\mathcal{D}$ with respect to $n$-cotorsion pairs in abelian categories $\\mathcal{D}^{'}$, $\\mathcal{D}^{''}$ respectively.","On the other hand, we study the construction of $n$-cotorsion pairs in abelian categories $\\mathcal{D}^{'}$, $\\mathcal{D}^{''}$ obtained from $n$-cotorsion pairs in an abelian category $\\mathcal{D}$."],"url":"http://arxiv.org/abs/2403.04220v1","category":"math.CT"}
{"created":"2024-03-07 04:36:31","title":"Exponential decay property for eigenfunctions of quantum walks","abstract":"Under an abstract setting, we show that eigenvectors belong to discrete spectra of unitary operators have exponential decay properties. We apply the main theorem to multi-dimensional quantum walks and show that eigenfunctions belong to a discrete spectrum decay exponentially at infinity.","sentences":["Under an abstract setting, we show that eigenvectors belong to discrete spectra of unitary operators have exponential decay properties.","We apply the main theorem to multi-dimensional quantum walks and show that eigenfunctions belong to a discrete spectrum decay exponentially at infinity."],"url":"http://arxiv.org/abs/2403.04214v1","category":"math-ph"}
{"created":"2024-03-07 04:13:51","title":"Noise Reduction of Stochastic Density Functional Theory for Metals","abstract":"Density Functional Theory (DFT) has become a cornerstone in the modeling of metals. However, accurately simulating metals, particularly under extreme conditions, presents two significant challenges. First, simulating complex metallic systems at low electron temperatures is difficult due to their highly delocalized density matrix. Second, modeling metallic warm-dense materials at very high electron temperatures is challenging because it requires the computation of a large number of partially occupied orbitals. This study demonstrates that both challenges can be effectively addressed using the latest advances in linear-scaling stochastic DFT methodologies. Despite the inherent introduction of noise into all computed properties by stochastic DFT, this research evaluates the efficacy of various noise reduction techniques under different thermal conditions. Our observations indicate that the effectiveness of noise reduction strategies varies significantly with the electron temperature. Furthermore, we provide evidence that the computational cost of stochastic DFT methods scales linearly with system size for metal systems, regardless of the electron temperature regime.","sentences":["Density Functional Theory (DFT) has become a cornerstone in the modeling of metals.","However, accurately simulating metals, particularly under extreme conditions, presents two significant challenges.","First, simulating complex metallic systems at low electron temperatures is difficult due to their highly delocalized density matrix.","Second, modeling metallic warm-dense materials at very high electron temperatures is challenging because it requires the computation of a large number of partially occupied orbitals.","This study demonstrates that both challenges can be effectively addressed using the latest advances in linear-scaling stochastic DFT methodologies.","Despite the inherent introduction of noise into all computed properties by stochastic DFT, this research evaluates the efficacy of various noise reduction techniques under different thermal conditions.","Our observations indicate that the effectiveness of noise reduction strategies varies significantly with the electron temperature.","Furthermore, we provide evidence that the computational cost of stochastic DFT methods scales linearly with system size for metal systems, regardless of the electron temperature regime."],"url":"http://arxiv.org/abs/2403.04203v1","category":"physics.chem-ph"}
{"created":"2024-03-07 04:08:07","title":"Bi-Static Sensing in OFDM Wireless Systems for Indoor Scenarios","abstract":"The sixth generation (6G) systems will likely employ orthogonal frequency division multiplexing (OFDM) waveform for performing the joint task of sensing and communication. In this paper, we design an OFDM system for integrated sensing and communication (ISAC) and propose a novel approach for passive target detection in an indoor deployment using a data driven AI approach. The delay-Doppler profile (DDP) and power delay profile (PDP) is used to train the proposed AI-based detector. We analyze the detection performance of the proposed methods under line of sight (LOS) and non-line of sight (NLOS) conditions for various training strategies. We show that the proposed method provides 10 dB performance improvement over the baseline for 80% target detection under LOS conditions and the performance drops by 10-20 dB for NLOS depending on the usecase scenarios.","sentences":["The sixth generation (6G) systems will likely employ orthogonal frequency division multiplexing (OFDM) waveform for performing the joint task of sensing and communication.","In this paper, we design an OFDM system for integrated sensing and communication (ISAC) and propose a novel approach for passive target detection in an indoor deployment using a data driven AI approach.","The delay-Doppler profile (DDP) and power delay profile (PDP) is used to train the proposed AI-based detector.","We analyze the detection performance of the proposed methods under line of sight (LOS) and non-line of sight (NLOS) conditions for various training strategies.","We show that the proposed method provides 10 dB performance improvement over the baseline for 80% target detection under LOS conditions and the performance drops by 10-20 dB for NLOS depending on the usecase scenarios."],"url":"http://arxiv.org/abs/2403.04201v1","category":"cs.IT"}
{"created":"2024-03-07 04:05:16","title":"ACC-ViT : Atrous Convolution's Comeback in Vision Transformers","abstract":"Transformers have elevated to the state-of-the-art vision architectures through innovations in attention mechanism inspired from visual perception. At present two classes of attentions prevail in vision transformers, regional and sparse attention. The former bounds the pixel interactions within a region; the latter spreads them across sparse grids. The opposing natures of them have resulted in a dilemma between either preserving hierarchical relation or attaining a global context. In this work, taking inspiration from atrous convolution, we introduce Atrous Attention, a fusion of regional and sparse attention, which can adaptively consolidate both local and global information, while maintaining hierarchical relations. As a further tribute to atrous convolution, we redesign the ubiquitous inverted residual convolution blocks with atrous convolution. Finally, we propose a generalized, hybrid vision transformer backbone, named ACC-ViT, following conventional practices for standard vision tasks. Our tiny version model achieves $\\sim 84 \\%$ accuracy on ImageNet-1K, with less than $28.5$ million parameters, which is $0.42\\%$ improvement over state-of-the-art MaxViT while having $8.4\\%$ less parameters. In addition, we have investigated the efficacy of ACC-ViT backbone under different evaluation settings, such as finetuning, linear probing, and zero-shot learning on tasks involving medical image analysis, object detection, and language-image contrastive learning. ACC-ViT is therefore a strong vision backbone, which is also competitive in mobile-scale versions, ideal for niche applications with small datasets.","sentences":["Transformers have elevated to the state-of-the-art vision architectures through innovations in attention mechanism inspired from visual perception.","At present two classes of attentions prevail in vision transformers, regional and sparse attention.","The former bounds the pixel interactions within a region; the latter spreads them across sparse grids.","The opposing natures of them have resulted in a dilemma between either preserving hierarchical relation or attaining a global context.","In this work, taking inspiration from atrous convolution, we introduce Atrous Attention, a fusion of regional and sparse attention, which can adaptively consolidate both local and global information, while maintaining hierarchical relations.","As a further tribute to atrous convolution, we redesign the ubiquitous inverted residual convolution blocks with atrous convolution.","Finally, we propose a generalized, hybrid vision transformer backbone, named ACC-ViT, following conventional practices for standard vision tasks.","Our tiny version model achieves $\\sim 84 \\%$ accuracy on ImageNet-1K, with less than $28.5$ million parameters, which is $0.42\\%$ improvement over state-of-the-art MaxViT while having $8.4\\%$ less parameters.","In addition, we have investigated the efficacy of ACC-ViT backbone under different evaluation settings, such as finetuning, linear probing, and zero-shot learning on tasks involving medical image analysis, object detection, and language-image contrastive learning.","ACC-ViT is therefore a strong vision backbone, which is also competitive in mobile-scale versions, ideal for niche applications with small datasets."],"url":"http://arxiv.org/abs/2403.04200v1","category":"cs.CV"}
{"created":"2024-03-07 04:02:37","title":"B\u00f6ttcher-Wenzel inequality for weighted Frobenius norms and its application to quantum physics","abstract":"By employing a weighted Frobenius norm with a positive matrix $\\omega$, we introduce natural generalizations of the famous B\\\"ottcher-Wenzel (BW) inequality. Specifically, we explore six types of bounds, labeled (i) through (vi), on the norms of the commutator $[A,B]:= AB - BA$, based on the combination of the weighted Frobenius norm $\\|A\\|_\\omega := \\sqrt{{\\rm tr}(A^\\ast A \\omega)}$ and the usual Frobenius norm $\\|A\\| := \\sqrt{{\\rm tr}(A^\\ast A)}$. While the tight bound for the case (vi) corresponds to the BW inequality itself, we establish the tight bounds for cases (iii) and (v), and propose conjectures for the tight bounds of cases (i) and (ii), with the tight bound for case (iv) presented as a corollary of case (i). Conversely, all these bounds (i)-(v) serve as generalizations of the BW inequality. The conjectured bounds for cases (i) and (ii) are numerically supported for matrices up to size $n=15$, and we provide proofs for $2\\times 2$ matrices. Additionally, we present applications of these bounds in quantum physics, particularly in the contexts of the uncertainty relation and open quantum dynamics.","sentences":["By employing a weighted Frobenius norm with a positive matrix $\\omega$, we introduce natural generalizations of the famous B\\\"ottcher-Wenzel (BW) inequality.","Specifically, we explore six types of bounds, labeled (i) through (vi), on the norms of the commutator $","[A,B]:= AB - BA$, based on the combination of the weighted Frobenius norm $\\|A\\|_\\omega := \\sqrt{{\\rm tr}(A^\\ast A \\omega)}$ and the usual Frobenius norm $\\|A\\| := \\sqrt{{\\rm tr}(A^\\ast A)}$.","While the tight bound for the case (vi) corresponds to the BW inequality itself, we establish the tight bounds for cases (iii) and (v), and propose conjectures for the tight bounds of cases (i) and (ii), with the tight bound for case (iv) presented as a corollary of case (i).","Conversely, all these bounds (i)-(v) serve as generalizations of the BW inequality.","The conjectured bounds for cases (i) and (ii) are numerically supported for matrices up to size $n=15$, and we provide proofs for $2\\times 2$ matrices.","Additionally, we present applications of these bounds in quantum physics, particularly in the contexts of the uncertainty relation and open quantum dynamics."],"url":"http://arxiv.org/abs/2403.04199v1","category":"math-ph"}
{"created":"2024-03-07 03:45:02","title":"Probing the mixing between sterile and tau neutrinos in the SHiP experiment","abstract":"We study the expected sensitivity to the mixing between sterile and tau neutrinos directly from the tau neutrino disappearance in the high-energy fixed target experiment. Here, the beam energy is large enough to produce tau neutrinos at the target with large luminosity. During their propagation to the detector, the tau neutrino may oscillate into sterile neutrino. By examining the energy spectrum of the observed tau neutrino events, we can probe the mixing between sterile and tau neutrinos directly. In this paper, we consider Scattering and Neutrino Detector (SND) at SHiP experiment as a showcase, which uses 400 GeV protons from SPS at CERN, and expect to observe 6,300 tau and anti-tau neutrinos from the $2\\times 10^{20}$ POT for 5 years operation. Assuming the uncertainty of 10\\%, we find the sensitivity $|U_{\\tau 4}|^2 \\sim 0.08$\\, (90\\% CL) for $\\Delta m_{41}^2 \\sim 500\\ \\mathrm{eV}^2$ with 10\\% signal-to-background ratio. We also consider a far SND at the end of the SHiP hidden sector detector, in which case the sensitivity would be enhanced to $|U_{\\tau 4}|^2 \\sim 0.02$.","sentences":["We study the expected sensitivity to the mixing between sterile and tau neutrinos directly from the tau neutrino disappearance in the high-energy fixed target experiment.","Here, the beam energy is large enough to produce tau neutrinos at the target with large luminosity.","During their propagation to the detector, the tau neutrino may oscillate into sterile neutrino.","By examining the energy spectrum of the observed tau neutrino events, we can probe the mixing between sterile and tau neutrinos directly.","In this paper, we consider Scattering and Neutrino Detector (SND) at SHiP experiment as a showcase, which uses 400 GeV protons from SPS at CERN, and expect to observe 6,300 tau and anti-tau neutrinos from the $2\\times 10^{20}$ POT for 5 years operation.","Assuming the uncertainty of 10\\%, we find the sensitivity $|U_{\\tau 4}|^2 \\sim 0.08$\\, (90\\% CL) for $\\Delta m_{41}^2 \\sim 500\\ \\mathrm{eV}^2$ with 10\\% signal-to-background ratio.","We also consider a far SND at the end of the SHiP hidden sector detector, in which case the sensitivity would be enhanced to $|U_{\\tau 4}|^2 \\sim 0.02$."],"url":"http://arxiv.org/abs/2403.04191v1","category":"hep-ph"}
{"created":"2024-03-07 03:35:01","title":"The stochastic relativistic advection diffusion equation from the Metropolis algorithm","abstract":"We study an approach to simulating the stochastic relativistic advection-diffusion equation based on the Metropolis algorithm. We show that the dissipative dynamics of the boosted fluctuating fluid can be simulated by making random transfers of charge between fluid cells, interspersed with ideal hydrodynamic time steps. The random charge transfers are accepted or rejected in a Metropolis step using the entropy as a statistical weight. This procedure reproduces the expected strains of dissipative relativistic hydrodynamics in a specific (and non-covariant) hydrodynamic frame known as the density frame. Numerical results, both with and without noise, are presented and compared to relativistic kinetics and analytical expectations. An all order resummation of the density frame gradient expansion reproduces the covariant dynamics in a specific model. In contrast to all other numerical approaches to relativistic dissipative fluids, the dissipative fluid formalism presented here is strictly first order in gradients and has no non-hydrodynamic modes. The physical naturalness and simplicity of the Metropolis algorithm, together with its convergence properties, make it a promising tool for simulating stochastic relativistic fluids in heavy ion collisions and for critical phenomena in the relativistic domain.","sentences":["We study an approach to simulating the stochastic relativistic advection-diffusion equation based on the Metropolis algorithm.","We show that the dissipative dynamics of the boosted fluctuating fluid can be simulated by making random transfers of charge between fluid cells, interspersed with ideal hydrodynamic time steps.","The random charge transfers are accepted or rejected in a Metropolis step using the entropy as a statistical weight.","This procedure reproduces the expected strains of dissipative relativistic hydrodynamics in a specific (and non-covariant) hydrodynamic frame known as the density frame.","Numerical results, both with and without noise, are presented and compared to relativistic kinetics and analytical expectations.","An all order resummation of the density frame gradient expansion reproduces the covariant dynamics in a specific model.","In contrast to all other numerical approaches to relativistic dissipative fluids, the dissipative fluid formalism presented here is strictly first order in gradients and has no non-hydrodynamic modes.","The physical naturalness and simplicity of the Metropolis algorithm, together with its convergence properties, make it a promising tool for simulating stochastic relativistic fluids in heavy ion collisions and for critical phenomena in the relativistic domain."],"url":"http://arxiv.org/abs/2403.04185v1","category":"nucl-th"}
{"created":"2024-03-07 03:22:31","title":"Mining Transactional Data To Produce Extended Association Rules Using Collaborative Apriori, Fsa-Red And M5p Predictive Algorithm As A Basis Of Business Actions","abstract":"There are large amounts of transactional data which showed consumer shopping cart at a store that sells more than 150 types of products. In this case, the company is utilizing these data in making business action. In previous studies, the data that has a lot of attributes and record data reduction algorithms handled by the FSA Red (Feature Selection for Association Rules)are then mined using Apriori algorithm. The resulting association rules have high levels of accuracy and excellent test results, which rely more than 90%. In this study, the association rules generated in previous research will be updated by using prediction algorithms M5P, so that the association rules can be used within a period of several months in the future. Furthermore, some data mining technique such as: clustering and time series pattern will be implemented to examine the truth and extend the validity of association rules which were built. It can be concluded that the association rules were established after will generate strong association rules with confidence equal or higher than 70% and the rules established truth can be seen from the time series pattern on each group of goods which are then used as the basis of business actions.","sentences":["There are large amounts of transactional data which showed consumer shopping cart at a store that sells more than 150 types of products.","In this case, the company is utilizing these data in making business action.","In previous studies, the data that has a lot of attributes and record data reduction algorithms handled by the FSA Red (Feature Selection for Association Rules)are then mined using Apriori algorithm.","The resulting association rules have high levels of accuracy and excellent test results, which rely more than 90%.","In this study, the association rules generated in previous research will be updated by using prediction algorithms M5P, so that the association rules can be used within a period of several months in the future.","Furthermore, some data mining technique such as: clustering and time series pattern will be implemented to examine the truth and extend the validity of association rules which were built.","It can be concluded that the association rules were established after will generate strong association rules with confidence equal or higher than 70% and the rules established truth can be seen from the time series pattern on each group of goods which are then used as the basis of business actions."],"url":"http://arxiv.org/abs/2403.04179v1","category":"cs.DB"}
{"created":"2024-03-07 03:07:59","title":"Image Coding for Machines with Edge Information Learning Using Segment Anything","abstract":"Image Coding for Machines (ICM) is an image compression technique for image recognition.   This technique is essential due to the growing demand for image recognition AI.   In this paper, we propose a method for ICM that focuses on encoding and decoding only the edge information of object parts in an image, which we call SA-ICM.   This is an Learned Image Compression (LIC) model trained using edge information created by Segment Anything.   Our method can be used for image recognition models with various tasks.   SA-ICM is also robust to changes in input data, making it effective for a variety of use cases.   Additionally, our method provides benefits from a privacy point of view, as it removes human facial information on the encoder's side, thus protecting one's privacy.   Furthermore, this LIC model training method can be used to train Neural Representations for Videos (NeRV), which is a video compression model.   By training NeRV using edge information created by Segment Anything, it is possible to create a NeRV that is effective for image recognition (SA-NeRV).   Experimental results confirm the advantages of SA-ICM, presenting the best performance in image compression for image recognition.   We also show that SA-NeRV is superior to ordinary NeRV in video compression for machines.","sentences":["Image Coding for Machines (ICM) is an image compression technique for image recognition.   ","This technique is essential due to the growing demand for image recognition AI.   ","In this paper, we propose a method for ICM that focuses on encoding and decoding only the edge information of object parts in an image, which we call SA-ICM.   ","This is an Learned Image Compression (LIC) model trained using edge information created by Segment Anything.   ","Our method can be used for image recognition models with various tasks.   ","SA-ICM is also robust to changes in input data, making it effective for a variety of use cases.   ","Additionally, our method provides benefits from a privacy point of view, as it removes human facial information on the encoder's side, thus protecting one's privacy.   ","Furthermore, this LIC model training method can be used to train Neural Representations for Videos (NeRV), which is a video compression model.   ","By training NeRV using edge information created by Segment Anything, it is possible to create a NeRV that is effective for image recognition (SA-NeRV).   ","Experimental results confirm the advantages of SA-ICM, presenting the best performance in image compression for image recognition.   ","We also show that SA-NeRV is superior to ordinary NeRV in video compression for machines."],"url":"http://arxiv.org/abs/2403.04173v1","category":"cs.CV"}
{"created":"2024-03-07 02:57:44","title":"Impact of the Antenna on the Sub-Terahertz Indoor Channel Characteristics: An Experimental Approach","abstract":"Terahertz-band (100 GHz-10 THz) communication is a promising radio technology envisioned to enable ultra-high data rate, reliable and low-latency wireless connectivity in next-generation wireless systems. However, the low transmission power of THz transmitters, the need for high gain directional antennas, and the complex interaction of THz radiation with common objects along the propagation path make crucial the understanding of the THz channel. In this paper, we conduct an extensive channel measurement campaign in an indoor setting (i.e., a conference room) through a channel sounder with 0.1 ns time resolution and 20 GHz bandwidth at 140 GHz. Particularly, the impact of different antenna directivities (and, thus, beam widths) on the channel characteristics is extensively studied. The experimentally obtained dataset is processed to develop the path loss model and, subsequently, derive key channel metrics such as the path loss exponent, delay spread, and K-factor. The results highlight the multi-faceted impact of the antenna gain on the channel and, by extension, the wireless system and, thus, show that an antenna-agnostic channel model cannot capture the propagation characteristics of the THz channel.","sentences":["Terahertz-band (100 GHz-10 THz) communication is a promising radio technology envisioned to enable ultra-high data rate, reliable and low-latency wireless connectivity in next-generation wireless systems.","However, the low transmission power of THz transmitters, the need for high gain directional antennas, and the complex interaction of THz radiation with common objects along the propagation path make crucial the understanding of the THz channel.","In this paper, we conduct an extensive channel measurement campaign in an indoor setting (i.e., a conference room) through a channel sounder with 0.1 ns time resolution and 20 GHz bandwidth at 140 GHz.","Particularly, the impact of different antenna directivities (and, thus, beam widths) on the channel characteristics is extensively studied.","The experimentally obtained dataset is processed to develop the path loss model and, subsequently, derive key channel metrics such as the path loss exponent, delay spread, and K-factor.","The results highlight the multi-faceted impact of the antenna gain on the channel and, by extension, the wireless system and, thus, show that an antenna-agnostic channel model cannot capture the propagation characteristics of the THz channel."],"url":"http://arxiv.org/abs/2403.04168v1","category":"eess.SP"}
{"created":"2024-03-07 02:47:21","title":"Error Correction in Dynamical Codes","abstract":"We ask what is the general framework for a quantum error correcting code that is defined by a sequence of measurements. Recently, there has been much interest in Floquet codes and space-time codes. In this work, we define and study the distance of a dynamical code. This is a subtle concept and difficult to determine: At any given time, the system will be in a subspace which forms a quantum error-correcting code with a given distance, but the full error correction capability of that code may not be available due to the schedule of measurements associated with the code. We address this challenge by developing an algorithm that tracks information we have learned about the error syndromes through the protocol and put that together to determine the distance of a dynamical code, in a non-fault-tolerant context. We use the tools developed for the algorithm to analyze the initialization and masking properties of a generic Floquet code. Further, we look at properties of dynamical codes under the constraint of geometric locality with a view to understand whether the fundamental limitations on logical gates and code parameters imposed by geometric locality for traditional codes can be surpassed in the dynamical paradigm. We find that codes with a limited number of long range connectivity will not allow non-Clifford gates to be implemented with finite depth circuits in the 2D setting.","sentences":["We ask what is the general framework for a quantum error correcting code that is defined by a sequence of measurements.","Recently, there has been much interest in Floquet codes and space-time codes.","In this work, we define and study the distance of a dynamical code.","This is a subtle concept and difficult to determine: At any given time, the system will be in a subspace which forms a quantum error-correcting code with a given distance, but the full error correction capability of that code may not be available due to the schedule of measurements associated with the code.","We address this challenge by developing an algorithm that tracks information we have learned about the error syndromes through the protocol and put that together to determine the distance of a dynamical code, in a non-fault-tolerant context.","We use the tools developed for the algorithm to analyze the initialization and masking properties of a generic Floquet code.","Further, we look at properties of dynamical codes under the constraint of geometric locality with a view to understand whether the fundamental limitations on logical gates and code parameters imposed by geometric locality for traditional codes can be surpassed in the dynamical paradigm.","We find that codes with a limited number of long range connectivity will not allow non-Clifford gates to be implemented with finite depth circuits in the 2D setting."],"url":"http://arxiv.org/abs/2403.04163v1","category":"quant-ph"}
{"created":"2024-03-07 02:47:08","title":"Noisy Spiking Actor Network for Exploration","abstract":"As a general method for exploration in deep reinforcement learning (RL), NoisyNet can produce problem-specific exploration strategies. Spiking neural networks (SNNs), due to their binary firing mechanism, have strong robustness to noise, making it difficult to realize efficient exploration with local disturbances. To solve this exploration problem, we propose a noisy spiking actor network (NoisySAN) that introduces time-correlated noise during charging and transmission. Moreover, a noise reduction method is proposed to find a stable policy for the agent. Extensive experimental results demonstrate that our method outperforms the state-of-the-art performance on a wide range of continuous control tasks from OpenAI gym.","sentences":["As a general method for exploration in deep reinforcement learning (RL), NoisyNet can produce problem-specific exploration strategies.","Spiking neural networks (SNNs), due to their binary firing mechanism, have strong robustness to noise, making it difficult to realize efficient exploration with local disturbances.","To solve this exploration problem, we propose a noisy spiking actor network (NoisySAN) that introduces time-correlated noise during charging and transmission.","Moreover, a noise reduction method is proposed to find a stable policy for the agent.","Extensive experimental results demonstrate that our method outperforms the state-of-the-art performance on a wide range of continuous control tasks from OpenAI gym."],"url":"http://arxiv.org/abs/2403.04162v1","category":"cs.LG"}
{"created":"2024-03-07 02:10:59","title":"MAP: MAsk-Pruning for Source-Free Model Intellectual Property Protection","abstract":"Deep learning has achieved remarkable progress in various applications, heightening the importance of safeguarding the intellectual property (IP) of well-trained models. It entails not only authorizing usage but also ensuring the deployment of models in authorized data domains, i.e., making models exclusive to certain target domains. Previous methods necessitate concurrent access to source training data and target unauthorized data when performing IP protection, making them risky and inefficient for decentralized private data. In this paper, we target a practical setting where only a well-trained source model is available and investigate how we can realize IP protection. To achieve this, we propose a novel MAsk Pruning (MAP) framework. MAP stems from an intuitive hypothesis, i.e., there are target-related parameters in a well-trained model, locating and pruning them is the key to IP protection. Technically, MAP freezes the source model and learns a target-specific binary mask to prevent unauthorized data usage while minimizing performance degradation on authorized data. Moreover, we introduce a new metric aimed at achieving a better balance between source and target performance degradation. To verify the effectiveness and versatility, we have evaluated MAP in a variety of scenarios, including vanilla source-available, practical source-free, and challenging data-free. Extensive experiments indicate that MAP yields new state-of-the-art performance.","sentences":["Deep learning has achieved remarkable progress in various applications, heightening the importance of safeguarding the intellectual property (IP) of well-trained models.","It entails not only authorizing usage but also ensuring the deployment of models in authorized data domains, i.e., making models exclusive to certain target domains.","Previous methods necessitate concurrent access to source training data and target unauthorized data when performing IP protection, making them risky and inefficient for decentralized private data.","In this paper, we target a practical setting where only a well-trained source model is available and investigate how we can realize IP protection.","To achieve this, we propose a novel MAsk Pruning (MAP) framework.","MAP stems from an intuitive hypothesis, i.e., there are target-related parameters in a well-trained model, locating and pruning them is the key to IP protection.","Technically, MAP freezes the source model and learns a target-specific binary mask to prevent unauthorized data usage while minimizing performance degradation on authorized data.","Moreover, we introduce a new metric aimed at achieving a better balance between source and target performance degradation.","To verify the effectiveness and versatility, we have evaluated MAP in a variety of scenarios, including vanilla source-available, practical source-free, and challenging data-free.","Extensive experiments indicate that MAP yields new state-of-the-art performance."],"url":"http://arxiv.org/abs/2403.04149v1","category":"cs.CV"}
{"created":"2024-03-07 01:47:34","title":"Incremental Bayesian Learning for Fail-Operational Control in Autonomous Driving","abstract":"Abrupt maneuvers by surrounding vehicles (SVs) can typically lead to safety concerns and affect the task efficiency of the ego vehicle (EV), especially with model uncertainties stemming from environmental disturbances. This paper presents a real-time fail-operational controller that ensures the asymptotic convergence of an uncertain EV to a safe state, while preserving task efficiency in dynamic environments. An incremental Bayesian learning approach is developed to facilitate online learning and inference of changing environmental disturbances. Leveraging disturbance quantification and constraint transformation, we develop a stochastic fail-operational barrier based on the control barrier function (CBF). With this development, the uncertain EV is able to converge asymptotically from an unsafe state to a defined safe state with probabilistic stability. Subsequently, the stochastic fail-operational barrier is integrated into an efficient fail-operational controller based on quadratic programming (QP). This controller is tailored for the EV operating under control constraints in the presence of environmental disturbances, with both safety and efficiency objectives taken into consideration. We validate the proposed framework in connected cruise control (CCC) tasks, where SVs perform aggressive driving maneuvers. The simulation results demonstrate that our method empowers the EV to swiftly return to a safe state while upholding task efficiency in real time, even under time-varying environmental disturbances.","sentences":["Abrupt maneuvers by surrounding vehicles (SVs) can typically lead to safety concerns and affect the task efficiency of the ego vehicle (EV), especially with model uncertainties stemming from environmental disturbances.","This paper presents a real-time fail-operational controller that ensures the asymptotic convergence of an uncertain EV to a safe state, while preserving task efficiency in dynamic environments.","An incremental Bayesian learning approach is developed to facilitate online learning and inference of changing environmental disturbances.","Leveraging disturbance quantification and constraint transformation, we develop a stochastic fail-operational barrier based on the control barrier function (CBF).","With this development, the uncertain EV is able to converge asymptotically from an unsafe state to a defined safe state with probabilistic stability.","Subsequently, the stochastic fail-operational barrier is integrated into an efficient fail-operational controller based on quadratic programming (QP).","This controller is tailored for the EV operating under control constraints in the presence of environmental disturbances, with both safety and efficiency objectives taken into consideration.","We validate the proposed framework in connected cruise control (CCC) tasks, where SVs perform aggressive driving maneuvers.","The simulation results demonstrate that our method empowers the EV to swiftly return to a safe state while upholding task efficiency in real time, even under time-varying environmental disturbances."],"url":"http://arxiv.org/abs/2403.04143v1","category":"cs.RO"}
{"created":"2024-03-07 01:41:43","title":"VLBI Astrometry of Radio Stars to Link Radio and Optical Celestial Reference Frames: Observing Strategies","abstract":"The Gaia celestial reference frame (Gaia-CRF) will benefit from a close assessment with independent methods, such as Very Long Baseline Interferometry (VLBI) measurements of radio stars at bright magnitudes. However, obtaining full astrometric parameters for each radio star through VLBI measurements demands a significant amount of observation time. This study proposes an efficient observing strategy that acquires double-epoch VLBI positions to measure the positions and proper motions of radio stars at a reduced cost. The solution for CRF link compatible with individual VLBI position measurements is introduced, and the optimized observing epoch scheduling is discussed. Applying this solution to observational data yields results sensitive to sample increase or decrease, yet they remain consistently in line with the literature at the 1-sigma level. This suggests the potential for improvement with a larger sample size. Simulations for adding observations demonstrate the double-epoch strategy reduces CRF link parameter uncertainties by over 30% compared to the five-parameter strategy.","sentences":["The Gaia celestial reference frame (Gaia-CRF) will benefit from a close assessment with independent methods, such as Very Long Baseline Interferometry (VLBI) measurements of radio stars at bright magnitudes.","However, obtaining full astrometric parameters for each radio star through VLBI measurements demands a significant amount of observation time.","This study proposes an efficient observing strategy that acquires double-epoch VLBI positions to measure the positions and proper motions of radio stars at a reduced cost.","The solution for CRF link compatible with individual VLBI position measurements is introduced, and the optimized observing epoch scheduling is discussed.","Applying this solution to observational data yields results sensitive to sample increase or decrease, yet they remain consistently in line with the literature at the 1-sigma level.","This suggests the potential for improvement with a larger sample size.","Simulations for adding observations demonstrate the double-epoch strategy reduces CRF link parameter uncertainties by over 30% compared to the five-parameter strategy."],"url":"http://arxiv.org/abs/2403.04141v1","category":"astro-ph.IM"}
{"created":"2024-03-07 01:34:25","title":"Snevily's Conjecture about $\\mathcal{L}$-intersecting Families on Set Systems and its Analogue on Vector Spaces","abstract":"The classical Erd\\H{o}s-Ko-Rado theorem on the size of an intersecting family of $k$-subsets of the set $[n] = \\{1, 2, \\dots, n\\}$ is one of the fundamental intersection theorems for set systems. After the establishment of the EKR theorem, many intersection theorems on set systems have appeared in the literature, such as the well-known Frankl-Wilson theorem, Alon-Babai-Suzuki theorem, and Grolmusz-Sudakov theorem. In 1995, Snevily proposed the conjecture that the upper bound for the size of an $\\mathcal{L}$-intersecting family of subsets of $[n]$ is ${{n} \\choose {s}}$ under the condition $\\max \\{l_{i}\\} < \\min \\{k_{j}\\}$, where $\\mathcal{L} = \\{l_{1}, \\dots, l_{s}\\}$ with $0 \\leq l_{1} < \\cdots < l_{s}$ and $k_{j}$ are subset sizes in the family. In this paper, we prove that Snevily's conjecture holds for $n \\geq {{k^{2}} \\choose {l_{1}+1}}s + l_{1}$, where $k$ is the maximum subset size in the family. We then derive an analogous result for $\\mathcal{L}$-intersecting families of subspaces of an $n$-dimensional vector space over a finite field $\\mathbb{F}_{q}$.","sentences":["The classical Erd\\H{o}s-Ko-Rado theorem on the size of an intersecting family of $k$-subsets of the set $[n] = \\{1, 2, \\dots, n\\}$ is one of the fundamental intersection theorems for set systems.","After the establishment of the EKR theorem, many intersection theorems on set systems have appeared in the literature, such as the well-known Frankl-Wilson theorem, Alon-Babai-Suzuki theorem, and Grolmusz-Sudakov theorem.","In 1995, Snevily proposed the conjecture that the upper bound for the size of an $\\mathcal{L}$-intersecting family of subsets of $[n]$ is ${{n} \\choose {s}}$ under the condition $\\max \\{l_{i}\\} < \\min \\{k_{j}\\}$, where $\\mathcal{L} = \\{l_{1}, \\dots, l_{s}\\}$ with $0 \\leq l_{1} <","\\cdots < l_{s}$ and $k_{j}$ are subset sizes in the family.","In this paper, we prove that Snevily's conjecture holds for $n \\geq {{k^{2}} \\choose {l_{1}+1}}s + l_{1}$, where $k$ is the maximum subset size in the family.","We then derive an analogous result for $\\mathcal{L}$-intersecting families of subspaces of an $n$-dimensional vector space over a finite field $\\mathbb{F}_{q}$."],"url":"http://arxiv.org/abs/2403.04139v1","category":"math.CO"}
{"created":"2024-03-07 01:34:06","title":"Astrochemical effect of the fundamental grain surface processes I. The diffusion of grain surface species and the pre-exponential factor","abstract":"Abbreviation. Thermal diffusion is one of the basic processes for the mobility and formation of species on cosmic dust grains. Recent laboratory measurements have found that the diffusion pre-exponential factor can differ from that for desorption by several orders of magnitude. We aim to evaluate the effect of the newly experimentally measured diffusion pre-exponential factor on the chemistry under cold molecular cloud conditions. We found that statistically, more than half of the total gas-phase and grain surface species are not affected by the new pre-exponential factor after a chemical evolution of 10$^5$ yr. The most abundant gas-phase CO and grain surface water ice are not affected by the new pre-exponential factor. For the grain surface species that are affected, compared to the commonly adopted value of the pre-exponential factor for diffusion used in the chemical models, they could be either overproduced or underproduced with the lower diffusion pre-factor used in this work. The former case applies to radicals and the species that serve as reactants, while the latter case applies to complex organic molecules (COMs) on the grain and the species that rarely react with other species. Gas-phase species could also be affected due to the desorption of the grain surface species. The abundance of some gas-phase COMs could be varied by over one order of magnitude depending on the adopted grain surface temperature and/or the ratio of diffusion to desorption energy in the model. Key species whose diffusion pre-exponential factor significantly affects the model predictions were also evaluated, and these specie include CH3OH, H2CO, and NO. The results presented in this study show that the pre-exponential factor is one of the basic and important parameters in astrochemical models.","sentences":["Abbreviation.","Thermal diffusion is one of the basic processes for the mobility and formation of species on cosmic dust grains.","Recent laboratory measurements have found that the diffusion pre-exponential factor can differ from that for desorption by several orders of magnitude.","We aim to evaluate the effect of the newly experimentally measured diffusion pre-exponential factor on the chemistry under cold molecular cloud conditions.","We found that statistically, more than half of the total gas-phase and grain surface species are not affected by the new pre-exponential factor after a chemical evolution of 10$^5$ yr.","The most abundant gas-phase CO and grain surface water ice are not affected by the new pre-exponential factor.","For the grain surface species that are affected, compared to the commonly adopted value of the pre-exponential factor for diffusion used in the chemical models, they could be either overproduced or underproduced with the lower diffusion pre-factor used in this work.","The former case applies to radicals and the species that serve as reactants, while the latter case applies to complex organic molecules (COMs) on the grain and the species that rarely react with other species.","Gas-phase species could also be affected due to the desorption of the grain surface species.","The abundance of some gas-phase COMs could be varied by over one order of magnitude depending on the adopted grain surface temperature and/or the ratio of diffusion to desorption energy in the model.","Key species whose diffusion pre-exponential factor significantly affects the model predictions were also evaluated, and these specie include CH3OH, H2CO, and NO.","The results presented in this study show that the pre-exponential factor is one of the basic and important parameters in astrochemical models."],"url":"http://arxiv.org/abs/2403.04138v1","category":"astro-ph.GA"}
{"created":"2024-03-07 01:24:59","title":"Towards learning-based planning:The nuPlan benchmark for real-world autonomous driving","abstract":"Machine Learning (ML) has replaced traditional handcrafted methods for perception and prediction in autonomous vehicles. Yet for the equally important planning task, the adoption of ML-based techniques is slow. We present nuPlan, the world's first real-world autonomous driving dataset, and benchmark. The benchmark is designed to test the ability of ML-based planners to handle diverse driving situations and to make safe and efficient decisions. To that end, we introduce a new large-scale dataset that consists of 1282 hours of diverse driving scenarios from 4 cities (Las Vegas, Boston, Pittsburgh, and Singapore) and includes high-quality auto-labeled object tracks and traffic light data. We exhaustively mine and taxonomize common and rare driving scenarios which are used during evaluation to get fine-grained insights into the performance and characteristics of a planner. Beyond the dataset, we provide a simulation and evaluation framework that enables a planner's actions to be simulated in closed-loop to account for interactions with other traffic participants. We present a detailed analysis of numerous baselines and investigate gaps between ML-based and traditional methods. Find the nuPlan dataset and code at nuplan.org.","sentences":["Machine Learning (ML) has replaced traditional handcrafted methods for perception and prediction in autonomous vehicles.","Yet for the equally important planning task, the adoption of ML-based techniques is slow.","We present nuPlan, the world's first real-world autonomous driving dataset, and benchmark.","The benchmark is designed to test the ability of ML-based planners to handle diverse driving situations and to make safe and efficient decisions.","To that end, we introduce a new large-scale dataset that consists of 1282 hours of diverse driving scenarios from 4 cities (Las Vegas, Boston, Pittsburgh, and Singapore) and includes high-quality auto-labeled object tracks and traffic light data.","We exhaustively mine and taxonomize common and rare driving scenarios which are used during evaluation to get fine-grained insights into the performance and characteristics of a planner.","Beyond the dataset, we provide a simulation and evaluation framework that enables a planner's actions to be simulated in closed-loop to account for interactions with other traffic participants.","We present a detailed analysis of numerous baselines and investigate gaps between ML-based and traditional methods.","Find the nuPlan dataset and code at nuplan.org."],"url":"http://arxiv.org/abs/2403.04133v1","category":"cs.CV"}
{"created":"2024-03-07 00:44:21","title":"Scalable and Robust Transformer Decoders for Interpretable Image Classification with Foundation Models","abstract":"Interpretable computer vision models can produce transparent predictions, where the features of an image are compared with prototypes from a training dataset and the similarity between them forms a basis for classification. Nevertheless these methods are computationally expensive to train, introduce additional complexity and may require domain knowledge to adapt hyper-parameters to a new dataset. Inspired by developments in object detection, segmentation and large-scale self-supervised foundation vision models, we introduce Component Features (ComFe), a novel explainable-by-design image classification approach using a transformer-decoder head and hierarchical mixture-modelling. With only global image labels and no segmentation or part annotations, ComFe can identify consistent image components, such as the head, body, wings and tail of a bird, and the image background, and determine which of these features are informative in making a prediction. We demonstrate that ComFe obtains higher accuracy compared to previous interpretable models across a range of fine-grained vision benchmarks, without the need to individually tune hyper-parameters for each dataset. We also show that ComFe outperforms a non-interpretable linear head across a range of datasets, including ImageNet, and improves performance on generalisation and robustness benchmarks.","sentences":["Interpretable computer vision models can produce transparent predictions, where the features of an image are compared with prototypes from a training dataset and the similarity between them forms a basis for classification.","Nevertheless these methods are computationally expensive to train, introduce additional complexity and may require domain knowledge to adapt hyper-parameters to a new dataset.","Inspired by developments in object detection, segmentation and large-scale self-supervised foundation vision models, we introduce Component Features (ComFe), a novel explainable-by-design image classification approach using a transformer-decoder head and hierarchical mixture-modelling.","With only global image labels and no segmentation or part annotations, ComFe can identify consistent image components, such as the head, body, wings and tail of a bird, and the image background, and determine which of these features are informative in making a prediction.","We demonstrate that ComFe obtains higher accuracy compared to previous interpretable models across a range of fine-grained vision benchmarks, without the need to individually tune hyper-parameters for each dataset.","We also show that ComFe outperforms a non-interpretable linear head across a range of datasets, including ImageNet, and improves performance on generalisation and robustness benchmarks."],"url":"http://arxiv.org/abs/2403.04125v1","category":"cs.CV"}
{"created":"2024-03-07 00:20:11","title":"Globally Stable Neural Imitation Policies","abstract":"Imitation learning presents an effective approach to alleviate the resource-intensive and time-consuming nature of policy learning from scratch in the solution space. Even though the resulting policy can mimic expert demonstrations reliably, it often lacks predictability in unexplored regions of the state-space, giving rise to significant safety concerns in the face of perturbations. To address these challenges, we introduce the Stable Neural Dynamical System (SNDS), an imitation learning regime which produces a policy with formal stability guarantees. We deploy a neural policy architecture that facilitates the representation of stability based on Lyapunov theorem, and jointly train the policy and its corresponding Lyapunov candidate to ensure global stability. We validate our approach by conducting extensive experiments in simulation and successfully deploying the trained policies on a real-world manipulator arm. The experimental results demonstrate that our method overcomes the instability, accuracy, and computational intensity problems associated with previous imitation learning methods, making our method a promising solution for stable policy learning in complex planning scenarios.","sentences":["Imitation learning presents an effective approach to alleviate the resource-intensive and time-consuming nature of policy learning from scratch in the solution space.","Even though the resulting policy can mimic expert demonstrations reliably, it often lacks predictability in unexplored regions of the state-space, giving rise to significant safety concerns in the face of perturbations.","To address these challenges, we introduce the Stable Neural Dynamical System (SNDS), an imitation learning regime which produces a policy with formal stability guarantees.","We deploy a neural policy architecture that facilitates the representation of stability based on Lyapunov theorem, and jointly train the policy and its corresponding Lyapunov candidate to ensure global stability.","We validate our approach by conducting extensive experiments in simulation and successfully deploying the trained policies on a real-world manipulator arm.","The experimental results demonstrate that our method overcomes the instability, accuracy, and computational intensity problems associated with previous imitation learning methods, making our method a promising solution for stable policy learning in complex planning scenarios."],"url":"http://arxiv.org/abs/2403.04118v1","category":"cs.RO"}
{"created":"2024-03-06 23:16:19","title":"Testing Business Cycle Theories: Evidence from the Great Recession","abstract":"Empirical business cycle studies using cross-country data usually cannot achieve causal relationships while within-country studies mostly focus on the bust period. We provide the first causal investigation into the boom period of the 1999-2010 U.S. cross-metropolitan business cycle. Using a novel research design, we show that credit expansion in private-label mortgages causes a differentially stronger boom (2000-2006) and bust (2007-2010) cycle in the house-related industries in the high net-export-growth areas. Most importantly, our unique research design enables us to perform the most comprehensive tests on theories (hypotheses) regarding the business cycle. We show that the following theories (hypotheses) cannot explain the cause of the 1999-2010 U.S. business cycle: the speculative euphoria hypothesis, the real business cycle theory, the collateral-driven credit cycle theory, the business uncertainty theory, and the extrapolative expectation theory.","sentences":["Empirical business cycle studies using cross-country data usually cannot achieve causal relationships while within-country studies mostly focus on the bust period.","We provide the first causal investigation into the boom period of the 1999-2010 U.S. cross-metropolitan business cycle.","Using a novel research design, we show that credit expansion in private-label mortgages causes a differentially stronger boom (2000-2006) and bust (2007-2010) cycle in the house-related industries in the high net-export-growth areas.","Most importantly, our unique research design enables us to perform the most comprehensive tests on theories (hypotheses) regarding the business cycle.","We show that the following theories (hypotheses) cannot explain the cause of the 1999-2010 U.S. business cycle: the speculative euphoria hypothesis, the real business cycle theory, the collateral-driven credit cycle theory, the business uncertainty theory, and the extrapolative expectation theory."],"url":"http://arxiv.org/abs/2403.04104v1","category":"q-fin.GN"}
{"created":"2024-03-06 22:43:29","title":"Galaxies lensing quadruply imaged quasars lie close to the centers of their lensing potentials","abstract":"In modeling the potentials of quadruply lensed quasars, investigators often employ models for the stellar surface brightness profile of the lensing galaxy for the sole purpose of eliminating its contamination of the light from the quasar images and their hosts. When the center of that profile is used, it is usually as a weak prior on the center of the potential. But the central maxima in the light from single lensing galaxies lie closer the centers of the lens potentials than has heretofore been thought. New measurements are presented here of the positions of the compact central regions of lensing galaxies observed with the Hubble Space Telescope. Comparing these with modeled positions for the centers of the lensing potential (constrained only by the positions of the four quasar images) we find agreement consistent with the larger of either our measurement uncertainties or 1% of the radius of the Einstein ring.","sentences":["In modeling the potentials of quadruply lensed quasars, investigators often employ models for the stellar surface brightness profile of the lensing galaxy for the sole purpose of eliminating its contamination of the light from the quasar images and their hosts.","When the center of that profile is used, it is usually as a weak prior on the center of the potential.","But the central maxima in the light from single lensing galaxies lie closer the centers of the lens potentials than has heretofore been thought.","New measurements are presented here of the positions of the compact central regions of lensing galaxies observed with the Hubble Space Telescope.","Comparing these with modeled positions for the centers of the lensing potential (constrained only by the positions of the four quasar images) we find agreement consistent with the larger of either our measurement uncertainties or 1% of the radius of the Einstein ring."],"url":"http://arxiv.org/abs/2403.04092v1","category":"astro-ph.CO"}
{"created":"2024-03-06 22:39:55","title":"Asymptotic Product-form Steady-state for Multiclass Queueing Networks with SBP Service Policies in Multi-scale Heavy Traffic","abstract":"In this work, we study the stationary distribution of the scaled queue length vector process in multi-class queueing networks operating under static buffer priority service policies. We establish that when subjected to a multi-scale heavy traffic condition, the stationary distribution converges to a product-form limit, with each component in the product form following an exponential distribution. This convergence is achieved under the assumption of uniform moment bounds. As an example, the uniform moment bound condition is shown to hold in a two-station five-class re-entrant line. Whether such a bound holds for general multi-class queueing networks remains an open problem.","sentences":["In this work, we study the stationary distribution of the scaled queue length vector process in multi-class queueing networks operating under static buffer priority service policies.","We establish that when subjected to a multi-scale heavy traffic condition, the stationary distribution converges to a product-form limit, with each component in the product form following an exponential distribution.","This convergence is achieved under the assumption of uniform moment bounds.","As an example, the uniform moment bound condition is shown to hold in a two-station five-class re-entrant line.","Whether such a bound holds for general multi-class queueing networks remains an open problem."],"url":"http://arxiv.org/abs/2403.04090v1","category":"math.PR"}
{"created":"2024-03-06 22:15:37","title":"Energy-Energy Correlation in the back-to-back region at N$^3$LL+NNLO in QCD","abstract":"We consider the Energy-Energy Correlation function in electron-positron annihilation to hadrons. We concentrate on the back-to-back region, performing all-order resummation of the logarithmically enhanced contributions in QCD perturbation theory, up to next-to-next-to-next-to-leading logarithmic (N$^3$LL) accuracy. Away from the back-to-back region, we consistently combine resummed predictions with the known fixed-order results up to next-to-next-to-leading order (NNLO). All perturbative terms up to order {\\alpha}S3 are included in our calculation, which exactly reproduces, after integration over the angular separation variable, the next-to-next-to-next-to-leading order (N$^3$LO) result for the total cross section. We regularize the Landau singularity of the QCD coupling within the so-called Minimal Prescription. We exhibit and discuss the reduction of the perturbative scale dependence of distributions at higher orders, as a means to estimate the corresponding residual perturbative uncertainty. We finally present an illustrative comparison with LEP data.","sentences":["We consider the Energy-Energy Correlation function in electron-positron annihilation to hadrons.","We concentrate on the back-to-back region, performing all-order resummation of the logarithmically enhanced contributions in QCD perturbation theory, up to next-to-next-to-next-to-leading logarithmic (N$^3$LL) accuracy.","Away from the back-to-back region, we consistently combine resummed predictions with the known fixed-order results up to next-to-next-to-leading order (NNLO).","All perturbative terms up to order {\\alpha}S3 are included in our calculation, which exactly reproduces, after integration over the angular separation variable, the next-to-next-to-next-to-leading order (N$^3$LO) result for the total cross section.","We regularize the Landau singularity of the QCD coupling within the so-called Minimal Prescription.","We exhibit and discuss the reduction of the perturbative scale dependence of distributions at higher orders, as a means to estimate the corresponding residual perturbative uncertainty.","We finally present an illustrative comparison with LEP data."],"url":"http://arxiv.org/abs/2403.04077v1","category":"hep-ph"}
{"created":"2024-03-06 21:27:31","title":"Chance-Constrained Control for Safe Spacecraft Autonomy: Convex Programming Approach","abstract":"This paper presents a robust path-planning framework for safe spacecraft autonomy under uncertainty and develops a computationally tractable formulation based on convex programming. We utilize chance-constrained control to formulate the problem. It provides a mathematical framework to solve for a sequence of control policies that minimizes a probabilistic cost under probabilistic constraints with a user-defined confidence level (e.g., safety with 99.9% confidence). The framework enables the planner to directly control state distributions under operational uncertainties while ensuring the vehicle safety. This paper rigorously formulates the safe autonomy problem, gathers and extends techniques in literature to accommodate key cost/constraint functions that often arise in spacecraft path planning, and develops a tractable solution method. The presented framework is demonstrated via two representative numerical examples: safe autonomous rendezvous and orbit maintenance in cislunar space, both under uncertainties due to navigation error from Kalman filter, execution error via Gates model, and imperfect force models.","sentences":["This paper presents a robust path-planning framework for safe spacecraft autonomy under uncertainty and develops a computationally tractable formulation based on convex programming.","We utilize chance-constrained control to formulate the problem.","It provides a mathematical framework to solve for a sequence of control policies that minimizes a probabilistic cost under probabilistic constraints with a user-defined confidence level (e.g., safety with 99.9% confidence).","The framework enables the planner to directly control state distributions under operational uncertainties while ensuring the vehicle safety.","This paper rigorously formulates the safe autonomy problem, gathers and extends techniques in literature to accommodate key cost/constraint functions that often arise in spacecraft path planning, and develops a tractable solution method.","The presented framework is demonstrated via two representative numerical examples: safe autonomous rendezvous and orbit maintenance in cislunar space, both under uncertainties due to navigation error from Kalman filter, execution error via Gates model, and imperfect force models."],"url":"http://arxiv.org/abs/2403.04062v1","category":"math.OC"}
{"created":"2024-03-06 21:18:25","title":"A contact map method to capture the features of knot conformations","abstract":"Inspired by recent advances in the chromosome capture techniques, a method is proposed to study the structural organization of systems of polymers rings with topological constraints.To this purpose, the system is divided into compartments and a simple condition is provided in order to determine if two compartments are in contact or not. Next, a set of contact matrices $\\bar T_{ab}$ is defined that count how many times during a simulation a compartment $a$ was found in contact with a non-contiguous compartment $b$ in conformations with a given energy or temperature. Similar strategies based on correlation maps have been applied to the study of knotted polymers in the recent past. The advantage of the present approach is that is coupled with the Wang-Landau algorithm. Once the density of states is computed, it is possible to generate the contact matrices at any temperature. This gives an immediate overview over the changes of phases that polymer systems undergo. The information on the structure of knotted polymers and links stored in the contact matrices is the result of averaging hundred of billions of conformations and visualized by means of colormaps. The obtained color patterns allow to identify the main properties of the structure of the system under investigation at any temperature. The method is applied to detect the structural rearrangements following the phase transitions of a knotted polymer ring and a circular polycatenane composed by four rings in a solution. It is shown that the colormaps have a finite number of patterns that can be clearly associated with the different phases of these systems. Colormaps also bring new knowledge, for instance predicting the average number of tails appearing in the conformations of the considered polymers at a given temperature.","sentences":["Inspired by recent advances in the chromosome capture techniques, a method is proposed to study the structural organization of systems of polymers rings with topological constraints.","To this purpose, the system is divided into compartments and a simple condition is provided in order to determine if two compartments are in contact or not.","Next, a set of contact matrices $\\bar T_{ab}$ is defined that count how many times during a simulation a compartment $a$ was found in contact with a non-contiguous compartment $b$ in conformations with a given energy or temperature.","Similar strategies based on correlation maps have been applied to the study of knotted polymers in the recent past.","The advantage of the present approach is that is coupled with the Wang-Landau algorithm.","Once the density of states is computed, it is possible to generate the contact matrices at any temperature.","This gives an immediate overview over the changes of phases that polymer systems undergo.","The information on the structure of knotted polymers and links stored in the contact matrices is the result of averaging hundred of billions of conformations and visualized by means of colormaps.","The obtained color patterns allow to identify the main properties of the structure of the system under investigation at any temperature.","The method is applied to detect the structural rearrangements following the phase transitions of a knotted polymer ring and a circular polycatenane composed by four rings in a solution.","It is shown that the colormaps have a finite number of patterns that can be clearly associated with the different phases of these systems.","Colormaps also bring new knowledge, for instance predicting the average number of tails appearing in the conformations of the considered polymers at a given temperature."],"url":"http://arxiv.org/abs/2403.04060v1","category":"cond-mat.soft"}
{"created":"2024-03-06 21:13:17","title":"Plant-Capture Methods for Estimating Population Size from Uncertain Plant Captures","abstract":"Plant-capture is a variant of classical capture-recapture methods used to estimate the size of a population. In this method, decoys referred to as \"plants\" are introduced into the population in order to estimate the capture probability. The method has shown considerable success in estimating population sizes from limited samples in many epidemiological, ecological, and demographic studies. However, previous plant-recapture studies have not systematically accounted for uncertainty in the capture status of each individual plant. In this work, we propose various approaches to formally incorporate uncertainty into the plant-capture model arising from (i) the capture status of plants and (ii) the heterogeneity between multiple survey sites. We present two inference methods and compare their performance in simulation studies. We then apply our methods to estimate the size of the homeless population in several US cities using the large-scale \"S-night\" study conducted by the US Census Bureau.","sentences":["Plant-capture is a variant of classical capture-recapture methods used to estimate the size of a population.","In this method, decoys referred to as \"plants\" are introduced into the population in order to estimate the capture probability.","The method has shown considerable success in estimating population sizes from limited samples in many epidemiological, ecological, and demographic studies.","However, previous plant-recapture studies have not systematically accounted for uncertainty in the capture status of each individual plant.","In this work, we propose various approaches to formally incorporate uncertainty into the plant-capture model arising from (i) the capture status of plants and (ii) the heterogeneity between multiple survey sites.","We present two inference methods and compare their performance in simulation studies.","We then apply our methods to estimate the size of the homeless population in several US cities using the large-scale \"S-night\" study conducted by the US Census Bureau."],"url":"http://arxiv.org/abs/2403.04058v1","category":"stat.ME"}
{"created":"2024-03-06 21:08:20","title":"Conformal Symmetry in Quantum Gravity","abstract":"We study the problem of how to derive conformal symmetry in the framework of quantum gravity. We start with a generic gravitational theory which is invariant under both the general coordinate transformation (GCT) and Weyl transformation (or equivalently, local scale transformation), and then construct its BRST formalism by fixing the gauge symmetries by the extended de Donder gauge and scalar gauge conditions. These gauge-fixing conditions are invariant under global $GL(4)$ and global scale transformations. The gauge-fixed and BRST invariant quantum action possesses a huge Poincar\\'e-like $IOSp(10|10)$ global symmetry, from which we can construct an extended conformal symmetry in a flat Minkowski background in the sense that the Lorentz symmetry is replaced with the $GL(4)$ symmetry. Moreover, we construct the conventional conformal symmetry out of this extended symmetry. With a flat Minkowski background $\\langle g_{\\mu\\nu} \\rangle = \\eta_{\\mu\\nu}$ and a non-zero scalar field $\\langle \\phi \\rangle \\neq 0$, the $GL(4)$ and global scale symmetries are spontaneously broken to the Lorentz symmetry, thereby proving that the graviton and the dilaton are respectively the corresponding Nambu-Goldstone bosons, and therefore they must be exactly massless at nonperturbative level. One of remarkable aspects in our findings is that in quantum gravity, a derivation of conformal symmetry does not depend on a classical action, and its generators are built from only the gauge-fixing and the FP ghost actions. Finally, we address a generalized Zumino theorem in quantum gravity.","sentences":["We study the problem of how to derive conformal symmetry in the framework of quantum gravity.","We start with a generic gravitational theory which is invariant under both the general coordinate transformation (GCT) and Weyl transformation (or equivalently, local scale transformation), and then construct its BRST formalism by fixing the gauge symmetries by the extended de Donder gauge and scalar gauge conditions.","These gauge-fixing conditions are invariant under global $GL(4)$ and global scale transformations.","The gauge-fixed and BRST invariant quantum action possesses a huge Poincar\\'e-like $IOSp(10|10)$ global symmetry, from which we can construct an extended conformal symmetry in a flat Minkowski background in the sense that the Lorentz symmetry is replaced with the $GL(4)$ symmetry.","Moreover, we construct the conventional conformal symmetry out of this extended symmetry.","With a flat Minkowski background $\\langle g_{\\mu\\nu} \\rangle = \\eta_{\\mu\\nu}$ and a non-zero scalar field $\\langle \\phi \\rangle \\neq 0$, the $GL(4)$ and global scale symmetries are spontaneously broken to the Lorentz symmetry, thereby proving that the graviton and the dilaton are respectively the corresponding Nambu-Goldstone bosons, and therefore they must be exactly massless at nonperturbative level.","One of remarkable aspects in our findings is that in quantum gravity, a derivation of conformal symmetry does not depend on a classical action, and its generators are built from only the gauge-fixing and the FP ghost actions.","Finally, we address a generalized Zumino theorem in quantum gravity."],"url":"http://arxiv.org/abs/2403.04056v1","category":"hep-th"}
{"created":"2024-03-06 21:04:01","title":"Bounds for Rainbow-uncommon Graphs","abstract":"We say a graph $H$ is $r$-rainbow-uncommon if the maximum number of rainbow copies of $H$ under an $r$-coloring of $E(K_n)$ is asymptotically (as $n \\to \\infty$) greater than what is expected from uniformly random $r$-colorings. Via explicit constructions, we show that for $H\\in\\{K_3,K_4, K_5\\}$, $H$ is $r$-rainbow-uncommon for all $r\\geq {|V(H)|\\choose 2}$. We also construct colorings to show that for $t \\geq 6$, $K_t$ is $r$-rainbow-uncommon for sufficiently large $r$.","sentences":["We say a graph $H$ is $r$-rainbow-uncommon if the maximum number of rainbow copies of $H$ under an $r$-coloring of $E(K_n)$ is asymptotically (as $n \\to \\infty$) greater than what is expected from uniformly random $r$-colorings.","Via explicit constructions, we show that for $H\\in\\{K_3,K_4, K_5\\}$, $H$ is $r$-rainbow-uncommon for all $r\\geq {|V(H)|\\choose 2}$. We also construct colorings to show that for $t \\geq 6$, $K_t$ is $r$-rainbow-uncommon for sufficiently large $r$."],"url":"http://arxiv.org/abs/2403.04055v1","category":"math.CO"}
{"created":"2024-03-06 20:52:49","title":"Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations","abstract":"Reinforcement learning (RL) has achieved phenomenal success in various domains. However, its data-driven nature also introduces new vulnerabilities that can be exploited by malicious opponents. Recent work shows that a well-trained RL agent can be easily manipulated by strategically perturbing its state observations at the test stage. Existing solutions either introduce a regularization term to improve the smoothness of the trained policy against perturbations or alternatively train the agent's policy and the attacker's policy. However, the former does not provide sufficient protection against strong attacks, while the latter is computationally prohibitive for large environments. In this work, we propose a new robust RL algorithm for deriving a pessimistic policy to safeguard against an agent's uncertainty about true states. This approach is further enhanced with belief state inference and diffusion-based state purification to reduce uncertainty. Empirical results show that our approach obtains superb performance under strong attacks and has a comparable training overhead with regularization-based methods. Our code is available at https://github.com/SliencerX/Belief-enriched-robust-Q-learning.","sentences":["Reinforcement learning (RL) has achieved phenomenal success in various domains.","However, its data-driven nature also introduces new vulnerabilities that can be exploited by malicious opponents.","Recent work shows that a well-trained RL agent can be easily manipulated by strategically perturbing its state observations at the test stage.","Existing solutions either introduce a regularization term to improve the smoothness of the trained policy against perturbations or alternatively train the agent's policy and the attacker's policy.","However, the former does not provide sufficient protection against strong attacks, while the latter is computationally prohibitive for large environments.","In this work, we propose a new robust RL algorithm for deriving a pessimistic policy to safeguard against an agent's uncertainty about true states.","This approach is further enhanced with belief state inference and diffusion-based state purification to reduce uncertainty.","Empirical results show that our approach obtains superb performance under strong attacks and has a comparable training overhead with regularization-based methods.","Our code is available at https://github.com/SliencerX/Belief-enriched-robust-Q-learning."],"url":"http://arxiv.org/abs/2403.04050v1","category":"cs.LG"}
{"created":"2024-03-06 20:42:28","title":"Experimental and theoretical total cross sections for single and double ionization of the open-$4d$-shell ions Xe$^{12+}$, Xe$^{13+}$, and Xe$^{14+}$ by electron impact","abstract":"We present new experimental and theoretical cross sections for electron-impact single ionization of Xe$^{12+}$ and Xe$^{13+}$ ions, and double ionization of Xe$^{12+}$, Xe$^{13+}$ and Xe$^{14+}$ ions for collision energies from the respective ionization thresholds up to 3500 eV. The calculations use the fully relativistic subconfiguration-averaged distorted-wave (SCADW) approach and, partly, the more detailed level-to-level distorted wave (LLDW) method. We find that, unlike in previous work, our theoretical cross sections agree with our experimental ones within the experimental uncertainties, except for the near-threshold double-ionization cross sections. We attribute this remaining discrepancy to the neglect of direct-double ionization in the present theoretical treatment.","sentences":["We present new experimental and theoretical cross sections for electron-impact single ionization of Xe$^{12+}$ and Xe$^{13+}$ ions, and double ionization of Xe$^{12+}$, Xe$^{13+}$ and Xe$^{14+}$ ions for collision energies from the respective ionization thresholds up to 3500 eV. The calculations use the fully relativistic subconfiguration-averaged distorted-wave (SCADW) approach and, partly, the more detailed level-to-level distorted wave (LLDW) method.","We find that, unlike in previous work, our theoretical cross sections agree with our experimental ones within the experimental uncertainties, except for the near-threshold double-ionization cross sections.","We attribute this remaining discrepancy to the neglect of direct-double ionization in the present theoretical treatment."],"url":"http://arxiv.org/abs/2403.04042v1","category":"physics.atom-ph"}
{"created":"2024-03-06 20:41:53","title":"Cascaded Self-supervised Learning for Subject-independent EEG-based Emotion Recognition","abstract":"EEG-based Emotion recognition holds significant promise for applications in human-computer interaction, medicine, and neuroscience. While deep learning has shown potential in this field, current approaches usually rely on large-scale high-quality labeled datasets, limiting the performance of deep learning. Self-supervised learning offers a solution by automatically generating labels, but its inter-subject generalizability remains under-explored. For this reason, our interest lies in offering a self-supervised learning paradigm with better inter-subject generalizability. Inspired by recent efforts in combining low-level and high-level tasks in deep learning, we propose a cascaded self-supervised architecture for EEG emotion recognition. Then, we introduce a low-level task, time-to-frequency reconstruction (TFR). This task leverages the inherent time-frequency relationship in EEG signals. Our architecture integrates it with the high-level contrastive learning modules, performing self-supervised learning for EEG-based emotion recognition. Experiment on DEAP and DREAMER datasets demonstrates superior performance of our method over similar works. The outcome results also highlight the indispensability of the TFR task and the robustness of our method to label scarcity, validating the effectiveness of the proposed method.","sentences":["EEG-based Emotion recognition holds significant promise for applications in human-computer interaction, medicine, and neuroscience.","While deep learning has shown potential in this field, current approaches usually rely on large-scale high-quality labeled datasets, limiting the performance of deep learning.","Self-supervised learning offers a solution by automatically generating labels, but its inter-subject generalizability remains under-explored.","For this reason, our interest lies in offering a self-supervised learning paradigm with better inter-subject generalizability.","Inspired by recent efforts in combining low-level and high-level tasks in deep learning, we propose a cascaded self-supervised architecture for EEG emotion recognition.","Then, we introduce a low-level task, time-to-frequency reconstruction (TFR).","This task leverages the inherent time-frequency relationship in EEG signals.","Our architecture integrates it with the high-level contrastive learning modules, performing self-supervised learning for EEG-based emotion recognition.","Experiment on DEAP and DREAMER datasets demonstrates superior performance of our method over similar works.","The outcome results also highlight the indispensability of the TFR task and the robustness of our method to label scarcity, validating the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.04041v1","category":"eess.SP"}
{"created":"2024-03-06 20:37:29","title":"Sample size planning for conditional counterfactual mean estimation with a K-armed randomized experiment","abstract":"We cover how to determine a sufficiently large sample size for a $K$-armed randomized experiment in order to estimate conditional counterfactual expectations in data-driven subgroups. The sub-groups can be output by any feature space partitioning algorithm, including as defined by binning users having similar predictive scores or as defined by a learned policy tree. After carefully specifying the inference target, a minimum confidence level, and a maximum margin of error, the key is to turn the original goal into a simultaneous inference problem where the recommended sample size to offset an increased possibility of estimation error is directly related to the number of inferences to be conducted. Given a fixed sample size budget, our result allows us to invert the question to one about the feasible number of treatment arms or partition complexity (e.g. number of decision tree leaves). Using policy trees to learn sub-groups, we evaluate our nominal guarantees on a large publicly-available randomized experiment test data set.","sentences":["We cover how to determine a sufficiently large sample size for a $K$-armed randomized experiment in order to estimate conditional counterfactual expectations in data-driven subgroups.","The sub-groups can be output by any feature space partitioning algorithm, including as defined by binning users having similar predictive scores or as defined by a learned policy tree.","After carefully specifying the inference target, a minimum confidence level, and a maximum margin of error, the key is to turn the original goal into a simultaneous inference problem where the recommended sample size to offset an increased possibility of estimation error is directly related to the number of inferences to be conducted.","Given a fixed sample size budget, our result allows us to invert the question to one about the feasible number of treatment arms or partition complexity (e.g. number of decision tree leaves).","Using policy trees to learn sub-groups, we evaluate our nominal guarantees on a large publicly-available randomized experiment test data set."],"url":"http://arxiv.org/abs/2403.04039v1","category":"cs.LG"}
{"created":"2024-03-06 20:19:48","title":"Two-Person adversarial games are zero-sum: A resolution of the Luce-Raiffa-Aumann (LRA) conjecture","abstract":"This letter: (i) reformulates the theorems of Adler-Daskalakis-Papadimitriou (2009) and Raimondo (2023) on two-player adversarial games as a generalized result with a simplified proof, (ii) forges connections to work on strategically zero-sum games by Moulin-Vial (1978), and on axiomatizations of multi-linear utilities of n-person games by Fishburn-Roberts (1976, 1978). The simplification and the connections on offer give prominence to two-person zero-sum games studied by Aumann (1961), Shapley (1964) and Rosenthal (1974), and also to recent algorithmic work in computer science. We give a productive reorientation to the subject by bringing the two communities together under the rubric of adversarial games.","sentences":["This letter: (i) reformulates the theorems of Adler-Daskalakis-Papadimitriou (2009) and Raimondo (2023) on two-player adversarial games as a generalized result with a simplified proof, (ii) forges connections to work on strategically zero-sum games by Moulin-Vial (1978), and on axiomatizations of multi-linear utilities of n-person games by Fishburn-Roberts (1976, 1978).","The simplification and the connections on offer give prominence to two-person zero-sum games studied by Aumann (1961), Shapley (1964) and Rosenthal (1974), and also to recent algorithmic work in computer science.","We give a productive reorientation to the subject by bringing the two communities together under the rubric of adversarial games."],"url":"http://arxiv.org/abs/2403.04029v1","category":"econ.TH"}
{"created":"2024-03-06 20:11:20","title":"Worldsheet patching, 1-form symmetries, and \"Landau-star\" phase transitions","abstract":"The analysis of phase transitions of gauge theories has relied heavily on simplifications that arise at the boundaries of phase diagrams, where certain excitations are forbidden. Taking 2+1 dimensional $\\mathbb{Z}_2$ gauge theory as an example, the simplification can be visualized geometrically: on the phase diagram boundaries the partition function is an ensemble of closed membranes. More generally, however, the membranes have \"holes\" in them, representing worldlines of virtual anyon excitations. If the holes are of a finite size, then typically they do not affect the universality class, but they destroy microscopic (higher-form) symmetries and microscopic (string) observables. We demonstrate how these symmetries and observables can be restored using a \"membrane patching\" procedure, which maps the ensemble of membranes back to an ensemble of closed membranes. (This is closely related to the idea of gauge fixing in the \"minimal gauge\", though not equivalent.) Membrane patching makes the emergence of higher symmetry concrete. Performing patching in a Monte Carlo simulation with an appropriate algorithm, we show that it gives access to numerically useful observables. For example, the confinement transition can be analyzed using a correlation function that is a power law at the critical point. We analyze the quasi-locality of the patching procedure and discuss what happens at a self-dual multicritical point in the gauge-Higgs model, where the lengthscale $\\ell$ characterizing the holes diverges.","sentences":["The analysis of phase transitions of gauge theories has relied heavily on simplifications that arise at the boundaries of phase diagrams, where certain excitations are forbidden.","Taking 2+1 dimensional $\\mathbb{Z}_2$ gauge theory as an example, the simplification can be visualized geometrically: on the phase diagram boundaries the partition function is an ensemble of closed membranes.","More generally, however, the membranes have \"holes\" in them, representing worldlines of virtual anyon excitations.","If the holes are of a finite size, then typically they do not affect the universality class, but they destroy microscopic (higher-form) symmetries and microscopic (string) observables.","We demonstrate how these symmetries and observables can be restored using a \"membrane patching\" procedure, which maps the ensemble of membranes back to an ensemble of closed membranes.","(This is closely related to the idea of gauge fixing in the \"minimal gauge\", though not equivalent.)","Membrane patching makes the emergence of higher symmetry concrete.","Performing patching in a Monte Carlo simulation with an appropriate algorithm, we show that it gives access to numerically useful observables.","For example, the confinement transition can be analyzed using a correlation function that is a power law at the critical point.","We analyze the quasi-locality of the patching procedure and discuss what happens at a self-dual multicritical point in the gauge-Higgs model, where the lengthscale $\\ell$ characterizing the holes diverges."],"url":"http://arxiv.org/abs/2403.04025v1","category":"cond-mat.str-el"}
{"created":"2024-03-06 20:10:41","title":"Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification","abstract":"In chest X-ray (CXR) image analysis, rule-based systems are usually employed to extract labels from reports, but concerns exist about label quality. These datasets typically offer only presence labels, sometimes with binary uncertainty indicators, which limits their usefulness. In this work, we present MAPLEZ (Medical report Annotations with Privacy-preserving Large language model using Expeditious Zero shot answers), a novel approach leveraging a locally executable Large Language Model (LLM) to extract and enhance findings labels on CXR reports. MAPLEZ extracts not only binary labels indicating the presence or absence of a finding but also the location, severity, and radiologists' uncertainty about the finding. Over eight abnormalities from five test sets, we show that our method can extract these annotations with an increase of 5 percentage points (pp) in F1 score for categorical presence annotations and more than 30 pp increase in F1 score for the location annotations over competing labelers. Additionally, using these improved annotations in classification supervision, we demonstrate substantial advancements in model quality, with an increase of 1.7 pp in AUROC over models trained with annotations from the state-of-the-art approach. We share code and annotations.","sentences":["In chest X-ray (CXR) image analysis, rule-based systems are usually employed to extract labels from reports, but concerns exist about label quality.","These datasets typically offer only presence labels, sometimes with binary uncertainty indicators, which limits their usefulness.","In this work, we present MAPLEZ (Medical report Annotations with Privacy-preserving Large language model using Expeditious Zero shot answers), a novel approach leveraging a locally executable Large Language Model (LLM) to extract and enhance findings labels on CXR reports.","MAPLEZ extracts not only binary labels indicating the presence or absence of a finding but also the location, severity, and radiologists' uncertainty about the finding.","Over eight abnormalities from five test sets, we show that our method can extract these annotations with an increase of 5 percentage points (pp) in F1 score for categorical presence annotations and more than 30 pp increase in F1 score for the location annotations over competing labelers.","Additionally, using these improved annotations in classification supervision, we demonstrate substantial advancements in model quality, with an increase of 1.7 pp in AUROC over models trained with annotations from the state-of-the-art approach.","We share code and annotations."],"url":"http://arxiv.org/abs/2403.04024v1","category":"eess.IV"}
{"created":"2024-03-06 20:03:27","title":"Multi-Robot Autonomous Exploration and Mapping Under Localization Uncertainty with Expectation-Maximization","abstract":"We propose an autonomous exploration algorithm designed for decentralized multi-robot teams, which takes into account map and localization uncertainties of range-sensing mobile robots. Virtual landmarks are used to quantify the combined impact of process noise and sensor noise on map uncertainty. Additionally, we employ an iterative expectation-maximization inspired algorithm to assess the potential outcomes of both a local robot's and its neighbors' next-step actions. To evaluate the effectiveness of our framework, we conduct a comparative analysis with state-of-the-art algorithms. The results of our experiments show the proposed algorithm's capacity to strike a balance between curbing map uncertainty and achieving efficient task allocation among robots.","sentences":["We propose an autonomous exploration algorithm designed for decentralized multi-robot teams, which takes into account map and localization uncertainties of range-sensing mobile robots.","Virtual landmarks are used to quantify the combined impact of process noise and sensor noise on map uncertainty.","Additionally, we employ an iterative expectation-maximization inspired algorithm to assess the potential outcomes of both a local robot's and its neighbors' next-step actions.","To evaluate the effectiveness of our framework, we conduct a comparative analysis with state-of-the-art algorithms.","The results of our experiments show the proposed algorithm's capacity to strike a balance between curbing map uncertainty and achieving efficient task allocation among robots."],"url":"http://arxiv.org/abs/2403.04021v1","category":"cs.RO"}
{"created":"2024-03-06 19:29:08","title":"On the Efficient Marginalization of Probabilistic Sequence Models","abstract":"Real-world data often exhibits sequential dependence, across diverse domains such as human behavior, medicine, finance, and climate modeling. Probabilistic methods capture the inherent uncertainty associated with prediction in these contexts, with autoregressive models being especially prominent. This dissertation focuses on using autoregressive models to answer complex probabilistic queries that go beyond single-step prediction, such as the timing of future events or the likelihood of a specific event occurring before another. In particular, we develop a broad class of novel and efficient approximation techniques for marginalization in sequential models that are model-agnostic. These techniques rely solely on access to and sampling from next-step conditional distributions of a pre-trained autoregressive model, including both traditional parametric models as well as more recent neural autoregressive models. Specific approaches are presented for discrete sequential models, for marked temporal point processes, and for stochastic jump processes, each tailored to a well-defined class of informative, long-range probabilistic queries.","sentences":["Real-world data often exhibits sequential dependence, across diverse domains such as human behavior, medicine, finance, and climate modeling.","Probabilistic methods capture the inherent uncertainty associated with prediction in these contexts, with autoregressive models being especially prominent.","This dissertation focuses on using autoregressive models to answer complex probabilistic queries that go beyond single-step prediction, such as the timing of future events or the likelihood of a specific event occurring before another.","In particular, we develop a broad class of novel and efficient approximation techniques for marginalization in sequential models that are model-agnostic.","These techniques rely solely on access to and sampling from next-step conditional distributions of a pre-trained autoregressive model, including both traditional parametric models as well as more recent neural autoregressive models.","Specific approaches are presented for discrete sequential models, for marked temporal point processes, and for stochastic jump processes, each tailored to a well-defined class of informative, long-range probabilistic queries."],"url":"http://arxiv.org/abs/2403.04005v1","category":"stat.ML"}
{"created":"2024-03-06 19:08:34","title":"Video Relationship Detection Using Mixture of Experts","abstract":"Machine comprehension of visual information from images and videos by neural networks faces two primary challenges. Firstly, there exists a computational and inference gap in connecting vision and language, making it difficult to accurately determine which object a given agent acts on and represent it through language. Secondly, classifiers trained by a single, monolithic neural network often lack stability and generalization. To overcome these challenges, we introduce MoE-VRD, a novel approach to visual relationship detection utilizing a mixture of experts. MoE-VRD identifies language triplets in the form of < subject, predicate, object> tuples to extract relationships from visual processing. Leveraging recent advancements in visual relationship detection, MoE-VRD addresses the requirement for action recognition in establishing relationships between subjects (acting) and objects (being acted upon). In contrast to single monolithic networks, MoE-VRD employs multiple small models as experts, whose outputs are aggregated. Each expert in MoE-VRD specializes in visual relationship learning and object tagging. By utilizing a sparsely-gated mixture of experts, MoE-VRD enables conditional computation and significantly enhances neural network capacity without increasing computational complexity. Our experimental results demonstrate that the conditional computation capabilities and scalability of the mixture-of-experts approach lead to superior performance in visual relationship detection compared to state-of-the-art methods.","sentences":["Machine comprehension of visual information from images and videos by neural networks faces two primary challenges.","Firstly, there exists a computational and inference gap in connecting vision and language, making it difficult to accurately determine which object a given agent acts on and represent it through language.","Secondly, classifiers trained by a single, monolithic neural network often lack stability and generalization.","To overcome these challenges, we introduce MoE-VRD, a novel approach to visual relationship detection utilizing a mixture of experts.","MoE-VRD identifies language triplets in the form of < subject, predicate, object> tuples to extract relationships from visual processing.","Leveraging recent advancements in visual relationship detection, MoE-VRD addresses the requirement for action recognition in establishing relationships between subjects (acting) and objects (being acted upon).","In contrast to single monolithic networks, MoE-VRD employs multiple small models as experts, whose outputs are aggregated.","Each expert in MoE-VRD specializes in visual relationship learning and object tagging.","By utilizing a sparsely-gated mixture of experts, MoE-VRD enables conditional computation and significantly enhances neural network capacity without increasing computational complexity.","Our experimental results demonstrate that the conditional computation capabilities and scalability of the mixture-of-experts approach lead to superior performance in visual relationship detection compared to state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.03994v1","category":"cs.CV"}
{"created":"2024-03-06 19:01:04","title":"A Sierpinski Triangle Data Structure for Efficient Array Value Update and Prefix Sum Calculation","abstract":"The binary indexed tree, or Fenwick tree, is a data structure that can efficiently update values and calculate prefix sums in an array. It allows both of these operations to be performed in $O(\\log_2 N)$ time. Here we present a novel data structure resembling the Sierpinski triangle, which accomplishes these operations with the same memory usage in $O(\\log_3 N)$ time instead. We show this order to be optimal by making use of a connection to quantum computing.","sentences":["The binary indexed tree, or Fenwick tree, is a data structure that can efficiently update values and calculate prefix sums in an array.","It allows both of these operations to be performed in $O(\\log_2","N)$ time.","Here we present a novel data structure resembling the Sierpinski triangle, which accomplishes these operations with the same memory usage in $O(\\log_3 N)$ time instead.","We show this order to be optimal by making use of a connection to quantum computing."],"url":"http://arxiv.org/abs/2403.03990v1","category":"cs.DS"}
{"created":"2024-03-06 19:00:21","title":"Metal enrichment: the apex accretor perspective","abstract":"Aims. The goal of this work is to devise a description of the enrichment process in large-scale structure that explains the available observations and makes predictions for future measurements. Methods. We took a spartan approach to this study, employing observational results and algebra to connect stellar assembly in star-forming halos with metal enrichment of the intra-cluster and group medium. Results. On one hand, our construct is the first to provide an explanation for much of the phenomenology of metal enrichment in clusters and groups. It sheds light on the lack of redshift evolution in metal abundance, as well as the small scatter of metal abundance profiles, the entropy versus abundance anti-correlation found in cool core clusters, and the so-called Fe conundrum, along with several other aspects of cluster enrichment. On the other hand, it also allows us to infer the properties of other constituents of large-scale structure. We find that gas that is not bound to halos must have a metal abundance similar to that of the ICM and only about one-seventh to one-third of the Fe in the Universe is locked in stars. A comparable amount is found in gas in groups and clusters and, lastly and most importantly, about three-fifths of the total Fe is contained in a tenuous warm or hot gaseous medium in or between galaxies. We point out that several of our results follow from two critical but well motivated assumptions: 1) the stellar mass in massive halos is currently underestimated and 2) the adopted Fe yield is only marginally consistent with predictions from synthesis models and SN rates. Conclusions. One of the most appealing features of the work presented here is that it provides an observationally grounded construct where vital questions on chemical enrichment in the large-scale structure can be addressed. We hope that it may serve as a useful baseline for future works.","sentences":["Aims.","The goal of this work is to devise a description of the enrichment process in large-scale structure that explains the available observations and makes predictions for future measurements.","Methods.","We took a spartan approach to this study, employing observational results and algebra to connect stellar assembly in star-forming halos with metal enrichment of the intra-cluster and group medium.","Results.","On one hand, our construct is the first to provide an explanation for much of the phenomenology of metal enrichment in clusters and groups.","It sheds light on the lack of redshift evolution in metal abundance, as well as the small scatter of metal abundance profiles, the entropy versus abundance anti-correlation found in cool core clusters, and the so-called Fe conundrum, along with several other aspects of cluster enrichment.","On the other hand, it also allows us to infer the properties of other constituents of large-scale structure.","We find that gas that is not bound to halos must have a metal abundance similar to that of the ICM and only about one-seventh to one-third of the Fe in the Universe is locked in stars.","A comparable amount is found in gas in groups and clusters and, lastly and most importantly, about three-fifths of the total Fe is contained in a tenuous warm or hot gaseous medium in or between galaxies.","We point out that several of our results follow from two critical but well motivated assumptions: 1) the stellar mass in massive halos is currently underestimated and 2) the adopted Fe yield is only marginally consistent with predictions from synthesis models and SN rates.","Conclusions.","One of the most appealing features of the work presented here is that it provides an observationally grounded construct where vital questions on chemical enrichment in the large-scale structure can be addressed.","We hope that it may serve as a useful baseline for future works."],"url":"http://arxiv.org/abs/2403.03987v1","category":"astro-ph.GA"}
{"created":"2024-03-06 19:00:01","title":"Robust covariance estimation and explainable outlier detection for matrix-valued data","abstract":"The minimum covariance determinant (MCD) estimator is a popular method for robustly estimating the mean and covariance of multivariate data. We extend the MCD to the setting where the observations are matrices rather than vectors and introduce the matrix minimum covariance determinant (MMCD) estimators for robust parameter estimation. These estimators hold equivariance properties, achieve a high breakdown point, and are consistent under elliptical matrix-variate distributions. We have also developed an efficient algorithm with convergence guarantees to compute the MMCD estimators. Using the MMCD estimators, we can compute robust Mahalanobis distances that can be used for outlier detection. Those distances can be decomposed into outlyingness contributions from each cell, row, or column of a matrix-variate observation using Shapley values, a concept for outlier explanation recently introduced in the multivariate setting. Simulations and examples reveal the excellent properties and usefulness of the robust estimators.","sentences":["The minimum covariance determinant (MCD) estimator is a popular method for robustly estimating the mean and covariance of multivariate data.","We extend the MCD to the setting where the observations are matrices rather than vectors and introduce the matrix minimum covariance determinant (MMCD) estimators for robust parameter estimation.","These estimators hold equivariance properties, achieve a high breakdown point, and are consistent under elliptical matrix-variate distributions.","We have also developed an efficient algorithm with convergence guarantees to compute the MMCD estimators.","Using the MMCD estimators, we can compute robust Mahalanobis distances that can be used for outlier detection.","Those distances can be decomposed into outlyingness contributions from each cell, row, or column of a matrix-variate observation using Shapley values, a concept for outlier explanation recently introduced in the multivariate setting.","Simulations and examples reveal the excellent properties and usefulness of the robust estimators."],"url":"http://arxiv.org/abs/2403.03975v1","category":"stat.ME"}
{"created":"2024-03-07 18:58:40","title":"Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed","abstract":"We present a novel method for efficiently producing semi-dense matches across images. Previous detector-free matcher LoFTR has shown remarkable matching capability in handling large-viewpoint change and texture-poor scenarios but suffers from low efficiency. We revisit its design choices and derive multiple improvements for both efficiency and accuracy. One key observation is that performing the transformer over the entire feature map is redundant due to shared local information, therefore we propose an aggregated attention mechanism with adaptive token selection for efficiency. Furthermore, we find spatial variance exists in LoFTR's fine correlation module, which is adverse to matching accuracy. A novel two-stage correlation layer is proposed to achieve accurate subpixel correspondences for accuracy improvement. Our efficiency optimized model is $\\sim 2.5\\times$ faster than LoFTR which can even surpass state-of-the-art efficient sparse matching pipeline SuperPoint + LightGlue. Moreover, extensive experiments show that our method can achieve higher accuracy compared with competitive semi-dense matchers, with considerable efficiency benefits. This opens up exciting prospects for large-scale or latency-sensitive applications such as image retrieval and 3D reconstruction. Project page: https://zju3dv.github.io/efficientloftr.","sentences":["We present a novel method for efficiently producing semi-dense matches across images.","Previous detector-free matcher LoFTR has shown remarkable matching capability in handling large-viewpoint change and texture-poor scenarios but suffers from low efficiency.","We revisit its design choices and derive multiple improvements for both efficiency and accuracy.","One key observation is that performing the transformer over the entire feature map is redundant due to shared local information, therefore we propose an aggregated attention mechanism with adaptive token selection for efficiency.","Furthermore, we find spatial variance exists in LoFTR's fine correlation module, which is adverse to matching accuracy.","A novel two-stage correlation layer is proposed to achieve accurate subpixel correspondences for accuracy improvement.","Our efficiency optimized model is $\\sim 2.5\\times$ faster than LoFTR which can even surpass state-of-the-art efficient sparse matching pipeline SuperPoint + LightGlue.","Moreover, extensive experiments show that our method can achieve higher accuracy compared with competitive semi-dense matchers, with considerable efficiency benefits.","This opens up exciting prospects for large-scale or latency-sensitive applications such as image retrieval and 3D reconstruction.","Project page: https://zju3dv.github.io/efficientloftr."],"url":"http://arxiv.org/abs/2403.04765v1","category":"cs.CV"}
{"created":"2024-03-07 17:18:32","title":"Molecular Arithmetic Coding (MAC) for Internet of Bio-Nano Things (IoBNT)","abstract":"Molecular Communication (MC) has emerged as a promising paradigm employing molecules to transfer information at the nano-scale. Unlike MC channel coding, MC source coding has remained mostly an unexplored area of research. In a recent paper, prefix source coding was introduced into the field, through an MC-adapted version of the Huffman Coding. In the context of MC source coding, this paper proposes the Molecular Arithmetic Coding (MAC) whose algorithmic implementation and code-structure is non-arbitrarily different than that of the widely-known classical arithmetic coding. MAC is designed to mitigate Inter-Symbol Interference (ISI) for alphabets with known symbol probabilities through, in a highly efficient way, avoiding consecutive 1-bits. However, due to bit precision limitations any arithmetic coding method faces, without any assumption made on the structure of the symbol alphabet, unique-decodability of MAC is not guaranteed. Accordingly, a uniquely-decodable new coding scheme named Molecular Arithmetic with Prefix Coding (MAPC) is also introduced. Across multiple alphabets, we show that MAPC provides a better compression performance compared to the optimal MC-adapted prefix coding. Simulation results of an exemplary alphabet demonstrates the superior symbol and word error rate performance of MAPC compared to the optimal MC-adapted prefix coding and to the uncoded BCSK schemes.","sentences":["Molecular Communication (MC) has emerged as a promising paradigm employing molecules to transfer information at the nano-scale.","Unlike MC channel coding, MC source coding has remained mostly an unexplored area of research.","In a recent paper, prefix source coding was introduced into the field, through an MC-adapted version of the Huffman Coding.","In the context of MC source coding, this paper proposes the Molecular Arithmetic Coding (MAC) whose algorithmic implementation and code-structure is non-arbitrarily different than that of the widely-known classical arithmetic coding.","MAC is designed to mitigate Inter-Symbol Interference (ISI) for alphabets with known symbol probabilities through, in a highly efficient way, avoiding consecutive 1-bits.","However, due to bit precision limitations any arithmetic coding method faces, without any assumption made on the structure of the symbol alphabet, unique-decodability of MAC is not guaranteed.","Accordingly, a uniquely-decodable new coding scheme named Molecular Arithmetic with Prefix Coding (MAPC) is also introduced.","Across multiple alphabets, we show that MAPC provides a better compression performance compared to the optimal MC-adapted prefix coding.","Simulation results of an exemplary alphabet demonstrates the superior symbol and word error rate performance of MAPC compared to the optimal MC-adapted prefix coding and to the uncoded BCSK schemes."],"url":"http://arxiv.org/abs/2403.04672v1","category":"cs.IT"}
{"created":"2024-03-07 17:15:22","title":"No anomalous dissipation in two-dimensional incompressible fluids","abstract":"We prove that any sequence of vanishing viscosity Leray--Hopf solutions to the periodic two-dimensional incompressible Navier--Stokes equations does not display anomalous dissipation if the initial vorticity is a measure with positive singular part. A key step in the proof is the use of the Delort--Majda concentration-compactness argument to exclude formation of atoms in the vorticity measure, which in particular implies that the limiting velocity is an admissible weak solution to Euler. Moreover, our proof reveals that the amount of energy dissipation can be bounded by the vorticity contained in an arbitrarily small disk.","sentences":["We prove that any sequence of vanishing viscosity Leray--Hopf solutions to the periodic two-dimensional incompressible Navier--Stokes equations does not display anomalous dissipation if the initial vorticity is a measure with positive singular part.","A key step in the proof is the use of the Delort--Majda concentration-compactness argument to exclude formation of atoms in the vorticity measure, which in particular implies that the limiting velocity is an admissible weak solution to Euler.","Moreover, our proof reveals that the amount of energy dissipation can be bounded by the vorticity contained in an arbitrarily small disk."],"url":"http://arxiv.org/abs/2403.04668v1","category":"math.AP"}
{"created":"2024-03-07 13:49:29","title":"Discriminative Sample-Guided and Parameter-Efficient Feature Space Adaptation for Cross-Domain Few-Shot Learning","abstract":"In this paper, we look at cross-domain few-shot classification which presents the challenging task of learning new classes in unseen domains with few labelled examples. Existing methods, though somewhat effective, encounter several limitations, which we address in this work through two significant improvements. First, to address overfitting associated with fine-tuning a large number of parameters on small datasets, we introduce a lightweight parameter-efficient adaptation strategy. This strategy employs a linear transformation of pre-trained features, significantly reducing the trainable parameter count. Second, we replace the traditional nearest centroid classifier with a variance-aware loss function, enhancing the model's sensitivity to the inter- and intra-class variances within the training set for improved clustering in feature space. Empirical evaluations on the Meta-Dataset benchmark showcase that our approach not only improves accuracy up to 7.7% and 5.3% on seen and unseen datasets respectively but also achieves this performance while being at least ~3x more parameter-efficient than existing methods, establishing a new state-of-the-art in cross-domain few-shot learning. Our code can be found at https://github.com/rashindrie/DIPA.","sentences":["In this paper, we look at cross-domain few-shot classification which presents the challenging task of learning new classes in unseen domains with few labelled examples.","Existing methods, though somewhat effective, encounter several limitations, which we address in this work through two significant improvements.","First, to address overfitting associated with fine-tuning a large number of parameters on small datasets, we introduce a lightweight parameter-efficient adaptation strategy.","This strategy employs a linear transformation of pre-trained features, significantly reducing the trainable parameter count.","Second, we replace the traditional nearest centroid classifier with a variance-aware loss function, enhancing the model's sensitivity to the inter- and intra-class variances within the training set for improved clustering in feature space.","Empirical evaluations on the Meta-Dataset benchmark showcase that our approach not only improves accuracy up to 7.7% and 5.3% on seen and unseen datasets respectively but also achieves this performance while being at least ~3x more parameter-efficient than existing methods, establishing a new state-of-the-art in cross-domain few-shot learning.","Our code can be found at https://github.com/rashindrie/DIPA."],"url":"http://arxiv.org/abs/2403.04492v1","category":"cs.CV"}
{"created":"2024-03-07 10:10:24","title":"A Lagrangian approach for solving an axisymmetric thermo-electromagnetic problem. Application to time-varying geometry processes","abstract":"The aim of this work is to introduce a thermo-electromagnetic model for calculating the temperature and the power dissipated in cylindrical pieces whose geometry var\\'ies with time and undergoes large deformations; the motion will be a known data. The work will be a first step towards building a complete thermoelectromagnetic-mechanical model suitable for simulating electrically assisted forming processes, which is the main motivation of the work. The electromagnetic model will be obtained from the time-harmonic eddy current problem with an inplane current; the source will be given in terms of currents or voltages defined at sorne parts of the boundary. Finite element methods based on a Lagrangian weak formulation will be used for the numerical solution. This approach will avoid the need to compute and remesh the thermo-electromagnetic domain along the time. The numerical tools will be implemented in FEniCS and validated by using a suitable test also solved in Eulerian coordinates.","sentences":["The aim of this work is to introduce a thermo-electromagnetic model for calculating the temperature and the power dissipated in cylindrical pieces whose geometry var\\'ies with time and undergoes large deformations; the motion will be a known data.","The work will be a first step towards building a complete thermoelectromagnetic-mechanical model suitable for simulating electrically assisted forming processes, which is the main motivation of the work.","The electromagnetic model will be obtained from the time-harmonic eddy current problem with an inplane current; the source will be given in terms of currents or voltages defined at sorne parts of the boundary.","Finite element methods based on a Lagrangian weak formulation will be used for the numerical solution.","This approach will avoid the need to compute and remesh the thermo-electromagnetic domain along the time.","The numerical tools will be implemented in FEniCS and validated by using a suitable test also solved in Eulerian coordinates."],"url":"http://arxiv.org/abs/2403.04377v1","category":"math.NA"}
{"created":"2024-03-07 09:40:54","title":"Upper bound of heat flux in an anelastic model for Rayleigh-B\u00e9nard convection","abstract":"Bounds on heat transfer have been the subject of previous studies concerning convection in the Boussinesq approximation: in the Rayleigh-B\\'enard configuration, the first result obtained by \\cite{howard63} states that $Nu < (3/64 \\ Ra)^{1/2}$ for large values of the Rayleigh number $Ra$, independently of the Prandtl number $Pr$. This is still the best known upper bound, only with the prefactor improved to $Nu < 1/6 \\ Ra^{1/2}$ by \\cite{DoeringConstantin96}. In the present paper, this result is extended to compressible convection. An upper bound is obtained for the anelastic liquid approximation, which is similar to the anelastic model used in astrophysics based on a turbulent diffusivity for entropy. The anelastic bound is still scaling as $Ra^{1/2}$, independently of $Pr$, but depends on the dissipation number $\\mathcal{D}$ and on the equation of state. For monatomic gases and large Rayleigh numbers, the bound is $Nu < 146\\, Ra^{\\frac{1}{2}} / (2-\\mathcal{D} )^{\\frac{5}{2}}$.","sentences":["Bounds on heat transfer have been the subject of previous studies concerning convection in the Boussinesq approximation: in the Rayleigh-B\\'enard configuration, the first result obtained by \\cite{howard63} states that $Nu < (3/64 \\ Ra)^{1/2}$ for large values of the Rayleigh number $Ra$, independently of the Prandtl number $Pr$.","This is still the best known upper bound, only with the prefactor improved to $Nu < 1/6 \\ Ra^{1/2}$ by \\cite{DoeringConstantin96}.","In the present paper, this result is extended to compressible convection.","An upper bound is obtained for the anelastic liquid approximation, which is similar to the anelastic model used in astrophysics based on a turbulent diffusivity for entropy.","The anelastic bound is still scaling as $Ra^{1/2}$, independently of $Pr$, but depends on the dissipation number $\\mathcal{D}$ and on the equation of state.","For monatomic gases and large Rayleigh numbers, the bound is $Nu < 146\\, Ra^{\\frac{1}{2}} / (2-\\mathcal{D} )^{\\frac{5}{2}}$."],"url":"http://arxiv.org/abs/2403.04358v1","category":"physics.flu-dyn"}
{"created":"2024-03-07 05:23:06","title":"Equivalence Testing: The Power of Bounded Adaptivity","abstract":"Equivalence testing, a fundamental problem in the field of distribution testing, seeks to infer if two unknown distributions on $[n]$ are the same or far apart in the total variation distance. Conditional sampling has emerged as a powerful query model and has been investigated by theoreticians and practitioners alike, leading to the design of optimal algorithms albeit in a sequential setting (also referred to as adaptive tester). Given the profound impact of parallel computing over the past decades, there has been a strong desire to design algorithms that enable high parallelization. Despite significant algorithmic advancements over the last decade, parallelizable techniques (also termed non-adaptive testers) have $\\tilde{O}(\\log^{12}n)$ query complexity, a prohibitively large complexity to be of practical usage. Therefore, the primary challenge is whether it is possible to design algorithms that enable high parallelization while achieving efficient query complexity.   Our work provides an affirmative answer to the aforementioned challenge: we present a highly parallelizable tester with a query complexity of $\\tilde{O}(\\log n)$, achieved through a single round of adaptivity, marking a significant stride towards harmonizing parallelizability and efficiency in equivalence testing.","sentences":["Equivalence testing, a fundamental problem in the field of distribution testing, seeks to infer if two unknown distributions on $[n]$ are the same or far apart in the total variation distance.","Conditional sampling has emerged as a powerful query model and has been investigated by theoreticians and practitioners alike, leading to the design of optimal algorithms albeit in a sequential setting (also referred to as adaptive tester).","Given the profound impact of parallel computing over the past decades, there has been a strong desire to design algorithms that enable high parallelization.","Despite significant algorithmic advancements over the last decade, parallelizable techniques (also termed non-adaptive testers) have $\\tilde{O}(\\log^{12}n)$ query complexity, a prohibitively large complexity to be of practical usage.","Therefore, the primary challenge is whether it is possible to design algorithms that enable high parallelization while achieving efficient query complexity.   ","Our work provides an affirmative answer to the aforementioned challenge: we present a highly parallelizable tester with a query complexity of $\\tilde{O}(\\log n)$, achieved through a single round of adaptivity, marking a significant stride towards harmonizing parallelizability and efficiency in equivalence testing."],"url":"http://arxiv.org/abs/2403.04230v1","category":"cs.DS"}
{"created":"2024-03-07 03:07:54","title":"SDPL: Shifting-Dense Partition Learning for UAV-View Geo-Localization","abstract":"Cross-view geo-localization aims to match images of the same target from different platforms, e.g., drone and satellite. It is a challenging task due to the changing both appearance of targets and environmental content from different views. Existing methods mainly focus on digging more comprehensive information through feature maps segmentation, while inevitably destroy the image structure and are sensitive to the shifting and scale of the target in the query. To address the above issues, we introduce a simple yet effective part-based representation learning, called shifting-dense partition learning (SDPL). Specifically, we propose the dense partition strategy (DPS), which divides the image into multiple parts to explore contextual-information while explicitly maintain the global structure. To handle scenarios with non-centered targets, we further propose the shifting-fusion strategy, which generates multiple sets of parts in parallel based on various segmentation centers and then adaptively fuses all features to select the best partitions. Extensive experiments show that our SDPL is robust to position shifting and scale variations, and achieves competitive performance on two prevailing benchmarks, i.e., University-1652 and SUES-200.","sentences":["Cross-view geo-localization aims to match images of the same target from different platforms, e.g., drone and satellite.","It is a challenging task due to the changing both appearance of targets and environmental content from different views.","Existing methods mainly focus on digging more comprehensive information through feature maps segmentation, while inevitably destroy the image structure and are sensitive to the shifting and scale of the target in the query.","To address the above issues, we introduce a simple yet effective part-based representation learning, called shifting-dense partition learning (SDPL).","Specifically, we propose the dense partition strategy (DPS), which divides the image into multiple parts to explore contextual-information while explicitly maintain the global structure.","To handle scenarios with non-centered targets, we further propose the shifting-fusion strategy, which generates multiple sets of parts in parallel based on various segmentation centers and then adaptively fuses all features to select the best partitions.","Extensive experiments show that our SDPL is robust to position shifting and scale variations, and achieves competitive performance on two prevailing benchmarks, i.e., University-1652 and SUES-200."],"url":"http://arxiv.org/abs/2403.04172v1","category":"cs.CV"}
{"created":"2024-03-07 02:22:26","title":"Designing Social Robots that Engage Older Adults in Exercise: A Case Study","abstract":"We present and evaluate a prototype social robot to encourage daily exercise among older adults in a home setting. Our prototype system, designed to lead users through exercise sessions with motivational feedback, was assessed through a case study with a 78-year-old participant for one week. Our case study highlighted preferences for greater user control over exercise choices and questioned the necessity of precise motion tracking. Feedback also indicated a desire for more varied exercises and suggested improvements in user engagement techniques. The insights suggest that further research is needed to enhance system adaptability and effectiveness to better promote daily exercise. Future efforts will aim to refine the prototype based on participant feedback and extend the evaluation to broader in-home deployments.","sentences":["We present and evaluate a prototype social robot to encourage daily exercise among older adults in a home setting.","Our prototype system, designed to lead users through exercise sessions with motivational feedback, was assessed through a case study with a 78-year-old participant for one week.","Our case study highlighted preferences for greater user control over exercise choices and questioned the necessity of precise motion tracking.","Feedback also indicated a desire for more varied exercises and suggested improvements in user engagement techniques.","The insights suggest that further research is needed to enhance system adaptability and effectiveness to better promote daily exercise.","Future efforts will aim to refine the prototype based on participant feedback and extend the evaluation to broader in-home deployments."],"url":"http://arxiv.org/abs/2403.04153v1","category":"cs.HC"}
{"created":"2024-03-07 02:17:59","title":"Dual-path Frequency Discriminators for Few-shot Anomaly Detection","abstract":"Few-shot anomaly detection (FSAD) is essential in industrial manufacturing. However, existing FSAD methods struggle to effectively leverage a limited number of normal samples, and they may fail to detect and locate inconspicuous anomalies in the spatial domain. We further discover that these subtle anomalies would be more noticeable in the frequency domain. In this paper, we propose a Dual-Path Frequency Discriminators (DFD) network from a frequency perspective to tackle these issues. Specifically, we generate anomalies at both image-level and feature-level. Differential frequency components are extracted by the multi-frequency information construction module and supplied into the fine-grained feature construction module to provide adapted features. We consider anomaly detection as a discriminative classification problem, wherefore the dual-path feature discrimination module is employed to detect and locate the image-level and feature-level anomalies in the feature space. The discriminators aim to learn a joint representation of anomalous features and normal features in the latent space. Extensive experiments conducted on MVTec AD and VisA benchmarks demonstrate that our DFD surpasses current state-of-the-art methods. Source code will be available.","sentences":["Few-shot anomaly detection (FSAD) is essential in industrial manufacturing.","However, existing FSAD methods struggle to effectively leverage a limited number of normal samples, and they may fail to detect and locate inconspicuous anomalies in the spatial domain.","We further discover that these subtle anomalies would be more noticeable in the frequency domain.","In this paper, we propose a Dual-Path Frequency Discriminators (DFD) network from a frequency perspective to tackle these issues.","Specifically, we generate anomalies at both image-level and feature-level.","Differential frequency components are extracted by the multi-frequency information construction module and supplied into the fine-grained feature construction module to provide adapted features.","We consider anomaly detection as a discriminative classification problem, wherefore the dual-path feature discrimination module is employed to detect and locate the image-level and feature-level anomalies in the feature space.","The discriminators aim to learn a joint representation of anomalous features and normal features in the latent space.","Extensive experiments conducted on MVTec AD and VisA benchmarks demonstrate that our DFD surpasses current state-of-the-art methods.","Source code will be available."],"url":"http://arxiv.org/abs/2403.04151v1","category":"cs.CV"}
{"created":"2024-03-07 01:50:36","title":"FedClust: Optimizing Federated Learning on Non-IID Data through Weight-Driven Client Clustering","abstract":"Federated learning (FL) is an emerging distributed machine learning paradigm enabling collaborative model training on decentralized devices without exposing their local data. A key challenge in FL is the uneven data distribution across client devices, violating the well-known assumption of independent-and-identically-distributed (IID) training samples in conventional machine learning. Clustered federated learning (CFL) addresses this challenge by grouping clients based on the similarity of their data distributions. However, existing CFL approaches require a large number of communication rounds for stable cluster formation and rely on a predefined number of clusters, thus limiting their flexibility and adaptability. This paper proposes FedClust, a novel CFL approach leveraging correlations between local model weights and client data distributions. FedClust groups clients into clusters in a one-shot manner using strategically selected partial model weights and dynamically accommodates newcomers in real-time. Experimental results demonstrate FedClust outperforms baseline approaches in terms of accuracy and communication costs.","sentences":["Federated learning (FL) is an emerging distributed machine learning paradigm enabling collaborative model training on decentralized devices without exposing their local data.","A key challenge in FL is the uneven data distribution across client devices, violating the well-known assumption of independent-and-identically-distributed (IID) training samples in conventional machine learning.","Clustered federated learning (CFL) addresses this challenge by grouping clients based on the similarity of their data distributions.","However, existing CFL approaches require a large number of communication rounds for stable cluster formation and rely on a predefined number of clusters, thus limiting their flexibility and adaptability.","This paper proposes FedClust, a novel CFL approach leveraging correlations between local model weights and client data distributions.","FedClust groups clients into clusters in a one-shot manner using strategically selected partial model weights and dynamically accommodates newcomers in real-time.","Experimental results demonstrate FedClust outperforms baseline approaches in terms of accuracy and communication costs."],"url":"http://arxiv.org/abs/2403.04144v1","category":"cs.DC"}
{"created":"2024-03-07 01:26:43","title":"An Adaptable, Safe, and Portable Robot-Assisted Feeding System","abstract":"We demonstrate a robot-assisted feeding system that enables people with mobility impairments to feed themselves. Our system design embodies Safety, Portability, and User Control, with comprehensive full-stack safety checks, the ability to be mounted on and powered by any powered wheelchair, and a custom web-app allowing care-recipients to leverage their own assistive devices for robot control. For bite acquisition, we leverage multi-modal online learning to tractably adapt to unseen food types. For bite transfer, we leverage real-time mouth perception and interaction-aware control. Co-designed with community researchers, our system has been validated through multiple end-user studies.","sentences":["We demonstrate a robot-assisted feeding system that enables people with mobility impairments to feed themselves.","Our system design embodies Safety, Portability, and User Control, with comprehensive full-stack safety checks, the ability to be mounted on and powered by any powered wheelchair, and a custom web-app allowing care-recipients to leverage their own assistive devices for robot control.","For bite acquisition, we leverage multi-modal online learning to tractably adapt to unseen food types.","For bite transfer, we leverage real-time mouth perception and interaction-aware control.","Co-designed with community researchers, our system has been validated through multiple end-user studies."],"url":"http://arxiv.org/abs/2403.04134v1","category":"cs.RO"}
{"created":"2024-03-06 23:43:51","title":"Using Causal Trees to Estimate Personalized Task Difficulty in Post-Stroke Individuals","abstract":"Adaptive training programs are crucial for recovery post stroke. However, developing programs that automatically adapt depends on quantifying how difficult a task is for a specific individual at a particular stage of their recovery. In this work, we propose a method that automatically generates regions of different task difficulty levels based on an individual's performance. We show that this technique explains the variance in user performance for a reaching task better than previous approaches to estimating task difficulty.","sentences":["Adaptive training programs are crucial for recovery post stroke.","However, developing programs that automatically adapt depends on quantifying how difficult a task is for a specific individual at a particular stage of their recovery.","In this work, we propose a method that automatically generates regions of different task difficulty levels based on an individual's performance.","We show that this technique explains the variance in user performance for a reaching task better than previous approaches to estimating task difficulty."],"url":"http://arxiv.org/abs/2403.04109v1","category":"cs.RO"}
{"created":"2024-03-06 22:52:15","title":"Assisting International Migrants with Everyday Information Seeking: From the Providers' Lens","abstract":"International migrants face difficulties obtaining information for a quality life and well-being in the host country. Prior research indicates that international migrants often seek information from their co-national cohort or contacts from the same country. The downside of this practice, however, is that people can end up clustering in a small-world environment, hindering the information seekers' social adaptation in the long run. In the current research, we investigated the ongoing practices and future opportunities to connect international migrants with others beyond their co-national contacts. Our work zooms in on the providers' perspectives, which complements previous studies that pay exclusive attention to the information seekers. Specifically, we conducted in-depth interviews with 21 participants assisting the needs of informational migrants in the United States. Some of these people are fellow migrants from a different home country than the information seeker, whereas the rest are domestic residents. Our data revealed how these participants dealt with language barriers, overcame knowledge disparities, and calibrated their effort commitment as information providers. Based on these findings, we discuss directions for future information and communication technologies (ICT) design that can facilitate international migrants' daily information seeking by accounting for the provider's needs and concerns.","sentences":["International migrants face difficulties obtaining information for a quality life and well-being in the host country.","Prior research indicates that international migrants often seek information from their co-national cohort or contacts from the same country.","The downside of this practice, however, is that people can end up clustering in a small-world environment, hindering the information seekers' social adaptation in the long run.","In the current research, we investigated the ongoing practices and future opportunities to connect international migrants with others beyond their co-national contacts.","Our work zooms in on the providers' perspectives, which complements previous studies that pay exclusive attention to the information seekers.","Specifically, we conducted in-depth interviews with 21 participants assisting the needs of informational migrants in the United States.","Some of these people are fellow migrants from a different home country than the information seeker, whereas the rest are domestic residents.","Our data revealed how these participants dealt with language barriers, overcame knowledge disparities, and calibrated their effort commitment as information providers.","Based on these findings, we discuss directions for future information and communication technologies (ICT) design that can facilitate international migrants' daily information seeking by accounting for the provider's needs and concerns."],"url":"http://arxiv.org/abs/2403.04096v1","category":"cs.HC"}
{"created":"2024-03-06 22:24:05","title":"Directional Smoothness and Gradient Methods: Convergence and Adaptivity","abstract":"We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization, rather than on global, worst-case constants. Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective. Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes. For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness. Experiments on logistic regression show our convergence guarantees are tighter than the classical theory based on L-smoothness.","sentences":["We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization, rather than on global, worst-case constants.","Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective.","Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes.","For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness.","Experiments on logistic regression show our convergence guarantees are tighter than the classical theory based on L-smoothness."],"url":"http://arxiv.org/abs/2403.04081v1","category":"cs.LG"}
{"created":"2024-03-06 21:14:49","title":"Power efficiency of Hall-like devices: comparison between reciprocal and anti-reciprocal Onsager relations","abstract":"Two well-known Hall-like effects are occurring in ferromagnets: the Anomalous Hall effect and the Planar Hall effect. The former is analogous to the classical Hall effect and is defined by the Onsager reciprocity relation of the second kind (antisymmetric conductivity matrix), while the latter is defined by the Onsager reciprocity relation of the first kind (symmetric conductivity matrix). The difference is fundamental, as it is based on time-invariance symmetry breaking at the microscopic scale. We study the Hall current generated in both cases, together with the power that can be extracted from the edges of Hall device. The expressions of the distribution of the electric currents, the distribution of electric carriers, and the power efficiencies (i.e. the power that can be injected into a load circuit) are derived at stationary regime from a variational method based on the second law of thermodynamics. It is shown that the distribution of the transverse Hall-current is identical in both cases but the longitudinal current and the power dissipated differ at the second order in the Hall angle.","sentences":["Two well-known Hall-like effects are occurring in ferromagnets: the Anomalous Hall effect and the Planar Hall effect.","The former is analogous to the classical Hall effect and is defined by the Onsager reciprocity relation of the second kind (antisymmetric conductivity matrix), while the latter is defined by the Onsager reciprocity relation of the first kind (symmetric conductivity matrix).","The difference is fundamental, as it is based on time-invariance symmetry breaking at the microscopic scale.","We study the Hall current generated in both cases, together with the power that can be extracted from the edges of Hall device.","The expressions of the distribution of the electric currents, the distribution of electric carriers, and the power efficiencies (i.e. the power that can be injected into a load circuit) are derived at stationary regime from a variational method based on the second law of thermodynamics.","It is shown that the distribution of the transverse Hall-current is identical in both cases but the longitudinal current and the power dissipated differ at the second order in the Hall angle."],"url":"http://arxiv.org/abs/2403.04059v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-06 21:08:54","title":"To Spend or to Gain: Online Learning in Repeated Karma Auctions","abstract":"Recent years have seen a surge of artificial currency-based mechanisms in contexts where monetary instruments are deemed unfair or inappropriate, e.g., in allocating food donations to food banks, course seats to students, and, more recently, even for traffic congestion management. Yet the applicability of these mechanisms remains limited in repeated auction settings, as it is challenging for users to learn how to bid an artificial currency that has no value outside the auctions. Indeed, users must jointly learn the value of the currency in addition to how to spend it optimally. In this work, we study the problem of learning to bid in two prominent classes of artificial currency auctions: those in which currency, which users spend to obtain public resources, is only issued at the beginning of a finite period; and those where, in addition to the initial currency endowment, currency payments are redistributed to users at each time step. In the latter class, the currency has been referred to as karma, since users do not only spend karma to obtain public resources but also gain karma for yielding them. In both classes, we propose a simple learning strategy, called adaptive karma pacing, and show that this strategy a) is asymptotically optimal for a single user bidding against competing bids drawn from a stationary distribution; b) leads to convergent learning dynamics when all users adopt it; and c) constitutes an approximate Nash equilibrium as the number of users grows. Our results require a novel analysis in comparison to adaptive pacing strategies in monetary auctions, since we depart from the classical assumption that the currency has known value outside the auctions, and moreover consider that the currency is both spent and gained in the class of auctions with redistribution.","sentences":["Recent years have seen a surge of artificial currency-based mechanisms in contexts where monetary instruments are deemed unfair or inappropriate, e.g., in allocating food donations to food banks, course seats to students, and, more recently, even for traffic congestion management.","Yet the applicability of these mechanisms remains limited in repeated auction settings, as it is challenging for users to learn how to bid an artificial currency that has no value outside the auctions.","Indeed, users must jointly learn the value of the currency in addition to how to spend it optimally.","In this work, we study the problem of learning to bid in two prominent classes of artificial currency auctions: those in which currency, which users spend to obtain public resources, is only issued at the beginning of a finite period; and those where, in addition to the initial currency endowment, currency payments are redistributed to users at each time step.","In the latter class, the currency has been referred to as karma, since users do not only spend karma to obtain public resources but also gain karma for yielding them.","In both classes, we propose a simple learning strategy, called adaptive karma pacing, and show that this strategy a) is asymptotically optimal for a single user bidding against competing bids drawn from a stationary distribution; b) leads to convergent learning dynamics when all users adopt it; and c) constitutes an approximate Nash equilibrium as the number of users grows.","Our results require a novel analysis in comparison to adaptive pacing strategies in monetary auctions, since we depart from the classical assumption that the currency has known value outside the auctions, and moreover consider that the currency is both spent and gained in the class of auctions with redistribution."],"url":"http://arxiv.org/abs/2403.04057v1","category":"cs.GT"}
{"created":"2024-03-06 20:44:09","title":"A Swarm Coherence Mechanism for Jellyfish","abstract":"We present a theory of jellyfish swarm formation and exemplify it with simulations of active Brownian particles. The motivation for our analysis is the phenomenon of jellyfish blooms in the ocean and clustering of jellyfish in tank experiments. We argue that such clusters emerge due to an externally induced phase transition of jellyfish density, such as convergent flows, which is then maintained and amplified by self-induced stimuli. Our study introduces three mechanisms relevant for a better understanding of jellyfish blooming that have not been taken into account before which are a signaling tracer, jellyfish-wall interaction and ignorance of external stimuli. Our results agree with the biological fact that jellyfish exhibit an extreme sensitivity to stimuli in order to achieve favorable aggregations. Based on our theoretical framework, we are able to provide a clear terminology for future experimental analysis of jellyfish swarming and we pinpoint potential limitations of tank experiments.","sentences":["We present a theory of jellyfish swarm formation and exemplify it with simulations of active Brownian particles.","The motivation for our analysis is the phenomenon of jellyfish blooms in the ocean and clustering of jellyfish in tank experiments.","We argue that such clusters emerge due to an externally induced phase transition of jellyfish density, such as convergent flows, which is then maintained and amplified by self-induced stimuli.","Our study introduces three mechanisms relevant for a better understanding of jellyfish blooming that have not been taken into account before which are a signaling tracer, jellyfish-wall interaction and ignorance of external stimuli.","Our results agree with the biological fact that jellyfish exhibit an extreme sensitivity to stimuli in order to achieve favorable aggregations.","Based on our theoretical framework, we are able to provide a clear terminology for future experimental analysis of jellyfish swarming and we pinpoint potential limitations of tank experiments."],"url":"http://arxiv.org/abs/2403.04044v1","category":"nlin.AO"}
{"created":"2024-03-06 19:40:49","title":"Human I/O: Towards a Unified Approach to Detecting Situational Impairments","abstract":"Situationally Induced Impairments and Disabilities (SIIDs) can significantly hinder user experience in contexts such as poor lighting, noise, and multi-tasking. While prior research has introduced algorithms and systems to address these impairments, they predominantly cater to specific tasks or environments and fail to accommodate the diverse and dynamic nature of SIIDs. We introduce Human I/O, a unified approach to detecting a wide range of SIIDs by gauging the availability of human input/output channels. Leveraging egocentric vision, multimodal sensing and reasoning with large language models, Human I/O achieves a 0.22 mean absolute error and a 82% accuracy in availability prediction across 60 in-the-wild egocentric video recordings in 32 different scenarios. Furthermore, while the core focus of our work is on the detection of SIIDs rather than the creation of adaptive user interfaces, we showcase the efficacy of our prototype via a user study with 10 participants. Findings suggest that Human I/O significantly reduces effort and improves user experience in the presence of SIIDs, paving the way for more adaptive and accessible interactive systems in the future.","sentences":["Situationally Induced Impairments and Disabilities (SIIDs) can significantly hinder user experience in contexts such as poor lighting, noise, and multi-tasking.","While prior research has introduced algorithms and systems to address these impairments, they predominantly cater to specific tasks or environments and fail to accommodate the diverse and dynamic nature of SIIDs.","We introduce Human I/O, a unified approach to detecting a wide range of SIIDs by gauging the availability of human input/output channels.","Leveraging egocentric vision, multimodal sensing and reasoning with large language models, Human I/O achieves a 0.22 mean absolute error and a 82% accuracy in availability prediction across 60 in-the-wild egocentric video recordings in 32 different scenarios.","Furthermore, while the core focus of our work is on the detection of SIIDs rather than the creation of adaptive user interfaces, we showcase the efficacy of our prototype via a user study with 10 participants.","Findings suggest that Human I/O significantly reduces effort and improves user experience in the presence of SIIDs, paving the way for more adaptive and accessible interactive systems in the future."],"url":"http://arxiv.org/abs/2403.04008v1","category":"cs.HC"}
{"created":"2024-03-06 19:05:53","title":"Treespilation: Architecture- and State-Optimised Fermion-to-Qubit Mappings","abstract":"Quantum computers hold great promise for efficiently simulating Fermionic systems, benefiting fields like quantum chemistry and materials science. To achieve this, algorithms typically begin by choosing a Fermion-to-qubit mapping to encode the Fermioinc problem in the qubits of a quantum computer. In this work, we introduce \"treespilation,\" a technique for efficiently mapping Fermionic systems using a large family of favourable tree-based mappings previously introduced by some of us. We use this technique to minimise the number of CNOT gates required to simulate chemical groundstates found numerically using the ADAPT-VQE algorithm. We observe significant reductions, up to $74\\%$, in CNOT counts on full connectivity. limited qubit connectivity-type devices such as IBM Eagle and Google Sycamore, we observe similar reductions in CNOT counts. In fact, in many instances, the reductions achieved on these limited connectivity devices even surpass the initial full connectivity CNOT count. Additionally, we find our method improves the CNOT and parameter efficiency of QEB- and qubit-ADAPT-VQE protocols, which are, to our knowledge, the most CNOT-efficient VQE protocols for molecular state preparation.","sentences":["Quantum computers hold great promise for efficiently simulating Fermionic systems, benefiting fields like quantum chemistry and materials science.","To achieve this, algorithms typically begin by choosing a Fermion-to-qubit mapping to encode the Fermioinc problem in the qubits of a quantum computer.","In this work, we introduce \"treespilation,\" a technique for efficiently mapping Fermionic systems using a large family of favourable tree-based mappings previously introduced by some of us.","We use this technique to minimise the number of CNOT gates required to simulate chemical groundstates found numerically using the ADAPT-VQE algorithm.","We observe significant reductions, up to $74\\%$, in CNOT counts on full connectivity.","limited qubit connectivity-type devices such as IBM Eagle and Google Sycamore, we observe similar reductions in CNOT counts.","In fact, in many instances, the reductions achieved on these limited connectivity devices even surpass the initial full connectivity CNOT count.","Additionally, we find our method improves the CNOT and parameter efficiency of QEB- and qubit-ADAPT-VQE protocols, which are, to our knowledge, the most CNOT-efficient VQE protocols for molecular state preparation."],"url":"http://arxiv.org/abs/2403.03992v1","category":"quant-ph"}
{"created":"2024-03-07 18:49:29","title":"Speech Emotion Recognition Via CNN-Transforemr and Multidimensional Attention Mechanism","abstract":"Speech Emotion Recognition (SER) is crucial in human-machine interactions. Mainstream approaches utilize Convolutional Neural Networks or Recurrent Neural Networks to learn local energy feature representations of speech segments from speech information, but struggle with capturing global information such as the duration of energy in speech. Some use Transformers to capture global information, but there is room for improvement in terms of parameter count and performance. Furthermore, existing attention mechanisms focus on spatial or channel dimensions, hindering learning of important temporal information in speech. In this paper, to model local and global information at different levels of granularity in speech and capture temporal, spatial and channel dependencies in speech signals, we propose a Speech Emotion Recognition network based on CNN-Transformer and multi-dimensional attention mechanisms. Specifically, a stack of CNN blocks is dedicated to capturing local information in speech from a time-frequency perspective. In addition, a time-channel-space attention mechanism is used to enhance features across three dimensions. Moreover, we model local and global dependencies of feature sequences using large convolutional kernels with depthwise separable convolutions and lightweight Transformer modules. We evaluate the proposed method on IEMOCAP and Emo-DB datasets and show our approach significantly improves the performance over the state-of-the-art methods. Our code is available on https://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism","sentences":["Speech Emotion Recognition (SER) is crucial in human-machine interactions.","Mainstream approaches utilize Convolutional Neural Networks or Recurrent Neural Networks to learn local energy feature representations of speech segments from speech information, but struggle with capturing global information such as the duration of energy in speech.","Some use Transformers to capture global information, but there is room for improvement in terms of parameter count and performance.","Furthermore, existing attention mechanisms focus on spatial or channel dimensions, hindering learning of important temporal information in speech.","In this paper, to model local and global information at different levels of granularity in speech and capture temporal, spatial and channel dependencies in speech signals, we propose a Speech Emotion Recognition network based on CNN-Transformer and multi-dimensional attention mechanisms.","Specifically, a stack of CNN blocks is dedicated to capturing local information in speech from a time-frequency perspective.","In addition, a time-channel-space attention mechanism is used to enhance features across three dimensions.","Moreover, we model local and global dependencies of feature sequences using large convolutional kernels with depthwise separable convolutions and lightweight Transformer modules.","We evaluate the proposed method on IEMOCAP and Emo-DB datasets and show our approach significantly improves the performance over the state-of-the-art methods.","Our code is available on https://github.com/SCNU-RISLAB/CNN-Transforemr-and-Multidimensional-Attention-Mechanism"],"url":"http://arxiv.org/abs/2403.04743v1","category":"eess.AS"}
{"created":"2024-03-07 16:21:09","title":"Entropy Aware Message Passing in Graph Neural Networks","abstract":"Deep Graph Neural Networks struggle with oversmoothing. This paper introduces a novel, physics-inspired GNN model designed to mitigate this issue. Our approach integrates with existing GNN architectures, introducing an entropy-aware message passing term. This term performs gradient ascent on the entropy during node aggregation, thereby preserving a certain degree of entropy in the embeddings. We conduct a comparative analysis of our model against state-of-the-art GNNs across various common datasets.","sentences":["Deep Graph Neural Networks struggle with oversmoothing.","This paper introduces a novel, physics-inspired GNN model designed to mitigate this issue.","Our approach integrates with existing GNN architectures, introducing an entropy-aware message passing term.","This term performs gradient ascent on the entropy during node aggregation, thereby preserving a certain degree of entropy in the embeddings.","We conduct a comparative analysis of our model against state-of-the-art GNNs across various common datasets."],"url":"http://arxiv.org/abs/2403.04636v1","category":"cs.LG"}
{"created":"2024-03-07 16:16:23","title":"Software Compensation for Highly Granular Calorimeters using Machine Learning","abstract":"A neural network for software compensation was developed for the highly granular CALICE Analogue Hadronic Calorimeter (AHCAL). The neural network uses spatial and temporal event information from the AHCAL and energy information, which is expected to improve sensitivity to shower development and the neutron fraction of the hadron shower. The neural network method produced a depth-dependent energy weighting and a time-dependent threshold for enhancing energy deposits consistent with the timescale of evaporation neutrons. Additionally, it was observed to learn an energy-weighting indicative of longitudinal leakage correction. In addition, the method produced a linear detector response and outperformed a published control method regarding resolution for every particle energy studied.","sentences":["A neural network for software compensation was developed for the highly granular CALICE Analogue Hadronic Calorimeter (AHCAL).","The neural network uses spatial and temporal event information from the AHCAL and energy information, which is expected to improve sensitivity to shower development and the neutron fraction of the hadron shower.","The neural network method produced a depth-dependent energy weighting and a time-dependent threshold for enhancing energy deposits consistent with the timescale of evaporation neutrons.","Additionally, it was observed to learn an energy-weighting indicative of longitudinal leakage correction.","In addition, the method produced a linear detector response and outperformed a published control method regarding resolution for every particle energy studied."],"url":"http://arxiv.org/abs/2403.04632v1","category":"physics.ins-det"}
{"created":"2024-03-07 15:54:46","title":"In-n-Out: Calibrating Graph Neural Networks for Link Prediction","abstract":"Deep neural networks are notoriously miscalibrated, i.e., their outputs do not reflect the true probability of the event we aim to predict. While networks for tabular or image data are usually overconfident, recent works have shown that graph neural networks (GNNs) show the opposite behavior for node-level classification. But what happens when we are predicting links? We show that, in this case, GNNs often exhibit a mixed behavior. More specifically, they may be overconfident in negative predictions while being underconfident in positive ones. Based on this observation, we propose IN-N-OUT, the first-ever method to calibrate GNNs for link prediction. IN-N-OUT is based on two simple intuitions: i) attributing true/false labels to an edge while respecting a GNNs prediction should cause but small fluctuations in that edge's embedding; and, conversely, ii) if we label that same edge contradicting our GNN, embeddings should change more substantially. An extensive experimental campaign shows that IN-N-OUT significantly improves the calibration of GNNs in link prediction, consistently outperforming the baselines available -- which are not designed for this specific task.","sentences":["Deep neural networks are notoriously miscalibrated, i.e., their outputs do not reflect the true probability of the event we aim to predict.","While networks for tabular or image data are usually overconfident, recent works have shown that graph neural networks (GNNs) show the opposite behavior for node-level classification.","But what happens when we are predicting links?","We show that, in this case, GNNs often exhibit a mixed behavior.","More specifically, they may be overconfident in negative predictions while being underconfident in positive ones.","Based on this observation, we propose IN-N-OUT, the first-ever method to calibrate GNNs for link prediction.","IN-N-OUT is based on two simple intuitions: i) attributing true/false labels to an edge while respecting a GNNs prediction should cause but small fluctuations in that edge's embedding; and, conversely, ii) if we label that same edge contradicting our GNN, embeddings should change more substantially.","An extensive experimental campaign shows that IN-N-OUT significantly improves the calibration of GNNs in link prediction, consistently outperforming the baselines available -- which are not designed for this specific task."],"url":"http://arxiv.org/abs/2403.04605v1","category":"cs.LG"}
{"created":"2024-03-07 15:52:32","title":"Minimum-Time Planar Paths with up to Two Constant Acceleration Inputs and $L_2$ Velocity and Acceleration Constraints","abstract":"Given starting and ending positions and velocities, $L_2$ bounds on the acceleration and velocity, and the restriction to no more than two constant control inputs, this paper provides routines to compute the minimal-time path. Closed form solutions are provided for reaching a position in minimum time with and without a velocity bound, and for stopping at the goal position.   A numeric solver is used to reach a goal position and velocity with no more than two constant control inputs. If a cruising phase at the terminal velocity is needed, this requires solving a non-linear equation with a single parameter. Code is provided on GitHub at https://github.com/RoboticSwarmControl/MinTimeL2pathsConstraints.","sentences":["Given starting and ending positions and velocities, $L_2$ bounds on the acceleration and velocity, and the restriction to no more than two constant control inputs, this paper provides routines to compute the minimal-time path.","Closed form solutions are provided for reaching a position in minimum time with and without a velocity bound, and for stopping at the goal position.   ","A numeric solver is used to reach a goal position and velocity with no more than two constant control inputs.","If a cruising phase at the terminal velocity is needed, this requires solving a non-linear equation with a single parameter.","Code is provided on GitHub at https://github.com/RoboticSwarmControl/MinTimeL2pathsConstraints."],"url":"http://arxiv.org/abs/2403.04602v1","category":"cs.RO"}
{"created":"2024-03-07 15:42:05","title":"Free boundary CMC annuli in spherical and hyperbolic balls","abstract":"We construct, for any $H\\in \\mathbb{R}$, infinitely many free boundary annuli in geodesic balls of $\\mathbb{S}^3$ with constant mean curvature $H$ and a discrete, non-rotational, symmetry group. Some of these free boundary CMC annuli are actually embedded if $H\\geq 1/\\sqrt{3}$. We also construct embedded, non-rotational, free boundary CMC annuli in geodesic balls of $\\mathbb{H}^3$, for all values $H>1$ of the mean curvature $H$.","sentences":["We construct, for any $H\\in \\mathbb{R}$, infinitely many free boundary annuli in geodesic balls of $\\mathbb{S}^3$ with constant mean curvature $H$ and a discrete, non-rotational, symmetry group.","Some of these free boundary CMC annuli are actually embedded if $H\\geq 1/\\sqrt{3}$. We also construct embedded, non-rotational, free boundary CMC annuli in geodesic balls of $\\mathbb{H}^3$, for all values $H>1$ of the mean curvature $H$."],"url":"http://arxiv.org/abs/2403.04595v1","category":"math.DG"}
{"created":"2024-03-07 14:36:18","title":"Chirped pulse control over the melting of superconductors","abstract":"Strong field terahertz pulses are increasingly used to excite and control quantum materials at the ultrafast timescale. They have found widespread application by enabling direct addressing of the superconducting gap or Josephson resonances and are essential in Higgs spectroscopy. Large non-linear optical signals can be induced by the strong coupling of the THz and superconducting degrees of freedom. However, far less attention has been paid to the strong bi-directional coupling between field and material this implies. Here, we use the framework of the time-dependent Ginzburg-Landau equations to study the full field and material evolution of a superconductor driven by strong field terahertz pulses. We find that at high field strengths, the backreaction of the superconductor induces large changes to the driving pulse, which in turn leads to a runaway melting of the superconducting condensate. This results in a surprisingly large sensitivity to the initial driving pulse chirp, enabling these purely dynamical changes to result in order of magnitude different levels of melting. We also find large-scale spectral shifting of the driving pulse to occur in just a few hundred nanometers of propagation through a superconductor. We attribute these effects to an inverse plasma redshift, in which the driving field breaks Cooper pairs and decreases the free-electron mobility, analogous to reducing the density of a plasma.","sentences":["Strong field terahertz pulses are increasingly used to excite and control quantum materials at the ultrafast timescale.","They have found widespread application by enabling direct addressing of the superconducting gap or Josephson resonances and are essential in Higgs spectroscopy.","Large non-linear optical signals can be induced by the strong coupling of the THz and superconducting degrees of freedom.","However, far less attention has been paid to the strong bi-directional coupling between field and material this implies.","Here, we use the framework of the time-dependent Ginzburg-Landau equations to study the full field and material evolution of a superconductor driven by strong field terahertz pulses.","We find that at high field strengths, the backreaction of the superconductor induces large changes to the driving pulse, which in turn leads to a runaway melting of the superconducting condensate.","This results in a surprisingly large sensitivity to the initial driving pulse chirp, enabling these purely dynamical changes to result in order of magnitude different levels of melting.","We also find large-scale spectral shifting of the driving pulse to occur in just a few hundred nanometers of propagation through a superconductor.","We attribute these effects to an inverse plasma redshift, in which the driving field breaks Cooper pairs and decreases the free-electron mobility, analogous to reducing the density of a plasma."],"url":"http://arxiv.org/abs/2403.04538v1","category":"cond-mat.supr-con"}
{"created":"2024-03-07 14:27:59","title":"Resurgent Wilson loops in refined topological string","abstract":"We study the resurgent structures of Wilson loops in refined topological string theory. We argue that the Borel singularities should be integral periods, and that the associated Stokes constants are refined Donaldson-Thomas invariants, just like the free energies, except that the Borel singularities cannot be local flat coordinates. We also solve the non-perturbative series in closed form from the holomorphic anomaly equations for the refined Wilson loops. We illustrate these results with the examples of local P^2 and local P^1 x P^1.","sentences":["We study the resurgent structures of Wilson loops in refined topological string theory.","We argue that the Borel singularities should be integral periods, and that the associated Stokes constants are refined Donaldson-Thomas invariants, just like the free energies, except that the Borel singularities cannot be local flat coordinates.","We also solve the non-perturbative series in closed form from the holomorphic anomaly equations for the refined Wilson loops.","We illustrate these results with the examples of local P^2 and local P^1 x P^1."],"url":"http://arxiv.org/abs/2403.04528v1","category":"hep-th"}
{"created":"2024-03-07 14:17:09","title":"A finite element contour integral method for computing the resonances of metallic grating structures with subwavelength holes","abstract":"We consider the numerical computation of resonances for metallic grating structures with dispersive media and small slit holes. The underlying eigenvalue problem is nonlinear and the mathematical model is multiscale due to the existence of several length scales in problem geometry and material contrast. We discretize the partial differential equation model over the truncated domain using the finite element method and develop a multi-step contour integral eigensolver to compute the resonances. The eigensolver first locates eigenvalues using a spectral indicator and then computes eigenvalues by a subspace projection scheme. The proposed numerical method is robust and scalable, and does not require initial guess as the iteration methods. Numerical examples are presented to demonstrate its effectiveness.","sentences":["We consider the numerical computation of resonances for metallic grating structures with dispersive media and small slit holes.","The underlying eigenvalue problem is nonlinear and the mathematical model is multiscale due to the existence of several length scales in problem geometry and material contrast.","We discretize the partial differential equation model over the truncated domain using the finite element method and develop a multi-step contour integral eigensolver to compute the resonances.","The eigensolver first locates eigenvalues using a spectral indicator and then computes eigenvalues by a subspace projection scheme.","The proposed numerical method is robust and scalable, and does not require initial guess as the iteration methods.","Numerical examples are presented to demonstrate its effectiveness."],"url":"http://arxiv.org/abs/2403.04514v1","category":"math.NA"}
{"created":"2024-03-07 14:08:01","title":"Finding Waldo: Towards Efficient Exploration of NeRF Scene Space","abstract":"Neural Radiance Fields (NeRF) have quickly become the primary approach for 3D reconstruction and novel view synthesis in recent years due to their remarkable performance. Despite the huge interest in NeRF methods, a practical use case of NeRFs has largely been ignored; the exploration of the scene space modelled by a NeRF. In this paper, for the first time in the literature, we propose and formally define the scene exploration framework as the efficient discovery of NeRF model inputs (i.e. coordinates and viewing angles), using which one can render novel views that adhere to user-selected criteria. To remedy the lack of approaches addressing scene exploration, we first propose two baseline methods called Guided-Random Search (GRS) and Pose Interpolation-based Search (PIBS). We then cast scene exploration as an optimization problem, and propose the criteria-agnostic Evolution-Guided Pose Search (EGPS) for efficient exploration. We test all three approaches with various criteria (e.g. saliency maximization, image quality maximization, photo-composition quality improvement) and show that our EGPS performs more favourably than other baselines. We finally highlight key points and limitations, and outline directions for future research in scene exploration.","sentences":["Neural Radiance Fields (NeRF) have quickly become the primary approach for 3D reconstruction and novel view synthesis in recent years due to their remarkable performance.","Despite the huge interest in NeRF methods, a practical use case of NeRFs has largely been ignored; the exploration of the scene space modelled by a NeRF.","In this paper, for the first time in the literature, we propose and formally define the scene exploration framework as the efficient discovery of NeRF model inputs (i.e. coordinates and viewing angles), using which one can render novel views that adhere to user-selected criteria.","To remedy the lack of approaches addressing scene exploration, we first propose two baseline methods called Guided-Random Search (GRS) and Pose Interpolation-based Search (PIBS).","We then cast scene exploration as an optimization problem, and propose the criteria-agnostic Evolution-Guided Pose Search (EGPS) for efficient exploration.","We test all three approaches with various criteria (e.g. saliency maximization, image quality maximization, photo-composition quality improvement) and show that our EGPS performs more favourably than other baselines.","We finally highlight key points and limitations, and outline directions for future research in scene exploration."],"url":"http://arxiv.org/abs/2403.04508v1","category":"cs.CV"}
{"created":"2024-03-07 14:00:17","title":"Langevin equations and a geometric integration scheme for the overdamped limit of homogeneous rotational Brownian motion","abstract":"The translational motion of anisotropic and self-propelled colloidal particles is closely linked with the particle's orientation and its rotational Brownian motion. In the overdamped limit, the stochastic evolution of the orientation vector follows a diffusion process on the unit sphere and is characterised by an orientation-dependent (\"multiplicative\") noise. As a consequence, the corresponding Langevin equation attains different forms depending on whether It\\=o's or Stratonovich's stochastic calculus is used. We clarify that both forms are equivalent and derive them from a geometric construction of Brownian motion on the unit sphere, based on infinitesimal random rotations. Our approach suggests further a geometric integration scheme for rotational Brownian motion, which preserves the normalisation constraint of the orientation vector exactly. We show that a simple implementation of the scheme converges weakly at order 1 of the integration time step, and we outline an advanced variant of the scheme that is weakly exact for an arbitrarily large time step. The discussion is restricted to time-homogeneous rotational Brownian motion (i.e., constant rotational diffusion tensor), which is relevant for chemically anisotropic spheres such as self-propelled Janus particles.","sentences":["The translational motion of anisotropic and self-propelled colloidal particles is closely linked with the particle's orientation and its rotational Brownian motion.","In the overdamped limit, the stochastic evolution of the orientation vector follows a diffusion process on the unit sphere and is characterised by an orientation-dependent (\"multiplicative\") noise.","As a consequence, the corresponding Langevin equation attains different forms depending on whether It\\=o's or Stratonovich's stochastic calculus is used.","We clarify that both forms are equivalent and derive them from a geometric construction of Brownian motion on the unit sphere, based on infinitesimal random rotations.","Our approach suggests further a geometric integration scheme for rotational Brownian motion, which preserves the normalisation constraint of the orientation vector exactly.","We show that a simple implementation of the scheme converges weakly at order 1 of the integration time step, and we outline an advanced variant of the scheme that is weakly exact for an arbitrarily large time step.","The discussion is restricted to time-homogeneous rotational Brownian motion (i.e., constant rotational diffusion tensor), which is relevant for chemically anisotropic spheres such as self-propelled Janus particles."],"url":"http://arxiv.org/abs/2403.04501v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-07 13:58:29","title":"Pressure-improved Scott-Vogelius type elements","abstract":"The Scott-Vogelius element is a popular finite element for the discretization of the Stokes equations which enjoys inf-sup stability and gives divergence-free velocity approximation. However, it is well known that the convergence rates for the discrete pressure deteriorate in the presence of certain $critical$ $vertices$ in a triangulation of the domain. Modifications of the Scott-Vogelius element such as the recently introduced pressure-wired Stokes element also suffer from this effect. In this paper we introduce a simple modification strategy for these pressure spaces that preserves the inf-sup stability while the pressure converges at an optimal rate.","sentences":["The Scott-Vogelius element is a popular finite element for the discretization of the Stokes equations which enjoys inf-sup stability and gives divergence-free velocity approximation.","However, it is well known that the convergence rates for the discrete pressure deteriorate in the presence of certain $critical$ $vertices$ in a triangulation of the domain.","Modifications of the Scott-Vogelius element such as the recently introduced pressure-wired Stokes element also suffer from this effect.","In this paper we introduce a simple modification strategy for these pressure spaces that preserves the inf-sup stability while the pressure converges at an optimal rate."],"url":"http://arxiv.org/abs/2403.04499v1","category":"math.NA"}
{"created":"2024-03-07 13:11:25","title":"Characterization of Besov spaces with dominating mixed smoothness by differences","abstract":"Besov spaces with dominating mixed smoothness, on the product of the real line and the torus as well as bounded domains, are studied. A characterization of these function spaces in terms of differences is provided. Applications to random fields, like Gaussian fields and the stochastic heat equation, are discussed, based on a Kolmogorov criterion for Besov regularity with dominating mixed smoothness.","sentences":["Besov spaces with dominating mixed smoothness, on the product of the real line and the torus as well as bounded domains, are studied.","A characterization of these function spaces in terms of differences is provided.","Applications to random fields, like Gaussian fields and the stochastic heat equation, are discussed, based on a Kolmogorov criterion for Besov regularity with dominating mixed smoothness."],"url":"http://arxiv.org/abs/2403.04469v1","category":"math.CA"}
{"created":"2024-03-07 12:14:29","title":"Episodic eruptions of young accreting stars: the key role of disc thermal instability due to Hydrogen ionisation","abstract":"In the classical grouping of large magnitude episodic variability of young accreting stars, FUORs outshine their stars by a factor of $\\sim$ 100, and can last for up to centuries; EXORs are dimmer, and last months to a year. A disc Hydrogen ionisation Thermal Instability (TI) scenario was previously proposed for FUORs but required unrealistically low disc viscosity. In the last decade, many intermediate type objects, e.g., FUOR-like in luminosity and spectra but EXOR-like in duration were found. Here we show that the intermediate type bursters Gaia20eae, PTF14jg, Gaia19bey and Gaia21bty may be naturally explained by the TI scenario with realistic viscosity values. We argue that TI predicts a dearth (desert) of bursts with peak accretion rates between $\\dot M \\sim 10^{-6} M_\\odot$/yr and $\\dot M \\sim 10^{-5} M_\\odot$/yr, and that this desert is seen in the sample of all the bursters with previously determined $\\dot M$ burst. Most classic EXORs (FUORs) appear to be on the cold (hot) branch of the S-curve during the peak light of their eruptions; thus TI may play a role in this class differentiation. At the same time, TI is unable to explain how classic FUORs can last for up to centuries, and over-predicts the occurrence rate of short FUORs by at least an order of magnitude. We conclude that TI is a required ingredient of episodic accretion operating at R < 0.1 au, but additional physics must play a role at larger scales. Knowledge of TI inner workings from related disciplines may enable its use as a tool to constrain the nature of this additional physics.","sentences":["In the classical grouping of large magnitude episodic variability of young accreting stars, FUORs outshine their stars by a factor of $\\sim$ 100, and can last for up to centuries; EXORs are dimmer, and last months to a year.","A disc Hydrogen ionisation Thermal Instability (TI) scenario was previously proposed for FUORs but required unrealistically low disc viscosity.","In the last decade, many intermediate type objects, e.g., FUOR-like in luminosity and spectra but EXOR-like in duration were found.","Here we show that the intermediate type bursters Gaia20eae, PTF14jg, Gaia19bey and Gaia21bty may be naturally explained by the TI scenario with realistic viscosity values.","We argue that TI predicts a dearth (desert) of bursts with peak accretion rates between $\\dot M \\sim 10^{-6} M_\\odot$/yr and $\\dot M \\sim 10^{-5} M_\\odot$/yr, and that this desert is seen in the sample of all the bursters with previously determined $\\dot M$ burst.","Most classic EXORs (FUORs) appear to be on the cold (hot) branch of the S-curve during the peak light of their eruptions; thus TI may play a role in this class differentiation.","At the same time, TI is unable to explain how classic FUORs can last for up to centuries, and over-predicts the occurrence rate of short FUORs by at least an order of magnitude.","We conclude that TI is a required ingredient of episodic accretion operating at R < 0.1 au, but additional physics must play a role at larger scales.","Knowledge of TI inner workings from related disciplines may enable its use as a tool to constrain the nature of this additional physics."],"url":"http://arxiv.org/abs/2403.04439v1","category":"astro-ph.SR"}
{"created":"2024-03-07 09:54:02","title":"Insight into properties of sizable glass former from volumetric measurements","abstract":"Sizable glass formers feature numerous unique properties and potential applications, but many questions regarding their glass transition dynamics have not been resolved yet. Here we analyzed structural relaxation times measured as a function of temperature and pressure in combination with the equation of state obtained from pressure-volume-temperature (PVT) measurements. Despite evidence from previous dielectric studies indicating a remarkable sensitivity of supercooled dynamics to compression, and contrary to intuition, our results demonstrated the temperate proof for the almost equivalent importance of thermal energy and free volume fluctuations in controlling reorientation dynamics of sizable molecules. The found scaling exponent equals 3.0 and Ev/Ep ratio of 0.6 were typical for glass-forming materials with relaxation dynamics determined by both effects with a minor advantage of thermal fluctuations involvement. It shows that the high values of key parameters characterizing the sensitivity of the glass transition dynamics to pressure changes, i.e. activation volume and dTg/dP, are not a valid premise for a remarkable contribution of volume on glass transition dynamics.","sentences":["Sizable glass formers feature numerous unique properties and potential applications, but many questions regarding their glass transition dynamics have not been resolved yet.","Here we analyzed structural relaxation times measured as a function of temperature and pressure in combination with the equation of state obtained from pressure-volume-temperature (PVT) measurements.","Despite evidence from previous dielectric studies indicating a remarkable sensitivity of supercooled dynamics to compression, and contrary to intuition, our results demonstrated the temperate proof for the almost equivalent importance of thermal energy and free volume fluctuations in controlling reorientation dynamics of sizable molecules.","The found scaling exponent equals 3.0 and Ev/Ep ratio of 0.6 were typical for glass-forming materials with relaxation dynamics determined by both effects with a minor advantage of thermal fluctuations involvement.","It shows that the high values of key parameters characterizing the sensitivity of the glass transition dynamics to pressure changes, i.e. activation volume and dTg/dP, are not a valid premise for a remarkable contribution of volume on glass transition dynamics."],"url":"http://arxiv.org/abs/2403.04367v1","category":"cond-mat.soft"}
{"created":"2024-03-07 09:00:18","title":"A High-order Nystr\u00f6m-based Scheme Explicitly Enforcing Surface Density Continuity for the Electric Field Integral Equation","abstract":"This paper introduces an efficient approach for solving the Electric Field Integral Equation (EFIE) with high-order accuracy by explicitly enforcing the continuity of the impressed current densities across boundaries of the surface patch discretization. The integral operator involved is discretized via a Nystr\\\"om-collocation approach based on Chebyshev polynomial expansion within each patch and a closed quadrature rule is utilized such that the discretization points inside one patch coincide with those inside another patch on the shared boundary of those two patches. The continuity enforcement is achieved by constructing a mapping from those coninciding points to a vector containing unique discretization points used in the GMRES iterative solver. The proposed approach is applied to the scattering of several different geometries including a sphere, a cube, a NURBS model imported from CAD software, and a dipole structure and results are compared with the Magnetic Field Integral Equation (MFIE) and the EFIE without enforcing continuity to illustrate the effectiveness of the approach.","sentences":["This paper introduces an efficient approach for solving the Electric Field Integral Equation (EFIE) with high-order accuracy by explicitly enforcing the continuity of the impressed current densities across boundaries of the surface patch discretization.","The integral operator involved is discretized via a Nystr\\\"om-collocation approach based on Chebyshev polynomial expansion within each patch and a closed quadrature rule is utilized such that the discretization points inside one patch coincide with those inside another patch on the shared boundary of those two patches.","The continuity enforcement is achieved by constructing a mapping from those coninciding points to a vector containing unique discretization points used in the GMRES iterative solver.","The proposed approach is applied to the scattering of several different geometries including a sphere, a cube, a NURBS model imported from CAD software, and a dipole structure and results are compared with the Magnetic Field Integral Equation (MFIE) and the EFIE without enforcing continuity to illustrate the effectiveness of the approach."],"url":"http://arxiv.org/abs/2403.04334v1","category":"math.NA"}
{"created":"2024-03-07 08:38:36","title":"Second-order McKean-Vlasov stochastic evolution equation driven by Poisson jumps: existence, uniqueness and averaging principle","abstract":"In the paper, a class of second-order McKean-Vlasov stochastic evolution equation driven by Poisson jumps with non-Lipschitz conditions is considered. The existence and uniqueness of the mild solution is established by means of the Carath${\\rm \\acute{e}}$odory approximation technique. Furthermore, an averaging principle is obtained between the solution of the second-order McKean-Vlasov stochastic evolution equation and that of the simplified equation in mean square sense.","sentences":["In the paper, a class of second-order McKean-Vlasov stochastic evolution equation driven by Poisson jumps with non-Lipschitz conditions is considered.","The existence and uniqueness of the mild solution is established by means of the Carath${\\rm \\acute{e}}$odory approximation technique.","Furthermore, an averaging principle is obtained between the solution of the second-order McKean-Vlasov stochastic evolution equation and that of the simplified equation in mean square sense."],"url":"http://arxiv.org/abs/2403.04323v1","category":"math.PR"}
{"created":"2024-03-07 07:39:24","title":"Positivity preserving and mass conservative projection method for the Poisson-Nernst-Planck equation","abstract":"We propose and analyze a novel approach to construct structure preserving approximations for the Poisson-Nernst-Planck equations, focusing on the positivity preserving and mass conservation properties. The strategy consists of a standard time marching step with a projection (or correction) step to satisfy the desired physical constraints (positivity and mass conservation). Based on the $L^2$ projection, we construct a second order Crank-Nicolson type finite difference scheme, which is linear (exclude the very efficient $L^2$ projection part), positivity preserving and mass conserving. Rigorous error estimates in $L^2$ norm are established, which are both second order accurate in space and time. The other choice of projection, e.g. $H^1$ projection, is discussed.   Numerical examples are presented to verify the theoretical results and demonstrate the efficiency of the proposed method.","sentences":["We propose and analyze a novel approach to construct structure preserving approximations for the Poisson-Nernst-Planck equations, focusing on the positivity preserving and mass conservation properties.","The strategy consists of a standard time marching step with a projection (or correction) step to satisfy the desired physical constraints (positivity and mass conservation).","Based on the $L^2$ projection, we construct a second order Crank-Nicolson type finite difference scheme, which is linear (exclude the very efficient $L^2$ projection part), positivity preserving and mass conserving.","Rigorous error estimates in $L^2$ norm are established, which are both second order accurate in space and time.","The other choice of projection, e.g. $H^1$ projection, is discussed.   ","Numerical examples are presented to verify the theoretical results and demonstrate the efficiency of the proposed method."],"url":"http://arxiv.org/abs/2403.04291v1","category":"math.NA"}
{"created":"2024-03-07 07:24:41","title":"Topology Change from Pointlike Sources","abstract":"In this paper we study topology-changing spacetimes occurring from pointlike sources. Following an old idea of Penrose, we will opt for a non-Hausdorff model of topology change in which an initial pointlike source is \"doubled\" and allowed to propagate along null rays into an eventual cobordism. By appealing to recent developments in non-Hausdorff differential geometry, we will describe and evaluate gravitational actions on these pointlike topology-changing spacetimes. Motivated by analogous results for the Trousers space, we describe a sign convention for Lorentzian angles that will ensure the dampening of our non-Hausdorff topology-changing spacetimes within a two-dimensional path integral for gravity.","sentences":["In this paper we study topology-changing spacetimes occurring from pointlike sources.","Following an old idea of Penrose, we will opt for a non-Hausdorff model of topology change in which an initial pointlike source is \"doubled\" and allowed to propagate along null rays into an eventual cobordism.","By appealing to recent developments in non-Hausdorff differential geometry, we will describe and evaluate gravitational actions on these pointlike topology-changing spacetimes.","Motivated by analogous results for the Trousers space, we describe a sign convention for Lorentzian angles that will ensure the dampening of our non-Hausdorff topology-changing spacetimes within a two-dimensional path integral for gravity."],"url":"http://arxiv.org/abs/2403.04281v1","category":"gr-qc"}
{"created":"2024-03-07 07:08:57","title":"Qubit-Wise Architecture Search Method for Variational Quantum Circuits","abstract":"Considering the noise level limit, one crucial aspect for quantum machine learning is to design a high-performing variational quantum circuit architecture with small number of quantum gates. As the classical neural architecture search (NAS), quantum architecture search methods (QAS) employ methods like reinforcement learning, evolutionary algorithms and supernet optimiza-tion to improve the search efficiency. In this paper, we propose a novel qubit-wise architec-ture search (QWAS) method, which progres-sively search one-qubit configuration per stage, and combine with Monte Carlo Tree Search al-gorithm to find good quantum architectures by partitioning the search space into several good and bad subregions. The numerical experimental results indicate that our proposed method can balance the exploration and exploitation of cir-cuit performance and size in some real-world tasks, such as MNIST, Fashion and MOSI. As far as we know, QWAS achieves the state-of-art re-sults of all tasks in the terms of accuracy and circuit size.","sentences":["Considering the noise level limit, one crucial aspect for quantum machine learning is to design a high-performing variational quantum circuit architecture with small number of quantum gates.","As the classical neural architecture search (NAS), quantum architecture search methods (QAS) employ methods like reinforcement learning, evolutionary algorithms and supernet optimiza-tion to improve the search efficiency.","In this paper, we propose a novel qubit-wise architec-ture search (QWAS) method, which progres-sively search one-qubit configuration per stage, and combine with Monte Carlo Tree Search al-gorithm to find good quantum architectures by partitioning the search space into several good and bad subregions.","The numerical experimental results indicate that our proposed method can balance the exploration and exploitation of cir-cuit performance and size in some real-world tasks, such as MNIST, Fashion and MOSI.","As far as we know, QWAS achieves the state-of-art re-sults of all tasks in the terms of accuracy and circuit size."],"url":"http://arxiv.org/abs/2403.04268v1","category":"quant-ph"}
{"created":"2024-03-07 06:54:26","title":"Coderivative-Based Newton Methods in Structured Nonconvex and Nonsmooth Optimization","abstract":"This paper proposes and develops new Newton-type methods to solve structured nonconvex and nonsmooth optimization problems with justifying their fast local and global convergence by means of advanced tools of variational analysis and generalized differentiation. The objective functions belong to a broad class of prox-regular functions with specification to constrained optimization of nonconvex structured sums. We also develop a novel line search method, which is an extension of the proximal gradient algorithm while allowing us to globalize the proposed coderivative-based Newton methods by incorporating the machinery of forward-backward envelopes. Further applications and numerical experiments are conducted for the $\\ell_0$-$\\ell_2$ regularized least-square model appearing in statistics and machine learning.","sentences":["This paper proposes and develops new Newton-type methods to solve structured nonconvex and nonsmooth optimization problems with justifying their fast local and global convergence by means of advanced tools of variational analysis and generalized differentiation.","The objective functions belong to a broad class of prox-regular functions with specification to constrained optimization of nonconvex structured sums.","We also develop a novel line search method, which is an extension of the proximal gradient algorithm while allowing us to globalize the proposed coderivative-based Newton methods by incorporating the machinery of forward-backward envelopes.","Further applications and numerical experiments are conducted for the $\\ell_0$-$\\ell_2$ regularized least-square model appearing in statistics and machine learning."],"url":"http://arxiv.org/abs/2403.04262v1","category":"math.OC"}
{"created":"2024-03-07 06:29:16","title":"Portable GPU implementation of the WP-CCC ion-atom collisions code","abstract":"We present our experience of porting the code used in the wave-packet convergent-close-coupling (WP-CCC) approach to run on NVIDIA V100 and AMD MI250X GPUs. The WP-CCC approach is a method used in the field of ion-atom collision physics to describe various processes such as elastic scattering, target excitation and electron-capture by the projectile. It has demonstrated its effectiveness in modelling collisions involving proton or bare ion projectiles with various atomic and molecular targets, especially those which can be considered as one or two-electron systems. Such calculations find their application in computational atomic physics as well as in the modelling of fusion plasmas and in hadron therapy for cancer treatment. The main computational cost of the method lies in the solution of an emerging set of coupled first-order differential equations. This involves implementing the standard Runge-Kutta method while varying the projectile position along multiple straight-line paths. At each projectile position several millions of matrix elements need to be calculated which is accomplished using the OpenACC programming model. Once these matrix elements are computed, the subsequent steps involve matrix inversion and multiplication with another matrix. To expedite these operations, a GPU-accelerated LAPACK routine, specialised for solving systems of linear equations, is employed. For AMD GPUs, this routine is accessible through the hipSOLVER library, while for NVIDIA GPUs, it can be obtained from the cuSOLVER library. The portability, performance and energy efficiency of the CPU-only code have been compared with the GPU-accelerated version running on AMD and NVIDIA GPUs. The implementation of GPU-accelerated WP-CCC code opens up avenues for exploring more sophisticated collision processes involving complex projectile and target structures, which were previously considered infeasible.","sentences":["We present our experience of porting the code used in the wave-packet convergent-close-coupling (WP-CCC) approach to run on NVIDIA V100 and AMD MI250X GPUs.","The WP-CCC approach is a method used in the field of ion-atom collision physics to describe various processes such as elastic scattering, target excitation and electron-capture by the projectile.","It has demonstrated its effectiveness in modelling collisions involving proton or bare ion projectiles with various atomic and molecular targets, especially those which can be considered as one or two-electron systems.","Such calculations find their application in computational atomic physics as well as in the modelling of fusion plasmas and in hadron therapy for cancer treatment.","The main computational cost of the method lies in the solution of an emerging set of coupled first-order differential equations.","This involves implementing the standard Runge-Kutta method while varying the projectile position along multiple straight-line paths.","At each projectile position several millions of matrix elements need to be calculated which is accomplished using the OpenACC programming model.","Once these matrix elements are computed, the subsequent steps involve matrix inversion and multiplication with another matrix.","To expedite these operations, a GPU-accelerated LAPACK routine, specialised for solving systems of linear equations, is employed.","For AMD GPUs, this routine is accessible through the hipSOLVER library, while for NVIDIA GPUs, it can be obtained from the cuSOLVER library.","The portability, performance and energy efficiency of the CPU-only code have been compared with the GPU-accelerated version running on AMD and NVIDIA GPUs.","The implementation of GPU-accelerated WP-CCC code opens up avenues for exploring more sophisticated collision processes involving complex projectile and target structures, which were previously considered infeasible."],"url":"http://arxiv.org/abs/2403.04252v1","category":"physics.comp-ph"}
{"created":"2024-03-07 05:49:47","title":"Modeling beam propagation in a moving nonlinear medium","abstract":"Fully describing light propagation in a rotating, anisotropic medium with thermal nonlinearity requires modeling the interplay between nonlinear refraction, birefringence, and the nonlinear group index. Incorporating these factors into a generalized nonlinear Schr\\\"odinger equation and fitting them to recent experimental results reveals two key relationships: the photon drag effect can have a nonlinear component that is dependent on the motion of the medium, and the temporal dynamics of the moving birefringent nonlinear medium create distorted figure-eight-like transverse trajectories at the output. The beam trajectory can be accurately modelled with a full understanding of the propagation effects. Efficiently modeling these effects and accurately predicting the beam's output position has implications for optimizing applications in velocimetry and beam-steering. Understanding the roles of competitive nonlinearities gives insight into the creation or suppression of nonlinear phenomena like self-action effects.","sentences":["Fully describing light propagation in a rotating, anisotropic medium with thermal nonlinearity requires modeling the interplay between nonlinear refraction, birefringence, and the nonlinear group index.","Incorporating these factors into a generalized nonlinear Schr\\\"odinger equation and fitting them to recent experimental results reveals two key relationships: the photon drag effect can have a nonlinear component that is dependent on the motion of the medium, and the temporal dynamics of the moving birefringent nonlinear medium create distorted figure-eight-like transverse trajectories at the output.","The beam trajectory can be accurately modelled with a full understanding of the propagation effects.","Efficiently modeling these effects and accurately predicting the beam's output position has implications for optimizing applications in velocimetry and beam-steering.","Understanding the roles of competitive nonlinearities gives insight into the creation or suppression of nonlinear phenomena like self-action effects."],"url":"http://arxiv.org/abs/2403.04238v1","category":"physics.optics"}
{"created":"2024-03-07 05:48:06","title":"The Smoluchowski-Kramers approximation with distribution-dependent potential and highly oscillating force","abstract":"An approximation is derived for a Langevin equation with distribution-dependent potential and state-dependent, randomly fast oscillation. By some estimates and a diffusion approximation the limiting equation is shown to be distribution-dependent stochastic differential equation (SDEs) driven by white noise.","sentences":["An approximation is derived for a Langevin equation with distribution-dependent potential and state-dependent, randomly fast oscillation.","By some estimates and a diffusion approximation the limiting equation is shown to be distribution-dependent stochastic differential equation (SDEs) driven by white noise."],"url":"http://arxiv.org/abs/2403.04237v1","category":"math.PR"}
{"created":"2024-03-07 04:54:15","title":"Spectrum of the Laplacian and the Jacobi operator on Generalized rotational minimal hypersurfaces of spheres","abstract":"Let $M\\subset S^{n+1}$ be the hypersurface generated by rotating a hypersurface $M_0$ contained in the interior of the unit ball of $\\mathbb{R}^{n-k+1}$. More precisely, $M=\\{(\\sqrt{1-|m|^2}\\, y\\, , m):y\\in S^k,\\, m\\in M_0\\}$. We deduce the equation for the mean curvature of $M$ in terms of the principal curvatures of $M_0$ and in the particular case when $M_0$ is a surface of revolution in $\\mathbb{R}^3$, we provide a way to find the eigenvalues of the Laplace and the Stability operators. Numerical examples of embedded minimal hypersurface in $S^{n+1}$ will be provided for several values of $n$. To illustrate the method for finding the eigenvalues, we will compute all the eigenvalues of the Laplace operator smaller than 12 and we compute all non positive eigenvalues of the Stability operators for a particular minimal embedded hypersurface in $S^6$. We show that the stability index (the number of negative eigenvalues of the stability operator counted with multiplicity) for this example is 77 and the nullity (the multiplicity of the eigenvalue $\\lambda=0$ of the Stability operator) is 14. Similar results are found in the case where $M_0$ is a hypersurface in $\\mathbb{R}^{l+2}$ of the form $(f_2(u) z, f_1(u))$ with $z$ in the $l$-dimensional unit sphere $S^l$","sentences":["Let $M\\subset S^{n+1}$ be the hypersurface generated by rotating a hypersurface $M_0$ contained in the interior of the unit ball of $\\mathbb{R}^{n-k+1}$. More precisely, $M=\\{(\\sqrt{1-|m|^2}\\, y\\, , m):y\\in S^k,\\,","m\\in M_0\\}$.","We deduce the equation for the mean curvature of $M$ in terms of the principal curvatures of $M_0$ and in the particular case when $M_0$ is a surface of revolution in $\\mathbb{R}^3$, we provide a way to find the eigenvalues of the Laplace and the Stability operators.","Numerical examples of embedded minimal hypersurface in $S^{n+1}$ will be provided for several values of $n$. To illustrate the method for finding the eigenvalues, we will compute all the eigenvalues of the Laplace operator smaller than 12 and we compute all non positive eigenvalues of the Stability operators for a particular minimal embedded hypersurface in $S^6$. We show that the stability index (the number of negative eigenvalues of the stability operator counted with multiplicity) for this example is 77 and the nullity (the multiplicity of the eigenvalue $\\lambda=0$ of the Stability operator) is 14.","Similar results are found in the case where $M_0$ is a hypersurface in $\\mathbb{R}^{l+2}$ of the form $(f_2(u) z, f_1(u))$ with $z$ in the $l$-dimensional unit sphere $S^l$"],"url":"http://arxiv.org/abs/2403.04223v1","category":"math.DG"}
{"created":"2024-03-07 04:47:36","title":"Eulerian uniqueness of the $\u03b1$-SQG patch problem","abstract":"We consider the patch problem of the $\\alpha$-SQG equation with $\\alpha=0$ being the 2D Euler and $\\alpha= \\frac{1}{2}$ the SQG equations respectively. In the Eulerian setting, we prove the uniqueness of patch solutions of regularity $W^{2, \\frac{1}{1-2\\alpha} +} $ when $0<\\alpha< \\frac{1}{2}$ and $C^{1, 4\\alpha+ }$ when $0<\\alpha< \\frac{1}{4} $. The proof is intrinsic to the modified Biot-Savart law and independent of the local existence of patch solutions.","sentences":["We consider the patch problem of the $\\alpha$-SQG equation with $\\alpha=0$ being the 2D Euler and $\\alpha= \\frac{1}{2}$ the SQG equations respectively.","In the Eulerian setting, we prove the uniqueness of patch solutions of regularity $W^{2, \\frac{1}{1-2\\alpha} +} $ when $0<\\alpha< \\frac{1}{2}$ and $C^{1, 4\\alpha+ }$ when $0<\\alpha< \\frac{1}{4} $.","The proof is intrinsic to the modified Biot-Savart law and independent of the local existence of patch solutions."],"url":"http://arxiv.org/abs/2403.04219v1","category":"math.AP"}
{"created":"2024-03-07 04:39:48","title":"Performance Assessment of Universal Machine Learning Interatomic Potentials: Challenges and Directions for Materials' Surfaces","abstract":"Machine learning interatomic potentials (MLIPs) are one of the main techniques in the materials science toolbox, able to bridge ab initio accuracy with the computational efficiency of classical force fields. This allows simulations ranging from atoms, molecules, and biosystems, to solid and bulk materials, surfaces, nanomaterials, and their interfaces and complex interactions. A recent class of advanced MLIPs, which use equivariant representations and deep graph neural networks, is known as universal models. These models are proposed as foundational models suitable for any system, covering most elements from the periodic table. Current universal MLIPs (UIPs) have been trained with the largest consistent dataset available nowadays. However, these are composed mostly of bulk materials' DFT calculations. In this article, we assess the universality of all openly available UIPs, namely MACE, CHGNet, and M3GNet, in a representative task of generalization: calculation of surface energies. We find that the out-of-the-box foundational models have significant shortcomings in this task, with errors correlated to the total energy of surface simulations, having an out-of-domain distance from the training dataset. Our results show that while UIPs are an efficient starting point for fine-tuning specialized models, we envision the potential of increasing the coverage of the materials space towards universal training datasets for MLIPs.","sentences":["Machine learning interatomic potentials (MLIPs) are one of the main techniques in the materials science toolbox, able to bridge ab initio accuracy with the computational efficiency of classical force fields.","This allows simulations ranging from atoms, molecules, and biosystems, to solid and bulk materials, surfaces, nanomaterials, and their interfaces and complex interactions.","A recent class of advanced MLIPs, which use equivariant representations and deep graph neural networks, is known as universal models.","These models are proposed as foundational models suitable for any system, covering most elements from the periodic table.","Current universal MLIPs (UIPs) have been trained with the largest consistent dataset available nowadays.","However, these are composed mostly of bulk materials' DFT calculations.","In this article, we assess the universality of all openly available UIPs, namely MACE, CHGNet, and M3GNet, in a representative task of generalization: calculation of surface energies.","We find that the out-of-the-box foundational models have significant shortcomings in this task, with errors correlated to the total energy of surface simulations, having an out-of-domain distance from the training dataset.","Our results show that while UIPs are an efficient starting point for fine-tuning specialized models, we envision the potential of increasing the coverage of the materials space towards universal training datasets for MLIPs."],"url":"http://arxiv.org/abs/2403.04217v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-07 04:28:13","title":"Partial tidal disruption events: The elixir of life","abstract":"In our Galactic Center, about 10,000 to 100,000 stars are estimated to have survived tidal disruption events, resulting in partially disrupted remnants. These events occur when a supermassive black hole (SMBH) tidally interacts with a star, but not enough to completely disrupt the star. We use the 1D stellar evolution code Kepler and the 3D smoothed particle hydrodynamics code Phantom to model the tidal disruption of 1, 3, and 10 solar mass stars at zero-age (ZAMS), middle-age (MAMS), and terminal-age main-sequence (TAMS). We map the disruption remnants into Kepler in order to understand their post-distribution evolution. We find distinct characteristics in the remnants, including increased radius, rapid core rotation, and differential rotation in the envelope. The remnants undergo composition mixing that affects their stellar evolution. Whereas the remnants formed by disruption of ZAMS models evolve similarly to unperturbed models of the same mass, for MAMS and TAMS stars, the remnants have higher luminosity and effective temperature. Potential observational signatures include peculiarities in nitrogen and carbon abundances, higher luminosity, rapid rotation, faster evolution, and unique tracks in the Hertzsprung-Russell diagram","sentences":["In our Galactic Center, about 10,000 to 100,000 stars are estimated to have survived tidal disruption events, resulting in partially disrupted remnants.","These events occur when a supermassive black hole (SMBH) tidally interacts with a star, but not enough to completely disrupt the star.","We use the 1D stellar evolution code Kepler and the 3D smoothed particle hydrodynamics code Phantom to model the tidal disruption of 1, 3, and 10 solar mass stars at zero-age (ZAMS), middle-age (MAMS), and terminal-age main-sequence (TAMS).","We map the disruption remnants into Kepler in order to understand their post-distribution evolution.","We find distinct characteristics in the remnants, including increased radius, rapid core rotation, and differential rotation in the envelope.","The remnants undergo composition mixing that affects their stellar evolution.","Whereas the remnants formed by disruption of ZAMS models evolve similarly to unperturbed models of the same mass, for MAMS and TAMS stars, the remnants have higher luminosity and effective temperature.","Potential observational signatures include peculiarities in nitrogen and carbon abundances, higher luminosity, rapid rotation, faster evolution, and unique tracks in the Hertzsprung-Russell diagram"],"url":"http://arxiv.org/abs/2403.04211v1","category":"astro-ph.HE"}
{"created":"2024-03-07 04:25:04","title":"Controllable Skyrmion Islands in a Moir\u00e9 Magnet","abstract":"Antiferromagnetic(AFM) skyrmions have been in the spotlight as ideal topological magnetic bits. Although they are topologically protected, they do not exhibit the skyrmion Hall effect unlike the ferromagnetic ones. Thus, AFM skyrmions are considered to provide a better control of the skyrmion's motion due to the absence of the skyrmion Magnus effect. In this work, we propose a possible realization of controllable AFM skyrmions in a twisted Moir\\'e magnet. The tunability of Moir\\'e materials is not only a good platform for the provision of rich phases, but also for the stabilization of skyrmion phase. We investigate the ground state of twisted bilayer AFM system by solving the Landau-Lifshitz-Gilbert equation in a continuum model. We show that the AFM skyrmions are stabilized even in the absence of the external/dipolar magnetic field, as a consequence of the interplay of interlayer coupling, Dzyaloshinskii-Moriya (DM) interaction and Ising anisotropy. More interestingly, due to the magnetoelectric effect, the application of an external electric field locally stabilizes the skyrmions in the twisted bilayer AFM systems, even in the absence of DM interaction. It also allows the skyrmion helicity to change continuously when both the DM interaction and an electric field are present. We show the phase diagram with respect to the strength of interlayer coupling, the DM interaction and an electric field. Our results suggest the possibility of using AFM skyrmions as stable, controllable topological magnetic bits.","sentences":["Antiferromagnetic(AFM) skyrmions have been in the spotlight as ideal topological magnetic bits.","Although they are topologically protected, they do not exhibit the skyrmion Hall effect unlike the ferromagnetic ones.","Thus, AFM skyrmions are considered to provide a better control of the skyrmion's motion due to the absence of the skyrmion Magnus effect.","In this work, we propose a possible realization of controllable AFM skyrmions in a twisted Moir\\'e magnet.","The tunability of Moir\\'e materials is not only a good platform for the provision of rich phases, but also for the stabilization of skyrmion phase.","We investigate the ground state of twisted bilayer AFM system by solving the Landau-Lifshitz-Gilbert equation in a continuum model.","We show that the AFM skyrmions are stabilized even in the absence of the external/dipolar magnetic field, as a consequence of the interplay of interlayer coupling, Dzyaloshinskii-Moriya (DM) interaction and Ising anisotropy.","More interestingly, due to the magnetoelectric effect, the application of an external electric field locally stabilizes the skyrmions in the twisted bilayer AFM systems, even in the absence of DM interaction.","It also allows the skyrmion helicity to change continuously when both the DM interaction and an electric field are present.","We show the phase diagram with respect to the strength of interlayer coupling, the DM interaction and an electric field.","Our results suggest the possibility of using AFM skyrmions as stable, controllable topological magnetic bits."],"url":"http://arxiv.org/abs/2403.04208v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-07 03:48:47","title":"VAEMax: Open-Set Intrusion Detection based on OpenMax and Variational Autoencoder","abstract":"Promptly discovering unknown network attacks is critical for reducing the risk of major loss imposed on system or equipment. This paper aims to develop an open-set intrusion detection model to classify known attacks as well as inferring unknown ones. To achieve this, we employ OpenMax and variational autoencoder to propose a dual detection model, VAEMax. First, we extract flow payload feature based on one-dimensional convolutional neural network. Then, the OpenMax is used to classify flows, during which some unknown attacks can be detected, while the rest are misclassified into a certain class of known flows. Finally, use VAE to perform secondary detection on each class of flows, and determine whether the flow is an unknown attack based on the reconstruction loss. Experiments performed on dataset CIC-IDS2017 and CSE-CIC-IDS2018 show our approach is better than baseline models and can be effectively applied to realistic network environments.","sentences":["Promptly discovering unknown network attacks is critical for reducing the risk of major loss imposed on system or equipment.","This paper aims to develop an open-set intrusion detection model to classify known attacks as well as inferring unknown ones.","To achieve this, we employ OpenMax and variational autoencoder to propose a dual detection model, VAEMax.","First, we extract flow payload feature based on one-dimensional convolutional neural network.","Then, the OpenMax is used to classify flows, during which some unknown attacks can be detected, while the rest are misclassified into a certain class of known flows.","Finally, use VAE to perform secondary detection on each class of flows, and determine whether the flow is an unknown attack based on the reconstruction loss.","Experiments performed on dataset CIC-IDS2017 and CSE-CIC-IDS2018 show our approach is better than baseline models and can be effectively applied to realistic network environments."],"url":"http://arxiv.org/abs/2403.04193v1","category":"cs.CR"}
{"created":"2024-03-07 03:26:10","title":"Exploring the Impact of Opinion Polarization on Short Video Consumption","abstract":"Investigating the increasingly popular domain of short video consumption, this study focuses on the impact of Opinion Polarization (OP), a significant factor in the digital landscape influencing public opinions and social interactions. We analyze OP's effect on viewers' perceptions and behaviors, finding that traditional feedback metrics like likes and watch time fail to fully capture and measure OP. Addressing this gap, our research utilizes Electroencephalogram (EEG) signals to introduce a novel, non-invasive approach for evaluating neural responses to OP, affecting perception and cognition. Empirical analysis reveals OP's considerable impact on viewers' emotions, evidenced by changes in brain activity. Our findings also highlight the potential of EEG data in predicting exposure to polarized short video content, offering a new perspective on the dynamics of short video consumption and a unique method for quantifying OP's effects.","sentences":["Investigating the increasingly popular domain of short video consumption, this study focuses on the impact of Opinion Polarization (OP), a significant factor in the digital landscape influencing public opinions and social interactions.","We analyze OP's effect on viewers' perceptions and behaviors, finding that traditional feedback metrics like likes and watch time fail to fully capture and measure OP.","Addressing this gap, our research utilizes Electroencephalogram (EEG) signals to introduce a novel, non-invasive approach for evaluating neural responses to OP, affecting perception and cognition.","Empirical analysis reveals OP's considerable impact on viewers' emotions, evidenced by changes in brain activity.","Our findings also highlight the potential of EEG data in predicting exposure to polarized short video content, offering a new perspective on the dynamics of short video consumption and a unique method for quantifying OP's effects."],"url":"http://arxiv.org/abs/2403.04184v1","category":"cs.SI"}
{"created":"2024-03-07 02:40:42","title":"SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS","abstract":"Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks. Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric. It measures the expressivity of networks over a batch of input samples. The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be further enhanced by regularisation, which leads to even higher correlations in cell-based search space and enables model size control during the search. For example, Spearman's rank correlation coefficient between regularised SWAP-Score and CIFAR-100 validation accuracies on NAS-Bench-201 networks is 0.90, significantly higher than 0.80 from the second-best metric, NWOT. When integrated with an evolutionary algorithm for NAS, our SWAP-NAS achieves competitive performance on CIFAR-10 and ImageNet in approximately 6 minutes and 9 minutes of GPU time respectively.","sentences":["Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS).","Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks.","Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric.","It measures the expressivity of networks over a batch of input samples.","The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101.","The SWAP-Score can be further enhanced by regularisation, which leads to even higher correlations in cell-based search space and enables model size control during the search.","For example, Spearman's rank correlation coefficient between regularised SWAP-Score and CIFAR-100 validation accuracies on NAS-Bench-201 networks is 0.90, significantly higher than 0.80 from the second-best metric, NWOT.","When integrated with an evolutionary algorithm for NAS, our SWAP-NAS achieves competitive performance on CIFAR-10 and ImageNet in approximately 6 minutes and 9 minutes of GPU time respectively."],"url":"http://arxiv.org/abs/2403.04161v1","category":"cs.LG"}
{"created":"2024-03-07 02:24:45","title":"Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process","abstract":"Considering generating samples with high rewards, we focus on optimizing deep neural networks parameterized stochastic differential equations (SDEs), the advanced generative models with high expressiveness, with policy gradient, the leading algorithm in reinforcement learning. Nevertheless, when applying policy gradients to SDEs, since the policy gradient is estimated on a finite set of trajectories, it can be ill-defined, and the policy behavior in data-scarce regions may be uncontrolled. This challenge compromises the stability of policy gradients and negatively impacts sample complexity. To address these issues, we propose constraining the SDE to be consistent with its associated perturbation process. Since the perturbation process covers the entire space and is easy to sample, we can mitigate the aforementioned problems. Our framework offers a general approach allowing for a versatile selection of policy gradient methods to effectively and efficiently train SDEs. We evaluate our algorithm on the task of structure-based drug design and optimize the binding affinity of generated ligand molecules. Our method achieves the best Vina score -9.07 on the CrossDocked2020 dataset.","sentences":["Considering generating samples with high rewards, we focus on optimizing deep neural networks parameterized stochastic differential equations (SDEs), the advanced generative models with high expressiveness, with policy gradient, the leading algorithm in reinforcement learning.","Nevertheless, when applying policy gradients to SDEs, since the policy gradient is estimated on a finite set of trajectories, it can be ill-defined, and the policy behavior in data-scarce regions may be uncontrolled.","This challenge compromises the stability of policy gradients and negatively impacts sample complexity.","To address these issues, we propose constraining the SDE to be consistent with its associated perturbation process.","Since the perturbation process covers the entire space and is easy to sample, we can mitigate the aforementioned problems.","Our framework offers a general approach allowing for a versatile selection of policy gradient methods to effectively and efficiently train SDEs.","We evaluate our algorithm on the task of structure-based drug design and optimize the binding affinity of generated ligand molecules.","Our method achieves the best Vina score -9.07 on the CrossDocked2020 dataset."],"url":"http://arxiv.org/abs/2403.04154v1","category":"cs.LG"}
{"created":"2024-03-07 02:17:55","title":"Simultaneous Localization and Recognition of Subwavelength Non-Cooperative Entities Based on SISO Time Reversal and Neural Networks","abstract":"The simultaneous localization and recognition of subwavelength non-cooperative entities within complex multi-scattering environments using a simplified system continues to pose a substantial challenge. This communication addresses this challenge by synergistically integrating time reversal time-frequency phase prints (TRTFPPs) and neural networks. Initially, a time reversal (TR) single-input single-output (SISO) framework is employed to generate TRTFPPs. Through the devised approach, two types of subwavelength entities are successfully identified and precisely localized through numerical simulations. Then, the method is verified experimentally in laboratory environment. The proposed methodology holds applicability across various electromagnetic systems, including but not limited to detection, imaging, human-computer interaction, and the Internet of Things (IoT).","sentences":["The simultaneous localization and recognition of subwavelength non-cooperative entities within complex multi-scattering environments using a simplified system continues to pose a substantial challenge.","This communication addresses this challenge by synergistically integrating time reversal time-frequency phase prints (TRTFPPs) and neural networks.","Initially, a time reversal (TR) single-input single-output (SISO) framework is employed to generate TRTFPPs.","Through the devised approach, two types of subwavelength entities are successfully identified and precisely localized through numerical simulations.","Then, the method is verified experimentally in laboratory environment.","The proposed methodology holds applicability across various electromagnetic systems, including but not limited to detection, imaging, human-computer interaction, and the Internet of Things (IoT)."],"url":"http://arxiv.org/abs/2403.04150v1","category":"physics.app-ph"}
{"created":"2024-03-07 00:12:16","title":"New quasi-Einstein metrics on a two--sphere","abstract":"We construct all axi-symmetric non--gradient $m$--quasi--Einstein structures on a two--sphere. This includes the spatial cross-section of the extreme Kerr black hole horizon corresponding to $m=2$, as well as a family of new regular metrics with $m\\neq 2$ given in terms of hypergeometric functions. We also show that in the case $m=-1$ with vanishing cosmological constant the only orientable compact solution in dimension two is the flat torus, which proves that there are no compact surfaces with a metrisable affine connection with skew Ricci tensor.","sentences":["We construct all axi-symmetric non--gradient $m$--quasi--Einstein structures on a two--sphere.","This includes the spatial cross-section of the extreme Kerr black hole horizon corresponding to $m=2$, as well as a family of new regular metrics with $m\\neq 2$ given in terms of hypergeometric functions.","We also show that in the case $m=-1$ with vanishing cosmological constant the only orientable compact solution in dimension two is the flat torus, which proves that there are no compact surfaces with a metrisable affine connection with skew Ricci tensor."],"url":"http://arxiv.org/abs/2403.04117v1","category":"math.DG"}
{"created":"2024-03-07 00:12:08","title":"Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis","abstract":"X-ray is widely applied for transmission imaging due to its stronger penetration than natural light. When rendering novel view X-ray projections, existing methods mainly based on NeRF suffer from long training time and slow inference speed. In this paper, we propose a 3D Gaussian splatting-based framework, namely X-Gaussian, for X-ray novel view synthesis. Firstly, we redesign a radiative Gaussian point cloud model inspired by the isotropic nature of X-ray imaging. Our model excludes the influence of view direction when learning to predict the radiation intensity of 3D points. Based on this model, we develop a Differentiable Radiative Rasterization (DRR) with CUDA implementation. Secondly, we customize an Angle-pose Cuboid Uniform Initialization (ACUI) strategy that directly uses the parameters of the X-ray scanner to compute the camera information and then uniformly samples point positions within a cuboid enclosing the scanned object. Experiments show that our X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoying less than 15% training time and over 73x inference speed. The application on sparse-view CT reconstruction also reveals the practical values of our method. Code and models will be publicly available at https://github.com/caiyuanhao1998/X-Gaussian . A video demo of the training process visualization is at https://www.youtube.com/watch?v=gDVf_Ngeghg .","sentences":["X-ray is widely applied for transmission imaging due to its stronger penetration than natural light.","When rendering novel view X-ray projections, existing methods mainly based on NeRF suffer from long training time and slow inference speed.","In this paper, we propose a 3D Gaussian splatting-based framework, namely X-Gaussian, for X-ray novel view synthesis.","Firstly, we redesign a radiative Gaussian point cloud model inspired by the isotropic nature of X-ray imaging.","Our model excludes the influence of view direction when learning to predict the radiation intensity of 3D points.","Based on this model, we develop a Differentiable Radiative Rasterization (DRR) with CUDA implementation.","Secondly, we customize an Angle-pose Cuboid Uniform Initialization (ACUI) strategy that directly uses the parameters of the X-ray scanner to compute the camera information and then uniformly samples point positions within a cuboid enclosing the scanned object.","Experiments show that our X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoying less than 15% training time and over 73x inference speed.","The application on sparse-view CT reconstruction also reveals the practical values of our method.","Code and models will be publicly available at https://github.com/caiyuanhao1998/X-Gaussian .","A video demo of the training process visualization is at https://www.youtube.com/watch?v=gDVf_Ngeghg ."],"url":"http://arxiv.org/abs/2403.04116v1","category":"eess.IV"}
{"created":"2024-03-06 22:51:01","title":"Helmholtz preconditioning for the compressible Euler equations using mixed finite elements with Lorenz staggering","abstract":"Implicit solvers for atmospheric models are often accelerated via the solution of a preconditioned system. For block preconditioners this typically involves the factorisation of the (approximate) Jacobian for the coupled system into a Helmholtz equation for some function of the pressure. Here we present a preconditioner for the compressible Euler equations with a flux form representation of the potential temperature on the Lorenz grid using mixed finite elements. This formulation allows for spatial discretisations that conserve both energy and potential temperature variance. By introducing the dry thermodynamic entropy as an auxiliary variable for the solution of the algebraic system, the resulting preconditioner is shown to have a similar block structure to an existing preconditioner for the material form transport of potential temperature on the Charney-Phillips grid, and to be more efficient and stable than either this or a previous Helmholtz preconditioner for the flux form transport of density weighted potential temperature on the Lorenz grid for a one dimensional thermal bubble configuration. The new preconditioner is further verified against standard two dimensional test cases in a vertical slice geometry.","sentences":["Implicit solvers for atmospheric models are often accelerated via the solution of a preconditioned system.","For block preconditioners this typically involves the factorisation of the (approximate) Jacobian for the coupled system into a Helmholtz equation for some function of the pressure.","Here we present a preconditioner for the compressible Euler equations with a flux form representation of the potential temperature on the Lorenz grid using mixed finite elements.","This formulation allows for spatial discretisations that conserve both energy and potential temperature variance.","By introducing the dry thermodynamic entropy as an auxiliary variable for the solution of the algebraic system, the resulting preconditioner is shown to have a similar block structure to an existing preconditioner for the material form transport of potential temperature on the Charney-Phillips grid, and to be more efficient and stable than either this or a previous Helmholtz preconditioner for the flux form transport of density weighted potential temperature on the Lorenz grid for a one dimensional thermal bubble configuration.","The new preconditioner is further verified against standard two dimensional test cases in a vertical slice geometry."],"url":"http://arxiv.org/abs/2403.04095v1","category":"math.NA"}
{"created":"2024-03-06 22:45:52","title":"Multiple scattering simulation via physics-informed neural networks","abstract":"This work presents a physics-driven machine learning framework for the simulation of acoustic scattering problems. The proposed framework relies on a physics-informed neural network (PINN) architecture that leverages prior knowledge based on the physics of the scattering problem as well as a tailored network structure that embodies the concept of the superposition principle of linear wave interaction. The framework can also simulate the scattered field due to rigid scatterers having arbitrary shape as well as high-frequency problems. Unlike conventional data-driven neural networks, the PINN is trained by directly enforcing the governing equations describing the underlying physics, hence without relying on any labeled training dataset. Remarkably, the network model has significantly lower discretization dependence and offers simulation capabilities akin to parallel computation. This feature is particularly beneficial to address computational challenges typically associated with conventional mesh-dependent simulation methods. The performance of the network is investigated via a comprehensive numerical study that explores different application scenarios based on acoustic scattering.","sentences":["This work presents a physics-driven machine learning framework for the simulation of acoustic scattering problems.","The proposed framework relies on a physics-informed neural network (PINN) architecture that leverages prior knowledge based on the physics of the scattering problem as well as a tailored network structure that embodies the concept of the superposition principle of linear wave interaction.","The framework can also simulate the scattered field due to rigid scatterers having arbitrary shape as well as high-frequency problems.","Unlike conventional data-driven neural networks, the PINN is trained by directly enforcing the governing equations describing the underlying physics, hence without relying on any labeled training dataset.","Remarkably, the network model has significantly lower discretization dependence and offers simulation capabilities akin to parallel computation.","This feature is particularly beneficial to address computational challenges typically associated with conventional mesh-dependent simulation methods.","The performance of the network is investigated via a comprehensive numerical study that explores different application scenarios based on acoustic scattering."],"url":"http://arxiv.org/abs/2403.04094v1","category":"physics.comp-ph"}
{"created":"2024-03-06 22:34:01","title":"A family of K\u00e4hler flying wing steady Ricci solitons","abstract":"In $1996$, H.-D. Cao constructed a $U(n)$-invariant steady gradient K\\\"ahler-Ricci soliton on $\\mathbb{C}^{n}$ and asked whether every steady gradient K\\\"ahler-Ricci soliton of positive curvature on $\\mathbb{C}^{n}$ is necessarily $U(n)$-invariant (and hence unique up to scaling). Recently, Apostolov-Cifarelli answered this question in the negative for $n=2$. Here, we construct a family of $U(1)\\times U(n-1)$-invariant, but not $U(n)$-invariant, complete steady gradient K\\\"ahler-Ricci solitons with strictly positive curvature operator on real $(1,\\,1)$-forms (in particular, with strictly positive sectional curvature) on $\\mathbb{C}^{n}$ for $n\\geq3$, thereby answering Cao's question in the negative for $n\\geq3$. This family of steady Ricci solitons interpolates between Cao's $U(n)$-invariant steady K\\\"ahler-Ricci soliton and the product of the cigar soliton and Cao's $U(n-1)$-invariant steady K\\\"ahler-Ricci soliton. This provides the K\\\"ahler analog of the Riemannian flying wings construction of Lai. In the process of the proof, we also demonstrate that the almost diameter rigidity of $\\mathbb{P}^{n}$ endowed with the Fubini-Study metric does not hold even if the curvature operator is bounded below by $2$ on real $(1,\\,1)$-forms.","sentences":["In $1996$, H.-D. Cao constructed a $U(n)$-invariant steady gradient K\\\"ahler-Ricci soliton on $\\mathbb{C}^{n}$ and asked whether every steady gradient K\\\"ahler-Ricci soliton of positive curvature on $\\mathbb{C}^{n}$ is necessarily $U(n)$-invariant (and hence unique up to scaling).","Recently, Apostolov-Cifarelli answered this question in the negative for $n=2$. Here, we construct a family of $U(1)\\times U(n-1)$-invariant, but not $U(n)$-invariant, complete steady gradient K\\\"ahler-Ricci solitons with strictly positive curvature operator on real $(1,\\,1)$-forms (in particular, with strictly positive sectional curvature) on $\\mathbb{C}^{n}$ for $n\\geq3$, thereby answering Cao's question in the negative for $n\\geq3$. This family of steady Ricci solitons interpolates between Cao's $U(n)$-invariant steady K\\\"ahler-Ricci soliton and the product of the cigar soliton and Cao's $U(n-1)$-invariant steady K\\\"ahler-Ricci soliton.","This provides the K\\\"ahler analog of the Riemannian flying wings construction of Lai.","In the process of the proof, we also demonstrate that the almost diameter rigidity of $\\mathbb{P}^{n}$ endowed with the Fubini-Study metric does not hold even if the curvature operator is bounded below by $2$ on real $(1,\\,1)$-forms."],"url":"http://arxiv.org/abs/2403.04089v1","category":"math.DG"}
{"created":"2024-03-06 20:48:36","title":"Some metric geometry of the icosahedron","abstract":"This paper constructs a Riemann surface associated to the icosahedron and discusses the geodesics associated to a flat metric on this surface. Because of the icosahedral symmetry, this is a distinguished special case of the example treated in [2].","sentences":["This paper constructs a Riemann surface associated to the icosahedron and discusses the geodesics associated to a flat metric on this surface.","Because of the icosahedral symmetry, this is a distinguished special case of the example treated in [2]."],"url":"http://arxiv.org/abs/2403.04049v1","category":"math.DG"}
{"created":"2024-03-06 20:46:34","title":"Radial-angular coupling in self phase modulation with structured light","abstract":"In this work we study the evolution of an optical vortex undergoing self phase modulation inside a nonlinear Kerr medium. The intensity dependent phase evolution couples the angular and radial degrees of freedom of the input vortex, giving rise to a rich dynamics where new radial modes are created. In the short propagation range, this dynamics is well described by a perturbative approach, predicting the generation of modes with radial numbers between zero and the absolute value of the vortex topological charge. This prediction is confirmed by numerical simulations of the nonlinear propagation equation.","sentences":["In this work we study the evolution of an optical vortex undergoing self phase modulation inside a nonlinear Kerr medium.","The intensity dependent phase evolution couples the angular and radial degrees of freedom of the input vortex, giving rise to a rich dynamics where new radial modes are created.","In the short propagation range, this dynamics is well described by a perturbative approach, predicting the generation of modes with radial numbers between zero and the absolute value of the vortex topological charge.","This prediction is confirmed by numerical simulations of the nonlinear propagation equation."],"url":"http://arxiv.org/abs/2403.04048v1","category":"physics.optics"}
{"created":"2024-03-06 20:24:51","title":"Sobolev regularity of compactified 3-manifolds and the ADM Center of Mass","abstract":"In this paper, we address the existence of preferred asymptotic coordinates on asymptotically Euclidean (AE) manifolds $(M^3,g)$ such that $g$ admits an asymptotically Schwarzschildian first order expansion, based purely on a priori geometric conditions, which will then be used to establish geometric criteria guaranteeing the convergence of the ADM center of mass (COM). This question is analysed relating it to the study of the regularity of conformal compactifications of such manifolds, which is itself explored via elliptic theory for operators with coefficients of very limited regularity. With these related problems in mind, we shall first establish regularity properties of $L^{q'}$-solutions associated to the operator $\\Delta_{\\hat{g}}:W^{k,p}(S_2\\hat{M})\\to W^{k-2,p}(S_2\\hat{M})$, for $k=1,2$ and $1<p\\leq q$, where $\\hat{M}^n$ is a closed manifold, $\\hat{g}\\in W^{2,q}(\\hat{M})$, with $q>\\frac{n}{2}$ and $S_2\\hat{M}$ denotes the bundle of symmetric $(0,2)$-tensor fields. Appealing to these results, we establish Sobolev regularity of conformally compactified AE 3-manifolds via the decay of the Cotton tensor, improving on previous results. This allows us to construct preferred asymptotic coordinates on such AE manifolds where the metric has a first order Schwarzschildian expansion, which in turn will allow us to address a version of a conjecture posed by C. Cederbaum and A. Sakovich concerning the convergence of the COM of such manifolds.","sentences":["In this paper, we address the existence of preferred asymptotic coordinates on asymptotically Euclidean (AE) manifolds $(M^3,g)$ such that $g$ admits an asymptotically Schwarzschildian first order expansion, based purely on a priori geometric conditions, which will then be used to establish geometric criteria guaranteeing the convergence of the ADM center of mass (COM).","This question is analysed relating it to the study of the regularity of conformal compactifications of such manifolds, which is itself explored via elliptic theory for operators with coefficients of very limited regularity.","With these related problems in mind, we shall first establish regularity properties of $L^{q'}$-solutions associated to the operator $\\Delta_{\\hat{g}}:W^{k,p}(S_2\\hat{M})\\to W^{k-2,p}(S_2\\hat{M})$, for $k=1,2$ and $1<p\\leq q$, where $\\hat{M}^n$ is a closed manifold, $\\hat{g}\\in W^{2,q}(\\hat{M})$, with $q>\\frac{n}{2}$ and $S_2\\hat{M}$ denotes the bundle of symmetric $(0,2)$-tensor fields.","Appealing to these results, we establish Sobolev regularity of conformally compactified AE 3-manifolds via the decay of the Cotton tensor, improving on previous results.","This allows us to construct preferred asymptotic coordinates on such AE manifolds where the metric has a first order Schwarzschildian expansion, which in turn will allow us to address a version of a conjecture posed by C. Cederbaum and A. Sakovich concerning the convergence of the COM of such manifolds."],"url":"http://arxiv.org/abs/2403.04034v1","category":"math.DG"}
{"created":"2024-03-06 20:09:04","title":"The Bulk Properties of Isolated Neutron Stars Inferred from the Gravitational Redshift Measurements","abstract":"The measurements of the bulk properties of most isolated neutron stars (INSs) are challenging tasks. Tang et al. (2020) have developed a new method, based on the equation of state (EoS) of neutron star (NS) material constrained by the observational data, to infer the gravitational masses of a few INSs whose gravitational redshifts are available. However, in that work, the authors only considered the constraints on the EoS from nuclear experiments/theories and the gravitational wave data of GW170817; the possible phase transition has not been taken into account. In this work, we adopt three EoS models (including the one incorporates a first-order strong phase transition) that are constrained by the latest multimessenger NS data, including in particular the recent mass\\textendash radius measurements of two NSs by Neutron Star Interior Composition Explorer, to update the estimation of the gravitational masses of RBS 1223, RX J0720.4-3125, and RX J1856.5-3754. In comparison to our previous approach, the new constraints are tighter, and the gravitational masses are larger by about $0.1M_\\odot$. All the inferred gravitational masses are within the range of the NS masses measured in other ways. We have also calculated the radius, tidal-deformability, and moment of inertia of these sources. The inclusion of the first-order strong phase transition has little influence on modifying the results.","sentences":["The measurements of the bulk properties of most isolated neutron stars (INSs) are challenging tasks.","Tang et al. (2020) have developed a new method, based on the equation of state (EoS) of neutron star (NS) material constrained by the observational data, to infer the gravitational masses of a few INSs whose gravitational redshifts are available.","However, in that work, the authors only considered the constraints on the EoS from nuclear experiments/theories and the gravitational wave data of GW170817; the possible phase transition has not been taken into account.","In this work, we adopt three EoS models (including the one incorporates a first-order strong phase transition) that are constrained by the latest multimessenger NS data, including in particular the recent mass\\textendash radius measurements of two NSs by Neutron Star Interior Composition Explorer, to update the estimation of the gravitational masses of RBS 1223, RX J0720.4-3125, and RX J1856.5-3754.","In comparison to our previous approach, the new constraints are tighter, and the gravitational masses are larger by about $0.1M_\\odot$. All the inferred gravitational masses are within the range of the NS masses measured in other ways.","We have also calculated the radius, tidal-deformability, and moment of inertia of these sources.","The inclusion of the first-order strong phase transition has little influence on modifying the results."],"url":"http://arxiv.org/abs/2403.04023v1","category":"astro-ph.HE"}
{"created":"2024-03-06 19:58:28","title":"Stability Analysis of Feedback Systems with ReLU Nonlinearities via Semialgebraic Set Representation","abstract":"This paper is concerned with the stability analysis problem of feedback systems with rectified linear unit (ReLU) nonlinearities. Such feedback systems arise when we model dynamical (recurrent) neural networks (NNs) and NN-driven control systems where all the activation functions of NNs are ReLUs. In this study, we focus on the semialgebraic set representation characterizing the input-output properties of ReLUs. This allows us to employ a novel copositive multiplier in the framework of the integral quadratic constraint and, thus, to derive a linear matrix inequality (LMI) condition for the stability analysis of the feedback systems. However, the infeasibility of this LMI does not allow us to obtain any conclusion on the system's stability due to its conservativeness. This motivates us to consider its dual LMI. By investigating the structure of the dual solution, we derive a rank condition on the dual variable certificating that the system at hand is never stable. In addition, we construct a hierarchy of dual LMIs allowing for improved instability detection. We illustrate the effectiveness of the proposed approach by several numerical examples.","sentences":["This paper is concerned with the stability analysis problem of feedback systems with rectified linear unit (ReLU) nonlinearities.","Such feedback systems arise when we model dynamical (recurrent) neural networks (NNs) and NN-driven control systems where all the activation functions of NNs are ReLUs.","In this study, we focus on the semialgebraic set representation characterizing the input-output properties of ReLUs.","This allows us to employ a novel copositive multiplier in the framework of the integral quadratic constraint and, thus, to derive a linear matrix inequality (LMI) condition for the stability analysis of the feedback systems.","However, the infeasibility of this LMI does not allow us to obtain any conclusion on the system's stability due to its conservativeness.","This motivates us to consider its dual LMI.","By investigating the structure of the dual solution, we derive a rank condition on the dual variable certificating that the system at hand is never stable.","In addition, we construct a hierarchy of dual LMIs allowing for improved instability detection.","We illustrate the effectiveness of the proposed approach by several numerical examples."],"url":"http://arxiv.org/abs/2403.04016v1","category":"math.OC"}
{"created":"2024-03-06 19:46:26","title":"Expanding CGL: The CGL double-adiabatic approximation in the Expanding Solar Wind","abstract":"Different in situ satellite observations within 0.3 to 1 AU from the Sun reveal deviations in the thermodynamics of solar wind expansion. Specifically, these deviations challenge the applicability of the double adiabatic or CGL theory, indicating potential influences such as perpendicular heating and/or parallel cooling of ions. The study aims to investigate the plasma expansion phenomena using the Expanding Box Model (EBM) coupled with an ideal MHD description of the plasma. The primary objective is to understand the observed deviations from the CGL predictions, and how the expansion can affect the conservation of the adiabatic invariants, particularly focusing on the impact of transverse expansion on the CGL equations. To address the plasma expansion, we employed the Expanding Box Model (EBM) coupled with the ideal-MHD formalism used for CGL theory. This model provides a unique system of reference co-moving with the solar wind, allowing for the incorporation of transverse expansion into the double adiabatic equations. Solving the equations for different magnetic field profiles, we compute the evolution of anisotropy and plasma beta, which deviates from CGL predictions and empirical observations. This deviation is attributed to the plasma cooling effect induced by the Expanding Box Model (EBM). Results suggest that heating mechanisms play a crucial role in counteracting plasma cooling during expansion.","sentences":["Different in situ satellite observations within 0.3 to 1 AU from the Sun reveal deviations in the thermodynamics of solar wind expansion.","Specifically, these deviations challenge the applicability of the double adiabatic or CGL theory, indicating potential influences such as perpendicular heating and/or parallel cooling of ions.","The study aims to investigate the plasma expansion phenomena using the Expanding Box Model (EBM) coupled with an ideal MHD description of the plasma.","The primary objective is to understand the observed deviations from the CGL predictions, and how the expansion can affect the conservation of the adiabatic invariants, particularly focusing on the impact of transverse expansion on the CGL equations.","To address the plasma expansion, we employed the Expanding Box Model (EBM) coupled with the ideal-MHD formalism used for CGL theory.","This model provides a unique system of reference co-moving with the solar wind, allowing for the incorporation of transverse expansion into the double adiabatic equations.","Solving the equations for different magnetic field profiles, we compute the evolution of anisotropy and plasma beta, which deviates from CGL predictions and empirical observations.","This deviation is attributed to the plasma cooling effect induced by the Expanding Box Model (EBM).","Results suggest that heating mechanisms play a crucial role in counteracting plasma cooling during expansion."],"url":"http://arxiv.org/abs/2403.04011v1","category":"physics.plasm-ph"}
{"created":"2024-03-06 19:42:34","title":"Three Revisits to Node-Level Graph Anomaly Detection: Outliers, Message Passing and Hyperbolic Neural Networks","abstract":"Graph anomaly detection plays a vital role for identifying abnormal instances in complex networks. Despite advancements of methodology based on deep learning in recent years, existing benchmarking approaches exhibit limitations that hinder a comprehensive comparison. In this paper, we revisit datasets and approaches for unsupervised node-level graph anomaly detection tasks from three aspects. Firstly, we introduce outlier injection methods that create more diverse and graph-based anomalies in graph datasets. Secondly, we compare methods employing message passing against those without, uncovering the unexpected decline in performance associated with message passing. Thirdly, we explore the use of hyperbolic neural networks, specifying crucial architecture and loss design that contribute to enhanced performance. Through rigorous experiments and evaluations, our study sheds light on general strategies for improving node-level graph anomaly detection methods.","sentences":["Graph anomaly detection plays a vital role for identifying abnormal instances in complex networks.","Despite advancements of methodology based on deep learning in recent years, existing benchmarking approaches exhibit limitations that hinder a comprehensive comparison.","In this paper, we revisit datasets and approaches for unsupervised node-level graph anomaly detection tasks from three aspects.","Firstly, we introduce outlier injection methods that create more diverse and graph-based anomalies in graph datasets.","Secondly, we compare methods employing message passing against those without, uncovering the unexpected decline in performance associated with message passing.","Thirdly, we explore the use of hyperbolic neural networks, specifying crucial architecture and loss design that contribute to enhanced performance.","Through rigorous experiments and evaluations, our study sheds light on general strategies for improving node-level graph anomaly detection methods."],"url":"http://arxiv.org/abs/2403.04010v1","category":"cs.LG"}
{"created":"2024-03-06 19:24:06","title":"The Maslov index, degenerate crossings and the stability of pulse solutions to the Swift-Hohenberg equation","abstract":"In the scalar Swift-Hohenberg equation with quadratic-cubic nonlinearity, it is known that symmetric pulse solutions exist for certain parameter regions. In this paper we develop a method to determine the spectral stability of these solutions. We first associate a Maslov index to each solution and then argue that this index coincides with the number of unstable eigenvalues for the linearized evolution equation. This requires extending the method of computing the Maslov index introduced by Robbin and Salamon to so-called degenerate crossings. We extend their formulation of the Maslov index to degenerate crossings of general order. Furthermore, we develop a numerical method to compute the Maslov index associated to symmetric pulse solutions. Finally, we consider several solutions to the Swift-Hohenberg equation and use our method to characterize their stability.","sentences":["In the scalar Swift-Hohenberg equation with quadratic-cubic nonlinearity, it is known that symmetric pulse solutions exist for certain parameter regions.","In this paper we develop a method to determine the spectral stability of these solutions.","We first associate a Maslov index to each solution and then argue that this index coincides with the number of unstable eigenvalues for the linearized evolution equation.","This requires extending the method of computing the Maslov index introduced by Robbin and Salamon to so-called degenerate crossings.","We extend their formulation of the Maslov index to degenerate crossings of general order.","Furthermore, we develop a numerical method to compute the Maslov index associated to symmetric pulse solutions.","Finally, we consider several solutions to the Swift-Hohenberg equation and use our method to characterize their stability."],"url":"http://arxiv.org/abs/2403.04003v1","category":"math.AP"}
{"created":"2024-03-06 18:15:47","title":"Inverse resolution of spatially varying diffusion coefficient using Physics-Informed neural networks","abstract":"Resolving the diffusion coefficient is a key element in many biological and engineering systems, including pharmacological drug transport and fluid mechanics analyses. Additionally, these systems often have spatial variation in the diffusion coefficient which must be determined, such as for injectable drug-eluting implants into heterogeneous tissues. Unfortunately, obtaining the diffusion coefficient from images in such cases is an inverse problem with only discrete data points. The development of a robust method that can work with such noisy and ill-posed datasets to accurately determine spatially-varying diffusion coefficients is of great value across a large range of disciplines. Here, we developed an inverse solver that uses physics informed neural networks (PINNs) to calculate spatially-varying diffusion coefficients from numerical and experimental image data in varying biological and engineering applications. The residual of the transient diffusion equation for a concentration field is minimized to find the diffusion coefficient. The robustness of the method as an inverse solver was tested using both numerical and experimental datasets. The predictions show good agreement with both the numerical and experimental benchmarks; an error of less than 6.31% was obtained against all numerical benchmarks, while the diffusion coefficient calculated in experimental datasets matches the appropriate ranges of other reported literature values. Our work demonstrates the potential of using PINNs to resolve spatially-varying diffusion coefficients, which may aid a wide-range of applications, such as enabling better-designed drug-eluting implants for regenerative medicine or oncology fields.","sentences":["Resolving the diffusion coefficient is a key element in many biological and engineering systems, including pharmacological drug transport and fluid mechanics analyses.","Additionally, these systems often have spatial variation in the diffusion coefficient which must be determined, such as for injectable drug-eluting implants into heterogeneous tissues.","Unfortunately, obtaining the diffusion coefficient from images in such cases is an inverse problem with only discrete data points.","The development of a robust method that can work with such noisy and ill-posed datasets to accurately determine spatially-varying diffusion coefficients is of great value across a large range of disciplines.","Here, we developed an inverse solver that uses physics informed neural networks (PINNs) to calculate spatially-varying diffusion coefficients from numerical and experimental image data in varying biological and engineering applications.","The residual of the transient diffusion equation for a concentration field is minimized to find the diffusion coefficient.","The robustness of the method as an inverse solver was tested using both numerical and experimental datasets.","The predictions show good agreement with both the numerical and experimental benchmarks; an error of less than 6.31% was obtained against all numerical benchmarks, while the diffusion coefficient calculated in experimental datasets matches the appropriate ranges of other reported literature values.","Our work demonstrates the potential of using PINNs to resolve spatially-varying diffusion coefficients, which may aid a wide-range of applications, such as enabling better-designed drug-eluting implants for regenerative medicine or oncology fields."],"url":"http://arxiv.org/abs/2403.03970v1","category":"q-bio.QM"}
{"created":"2024-03-07 17:07:51","title":"Dynamic Cross Attention for Audio-Visual Person Verification","abstract":"Although person or identity verification has been predominantly explored using individual modalities such as face and voice, audio-visual fusion has recently shown immense potential to outperform unimodal approaches. Audio and visual modalities are often expected to pose strong complementary relationships, which plays a crucial role in effective audio-visual fusion. However, they may not always strongly complement each other, they may also exhibit weak complementary relationships, resulting in poor audio-visual feature representations. In this paper, we propose a Dynamic Cross-Attention (DCA) model that can dynamically select the cross-attended or unattended features on the fly based on the strong or weak complementary relationships, respectively, across audio and visual modalities. In particular, a conditional gating layer is designed to evaluate the contribution of the cross-attention mechanism and choose cross-attended features only when they exhibit strong complementary relationships, otherwise unattended features. Extensive experiments are conducted on the Voxceleb1 dataset to demonstrate the robustness of the proposed model. Results indicate that the proposed model consistently improves the performance on multiple variants of cross-attention while outperforming the state-of-the-art methods.","sentences":["Although person or identity verification has been predominantly explored using individual modalities such as face and voice, audio-visual fusion has recently shown immense potential to outperform unimodal approaches.","Audio and visual modalities are often expected to pose strong complementary relationships, which plays a crucial role in effective audio-visual fusion.","However, they may not always strongly complement each other, they may also exhibit weak complementary relationships, resulting in poor audio-visual feature representations.","In this paper, we propose a Dynamic Cross-Attention (DCA) model that can dynamically select the cross-attended or unattended features on the fly based on the strong or weak complementary relationships, respectively, across audio and visual modalities.","In particular, a conditional gating layer is designed to evaluate the contribution of the cross-attention mechanism and choose cross-attended features only when they exhibit strong complementary relationships, otherwise unattended features.","Extensive experiments are conducted on the Voxceleb1 dataset to demonstrate the robustness of the proposed model.","Results indicate that the proposed model consistently improves the performance on multiple variants of cross-attention while outperforming the state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.04661v1","category":"cs.CV"}
{"created":"2024-03-07 16:11:43","title":"MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder","abstract":"Within the domain of medical analysis, extensive research has explored the potential of mutual learning between Masked Autoencoders(MAEs) and multimodal data. However, the impact of MAEs on intermodality remains a key challenge. We introduce MedFLIP, a Fast Language-Image Pre-training method for Medical analysis. We explore MAEs for zero-shot learning with crossed domains, which enhances the model ability to learn from limited data, a common scenario in medical diagnostics. We verify that masking an image does not affect intermodal learning. Furthermore, we propose the SVD loss to enhance the representation learning for characteristics of medical images, aiming to improve classification accuracy by leveraging the structural intricacies of such data. Lastly, we validate using language will improve the zero-shot performance for the medical image analysis. MedFLIP scaling of the masking process marks an advancement in the field, offering a pathway to rapid and precise medical image analysis without the traditional computational bottlenecks. Through experiments and validation, MedFLIP demonstrates efficient performance improvements, setting an explored standard for future research and application in medical diagnostics.","sentences":["Within the domain of medical analysis, extensive research has explored the potential of mutual learning between Masked Autoencoders(MAEs) and multimodal data.","However, the impact of MAEs on intermodality remains a key challenge.","We introduce MedFLIP, a Fast Language-Image Pre-training method for Medical analysis.","We explore MAEs for zero-shot learning with crossed domains, which enhances the model ability to learn from limited data, a common scenario in medical diagnostics.","We verify that masking an image does not affect intermodal learning.","Furthermore, we propose the SVD loss to enhance the representation learning for characteristics of medical images, aiming to improve classification accuracy by leveraging the structural intricacies of such data.","Lastly, we validate using language will improve the zero-shot performance for the medical image analysis.","MedFLIP scaling of the masking process marks an advancement in the field, offering a pathway to rapid and precise medical image analysis without the traditional computational bottlenecks.","Through experiments and validation, MedFLIP demonstrates efficient performance improvements, setting an explored standard for future research and application in medical diagnostics."],"url":"http://arxiv.org/abs/2403.04626v1","category":"eess.IV"}
{"created":"2024-03-07 15:47:52","title":"Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation","abstract":"Recently, because of the high-quality representations of contrastive learning methods, rehearsal-based contrastive continual learning has been proposed to explore how to continually learn transferable representation embeddings to avoid the catastrophic forgetting issue in traditional continual settings. Based on this framework, we propose Contrastive Continual Learning via Importance Sampling (CCLIS) to preserve knowledge by recovering previous data distributions with a new strategy for Replay Buffer Selection (RBS), which minimize estimated variance to save hard negative samples for representation learning with high quality. Furthermore, we present the Prototype-instance Relation Distillation (PRD) loss, a technique designed to maintain the relationship between prototypes and sample representations using a self-distillation process. Experiments on standard continual learning benchmarks reveal that our method notably outperforms existing baselines in terms of knowledge preservation and thereby effectively counteracts catastrophic forgetting in online contexts. The code is available at https://github.com/lijy373/CCLIS.","sentences":["Recently, because of the high-quality representations of contrastive learning methods, rehearsal-based contrastive continual learning has been proposed to explore how to continually learn transferable representation embeddings to avoid the catastrophic forgetting issue in traditional continual settings.","Based on this framework, we propose Contrastive Continual Learning via Importance Sampling (CCLIS) to preserve knowledge by recovering previous data distributions with a new strategy for Replay Buffer Selection (RBS), which minimize estimated variance to save hard negative samples for representation learning with high quality.","Furthermore, we present the Prototype-instance Relation Distillation (PRD) loss, a technique designed to maintain the relationship between prototypes and sample representations using a self-distillation process.","Experiments on standard continual learning benchmarks reveal that our method notably outperforms existing baselines in terms of knowledge preservation and thereby effectively counteracts catastrophic forgetting in online contexts.","The code is available at https://github.com/lijy373/CCLIS."],"url":"http://arxiv.org/abs/2403.04599v1","category":"cs.LG"}
{"created":"2024-03-07 15:40:01","title":"A Detailed Audio-Text Data Simulation Pipeline using Single-Event Sounds","abstract":"Recently, there has been an increasing focus on audio-text cross-modal learning. However, most of the existing audio-text datasets contain only simple descriptions of sound events. Compared with classification labels, the advantages of such descriptions are significantly limited. In this paper, we first analyze the detailed information that human descriptions of audio may contain beyond sound event labels. Based on the analysis, we propose an automatic pipeline for curating audio-text pairs with rich details. Leveraging the property that sounds can be mixed and concatenated in the time domain, we control details in four aspects: temporal relationship, loudness, speaker identity, and occurrence number, in simulating audio mixtures. Corresponding details are transformed into captions by large language models. Audio-text pairs with rich details in text descriptions are thereby obtained. We validate the effectiveness of our pipeline with a small amount of simulated data, demonstrating that the simulated data enables models to learn detailed audio captioning.","sentences":["Recently, there has been an increasing focus on audio-text cross-modal learning.","However, most of the existing audio-text datasets contain only simple descriptions of sound events.","Compared with classification labels, the advantages of such descriptions are significantly limited.","In this paper, we first analyze the detailed information that human descriptions of audio may contain beyond sound event labels.","Based on the analysis, we propose an automatic pipeline for curating audio-text pairs with rich details.","Leveraging the property that sounds can be mixed and concatenated in the time domain, we control details in four aspects: temporal relationship, loudness, speaker identity, and occurrence number, in simulating audio mixtures.","Corresponding details are transformed into captions by large language models.","Audio-text pairs with rich details in text descriptions are thereby obtained.","We validate the effectiveness of our pipeline with a small amount of simulated data, demonstrating that the simulated data enables models to learn detailed audio captioning."],"url":"http://arxiv.org/abs/2403.04594v1","category":"cs.SD"}
{"created":"2024-03-07 15:26:23","title":"Beyond Major Product Prediction: Reproducing Reaction Mechanisms with Machine Learning Models Trained on a Large-Scale Mechanistic Dataset","abstract":"Mechanistic understanding of organic reactions can facilitate reaction development, impurity prediction, and in principle, reaction discovery. While several machine learning models have sought to address the task of predicting reaction products, their extension to predicting reaction mechanisms has been impeded by the lack of a corresponding mechanistic dataset. In this study, we construct such a dataset by imputing intermediates between experimentally reported reactants and products using expert reaction templates and train several machine learning models on the resulting dataset of 5,184,184 elementary steps. We explore the performance and capabilities of these models, focusing on their ability to predict reaction pathways and recapitulate the roles of catalysts and reagents. Additionally, we demonstrate the potential of mechanistic models in predicting impurities, often overlooked by conventional models. We conclude by evaluating the generalizability of mechanistic models to new reaction types, revealing challenges related to dataset diversity, consecutive predictions, and violations of atom conservation.","sentences":["Mechanistic understanding of organic reactions can facilitate reaction development, impurity prediction, and in principle, reaction discovery.","While several machine learning models have sought to address the task of predicting reaction products, their extension to predicting reaction mechanisms has been impeded by the lack of a corresponding mechanistic dataset.","In this study, we construct such a dataset by imputing intermediates between experimentally reported reactants and products using expert reaction templates and train several machine learning models on the resulting dataset of 5,184,184 elementary steps.","We explore the performance and capabilities of these models, focusing on their ability to predict reaction pathways and recapitulate the roles of catalysts and reagents.","Additionally, we demonstrate the potential of mechanistic models in predicting impurities, often overlooked by conventional models.","We conclude by evaluating the generalizability of mechanistic models to new reaction types, revealing challenges related to dataset diversity, consecutive predictions, and violations of atom conservation."],"url":"http://arxiv.org/abs/2403.04580v1","category":"cs.LG"}
{"created":"2024-03-07 15:03:50","title":"Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit Feedback and Unknown Transition","abstract":"We study reinforcement learning with linear function approximation, unknown transition, and adversarial losses in the bandit feedback setting. Specifically, we focus on linear mixture MDPs whose transition kernel is a linear mixture model. We propose a new algorithm that attains an $\\widetilde{O}(d\\sqrt{HS^3K} + \\sqrt{HSAK})$ regret with high probability, where $d$ is the dimension of feature mappings, $S$ is the size of state space, $A$ is the size of action space, $H$ is the episode length and $K$ is the number of episodes. Our result strictly improves the previous best-known $\\widetilde{O}(dS^2 \\sqrt{K} + \\sqrt{HSAK})$ result in Zhao et al. (2023a) since $H \\leq S$ holds by the layered MDP structure. Our advancements are primarily attributed to (i) a new least square estimator for the transition parameter that leverages the visit information of all states, as opposed to only one state in prior work, and (ii) a new self-normalized concentration tailored specifically to handle non-independent noises, originally proposed in the dynamic assortment area and firstly applied in reinforcement learning to handle correlations between different states.","sentences":["We study reinforcement learning with linear function approximation, unknown transition, and adversarial losses in the bandit feedback setting.","Specifically, we focus on linear mixture MDPs whose transition kernel is a linear mixture model.","We propose a new algorithm that attains an $\\widetilde{O}(d\\sqrt{HS^3K} + \\sqrt{HSAK})$ regret with high probability, where $d$ is the dimension of feature mappings, $S$ is the size of state space, $A$ is the size of action space, $H$ is the episode length and $K$ is the number of episodes.","Our result strictly improves the previous best-known $\\widetilde{O}(dS^2 \\sqrt{K} + \\sqrt{HSAK})$ result in Zhao et al. (2023a) since $H \\leq S$ holds by the layered MDP structure.","Our advancements are primarily attributed to (i) a new least square estimator for the transition parameter that leverages the visit information of all states, as opposed to only one state in prior work, and (ii) a new self-normalized concentration tailored specifically to handle non-independent noises, originally proposed in the dynamic assortment area and firstly applied in reinforcement learning to handle correlations between different states."],"url":"http://arxiv.org/abs/2403.04568v1","category":"cs.LG"}
{"created":"2024-03-07 14:45:03","title":"Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness Characterization Methods for Data-Centric AI","abstract":"Characterizing samples that are difficult to learn from is crucial to developing highly performant ML models. This has led to numerous Hardness Characterization Methods (HCMs) that aim to identify \"hard\" samples. However, there is a lack of consensus regarding the definition and evaluation of \"hardness\". Unfortunately, current HCMs have only been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance, overlooking the fundamental quantitative identification task. We address this gap by presenting a fine-grained taxonomy of hardness types. Additionally, we propose the Hardness Characterization Analysis Toolkit (H-CAT), which supports comprehensive and quantitative benchmarking of HCMs across the hardness taxonomy and can easily be extended to new HCMs, hardness types, and datasets. We use H-CAT to evaluate 13 different HCMs across 8 hardness types. This comprehensive evaluation encompassing over 14K setups uncovers strengths and weaknesses of different HCMs, leading to practical tips to guide HCM selection and future development. Our findings highlight the need for more comprehensive HCM evaluation, while we hope our hardness taxonomy and toolkit will advance the principled evaluation and uptake of data-centric AI methods.","sentences":["Characterizing samples that are difficult to learn from is crucial to developing highly performant ML models.","This has led to numerous Hardness Characterization Methods (HCMs) that aim to identify \"hard\" samples.","However, there is a lack of consensus regarding the definition and evaluation of \"hardness\".","Unfortunately, current HCMs have only been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance, overlooking the fundamental quantitative identification task.","We address this gap by presenting a fine-grained taxonomy of hardness types.","Additionally, we propose the Hardness Characterization Analysis Toolkit (H-CAT), which supports comprehensive and quantitative benchmarking of HCMs across the hardness taxonomy and can easily be extended to new HCMs, hardness types, and datasets.","We use H-CAT to evaluate 13 different HCMs across 8 hardness types.","This comprehensive evaluation encompassing over 14K setups uncovers strengths and weaknesses of different HCMs, leading to practical tips to guide HCM selection and future development.","Our findings highlight the need for more comprehensive HCM evaluation, while we hope our hardness taxonomy and toolkit will advance the principled evaluation and uptake of data-centric AI methods."],"url":"http://arxiv.org/abs/2403.04551v1","category":"cs.LG"}
{"created":"2024-03-07 14:42:33","title":"Architectural Blueprint For Heterogeneity-Resilient Federated Learning","abstract":"This paper proposes a novel three tier architecture for federated learning to optimize edge computing environments. The proposed architecture addresses the challenges associated with client data heterogeneity and computational constraints. It introduces a scalable, privacy preserving framework that enhances the efficiency of distributed machine learning. Through experimentation, the paper demonstrates the architecture capability to manage non IID data sets more effectively than traditional federated learning models. Additionally, the paper highlights the potential of this innovative approach to significantly improve model accuracy, reduce communication overhead, and facilitate broader adoption of federated learning technologies.","sentences":["This paper proposes a novel three tier architecture for federated learning to optimize edge computing environments.","The proposed architecture addresses the challenges associated with client data heterogeneity and computational constraints.","It introduces a scalable, privacy preserving framework that enhances the efficiency of distributed machine learning.","Through experimentation, the paper demonstrates the architecture capability to manage non IID data sets more effectively than traditional federated learning models.","Additionally, the paper highlights the potential of this innovative approach to significantly improve model accuracy, reduce communication overhead, and facilitate broader adoption of federated learning technologies."],"url":"http://arxiv.org/abs/2403.04546v1","category":"cs.LG"}
{"created":"2024-03-07 13:22:53","title":"Improved Focus on Hard Samples for Lung Nodule Detection","abstract":"Recently, lung nodule detection methods based on deep learning have shown excellent performance in the medical image processing field. Considering that only a few public lung datasets are available and lung nodules are more difficult to detect in CT images than in natural images, the existing methods face many bottlenecks when detecting lung nodules, especially hard ones in CT images. In order to solve these problems, we plan to enhance the focus of our network. In this work, we present an improved detection network that pays more attention to hard samples and datasets to deal with lung nodules by introducing deformable convolution and self-paced learning. Experiments on the LUNA16 dataset demonstrate the effectiveness of our proposed components and show that our method has reached competitive performance.","sentences":["Recently, lung nodule detection methods based on deep learning have shown excellent performance in the medical image processing field.","Considering that only a few public lung datasets are available and lung nodules are more difficult to detect in CT images than in natural images, the existing methods face many bottlenecks when detecting lung nodules, especially hard ones in CT images.","In order to solve these problems, we plan to enhance the focus of our network.","In this work, we present an improved detection network that pays more attention to hard samples and datasets to deal with lung nodules by introducing deformable convolution and self-paced learning.","Experiments on the LUNA16 dataset demonstrate the effectiveness of our proposed components and show that our method has reached competitive performance."],"url":"http://arxiv.org/abs/2403.04478v1","category":"eess.IV"}
{"created":"2024-03-07 13:22:25","title":"Hyperparameter Tuning MLPs for Probabilistic Time Series Forecasting","abstract":"Time series forecasting attempts to predict future events by analyzing past trends and patterns. Although well researched, certain critical aspects pertaining to the use of deep learning in time series forecasting remain ambiguous. Our research primarily focuses on examining the impact of specific hyperparameters related to time series, such as context length and validation strategy, on the performance of the state-of-the-art MLP model in time series forecasting. We have conducted a comprehensive series of experiments involving 4800 configurations per dataset across 20 time series forecasting datasets, and our findings demonstrate the importance of tuning these parameters. Furthermore, in this work, we introduce the largest metadataset for timeseries forecasting to date, named TSBench, comprising 97200 evaluations, which is a twentyfold increase compared to previous works in the field. Finally, we demonstrate the utility of the created metadataset on multi-fidelity hyperparameter optimization tasks.","sentences":["Time series forecasting attempts to predict future events by analyzing past trends and patterns.","Although well researched, certain critical aspects pertaining to the use of deep learning in time series forecasting remain ambiguous.","Our research primarily focuses on examining the impact of specific hyperparameters related to time series, such as context length and validation strategy, on the performance of the state-of-the-art MLP model in time series forecasting.","We have conducted a comprehensive series of experiments involving 4800 configurations per dataset across 20 time series forecasting datasets, and our findings demonstrate the importance of tuning these parameters.","Furthermore, in this work, we introduce the largest metadataset for timeseries forecasting to date, named TSBench, comprising 97200 evaluations, which is a twentyfold increase compared to previous works in the field.","Finally, we demonstrate the utility of the created metadataset on multi-fidelity hyperparameter optimization tasks."],"url":"http://arxiv.org/abs/2403.04477v1","category":"cs.LG"}
{"created":"2024-03-07 12:20:13","title":"Disentangled Diffusion-Based 3D Human Pose Estimation with Hierarchical Spatial and Temporal Denoiser","abstract":"Recently, diffusion-based methods for monocular 3D human pose estimation have achieved state-of-the-art (SOTA) performance by directly regressing the 3D joint coordinates from the 2D pose sequence. Although some methods decompose the task into bone length and bone direction prediction based on the human anatomical skeleton to explicitly incorporate more human body prior constraints, the performance of these methods is significantly lower than that of the SOTA diffusion-based methods. This can be attributed to the tree structure of the human skeleton. Direct application of the disentangled method could amplify the accumulation of hierarchical errors, propagating through each hierarchy. Meanwhile, the hierarchical information has not been fully explored by the previous methods. To address these problems, a Disentangled Diffusion-based 3D Human Pose Estimation method with Hierarchical Spatial and Temporal Denoiser is proposed, termed DDHPose. In our approach: (1) We disentangle the 3D pose and diffuse the bone length and bone direction during the forward process of the diffusion model to effectively model the human pose prior. A disentanglement loss is proposed to supervise diffusion model learning. (2) For the reverse process, we propose Hierarchical Spatial and Temporal Denoiser (HSTDenoiser) to improve the hierarchical modeling of each joint. Our HSTDenoiser comprises two components: the Hierarchical-Related Spatial Transformer (HRST) and the Hierarchical-Related Temporal Transformer (HRTT). HRST exploits joint spatial information and the influence of the parent joint on each joint for spatial modeling, while HRTT utilizes information from both the joint and its hierarchical adjacent joints to explore the hierarchical temporal correlations among joints.","sentences":["Recently, diffusion-based methods for monocular 3D human pose estimation have achieved state-of-the-art (SOTA) performance by directly regressing the 3D joint coordinates from the 2D pose sequence.","Although some methods decompose the task into bone length and bone direction prediction based on the human anatomical skeleton to explicitly incorporate more human body prior constraints, the performance of these methods is significantly lower than that of the SOTA diffusion-based methods.","This can be attributed to the tree structure of the human skeleton.","Direct application of the disentangled method could amplify the accumulation of hierarchical errors, propagating through each hierarchy.","Meanwhile, the hierarchical information has not been fully explored by the previous methods.","To address these problems, a Disentangled Diffusion-based 3D Human Pose Estimation method with Hierarchical Spatial and Temporal Denoiser is proposed, termed DDHPose.","In our approach: (1) We disentangle the 3D pose and diffuse the bone length and bone direction during the forward process of the diffusion model to effectively model the human pose prior.","A disentanglement loss is proposed to supervise diffusion model learning.","(2) For the reverse process, we propose Hierarchical Spatial and Temporal Denoiser (HSTDenoiser) to improve the hierarchical modeling of each joint.","Our HSTDenoiser comprises two components: the Hierarchical-Related Spatial Transformer (HRST) and the Hierarchical-Related Temporal Transformer (HRTT).","HRST exploits joint spatial information and the influence of the parent joint on each joint for spatial modeling, while HRTT utilizes information from both the joint and its hierarchical adjacent joints to explore the hierarchical temporal correlations among joints."],"url":"http://arxiv.org/abs/2403.04444v1","category":"cs.CV"}
{"created":"2024-03-07 11:12:35","title":"Collaborative Cybersecurity Using Blockchain: A Survey","abstract":"Collaborative cybersecurity relies on organizations sharing information to boost security, but trust management is a key concern. Decentralized solutions like distributed ledgers, particularly blockchain, are crucial for eliminating single points of failure. However, the existing literature on blockchain-based collaborative cybersecurity is limited, lacking comprehensive insights. This paper addresses this gap by surveying blockchain's role in collaborative cybersecurity from 2016 to 2023. It explores various applications, trends, and the evolution of blockchain technology, focusing on access control, data validation policies, underlying tech, and consensus mechanisms. A key finding is the fragmentation of the field with no dominant research group or venue. Many recent projects poorly select consensus protocols for their blockchain. To aid researchers and practitioners, this paper offers guidelines for choosing the right blockchain for specific purposes and highlights open research areas and lessons learned from past blockchain applications in collaborative cybersecurity, encouraging further exploration in this field.","sentences":["Collaborative cybersecurity relies on organizations sharing information to boost security, but trust management is a key concern.","Decentralized solutions like distributed ledgers, particularly blockchain, are crucial for eliminating single points of failure.","However, the existing literature on blockchain-based collaborative cybersecurity is limited, lacking comprehensive insights.","This paper addresses this gap by surveying blockchain's role in collaborative cybersecurity from 2016 to 2023.","It explores various applications, trends, and the evolution of blockchain technology, focusing on access control, data validation policies, underlying tech, and consensus mechanisms.","A key finding is the fragmentation of the field with no dominant research group or venue.","Many recent projects poorly select consensus protocols for their blockchain.","To aid researchers and practitioners, this paper offers guidelines for choosing the right blockchain for specific purposes and highlights open research areas and lessons learned from past blockchain applications in collaborative cybersecurity, encouraging further exploration in this field."],"url":"http://arxiv.org/abs/2403.04410v1","category":"cs.CR"}
{"created":"2024-03-07 10:06:54","title":"Computational Modelling of Plurality and Definiteness in Chinese Noun Phrases","abstract":"Theoretical linguists have suggested that some languages (e.g., Chinese and Japanese) are \"cooler\" than other languages based on the observation that the intended meaning of phrases in these languages depends more on their contexts. As a result, many expressions in these languages are shortened, and their meaning is inferred from the context. In this paper, we focus on the omission of the plurality and definiteness markers in Chinese noun phrases (NPs) to investigate the predictability of their intended meaning given the contexts. To this end, we built a corpus of Chinese NPs, each of which is accompanied by its corresponding context, and by labels indicating its singularity/plurality and definiteness/indefiniteness. We carried out corpus assessments and analyses. The results suggest that Chinese speakers indeed drop plurality and definiteness markers very frequently. Building on the corpus, we train a bank of computational models using both classic machine learning models and state-of-the-art pre-trained language models to predict the plurality and definiteness of each NP. We report on the performance of these models and analyse their behaviours.","sentences":["Theoretical linguists have suggested that some languages (e.g., Chinese and Japanese) are \"cooler\" than other languages based on the observation that the intended meaning of phrases in these languages depends more on their contexts.","As a result, many expressions in these languages are shortened, and their meaning is inferred from the context.","In this paper, we focus on the omission of the plurality and definiteness markers in Chinese noun phrases (NPs) to investigate the predictability of their intended meaning given the contexts.","To this end, we built a corpus of Chinese NPs, each of which is accompanied by its corresponding context, and by labels indicating its singularity/plurality and definiteness/indefiniteness.","We carried out corpus assessments and analyses.","The results suggest that Chinese speakers indeed drop plurality and definiteness markers very frequently.","Building on the corpus, we train a bank of computational models using both classic machine learning models and state-of-the-art pre-trained language models to predict the plurality and definiteness of each NP.","We report on the performance of these models and analyse their behaviours."],"url":"http://arxiv.org/abs/2403.04376v1","category":"cs.CL"}
{"created":"2024-03-07 08:10:59","title":"LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking","abstract":"Deep learning models, particularly those based on transformers, often employ numerous stacked structures, which possess identical architectures and perform similar functions. While effective, this stacking paradigm leads to a substantial increase in the number of parameters, posing challenges for practical applications. In today's landscape of increasingly large models, stacking depth can even reach dozens, further exacerbating this issue. To mitigate this problem, we introduce LORS (LOw-rank Residual Structure). LORS allows stacked modules to share the majority of parameters, requiring a much smaller number of unique ones per module to match or even surpass the performance of using entirely distinct ones, thereby significantly reducing parameter usage. We validate our method by applying it to the stacked decoders of a query-based object detector, and conduct extensive experiments on the widely used MS COCO dataset. Experimental results demonstrate the effectiveness of our method, as even with a 70\\% reduction in the parameters of the decoder, our method still enables the model to achieve comparable or","sentences":["Deep learning models, particularly those based on transformers, often employ numerous stacked structures, which possess identical architectures and perform similar functions.","While effective, this stacking paradigm leads to a substantial increase in the number of parameters, posing challenges for practical applications.","In today's landscape of increasingly large models, stacking depth can even reach dozens, further exacerbating this issue.","To mitigate this problem, we introduce LORS (LOw-rank Residual Structure).","LORS allows stacked modules to share the majority of parameters, requiring a much smaller number of unique ones per module to match or even surpass the performance of using entirely distinct ones, thereby significantly reducing parameter usage.","We validate our method by applying it to the stacked decoders of a query-based object detector, and conduct extensive experiments on the widely used MS COCO dataset.","Experimental results demonstrate the effectiveness of our method, as even with a 70\\% reduction in the parameters of the decoder, our method still enables the model to achieve comparable or"],"url":"http://arxiv.org/abs/2403.04303v1","category":"cs.CV"}
{"created":"2024-03-07 06:49:37","title":"Can Small Language Models be Good Reasoners for Sequential Recommendation?","abstract":"Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a \"slim\" (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larger teacher model. The rationales generated by the teacher model are then utilized as labels to distill the downstream smaller student model (e.g., LLaMA2-7B). In this way, the student model acquires the step-by-step reasoning capabilities in recommendation tasks. We encode the generated rationales from the student model into a dense vector, which empowers recommendation in both ID-based and ID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of SLIM over state-of-the-art baselines, and further analysis showcasing its ability to generate meaningful recommendation reasoning at affordable costs.","sentences":["Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities.","However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs.","Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses.","Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems.","In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a \"slim\" (i.e., resource-efficient) manner.","We introduce CoT prompting based on user behavior sequences for the larger teacher model.","The rationales generated by the teacher model are then utilized as labels to distill the downstream smaller student model (e.g., LLaMA2-7B).","In this way, the student model acquires the step-by-step reasoning capabilities in recommendation tasks.","We encode the generated rationales from the student model into a dense vector, which empowers recommendation in both ID-based and ID-agnostic scenarios.","Extensive experiments demonstrate the effectiveness of SLIM over state-of-the-art baselines, and further analysis showcasing its ability to generate meaningful recommendation reasoning at affordable costs."],"url":"http://arxiv.org/abs/2403.04260v1","category":"cs.IR"}
{"created":"2024-03-07 06:47:45","title":"Decentralized and Equitable Optimal Transport","abstract":"This paper considers the decentralized (discrete) optimal transport (D-OT) problem. In this setting, a network of agents seeks to design a transportation plan jointly, where the cost function is the sum of privately held costs for each agent. We reformulate the D-OT problem as a constraint-coupled optimization problem and propose a single-loop decentralized algorithm with an iteration complexity of O(1/{\\epsilon}) that matches existing centralized first-order approaches. Moreover, we propose the decentralized equitable optimal transport (DE-OT) problem. In DE-OT, in addition to cooperatively designing a transportation plan that minimizes transportation costs, agents seek to ensure equity in their individual costs. The iteration complexity of the proposed method to solve DE-OT is also O(1/{\\epsilon}). This rate improves existing centralized algorithms, where the best iteration complexity obtained is O(1/{\\epsilon}^2).","sentences":["This paper considers the decentralized (discrete) optimal transport (D-OT) problem.","In this setting, a network of agents seeks to design a transportation plan jointly, where the cost function is the sum of privately held costs for each agent.","We reformulate the D-OT problem as a constraint-coupled optimization problem and propose a single-loop decentralized algorithm with an iteration complexity of O(1/{\\epsilon}) that matches existing centralized first-order approaches.","Moreover, we propose the decentralized equitable optimal transport (DE-OT) problem.","In DE-OT, in addition to cooperatively designing a transportation plan that minimizes transportation costs, agents seek to ensure equity in their individual costs.","The iteration complexity of the proposed method to solve DE-OT is also O(1/{\\epsilon}).","This rate improves existing centralized algorithms, where the best iteration complexity obtained is O(1/{\\epsilon}^2)."],"url":"http://arxiv.org/abs/2403.04259v1","category":"math.OC"}
{"created":"2024-03-07 06:40:01","title":"Towards Robustness Analysis of E-Commerce Ranking System","abstract":"Information retrieval (IR) is a pivotal component in various applications. Recent advances in machine learning (ML) have enabled the integration of ML algorithms into IR, particularly in ranking systems. While there is a plethora of research on the robustness of ML-based ranking systems, these studies largely neglect commercial e-commerce systems and fail to establish a connection between real-world and manipulated query relevance. In this paper, we present the first systematic measurement study on the robustness of e-commerce ranking systems. We define robustness as the consistency of ranking outcomes for semantically identical queries. To quantitatively analyze robustness, we propose a novel metric that considers both ranking position and item-specific information that are absent in existing metrics. Our large-scale measurement study with real-world data from e-commerce retailers reveals an open opportunity to measure and improve robustness since semantically identical queries often yield inconsistent ranking results. Based on our observations, we propose several solution directions to enhance robustness, such as the use of Large Language Models. Note that the issue of robustness discussed herein does not constitute an error or oversight. Rather, in scenarios where there exists a vast array of choices, it is feasible to present a multitude of products in various permutations, all of which could be equally appealing. However, this extensive selection may lead to customer confusion. As e-commerce retailers use various techniques to improve the quality of search results, we hope that this research offers valuable guidance for measuring the robustness of the ranking systems.","sentences":["Information retrieval (IR) is a pivotal component in various applications.","Recent advances in machine learning (ML) have enabled the integration of ML algorithms into IR, particularly in ranking systems.","While there is a plethora of research on the robustness of ML-based ranking systems, these studies largely neglect commercial e-commerce systems and fail to establish a connection between real-world and manipulated query relevance.","In this paper, we present the first systematic measurement study on the robustness of e-commerce ranking systems.","We define robustness as the consistency of ranking outcomes for semantically identical queries.","To quantitatively analyze robustness, we propose a novel metric that considers both ranking position and item-specific information that are absent in existing metrics.","Our large-scale measurement study with real-world data from e-commerce retailers reveals an open opportunity to measure and improve robustness since semantically identical queries often yield inconsistent ranking results.","Based on our observations, we propose several solution directions to enhance robustness, such as the use of Large Language Models.","Note that the issue of robustness discussed herein does not constitute an error or oversight.","Rather, in scenarios where there exists a vast array of choices, it is feasible to present a multitude of products in various permutations, all of which could be equally appealing.","However, this extensive selection may lead to customer confusion.","As e-commerce retailers use various techniques to improve the quality of search results, we hope that this research offers valuable guidance for measuring the robustness of the ranking systems."],"url":"http://arxiv.org/abs/2403.04257v1","category":"cs.IR"}
{"created":"2024-03-07 06:35:59","title":"Mastering Memory Tasks with World Models","abstract":"Current model-based reinforcement learning (MBRL) agents struggle with long-term dependencies. This limits their ability to effectively solve tasks involving extended time gaps between actions and outcomes, or tasks demanding the recalling of distant observations to inform current actions. To improve temporal coherence, we integrate a new family of state space models (SSMs) in world models of MBRL agents to present a new method, Recall to Imagine (R2I). This integration aims to enhance both long-term memory and long-horizon credit assignment. Through a diverse set of illustrative tasks, we systematically demonstrate that R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze. At the same time, it upholds comparable performance in classic RL tasks, such as Atari and DMC, suggesting the generality of our method. We also show that R2I is faster than the state-of-the-art MBRL method, DreamerV3, resulting in faster wall-time convergence.","sentences":["Current model-based reinforcement learning (MBRL) agents struggle with long-term dependencies.","This limits their ability to effectively solve tasks involving extended time gaps between actions and outcomes, or tasks demanding the recalling of distant observations to inform current actions.","To improve temporal coherence, we integrate a new family of state space models (SSMs) in world models of MBRL agents to present a new method, Recall to Imagine (R2I).","This integration aims to enhance both long-term memory and long-horizon credit assignment.","Through a diverse set of illustrative tasks, we systematically demonstrate that R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze.","At the same time, it upholds comparable performance in classic RL tasks, such as Atari and DMC, suggesting the generality of our method.","We also show that R2I is faster than the state-of-the-art MBRL method, DreamerV3, resulting in faster wall-time convergence."],"url":"http://arxiv.org/abs/2403.04253v1","category":"cs.LG"}
{"created":"2024-03-07 06:10:02","title":"UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities","abstract":"Entity Set Expansion (ESE) aims to identify new entities belonging to the same semantic class as a given set of seed entities. Traditional methods primarily relied on positive seed entities to represent a target semantic class, which poses challenge for the representation of ultra-fine-grained semantic classes. Ultra-fine-grained semantic classes are defined based on fine-grained semantic classes with more specific attribute constraints. Describing it with positive seed entities alone cause two issues: (i) Ambiguity among ultra-fine-grained semantic classes. (ii) Inability to define \"unwanted\" semantic. Due to these inherent shortcomings, previous methods struggle to address the ultra-fine-grained ESE (Ultra-ESE). To solve this issue, we first introduce negative seed entities in the inputs, which belong to the same fine-grained semantic class as the positive seed entities but differ in certain attributes. Negative seed entities eliminate the semantic ambiguity by contrast between positive and negative attributes. Meanwhile, it provide a straightforward way to express \"unwanted\". To assess model performance in Ultra-ESE, we constructed UltraWiki, the first large-scale dataset tailored for Ultra-ESE. UltraWiki encompasses 236 ultra-fine-grained semantic classes, where each query of them is represented with 3-5 positive and negative seed entities. A retrieval-based framework RetExpan and a generation-based framework GenExpan are proposed to comprehensively assess the efficacy of large language models from two different paradigms in Ultra-ESE. Moreover, we devised three strategies to enhance models' comprehension of ultra-fine-grained entities semantics: contrastive learning, retrieval augmentation, and chain-of-thought reasoning. Extensive experiments confirm the effectiveness of our proposed strategies and also reveal that there remains a large space for improvement in Ultra-ESE.","sentences":["Entity Set Expansion (ESE) aims to identify new entities belonging to the same semantic class as a given set of seed entities.","Traditional methods primarily relied on positive seed entities to represent a target semantic class, which poses challenge for the representation of ultra-fine-grained semantic classes.","Ultra-fine-grained semantic classes are defined based on fine-grained semantic classes with more specific attribute constraints.","Describing it with positive seed entities alone cause two issues: (i) Ambiguity among ultra-fine-grained semantic classes.","(ii) Inability to define \"unwanted\" semantic.","Due to these inherent shortcomings, previous methods struggle to address the ultra-fine-grained ESE (Ultra-ESE).","To solve this issue, we first introduce negative seed entities in the inputs, which belong to the same fine-grained semantic class as the positive seed entities but differ in certain attributes.","Negative seed entities eliminate the semantic ambiguity by contrast between positive and negative attributes.","Meanwhile, it provide a straightforward way to express \"unwanted\".","To assess model performance in Ultra-ESE, we constructed UltraWiki, the first large-scale dataset tailored for Ultra-ESE.","UltraWiki encompasses 236 ultra-fine-grained semantic classes, where each query of them is represented with 3-5 positive and negative seed entities.","A retrieval-based framework RetExpan and a generation-based framework GenExpan are proposed to comprehensively assess the efficacy of large language models from two different paradigms in Ultra-ESE.","Moreover, we devised three strategies to enhance models' comprehension of ultra-fine-grained entities semantics: contrastive learning, retrieval augmentation, and chain-of-thought reasoning.","Extensive experiments confirm the effectiveness of our proposed strategies and also reveal that there remains a large space for improvement in Ultra-ESE."],"url":"http://arxiv.org/abs/2403.04247v1","category":"cs.CL"}
{"created":"2024-03-07 05:38:56","title":"Regularized DeepIV with Model Selection","abstract":"In this paper, we study nonparametric estimation of instrumental variable (IV) regressions. While recent advancements in machine learning have introduced flexible methods for IV estimation, they often encounter one or more of the following limitations: (1) restricting the IV regression to be uniquely identified; (2) requiring minimax computation oracle, which is highly unstable in practice; (3) absence of model selection procedure. In this paper, we present the first method and analysis that can avoid all three limitations, while still enabling general function approximation. Specifically, we propose a minimax-oracle-free method called Regularized DeepIV (RDIV) regression that can converge to the least-norm IV solution. Our method consists of two stages: first, we learn the conditional distribution of covariates, and by utilizing the learned distribution, we learn the estimator by minimizing a Tikhonov-regularized loss function. We further show that our method allows model selection procedures that can achieve the oracle rates in the misspecified regime. When extended to an iterative estimator, our method matches the current state-of-the-art convergence rate. Our method is a Tikhonov regularized variant of the popular DeepIV method with a non-parametric MLE first-stage estimator, and our results provide the first rigorous guarantees for this empirically used method, showcasing the importance of regularization which was absent from the original work.","sentences":["In this paper, we study nonparametric estimation of instrumental variable (IV) regressions.","While recent advancements in machine learning have introduced flexible methods for IV estimation, they often encounter one or more of the following limitations: (1) restricting the IV regression to be uniquely identified; (2) requiring minimax computation oracle, which is highly unstable in practice; (3) absence of model selection procedure.","In this paper, we present the first method and analysis that can avoid all three limitations, while still enabling general function approximation.","Specifically, we propose a minimax-oracle-free method called Regularized DeepIV (RDIV) regression that can converge to the least-norm IV solution.","Our method consists of two stages: first, we learn the conditional distribution of covariates, and by utilizing the learned distribution, we learn the estimator by minimizing a Tikhonov-regularized loss function.","We further show that our method allows model selection procedures that can achieve the oracle rates in the misspecified regime.","When extended to an iterative estimator, our method matches the current state-of-the-art convergence rate.","Our method is a Tikhonov regularized variant of the popular DeepIV method with a non-parametric MLE first-stage estimator, and our results provide the first rigorous guarantees for this empirically used method, showcasing the importance of regularization which was absent from the original work."],"url":"http://arxiv.org/abs/2403.04236v1","category":"cs.LG"}
{"created":"2024-03-07 04:22:34","title":"GRAWA: Gradient-based Weighted Averaging for Distributed Training of Deep Learning Models","abstract":"We study distributed training of deep learning models in time-constrained environments. We propose a new algorithm that periodically pulls workers towards the center variable computed as a weighted average of workers, where the weights are inversely proportional to the gradient norms of the workers such that recovering the flat regions in the optimization landscape is prioritized. We develop two asynchronous variants of the proposed algorithm that we call Model-level and Layer-level Gradient-based Weighted Averaging (resp. MGRAWA and LGRAWA), which differ in terms of the weighting scheme that is either done with respect to the entire model or is applied layer-wise. On the theoretical front, we prove the convergence guarantee for the proposed approach in both convex and non-convex settings. We then experimentally demonstrate that our algorithms outperform the competitor methods by achieving faster convergence and recovering better quality and flatter local optima. We also carry out an ablation study to analyze the scalability of the proposed algorithms in more crowded distributed training environments. Finally, we report that our approach requires less frequent communication and fewer distributed updates compared to the state-of-the-art baselines.","sentences":["We study distributed training of deep learning models in time-constrained environments.","We propose a new algorithm that periodically pulls workers towards the center variable computed as a weighted average of workers, where the weights are inversely proportional to the gradient norms of the workers such that recovering the flat regions in the optimization landscape is prioritized.","We develop two asynchronous variants of the proposed algorithm that we call Model-level and Layer-level Gradient-based Weighted Averaging (resp.","MGRAWA and LGRAWA), which differ in terms of the weighting scheme that is either done with respect to the entire model or is applied layer-wise.","On the theoretical front, we prove the convergence guarantee for the proposed approach in both convex and non-convex settings.","We then experimentally demonstrate that our algorithms outperform the competitor methods by achieving faster convergence and recovering better quality and flatter local optima.","We also carry out an ablation study to analyze the scalability of the proposed algorithms in more crowded distributed training environments.","Finally, we report that our approach requires less frequent communication and fewer distributed updates compared to the state-of-the-art baselines."],"url":"http://arxiv.org/abs/2403.04206v1","category":"cs.LG"}
{"created":"2024-03-07 04:21:12","title":"OGMP: Oracle Guided Multimodal Policies for Agile and Versatile Robot Control","abstract":"Amidst task-specific learning-based control synthesis frameworks that achieve impressive empirical results, a unified framework that systematically constructs an optimal policy for sufficiently solving a general notion of a task is absent. Hence, we propose a theoretical framework for a task-centered control synthesis leveraging two critical ideas: 1) oracle-guided policy optimization for the non-limiting integration of sub-optimal task-based priors to guide the policy optimization and 2) task-vital multimodality to break down solving a task into executing a sequence of behavioral modes. The proposed approach results in highly agile parkour and diving on a 16-DoF dynamic bipedal robot. The obtained policy advances indefinitely on a track, performing leaps and jumps of varying lengths and heights for the parkour task. Corresponding to the dive task, the policy demonstrates front, back, and side flips from various initial heights. Finally, we introduce a novel latent mode space reachability analysis to study our policies' versatility and generalization by computing a feasible mode set function through which we certify a set of failure-free modes for our policy to perform at any given state.","sentences":["Amidst task-specific learning-based control synthesis frameworks that achieve impressive empirical results, a unified framework that systematically constructs an optimal policy for sufficiently solving a general notion of a task is absent.","Hence, we propose a theoretical framework for a task-centered control synthesis leveraging two critical ideas: 1) oracle-guided policy optimization for the non-limiting integration of sub-optimal task-based priors to guide the policy optimization and 2) task-vital multimodality to break down solving a task into executing a sequence of behavioral modes.","The proposed approach results in highly agile parkour and diving on a 16-DoF dynamic bipedal robot.","The obtained policy advances indefinitely on a track, performing leaps and jumps of varying lengths and heights for the parkour task.","Corresponding to the dive task, the policy demonstrates front, back, and side flips from various initial heights.","Finally, we introduce a novel latent mode space reachability analysis to study our policies' versatility and generalization by computing a feasible mode set function through which we certify a set of failure-free modes for our policy to perform at any given state."],"url":"http://arxiv.org/abs/2403.04205v1","category":"cs.RO"}
{"created":"2024-03-07 03:38:35","title":"Silicon Photonic 2.5D Interposer Networks for Overcoming Communication Bottlenecks in Scale-out Machine Learning Hardware Accelerators","abstract":"Modern machine learning (ML) applications are becoming increasingly complex and monolithic (single chip) accelerator architectures cannot keep up with their energy efficiency and throughput demands. Even though modern digital electronic accelerators are gradually adopting 2.5D architectures with multiple smaller chiplets to improve scalability, they face fundamental limitations due to a reliance on slow metallic interconnects. This paper outlines how optical communication and computation can be leveraged in 2.5D platforms to realize energy-efficient and high throughput 2.5D ML accelerator architectures.","sentences":["Modern machine learning (ML) applications are becoming increasingly complex and monolithic (single chip) accelerator architectures cannot keep up with their energy efficiency and throughput demands.","Even though modern digital electronic accelerators are gradually adopting 2.5D architectures with multiple smaller chiplets to improve scalability, they face fundamental limitations due to a reliance on slow metallic interconnects.","This paper outlines how optical communication and computation can be leveraged in 2.5D platforms to realize energy-efficient and high throughput 2.5D ML accelerator architectures."],"url":"http://arxiv.org/abs/2403.04189v1","category":"cs.AR"}
{"created":"2024-03-07 03:26:02","title":"YYDS: Visible-Infrared Person Re-Identification with Coarse Descriptions","abstract":"Visible-infrared person re-identification (VI-ReID) is challenging due to considerable cross-modality discrepancies. Existing works mainly focus on learning modality-invariant features while suppressing modality-specific ones. However, retrieving visible images only depends on infrared samples is an extreme problem because of the absence of color information. To this end, we present the Refer-VI-ReID settings, which aims to match target visible images from both infrared images and coarse language descriptions (e.g., \"a man with red top and black pants\") to complement the missing color information. To address this task, we design a Y-Y-shape decomposition structure, dubbed YYDS, to decompose and aggregate texture and color features of targets. Specifically, the text-IoU regularization strategy is firstly presented to facilitate the decomposition training, and a joint relation module is then proposed to infer the aggregation. Furthermore, the cross-modal version of k-reciprocal re-ranking algorithm is investigated, named CMKR, in which three neighbor search strategies and one local query expansion method are explored to alleviate the modality bias problem of the near neighbors. We conduct experiments on SYSU-MM01, RegDB and LLCM datasets with our manually annotated descriptions. Both YYDS and CMKR achieve remarkable improvements over SOTA methods on all three datasets. Codes are available at https://github.com/dyhBUPT/YYDS.","sentences":["Visible-infrared person re-identification (VI-ReID) is challenging due to considerable cross-modality discrepancies.","Existing works mainly focus on learning modality-invariant features while suppressing modality-specific ones.","However, retrieving visible images only depends on infrared samples is an extreme problem because of the absence of color information.","To this end, we present the Refer-VI-ReID settings, which aims to match target visible images from both infrared images and coarse language descriptions (e.g., \"a man with red top and black pants\") to complement the missing color information.","To address this task, we design a Y-Y-shape decomposition structure, dubbed YYDS, to decompose and aggregate texture and color features of targets.","Specifically, the text-IoU regularization strategy is firstly presented to facilitate the decomposition training, and a joint relation module is then proposed to infer the aggregation.","Furthermore, the cross-modal version of k-reciprocal re-ranking algorithm is investigated, named CMKR, in which three neighbor search strategies and one local query expansion method are explored to alleviate the modality bias problem of the near neighbors.","We conduct experiments on SYSU-MM01, RegDB and LLCM datasets with our manually annotated descriptions.","Both YYDS and CMKR achieve remarkable improvements over SOTA methods on all three datasets.","Codes are available at https://github.com/dyhBUPT/YYDS."],"url":"http://arxiv.org/abs/2403.04183v1","category":"cs.CV"}
{"created":"2024-03-07 01:50:56","title":"A Crosstalk-Aware Timing Prediction Method in Routing","abstract":"With shrinking interconnect spacing in advanced technology nodes, existing timing predictions become less precise due to the challenging quantification of crosstalk-induced delay. During the routing, the crosstalk effect is typically modeled by predicting coupling capacitance with congestion information. However, the timing estimation tends to be overly pessimistic, as the crosstalk-induced delay depends not only on the coupling capacitance but also on the signal arrival time. This work presents a crosstalk-aware timing estimation method using a two-step machine learning approach. Interconnects that are physically adjacent and overlap in signal timing windows are filtered first. Crosstalk delay is predicted by integrating physical topology and timing features without relying on post-routing results and the parasitic extraction. Experimental results show a match rate of over 99% for identifying crosstalk nets compared to the commercial tool on the OpenCores benchmarks, with prediction results being more accurate than those of other state-of-the-art methods.","sentences":["With shrinking interconnect spacing in advanced technology nodes, existing timing predictions become less precise due to the challenging quantification of crosstalk-induced delay.","During the routing, the crosstalk effect is typically modeled by predicting coupling capacitance with congestion information.","However, the timing estimation tends to be overly pessimistic, as the crosstalk-induced delay depends not only on the coupling capacitance but also on the signal arrival time.","This work presents a crosstalk-aware timing estimation method using a two-step machine learning approach.","Interconnects that are physically adjacent and overlap in signal timing windows are filtered first.","Crosstalk delay is predicted by integrating physical topology and timing features without relying on post-routing results and the parasitic extraction.","Experimental results show a match rate of over 99% for identifying crosstalk nets compared to the commercial tool on the OpenCores benchmarks, with prediction results being more accurate than those of other state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.04145v1","category":"eess.SY"}
{"created":"2024-03-07 00:00:02","title":"Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs","abstract":"Deep learning methods for perception are the cornerstone of many robotic systems. Despite their potential for impressive performance, obtaining real-world training data is expensive, and can be impractically difficult for some tasks. Sim-to-real transfer with domain randomization offers a potential workaround, but often requires extensive manual tuning and results in models that are brittle to distribution shift between sim and real. In this work, we introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF model that is the centerpiece of a real-to-sim pipeline for synthesizing training data targeted to scenes and objects from the real world. COV-NeRF extracts objects from real images and composes them into new scenes, generating photorealistic renderings and many types of 2D and 3D supervision, including depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the rendering quality of modern NeRF methods, and can be used to rapidly close the sim-to-real gap across a variety of perceptual modalities.","sentences":["Deep learning methods for perception are the cornerstone of many robotic systems.","Despite their potential for impressive performance, obtaining real-world training data is expensive, and can be impractically difficult for some tasks.","Sim-to-real transfer with domain randomization offers a potential workaround, but often requires extensive manual tuning and results in models that are brittle to distribution shift between sim and real.","In this work, we introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF model that is the centerpiece of a real-to-sim pipeline for synthesizing training data targeted to scenes and objects from the real world.","COV-NeRF extracts objects from real images and composes them into new scenes, generating photorealistic renderings and many types of 2D and 3D supervision, including depth maps, segmentation masks, and meshes.","We show that COV-NeRF matches the rendering quality of modern NeRF methods, and can be used to rapidly close the sim-to-real gap across a variety of perceptual modalities."],"url":"http://arxiv.org/abs/2403.04114v1","category":"cs.RO"}
{"created":"2024-03-06 22:32:48","title":"Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records","abstract":"In the realm of big data and digital healthcare, Electronic Health Records (EHR) have become a rich source of information with the potential to improve patient care and medical research. In recent years, machine learning models have proliferated for analyzing EHR data to predict patients future health conditions. Among them, some studies advocate for multi-task learning (MTL) to jointly predict multiple target diseases for improving the prediction performance over single task learning. Nevertheless, current MTL frameworks for EHR data have significant limitations due to their heavy reliance on human experts to identify task groups for joint training and design model architectures. To reduce human intervention and improve the framework design, we propose an automated approach named AutoDP, which can search for the optimal configuration of task grouping and architectures simultaneously. To tackle the vast joint search space encompassing task combinations and architectures, we employ surrogate model-based optimization, enabling us to efficiently discover the optimal solution. Experimental results on real-world EHR data demonstrate the efficacy of the proposed AutoDP framework. It achieves significant performance improvements over both hand-crafted and automated state-of-the-art methods, also maintains a feasible search cost at the same time.","sentences":["In the realm of big data and digital healthcare, Electronic Health Records (EHR) have become a rich source of information with the potential to improve patient care and medical research.","In recent years, machine learning models have proliferated for analyzing EHR data to predict patients future health conditions.","Among them, some studies advocate for multi-task learning (MTL) to jointly predict multiple target diseases for improving the prediction performance over single task learning.","Nevertheless, current MTL frameworks for EHR data have significant limitations due to their heavy reliance on human experts to identify task groups for joint training and design model architectures.","To reduce human intervention and improve the framework design, we propose an automated approach named AutoDP, which can search for the optimal configuration of task grouping and architectures simultaneously.","To tackle the vast joint search space encompassing task combinations and architectures, we employ surrogate model-based optimization, enabling us to efficiently discover the optimal solution.","Experimental results on real-world EHR data demonstrate the efficacy of the proposed AutoDP framework.","It achieves significant performance improvements over both hand-crafted and automated state-of-the-art methods, also maintains a feasible search cost at the same time."],"url":"http://arxiv.org/abs/2403.04086v1","category":"cs.LG"}
{"created":"2024-03-06 22:30:04","title":"Don't Blame the Data, Blame the Model: Understanding Noise and Bias When Learning from Subjective Annotations","abstract":"Researchers have raised awareness about the harms of aggregating labels especially in subjective tasks that naturally contain disagreements among human annotators. In this work we show that models that are only provided aggregated labels show low confidence on high-disagreement data instances. While previous studies consider such instances as mislabeled, we argue that the reason the high-disagreement text instances have been hard-to-learn is that the conventional aggregated models underperform in extracting useful signals from subjective tasks. Inspired by recent studies demonstrating the effectiveness of learning from raw annotations, we investigate classifying using Multiple Ground Truth (Multi-GT) approaches. Our experiments show an improvement of confidence for the high-disagreement instances.","sentences":["Researchers have raised awareness about the harms of aggregating labels especially in subjective tasks that naturally contain disagreements among human annotators.","In this work we show that models that are only provided aggregated labels show low confidence on high-disagreement data instances.","While previous studies consider such instances as mislabeled, we argue that the reason the high-disagreement text instances have been hard-to-learn is that the conventional aggregated models underperform in extracting useful signals from subjective tasks.","Inspired by recent studies demonstrating the effectiveness of learning from raw annotations, we investigate classifying using Multiple Ground Truth (Multi-GT) approaches.","Our experiments show an improvement of confidence for the high-disagreement instances."],"url":"http://arxiv.org/abs/2403.04085v1","category":"cs.CL"}
{"created":"2024-03-06 22:27:30","title":"Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference","abstract":"Given time series data, how can we answer questions like \"what will happen in the future?\" and \"how did we get here?\" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-dimensional matrix. In one special case, inferring intermediate representations will be equivalent to interpolating between the learned representations. We validate our theory using numerical simulations on tasks up to 46-dimensions.","sentences":["Given time series data, how can we answer questions like \"what will happen in the future?\"","and \"how did we get here?\"","These sorts of probabilistic inference questions are challenging when observations are high-dimensional.","In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations.","The key idea is to apply a variant of contrastive learning to time series data.","Prior work already shows that the representations learned by contrastive learning encode a probability ratio.","By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian.","Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-dimensional matrix.","In one special case, inferring intermediate representations will be equivalent to interpolating between the learned representations.","We validate our theory using numerical simulations on tasks up to 46-dimensions."],"url":"http://arxiv.org/abs/2403.04082v1","category":"cs.LG"}
{"created":"2024-03-06 21:36:38","title":"LoDisc: Learning Global-Local Discriminative Features for Self-Supervised Fine-Grained Visual Recognition","abstract":"Self-supervised contrastive learning strategy has attracted remarkable attention due to its exceptional ability in representation learning. However, current contrastive learning tends to learn global coarse-grained representations of the image that benefit generic object recognition, whereas such coarse-grained features are insufficient for fine-grained visual recognition. In this paper, we present to incorporate the subtle local fine-grained feature learning into global self-supervised contrastive learning through a pure self-supervised global-local fine-grained contrastive learning framework. Specifically, a novel pretext task called Local Discrimination (LoDisc) is proposed to explicitly supervise self-supervised model's focus towards local pivotal regions which are captured by a simple-but-effective location-wise mask sampling strategy. We show that Local Discrimination pretext task can effectively enhance fine-grained clues in important local regions, and the global-local framework further refines the fine-grained feature representations of images. Extensive experimental results on different fine-grained object recognition tasks demonstrate that the proposed method can lead to a decent improvement in different evaluation settings. Meanwhile, the proposed method is also effective in general object recognition tasks.","sentences":["Self-supervised contrastive learning strategy has attracted remarkable attention due to its exceptional ability in representation learning.","However, current contrastive learning tends to learn global coarse-grained representations of the image that benefit generic object recognition, whereas such coarse-grained features are insufficient for fine-grained visual recognition.","In this paper, we present to incorporate the subtle local fine-grained feature learning into global self-supervised contrastive learning through a pure self-supervised global-local fine-grained contrastive learning framework.","Specifically, a novel pretext task called Local Discrimination (LoDisc) is proposed to explicitly supervise self-supervised model's focus towards local pivotal regions which are captured by a simple-but-effective location-wise mask sampling strategy.","We show that Local Discrimination pretext task can effectively enhance fine-grained clues in important local regions, and the global-local framework further refines the fine-grained feature representations of images.","Extensive experimental results on different fine-grained object recognition tasks demonstrate that the proposed method can lead to a decent improvement in different evaluation settings.","Meanwhile, the proposed method is also effective in general object recognition tasks."],"url":"http://arxiv.org/abs/2403.04066v1","category":"cs.CV"}
{"created":"2024-03-06 20:01:44","title":"Empirical Game-Theoretic Analysis: A Survey","abstract":"In the empirical approach to game-theoretic analysis (EGTA), the model of the game comes not from declarative representation, but is derived by interrogation of a procedural description of the game environment. The motivation for developing this approach was to enable game-theoretic reasoning about strategic situations too complex for analytic specification and solution. Since its introduction over twenty years ago, EGTA has been applied to a wide range of multiagent domains, from auctions and markets to recreational games to cyber-security. We survey the extensive methodology developed for EGTA over the years, organized by the elemental subproblems comprising the EGTA process. We describe key EGTA concepts and techniques, and the questions at the frontier of EGTA research. Recent advances in machine learning have accelerated progress in EGTA, and promise to significantly expand our capacities for reasoning about complex game situations.","sentences":["In the empirical approach to game-theoretic analysis (EGTA), the model of the game comes not from declarative representation, but is derived by interrogation of a procedural description of the game environment.","The motivation for developing this approach was to enable game-theoretic reasoning about strategic situations too complex for analytic specification and solution.","Since its introduction over twenty years ago, EGTA has been applied to a wide range of multiagent domains, from auctions and markets to recreational games to cyber-security.","We survey the extensive methodology developed for EGTA over the years, organized by the elemental subproblems comprising the EGTA process.","We describe key EGTA concepts and techniques, and the questions at the frontier of EGTA research.","Recent advances in machine learning have accelerated progress in EGTA, and promise to significantly expand our capacities for reasoning about complex game situations."],"url":"http://arxiv.org/abs/2403.04018v1","category":"cs.GT"}
{"created":"2024-03-07 17:20:29","title":"Calibrated rank volatility stabilized models for large equity markets","abstract":"In the framework of stochastic portfolio theory we introduce rank volatility stabilized models for large equity markets over long time horizons. These models are rank-based extensions of the volatility stabilized models introduced by Fernholz & Karatzas in 2005. On the theoretical side we establish global existence of the model and ergodicity of the induced ranked market weights. We also derive explicit expressions for growth-optimal portfolios and show the existence of relative arbitrage with respect to the market portfolio. On the empirical side we calibrate the model to sixteen years of CRSP US equity data matching (i) rank-based volatilities, (ii) stock turnover as measured by market weight collisions, (iii) the average market rate of return and (iv) the capital distribution curve. Assessment of model fit and error analysis is conducted both in and out of sample. To the best of our knowledge this is the first model exhibiting relative arbitrage that has statistically been shown to have a good quantitative fit with the empirical features (i)-(iv). We additionally simulate trajectories of the calibrated model and compare them to historical trajectories, both in and out of sample.","sentences":["In the framework of stochastic portfolio theory we introduce rank volatility stabilized models for large equity markets over long time horizons.","These models are rank-based extensions of the volatility stabilized models introduced by Fernholz & Karatzas in 2005.","On the theoretical side we establish global existence of the model and ergodicity of the induced ranked market weights.","We also derive explicit expressions for growth-optimal portfolios and show the existence of relative arbitrage with respect to the market portfolio.","On the empirical side we calibrate the model to sixteen years of CRSP US equity data matching (i) rank-based volatilities, (ii) stock turnover as measured by market weight collisions, (iii) the average market rate of return and (iv) the capital distribution curve.","Assessment of model fit and error analysis is conducted both in and out of sample.","To the best of our knowledge this is the first model exhibiting relative arbitrage that has statistically been shown to have a good quantitative fit with the empirical features (i)-(iv).","We additionally simulate trajectories of the calibrated model and compare them to historical trajectories, both in and out of sample."],"url":"http://arxiv.org/abs/2403.04674v1","category":"q-fin.MF"}
{"created":"2024-03-07 17:00:16","title":"Computing the Edge Expansion of a Graph using SDP","abstract":"Computing the edge expansion of a graph is a famously hard combinatorial problem for which there have been many approximation studies. We present two versions of an exact algorithm using semidefinite programming (SDP) to compute this constant for any graph. The SDP relaxation is used to first reduce the search space considerably. One version applies then an SDP-based branch-and-bound algorithm, along with heuristic search. The other version transforms the problem into an instance of a max-cut problem and solves this using a state-of-the-art solver. Numerical results demonstrate that we clearly outperform mixed-integer quadratic solvers as well as another SDP-based algorithm from the literature.","sentences":["Computing the edge expansion of a graph is a famously hard combinatorial problem for which there have been many approximation studies.","We present two versions of an exact algorithm using semidefinite programming (SDP) to compute this constant for any graph.","The SDP relaxation is used to first reduce the search space considerably.","One version applies then an SDP-based branch-and-bound algorithm, along with heuristic search.","The other version transforms the problem into an instance of a max-cut problem and solves this using a state-of-the-art solver.","Numerical results demonstrate that we clearly outperform mixed-integer quadratic solvers as well as another SDP-based algorithm from the literature."],"url":"http://arxiv.org/abs/2403.04657v1","category":"math.OC"}
{"created":"2024-03-07 16:23:14","title":"Investigation of low band gap silicon alloy thin film solar cell for improving short and long wavelength response","abstract":"Numerical simulation of a solar cell can provide various information that can be useful to maximize its power conversion efficiency (PCE). In that respect we carried out a set of numerical simulation using AFORS-HET simulation program. Separately, in order to get a better understanding, the optical absorption in individual layers devices were analyzed. Current-voltage characteristic curve of a reference cell (Cell-A) was used as the starting device. The PCE of the reference device was $8.85\\%$ with short circuit current density $J_{sc}$ of 15.43 mA/cm$^{2}$ and fill factor (FF) of $68.3\\%$. However, it was noticed that the reference cell had high parasitic optical absorption at the window layer and the device structure was also not optimized. After suitable optimization the PCE of this device (Cell-B2) improves to $111.59\\%$ ($J_{sc}$ and FF of 13.0 mA/cm$^{2}$ and $87\\%$ respectively). The results show that the effective optical absorption in the active layer can be improved significantly by optimizing the device structure. The short wavelength response can be improved by reducing the parasitic optical absorption by the doped window layer, while its long wavelength response improves by raising effective absorption length of the active layer. Furthermore, its optimum thickness, for the highest possible PCE, is found to be dependent upon the material properties, more importantly on its defect density.","sentences":["Numerical simulation of a solar cell can provide various information that can be useful to maximize its power conversion efficiency (PCE).","In that respect we carried out a set of numerical simulation using AFORS-HET simulation program.","Separately, in order to get a better understanding, the optical absorption in individual layers devices were analyzed.","Current-voltage characteristic curve of a reference cell (Cell-A) was used as the starting device.","The PCE of the reference device was $8.85\\%$ with short circuit current density $J_{sc}$ of 15.43 mA/cm$^{2}$ and fill factor (FF) of $68.3\\%$. However, it was noticed that the reference cell had high parasitic optical absorption at the window layer and the device structure was also not optimized.","After suitable optimization the PCE of this device (Cell-B2) improves to $111.59\\%$ ($J_{sc}$ and FF of 13.0 mA/cm$^{2}$ and $87\\%$ respectively).","The results show that the effective optical absorption in the active layer can be improved significantly by optimizing the device structure.","The short wavelength response can be improved by reducing the parasitic optical absorption by the doped window layer, while its long wavelength response improves by raising effective absorption length of the active layer.","Furthermore, its optimum thickness, for the highest possible PCE, is found to be dependent upon the material properties, more importantly on its defect density."],"url":"http://arxiv.org/abs/2403.04637v1","category":"physics.optics"}
{"created":"2024-03-07 14:59:56","title":"Nonlinear and degenerate discounted approximation in discrete weak KAM theory","abstract":"In this paper, we introduce a discrete version of the nonlinear implicit Lax-Oleinik operator. We consider the associated vanishing discount problem with a non-degenerate condition and prove convergence of solutions as the discount factor goes to $0$. We also discuss the uniqueness of the discounted solution. The convergence result is a selection principle for fixed points of a family of nonlinear operators.","sentences":["In this paper, we introduce a discrete version of the nonlinear implicit Lax-Oleinik operator.","We consider the associated vanishing discount problem with a non-degenerate condition and prove convergence of solutions as the discount factor goes to $0$. We also discuss the uniqueness of the discounted solution.","The convergence result is a selection principle for fixed points of a family of nonlinear operators."],"url":"http://arxiv.org/abs/2403.04563v1","category":"math.OC"}
{"created":"2024-03-07 14:55:58","title":"Parameter identification in PDEs by the solution of monotone inclusion problems","abstract":"In this paper we consider a parameter identification problem for a semilinear parabolic PDE. For the regularized solution of this problem, we introduce a total variation based regularization method requiring the solution of a monotone inclusion problem. We show well-posedness in the sense of inverse problems of the resulting regularization scheme. In addition, we introduce and analyze a numerical algorithm for the solution of this inclusion problem using a nested inertial primal dual method. We demonstrate by means of numerical examples the convergence of both the numerical algorithm and the regularization method.","sentences":["In this paper we consider a parameter identification problem for a semilinear parabolic PDE.","For the regularized solution of this problem, we introduce a total variation based regularization method requiring the solution of a monotone inclusion problem.","We show well-posedness in the sense of inverse problems of the resulting regularization scheme.","In addition, we introduce and analyze a numerical algorithm for the solution of this inclusion problem using a nested inertial primal dual method.","We demonstrate by means of numerical examples the convergence of both the numerical algorithm and the regularization method."],"url":"http://arxiv.org/abs/2403.04557v1","category":"math.NA"}
{"created":"2024-03-07 14:03:31","title":"Ducho 2.0: Towards a More Up-to-Date Feature Extraction and Processing Framework for Multimodal Recommendation","abstract":"In this work, we introduce Ducho 2.0, the latest stable version of our framework. Differently from Ducho, Ducho 2.0 offers a more personalized user experience with the definition and import of custom extraction models fine-tuned on specific tasks and datasets. Moreover, the new version is capable of extracting and processing features through multimodal-by-design large models. Notably, all these new features are supported by optimized data loading and storing to the local memory. To showcase the capabilities of Ducho 2.0, we demonstrate a complete multimodal recommendation pipeline, from the extraction/processing to the final recommendation. The idea is to provide practitioners and experienced scholars with a ready-to-use tool that, put on top of any multimodal recommendation framework, may permit them to run extensive benchmarking analyses. All materials are accessible at: \\url{https://github.com/sisinflab/Ducho}.","sentences":["In this work, we introduce Ducho 2.0, the latest stable version of our framework.","Differently from Ducho, Ducho 2.0 offers a more personalized user experience with the definition and import of custom extraction models fine-tuned on specific tasks and datasets.","Moreover, the new version is capable of extracting and processing features through multimodal-by-design large models.","Notably, all these new features are supported by optimized data loading and storing to the local memory.","To showcase the capabilities of Ducho 2.0, we demonstrate a complete multimodal recommendation pipeline, from the extraction/processing to the final recommendation.","The idea is to provide practitioners and experienced scholars with a ready-to-use tool that, put on top of any multimodal recommendation framework, may permit them to run extensive benchmarking analyses.","All materials are accessible at: \\url{https://github.com/sisinflab/Ducho}."],"url":"http://arxiv.org/abs/2403.04503v1","category":"cs.IR"}
{"created":"2024-03-07 12:53:05","title":"Effect of turbulent diffusion in modeling anaerobic digestion","abstract":"In this study, the impact of turbulent diffusion on mixing of biochemical reaction models is explored by implementing and validating different models. An original codebase called CHAD (Coupled Hydrodynamics and Anaerobic Digestion) is extended to incorporate turbulent diffusion and validate it against results from OpenFOAM with 2D Rayleigh-Taylor Instability and lid-driven cavity simulations. The models are then tested for the applications with Anaerobic Digestion - a widely used wastewater treatment method. The findings demonstrate that the implemented models accurately capture turbulent diffusion when provided with an accurate flow field. Specifically, a minor effect of chemical turbulent diffusion on biochemical reactions within the anaerobic digestion tank is observed, while thermal turbulent diffusion significantly influences mixing. By successfully implementing turbulent diffusion models in CHAD, its capabilities for more accurate anaerobic digestion simulations are enhanced, aiding in optimizing the design and operation of anaerobic digestion reactors in real-world wastewater treatment applications.","sentences":["In this study, the impact of turbulent diffusion on mixing of biochemical reaction models is explored by implementing and validating different models.","An original codebase called CHAD (Coupled Hydrodynamics and Anaerobic Digestion) is extended to incorporate turbulent diffusion and validate it against results from OpenFOAM with 2D Rayleigh-Taylor Instability and lid-driven cavity simulations.","The models are then tested for the applications with Anaerobic Digestion - a widely used wastewater treatment method.","The findings demonstrate that the implemented models accurately capture turbulent diffusion when provided with an accurate flow field.","Specifically, a minor effect of chemical turbulent diffusion on biochemical reactions within the anaerobic digestion tank is observed, while thermal turbulent diffusion significantly influences mixing.","By successfully implementing turbulent diffusion models in CHAD, its capabilities for more accurate anaerobic digestion simulations are enhanced, aiding in optimizing the design and operation of anaerobic digestion reactors in real-world wastewater treatment applications."],"url":"http://arxiv.org/abs/2403.04457v1","category":"cs.CE"}
{"created":"2024-03-07 09:50:03","title":"DV-Hop localization based on Distance Estimation using Multinode and Hop Loss in WSNs","abstract":"Location awareness is a critical issue in wireless sensor network applications. For more accurate location estimation, the two issues should be considered extensively: 1) how to sufficiently utilize the connection information between multiple nodes and 2) how to select a suitable solution from multiple solutions obtained by the Euclidean distance loss. In this paper, a DV-Hop localization based on the distance estimation using multinode (DEMN) and the hop loss in WSNs is proposed to address the two issues. In DEMN, when multiple anchor nodes can detect an unknown node, the distance expectation between the unknown node and an anchor node is calculated using the cross-domain information and is considered as the expected distance between them, which narrows the search space. When minimizing the traditional Euclidean distance loss, multiple solutions may exist. To select a suitable solution, the hop loss is proposed, which minimizes the difference between the real and its predicted hops. Finally, the Euclidean distance loss calculated by the DEMN and the hop loss are embedded into the multi-objective optimization algorithm. The experimental results show that the proposed method gains 86.11\\% location accuracy in the randomly distributed network, which is 6.05% better than the DEM-DV-Hop, while DEMN and the hop loss can contribute 2.46% and 3.41%, respectively.","sentences":["Location awareness is a critical issue in wireless sensor network applications.","For more accurate location estimation, the two issues should be considered extensively: 1) how to sufficiently utilize the connection information between multiple nodes and 2) how to select a suitable solution from multiple solutions obtained by the Euclidean distance loss.","In this paper, a DV-Hop localization based on the distance estimation using multinode (DEMN) and the hop loss in WSNs is proposed to address the two issues.","In DEMN, when multiple anchor nodes can detect an unknown node, the distance expectation between the unknown node and an anchor node is calculated using the cross-domain information and is considered as the expected distance between them, which narrows the search space.","When minimizing the traditional Euclidean distance loss, multiple solutions may exist.","To select a suitable solution, the hop loss is proposed, which minimizes the difference between the real and its predicted hops.","Finally, the Euclidean distance loss calculated by the DEMN and the hop loss are embedded into the multi-objective optimization algorithm.","The experimental results show that the proposed method gains 86.11\\% location accuracy in the randomly distributed network, which is 6.05% better than the DEM-DV-Hop, while DEMN and the hop loss can contribute 2.46% and 3.41%, respectively."],"url":"http://arxiv.org/abs/2403.04365v1","category":"cs.NI"}
{"created":"2024-03-07 09:44:01","title":"Subsampling for Big Data Linear Models with Measurement Errors","abstract":"Subsampling algorithms for various parametric regression models with massive data have been extensively investigated in recent years. However, all existing studies on subsampling heavily rely on clean massive data. In practical applications, the observed covariates may suffer from inaccuracies due to measurement errors. To address the challenge of large datasets with measurement errors, this study explores two subsampling algorithms based on the corrected likelihood approach: the optimal subsampling algorithm utilizing inverse probability weighting and the perturbation subsampling algorithm employing random weighting assuming a perfectly known distribution. Theoretical properties for both algorithms are provided. Numerical simulations and two real-world examples demonstrate the effectiveness of these proposed methods compared to other uncorrected algorithms.","sentences":["Subsampling algorithms for various parametric regression models with massive data have been extensively investigated in recent years.","However, all existing studies on subsampling heavily rely on clean massive data.","In practical applications, the observed covariates may suffer from inaccuracies due to measurement errors.","To address the challenge of large datasets with measurement errors, this study explores two subsampling algorithms based on the corrected likelihood approach: the optimal subsampling algorithm utilizing inverse probability weighting and the perturbation subsampling algorithm employing random weighting assuming a perfectly known distribution.","Theoretical properties for both algorithms are provided.","Numerical simulations and two real-world examples demonstrate the effectiveness of these proposed methods compared to other uncorrected algorithms."],"url":"http://arxiv.org/abs/2403.04361v1","category":"math.ST"}
{"created":"2024-03-07 08:32:07","title":"A robust shifted proper orthogonal decomposition: Proximal methods for decomposing flows with multiple transports","abstract":"We present a new methodology for decomposing flows with multiple transports that further extends the shifted proper orthogonal decomposition (sPOD). The sPOD tries to approximate transport-dominated flows by a sum of co-moving data fields. The proposed methods stem from sPOD but optimize the co-moving fields directly and penalize their nuclear norm to promote low rank of the individual data in the decomposition. Furthermore, we add a robustness term to the decomposition that can deal with interpolation error and data noises. Leveraging tools from convex optimization, we derive three proximal algorithms to solve the decomposition problem. We report a numerical comparison with existing methods against synthetic data benchmarks and then show the separation ability of our methods on 1D and 2D incompressible and reactive flows. The resulting methodology is the basis of a new analysis paradigm that results in the same interpretability as the POD for the individual co-moving fields.","sentences":["We present a new methodology for decomposing flows with multiple transports that further extends the shifted proper orthogonal decomposition (sPOD).","The sPOD tries to approximate transport-dominated flows by a sum of co-moving data fields.","The proposed methods stem from sPOD but optimize the co-moving fields directly and penalize their nuclear norm to promote low rank of the individual data in the decomposition.","Furthermore, we add a robustness term to the decomposition that can deal with interpolation error and data noises.","Leveraging tools from convex optimization, we derive three proximal algorithms to solve the decomposition problem.","We report a numerical comparison with existing methods against synthetic data benchmarks and then show the separation ability of our methods on 1D and 2D incompressible and reactive flows.","The resulting methodology is the basis of a new analysis paradigm that results in the same interpretability as the POD for the individual co-moving fields."],"url":"http://arxiv.org/abs/2403.04313v1","category":"math.NA"}
{"created":"2024-03-07 07:45:47","title":"Variational quantum eigensolver with linear depth problem-inspired ansatz for solving portfolio optimization in finance","abstract":"Great efforts have been dedicated in recent years to explore practical applications for noisy intermediate-scale quantum (NISQ) computers, which is a fundamental and challenging problem in quantum computing. As one of the most promising methods, the variational quantum eigensolver (VQE) has been extensively studied. In this paper, VQE is applied to solve portfolio optimization problems in finance by designing two hardware-efficient Dicke state ansatze that reach a maximum of 2n two-qubit gate depth and n^2/4 parameters, with n being the number of qubits used. Both ansatze are partitioning-friendly, allowing for the proposal of a highly scalable quantum/classical hybrid distributed computing (HDC) scheme. Combining simultaneous sampling, problem-specific measurement error mitigation, and fragment reuse techniques, we successfully implement the HDC experiments on the superconducting quantum computer Wu Kong with up to 55 qubits. The simulation and experimental results illustrate that the restricted expressibility of the ansatze, induced by the small number of parameters and limited entanglement, is advantageous for solving classical optimization problems with the cost function of the conditional value-at-risk (CVaR) for the NISQ era and beyond. Furthermore, the HDC scheme shows great potential for achieving quantum advantage in the NISQ era. We hope that the heuristic idea presented in this paper can motivate fruitful investigations in current and future quantum computing paradigms.","sentences":["Great efforts have been dedicated in recent years to explore practical applications for noisy intermediate-scale quantum (NISQ) computers, which is a fundamental and challenging problem in quantum computing.","As one of the most promising methods, the variational quantum eigensolver (VQE) has been extensively studied.","In this paper, VQE is applied to solve portfolio optimization problems in finance by designing two hardware-efficient Dicke state ansatze that reach a maximum of 2n two-qubit gate depth and n^2/4 parameters, with n being the number of qubits used.","Both ansatze are partitioning-friendly, allowing for the proposal of a highly scalable quantum/classical hybrid distributed computing (HDC) scheme.","Combining simultaneous sampling, problem-specific measurement error mitigation, and fragment reuse techniques, we successfully implement the HDC experiments on the superconducting quantum computer Wu Kong with up to 55 qubits.","The simulation and experimental results illustrate that the restricted expressibility of the ansatze, induced by the small number of parameters and limited entanglement, is advantageous for solving classical optimization problems with the cost function of the conditional value-at-risk (CVaR) for the NISQ era and beyond.","Furthermore, the HDC scheme shows great potential for achieving quantum advantage in the NISQ era.","We hope that the heuristic idea presented in this paper can motivate fruitful investigations in current and future quantum computing paradigms."],"url":"http://arxiv.org/abs/2403.04296v1","category":"quant-ph"}
{"created":"2024-03-07 07:20:25","title":"Multimodal Analysis of Traction Forces and Temperature Dynamics of Living Cells with Diamond-Embedded Substrate","abstract":"Cells and tissues are constantly exposed to various chemical and physical signals that intricately regulate various physiological and pathological processes. This study explores the integration of two biophysical methods, Traction Force Microscopy (TFM) and Optically-Detected Magnetic Resonance (ODMR), to concurrently assess cellular traction forces and local relative temperature. We present a novel elastic substrate with embedded nitrogen-vacancy microdiamonds, that facilitate ODMR-TFM measurements. Optimization efforts have focused on minimizing the sample illumination and experiment duration to mitigate biological perturbations. Our hybrid ODMR-TFM technique yields precise TFM maps and achieves approximately 1K accuracy in relative temperature measurements. Notably, our setup, employing a simple wide-field fluorescence microscope with standard components, demonstrates the broader feasibility of these techniques in life-science laboratories. By elucidating the physical aspects of cellular behavior beyond the existing methods, this approach opens avenues for a deeper understanding and may inspire diverse biomedical applications.","sentences":["Cells and tissues are constantly exposed to various chemical and physical signals that intricately regulate various physiological and pathological processes.","This study explores the integration of two biophysical methods, Traction Force Microscopy (TFM) and Optically-Detected Magnetic Resonance (ODMR), to concurrently assess cellular traction forces and local relative temperature.","We present a novel elastic substrate with embedded nitrogen-vacancy microdiamonds, that facilitate ODMR-TFM measurements.","Optimization efforts have focused on minimizing the sample illumination and experiment duration to mitigate biological perturbations.","Our hybrid ODMR-TFM technique yields precise TFM maps and achieves approximately 1K accuracy in relative temperature measurements.","Notably, our setup, employing a simple wide-field fluorescence microscope with standard components, demonstrates the broader feasibility of these techniques in life-science laboratories.","By elucidating the physical aspects of cellular behavior beyond the existing methods, this approach opens avenues for a deeper understanding and may inspire diverse biomedical applications."],"url":"http://arxiv.org/abs/2403.04277v1","category":"physics.bio-ph"}
{"created":"2024-03-07 05:32:10","title":"$C^{1,\u03b1}$ regularity of variational problems with a convexity constraint","abstract":"In this paper, we establish the interior $C^{1,\\alpha}$ regularity of minimizers of a class of functionals with a convexity constraint, which includes the principal-agent problems studied by Figalli-Kim-McCann (\\textit{J. Econom. Theory} \\textbf{146} (2011), no. 2, 454-478). The $C^{1,1}$ regularity was previously proved by Caffarelli-Lions in an unpublished note when the cost is quadratic, and recently extended to the case where the cost is uniformly convex with respect to a general preference function by McCann-Rankin-Zhang(\\textit{arXiv:2303.04937v3}). Our main result does not require the uniform convexity assumption on the cost function. In particular, we show that the solutions to the principal-agent problems with $q$-power cost are $C^{1,\\frac{1}{q-1}}$ when $q > 2$ and $C^{1,1}$ when $1<q\\leq 2$. Examples can show that this regularity is optimal when $q\\geq 2$.","sentences":["In this paper, we establish the interior $C^{1,\\alpha}$ regularity of minimizers of a class of functionals with a convexity constraint, which includes the principal-agent problems studied by Figalli-Kim-McCann (\\textit{J. Econom.","Theory} \\textbf{146} (2011), no. 2, 454-478).","The $C^{1,1}$ regularity was previously proved by Caffarelli-Lions in an unpublished note when the cost is quadratic, and recently extended to the case where the cost is uniformly convex with respect to a general preference function by McCann-Rankin-Zhang(\\textit{arXiv:2303.04937v3}).","Our main result does not require the uniform convexity assumption on the cost function.","In particular, we show that the solutions to the principal-agent problems with $q$-power cost are $C^{1,\\frac{1}{q-1}}$ when $q > 2$ and $C^{1,1}$ when $1<q\\leq 2$. Examples can show that this regularity is optimal when $q\\geq 2$."],"url":"http://arxiv.org/abs/2403.04235v1","category":"math.AP"}
{"created":"2024-03-07 05:04:36","title":"Single-Image HDR Reconstruction Assisted Ghost Suppression and Detail Preservation Network for Multi-Exposure HDR Imaging","abstract":"The reconstruction of high dynamic range (HDR) images from multi-exposure low dynamic range (LDR) images in dynamic scenes presents significant challenges, especially in preserving and restoring information in oversaturated regions and avoiding ghosting artifacts. While current methods often struggle to address these challenges, our work aims to bridge this gap by developing a multi-exposure HDR image reconstruction network for dynamic scenes, complemented by single-frame HDR image reconstruction. This network, comprising single-frame HDR reconstruction with enhanced stop image (SHDR-ESI) and SHDR-ESI-assisted multi-exposure HDR reconstruction (SHDRA-MHDR), effectively leverages the ghost-free characteristic of single-frame HDR reconstruction and the detail-enhancing capability of ESI in oversaturated areas. Specifically, SHDR-ESI innovatively integrates single-frame HDR reconstruction with the utilization of ESI. This integration not only optimizes the single image HDR reconstruction process but also effectively guides the synthesis of multi-exposure HDR images in SHDR-AMHDR. In this method, the single-frame HDR reconstruction is specifically applied to reduce potential ghosting effects in multiexposure HDR synthesis, while the use of ESI images assists in enhancing the detail information in the HDR synthesis process. Technically, SHDR-ESI incorporates a detail enhancement mechanism, which includes a self-representation module and a mutual-representation module, designed to aggregate crucial information from both reference image and ESI. To fully leverage the complementary information from non-reference images, a feature interaction fusion module is integrated within SHDRA-MHDR. Additionally, a ghost suppression module, guided by the ghost-free results of SHDR-ESI, is employed to suppress the ghosting artifacts.","sentences":["The reconstruction of high dynamic range (HDR) images from multi-exposure low dynamic range (LDR) images in dynamic scenes presents significant challenges, especially in preserving and restoring information in oversaturated regions and avoiding ghosting artifacts.","While current methods often struggle to address these challenges, our work aims to bridge this gap by developing a multi-exposure HDR image reconstruction network for dynamic scenes, complemented by single-frame HDR image reconstruction.","This network, comprising single-frame HDR reconstruction with enhanced stop image (SHDR-ESI) and SHDR-ESI-assisted multi-exposure HDR reconstruction (SHDRA-MHDR), effectively leverages the ghost-free characteristic of single-frame HDR reconstruction and the detail-enhancing capability of ESI in oversaturated areas.","Specifically, SHDR-ESI innovatively integrates single-frame HDR reconstruction with the utilization of ESI.","This integration not only optimizes the single image HDR reconstruction process but also effectively guides the synthesis of multi-exposure HDR images in SHDR-AMHDR.","In this method, the single-frame HDR reconstruction is specifically applied to reduce potential ghosting effects in multiexposure HDR synthesis, while the use of ESI images assists in enhancing the detail information in the HDR synthesis process.","Technically, SHDR-ESI incorporates a detail enhancement mechanism, which includes a self-representation module and a mutual-representation module, designed to aggregate crucial information from both reference image and ESI.","To fully leverage the complementary information from non-reference images, a feature interaction fusion module is integrated within SHDRA-MHDR.","Additionally, a ghost suppression module, guided by the ghost-free results of SHDR-ESI, is employed to suppress the ghosting artifacts."],"url":"http://arxiv.org/abs/2403.04228v1","category":"cs.CV"}
{"created":"2024-03-07 03:06:04","title":"Implicit Redundancy and Degeneracy in Conic Program","abstract":"This paper examines the feasible region of a standard conic program represented as the intersection of a closed convex cone and a set of linear equalities. It is recently shown that when Slater constraint qualification (strict feasibility) fails for the classes of linear and semidefinite programs, two key properties emerge within the feasible region; (a) every point in the feasible region is degenerate; (b) the constraint system inherits implicit redundancies. In this paper we show that degeneracy and implicit redundancies are inherent and universal traits of all conic programs in the absence of strict feasibility.","sentences":["This paper examines the feasible region of a standard conic program represented as the intersection of a closed convex cone and a set of linear equalities.","It is recently shown that when Slater constraint qualification (strict feasibility) fails for the classes of linear and semidefinite programs, two key properties emerge within the feasible region; (a) every point in the feasible region is degenerate; (b) the constraint system inherits implicit redundancies.","In this paper we show that degeneracy and implicit redundancies are inherent and universal traits of all conic programs in the absence of strict feasibility."],"url":"http://arxiv.org/abs/2403.04171v1","category":"math.OC"}
{"created":"2024-03-07 00:50:02","title":"Optimal Scheduling of Graph States via Path Decompositions","abstract":"We study the optimal scheduling of graph states in measurement-based quantum computation, establishing an equivalence between measurement schedules and path decompositions of graphs. We define the spatial cost of a measurement schedule based on the number of simultaneously active qubits and prove that an optimal measurement schedule corresponds to a path decomposition of minimal width. Our analysis shows that approximating the spatial cost of a graph is \\textsf{NP}-hard, while for graphs with bounded spatial cost, we establish an efficient algorithm for computing an optimal measurement schedule.","sentences":["We study the optimal scheduling of graph states in measurement-based quantum computation, establishing an equivalence between measurement schedules and path decompositions of graphs.","We define the spatial cost of a measurement schedule based on the number of simultaneously active qubits and prove that an optimal measurement schedule corresponds to a path decomposition of minimal width.","Our analysis shows that approximating the spatial cost of a graph is \\textsf{NP}-hard, while for graphs with bounded spatial cost, we establish an efficient algorithm for computing an optimal measurement schedule."],"url":"http://arxiv.org/abs/2403.04126v1","category":"quant-ph"}
{"created":"2024-03-06 22:19:24","title":"Optical MEMS Design for Telecommunications Applications","abstract":"As optical telecommunication networks become more complex, there is an emerging need for systems capable of very complex switching and manipulation of large numbers of optical signals. MEMS enable these systems by combining excellent capabilities and optical properties of macroscopic optomechanics with dense integration of multiple actuators on a single chip. Such optical MEMS present common design and process challenges, such as multiple electrical and optical IO, optical surface quality, optical integration density (fill factor) and actuator performance and reliability. We have used general design approaches such as pure-flexure design, electrostatic actuation and residual stress engineering in addressing these challenges. On several examples in this paper we illustrate these approaches along with underlying design tradeoffs and process requirements. We also describe specific numerical techniques useful for electrostatic actuator optimization and for analyzing the effects of residual stress.","sentences":["As optical telecommunication networks become more complex, there is an emerging need for systems capable of very complex switching and manipulation of large numbers of optical signals.","MEMS enable these systems by combining excellent capabilities and optical properties of macroscopic optomechanics with dense integration of multiple actuators on a single chip.","Such optical MEMS present common design and process challenges, such as multiple electrical and optical IO, optical surface quality, optical integration density (fill factor) and actuator performance and reliability.","We have used general design approaches such as pure-flexure design, electrostatic actuation and residual stress engineering in addressing these challenges.","On several examples in this paper we illustrate these approaches along with underlying design tradeoffs and process requirements.","We also describe specific numerical techniques useful for electrostatic actuator optimization and for analyzing the effects of residual stress."],"url":"http://arxiv.org/abs/2403.04078v1","category":"physics.app-ph"}
{"created":"2024-03-06 21:30:53","title":"A mechanistic model for smallpox transmission via inhaled aerosols inside respiratory pathways","abstract":"Investigations on airborne transmission of pathogens constitute a rapidly expanding field, primarily focused on understanding the expulsion patterns of respiratory particulates from infected hosts and their dispersion in confined spaces. Largely overlooked has been the crucial role of fluid dynamics in guiding inhaled virus-laden particulates within the respiratory cavity, thereby directing the pathogens to the infection-prone upper airway sites. Here, we discuss a multi-scale approach for modeling the onset parameters of airway infection based on flow physics. The findings are backed by Large Eddy Simulations of inhaled airflow and computed trajectories of pathogen-bearing aerosols/droplets within two clinically healthy and anatomically realistic airway geometries reconstructed from computed tomography imaging. As a representative anisotropic pathogen that can transmit aerially, we have picked smallpox from the Poxviridae family to demonstrate the approach. The fluid dynamics findings on inhaled transmission trends are integrated with virological and epidemiological parameters for smallpox (e.g., viral concentration in host ejecta, physical properties of virions, and typical exposure durations) to establish the corresponding infectious dose (i.e., the number of virions potent enough to launch infection in an exposed subject) to be, at maximum, of the order of O(2), or more precisely 1 to 180. The projection agrees remarkably well with the known virological parameters for smallpox.","sentences":["Investigations on airborne transmission of pathogens constitute a rapidly expanding field, primarily focused on understanding the expulsion patterns of respiratory particulates from infected hosts and their dispersion in confined spaces.","Largely overlooked has been the crucial role of fluid dynamics in guiding inhaled virus-laden particulates within the respiratory cavity, thereby directing the pathogens to the infection-prone upper airway sites.","Here, we discuss a multi-scale approach for modeling the onset parameters of airway infection based on flow physics.","The findings are backed by Large Eddy Simulations of inhaled airflow and computed trajectories of pathogen-bearing aerosols/droplets within two clinically healthy and anatomically realistic airway geometries reconstructed from computed tomography imaging.","As a representative anisotropic pathogen that can transmit aerially, we have picked smallpox from the Poxviridae family to demonstrate the approach.","The fluid dynamics findings on inhaled transmission trends are integrated with virological and epidemiological parameters for smallpox (e.g., viral concentration in host ejecta, physical properties of virions, and typical exposure durations) to establish the corresponding infectious dose (i.e., the number of virions potent enough to launch infection in an exposed subject) to be, at maximum, of the order of O(2), or more precisely 1 to 180.","The projection agrees remarkably well with the known virological parameters for smallpox."],"url":"http://arxiv.org/abs/2403.04064v1","category":"physics.flu-dyn"}
{"created":"2024-03-06 21:30:44","title":"Assigning Entities to Teams as a Hypergraph Discovery Problem","abstract":"We propose a team assignment algorithm based on a hypergraph approach focusing on resilience and diffusion optimization. Specifically, our method is based on optimizing the algebraic connectivity of the Laplacian matrix of an edge-dependent vertex-weighted hypergraph. We used constrained simulated annealing, where we constrained the effort agents can exert to perform a task and the minimum effort a task requires to be completed. We evaluated our methods in terms of the number of unsuccessful patches to drive our solution into the feasible region and the cost of patching. We showed that our formulation provides more robust solutions than the original data and the greedy approach. We hope that our methods motivate further research in applying hypergraphs to similar problems in different research areas and in exploring variations of our methods.","sentences":["We propose a team assignment algorithm based on a hypergraph approach focusing on resilience and diffusion optimization.","Specifically, our method is based on optimizing the algebraic connectivity of the Laplacian matrix of an edge-dependent vertex-weighted hypergraph.","We used constrained simulated annealing, where we constrained the effort agents can exert to perform a task and the minimum effort a task requires to be completed.","We evaluated our methods in terms of the number of unsuccessful patches to drive our solution into the feasible region and the cost of patching.","We showed that our formulation provides more robust solutions than the original data and the greedy approach.","We hope that our methods motivate further research in applying hypergraphs to similar problems in different research areas and in exploring variations of our methods."],"url":"http://arxiv.org/abs/2403.04063v1","category":"cs.SI"}
{"created":"2024-03-06 20:58:25","title":"An Identity of Hankel Matrices Generated from the Moments of Gaussian Distribution","abstract":"In this letter, we proved a matrix identity of Hankel matrices that seems unrevealed before, generated from the moments of Gaussian distributions. In particular, we derived the Cholesky decompositions of the Hankel matrices in closed-forms, and showed some interesting connections between them. The results have potential applications in such as optimizing a nonlinear (NL) distortion function that maximizes the receiving gain in wireless communication systems.","sentences":["In this letter, we proved a matrix identity of Hankel matrices that seems unrevealed before, generated from the moments of Gaussian distributions.","In particular, we derived the Cholesky decompositions of the Hankel matrices in closed-forms, and showed some interesting connections between them.","The results have potential applications in such as optimizing a nonlinear (NL) distortion function that maximizes the receiving gain in wireless communication systems."],"url":"http://arxiv.org/abs/2403.04052v1","category":"cs.IT"}
{"created":"2024-03-06 20:15:25","title":"Spanning Tree-based Query Plan Enumeration","abstract":"In this work, we define the problem of finding an optimal query plan as finding spanning trees with low costs. This approach empowers the utilization of a series of spanning tree algorithms, thereby enabling systematic exploration of the plan search space over a join graph. Capitalizing on the polynomial time complexity of spanning tree algorithms, we present the Ensemble Spanning Tree Enumeration (ESTE) strategy. ESTE employs two conventional spanning tree algorithms, Prim's and Kruskal's, together to enhance the robustness of the query optimizer. In ESTE, multiple query plans are enumerated exploring different areas of the search space. This positions ESTE as an intermediate strategy between exhaustive and heuristic enumeration strategies. We show that ESTE is more robust in identifying efficient query plans for large queries. In the case of data modifications and workload demand increase, we believe that our approach can be a cheaper alternative to maintain optimizer robustness by integrating additional spanning tree algorithms rather than completely changing the optimizer to another plan enumeration algorithm. The experimental evaluation shows that ESTE achieves better consistency in plan quality and optimization time than existing solutions while identifying similarly optimal plans.","sentences":["In this work, we define the problem of finding an optimal query plan as finding spanning trees with low costs.","This approach empowers the utilization of a series of spanning tree algorithms, thereby enabling systematic exploration of the plan search space over a join graph.","Capitalizing on the polynomial time complexity of spanning tree algorithms, we present the Ensemble Spanning Tree Enumeration (ESTE) strategy.","ESTE employs two conventional spanning tree algorithms, Prim's and Kruskal's, together to enhance the robustness of the query optimizer.","In ESTE, multiple query plans are enumerated exploring different areas of the search space.","This positions ESTE as an intermediate strategy between exhaustive and heuristic enumeration strategies.","We show that ESTE is more robust in identifying efficient query plans for large queries.","In the case of data modifications and workload demand increase, we believe that our approach can be a cheaper alternative to maintain optimizer robustness by integrating additional spanning tree algorithms rather than completely changing the optimizer to another plan enumeration algorithm.","The experimental evaluation shows that ESTE achieves better consistency in plan quality and optimization time than existing solutions while identifying similarly optimal plans."],"url":"http://arxiv.org/abs/2403.04026v1","category":"cs.DB"}
{"created":"2024-03-06 19:46:44","title":"Temporal Cross-Attention for Dynamic Embedding and Tokenization of Multimodal Electronic Health Records","abstract":"The breadth, scale, and temporal granularity of modern electronic health records (EHR) systems offers great potential for estimating personalized and contextual patient health trajectories using sequential deep learning. However, learning useful representations of EHR data is challenging due to its high dimensionality, sparsity, multimodality, irregular and variable-specific recording frequency, and timestamp duplication when multiple measurements are recorded simultaneously. Although recent efforts to fuse structured EHR and unstructured clinical notes suggest the potential for more accurate prediction of clinical outcomes, less focus has been placed on EHR embedding approaches that directly address temporal EHR challenges by learning time-aware representations from multimodal patient time series. In this paper, we introduce a dynamic embedding and tokenization framework for precise representation of multimodal clinical time series that combines novel methods for encoding time and sequential position with temporal cross-attention. Our embedding and tokenization framework, when integrated into a multitask transformer classifier with sliding window attention, outperformed baseline approaches on the exemplar task of predicting the occurrence of nine postoperative complications of more than 120,000 major inpatient surgeries using multimodal data from three hospitals and two academic health centers in the United States.","sentences":["The breadth, scale, and temporal granularity of modern electronic health records (EHR) systems offers great potential for estimating personalized and contextual patient health trajectories using sequential deep learning.","However, learning useful representations of EHR data is challenging due to its high dimensionality, sparsity, multimodality, irregular and variable-specific recording frequency, and timestamp duplication when multiple measurements are recorded simultaneously.","Although recent efforts to fuse structured EHR and unstructured clinical notes suggest the potential for more accurate prediction of clinical outcomes, less focus has been placed on EHR embedding approaches that directly address temporal EHR challenges by learning time-aware representations from multimodal patient time series.","In this paper, we introduce a dynamic embedding and tokenization framework for precise representation of multimodal clinical time series that combines novel methods for encoding time and sequential position with temporal cross-attention.","Our embedding and tokenization framework, when integrated into a multitask transformer classifier with sliding window attention, outperformed baseline approaches on the exemplar task of predicting the occurrence of nine postoperative complications of more than 120,000 major inpatient surgeries using multimodal data from three hospitals and two academic health centers in the United States."],"url":"http://arxiv.org/abs/2403.04012v1","category":"cs.LG"}
{"created":"2024-03-06 19:39:20","title":"Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical Systems","abstract":"We develop provably safe and convergent reinforcement learning (RL) algorithms for control of nonlinear dynamical systems, bridging the gap between the hard safety guarantees of control theory and the convergence guarantees of RL theory. Recent advances at the intersection of control and RL follow a two-stage, safety filter approach to enforcing hard safety constraints: model-free RL is used to learn a potentially unsafe controller, whose actions are projected onto safe sets prescribed, for example, by a control barrier function. Though safe, such approaches lose any convergence guarantees enjoyed by the underlying RL methods. In this paper, we develop a single-stage, sampling-based approach to hard constraint satisfaction that learns RL controllers enjoying classical convergence guarantees while satisfying hard safety constraints throughout training and deployment. We validate the efficacy of our approach in simulation, including safe control of a quadcopter in a challenging obstacle avoidance problem, and demonstrate that it outperforms existing benchmarks.","sentences":["We develop provably safe and convergent reinforcement learning (RL) algorithms for control of nonlinear dynamical systems, bridging the gap between the hard safety guarantees of control theory and the convergence guarantees of RL theory.","Recent advances at the intersection of control and RL follow a two-stage, safety filter approach to enforcing hard safety constraints: model-free RL is used to learn a potentially unsafe controller, whose actions are projected onto safe sets prescribed, for example, by a control barrier function.","Though safe, such approaches lose any convergence guarantees enjoyed by the underlying RL methods.","In this paper, we develop a single-stage, sampling-based approach to hard constraint satisfaction that learns RL controllers enjoying classical convergence guarantees while satisfying hard safety constraints throughout training and deployment.","We validate the efficacy of our approach in simulation, including safe control of a quadcopter in a challenging obstacle avoidance problem, and demonstrate that it outperforms existing benchmarks."],"url":"http://arxiv.org/abs/2403.04007v1","category":"cs.LG"}
{"created":"2024-03-06 19:16:57","title":"Fair Artificial Currency Incentives in Repeated Weighted Congestion Games: Equity vs. Equality","abstract":"When users access shared resources in a selfish manner, the resulting societal cost and perceived users' cost is often higher than what would result from a centrally coordinated optimal allocation. While several contributions in mechanism design manage to steer the aggregate users choices to the desired optimum by using monetary tolls, such approaches bear the inherent drawback of discriminating against users with a lower income. More recently, incentive schemes based on artificial currencies have been studied with the goal of achieving a system-optimal resource allocation that is also fair. In this resource-sharing context, this paper focuses on repeated weighted congestion game with two resources, where users contribute to the congestion to different extents that are captured by individual weights. First, we address the broad concept of fairness by providing a rigorous mathematical characterization of the distinct societal metrics of equity and equality, i.e., the concepts of providing equal outcomes and equal opportunities, respectively. Second, we devise weight-dependent and time-invariant optimal pricing policies to maximize equity and equality, and prove convergence of the aggregate user choices to the system-optimum. In our framework it is always possible to achieve system-optimal allocations with perfect equity, while the maximum equality that can be reached may not be perfect, which is also shown via numerical simulations.","sentences":["When users access shared resources in a selfish manner, the resulting societal cost and perceived users' cost is often higher than what would result from a centrally coordinated optimal allocation.","While several contributions in mechanism design manage to steer the aggregate users choices to the desired optimum by using monetary tolls, such approaches bear the inherent drawback of discriminating against users with a lower income.","More recently, incentive schemes based on artificial currencies have been studied with the goal of achieving a system-optimal resource allocation that is also fair.","In this resource-sharing context, this paper focuses on repeated weighted congestion game with two resources, where users contribute to the congestion to different extents that are captured by individual weights.","First, we address the broad concept of fairness by providing a rigorous mathematical characterization of the distinct societal metrics of equity and equality, i.e., the concepts of providing equal outcomes and equal opportunities, respectively.","Second, we devise weight-dependent and time-invariant optimal pricing policies to maximize equity and equality, and prove convergence of the aggregate user choices to the system-optimum.","In our framework it is always possible to achieve system-optimal allocations with perfect equity, while the maximum equality that can be reached may not be perfect, which is also shown via numerical simulations."],"url":"http://arxiv.org/abs/2403.03999v1","category":"cs.GT"}
{"created":"2024-03-06 19:11:08","title":"Cafe-Mpc: A Cascaded-Fidelity Model Predictive Control Framework with Tuning-Free Whole-Body Control","abstract":"This work introduces an optimization-based locomotion control framework for on-the-fly synthesis of complex dynamic maneuvers. At the core of the proposed framework is a cascaded-fidelity model predictive controller (Cafe-Mpc). Cafe-Mpc strategically relaxes the planning problem along the prediction horizon (i.e., with descending model fidelity, increasingly coarse time steps, and relaxed constraints) for computational and performance gains. This problem is numerically solved with an efficient customized multiple-shooting iLQR (MS-iLQR) solver that is tailored for hybrid systems. The action-value function from Cafe-Mpc is then used as the basis for a new value-function-based whole-body control (VWBC) technique that avoids additional tuning for the WBC. In this respect, the proposed framework unifies whole-body MPC and more conventional whole-body quadratic programming (QP), which have been treated as separate components in previous works. We study the effects of the cascaded relaxations in Cafe-Mpc on the tracking performance and required computation time. We also show that the \\cmpc , if configured appropriately, advances the performance of whole-body MPC without necessarily increasing computational cost. Further, we show the superior performance of the proposed VWBC over the Ricatti feedback controller in terms of constraint handling. The proposed framework enables accomplishing for the first time gymnastic-style running barrel roll on the MIT Mini Cheetah, a task where conventional MPC fails. Video: https://youtu.be/YiNqrgj9mb8.","sentences":["This work introduces an optimization-based locomotion control framework for on-the-fly synthesis of complex dynamic maneuvers.","At the core of the proposed framework is a cascaded-fidelity model predictive controller (Cafe-Mpc).","Cafe-Mpc strategically relaxes the planning problem along the prediction horizon (i.e., with descending model fidelity, increasingly coarse time steps, and relaxed constraints) for computational and performance gains.","This problem is numerically solved with an efficient customized multiple-shooting iLQR (MS-iLQR) solver that is tailored for hybrid systems.","The action-value function from Cafe-Mpc is then used as the basis for a new value-function-based whole-body control (VWBC) technique that avoids additional tuning for the WBC.","In this respect, the proposed framework unifies whole-body MPC and more conventional whole-body quadratic programming (QP), which have been treated as separate components in previous works.","We study the effects of the cascaded relaxations in Cafe-Mpc on the tracking performance and required computation time.","We also show that the \\cmpc , if configured appropriately, advances the performance of whole-body MPC without necessarily increasing computational cost.","Further, we show the superior performance of the proposed VWBC over the Ricatti feedback controller in terms of constraint handling.","The proposed framework enables accomplishing for the first time gymnastic-style running barrel roll on the MIT Mini Cheetah, a task where conventional MPC fails.","Video: https://youtu.be/YiNqrgj9mb8."],"url":"http://arxiv.org/abs/2403.03995v1","category":"cs.RO"}
{"created":"2024-03-06 19:00:10","title":"GLANCE -- Gravitational Lensing Authenticator using Non-Modelled Cross-Correlation Exploration of Gravitational Wave Signals","abstract":"Gravitational lensing is the effect on a lightlike trajectory by the presence of matter, affecting its trajectory in spacetime. Gravitational lensing of gravitational waves can occur in geometric optics limit (when GW wavelength is much smaller than the Schwarzschild radius of the lens i.e. $\\lambda_{GW} \\ll$ R$^{\\rm sc}_{\\rm lens}$, multiple images with different magnifications are formed) known as strong-lensing or in wave optics limit (when the wavelength of GW is larger than the Schwarzschild radius i.e. R$^{\\rm sc}_{\\rm lens}$ $\\lesssim \\lambda_{GW} $, interfering signals produce beating pattern in the waveform envelope) known as micro-lensing. Currently, large sky-localization errors of GW sources and strong noise-PSD have barred us from evidencing lensed GWs. Considering this aspect, we have developed $\\texttt{GLANCE}$, a novel technique to detect lensed GWs. We demonstrate that cross-correlation between the data pieces containing lensed signals shows a very specific trend. The strength of the cross-correlation signal can quantify the significance of the event(s) being lensed. Since lensing impacts the inference of the source parameters, primarily the luminosity distance for the strong lensing case, a joint parameter estimation of the source and lens-induced parameters is incorporated in $\\texttt{GLANCE}$ using a Bayesian framework. We applied our method to simulated strongly lensed data and we have shown that $\\texttt{GLANCE}$ not only can detect lensed GW signals but also can correctly infer the injected source and lens parameters even when one of the signals is below the match-filtered threshold SNR. This demonstrates the capability of $\\texttt{GLANCE}$ for a robust detection of lensed GW signal from noisy data.","sentences":["Gravitational lensing is the effect on a lightlike trajectory by the presence of matter, affecting its trajectory in spacetime.","Gravitational lensing of gravitational waves can occur in geometric optics limit (when GW wavelength is much smaller than the Schwarzschild radius of the lens i.e. $\\lambda_{GW} \\ll$ R$^{\\rm sc}_{\\rm lens}$, multiple images with different magnifications are formed) known as strong-lensing or in wave optics limit (when the wavelength of GW is larger than the Schwarzschild radius i.e. R$^{\\rm sc}_{\\rm lens}$ $\\lesssim \\lambda_{GW} $, interfering signals produce beating pattern in the waveform envelope) known as micro-lensing.","Currently, large sky-localization errors of GW sources and strong noise-PSD have barred us from evidencing lensed GWs.","Considering this aspect, we have developed $\\texttt{GLANCE}$, a novel technique to detect lensed GWs.","We demonstrate that cross-correlation between the data pieces containing lensed signals shows a very specific trend.","The strength of the cross-correlation signal can quantify the significance of the event(s) being lensed.","Since lensing impacts the inference of the source parameters, primarily the luminosity distance for the strong lensing case, a joint parameter estimation of the source and lens-induced parameters is incorporated in $\\texttt{GLANCE}$ using a Bayesian framework.","We applied our method to simulated strongly lensed data and we have shown that $\\texttt{GLANCE}$ not only can detect lensed GW signals but also can correctly infer the injected source and lens parameters even when one of the signals is below the match-filtered threshold SNR.","This demonstrates the capability of $\\texttt{GLANCE}$ for a robust detection of lensed GW signal from noisy data."],"url":"http://arxiv.org/abs/2403.03982v1","category":"gr-qc"}
{"created":"2024-03-06 18:24:07","title":"Creating and troubleshooting microscopy analysis workflows: common challenges and common solutions","abstract":"As microscopy diversifies and becomes ever-more complex, the problem of quantification of microscopy images has emerged as a major roadblock for many researchers. All researchers must face certain challenges in turning microscopy images into answers, independent of their scientific question and the images they've generated. Challenges may arise at many stages throughout the analysis process, including handling of the image files, image pre-processing, object finding, or measurement, and statistical analysis. While the exact solution required for each obstacle will be problem-specific, by understanding tools and tradeoffs, optimizing data quality, breaking workflows and data sets into chunks, talking to experts, and thoroughly documenting what has been done, analysts at any experience level can learn to overcome these challenges and create better and easier image analyses.","sentences":["As microscopy diversifies and becomes ever-more complex, the problem of quantification of microscopy images has emerged as a major roadblock for many researchers.","All researchers must face certain challenges in turning microscopy images into answers, independent of their scientific question and the images they've generated.","Challenges may arise at many stages throughout the analysis process, including handling of the image files, image pre-processing, object finding, or measurement, and statistical analysis.","While the exact solution required for each obstacle will be problem-specific, by understanding tools and tradeoffs, optimizing data quality, breaking workflows and data sets into chunks, talking to experts, and thoroughly documenting what has been done, analysts at any experience level can learn to overcome these challenges and create better and easier image analyses."],"url":"http://arxiv.org/abs/2403.04520v1","category":"q-bio.OT"}
{"created":"2024-03-06 18:04:45","title":"On Outer Bi-Lipschitz Extensions of Linear Johnson-Lindenstrauss Embeddings of Subsets of $\\mathbb{R}^N$","abstract":"The celebrated Johnson-Lindenstrauss lemma states that for all $\\varepsilon \\in (0,1)$ and finite sets $X \\subseteq \\mathbb{R}^N$ with $n>1$ elements, there exists a matrix $\\Phi \\in \\mathbb{R}^{m \\times N}$ with $m=\\mathcal{O}(\\varepsilon^{-2}\\log n)$ such that \\[ (1 - \\varepsilon) \\|x-y\\|_2 \\leq \\|\\Phi x-\\Phi y\\|_2 \\leq (1+\\varepsilon)\\| x- y\\|_2 \\quad \\forall\\, x, y \\in X.\\] Herein we consider terminal embedding results which have recently been introduced in the computer science literature as stronger extensions of the Johnson-Lindenstrauss lemma for finite sets. After a short survey of this relatively recent line of work, we extend the theory of terminal embeddings to hold for arbitrary (e.g., infinite) subsets $X \\subseteq \\mathbb{R}^N$, and then specialize our generalized results to the case where $X$ is a low-dimensional compact submanifold of $\\mathbb{R}^N$. In particular, we prove the following generalization of the Johnson-Lindenstrauss lemma: For all $\\varepsilon \\in (0,1)$ and $X\\subseteq\\mathbb{R}^N$, there exists a terminal embedding $f: \\mathbb{R}^N \\longrightarrow \\mathbb{R}^{m}$ such that $$(1 - \\varepsilon) \\| x - y \\|_2 \\leq \\left\\| f(x) - f(y) \\right\\|_2 \\leq (1 + \\varepsilon) \\| x - y \\|_2 \\quad \\forall \\, x \\in X ~{\\rm and}~ \\forall \\, y \\in \\mathbb{R}^N.$$ Crucially, we show that the dimension $m$ of the range of $f$ above is optimal up to multiplicative constants, satisfying $m=\\mathcal{O}(\\varepsilon^{-2} \\omega^2(S_X))$, where $\\omega(S_X)$ is the Gaussian width of the set of unit secants of $X$, $S_X=\\overline{\\{(x-y)/\\|x-y\\|_2 \\colon x \\neq y \\in X\\}}$. Furthermore, our proofs are constructive and yield algorithms for computing a general class of terminal embeddings $f$, an instance of which is demonstrated herein to allow for more accurate compressive nearest neighbor classification than standard linear Johnson-Lindenstrauss embeddings do in practice.","sentences":["The celebrated Johnson-Lindenstrauss lemma states that for all $\\varepsilon \\in (0,1)$ and finite sets $X \\subseteq \\mathbb{R}^N$ with $n>1$ elements, there exists a matrix $\\Phi \\in \\mathbb{R}^{m \\times","N}$ with $m=\\mathcal{O}(\\varepsilon^{-2}\\log n)$ such that \\[ (1 - \\varepsilon) \\|x-y\\|_2 \\leq \\|\\Phi","x-\\Phi","y\\|_2 \\leq (1+\\varepsilon)\\| x-","y\\|_2","\\quad \\forall\\, x, y \\in X.\\]","Herein we consider terminal embedding results which have recently been introduced in the computer science literature as stronger extensions of the Johnson-Lindenstrauss lemma for finite sets.","After a short survey of this relatively recent line of work, we extend the theory of terminal embeddings to hold for arbitrary (e.g., infinite) subsets $X \\subseteq \\mathbb{R}^N$, and then specialize our generalized results to the case where $X$ is a low-dimensional compact submanifold of $\\mathbb{R}^N$. In particular, we prove the following generalization of the Johnson-Lindenstrauss lemma: For all $\\varepsilon \\in (0,1)$ and $X\\subseteq\\mathbb{R}^N$, there exists a terminal embedding $f: \\mathbb{R}^N","\\longrightarrow \\mathbb{R}^{m}$ such that $$(1 - \\varepsilon) \\|","x - y \\|_2 \\leq \\left\\| f(x) - f(y) \\right\\|_2 \\leq (1 + \\varepsilon) \\| x - y \\|_2 \\quad \\forall \\, x \\in X ~{\\rm and}~ \\forall \\, y \\in \\mathbb{R}^N.$$ Crucially, we show that the dimension $m$ of the range of $f$ above is optimal up to multiplicative constants, satisfying $m=\\mathcal{O}(\\varepsilon^{-2} \\omega^2(S_X))$, where $\\omega(S_X)$ is the Gaussian width of the set of unit secants of $X$, $S_X=\\overline{\\{(x-y)/\\|x-y\\|_2 \\colon x \\neq y \\in X\\}}$.","Furthermore, our proofs are constructive and yield algorithms for computing a general class of terminal embeddings $f$, an instance of which is demonstrated herein to allow for more accurate compressive nearest neighbor classification than standard linear Johnson-Lindenstrauss embeddings do in practice."],"url":"http://arxiv.org/abs/2403.03969v1","category":"math.MG"}
{"created":"2024-03-07 18:56:58","title":"Exclusive production of double light neutral mesons at the $e^+e^-$ colliders","abstract":"In this work we investigate the exclusive production of a pair of light neutral mesons in $e^+e^-$ annihilation, where the final state bears an even $C$-parity. The production processes can be initiated via the photon fragmentation or the non-fragmentation mechanism. While the fragmentation contribution can be rigorously accounted, the non-fragmentation contributions are calculated within the framework of collinear factorization, where only the leading-twist light-cone distribution amplitudes (LCDAs) of mesons are considered. Mediately solely by the non-fragmentation mechanism, the production rates of double light neutral pseudoscalar mesons are too small to be observed at the commissioning $e^+e^-$ facilities. In contrast, the production rates of a pair of light neutral vector mesons are greatly amplified owing to the significant kinematic enhancement brought by the fragmentation mechanism. It is found that, at $\\sqrt{s}=3.77$ GeV, after including the destructive interference between the non-fragmentation and fragmentation contributions, the production rates for $e^+e^-\\to \\rho^{0}\\rho^{0}$ and $\\rho^0\\omega$ can be lowered by about 10\\% and 30\\% relative to the fragmentation predictions. Future precise measurement of these exclusive double neutral vector meson production channels at {\\tt BESIII} experiment may provide useful constraints on the LCDAs of light vector mesons.","sentences":["In this work we investigate the exclusive production of a pair of light neutral mesons in $e^+e^-$ annihilation, where the final state bears an even $C$-parity.","The production processes can be initiated via the photon fragmentation or the non-fragmentation mechanism.","While the fragmentation contribution can be rigorously accounted, the non-fragmentation contributions are calculated within the framework of collinear factorization, where only the leading-twist light-cone distribution amplitudes (LCDAs) of mesons are considered.","Mediately solely by the non-fragmentation mechanism, the production rates of double light neutral pseudoscalar mesons are too small to be observed at the commissioning $e^+e^-$ facilities.","In contrast, the production rates of a pair of light neutral vector mesons are greatly amplified owing to the significant kinematic enhancement brought by the fragmentation mechanism.","It is found that, at $\\sqrt{s}=3.77$ GeV, after including the destructive interference between the non-fragmentation and fragmentation contributions, the production rates for $e^+e^-\\to \\rho^{0}\\rho^{0}$ and $\\rho^0\\omega$ can be lowered by about 10\\% and 30\\% relative to the fragmentation predictions.","Future precise measurement of these exclusive double neutral vector meson production channels at {\\tt BESIII} experiment may provide useful constraints on the LCDAs of light vector mesons."],"url":"http://arxiv.org/abs/2403.04762v1","category":"hep-ph"}
{"created":"2024-03-07 18:23:45","title":"Discrete-to-continuous crystalline curvature flows","abstract":"We consider here a fully discrete variant of the implicit variational scheme for mean curvature flow [AlmTayWan,LucStu], in a setting where the flow is governed by a crystalline surface tension defined by the limit of pairwise interactions energy on the discrete grid. The algorithm is based on a new discrete distance from the evolving sets, which prevents the occurrence of the spatial drift and pinning phenomena identified in [BraGelNov] in a similar discrete framework. We provide the first rigorous convergence result holding in any dimension, for any initial set and for a large class of purely crystalline anisotropies, in which the spatial discretization mesh can be of the same order or coarser than the time step.","sentences":["We consider here a fully discrete variant of the implicit variational scheme for mean curvature flow [AlmTayWan,LucStu], in a setting where the flow is governed by a crystalline surface tension defined by the limit of pairwise interactions energy on the discrete grid.","The algorithm is based on a new discrete distance from the evolving sets, which prevents the occurrence of the spatial drift and pinning phenomena identified in [BraGelNov] in a similar discrete framework.","We provide the first rigorous convergence result holding in any dimension, for any initial set and for a large class of purely crystalline anisotropies, in which the spatial discretization mesh can be of the same order or coarser than the time step."],"url":"http://arxiv.org/abs/2403.04725v1","category":"math.AP"}
{"created":"2024-03-07 18:06:56","title":"Isomeric states of fission fragments explored via Penning trap mass spectrometry at IGISOL","abstract":"The masses of $^{84}$Br, $^{105}$Mo, $^{115,119,121}$Pd, $^{122}$Ag, $^{127,129}$In, $^{132}$Sb and their respective isomeric states have been measured with the JYFLTRAP Penning trap mass spectrometer using the phase-imaging ion-cyclotron-resonance technique. The excitation energies of the isomeric states in $^{132}$Sb and $^{119}$Pd were experimentally determined for the first time, while for $^{84}$Br, $^{115}$Pd and $^{127,129}$In, the precision of the mass values was substantially improved. In $^{105}$Mo and $^{121}$Pd there were no signs of a long-lived isomeric state. The ground-state measurements of $^{119}$Pd and $^{122}$Ag indicated that both are significantly more bound than the literature values. For $^{122}$Ag, there was no indication of a proposed third long-lived state. The results for the $N=49$ nucleus $^{84}$Br and isomers close to doubly magic $^{132}$Sn have been compared to the shell-model, proton-neutron quasi-particle random-phase approximation (pnQRPA) and the microscopic quasiparticle-phonon model (MQPM) calculations.","sentences":["The masses of $^{84}$Br, $^{105}$Mo, $^{115,119,121}$Pd, $^{122}$Ag, $^{127,129}$In, $^{132}$Sb and their respective isomeric states have been measured with the JYFLTRAP Penning trap mass spectrometer using the phase-imaging ion-cyclotron-resonance technique.","The excitation energies of the isomeric states in $^{132}$Sb and $^{119}$Pd were experimentally determined for the first time, while for $^{84}$Br, $^{115}$Pd and $^{127,129}$In, the precision of the mass values was substantially improved.","In $^{105}$Mo and $^{121}$Pd there were no signs of a long-lived isomeric state.","The ground-state measurements of $^{119}$Pd and $^{122}$Ag indicated that both are significantly more bound than the literature values.","For $^{122}$Ag, there was no indication of a proposed third long-lived state.","The results for the $N=49$ nucleus $^{84}$Br and isomers close to doubly magic $^{132}$Sn have been compared to the shell-model, proton-neutron quasi-particle random-phase approximation (pnQRPA) and the microscopic quasiparticle-phonon model (MQPM) calculations."],"url":"http://arxiv.org/abs/2403.04710v1","category":"nucl-ex"}
{"created":"2024-03-07 17:34:02","title":"Invariant amplitudes, unpolarized cross sections, and polarization asymmetries in (anti)neutrino-nucleon elastic scattering","abstract":"At leading order in weak and electromagnetic couplings, cross sections for (anti)neutrino-nucleon elastic scattering are determined by four nucleon form factors that depend on the momentum transfer $Q^2$. Including radiative corrections in the Standard Model and potential new physics contributions beyond the Standard Model, eight invariant amplitudes are possible, depending on both $Q^2$ and the (anti)neutrino energy $E_\\nu$. We review the definition of these amplitudes and use them to compute both unpolarized and polarized observables including radiative corrections. We show that unpolarized accelerator neutrino cross-section measurements can probe new physics parameter space within the constraints inferred from precision beta decay measurements.","sentences":["At leading order in weak and electromagnetic couplings, cross sections for (anti)neutrino-nucleon elastic scattering are determined by four nucleon form factors that depend on the momentum transfer $Q^2$. Including radiative corrections in the Standard Model and potential new physics contributions beyond the Standard Model, eight invariant amplitudes are possible, depending on both $Q^2$ and the (anti)neutrino energy $E_\\nu$. We review the definition of these amplitudes and use them to compute both unpolarized and polarized observables including radiative corrections.","We show that unpolarized accelerator neutrino cross-section measurements can probe new physics parameter space within the constraints inferred from precision beta decay measurements."],"url":"http://arxiv.org/abs/2403.04687v1","category":"hep-ph"}
{"created":"2024-03-07 17:15:34","title":"Self-focused pulse propagation is mediated by spatiotemporal optical vortices","abstract":"We show that the dynamics of high-intensity laser pulses undergoing self-focused propagation in a nonlinear medium can be understood in terms of the topological constraints imposed by the formation and evolution of spatiotemporal optical vortices (STOVs). STOVs are born from point phase defects on the sides of the pulse nucleated by spatiotemporal phase shear. These defects grow into closed loops of spatiotemporal vorticity that initially exclude the pulse propagation axis, but then reconnect to form a pair of toroidal vortex rings that wrap around it. STOVs constrain the intrapulse flow of electromagnetic energy, controlling the focusing-defocusing cycles and pulse splitting inherent to nonlinear pulse propagation. We illustrate this in two widely studied but very different regimes, relativistic self-focusing in plasma and non-relativistic self-focusing in gas, demonstrating that STOVs mediate nonlinear propagation irrespective of the detailed physics.","sentences":["We show that the dynamics of high-intensity laser pulses undergoing self-focused propagation in a nonlinear medium can be understood in terms of the topological constraints imposed by the formation and evolution of spatiotemporal optical vortices (STOVs).","STOVs are born from point phase defects on the sides of the pulse nucleated by spatiotemporal phase shear.","These defects grow into closed loops of spatiotemporal vorticity that initially exclude the pulse propagation axis, but then reconnect to form a pair of toroidal vortex rings that wrap around it.","STOVs constrain the intrapulse flow of electromagnetic energy, controlling the focusing-defocusing cycles and pulse splitting inherent to nonlinear pulse propagation.","We illustrate this in two widely studied but very different regimes, relativistic self-focusing in plasma and non-relativistic self-focusing in gas, demonstrating that STOVs mediate nonlinear propagation irrespective of the detailed physics."],"url":"http://arxiv.org/abs/2403.04669v1","category":"physics.optics"}
{"created":"2024-03-07 17:08:05","title":"Prospects for Heavy Neutral Lepton Searches at Short and Medium Baseline Reactor Experiments","abstract":"Heavy neutrinos with masses in the MeV range can in principle simultaneously explain the light neutrino masses and the origin of baryonic matter in the universe. The strongest constraints on their properties come from their potential impact on the formation of light elements in the early universe. Since these constraints rely on assumptions about the cosmic history, independent checks in the laboratory are highly desirable. In this paper, we discuss the opportunity to search for heavy neutrinos within the MeV mass range in short and medium baseline reactor neutrino experiments, using the SoLid, JUNO and TAO experiments as examples. This kind of experiments can give the currently strongest upper bound on the mixing between the light electron neutrinos and the heavy neutrino in the 2-9 MeV mass range.","sentences":["Heavy neutrinos with masses in the MeV range can in principle simultaneously explain the light neutrino masses and the origin of baryonic matter in the universe.","The strongest constraints on their properties come from their potential impact on the formation of light elements in the early universe.","Since these constraints rely on assumptions about the cosmic history, independent checks in the laboratory are highly desirable.","In this paper, we discuss the opportunity to search for heavy neutrinos within the MeV mass range in short and medium baseline reactor neutrino experiments, using the SoLid, JUNO and TAO experiments as examples.","This kind of experiments can give the currently strongest upper bound on the mixing between the light electron neutrinos and the heavy neutrino in the 2-9 MeV mass range."],"url":"http://arxiv.org/abs/2403.04662v1","category":"hep-ph"}
{"created":"2024-03-07 16:04:01","title":"Correction and standardisation of lung oscillometry techniques using parameter inference: A study group report","abstract":"This report relates to a study group hosted by the EPSRC funded network, Integrating data-driven BIOphysical models into REspiratory MEdicine (BIOREME), and supported by The Insigneo Institute and The Knowledge Transfer Network. The BIOREME network hosts events, including this study group, to bring together multi-disciplinary researchers, clinicians, companies and charities to catalyse research in the applications of mathematical modelling for respiratory medicine. The goal of this study group was to provide an interface between companies, clinicians, and mathematicians to develop mathematical tools to the problems presented. The study group was held at The University of Sheffield on the 17 - 20 April 2023 and was attended by 24 researchers from 13 different institutions.   This report relates to a challenge presented by Arete Medical Technologies relating to impulse oscillometry (IOS), whereby a short pressure oscillation is imposed at a person's mouth during normal breathing, usually by a loudspeaker. The resulting pressure and flow rate changes can be used to the impedance of the airways, which in turn can provide proxy measurements for (patho)physiological changes in the small airways. Disentangling the signal so that airway mechanics can be measured accurately (and device properties/environmental effects can be accounted for) remains an open challenge that has the potential to significantly improve the device and its translation to clinic. In this report, several approaches to this problem, and the wider problem of interpreting oscillometry resuts are explored.","sentences":["This report relates to a study group hosted by the EPSRC funded network, Integrating data-driven BIOphysical models into REspiratory MEdicine (BIOREME), and supported by The Insigneo Institute and The Knowledge Transfer Network.","The BIOREME network hosts events, including this study group, to bring together multi-disciplinary researchers, clinicians, companies and charities to catalyse research in the applications of mathematical modelling for respiratory medicine.","The goal of this study group was to provide an interface between companies, clinicians, and mathematicians to develop mathematical tools to the problems presented.","The study group was held at The University of Sheffield on the 17 - 20 April 2023 and was attended by 24 researchers from 13 different institutions.   ","This report relates to a challenge presented by Arete Medical Technologies relating to impulse oscillometry (IOS), whereby a short pressure oscillation is imposed at a person's mouth during normal breathing, usually by a loudspeaker.","The resulting pressure and flow rate changes can be used to the impedance of the airways, which in turn can provide proxy measurements for (patho)physiological changes in the small airways.","Disentangling the signal so that airway mechanics can be measured accurately (and device properties/environmental effects can be accounted for) remains an open challenge that has the potential to significantly improve the device and its translation to clinic.","In this report, several approaches to this problem, and the wider problem of interpreting oscillometry resuts are explored."],"url":"http://arxiv.org/abs/2403.04621v1","category":"physics.med-ph"}
{"created":"2024-03-07 15:58:42","title":"Kinematic Hopf algebra and BCJ numerators at finite $\u03b1'$","abstract":"In this letter, starting from a kinematic Hopf algebra, we first construct a closed-form formula for all Bern-Carrasco-Johansson (BCJ) numerators in Yang-Mills (YM) theory with infinite orders of $\\alpha'$ corrections, known as $\\rm DF^2+YM$ theory, when coupled to two heavy particles which can be removed through a simple factorization limit. The full $\\alpha'$ dependence appears simply in massive physical propagator factors, with factorization strongly constraining the construction. The intricate structure induced by the massive poles also naturally leads us to find a novel closed-form and local expression for BCJ numerators in usual pure YM theory, based directly on the kinematic Hopf algebra.","sentences":["In this letter, starting from a kinematic Hopf algebra, we first construct a closed-form formula for all Bern-Carrasco-Johansson (BCJ) numerators in Yang-Mills (YM) theory with infinite orders of $\\alpha'$ corrections, known as $\\rm DF^2+YM$ theory, when coupled to two heavy particles which can be removed through a simple factorization limit.","The full $\\alpha'$ dependence appears simply in massive physical propagator factors, with factorization strongly constraining the construction.","The intricate structure induced by the massive poles also naturally leads us to find a novel closed-form and local expression for BCJ numerators in usual pure YM theory, based directly on the kinematic Hopf algebra."],"url":"http://arxiv.org/abs/2403.04614v1","category":"hep-th"}
{"created":"2024-03-07 15:53:23","title":"Next-to-eikonal corrections to dijet production in Deep Inelastic Scattering in the dilute limit of the Color Glass Condensate","abstract":"We analyze the effects of next-to-eikonal corrections on dijet production in Deep Inelastic Scattering off nuclear targets in the framework of the Color Glass Condensate. They require the knowledge of correlators of fields in the target beyond those computed in the standard McLerran-Venugopalan model, specifically those between transverse and boost-enhanced components, and of the recoil of the fields. We neglect the latter, while for the former we develop a linear model valid for large nuclei. We considered the unpolarized cross sections for dijet production in the approximation of a homogenous dilute nucleus, obtaining simple analytic expressions for the cross sections at nex-to-eikonal accuracy, valid in the limit of total dijet momentum and dijet momentum imbalance larger than the saturation scale of the nucleus. We perform a numerical study of the results at energies of the Electron Ion Collider, finding $\\mathcal{O}(10\\%)$ effects in the cross sections at large total momentum. We also analyze the azimuthal asymmetries between total momentum and imbalance, finding that non-eikonal corrections induce odd azimuthal harmonics for the situation of jets with equal momentum fractions from the virtual photon, where they are absent in the eikonal approximation. Finally, in the eikonal approximation we have compared the results of our analytic expansion valid in the dilute limit of the target, and the full Color Glass Condensate results in the McLerran-Venugopalan model and their correlation limit. Our analytic expressions match the correlation limit ones in the region where both should be simultaneously valid and reproduce very well the full Color Glass Condensate results in its validity region.","sentences":["We analyze the effects of next-to-eikonal corrections on dijet production in Deep Inelastic Scattering off nuclear targets in the framework of the Color Glass Condensate.","They require the knowledge of correlators of fields in the target beyond those computed in the standard McLerran-Venugopalan model, specifically those between transverse and boost-enhanced components, and of the recoil of the fields.","We neglect the latter, while for the former we develop a linear model valid for large nuclei.","We considered the unpolarized cross sections for dijet production in the approximation of a homogenous dilute nucleus, obtaining simple analytic expressions for the cross sections at nex-to-eikonal accuracy, valid in the limit of total dijet momentum and dijet momentum imbalance larger than the saturation scale of the nucleus.","We perform a numerical study of the results at energies of the Electron Ion Collider, finding $\\mathcal{O}(10\\%)$ effects in the cross sections at large total momentum.","We also analyze the azimuthal asymmetries between total momentum and imbalance, finding that non-eikonal corrections induce odd azimuthal harmonics for the situation of jets with equal momentum fractions from the virtual photon, where they are absent in the eikonal approximation.","Finally, in the eikonal approximation we have compared the results of our analytic expansion valid in the dilute limit of the target, and the full Color Glass Condensate results in the McLerran-Venugopalan model and their correlation limit.","Our analytic expressions match the correlation limit ones in the region where both should be simultaneously valid and reproduce very well the full Color Glass Condensate results in its validity region."],"url":"http://arxiv.org/abs/2403.04603v1","category":"hep-ph"}
{"created":"2024-03-07 15:03:39","title":"Energy barriers for boundary nucleation in a two-well model without gauge invariance","abstract":"We study energy scaling laws for a simplified, singularly perturbed, double-well nucleation problem confined in a half-space, in the absence of gauge invariance and for an inclusion of fixed volume. Motivated by models for boundary nucleation of a single-phase martensite inside a parental phase of austenite, our main focus in this nonlocal isoperimetric problem is how the relationship between the rank-1 direction and the orientation of the half-space influences the energy scaling with respect to the fixed volume of the inclusion. Up to prefactors depending on this relative orientation, the scaling laws coincide with the corresponding ones for bulk nucleation \\cite{knupfer2011minimal} for all rank-1 directions, \\textit{but} the ones normal to the confining hyperplane, where the scaling is as in a three-well problem in full space, resulting in a lower energy barrier \\cite{Tribuzio-Rueland_1}.","sentences":["We study energy scaling laws for a simplified, singularly perturbed, double-well nucleation problem confined in a half-space, in the absence of gauge invariance and for an inclusion of fixed volume.","Motivated by models for boundary nucleation of a single-phase martensite inside a parental phase of austenite, our main focus in this nonlocal isoperimetric problem is how the relationship between the rank-1 direction and the orientation of the half-space influences the energy scaling with respect to the fixed volume of the inclusion.","Up to prefactors depending on this relative orientation, the scaling laws coincide with the corresponding ones for bulk nucleation \\cite{knupfer2011minimal} for all rank-1 directions, \\textit{but} the ones normal to the confining hyperplane, where the scaling is as in a three-well problem in full space, resulting in a lower energy barrier \\cite{Tribuzio-Rueland_1}."],"url":"http://arxiv.org/abs/2403.04567v1","category":"math.AP"}
{"created":"2024-03-07 14:21:14","title":"Revealing the variation mechanism of ON 231 via the two-components shock-in-jet model","abstract":"The variation mechanism of blazars is a long-standing unresolved problem. In this work, we present a scenario to explain diverse variation phenomena for ON 231, where the jet emissions are composed of the flaring and the less variable components (most probably from the post-flaring blobs), and the variation is dominated by shock-in-jet instead of the Doppler effect. We perform correlation analysis for the multiwavelength light curves and find no significant correlations. For optical band, ON 231 exhibits a harder when brighter (HWB) trend, and the trend seems to shift at different periods. Correspondingly, the correlation between polarization degree and flux exhibits a V-shaped behavior, and a similar translation relation during different periods is also found. These phenomena could be understood via the superposition of the flaring component and slowly varying background component. We also find that the slopes of HWB trend become smaller at higher flux levels, which indicates the energy-dependent acceleration processes of the radiative particles. For X-ray, we discover a trend transition from HWB to softer when brighter (SWB) to HWB. We consider that the X-ray emission is composed of both the synchrotron tail and the Synchrotron Self-Compton components, which could be described by two log-parabolic functions. By varying the peak frequency, we reproduce the observed trend transition in a quantitative manner. For $\\gamma$-ray, we find the SWB trend, which could be explained naturally if a very-high-energy $\\gamma$-ray background component exists. Our study elucidates the variation mechanism of intermediate synchrotron-peaked BL Lac objects.","sentences":["The variation mechanism of blazars is a long-standing unresolved problem.","In this work, we present a scenario to explain diverse variation phenomena for ON 231, where the jet emissions are composed of the flaring and the less variable components (most probably from the post-flaring blobs), and the variation is dominated by shock-in-jet instead of the Doppler effect.","We perform correlation analysis for the multiwavelength light curves and find no significant correlations.","For optical band, ON 231 exhibits a harder when brighter (HWB) trend, and the trend seems to shift at different periods.","Correspondingly, the correlation between polarization degree and flux exhibits a V-shaped behavior, and a similar translation relation during different periods is also found.","These phenomena could be understood via the superposition of the flaring component and slowly varying background component.","We also find that the slopes of HWB trend become smaller at higher flux levels, which indicates the energy-dependent acceleration processes of the radiative particles.","For X-ray, we discover a trend transition from HWB to softer when brighter (SWB) to HWB.","We consider that the X-ray emission is composed of both the synchrotron tail and the Synchrotron Self-Compton components, which could be described by two log-parabolic functions.","By varying the peak frequency, we reproduce the observed trend transition in a quantitative manner.","For $\\gamma$-ray, we find the SWB trend, which could be explained naturally if a very-high-energy $\\gamma$-ray background component exists.","Our study elucidates the variation mechanism of intermediate synchrotron-peaked BL Lac objects."],"url":"http://arxiv.org/abs/2403.04518v1","category":"astro-ph.HE"}
{"created":"2024-03-07 13:29:53","title":"The role of conformity in opinion dynamics modelling with multiple social circles","abstract":"Interaction with others influences our opinions and behaviours. Our activities within various social circles lead to different opinions expressed in various situations, groups, and ways of communication. Earlier studies on agent-based modelling of conformism within networks were based on a single-layer approach. Contrary to that, in this work, we propose a model incorporating conformism in which a person can share different continuous opinions on different layers depending on the social circle. Afterwards, we extend the model with more components that are known to influence opinions, e.g. authority or openness to new views. These two models are then compared to show that only sole conformism leads to opinion convergence.","sentences":["Interaction with others influences our opinions and behaviours.","Our activities within various social circles lead to different opinions expressed in various situations, groups, and ways of communication.","Earlier studies on agent-based modelling of conformism within networks were based on a single-layer approach.","Contrary to that, in this work, we propose a model incorporating conformism in which a person can share different continuous opinions on different layers depending on the social circle.","Afterwards, we extend the model with more components that are known to influence opinions, e.g. authority or openness to new views.","These two models are then compared to show that only sole conformism leads to opinion convergence."],"url":"http://arxiv.org/abs/2403.04480v1","category":"cs.SI"}
{"created":"2024-03-07 13:25:12","title":"Modeling Methane Intensity of Oil and Gas Upstream Activities by Production Profile","abstract":"We propose a methodology for modelling methane intensities of Oil and Gas upstream activities for different production profiles with diverse combinations of region of operation and production volumes associated. This methodology leverages different data sources, including satellite measurements and public estimates of methane emissions but also country-level oil and gas production data and company reporting. The obtained methane intensity models are compared to the reference companies' own reporting in order to better understand methane emissions for different types of companies. The results show that regions of operation within the different production profiles have a significant impact on the value of modelled methane intensities, especially for operators located in a single or few countries, such as national and medium-sized international operators. This paper also shows that methane intensities reported by the companies tend to be on average 16.1 times smaller than that obtained using the methodology presented here, and cannot account for total methane emissions that are estimated for upstream operations in the different regions observed.","sentences":["We propose a methodology for modelling methane intensities of Oil and Gas upstream activities for different production profiles with diverse combinations of region of operation and production volumes associated.","This methodology leverages different data sources, including satellite measurements and public estimates of methane emissions but also country-level oil and gas production data and company reporting.","The obtained methane intensity models are compared to the reference companies' own reporting in order to better understand methane emissions for different types of companies.","The results show that regions of operation within the different production profiles have a significant impact on the value of modelled methane intensities, especially for operators located in a single or few countries, such as national and medium-sized international operators.","This paper also shows that methane intensities reported by the companies tend to be on average 16.1 times smaller than that obtained using the methodology presented here, and cannot account for total methane emissions that are estimated for upstream operations in the different regions observed."],"url":"http://arxiv.org/abs/2403.04479v1","category":"cs.CE"}
{"created":"2024-03-07 13:05:57","title":"Classifying bulk-edge anomalies in the Dirac Hamiltonian","abstract":"We study the Dirac Hamiltonian in dimension two with a mass term and a large momentum regularization, and show that bulk-edge correspondence fails. Despite a well defined bulk topological index --the Chern number--, the number of edge modes depends on the boundary condition. The origin of this anomaly is rooted in the unbounded nature of the spectrum. It is detected with Levinson's theorem from scattering theory and quantified via an anomalous winding number at infinite energy, dubbed ghost charge. First we classify, up to equivalence, all self-adjoint boundary conditions, using Schubert cell decomposition of a Grassmanian. Then, we investigate which ones are anomalous. We expand the scattering amplitude near infinite energy, for which a dominant scale captures the asymptotic winding number. Remarkably, this can be achieved for every self-adjoint boundary condition, leading to an exhaustive anomaly classification. It shows that anomalies are ubiquitous and stable. Boundary condition with a ghost charge of 2 is also revealed within the process.","sentences":["We study the Dirac Hamiltonian in dimension two with a mass term and a large momentum regularization, and show that bulk-edge correspondence fails.","Despite a well defined bulk topological index --the Chern number--, the number of edge modes depends on the boundary condition.","The origin of this anomaly is rooted in the unbounded nature of the spectrum.","It is detected with Levinson's theorem from scattering theory and quantified via an anomalous winding number at infinite energy, dubbed ghost charge.","First we classify, up to equivalence, all self-adjoint boundary conditions, using Schubert cell decomposition of a Grassmanian.","Then, we investigate which ones are anomalous.","We expand the scattering amplitude near infinite energy, for which a dominant scale captures the asymptotic winding number.","Remarkably, this can be achieved for every self-adjoint boundary condition, leading to an exhaustive anomaly classification.","It shows that anomalies are ubiquitous and stable.","Boundary condition with a ghost charge of 2 is also revealed within the process."],"url":"http://arxiv.org/abs/2403.04465v1","category":"math-ph"}
{"created":"2024-03-07 12:59:20","title":"Non-Abelian Exponential Yang-Mills AdS Black Brane and Transport Coefficients","abstract":"In this paper, AdS black brane solution of Einstein-Hilbert gravity with non-abelian exponential guage theory of Yang-Mills type is introduced. DC conductivity and the ratio of shear viscosity to entropy density as two important transport coefficients are calculated by using of Kubo formula in the context of AdS/CFT duality. Our results recover the Yang-Mills model in $q\\to \\infty$ limit.","sentences":["In this paper, AdS black brane solution of Einstein-Hilbert gravity with non-abelian exponential guage theory of Yang-Mills type is introduced.","DC conductivity and the ratio of shear viscosity to entropy density as two important transport coefficients are calculated by using of Kubo formula in the context of AdS/CFT duality.","Our results recover the Yang-Mills model in $q\\to \\infty$ limit."],"url":"http://arxiv.org/abs/2403.04463v1","category":"hep-th"}
{"created":"2024-03-07 12:57:34","title":"Enhanced near-complete absorption of electromagnetic waves by dual resonance in a magnetized plasma","abstract":"There has been significant interest lately in the study of Electromagnetic (EM) waves interacting with magnetized plasmas. The variety of resonances and the existence of several pass and stop bands in the dispersion curve for different orientations of the magnetic field offer new mechanisms of EM wave energy absorption (PhysRevE.105.055209, Juneja_2023,vashistha2022localized,vashistha2020new). By an appropriate choice of inhomogeneous magnetic field, one can construct a configuration wherein the same EM wave pulse encounters more than one resonance in the plasma. A 2-D Particle - In - Cell (PIC) simulation using the OSIRIS4.0 platform has been carried out for the case of dual resonance. It is observed that in the presence of dual resonance, there is a significant enhancement in leading to almost complete absorption of laser energy by the plasma in certain cases. A detailed study of the influence of the relative location of the resonances, the effect of high input EM wave intensity, etc., has also been carried out.","sentences":["There has been significant interest lately in the study of Electromagnetic (EM) waves interacting with magnetized plasmas.","The variety of resonances and the existence of several pass and stop bands in the dispersion curve for different orientations of the magnetic field offer new mechanisms of EM wave energy absorption (PhysRevE.105.055209, Juneja_2023,vashistha2022localized,vashistha2020new).","By an appropriate choice of inhomogeneous magnetic field, one can construct a configuration wherein the same EM wave pulse encounters more than one resonance in the plasma.","A 2-D Particle - In - Cell (PIC) simulation using the OSIRIS4.0 platform has been carried out for the case of dual resonance.","It is observed that in the presence of dual resonance, there is a significant enhancement in leading to almost complete absorption of laser energy by the plasma in certain cases.","A detailed study of the influence of the relative location of the resonances, the effect of high input EM wave intensity, etc., has also been carried out."],"url":"http://arxiv.org/abs/2403.04462v1","category":"physics.plasm-ph"}
{"created":"2024-03-07 12:56:53","title":"An efficient method for calculating resonant modes in biperiodic photonic structures","abstract":"Many photonic devices, such as photonic crystal slabs, cross gratings, and periodic metasurfaces, are biperiodic structures with two independent periodic directions, and are sandwiched between two homogeneous media. Many applications of these devices are closely related to resonance phenomena. Therefore, efficient computation of resonant modes is crucial in device design and structure analysis. Since resonant modes satisfy outgoing radiation conditions, perfectly matched layers (PMLs) are usually used to truncate the unbounded spatial variable perpendicular to the periodic directions. In this paper, we develop an efficient method without using PMLs to calculate resonant modes in biperiodic structures. We reduce the original eigenvalue problem to a small matrix nonlinear eigenvalue problem which is solved by the contour integral method. Numerical examples show that our method is efficient with respect to memory usage and CPU time, free of spurious solutions, and determines degenerate resonant modes without any difficulty.","sentences":["Many photonic devices, such as photonic crystal slabs, cross gratings, and periodic metasurfaces, are biperiodic structures with two independent periodic directions, and are sandwiched between two homogeneous media.","Many applications of these devices are closely related to resonance phenomena.","Therefore, efficient computation of resonant modes is crucial in device design and structure analysis.","Since resonant modes satisfy outgoing radiation conditions, perfectly matched layers (PMLs) are usually used to truncate the unbounded spatial variable perpendicular to the periodic directions.","In this paper, we develop an efficient method without using PMLs to calculate resonant modes in biperiodic structures.","We reduce the original eigenvalue problem to a small matrix nonlinear eigenvalue problem which is solved by the contour integral method.","Numerical examples show that our method is efficient with respect to memory usage and CPU time, free of spurious solutions, and determines degenerate resonant modes without any difficulty."],"url":"http://arxiv.org/abs/2403.04459v1","category":"physics.comp-ph"}
{"created":"2024-03-07 12:06:21","title":"Emergent impervious band crossing in the bulk in topological nodal line semimetal ZrAs$_2$","abstract":"Topological nodal line semimetals (TSMs) represent a unique class of materials with intriguing electronic structures and rich of symmetries, hosting electronic states with non-trivial topological properties. Among these, ZrAs$_2$ stands out, characterized by its nodal lines forming continuous loops in momentum space, governed by non-symmorphic symmetries. This study integrates angle-resolved photoemission spectroscopy (ARPES) with density functional theory (DFT) calculations to explore the electronic states of ZrAs$_2$. In ARPES scans, we observed a distinctive nodal loop structure observed at lower excitation energies of 30 and 50 eV. Our results, supported by calculations based on DFT, unveil symmetry-enforced Dirac-like band crossings anchored at specific points in the Brillouin zone, with particular emphasis on the S point. Surface bands and bulk states near the crossing are elucidated through slab calculations, corroborating experimental findings. DFT calculations also show the existence of several spin-orbit coupling (SOC) resilient semi-Dirac crossings pinned at Z point. This comprehensive investigation sheds light on the intricate electronic behaviors of ZrAs$_2$ with the involved symmetries, important for fundamental understanding of topological nodal line semimetals.","sentences":["Topological nodal line semimetals (TSMs) represent a unique class of materials with intriguing electronic structures and rich of symmetries, hosting electronic states with non-trivial topological properties.","Among these, ZrAs$_2$ stands out, characterized by its nodal lines forming continuous loops in momentum space, governed by non-symmorphic symmetries.","This study integrates angle-resolved photoemission spectroscopy (ARPES) with density functional theory (DFT) calculations to explore the electronic states of ZrAs$_2$.","In ARPES scans, we observed a distinctive nodal loop structure observed at lower excitation energies of 30 and 50 eV. Our results, supported by calculations based on DFT, unveil symmetry-enforced Dirac-like band crossings anchored at specific points in the Brillouin zone, with particular emphasis on the S point.","Surface bands and bulk states near the crossing are elucidated through slab calculations, corroborating experimental findings.","DFT calculations also show the existence of several spin-orbit coupling (SOC) resilient semi-Dirac crossings pinned at Z point.","This comprehensive investigation sheds light on the intricate electronic behaviors of ZrAs$_2$ with the involved symmetries, important for fundamental understanding of topological nodal line semimetals."],"url":"http://arxiv.org/abs/2403.04434v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-07 11:57:41","title":"Medium Assisted Low Energy Nuclear Fusion","abstract":"We study the process of nuclear fusion at low energies in a medium using the second order time dependent perturbation theory. We consider a specific process which involves fusion of a low energy proton with a Nickel nucleus. The reaction proceeds in two steps or interactions. We refer to the amplitudes corresponding to these two interactions as the the molecular and the nuclear matrix elements. The first amplitude involves Coulomb interaction with another nucleus in the medium while the second corresponds to the nuclear fusion process. It has been shown in earlier papers that such a second order process has negligible amplitude unless it is assisted by special medium effects. In the present paper we show the presence of a special configuration of atoms which greatly enhances the process. We find that if the spacings among the atoms can be tuned, the rate can be sufficiently enhanced so that easily observable. The spacings do not require acute fine tuning, however, if they are significantly off the rate falls sharply to negligible values. This might also explain both the successes and failures experienced by many experimentalists studying this phenomenon. We study only a particular final state which involves emission of one photon. However we show that many final states are possible which need not involve photon emission.","sentences":["We study the process of nuclear fusion at low energies in a medium using the second order time dependent perturbation theory.","We consider a specific process which involves fusion of a low energy proton with a Nickel nucleus.","The reaction proceeds in two steps or interactions.","We refer to the amplitudes corresponding to these two interactions as the the molecular and the nuclear matrix elements.","The first amplitude involves Coulomb interaction with another nucleus in the medium while the second corresponds to the nuclear fusion process.","It has been shown in earlier papers that such a second order process has negligible amplitude unless it is assisted by special medium effects.","In the present paper we show the presence of a special configuration of atoms which greatly enhances the process.","We find that if the spacings among the atoms can be tuned, the rate can be sufficiently enhanced so that easily observable.","The spacings do not require acute fine tuning, however, if they are significantly off the rate falls sharply to negligible values.","This might also explain both the successes and failures experienced by many experimentalists studying this phenomenon.","We study only a particular final state which involves emission of one photon.","However we show that many final states are possible which need not involve photon emission."],"url":"http://arxiv.org/abs/2403.04428v1","category":"nucl-th"}
{"created":"2024-03-07 11:46:34","title":"Estimation of the lifetime of slow-decaying unipolar active regions in the framework of the turbulent erosion model","abstract":"We explore properties and behavior of slow-decaying unipolar sunspot groups in the framework of the turbulent erosion model suggested by Petrovay and Moreno-Insertis (1997). The basic concept of the model is the suppression of a turbulent diffusivity inside a magnetic flux tube by strong magnetic fields. As a result, the outer turbulent plasma detaches magnetic features primarily from the external border of a magnetic flux tube. The radius of the tube exhibits inward progression at a constant rate. The model predicts older sunspots to decay slower and it seems to be very promising to explain the slow decay of long-living unipolar sunspot groups. By analyzing the magnetic structure associated with a sunspot, we did reveal a gradual decrease of the magnetic structure radius at a constant rate implying the validity of the model. However, in some cases the derived velocity of the radius decrease was too low: our calculations provided implausibly high estimations for the lifetime and maximal area of such sunspots. We discuss possible additional mechanisms affecting the decay rate of such peculiar sunspots.","sentences":["We explore properties and behavior of slow-decaying unipolar sunspot groups in the framework of the turbulent erosion model suggested by Petrovay and Moreno-Insertis (1997).","The basic concept of the model is the suppression of a turbulent diffusivity inside a magnetic flux tube by strong magnetic fields.","As a result, the outer turbulent plasma detaches magnetic features primarily from the external border of a magnetic flux tube.","The radius of the tube exhibits inward progression at a constant rate.","The model predicts older sunspots to decay slower and it seems to be very promising to explain the slow decay of long-living unipolar sunspot groups.","By analyzing the magnetic structure associated with a sunspot, we did reveal a gradual decrease of the magnetic structure radius at a constant rate implying the validity of the model.","However, in some cases the derived velocity of the radius decrease was too low: our calculations provided implausibly high estimations for the lifetime and maximal area of such sunspots.","We discuss possible additional mechanisms affecting the decay rate of such peculiar sunspots."],"url":"http://arxiv.org/abs/2403.04422v1","category":"astro-ph.SR"}
{"created":"2024-03-07 11:27:19","title":"Activation measurements of an iodinated contrast media for online range verification in proton therapy","abstract":"The use of contrast agents has previously been proposed as a novel method to increase the activation close to the Bragg peak, aiming to improve the quality of proton range monitoring in vivo. In a recent work, we demonstrated the feasibility of $^{127}$I for online verification, thanks to its high cross-section (200 mbarn at 10 MeV) and low energy production threshold for $^{127m}$Xe. The spectroscopy-based method relies on identifying two specific delayed $\\gamma$ lines (at 124.6 and 172.5 keV), which can be analyzed via single-photon emission computed tomography (SPECT). In this work, we present a proof-of-principle study to investigate proton activation in a commercial iodinated contrast media (ICM) for radiology. Five measurements were conducted at different proton energies (6-10 MeV), equivalent to the last millimeters of clinical proton range. Activation in the ICM was measured with four LaBr$_3$(Ce) scintillators. The contribution from iodine was separated from the activation of the solvent, yielding excellent agreement with previously reported data. These results demonstrate the potential of this technique and pave the way for further testing in clinically relevant scenarios.","sentences":["The use of contrast agents has previously been proposed as a novel method to increase the activation close to the Bragg peak, aiming to improve the quality of proton range monitoring in vivo.","In a recent work, we demonstrated the feasibility of $^{127}$I for online verification, thanks to its high cross-section (200 mbarn at 10 MeV) and low energy production threshold for $^{127m}$Xe.","The spectroscopy-based method relies on identifying two specific delayed $\\gamma$ lines (at 124.6 and 172.5 keV), which can be analyzed via single-photon emission computed tomography (SPECT).","In this work, we present a proof-of-principle study to investigate proton activation in a commercial iodinated contrast media (ICM) for radiology.","Five measurements were conducted at different proton energies (6-10 MeV), equivalent to the last millimeters of clinical proton range.","Activation in the ICM was measured with four LaBr$_3$(Ce) scintillators.","The contribution from iodine was separated from the activation of the solvent, yielding excellent agreement with previously reported data.","These results demonstrate the potential of this technique and pave the way for further testing in clinically relevant scenarios."],"url":"http://arxiv.org/abs/2403.04415v1","category":"physics.med-ph"}
{"created":"2024-03-07 10:28:21","title":"Emission lines due to ionizing radiation from a compact object in the remnant of Supernova 1987A","abstract":"The nearby Supernova 1987A was accompanied by a burst of neutrino emission, which indicates that a compact object (a neutron star or black hole) was formed in the explosion. There has been no direct observation of this compact object. In this work, we observe the supernova remnant with JWST spectroscopy finding narrow infrared emission lines of argon and sulphur. The line emission is spatially unresolved and blueshifted in velocity relative to the supernova rest frame. We interpret the lines as gas illuminated by a source of ionizing photons located close to the center of the expanding ejecta. Photoionization models show that the line ratios are consistent with ionization by a cooling neutron star or pulsar wind nebula. The velocity shift could be evidence for a neutron star natal kick.","sentences":["The nearby Supernova 1987A was accompanied by a burst of neutrino emission, which indicates that a compact object (a neutron star or black hole) was formed in the explosion.","There has been no direct observation of this compact object.","In this work, we observe the supernova remnant with JWST spectroscopy finding narrow infrared emission lines of argon and sulphur.","The line emission is spatially unresolved and blueshifted in velocity relative to the supernova rest frame.","We interpret the lines as gas illuminated by a source of ionizing photons located close to the center of the expanding ejecta.","Photoionization models show that the line ratios are consistent with ionization by a cooling neutron star or pulsar wind nebula.","The velocity shift could be evidence for a neutron star natal kick."],"url":"http://arxiv.org/abs/2403.04386v1","category":"astro-ph.HE"}
{"created":"2024-03-07 10:01:43","title":"Binned top quark spin correlation and polarization observables for the LHC at 13.6 TeV","abstract":"We consider top-antitop quark $(t{\\bar t})$ production at the Large Hadron Collider (LHC) with subsequent decays into dileptonic final states. We use and investigate a set of leptonic angular correlations and distributions with which all the independent coefficient functions of the top-spin dependent parts of the $t{\\bar t}$ production spin density matrices can be experimentally probed. We compute these observables for the LHC center-of-mass energy 13.6 TeV within the Standard Model at next-to-leading order in the QCD coupling including the mixed QCD-weak corrections. We determine also the $t{\\bar t}$ charge asymmetry where we take in addition also the mixed QCD-QED corrections into account. In addition we analyze and compute possible new physics (NP) effects on these observables in terms of a gauge-invariant effective Lagrangian that contains the operators up to mass dimension six that are relevant for hadronic $(t{\\bar t})$ production. First we compute our observables inclusive in phase space. In order to investigate which region in phase space has, for a specific observable, a high NP sensitivity, we determine our observables also in two-dimensional $(M_{t{\\bar t}},\\cos\\theta_t^*)$ bins, where $M_{t{\\bar t}}$ denotes the $t{\\bar t}$ invariant mass and $\\theta_t^*$ is the top-quark scattering angle in the $t{\\bar t}$ zero-momentum frame.","sentences":["We consider top-antitop quark $(t{\\bar t})$ production at the Large Hadron Collider (LHC) with subsequent decays into dileptonic final states.","We use and investigate a set of leptonic angular correlations and distributions with which all the independent coefficient functions of the top-spin dependent parts of the $t{\\bar t}$ production spin density matrices can be experimentally probed.","We compute these observables for the LHC center-of-mass energy 13.6 TeV within the Standard Model at next-to-leading order in the QCD coupling including the mixed QCD-weak corrections.","We determine also the $t{\\bar t}$ charge asymmetry where we take in addition also the mixed QCD-QED corrections into account.","In addition we analyze and compute possible new physics (NP) effects on these observables in terms of a gauge-invariant effective Lagrangian that contains the operators up to mass dimension six that are relevant for hadronic $(t{\\bar t})$ production.","First we compute our observables inclusive in phase space.","In order to investigate which region in phase space has, for a specific observable, a high NP sensitivity, we determine our observables also in two-dimensional $(M_{t{\\bar t}},\\cos\\theta_t^*)$ bins, where $M_{t{\\bar t}}$ denotes the $t{\\bar t}$ invariant mass and $\\theta_t^*$ is the top-quark scattering angle in the $t{\\bar t}$ zero-momentum frame."],"url":"http://arxiv.org/abs/2403.04371v1","category":"hep-ph"}
{"created":"2024-03-07 09:05:25","title":"On a conjecture on aCM and Ulrich sheaves on degeneracy loci","abstract":"In this paper we address a conjecture by Kleppe and Mir\\'o-Roig stating that suitable twists by line bundles (on the smooth locus) of the exterior powers of the normal sheaf of a standard determinantal locus are arithmetically Cohen--Macaulay, and even Ulrich when the locus is linear determinantal. We do so by providing a very simple locally free resolution of such sheaves obtained through the so-called Weyman's Geometric Method.","sentences":["In this paper we address a conjecture by Kleppe and Mir\\'o-Roig stating that suitable twists by line bundles (on the smooth locus) of the exterior powers of the normal sheaf of a standard determinantal locus are arithmetically Cohen--Macaulay, and even Ulrich when the locus is linear determinantal.","We do so by providing a very simple locally free resolution of such sheaves obtained through the so-called Weyman's Geometric Method."],"url":"http://arxiv.org/abs/2403.04339v1","category":"math.AG"}
{"created":"2024-03-07 08:36:18","title":"Detection of proton tracks with LiF Fluorescent Nuclear Track Detectors","abstract":"Fluorescent nuclear track detectors based on LiF crystals were successfully applied for detection of proton induced tracks. Irradiations were performed with protons with energy ranging from 1 MeV up to about 56 MeV and for all proton energies the fluorescent tracks were observed. The tracks are not continuous, but consist of a series of bright spots. The gaps between spots tend to narrow with decreasing proton energy (increasing ionization density). For the highest of the studied energies, the spots are scattered so sparsely, that it is not possible to link spots belonging to one track. The intensity (brightness) of the fluorescent tracks increases with the increasing LET and agrees well with the trend established earlier for various heavier ions.","sentences":["Fluorescent nuclear track detectors based on LiF crystals were successfully applied for detection of proton induced tracks.","Irradiations were performed with protons with energy ranging from 1 MeV up to about 56 MeV and for all proton energies the fluorescent tracks were observed.","The tracks are not continuous, but consist of a series of bright spots.","The gaps between spots tend to narrow with decreasing proton energy (increasing ionization density).","For the highest of the studied energies, the spots are scattered so sparsely, that it is not possible to link spots belonging to one track.","The intensity (brightness) of the fluorescent tracks increases with the increasing LET and agrees well with the trend established earlier for various heavier ions."],"url":"http://arxiv.org/abs/2403.04320v1","category":"physics.ins-det"}
{"created":"2024-03-07 08:30:18","title":"Endo-exo framework for a unifying classification of episodic landslide movements","abstract":"Landslides exhibit intermittent gravity-driven downslope movements developing over days to years before a possible major collapse, commonly boosted by external events like precipitations and earthquakes. The reasons behind these episodic movements and how they relate to the final instability remain poorly understood. Here, we develop a novel 'endo-exo' theory to quantitatively diagnose landslide dynamics, capturing the interplay between exogenous stressors such as rainfall and endogenous damage/healing processes. We predict four distinct types of episodic landslide dynamics (endogenous/exogenous-subcritical/critical), characterized by power law relaxations with different exponents, all related to a single parameter $\\vartheta$. These predictions are tested on the dataset of the Preonzo landslide, which exhibited multi-year episodic movements prior to a catastrophic collapse. All its episodic activities can be accounted for within this classification with $\\vartheta \\approx 0.45\\pm0.1$, providing strong support for our parsimonious theory. We find that the final collapse of this landslide is clearly preceded over 1-2 months by an increased frequency of medium/large velocities, signaling the transition into a catastrophic regime with amplifying positive feedbacks.","sentences":["Landslides exhibit intermittent gravity-driven downslope movements developing over days to years before a possible major collapse, commonly boosted by external events like precipitations and earthquakes.","The reasons behind these episodic movements and how they relate to the final instability remain poorly understood.","Here, we develop a novel 'endo-exo' theory to quantitatively diagnose landslide dynamics, capturing the interplay between exogenous stressors such as rainfall and endogenous damage/healing processes.","We predict four distinct types of episodic landslide dynamics (endogenous/exogenous-subcritical/critical), characterized by power law relaxations with different exponents, all related to a single parameter $\\vartheta$. These predictions are tested on the dataset of the Preonzo landslide, which exhibited multi-year episodic movements prior to a catastrophic collapse.","All its episodic activities can be accounted for within this classification with $\\vartheta \\approx 0.45\\pm0.1$, providing strong support for our parsimonious theory.","We find that the final collapse of this landslide is clearly preceded over 1-2 months by an increased frequency of medium/large velocities, signaling the transition into a catastrophic regime with amplifying positive feedbacks."],"url":"http://arxiv.org/abs/2403.04310v1","category":"physics.geo-ph"}
{"created":"2024-03-07 07:35:20","title":"On the structures of the Johnson cokernels of the basis-conjugating automorphism groups of free groups","abstract":"In this paper, we study the Johnson homomorphisms of basis-conjugating automorphism groups of free groups. We construct obstructions for the surjectivity of the Johnson homomorphisms. By using it, we determine its cokernels of degree up to four, and give further observations for degree greater than four.   As applications, we give the affirmative answer for the Andreadakis problem for degree four. We show that the cup product map of the first cohomology groups of the basis-conjugating automorphism group of a free group into the second cohomology group is surjective. Finally, we calculate the twisted first cohomology groups of the braid-permutation automorphism groups of a free group.","sentences":["In this paper, we study the Johnson homomorphisms of basis-conjugating automorphism groups of free groups.","We construct obstructions for the surjectivity of the Johnson homomorphisms.","By using it, we determine its cokernels of degree up to four, and give further observations for degree greater than four.   ","As applications, we give the affirmative answer for the Andreadakis problem for degree four.","We show that the cup product map of the first cohomology groups of the basis-conjugating automorphism group of a free group into the second cohomology group is surjective.","Finally, we calculate the twisted first cohomology groups of the braid-permutation automorphism groups of a free group."],"url":"http://arxiv.org/abs/2403.04286v1","category":"math.AT"}
{"created":"2024-03-07 07:15:47","title":"Relative alignment between gas structures and magnetic field in Orion A at different scales using different molecular gas tracers","abstract":"Context: Magnetic fields can play crucial roles in high-mass star formation. Nonetheless, the significance of magnetic fields at various scales and their relationship with gas structures is largely overlooked. Aims: Our goal is to examine the relationship between the magnetic field and molecular gas structures within the Orion A giant molecular cloud at different scales and density regimes. Methods: We assess the gas intensity structures and column densities in Orion A by utilizing $^{12}$CO, $^{13}$CO, and C$^{18}$O from Nobeyama observations. Through comparing Nobeyama observations with {\\it{Planck}} polarization observations on large scales ($\\sim0.6$ pc) and JCMT polarization observations on small scales ($\\sim0.04$ pc), we investigate how the role of magnetic fields change with scale and density. Results: We find a similar trend from parallel to perpendicular alignment with increasing column densities in Orion A at both large and small spatial scales. Besides, when changing from low-density to high-density tracers, the relative orientation preference changes from random to perpendicular. The self-similar results at different scales indicate that magnetic fields are dynamically important in both cloud formation and filament formation. However, magnetic fields properties at small scales are relative complicated, and the interplay between magnetic field and star-forming activities needs to be discussed case-by-case.","sentences":["Context: Magnetic fields can play crucial roles in high-mass star formation.","Nonetheless, the significance of magnetic fields at various scales and their relationship with gas structures is largely overlooked.","Aims:","Our goal is to examine the relationship between the magnetic field and molecular gas structures within the Orion","A giant molecular cloud at different scales and density regimes.","Methods: We assess the gas intensity structures and column densities in Orion A by utilizing $^{12}$CO, $^{13}$CO, and C$^{18}$O from Nobeyama observations.","Through comparing Nobeyama observations with {\\it{Planck}} polarization observations on large scales ($\\sim0.6$ pc) and JCMT polarization observations on small scales ($\\sim0.04$ pc), we investigate how the role of magnetic fields change with scale and density.","Results: We find a similar trend from parallel to perpendicular alignment with increasing column densities in Orion A at both large and small spatial scales.","Besides, when changing from low-density to high-density tracers, the relative orientation preference changes from random to perpendicular.","The self-similar results at different scales indicate that magnetic fields are dynamically important in both cloud formation and filament formation.","However, magnetic fields properties at small scales are relative complicated, and the interplay between magnetic field and star-forming activities needs to be discussed case-by-case."],"url":"http://arxiv.org/abs/2403.04274v1","category":"astro-ph.GA"}
{"created":"2024-03-07 07:10:53","title":"Cooper pairing, flat-band superconductivity and quantum geometry in the pyrochlore-Hubbard model","abstract":"We investigate the impacts of the quantum geometry of Bloch states, specifically through the band-resolved quantum-metric tensor, on Cooper pairing and flat-band superconductivity in a three-dimensional pyrochlore-Hubbard model. First we analyze the low-lying two-body spectrum exactly, and show that the pairing order parameter is uniform in this four-band lattice. This allowed us to establish direct relations between the superfluid weight of a multiband superconductor and ($i$) the effective mass of the lowest-lying two-body branch at zero temperature, ($ii$) the kinetic coefficient of the Ginzburg-Landau theory in proximity to the critical temperature, and ($iii$) the velocity of the low-energy Goldstone modes at zero temperature. Furthermore, we perform a comprehensive numerical analysis of the superfluid weight and Goldstone modes, exploring both their conventional and geometric components at zero temperature.","sentences":["We investigate the impacts of the quantum geometry of Bloch states, specifically through the band-resolved quantum-metric tensor, on Cooper pairing and flat-band superconductivity in a three-dimensional pyrochlore-Hubbard model.","First we analyze the low-lying two-body spectrum exactly, and show that the pairing order parameter is uniform in this four-band lattice.","This allowed us to establish direct relations between the superfluid weight of a multiband superconductor and ($i$) the effective mass of the lowest-lying two-body branch at zero temperature, ($ii$) the kinetic coefficient of the Ginzburg-Landau theory in proximity to the critical temperature, and ($iii$) the velocity of the low-energy Goldstone modes at zero temperature.","Furthermore, we perform a comprehensive numerical analysis of the superfluid weight and Goldstone modes, exploring both their conventional and geometric components at zero temperature."],"url":"http://arxiv.org/abs/2403.04270v1","category":"cond-mat.supr-con"}
{"created":"2024-03-07 06:37:12","title":"Non-equilibrium Green's function approach to low-energy fission dynamics","abstract":"The concept of a compound nucleus was proposed by Bohr in 1936 to explain narrow resonances in neutron scattering off a nucleus. While a compound nucleus has been understood in terms of statistical mechanics, its description based on a many-body Hamiltonian has yet to be developed. Here we present a microscopic modeling of a compound nucleus starting from a nucleonic degree of freedom. We focus in particular on a decay of a heavy compound nucleus, that is, fission and radiative capture. To this end, we develop an approach based on a non-equilibrium Green's function, which is combined with a configuration interaction (CI) approach based on a constrained density-functional theory (DFT). We apply this approach to a barrier-top fission of $^{236}$U, restricting the model space to seniority zero configurations of neutrons and protons. Our calculation with a Skyrme energy functional yields the fission-to-capture branching ratio of around 0.07. While this value is still reasonable, the calculation underestimates the branching ratio by about a factor of 40 as compared to the empirical value, indicating a necessity of seniority non-zero configurations in the model space. We also find that the distribution of the fission probability approximately follows the chi-squared distribution with the number of degrees of freedom of the order of 1, which is consistent with the experimental finding.","sentences":["The concept of a compound nucleus was proposed by Bohr in 1936 to explain narrow resonances in neutron scattering off a nucleus.","While a compound nucleus has been understood in terms of statistical mechanics, its description based on a many-body Hamiltonian has yet to be developed.","Here we present a microscopic modeling of a compound nucleus starting from a nucleonic degree of freedom.","We focus in particular on a decay of a heavy compound nucleus, that is, fission and radiative capture.","To this end, we develop an approach based on a non-equilibrium Green's function, which is combined with a configuration interaction (CI) approach based on a constrained density-functional theory (DFT).","We apply this approach to a barrier-top fission of $^{236}$U, restricting the model space to seniority zero configurations of neutrons and protons.","Our calculation with a Skyrme energy functional yields the fission-to-capture branching ratio of around 0.07.","While this value is still reasonable, the calculation underestimates the branching ratio by about a factor of 40 as compared to the empirical value, indicating a necessity of seniority non-zero configurations in the model space.","We also find that the distribution of the fission probability approximately follows the chi-squared distribution with the number of degrees of freedom of the order of 1, which is consistent with the experimental finding."],"url":"http://arxiv.org/abs/2403.04255v1","category":"nucl-th"}
{"created":"2024-03-07 04:40:22","title":"Possible $\u03a3_c^* \\bar\u03a3$ molecular states","abstract":"We investigate the possibility of deuteron-like $\\Sigma_c^*\\bar{\\Sigma}$ bound states within the one-boson-exchange model and systematically analyze the effects of the contact-range $\\delta^{3}(\\vec{r}\\,)$ potential, the tensor term from the vector-meson exchange, and nonlocal potentials due to the dependence on the sum of the initial and final state center-of-mass momenta. We find that the pion-exchange potential including the $\\delta^{3}(\\vec{r}\\,)$ term and the tensor term of the $\\rho$-exchange potential exhibit comparable magnitudes but opposite signs for any $S$-wave baryon-antibaryon systems. For the $\\Sigma_c^*\\bar{\\Sigma}$ system, it is most likely to form bound states with mass around 3.7 GeV in the $I(J^P)=0(2^-)$ and $1(2^-)$ channels.","sentences":["We investigate the possibility of deuteron-like $\\Sigma_c^*\\bar{\\Sigma}$ bound states within the one-boson-exchange model and systematically analyze the effects of the contact-range $\\delta^{3}(\\vec{r}\\,)$ potential, the tensor term from the vector-meson exchange, and nonlocal potentials due to the dependence on the sum of the initial and final state center-of-mass momenta.","We find that the pion-exchange potential including the $\\delta^{3}(\\vec{r}\\,)$ term and the tensor term of the $\\rho$-exchange potential exhibit comparable magnitudes but opposite signs for any $S$-wave baryon-antibaryon systems.","For the $\\Sigma_c^*\\bar{\\Sigma}$ system, it is most likely to form bound states with mass around 3.7 GeV in the $I(J^P)=0(2^-)$ and $1(2^-)$ channels."],"url":"http://arxiv.org/abs/2403.04218v1","category":"hep-ph"}
{"created":"2024-03-07 04:39:29","title":"Multi-frequency VLBI Imaging of the Sub-parsec Scale Jet In the Sombrero Galaxy (M 104)","abstract":"We report multi-frequency and multi-epoch VLBI studies of the sub-parsec jet in Sombrero galaxy (M 104, NGC 4594). Using Very Long Baseline Array data at 12, 22, 44, and 88 GHz, we study the kinematics of the jet and the properties of the compact core. The sub-parsec jet is clearly detected at 12 and 22 GHz, and the inner jet base is resolved down to $\\sim70$ Schwarzschild radii ($R_{\\rm s}$) at 44 GHz. The proper motions of the jet are measured with apparent sub-relativistic speeds of $0.20\\pm0.08 c$ and $0.05\\pm0.02 c$ for the approaching and the receding jet, respectively. Based on the apparent speed and jet-to-counter-jet brightness ratio, we estimate the jet viewing angle to be larger than $\\sim37^{\\circ}$, and the intrinsic speed to be between $\\sim0.10 c$ and $0.40 c$. Their joint probability distribution suggests the most probable values of the viewing angle and intrinsic speed to be ${66^{\\circ}}^{+4^\\circ}_{-6^\\circ}$ and $0.19\\pm0.04 c$, respectively. We also find that the measured brightness temperatures of the core at 12, 22 and 44 GHz are close to the equipartition brightness temperature, indicating that the energy density of the radiating particles is comparable to the energy density of the magnetic field in the sub-parsec jet region. Interestingly, the measured core size at 88 GHz ($\\sim25\\pm5 R_{s}$) deviates from the expected frequency dependence seen at lower frequencies. This may indicate a different origin for the millimeter emission, which can explained by an Advection Dominated Accretion Flow (ADAF) model. This model further predicts that at 230 and 340 GHz, the ADAF may dominate the radio emission over the jet.","sentences":["We report multi-frequency and multi-epoch VLBI studies of the sub-parsec jet in Sombrero galaxy (M 104, NGC 4594).","Using Very Long Baseline Array data at 12, 22, 44, and 88 GHz, we study the kinematics of the jet and the properties of the compact core.","The sub-parsec jet is clearly detected at 12 and 22 GHz, and the inner jet base is resolved down to $\\sim70$ Schwarzschild radii ($R_{\\rm s}$) at 44 GHz.","The proper motions of the jet are measured with apparent sub-relativistic speeds of $0.20\\pm0.08 c$ and $0.05\\pm0.02 c$ for the approaching and the receding jet, respectively.","Based on the apparent speed and jet-to-counter-jet brightness ratio, we estimate the jet viewing angle to be larger than $\\sim37^{\\circ}$, and the intrinsic speed to be between $\\sim0.10 c$ and $0.40 c$.","Their joint probability distribution suggests the most probable values of the viewing angle and intrinsic speed to be ${66^{\\circ}}^{+4^\\circ}_{-6^\\circ}$ and $0.19\\pm0.04 c$, respectively.","We also find that the measured brightness temperatures of the core at 12, 22 and 44 GHz are close to the equipartition brightness temperature, indicating that the energy density of the radiating particles is comparable to the energy density of the magnetic field in the sub-parsec jet region.","Interestingly, the measured core size at 88 GHz ($\\sim25\\pm5 R_{s}$) deviates from the expected frequency dependence seen at lower frequencies.","This may indicate a different origin for the millimeter emission, which can explained by an Advection Dominated Accretion Flow (ADAF) model.","This model further predicts that at 230 and 340 GHz, the ADAF may dominate the radio emission over the jet."],"url":"http://arxiv.org/abs/2403.04215v1","category":"astro-ph.GA"}
{"created":"2024-03-07 04:34:47","title":"Representations of not-finitely graded Lie algebras related to Virasoro algebra","abstract":"In this paper, we study representations of not-finitely graded Lie algebras $\\mathcal{W}(\\epsilon)$ related to Virasoro algebra, where $\\epsilon = \\pm 1$. Precisely speaking, we completely classify the free $\\mathcal{U}(\\mathfrak h)$-modules of rank one over $\\mathcal{W}(\\epsilon)$,and find that these module structures are rather different from those of other graded Lie algebras. We also determine the simplicity and isomorphism classes of these modules.","sentences":["In this paper, we study representations of not-finitely graded Lie algebras $\\mathcal{W}(\\epsilon)$ related to Virasoro algebra, where $\\epsilon = \\pm 1$. Precisely speaking, we completely classify the free $\\mathcal{U}(\\mathfrak h)$-modules of rank one over $\\mathcal{W}(\\epsilon)$,and find that these module structures are rather different from those of other graded Lie algebras.","We also determine the simplicity and isomorphism classes of these modules."],"url":"http://arxiv.org/abs/2403.04213v1","category":"math.RT"}
{"created":"2024-03-07 04:33:11","title":"Persona Extraction Through Semantic Similarity for Emotional Support Conversation Generation","abstract":"Providing emotional support through dialogue systems is becoming increasingly important in today's world, as it can support both mental health and social interactions in many conversation scenarios. Previous works have shown that using persona is effective for generating empathetic and supportive responses. They have often relied on pre-provided persona rather than inferring them during conversations. However, it is not always possible to obtain a user persona before the conversation begins. To address this challenge, we propose PESS (Persona Extraction through Semantic Similarity), a novel framework that can automatically infer informative and consistent persona from dialogues. We devise completeness loss and consistency loss based on semantic similarity scores. The completeness loss encourages the model to generate missing persona information, and the consistency loss guides the model to distinguish between consistent and inconsistent persona. Our experimental results demonstrate that high-quality persona information inferred by PESS is effective in generating emotionally supportive responses.","sentences":["Providing emotional support through dialogue systems is becoming increasingly important in today's world, as it can support both mental health and social interactions in many conversation scenarios.","Previous works have shown that using persona is effective for generating empathetic and supportive responses.","They have often relied on pre-provided persona rather than inferring them during conversations.","However, it is not always possible to obtain a user persona before the conversation begins.","To address this challenge, we propose PESS (Persona Extraction through Semantic Similarity), a novel framework that can automatically infer informative and consistent persona from dialogues.","We devise completeness loss and consistency loss based on semantic similarity scores.","The completeness loss encourages the model to generate missing persona information, and the consistency loss guides the model to distinguish between consistent and inconsistent persona.","Our experimental results demonstrate that high-quality persona information inferred by PESS is effective in generating emotionally supportive responses."],"url":"http://arxiv.org/abs/2403.04212v1","category":"cs.CL"}
{"created":"2024-03-07 03:46:26","title":"Orbital Magneto-Nonlinear Anomalous Hall Effect in Kagome Magnet Fe$_3$Sn$_2$","abstract":"It has been theoretically predicted that perturbation of the Berry curvature by electromagnetic fields gives rise to intrinsic nonlinear anomalous Hall effects that are independent of scattering. Two types of nonlinear anomalous Hall effects are expected. The electric nonlinear Hall effect has recently begun to receive attention, while very few studies are concerned with the magneto-nonlinear Hall effect. Here, we combine experiment and first-principles calculations to show that the kagome ferromagnet Fe$_3$Sn$_2$ displays such a magneto-nonlinear Hall effect. By systematic field angular and temperature-dependent transport measurements, we unambiguously identify a large anomalous Hall current that is linear in both applied in-plane electric and magnetic fields, utilizing a unique in-plane configuration. We clarify its dominant orbital origin and connect it to the magneto-nonlinear Hall effect. The effect is governed by the intrinsic quantum geometric properties of Bloch electrons. Our results demonstrate the significance of the quantum geometry of electron wave functions from the orbital degree of freedom and open up a new direction in Hall transport effects.","sentences":["It has been theoretically predicted that perturbation of the Berry curvature by electromagnetic fields gives rise to intrinsic nonlinear anomalous Hall effects that are independent of scattering.","Two types of nonlinear anomalous Hall effects are expected.","The electric nonlinear Hall effect has recently begun to receive attention, while very few studies are concerned with the magneto-nonlinear Hall effect.","Here, we combine experiment and first-principles calculations to show that the kagome ferromagnet Fe$_3$Sn$_2$ displays such a magneto-nonlinear Hall effect.","By systematic field angular and temperature-dependent transport measurements, we unambiguously identify a large anomalous Hall current that is linear in both applied in-plane electric and magnetic fields, utilizing a unique in-plane configuration.","We clarify its dominant orbital origin and connect it to the magneto-nonlinear Hall effect.","The effect is governed by the intrinsic quantum geometric properties of Bloch electrons.","Our results demonstrate the significance of the quantum geometry of electron wave functions from the orbital degree of freedom and open up a new direction in Hall transport effects."],"url":"http://arxiv.org/abs/2403.04192v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-07 03:23:35","title":"Estimating the production of dark photons with $\u03b7$ decay in Ultra-peripheral Collisions","abstract":"We propose that the signal of dark photons can be found in the decay channel of $\\eta$, and that a certain number of events of dark photon leptonic decay can be observed in the double-photon collision process of heavy-ion ultra-peripheral collisions (UPC). We estimate the total cross-section for producing dark photons in ultra-peripheral $PbPb$ collisions at existing and planned future hadron colliders, as well as the number of signal events. Our results support the search for dark photon signals in ultra-peripheral $PbPb$ collisions. We consider the recent signals of $\\eta$ decays produced in LHC $pp$ collisions and estimate the number of dark photon events from the same number of $\\eta$ decays from CMS. Based on CMS results, we estimate that there will be at least 1000 signal events of dark photons observed for the mass $m_{A^\\prime}$ less than 500 MeV. In this study, we propose a method using $\\eta$ meson decay to search for signs of dark photons at heavy-ion colliders, such as LHC and RHIC.","sentences":["We propose that the signal of dark photons can be found in the decay channel of $\\eta$, and that a certain number of events of dark photon leptonic decay can be observed in the double-photon collision process of heavy-ion ultra-peripheral collisions (UPC).","We estimate the total cross-section for producing dark photons in ultra-peripheral $PbPb$ collisions at existing and planned future hadron colliders, as well as the number of signal events.","Our results support the search for dark photon signals in ultra-peripheral $PbPb$ collisions.","We consider the recent signals of $\\eta$ decays produced in LHC $pp$ collisions and estimate the number of dark photon events from the same number of $\\eta$ decays from CMS.","Based on CMS results, we estimate that there will be at least 1000 signal events of dark photons observed for the mass $m_{A^\\prime}$ less than 500 MeV.","In this study, we propose a method using $\\eta$ meson decay to search for signs of dark photons at heavy-ion colliders, such as LHC and RHIC."],"url":"http://arxiv.org/abs/2403.04181v1","category":"hep-ph"}
{"created":"2024-03-07 02:55:10","title":"Nonequilibrium magnonic thermal transport engineering","abstract":"Thermal conductivity, a fundamental parameter characterizing thermal transport in solids, is typically determined by electron and phonon transport. Although other transport properties including electrical conductivity and thermoelectric conversion coefficients have material-specific values, it is known that thermal conductivity can be modulated artificially via phonon engineering techniques. Here, we demonstrate another way of artificially modulating the heat conduction in solids: magnonic thermal transport engineering. The time-domain thermoreflectance measurements using ferromagnetic metal/insulator junction systems reveal that the thermal conductivity of the ferromagnetic metals and interfacial thermal conductance vary significantly depending on the spatial distribution of nonequilibrium spin currents. Systematic measurements of the thermal transport properties with changing the boundary conditions for spin currents show that the observed thermal transport modulation stems from magnon origin. This observation unveils that magnons significantly contribute to the heat conduction even in ferromagnetic metals at room temperature, upsetting the conventional wisdom that the thermal conductivity mediated by magnons is very small in metals except at low temperatures. The magnonic thermal transport engineering offers a new principle and method for active thermal management.","sentences":["Thermal conductivity, a fundamental parameter characterizing thermal transport in solids, is typically determined by electron and phonon transport.","Although other transport properties including electrical conductivity and thermoelectric conversion coefficients have material-specific values, it is known that thermal conductivity can be modulated artificially via phonon engineering techniques.","Here, we demonstrate another way of artificially modulating the heat conduction in solids: magnonic thermal transport engineering.","The time-domain thermoreflectance measurements using ferromagnetic metal/insulator junction systems reveal that the thermal conductivity of the ferromagnetic metals and interfacial thermal conductance vary significantly depending on the spatial distribution of nonequilibrium spin currents.","Systematic measurements of the thermal transport properties with changing the boundary conditions for spin currents show that the observed thermal transport modulation stems from magnon origin.","This observation unveils that magnons significantly contribute to the heat conduction even in ferromagnetic metals at room temperature, upsetting the conventional wisdom that the thermal conductivity mediated by magnons is very small in metals except at low temperatures.","The magnonic thermal transport engineering offers a new principle and method for active thermal management."],"url":"http://arxiv.org/abs/2403.04166v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-07 02:25:47","title":"Destratification in the Progenitor Interior of the Mg-rich Supernova Remnant N49B","abstract":"Simulations of pre-supernova evolution suggest that some intense shell burning can be so active that, in extreme cases, it can merge with the outer shell, changing the initial conditions for the supernova explosion. However, such violent activity in the interior of stars has been difficult to confirm from observations of stars. Here we propose that the elemental composition of O-rich ejecta in supernova remnants can be a tool to test for this kind of intense shell burning activity in the final stages of progenitor evolution. As an example, we discuss the origin of \"Mg-rich\" ejecta in the supernova remnant N49B. A high Mg/Ne mass ratio $\\gtrsim 1$ suggests that the Ne- or O-burning shell has broken into or merged with the outer shell before the collapse. Such Mg-rich (or Ne-poor) ejecta has been identified in some other supernova remnants, supporting the idea that some destratification process, such as a shell merger, does indeed occur in the interiors of some massive stars, although they may not be the majority. Our results suggest that X-ray observations of O-rich ejecta in core-collapse supernova remnants will be a unique tool to probe the shell burning activity during the final stage of a massive star's interior.","sentences":["Simulations of pre-supernova evolution suggest that some intense shell burning can be so active that, in extreme cases, it can merge with the outer shell, changing the initial conditions for the supernova explosion.","However, such violent activity in the interior of stars has been difficult to confirm from observations of stars.","Here we propose that the elemental composition of O-rich ejecta in supernova remnants can be a tool to test for this kind of intense shell burning activity in the final stages of progenitor evolution.","As an example, we discuss the origin of \"Mg-rich\" ejecta in the supernova remnant N49B. A high Mg/Ne mass ratio $\\gtrsim 1$ suggests that the Ne- or O-burning shell has broken into or merged with the outer shell before the collapse.","Such Mg-rich (or Ne-poor) ejecta has been identified in some other supernova remnants, supporting the idea that some destratification process, such as a shell merger, does indeed occur in the interiors of some massive stars, although they may not be the majority.","Our results suggest that X-ray observations of O-rich ejecta in core-collapse supernova remnants will be a unique tool to probe the shell burning activity during the final stage of a massive star's interior."],"url":"http://arxiv.org/abs/2403.04156v1","category":"astro-ph.HE"}
{"created":"2024-03-07 02:09:53","title":"Optical turbulence profiling at the Table Mountain Facility with the Laser Communication Relay Demonstration GEO downlink","abstract":"We present the first time the profile of atmospheric optical turbulence has been measured using the transmitted beam from a satellite laser communication terminal. A Ring Image Next Generation Scintillation Sensor (RINGSS) instrument for turbulence profiling, as described in Tokovinin (MNRAS, 502.1, 2021), was deployed at the NASA/Jet Propulsion Laboratory's Table Mountain Facility (TMF) in California. The optical turbulence profile was measured with the downlink optical beam from the Laser Communication Relay Demonstration (LCRD) Geostationary satellite. LCRD conducts links with the Optical Communication Telescope Laboratory ground station and the RINGSS instrument was co-located at TMF to conduct measurements. Turbulence profiles were measured at day and night and atmospheric coherence lengths were compared with other turbulence monitors such as a solar scintillometer and Polaris monitor. RINGSS sensitivity to boundary layer turbulence, a feature not provided by many profilers, is also shown to agree well with a boundary layer scintillometer at TMF. Diurnal evolution of optical turbulence and measured profiles are presented. The robust correlation of RINGSS with other turbulence monitors demonstrates the concept of free-space optical communications turbulence profiling, which could be adopted as a way to support optical ground stations in a future Geostationary feeder link network. These results also provide further evidence that RINGSS, a relatively new instrument concept, is effective even in strong daytime turbulence and with reasonable ground layer sensitivity.","sentences":["We present the first time the profile of atmospheric optical turbulence has been measured using the transmitted beam from a satellite laser communication terminal.","A Ring Image Next Generation Scintillation Sensor (RINGSS) instrument for turbulence profiling, as described in Tokovinin (MNRAS, 502.1, 2021), was deployed at the NASA/Jet Propulsion Laboratory's Table Mountain Facility (TMF) in California.","The optical turbulence profile was measured with the downlink optical beam from the Laser Communication Relay Demonstration (LCRD)","Geostationary satellite.","LCRD conducts links with the Optical Communication Telescope Laboratory ground station and the RINGSS instrument was co-located at TMF to conduct measurements.","Turbulence profiles were measured at day and night and atmospheric coherence lengths were compared with other turbulence monitors such as a solar scintillometer and Polaris monitor.","RINGSS sensitivity to boundary layer turbulence, a feature not provided by many profilers, is also shown to agree well with a boundary layer scintillometer at TMF.","Diurnal evolution of optical turbulence and measured profiles are presented.","The robust correlation of RINGSS with other turbulence monitors demonstrates the concept of free-space optical communications turbulence profiling, which could be adopted as a way to support optical ground stations in a future Geostationary feeder link network.","These results also provide further evidence that RINGSS, a relatively new instrument concept, is effective even in strong daytime turbulence and with reasonable ground layer sensitivity."],"url":"http://arxiv.org/abs/2403.04148v1","category":"eess.SP"}
{"created":"2024-03-07 00:56:05","title":"Constraints on the topology of Type IIB string theory","abstract":"We discuss some topological constraints on Type IIB string theory that cannot be described by elementary characteristic classes. Anomaly cancellation of the worldvolume theory of a D3-brane requires a shifted Dirac quantization condition of the Ramond-Ramond 5-form flux. However, the required shift is sometimes inconsistent depending on the topology of spacetime backgrounds. The obstruction to the existence of a shifted quantization is described by a degree-6 cohomology class whose definition involves spin structure of spacetime manifolds. The cohomology class is interpreted as a background D3-charge, and the Gauss law constraint requires inclusion of additional D3-branes to cancel it.","sentences":["We discuss some topological constraints on Type IIB string theory that cannot be described by elementary characteristic classes.","Anomaly cancellation of the worldvolume theory of a D3-brane requires a shifted Dirac quantization condition of the Ramond-Ramond 5-form flux.","However, the required shift is sometimes inconsistent depending on the topology of spacetime backgrounds.","The obstruction to the existence of a shifted quantization is described by a degree-6 cohomology class whose definition involves spin structure of spacetime manifolds.","The cohomology class is interpreted as a background D3-charge, and the Gauss law constraint requires inclusion of additional D3-branes to cancel it."],"url":"http://arxiv.org/abs/2403.04127v1","category":"hep-th"}
{"created":"2024-03-07 00:41:35","title":"Sliding into DM: Determining the local dark matter density and speed distribution using only the local circular speed of the Galaxy","abstract":"We use FIRE-2 zoom simulations of Milky Way size disk galaxies to derive easy-to-use relationships between the observed circular speed of the Galaxy at the Solar location, $v_\\mathrm{c}$, and dark matter properties of relevance for direct detection experiments: the dark matter density, the dark matter velocity dispersion, and the speed distribution of dark matter particles near the Solar location. We find that both the local dark matter density and 3D velocity dispersion follow tight power laws with $v_\\mathrm{c}$. Using this relation together with the observed circular speed of the Milky Way at the Solar radius, we infer the local dark matter density and velocity dispersion near the Sun to be $\\rho = 0.42\\pm 0.06\\,\\mathrm{GeV\\,cm^{-3}}$ and $\\sigma_\\mathrm{3D} = 280^{+19}_{-18}\\,\\mathrm{km\\,s^{-1}}$. We also find that the distribution of dark matter particle speeds is well-described by a modified Maxwellian with two shape parameters, both of which correlate with the observed $v_{\\rm c}$. We use that modified Maxwellian to predict the speed distribution of dark matter near the Sun and find that it peaks at a most probable speed of $250\\,\\mathrm{km\\,s^{-1}}$ and begins to truncate sharply above $470\\,\\mathrm{km\\,s^{-1}}$. This peak speed is somewhat higher than expected from the standard halo model, and the truncation occurs well below the formal escape speed to infinity, with fewer very-high-speed particles than assumed in the standard halo model.","sentences":["We use FIRE-2 zoom simulations of Milky Way size disk galaxies to derive easy-to-use relationships between the observed circular speed of the Galaxy at the Solar location, $v_\\mathrm{c}$, and dark matter properties of relevance for direct detection experiments: the dark matter density, the dark matter velocity dispersion, and the speed distribution of dark matter particles near the Solar location.","We find that both the local dark matter density and 3D velocity dispersion follow tight power laws with $v_\\mathrm{c}$. Using this relation together with the observed circular speed of the Milky Way at the Solar radius, we infer the local dark matter density and velocity dispersion near the Sun to be $\\rho = 0.42\\pm 0.06\\,\\mathrm{GeV\\,cm^{-3}}$ and $\\sigma_\\mathrm{3D} = 280^{+19}_{-18}\\,\\mathrm{km\\,s^{-1}}$.","We also find that the distribution of dark matter particle speeds is well-described by a modified Maxwellian with two shape parameters, both of which correlate with the observed $v_{\\rm c}$.","We use that modified Maxwellian to predict the speed distribution of dark matter near the Sun and find that it peaks at a most probable speed of $250\\,\\mathrm{km\\,s^{-1}}$ and begins to truncate sharply above $470\\,\\mathrm{km\\,s^{-1}}$. This peak speed is somewhat higher than expected from the standard halo model, and the truncation occurs well below the formal escape speed to infinity, with fewer very-high-speed particles than assumed in the standard halo model."],"url":"http://arxiv.org/abs/2403.04122v1","category":"astro-ph.GA"}
{"created":"2024-03-06 23:13:24","title":"How do polymers stretch in capillary-driven extensional flows?","abstract":"Measurements of the capillary-driven thinning and breakup of fluid filaments are widely used to extract extensional rheological properties of complex materials. For viscoelastic (e.g., polymeric) fluids, the determination of the longest relaxation time depends on several assumptions concerning the polymeric response to the flow that are derived from constitutive models. Our capillary thinning experiments using polymeric fluids with a wide range of extensibility, suggest that these assumptions are likely only valid for highly extensible polymers but do not hold in general. For polymers with relatively low extensibility, such as polyectrolytes in salt-free media, conventional extrapolation of the longest relaxation time from capillary thinning techniques leads to a significant underestimation.","sentences":["Measurements of the capillary-driven thinning and breakup of fluid filaments are widely used to extract extensional rheological properties of complex materials.","For viscoelastic (e.g., polymeric) fluids, the determination of the longest relaxation time depends on several assumptions concerning the polymeric response to the flow that are derived from constitutive models.","Our capillary thinning experiments using polymeric fluids with a wide range of extensibility, suggest that these assumptions are likely only valid for highly extensible polymers but do not hold in general.","For polymers with relatively low extensibility, such as polyectrolytes in salt-free media, conventional extrapolation of the longest relaxation time from capillary thinning techniques leads to a significant underestimation."],"url":"http://arxiv.org/abs/2403.04103v1","category":"cond-mat.soft"}
{"created":"2024-03-06 22:57:31","title":"Flat-band hybridization between $f$ and $d$ states near the Fermi energy of SmCoIn$_5$","abstract":"We present high-quality angle-resolved photoemission (ARPES) and density functional theory calculations (DFT+U) of SmCoIn$_5$. We find broad agreement with previously published studies of LaCoIn$_5$ and CeCoIn$_5$, confirming that the Sm $4f$ electrons are mostly localized. Nevertheless, our model is consistent with an additional delocalized Sm component, stemming from hybridization between the $4f$ electrons and the metallic bands at \"hot spot\" positions in the Brillouin zone. The dominant hot spot, called $\\gamma_Z$, is similar to a source of delocalized $f$ states found in previous experimental and theoretical studies of CeCoIn$_5$. In this work, we identify and focus on the role of the Co $d$ states in exploring the relationship between heavy quasiparticles and the magnetic interactions in SmCoIn$_5$, which lead to a magnetically ordered ground state from within an intermediate valence scenario. Specifically, we find a globally flat band consisting of Co $d$ states near $E=-0.7$ eV, indicating the possibility of enhanced electronic and magnetic interactions in the \"115\" family of materials through localization in the Co layer, and we discuss a possible origin in geometric frustration. We also show that the delocalized Sm $4f$ states can hybridize directly with the Co $3d_{xz}$/$3d_{yz}$ orbitals, which occurs in our model at the Brillouin zone boundary point $R$ in a band that is locally flat and touches the Fermi level from above. Our work identifies microscopic ingredients for additional magnetic interactions in the \"115\" materials beyond the RKKY mechanism, and strongly suggests that the Co $d$ bands are an important ingredient in the formation of both magnetic and superconducting ground states.","sentences":["We present high-quality angle-resolved photoemission (ARPES) and density functional theory calculations (DFT+U) of SmCoIn$_5$. We find broad agreement with previously published studies of LaCoIn$_5$ and CeCoIn$_5$, confirming that the Sm $4f$ electrons are mostly localized.","Nevertheless, our model is consistent with an additional delocalized Sm component, stemming from hybridization between the $4f$ electrons and the metallic bands at \"hot spot\" positions in the Brillouin zone.","The dominant hot spot, called $\\gamma_Z$, is similar to a source of delocalized $f$ states found in previous experimental and theoretical studies of CeCoIn$_5$.","In this work, we identify and focus on the role of the Co $d$ states in exploring the relationship between heavy quasiparticles and the magnetic interactions in SmCoIn$_5$, which lead to a magnetically ordered ground state from within an intermediate valence scenario.","Specifically, we find a globally flat band consisting of Co $d$ states near $E=-0.7$ eV, indicating the possibility of enhanced electronic and magnetic interactions in the \"115\" family of materials through localization in the Co layer, and we discuss a possible origin in geometric frustration.","We also show that the delocalized Sm $4f$ states can hybridize directly with the Co $3d_{xz}$/$3d_{yz}$ orbitals, which occurs in our model at the Brillouin zone boundary point $R$ in a band that is locally flat and touches the Fermi level from above.","Our work identifies microscopic ingredients for additional magnetic interactions in the \"115\" materials beyond the RKKY mechanism, and strongly suggests that the Co $d$ bands are an important ingredient in the formation of both magnetic and superconducting ground states."],"url":"http://arxiv.org/abs/2403.04097v1","category":"cond-mat.str-el"}
{"created":"2024-03-06 22:14:08","title":"Aggregate morphing of self-aligining soft active disks in semi-confined geometry","abstract":"We study the dependence of alignment and confinement on the aggregate morphology of self-aligning soft disks in a planer box geometry confined along y direction. We show that the wall accumulation of aggregates becomes non-uniform upon increase in alignment strength and decrease in box width. The height of these structures is found to be a non-monotonic function of alignment strength. Additionally, we identify two distinct categories of wall aggregates: layered and non-layered structures each exhibiting distinct local structural properties. For non-layered structures, local properties stay nearly constant as we move away from the boundary, while for layered structures, they increase with distance from the boundary. Our analysis shows that active pressure difference is a useful indicator for different aggregate morphologies and the peaks in the pressure curve are indicative of the average and minimum height of the structure.","sentences":["We study the dependence of alignment and confinement on the aggregate morphology of self-aligning soft disks in a planer box geometry confined along y direction.","We show that the wall accumulation of aggregates becomes non-uniform upon increase in alignment strength and decrease in box width.","The height of these structures is found to be a non-monotonic function of alignment strength.","Additionally, we identify two distinct categories of wall aggregates: layered and non-layered structures each exhibiting distinct local structural properties.","For non-layered structures, local properties stay nearly constant as we move away from the boundary, while for layered structures, they increase with distance from the boundary.","Our analysis shows that active pressure difference is a useful indicator for different aggregate morphologies and the peaks in the pressure curve are indicative of the average and minimum height of the structure."],"url":"http://arxiv.org/abs/2403.04075v1","category":"cond-mat.soft"}
{"created":"2024-03-06 21:48:41","title":"Relational Quantum Mechanics, Quantum Relativism, and the Iteration of Relativity","abstract":"The idea that the dynamical properties of quantum systems are invariably relative to other systems has recently regained currency. Using Relational Quantum Mechanics (RQM) for a case study, this paper calls attention to a question that has been underappreciated in the debate about quantum relativism: the question of whether relativity iterates. Are there absolute facts about the properties one system possesses relative to a specified reference, or is this again a relative matter, and so on? It is argued that RQM (in its best-known form) is committed to what I call the Unrestricted Iteration Principle (UIP), and thus to an infinite regress of relativisations. This principle plays a crucial role in ensuring the communicability and coherence of interaction outcomes across observers. It is, however, shown to be incompatible with the widespread, conservative reading of RQM in terms of relations, instead necessitating the adoption of the more unorthodox notion of perspectival facts. I conclude with some reflections on the current state of play in perspectivist versions of RQM and quantum relativism more generally, underscoring both the need for further conceptual development and the importance of the iteration principle for an accurate cost-benefit analysis of such interpretations.","sentences":["The idea that the dynamical properties of quantum systems are invariably relative to other systems has recently regained currency.","Using Relational Quantum Mechanics (RQM) for a case study, this paper calls attention to a question that has been underappreciated in the debate about quantum relativism: the question of whether relativity iterates.","Are there absolute facts about the properties one system possesses relative to a specified reference, or is this again a relative matter, and so on?","It is argued that RQM (in its best-known form) is committed to what I call the Unrestricted Iteration Principle (UIP), and thus to an infinite regress of relativisations.","This principle plays a crucial role in ensuring the communicability and coherence of interaction outcomes across observers.","It is, however, shown to be incompatible with the widespread, conservative reading of RQM in terms of relations, instead necessitating the adoption of the more unorthodox notion of perspectival facts.","I conclude with some reflections on the current state of play in perspectivist versions of RQM and quantum relativism more generally, underscoring both the need for further conceptual development and the importance of the iteration principle for an accurate cost-benefit analysis of such interpretations."],"url":"http://arxiv.org/abs/2403.04069v1","category":"physics.hist-ph"}
{"created":"2024-03-06 21:42:57","title":"A Simple Model of Pentaquarks","abstract":"We describe pentaquarks as baryo-charmonia with a color octet $c\\bar{c}$ core bonded to a color octet three-quark system. Fermi statistics of the light quark cloud allows to describe two pentaquark triplets: a lower one, well supported by experiment, and a higher one with strangeness. For the time being, the lowest line of the strange triplet has been experimentally identified in a $3\\sigma$ peak. Data also suggest (at least) two different production mechanisms for pentaquarks. We show how this can be described in the proposed scheme.","sentences":["We describe pentaquarks as baryo-charmonia with a color octet $c\\bar{c}$ core bonded to a color octet three-quark system.","Fermi statistics of the light quark cloud allows to describe two pentaquark triplets: a lower one, well supported by experiment, and a higher one with strangeness.","For the time being, the lowest line of the strange triplet has been experimentally identified in a $3\\sigma$ peak.","Data also suggest (at least) two different production mechanisms for pentaquarks.","We show how this can be described in the proposed scheme."],"url":"http://arxiv.org/abs/2403.04068v1","category":"hep-ph"}
{"created":"2024-03-06 20:59:02","title":"A General PSTD Method to Solve Quantum Scattering in the Fresnel and Far-field regions by A Localized Potential of Arbitrary Form","abstract":"We present a time domain method to solve quantum scattering by an arbitrary potential of finite range. The scattering wave function in full space can be obtained, including the near field, the mid field (i.e. Fresnel region) and the far field. This is achieved by extending several techniques of FDTD computational electrodynamics into the quantum realm. The total-field/scattered-field scheme naturally incorporates the incidence source condition. The wave function in the internal model, including the interaction region and the close near field, is directly computed through PSTD/FDTD iterations. The quantum version of surface equivalence theorem is proven and links the wave function in the external free space to the PSTD/FDTD solution in the internal model. Parallel implementation of PSTD based on overlapping domain decomposition and FFT on local Fourier-basis is briefly discussed. These building blocks unite into a numerical system that provides a general, robust solver to potential scattering problems. Its accuracy is verified by the established partial wave method, by comparing the predictions of both on the central square potential scattering. Further investigations show the far-field solution is inadequate for simulating Fresnel-region effects.","sentences":["We present a time domain method to solve quantum scattering by an arbitrary potential of finite range.","The scattering wave function in full space can be obtained, including the near field, the mid field (i.e. Fresnel region) and the far field.","This is achieved by extending several techniques of FDTD computational electrodynamics into the quantum realm.","The total-field/scattered-field scheme naturally incorporates the incidence source condition.","The wave function in the internal model, including the interaction region and the close near field, is directly computed through PSTD/FDTD iterations.","The quantum version of surface equivalence theorem is proven and links the wave function in the external free space to the PSTD/FDTD solution in the internal model.","Parallel implementation of PSTD based on overlapping domain decomposition and FFT on local Fourier-basis is briefly discussed.","These building blocks unite into a numerical system that provides a general, robust solver to potential scattering problems.","Its accuracy is verified by the established partial wave method, by comparing the predictions of both on the central square potential scattering.","Further investigations show the far-field solution is inadequate for simulating Fresnel-region effects."],"url":"http://arxiv.org/abs/2403.04053v1","category":"quant-ph"}
