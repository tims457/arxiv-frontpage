{"created":"2024-05-09 17:59:56","title":"Gravitational wave signatures of departures from classical black hole scattering","abstract":"We initiate a general investigation into gravitational wave signatures of modifications to scattering of gravitational radiation from black holes. Such modifications may be present due to the quantum dynamics that makes black holes consistent with quantum mechanics, or in other models for departures from classical black hole behavior. We propose a parameterization of the corrections to scattering as a physically meaningful, model-independent, and practical bridge between theoretical and observational aspects of the problem; this parameterization can incorporate different models in the literature. We then describe how these corrections influence the gravitational wave signal, e.g. of a body orbiting a much more massive black hole. In particular, they generically change the rate of energy emission; this effect can be leveraged over many orbits of inspiral to enhance the sensitivity to small corrections, as has been noticed in simple models. We provide preliminary estimates of the sensitivity of future gravitational wave observations to these corrections, and outline further work to be done to connect both to a more fundamental theory of quantum black holes, and to realistic observational situations.","sentences":["We initiate a general investigation into gravitational wave signatures of modifications to scattering of gravitational radiation from black holes.","Such modifications may be present due to the quantum dynamics that makes black holes consistent with quantum mechanics, or in other models for departures from classical black hole behavior.","We propose a parameterization of the corrections to scattering as a physically meaningful, model-independent, and practical bridge between theoretical and observational aspects of the problem; this parameterization can incorporate different models in the literature.","We then describe how these corrections influence the gravitational wave signal, e.g. of a body orbiting a much more massive black hole.","In particular, they generically change the rate of energy emission; this effect can be leveraged over many orbits of inspiral to enhance the sensitivity to small corrections, as has been noticed in simple models.","We provide preliminary estimates of the sensitivity of future gravitational wave observations to these corrections, and outline further work to be done to connect both to a more fundamental theory of quantum black holes, and to realistic observational situations."],"url":"http://arxiv.org/abs/2405.05970v1","category":"gr-qc"}
{"created":"2024-05-09 17:59:55","title":"A Universal Growth Rate for Learning with Smooth Surrogate Losses","abstract":"This paper presents a comprehensive analysis of the growth rate of $H$-consistency bounds (and excess error bounds) for various surrogate losses used in classification. We prove a square-root growth rate near zero for smooth margin-based surrogate losses in binary classification, providing both upper and lower bounds under mild assumptions. This result also translates to excess error bounds. Our lower bound requires weaker conditions than those in previous work for excess error bounds, and our upper bound is entirely novel. Moreover, we extend this analysis to multi-class classification with a series of novel results, demonstrating a universal square-root growth rate for smooth comp-sum and constrained losses, covering common choices for training neural networks in multi-class classification. Given this universal rate, we turn to the question of choosing among different surrogate losses. We first examine how $H$-consistency bounds vary across surrogates based on the number of classes. Next, ignoring constants and focusing on behavior near zero, we identify minimizability gaps as the key differentiating factor in these bounds. Thus, we thoroughly analyze these gaps, to guide surrogate loss selection, covering: comparisons across different comp-sum losses, conditions where gaps become zero, and general conditions leading to small gaps. Additionally, we demonstrate the key role of minimizability gaps in comparing excess error bounds and $H$-consistency bounds.","sentences":["This paper presents a comprehensive analysis of the growth rate of $H$-consistency bounds (and excess error bounds) for various surrogate losses used in classification.","We prove a square-root growth rate near zero for smooth margin-based surrogate losses in binary classification, providing both upper and lower bounds under mild assumptions.","This result also translates to excess error bounds.","Our lower bound requires weaker conditions than those in previous work for excess error bounds, and our upper bound is entirely novel.","Moreover, we extend this analysis to multi-class classification with a series of novel results, demonstrating a universal square-root growth rate for smooth comp-sum and constrained losses, covering common choices for training neural networks in multi-class classification.","Given this universal rate, we turn to the question of choosing among different surrogate losses.","We first examine how $H$-consistency bounds vary across surrogates based on the number of classes.","Next, ignoring constants and focusing on behavior near zero, we identify minimizability gaps as the key differentiating factor in these bounds.","Thus, we thoroughly analyze these gaps, to guide surrogate loss selection, covering: comparisons across different comp-sum losses, conditions where gaps become zero, and general conditions leading to small gaps.","Additionally, we demonstrate the key role of minimizability gaps in comparing excess error bounds and $H$-consistency bounds."],"url":"http://arxiv.org/abs/2405.05968v1","category":"cs.LG"}
{"created":"2024-05-09 17:59:40","title":"Distilling Diffusion Models into Conditional GANs","abstract":"We propose a method to distill a complex multistep diffusion model into a single-step conditional GAN student model, dramatically accelerating inference, while preserving image quality. Our approach interprets diffusion distillation as a paired image-to-image translation task, using noise-to-image pairs of the diffusion model's ODE trajectory. For efficient regression loss computation, we propose E-LatentLPIPS, a perceptual loss operating directly in diffusion model's latent space, utilizing an ensemble of augmentations. Furthermore, we adapt a diffusion model to construct a multi-scale discriminator with a text alignment loss to build an effective conditional GAN-based formulation. E-LatentLPIPS converges more efficiently than many existing distillation methods, even accounting for dataset construction costs. We demonstrate that our one-step generator outperforms cutting-edge one-step diffusion distillation models - DMD, SDXL-Turbo, and SDXL-Lightning - on the zero-shot COCO benchmark.","sentences":["We propose a method to distill a complex multistep diffusion model into a single-step conditional GAN student model, dramatically accelerating inference, while preserving image quality.","Our approach interprets diffusion distillation as a paired image-to-image translation task, using noise-to-image pairs of the diffusion model's ODE trajectory.","For efficient regression loss computation, we propose E-LatentLPIPS, a perceptual loss operating directly in diffusion model's latent space, utilizing an ensemble of augmentations.","Furthermore, we adapt a diffusion model to construct a multi-scale discriminator with a text alignment loss to build an effective conditional GAN-based formulation.","E-LatentLPIPS converges more efficiently than many existing distillation methods, even accounting for dataset construction costs.","We demonstrate that our one-step generator outperforms cutting-edge one-step diffusion distillation models - DMD, SDXL-Turbo, and SDXL-Lightning - on the zero-shot COCO benchmark."],"url":"http://arxiv.org/abs/2405.05967v1","category":"cs.CV"}
{"created":"2024-05-09 17:59:32","title":"Natural Language Processing RELIES on Linguistics","abstract":"Large Language Models (LLMs) have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence. What does this mean for the future of linguistic expertise in NLP? We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions. We argue our case around the acronym $RELIES$ that encapsulates six major facets where linguistics contributes to NLP: $R$esources, $E$valuation, $L$ow-resource settings, $I$nterpretability, $E$xplanation, and the $S$tudy of language. This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-a-vis systems of human language.","sentences":["Large Language Models (LLMs) have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence.","What does this mean for the future of linguistic expertise in NLP?","We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions.","We argue our case around the acronym $RELIES$ that encapsulates six major facets where linguistics contributes to NLP: $R$esources, $E$valuation, $L$ow-resource settings, $I$nterpretability, $E$xplanation, and the $S$tudy of language.","This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-a-vis systems of human language."],"url":"http://arxiv.org/abs/2405.05966v1","category":"cs.CL"}
{"created":"2024-05-09 17:59:05","title":"Quantum Communication and Mixed-State Order in Decohered Symmetry-Protected Topological States","abstract":"Certain pure-state symmetry-protected topological orders (SPT) can be used as a resource for transmitting quantum information. Here, we investigate the ability to transmit quantum information using decohered SPT states, and relate this property to the \"strange correlation functions\" which diagnose quantum many-body orders in these mixed-states. This perspective leads to the identification of a class of quantum channels -- termed symmetry-decoupling channels -- which do not necessarily preserve any weak or strong symmetries of the SPT state, but nevertheless protect quantum many-body order in the decohered mixed-state. We quantify the ability to transmit quantum information in decohered SPT states through the coherent quantum information, whose behavior is generally related to a decoding problem, whereby local measurements in the system are used to attempt to \"learn\" the symmetry charge of the SPT state before decoherence.","sentences":["Certain pure-state symmetry-protected topological orders (SPT) can be used as a resource for transmitting quantum information.","Here, we investigate the ability to transmit quantum information using decohered SPT states, and relate this property to the \"strange correlation functions\" which diagnose quantum many-body orders in these mixed-states.","This perspective leads to the identification of a class of quantum channels -- termed symmetry-decoupling channels -- which do not necessarily preserve any weak or strong symmetries of the SPT state, but nevertheless protect quantum many-body order in the decohered mixed-state.","We quantify the ability to transmit quantum information in decohered SPT states through the coherent quantum information, whose behavior is generally related to a decoding problem, whereby local measurements in the system are used to attempt to \"learn\" the symmetry charge of the SPT state before decoherence."],"url":"http://arxiv.org/abs/2405.05965v1","category":"quant-ph"}
{"created":"2024-05-09 17:59:00","title":"Lattice Models for Phases and Transitions with Non-Invertible Symmetries","abstract":"Non-invertible categorical symmetries have emerged as a powerful tool to uncover new beyond-Landau phases of matter, both gapped and gapless, along with second order phase transitions between them. The general theory of such phases in (1+1)d has been studied using the Symmetry Topological Field Theory (SymTFT), also known as topological holography. This has unearthed the infrared (IR) structure of these phases and transitions. In this paper, we describe how the SymTFT information can be converted into an ultraviolet (UV) anyonic chain lattice model realizing in the IR limit these phases and transitions. In many cases, the Hilbert space of the anyonic chain is tensor product decomposable and the model can be realized as a quantum spin-chain Hamiltonian. We also describe operators acting on the lattice models that are charged under non-invertible symmetries and act as order parameters for the phases and transitions. In order to fully describe the action of non-invertible symmetries, it is crucial to understand the symmetry twisted sectors of the lattice models, which we describe in detail. Throughout the paper, we illustrate the general concepts using the symmetry category $\\mathsf{Rep}(S_3)$ formed by representations of the permutation group $S_3$, but our procedure can be applied to any fusion category symmetry.","sentences":["Non-invertible categorical symmetries have emerged as a powerful tool to uncover new beyond-Landau phases of matter, both gapped and gapless, along with second order phase transitions between them.","The general theory of such phases in (1+1)d has been studied using the Symmetry Topological Field Theory (SymTFT), also known as topological holography.","This has unearthed the infrared (IR) structure of these phases and transitions.","In this paper, we describe how the SymTFT information can be converted into an ultraviolet (UV) anyonic chain lattice model realizing in the IR limit these phases and transitions.","In many cases, the Hilbert space of the anyonic chain is tensor product decomposable and the model can be realized as a quantum spin-chain Hamiltonian.","We also describe operators acting on the lattice models that are charged under non-invertible symmetries and act as order parameters for the phases and transitions.","In order to fully describe the action of non-invertible symmetries, it is crucial to understand the symmetry twisted sectors of the lattice models, which we describe in detail.","Throughout the paper, we illustrate the general concepts using the symmetry category $\\mathsf{Rep}(S_3)$ formed by representations of the permutation group $S_3$, but our procedure can be applied to any fusion category symmetry."],"url":"http://arxiv.org/abs/2405.05964v1","category":"cond-mat.str-el"}
{"created":"2024-05-09 17:58:40","title":"Wormhole restrictions from quantum energy inequalities","abstract":"Wormhole solutions, bridges that connect different parts of spacetime, were proposed early in the history of General Relativity. Soon after, it was shown that all wormholes violate classical energy conditions, non-negativity constraints on contractions of the stress-energy tensor. Since these are violated by quantum fields, it was believed that wormholes can be constructed in the context of semiclassical gravity. But negative energies in quantum field theory are not without restriction: quantum energy inequalities (QEIs) control renormalized negative energies averaged over a geodesic. Thus, QEIs provide restrictions on the construction of wormholes. This work is a review of the relevant literature, focusing on results where QEIs restrict traversable wormholes. Both 'short' and 'long' (without causality violations) wormhole solutions in the context of semiclassical gravity are examined. A new result is presented on constraints on the Maldacena, Milekhin and Popov 'long' wormhole from the recently derived doubled smeared null energy condition.","sentences":["Wormhole solutions, bridges that connect different parts of spacetime, were proposed early in the history of General Relativity.","Soon after, it was shown that all wormholes violate classical energy conditions, non-negativity constraints on contractions of the stress-energy tensor.","Since these are violated by quantum fields, it was believed that wormholes can be constructed in the context of semiclassical gravity.","But negative energies in quantum field theory are not without restriction: quantum energy inequalities (QEIs) control renormalized negative energies averaged over a geodesic.","Thus, QEIs provide restrictions on the construction of wormholes.","This work is a review of the relevant literature, focusing on results where QEIs restrict traversable wormholes.","Both 'short' and 'long' (without causality violations) wormhole solutions in the context of semiclassical gravity are examined.","A new result is presented on constraints on the Maldacena, Milekhin and Popov 'long' wormhole from the recently derived doubled smeared null energy condition."],"url":"http://arxiv.org/abs/2405.05963v1","category":"gr-qc"}
{"created":"2024-05-09 17:57:46","title":"Towards comprehensive coverage of chemical space: Quantum mechanical properties of 836k constitutional and conformational closed shell neutral isomers consisting of HCNOFSiPSClBr","abstract":"The Vector-QM24 (VQM24) dataset attempts to more comprehensively cover all possible neutral closed shell small organic and inorganic molecules and their conformers at state of the art level of theory. We have used density functional theory ($\\omega$B97X-D3/cc-pVDZ) to optimize 577k conformational isomers corresponding to 258k constitutional isomers.Isomers included contain up to five heavy atoms (non-hydrogen) consisting of $p$-block elements C, N, O, F, Si, P, S, Cl, Br. Single point diffusion quantum Monte Carlo (DMC@PBE0(ccECP/cc-pVQZ)) energies are reported for the sub-set of the lowest conformers of 10,793 molecules with up to 4 heavy atoms.This dataset has been systematically generated by considering all combinatorially possible stoichiometries, and graphs (according to Lewis rules as implemented in the {\\tt SURGE} package), along with all stable conformers identified by GFN2-xTB. Apart from graphs, geometries, rotational constants, and vibrational normal modes, VQM24 includes internal, atomization, electron-electron repulsion, exchange correlation, dispersion, vibrational frequency, Gibbs free, enthalpy, ZPV, molecular orbital energies; as well as entropy, and heat capacities. Electronic properties include multipole moments (dipole, quadrupole, octupole, hexadecapole), electrostatic potentials at nuclei (alchemical potential), Mulliken charges, and molecular wavefunctions. VQM24 represents a highly accurate and unbiased dataset of molecules, ideal for testing and training transferable, scalable, and generative ML models of real quantum systems.","sentences":["The Vector-QM24 (VQM24) dataset attempts to more comprehensively cover all possible neutral closed shell small organic and inorganic molecules and their conformers at state of the art level of theory.","We have used density functional theory ($\\omega$B97X-D3/cc-pVDZ) to optimize 577k conformational isomers corresponding to 258k constitutional isomers.","Isomers included contain up to five heavy atoms (non-hydrogen) consisting of $p$-block elements C, N, O, F, Si, P, S, Cl, Br.","Single point diffusion quantum Monte Carlo (DMC@PBE0(ccECP/cc-pVQZ))","energies are reported for the sub-set of the lowest conformers of 10,793 molecules with up to 4 heavy atoms.","This dataset has been systematically generated by considering all combinatorially possible stoichiometries, and graphs (according to Lewis rules as implemented in the {\\tt SURGE} package), along with all stable conformers identified by GFN2-xTB.","Apart from graphs, geometries, rotational constants, and vibrational normal modes, VQM24 includes internal, atomization, electron-electron repulsion, exchange correlation, dispersion, vibrational frequency, Gibbs free, enthalpy, ZPV, molecular orbital energies; as well as entropy, and heat capacities.","Electronic properties include multipole moments (dipole, quadrupole, octupole, hexadecapole), electrostatic potentials at nuclei (alchemical potential), Mulliken charges, and molecular wavefunctions.","VQM24 represents a highly accurate and unbiased dataset of molecules, ideal for testing and training transferable, scalable, and generative ML models of real quantum systems."],"url":"http://arxiv.org/abs/2405.05961v1","category":"physics.chem-ph"}
{"created":"2024-05-09 17:55:16","title":"Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting Mask","abstract":"Time Series Representation Learning (TSRL) focuses on generating informative representations for various Time Series (TS) modeling tasks. Traditional Self-Supervised Learning (SSL) methods in TSRL fall into four main categories: reconstructive, adversarial, contrastive, and predictive, each with a common challenge of sensitivity to noise and intricate data nuances. Recently, diffusion-based methods have shown advanced generative capabilities. However, they primarily target specific application scenarios like imputation and forecasting, leaving a gap in leveraging diffusion models for generic TSRL. Our work, Time Series Diffusion Embedding (TSDE), bridges this gap as the first diffusion-based SSL TSRL approach. TSDE segments TS data into observed and masked parts using an Imputation-Interpolation-Forecasting (IIF) mask. It applies a trainable embedding function, featuring dual-orthogonal Transformer encoders with a crossover mechanism, to the observed part. We train a reverse diffusion process conditioned on the embeddings, designed to predict noise added to the masked part. Extensive experiments demonstrate TSDE's superiority in imputation, interpolation, forecasting, anomaly detection, classification, and clustering. We also conduct an ablation study, present embedding visualizations, and compare inference speed, further substantiating TSDE's efficiency and validity in learning representations of TS data.","sentences":["Time Series Representation Learning (TSRL) focuses on generating informative representations for various Time Series (TS) modeling tasks.","Traditional Self-Supervised Learning (SSL) methods in TSRL fall into four main categories: reconstructive, adversarial, contrastive, and predictive, each with a common challenge of sensitivity to noise and intricate data nuances.","Recently, diffusion-based methods have shown advanced generative capabilities.","However, they primarily target specific application scenarios like imputation and forecasting, leaving a gap in leveraging diffusion models for generic TSRL.","Our work, Time Series Diffusion Embedding (TSDE), bridges this gap as the first diffusion-based SSL TSRL approach.","TSDE segments TS data into observed and masked parts using an Imputation-Interpolation-Forecasting (IIF) mask.","It applies a trainable embedding function, featuring dual-orthogonal Transformer encoders with a crossover mechanism, to the observed part.","We train a reverse diffusion process conditioned on the embeddings, designed to predict noise added to the masked part.","Extensive experiments demonstrate TSDE's superiority in imputation, interpolation, forecasting, anomaly detection, classification, and clustering.","We also conduct an ablation study, present embedding visualizations, and compare inference speed, further substantiating TSDE's efficiency and validity in learning representations of TS data."],"url":"http://arxiv.org/abs/2405.05959v1","category":"cs.LG"}
{"created":"2024-05-09 17:53:41","title":"Stability of slow Hamiltonian dynamics from Lieb-Robinson bounds","abstract":"We rigorously show that a local spin system giving rise to a slow Hamiltonian dynamics is stable against generic, even time-dependent, local perturbations. The sum of these perturbations can cover a significant amount of the system's size. The stability of the slow dynamics follows from proving that the Lieb-Robinson bound for the dynamics of the total Hamiltonian is the sum of two contributions: the Lieb-Robinson bound of the unperturbed dynamics and an additional term coming from the Lieb-Robinson bound of the perturbations with respect to the unperturbed Hamiltonian. Our results are particularly relevant in the context of the study of the stability of Many-Body-Localized systems, implying that if a so called ergodic region is present in the system, to spread across a certain distance it takes a time proportional to the exponential of such distance. The non-perturbative nature of our result allows us to develop a dual description of the dynamics of a system. As a consequence we are able to prove that the presence of a region of disorder in a ergodic system implies the slowing down of the dynamics in the vicinity of that region.","sentences":["We rigorously show that a local spin system giving rise to a slow Hamiltonian dynamics is stable against generic, even time-dependent, local perturbations.","The sum of these perturbations can cover a significant amount of the system's size.","The stability of the slow dynamics follows from proving that the Lieb-Robinson bound for the dynamics of the total Hamiltonian is the sum of two contributions: the Lieb-Robinson bound of the unperturbed dynamics and an additional term coming from the Lieb-Robinson bound of the perturbations with respect to the unperturbed Hamiltonian.","Our results are particularly relevant in the context of the study of the stability of Many-Body-Localized systems, implying that if a so called ergodic region is present in the system, to spread across a certain distance it takes a time proportional to the exponential of such distance.","The non-perturbative nature of our result allows us to develop a dual description of the dynamics of a system.","As a consequence we are able to prove that the presence of a region of disorder in a ergodic system implies the slowing down of the dynamics in the vicinity of that region."],"url":"http://arxiv.org/abs/2405.05958v1","category":"quant-ph"}
{"created":"2024-05-09 17:52:42","title":"Probing Multimodal LLMs as World Models for Driving","abstract":"We provide a sober look at the application of Multimodal Large Language Models (MLLMs) within the domain of autonomous driving and challenge/verify some common assumptions, focusing on their ability to reason and interpret dynamic driving scenarios through sequences of images/frames in a closed-loop control environment. Despite the significant advancements in MLLMs like GPT-4V, their performance in complex, dynamic driving environments remains largely untested and presents a wide area of exploration. We conduct a comprehensive experimental study to evaluate the capability of various MLLMs as world models for driving from the perspective of a fixed in-car camera. Our findings reveal that, while these models proficiently interpret individual images, they struggle significantly with synthesizing coherent narratives or logical sequences across frames depicting dynamic behavior. The experiments demonstrate considerable inaccuracies in predicting (i) basic vehicle dynamics (forward/backward, acceleration/deceleration, turning right or left), (ii) interactions with other road actors (e.g., identifying speeding cars or heavy traffic), (iii) trajectory planning, and (iv) open-set dynamic scene reasoning, suggesting biases in the models' training data. To enable this experimental study we introduce a specialized simulator, DriveSim, designed to generate diverse driving scenarios, providing a platform for evaluating MLLMs in the realms of driving. Additionally, we contribute the full open-source code and a new dataset, \"Eval-LLM-Drive\", for evaluating MLLMs in driving. Our results highlight a critical gap in the current capabilities of state-of-the-art MLLMs, underscoring the need for enhanced foundation models to improve their applicability in real-world dynamic environments.","sentences":["We provide a sober look at the application of Multimodal Large Language Models (MLLMs) within the domain of autonomous driving and challenge/verify some common assumptions, focusing on their ability to reason and interpret dynamic driving scenarios through sequences of images/frames in a closed-loop control environment.","Despite the significant advancements in MLLMs like GPT-4V, their performance in complex, dynamic driving environments remains largely untested and presents a wide area of exploration.","We conduct a comprehensive experimental study to evaluate the capability of various MLLMs as world models for driving from the perspective of a fixed in-car camera.","Our findings reveal that, while these models proficiently interpret individual images, they struggle significantly with synthesizing coherent narratives or logical sequences across frames depicting dynamic behavior.","The experiments demonstrate considerable inaccuracies in predicting (i) basic vehicle dynamics (forward/backward, acceleration/deceleration, turning right or left), (ii) interactions with other road actors (e.g., identifying speeding cars or heavy traffic), (iii) trajectory planning, and (iv) open-set dynamic scene reasoning, suggesting biases in the models' training data.","To enable this experimental study we introduce a specialized simulator, DriveSim, designed to generate diverse driving scenarios, providing a platform for evaluating MLLMs in the realms of driving.","Additionally, we contribute the full open-source code and a new dataset, \"Eval-LLM-Drive\", for evaluating MLLMs in driving.","Our results highlight a critical gap in the current capabilities of state-of-the-art MLLMs, underscoring the need for enhanced foundation models to improve their applicability in real-world dynamic environments."],"url":"http://arxiv.org/abs/2405.05956v1","category":"cs.RO"}
{"created":"2024-05-09 17:46:22","title":"Frame Interpolation with Consecutive Brownian Bridge Diffusion","abstract":"Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a diffusion-based conditional image generation problem, synthesizing the intermediate frame given a random noise and neighboring frames. Due to the relatively high resolution of videos, Latent Diffusion Models (LDMs) are employed as the conditional generation model, where the autoencoder compresses images into latent representations for diffusion and then reconstructs images from these latent representations. Such a formulation poses a crucial challenge: VFI expects that the output is deterministically equal to the ground truth intermediate frame, but LDMs randomly generate a diverse set of different images when the model runs multiple times. The reason for the diverse generation is that the cumulative variance (variance accumulated at each step of generation) of generated latent representations in LDMs is large. This makes the sampling trajectory random, resulting in diverse rather than deterministic generations. To address this problem, we propose our unique solution: Frame Interpolation with Consecutive Brownian Bridge Diffusion. Specifically, we propose consecutive Brownian Bridge diffusion that takes a deterministic initial value as input, resulting in a much smaller cumulative variance of generated latent representations. Our experiments suggest that our method can improve together with the improvement of the autoencoder and achieve state-of-the-art performance in VFI, leaving strong potential for further enhancement.","sentences":["Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a diffusion-based conditional image generation problem, synthesizing the intermediate frame given a random noise and neighboring frames.","Due to the relatively high resolution of videos, Latent Diffusion Models (LDMs) are employed as the conditional generation model, where the autoencoder compresses images into latent representations for diffusion and then reconstructs images from these latent representations.","Such a formulation poses a crucial challenge: VFI expects that the output is deterministically equal to the ground truth intermediate frame, but LDMs randomly generate a diverse set of different images when the model runs multiple times.","The reason for the diverse generation is that the cumulative variance (variance accumulated at each step of generation) of generated latent representations in LDMs is large.","This makes the sampling trajectory random, resulting in diverse rather than deterministic generations.","To address this problem, we propose our unique solution: Frame Interpolation with Consecutive Brownian Bridge Diffusion.","Specifically, we propose consecutive Brownian Bridge diffusion that takes a deterministic initial value as input, resulting in a much smaller cumulative variance of generated latent representations.","Our experiments suggest that our method can improve together with the improvement of the autoencoder and achieve state-of-the-art performance in VFI, leaving strong potential for further enhancement."],"url":"http://arxiv.org/abs/2405.05953v1","category":"cs.CV"}
{"created":"2024-05-09 17:44:29","title":"New Algorithms and Lower Bounds for Streaming Tournaments","abstract":"We study fundamental directed graph (digraph) problems in the streaming model. An initial investigation by Chakrabarti, Ghosh, McGregor, and Vorotnikova [SODA'20] on streaming digraphs showed that while most of these problems are provably hard in general, some of them become tractable when restricted to the well-studied class of tournament graphs where every pair of nodes shares exactly one directed edge. Thus, we focus on tournaments and improve the state of the art for multiple problems in terms of both upper and lower bounds.   Our primary upper bound is a deterministic single-pass semi-streaming algorithm (using $\\tilde{O}(n)$ space for $n$-node graphs, where $\\tilde{O}(.)$ hides polylog$(n)$ factors) for decomposing a tournament into strongly connected components (SCC). it improves upon the previously best-known algorithm by Baweja, Jia, and Woodruff [ITCS'22] in terms of both space and passes: for $p\\geq 1$, they used $(p+1)$-passes and $\\tilde{O}(n^{1+1/p})$-space. We further extend our algorithm to digraphs that are close to tournaments and establish tight bounds demonstrating that the problem's complexity grows smoothly with the \"distance\" from tournaments. Applying our framework, we obtain improved tournament algorithms for $s,t$-reachability, strong connectivity, Hamiltonian paths and cycles, and feedback arc set.   On the other hand, we prove the first $\\Omega(n^2)$-space lower bounds for this class, exhibiting that some well-studied problems -- such as (exact) feedback arc set on tournaments (FAST) and $s,t$-distance -- remain hard here. We obtain a generalized lower bound on space-approximation tradeoffs for FAST: any single-pass $(1\\pm \\varepsilon)$-approximation algorithm requires $\\Omega(n/\\sqrt{\\varepsilon})$ space. As a whole, our collection of results contributes significantly to the growing literature on streaming digraphs.","sentences":["We study fundamental directed graph (digraph) problems in the streaming model.","An initial investigation by Chakrabarti, Ghosh, McGregor, and Vorotnikova [SODA'20] on streaming digraphs showed that while most of these problems are provably hard in general, some of them become tractable when restricted to the well-studied class of tournament graphs where every pair of nodes shares exactly one directed edge.","Thus, we focus on tournaments and improve the state of the art for multiple problems in terms of both upper and lower bounds.   ","Our primary upper bound is a deterministic single-pass semi-streaming algorithm (using $\\tilde{O}(n)$ space for $n$-node graphs, where $\\tilde{O}(.)$ hides polylog$(n)$ factors) for decomposing a tournament into strongly connected components (SCC).","it improves upon the previously best-known algorithm by Baweja, Jia, and Woodruff","[ITCS'22] in terms of both space and passes: for $p\\geq 1$, they used $(p+1)$-passes and $\\tilde{O}(n^{1+1/p})$-space.","We further extend our algorithm to digraphs that are close to tournaments and establish tight bounds demonstrating that the problem's complexity grows smoothly with the \"distance\" from tournaments.","Applying our framework, we obtain improved tournament algorithms for $s,t$-reachability, strong connectivity, Hamiltonian paths and cycles, and feedback arc set.   ","On the other hand, we prove the first $\\Omega(n^2)$-space lower bounds for this class, exhibiting that some well-studied problems -- such as (exact) feedback arc set on tournaments (FAST) and $s,t$-distance -- remain hard here.","We obtain a generalized lower bound on space-approximation tradeoffs for FAST: any single-pass $(1\\pm \\varepsilon)$-approximation algorithm requires $\\Omega(n/\\sqrt{\\varepsilon})$ space.","As a whole, our collection of results contributes significantly to the growing literature on streaming digraphs."],"url":"http://arxiv.org/abs/2405.05952v1","category":"cs.DS"}
{"created":"2024-05-09 17:44:25","title":"$\\mathcal{H}_2$ optimal model reduction of linear systems with multiple quadratic outputs","abstract":"In this work, we consider the $\\mathcal{H}_2$ optimal model reduction of dynamical systems that are linear in the state equation and up to quadratic nonlinearity in the output equation. As our primary theoretical contributions, we derive gradients of the squared $\\mathcal{H}_2$ system error with respect to the reduced model quantities and, from the stationary points of these gradients, introduce Gramian-based first-order necessary conditions for the $\\mathcal{H}_2$ optimal approximation of a linear quadratic output (LQO) system. The resulting $\\mathcal{H}_2$ optimality framework neatly generalizes the analogous Gramian-based optimality framework for purely linear systems. Computationally, we show how to enforce the necessary optimality conditions using Petrov-Galerkin projection; the corresponding projection matrices are obtained from a pair of Sylvester equations. Based on this result, we propose an iteratively corrected algorithm for the $\\mathcal{H}_2$ model reduction of LQO systems, which we refer to as LQO-TSIA (linear quadratic output two-sided iteration algorithm). Numerical examples are included to illustrate the effectiveness of the proposed computational method against other existing approaches.","sentences":["In this work, we consider the $\\mathcal{H}_2$ optimal model reduction of dynamical systems that are linear in the state equation and up to quadratic nonlinearity in the output equation.","As our primary theoretical contributions, we derive gradients of the squared $\\mathcal{H}_2$ system error with respect to the reduced model quantities and, from the stationary points of these gradients, introduce Gramian-based first-order necessary conditions for the $\\mathcal{H}_2$ optimal approximation of a linear quadratic output (LQO) system.","The resulting $\\mathcal{H}_2$ optimality framework neatly generalizes the analogous Gramian-based optimality framework for purely linear systems.","Computationally, we show how to enforce the necessary optimality conditions using Petrov-Galerkin projection; the corresponding projection matrices are obtained from a pair of Sylvester equations.","Based on this result, we propose an iteratively corrected algorithm for the $\\mathcal{H}_2$ model reduction of LQO systems, which we refer to as LQO-TSIA (linear quadratic output two-sided iteration algorithm).","Numerical examples are included to illustrate the effectiveness of the proposed computational method against other existing approaches."],"url":"http://arxiv.org/abs/2405.05951v1","category":"math.NA"}
{"created":"2024-05-09 17:40:09","title":"Federated Combinatorial Multi-Agent Multi-Armed Bandits","abstract":"This paper introduces a federated learning framework tailored for online combinatorial optimization with bandit feedback. In this setting, agents select subsets of arms, observe noisy rewards for these subsets without accessing individual arm information, and can cooperate and share information at specific intervals. Our framework transforms any offline resilient single-agent $(\\alpha-\\epsilon)$-approximation algorithm, having a complexity of $\\tilde{\\mathcal{O}}(\\frac{\\psi}{\\epsilon^\\beta})$, where the logarithm is omitted, for some function $\\psi$ and constant $\\beta$, into an online multi-agent algorithm with $m$ communicating agents and an $\\alpha$-regret of no more than $\\tilde{\\mathcal{O}}(m^{-\\frac{1}{3+\\beta}} \\psi^\\frac{1}{3+\\beta} T^\\frac{2+\\beta}{3+\\beta})$. This approach not only eliminates the $\\epsilon$ approximation error but also ensures sublinear growth with respect to the time horizon $T$ and demonstrates a linear speedup with an increasing number of communicating agents. Additionally, the algorithm is notably communication-efficient, requiring only a sublinear number of communication rounds, quantified as $\\tilde{\\mathcal{O}}\\left(\\psi T^\\frac{\\beta}{\\beta+1}\\right)$. Furthermore, the framework has been successfully applied to online stochastic submodular maximization using various offline algorithms, yielding the first results for both single-agent and multi-agent settings and recovering specialized single-agent theoretical guarantees. We empirically validate our approach to a stochastic data summarization problem, illustrating the effectiveness of the proposed framework, even in single-agent scenarios.","sentences":["This paper introduces a federated learning framework tailored for online combinatorial optimization with bandit feedback.","In this setting, agents select subsets of arms, observe noisy rewards for these subsets without accessing individual arm information, and can cooperate and share information at specific intervals.","Our framework transforms any offline resilient single-agent $(\\alpha-\\epsilon)$-approximation algorithm, having a complexity of $\\tilde{\\mathcal{O}}(\\frac{\\psi}{\\epsilon^\\beta})$, where the logarithm is omitted, for some function $\\psi$ and constant $\\beta$, into an online multi-agent algorithm with $m$ communicating agents and an $\\alpha$-regret of no more than $\\tilde{\\mathcal{O}}(m^{-\\frac{1}{3+\\beta}} \\psi^\\frac{1}{3+\\beta} T^\\frac{2+\\beta}{3+\\beta})$. This approach not only eliminates the $\\epsilon$ approximation error but also ensures sublinear growth with respect to the time horizon $T$ and demonstrates a linear speedup with an increasing number of communicating agents.","Additionally, the algorithm is notably communication-efficient, requiring only a sublinear number of communication rounds, quantified as $\\tilde{\\mathcal{O}}\\left(\\psi T^\\frac{\\beta}{\\beta+1}\\right)$.","Furthermore, the framework has been successfully applied to online stochastic submodular maximization using various offline algorithms, yielding the first results for both single-agent and multi-agent settings and recovering specialized single-agent theoretical guarantees.","We empirically validate our approach to a stochastic data summarization problem, illustrating the effectiveness of the proposed framework, even in single-agent scenarios."],"url":"http://arxiv.org/abs/2405.05950v1","category":"cs.LG"}
{"created":"2024-05-09 17:37:02","title":"Uniformly global observables for 1D maps with an indifferent fixed point","abstract":"We study the property of global-local mixing for full-branched expanding maps of either the half-line or the interval, with one indifferent fixed point. Global-local mixing expresses the decorrelation of global vs local observables w.r.t. to an infinite measure $\\mu$. Global observables are essentially bounded functions admitting an infinite-volume average, i.e., a limit for the average of the function over bigger and bigger intervals; local observables are integrable functions (both notions are relative to $\\mu$). Of course, the definition of global observable depends on the exact definition of infinite-volume average. The first choice for it would be to consider averages over the entire space minus a neighborhood of the indifferent fixed point (a.k.a. the \"point at infinity\"), in the limit where such neighborhood vanishes. This is the choice that was made in previous papers on the subject. The classes of systems for which global-local mixing was proved, with this natural choice of global observables, are ample but not really general. In this paper we consider uniformly global observables, i.e., $L^\\infty$ functions whose averages over any interval $V$ converges to a limit, uniformly as $\\mu(V) \\to \\infty$. Uniformly global observables form quite an extensive subclass of all global observables. We prove global-local mixing in the sense of uniformly global observables, for two truly general classes of expanding maps with one indifferent fixed point, respectively on $\\mathbb{R}_0^+$ and on $(0,1]$. The technical core of the proofs is rather different from previous work.","sentences":["We study the property of global-local mixing for full-branched expanding maps of either the half-line or the interval, with one indifferent fixed point.","Global-local mixing expresses the decorrelation of global vs local observables w.r.t.","to an infinite measure $\\mu$. Global observables are essentially bounded functions admitting an infinite-volume average, i.e., a limit for the average of the function over bigger and bigger intervals; local observables are integrable functions (both notions are relative to $\\mu$).","Of course, the definition of global observable depends on the exact definition of infinite-volume average.","The first choice for it would be to consider averages over the entire space minus a neighborhood of the indifferent fixed point (a.k.a. the \"point at infinity\"), in the limit where such neighborhood vanishes.","This is the choice that was made in previous papers on the subject.","The classes of systems for which global-local mixing was proved, with this natural choice of global observables, are ample but not really general.","In this paper we consider uniformly global observables, i.e., $L^\\infty$ functions whose averages over any interval $V$ converges to a limit, uniformly as $\\mu(V) \\to \\infty$. Uniformly global observables form quite an extensive subclass of all global observables.","We prove global-local mixing in the sense of uniformly global observables, for two truly general classes of expanding maps with one indifferent fixed point, respectively on $\\mathbb{R}_0^+$ and on $(0,1]$. The technical core of the proofs is rather different from previous work."],"url":"http://arxiv.org/abs/2405.05948v1","category":"math.DS"}
{"created":"2024-05-09 17:35:49","title":"Motion from Measurement: The Role of Symmetry of Quantum Measurements","abstract":"In quantum mechanics, measurements are dynamical processes and thus they should be capable of inducing currents. The symmetries of the Hamiltonian and measurement operator provide an organizing principle for understanding the conditions for such currents to emerge. The central role is played by the inversion and time-reversal symmetries. We classify the distinct behaviors that emerge from single and repeated measurements, with and without coupling to a dissipative bath. While the breaking of inversion symmetry alone is sufficient to generate currents through measurements, the breaking of time-reversal symmetry by the measurement operator leads to a dramatic increase in the magnitude of the currents. We consider the dependence on the measurement rate and find that the current is non-monotonic. Furthermore, nondegenerate measurements can lead to current loops within the steady state even in the Zeno limit.","sentences":["In quantum mechanics, measurements are dynamical processes and thus they should be capable of inducing currents.","The symmetries of the Hamiltonian and measurement operator provide an organizing principle for understanding the conditions for such currents to emerge.","The central role is played by the inversion and time-reversal symmetries.","We classify the distinct behaviors that emerge from single and repeated measurements, with and without coupling to a dissipative bath.","While the breaking of inversion symmetry alone is sufficient to generate currents through measurements, the breaking of time-reversal symmetry by the measurement operator leads to a dramatic increase in the magnitude of the currents.","We consider the dependence on the measurement rate and find that the current is non-monotonic.","Furthermore, nondegenerate measurements can lead to current loops within the steady state even in the Zeno limit."],"url":"http://arxiv.org/abs/2405.05946v1","category":"quant-ph"}
{"created":"2024-05-09 17:35:16","title":"Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers","abstract":"Sora unveils the potential of scaling Diffusion Transformer for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details. In this technical report, we introduce the Lumina-T2X family - a series of Flow-based Large Diffusion Transformers (Flag-DiT) equipped with zero-initialized attention, as a unified framework designed to transform noise into images, videos, multi-view 3D objects, and audio clips conditioned on text instructions. By tokenizing the latent spatial-temporal space and incorporating learnable placeholders such as [nextline] and [nextframe] tokens, Lumina-T2X seamlessly unifies the representations of different modalities across various spatial-temporal resolutions. This unified approach enables training within a single framework for different modalities and allows for flexible generation of multimodal data at any resolution, aspect ratio, and length during inference. Advanced techniques like RoPE, RMSNorm, and flow matching enhance the stability, flexibility, and scalability of Flag-DiT, enabling models of Lumina-T2X to scale up to 7 billion parameters and extend the context window to 128K tokens. This is particularly beneficial for creating ultra-high-definition images with our Lumina-T2I model and long 720p videos with our Lumina-T2V model. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT, requires only 35% of the training computational costs of a 600-million-parameter naive DiT. Our further comprehensive analysis underscores Lumina-T2X's preliminary capability in resolution extrapolation, high-resolution editing, generating consistent 3D views, and synthesizing videos with seamless transitions. We expect that the open-sourcing of Lumina-T2X will further foster creativity, transparency, and diversity in the generative AI community.","sentences":["Sora unveils the potential of scaling Diffusion Transformer for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details.","In this technical report, we introduce the Lumina-T2X family - a series of Flow-based Large Diffusion Transformers (Flag-DiT) equipped with zero-initialized attention, as a unified framework designed to transform noise into images, videos, multi-view 3D objects, and audio clips conditioned on text instructions.","By tokenizing the latent spatial-temporal space and incorporating learnable placeholders such as [nextline] and [nextframe] tokens, Lumina-T2X seamlessly unifies the representations of different modalities across various spatial-temporal resolutions.","This unified approach enables training within a single framework for different modalities and allows for flexible generation of multimodal data at any resolution, aspect ratio, and length during inference.","Advanced techniques like RoPE, RMSNorm, and flow matching enhance the stability, flexibility, and scalability of Flag-DiT, enabling models of Lumina-T2X to scale up to 7 billion parameters and extend the context window to 128K tokens.","This is particularly beneficial for creating ultra-high-definition images with our Lumina-T2I model and long 720p videos with our Lumina-T2V model.","Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT, requires only 35% of the training computational costs of a 600-million-parameter naive DiT. Our further comprehensive analysis underscores Lumina-T2X's preliminary capability in resolution extrapolation, high-resolution editing, generating consistent 3D views, and synthesizing videos with seamless transitions.","We expect that the open-sourcing of Lumina-T2X will further foster creativity, transparency, and diversity in the generative AI community."],"url":"http://arxiv.org/abs/2405.05945v1","category":"cs.CV"}
{"created":"2024-05-09 17:32:50","title":"Quantitative fluid approximation in fractional regimes of transport equations with more invariants","abstract":"We present an extension of results in a previous paper by the first and the last author [PMP, 2022] about macroscopic limits of linear kinetic equations in (potentially) fractional regimes. More precisely, we develop a unified framework inspired by Ellis and Pinsky [J. Math. Pures Appl., 1975] for operators that preserve mass, momentum and energy, and have microscopic equilibrium with heavy tails (typically polynomial). This paper also generalizes one of Hittmeir and Merino [KRM, 2016] in a related framework. The main difficulty, that leads to our main contribution, is the understanding of the spectrum of the generator in the Fourier space, which is significantly complicated by the lack of spectral gap and the fat tails of the equilibrium. Indeed, the scaling of the eigenelements in the suitable macroscopic rescaling is subtle to handle. In particular, our study uncovered an interesting difference in scaling in the fractional regime, where the transversal wave eigenvalues converge faster to zero than the Boussinesq and acoustic wave eigenvalues.","sentences":["We present an extension of results in a previous paper by the first and the last author","[PMP, 2022] about macroscopic limits of linear kinetic equations in (potentially) fractional regimes.","More precisely, we develop a unified framework inspired by Ellis and Pinsky","[J. Math.","Pures Appl., 1975] for operators that preserve mass, momentum and energy, and have microscopic equilibrium with heavy tails (typically polynomial).","This paper also generalizes one of Hittmeir and Merino [KRM, 2016] in a related framework.","The main difficulty, that leads to our main contribution, is the understanding of the spectrum of the generator in the Fourier space, which is significantly complicated by the lack of spectral gap and the fat tails of the equilibrium.","Indeed, the scaling of the eigenelements in the suitable macroscopic rescaling is subtle to handle.","In particular, our study uncovered an interesting difference in scaling in the fractional regime, where the transversal wave eigenvalues converge faster to zero than the Boussinesq and acoustic wave eigenvalues."],"url":"http://arxiv.org/abs/2405.05943v1","category":"math.AP"}
{"created":"2024-05-09 17:30:16","title":"Evaluating Real-World Robot Manipulation Policies in Simulation","abstract":"The field of robotics has made significant advances towards generalist robot manipulation policies. However, real-world evaluation of such policies is not scalable and faces reproducibility challenges, which are likely to worsen as policies broaden the spectrum of tasks they can perform. We identify control and visual disparities between real and simulated environments as key challenges for reliable simulated evaluation and propose approaches for mitigating these gaps without needing to craft full-fidelity digital twins of real-world environments. We then employ these approaches to create SIMPLER, a collection of simulated environments for manipulation policy evaluation on common real robot setups. Through paired sim-and-real evaluations of manipulation policies, we demonstrate strong correlation between policy performance in SIMPLER environments and in the real world. Additionally, we find that SIMPLER evaluations accurately reflect real-world policy behavior modes such as sensitivity to various distribution shifts. We open-source all SIMPLER environments along with our workflow for creating new environments at https://simpler-env.github.io to facilitate research on general-purpose manipulation policies and simulated evaluation frameworks.","sentences":["The field of robotics has made significant advances towards generalist robot manipulation policies.","However, real-world evaluation of such policies is not scalable and faces reproducibility challenges, which are likely to worsen as policies broaden the spectrum of tasks they can perform.","We identify control and visual disparities between real and simulated environments as key challenges for reliable simulated evaluation and propose approaches for mitigating these gaps without needing to craft full-fidelity digital twins of real-world environments.","We then employ these approaches to create SIMPLER, a collection of simulated environments for manipulation policy evaluation on common real robot setups.","Through paired sim-and-real evaluations of manipulation policies, we demonstrate strong correlation between policy performance in SIMPLER environments and in the real world.","Additionally, we find that SIMPLER evaluations accurately reflect real-world policy behavior modes such as sensitivity to various distribution shifts.","We open-source all SIMPLER environments along with our workflow for creating new environments at https://simpler-env.github.io to facilitate research on general-purpose manipulation policies and simulated evaluation frameworks."],"url":"http://arxiv.org/abs/2405.05941v1","category":"cs.RO"}
{"created":"2024-05-09 17:29:50","title":"Generalized Campanato Space Over Non-homogeneous Space and Its Applications","abstract":"The authors introduce generalized Campanato space with regularized condition over non-homogeneous space, and study its basic properties including the John-Nirenberg inequality and equivalent characterizations. As applications, the boundedness of fractional type Marcinkiewicz integral operator and its commutator on generalized Morrey space over non-homogeneous space is obtained.","sentences":["The authors introduce generalized Campanato space with regularized condition over non-homogeneous space, and study its basic properties including the John-Nirenberg inequality and equivalent characterizations.","As applications, the boundedness of fractional type Marcinkiewicz integral operator and its commutator on generalized Morrey space over non-homogeneous space is obtained."],"url":"http://arxiv.org/abs/2405.05940v1","category":"math.FA"}
{"created":"2024-05-09 17:26:41","title":"Bounded Generation of Submonoids of Heisenberg Groups","abstract":"If $G$ is a nilpotent group and $[G,G]$ has Hirsch length $1$, then every f.g. submonoid of $G$ is boundedly generated, i.e. a product of cyclic submonoids. Using a reduction of Bodart, this implies the decidability of the submonoid membership problem for nilpotent groups $G$ where $[G,G]$ has Hirsch length $2$.","sentences":["If $G$ is a nilpotent group and $[G,G]$ has Hirsch length $1$, then every f.g. submonoid of $G$ is boundedly generated, i.e. a product of cyclic submonoids.","Using a reduction of Bodart, this implies the decidability of the submonoid membership problem for nilpotent groups $G$ where $[G,G]$","has Hirsch length $2$."],"url":"http://arxiv.org/abs/2405.05939v1","category":"math.GR"}
{"created":"2024-05-09 17:25:31","title":"DOLOMITES: Domain-Specific Long-Form Methodical Tasks","abstract":"Experts in various fields routinely perform methodical writing tasks to plan, organize, and report their work. From a clinician writing a differential diagnosis for a patient, to a teacher writing a lesson plan for students, these tasks are pervasive, requiring to methodically generate structured long-form output for a given input. We develop a typology of methodical tasks structured in the form of a task objective, procedure, input, and output, and introduce DoLoMiTes, a novel benchmark with specifications for 519 such tasks elicited from hundreds of experts from across 25 fields. Our benchmark further contains specific instantiations of methodical tasks with concrete input and output examples (1,857 in total) which we obtain by collecting expert revisions of up to 10 model-generated examples of each task. We use these examples to evaluate contemporary language models highlighting that automating methodical tasks is a challenging long-form generation problem, as it requires performing complex inferences, while drawing upon the given context as well as domain knowledge.","sentences":["Experts in various fields routinely perform methodical writing tasks to plan, organize, and report their work.","From a clinician writing a differential diagnosis for a patient, to a teacher writing a lesson plan for students, these tasks are pervasive, requiring to methodically generate structured long-form output for a given input.","We develop a typology of methodical tasks structured in the form of a task objective, procedure, input, and output, and introduce DoLoMiTes, a novel benchmark with specifications for 519 such tasks elicited from hundreds of experts from across 25 fields.","Our benchmark further contains specific instantiations of methodical tasks with concrete input and output examples (1,857 in total) which we obtain by collecting expert revisions of up to 10 model-generated examples of each task.","We use these examples to evaluate contemporary language models highlighting that automating methodical tasks is a challenging long-form generation problem, as it requires performing complex inferences, while drawing upon the given context as well as domain knowledge."],"url":"http://arxiv.org/abs/2405.05938v1","category":"cs.CL"}
{"created":"2024-05-09 17:21:17","title":"Scalar Perturbations in Nonsingular Universes from Interacting Vacuum","abstract":"In this paper we examine the stability of scalar perturbations in nonsingular models which emerge from an interacting vacuum component. The analysis developed in this paper relies on two phenomenological choices for the energy exchange between a nonrelativistic fluid and a vacuum component. In both scenarios it can be shown that closed models may furnish nonsingular orbits of physical interest in phase space once a decelerated past era is connected to a graceful exit to late-time acceleration. Regarding such configurations as background spacetimes we introduce scalar perturbations in order to examine the stability of these models in a high energy domain. We explicitly show that the vacuum perturbation is not an independent variable and diverges as dynamics approaches the bounce. This feature assigns a rather unstable signature to the dynamics making the choices for the energy transfer ill defined at least for nonsingular configurations at the bounce scale.","sentences":["In this paper we examine the stability of scalar perturbations in nonsingular models which emerge from an interacting vacuum component.","The analysis developed in this paper relies on two phenomenological choices for the energy exchange between a nonrelativistic fluid and a vacuum component.","In both scenarios it can be shown that closed models may furnish nonsingular orbits of physical interest in phase space once a decelerated past era is connected to a graceful exit to late-time acceleration.","Regarding such configurations as background spacetimes we introduce scalar perturbations in order to examine the stability of these models in a high energy domain.","We explicitly show that the vacuum perturbation is not an independent variable and diverges as dynamics approaches the bounce.","This feature assigns a rather unstable signature to the dynamics making the choices for the energy transfer ill defined at least for nonsingular configurations at the bounce scale."],"url":"http://arxiv.org/abs/2405.05936v1","category":"gr-qc"}
{"created":"2024-05-09 17:16:36","title":"Non-symplectic automorphisms of prime order of O'Grady's tenfolds and cubic fourfolds","abstract":"We give a lattice-theoretic classification of non-symplectic automorphisms of prime order of irreducible holomorphic symplectic manifolds of OG10 type. We determine which automorphisms are induced by a non-symplectic automorphism of prime order of a cubic fourfold on the associated LSV manifolds, giving a geometric and lattice-theoretic description of the algebraic and transcendental lattices of the cubic fourfold. As an application we discuss the rationality conjecture for a general cubic fourfold with a non-symplectic automorphism of prime order.","sentences":["We give a lattice-theoretic classification of non-symplectic automorphisms of prime order of irreducible holomorphic symplectic manifolds of OG10 type.","We determine which automorphisms are induced by a non-symplectic automorphism of prime order of a cubic fourfold on the associated LSV manifolds, giving a geometric and lattice-theoretic description of the algebraic and transcendental lattices of the cubic fourfold.","As an application we discuss the rationality conjecture for a general cubic fourfold with a non-symplectic automorphism of prime order."],"url":"http://arxiv.org/abs/2405.05932v1","category":"math.AG"}
{"created":"2024-05-09 17:16:20","title":"Trustworthy AI-Generative Content in Intelligent 6G Network: Adversarial, Privacy, and Fairness","abstract":"AI-generated content (AIGC) models, represented by large language models (LLM), have brought revolutionary changes to the content generation fields. The high-speed and extensive 6G technology is an ideal platform for providing powerful AIGC mobile service applications, while future 6G mobile networks also need to support intelligent and personalized mobile generation services. However, the significant ethical and security issues of current AIGC models, such as adversarial attacks, privacy, and fairness, greatly affect the credibility of 6G intelligent networks, especially in ensuring secure, private, and fair AIGC applications. In this paper, we propose TrustGAIN, a novel paradigm for trustworthy AIGC in 6G networks, to ensure trustworthy large-scale AIGC services in future 6G networks. We first discuss the adversarial attacks and privacy threats faced by AIGC systems in 6G networks, as well as the corresponding protection issues. Subsequently, we emphasize the importance of ensuring the unbiasedness and fairness of the mobile generative service in future intelligent networks. In particular, we conduct a use case to demonstrate that TrustGAIN can effectively guide the resistance against malicious or generated false information. We believe that TrustGAIN is a necessary paradigm for intelligent and trustworthy 6G networks to support AIGC services, ensuring the security, privacy, and fairness of AIGC network services.","sentences":["AI-generated content (AIGC) models, represented by large language models (LLM), have brought revolutionary changes to the content generation fields.","The high-speed and extensive 6G technology is an ideal platform for providing powerful AIGC mobile service applications, while future 6G mobile networks also need to support intelligent and personalized mobile generation services.","However, the significant ethical and security issues of current AIGC models, such as adversarial attacks, privacy, and fairness, greatly affect the credibility of 6G intelligent networks, especially in ensuring secure, private, and fair AIGC applications.","In this paper, we propose TrustGAIN, a novel paradigm for trustworthy AIGC in 6G networks, to ensure trustworthy large-scale AIGC services in future 6G networks.","We first discuss the adversarial attacks and privacy threats faced by AIGC systems in 6G networks, as well as the corresponding protection issues.","Subsequently, we emphasize the importance of ensuring the unbiasedness and fairness of the mobile generative service in future intelligent networks.","In particular, we conduct a use case to demonstrate that TrustGAIN can effectively guide the resistance against malicious or generated false information.","We believe that TrustGAIN is a necessary paradigm for intelligent and trustworthy 6G networks to support AIGC services, ensuring the security, privacy, and fairness of AIGC network services."],"url":"http://arxiv.org/abs/2405.05930v1","category":"cs.CR"}
{"created":"2024-05-09 17:15:24","title":"On the Effect of Geometry on Scaling Laws for a Class of Martensitic Phase Transformations","abstract":"We study scaling laws for singular perturbation problems associated with a class of two-dimensional martensitic phase transformations and deduce a domain dependence of the scaling law in the singular perturbation parameter. In these settings the respective scaling laws give rise to a selection principle for specific, highly symmetric domain geometries for the associated nucleation microstructure. More precisely, firstly, we prove a general lower bound estimate illustrating that in settings in which the domain and well geometry are incompatible in the sense of the Hadamard-jump condition, then necessarily at least logarithmic losses in the singular perturbation parameter occur in the associated scaling laws. Secondly, for specific phase transformations in two-dimensional settings we prove that this gives rise to a dichotomy involving logarithmic losses in the scaling law for generic domains and optimal linear scaling laws for very specific, highly compatible polygonal domains. In these situations the scaling law thus gives important insight into optimal isoperimetric domains. We discuss both the geometrically linearized and nonlinear settings.","sentences":["We study scaling laws for singular perturbation problems associated with a class of two-dimensional martensitic phase transformations and deduce a domain dependence of the scaling law in the singular perturbation parameter.","In these settings the respective scaling laws give rise to a selection principle for specific, highly symmetric domain geometries for the associated nucleation microstructure.","More precisely, firstly, we prove a general lower bound estimate illustrating that in settings in which the domain and well geometry are incompatible in the sense of the Hadamard-jump condition, then necessarily at least logarithmic losses in the singular perturbation parameter occur in the associated scaling laws.","Secondly, for specific phase transformations in two-dimensional settings we prove that this gives rise to a dichotomy involving logarithmic losses in the scaling law for generic domains and optimal linear scaling laws for very specific, highly compatible polygonal domains.","In these situations the scaling law thus gives important insight into optimal isoperimetric domains.","We discuss both the geometrically linearized and nonlinear settings."],"url":"http://arxiv.org/abs/2405.05927v1","category":"math.AP"}
{"created":"2024-05-09 17:15:09","title":"FuXi-ENS: A machine learning model for medium-range ensemble weather forecasting","abstract":"Ensemble weather forecasting is essential for weather predictions and mitigating the impacts of extreme weather events. Constructing an ensemble prediction system (EPS) based on conventional numerical weather prediction (NWP) models is highly computationally expensive. Machine learning (ML) models have emerged as valuable tools for deterministic weather forecasts, providing forecasts with significantly reduced computational requirements and even surpassing the forecast performance of traditional NWP models. However, challenges arise when applying ML models to ensemble forecasting. Recent ML models, such as GenCast and SEEDS model, rely on the ERA5 Ensemble of Data Assimilations (EDA) or two operational NWP ensemble members for forecast generation. The spatial resolution of 1{\\deg} or 2{\\deg} in these models is often considered too coarse for many applications. To overcome these limitations, we introduce FuXi-ENS, an advanced ML model designed to deliver 6-hourly global ensemble weather forecasts up to 15 days. This model runs at a significantly improved spatial resolution of 0.25{\\deg}, incorporating 5 upper-air atmospheric variables at 13 pressure levels, along with 13 surface variables. By leveraging the inherent probabilistic nature of Variational AutoEncoder (VAE), FuXi-ENS optimizes a loss function that combines the continuous ranked probability score (CRPS) and the KL divergence between the predicted and target distribution. This innovative approach represents an advancement over the traditional use of L1 loss combined with the KL loss in standard VAE models when VAE for ensemble weather forecasts. Evaluation results demonstrate that FuXi-ENS outperforms ensemble forecasts from the European Centre for Medium-Range Weather Forecasts (ECMWF), a world leading NWP model, on 98.1% of 360 variable and forecast lead time combinations on CRPS.","sentences":["Ensemble weather forecasting is essential for weather predictions and mitigating the impacts of extreme weather events.","Constructing an ensemble prediction system (EPS) based on conventional numerical weather prediction (NWP) models is highly computationally expensive.","Machine learning (ML) models have emerged as valuable tools for deterministic weather forecasts, providing forecasts with significantly reduced computational requirements and even surpassing the forecast performance of traditional NWP models.","However, challenges arise when applying ML models to ensemble forecasting.","Recent ML models, such as GenCast and SEEDS model, rely on the ERA5 Ensemble of Data Assimilations (EDA) or two operational NWP ensemble members for forecast generation.","The spatial resolution of 1{\\deg} or 2{\\deg} in these models is often considered too coarse for many applications.","To overcome these limitations, we introduce FuXi-ENS, an advanced ML model designed to deliver 6-hourly global ensemble weather forecasts up to 15 days.","This model runs at a significantly improved spatial resolution of 0.25{\\deg}, incorporating 5 upper-air atmospheric variables at 13 pressure levels, along with 13 surface variables.","By leveraging the inherent probabilistic nature of Variational AutoEncoder (VAE), FuXi-ENS optimizes a loss function that combines the continuous ranked probability score (CRPS) and the KL divergence between the predicted and target distribution.","This innovative approach represents an advancement over the traditional use of L1 loss combined with the KL loss in standard VAE models when VAE for ensemble weather forecasts.","Evaluation results demonstrate that FuXi-ENS outperforms ensemble forecasts from the European Centre for Medium-Range Weather Forecasts (ECMWF), a world leading NWP model, on 98.1% of 360 variable and forecast lead time combinations on CRPS."],"url":"http://arxiv.org/abs/2405.05925v1","category":"cs.LG"}
{"created":"2024-05-09 17:11:00","title":"Generalized R\u00e9nyi entropy accumulation theorem and generalized quantum probability estimation","abstract":"The entropy accumulation theorem, and its subsequent generalized version, is a powerful tool in the security analysis of many device-dependent and device-independent cryptography protocols. However, it has the drawback that the finite-size bounds it yields are not necessarily optimal, and furthermore it relies on the construction of an affine min-tradeoff function, which can often be challenging to construct optimally in practice. In this work, we address both of these challenges simultaneously by deriving a new entropy accumulation bound. Our bound yields significantly better finite-size performance, and can be computed as an intuitively interpretable convex optimization, without any specification of affine min-tradeoff functions. Furthermore, it can be applied directly at the level of R\\'enyi entropies if desired, yielding fully-R\\'enyi security proofs. Our proof techniques are based on elaborating on a connection between entropy accumulation and the frameworks of quantum probability estimation or $f$-weighted R\\'enyi entropies, and in the process we obtain some new results with respect to those frameworks as well.","sentences":["The entropy accumulation theorem, and its subsequent generalized version, is a powerful tool in the security analysis of many device-dependent and device-independent cryptography protocols.","However, it has the drawback that the finite-size bounds it yields are not necessarily optimal, and furthermore it relies on the construction of an affine min-tradeoff function, which can often be challenging to construct optimally in practice.","In this work, we address both of these challenges simultaneously by deriving a new entropy accumulation bound.","Our bound yields significantly better finite-size performance, and can be computed as an intuitively interpretable convex optimization, without any specification of affine min-tradeoff functions.","Furthermore, it can be applied directly at the level of R\\'enyi entropies if desired, yielding fully-R\\'enyi security proofs.","Our proof techniques are based on elaborating on a connection between entropy accumulation and the frameworks of quantum probability estimation or $f$-weighted R\\'enyi entropies, and in the process we obtain some new results with respect to those frameworks as well."],"url":"http://arxiv.org/abs/2405.05912v1","category":"quant-ph"}
{"created":"2024-05-09 17:10:56","title":"Small-Scale Testbed for Evaluating C-V2X Applications on 5G Cellular Networks","abstract":"In this work, we present a small-scale testbed for evaluating the real-life performance of cellular V2X (C-V2X) applications on 5G cellular networks. Despite the growing interest and rapid technology development for V2X applications, researchers still struggle to prototype V2X applications with real wireless networks, hardware, and software in the loop in a controlled environment. To help alleviate this challenge, we present a testbed designed to accelerate development and evaluation of C-V2X applications on 5G cellular networks. By including a small-scale vehicle platform into the testbed design, we significantly reduce the time and effort required to test new C-V2X applications on 5G cellular networks. With a focus around the integration of small-scale vehicle platforms, we detail the design decisions behind the full software and hardware setup of commonly needed intelligent transport system agents (e.g. sensors, servers, vehicles). Moreover, to showcase the testbed's capability to produce industrially-relevant, real world performance evaluations, we present an evaluation of a simple test case inspired from shared situational awareness. Finally, we discuss the upcoming use of the testbed for evaluating 5G cellular network-based shared situational awareness and other C-V2X applications.","sentences":["In this work, we present a small-scale testbed for evaluating the real-life performance of cellular V2X (C-V2X) applications on 5G cellular networks.","Despite the growing interest and rapid technology development for V2X applications, researchers still struggle to prototype V2X applications with real wireless networks, hardware, and software in the loop in a controlled environment.","To help alleviate this challenge, we present a testbed designed to accelerate development and evaluation of C-V2X applications on 5G cellular networks.","By including a small-scale vehicle platform into the testbed design, we significantly reduce the time and effort required to test new C-V2X applications on 5G cellular networks.","With a focus around the integration of small-scale vehicle platforms, we detail the design decisions behind the full software and hardware setup of commonly needed intelligent transport system agents (e.g. sensors, servers, vehicles).","Moreover, to showcase the testbed's capability to produce industrially-relevant, real world performance evaluations, we present an evaluation of a simple test case inspired from shared situational awareness.","Finally, we discuss the upcoming use of the testbed for evaluating 5G cellular network-based shared situational awareness and other C-V2X applications."],"url":"http://arxiv.org/abs/2405.05911v1","category":"eess.SY"}
{"created":"2024-05-09 17:06:51","title":"Diag2Diag: Multi modal super resolution for physics discovery with application to fusion","abstract":"This paper introduces a groundbreaking multi-modal neural network model designed for resolution enhancement, which innovatively leverages inter-diagnostic correlations within a system. Traditional approaches have primarily focused on uni-modal enhancement strategies, such as pixel-based image enhancement or heuristic signal interpolation. In contrast, our model employs a novel methodology by harnessing the diagnostic relationships within the physics of fusion plasma. Initially, we establish the correlation among diagnostics within the tokamak. Subsequently, we utilize these correlations to substantially enhance the temporal resolution of the Thomson Scattering diagnostic, which assesses plasma density and temperature. By increasing its resolution from conventional 200Hz to 500kHz, we facilitate a new level of insight into plasma behavior, previously attainable only through computationally intensive simulations. This enhancement goes beyond simple interpolation, offering novel perspectives on the underlying physical phenomena governing plasma dynamics.","sentences":["This paper introduces a groundbreaking multi-modal neural network model designed for resolution enhancement, which innovatively leverages inter-diagnostic correlations within a system.","Traditional approaches have primarily focused on uni-modal enhancement strategies, such as pixel-based image enhancement or heuristic signal interpolation.","In contrast, our model employs a novel methodology by harnessing the diagnostic relationships within the physics of fusion plasma.","Initially, we establish the correlation among diagnostics within the tokamak.","Subsequently, we utilize these correlations to substantially enhance the temporal resolution of the Thomson Scattering diagnostic, which assesses plasma density and temperature.","By increasing its resolution from conventional 200Hz to 500kHz, we facilitate a new level of insight into plasma behavior, previously attainable only through computationally intensive simulations.","This enhancement goes beyond simple interpolation, offering novel perspectives on the underlying physical phenomena governing plasma dynamics."],"url":"http://arxiv.org/abs/2405.05908v1","category":"physics.plasm-ph"}
{"created":"2024-05-09 17:03:04","title":"On the Ground State Energies of Discrete and Semiclassical Schr\u00f6dinger Operators","abstract":"We study the infimum of the spectrum, or ground state energy, of a discrete Schr\\\"odinger operator on $\\theta\\mathbb{Z}^d$ parameterized by a potential $V:\\mathbb{R}^d\\rightarrow\\mathbb{R}_{\\ge 0}$ and a frequency parameter $\\theta\\in (0,1)$. We prove a general comparison result relating this ground state energy to that of a corresponding semiclassical Schr\\\"odinger operator on $\\mathbb{R}^d$ with parameter $\\theta$, arising from the same choice of potential. We show that for smooth periodic potentials these ground state energies are multiplicatively related by a factor approaching $1$ as $\\theta\\rightarrow 0$, and that they are multiplicatively related by a dimension-dependent constant for a more general class of bounded potentials and all irrational $\\theta$.","sentences":["We study the infimum of the spectrum, or ground state energy, of a discrete Schr\\\"odinger operator on $\\theta\\mathbb{Z}^d$ parameterized by a potential $V:\\mathbb{R}^d\\rightarrow\\mathbb{R}_{\\ge 0}$ and a frequency parameter $\\theta\\in (0,1)$. We prove a general comparison result relating this ground state energy to that of a corresponding semiclassical Schr\\\"odinger operator on $\\mathbb{R}^d$ with parameter $\\theta$, arising from the same choice of potential.","We show that for smooth periodic potentials these ground state energies are multiplicatively related by a factor approaching $1$ as $\\theta\\rightarrow 0$, and that they are multiplicatively related by a dimension-dependent constant for a more general class of bounded potentials and all irrational $\\theta$."],"url":"http://arxiv.org/abs/2405.05907v1","category":"math.SP"}
{"created":"2024-05-09 17:02:06","title":"Deep Multi-Task Learning for Malware Image Classification","abstract":"Malicious software is a pernicious global problem. A novel multi-task learning framework is proposed in this paper for malware image classification for accurate and fast malware detection. We generate bitmap (BMP) and (PNG) images from malware features, which we feed to a deep learning classifier. Our state-of-the-art multi-task learning approach has been tested on a new dataset, for which we have collected approximately 100,000 benign and malicious PE, APK, Mach-o, and ELF examples. Experiments with seven tasks tested with 4 activation functions, ReLU, LeakyReLU, PReLU, and ELU separately demonstrate that PReLU gives the highest accuracy of more than 99.87% on all tasks. Our model can effectively detect a variety of obfuscation methods like packing, encryption, and instruction overlapping, strengthing the beneficial claims of our model, in addition to achieving the state-of-art methods in terms of accuracy.","sentences":["Malicious software is a pernicious global problem.","A novel multi-task learning framework is proposed in this paper for malware image classification for accurate and fast malware detection.","We generate bitmap (BMP) and (PNG) images from malware features, which we feed to a deep learning classifier.","Our state-of-the-art multi-task learning approach has been tested on a new dataset, for which we have collected approximately 100,000 benign and malicious PE, APK, Mach-o, and ELF examples.","Experiments with seven tasks tested with 4 activation functions, ReLU, LeakyReLU, PReLU, and ELU separately demonstrate that PReLU gives the highest accuracy of more than 99.87% on all tasks.","Our model can effectively detect a variety of obfuscation methods like packing, encryption, and instruction overlapping, strengthing the beneficial claims of our model, in addition to achieving the state-of-art methods in terms of accuracy."],"url":"http://arxiv.org/abs/2405.05906v1","category":"cs.CR"}
{"created":"2024-05-09 17:01:31","title":"Truthful Aggregation of LLMs with an Application to Online Advertising","abstract":"We address the challenge of aggregating the preferences of multiple agents over LLM-generated replies to user queries, where agents might modify or exaggerate their preferences. New agents may participate for each new query, making fine-tuning LLMs on these preferences impractical. To overcome these challenges, we propose an auction mechanism that operates without fine-tuning or access to model weights. This mechanism is designed to provably converge to the ouput of the optimally fine-tuned LLM as computational resources are increased. The mechanism can also incorporate contextual information about the agents when avaiable, which significantly accelerates its convergence. A well-designed payment rule ensures that truthful reporting is the optimal strategy for all agents, while also promoting an equity property by aligning each agent's utility with her contribution to social welfare - an essential feature for the mechanism's long-term viability. While our approach can be applied whenever monetary transactions are permissible, our flagship application is in online advertising. In this context, advertisers try to steer LLM-generated responses towards their brand interests, while the platform aims to maximize advertiser value and ensure user satisfaction. Experimental results confirm that our mechanism not only converges efficiently to the optimally fine-tuned LLM but also significantly boosts advertiser value and platform revenue, all with minimal computational overhead.","sentences":["We address the challenge of aggregating the preferences of multiple agents over LLM-generated replies to user queries, where agents might modify or exaggerate their preferences.","New agents may participate for each new query, making fine-tuning LLMs on these preferences impractical.","To overcome these challenges, we propose an auction mechanism that operates without fine-tuning or access to model weights.","This mechanism is designed to provably converge to the ouput of the optimally fine-tuned LLM as computational resources are increased.","The mechanism can also incorporate contextual information about the agents when avaiable, which significantly accelerates its convergence.","A well-designed payment rule ensures that truthful reporting is the optimal strategy for all agents, while also promoting an equity property by aligning each agent's utility with her contribution to social welfare - an essential feature for the mechanism's long-term viability.","While our approach can be applied whenever monetary transactions are permissible, our flagship application is in online advertising.","In this context, advertisers try to steer LLM-generated responses towards their brand interests, while the platform aims to maximize advertiser value and ensure user satisfaction.","Experimental results confirm that our mechanism not only converges efficiently to the optimally fine-tuned LLM but also significantly boosts advertiser value and platform revenue, all with minimal computational overhead."],"url":"http://arxiv.org/abs/2405.05905v1","category":"cs.GT"}
{"created":"2024-05-09 17:00:22","title":"Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?","abstract":"When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently.","sentences":["When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training.","It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge.","In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge.","To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge.","We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge.","However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate.","Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently."],"url":"http://arxiv.org/abs/2405.05904v1","category":"cs.CL"}
{"created":"2024-05-09 16:59:59","title":"The Other Side of the Coin: Recipient Norms and Their Impact on Indirect Reciprocity and Cooperation","abstract":"Human cooperation depends on indirect reciprocity. In this work, we explore the concept of indirect reciprocity using a donation game in an infinitely large population. In particular, we examine how updating the reputations of recipients influences cooperation. Our work adds a time-scale parameter for updating donor and recipient reputations. We find a trade-off between the level of cooperation and evolutionary stability influenced by social norms. `Forgiving' recipient norms enhance cooperation but increase susceptibility to defectors, whereas `unforgiving' norms reduce cooperation but defend against invasion by defectors. Expanding to include gossip groups allows us to analyze the evolutionary dynamics of the time-scale parameter, identifying `generous' norms that support cooperation, and `strict' norms that discourage such generosity, ultimately showing vulnerability to defector invasions and potential cooperation collapse.","sentences":["Human cooperation depends on indirect reciprocity.","In this work, we explore the concept of indirect reciprocity using a donation game in an infinitely large population.","In particular, we examine how updating the reputations of recipients influences cooperation.","Our work adds a time-scale parameter for updating donor and recipient reputations.","We find a trade-off between the level of cooperation and evolutionary stability influenced by social norms.","`Forgiving' recipient norms enhance cooperation but increase susceptibility to defectors, whereas `unforgiving' norms reduce cooperation but defend against invasion by defectors.","Expanding to include gossip groups allows us to analyze the evolutionary dynamics of the time-scale parameter, identifying `generous' norms that support cooperation, and `strict' norms that discourage such generosity, ultimately showing vulnerability to defector invasions and potential cooperation collapse."],"url":"http://arxiv.org/abs/2405.05903v1","category":"physics.soc-ph"}
{"created":"2024-05-09 16:54:47","title":"The largest subgraph without a forbidden induced subgraph","abstract":"We initiate the systematic study of the following Tur\\'an-type question. Suppose $\\Gamma$ is a graph with $n$ vertices such that the edge density between any pair of subsets of vertices of size at least $t$ is at most $1 - c$, for some $t$ and $c > 0$. What is the largest number of edges in a subgraph $G \\subseteq \\Gamma$ which does not contain a fixed graph $H$ as an induced subgraph or, more generally, which belongs to a hereditary property $\\mathcal{P}$? This provides a common generalization of two recently studied cases, namely $\\Gamma$ being a (pseudo-)random graph and a graph without a large complete bipartite subgraph. We focus on the interesting case where $H$ is a bipartite graph.   We determine the answer up to a constant factor with respect to $n$ and $t$, for certain bipartite $H$ and for $\\Gamma$ either a dense random graph or a Paley graph with a square number of vertices. In particular, our bounds match if $H$ is a tree, or if one part of $H$ has $d$ vertices complete to the other part, all other vertices in that part have degree at most $d$, and the other part has sufficiently many vertices. As applications of the latter result, we answer a question of Alon, Krivelevich, and Samotij on the largest subgraph with a hereditary property which misses a bipartite graph, and determine up to a constant factor the largest number of edges in a string subgraph of $\\Gamma$. The proofs are based on a variant of the dependent random choice and a novel approach for finding induced copies by inductively defining probability distributions supported on induced copies of smaller subgraphs.","sentences":["We initiate the systematic study of the following Tur\\'an-type question.","Suppose $\\Gamma$ is a graph with $n$ vertices such that the edge density between any pair of subsets of vertices of size at least $t$ is at most $1 - c$, for some $t$ and $c >","0$.","What is the largest number of edges in a subgraph $G \\subseteq \\Gamma$ which does not contain a fixed graph $H$ as an induced subgraph or, more generally, which belongs to a hereditary property $\\mathcal{P}$?","This provides a common generalization of two recently studied cases, namely $\\Gamma$ being a (pseudo-)random graph and a graph without a large complete bipartite subgraph.","We focus on the interesting case where $H$ is a bipartite graph.   ","We determine the answer up to a constant factor with respect to $n$ and $t$, for certain bipartite $H$ and for $\\Gamma$ either a dense random graph or a Paley graph with a square number of vertices.","In particular, our bounds match if $H$ is a tree, or if one part of $H$ has $d$ vertices complete to the other part, all other vertices in that part have degree at most $d$, and the other part has sufficiently many vertices.","As applications of the latter result, we answer a question of Alon, Krivelevich, and Samotij on the largest subgraph with a hereditary property which misses a bipartite graph, and determine up to a constant factor the largest number of edges in a string subgraph of $\\Gamma$.","The proofs are based on a variant of the dependent random choice and a novel approach for finding induced copies by inductively defining probability distributions supported on induced copies of smaller subgraphs."],"url":"http://arxiv.org/abs/2405.05902v1","category":"math.CO"}
{"created":"2024-05-09 16:49:09","title":"On the construction of a family of well - posed approximate formulations for the stationary Stokes problem using an extended system","abstract":"We introduce an exact parameterized extended system such that, under adequate data, between the components of its solution, there is the solution of the weak formulation of the homogeneous Dirichlet problem for the stationary Stokes equations. In the extended system, we introduce the momentum equation together with two other forms of this one. This allows us to reformulate, for the stationary case, the consistent pressure Poisson equation of Sani, Shen, Pironneau, Gresho [\\textit{Int. J. Numer. Meth. Fluids}, \\textbf{50} (2006), pp. 673-682], from the unsteady case. In this way, we can retain the information we need for the approximate pressure on the boundary. We obtain a parameterized perturbed pressure Poisson equation for the stationary Stokes problem. We prove that to solve the stationary Stokes problem is equivalent to solve a problem for the momentum equation, the parameterized equation and the equation that defines the Laplace operator acting on velocity. The approximation of this last problem give a family of well - posed approximate formulations. The solution of each element of this family approximates the solution of the stationary Stokes problem. Some necessary variants of existing results on general Banach spaces are also developed. These concern the density of subspaces related to isomorphisms and the connection between the exact and the approximate problems. First, the well - posedness of the exact extended system is proved in some dense subspaces. The well posed approximate problem does not necessitate the discrete inf-sup condition. The paper is also related to the existing discussions on the boundary conditions for the pressure and on the imposing of the incompressibility constraint on the boundary.","sentences":["We introduce an exact parameterized extended system such that, under adequate data, between the components of its solution, there is the solution of the weak formulation of the homogeneous Dirichlet problem for the stationary Stokes equations.","In the extended system, we introduce the momentum equation together with two other forms of this one.","This allows us to reformulate, for the stationary case, the consistent pressure Poisson equation of Sani, Shen, Pironneau, Gresho [\\textit{Int.","J. Numer.","Meth.","Fluids}, \\textbf{50} (2006), pp.","673-682], from the unsteady case.","In this way, we can retain the information we need for the approximate pressure on the boundary.","We obtain a parameterized perturbed pressure Poisson equation for the stationary Stokes problem.","We prove that to solve the stationary Stokes problem is equivalent to solve a problem for the momentum equation, the parameterized equation and the equation that defines the Laplace operator acting on velocity.","The approximation of this last problem give a family of well - posed approximate formulations.","The solution of each element of this family approximates the solution of the stationary Stokes problem.","Some necessary variants of existing results on general Banach spaces are also developed.","These concern the density of subspaces related to isomorphisms and the connection between the exact and the approximate problems.","First, the well - posedness of the exact extended system is proved in some dense subspaces.","The well posed approximate problem does not necessitate the discrete inf-sup condition.","The paper is also related to the existing discussions on the boundary conditions for the pressure and on the imposing of the incompressibility constraint on the boundary."],"url":"http://arxiv.org/abs/2405.05898v1","category":"math.NA"}
{"created":"2024-05-09 16:45:27","title":"Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons","abstract":"LLM-as-a-judge approaches are a practical and effective way of assessing a range of text tasks, aligning with human judgements especially when applied in a comparative assessment fashion. However, when using pairwise comparisons to rank a set of candidates the computational costs scale quadratically with the number of candidates, which can have practical limitations. This paper introduces a Product of Expert (PoE) framework for efficient LLM Comparative Assessment. Here individual comparisons are considered experts that provide information on a pair's score difference. The PoE framework combines the information from these experts to yield an expression that can be maximized with respect to the underlying set of candidates, and is highly flexible where any form of expert can be assumed. When Gaussian experts are used one can derive simple closed-form solutions for the optimal candidate ranking, as well as expressions for selecting which comparisons should be made to maximize the probability of this ranking. Our approach enables efficient comparative assessment, where by using only a small subset of the possible comparisons, one can generate score predictions that correlate as well to human judgements as the predictions when all comparisons are used. We evaluate the approach on multiple NLG tasks and demonstrate that our framework can yield considerable computational savings when performing pairwise comparative assessment. When N is large, with as few as 2% of comparisons the PoE solution can achieve similar performance to when all comparisons are used.","sentences":["LLM-as-a-judge approaches are a practical and effective way of assessing a range of text tasks, aligning with human judgements especially when applied in a comparative assessment fashion.","However, when using pairwise comparisons to rank a set of candidates the computational costs scale quadratically with the number of candidates, which can have practical limitations.","This paper introduces a Product of Expert (PoE) framework for efficient LLM Comparative Assessment.","Here individual comparisons are considered experts that provide information on a pair's score difference.","The PoE framework combines the information from these experts to yield an expression that can be maximized with respect to the underlying set of candidates, and is highly flexible where any form of expert can be assumed.","When Gaussian experts are used one can derive simple closed-form solutions for the optimal candidate ranking, as well as expressions for selecting which comparisons should be made to maximize the probability of this ranking.","Our approach enables efficient comparative assessment, where by using only a small subset of the possible comparisons, one can generate score predictions that correlate as well to human judgements as the predictions when all comparisons are used.","We evaluate the approach on multiple NLG tasks and demonstrate that our framework can yield considerable computational savings when performing pairwise comparative assessment.","When N is large, with as few as 2% of comparisons the PoE solution can achieve similar performance to when all comparisons are used."],"url":"http://arxiv.org/abs/2405.05894v1","category":"cs.CL"}
{"created":"2024-05-09 16:44:23","title":"Financial knowledge and borrower discouragement","abstract":"This study provides first empirical evidence on the impact of entrepreneurs' financial knowledge on borrower discouragement. Using novel survey data on Italian micro-enterprises, we find that less financially knowledgeable entrepreneurs are more likely to be discouraged from applying for new financing, due to higher application costs and expected rejection. Our main results are robust to several sensitivity checks, including accounting for potential endogeneity. Furthermore, we show that the observed self-rationing mechanism is rather inefficient, suggesting that financial knowledge might play a key role in reducing credit market imperfections.","sentences":["This study provides first empirical evidence on the impact of entrepreneurs' financial knowledge on borrower discouragement.","Using novel survey data on Italian micro-enterprises, we find that less financially knowledgeable entrepreneurs are more likely to be discouraged from applying for new financing, due to higher application costs and expected rejection.","Our main results are robust to several sensitivity checks, including accounting for potential endogeneity.","Furthermore, we show that the observed self-rationing mechanism is rather inefficient, suggesting that financial knowledge might play a key role in reducing credit market imperfections."],"url":"http://arxiv.org/abs/2405.05891v1","category":"econ.GN"}
{"created":"2024-05-09 16:42:39","title":"Safe Exploration Using Bayesian World Models and Log-Barrier Optimization","abstract":"A major challenge in deploying reinforcement learning in online tasks is ensuring that safety is maintained throughout the learning process. In this work, we propose CERL, a new method for solving constrained Markov decision processes while keeping the policy safe during learning. Our method leverages Bayesian world models and suggests policies that are pessimistic w.r.t. the model's epistemic uncertainty. This makes CERL robust towards model inaccuracies and leads to safe exploration during learning. In our experiments, we demonstrate that CERL outperforms the current state-of-the-art in terms of safety and optimality in solving CMDPs from image observations.","sentences":["A major challenge in deploying reinforcement learning in online tasks is ensuring that safety is maintained throughout the learning process.","In this work, we propose CERL, a new method for solving constrained Markov decision processes while keeping the policy safe during learning.","Our method leverages Bayesian world models and suggests policies that are pessimistic w.r.t.","the model's epistemic uncertainty.","This makes CERL robust towards model inaccuracies and leads to safe exploration during learning.","In our experiments, we demonstrate that CERL outperforms the current state-of-the-art in terms of safety and optimality in solving CMDPs from image observations."],"url":"http://arxiv.org/abs/2405.05890v1","category":"cs.LG"}
{"created":"2024-05-09 16:40:42","title":"Perturbing Dynamics of Active Emulsions and Their Collectives","abstract":"Controlling fluidic flows in active droplets is crucial in developing intelligent models to understand and mimic single-celled microorganisms. Typically, these fluidic flows are affected by the interfacial dynamics of chemical agents. We found that these flows can be reconfigured by the mere presence of anisotropic solid boundary embedded within active droplets. Spontaneous fluidic flows dynamically orient an embedded magnetic cluster and the magnetic cluster, when realigned, causes these flows to reorient. Thus, providing an unprecedented control over the propulsion dynamics of chemotactic emulsions. When continuously perturbed, achiral emulsions exhibit emergent chiral motion with rotating fluidic flows. Such solid-fluid interactions removes barriers of specific emulsion chemistries and complements their inherent abilities thereby also enabling control over emergent collective behaviors of active droplets.","sentences":["Controlling fluidic flows in active droplets is crucial in developing intelligent models to understand and mimic single-celled microorganisms.","Typically, these fluidic flows are affected by the interfacial dynamics of chemical agents.","We found that these flows can be reconfigured by the mere presence of anisotropic solid boundary embedded within active droplets.","Spontaneous fluidic flows dynamically orient an embedded magnetic cluster and the magnetic cluster, when realigned, causes these flows to reorient.","Thus, providing an unprecedented control over the propulsion dynamics of chemotactic emulsions.","When continuously perturbed, achiral emulsions exhibit emergent chiral motion with rotating fluidic flows.","Such solid-fluid interactions removes barriers of specific emulsion chemistries and complements their inherent abilities thereby also enabling control over emergent collective behaviors of active droplets."],"url":"http://arxiv.org/abs/2405.05889v1","category":"cond-mat.soft"}
{"created":"2024-05-09 16:26:55","title":"Convergence Rates of Online Critic Value Function Approximation in Native Spaces","abstract":"In this paper, the evolution equation that defines the online critic for the approximation of the optimal value function is cast in a general class of reproducing kernel Hilbert spaces (RKHSs). Exploiting some core tools of RKHS theory, this formulation allows deriving explicit bounds on the performance of the critic in terms of the kernel and definition of the RKHS, the number of basis functions, and the location of centers used to define scattered bases. The performance of the critic is precisely measured in terms of the power function of the scattered basis used in approximations, and it can be used either in an a priori evaluation of potential bases or in an a posteriori assessments of value function error for basis enrichment or pruning. The most concise bounds in the paper describe explicitly how the critic performance depends on the placement of centers, as measured by their fill distance in a subset that contains the trajectory of the critic.","sentences":["In this paper, the evolution equation that defines the online critic for the approximation of the optimal value function is cast in a general class of reproducing kernel Hilbert spaces (RKHSs).","Exploiting some core tools of RKHS theory, this formulation allows deriving explicit bounds on the performance of the critic in terms of the kernel and definition of the RKHS, the number of basis functions, and the location of centers used to define scattered bases.","The performance of the critic is precisely measured in terms of the power function of the scattered basis used in approximations, and it can be used either in an a priori evaluation of potential bases or in an a posteriori assessments of value function error for basis enrichment or pruning.","The most concise bounds in the paper describe explicitly how the critic performance depends on the placement of centers, as measured by their fill distance in a subset that contains the trajectory of the critic."],"url":"http://arxiv.org/abs/2405.05887v1","category":"math.OC"}
{"created":"2024-05-09 16:22:24","title":"Exploiting Autoencoder's Weakness to Generate Pseudo Anomalies","abstract":"Due to the rare occurrence of anomalous events, a typical approach to anomaly detection is to train an autoencoder (AE) with normal data only so that it learns the patterns or representations of the normal training data. At test time, the trained AE is expected to well reconstruct normal but to poorly reconstruct anomalous data. However, contrary to the expectation, anomalous data is often well reconstructed as well. In order to further separate the reconstruction quality between normal and anomalous data, we propose creating pseudo anomalies from learned adaptive noise by exploiting the aforementioned weakness of AE, i.e., reconstructing anomalies too well. The generated noise is added to the normal data to create pseudo anomalies. Extensive experiments on Ped2, Avenue, ShanghaiTech, CIFAR-10, and KDDCUP datasets demonstrate the effectiveness and generic applicability of our approach in improving the discriminative capability of AEs for anomaly detection.","sentences":["Due to the rare occurrence of anomalous events, a typical approach to anomaly detection is to train an autoencoder (AE) with normal data only so that it learns the patterns or representations of the normal training data.","At test time, the trained AE is expected to well reconstruct normal but to poorly reconstruct anomalous data.","However, contrary to the expectation, anomalous data is often well reconstructed as well.","In order to further separate the reconstruction quality between normal and anomalous data, we propose creating pseudo anomalies from learned adaptive noise by exploiting the aforementioned weakness of AE, i.e., reconstructing anomalies too well.","The generated noise is added to the normal data to create pseudo anomalies.","Extensive experiments on Ped2, Avenue, ShanghaiTech, CIFAR-10, and KDDCUP datasets demonstrate the effectiveness and generic applicability of our approach in improving the discriminative capability of AEs for anomaly detection."],"url":"http://arxiv.org/abs/2405.05886v1","category":"cs.LG"}
{"created":"2024-05-09 16:16:34","title":"A meta inspiral-merger-ringdown consistency test of general relativity with gravitational wave signals from compact binaries","abstract":"The observation of gravitational waves from compact binary coalescences is a promising tool to test the validity of general relativity (GR) in a highly dynamical strong-field regime. There are now a variety of tests of GR performed on the observed compact binary signals. In this paper, we propose a new test of GR that compares the results of these individual tests. This meta inspiral-merger-ringdown consistency test (IMRCT) involves inferring the final mass and spin of the remnant black hole obtained from the analyses of two different tests of GR and checking for consistency. If there is a deviation from GR, we expect that different tests of GR will recover different values for the final mass and spin, in general. We check the performance of the meta IMRCT using a standard set of null tests used in various gravitational-wave analyses: the original IMRCT, parameterized phasing tests (TIGER and FTI) and the modified dispersion test. However, the meta IMRCT is applicable to any tests of GR that infer the initial masses and spins or the final mass and spin, including ones that are applied to binary neutron star or neutron star--black hole signals. We apply the meta IMRCT to simulated quasi-circular GR and non-GR binary black hole (BBH) signals as well as to eccentric BBH signals in GR (analyzed with quasicircular waveforms). We find that the meta IMRCT gives consistency with GR for the quasi-circular GR signals and picks up a deviation from GR in the other cases, as do other tests. In some cases, the meta IMRCT finds a significant GR deviation for a given pair of tests (and specific testing parameters) while the individual tests do not, showing that it is more sensitive than the individual tests to certain types of deviations. In addition, we also apply this test to a few selected real compact binary signals and find them consistent with GR.","sentences":["The observation of gravitational waves from compact binary coalescences is a promising tool to test the validity of general relativity (GR) in a highly dynamical strong-field regime.","There are now a variety of tests of GR performed on the observed compact binary signals.","In this paper, we propose a new test of GR that compares the results of these individual tests.","This meta inspiral-merger-ringdown consistency test (IMRCT) involves inferring the final mass and spin of the remnant black hole obtained from the analyses of two different tests of GR and checking for consistency.","If there is a deviation from GR, we expect that different tests of GR will recover different values for the final mass and spin, in general.","We check the performance of the meta IMRCT using a standard set of null tests used in various gravitational-wave analyses: the original IMRCT, parameterized phasing tests (TIGER and FTI) and the modified dispersion test.","However, the meta IMRCT is applicable to any tests of GR that infer the initial masses and spins or the final mass and spin, including ones that are applied to binary neutron star or neutron star--black hole signals.","We apply the meta IMRCT to simulated quasi-circular GR and non-GR binary black hole (BBH) signals as well as to eccentric BBH signals in GR (analyzed with quasicircular waveforms).","We find that the meta IMRCT gives consistency with GR for the quasi-circular GR signals and picks up a deviation from GR in the other cases, as do other tests.","In some cases, the meta IMRCT finds a significant GR deviation for a given pair of tests (and specific testing parameters) while the individual tests do not, showing that it is more sensitive than the individual tests to certain types of deviations.","In addition, we also apply this test to a few selected real compact binary signals and find them consistent with GR."],"url":"http://arxiv.org/abs/2405.05884v1","category":"gr-qc"}
{"created":"2024-05-09 16:16:14","title":"supDQN: Supervised Rewarding Strategy Driven Deep Q-Network for sEMG Signal Decontamination","abstract":"The presence of muscles throughout the active parts of the body such as the upper and lower limbs, makes electromyography-based human-machine interaction prevalent. However, muscle signals are stochastic and noisy. These noises can be regular and irregular. Irregular noises due to movements or electrical switching require dynamic filtering. Conventionally, filters are stacked, which trims and delays the signal unnecessarily. This study introduces a decontamination technique involving a supervised rewarding strategy to drive a deep Q-network-based agent (supDQN). It applies one of three filters to decontaminate a 1sec long surface electromyography signal, which is dynamically contaminated. A machine learning agent identifies whether the signal after filtering is clean or noisy. Accordingly, a reward is generated. The identification accuracy is enhanced by using a local interpretable model-agnostic explanation. The deep Q-network is guided by this reward to select filter optimally while decontaminating a signal. The proposed filtering strategy is tested on four noise levels (-5 dB, -1 dB, +1 dB, +5 dB). supDQN filters the signal desirably when the signal-to-noise ratio (SNR) is between -5 dB to +1 dB. It filters less desirably at high SNR (+5 dB). A normalized root mean square (nRMSE) is formulated to depict the difference of filtered signal from ground truth. This is used to compare supDQN and conventional methods including wavelet denoising with debauchies and symlet wavelet, high order low pass filter, notch filter, and high pass filter. The proposed filtering strategy gives an average value nRMSE of 1.1974, which is lower than the conventional filters.","sentences":["The presence of muscles throughout the active parts of the body such as the upper and lower limbs, makes electromyography-based human-machine interaction prevalent.","However, muscle signals are stochastic and noisy.","These noises can be regular and irregular.","Irregular noises due to movements or electrical switching require dynamic filtering.","Conventionally, filters are stacked, which trims and delays the signal unnecessarily.","This study introduces a decontamination technique involving a supervised rewarding strategy to drive a deep Q-network-based agent (supDQN).","It applies one of three filters to decontaminate a 1sec long surface electromyography signal, which is dynamically contaminated.","A machine learning agent identifies whether the signal after filtering is clean or noisy.","Accordingly, a reward is generated.","The identification accuracy is enhanced by using a local interpretable model-agnostic explanation.","The deep Q-network is guided by this reward to select filter optimally while decontaminating a signal.","The proposed filtering strategy is tested on four noise levels (-5 dB, -1 dB, +1 dB, +5 dB).","supDQN filters the signal desirably when the signal-to-noise ratio (SNR) is between -5 dB to +1 dB. It filters less desirably at high SNR (+5 dB).","A normalized root mean square (nRMSE) is formulated to depict the difference of filtered signal from ground truth.","This is used to compare supDQN and conventional methods including wavelet denoising with debauchies and symlet wavelet, high order low pass filter, notch filter, and high pass filter.","The proposed filtering strategy gives an average value nRMSE of 1.1974, which is lower than the conventional filters."],"url":"http://arxiv.org/abs/2405.05883v1","category":"eess.SP"}
{"created":"2024-05-09 16:04:14","title":"Composable Part-Based Manipulation","abstract":"In this paper, we propose composable part-based manipulation (CPM), a novel approach that leverages object-part decomposition and part-part correspondences to improve learning and generalization of robotic manipulation skills. By considering the functional correspondences between object parts, we conceptualize functional actions, such as pouring and constrained placing, as combinations of different correspondence constraints. CPM comprises a collection of composable diffusion models, where each model captures a different inter-object correspondence. These diffusion models can generate parameters for manipulation skills based on the specific object parts. Leveraging part-based correspondences coupled with the task decomposition into distinct constraints enables strong generalization to novel objects and object categories. We validate our approach in both simulated and real-world scenarios, demonstrating its effectiveness in achieving robust and generalized manipulation capabilities.","sentences":["In this paper, we propose composable part-based manipulation (CPM), a novel approach that leverages object-part decomposition and part-part correspondences to improve learning and generalization of robotic manipulation skills.","By considering the functional correspondences between object parts, we conceptualize functional actions, such as pouring and constrained placing, as combinations of different correspondence constraints.","CPM comprises a collection of composable diffusion models, where each model captures a different inter-object correspondence.","These diffusion models can generate parameters for manipulation skills based on the specific object parts.","Leveraging part-based correspondences coupled with the task decomposition into distinct constraints enables strong generalization to novel objects and object categories.","We validate our approach in both simulated and real-world scenarios, demonstrating its effectiveness in achieving robust and generalized manipulation capabilities."],"url":"http://arxiv.org/abs/2405.05876v1","category":"cs.RO"}
{"created":"2024-05-09 16:03:41","title":"A Genetic Approach to Minimising Gate and Qubit Teleportations for Multi-Processor Quantum Circuit Distribution","abstract":"Distributed Quantum Computing (DQC) provides a means for scaling available quantum computation by interconnecting multiple quantum processor units (QPUs). A key challenge in this domain is efficiently allocating logical qubits from quantum circuits to the physical qubits within QPUs, a task known to be NP-hard. Traditional approaches, primarily focused on graph partitioning strategies, have sought to reduce the number of required Bell pairs for executing non-local CNOT operations, a form of gate teleportation. However, these methods have limitations in terms of efficiency and scalability. Addressing this, our work jointly considers gate and qubit teleportations introducing a novel meta-heuristic algorithm to minimise the network cost of executing a quantum circuit. By allowing dynamic reallocation of qubits along with gate teleportations during circuit execution, our method significantly enhances the overall efficacy and potential scalability of DQC frameworks. In our numerical analysis, we demonstrate that integrating qubit teleportations into our genetic algorithm for optimising circuit blocking reduces the required resources, specifically the number of EPR pairs, compared to traditional graph partitioning methods. Our results, derived from both benchmark and randomly generated circuits, show that as circuit complexity increases - demanding more qubit teleportations - our approach effectively optimises these teleportations throughout the execution, thereby enhancing performance through strategic circuit partitioning. This is a step forward in the pursuit of a global quantum compiler which will ultimately enable the efficient use of a 'quantum data center' in the future.","sentences":["Distributed Quantum Computing (DQC) provides a means for scaling available quantum computation by interconnecting multiple quantum processor units (QPUs).","A key challenge in this domain is efficiently allocating logical qubits from quantum circuits to the physical qubits within QPUs, a task known to be NP-hard.","Traditional approaches, primarily focused on graph partitioning strategies, have sought to reduce the number of required Bell pairs for executing non-local CNOT operations, a form of gate teleportation.","However, these methods have limitations in terms of efficiency and scalability.","Addressing this, our work jointly considers gate and qubit teleportations introducing a novel meta-heuristic algorithm to minimise the network cost of executing a quantum circuit.","By allowing dynamic reallocation of qubits along with gate teleportations during circuit execution, our method significantly enhances the overall efficacy and potential scalability of DQC frameworks.","In our numerical analysis, we demonstrate that integrating qubit teleportations into our genetic algorithm for optimising circuit blocking reduces the required resources, specifically the number of EPR pairs, compared to traditional graph partitioning methods.","Our results, derived from both benchmark and randomly generated circuits, show that as circuit complexity increases - demanding more qubit teleportations - our approach effectively optimises these teleportations throughout the execution, thereby enhancing performance through strategic circuit partitioning.","This is a step forward in the pursuit of a global quantum compiler which will ultimately enable the efficient use of a 'quantum data center' in the future."],"url":"http://arxiv.org/abs/2405.05875v1","category":"quant-ph"}
{"created":"2024-05-09 16:03:23","title":"Performance Parameters of Infra-red and Visible-active MXene Photocatalysts for Water Splitting","abstract":"Water splitting reactions through photocatalysis is an efficient and sustainable technique for the generation of green energy. The photocatalyst's ability to effect simultaneous generation of hydrogen and oxygen, along with efficiency in utilisation of charged carriers, conversion of solar energy to hydrogen, fast migration, and low recombination rates of carriers, are the parameters to decide its suitability in water splitting. In literature, comprehensive calculation and analysis of all these performance parameters for a potential photocatalyst are rare. In this work, we have performed first-principles-based computations to find new efficient photocatalysts from the family of Janus MXenes and assessed their performance parameters. Strain engineering has been invoked in search of new materials. Out of 14 studied materials, we find 5 materials: Sc$_{2}$COS, Zr$_{2}$COS, Hf$_{2}$COS, and ZrHfCO$_{2}$ under zero or finite tensile strain and Hf$_{2}$COSe at 6\\% tensile strain meeting the requirements of simultaneous reactions to split water. The computations of various efficiency-related parameters demonstrate that Zr$_{2}$COS, Hf$_{2}$COS, and Hf$_{2}$COSe have excellent efficiencies, significantly better than the well-known photocatalysts. The origin of such performances lies in their electronic and optical properties, which are analysed systematically.","sentences":["Water splitting reactions through photocatalysis is an efficient and sustainable technique for the generation of green energy.","The photocatalyst's ability to effect simultaneous generation of hydrogen and oxygen, along with efficiency in utilisation of charged carriers, conversion of solar energy to hydrogen, fast migration, and low recombination rates of carriers, are the parameters to decide its suitability in water splitting.","In literature, comprehensive calculation and analysis of all these performance parameters for a potential photocatalyst are rare.","In this work, we have performed first-principles-based computations to find new efficient photocatalysts from the family of Janus MXenes and assessed their performance parameters.","Strain engineering has been invoked in search of new materials.","Out of 14 studied materials, we find 5 materials: Sc$_{2}$COS, Zr$_{2}$COS, Hf$_{2}$COS, and ZrHfCO$_{2}$ under zero or finite tensile strain and Hf$_{2}$COSe at 6\\% tensile strain meeting the requirements of simultaneous reactions to split water.","The computations of various efficiency-related parameters demonstrate that Zr$_{2}$COS, Hf$_{2}$COS, and Hf$_{2}$COSe have excellent efficiencies, significantly better than the well-known photocatalysts.","The origin of such performances lies in their electronic and optical properties, which are analysed systematically."],"url":"http://arxiv.org/abs/2405.05874v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 16:00:54","title":"FlockGPT: Guiding UAV Flocking with Linguistic Orchestration","abstract":"This article presents the world's first rapid drone flocking control using natural language through generative AI. The described approach enables the intuitive orchestration of a flock of any size to achieve the desired geometry. The key feature of the method is the development of a new interface based on Large Language Models to communicate with the user and to generate the target geometry descriptions. Users can interactively modify or provide comments during the construction of the flock geometry model. By combining flocking technology and defining the target surface using a signed distance function, smooth and adaptive movement of the drone swarm between target states is achieved.   Our user study on FlockGPT confirmed a high level of intuitive control over drone flocking by users. Subjects who had never previously controlled a swarm of drones were able to construct complex figures in just a few iterations and were able to accurately distinguish the formed swarm drone figures. The results revealed a high recognition rate for six different geometric patterns generated through the LLM-based interface and performed by a simulated drone flock (mean of 80% with a maximum of 93\\% for cube and tetrahedron patterns). Users commented on low temporal demand (19.2 score in NASA-TLX), high performance (26 score in NASA-TLX), attractiveness (1.94 UEQ score), and hedonic quality (1.81 UEQ score) of the developed system. The FlockGPT demo code repository can be found at: coming soon","sentences":["This article presents the world's first rapid drone flocking control using natural language through generative AI.","The described approach enables the intuitive orchestration of a flock of any size to achieve the desired geometry.","The key feature of the method is the development of a new interface based on Large Language Models to communicate with the user and to generate the target geometry descriptions.","Users can interactively modify or provide comments during the construction of the flock geometry model.","By combining flocking technology and defining the target surface using a signed distance function, smooth and adaptive movement of the drone swarm between target states is achieved.   ","Our user study on FlockGPT confirmed a high level of intuitive control over drone flocking by users.","Subjects who had never previously controlled a swarm of drones were able to construct complex figures in just a few iterations and were able to accurately distinguish the formed swarm drone figures.","The results revealed a high recognition rate for six different geometric patterns generated through the LLM-based interface and performed by a simulated drone flock (mean of 80% with a maximum of 93\\% for cube and tetrahedron patterns).","Users commented on low temporal demand (19.2 score in NASA-TLX), high performance (26 score in NASA-TLX), attractiveness (1.94 UEQ score), and hedonic quality (1.81 UEQ score) of the developed system.","The FlockGPT demo code repository can be found at: coming soon"],"url":"http://arxiv.org/abs/2405.05872v1","category":"cs.RO"}
{"created":"2024-05-09 16:00:20","title":"Selecting the Most Conflicting Pair of Candidates","abstract":"We study committee elections from a perspective of finding the most conflicting candidates, that is, candidates that imply the largest amount of conflict, as per voter preferences. By proposing basic axioms to capture this objective, we show that none of the prominent multiwinner voting rules meet them. Consequently, we design committee voting rules compliant with our desiderata, introducing conflictual voting rules. A subsequent deepened analysis sheds more light on how they operate. Our investigation identifies various aspects of conflict, for which we come up with relevant axioms and quantitative measures, which may be of independent interest. We support our theoretical study with experiments on both real-life and synthetic data.","sentences":["We study committee elections from a perspective of finding the most conflicting candidates, that is, candidates that imply the largest amount of conflict, as per voter preferences.","By proposing basic axioms to capture this objective, we show that none of the prominent multiwinner voting rules meet them.","Consequently, we design committee voting rules compliant with our desiderata, introducing conflictual voting rules.","A subsequent deepened analysis sheds more light on how they operate.","Our investigation identifies various aspects of conflict, for which we come up with relevant axioms and quantitative measures, which may be of independent interest.","We support our theoretical study with experiments on both real-life and synthetic data."],"url":"http://arxiv.org/abs/2405.05870v1","category":"cs.GT"}
{"created":"2024-05-09 15:52:40","title":"SMARTINI3: Systematic Parametrization of Realistic Multi-Scale Membrane Models via Unsupervised Learning and Multi-Objective Evolutionary Algorithms","abstract":"In this study, we utilize genetic algorithms to develop a realistic implicit solvent ultra-coarse-grained (PC) membrane model comprising only three interaction sites. The key philosophy of the ultra-CG membrane model SMARTINI3 is its compatibility with realistic membrane proteins, for example, modeled within the Martini coarse-grained (CG) model, as well as with the widely used GROMACS software for molecular simulations. Our objective is to parameterize this ultra-CG model to accurately reproduce the experimentally observed structural and thermodynamic properties of PC membranes in real units, including properties such as area per lipid, area compressibility, bending modulus, line tension, phase transition temperature, density profile, and radial distribution function. In our example, we specifically focus on the properties of a POPC membrane, although the developed membrane model could be perceived as a generic model of lipid membranes. To optimize the performance of the model (the fitness), we conduct a series of evolutionary runs with diverse random initial population sizes (ranging from 96 to 384). We demonstrate that the ultra-CG membrane model we developed exhibits authentic lipid membrane behaviors, encompassing self-assembly into bilayers, vesicle formation, membrane fusion, and gel phase formation. Moreover, we demonstrate compatibility with the Martini coarse-grained model by successfully reproducing the behavior of a transmembrane domain embedded within a lipid bilayer. This facilitates the simulation of realistic membrane proteins within an ultra-CG bilayer membrane, enhancing the accuracy and applicability of our model in biophysical studies.","sentences":["In this study, we utilize genetic algorithms to develop a realistic implicit solvent ultra-coarse-grained (PC) membrane model comprising only three interaction sites.","The key philosophy of the ultra-CG membrane model SMARTINI3 is its compatibility with realistic membrane proteins, for example, modeled within the Martini coarse-grained (CG) model, as well as with the widely used GROMACS software for molecular simulations.","Our objective is to parameterize this ultra-CG model to accurately reproduce the experimentally observed structural and thermodynamic properties of PC membranes in real units, including properties such as area per lipid, area compressibility, bending modulus, line tension, phase transition temperature, density profile, and radial distribution function.","In our example, we specifically focus on the properties of a POPC membrane, although the developed membrane model could be perceived as a generic model of lipid membranes.","To optimize the performance of the model (the fitness), we conduct a series of evolutionary runs with diverse random initial population sizes (ranging from 96 to 384).","We demonstrate that the ultra-CG membrane model we developed exhibits authentic lipid membrane behaviors, encompassing self-assembly into bilayers, vesicle formation, membrane fusion, and gel phase formation.","Moreover, we demonstrate compatibility with the Martini coarse-grained model by successfully reproducing the behavior of a transmembrane domain embedded within a lipid bilayer.","This facilitates the simulation of realistic membrane proteins within an ultra-CG bilayer membrane, enhancing the accuracy and applicability of our model in biophysical studies."],"url":"http://arxiv.org/abs/2405.05864v1","category":"q-bio.BM"}
{"created":"2024-05-09 15:48:39","title":"ExACT: An End-to-End Autonomous Excavator System Using Action Chunking With Transformers","abstract":"Excavators are crucial for diverse tasks such as construction and mining, while autonomous excavator systems enhance safety and efficiency, address labor shortages, and improve human working conditions. Different from the existing modularized approaches, this paper introduces ExACT, an end-to-end autonomous excavator system that processes raw LiDAR, camera data, and joint positions to control excavator valves directly. Utilizing the Action Chunking with Transformers (ACT) architecture, ExACT employs imitation learning to take observations from multi-modal sensors as inputs and generate actionable sequences. In our experiment, we build a simulator based on the captured real-world data to model the relations between excavator valve states and joint velocities. With a few human-operated demonstration data trajectories, ExACT demonstrates the capability of completing different excavation tasks, including reaching, digging and dumping through imitation learning in validations with the simulator. To the best of our knowledge, ExACT represents the first instance towards building an end-to-end autonomous excavator system via imitation learning methods with a minimal set of human demonstrations. The video about this work can be accessed at https://youtu.be/NmzR_Rf-aEk.","sentences":["Excavators are crucial for diverse tasks such as construction and mining, while autonomous excavator systems enhance safety and efficiency, address labor shortages, and improve human working conditions.","Different from the existing modularized approaches, this paper introduces ExACT, an end-to-end autonomous excavator system that processes raw LiDAR, camera data, and joint positions to control excavator valves directly.","Utilizing the Action Chunking with Transformers (ACT) architecture, ExACT employs imitation learning to take observations from multi-modal sensors as inputs and generate actionable sequences.","In our experiment, we build a simulator based on the captured real-world data to model the relations between excavator valve states and joint velocities.","With a few human-operated demonstration data trajectories, ExACT demonstrates the capability of completing different excavation tasks, including reaching, digging and dumping through imitation learning in validations with the simulator.","To the best of our knowledge, ExACT represents the first instance towards building an end-to-end autonomous excavator system via imitation learning methods with a minimal set of human demonstrations.","The video about this work can be accessed at https://youtu.be/NmzR_Rf-aEk."],"url":"http://arxiv.org/abs/2405.05861v1","category":"cs.RO"}
{"created":"2024-05-09 15:45:08","title":"Free-Moving Object Reconstruction and Pose Estimation with Virtual Camera","abstract":"We propose an approach for reconstructing free-moving object from a monocular RGB video. Most existing methods either assume scene prior, hand pose prior, object category pose prior, or rely on local optimization with multiple sequence segments. We propose a method that allows free interaction with the object in front of a moving camera without relying on any prior, and optimizes the sequence globally without any segments. We progressively optimize the object shape and pose simultaneously based on an implicit neural representation. A key aspect of our method is a virtual camera system that reduces the search space of the optimization significantly. We evaluate our method on the standard HO3D dataset and a collection of egocentric RGB sequences captured with a head-mounted device. We demonstrate that our approach outperforms most methods significantly, and is on par with recent techniques that assume prior information.","sentences":["We propose an approach for reconstructing free-moving object from a monocular RGB video.","Most existing methods either assume scene prior, hand pose prior, object category pose prior, or rely on local optimization with multiple sequence segments.","We propose a method that allows free interaction with the object in front of a moving camera without relying on any prior, and optimizes the sequence globally without any segments.","We progressively optimize the object shape and pose simultaneously based on an implicit neural representation.","A key aspect of our method is a virtual camera system that reduces the search space of the optimization significantly.","We evaluate our method on the standard HO3D dataset and a collection of egocentric RGB sequences captured with a head-mounted device.","We demonstrate that our approach outperforms most methods significantly, and is on par with recent techniques that assume prior information."],"url":"http://arxiv.org/abs/2405.05858v1","category":"cs.CV"}
{"created":"2024-05-09 15:44:15","title":"Fukaya Categories of Hyperplane Arrangements","abstract":"To a simple polarized hyperplane arrangement (not necessarily cyclic) $\\mathbb{V}$, one can associate a stopped Liouville manifold (equivalently, a Liouville sector) $\\left(M(\\mathbb{V}),\\xi\\right)$, where $M(\\mathbb{V})$ is the complement of finitely many hyperplanes in $\\mathbb{C}^d$, obtained as the complexifications of the real hyperplanes in $\\mathbb{V}$. The Liouville structure on $M(\\mathbb{V})$ comes from a very affine embedding, and the stop $\\xi$ is determined by the polarization. In this article, we study the symplectic topology of $\\left(M(\\mathbb{V}),\\xi\\right)$. In particular, we prove that their partially wrapped Fukaya categories are generated by Lagrangian submanifolds associated to the bounded and feasible chambers of $\\mathbb{V}$. A computation of the Fukaya $A_\\infty$-algebra of these Lagrangians then enables us to identity these wrapped Fukaya categories with the $\\mathbb{G}_m^d$-equivariant hypertoric convolution algebras $\\widetilde{B}(\\mathbb{V})$ associated to $\\mathbb{V}$. This confirms a conjecture of Lauda-Licata-Manion and provides evidence for the general conjecture of Lekili-Segal on the equivariant Fukaya categories of symplectic manifolds with Hamiltonian torus actions.","sentences":["To a simple polarized hyperplane arrangement (not necessarily cyclic) $\\mathbb{V}$, one can associate a stopped Liouville manifold (equivalently, a Liouville sector) $\\left(M(\\mathbb{V}),\\xi\\right)$, where $M(\\mathbb{V})$ is the complement of finitely many hyperplanes in $\\mathbb{C}^d$, obtained as the complexifications of the real hyperplanes in $\\mathbb{V}$. The Liouville structure on $M(\\mathbb{V})$ comes from a very affine embedding, and the stop $\\xi$ is determined by the polarization.","In this article, we study the symplectic topology of $\\left(M(\\mathbb{V}),\\xi\\right)$. In particular, we prove that their partially wrapped Fukaya categories are generated by Lagrangian submanifolds associated to the bounded and feasible chambers of $\\mathbb{V}$. A computation of the Fukaya $A_\\infty$-algebra of these Lagrangians then enables us to identity these wrapped Fukaya categories with the $\\mathbb{G}_m^d$-equivariant hypertoric convolution algebras $\\widetilde{B}(\\mathbb{V})$ associated to $\\mathbb{V}$. This confirms a conjecture of Lauda-Licata-Manion and provides evidence for the general conjecture of Lekili-Segal on the equivariant Fukaya categories of symplectic manifolds with Hamiltonian torus actions."],"url":"http://arxiv.org/abs/2405.05856v1","category":"math.SG"}
{"created":"2024-05-09 15:44:11","title":"Compressed Bayesian Federated Learning for Reliable Passive Radio Sensing in Industrial IoT","abstract":"Bayesian Federated Learning (FL) has been recently introduced to provide well-calibrated Machine Learning (ML) models quantifying the uncertainty of their predictions. Despite their advantages compared to frequentist FL setups, Bayesian FL tools implemented over decentralized networks are subject to high communication costs due to the iterated exchange of local posterior distributions among cooperating devices. Therefore, this paper proposes a communication-efficient decentralized Bayesian FL policy to reduce the communication overhead without sacrificing final learning accuracy and calibration. The proposed method integrates compression policies and allows devices to perform multiple optimization steps before sending the local posterior distributions. We integrate the developed tool in an Industrial Internet of Things (IIoT) use case where collaborating nodes equipped with autonomous radar sensors are tasked to reliably localize human operators in a workplace shared with robots. Numerical results show that the developed approach obtains highly accurate yet well-calibrated ML models compatible with the ones provided by conventional (uncompressed) Bayesian FL tools while substantially decreasing the communication overhead (i.e., up to 99%). Furthermore, the proposed approach is advantageous when compared with state-of-the-art compressed frequentist FL setups in terms of calibration, especially when the statistical distribution of the testing dataset changes.","sentences":["Bayesian Federated Learning (FL) has been recently introduced to provide well-calibrated Machine Learning (ML) models quantifying the uncertainty of their predictions.","Despite their advantages compared to frequentist FL setups, Bayesian FL tools implemented over decentralized networks are subject to high communication costs due to the iterated exchange of local posterior distributions among cooperating devices.","Therefore, this paper proposes a communication-efficient decentralized Bayesian FL policy to reduce the communication overhead without sacrificing final learning accuracy and calibration.","The proposed method integrates compression policies and allows devices to perform multiple optimization steps before sending the local posterior distributions.","We integrate the developed tool in an Industrial Internet of Things (IIoT) use case where collaborating nodes equipped with autonomous radar sensors are tasked to reliably localize human operators in a workplace shared with robots.","Numerical results show that the developed approach obtains highly accurate yet well-calibrated ML models compatible with the ones provided by conventional (uncompressed) Bayesian FL tools while substantially decreasing the communication overhead (i.e., up to 99%).","Furthermore, the proposed approach is advantageous when compared with state-of-the-art compressed frequentist FL setups in terms of calibration, especially when the statistical distribution of the testing dataset changes."],"url":"http://arxiv.org/abs/2405.05855v1","category":"cs.LG"}
{"created":"2024-05-09 15:39:54","title":"Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control","abstract":"Embodied AI agents require a fine-grained understanding of the physical world mediated through visual and language inputs. Such capabilities are difficult to learn solely from task-specific data. This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains. However, commonly used contrastively trained representations such as in CLIP have been shown to fail at enabling embodied agents to gain a sufficiently fine-grained scene understanding -- a capability vital for control. To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information. Using pre-trained text-to-image diffusion models, we construct Stable Control Representations which allow learning downstream control policies that generalize to complex, open-ended environments. We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks. Most notably, we show that Stable Control Representations enable learning policies that exhibit state-of-the-art performance on OVMM, a difficult open-vocabulary navigation benchmark.","sentences":["Embodied AI agents require a fine-grained understanding of the physical world mediated through visual and language inputs.","Such capabilities are difficult to learn solely from task-specific data.","This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains.","However, commonly used contrastively trained representations such as in CLIP have been shown to fail at enabling embodied agents to gain a sufficiently fine-grained scene understanding -- a capability vital for control.","To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information.","Using pre-trained text-to-image diffusion models, we construct Stable Control Representations which allow learning downstream control policies that generalize to complex, open-ended environments.","We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks.","Most notably, we show that Stable Control Representations enable learning policies that exhibit state-of-the-art performance on OVMM, a difficult open-vocabulary navigation benchmark."],"url":"http://arxiv.org/abs/2405.05852v1","category":"cs.CV"}
{"created":"2024-05-09 15:34:15","title":"Learned feature representations are biased by complexity, learning order, position, and more","abstract":"Representation learning, and interpreting learned representations, are key areas of focus in machine learning and neuroscience. Both fields generally use representations as a means to understand or improve a system's computations. In this work, however, we explore surprising dissociations between representation and computation that may pose challenges for such efforts. We create datasets in which we attempt to match the computational role that different features play, while manipulating other properties of the features or the data. We train various deep learning architectures to compute these multiple abstract features about their inputs. We find that their learned feature representations are systematically biased towards representing some features more strongly than others, depending upon extraneous properties such as feature complexity, the order in which features are learned, and the distribution of features over the inputs. For example, features that are simpler to compute or learned first tend to be represented more strongly and densely than features that are more complex or learned later, even if all features are learned equally well. We also explore how these biases are affected by architectures, optimizers, and training regimes (e.g., in transformers, features decoded earlier in the output sequence also tend to be represented more strongly). Our results help to characterize the inductive biases of gradient-based representation learning. These results also highlight a key challenge for interpretability $-$ or for comparing the representations of models and brains $-$ disentangling extraneous biases from the computationally important aspects of a system's internal representations.","sentences":["Representation learning, and interpreting learned representations, are key areas of focus in machine learning and neuroscience.","Both fields generally use representations as a means to understand or improve a system's computations.","In this work, however, we explore surprising dissociations between representation and computation that may pose challenges for such efforts.","We create datasets in which we attempt to match the computational role that different features play, while manipulating other properties of the features or the data.","We train various deep learning architectures to compute these multiple abstract features about their inputs.","We find that their learned feature representations are systematically biased towards representing some features more strongly than others, depending upon extraneous properties such as feature complexity, the order in which features are learned, and the distribution of features over the inputs.","For example, features that are simpler to compute or learned first tend to be represented more strongly and densely than features that are more complex or learned later, even if all features are learned equally well.","We also explore how these biases are affected by architectures, optimizers, and training regimes (e.g., in transformers, features decoded earlier in the output sequence also tend to be represented more strongly).","Our results help to characterize the inductive biases of gradient-based representation learning.","These results also highlight a key challenge for interpretability $-$ or for comparing the representations of models and brains $-$ disentangling extraneous biases from the computationally important aspects of a system's internal representations."],"url":"http://arxiv.org/abs/2405.05847v1","category":"cs.LG"}
{"created":"2024-05-09 15:32:00","title":"Could It Be Generated? Towards Practical Analysis of Memorization in Text-To-Image Diffusion Models","abstract":"The past few years have witnessed substantial advancement in text-guided image generation powered by diffusion models. However, it was shown that text-to-image diffusion models are vulnerable to training image memorization, raising concerns on copyright infringement and privacy invasion. In this work, we perform practical analysis of memorization in text-to-image diffusion models. Targeting a set of images to protect, we conduct quantitive analysis on them without need to collect any prompts. Specifically, we first formally define the memorization of image and identify three necessary conditions of memorization, respectively similarity, existence and probability. We then reveal the correlation between the model's prediction error and image replication. Based on the correlation, we propose to utilize inversion techniques to verify the safety of target images against memorization and measure the extent to which they are memorized. Model developers can utilize our analysis method to discover memorized images or reliably claim safety against memorization. Extensive experiments on the Stable Diffusion, a popular open-source text-to-image diffusion model, demonstrate the effectiveness of our analysis method.","sentences":["The past few years have witnessed substantial advancement in text-guided image generation powered by diffusion models.","However, it was shown that text-to-image diffusion models are vulnerable to training image memorization, raising concerns on copyright infringement and privacy invasion.","In this work, we perform practical analysis of memorization in text-to-image diffusion models.","Targeting a set of images to protect, we conduct quantitive analysis on them without need to collect any prompts.","Specifically, we first formally define the memorization of image and identify three necessary conditions of memorization, respectively similarity, existence and probability.","We then reveal the correlation between the model's prediction error and image replication.","Based on the correlation, we propose to utilize inversion techniques to verify the safety of target images against memorization and measure the extent to which they are memorized.","Model developers can utilize our analysis method to discover memorized images or reliably claim safety against memorization.","Extensive experiments on the Stable Diffusion, a popular open-source text-to-image diffusion model, demonstrate the effectiveness of our analysis method."],"url":"http://arxiv.org/abs/2405.05846v1","category":"cs.CR"}
{"created":"2024-05-09 15:30:28","title":"Non-Binary Covering Codes for Low-Access Computations","abstract":"Given a real dataset and a computation family, we wish to encode and store the dataset in a distributed system so that any computation from the family can be performed by accessing a small number of nodes. In this work, we focus on the families of linear computations where the coefficients are restricted to a finite set of real values. For two-valued computations, a recent work presented a scheme that gives good feasible points on the access-redundancy tradeoff. This scheme is based on binary covering codes having a certain closure property. In a follow-up work, this scheme was extended to all finite coefficient sets, using a new additive-combinatorics notion called coefficient complexity. In the present paper, we explore non-binary covering codes and develop schemes that outperform the state-of-the-art for some coefficient sets. We provide a more general coefficient complexity definition and show its applicability to the access-redundancy tradeoff.","sentences":["Given a real dataset and a computation family, we wish to encode and store the dataset in a distributed system so that any computation from the family can be performed by accessing a small number of nodes.","In this work, we focus on the families of linear computations where the coefficients are restricted to a finite set of real values.","For two-valued computations, a recent work presented a scheme that gives good feasible points on the access-redundancy tradeoff.","This scheme is based on binary covering codes having a certain closure property.","In a follow-up work, this scheme was extended to all finite coefficient sets, using a new additive-combinatorics notion called coefficient complexity.","In the present paper, we explore non-binary covering codes and develop schemes that outperform the state-of-the-art for some coefficient sets.","We provide a more general coefficient complexity definition and show its applicability to the access-redundancy tradeoff."],"url":"http://arxiv.org/abs/2405.05845v1","category":"cs.IT"}
{"created":"2024-05-09 15:24:36","title":"The Correlated Spatial Structure of the Proton: Two-body densities as a framework for dynamical imaging","abstract":"We present results on two-parton densities in coordinate space, which capture a fuller dynamical picture of the proton's internal structure, including information on the relative position between quarks and gluons in the transverse plane. The connection of such two body densities to observables proceeds in QCD via the definition of double generalized parton distributions.","sentences":["We present results on two-parton densities in coordinate space, which capture a fuller dynamical picture of the proton's internal structure, including information on the relative position between quarks and gluons in the transverse plane.","The connection of such two body densities to observables proceeds in QCD via the definition of double generalized parton distributions."],"url":"http://arxiv.org/abs/2405.05842v1","category":"hep-ph"}
{"created":"2024-05-09 15:23:38","title":"Self-Supervised Pre-training with Symmetric Superimposition Modeling for Scene Text Recognition","abstract":"In text recognition, self-supervised pre-training emerges as a good solution to reduce dependence on expansive annotated real data. Previous studies primarily focus on local visual representation by leveraging mask image modeling or sequence contrastive learning. However, they omit modeling the linguistic information in text images, which is crucial for recognizing text. To simultaneously capture local character features and linguistic information in visual space, we propose Symmetric Superimposition Modeling (SSM). The objective of SSM is to reconstruct the direction-specific pixel and feature signals from the symmetrically superimposed input. Specifically, we add the original image with its inverted views to create the symmetrically superimposed inputs. At the pixel level, we reconstruct the original and inverted images to capture character shapes and texture-level linguistic context. At the feature level, we reconstruct the feature of the same original image and inverted image with different augmentations to model the semantic-level linguistic context and the local character discrimination. In our design, we disrupt the character shape and linguistic rules. Consequently, the dual-level reconstruction facilitates understanding character shapes and linguistic information from the perspective of visual texture and feature semantics. Experiments on various text recognition benchmarks demonstrate the effectiveness and generality of SSM, with 4.1% average performance gains and 86.6% new state-of-the-art average word accuracy on Union14M benchmarks.","sentences":["In text recognition, self-supervised pre-training emerges as a good solution to reduce dependence on expansive annotated real data.","Previous studies primarily focus on local visual representation by leveraging mask image modeling or sequence contrastive learning.","However, they omit modeling the linguistic information in text images, which is crucial for recognizing text.","To simultaneously capture local character features and linguistic information in visual space, we propose Symmetric Superimposition Modeling (SSM).","The objective of SSM is to reconstruct the direction-specific pixel and feature signals from the symmetrically superimposed input.","Specifically, we add the original image with its inverted views to create the symmetrically superimposed inputs.","At the pixel level, we reconstruct the original and inverted images to capture character shapes and texture-level linguistic context.","At the feature level, we reconstruct the feature of the same original image and inverted image with different augmentations to model the semantic-level linguistic context and the local character discrimination.","In our design, we disrupt the character shape and linguistic rules.","Consequently, the dual-level reconstruction facilitates understanding character shapes and linguistic information from the perspective of visual texture and feature semantics.","Experiments on various text recognition benchmarks demonstrate the effectiveness and generality of SSM, with 4.1% average performance gains and 86.6% new state-of-the-art average word accuracy on Union14M benchmarks."],"url":"http://arxiv.org/abs/2405.05841v1","category":"cs.CV"}
{"created":"2024-05-09 15:21:22","title":"FREmu: Power Spectrum Emulator for $f(R)$ Gravity","abstract":"To investigate gravity in the non-linear regime of cosmic structure using measurements from Stage-IV surveys, it is imperative to accurately compute large-scale structure observables, such as non-linear matter power spectra, for gravity models that extend beyond general relativity. However, the theoretical predictions of non-linear observables are typically derived from N-body simulations, which demand substantial computational resources. In this study, we introduce a novel public emulator, termed FREmu, designed to provide rapid and precise forecasts of non-linear power spectra specifically for the Hu-Sawicki $f(R)$ gravity model across scales $0.0089 h \\mathrm{Mpc}^{-1}<k<0.5 h \\mathrm{Mpc}^{-1}$ and redshifts $0<z<3$. FREmu leverages Principal Component Analysis and Artificial Neural Networks to establish a mapping from parameters to power spectra, utilizing training data derived from the Quijote-MG simulation suite. With a parameter space encompassing 7 dimensions, including $\\Omega_m$, $\\Omega_b$, $h$, $n_s$, $\\sigma_8$, $M_{\\nu}$ and $f_{R_0}$, the emulator achieves an accuracy exceeding 95% for the majority of cases, thus proving to be highly efficient for constraining parameters.","sentences":["To investigate gravity in the non-linear regime of cosmic structure using measurements from Stage-IV surveys, it is imperative to accurately compute large-scale structure observables, such as non-linear matter power spectra, for gravity models that extend beyond general relativity.","However, the theoretical predictions of non-linear observables are typically derived from N-body simulations, which demand substantial computational resources.","In this study, we introduce a novel public emulator, termed FREmu, designed to provide rapid and precise forecasts of non-linear power spectra specifically for the Hu-Sawicki $f(R)$ gravity model across scales $0.0089 h \\mathrm{Mpc}^{-1}<k<0.5 h \\mathrm{Mpc}^{-1}$ and redshifts $0<z<3$. FREmu leverages Principal Component Analysis and Artificial Neural Networks to establish a mapping from parameters to power spectra, utilizing training data derived from the Quijote-MG simulation suite.","With a parameter space encompassing 7 dimensions, including $\\Omega_m$, $\\Omega_b$, $h$, $n_s$, $\\sigma_8$, $M_{\\nu}$ and $f_{R_0}$, the emulator achieves an accuracy exceeding 95% for the majority of cases, thus proving to be highly efficient for constraining parameters."],"url":"http://arxiv.org/abs/2405.05840v1","category":"astro-ph.CO"}
{"created":"2024-05-09 15:18:41","title":"Exploring solutions to the muon g-2 anomaly in a THDM-like model under flavor constraints","abstract":"The magnetic moment of the muon can receive significant two-loop contributions from the scalar spectrum in Two Higgs Doublet Models (THDM), particularly for a light pseudoscalar. It is established that the symmetry breaking from $SU(3)_L \\times U(1)_N$ to $SU(2)_L \\times U(1)_Y$ generates an effective Type-III THDM, which invariably leads to flavor-changing neutral current (FCNC) processes. In this study, we examine whether such an effective THDM framework can account for the anomalous magnetic moment of the muon, considering the constraints imposed by $B$-meson decays, meson mixing, and invisible Higgs decays. Our principal finding reveals that a pseudoscalar with a mass of 66 GeV and a $\\tan \\beta$ value of 58 can account for the $g-2$ anomaly without conflicting with existing experimental limits.","sentences":["The magnetic moment of the muon can receive significant two-loop contributions from the scalar spectrum in Two Higgs Doublet Models (THDM), particularly for a light pseudoscalar.","It is established that the symmetry breaking from $SU(3)_L \\times U(1)_N$ to $SU(2)_L \\times U(1)_Y$ generates an effective Type-III THDM, which invariably leads to flavor-changing neutral current (FCNC) processes.","In this study, we examine whether such an effective THDM framework can account for the anomalous magnetic moment of the muon, considering the constraints imposed by $B$-meson decays, meson mixing, and invisible Higgs decays.","Our principal finding reveals that a pseudoscalar with a mass of 66 GeV and a $\\tan \\beta$ value of 58 can account for the $g-2$ anomaly without conflicting with existing experimental limits."],"url":"http://arxiv.org/abs/2405.05839v1","category":"hep-ph"}
{"created":"2024-05-09 15:15:51","title":"Role of coupled electrochemistry and stress on the Li-anode instability: A continuum approach","abstract":"We present a coupled mechanistic approach that elucidates the intricate interplay between stress and electrochemistry, enabling the prediction of the onset of instabilities in Li-metal anodes and the solid electrolyte interphase (SEI) in liquid-electrolyte Li-metal batteries. Our continuum theory considers a two-way coupling between stress and electrochemistry, includes Li and electron transport through SEI, incorporates effects of Li viscoplasticity, includes SEI and electrolyte interface surface energy and evaluates crucial roles of these mechanistic effects on the continuously evolving anode surface due to the viscoplastic deformation of lithium. In the model, spatial current density evolves with the stress-induced potential across the deformed anode/SEI interface. We assume SEI as a homogeneous, artificial layer on the Li-anode, which allows the investigation of the mechanical and electrochemical properties of the SEI systematically. Subsequently, we solve a set of coupled electrochemistry and displacement equations within the SEI and anode domains. The model is implemented numerically by writing a user element subroutine in Abaqus/Standard. We conduct numerical simulations under various galvanostatic conditions and SEI properties and predict conditions for anode instability. We find that Li viscoplasticity is one of the key attributes that drives instability in the Li-anode and show that applying a soft artificial SEI layer on the Li-anode to minimize viscoplastic deformation can be an effective method. We also report the role of artificial SEI elasticity and thickness on anode stability. Selected stability maps are provided as a design aid for artificial SEI.","sentences":["We present a coupled mechanistic approach that elucidates the intricate interplay between stress and electrochemistry, enabling the prediction of the onset of instabilities in Li-metal anodes and the solid electrolyte interphase (SEI) in liquid-electrolyte Li-metal batteries.","Our continuum theory considers a two-way coupling between stress and electrochemistry, includes Li and electron transport through SEI, incorporates effects of Li viscoplasticity, includes SEI and electrolyte interface surface energy and evaluates crucial roles of these mechanistic effects on the continuously evolving anode surface due to the viscoplastic deformation of lithium.","In the model, spatial current density evolves with the stress-induced potential across the deformed anode/SEI interface.","We assume SEI as a homogeneous, artificial layer on the Li-anode, which allows the investigation of the mechanical and electrochemical properties of the SEI systematically.","Subsequently, we solve a set of coupled electrochemistry and displacement equations within the SEI and anode domains.","The model is implemented numerically by writing a user element subroutine in Abaqus/Standard.","We conduct numerical simulations under various galvanostatic conditions and SEI properties and predict conditions for anode instability.","We find that Li viscoplasticity is one of the key attributes that drives instability in the Li-anode and show that applying a soft artificial SEI layer on the Li-anode to minimize viscoplastic deformation can be an effective method.","We also report the role of artificial SEI elasticity and thickness on anode stability.","Selected stability maps are provided as a design aid for artificial SEI."],"url":"http://arxiv.org/abs/2405.05837v1","category":"math.NA"}
{"created":"2024-05-09 15:00:39","title":"Measurement-based Verification of Quantum Markov Chains","abstract":"Model-checking techniques have been extended to analyze quantum programs and communication protocols represented as quantum Markov chains, an extension of classical Markov chains. To specify qualitative temporal properties, a subspace-based quantum temporal logic is used, which is built on Birkhoff-von Neumann atomic propositions. These propositions determine whether a quantum state is within a subspace of the entire state space. In this paper, we propose the measurement-based linear-time temporal logic MLTL to check quantitative properties. MLTL builds upon classical linear-time temporal logic (LTL) but introduces quantum atomic propositions that reason about the probability distribution after measuring a quantum state. To facilitate verification, we extend the symbolic dynamics-based techniques for stochastic matrices described by Agrawal et al. (JACM 2015) to handle more general quantum linear operators (super-operators) through eigenvalue analysis. This extension enables the development of an efficient algorithm for approximately model checking a quantum Markov chain against an MLTL formula. To demonstrate the utility of our model-checking algorithm, we use it to simultaneously verify linear-time properties of both quantum and classical random walks. Through this verification, we confirm the previously established advantages discovered by Ambainis et al. (STOC 2001) of quantum walks over classical random walks and discover new phenomena unique to quantum walks.","sentences":["Model-checking techniques have been extended to analyze quantum programs and communication protocols represented as quantum Markov chains, an extension of classical Markov chains.","To specify qualitative temporal properties, a subspace-based quantum temporal logic is used, which is built on Birkhoff-von Neumann atomic propositions.","These propositions determine whether a quantum state is within a subspace of the entire state space.","In this paper, we propose the measurement-based linear-time temporal logic MLTL to check quantitative properties.","MLTL builds upon classical linear-time temporal logic (LTL) but introduces quantum atomic propositions that reason about the probability distribution after measuring a quantum state.","To facilitate verification, we extend the symbolic dynamics-based techniques for stochastic matrices described by Agrawal et al.","(JACM 2015)","to handle more general quantum linear operators (super-operators) through eigenvalue analysis.","This extension enables the development of an efficient algorithm for approximately model checking a quantum Markov chain against an MLTL formula.","To demonstrate the utility of our model-checking algorithm, we use it to simultaneously verify linear-time properties of both quantum and classical random walks.","Through this verification, we confirm the previously established advantages discovered by Ambainis et al.","(STOC 2001) of quantum walks over classical random walks and discover new phenomena unique to quantum walks."],"url":"http://arxiv.org/abs/2405.05825v1","category":"quant-ph"}
{"created":"2024-05-09 14:57:30","title":"Equivariant formality in complex-oriented theories","abstract":"Let $G$ be a product of unitary groups and let $(M,\\omega)$ be a compact symplectic manifold with Hamiltonian $G$-action. We prove an equivariant formality result for any complex-oriented cohomology theory $\\mathbb{E}^*$ (in particular, integral cohomology). This generalizes the celebrated result of Atiyah-Bott-Kirwan for rational cohomology. The proof does not use classical ideas but instead relies on a recent cohomological splitting result of Abouzaid-McLean-Smith for Hamiltonian fibrations over $\\mathbb{CP}^1$. Moreover, we establish analogues of the \"localization\" and \"injectivity to fixed points\" theorems for certain cohomology theories studied by Hopkins-Kuhn-Ravenel. As an application of these results, we establish a Goresky-Kottwitz-MacPherson theorem with Morava $K$-theory coefficients for Hamiltonian $T$-manifolds.","sentences":["Let $G$ be a product of unitary groups and let $(M,\\omega)$ be a compact symplectic manifold with Hamiltonian $G$-action.","We prove an equivariant formality result for any complex-oriented cohomology theory $\\mathbb{E}^*$ (in particular, integral cohomology).","This generalizes the celebrated result of Atiyah-Bott-Kirwan for rational cohomology.","The proof does not use classical ideas but instead relies on a recent cohomological splitting result of Abouzaid-McLean-Smith for Hamiltonian fibrations over $\\mathbb{CP}^1$. Moreover, we establish analogues of the \"localization\" and \"injectivity to fixed points\" theorems for certain cohomology theories studied by Hopkins-Kuhn-Ravenel.","As an application of these results, we establish a Goresky-Kottwitz-MacPherson theorem with Morava $K$-theory coefficients for Hamiltonian $T$-manifolds."],"url":"http://arxiv.org/abs/2405.05821v1","category":"math.SG"}
{"created":"2024-05-09 14:56:11","title":"Semi-Autonomous Laparoscopic Robot Docking with Learned Hand-Eye Information Fusion","abstract":"In this study, we introduce a novel shared-control system for key-hole docking operations, combining a commercial camera with occlusion-robust pose estimation and a hand-eye information fusion technique. This system is used to enhance docking precision and force-compliance safety. To train a hand-eye information fusion network model, we generated a self-supervised dataset using this docking system. After training, our pose estimation method showed improved accuracy compared to traditional methods, including observation-only approaches, hand-eye calibration, and conventional state estimation filters. In real-world phantom experiments, our approach demonstrated its effectiveness with reduced position dispersion (1.23\\pm 0.81 mm vs. 2.47 \\pm 1.22 mm) and force dispersion (0.78\\pm 0.57 N vs. 1.15 \\pm 0.97 N) compared to the control group. These advancements in semi-autonomy co-manipulation scenarios enhance interaction and stability. The study presents an anti-interference, steady, and precision solution with potential applications extending beyond laparoscopic surgery to other minimally invasive procedures.","sentences":["In this study, we introduce a novel shared-control system for key-hole docking operations, combining a commercial camera with occlusion-robust pose estimation and a hand-eye information fusion technique.","This system is used to enhance docking precision and force-compliance safety.","To train a hand-eye information fusion network model, we generated a self-supervised dataset using this docking system.","After training, our pose estimation method showed improved accuracy compared to traditional methods, including observation-only approaches, hand-eye calibration, and conventional state estimation filters.","In real-world phantom experiments, our approach demonstrated its effectiveness with reduced position dispersion (1.23\\pm 0.81 mm vs. 2.47 \\pm 1.22 mm) and force dispersion (0.78\\pm 0.57 N vs. 1.15 \\pm 0.97 N) compared to the control group.","These advancements in semi-autonomy co-manipulation scenarios enhance interaction and stability.","The study presents an anti-interference, steady, and precision solution with potential applications extending beyond laparoscopic surgery to other minimally invasive procedures."],"url":"http://arxiv.org/abs/2405.05817v1","category":"cs.RO"}
{"created":"2024-05-09 14:52:32","title":"MSDiff: Multi-Scale Diffusion Model for Ultra-Sparse View CT Reconstruction","abstract":"Computed Tomography (CT) technology reduces radiation haz-ards to the human body through sparse sampling, but fewer sampling angles pose challenges for image reconstruction. Score-based generative models are widely used in sparse-view CT re-construction, performance diminishes significantly with a sharp reduction in projection angles. Therefore, we propose an ultra-sparse view CT reconstruction method utilizing multi-scale dif-fusion models (MSDiff), designed to concentrate on the global distribution of information and facilitate the reconstruction of sparse views with local image characteristics. Specifically, the proposed model ingeniously integrates information from both comprehensive sampling and selectively sparse sampling tech-niques. Through precise adjustments in diffusion model, it is capable of extracting diverse noise distribution, furthering the understanding of the overall structure of images, and aiding the fully sampled model in recovering image information more effec-tively. By leveraging the inherent correlations within the projec-tion data, we have designed an equidistant mask, enabling the model to focus its attention more effectively. Experimental re-sults demonstrated that the multi-scale model approach signifi-cantly improved the quality of image reconstruction under ultra-sparse angles, with good generalization across various datasets.","sentences":["Computed Tomography (CT) technology reduces radiation haz-ards to the human body through sparse sampling, but fewer sampling angles pose challenges for image reconstruction.","Score-based generative models are widely used in sparse-view CT re-construction, performance diminishes significantly with a sharp reduction in projection angles.","Therefore, we propose an ultra-sparse view CT reconstruction method utilizing multi-scale dif-fusion models (MSDiff), designed to concentrate on the global distribution of information and facilitate the reconstruction of sparse views with local image characteristics.","Specifically, the proposed model ingeniously integrates information from both comprehensive sampling and selectively sparse sampling tech-niques.","Through precise adjustments in diffusion model, it is capable of extracting diverse noise distribution, furthering the understanding of the overall structure of images, and aiding the fully sampled model in recovering image information more effec-tively.","By leveraging the inherent correlations within the projec-tion data, we have designed an equidistant mask, enabling the model to focus its attention more effectively.","Experimental re-sults demonstrated that the multi-scale model approach signifi-cantly improved the quality of image reconstruction under ultra-sparse angles, with good generalization across various datasets."],"url":"http://arxiv.org/abs/2405.05814v1","category":"eess.IV"}
{"created":"2024-05-09 14:50:17","title":"The $cd$-index of semi-Eulerian posets","abstract":"We generalize the definition of the $cd$-index of an Eulerian poset to the class of semi-Eulerian posets. For simplicial semi-Eulerian Buchsbaum posets, we show that all coefficients of the $cd$-index are non-negative. This proves a conjecture of Novik for odd dimensional manifolds and extends it to the even dimensional case.","sentences":["We generalize the definition of the $cd$-index of an Eulerian poset to the class of semi-Eulerian posets.","For simplicial semi-Eulerian Buchsbaum posets, we show that all coefficients of the $cd$-index are non-negative.","This proves a conjecture of Novik for odd dimensional manifolds and extends it to the even dimensional case."],"url":"http://arxiv.org/abs/2405.05812v1","category":"math.CO"}
{"created":"2024-05-09 14:49:31","title":"Series involving rational, factorial and power functions","abstract":"This is an anthology of series involving rational, factorial, and power functions expressed in terms of special functions. New finite expansions involving quotient functions expressed in terms of the Hurwitz-Lerch zeta function are given. These results represent a new form of expressing this special function as a finite series where contour integration is required for derivation. Extended series previously known and derived are extended using differential equations and algebraic methods.","sentences":["This is an anthology of series involving rational, factorial, and power functions expressed in terms of special functions.","New finite expansions involving quotient functions expressed in terms of the Hurwitz-Lerch zeta function are given.","These results represent a new form of expressing this special function as a finite series where contour integration is required for derivation.","Extended series previously known and derived are extended using differential equations and algebraic methods."],"url":"http://arxiv.org/abs/2405.05810v1","category":"math.GM"}
{"created":"2024-05-09 14:48:17","title":"Aequitas Flow: Streamlining Fair ML Experimentation","abstract":"Aequitas Flow is an open-source framework for end-to-end Fair Machine Learning (ML) experimentation in Python. This package fills the existing integration gaps in other Fair ML packages of complete and accessible experimentation. It provides a pipeline for fairness-aware model training, hyperparameter optimization, and evaluation, enabling rapid and simple experiments and result analysis. Aimed at ML practitioners and researchers, the framework offers implementations of methods, datasets, metrics, and standard interfaces for these components to improve extensibility. By facilitating the development of fair ML practices, Aequitas Flow seeks to enhance the adoption of these concepts in AI technologies.","sentences":["Aequitas Flow is an open-source framework for end-to-end Fair Machine Learning (ML) experimentation in Python.","This package fills the existing integration gaps in other Fair ML packages of complete and accessible experimentation.","It provides a pipeline for fairness-aware model training, hyperparameter optimization, and evaluation, enabling rapid and simple experiments and result analysis.","Aimed at ML practitioners and researchers, the framework offers implementations of methods, datasets, metrics, and standard interfaces for these components to improve extensibility.","By facilitating the development of fair ML practices, Aequitas Flow seeks to enhance the adoption of these concepts in AI technologies."],"url":"http://arxiv.org/abs/2405.05809v1","category":"cs.LG"}
{"created":"2024-05-09 14:42:16","title":"MasterWeaver: Taming Editability and Identity for Personalized Text-to-Image Generation","abstract":"Text-to-image (T2I) diffusion models have shown significant success in personalized text-to-image generation, which aims to generate novel images with human identities indicated by the reference images. Despite promising identity fidelity has been achieved by several tuning-free methods, they usually suffer from overfitting issues. The learned identity tends to entangle with irrelevant information, resulting in unsatisfied text controllability, especially on faces. In this work, we present MasterWeaver, a test-time tuning-free method designed to generate personalized images with both faithful identity fidelity and flexible editability. Specifically, MasterWeaver adopts an encoder to extract identity features and steers the image generation through additional introduced cross attention. To improve editability while maintaining identity fidelity, we propose an editing direction loss for training, which aligns the editing directions of our MasterWeaver with those of the original T2I model. Additionally, a face-augmented dataset is constructed to facilitate disentangled identity learning, and further improve the editability. Extensive experiments demonstrate that our MasterWeaver can not only generate personalized images with faithful identity, but also exhibit superiority in text controllability. Our code will be publicly available at https://github.com/csyxwei/MasterWeaver.","sentences":["Text-to-image (T2I) diffusion models have shown significant success in personalized text-to-image generation, which aims to generate novel images with human identities indicated by the reference images.","Despite promising identity fidelity has been achieved by several tuning-free methods, they usually suffer from overfitting issues.","The learned identity tends to entangle with irrelevant information, resulting in unsatisfied text controllability, especially on faces.","In this work, we present MasterWeaver, a test-time tuning-free method designed to generate personalized images with both faithful identity fidelity and flexible editability.","Specifically, MasterWeaver adopts an encoder to extract identity features and steers the image generation through additional introduced cross attention.","To improve editability while maintaining identity fidelity, we propose an editing direction loss for training, which aligns the editing directions of our MasterWeaver with those of the original T2I model.","Additionally, a face-augmented dataset is constructed to facilitate disentangled identity learning, and further improve the editability.","Extensive experiments demonstrate that our MasterWeaver can not only generate personalized images with faithful identity, but also exhibit superiority in text controllability.","Our code will be publicly available at https://github.com/csyxwei/MasterWeaver."],"url":"http://arxiv.org/abs/2405.05806v1","category":"cs.CV"}
{"created":"2024-05-09 14:39:03","title":"Attochaos I: The classically chaotic postcursor of high harmonic generation","abstract":"Attosecond physics provides unique insights into light-matter interaction on ultrafast time scales. Its core phenomenon, High Harmonic Generation (HHG), is often described by a classical recollision model, the simple-man or three-step model, where the atomic potential is disregarded. Many features are already well explained using this model; however, the simplicity of the model does not allow the possibility of classical chaotic motion. We show that beyond this model, classical chaotic motion does exist albeit on timescales that are generally longer than the first recollision time. Chaos is analyzed using tools from the theory of dynamical systems, such as Lyapunov exponents and stroboscopic maps. The calculations are done for a one-dimensional Coulomb potential subjected to a linearly polarized electric field.","sentences":["Attosecond physics provides unique insights into light-matter interaction on ultrafast time scales.","Its core phenomenon, High Harmonic Generation (HHG), is often described by a classical recollision model, the simple-man or three-step model, where the atomic potential is disregarded.","Many features are already well explained using this model; however, the simplicity of the model does not allow the possibility of classical chaotic motion.","We show that beyond this model, classical chaotic motion does exist albeit on timescales that are generally longer than the first recollision time.","Chaos is analyzed using tools from the theory of dynamical systems, such as Lyapunov exponents and stroboscopic maps.","The calculations are done for a one-dimensional Coulomb potential subjected to a linearly polarized electric field."],"url":"http://arxiv.org/abs/2405.05804v1","category":"physics.class-ph"}
{"created":"2024-05-09 14:38:53","title":"Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference","abstract":"Multimodal large language models (MLLMs) demand considerable computations for inference due to the extensive parameters and the additional input tokens needed for visual information representation. Herein, we introduce Visual Tokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapid inference. Our approach is inspired by two intriguing phenomena we have observed: (1) the attention sink phenomenon that is prevalent in LLMs also persists in MLLMs, suggesting that initial tokens and nearest tokens receive the majority of attention, while middle vision tokens garner minimal attention in deep layers; (2) the presence of information migration, which implies that visual information is transferred to subsequent text tokens within the first few layers of MLLMs. As per our findings, we conclude that vision tokens are not necessary in the deep layers of MLLMs. Thus, we strategically withdraw them at a certain layer, enabling only text tokens to engage in subsequent layers. To pinpoint the ideal layer for vision tokens withdrawal, we initially analyze a limited set of tiny datasets and choose the first layer that meets the Kullback-Leibler divergence criterion. Our VTW approach can cut computational overhead by over 40\\% across diverse multimodal tasks while maintaining performance. Our code is released at https://github.com/lzhxmu/VTW.","sentences":["Multimodal large language models (MLLMs) demand considerable computations for inference due to the extensive parameters and the additional input tokens needed for visual information representation.","Herein, we introduce Visual Tokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapid inference.","Our approach is inspired by two intriguing phenomena we have observed: (1) the attention sink phenomenon that is prevalent in LLMs also persists in MLLMs, suggesting that initial tokens and nearest tokens receive the majority of attention, while middle vision tokens garner minimal attention in deep layers; (2) the presence of information migration, which implies that visual information is transferred to subsequent text tokens within the first few layers of MLLMs.","As per our findings, we conclude that vision tokens are not necessary in the deep layers of MLLMs.","Thus, we strategically withdraw them at a certain layer, enabling only text tokens to engage in subsequent layers.","To pinpoint the ideal layer for vision tokens withdrawal, we initially analyze a limited set of tiny datasets and choose the first layer that meets the Kullback-Leibler divergence criterion.","Our VTW approach can cut computational overhead by over 40\\% across diverse multimodal tasks while maintaining performance.","Our code is released at https://github.com/lzhxmu/VTW."],"url":"http://arxiv.org/abs/2405.05803v1","category":"cs.CV"}
{"created":"2024-05-09 14:37:08","title":"Deploying Graph Neural Networks in Wireless Networks: A Link Stability Viewpoint","abstract":"As an emerging artificial intelligence technology, graph neural networks (GNNs) have exhibited promising performance across a wide range of graph-related applications. However, information exchanges among neighbor nodes in GNN pose new challenges in the resource-constrained scenario, especially in wireless systems. In practical wireless systems, the communication links among nodes are usually unreliable due to wireless fading and receiver noise, consequently resulting in performance degradation of GNNs. To improve the learning performance of GNNs, we aim to maximize the number of long-term average (LTA) communication links by the optimized power control under energy consumption constraints. Using the Lyapunov optimization method, we first transform the intractable long-term problem into a deterministic problem in each time slot by converting the long-term energy constraints into the objective function. In spite of this non-convex combinatorial optimization problem, we address this problem via equivalently solving a sequence of convex feasibility problems together with a greedy based solver. Simulation results demonstrate the superiority of our proposed scheme over the baselines.","sentences":["As an emerging artificial intelligence technology, graph neural networks (GNNs) have exhibited promising performance across a wide range of graph-related applications.","However, information exchanges among neighbor nodes in GNN pose new challenges in the resource-constrained scenario, especially in wireless systems.","In practical wireless systems, the communication links among nodes are usually unreliable due to wireless fading and receiver noise, consequently resulting in performance degradation of GNNs.","To improve the learning performance of GNNs, we aim to maximize the number of long-term average (LTA) communication links by the optimized power control under energy consumption constraints.","Using the Lyapunov optimization method, we first transform the intractable long-term problem into a deterministic problem in each time slot by converting the long-term energy constraints into the objective function.","In spite of this non-convex combinatorial optimization problem, we address this problem via equivalently solving a sequence of convex feasibility problems together with a greedy based solver.","Simulation results demonstrate the superiority of our proposed scheme over the baselines."],"url":"http://arxiv.org/abs/2405.05802v1","category":"cs.DC"}
{"created":"2024-05-09 14:34:05","title":"DragGaussian: Enabling Drag-style Manipulation on 3D Gaussian Representation","abstract":"User-friendly 3D object editing is a challenging task that has attracted significant attention recently. The limitations of direct 3D object editing without 2D prior knowledge have prompted increased attention towards utilizing 2D generative models for 3D editing. While existing methods like Instruct NeRF-to-NeRF offer a solution, they often lack user-friendliness, particularly due to semantic guided editing. In the realm of 3D representation, 3D Gaussian Splatting emerges as a promising approach for its efficiency and natural explicit property, facilitating precise editing tasks. Building upon these insights, we propose DragGaussian, a 3D object drag-editing framework based on 3D Gaussian Splatting, leveraging diffusion models for interactive image editing with open-vocabulary input. This framework enables users to perform drag-based editing on pre-trained 3D Gaussian object models, producing modified 2D images through multi-view consistent editing. Our contributions include the introduction of a new task, the development of DragGaussian for interactive point-based 3D editing, and comprehensive validation of its effectiveness through qualitative and quantitative experiments.","sentences":["User-friendly 3D object editing is a challenging task that has attracted significant attention recently.","The limitations of direct 3D object editing without 2D prior knowledge have prompted increased attention towards utilizing 2D generative models for 3D editing.","While existing methods like Instruct NeRF-to-NeRF offer a solution, they often lack user-friendliness, particularly due to semantic guided editing.","In the realm of 3D representation, 3D Gaussian Splatting emerges as a promising approach for its efficiency and natural explicit property, facilitating precise editing tasks.","Building upon these insights, we propose DragGaussian, a 3D object drag-editing framework based on 3D Gaussian Splatting, leveraging diffusion models for interactive image editing with open-vocabulary input.","This framework enables users to perform drag-based editing on pre-trained 3D Gaussian object models, producing modified 2D images through multi-view consistent editing.","Our contributions include the introduction of a new task, the development of DragGaussian for interactive point-based 3D editing, and comprehensive validation of its effectiveness through qualitative and quantitative experiments."],"url":"http://arxiv.org/abs/2405.05800v1","category":"cs.GR"}
{"created":"2024-05-09 14:25:25","title":"Enhancing Suicide Risk Detection on Social Media through Semi-Supervised Deep Label Smoothing","abstract":"Suicide is a prominent issue in society. Unfortunately, many people at risk for suicide do not receive the support required. Barriers to people receiving support include social stigma and lack of access to mental health care. With the popularity of social media, people have turned to online forums, such as Reddit to express their feelings and seek support. This provides the opportunity to support people with the aid of artificial intelligence. Social media posts can be classified, using text classification, to help connect people with professional help. However, these systems fail to account for the inherent uncertainty in classifying mental health conditions. Unlike other areas of healthcare, mental health conditions have no objective measurements of disease often relying on expert opinion. Thus when formulating deep learning problems involving mental health, using hard, binary labels does not accurately represent the true nature of the data. In these settings, where human experts may disagree, fuzzy or soft labels may be more appropriate. The current work introduces a novel label smoothing method which we use to capture any uncertainty within the data. We test our approach on a five-label multi-class classification problem. We show, our semi-supervised deep label smoothing method improves classification accuracy above the existing state of the art. Where existing research reports an accuracy of 43\\% on the Reddit C-SSRS dataset, using empirical experiments to evaluate our novel label smoothing method, we improve upon this existing benchmark to 52\\%. These improvements in model performance have the potential to better support those experiencing mental distress. Future work should explore the use of probabilistic methods in both natural language processing and quantifying contributions of both epistemic and aleatoric uncertainty in noisy datasets.","sentences":["Suicide is a prominent issue in society.","Unfortunately, many people at risk for suicide do not receive the support required.","Barriers to people receiving support include social stigma and lack of access to mental health care.","With the popularity of social media, people have turned to online forums, such as Reddit to express their feelings and seek support.","This provides the opportunity to support people with the aid of artificial intelligence.","Social media posts can be classified, using text classification, to help connect people with professional help.","However, these systems fail to account for the inherent uncertainty in classifying mental health conditions.","Unlike other areas of healthcare, mental health conditions have no objective measurements of disease often relying on expert opinion.","Thus when formulating deep learning problems involving mental health, using hard, binary labels does not accurately represent the true nature of the data.","In these settings, where human experts may disagree, fuzzy or soft labels may be more appropriate.","The current work introduces a novel label smoothing method which we use to capture any uncertainty within the data.","We test our approach on a five-label multi-class classification problem.","We show, our semi-supervised deep label smoothing method improves classification accuracy above the existing state of the art.","Where existing research reports an accuracy of 43\\% on the Reddit C-SSRS dataset, using empirical experiments to evaluate our novel label smoothing method, we improve upon this existing benchmark to 52\\%.","These improvements in model performance have the potential to better support those experiencing mental distress.","Future work should explore the use of probabilistic methods in both natural language processing and quantifying contributions of both epistemic and aleatoric uncertainty in noisy datasets."],"url":"http://arxiv.org/abs/2405.05795v1","category":"cs.LG"}
{"created":"2024-05-09 14:20:54","title":"Quantum vs. classical $P$-divisibility","abstract":"$P$-divisibility is a central concept in both classical and quantum non-Markovian processes; in particular, it is strictly related to the notion of information backflow. When restricted to a fixed commutative algebra generated by a complete set of orthogonal projections, any quantum dynamics naturally provides a classical stochastic process. It is indeed well known that a quantum generator gives rise to a $P$-divisible quantum dynamics if and only if all its possible classical reductions give rise to divisible classical stochastic processes. Yet, this property does not hold if one classically reduces the quantum dynamical maps instead of their generators: for a unitary dynamics, as an example, $P$-divisibility of its classical reduction is inevitably lost, which is thus, non-Markovian and exhibits information backflow. Instead, for some important classes of purely dissipative evolutions, quantum $P$-divisibility always implies classical $P$-divisibility and thus lack of information backflow both in the quantum and classical scenarios. On the contrary, for a wide class of orthogonally covariant qubit dynamics, we show that loss of classical $P$-divisibility can originate from the classical reduction of a purely dissipative $P$-divisible quantum dynamics as in the unitary case. Moreover, such an effect can be interpreted in terms of information backflow, the information coming in being stored in the coherences of the time-evolving quantum state.","sentences":["$P$-divisibility is a central concept in both classical and quantum non-Markovian processes; in particular, it is strictly related to the notion of information backflow.","When restricted to a fixed commutative algebra generated by a complete set of orthogonal projections, any quantum dynamics naturally provides a classical stochastic process.","It is indeed well known that a quantum generator gives rise to a $P$-divisible quantum dynamics if and only if all its possible classical reductions give rise to divisible classical stochastic processes.","Yet, this property does not hold if one classically reduces the quantum dynamical maps instead of their generators: for a unitary dynamics, as an example, $P$-divisibility of its classical reduction is inevitably lost, which is thus, non-Markovian and exhibits information backflow.","Instead, for some important classes of purely dissipative evolutions, quantum $P$-divisibility always implies classical $P$-divisibility and thus lack of information backflow both in the quantum and classical scenarios.","On the contrary, for a wide class of orthogonally covariant qubit dynamics, we show that loss of classical $P$-divisibility can originate from the classical reduction of a purely dissipative $P$-divisible quantum dynamics as in the unitary case.","Moreover, such an effect can be interpreted in terms of information backflow, the information coming in being stored in the coherences of the time-evolving quantum state."],"url":"http://arxiv.org/abs/2405.05794v1","category":"quant-ph"}
{"created":"2024-05-09 14:17:26","title":"RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation","abstract":"Mapping is crucial for spatial reasoning, planning and robot navigation. Existing approaches range from metric, which require precise geometry-based optimization, to purely topological, where image-as-node based graphs lack explicit object-level reasoning and interconnectivity. In this paper, we propose a novel topological representation of an environment based on \"image segments\", which are semantically meaningful and open-vocabulary queryable, conferring several advantages over previous works based on pixel-level features. Unlike 3D scene graphs, we create a purely topological graph with segments as nodes, where edges are formed by a) associating segment-level descriptors between pairs of consecutive images and b) connecting neighboring segments within an image using their pixel centroids. This unveils a \"continuous sense of a place\", defined by inter-image persistence of segments along with their intra-image neighbours. It further enables us to represent and update segment-level descriptors through neighborhood aggregation using graph convolution layers, which improves robot localization based on segment-level retrieval. Using real-world data, we show how our proposed map representation can be used to i) generate navigation plans in the form of \"hops over segments\" and ii) search for target objects using natural language queries describing spatial relations of objects. Furthermore, we quantitatively analyze data association at the segment level, which underpins inter-image connectivity during mapping and segment-level localization when revisiting the same place. Finally, we show preliminary trials on segment-level `hopping' based zero-shot real-world navigation. Project page with supplementary details: oravus.github.io/RoboHop/","sentences":["Mapping is crucial for spatial reasoning, planning and robot navigation.","Existing approaches range from metric, which require precise geometry-based optimization, to purely topological, where image-as-node based graphs lack explicit object-level reasoning and interconnectivity.","In this paper, we propose a novel topological representation of an environment based on \"image segments\", which are semantically meaningful and open-vocabulary queryable, conferring several advantages over previous works based on pixel-level features.","Unlike 3D scene graphs, we create a purely topological graph with segments as nodes, where edges are formed by a) associating segment-level descriptors between pairs of consecutive images and b) connecting neighboring segments within an image using their pixel centroids.","This unveils a \"continuous sense of a place\", defined by inter-image persistence of segments along with their intra-image neighbours.","It further enables us to represent and update segment-level descriptors through neighborhood aggregation using graph convolution layers, which improves robot localization based on segment-level retrieval.","Using real-world data, we show how our proposed map representation can be used to i) generate navigation plans in the form of \"hops over segments\" and ii) search for target objects using natural language queries describing spatial relations of objects.","Furthermore, we quantitatively analyze data association at the segment level, which underpins inter-image connectivity during mapping and segment-level localization when revisiting the same place.","Finally, we show preliminary trials on segment-level `hopping' based zero-shot real-world navigation.","Project page with supplementary details: oravus.github.io/RoboHop/"],"url":"http://arxiv.org/abs/2405.05792v1","category":"cs.RO"}
{"created":"2024-05-09 14:15:00","title":"A Robust eLORETA Technique for Localization of Brain Sources in the Presence of Forward Model Uncertainties","abstract":"In this paper, we present a robust version of the well-known exact low-resolution electromagnetic tomography (eLORETA) technique, named ReLORETA, to localize brain sources in the presence of different forward model uncertainties. Methods: We first assume that the true lead field matrix is a transformation of the existing lead field matrix distorted by uncertainties and propose an iterative approach to estimate this transformation accurately. Major sources of the forward model uncertainties, including differences in geometry, conductivity, and source space resolution between the real and simulated head models, and misaligned electrode positions, are then simulated to test the proposed method. Results: ReLORETA and eLORETA are applied to simulated focal sources in different regions of the brain and the presence of various noise levels as well as real data from a patient with focal epilepsy. The results show that ReLORETA is considerably more robust and accurate than eLORETA in all cases. Conclusion: Having successfully dealt with the forward model uncertainties, ReLORETA proved to be a promising method for real-world clinical applications. Significance: eLORETA is one of the localization techniques that could be used to study brain activity for medical applications such as determining the epileptogenic zone in patients with medically refractory epilepsy. However, the major limitation of eLORETA is sensitivity to the uncertainties in the forward model. Since this problem can substantially undermine its performance in real-world applications where the exact lead field matrix is unknown, developing a more robust method capable of dealing with these uncertainties is of significant interest.","sentences":["In this paper, we present a robust version of the well-known exact low-resolution electromagnetic tomography (eLORETA) technique, named ReLORETA, to localize brain sources in the presence of different forward model uncertainties.","Methods: We first assume that the true lead field matrix is a transformation of the existing lead field matrix distorted by uncertainties and propose an iterative approach to estimate this transformation accurately.","Major sources of the forward model uncertainties, including differences in geometry, conductivity, and source space resolution between the real and simulated head models, and misaligned electrode positions, are then simulated to test the proposed method.","Results: ReLORETA and eLORETA are applied to simulated focal sources in different regions of the brain and the presence of various noise levels as well as real data from a patient with focal epilepsy.","The results show that ReLORETA is considerably more robust and accurate than eLORETA in all cases.","Conclusion: Having successfully dealt with the forward model uncertainties, ReLORETA proved to be a promising method for real-world clinical applications.","Significance: eLORETA is one of the localization techniques that could be used to study brain activity for medical applications such as determining the epileptogenic zone in patients with medically refractory epilepsy.","However, the major limitation of eLORETA is sensitivity to the uncertainties in the forward model.","Since this problem can substantially undermine its performance in real-world applications where the exact lead field matrix is unknown, developing a more robust method capable of dealing with these uncertainties is of significant interest."],"url":"http://arxiv.org/abs/2405.05790v1","category":"cs.CE"}
{"created":"2024-05-09 14:11:20","title":"Autonomous Robotic Ultrasound System for Liver Follow-up Diagnosis: Pilot Phantom Study","abstract":"The paper introduces a novel autonomous robot ultrasound (US) system targeting liver follow-up scans for outpatients in local communities. Given a computed tomography (CT) image with specific target regions of interest, the proposed system carries out the autonomous follow-up scan in three steps: (i) initial robot contact to surface, (ii) coordinate mapping between CT image and robot, and (iii) target US scan. Utilizing 3D US-CT registration and deep learning-based segmentation networks, we can achieve precise imaging of 3D hepatic veins, facilitating accurate coordinate mapping between CT and the robot. This enables the automatic localization of follow-up targets within the CT image, allowing the robot to navigate precisely to the target's surface. Evaluation of the ultrasound phantom confirms the quality of the US-CT registration and shows the robot reliably locates the targets in repeated trials. The proposed framework holds the potential to significantly reduce time and costs for healthcare providers, clinicians, and follow-up patients, thereby addressing the increasing healthcare burden associated with chronic disease in local communities.","sentences":["The paper introduces a novel autonomous robot ultrasound (US) system targeting liver follow-up scans for outpatients in local communities.","Given a computed tomography (CT) image with specific target regions of interest, the proposed system carries out the autonomous follow-up scan in three steps: (i) initial robot contact to surface, (ii) coordinate mapping between CT image and robot, and (iii) target US scan.","Utilizing 3D US-CT registration and deep learning-based segmentation networks, we can achieve precise imaging of 3D hepatic veins, facilitating accurate coordinate mapping between CT and the robot.","This enables the automatic localization of follow-up targets within the CT image, allowing the robot to navigate precisely to the target's surface.","Evaluation of the ultrasound phantom confirms the quality of the US-CT registration and shows the robot reliably locates the targets in repeated trials.","The proposed framework holds the potential to significantly reduce time and costs for healthcare providers, clinicians, and follow-up patients, thereby addressing the increasing healthcare burden associated with chronic disease in local communities."],"url":"http://arxiv.org/abs/2405.05787v1","category":"cs.RO"}
{"created":"2024-05-09 14:09:36","title":"FusionTransNet for Smart Urban Mobility: Spatiotemporal Traffic Forecasting Through Multimodal Network Integration","abstract":"This study develops FusionTransNet, a framework designed for Origin-Destination (OD) flow predictions within smart and multimodal urban transportation systems. Urban transportation complexity arises from the spatiotemporal interactions among various traffic modes. Motivated by analyzing multimodal data from Shenzhen, a framework that can dissect complicated spatiotemporal interactions between these modes, from the microscopic local level to the macroscopic city-wide perspective, is essential. The framework contains three core components: the Intra-modal Learning Module, the Inter-modal Learning Module, and the Prediction Decoder. The Intra-modal Learning Module is designed to analyze spatial dependencies within individual transportation modes, facilitating a granular understanding of single-mode spatiotemporal dynamics. The Inter-modal Learning Module extends this analysis, integrating data across different modes to uncover cross-modal interdependencies, by breaking down the interactions at both local and global scales. Finally, the Prediction Decoder synthesizes insights from the preceding modules to generate accurate OD flow predictions, translating complex multimodal interactions into forecasts. Empirical evaluations conducted in metropolitan contexts, including Shenzhen and New York, demonstrate FusionTransNet's superior predictive accuracy compared to existing state-of-the-art methods. The implication of this study extends beyond urban transportation, as the method for transferring information across different spatiotemporal graphs at both local and global scales can be instrumental in other spatial systems, such as supply chain logistics and epidemics spreading.","sentences":["This study develops FusionTransNet, a framework designed for Origin-Destination (OD) flow predictions within smart and multimodal urban transportation systems.","Urban transportation complexity arises from the spatiotemporal interactions among various traffic modes.","Motivated by analyzing multimodal data from Shenzhen, a framework that can dissect complicated spatiotemporal interactions between these modes, from the microscopic local level to the macroscopic city-wide perspective, is essential.","The framework contains three core components: the Intra-modal Learning Module, the Inter-modal Learning Module, and the Prediction Decoder.","The Intra-modal Learning Module is designed to analyze spatial dependencies within individual transportation modes, facilitating a granular understanding of single-mode spatiotemporal dynamics.","The Inter-modal Learning Module extends this analysis, integrating data across different modes to uncover cross-modal interdependencies, by breaking down the interactions at both local and global scales.","Finally, the Prediction Decoder synthesizes insights from the preceding modules to generate accurate OD flow predictions, translating complex multimodal interactions into forecasts.","Empirical evaluations conducted in metropolitan contexts, including Shenzhen and New York, demonstrate FusionTransNet's superior predictive accuracy compared to existing state-of-the-art methods.","The implication of this study extends beyond urban transportation, as the method for transferring information across different spatiotemporal graphs at both local and global scales can be instrumental in other spatial systems, such as supply chain logistics and epidemics spreading."],"url":"http://arxiv.org/abs/2405.05786v1","category":"cs.LG"}
{"created":"2024-05-09 14:03:52","title":"Link Stealing Attacks Against Inductive Graph Neural Networks","abstract":"A graph neural network (GNN) is a type of neural network that is specifically designed to process graph-structured data. Typically, GNNs can be implemented in two settings, including the transductive setting and the inductive setting. In the transductive setting, the trained model can only predict the labels of nodes that were observed at the training time. In the inductive setting, the trained model can be generalized to new nodes/graphs. Due to its flexibility, the inductive setting is the most popular GNN setting at the moment. Previous work has shown that transductive GNNs are vulnerable to a series of privacy attacks. However, a comprehensive privacy analysis of inductive GNN models is still missing. This paper fills the gap by conducting a systematic privacy analysis of inductive GNNs through the lens of link stealing attacks, one of the most popular attacks that are specifically designed for GNNs. We propose two types of link stealing attacks, i.e., posterior-only attacks and combined attacks. We define threat models of the posterior-only attacks with respect to node topology and the combined attacks by considering combinations of posteriors, node attributes, and graph features. Extensive evaluation on six real-world datasets demonstrates that inductive GNNs leak rich information that enables link stealing attacks with advantageous properties. Even attacks with no knowledge about graph structures can be effective. We also show that our attacks are robust to different node similarities and different graph features. As a counterpart, we investigate two possible defenses and discover they are ineffective against our attacks, which calls for more effective defenses.","sentences":["A graph neural network (GNN) is a type of neural network that is specifically designed to process graph-structured data.","Typically, GNNs can be implemented in two settings, including the transductive setting and the inductive setting.","In the transductive setting, the trained model can only predict the labels of nodes that were observed at the training time.","In the inductive setting, the trained model can be generalized to new nodes/graphs.","Due to its flexibility, the inductive setting is the most popular GNN setting at the moment.","Previous work has shown that transductive GNNs are vulnerable to a series of privacy attacks.","However, a comprehensive privacy analysis of inductive GNN models is still missing.","This paper fills the gap by conducting a systematic privacy analysis of inductive GNNs through the lens of link stealing attacks, one of the most popular attacks that are specifically designed for GNNs.","We propose two types of link stealing attacks, i.e., posterior-only attacks and combined attacks.","We define threat models of the posterior-only attacks with respect to node topology and the combined attacks by considering combinations of posteriors, node attributes, and graph features.","Extensive evaluation on six real-world datasets demonstrates that inductive GNNs leak rich information that enables link stealing attacks with advantageous properties.","Even attacks with no knowledge about graph structures can be effective.","We also show that our attacks are robust to different node similarities and different graph features.","As a counterpart, we investigate two possible defenses and discover they are ineffective against our attacks, which calls for more effective defenses."],"url":"http://arxiv.org/abs/2405.05784v1","category":"cs.CR"}
{"created":"2024-05-09 13:54:22","title":"Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the S\u00e1mi Language","abstract":"S\\'ami, an indigenous language group comprising multiple languages, faces digital marginalization due to the limited availability of data and sophisticated language models designed for its linguistic intricacies. This work focuses on increasing technological participation for the S\\'ami language. We draw the attention of the ML community towards the language modeling problem of Ultra Low Resource (ULR) languages. ULR languages are those for which the amount of available textual resources is very low, and the speaker count for them is also very low. ULRLs are also not supported by mainstream Large Language Models (LLMs) like ChatGPT, due to which gathering artificial training data for them becomes even more challenging. Mainstream AI foundational model development has given less attention to this category of languages. Generally, these languages have very few speakers, making it hard to find them. However, it is important to develop foundational models for these ULR languages to promote inclusion and the tangible abilities and impact of LLMs. To this end, we have compiled the available S\\'ami language resources from the web to create a clean dataset for training language models. In order to study the behavior of modern LLM models with ULR languages (S\\'ami), we have experimented with different kinds of LLMs, mainly at the order of $\\sim$ seven billion parameters. We have also explored the effect of multilingual LLM training for ULRLs. We found that the decoder-only models under a sequential multilingual training scenario perform better than joint multilingual training, whereas multilingual training with high semantic overlap, in general, performs better than training from scratch.This is the first study on the S\\'ami language for adapting non-statistical language models that use the latest developments in the field of natural language processing (NLP).","sentences":["S\\'ami, an indigenous language group comprising multiple languages, faces digital marginalization due to the limited availability of data and sophisticated language models designed for its linguistic intricacies.","This work focuses on increasing technological participation for the S\\'ami language.","We draw the attention of the ML community towards the language modeling problem of Ultra Low Resource (ULR) languages.","ULR languages are those for which the amount of available textual resources is very low, and the speaker count for them is also very low.","ULRLs are also not supported by mainstream Large Language Models (LLMs) like ChatGPT, due to which gathering artificial training data for them becomes even more challenging.","Mainstream AI foundational model development has given less attention to this category of languages.","Generally, these languages have very few speakers, making it hard to find them.","However, it is important to develop foundational models for these ULR languages to promote inclusion and the tangible abilities and impact of LLMs.","To this end, we have compiled the available S\\'ami language resources from the web to create a clean dataset for training language models.","In order to study the behavior of modern LLM models with ULR languages (S\\'ami), we have experimented with different kinds of LLMs, mainly at the order of $\\sim$ seven billion parameters.","We have also explored the effect of multilingual LLM training for ULRLs.","We found that the decoder-only models under a sequential multilingual training scenario perform better than joint multilingual training, whereas multilingual training with high semantic overlap, in general, performs better than training from scratch.","This is the first study on the S\\'ami language for adapting non-statistical language models that use the latest developments in the field of natural language processing (NLP)."],"url":"http://arxiv.org/abs/2405.05777v1","category":"cs.CL"}
{"created":"2024-05-09 13:46:54","title":"Exploration of morphological coherence in open clusters with \"core-shell'' structure","abstract":"The study of their morphological coherence allows for a better understanding of the morphological evolution of open clusters. We employ the ellipsoid fitting method to delineate the 3D spatial structure of the sample clusters while using the morphological dislocation (MD) defined in our previous work and the ellipticity ratio (ER) of the clusters' inner and outer structures to characterize the morphological coherence of the sample clusters. The results show an inverse correlation between the ER of the sample clusters and the number of their members, indicating that sample clusters with much more elliptical external morphology than internal shape have generally a large number of members. Meanwhile, a slight shrinking of the MD of the sample clusters with their members' number may shed light on the significant role of the gravitational binding of the sample clusters in maintaining their morphological stability. Moreover, there are no correlations between the MD and ER of the sample clusters and their age. They are also not significantly correlated with the X-axis, the Y-axis, their orbital eccentricities, and the radial and vertical forces on them. However, the ER of the sample clusters displays some fluctuations in the distributions between it and the above covariates, implying that the morphologies of the sample clusters are sensitive to the external environment if sample effects are not taken into account. Finally, the analysis of the 3D spatial shapes of sample clusters with a small ER or a large ER demonstrates that the number of members lays an important foundation for forming a dense internal system for sample clusters. At the same time, the MD of the sample clusters can serve well as an indicator of their morphological stability, which is built on a certain amount of member stars.","sentences":["The study of their morphological coherence allows for a better understanding of the morphological evolution of open clusters.","We employ the ellipsoid fitting method to delineate the 3D spatial structure of the sample clusters while using the morphological dislocation (MD) defined in our previous work and the ellipticity ratio (ER) of the clusters' inner and outer structures to characterize the morphological coherence of the sample clusters.","The results show an inverse correlation between the ER of the sample clusters and the number of their members, indicating that sample clusters with much more elliptical external morphology than internal shape have generally a large number of members.","Meanwhile, a slight shrinking of the MD of the sample clusters with their members' number may shed light on the significant role of the gravitational binding of the sample clusters in maintaining their morphological stability.","Moreover, there are no correlations between the MD and ER of the sample clusters and their age.","They are also not significantly correlated with the X-axis, the Y-axis, their orbital eccentricities, and the radial and vertical forces on them.","However, the ER of the sample clusters displays some fluctuations in the distributions between it and the above covariates, implying that the morphologies of the sample clusters are sensitive to the external environment if sample effects are not taken into account.","Finally, the analysis of the 3D spatial shapes of sample clusters with a small ER or a large ER demonstrates that the number of members lays an important foundation for forming a dense internal system for sample clusters.","At the same time, the MD of the sample clusters can serve well as an indicator of their morphological stability, which is built on a certain amount of member stars."],"url":"http://arxiv.org/abs/2405.05771v1","category":"astro-ph.GA"}
{"created":"2024-05-09 13:45:12","title":"A minimal dynamical system and analog circuit for non-associative learning","abstract":"Learning in living organisms is typically associated with networks of neurons. The use of large numbers of adjustable units has also been a crucial factor in the continued success of artificial neural networks. In light of the complexity of both living and artificial neural networks, it is surprising to see that very simple organisms -- even unicellular organisms that do not possess a nervous system -- are capable of certain forms of learning. Since in these cases learning may be implemented with much simpler structures than neural networks, it is natural to ask how simple the building blocks required for basic forms of learning may be. The purpose of this study is to discuss the simplest dynamical systems that model a fundamental form of non-associative learning, habituation, and to elucidate technical implementations of such systems, which may be used to implement non-associative learning in neuromorphic computing and related applications.","sentences":["Learning in living organisms is typically associated with networks of neurons.","The use of large numbers of adjustable units has also been a crucial factor in the continued success of artificial neural networks.","In light of the complexity of both living and artificial neural networks, it is surprising to see that very simple organisms -- even unicellular organisms that do not possess a nervous system -- are capable of certain forms of learning.","Since in these cases learning may be implemented with much simpler structures than neural networks, it is natural to ask how simple the building blocks required for basic forms of learning may be.","The purpose of this study is to discuss the simplest dynamical systems that model a fundamental form of non-associative learning, habituation, and to elucidate technical implementations of such systems, which may be used to implement non-associative learning in neuromorphic computing and related applications."],"url":"http://arxiv.org/abs/2405.05770v1","category":"q-bio.NC"}
{"created":"2024-05-09 13:45:04","title":"Exploring Text-Guided Single Image Editing for Remote Sensing Images","abstract":"Artificial Intelligence Generative Content (AIGC) technologies have significantly influenced the remote sensing domain, particularly in the realm of image generation. However, remote sensing image editing, an equally vital research area, has not garnered sufficient attention. Different from text-guided editing in natural images, which relies on extensive text-image paired data for semantic correlation, the application scenarios of remote sensing image editing are often extreme, such as forest on fire, so it is difficult to obtain sufficient paired samples. At the same time, the lack of remote sensing semantics and the ambiguity of text also restrict the further application of image editing in remote sensing field. To solve above problems, this letter proposes a diffusion based method to fulfill stable and controllable remote sensing image editing with text guidance. Our method avoids the use of a large number of paired image, and can achieve good image editing results using only a single image. The quantitative evaluation system including CLIP score and subjective evaluation metrics shows that our method has better editing effect on remote sensing images than the existing image editing model.","sentences":["Artificial Intelligence Generative Content (AIGC) technologies have significantly influenced the remote sensing domain, particularly in the realm of image generation.","However, remote sensing image editing, an equally vital research area, has not garnered sufficient attention.","Different from text-guided editing in natural images, which relies on extensive text-image paired data for semantic correlation, the application scenarios of remote sensing image editing are often extreme, such as forest on fire, so it is difficult to obtain sufficient paired samples.","At the same time, the lack of remote sensing semantics and the ambiguity of text also restrict the further application of image editing in remote sensing field.","To solve above problems, this letter proposes a diffusion based method to fulfill stable and controllable remote sensing image editing with text guidance.","Our method avoids the use of a large number of paired image, and can achieve good image editing results using only a single image.","The quantitative evaluation system including CLIP score and subjective evaluation metrics shows that our method has better editing effect on remote sensing images than the existing image editing model."],"url":"http://arxiv.org/abs/2405.05769v1","category":"cs.CV"}
{"created":"2024-05-09 13:44:16","title":"FastScene: Text-Driven Fast 3D Indoor Scene Generation via Panoramic Gaussian Splatting","abstract":"Text-driven 3D indoor scene generation holds broad applications, ranging from gaming and smart homes to AR/VR applications. Fast and high-fidelity scene generation is paramount for ensuring user-friendly experiences. However, existing methods are characterized by lengthy generation processes or necessitate the intricate manual specification of motion parameters, which introduces inconvenience for users. Furthermore, these methods often rely on narrow-field viewpoint iterative generations, compromising global consistency and overall scene quality. To address these issues, we propose FastScene, a framework for fast and higher-quality 3D scene generation, while maintaining the scene consistency. Specifically, given a text prompt, we generate a panorama and estimate its depth, since the panorama encompasses information about the entire scene and exhibits explicit geometric constraints. To obtain high-quality novel views, we introduce the Coarse View Synthesis (CVS) and Progressive Novel View Inpainting (PNVI) strategies, ensuring both scene consistency and view quality. Subsequently, we utilize Multi-View Projection (MVP) to form perspective views, and apply 3D Gaussian Splatting (3DGS) for scene reconstruction. Comprehensive experiments demonstrate FastScene surpasses other methods in both generation speed and quality with better scene consistency. Notably, guided only by a text prompt, FastScene can generate a 3D scene within a mere 15 minutes, which is at least one hour faster than state-of-the-art methods, making it a paradigm for user-friendly scene generation.","sentences":["Text-driven 3D indoor scene generation holds broad applications, ranging from gaming and smart homes to AR/VR applications.","Fast and high-fidelity scene generation is paramount for ensuring user-friendly experiences.","However, existing methods are characterized by lengthy generation processes or necessitate the intricate manual specification of motion parameters, which introduces inconvenience for users.","Furthermore, these methods often rely on narrow-field viewpoint iterative generations, compromising global consistency and overall scene quality.","To address these issues, we propose FastScene, a framework for fast and higher-quality 3D scene generation, while maintaining the scene consistency.","Specifically, given a text prompt, we generate a panorama and estimate its depth, since the panorama encompasses information about the entire scene and exhibits explicit geometric constraints.","To obtain high-quality novel views, we introduce the Coarse View Synthesis (CVS) and Progressive Novel View Inpainting (PNVI) strategies, ensuring both scene consistency and view quality.","Subsequently, we utilize Multi-View Projection (MVP) to form perspective views, and apply 3D Gaussian Splatting (3DGS) for scene reconstruction.","Comprehensive experiments demonstrate FastScene surpasses other methods in both generation speed and quality with better scene consistency.","Notably, guided only by a text prompt, FastScene can generate a 3D scene within a mere 15 minutes, which is at least one hour faster than state-of-the-art methods, making it a paradigm for user-friendly scene generation."],"url":"http://arxiv.org/abs/2405.05768v1","category":"cs.CV"}
{"created":"2024-05-09 13:44:04","title":"Large Language Model-Aided Evolutionary Search for Constrained Multiobjective Optimization","abstract":"Evolutionary algorithms excel in solving complex optimization problems, especially those with multiple objectives. However, their stochastic nature can sometimes hinder rapid convergence to the global optima, particularly in scenarios involving constraints. In this study, we employ a large language model (LLM) to enhance evolutionary search for solving constrained multi-objective optimization problems. Our aim is to speed up the convergence of the evolutionary population. To achieve this, we finetune the LLM through tailored prompt engineering, integrating information concerning both objective values and constraint violations of solutions. This process enables the LLM to grasp the relationship between well-performing and poorly performing solutions based on the provided input data. Solution's quality is assessed based on their constraint violations and objective-based performance. By leveraging the refined LLM, it can be used as a search operator to generate superior-quality solutions. Experimental evaluations across various test benchmarks illustrate that LLM-aided evolutionary search can significantly accelerate the population's convergence speed and stands out competitively against cutting-edge evolutionary algorithms.","sentences":["Evolutionary algorithms excel in solving complex optimization problems, especially those with multiple objectives.","However, their stochastic nature can sometimes hinder rapid convergence to the global optima, particularly in scenarios involving constraints.","In this study, we employ a large language model (LLM) to enhance evolutionary search for solving constrained multi-objective optimization problems.","Our aim is to speed up the convergence of the evolutionary population.","To achieve this, we finetune the LLM through tailored prompt engineering, integrating information concerning both objective values and constraint violations of solutions.","This process enables the LLM to grasp the relationship between well-performing and poorly performing solutions based on the provided input data.","Solution's quality is assessed based on their constraint violations and objective-based performance.","By leveraging the refined LLM, it can be used as a search operator to generate superior-quality solutions.","Experimental evaluations across various test benchmarks illustrate that LLM-aided evolutionary search can significantly accelerate the population's convergence speed and stands out competitively against cutting-edge evolutionary algorithms."],"url":"http://arxiv.org/abs/2405.05767v1","category":"cs.NE"}
{"created":"2024-05-09 13:42:54","title":"On the mixing between flavor singlets in lattice gauge theories coupled to matter fields in multiple representations","abstract":"We provide the first extensive, numerical study of the non-trivial problem of mixing between flavor-singlet composite states emerging in strongly coupled lattice field theories with matter field content consisting of fermions transforming in different representations of the gauge group. The theory of interest is the minimal candidate for a composite Higgs model that also accommodates a mechanism for top partial compositeness: the $Sp(4)$ gauge theory coupled to two (Dirac) fermions transforming as the fundamental and three as the two-index antisymmetric representation of the gauge group, respectively. We apply an admixture of APE and Wuppertal smearings, as well as the generalized eigenvalue problem approach, to two-point functions involving flavor-singlet mesons, for ensembles having time extent longer than the space extent. We demonstrate that, in the region of lattice parameter space accessible to this study, both masses and mixing angles can be measured effectively, despite the presence of (numerically noisy) contributions from disconnected diagrams.","sentences":["We provide the first extensive, numerical study of the non-trivial problem of mixing between flavor-singlet composite states emerging in strongly coupled lattice field theories with matter field content consisting of fermions transforming in different representations of the gauge group.","The theory of interest is the minimal candidate for a composite Higgs model that also accommodates a mechanism for top partial compositeness: the $Sp(4)$ gauge theory coupled to two (Dirac) fermions transforming as the fundamental and three as the two-index antisymmetric representation of the gauge group, respectively.","We apply an admixture of APE and Wuppertal smearings, as well as the generalized eigenvalue problem approach, to two-point functions involving flavor-singlet mesons, for ensembles having time extent longer than the space extent.","We demonstrate that, in the region of lattice parameter space accessible to this study, both masses and mixing angles can be measured effectively, despite the presence of (numerically noisy) contributions from disconnected diagrams."],"url":"http://arxiv.org/abs/2405.05765v1","category":"hep-lat"}
{"created":"2024-05-09 13:42:54","title":"To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems","abstract":"The increasing reliance on Deep Learning models, combined with their inherent lack of transparency, has spurred the development of a novel field of study known as eXplainable AI (XAI) methods. These methods seek to enhance the trust of end-users in automated systems by providing insights into the rationale behind their decisions. This paper presents a novel approach for measuring user trust in XAI systems, allowing their refinement. Our proposed metric combines both performance metrics and trust indicators from an objective perspective. To validate this novel methodology, we conducted a case study in a realistic medical scenario: the usage of XAI system for the detection of pneumonia from x-ray images.","sentences":["The increasing reliance on Deep Learning models, combined with their inherent lack of transparency, has spurred the development of a novel field of study known as eXplainable AI (XAI) methods.","These methods seek to enhance the trust of end-users in automated systems by providing insights into the rationale behind their decisions.","This paper presents a novel approach for measuring user trust in XAI systems, allowing their refinement.","Our proposed metric combines both performance metrics and trust indicators from an objective perspective.","To validate this novel methodology, we conducted a case study in a realistic medical scenario: the usage of XAI system for the detection of pneumonia from x-ray images."],"url":"http://arxiv.org/abs/2405.05766v1","category":"cs.CV"}
{"created":"2024-05-09 13:37:18","title":"DP-MDM: Detail-Preserving MR Reconstruction via Multiple Diffusion Models","abstract":"Detail features of magnetic resonance images play a cru-cial role in accurate medical diagnosis and treatment, as they capture subtle changes that pose challenges for doc-tors when performing precise judgments. However, the widely utilized naive diffusion model has limitations, as it fails to accurately capture more intricate details. To en-hance the quality of MRI reconstruction, we propose a comprehensive detail-preserving reconstruction method using multiple diffusion models to extract structure and detail features in k-space domain instead of image do-main. Moreover, virtual binary modal masks are utilized to refine the range of values in k-space data through highly adaptive center windows, which allows the model to focus its attention more efficiently. Last but not least, an inverted pyramid structure is employed, where the top-down image information gradually decreases, ena-bling a cascade representation. The framework effective-ly represents multi-scale sampled data, taking into ac-count the sparsity of the inverted pyramid architecture, and utilizes cascade training data distribution to repre-sent multi-scale data. Through a step-by-step refinement approach, the method refines the approximation of de-tails. Finally, the proposed method was evaluated by con-ducting experiments on clinical and public datasets. The results demonstrate that the proposed method outper-forms other methods.","sentences":["Detail features of magnetic resonance images play a cru-cial role in accurate medical diagnosis and treatment, as they capture subtle changes that pose challenges for doc-tors when performing precise judgments.","However, the widely utilized naive diffusion model has limitations, as it fails to accurately capture more intricate details.","To en-hance the quality of MRI reconstruction, we propose a comprehensive detail-preserving reconstruction method using multiple diffusion models to extract structure and detail features in k-space domain instead of image do-main.","Moreover, virtual binary modal masks are utilized to refine the range of values in k-space data through highly adaptive center windows, which allows the model to focus its attention more efficiently.","Last but not least, an inverted pyramid structure is employed, where the top-down image information gradually decreases, ena-bling a cascade representation.","The framework effective-ly represents multi-scale sampled data, taking into ac-count the sparsity of the inverted pyramid architecture, and utilizes cascade training data distribution to repre-sent multi-scale data.","Through a step-by-step refinement approach, the method refines the approximation of de-tails.","Finally, the proposed method was evaluated by con-ducting experiments on clinical and public datasets.","The results demonstrate that the proposed method outper-forms other methods."],"url":"http://arxiv.org/abs/2405.05763v1","category":"cs.CV"}
{"created":"2024-05-09 13:36:23","title":"Introducing two improved methods for approximating radiative cooling in hydrodynamical simulations of accretion discs","abstract":"The evolution of many astrophysical systems depends strongly on the balance between heating and cooling, in particular star formation in giant molecular clouds and the evolution of young protostellar systems. Protostellar discs are susceptible to the gravitational instability, which can play a key role in their evolution and in planet formation. The strength of the instability depends on the rate at which the system loses thermal energy. To study the evolution of these systems, we require radiative cooling approximations because full radiative transfer is generally too expensive to be coupled to hydrodynamical models. Here we present two new approximate methods for computing radiative cooling that make use of the polytropic cooling approximation. This approach invokes the assumption that each parcel of gas is located within a spherical pseudo-cloud which can then be used to approximate the optical depth. The first method combines the methods introduced by Stamatellos et al. and Lombardi et al. to overcome the limitations of each method at low and high optical depths respectively. The second, the \"Modified Lombardi\" method, is specifically tailored for self-gravitating discs. This modifies the scale height estimate from the method of Lombardi et al. using the analytical scale height for a self-gravitating disc. We show that the Modified Lombardi method provides an excellent approximation for the column density in a fragmenting disc, a regime in which the existing methods fail to recover the clumps and spiral structures. We therefore recommend this improved radiative cooling method for more realistic simulations of self-gravitating discs.","sentences":["The evolution of many astrophysical systems depends strongly on the balance between heating and cooling, in particular star formation in giant molecular clouds and the evolution of young protostellar systems.","Protostellar discs are susceptible to the gravitational instability, which can play a key role in their evolution and in planet formation.","The strength of the instability depends on the rate at which the system loses thermal energy.","To study the evolution of these systems, we require radiative cooling approximations because full radiative transfer is generally too expensive to be coupled to hydrodynamical models.","Here we present two new approximate methods for computing radiative cooling that make use of the polytropic cooling approximation.","This approach invokes the assumption that each parcel of gas is located within a spherical pseudo-cloud which can then be used to approximate the optical depth.","The first method combines the methods introduced by Stamatellos et al. and Lombardi et al. to overcome the limitations of each method at low and high optical depths respectively.","The second, the \"Modified Lombardi\" method, is specifically tailored for self-gravitating discs.","This modifies the scale height estimate from the method of Lombardi et al. using the analytical scale height for a self-gravitating disc.","We show that the Modified Lombardi method provides an excellent approximation for the column density in a fragmenting disc, a regime in which the existing methods fail to recover the clumps and spiral structures.","We therefore recommend this improved radiative cooling method for more realistic simulations of self-gravitating discs."],"url":"http://arxiv.org/abs/2405.05762v1","category":"astro-ph.SR"}
{"created":"2024-05-09 13:28:07","title":"Advancing Distribution Decomposition Methods Beyond Common Supports: Applications to Racial Wealth Disparities","abstract":"I generalize state-of-the-art approaches that decompose differences in the distribution of a variable of interest between two groups into a portion explained by covariates and a residual portion. The method that I propose relaxes the overlapping supports assumption, allowing the groups being compared to not necessarily share exactly the same covariate support. I illustrate my method revisiting the black-white wealth gap in the U.S. as a function of labor income and other variables. Traditionally used decomposition methods would trim (or assign zero weight to) observations that lie outside the common covariate support region. On the other hand, by allowing all observations to contribute to the existing wealth gap, I find that otherwise trimmed observations contribute from 3% to 19% to the overall wealth gap, at different portions of the wealth distribution.","sentences":["I generalize state-of-the-art approaches that decompose differences in the distribution of a variable of interest between two groups into a portion explained by covariates and a residual portion.","The method that I propose relaxes the overlapping supports assumption, allowing the groups being compared to not necessarily share exactly the same covariate support.","I illustrate my method revisiting the black-white wealth gap in the U.S. as a function of labor income and other variables.","Traditionally used decomposition methods would trim (or assign zero weight to) observations that lie outside the common covariate support region.","On the other hand, by allowing all observations to contribute to the existing wealth gap, I find that otherwise trimmed observations contribute from 3% to 19% to the overall wealth gap, at different portions of the wealth distribution."],"url":"http://arxiv.org/abs/2405.05759v1","category":"econ.EM"}
{"created":"2024-05-09 13:27:22","title":"Exploring the Potential of Human-LLM Synergy in Advancing Qualitative Analysis: A Case Study on Mental-Illness Stigma","abstract":"Qualitative analysis is a challenging, yet crucial aspect of advancing research in the field of Human-Computer Interaction (HCI). Recent studies show that large language models (LLMs) can perform qualitative coding within existing schemes, but their potential for collaborative human-LLM discovery and new insight generation in qualitative analysis is still underexplored. To bridge this gap and advance qualitative analysis by harnessing the power of LLMs, we propose CHALET, a novel methodology that leverages the human-LLM collaboration paradigm to facilitate conceptualization and empower qualitative research. The CHALET approach involves LLM-supported data collection, performing both human and LLM deductive coding to identify disagreements, and performing collaborative inductive coding on these disagreement cases to derive new conceptual insights. We validated the effectiveness of CHALET through its application to the attribution model of mental-illness stigma, uncovering implicit stigmatization themes on cognitive, emotional and behavioral dimensions. We discuss the implications for future research, methodology, and the transdisciplinary opportunities CHALET presents for the HCI community and beyond.","sentences":["Qualitative analysis is a challenging, yet crucial aspect of advancing research in the field of Human-Computer Interaction (HCI).","Recent studies show that large language models (LLMs) can perform qualitative coding within existing schemes, but their potential for collaborative human-LLM discovery and new insight generation in qualitative analysis is still underexplored.","To bridge this gap and advance qualitative analysis by harnessing the power of LLMs, we propose CHALET, a novel methodology that leverages the human-LLM collaboration paradigm to facilitate conceptualization and empower qualitative research.","The CHALET approach involves LLM-supported data collection, performing both human and LLM deductive coding to identify disagreements, and performing collaborative inductive coding on these disagreement cases to derive new conceptual insights.","We validated the effectiveness of CHALET through its application to the attribution model of mental-illness stigma, uncovering implicit stigmatization themes on cognitive, emotional and behavioral dimensions.","We discuss the implications for future research, methodology, and the transdisciplinary opportunities CHALET presents for the HCI community and beyond."],"url":"http://arxiv.org/abs/2405.05758v1","category":"cs.HC"}
{"created":"2024-05-09 13:23:09","title":"Design and Implementation of Energy-Efficient Wireless Tire Sensing System with Delay Analysis for Intelligent Vehicles","abstract":"The growing prevalence of Internet of Things (IoT) technologies has led to a rise in the popularity of intelligent vehicles that incorporate a range of sensors to monitor various aspects, such as driving speed, fuel usage, distance proximity and tire anomalies. Nowadays, real-time tire sensing systems play important roles for intelligent vehicles in increasing mileage, reducing fuel consumption, improving driving safety, and reducing the potential for traffic accidents. However, the current tire sensing system drains a significant vehicle' energy and lacks effective collection of sensing data, which may not guarantee the immediacy of driving safety. Thus, this paper designs an energy-efficient wireless tire sensing system (WTSS), which leverages energy-saving techniques to significantly reduce power consumption while ensuring data retrieval delays during real-time monitoring. Additionally, we mathematically analyze the worst-case transmission delay and sensor reception ratio of the system to ensure the immediacy based on the collision probabilities of sensor transmissions. This system has been implemented and verified by the simulation and field train experiments. These results show that the proposed scheme provides enhanced performance in energy efficiency up to 76.5% in average and identifies the worst transmission delay accurately.","sentences":["The growing prevalence of Internet of Things (IoT) technologies has led to a rise in the popularity of intelligent vehicles that incorporate a range of sensors to monitor various aspects, such as driving speed, fuel usage, distance proximity and tire anomalies.","Nowadays, real-time tire sensing systems play important roles for intelligent vehicles in increasing mileage, reducing fuel consumption, improving driving safety, and reducing the potential for traffic accidents.","However, the current tire sensing system drains a significant vehicle' energy and lacks effective collection of sensing data, which may not guarantee the immediacy of driving safety.","Thus, this paper designs an energy-efficient wireless tire sensing system (WTSS), which leverages energy-saving techniques to significantly reduce power consumption while ensuring data retrieval delays during real-time monitoring.","Additionally, we mathematically analyze the worst-case transmission delay and sensor reception ratio of the system to ensure the immediacy based on the collision probabilities of sensor transmissions.","This system has been implemented and verified by the simulation and field train experiments.","These results show that the proposed scheme provides enhanced performance in energy efficiency up to 76.5% in average and identifies the worst transmission delay accurately."],"url":"http://arxiv.org/abs/2405.05757v1","category":"cs.ET"}
{"created":"2024-05-09 13:22:10","title":"Everything is Entangled in Quantum Mechanics: Are the Orthodox Measures Physically Meaningful?","abstract":"Even though quantum entanglement is today's most essential concept within the new technological era of quantum information processing, we do not only lack a consistent definition of this kernel notion, we are also far from understanding its physical meaning [35]. These failures have lead to many problems when attempting to provide a consistent measure or quantification of entanglement. In fact, the two main lines of contemporary research within the orthodox literature have created mazes where inconsistencies and problems are found everywhere. While the operational-instrumentalist approach has failed to explain how inequalities are able to distinguish the classical from the quantum, the geometrical approach has failed to provide a consistent meaningful account of their entropic measure. Taking distance from orthodoxy, in this work we address the quantification and measure of quantum entanglement by considering a recently presented objective-invariant definition in terms of the coding of intensive relations [21] which allows to escape the widespread relativist account of bases and factorizations [24, 25]. Going beyond the orthodox dualistic reference to \"quantum particles\" and \"clicks\" in detectors, we will argue that this new line of research is capable not only to evade the many open problems which appear within the mainstream literature, but is also able to present a consistent and coherent physical understanding of entanglement. The main conclusion of this work is that in quantum mechanics --contrary to what is generally presupposed-- all operational expressions found within the laboratory are intrinsically entangled.","sentences":["Even though quantum entanglement is today's most essential concept within the new technological era of quantum information processing, we do not only lack a consistent definition of this kernel notion, we are also far from understanding its physical meaning [35].","These failures have lead to many problems when attempting to provide a consistent measure or quantification of entanglement.","In fact, the two main lines of contemporary research within the orthodox literature have created mazes where inconsistencies and problems are found everywhere.","While the operational-instrumentalist approach has failed to explain how inequalities are able to distinguish the classical from the quantum, the geometrical approach has failed to provide a consistent meaningful account of their entropic measure.","Taking distance from orthodoxy, in this work we address the quantification and measure of quantum entanglement by considering a recently presented objective-invariant definition in terms of the coding of intensive relations","[21] which allows to escape the widespread relativist account of bases and factorizations","[24, 25].","Going beyond the orthodox dualistic reference to \"quantum particles\" and \"clicks\" in detectors, we will argue that this new line of research is capable not only to evade the many open problems which appear within the mainstream literature, but is also able to present a consistent and coherent physical understanding of entanglement.","The main conclusion of this work is that in quantum mechanics --contrary to what is generally presupposed-- all operational expressions found within the laboratory are intrinsically entangled."],"url":"http://arxiv.org/abs/2405.05756v1","category":"quant-ph"}
{"created":"2024-05-09 13:21:03","title":"CSA-Net: Channel-wise Spatially Autocorrelated Attention Networks","abstract":"In recent years, convolutional neural networks (CNNs) with channel-wise feature refining mechanisms have brought noticeable benefits to modelling channel dependencies. However, current attention paradigms fail to infer an optimal channel descriptor capable of simultaneously exploiting statistical and spatial relationships among feature maps. In this paper, to overcome this shortcoming, we present a novel channel-wise spatially autocorrelated (CSA) attention mechanism. Inspired by geographical analysis, the proposed CSA exploits the spatial relationships between channels of feature maps to produce an effective channel descriptor. To the best of our knowledge, this is the f irst time that the concept of geographical spatial analysis is utilized in deep CNNs. The proposed CSA imposes negligible learning parameters and light computational overhead to the deep model, making it a powerful yet efficient attention module of choice. We validate the effectiveness of the proposed CSA networks (CSA-Nets) through extensive experiments and analysis on ImageNet, and MS COCO benchmark datasets for image classification, object detection, and instance segmentation. The experimental results demonstrate that CSA-Nets are able to consistently achieve competitive performance and superior generalization than several state-of-the-art attention-based CNNs over different benchmark tasks and datasets.","sentences":["In recent years, convolutional neural networks (CNNs) with channel-wise feature refining mechanisms have brought noticeable benefits to modelling channel dependencies.","However, current attention paradigms fail to infer an optimal channel descriptor capable of simultaneously exploiting statistical and spatial relationships among feature maps.","In this paper, to overcome this shortcoming, we present a novel channel-wise spatially autocorrelated (CSA) attention mechanism.","Inspired by geographical analysis, the proposed CSA exploits the spatial relationships between channels of feature maps to produce an effective channel descriptor.","To the best of our knowledge, this is the f irst time that the concept of geographical spatial analysis is utilized in deep CNNs.","The proposed CSA imposes negligible learning parameters and light computational overhead to the deep model, making it a powerful yet efficient attention module of choice.","We validate the effectiveness of the proposed CSA networks (CSA-Nets) through extensive experiments and analysis on ImageNet, and MS COCO benchmark datasets for image classification, object detection, and instance segmentation.","The experimental results demonstrate that CSA-Nets are able to consistently achieve competitive performance and superior generalization than several state-of-the-art attention-based CNNs over different benchmark tasks and datasets."],"url":"http://arxiv.org/abs/2405.05755v1","category":"cs.CV"}
{"created":"2024-05-09 13:18:22","title":"Manipulating Topological Polaritons in Optomechanical Ladders","abstract":"We propose to manipulate topological polaritons in optomechanical ladders consisting of an optical Su-Schrieffer-Heeger (SSH) chain and a mechanical SSH chain connected through optomechanical (interchain) interactions. We show that the topological phase diagrams are divided into six areas by four boundaries and that there are four topological phases characterized by the Berry phases. We find that a topologically nontrivial phase of the polaritons is generated by the optomechanical interaction between the optical and mechanical SSH chains even though they are both in the topologically trivial phases. Counter-intuitively, six edge states appear in one of the topological phases with only two topological nontrivial bands, and some edge states are localized near but not at the boundaries of an open-boundary ladder. Moreover, a two-dimensional Chern insulator with higher Chern numbers is simulated by introducing proper periodical adiabatic modulations of the driving amplitude and frequency. Our work not only opens a route towards topological polaritons manipulation by optomachanical interactions, but also will exert a far-reaching influence on designing topologically protected polaritonic devices.","sentences":["We propose to manipulate topological polaritons in optomechanical ladders consisting of an optical Su-Schrieffer-Heeger (SSH) chain and a mechanical SSH chain connected through optomechanical (interchain) interactions.","We show that the topological phase diagrams are divided into six areas by four boundaries and that there are four topological phases characterized by the Berry phases.","We find that a topologically nontrivial phase of the polaritons is generated by the optomechanical interaction between the optical and mechanical SSH chains even though they are both in the topologically trivial phases.","Counter-intuitively, six edge states appear in one of the topological phases with only two topological nontrivial bands, and some edge states are localized near but not at the boundaries of an open-boundary ladder.","Moreover, a two-dimensional Chern insulator with higher Chern numbers is simulated by introducing proper periodical adiabatic modulations of the driving amplitude and frequency.","Our work not only opens a route towards topological polaritons manipulation by optomachanical interactions, but also will exert a far-reaching influence on designing topologically protected polaritonic devices."],"url":"http://arxiv.org/abs/2405.05753v1","category":"quant-ph"}
{"created":"2024-05-09 13:17:07","title":"Refinements and Extensions of Ziv's Model of Perfect Secrecy for Individual Sequences","abstract":"We refine and extend Ziv's model and results regarding perfectly secure encryption of individual sequences. According to this model, the encrypter and the legitimate decrypter share in common a secret key, not shared with the unauthorized eavesdropper, who is aware of the encryption scheme and has some prior knowledge concerning the individual plaintext source sequence. This prior knowledge, combined with the cryptogram, is harnessed by eavesdropper which implements a finite-state machine as a mechanism for accepting or rejecting attempted guesses of the source plaintext. The encryption is considered perfectly secure if the cryptogram does not provide any new information to the eavesdropper that may enhance its knowledge concerning the plaintext beyond his prior knowledge. Ziv has shown that the key rate needed for perfect secrecy is essentially lower bounded by the finite-state compressibility of the plaintext sequence, a bound which is clearly asymptotically attained by Lempel-Ziv compression followed by one-time pad encryption. In this work, we consider some more general classes of finite-state eavesdroppers and derive the respective lower bounds on the key rates needed for perfect secrecy. These bounds are tighter and more refined than Ziv's bound and they are attained by encryption schemes that are based on different universal lossless compression schemes. We also extend our findings to the case where side information is available to the eavesdropper and the legitimate decrypter, but may or may not be available to the encrypter as well.","sentences":["We refine and extend Ziv's model and results regarding perfectly secure encryption of individual sequences.","According to this model, the encrypter and the legitimate decrypter share in common a secret key, not shared with the unauthorized eavesdropper, who is aware of the encryption scheme and has some prior knowledge concerning the individual plaintext source sequence.","This prior knowledge, combined with the cryptogram, is harnessed by eavesdropper which implements a finite-state machine as a mechanism for accepting or rejecting attempted guesses of the source plaintext.","The encryption is considered perfectly secure if the cryptogram does not provide any new information to the eavesdropper that may enhance its knowledge concerning the plaintext beyond his prior knowledge.","Ziv has shown that the key rate needed for perfect secrecy is essentially lower bounded by the finite-state compressibility of the plaintext sequence, a bound which is clearly asymptotically attained by Lempel-Ziv compression followed by one-time pad encryption.","In this work, we consider some more general classes of finite-state eavesdroppers and derive the respective lower bounds on the key rates needed for perfect secrecy.","These bounds are tighter and more refined than Ziv's bound and they are attained by encryption schemes that are based on different universal lossless compression schemes.","We also extend our findings to the case where side information is available to the eavesdropper and the legitimate decrypter, but may or may not be available to the encrypter as well."],"url":"http://arxiv.org/abs/2405.05752v1","category":"cs.IT"}
{"created":"2024-05-09 13:15:40","title":"A Multi-Level Superoptimizer for Tensor Programs","abstract":"We introduce Mirage, the first multi-level superoptimizer for tensor programs. A key idea in Mirage is $\\mu$Graphs, a uniform representation of tensor programs at the kernel, thread block, and thread levels of the GPU compute hierarchy. $\\mu$Graphs enable Mirage to discover novel optimizations that combine algebraic transformations, schedule transformations, and generation of new custom kernels. To navigate the large search space, Mirage introduces a pruning technique based on abstraction that significantly reduces the search space and provides a certain optimality guarantee. To ensure that the optimized $\\mu$Graph is equivalent to the input program, Mirage introduces a probabilistic equivalence verification procedure with strong theoretical guarantees. Our evaluation shows that Mirage outperforms existing approaches by up to 3.5$\\times$ even for DNNs that are widely used and heavily optimized. Mirage is publicly available at https://github.com/mirage-project/mirage.","sentences":["We introduce Mirage, the first multi-level superoptimizer for tensor programs.","A key idea in Mirage is $\\mu$Graphs, a uniform representation of tensor programs at the kernel, thread block, and thread levels of the GPU compute hierarchy.","$\\mu$Graphs enable Mirage to discover novel optimizations that combine algebraic transformations, schedule transformations, and generation of new custom kernels.","To navigate the large search space, Mirage introduces a pruning technique based on abstraction that significantly reduces the search space and provides a certain optimality guarantee.","To ensure that the optimized $\\mu$Graph is equivalent to the input program, Mirage introduces a probabilistic equivalence verification procedure with strong theoretical guarantees.","Our evaluation shows that Mirage outperforms existing approaches by up to 3.5$\\times$ even for DNNs that are widely used and heavily optimized.","Mirage is publicly available at https://github.com/mirage-project/mirage."],"url":"http://arxiv.org/abs/2405.05751v1","category":"cs.LG"}
{"created":"2024-05-09 13:14:42","title":"Quantum Fluctuations in the Interior of Black Holes and Backreactions","abstract":"We study the propagation of the quantum field perturbations in the interior of the Schwarzschild black hole. The interior of the black hole is like an anisotropic cosmological background which expands in one extended direction while contracting in azimuthal directions. Solving the quantum mode functions approximately, we calculate the expectation values of physical quantities such as the energy density $\\langle \\rho \\rangle$ and pressure $\\langle P\\rangle$ as measured by an observer in the interior of the black hole. We employ a combination of the $\\zeta$ function regularization and dimensional regularization schemes to regularize the UV divergences in the energy momentum tensor associated to these quantum perturbations. By solving the Einstein field equations, we calculate the quantum backreactions induced in the background geometry. We speculate that the effects of quantum fluctuations in the interior of the black hole can be measured by the exterior observer.","sentences":["We study the propagation of the quantum field perturbations in the interior of the Schwarzschild black hole.","The interior of the black hole is like an anisotropic cosmological background which expands in one extended direction while contracting in azimuthal directions.","Solving the quantum mode functions approximately, we calculate the expectation values of physical quantities such as the energy density $\\langle \\rho \\rangle$ and pressure $\\langle P\\rangle$ as measured by an observer in the interior of the black hole.","We employ a combination of the $\\zeta$ function regularization and dimensional regularization schemes to regularize the UV divergences in the energy momentum tensor associated to these quantum perturbations.","By solving the Einstein field equations, we calculate the quantum backreactions induced in the background geometry.","We speculate that the effects of quantum fluctuations in the interior of the black hole can be measured by the exterior observer."],"url":"http://arxiv.org/abs/2405.05750v1","category":"gr-qc"}
{"created":"2024-05-09 13:14:06","title":"NeRFFaceSpeech: One-shot Audio-diven 3D Talking Head Synthesis via Generative Prior","abstract":"Audio-driven talking head generation is advancing from 2D to 3D content. Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to synthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based approach typically requires a large number of paired audio-visual data for each identity, thereby limiting the scalability of the method. Although there have been attempts to generate audio-driven 3D talking head animations with a single image, the results are often unsatisfactory due to insufficient information on obscured regions in the image. In this paper, we mainly focus on addressing the overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where facial animations are synthesized primarily in front-facing perspectives. We propose a novel method, NeRFFaceSpeech, which enables to produce high-quality 3D-aware talking head. Using prior knowledge of generative models combined with NeRF, our method can craft a 3D-consistent facial feature space corresponding to a single image. Our spatial synchronization method employs audio-correlated vertex dynamics of a parametric face model to transform static image features into dynamic visuals through ray deformation, ensuring realistic 3D facial motion. Moreover, we introduce LipaintNet that can replenish the lacking information in the inner-mouth area, which can not be obtained from a given single image. The network is trained in a self-supervised manner by utilizing the generative capabilities without additional data. The comprehensive experiments demonstrate the superiority of our method in generating audio-driven talking heads from a single image with enhanced 3D consistency compared to previous approaches. In addition, we introduce a quantitative way of measuring the robustness of a model against pose changes for the first time, which has been possible only qualitatively.","sentences":["Audio-driven talking head generation is advancing from 2D to 3D content.","Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to synthesize high-quality 3D talking head outputs.","Unfortunately, this NeRF-based approach typically requires a large number of paired audio-visual data for each identity, thereby limiting the scalability of the method.","Although there have been attempts to generate audio-driven 3D talking head animations with a single image, the results are often unsatisfactory due to insufficient information on obscured regions in the image.","In this paper, we mainly focus on addressing the overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where facial animations are synthesized primarily in front-facing perspectives.","We propose a novel method, NeRFFaceSpeech, which enables to produce high-quality 3D-aware talking head.","Using prior knowledge of generative models combined with NeRF, our method can craft a 3D-consistent facial feature space corresponding to a single image.","Our spatial synchronization method employs audio-correlated vertex dynamics of a parametric face model to transform static image features into dynamic visuals through ray deformation, ensuring realistic 3D facial motion.","Moreover, we introduce LipaintNet that can replenish the lacking information in the inner-mouth area, which can not be obtained from a given single image.","The network is trained in a self-supervised manner by utilizing the generative capabilities without additional data.","The comprehensive experiments demonstrate the superiority of our method in generating audio-driven talking heads from a single image with enhanced 3D consistency compared to previous approaches.","In addition, we introduce a quantitative way of measuring the robustness of a model against pose changes for the first time, which has been possible only qualitatively."],"url":"http://arxiv.org/abs/2405.05749v1","category":"cs.CV"}
{"created":"2024-05-09 13:13:34","title":"Learning to Slice Wi-Fi Networks: A State-Augmented Primal-Dual Approach","abstract":"Network slicing is a key feature in 5G/NG cellular networks that creates customized slices for different service types with various quality-of-service (QoS) requirements, which can achieve service differentiation and guarantee service-level agreement (SLA) for each service type. In Wi-Fi networks, there is limited prior work on slicing, and a potential solution is based on a multi-tenant architecture on a single access point (AP) that dedicates different channels to different slices. In this paper, we define a flexible, constrained learning framework to enable slicing in Wi-Fi networks subject to QoS requirements. We specifically propose an unsupervised learning-based network slicing method that leverages a state-augmented primal-dual algorithm, where a neural network policy is trained offline to optimize a Lagrangian function and the dual variable dynamics are updated online in the execution phase. We show that state augmentation is crucial for generating slicing decisions that meet the ergodic QoS requirements.","sentences":["Network slicing is a key feature in 5G/NG cellular networks that creates customized slices for different service types with various quality-of-service (QoS) requirements, which can achieve service differentiation and guarantee service-level agreement (SLA) for each service type.","In Wi-Fi networks, there is limited prior work on slicing, and a potential solution is based on a multi-tenant architecture on a single access point (AP) that dedicates different channels to different slices.","In this paper, we define a flexible, constrained learning framework to enable slicing in Wi-Fi networks subject to QoS requirements.","We specifically propose an unsupervised learning-based network slicing method that leverages a state-augmented primal-dual algorithm, where a neural network policy is trained offline to optimize a Lagrangian function and the dual variable dynamics are updated online in the execution phase.","We show that state augmentation is crucial for generating slicing decisions that meet the ergodic QoS requirements."],"url":"http://arxiv.org/abs/2405.05748v1","category":"eess.SP"}
{"created":"2024-05-09 13:10:54","title":"Efficient Pretraining Model based on Multi-Scale Local Visual Field Feature Reconstruction for PCB CT Image Element Segmentation","abstract":"Element segmentation is a key step in nondestructive testing of Printed Circuit Boards (PCB) based on Computed Tomography (CT) technology. In recent years, the rapid development of self-supervised pretraining technology can obtain general image features without labeled samples, and then use a small amount of labeled samples to solve downstream tasks, which has a good potential in PCB element segmentation. At present, Masked Image Modeling (MIM) pretraining model has been initially applied in PCB CT image element segmentation. However, due to the small and regular size of PCB elements such as vias, wires, and pads, the global visual field has redundancy for a single element reconstruction, which may damage the performance of the model. Based on this issue, we propose an efficient pretraining model based on multi-scale local visual field feature reconstruction for PCB CT image element segmentation (EMLR-seg). In this model, the teacher-guided MIM pretraining model is introduced into PCB CT image element segmentation for the first time, and a multi-scale local visual field extraction (MVE) module is proposed to reduce redundancy by focusing on local visual fields. At the same time, a simple 4-Transformer-blocks decoder is used. Experiments show that EMLR-seg can achieve 88.6% mIoU on the PCB CT image dataset we proposed, which exceeds 1.2% by the baseline model, and the training time is reduced by 29.6 hours, a reduction of 17.4% under the same experimental condition, which reflects the advantage of EMLR-seg in terms of performance and efficiency.","sentences":["Element segmentation is a key step in nondestructive testing of Printed Circuit Boards (PCB) based on Computed Tomography (CT) technology.","In recent years, the rapid development of self-supervised pretraining technology can obtain general image features without labeled samples, and then use a small amount of labeled samples to solve downstream tasks, which has a good potential in PCB element segmentation.","At present, Masked Image Modeling (MIM) pretraining model has been initially applied in PCB CT image element segmentation.","However, due to the small and regular size of PCB elements such as vias, wires, and pads, the global visual field has redundancy for a single element reconstruction, which may damage the performance of the model.","Based on this issue, we propose an efficient pretraining model based on multi-scale local visual field feature reconstruction for PCB CT image element segmentation (EMLR-seg).","In this model, the teacher-guided MIM pretraining model is introduced into PCB CT image element segmentation for the first time, and a multi-scale local visual field extraction (MVE) module is proposed to reduce redundancy by focusing on local visual fields.","At the same time, a simple 4-Transformer-blocks decoder is used.","Experiments show that EMLR-seg can achieve 88.6% mIoU on the PCB CT image dataset we proposed, which exceeds 1.2% by the baseline model, and the training time is reduced by 29.6 hours, a reduction of 17.4% under the same experimental condition, which reflects the advantage of EMLR-seg in terms of performance and efficiency."],"url":"http://arxiv.org/abs/2405.05745v1","category":"cs.CV"}
{"created":"2024-05-09 12:58:22","title":"Can large language models understand uncommon meanings of common words?","abstract":"Large language models (LLMs) like ChatGPT have shown significant advancements across diverse natural language understanding (NLU) tasks, including intelligent dialogue and autonomous agents. Yet, lacking widely acknowledged testing mechanisms, answering `whether LLMs are stochastic parrots or genuinely comprehend the world' remains unclear, fostering numerous studies and sparking heated debates. Prevailing research mainly focuses on surface-level NLU, neglecting fine-grained explorations. However, such explorations are crucial for understanding their unique comprehension mechanisms, aligning with human cognition, and finally enhancing LLMs' general NLU capacities. To address this gap, our study delves into LLMs' nuanced semantic comprehension capabilities, particularly regarding common words with uncommon meanings. The idea stems from foundational principles of human communication within psychology, which underscore accurate shared understandings of word semantics. Specifically, this paper presents the innovative construction of a Lexical Semantic Comprehension (LeSC) dataset with novel evaluation metrics, the first benchmark encompassing both fine-grained and cross-lingual dimensions. Introducing models of both open-source and closed-source, varied scales and architectures, our extensive empirical experiments demonstrate the inferior performance of existing models in this basic lexical-meaning understanding task. Notably, even the state-of-the-art LLMs GPT-4 and GPT-3.5 lag behind 16-year-old humans by 3.9% and 22.3%, respectively. Additionally, multiple advanced prompting techniques and retrieval-augmented generation are also introduced to help alleviate this trouble, yet limitations persist. By highlighting the above critical shortcomings, this research motivates further investigation and offers novel insights for developing more intelligent LLMs.","sentences":["Large language models (LLMs) like ChatGPT have shown significant advancements across diverse natural language understanding (NLU) tasks, including intelligent dialogue and autonomous agents.","Yet, lacking widely acknowledged testing mechanisms, answering `whether LLMs are stochastic parrots or genuinely comprehend the world' remains unclear, fostering numerous studies and sparking heated debates.","Prevailing research mainly focuses on surface-level NLU, neglecting fine-grained explorations.","However, such explorations are crucial for understanding their unique comprehension mechanisms, aligning with human cognition, and finally enhancing LLMs' general NLU capacities.","To address this gap, our study delves into LLMs' nuanced semantic comprehension capabilities, particularly regarding common words with uncommon meanings.","The idea stems from foundational principles of human communication within psychology, which underscore accurate shared understandings of word semantics.","Specifically, this paper presents the innovative construction of a Lexical Semantic Comprehension (LeSC) dataset with novel evaluation metrics, the first benchmark encompassing both fine-grained and cross-lingual dimensions.","Introducing models of both open-source and closed-source, varied scales and architectures, our extensive empirical experiments demonstrate the inferior performance of existing models in this basic lexical-meaning understanding task.","Notably, even the state-of-the-art LLMs GPT-4 and GPT-3.5 lag behind 16-year-old humans by 3.9% and 22.3%, respectively.","Additionally, multiple advanced prompting techniques and retrieval-augmented generation are also introduced to help alleviate this trouble, yet limitations persist.","By highlighting the above critical shortcomings, this research motivates further investigation and offers novel insights for developing more intelligent LLMs."],"url":"http://arxiv.org/abs/2405.05741v1","category":"cs.CL"}
{"created":"2024-05-09 12:55:32","title":"Preliminary Exploration on the Low-Pressure Ar-O2 Plasma Generated by Low-Frequency Alternating Current (AC) Power Supply","abstract":"This study reports a low-frequency alternating current (AC) power supply as a novel approach for generating low-pressure capacitively coupled Ar-O2 plasma, offering advantages in cost, compactness, and operational simplicity, which are crucial for both material science and biological applications. The effectiveness of low-frequency AC-generated plasma against traditional RF systems by examining key plasma parameters such as electron density, electron temperature, and electron energy distribution function (EEDF), are investigated. Experimental results revealed that AC power supply could effectively produce low pressure Ar-O2 plasma with comparable properties to RF systems. Most notably, the AC-generated plasma achieved a significant reduction in bacterial growth, suggesting its potential as a more economical and flexible alternative for enhancing plasma-assisted applications in sterilization and material processing.","sentences":["This study reports a low-frequency alternating current (AC) power supply as a novel approach for generating low-pressure capacitively coupled Ar-O2 plasma, offering advantages in cost, compactness, and operational simplicity, which are crucial for both material science and biological applications.","The effectiveness of low-frequency AC-generated plasma against traditional RF systems by examining key plasma parameters such as electron density, electron temperature, and electron energy distribution function (EEDF), are investigated.","Experimental results revealed that AC power supply could effectively produce low pressure Ar-O2 plasma with comparable properties to RF systems.","Most notably, the AC-generated plasma achieved a significant reduction in bacterial growth, suggesting its potential as a more economical and flexible alternative for enhancing plasma-assisted applications in sterilization and material processing."],"url":"http://arxiv.org/abs/2405.05739v1","category":"physics.plasm-ph"}
{"created":"2024-05-09 12:55:03","title":"End-to-End Generative Semantic Communication Powered by Shared Semantic Knowledge Base","abstract":"Semantic communication has drawn substantial attention as a promising paradigm to achieve effective and intelligent communications. However, efficient image semantic communication encounters challenges with a lower testing compression ratio (CR) compared to the training phase. To tackle this issue, we propose an innovative semantic knowledge base (SKB)-enabled generative semantic communication system for image classification and image generation tasks. Specifically, a lightweight SKB, comprising class-level information, is exploited to guide the semantic communication process, which enables us to transmit only the relevant indices. This approach promotes the completion of the image classification task at the source end and significantly reduces the transmission load. Meanwhile, the category-level knowledge in the SKB facilitates the image generation task by allowing controllable generation, making it possible to generate favorable images in resource-constrained scenarios. Additionally, semantic accuracy is introduced as a new metric to validate the performance of semantic transmission powered by the SKB. Evaluation results indicate that the proposed method outperforms the benchmarks and achieves superior performance with minimal transmission overhead, especially in the low SNR regime.","sentences":["Semantic communication has drawn substantial attention as a promising paradigm to achieve effective and intelligent communications.","However, efficient image semantic communication encounters challenges with a lower testing compression ratio (CR) compared to the training phase.","To tackle this issue, we propose an innovative semantic knowledge base (SKB)-enabled generative semantic communication system for image classification and image generation tasks.","Specifically, a lightweight SKB, comprising class-level information, is exploited to guide the semantic communication process, which enables us to transmit only the relevant indices.","This approach promotes the completion of the image classification task at the source end and significantly reduces the transmission load.","Meanwhile, the category-level knowledge in the SKB facilitates the image generation task by allowing controllable generation, making it possible to generate favorable images in resource-constrained scenarios.","Additionally, semantic accuracy is introduced as a new metric to validate the performance of semantic transmission powered by the SKB.","Evaluation results indicate that the proposed method outperforms the benchmarks and achieves superior performance with minimal transmission overhead, especially in the low SNR regime."],"url":"http://arxiv.org/abs/2405.05738v1","category":"cs.IT"}
{"created":"2024-05-09 12:53:14","title":"Neural Network Approach for Predicting Infrared Spectra from 3D Molecular Structure","abstract":"Accurately predicting infrared (IR) spectra in computational chemistry using ab initio methods remains a challenge. Current approaches often rely on an empirical approach or on tedious anharmonic calculations, mainly adapted to semi-rigid molecules. This limitation motivates us to explore alternative methodologies. Previous studies explored machine-learning techniques for potential and dipolar surface generation, followed by IR spectra calculation using classical molecular dynamics. However, these methods are computationally expensive and require molecule-by-molecule processing. Our article introduces a new approach to improve IR spectra prediction accuracy within a significantly reduced computing time. We developed a machine learning (ML) model to directly predict IR spectra from three-dimensional (3D) molecular structures. The spectra predicted by our model significantly outperform those from density functional theory (DFT) calculations, even after scaling. In a test set of 200 molecules, our model achieves a Spectral Information Similarity Metric of 0.92, surpassing the value achieved by DFT scaled frequencies, which is 0.57. Additionally, our model considers anharmonic effects, offering a fast alternative to laborious anharmonic calculations. Moreover, our model can be used to predict various types of spectra (Ultraviolet or Nuclear Magnetic Resonance for example) as a function of molecular structure. All it needs is a database of 3D structures and their associated spectra.","sentences":["Accurately predicting infrared (IR) spectra in computational chemistry using ab initio methods remains a challenge.","Current approaches often rely on an empirical approach or on tedious anharmonic calculations, mainly adapted to semi-rigid molecules.","This limitation motivates us to explore alternative methodologies.","Previous studies explored machine-learning techniques for potential and dipolar surface generation, followed by IR spectra calculation using classical molecular dynamics.","However, these methods are computationally expensive and require molecule-by-molecule processing.","Our article introduces a new approach to improve IR spectra prediction accuracy within a significantly reduced computing time.","We developed a machine learning (ML) model to directly predict IR spectra from three-dimensional (3D) molecular structures.","The spectra predicted by our model significantly outperform those from density functional theory (DFT) calculations, even after scaling.","In a test set of 200 molecules, our model achieves a Spectral Information Similarity Metric of 0.92, surpassing the value achieved by DFT scaled frequencies, which is 0.57.","Additionally, our model considers anharmonic effects, offering a fast alternative to laborious anharmonic calculations.","Moreover, our model can be used to predict various types of spectra (Ultraviolet or Nuclear Magnetic Resonance for example) as a function of molecular structure.","All it needs is a database of 3D structures and their associated spectra."],"url":"http://arxiv.org/abs/2405.05737v1","category":"physics.chem-ph"}
{"created":"2024-05-09 12:52:22","title":"Optimal Baseline Corrections for Off-Policy Contextual Bandits","abstract":"The off-policy learning paradigm allows for recommender systems and general ranking applications to be framed as decision-making problems, where we aim to learn decision policies that optimize an unbiased offline estimate of an online reward metric. With unbiasedness comes potentially high variance, and prevalent methods exist to reduce estimation variance. These methods typically make use of control variates, either additive (i.e., baseline corrections or doubly robust methods) or multiplicative (i.e., self-normalisation). Our work unifies these approaches by proposing a single framework built on their equivalence in learning scenarios. The foundation of our framework is the derivation of an equivalent baseline correction for all of the existing control variates. Consequently, our framework enables us to characterize the variance-optimal unbiased estimator and provide a closed-form solution for it. This optimal estimator brings significantly improved performance in both evaluation and learning, and minimizes data requirements. Empirical observations corroborate our theoretical findings.","sentences":["The off-policy learning paradigm allows for recommender systems and general ranking applications to be framed as decision-making problems, where we aim to learn decision policies that optimize an unbiased offline estimate of an online reward metric.","With unbiasedness comes potentially high variance, and prevalent methods exist to reduce estimation variance.","These methods typically make use of control variates, either additive (i.e., baseline corrections or doubly robust methods) or multiplicative (i.e., self-normalisation).","Our work unifies these approaches by proposing a single framework built on their equivalence in learning scenarios.","The foundation of our framework is the derivation of an equivalent baseline correction for all of the existing control variates.","Consequently, our framework enables us to characterize the variance-optimal unbiased estimator and provide a closed-form solution for it.","This optimal estimator brings significantly improved performance in both evaluation and learning, and minimizes data requirements.","Empirical observations corroborate our theoretical findings."],"url":"http://arxiv.org/abs/2405.05736v1","category":"cs.LG"}
{"created":"2024-05-09 12:48:29","title":"Constraints on primordial black holes from LIGO-Virgo-KAGRA O3 events","abstract":"Primordial black holes (PBH) can efficiently form black hole binaries in the early universe. We update the resulting constraints on PBH abundance using data from the third observational run (O3) of LIGO-Virgo-KAGRA. To capture a wide range of PBH scenarios, we consider a variety of mass functions, including critical collapse in the QCD epoch in the presence of non-Gaussianities. Applying hierarchical Bayesian analysis to a population binaries consisting of primordial and astrophysical black holes, we find that, in every scenario, the PBHs can make up at most $f_{\\rm PBH} \\lesssim 10^{-3}$ of dark matter in the mass range $1-200~M_\\odot$. The shape and strength of the constraints are insensitive to the type of non-Gaussianities, the modifications to the mass function during the QCD epoch, or the modelling of the astrophysical PBH population.","sentences":["Primordial black holes (PBH) can efficiently form black hole binaries in the early universe.","We update the resulting constraints on PBH abundance using data from the third observational run (O3) of LIGO-Virgo-KAGRA.","To capture a wide range of PBH scenarios, we consider a variety of mass functions, including critical collapse in the QCD epoch in the presence of non-Gaussianities.","Applying hierarchical Bayesian analysis to a population binaries consisting of primordial and astrophysical black holes, we find that, in every scenario, the PBHs can make up at most $f_{\\rm PBH} \\lesssim 10^{-3}$ of dark matter in the mass range $1-200~M_\\odot$. The shape and strength of the constraints are insensitive to the type of non-Gaussianities, the modifications to the mass function during the QCD epoch, or the modelling of the astrophysical PBH population."],"url":"http://arxiv.org/abs/2405.05732v1","category":"astro-ph.CO"}
{"created":"2024-05-09 12:48:18","title":"Robustness of the Hedgehog Skyrmion","abstract":"We investigate the radial profile function of the hedgehog Skyrmion with unit baryon number in generic EFTs (effective field theories) of pions. The analysis assumes chiral symmetry and ignores the pion mass term. The Skyrmion is always smooth, because it has no point source at the origin, and terms in the EFT with higher numbers of pion derivatives do not result in uncontrolled large corrections or singularities there. The profile varies in quite a limited way as the terms in the EFT change, and a universal profile function is proposed.","sentences":["We investigate the radial profile function of the hedgehog Skyrmion with unit baryon number in generic EFTs (effective field theories) of pions.","The analysis assumes chiral symmetry and ignores the pion mass term.","The Skyrmion is always smooth, because it has no point source at the origin, and terms in the EFT with higher numbers of pion derivatives do not result in uncontrolled large corrections or singularities there.","The profile varies in quite a limited way as the terms in the EFT change, and a universal profile function is proposed."],"url":"http://arxiv.org/abs/2405.05731v1","category":"hep-th"}
{"created":"2024-05-09 12:42:36","title":"Nonlinear vibrational spectrometer for bioapplications featuring narrowband 1-$\u03bc$m pulses and a recycled OPA pump beam","abstract":"Moving the detection wavelength in vibrational sum-frequency generation (VSFG) spectroscopy to the near-infrared (> 700 nm) can potentially enable the study of molecular interfaces absorbing in the visible and give access to buried bio-interfaces at minimal absorption, reduced scattering, and negligible autofluorescence. Here, we employ an ultra-narrow bandpass thin-film optical interference filter on 180-fs, 1.03-$\\mu$m laser pulses to generate an upconversion beam yielding a spectral resolution of 5 cm$^{-1}$ and VSFG wavelengths between 890 and 980 nm for molecular vibrations in the fingerprint region. We demonstrate that the beam rejected by the filter can be utilized for driving a supercontinuum-seeded near-infrared optical parametric amplifier serving as the front-end of a broadband LiGaS$_{2}$-based mid-infrared amplifier. Benchmark data on a phospholipid monolayer at the air-water interface acquired using the resulting VSFG spectrometer show the possibility of achieving high resolution and signal-to-noise ratio at short acquisition times. The scheme can also be utilized in other types of vibrational spectroscopy that derive their spectral resolution from bandpass-filtering of femtosecond near-infrared laser pulses, such as stimulated Raman scattering (SRS) and coherent anti-Stokes Raman scattering (CARS) spectroscopy.","sentences":["Moving the detection wavelength in vibrational sum-frequency generation (VSFG) spectroscopy to the near-infrared (> 700 nm) can potentially enable the study of molecular interfaces absorbing in the visible and give access to buried bio-interfaces at minimal absorption, reduced scattering, and negligible autofluorescence.","Here, we employ an ultra-narrow bandpass thin-film optical interference filter on 180-fs, 1.03-$\\mu$m laser pulses to generate an upconversion beam yielding a spectral resolution of 5 cm$^{-1}$ and VSFG wavelengths between 890 and 980 nm for molecular vibrations in the fingerprint region.","We demonstrate that the beam rejected by the filter can be utilized for driving a supercontinuum-seeded near-infrared optical parametric amplifier serving as the front-end of a broadband LiGaS$_{2}$-based mid-infrared amplifier.","Benchmark data on a phospholipid monolayer at the air-water interface acquired using the resulting VSFG spectrometer show the possibility of achieving high resolution and signal-to-noise ratio at short acquisition times.","The scheme can also be utilized in other types of vibrational spectroscopy that derive their spectral resolution from bandpass-filtering of femtosecond near-infrared laser pulses, such as stimulated Raman scattering (SRS) and coherent anti-Stokes Raman scattering (CARS) spectroscopy."],"url":"http://arxiv.org/abs/2405.05729v1","category":"physics.optics"}
{"created":"2024-05-09 12:35:33","title":"Computational lexical analysis of Flamenco genres","abstract":"Flamenco, recognized by UNESCO as part of the Intangible Cultural Heritage of Humanity, is a profound expression of cultural identity rooted in Andalusia, Spain. However, there is a lack of quantitative studies that help identify characteristic patterns in this long-lived music tradition. In this work, we present a computational analysis of Flamenco lyrics, employing natural language processing and machine learning to categorize over 2000 lyrics into their respective Flamenco genres, termed as $\\textit{palos}$. Using a Multinomial Naive Bayes classifier, we find that lexical variation across styles enables to accurately identify distinct $\\textit{palos}$. More importantly, from an automatic method of word usage, we obtain the semantic fields that characterize each style. Further, applying a metric that quantifies the inter-genre distance we perform a network analysis that sheds light on the relationship between Flamenco styles. Remarkably, our results suggest historical connections and $\\textit{palo}$ evolutions. Overall, our work illuminates the intricate relationships and cultural significance embedded within Flamenco lyrics, complementing previous qualitative discussions with quantitative analyses and sparking new discussions on the origin and development of traditional music genres.","sentences":["Flamenco, recognized by UNESCO as part of the Intangible Cultural Heritage of Humanity, is a profound expression of cultural identity rooted in Andalusia, Spain.","However, there is a lack of quantitative studies that help identify characteristic patterns in this long-lived music tradition.","In this work, we present a computational analysis of Flamenco lyrics, employing natural language processing and machine learning to categorize over 2000 lyrics into their respective Flamenco genres, termed as $\\textit{palos}$. Using a Multinomial Naive Bayes classifier, we find that lexical variation across styles enables to accurately identify distinct $\\textit{palos}$. More importantly, from an automatic method of word usage, we obtain the semantic fields that characterize each style.","Further, applying a metric that quantifies the inter-genre distance we perform a network analysis that sheds light on the relationship between Flamenco styles.","Remarkably, our results suggest historical connections and $\\textit{palo}$ evolutions.","Overall, our work illuminates the intricate relationships and cultural significance embedded within Flamenco lyrics, complementing previous qualitative discussions with quantitative analyses and sparking new discussions on the origin and development of traditional music genres."],"url":"http://arxiv.org/abs/2405.05723v1","category":"cs.CL"}
{"created":"2024-05-09 12:34:45","title":"A Framework of SO(3)-equivariant Non-linear Representation Learning and its Application to Electronic-Structure Hamiltonian Prediction","abstract":"We present both a theoretical and a methodological framework that addresses a critical challenge in applying deep learning to physical systems: the reconciliation of non-linear expressiveness with SO(3)-equivariance in predictions of SO(3)-equivariant quantities, such as the electronic-structure Hamiltonian. Inspired by covariant theory in physics, we address this problem by exploring the mathematical relationships between SO(3)-invariant and SO(3)-equivariant quantities and their representations. We first construct theoretical SO(3)-invariant quantities derived from the SO(3)-equivariant regression targets, and use these invariant quantities as supervisory labels to guide the learning of high-quality SO(3)-invariant features. Given that SO(3)-invariance is preserved under non-linear operations, the encoding process for invariant features can extensively utilize non-linear mappings, thereby fully capturing the non-linear patterns inherent in physical systems. Building on this foundation, we propose a gradient-based mechanism to induce SO(3)-equivariant encodings of various degrees from the learned SO(3)-invariant features. This mechanism can incorporate non-linear expressive capabilities into SO(3)-equivariant representations, while theoretically preserving their equivariant properties as we prove. Our approach offers a promising general solution to the critical dilemma between equivariance and non-linear expressiveness in deep learning methodologies. We apply our theory and method to the electronic-structure Hamiltonian prediction tasks, demonstrating state-of-the-art performance across six benchmark databases.","sentences":["We present both a theoretical and a methodological framework that addresses a critical challenge in applying deep learning to physical systems: the reconciliation of non-linear expressiveness with SO(3)-equivariance in predictions of SO(3)-equivariant quantities, such as the electronic-structure Hamiltonian.","Inspired by covariant theory in physics, we address this problem by exploring the mathematical relationships between SO(3)-invariant and SO(3)-equivariant quantities and their representations.","We first construct theoretical SO(3)-invariant quantities derived from the SO(3)-equivariant regression targets, and use these invariant quantities as supervisory labels to guide the learning of high-quality SO(3)-invariant features.","Given that SO(3)-invariance is preserved under non-linear operations, the encoding process for invariant features can extensively utilize non-linear mappings, thereby fully capturing the non-linear patterns inherent in physical systems.","Building on this foundation, we propose a gradient-based mechanism to induce SO(3)-equivariant encodings of various degrees from the learned SO(3)-invariant features.","This mechanism can incorporate non-linear expressive capabilities into SO(3)-equivariant representations, while theoretically preserving their equivariant properties as we prove.","Our approach offers a promising general solution to the critical dilemma between equivariance and non-linear expressiveness in deep learning methodologies.","We apply our theory and method to the electronic-structure Hamiltonian prediction tasks, demonstrating state-of-the-art performance across six benchmark databases."],"url":"http://arxiv.org/abs/2405.05722v1","category":"cs.LG"}
{"created":"2024-05-09 12:34:34","title":"A Newton Method for Hausdorff Approximations of the Pareto Front within Multi-objective Evolutionary Algorithms","abstract":"A common goal in evolutionary multi-objective optimization is to find suitable finite-size approximations of the Pareto front of a given multi-objective optimization problem. While many multi-objective evolutionary algorithms have proven to be very efficient in finding good Pareto front approximations, they may need quite a few resources or may even fail to obtain optimal or nearly approximations. Hereby, optimality is implicitly defined by the chosen performance indicator. In this work, we propose a set-based Newton method for Hausdorff approximations of the Pareto front to be used within multi-objective evolutionary algorithms. To this end, we first generalize the previously proposed Newton step for the performance indicator for the treatment of constrained problems for general reference sets. To approximate the target Pareto front, we propose a particular strategy for generating the reference set that utilizes the data gathered by the evolutionary algorithm during its run. Finally, we show the benefit of the Newton method as a post-processing step on several benchmark test functions and different base evolutionary algorithms.","sentences":["A common goal in evolutionary multi-objective optimization is to find suitable finite-size approximations of the Pareto front of a given multi-objective optimization problem.","While many multi-objective evolutionary algorithms have proven to be very efficient in finding good Pareto front approximations, they may need quite a few resources or may even fail to obtain optimal or nearly approximations.","Hereby, optimality is implicitly defined by the chosen performance indicator.","In this work, we propose a set-based Newton method for Hausdorff approximations of the Pareto front to be used within multi-objective evolutionary algorithms.","To this end, we first generalize the previously proposed Newton step for the performance indicator for the treatment of constrained problems for general reference sets.","To approximate the target Pareto front, we propose a particular strategy for generating the reference set that utilizes the data gathered by the evolutionary algorithm during its run.","Finally, we show the benefit of the Newton method as a post-processing step on several benchmark test functions and different base evolutionary algorithms."],"url":"http://arxiv.org/abs/2405.05721v1","category":"cs.NE"}
{"created":"2024-05-09 12:26:47","title":"A note on Jacquet modules of general linear groups","abstract":"Let F be a non-Archimedean local field. Consider G_n:= GL_n(F) and let M:= G_l * G_{n-l} be a maximal Levi subgroup of G_n. In this article, we compute the semisimplified Jacquet module of representations of G_n with respect to the maximal Levi subgroup M, belonging to a particular category of representations. Utilizing our results, we prove that the Jacquet module is multiplicity-free for a specific subcategory of representations. Our findings are based on the Zelevinsky classification.","sentences":["Let F be a non-Archimedean local field.","Consider G_n:= GL_n(F) and let M:= G_l * G_{n-l} be a maximal Levi subgroup of G_n.","In this article, we compute the semisimplified Jacquet module of representations of G_n with respect to the maximal Levi subgroup M, belonging to a particular category of representations.","Utilizing our results, we prove that the Jacquet module is multiplicity-free for a specific subcategory of representations.","Our findings are based on the Zelevinsky classification."],"url":"http://arxiv.org/abs/2405.05719v1","category":"math.RT"}
{"created":"2024-05-09 12:23:58","title":"A review on two types of sonic interfaces","abstract":"In this paper, two examples of sonic interfaces are presented. The first example shows the case of sonic interfaces as weak discontinuities in self-similar shock configurations of unsteady Euler system. The second example shows the case of sonic interfaces as regular interfaces in accelerating transonic flows governed by the steady Euler-Poisson system with self-generated electric forces. And, we discuss analytic differences of the two examples, and introduce an open problem on decelerating transonic solution to the steady Euler-Poisson system.","sentences":["In this paper, two examples of sonic interfaces are presented.","The first example shows the case of sonic interfaces as weak discontinuities in self-similar shock configurations of unsteady Euler system.","The second example shows the case of sonic interfaces as regular interfaces in accelerating transonic flows governed by the steady Euler-Poisson system with self-generated electric forces.","And, we discuss analytic differences of the two examples, and introduce an open problem on decelerating transonic solution to the steady Euler-Poisson system."],"url":"http://arxiv.org/abs/2405.05717v1","category":"math.AP"}
{"created":"2024-05-09 12:12:00","title":"Zeta-functions of Curves over Finite Fields","abstract":"Curves over finite fields are of great importance in cryptography and coding theory. Through studying their zeta-functions, we would be able to find out vital arithmetic and geometric information about them and their Jacobians, including the number of rational points on this kind of curves. In this paper, I investigate if it is possible to construct a curve over finite fields of a given genus $g$ whose zeta-function is given as a product of zeta-functions of $g$ elliptic curves, and find out alternative methods if it is not possible. Basically, I look for conditions which those $g$ elliptic curves should satisfy such that their product (of their Jacobians) is isogenous to the Jacobian of a curve of a given genus $g$. Then from this isogenous relationship I can determine the characteristic polynomial of the Frobenius endomorphism of the Jacobian of the new curve and by this characteristic polynomial I can thus determine the zeta-function of this new curve. By using the zeta-functions of curves in the form as generating functions, the number of rational points on curves can even be found out, which may lead to further researches relating to some applications in cryptography, coding theory and even information theory.","sentences":["Curves over finite fields are of great importance in cryptography and coding theory.","Through studying their zeta-functions, we would be able to find out vital arithmetic and geometric information about them and their Jacobians, including the number of rational points on this kind of curves.","In this paper, I investigate if it is possible to construct a curve over finite fields of a given genus $g$ whose zeta-function is given as a product of zeta-functions of $g$ elliptic curves, and find out alternative methods if it is not possible.","Basically, I look for conditions which those $g$ elliptic curves should satisfy such that their product (of their Jacobians) is isogenous to the Jacobian of a curve of a given genus $g$. Then from this isogenous relationship I can determine the characteristic polynomial of the Frobenius endomorphism of the Jacobian of the new curve and by this characteristic polynomial I can thus determine the zeta-function of this new curve.","By using the zeta-functions of curves in the form as generating functions, the number of rational points on curves can even be found out, which may lead to further researches relating to some applications in cryptography, coding theory and even information theory."],"url":"http://arxiv.org/abs/2405.05711v1","category":"math.NT"}
{"created":"2024-05-09 12:11:28","title":"On the applicability of Kolmogorov's theory of probability to the description of quantum phenomena. Part I","abstract":"It is a common view that von Neumann laid the foundations of a \"non-commutative probability theory\" with his axiomatization of quantum mechanics (QM). As such, it is regarded a generalization of the \"classical probability theory\" due to Kolmogorov. Outside of quantum physics, however, Kolmogorov's axioms enjoy universal applicability. This raises the question of whether quantum physics indeed requires such a generalization of our conception of probability or if von Neumann's axiomatization of QM was contingent on the absence of a general theory of probability in the 1920s.   In this work I argue in favor of the latter position. In particular, I show that for non-relativistic $N$-body quantum systems subject to a time-independent scalar potential, it is possible to construct a mathematically rigorous theory based on Kolmogorov's axioms and physically natural random variables, which reproduces central predictions of QM. The respective theories are distinct, so that an empirical comparison may be possible. Moreover, the approach can in principle be adapted to other classes of quantum-mechanical models.   Part II of this series will address the projection postulate and the question of measurement in this approach.","sentences":["It is a common view that von Neumann laid the foundations of a \"non-commutative probability theory\" with his axiomatization of quantum mechanics (QM).","As such, it is regarded a generalization of the \"classical probability theory\" due to Kolmogorov.","Outside of quantum physics, however, Kolmogorov's axioms enjoy universal applicability.","This raises the question of whether quantum physics indeed requires such a generalization of our conception of probability or if von Neumann's axiomatization of QM was contingent on the absence of a general theory of probability in the 1920s.   ","In this work I argue in favor of the latter position.","In particular, I show that for non-relativistic $N$-body quantum systems subject to a time-independent scalar potential, it is possible to construct a mathematically rigorous theory based on Kolmogorov's axioms and physically natural random variables, which reproduces central predictions of QM.","The respective theories are distinct, so that an empirical comparison may be possible.","Moreover, the approach can in principle be adapted to other classes of quantum-mechanical models.   ","Part II of this series will address the projection postulate and the question of measurement in this approach."],"url":"http://arxiv.org/abs/2405.05710v1","category":"quant-ph"}
{"created":"2024-05-09 12:06:06","title":"LatentColorization: Latent Diffusion-Based Speaker Video Colorization","abstract":"While current research predominantly focuses on image-based colorization, the domain of video-based colorization remains relatively unexplored. Most existing video colorization techniques operate on a frame-by-frame basis, often overlooking the critical aspect of temporal coherence between successive frames. This approach can result in inconsistencies across frames, leading to undesirable effects like flickering or abrupt color transitions between frames. To address these challenges, we harness the generative capabilities of a fine-tuned latent diffusion model designed specifically for video colorization, introducing a novel solution for achieving temporal consistency in video colorization, as well as demonstrating strong improvements on established image quality metrics compared to other existing methods. Furthermore, we perform a subjective study, where users preferred our approach to the existing state of the art. Our dataset encompasses a combination of conventional datasets and videos from television/movies. In short, by leveraging the power of a fine-tuned latent diffusion-based colorization system with a temporal consistency mechanism, we can improve the performance of automatic video colorization by addressing the challenges of temporal inconsistency. A short demonstration of our results can be seen in some example videos available at https://youtu.be/vDbzsZdFuxM.","sentences":["While current research predominantly focuses on image-based colorization, the domain of video-based colorization remains relatively unexplored.","Most existing video colorization techniques operate on a frame-by-frame basis, often overlooking the critical aspect of temporal coherence between successive frames.","This approach can result in inconsistencies across frames, leading to undesirable effects like flickering or abrupt color transitions between frames.","To address these challenges, we harness the generative capabilities of a fine-tuned latent diffusion model designed specifically for video colorization, introducing a novel solution for achieving temporal consistency in video colorization, as well as demonstrating strong improvements on established image quality metrics compared to other existing methods.","Furthermore, we perform a subjective study, where users preferred our approach to the existing state of the art.","Our dataset encompasses a combination of conventional datasets and videos from television/movies.","In short, by leveraging the power of a fine-tuned latent diffusion-based colorization system with a temporal consistency mechanism, we can improve the performance of automatic video colorization by addressing the challenges of temporal inconsistency.","A short demonstration of our results can be seen in some example videos available at https://youtu.be/vDbzsZdFuxM."],"url":"http://arxiv.org/abs/2405.05707v1","category":"cs.CV"}
{"created":"2024-05-09 12:03:53","title":"An Observation on the Beta Functions in Quadratic Gravity","abstract":"We study the beta functions for the dimensionless couplings in quadratic curvature gravity, and find that there is a simple argument to restrict the possible form of the beta functions as derived from the counterterms at an arbitrary loop. The relation to the recent different results on beta functions is also commented on.","sentences":["We study the beta functions for the dimensionless couplings in quadratic curvature gravity, and find that there is a simple argument to restrict the possible form of the beta functions as derived from the counterterms at an arbitrary loop.","The relation to the recent different results on beta functions is also commented on."],"url":"http://arxiv.org/abs/2405.05706v1","category":"hep-th"}
{"created":"2024-05-09 12:01:11","title":"Ising's roots and the transfer-matrix eigenvalues","abstract":"Today, the Ising model is an archetype describing collective ordering processes. And, as such, it is widely known in physics and far beyond. Less known is the fact that the thesis defended by Ernst Ising 100 years ago (in 1924) contained not only the solution of what we call now the `classical 1D Ising model' but also other problems. Some of these problems, as well as the method of their solution, are the subject of this note. In particular, we discuss the combinatorial method Ernst Ising used to calculate the partition function for a chain of elementary magnets. In the thermodynamic limit, this method leads to the result that the partition function is given by the roots of a certain polynomial. We explicitly show that `Ising's roots' that arise within the combinatorial treatment are also recovered by the eigenvalues of the transfer matrix, a concept that was introduced much later. Moreover, we discuss the generalization of the two-state model to a three-state one presented in Ising's thesis, but not included in his famous paper of 1925 ( \\i E. Ising, Z. Physik {\\bf 31} (1925) 253}). The latter model can be considered as a forerunner of the now abundant models with many-component order parameters.","sentences":["Today, the Ising model is an archetype describing collective ordering processes.","And, as such, it is widely known in physics and far beyond.","Less known is the fact that the thesis defended by Ernst Ising 100 years ago (in 1924) contained not only the solution of what we call now the `classical 1D Ising model' but also other problems.","Some of these problems, as well as the method of their solution, are the subject of this note.","In particular, we discuss the combinatorial method Ernst Ising used to calculate the partition function for a chain of elementary magnets.","In the thermodynamic limit, this method leads to the result that the partition function is given by the roots of a certain polynomial.","We explicitly show that `Ising's roots' that arise within the combinatorial treatment are also recovered by the eigenvalues of the transfer matrix, a concept that was introduced much later.","Moreover, we discuss the generalization of the two-state model to a three-state one presented in Ising's thesis, but not included in his famous paper of 1925 ( \\i E. Ising, Z. Physik {\\bf 31} (1925) 253}).","The latter model can be considered as a forerunner of the now abundant models with many-component order parameters."],"url":"http://arxiv.org/abs/2405.05703v1","category":"physics.hist-ph"}
{"created":"2024-05-09 11:57:42","title":"NGM-SLAM: Gaussian Splatting SLAM with Radiance Field Submap","abstract":"Gaussian Splatting has garnered widespread attention due to its exceptional performance. Consequently, SLAM systems based on Gaussian Splatting have emerged, leveraging its capabilities for rapid real-time rendering and high-fidelity mapping. However, current Gaussian Splatting SLAM systems usually struggle with large scene representation and lack effective loop closure adjustments and scene generalization capabilities. To address these issues, we introduce NGM-SLAM, the first GS-SLAM system that utilizes neural radiance field submaps for progressive scene expression, effectively integrating the strengths of neural radiance fields and 3D Gaussian Splatting. We have developed neural implicit submaps as supervision and achieve high-quality scene expression and online loop closure adjustments through Gaussian rendering of fused submaps. Our results on multiple real-world scenes and large-scale scene datasets demonstrate that our method can achieve accurate gap filling and high-quality scene expression, supporting both monocular, stereo, and RGB-D inputs, and achieving state-of-the-art scene reconstruction and tracking performance.","sentences":["Gaussian Splatting has garnered widespread attention due to its exceptional performance.","Consequently, SLAM systems based on Gaussian Splatting have emerged, leveraging its capabilities for rapid real-time rendering and high-fidelity mapping.","However, current Gaussian Splatting SLAM systems usually struggle with large scene representation and lack effective loop closure adjustments and scene generalization capabilities.","To address these issues, we introduce NGM-SLAM, the first GS-SLAM system that utilizes neural radiance field submaps for progressive scene expression, effectively integrating the strengths of neural radiance fields and 3D Gaussian Splatting.","We have developed neural implicit submaps as supervision and achieve high-quality scene expression and online loop closure adjustments through Gaussian rendering of fused submaps.","Our results on multiple real-world scenes and large-scale scene datasets demonstrate that our method can achieve accurate gap filling and high-quality scene expression, supporting both monocular, stereo, and RGB-D inputs, and achieving state-of-the-art scene reconstruction and tracking performance."],"url":"http://arxiv.org/abs/2405.05702v1","category":"cs.RO"}
{"created":"2024-05-09 11:54:10","title":"Detecting environmental effects in gravitational waves from binaries perturbed by periodic forces","abstract":"We study the gravitational wave (GW) emission of sources perturbed by periodic dynamical forces which do not cause secular evolution in the orbital elements. We construct a corresponding post-Newtonian waveform model and provide estimates for the detectability of the resulting GW phase perturbations, for both space-based and future ground-based detectors. We validate our results by performing a set of Bayesian parameter recovery experiments with post-Newtonian waveforms. We find that, in stark contrast to the more commonly studied secular dephasing, periodic phase perturbations do not suffer from degeneracies with any of the tested vacuum binary parameters. We discuss the applications of our findings to a range of possible astrophysical scenarios, finding that such periodic perturbations may be detectable for massive black hole binaries embedded in circum-binary discs, extreme mass-ratio inspirals in accretion discs, as well as stellar-mass compact objects perturbed by tidal fields. We argue that modelling conservative sub-orbital dynamics opens up a promising new avenue to detect environmental effects in binary sources of GWs that should be included in state-of-the-art waveform templates.","sentences":["We study the gravitational wave (GW) emission of sources perturbed by periodic dynamical forces which do not cause secular evolution in the orbital elements.","We construct a corresponding post-Newtonian waveform model and provide estimates for the detectability of the resulting GW phase perturbations, for both space-based and future ground-based detectors.","We validate our results by performing a set of Bayesian parameter recovery experiments with post-Newtonian waveforms.","We find that, in stark contrast to the more commonly studied secular dephasing, periodic phase perturbations do not suffer from degeneracies with any of the tested vacuum binary parameters.","We discuss the applications of our findings to a range of possible astrophysical scenarios, finding that such periodic perturbations may be detectable for massive black hole binaries embedded in circum-binary discs, extreme mass-ratio inspirals in accretion discs, as well as stellar-mass compact objects perturbed by tidal fields.","We argue that modelling conservative sub-orbital dynamics opens up a promising new avenue to detect environmental effects in binary sources of GWs that should be included in state-of-the-art waveform templates."],"url":"http://arxiv.org/abs/2405.05698v1","category":"gr-qc"}
{"created":"2024-05-09 11:50:19","title":"Aux-NAS: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost","abstract":"We aim at exploiting additional auxiliary labels from an independent (auxiliary) task to boost the primary task performance which we focus on, while preserving a single task inference cost of the primary task. While most existing auxiliary learning methods are optimization-based relying on loss weights/gradients manipulation, our method is architecture-based with a flexible asymmetric structure for the primary and auxiliary tasks, which produces different networks for training and inference. Specifically, starting from two single task networks/branches (each representing a task), we propose a novel method with evolving networks where only primary-to-auxiliary links exist as the cross-task connections after convergence. These connections can be removed during the primary task inference, resulting in a single-task inference cost. We achieve this by formulating a Neural Architecture Search (NAS) problem, where we initialize bi-directional connections in the search space and guide the NAS optimization converging to an architecture with only the single-side primary-to-auxiliary connections. Moreover, our method can be incorporated with optimization-based auxiliary learning approaches. Extensive experiments with six tasks on NYU v2, CityScapes, and Taskonomy datasets using VGG, ResNet, and ViT backbones validate the promising performance. The codes are available at https://github.com/ethanygao/Aux-NAS.","sentences":["We aim at exploiting additional auxiliary labels from an independent (auxiliary) task to boost the primary task performance which we focus on, while preserving a single task inference cost of the primary task.","While most existing auxiliary learning methods are optimization-based relying on loss weights/gradients manipulation, our method is architecture-based with a flexible asymmetric structure for the primary and auxiliary tasks, which produces different networks for training and inference.","Specifically, starting from two single task networks/branches (each representing a task), we propose a novel method with evolving networks where only primary-to-auxiliary links exist as the cross-task connections after convergence.","These connections can be removed during the primary task inference, resulting in a single-task inference cost.","We achieve this by formulating a Neural Architecture Search (NAS) problem, where we initialize bi-directional connections in the search space and guide the NAS optimization converging to an architecture with only the single-side primary-to-auxiliary connections.","Moreover, our method can be incorporated with optimization-based auxiliary learning approaches.","Extensive experiments with six tasks on NYU v2, CityScapes, and Taskonomy datasets using VGG, ResNet, and ViT backbones validate the promising performance.","The codes are available at https://github.com/ethanygao/Aux-NAS."],"url":"http://arxiv.org/abs/2405.05695v1","category":"cs.LG"}
{"created":"2024-05-09 11:45:21","title":"Meta Algebras and Biorthogonal Rational Functions: The Hahn Case","abstract":"The finite families of biorthogonal rational functions and orthogonal polynomials of Hahn type are interpreted algebraically in a unified way by considering the three-generated meta Hahn algebra and its finite-dimensional representations. The functions of interest arise as overlaps between eigensolutions of generalized and ordinary eigenvalue problems on the representation space. The orthogonality relations and bispectral properties naturally follow from the framework.","sentences":["The finite families of biorthogonal rational functions and orthogonal polynomials of Hahn type are interpreted algebraically in a unified way by considering the three-generated meta Hahn algebra and its finite-dimensional representations.","The functions of interest arise as overlaps between eigensolutions of generalized and ordinary eigenvalue problems on the representation space.","The orthogonality relations and bispectral properties naturally follow from the framework."],"url":"http://arxiv.org/abs/2405.05692v1","category":"math-ph"}
{"created":"2024-05-09 11:41:27","title":"StableMoFusion: Towards Robust and Efficient Diffusion-based Motion Generation Framework","abstract":"Thanks to the powerful generative capacity of diffusion models, recent years have witnessed rapid progress in human motion generation. Existing diffusion-based methods employ disparate network architectures and training strategies. The effect of the design of each component is still unclear. In addition, the iterative denoising process consumes considerable computational overhead, which is prohibitive for real-time scenarios such as virtual characters and humanoid robots. For this reason, we first conduct a comprehensive investigation into network architectures, training strategies, and inference processs. Based on the profound analysis, we tailor each component for efficient high-quality human motion generation. Despite the promising performance, the tailored model still suffers from foot skating which is an ubiquitous issue in diffusion-based solutions. To eliminate footskate, we identify foot-ground contact and correct foot motions along the denoising process. By organically combining these well-designed components together, we present StableMoFusion, a robust and efficient framework for human motion generation. Extensive experimental results show that our StableMoFusion performs favorably against current state-of-the-art methods. Project page: https://h-y1heng.github.io/StableMoFusion-page/","sentences":["Thanks to the powerful generative capacity of diffusion models, recent years have witnessed rapid progress in human motion generation.","Existing diffusion-based methods employ disparate network architectures and training strategies.","The effect of the design of each component is still unclear.","In addition, the iterative denoising process consumes considerable computational overhead, which is prohibitive for real-time scenarios such as virtual characters and humanoid robots.","For this reason, we first conduct a comprehensive investigation into network architectures, training strategies, and inference processs.","Based on the profound analysis, we tailor each component for efficient high-quality human motion generation.","Despite the promising performance, the tailored model still suffers from foot skating which is an ubiquitous issue in diffusion-based solutions.","To eliminate footskate, we identify foot-ground contact and correct foot motions along the denoising process.","By organically combining these well-designed components together, we present StableMoFusion, a robust and efficient framework for human motion generation.","Extensive experimental results show that our StableMoFusion performs favorably against current state-of-the-art methods.","Project page: https://h-y1heng.github.io/StableMoFusion-page/"],"url":"http://arxiv.org/abs/2405.05691v1","category":"cs.CV"}
{"created":"2024-05-09 11:38:23","title":"Evaluating Dialect Robustness of Language Models via Conversation Understanding","abstract":"With an evergrowing number of LLMs reporting superlative performance for English, their ability to perform equitably for different dialects of English (i.e., dialect robustness) needs to be ascertained. Specifically, we use English language (US English or Indian English) conversations between humans who play the word-guessing game of `taboo'. We formulate two evaluative tasks: target word prediction (TWP) (i.e.predict the masked target word in a conversation) and target word selection (TWS) (i.e., select the most likely masked target word in a conversation, from among a set of candidate words). Extending MD3, an existing dialectic dataset of taboo-playing conversations, we introduce M-MD3, a target-word-masked version of MD3 with the USEng and IndEng subsets. We add two subsets: AITrans (where dialectic information is removed from IndEng) and AIGen (where LLMs are prompted to generate conversations). Our evaluation uses pre-trained and fine-tuned versions of two closed-source (GPT-4/3.5) and two open-source LLMs (Mistral and Gemma). LLMs perform significantly better for US English than Indian English for both TWP and TWS, for all settings. While GPT-based models perform the best, the comparatively smaller models work more equitably for short conversations (<8 turns). Our results on AIGen and AITrans (the best and worst-performing subset) respectively show that LLMs may learn a dialect of their own based on the composition of the training data, and that dialect robustness is indeed a challenging task. Our evaluation methodology exhibits a novel way to examine attributes of language models using pre-existing dialogue datasets.","sentences":["With an evergrowing number of LLMs reporting superlative performance for English, their ability to perform equitably for different dialects of English (i.e., dialect robustness) needs to be ascertained.","Specifically, we use English language (US English or Indian English) conversations between humans who play the word-guessing game of `taboo'.","We formulate two evaluative tasks: target word prediction (TWP) (i.e.predict the masked target word in a conversation) and target word selection (TWS) (i.e., select the most likely masked target word in a conversation, from among a set of candidate words).","Extending MD3, an existing dialectic dataset of taboo-playing conversations, we introduce M-MD3, a target-word-masked version of MD3 with the USEng and IndEng subsets.","We add two subsets: AITrans (where dialectic information is removed from IndEng) and AIGen (where LLMs are prompted to generate conversations).","Our evaluation uses pre-trained and fine-tuned versions of two closed-source (GPT-4/3.5) and two open-source LLMs (Mistral and Gemma).","LLMs perform significantly better for US English than Indian English for both TWP and TWS, for all settings.","While GPT-based models perform the best, the comparatively smaller models work more equitably for short conversations (<8 turns).","Our results on AIGen and AITrans (the best and worst-performing subset) respectively show that LLMs may learn a dialect of their own based on the composition of the training data, and that dialect robustness is indeed a challenging task.","Our evaluation methodology exhibits a novel way to examine attributes of language models using pre-existing dialogue datasets."],"url":"http://arxiv.org/abs/2405.05688v1","category":"cs.CL"}
{"created":"2024-05-09 11:32:30","title":"Diffraction casting","abstract":"Optical computing is considered a promising solution for the growing demand for parallel computing in various cutting-edge fields, requiring high integration and high speed computational capacity. In this paper, we propose a novel optical computation architecture called diffraction casting (DC) for flexible and scalable parallel logic operations. In DC, a diffractive neural network (DNN) is designed for single instruction, multiple data (SIMD) operations. This approach allows for the alteration of logic operations simply by changing the illumination patterns. Furthermore, it eliminates the need for encoding and decoding the input and output, respectively, by introducing a buffer around the input area, facilitating end-to-end all-optical computing. We numerically demonstrate DC by performing all 16 logic operations on two arbitrary 256 bits parallel binary inputs. Additionally, we showcase several distinctive attributes inherent in DC, such as the benefit of cohesively designing the diffractive elements for SIMD logic operations, assuring high scalability and integration capability. Our study offers a novel design architecture for optical computers and paves the way for a next-generation optical computing paradigm.","sentences":["Optical computing is considered a promising solution for the growing demand for parallel computing in various cutting-edge fields, requiring high integration and high speed computational capacity.","In this paper, we propose a novel optical computation architecture called diffraction casting (DC) for flexible and scalable parallel logic operations.","In DC, a diffractive neural network (DNN) is designed for single instruction, multiple data (SIMD) operations.","This approach allows for the alteration of logic operations simply by changing the illumination patterns.","Furthermore, it eliminates the need for encoding and decoding the input and output, respectively, by introducing a buffer around the input area, facilitating end-to-end all-optical computing.","We numerically demonstrate DC by performing all 16 logic operations on two arbitrary 256 bits parallel binary inputs.","Additionally, we showcase several distinctive attributes inherent in DC, such as the benefit of cohesively designing the diffractive elements for SIMD logic operations, assuring high scalability and integration capability.","Our study offers a novel design architecture for optical computers and paves the way for a next-generation optical computing paradigm."],"url":"http://arxiv.org/abs/2405.05686v1","category":"physics.optics"}
{"created":"2024-05-09 11:28:43","title":"Asymptotic preserving finite volume method for the compressible Euler equations: analysis via dissipative measure-valued solutions","abstract":"We propose and analyze a new asymptotic preserving (AP) finite volume scheme for the multidimensional compressible barotropic Euler equations to simulate low Mach number flows. The proposed scheme uses a stabilized upwind numerical flux, with the stabilization term being proportional to the stiff pressure gradient, and we prove its conditional energy stability and consistency. Utilizing the concept of dissipative measure-valued (DMV) solutions, we rigorously illustrate the AP properties of the scheme for well-prepared initial data. In particular, we prove that the numerical solutions will converge weakly to a DMV solution of the compressible Euler equations as the mesh parameter vanishes, while the Mach number is fixed. The DMV solutions then converge to a classical solution of the incompressible Euler system as the Mach number goes to zero. Conversely, we show that if the mesh parameter is kept fixed, we obtain an energy stable and consistent finite-volume scheme approximating the incompressible Euler equations as the Mach number goes to zero. The numerical solutions generated by this scheme then converge weakly to a DMV solution of the incompressible Euler system as the mesh parameter vanishes. Invoking the weak-strong uniqueness principle, we conclude that the DMV solution and classical solution of the incompressible Euler system coincide, proving the AP property of the scheme. We also present an extensive numerical case study in order to illustrate the theoretical convergences, wherein we utilize the techniques of K-convergence.","sentences":["We propose and analyze a new asymptotic preserving (AP) finite volume scheme for the multidimensional compressible barotropic Euler equations to simulate low Mach number flows.","The proposed scheme uses a stabilized upwind numerical flux, with the stabilization term being proportional to the stiff pressure gradient, and we prove its conditional energy stability and consistency.","Utilizing the concept of dissipative measure-valued (DMV) solutions, we rigorously illustrate the AP properties of the scheme for well-prepared initial data.","In particular, we prove that the numerical solutions will converge weakly to a DMV solution of the compressible Euler equations as the mesh parameter vanishes, while the Mach number is fixed.","The DMV solutions then converge to a classical solution of the incompressible Euler system as the Mach number goes to zero.","Conversely, we show that if the mesh parameter is kept fixed, we obtain an energy stable and consistent finite-volume scheme approximating the incompressible Euler equations as the Mach number goes to zero.","The numerical solutions generated by this scheme then converge weakly to a DMV solution of the incompressible Euler system as the mesh parameter vanishes.","Invoking the weak-strong uniqueness principle, we conclude that the DMV solution and classical solution of the incompressible Euler system coincide, proving the AP property of the scheme.","We also present an extensive numerical case study in order to illustrate the theoretical convergences, wherein we utilize the techniques of K-convergence."],"url":"http://arxiv.org/abs/2405.05685v1","category":"math.NA"}
{"created":"2024-05-09 11:27:38","title":"Comparison Principles for the Finsler Infinity Laplacian with Applications to Minimal Lipschitz Extensions","abstract":"This paper proves comparison principles for elliptic PDE involving the Finsler infinity Laplacian, a second-order differential operator with discontinuities in the gradient variable arising in $L^{\\infty}$-variational problems and tug-of-war games. The core of the paper consists in proving generalized cone comparison principles. Among other consequences, these results imply that, for any Finsler norm $\\varphi$ in $\\mathbb{R}^{d}$, a function $u$ is a $\\varphi$-absolutely minimizing Lipschitz extension if and only if it is a viscosity solution of the $\\varphi$-infinity Laplace equation, settling a longstanding question in the $L^{\\infty}$-calculus of variations. The proofs combine new geometric constructions with classical notions from convex analysis.","sentences":["This paper proves comparison principles for elliptic PDE involving the Finsler infinity Laplacian, a second-order differential operator with discontinuities in the gradient variable arising in $L^{\\infty}$-variational problems and tug-of-war games.","The core of the paper consists in proving generalized cone comparison principles.","Among other consequences, these results imply that, for any Finsler norm $\\varphi$ in $\\mathbb{R}^{d}$, a function $u$ is a $\\varphi$-absolutely minimizing Lipschitz extension if and only if it is a viscosity solution of the $\\varphi$-infinity Laplace equation, settling a longstanding question in the $L^{\\infty}$-calculus of variations.","The proofs combine new geometric constructions with classical notions from convex analysis."],"url":"http://arxiv.org/abs/2405.05684v1","category":"math.AP"}
{"created":"2024-05-09 11:16:38","title":"About generalized complex structures on $\\mathbb S^6$","abstract":"We study the existence of generalized complex structures on the six-dimensional sphere $\\mathbb S^6$. We work with the generalized tangent bundle $\\mathbb T\\mathbb S^6\\to \\mathbb S^6$ and define the integrability of generalized geometric structures in terms of the Dorfman bracket. Specifically, we prove that there is not a direct way to induce a generalized complex structure on $\\mathbb S^6$ from its usual nearly K\\\"ahler structure inherited from the octonions product.","sentences":["We study the existence of generalized complex structures on the six-dimensional sphere $\\mathbb S^6$. We work with the generalized tangent bundle $\\mathbb T\\mathbb S^6\\to \\mathbb S^6$ and define the integrability of generalized geometric structures in terms of the Dorfman bracket.","Specifically, we prove that there is not a direct way to induce a generalized complex structure on $\\mathbb S^6$ from its usual nearly K\\\"ahler structure inherited from the octonions product."],"url":"http://arxiv.org/abs/2405.05681v1","category":"math.DG"}
{"created":"2024-05-09 11:10:29","title":"Beyond Prompts: Learning from Human Communication for Enhanced AI Intent Alignment","abstract":"AI intent alignment, ensuring that AI produces outcomes as intended by users, is a critical challenge in human-AI interaction. The emergence of generative AI, including LLMs, has intensified the significance of this problem, as interactions increasingly involve users specifying desired results for AI systems. In order to support better AI intent alignment, we aim to explore human strategies for intent specification in human-human communication. By studying and comparing human-human and human-LLM communication, we identify key strategies that can be applied to the design of AI systems that are more effective at understanding and aligning with user intent. This study aims to advance toward a human-centered AI system by bringing together human communication strategies for the design of AI systems.","sentences":["AI intent alignment, ensuring that AI produces outcomes as intended by users, is a critical challenge in human-AI interaction.","The emergence of generative AI, including LLMs, has intensified the significance of this problem, as interactions increasingly involve users specifying desired results for AI systems.","In order to support better AI intent alignment, we aim to explore human strategies for intent specification in human-human communication.","By studying and comparing human-human and human-LLM communication, we identify key strategies that can be applied to the design of AI systems that are more effective at understanding and aligning with user intent.","This study aims to advance toward a human-centered AI system by bringing together human communication strategies for the design of AI systems."],"url":"http://arxiv.org/abs/2405.05678v1","category":"cs.HC"}
{"created":"2024-05-09 11:00:06","title":"TransAnaNet: Transformer-based Anatomy Change Prediction Network for Head and Neck Cancer Patient Radiotherapy","abstract":"Early identification of head and neck cancer (HNC) patients who would experience significant anatomical change during radiotherapy (RT) is important to optimize patient clinical benefit and treatment resources. This study aims to assess the feasibility of using a vision-transformer (ViT) based neural network to predict RT-induced anatomic change in HNC patients. We retrospectively included 121 HNC patients treated with definitive RT/CRT. We collected the planning CT (pCT), planned dose, CBCTs acquired at the initial treatment (CBCT01) and fraction 21 (CBCT21), and primary tumor volume (GTVp) and involved nodal volume (GTVn) delineated on both pCT and CBCTs for model construction and evaluation. A UNet-style ViT network was designed to learn spatial correspondence and contextual information from embedded CT, dose, CBCT01, GTVp, and GTVn image patches. The model estimated the deformation vector field between CBCT01 and CBCT21 as the prediction of anatomic change, and deformed CBCT01 was used as the prediction of CBCT21. We also generated binary masks of GTVp, GTVn, and patient body for volumetric change evaluation. The predicted image from the proposed method yielded the best similarity to the real image (CBCT21) over pCT, CBCT01, and predicted CBCTs from other comparison models. The average MSE and SSIM between the normalized predicted CBCT to CBCT21 are 0.009 and 0.933, while the average dice coefficient between body mask, GTVp mask, and GTVn mask are 0.972, 0.792, and 0.821 respectively. The proposed method showed promising performance for predicting radiotherapy-induced anatomic change, which has the potential to assist in the decision-making of HNC Adaptive RT.","sentences":["Early identification of head and neck cancer (HNC) patients who would experience significant anatomical change during radiotherapy (RT) is important to optimize patient clinical benefit and treatment resources.","This study aims to assess the feasibility of using a vision-transformer (ViT) based neural network to predict RT-induced anatomic change in HNC patients.","We retrospectively included 121 HNC patients treated with definitive RT/CRT.","We collected the planning CT (pCT), planned dose, CBCTs acquired at the initial treatment (CBCT01) and fraction 21 (CBCT21), and primary tumor volume (GTVp) and involved nodal volume (GTVn) delineated on both pCT and CBCTs for model construction and evaluation.","A UNet-style ViT network was designed to learn spatial correspondence and contextual information from embedded CT, dose, CBCT01, GTVp, and GTVn image patches.","The model estimated the deformation vector field between CBCT01 and CBCT21 as the prediction of anatomic change, and deformed CBCT01 was used as the prediction of CBCT21.","We also generated binary masks of GTVp, GTVn, and patient body for volumetric change evaluation.","The predicted image from the proposed method yielded the best similarity to the real image (CBCT21) over pCT, CBCT01, and predicted CBCTs from other comparison models.","The average MSE and SSIM between the normalized predicted CBCT to CBCT21 are 0.009 and 0.933, while the average dice coefficient between body mask, GTVp mask, and GTVn mask are 0.972, 0.792, and 0.821 respectively.","The proposed method showed promising performance for predicting radiotherapy-induced anatomic change, which has the potential to assist in the decision-making of HNC Adaptive RT."],"url":"http://arxiv.org/abs/2405.05674v1","category":"cs.CV"}
{"created":"2024-05-09 10:51:07","title":"Between proof construction and SAT-solving","abstract":"The classical satisfiability problem (SAT) is used as a natural and general tool to express and solve combinatorial problems that are in NP. We postulate that provability for implicational intuitionistic propositional logic (IIPC) can serve as a similar natural tool to express problems in Pspace. This approach can be particularly convenient for two reasons. One is that provability in full IPC (with all connectives) can be reduced to provability of implicational formulas of order three. Another advantage is a convenient interpretation in terms of simple alternating automata. Additionally, we distinguish some natural subclasses of IIPC corresponding to the complexity classes NP and co-NP. Our experimental results show that a simple decision procedure requires a significant amount of time only in a small fraction of cases.","sentences":["The classical satisfiability problem (SAT) is used as a natural and general tool to express and solve combinatorial problems that are in NP.","We postulate that provability for implicational intuitionistic propositional logic (IIPC) can serve as a similar natural tool to express problems in Pspace.","This approach can be particularly convenient for two reasons.","One is that provability in full IPC (with all connectives) can be reduced to provability of implicational formulas of order three.","Another advantage is a convenient interpretation in terms of simple alternating automata.","Additionally, we distinguish some natural subclasses of IIPC corresponding to the complexity classes NP and co-NP.","Our experimental results show that a simple decision procedure requires a significant amount of time only in a small fraction of cases."],"url":"http://arxiv.org/abs/2405.05670v1","category":"cs.LO"}
{"created":"2024-05-09 10:41:20","title":"Guess the Drift with LOP-UKF: LiDAR Odometry and Pacejka Model for Real-Time Racecar Sideslip Estimation","abstract":"The sideslip angle, crucial for vehicle safety and stability, is determined using both longitudinal and lateral velocities. However, measuring the lateral component often necessitates costly sensors, leading to its common estimation, a topic thoroughly explored in existing literature. This paper introduces LOP-UKF, a novel method for estimating vehicle lateral velocity by integrating Lidar Odometry with the Pacejka tire model predictions, resulting in a robust estimation via an Unscendent Kalman Filter (UKF). This combination represents a distinct alternative to more traditional methodologies, resulting in a reliable solution also in edge cases. We present experimental results obtained using the Dallara AV-21 across diverse circuits and track conditions, demonstrating the effectiveness of our method.","sentences":["The sideslip angle, crucial for vehicle safety and stability, is determined using both longitudinal and lateral velocities.","However, measuring the lateral component often necessitates costly sensors, leading to its common estimation, a topic thoroughly explored in existing literature.","This paper introduces LOP-UKF, a novel method for estimating vehicle lateral velocity by integrating Lidar Odometry with the Pacejka tire model predictions, resulting in a robust estimation via an Unscendent Kalman Filter (UKF).","This combination represents a distinct alternative to more traditional methodologies, resulting in a reliable solution also in edge cases.","We present experimental results obtained using the Dallara AV-21 across diverse circuits and track conditions, demonstrating the effectiveness of our method."],"url":"http://arxiv.org/abs/2405.05668v1","category":"cs.RO"}
{"created":"2024-05-09 10:41:18","title":"VM-DDPM: Vision Mamba Diffusion for Medical Image Synthesis","abstract":"In the realm of smart healthcare, researchers enhance the scale and diversity of medical datasets through medical image synthesis. However, existing methods are limited by CNN local perception and Transformer quadratic complexity, making it difficult to balance structural texture consistency. To this end, we propose the Vision Mamba DDPM (VM-DDPM) based on State Space Model (SSM), fully combining CNN local perception and SSM global modeling capabilities, while maintaining linear computational complexity. Specifically, we designed a multi-level feature extraction module called Multi-level State Space Block (MSSBlock), and a basic unit of encoder-decoder structure called State Space Layer (SSLayer) for medical pathological images. Besides, we designed a simple, Plug-and-Play, zero-parameter Sequence Regeneration strategy for the Cross-Scan Module (CSM), which enabled the S6 module to fully perceive the spatial features of the 2D image and stimulate the generalization potential of the model. To our best knowledge, this is the first medical image synthesis model based on the SSM-CNN hybrid architecture. Our experimental evaluation on three datasets of different scales, i.e., ACDC, BraTS2018, and ChestXRay, as well as qualitative evaluation by radiologists, demonstrate that VM-DDPM achieves state-of-the-art performance.","sentences":["In the realm of smart healthcare, researchers enhance the scale and diversity of medical datasets through medical image synthesis.","However, existing methods are limited by CNN local perception and Transformer quadratic complexity, making it difficult to balance structural texture consistency.","To this end, we propose the Vision Mamba DDPM (VM-DDPM) based on State Space Model (SSM), fully combining CNN local perception and SSM global modeling capabilities, while maintaining linear computational complexity.","Specifically, we designed a multi-level feature extraction module called Multi-level State Space Block (MSSBlock), and a basic unit of encoder-decoder structure called State Space Layer (SSLayer) for medical pathological images.","Besides, we designed a simple, Plug-and-Play, zero-parameter Sequence Regeneration strategy for the Cross-Scan Module (CSM), which enabled the S6 module to fully perceive the spatial features of the 2D image and stimulate the generalization potential of the model.","To our best knowledge, this is the first medical image synthesis model based on the SSM-CNN hybrid architecture.","Our experimental evaluation on three datasets of different scales, i.e., ACDC, BraTS2018, and ChestXRay, as well as qualitative evaluation by radiologists, demonstrate that VM-DDPM achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2405.05667v1","category":"eess.IV"}
{"created":"2024-05-09 10:35:44","title":"RPBG: Towards Robust Neural Point-based Graphics in the Wild","abstract":"Point-based representations have recently gained popularity in novel view synthesis, for their unique advantages, e.g., intuitive geometric representation, simple manipulation, and faster convergence. However, based on our observation, these point-based neural re-rendering methods are only expected to perform well under ideal conditions and suffer from noisy, patchy points and unbounded scenes, which are challenging to handle but defacto common in real applications. To this end, we revisit one such influential method, known as Neural Point-based Graphics (NPBG), as our baseline, and propose Robust Point-based Graphics (RPBG). We in-depth analyze the factors that prevent NPBG from achieving satisfactory renderings on generic datasets, and accordingly reform the pipeline to make it more robust to varying datasets in-the-wild. Inspired by the practices in image restoration, we greatly enhance the neural renderer to enable the attention-based correction of point visibility and the inpainting of incomplete rasterization, with only acceptable overheads. We also seek for a simple and lightweight alternative for environment modeling and an iterative method to alleviate the problem of poor geometry. By thorough evaluation on a wide range of datasets with different shooting conditions and camera trajectories, RPBG stably outperforms the baseline by a large margin, and exhibits its great robustness over state-of-the-art NeRF-based variants. Code available at https://github.com/QT-Zhu/RPBG.","sentences":["Point-based representations have recently gained popularity in novel view synthesis, for their unique advantages, e.g., intuitive geometric representation, simple manipulation, and faster convergence.","However, based on our observation, these point-based neural re-rendering methods are only expected to perform well under ideal conditions and suffer from noisy, patchy points and unbounded scenes, which are challenging to handle but defacto common in real applications.","To this end, we revisit one such influential method, known as Neural Point-based Graphics (NPBG), as our baseline, and propose Robust Point-based Graphics (RPBG).","We in-depth analyze the factors that prevent NPBG from achieving satisfactory renderings on generic datasets, and accordingly reform the pipeline to make it more robust to varying datasets in-the-wild.","Inspired by the practices in image restoration, we greatly enhance the neural renderer to enable the attention-based correction of point visibility and the inpainting of incomplete rasterization, with only acceptable overheads.","We also seek for a simple and lightweight alternative for environment modeling and an iterative method to alleviate the problem of poor geometry.","By thorough evaluation on a wide range of datasets with different shooting conditions and camera trajectories, RPBG stably outperforms the baseline by a large margin, and exhibits its great robustness over state-of-the-art NeRF-based variants.","Code available at https://github.com/QT-Zhu/RPBG."],"url":"http://arxiv.org/abs/2405.05663v1","category":"cs.CV"}
{"created":"2024-05-09 10:33:07","title":"Approximate Dec-POMDP Solving Using Multi-Agent A*","abstract":"We present an A*-based algorithm to compute policies for finite-horizon Dec-POMDPs. Our goal is to sacrifice optimality in favor of scalability for larger horizons. The main ingredients of our approach are (1) using clustered sliding window memory, (2) pruning the A* search tree, and (3) using novel A* heuristics. Our experiments show competitive performance to the state-of-the-art. Moreover, for multiple benchmarks, we achieve superior performance. In addition, we provide an A* algorithm that finds upper bounds for the optimum, tailored towards problems with long horizons. The main ingredient is a new heuristic that periodically reveals the state, thereby limiting the number of reachable beliefs. Our experiments demonstrate the efficacy and scalability of the approach.","sentences":["We present an A*-based algorithm to compute policies for finite-horizon Dec-POMDPs.","Our goal is to sacrifice optimality in favor of scalability for larger horizons.","The main ingredients of our approach are (1) using clustered sliding window memory, (2) pruning the A* search tree, and (3) using novel A* heuristics.","Our experiments show competitive performance to the state-of-the-art.","Moreover, for multiple benchmarks, we achieve superior performance.","In addition, we provide an A* algorithm that finds upper bounds for the optimum, tailored towards problems with long horizons.","The main ingredient is a new heuristic that periodically reveals the state, thereby limiting the number of reachable beliefs.","Our experiments demonstrate the efficacy and scalability of the approach."],"url":"http://arxiv.org/abs/2405.05662v1","category":"cs.AI"}
{"created":"2024-05-09 10:22:12","title":"Josephson effect in a Fibonacci quasicrystal","abstract":"Quasiperiodicity has recently been proposed to enhance superconductivity and its proximity effect. At the same time, there has been significant experimental progress in the fabrication of quasiperiodic structures, also in reduced dimensions. Motivated by these developments, we use microscopic tight-binding theory to investigate the DC Josephson effect through a ballistic Fibonacci chain attached to two superconducting leads. The Fibonacci chain is one of the most studied examples of quasicrystals, hosting a rich multifractal spectrum, containing topological gaps with different winding numbers. We study how the Andreev bound states (ABS), current-phase relation, and the critical current depend on the quasiperiodic degrees of freedom, from short to long junctions. While the current-phase relation shows a traditional $2\\pi$ sinusoidal or sawtooth profile, we find that the ABS obtain quasiperiodic oscillations and that the Andreev reflection is qualitatively altered, leading to quasiperiodic oscillations in the critical current as a function of junction length. Surprisingly, despite earlier proposals of enhanced superconductivity, we do not in general find an enhanced critical current. However, we find significant enhancement for reduced interface transparency due to the modified Andreev reflection. Furthermore, by varying the chemical potential, e.g.~by an applied gate voltage, we find a fractal oscillation between superconductor-normal metal-superconductor (SNS) and superconductor-insulator-superconductor (SIS) behavior. Finally, we show that the winding of the subgap states leads to an equivalent winding in the critical current, such that the winding numbers, and thus the topological invariant, can be determined.","sentences":["Quasiperiodicity has recently been proposed to enhance superconductivity and its proximity effect.","At the same time, there has been significant experimental progress in the fabrication of quasiperiodic structures, also in reduced dimensions.","Motivated by these developments, we use microscopic tight-binding theory to investigate the DC Josephson effect through a ballistic Fibonacci chain attached to two superconducting leads.","The Fibonacci chain is one of the most studied examples of quasicrystals, hosting a rich multifractal spectrum, containing topological gaps with different winding numbers.","We study how the Andreev bound states (ABS), current-phase relation, and the critical current depend on the quasiperiodic degrees of freedom, from short to long junctions.","While the current-phase relation shows a traditional $2\\pi$ sinusoidal or sawtooth profile, we find that the ABS obtain quasiperiodic oscillations and that the Andreev reflection is qualitatively altered, leading to quasiperiodic oscillations in the critical current as a function of junction length.","Surprisingly, despite earlier proposals of enhanced superconductivity, we do not in general find an enhanced critical current.","However, we find significant enhancement for reduced interface transparency due to the modified Andreev reflection.","Furthermore, by varying the chemical potential, e.g.~by an applied gate voltage, we find a fractal oscillation between superconductor-normal metal-superconductor (SNS) and superconductor-insulator-superconductor (SIS) behavior.","Finally, we show that the winding of the subgap states leads to an equivalent winding in the critical current, such that the winding numbers, and thus the topological invariant, can be determined."],"url":"http://arxiv.org/abs/2405.05660v1","category":"cond-mat.supr-con"}
{"created":"2024-05-09 10:14:49","title":"End-to-End Waveform and Beamforming Optimization for RF Wireless Power Transfer","abstract":"Radio frequency (RF) wireless power transfer (WPT) is a key technology for future low-power wireless systems. However, the inherently low end-to-end power transfer efficiency (PTE) is challenging for practical applications. The main factors contributing to it are the channel losses, transceivers' power consumption, and losses related, e.g., to the digital-to-analog converter (DAC), high-power amplifier, and rectenna. Optimizing PTE requires careful consideration of these factors, motivating the current work. Herein, we consider an analog multi-antenna power transmitter that aims to charge a single energy harvester. We first provide a mathematical framework to calculate the harvested power from multi-tone signal transmissions and the system power consumption. Then, we formulate the joint waveform and analog beamforming design problem to minimize power consumption and meet the charging requirements. Finally, we propose an optimization approach relying on swarm intelligence to solve the specified problem. Simulation results quantify the power consumption reduction as the DAC, phase shifters resolution, and antenna length are increased, while it is seen that increasing system frequency results in higher power consumption.","sentences":["Radio frequency (RF) wireless power transfer (WPT) is a key technology for future low-power wireless systems.","However, the inherently low end-to-end power transfer efficiency (PTE) is challenging for practical applications.","The main factors contributing to it are the channel losses, transceivers' power consumption, and losses related, e.g., to the digital-to-analog converter (DAC), high-power amplifier, and rectenna.","Optimizing PTE requires careful consideration of these factors, motivating the current work.","Herein, we consider an analog multi-antenna power transmitter that aims to charge a single energy harvester.","We first provide a mathematical framework to calculate the harvested power from multi-tone signal transmissions and the system power consumption.","Then, we formulate the joint waveform and analog beamforming design problem to minimize power consumption and meet the charging requirements.","Finally, we propose an optimization approach relying on swarm intelligence to solve the specified problem.","Simulation results quantify the power consumption reduction as the DAC, phase shifters resolution, and antenna length are increased, while it is seen that increasing system frequency results in higher power consumption."],"url":"http://arxiv.org/abs/2405.05659v1","category":"eess.SP"}
{"created":"2024-05-09 10:12:17","title":"Artificial intelligence for abnormality detection in high volume neuroimaging: a systematic review and meta-analysis","abstract":"Purpose: Most studies evaluating artificial intelligence (AI) models that detect abnormalities in neuroimaging are either tested on unrepresentative patient cohorts or are insufficiently well-validated, leading to poor generalisability to real-world tasks. The aim was to determine the diagnostic test accuracy and summarise the evidence supporting the use of AI models performing first-line, high-volume neuroimaging tasks.   Methods: Medline, Embase, Cochrane library and Web of Science were searched until September 2021 for studies that temporally or externally validated AI capable of detecting abnormalities in first-line CT or MR neuroimaging. A bivariate random-effects model was used for meta-analysis where appropriate. PROSPERO: CRD42021269563.   Results: Only 16 studies were eligible for inclusion. Included studies were not compromised by unrepresentative datasets or inadequate validation methodology. Direct comparison with radiologists was available in 4/16 studies. 15/16 had a high risk of bias. Meta-analysis was only suitable for intracranial haemorrhage detection in CT imaging (10/16 studies), where AI systems had a pooled sensitivity and specificity 0.90 (95% CI 0.85 - 0.94) and 0.90 (95% CI 0.83 - 0.95) respectively. Other AI studies using CT and MRI detected target conditions other than haemorrhage (2/16), or multiple target conditions (4/16). Only 3/16 studies implemented AI in clinical pathways, either for pre-read triage or as post-read discrepancy identifiers.   Conclusion: The paucity of eligible studies reflects that most abnormality detection AI studies were not adequately validated in representative clinical cohorts. The few studies describing how abnormality detection AI could impact patients and clinicians did not explore the full ramifications of clinical implementation.","sentences":["Purpose: Most studies evaluating artificial intelligence (AI) models that detect abnormalities in neuroimaging are either tested on unrepresentative patient cohorts or are insufficiently well-validated, leading to poor generalisability to real-world tasks.","The aim was to determine the diagnostic test accuracy and summarise the evidence supporting the use of AI models performing first-line, high-volume neuroimaging tasks.   ","Methods: Medline, Embase, Cochrane library and Web of Science were searched until September 2021 for studies that temporally or externally validated AI capable of detecting abnormalities in first-line CT or MR neuroimaging.","A bivariate random-effects model was used for meta-analysis where appropriate.","PROSPERO:","CRD42021269563.   ","Results: Only 16 studies were eligible for inclusion.","Included studies were not compromised by unrepresentative datasets or inadequate validation methodology.","Direct comparison with radiologists was available in 4/16 studies.","15/16 had a high risk of bias.","Meta-analysis was only suitable for intracranial haemorrhage detection in CT imaging (10/16 studies), where AI systems had a pooled sensitivity and specificity 0.90 (95% CI 0.85 - 0.94) and 0.90 (95% CI 0.83 - 0.95) respectively.","Other AI studies using CT and MRI detected target conditions other than haemorrhage (2/16), or multiple target conditions (4/16).","Only 3/16 studies implemented AI in clinical pathways, either for pre-read triage or as post-read discrepancy identifiers.   ","Conclusion: The paucity of eligible studies reflects that most abnormality detection AI studies were not adequately validated in representative clinical cohorts.","The few studies describing how abnormality detection AI could impact patients and clinicians did not explore the full ramifications of clinical implementation."],"url":"http://arxiv.org/abs/2405.05658v1","category":"eess.IV"}
{"created":"2024-05-09 10:08:17","title":"Dynamics of McMillan mappings II. Axially symmetric map","abstract":"In this article, we investigate the transverse dynamics of a single particle in a model integrable accelerator lattice, based on a McMillan axially-symmetric electron lens. Although the McMillan e-lens has been considered as a device potentially capable of mitigating collective space charge forces, some of its fundamental properties have not been described yet. The main goal of our work is to close this gap and understand the limitations and potentials of this device. It is worth mentioning that the McMillan axially symmetric map provides the first-order approximations of dynamics for a general linear lattice plus an arbitrary thin lens with motion separable in polar coordinates. Therefore, advancements in its understanding should give us a better picture of more generic and not necessarily integrable round beams. In the first part of the article, we classify all possible regimes with stable trajectories and find the canonical action-angle variables. This provides an evaluation of the dynamical aperture, Poincar\\'e rotation numbers as functions of amplitudes, and thus determines the spread in nonlinear tunes. Also, we provide a parameterization of invariant curves, allowing for the immediate determination of the map image forward and backward in time. The second part investigates the particle dynamics as a function of system parameters. We show that there are three fundamentally different configurations of the accelerator optics causing different regimes of nonlinear oscillations. Each regime is considered in great detail, including the limiting cases of large and small amplitudes. In addition, we analyze the dynamics in Cartesian coordinates and provide a description of observable variables and corresponding spectra.","sentences":["In this article, we investigate the transverse dynamics of a single particle in a model integrable accelerator lattice, based on a McMillan axially-symmetric electron lens.","Although the McMillan e-lens has been considered as a device potentially capable of mitigating collective space charge forces, some of its fundamental properties have not been described yet.","The main goal of our work is to close this gap and understand the limitations and potentials of this device.","It is worth mentioning that the McMillan axially symmetric map provides the first-order approximations of dynamics for a general linear lattice plus an arbitrary thin lens with motion separable in polar coordinates.","Therefore, advancements in its understanding should give us a better picture of more generic and not necessarily integrable round beams.","In the first part of the article, we classify all possible regimes with stable trajectories and find the canonical action-angle variables.","This provides an evaluation of the dynamical aperture, Poincar\\'e rotation numbers as functions of amplitudes, and thus determines the spread in nonlinear tunes.","Also, we provide a parameterization of invariant curves, allowing for the immediate determination of the map image forward and backward in time.","The second part investigates the particle dynamics as a function of system parameters.","We show that there are three fundamentally different configurations of the accelerator optics causing different regimes of nonlinear oscillations.","Each regime is considered in great detail, including the limiting cases of large and small amplitudes.","In addition, we analyze the dynamics in Cartesian coordinates and provide a description of observable variables and corresponding spectra."],"url":"http://arxiv.org/abs/2405.05657v1","category":"nlin.SI"}
{"created":"2024-05-09 09:58:31","title":"Dynamics of McMillan mappings I. McMillan multipoles","abstract":"In this article, we consider two dynamical systems: the McMillan sextupole and octupole integrable mappings, originally proposed by Edwin McMillan. Both represent the simplest symmetric McMillan maps, characterized by a single intrinsic parameter. While these systems find numerous applications across various domains of mathematics and physics, some of their dynamical properties remain unexplored. We aim to bridge this gap by providing a comprehensive description of all stable trajectories, including the parametrization of invariant curves, Poincar\\'e rotation numbers, and canonical action-angle variables.   In the second part, we establish connections between these maps and general chaotic maps in standard form. Our investigation reveals that the McMillan sextupole and octupole serve as first-order approximations of the dynamics around the fixed point, akin to the linear map and quadratic invariant (known as the Courant-Snyder invariant in accelerator physics), which represents zeroth-order approximations (referred to as linearization). Furthermore, we propose a novel formalism for nonlinear Twiss parameters, which accounts for the dependence of rotation number on amplitude. This stands in contrast to conventional betatron phase advance used in accelerator physics, which remains independent of amplitude. Notably, in the context of accelerator physics, this new formalism demonstrates its capability in predicting dynamical aperture around low-order resonances for flat beams, a critical aspect in beam injection/extraction scenarios.","sentences":["In this article, we consider two dynamical systems: the McMillan sextupole and octupole integrable mappings, originally proposed by Edwin McMillan.","Both represent the simplest symmetric McMillan maps, characterized by a single intrinsic parameter.","While these systems find numerous applications across various domains of mathematics and physics, some of their dynamical properties remain unexplored.","We aim to bridge this gap by providing a comprehensive description of all stable trajectories, including the parametrization of invariant curves, Poincar\\'e rotation numbers, and canonical action-angle variables.   ","In the second part, we establish connections between these maps and general chaotic maps in standard form.","Our investigation reveals that the McMillan sextupole and octupole serve as first-order approximations of the dynamics around the fixed point, akin to the linear map and quadratic invariant (known as the Courant-Snyder invariant in accelerator physics), which represents zeroth-order approximations (referred to as linearization).","Furthermore, we propose a novel formalism for nonlinear Twiss parameters, which accounts for the dependence of rotation number on amplitude.","This stands in contrast to conventional betatron phase advance used in accelerator physics, which remains independent of amplitude.","Notably, in the context of accelerator physics, this new formalism demonstrates its capability in predicting dynamical aperture around low-order resonances for flat beams, a critical aspect in beam injection/extraction scenarios."],"url":"http://arxiv.org/abs/2405.05652v1","category":"nlin.SI"}
{"created":"2024-05-09 09:44:51","title":"ASGrasp: Generalizable Transparent Object Reconstruction and Grasping from RGB-D Active Stereo Camera","abstract":"In this paper, we tackle the problem of grasping transparent and specular objects. This issue holds importance, yet it remains unsolved within the field of robotics due to failure of recover their accurate geometry by depth cameras. For the first time, we propose ASGrasp, a 6-DoF grasp detection network that uses an RGB-D active stereo camera. ASGrasp utilizes a two-layer learning-based stereo network for the purpose of transparent object reconstruction, enabling material-agnostic object grasping in cluttered environments. In contrast to existing RGB-D based grasp detection methods, which heavily depend on depth restoration networks and the quality of depth maps generated by depth cameras, our system distinguishes itself by its ability to directly utilize raw IR and RGB images for transparent object geometry reconstruction. We create an extensive synthetic dataset through domain randomization, which is based on GraspNet-1Billion. Our experiments demonstrate that ASGrasp can achieve over 90% success rate for generalizable transparent object grasping in both simulation and the real via seamless sim-to-real transfer. Our method significantly outperforms SOTA networks and even surpasses the performance upper bound set by perfect visible point cloud inputs.Project page: https://pku-epic.github.io/ASGrasp","sentences":["In this paper, we tackle the problem of grasping transparent and specular objects.","This issue holds importance, yet it remains unsolved within the field of robotics due to failure of recover their accurate geometry by depth cameras.","For the first time, we propose ASGrasp, a 6-DoF grasp detection network that uses an RGB-D active stereo camera.","ASGrasp utilizes a two-layer learning-based stereo network for the purpose of transparent object reconstruction, enabling material-agnostic object grasping in cluttered environments.","In contrast to existing RGB-D based grasp detection methods, which heavily depend on depth restoration networks and the quality of depth maps generated by depth cameras, our system distinguishes itself by its ability to directly utilize raw IR and RGB images for transparent object geometry reconstruction.","We create an extensive synthetic dataset through domain randomization, which is based on GraspNet-1Billion.","Our experiments demonstrate that ASGrasp can achieve over 90% success rate for generalizable transparent object grasping in both simulation and the real via seamless sim-to-real transfer.","Our method significantly outperforms SOTA networks and even surpasses the performance upper bound set by perfect visible point cloud inputs.","Project page: https://pku-epic.github.io/ASGrasp"],"url":"http://arxiv.org/abs/2405.05648v1","category":"cs.RO"}
{"created":"2024-05-09 09:41:19","title":"Letter to the Editor: What are the legal and ethical considerations of submitting radiology reports to ChatGPT?","abstract":"This letter critically examines the recent article by Infante et al. assessing the utility of large language models (LLMs) like GPT-4, Perplexity, and Bard in identifying urgent findings in emergency radiology reports. While acknowledging the potential of LLMs in generating labels for computer vision, concerns are raised about the ethical implications of using patient data without explicit approval, highlighting the necessity of stringent data protection measures under GDPR.","sentences":["This letter critically examines the recent article by Infante et al. assessing the utility of large language models (LLMs) like GPT-4, Perplexity, and Bard in identifying urgent findings in emergency radiology reports.","While acknowledging the potential of LLMs in generating labels for computer vision, concerns are raised about the ethical implications of using patient data without explicit approval, highlighting the necessity of stringent data protection measures under GDPR."],"url":"http://arxiv.org/abs/2405.05647v1","category":"cs.CV"}
{"created":"2024-05-09 09:27:18","title":"An Efficient Finite Difference Approximation via a Double Sample-Recycling Approach","abstract":"Estimating stochastic gradients is pivotal in fields like service systems within operations research. The classical method for this estimation is the finite difference approximation, which entails generating samples at perturbed inputs. Nonetheless, practical challenges persist in determining the perturbation and obtaining an optimal finite difference estimator in the sense of possessing the smallest mean squared error (MSE). To tackle this problem, we propose a double sample-recycling approach in this paper. Firstly, pilot samples are recycled to estimate the optimal perturbation. Secondly, recycling these pilot samples again and generating new samples at the estimated perturbation, lead to an efficient finite difference estimator. We analyze its bias, variance and MSE. Our analyses demonstrate a reduction in asymptotic variance, and in some cases, a decrease in asymptotic bias, compared to the optimal finite difference estimator. Therefore, our proposed estimator consistently coincides with, or even outperforms the optimal finite difference estimator. In numerical experiments, we apply the estimator in several examples, and numerical results demonstrate its robustness, as well as coincidence with the theory presented, especially in the case of small sample sizes.","sentences":["Estimating stochastic gradients is pivotal in fields like service systems within operations research.","The classical method for this estimation is the finite difference approximation, which entails generating samples at perturbed inputs.","Nonetheless, practical challenges persist in determining the perturbation and obtaining an optimal finite difference estimator in the sense of possessing the smallest mean squared error (MSE).","To tackle this problem, we propose a double sample-recycling approach in this paper.","Firstly, pilot samples are recycled to estimate the optimal perturbation.","Secondly, recycling these pilot samples again and generating new samples at the estimated perturbation, lead to an efficient finite difference estimator.","We analyze its bias, variance and MSE.","Our analyses demonstrate a reduction in asymptotic variance, and in some cases, a decrease in asymptotic bias, compared to the optimal finite difference estimator.","Therefore, our proposed estimator consistently coincides with, or even outperforms the optimal finite difference estimator.","In numerical experiments, we apply the estimator in several examples, and numerical results demonstrate its robustness, as well as coincidence with the theory presented, especially in the case of small sample sizes."],"url":"http://arxiv.org/abs/2405.05638v1","category":"stat.ME"}
{"created":"2024-05-09 09:24:54","title":"Thermal junctions controlled with magnetic phases","abstract":"Unlike charge, heat flows are difficult to control. We show that, in mesoscopic conductors, electronic thermal currents can be manipulated with a magnetic field by using the Aharonov-Bohm effect: the magnetic control of the interference pattern enhances the thermoelectric effect, while heat transport can be totally suppressed. In a three-terminal configuration, the flux-induced broken reciprocity generates a non-local thermoelectric response and translates to the circulation of heat. This way, efficient thermoelectric generators, thermal switches and thermal circulators, as well as energy harvesters can be defined for minimally disturbing thermal management at the nanoscale.","sentences":["Unlike charge, heat flows are difficult to control.","We show that, in mesoscopic conductors, electronic thermal currents can be manipulated with a magnetic field by using the Aharonov-Bohm effect: the magnetic control of the interference pattern enhances the thermoelectric effect, while heat transport can be totally suppressed.","In a three-terminal configuration, the flux-induced broken reciprocity generates a non-local thermoelectric response and translates to the circulation of heat.","This way, efficient thermoelectric generators, thermal switches and thermal circulators, as well as energy harvesters can be defined for minimally disturbing thermal management at the nanoscale."],"url":"http://arxiv.org/abs/2405.05637v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-09 09:22:09","title":"SwapTalk: Audio-Driven Talking Face Generation with One-Shot Customization in Latent Space","abstract":"Combining face swapping with lip synchronization technology offers a cost-effective solution for customized talking face generation. However, directly cascading existing models together tends to introduce significant interference between tasks and reduce video clarity because the interaction space is limited to the low-level semantic RGB space. To address this issue, we propose an innovative unified framework, SwapTalk, which accomplishes both face swapping and lip synchronization tasks in the same latent space. Referring to recent work on face generation, we choose the VQ-embedding space due to its excellent editability and fidelity performance. To enhance the framework's generalization capabilities for unseen identities, we incorporate identity loss during the training of the face swapping module. Additionally, we introduce expert discriminator supervision within the latent space during the training of the lip synchronization module to elevate synchronization quality. In the evaluation phase, previous studies primarily focused on the self-reconstruction of lip movements in synchronous audio-visual videos. To better approximate real-world applications, we expand the evaluation scope to asynchronous audio-video scenarios. Furthermore, we introduce a novel identity consistency metric to more comprehensively assess the identity consistency over time series in generated facial videos. Experimental results on the HDTF demonstrate that our method significantly surpasses existing techniques in video quality, lip synchronization accuracy, face swapping fidelity, and identity consistency. Our demo is available at http://swaptalk.cc.","sentences":["Combining face swapping with lip synchronization technology offers a cost-effective solution for customized talking face generation.","However, directly cascading existing models together tends to introduce significant interference between tasks and reduce video clarity because the interaction space is limited to the low-level semantic RGB space.","To address this issue, we propose an innovative unified framework, SwapTalk, which accomplishes both face swapping and lip synchronization tasks in the same latent space.","Referring to recent work on face generation, we choose the VQ-embedding space due to its excellent editability and fidelity performance.","To enhance the framework's generalization capabilities for unseen identities, we incorporate identity loss during the training of the face swapping module.","Additionally, we introduce expert discriminator supervision within the latent space during the training of the lip synchronization module to elevate synchronization quality.","In the evaluation phase, previous studies primarily focused on the self-reconstruction of lip movements in synchronous audio-visual videos.","To better approximate real-world applications, we expand the evaluation scope to asynchronous audio-video scenarios.","Furthermore, we introduce a novel identity consistency metric to more comprehensively assess the identity consistency over time series in generated facial videos.","Experimental results on the HDTF demonstrate that our method significantly surpasses existing techniques in video quality, lip synchronization accuracy, face swapping fidelity, and identity consistency.","Our demo is available at http://swaptalk.cc."],"url":"http://arxiv.org/abs/2405.05636v1","category":"cs.CV"}
{"created":"2024-05-09 09:21:15","title":"Large Bricks and Join-irreducible torsionfree classes","abstract":"We show that every join-irreducible torsionfree class in the category of finitely generated modules over an artinian ring is cogenerated by a single (not necessarily finitely generated) brick.   This is a partial extension of the characterisation of completely join-irreducible torsionfree classes given by Barnard, Carroll and Zhu.","sentences":["We show that every join-irreducible torsionfree class in the category of finitely generated modules over an artinian ring is cogenerated by a single (not necessarily finitely generated) brick.   ","This is a partial extension of the characterisation of completely join-irreducible torsionfree classes given by Barnard, Carroll and Zhu."],"url":"http://arxiv.org/abs/2405.05635v1","category":"math.RT"}
{"created":"2024-05-09 08:57:22","title":"Calculation of $6j$-symbols for the Lie algebra $\\mathfrak{gl}_n$","abstract":"We give an explicit description of the multiplicity space that describes an occurence of a certain irreducible representation into a splitting of a tensor product of two irreducible representations of $\\mathfrak{gl}_n$. Using this descripition an explicit formula for an arbitrary $6j$-symbol for the algebra $\\mathfrak{gl}_n$ is derived. It is expressed through a value of a generalized hypergeometric function.","sentences":["We give an explicit description of the multiplicity space that describes an occurence of a certain irreducible representation into a splitting of a tensor product of two irreducible representations of $\\mathfrak{gl}_n$. Using this descripition an explicit formula for an arbitrary $6j$-symbol for the algebra $\\mathfrak{gl}_n$ is derived.","It is expressed through a value of a generalized hypergeometric function."],"url":"http://arxiv.org/abs/2405.05628v1","category":"math.RT"}
{"created":"2024-05-09 08:54:30","title":"AI in Your Toolbox: A Plugin for Generating Renderings from 3D Models","abstract":"With the rapid development of LLMs and AIGC technology, we present a Rhino platform plugin utilizing stable diffusion technology. This plugin enables real-time application deployment from 3D modeling software, integrating stable diffusion models with Rhino's features. It offers intelligent design functions, real-time feedback, and cross-platform linkage, enhancing design efficiency and quality. Our ongoing efforts focus on optimizing the plugin to further advance AI applications in CAD, empowering designers with smarter and more efficient design tools. Our goal is to provide designers with enhanced capabilities for creating exceptional designs in an increasingly AI-driven CAD environment.","sentences":["With the rapid development of LLMs and AIGC technology, we present a Rhino platform plugin utilizing stable diffusion technology.","This plugin enables real-time application deployment from 3D modeling software, integrating stable diffusion models with Rhino's features.","It offers intelligent design functions, real-time feedback, and cross-platform linkage, enhancing design efficiency and quality.","Our ongoing efforts focus on optimizing the plugin to further advance AI applications in CAD, empowering designers with smarter and more efficient design tools.","Our goal is to provide designers with enhanced capabilities for creating exceptional designs in an increasingly AI-driven CAD environment."],"url":"http://arxiv.org/abs/2405.05627v1","category":"cs.HC"}
{"created":"2024-05-09 08:51:29","title":"Does Dynamical Wormhole Evolve From Emergent Scenario?","abstract":"In the present work we analyse a dynamical wormhole solution with two fluids system (one isotropic and homogeneous and the other being inhomogeneous and anisotropic in nature) as the matter at the throat. We choose two different forms of Equation of State(EoS) and investigate two solutions of the wormhole geometry. The properties to ensure existence and traversability has been analysed. Also, the model of the dynamic wormhole has been examined for a possibility of the Emergent Universe(EU) model in cosmological context. Finally, for the dynamical wormholes so obtained, Null Energy Condition(NEC) has been examined near the throat.","sentences":["In the present work we analyse a dynamical wormhole solution with two fluids system (one isotropic and homogeneous and the other being inhomogeneous and anisotropic in nature) as the matter at the throat.","We choose two different forms of Equation of State(EoS) and investigate two solutions of the wormhole geometry.","The properties to ensure existence and traversability has been analysed.","Also, the model of the dynamic wormhole has been examined for a possibility of the Emergent Universe(EU) model in cosmological context.","Finally, for the dynamical wormholes so obtained, Null Energy Condition(NEC) has been examined near the throat."],"url":"http://arxiv.org/abs/2405.05625v1","category":"gr-qc"}
{"created":"2024-05-09 08:46:30","title":"Current progress in corrosion of multi principal element alloys","abstract":"Whilst multi-principal element alloys (MPEAs) remain a promising class of materials owing to several attractive mechanical properties, their corrosion performance is also unique. In this concise review, we present an emerging overview of some of the general features related to MPEA corrosion, following a decade of work in the field. This includes highlighting some of the key aspects related to the electrochemical phenomena in MPEA corrosion, and the relevant future works required for a holistic mechanistic understanding. In addition, a comprehensive database of the reported corrosion performance of MPEAs is presented, based on works reported to date. The database is assembled to also allow users to undertake machine learning or their own data analysis, with a parsed representation of alloy composition, test electrolyte, and corrosion related parameters.","sentences":["Whilst multi-principal element alloys (MPEAs) remain a promising class of materials owing to several attractive mechanical properties, their corrosion performance is also unique.","In this concise review, we present an emerging overview of some of the general features related to MPEA corrosion, following a decade of work in the field.","This includes highlighting some of the key aspects related to the electrochemical phenomena in MPEA corrosion, and the relevant future works required for a holistic mechanistic understanding.","In addition, a comprehensive database of the reported corrosion performance of MPEAs is presented, based on works reported to date.","The database is assembled to also allow users to undertake machine learning or their own data analysis, with a parsed representation of alloy composition, test electrolyte, and corrosion related parameters."],"url":"http://arxiv.org/abs/2405.05623v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 08:35:21","title":"Rectified Gaussian kernel multi-view k-means clustering","abstract":"In this paper, we show two new variants of multi-view k-means (MVKM) algorithms to address multi-view data. The general idea is to outline the distance between $h$-th view data points $x_i^h$ and $h$-th view cluster centers $a_k^h$ in a different manner of centroid-based approach. Unlike other methods, our proposed methods learn the multi-view data by calculating the similarity using Euclidean norm in the space of Gaussian-kernel, namely as multi-view k-means with exponent distance (MVKM-ED). By simultaneously aligning the stabilizer parameter $p$ and kernel coefficients $\\beta^h$, the compression of Gaussian-kernel based weighted distance in Euclidean norm reduce the sensitivity of MVKM-ED. To this end, this paper designated as Gaussian-kernel multi-view k-means (GKMVKM) clustering algorithm. Numerical evaluation of five real-world multi-view data demonstrates the robustness and efficiency of our proposed MVKM-ED and GKMVKM approaches.","sentences":["In this paper, we show two new variants of multi-view k-means (MVKM) algorithms to address multi-view data.","The general idea is to outline the distance between $h$-th view data points $x_i^h$ and $h$-th view cluster centers $a_k^h$ in a different manner of centroid-based approach.","Unlike other methods, our proposed methods learn the multi-view data by calculating the similarity using Euclidean norm in the space of Gaussian-kernel, namely as multi-view k-means with exponent distance (MVKM-ED).","By simultaneously aligning the stabilizer parameter $p$ and kernel coefficients $\\beta^h$, the compression of Gaussian-kernel based weighted distance in Euclidean norm reduce the sensitivity of MVKM-ED.","To this end, this paper designated as Gaussian-kernel multi-view k-means (GKMVKM) clustering algorithm.","Numerical evaluation of five real-world multi-view data demonstrates the robustness and efficiency of our proposed MVKM-ED and GKMVKM approaches."],"url":"http://arxiv.org/abs/2405.05619v1","category":"cs.LG"}
{"created":"2024-05-09 08:32:55","title":"An Automatic Prompt Generation System for Tabular Data Tasks","abstract":"Efficient processing of tabular data is important in various industries, especially when working with datasets containing a large number of columns. Large language models (LLMs) have demonstrated their ability on several tasks through carefully crafted prompts. However, creating effective prompts for tabular datasets is challenging due to the structured nature of the data and the need to manage numerous columns. This paper presents an innovative auto-prompt generation system suitable for multiple LLMs, with minimal training. It proposes two novel methods; 1) A Reinforcement Learning-based algorithm for identifying and sequencing task-relevant columns 2) Cell-level similarity-based approach for enhancing few-shot example selection. Our approach has been extensively tested across 66 datasets, demonstrating improved performance in three downstream tasks: data imputation, error detection, and entity matching using two distinct LLMs; Google flan-t5-xxl and Mixtral 8x7B.","sentences":["Efficient processing of tabular data is important in various industries, especially when working with datasets containing a large number of columns.","Large language models (LLMs) have demonstrated their ability on several tasks through carefully crafted prompts.","However, creating effective prompts for tabular datasets is challenging due to the structured nature of the data and the need to manage numerous columns.","This paper presents an innovative auto-prompt generation system suitable for multiple LLMs, with minimal training.","It proposes two novel methods; 1) A Reinforcement Learning-based algorithm for identifying and sequencing task-relevant columns 2) Cell-level similarity-based approach for enhancing few-shot example selection.","Our approach has been extensively tested across 66 datasets, demonstrating improved performance in three downstream tasks: data imputation, error detection, and entity matching using two distinct LLMs; Google flan-t5-xxl and Mixtral 8x7B."],"url":"http://arxiv.org/abs/2405.05618v1","category":"cs.LG"}
{"created":"2024-05-09 08:28:12","title":"G-SAP: Graph-based Structure-Aware Prompt Learning over Heterogeneous Knowledge for Commonsense Reasoning","abstract":"Commonsense question answering has demonstrated considerable potential across various applications like assistants and social robots. Although fully fine-tuned pre-trained Language Models(LM) have achieved remarkable performance in commonsense reasoning, their tendency to excessively prioritize textual information hampers the precise transfer of structural knowledge and undermines interpretability. Some studies have explored combining LMs with Knowledge Graphs(KGs) by coarsely fusing the two modalities to perform Graph Neural Network(GNN)-based reasoning that lacks a profound interaction between heterogeneous modalities. In this paper, we propose a novel Graph-based Structure-Aware Prompt Learning Model for commonsense reasoning, named G-SAP, aiming to maintain a balance between heterogeneous knowledge and enhance the cross-modal interaction within the LM+GNNs model. In particular, an evidence graph is constructed by integrating multiple knowledge sources, i.e. ConceptNet, Wikipedia, and Cambridge Dictionary to boost the performance. Afterward, a structure-aware frozen PLM is employed to fully incorporate the structured and textual information from the evidence graph, where the generation of prompts is driven by graph entities and relations. Finally, a heterogeneous message-passing reasoning module is used to facilitate deep interaction of knowledge between the LM and graph-based networks. Empirical validation, conducted through extensive experiments on three benchmark datasets, demonstrates the notable performance of the proposed model. The results reveal a significant advancement over the existing models, especially, with 6.12% improvement over the SoTA LM+GNNs model on the OpenbookQA dataset.","sentences":["Commonsense question answering has demonstrated considerable potential across various applications like assistants and social robots.","Although fully fine-tuned pre-trained Language Models(LM) have achieved remarkable performance in commonsense reasoning, their tendency to excessively prioritize textual information hampers the precise transfer of structural knowledge and undermines interpretability.","Some studies have explored combining LMs with Knowledge Graphs(KGs) by coarsely fusing the two modalities to perform Graph Neural Network(GNN)-based reasoning that lacks a profound interaction between heterogeneous modalities.","In this paper, we propose a novel Graph-based Structure-Aware Prompt Learning Model for commonsense reasoning, named G-SAP, aiming to maintain a balance between heterogeneous knowledge and enhance the cross-modal interaction within the LM+GNNs model.","In particular, an evidence graph is constructed by integrating multiple knowledge sources, i.e. ConceptNet, Wikipedia, and Cambridge Dictionary to boost the performance.","Afterward, a structure-aware frozen PLM is employed to fully incorporate the structured and textual information from the evidence graph, where the generation of prompts is driven by graph entities and relations.","Finally, a heterogeneous message-passing reasoning module is used to facilitate deep interaction of knowledge between the LM and graph-based networks.","Empirical validation, conducted through extensive experiments on three benchmark datasets, demonstrates the notable performance of the proposed model.","The results reveal a significant advancement over the existing models, especially, with 6.12% improvement over the SoTA LM+GNNs model on the OpenbookQA dataset."],"url":"http://arxiv.org/abs/2405.05616v1","category":"cs.CL"}
{"created":"2024-05-09 08:15:31","title":"Privacy-Preserving Edge Federated Learning for Intelligent Mobile-Health Systems","abstract":"Machine Learning (ML) algorithms are generally designed for scenarios in which all data is stored in one data center, where the training is performed. However, in many applications, e.g., in the healthcare domain, the training data is distributed among several entities, e.g., different hospitals or patients' mobile devices/sensors. At the same time, transferring the data to a central location for learning is certainly not an option, due to privacy concerns and legal issues, and in certain cases, because of the communication and computation overheads. Federated Learning (FL) is the state-of-the-art collaborative ML approach for training an ML model across multiple parties holding local data samples, without sharing them. However, enabling learning from distributed data over such edge Internet of Things (IoT) systems (e.g., mobile-health and wearable technologies, involving sensitive personal/medical data) in a privacy-preserving fashion presents a major challenge mainly due to their stringent resource constraints, i.e., limited computing capacity, communication bandwidth, memory storage, and battery lifetime. In this paper, we propose a privacy-preserving edge FL framework for resource-constrained mobile-health and wearable technologies over the IoT infrastructure. We evaluate our proposed framework extensively and provide the implementation of our technique on Amazon's AWS cloud platform based on the seizure detection application in epilepsy monitoring using wearable technologies.","sentences":["Machine Learning (ML) algorithms are generally designed for scenarios in which all data is stored in one data center, where the training is performed.","However, in many applications, e.g., in the healthcare domain, the training data is distributed among several entities, e.g., different hospitals or patients' mobile devices/sensors.","At the same time, transferring the data to a central location for learning is certainly not an option, due to privacy concerns and legal issues, and in certain cases, because of the communication and computation overheads.","Federated Learning (FL) is the state-of-the-art collaborative ML approach for training an ML model across multiple parties holding local data samples, without sharing them.","However, enabling learning from distributed data over such edge Internet of Things (IoT) systems (e.g., mobile-health and wearable technologies, involving sensitive personal/medical data) in a privacy-preserving fashion presents a major challenge mainly due to their stringent resource constraints, i.e., limited computing capacity, communication bandwidth, memory storage, and battery lifetime.","In this paper, we propose a privacy-preserving edge FL framework for resource-constrained mobile-health and wearable technologies over the IoT infrastructure.","We evaluate our proposed framework extensively and provide the implementation of our technique on Amazon's AWS cloud platform based on the seizure detection application in epilepsy monitoring using wearable technologies."],"url":"http://arxiv.org/abs/2405.05611v1","category":"cs.LG"}
{"created":"2024-05-09 07:55:52","title":"Optimizing E-commerce Search: Toward a Generalizable and Rank-Consistent Pre-Ranking Model","abstract":"In large e-commerce platforms, search systems are typically composed of a series of modules, including recall, pre-ranking, and ranking phases. The pre-ranking phase, serving as a lightweight module, is crucial for filtering out the bulk of products in advance for the downstream ranking module. Industrial efforts on optimizing the pre-ranking model have predominantly focused on enhancing ranking consistency, model structure, and generalization towards long-tail items. Beyond these optimizations, meeting the system performance requirements presents a significant challenge. Contrasting with existing industry works, we propose a novel method: a Generalizable and RAnk-ConsistEnt Pre-Ranking Model (GRACE), which achieves: 1) Ranking consistency by introducing multiple binary classification tasks that predict whether a product is within the top-k results as estimated by the ranking model, which facilitates the addition of learning objectives on common point-wise ranking models; 2) Generalizability through contrastive learning of representation for all products by pre-training on a subset of ranking product embeddings; 3) Ease of implementation in feature construction and online deployment. Our extensive experiments demonstrate significant improvements in both offline metrics and online A/B test: a 0.75% increase in AUC and a 1.28% increase in CVR.","sentences":["In large e-commerce platforms, search systems are typically composed of a series of modules, including recall, pre-ranking, and ranking phases.","The pre-ranking phase, serving as a lightweight module, is crucial for filtering out the bulk of products in advance for the downstream ranking module.","Industrial efforts on optimizing the pre-ranking model have predominantly focused on enhancing ranking consistency, model structure, and generalization towards long-tail items.","Beyond these optimizations, meeting the system performance requirements presents a significant challenge.","Contrasting with existing industry works, we propose a novel method: a Generalizable and RAnk-ConsistEnt Pre-Ranking Model (GRACE), which achieves: 1) Ranking consistency by introducing multiple binary classification tasks that predict whether a product is within the top-k results as estimated by the ranking model, which facilitates the addition of learning objectives on common point-wise ranking models; 2) Generalizability through contrastive learning of representation for all products by pre-training on a subset of ranking product embeddings; 3) Ease of implementation in feature construction and online deployment.","Our extensive experiments demonstrate significant improvements in both offline metrics and online A/B test: a 0.75% increase in AUC and a 1.28% increase in CVR."],"url":"http://arxiv.org/abs/2405.05606v1","category":"cs.IR"}
{"created":"2024-05-09 07:52:43","title":"Modified extremal K\u00e4hler metrics and multiplier Hermitian-Einstein metrics","abstract":"Motivated by the notion of multiplier Hermitian-Einstein metric of type $\\sigma$ introduced by Mabuchi, we introduce the notion of $\\sigma$-extremal K\\\"{a}hler metrics on compact K\\\"{a}hler manifolds, which generalizes Calabi's extremal K\\\"{a}hler metrics. We characterize the existence of this metric in terms of the coercivity of a certain functional on the space of K\\\"{a}hler metrics to show that, on a Fano manifold, the existence of a $\\sigma$-extremal K\\\"{a}hler metric implies the existence of a multiplier Hermitian-Einstein metric of type $\\sigma$.","sentences":["Motivated by the notion of multiplier Hermitian-Einstein metric of type $\\sigma$ introduced by Mabuchi, we introduce the notion of $\\sigma$-extremal K\\\"{a}hler metrics on compact K\\\"{a}hler manifolds, which generalizes Calabi's extremal K\\\"{a}hler metrics.","We characterize the existence of this metric in terms of the coercivity of a certain functional on the space of K\\\"{a}hler metrics to show that, on a Fano manifold, the existence of a $\\sigma$-extremal K\\\"{a}hler metric implies the existence of a multiplier Hermitian-Einstein metric of type $\\sigma$."],"url":"http://arxiv.org/abs/2405.05604v1","category":"math.DG"}
{"created":"2024-05-09 07:52:30","title":"Twisting factors and fixed-time models in quantum field theory","abstract":"We construct a class of fixed-time models in which the commutations relations of a Dirac field with a bosonic field are non-trivial and depend on the choice of a given distribution (\"twisting factor\"). If the twisting factor is fundamental solution of a differential operator, then applying the differential operator to the bosonic field yields a generator of the local gauge transformations of the Dirac field. Charged vectors generated by the Dirac field define states of the bosonic field which in general are not local excitations of the given reference state. The Hamiltonian density of the bosonic field presents a non-trivial interaction term: besides creating and annihilating bosons, it acts on momenta of fermionic wave functions. When the twisting factor is the Coulomb potential, the bosonic field contributes to the divergence of an electric field and its Laplacian generates local gauge transformations of the Dirac field. In this way we get a fixed-time model fulfilling the equal time commutation relations of the interacting Coulomb gauge.","sentences":["We construct a class of fixed-time models in which the commutations relations of a Dirac field with a bosonic field are non-trivial and depend on the choice of a given distribution (\"twisting factor\").","If the twisting factor is fundamental solution of a differential operator, then applying the differential operator to the bosonic field yields a generator of the local gauge transformations of the Dirac field.","Charged vectors generated by the Dirac field define states of the bosonic field which in general are not local excitations of the given reference state.","The Hamiltonian density of the bosonic field presents a non-trivial interaction term: besides creating and annihilating bosons, it acts on momenta of fermionic wave functions.","When the twisting factor is the Coulomb potential, the bosonic field contributes to the divergence of an electric field and its Laplacian generates local gauge transformations of the Dirac field.","In this way we get a fixed-time model fulfilling the equal time commutation relations of the interacting Coulomb gauge."],"url":"http://arxiv.org/abs/2405.05603v1","category":"math-ph"}
{"created":"2024-05-09 07:47:05","title":"Efficient Algorithms for Top-k Stabbing Queries on Weighted Interval Data (Full Version)","abstract":"Intervals have been generated in many applications (e.g., temporal databases), and they are often associated with weights, such as prices. This paper addresses the problem of processing top-k weighted stabbing queries on interval data. Given a set of weighted intervals, a query value, and a result size k, this problem finds the k intervals that are stabbed by the query value and have the largest weights. Although this problem finds practical applications (e.g., purchase, vehicle, and cryptocurrency analysis), it has not been well studied. A state-of-the-art algorithm for this problem incurs O(nlogk) time, where n is the number of intervals, so it is not scalable to large n. We solve this inefficiency issue and propose an algorithm that runs in O(sqrt(n)logn + k) time. Furthermore, we propose an O(logn + k) algorithm to further accelerate the search efficiency. Experiments on two real large datasets demonstrate that our algorithms are faster than existing algorithms.","sentences":["Intervals have been generated in many applications (e.g., temporal databases), and they are often associated with weights, such as prices.","This paper addresses the problem of processing top-k weighted stabbing queries on interval data.","Given a set of weighted intervals, a query value, and a result size k, this problem finds the k intervals that are stabbed by the query value and have the largest weights.","Although this problem finds practical applications (e.g., purchase, vehicle, and cryptocurrency analysis), it has not been well studied.","A state-of-the-art algorithm for this problem incurs O(nlogk) time, where n is the number of intervals, so it is not scalable to large n. We solve this inefficiency issue and propose an algorithm that runs in O(sqrt(n)logn + k) time.","Furthermore, we propose an O(logn + k) algorithm to further accelerate the search efficiency.","Experiments on two real large datasets demonstrate that our algorithms are faster than existing algorithms."],"url":"http://arxiv.org/abs/2405.05601v1","category":"cs.DB"}
{"created":"2024-05-09 07:39:19","title":"Can We Use Large Language Models to Fill Relevance Judgment Holes?","abstract":"Incomplete relevance judgments limit the re-usability of test collections. When new systems are compared against previous systems used to build the pool of judged documents, they often do so at a disadvantage due to the ``holes'' in test collection (i.e., pockets of un-assessed documents returned by the new system). In this paper, we take initial steps towards extending existing test collections by employing Large Language Models (LLM) to fill the holes by leveraging and grounding the method using existing human judgments. We explore this problem in the context of Conversational Search using TREC iKAT, where information needs are highly dynamic and the responses (and, the results retrieved) are much more varied (leaving bigger holes). While previous work has shown that automatic judgments from LLMs result in highly correlated rankings, we find substantially lower correlates when human plus automatic judgments are used (regardless of LLM, one/two/few shot, or fine-tuned). We further find that, depending on the LLM employed, new runs will be highly favored (or penalized), and this effect is magnified proportionally to the size of the holes. Instead, one should generate the LLM annotations on the whole document pool to achieve more consistent rankings with human-generated labels. Future work is required to prompt engineering and fine-tuning LLMs to reflect and represent the human annotations, in order to ground and align the models, such that they are more fit for purpose.","sentences":["Incomplete relevance judgments limit the re-usability of test collections.","When new systems are compared against previous systems used to build the pool of judged documents, they often do so at a disadvantage due to the ``holes'' in test collection (i.e., pockets of un-assessed documents returned by the new system).","In this paper, we take initial steps towards extending existing test collections by employing Large Language Models (LLM) to fill the holes by leveraging and grounding the method using existing human judgments.","We explore this problem in the context of Conversational Search using TREC iKAT, where information needs are highly dynamic and the responses (and, the results retrieved) are much more varied (leaving bigger holes).","While previous work has shown that automatic judgments from LLMs result in highly correlated rankings, we find substantially lower correlates when human plus automatic judgments are used (regardless of LLM, one/two/few shot, or fine-tuned).","We further find that, depending on the LLM employed, new runs will be highly favored (or penalized), and this effect is magnified proportionally to the size of the holes.","Instead, one should generate the LLM annotations on the whole document pool to achieve more consistent rankings with human-generated labels.","Future work is required to prompt engineering and fine-tuning LLMs to reflect and represent the human annotations, in order to ground and align the models, such that they are more fit for purpose."],"url":"http://arxiv.org/abs/2405.05600v1","category":"cs.IR"}
{"created":"2024-05-09 07:37:47","title":"Denoising Diffusion Delensing Delight: Reconstructing the Non-Gaussian CMB Lensing Potential with Diffusion Models","abstract":"Optimal extraction of cosmological information from observations of the Cosmic Microwave Back- ground critically relies on our ability to accurately undo the distortions caused by weak gravitational lensing. In this work, we demonstrate the use of denoising diffusion models in performing Bayesian lensing reconstruction. We show that score-based generative models can produce accurate, uncor- related samples from the CMB lensing convergence map posterior, given noisy CMB observations. To validate our approach, we compare the samples of our model to those obtained using established Hamiltonian Monte Carlo methods, which assume a Gaussian lensing potential. We then go beyond this assumption of Gaussianity, and train and validate our model on non-Gaussian lensing data, obtained by ray-tracing N-body simulations. We demonstrate that in this case, samples from our model have accurate non-Gaussian statistics beyond the power spectrum. The method provides an avenue towards more efficient and accurate lensing reconstruction, that does not rely on an approx- imate analytic description of the posterior probability. The reconstructed lensing maps can be used as an unbiased tracer of the matter distribution, and to improve delensing of the CMB, resulting in more precise cosmological parameter inference.","sentences":["Optimal extraction of cosmological information from observations of the Cosmic Microwave Back- ground critically relies on our ability to accurately undo the distortions caused by weak gravitational lensing.","In this work, we demonstrate the use of denoising diffusion models in performing Bayesian lensing reconstruction.","We show that score-based generative models can produce accurate, uncor- related samples from the CMB lensing convergence map posterior, given noisy CMB observations.","To validate our approach, we compare the samples of our model to those obtained using established Hamiltonian Monte Carlo methods, which assume a Gaussian lensing potential.","We then go beyond this assumption of Gaussianity, and train and validate our model on non-Gaussian lensing data, obtained by ray-tracing N-body simulations.","We demonstrate that in this case, samples from our model have accurate non-Gaussian statistics beyond the power spectrum.","The method provides an avenue towards more efficient and accurate lensing reconstruction, that does not rely on an approx- imate analytic description of the posterior probability.","The reconstructed lensing maps can be used as an unbiased tracer of the matter distribution, and to improve delensing of the CMB, resulting in more precise cosmological parameter inference."],"url":"http://arxiv.org/abs/2405.05598v1","category":"astro-ph.CO"}
{"created":"2024-05-09 07:37:37","title":"The empirical copula process in high dimensions: Stute's representation and applications","abstract":"The empirical copula process, a fundamental tool for copula inference, is studied in the high dimensional regime where the dimension is allowed to grow to infinity exponentially in the sample size. Under natural, weak smoothness assumptions on the underlying copula, it is shown that Stute's representation is valid in the following sense: all low-dimensional margins of fixed dimension of the empirical copula process can be approximated by a functional of the low-dimensional margins of the standard empirical process, with the almost sure error term being uniform in the margins. The result has numerous potential applications, and is exemplary applied to the problem of testing pairwise stochastic independence in high dimensions, leading to various extensions of recent results in the literature: for certain test statistics based on pairwise association measures, type-I error control is obtained for models beyond mutual independence. Moreover, bootstrap-based critical values are shown to yield strong control of the familywise error rate for a large class of data generating processes.","sentences":["The empirical copula process, a fundamental tool for copula inference, is studied in the high dimensional regime where the dimension is allowed to grow to infinity exponentially in the sample size.","Under natural, weak smoothness assumptions on the underlying copula, it is shown that Stute's representation is valid in the following sense: all low-dimensional margins of fixed dimension of the empirical copula process can be approximated by a functional of the low-dimensional margins of the standard empirical process, with the almost sure error term being uniform in the margins.","The result has numerous potential applications, and is exemplary applied to the problem of testing pairwise stochastic independence in high dimensions, leading to various extensions of recent results in the literature: for certain test statistics based on pairwise association measures, type-I error control is obtained for models beyond mutual independence.","Moreover, bootstrap-based critical values are shown to yield strong control of the familywise error rate for a large class of data generating processes."],"url":"http://arxiv.org/abs/2405.05597v1","category":"math.ST"}
{"created":"2024-05-09 07:36:08","title":"Measuring Strategization in Recommendation: Users Adapt Their Behavior to Shape Future Content","abstract":"Most modern recommendation algorithms are data-driven: they generate personalized recommendations by observing users' past behaviors. A common assumption in recommendation is that how a user interacts with a piece of content (e.g., whether they choose to \"like\" it) is a reflection of the content, but not of the algorithm that generated it. Although this assumption is convenient, it fails to capture user strategization: that users may attempt to shape their future recommendations by adapting their behavior to the recommendation algorithm. In this work, we test for user strategization by conducting a lab experiment and survey. To capture strategization, we adopt a model in which strategic users select their engagement behavior based not only on the content, but also on how their behavior affects downstream recommendations. Using a custom music player that we built, we study how users respond to different information about their recommendation algorithm as well as to different incentives about how their actions affect downstream outcomes. We find strong evidence of strategization across outcome metrics, including participants' dwell time and use of \"likes.\" For example, participants who are told that the algorithm mainly pays attention to \"likes\" and \"dislikes\" use those functions 1.9x more than participants told that the algorithm mainly pays attention to dwell time. A close analysis of participant behavior (e.g., in response to our incentive conditions) rules out experimenter demand as the main driver of these trends. Further, in our post-experiment survey, nearly half of participants self-report strategizing \"in the wild,\" with some stating that they ignore content they actually like to avoid over-recommendation of that content in the future. Together, our findings suggest that user strategization is common and that platforms cannot ignore the effect of their algorithms on user behavior.","sentences":["Most modern recommendation algorithms are data-driven: they generate personalized recommendations by observing users' past behaviors.","A common assumption in recommendation is that how a user interacts with a piece of content (e.g., whether they choose to \"like\" it) is a reflection of the content, but not of the algorithm that generated it.","Although this assumption is convenient, it fails to capture user strategization: that users may attempt to shape their future recommendations by adapting their behavior to the recommendation algorithm.","In this work, we test for user strategization by conducting a lab experiment and survey.","To capture strategization, we adopt a model in which strategic users select their engagement behavior based not only on the content, but also on how their behavior affects downstream recommendations.","Using a custom music player that we built, we study how users respond to different information about their recommendation algorithm as well as to different incentives about how their actions affect downstream outcomes.","We find strong evidence of strategization across outcome metrics, including participants' dwell time and use of \"likes.\"","For example, participants who are told that the algorithm mainly pays attention to \"likes\" and \"dislikes\" use those functions 1.9x more than participants told that the algorithm mainly pays attention to dwell time.","A close analysis of participant behavior (e.g., in response to our incentive conditions) rules out experimenter demand as the main driver of these trends.","Further, in our post-experiment survey, nearly half of participants self-report strategizing \"in the wild,\" with some stating that they ignore content they actually like to avoid over-recommendation of that content in the future.","Together, our findings suggest that user strategization is common and that platforms cannot ignore the effect of their algorithms on user behavior."],"url":"http://arxiv.org/abs/2405.05596v1","category":"cs.CY"}
{"created":"2024-05-09 07:33:06","title":"Expected Work Search: Combining Win Rate and Proof Size Estimation","abstract":"We propose Expected Work Search (EWS), a new game solving algorithm. EWS combines win rate estimation, as used in Monte Carlo Tree Search, with proof size estimation, as used in Proof Number Search. The search efficiency of EWS stems from minimizing a novel notion of Expected Work, which predicts the expected computation required to solve a position. EWS outperforms traditional solving algorithms on the games of Go and Hex. For Go, we present the first solution to the empty 5x5 board with the commonly used positional superko ruleset. For Hex, our algorithm solves the empty 8x8 board in under 4 minutes. Experiments show that EWS succeeds both with and without extensive domain-specific knowledge.","sentences":["We propose Expected Work Search (EWS), a new game solving algorithm.","EWS combines win rate estimation, as used in Monte Carlo Tree Search, with proof size estimation, as used in Proof Number Search.","The search efficiency of EWS stems from minimizing a novel notion of Expected Work, which predicts the expected computation required to solve a position.","EWS outperforms traditional solving algorithms on the games of Go and Hex.","For Go, we present the first solution to the empty 5x5 board with the commonly used positional superko ruleset.","For Hex, our algorithm solves the empty 8x8 board in under 4 minutes.","Experiments show that EWS succeeds both with and without extensive domain-specific knowledge."],"url":"http://arxiv.org/abs/2405.05594v1","category":"cs.AI"}
{"created":"2024-05-09 07:25:38","title":"TroLLoc: Logic Locking and Layout Hardening for IC Security Closure against Hardware Trojans","abstract":"Due to cost benefits, supply chains of integrated circuits (ICs) are largely outsourced nowadays. However, passing ICs through various third-party providers gives rise to many security threats, like piracy of IC intellectual property or insertion of hardware Trojans, i.e., malicious circuit modifications.   In this work, we proactively and systematically protect the physical layouts of ICs against post-design insertion of Trojans. Toward that end, we propose TroLLoc, a novel scheme for IC security closure that employs, for the first time, logic locking and layout hardening in unison. TroLLoc is fully integrated into a commercial-grade design flow, and TroLLoc is shown to be effective, efficient, and robust. Our work provides in-depth layout and security analysis considering the challenging benchmarks of the ISPD'22/23 contests for security closure. We show that TroLLoc successfully renders layouts resilient, with reasonable overheads, against (i) general prospects for Trojan insertion as in the ISPD'22 contest, (ii) actual Trojan insertion as in the ISPD'23 contest, and (iii) potential second-order attacks where adversaries would first (i.e., before Trojan insertion) try to bypass the locking defense, e.g., using advanced machine learning attacks. Finally, we release all our artifacts for independent verification [2].","sentences":["Due to cost benefits, supply chains of integrated circuits (ICs) are largely outsourced nowadays.","However, passing ICs through various third-party providers gives rise to many security threats, like piracy of IC intellectual property or insertion of hardware Trojans, i.e., malicious circuit modifications.   ","In this work, we proactively and systematically protect the physical layouts of ICs against post-design insertion of Trojans.","Toward that end, we propose TroLLoc, a novel scheme for IC security closure that employs, for the first time, logic locking and layout hardening in unison.","TroLLoc is fully integrated into a commercial-grade design flow, and TroLLoc is shown to be effective, efficient, and robust.","Our work provides in-depth layout and security analysis considering the challenging benchmarks of the ISPD'22/23 contests for security closure.","We show that TroLLoc successfully renders layouts resilient, with reasonable overheads, against (i) general prospects for Trojan insertion as in the ISPD'22 contest, (ii) actual Trojan insertion as in the ISPD'23 contest, and (iii) potential second-order attacks where adversaries would first (i.e., before Trojan insertion) try to bypass the locking defense, e.g., using advanced machine learning attacks.","Finally, we release all our artifacts for independent verification [2]."],"url":"http://arxiv.org/abs/2405.05590v1","category":"cs.CR"}
{"created":"2024-05-09 07:23:37","title":"Navigate Beyond Shortcuts: Debiased Learning through the Lens of Neural Collapse","abstract":"Recent studies have noted an intriguing phenomenon termed Neural Collapse, that is, when the neural networks establish the right correlation between feature spaces and the training targets, their last-layer features, together with the classifier weights, will collapse into a stable and symmetric structure. In this paper, we extend the investigation of Neural Collapse to the biased datasets with imbalanced attributes. We observe that models will easily fall into the pitfall of shortcut learning and form a biased, non-collapsed feature space at the early period of training, which is hard to reverse and limits the generalization capability. To tackle the root cause of biased classification, we follow the recent inspiration of prime training, and propose an avoid-shortcut learning framework without additional training complexity. With well-designed shortcut primes based on Neural Collapse structure, the models are encouraged to skip the pursuit of simple shortcuts and naturally capture the intrinsic correlations. Experimental results demonstrate that our method induces better convergence properties during training, and achieves state-of-the-art generalization performance on both synthetic and real-world biased datasets.","sentences":["Recent studies have noted an intriguing phenomenon termed Neural Collapse, that is, when the neural networks establish the right correlation between feature spaces and the training targets, their last-layer features, together with the classifier weights, will collapse into a stable and symmetric structure.","In this paper, we extend the investigation of Neural Collapse to the biased datasets with imbalanced attributes.","We observe that models will easily fall into the pitfall of shortcut learning and form a biased, non-collapsed feature space at the early period of training, which is hard to reverse and limits the generalization capability.","To tackle the root cause of biased classification, we follow the recent inspiration of prime training, and propose an avoid-shortcut learning framework without additional training complexity.","With well-designed shortcut primes based on Neural Collapse structure, the models are encouraged to skip the pursuit of simple shortcuts and naturally capture the intrinsic correlations.","Experimental results demonstrate that our method induces better convergence properties during training, and achieves state-of-the-art generalization performance on both synthetic and real-world biased datasets."],"url":"http://arxiv.org/abs/2405.05587v1","category":"cs.CV"}
{"created":"2024-05-09 07:21:15","title":"Suppression of Coherent Synchrotron Radiation-Induced Emittance Growth in a Multi-Bend Deflection Line","abstract":"Preserving beam quality during the transportation of high-brightness electron beams is a significant and widespread challenge in the design of modern accelerators. The importance of this issue stems from the fact that the quality of the beam at the accelerator's output is crucial for various applications, including particle colliders, free-electron lasers, and synchrotron radiation sources. The coherent synchrotron radiation (CSR) effect can degrade beam quality when a bunch is deflected. Therefore, developing a structure that effectively suppresses the CSR effect, especially for the short bunches is critically important. This involves protecting both the transverse emittance and the longitudinal profile to ensure the production of a high-quality beam. In this study, an optimization based on the reverse lattice of the beamline is proposed. This method can simplify the optimization process. Based on this approach, the Quadruple Bend Achromat (QBA) deflection structure has been designed and optimized. Then we have derived a general solution to completely suppress the impact of steady-state CSR on the transverse plane for different topologies of QBA. Furthermore, a general condition is proposed for suppressing displacements caused by CSR in sequence drifts for isochronous structures. Simultaneously, QBA has proven to be the simplest structure that can simultaneously suppress both types of CSR effects. Simulation results for bunches with a peak current of up to $3000A$ show almost no change in transverse emittance for a large angle deflection.","sentences":["Preserving beam quality during the transportation of high-brightness electron beams is a significant and widespread challenge in the design of modern accelerators.","The importance of this issue stems from the fact that the quality of the beam at the accelerator's output is crucial for various applications, including particle colliders, free-electron lasers, and synchrotron radiation sources.","The coherent synchrotron radiation (CSR) effect can degrade beam quality when a bunch is deflected.","Therefore, developing a structure that effectively suppresses the CSR effect, especially for the short bunches is critically important.","This involves protecting both the transverse emittance and the longitudinal profile to ensure the production of a high-quality beam.","In this study, an optimization based on the reverse lattice of the beamline is proposed.","This method can simplify the optimization process.","Based on this approach, the Quadruple Bend Achromat (QBA) deflection structure has been designed and optimized.","Then we have derived a general solution to completely suppress the impact of steady-state CSR on the transverse plane for different topologies of QBA.","Furthermore, a general condition is proposed for suppressing displacements caused by CSR in sequence drifts for isochronous structures.","Simultaneously, QBA has proven to be the simplest structure that can simultaneously suppress both types of CSR effects.","Simulation results for bunches with a peak current of up to $3000A$ show almost no change in transverse emittance for a large angle deflection."],"url":"http://arxiv.org/abs/2405.05585v1","category":"physics.acc-ph"}
{"created":"2024-05-09 07:20:36","title":"A Survey on Backbones for Deep Video Action Recognition","abstract":"Action recognition is a key technology in building interactive metaverses. With the rapid development of deep learning, methods in action recognition have also achieved great advancement. Researchers design and implement the backbones referring to multiple standpoints, which leads to the diversity of methods and encountering new challenges. This paper reviews several action recognition methods based on deep neural networks. We introduce these methods in three parts: 1) Two-Streams networks and their variants, which, specifically in this paper, use RGB video frame and optical flow modality as input; 2) 3D convolutional networks, which make efforts in taking advantage of RGB modality directly while extracting different motion information is no longer necessary; 3) Transformer-based methods, which introduce the model from natural language processing into computer vision and video understanding. We offer objective sights in this review and hopefully provide a reference for future research.","sentences":["Action recognition is a key technology in building interactive metaverses.","With the rapid development of deep learning, methods in action recognition have also achieved great advancement.","Researchers design and implement the backbones referring to multiple standpoints, which leads to the diversity of methods and encountering new challenges.","This paper reviews several action recognition methods based on deep neural networks.","We introduce these methods in three parts: 1) Two-Streams networks and their variants, which, specifically in this paper, use RGB video frame and optical flow modality as input; 2) 3D convolutional networks, which make efforts in taking advantage of RGB modality directly while extracting different motion information is no longer necessary; 3) Transformer-based methods, which introduce the model from natural language processing into computer vision and video understanding.","We offer objective sights in this review and hopefully provide a reference for future research."],"url":"http://arxiv.org/abs/2405.05584v1","category":"cs.CV"}
{"created":"2024-05-09 07:12:45","title":"One vs. Many: Comprehending Accurate Information from Multiple Erroneous and Inconsistent AI Generations","abstract":"As Large Language Models (LLMs) are nondeterministic, the same input can generate different outputs, some of which may be incorrect or hallucinated. If run again, the LLM may correct itself and produce the correct answer. Unfortunately, most LLM-powered systems resort to single results which, correct or not, users accept. Having the LLM produce multiple outputs may help identify disagreements or alternatives. However, it is not obvious how the user will interpret conflicts or inconsistencies. To this end, we investigate how users perceive the AI model and comprehend the generated information when they receive multiple, potentially inconsistent, outputs. Through a preliminary study, we identified five types of output inconsistencies. Based on these categories, we conducted a study (N=252) in which participants were given one or more LLM-generated passages to an information-seeking question. We found that inconsistency within multiple LLM-generated outputs lowered the participants' perceived AI capacity, while also increasing their comprehension of the given information. Specifically, we observed that this positive effect of inconsistencies was most significant for participants who read two passages, compared to those who read three. Based on these findings, we present design implications that, instead of regarding LLM output inconsistencies as a drawback, we can reveal the potential inconsistencies to transparently indicate the limitations of these models and promote critical LLM usage.","sentences":["As Large Language Models (LLMs) are nondeterministic, the same input can generate different outputs, some of which may be incorrect or hallucinated.","If run again, the LLM may correct itself and produce the correct answer.","Unfortunately, most LLM-powered systems resort to single results which, correct or not, users accept.","Having the LLM produce multiple outputs may help identify disagreements or alternatives.","However, it is not obvious how the user will interpret conflicts or inconsistencies.","To this end, we investigate how users perceive the AI model and comprehend the generated information when they receive multiple, potentially inconsistent, outputs.","Through a preliminary study, we identified five types of output inconsistencies.","Based on these categories, we conducted a study (N=252) in which participants were given one or more LLM-generated passages to an information-seeking question.","We found that inconsistency within multiple LLM-generated outputs lowered the participants' perceived AI capacity, while also increasing their comprehension of the given information.","Specifically, we observed that this positive effect of inconsistencies was most significant for participants who read two passages, compared to those who read three.","Based on these findings, we present design implications that, instead of regarding LLM output inconsistencies as a drawback, we can reveal the potential inconsistencies to transparently indicate the limitations of these models and promote critical LLM usage."],"url":"http://arxiv.org/abs/2405.05581v1","category":"cs.HC"}
{"created":"2024-05-09 07:10:47","title":"Intelligent EC Rearview Mirror: Enhancing Driver Safety with Dynamic Glare Mitigation via Cloud Edge Collaboration","abstract":"Sudden glare from trailing vehicles significantly increases driving safety risks. Existing anti-glare technologies such as electronic, manually-adjusted, and electrochromic rearview mirrors, are expensive and lack effective adaptability in different lighting conditions. To address these issues, our research introduces an intelligent rearview mirror system utilizing novel all-liquid electrochromic technology. This system integrates IoT with ensemble and federated learning within a cloud edge collaboration framework, dynamically controlling voltage to effectively eliminate glare and maintain clear visibility. Utilizing an ensemble learning model, it automatically adjusts mirror transmittance based on light intensity, achieving a low RMSE of 0.109 on the test set. Furthermore, the system leverages federated learning for distributed data training across devices, which enhances privacy and updates the cloud model continuously. Distinct from conventional methods, our experiment utilizes the Schmidt-Clausen and Bindels de Boer 9-point scale with TOPSIS for comprehensive evaluation of rearview mirror glare. Designed to be convenient and costeffective, this system demonstrates how IoT and AI can significantly enhance rearview mirror anti-glare performance.","sentences":["Sudden glare from trailing vehicles significantly increases driving safety risks.","Existing anti-glare technologies such as electronic, manually-adjusted, and electrochromic rearview mirrors, are expensive and lack effective adaptability in different lighting conditions.","To address these issues, our research introduces an intelligent rearview mirror system utilizing novel all-liquid electrochromic technology.","This system integrates IoT with ensemble and federated learning within a cloud edge collaboration framework, dynamically controlling voltage to effectively eliminate glare and maintain clear visibility.","Utilizing an ensemble learning model, it automatically adjusts mirror transmittance based on light intensity, achieving a low RMSE of 0.109 on the test set.","Furthermore, the system leverages federated learning for distributed data training across devices, which enhances privacy and updates the cloud model continuously.","Distinct from conventional methods, our experiment utilizes the Schmidt-Clausen and Bindels de Boer 9-point scale with TOPSIS for comprehensive evaluation of rearview mirror glare.","Designed to be convenient and costeffective, this system demonstrates how IoT and AI can significantly enhance rearview mirror anti-glare performance."],"url":"http://arxiv.org/abs/2405.05579v1","category":"cs.HC"}
{"created":"2024-05-09 07:08:33","title":"Shaping the Future of Urban Mobility: Insights into Autonomous Vehicle Acceptance in Shanghai Through TAM and Perceived Risk Analysis","abstract":"Autonomous vehicles (AVs) have begun experimental commercialization initiatives in places such as Shanghai, China, and it is a valuable research question whether people's willingness to use AVs has changed from the prior. This study explores Shanghai residents' attitudes towards AVs by applying the Technology Acceptance Model (TAM), the Perceived Risk (BAR) model, and introducing perceived externalities as a new psychological variable. Through a survey in Shanghai, where AVs are operational, and structural equation modeling, it was found that perceived usefulness and ease of use positively influence willingness to use AVs, with perceived usefulness being the most significant factor. Perceived externalities have a positive impact, while perceived risk negatively affects willingness to use. Interestingly, ease of use increases perceived risk, but this is mitigated by the benefits perceived in usefulness. This research, differing significantly from previous studies, aims to guide government policy and industry strategies to enhance design, marketing, and popularization.","sentences":["Autonomous vehicles (AVs) have begun experimental commercialization initiatives in places such as Shanghai, China, and it is a valuable research question whether people's willingness to use AVs has changed from the prior.","This study explores Shanghai residents' attitudes towards AVs by applying the Technology Acceptance Model (TAM), the Perceived Risk (BAR) model, and introducing perceived externalities as a new psychological variable.","Through a survey in Shanghai, where AVs are operational, and structural equation modeling, it was found that perceived usefulness and ease of use positively influence willingness to use AVs, with perceived usefulness being the most significant factor.","Perceived externalities have a positive impact, while perceived risk negatively affects willingness to use.","Interestingly, ease of use increases perceived risk, but this is mitigated by the benefits perceived in usefulness.","This research, differing significantly from previous studies, aims to guide government policy and industry strategies to enhance design, marketing, and popularization."],"url":"http://arxiv.org/abs/2405.05578v1","category":"econ.GN"}
{"created":"2024-05-09 06:45:11","title":"Poisoning-based Backdoor Attacks for Arbitrary Target Label with Positive Triggers","abstract":"Poisoning-based backdoor attacks expose vulnerabilities in the data preparation stage of deep neural network (DNN) training. The DNNs trained on the poisoned dataset will be embedded with a backdoor, making them behave well on clean data while outputting malicious predictions whenever a trigger is applied. To exploit the abundant information contained in the input data to output label mapping, our scheme utilizes the network trained from the clean dataset as a trigger generator to produce poisons that significantly raise the success rate of backdoor attacks versus conventional approaches. Specifically, we provide a new categorization of triggers inspired by the adversarial technique and develop a multi-label and multi-payload Poisoning-based backdoor attack with Positive Triggers (PPT), which effectively moves the input closer to the target label on benign classifiers. After the classifier is trained on the poisoned dataset, we can generate an input-label-aware trigger to make the infected classifier predict any given input to any target label with a high possibility. Under both dirty- and clean-label settings, we show empirically that the proposed attack achieves a high attack success rate without sacrificing accuracy across various datasets, including SVHN, CIFAR10, GTSRB, and Tiny ImageNet. Furthermore, the PPT attack can elude a variety of classical backdoor defenses, proving its effectiveness.","sentences":["Poisoning-based backdoor attacks expose vulnerabilities in the data preparation stage of deep neural network (DNN) training.","The DNNs trained on the poisoned dataset will be embedded with a backdoor, making them behave well on clean data while outputting malicious predictions whenever a trigger is applied.","To exploit the abundant information contained in the input data to output label mapping, our scheme utilizes the network trained from the clean dataset as a trigger generator to produce poisons that significantly raise the success rate of backdoor attacks versus conventional approaches.","Specifically, we provide a new categorization of triggers inspired by the adversarial technique and develop a multi-label and multi-payload Poisoning-based backdoor attack with Positive Triggers (PPT), which effectively moves the input closer to the target label on benign classifiers.","After the classifier is trained on the poisoned dataset, we can generate an input-label-aware trigger to make the infected classifier predict any given input to any target label with a high possibility.","Under both dirty- and clean-label settings, we show empirically that the proposed attack achieves a high attack success rate without sacrificing accuracy across various datasets, including SVHN, CIFAR10, GTSRB, and Tiny ImageNet.","Furthermore, the PPT attack can elude a variety of classical backdoor defenses, proving its effectiveness."],"url":"http://arxiv.org/abs/2405.05573v1","category":"cs.CV"}
{"created":"2024-05-09 06:40:39","title":"From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences","abstract":"Current computational approaches for analysing or generating code-mixed sentences do not explicitly model \"naturalness\" or \"acceptability\" of code-mixed sentences, but rely on training corpora to reflect distribution of acceptable code-mixed sentences. Modelling human judgement for the acceptability of code-mixed text can help in distinguishing natural code-mixed text and enable quality-controlled generation of code-mixed text. To this end, we construct Cline - a dataset containing human acceptability judgements for English-Hindi (en-hi) code-mixed text. Cline is the largest of its kind with 16,642 sentences, consisting of samples sourced from two sources: synthetically generated code-mixed text and samples collected from online social media. Our analysis establishes that popular code-mixing metrics such as CMI, Number of Switch Points, Burstines, which are used to filter/curate/compare code-mixed corpora have low correlation with human acceptability judgements, underlining the necessity of our dataset. Experiments using Cline demonstrate that simple Multilayer Perceptron (MLP) models trained solely on code-mixing metrics are outperformed by fine-tuned pre-trained Multilingual Large Language Models (MLLMs). Specifically, XLM-Roberta and Bernice outperform IndicBERT across different configurations in challenging data settings. Comparison with ChatGPT's zero and fewshot capabilities shows that MLLMs fine-tuned on larger data outperform ChatGPT, providing scope for improvement in code-mixed tasks. Zero-shot transfer from English-Hindi to English-Telugu acceptability judgments using our model checkpoints proves superior to random baselines, enabling application to other code-mixed language pairs and providing further avenues of research. We publicly release our human-annotated dataset, trained checkpoints, code-mix corpus, and code for data generation and model training.","sentences":["Current computational approaches for analysing or generating code-mixed sentences do not explicitly model \"naturalness\" or \"acceptability\" of code-mixed sentences, but rely on training corpora to reflect distribution of acceptable code-mixed sentences.","Modelling human judgement for the acceptability of code-mixed text can help in distinguishing natural code-mixed text and enable quality-controlled generation of code-mixed text.","To this end, we construct Cline - a dataset containing human acceptability judgements for English-Hindi (en-hi) code-mixed text.","Cline is the largest of its kind with 16,642 sentences, consisting of samples sourced from two sources: synthetically generated code-mixed text and samples collected from online social media.","Our analysis establishes that popular code-mixing metrics such as CMI, Number of Switch Points, Burstines, which are used to filter/curate/compare code-mixed corpora have low correlation with human acceptability judgements, underlining the necessity of our dataset.","Experiments using Cline demonstrate that simple Multilayer Perceptron (MLP) models trained solely on code-mixing metrics are outperformed by fine-tuned pre-trained Multilingual Large Language Models (MLLMs).","Specifically, XLM-Roberta and Bernice outperform IndicBERT across different configurations in challenging data settings.","Comparison with ChatGPT's zero and fewshot capabilities shows that MLLMs fine-tuned on larger data outperform ChatGPT, providing scope for improvement in code-mixed tasks.","Zero-shot transfer from English-Hindi to English-Telugu acceptability judgments using our model checkpoints proves superior to random baselines, enabling application to other code-mixed language pairs and providing further avenues of research.","We publicly release our human-annotated dataset, trained checkpoints, code-mix corpus, and code for data generation and model training."],"url":"http://arxiv.org/abs/2405.05572v1","category":"cs.CL"}
{"created":"2024-05-09 06:31:25","title":"A Convergence result for a Stefan problem with phase relaxation","abstract":"In this paper we consider the model of phase relaxation introduced in [22], where an asymptotic analysis is performed toward an integral formulation of the Stefan problem when the relaxation parameter approaches zero. Assuming the natural physical assumption that the initial condition of the phase is constrained, but taking more general boundary conditions, we prove that the solution of this relaxed model converges in a stronger way to the solution of the classical weak Stefan problem.","sentences":["In this paper we consider the model of phase relaxation introduced in [22], where an asymptotic analysis is performed toward an integral formulation of the Stefan problem when the relaxation parameter approaches zero.","Assuming the natural physical assumption that the initial condition of the phase is constrained, but taking more general boundary conditions, we prove that the solution of this relaxed model converges in a stronger way to the solution of the classical weak Stefan problem."],"url":"http://arxiv.org/abs/2405.05570v1","category":"math.AP"}
{"created":"2024-05-09 06:22:46","title":"Geodesic motion of particles in the vicinity of the $\u03ba$-deformed Schwarzchild Black Hole","abstract":"In this study, we investigate the geodesic motion of a test particle around the Schwarzchild black hole in a $\\kappa$-deformed space-time. We compute a modified Lagrangian to obtain the $\\kappa$-deformed effective potential and find the particle trajectories based on the constants of motion. For the same value of angular momentum, we obtain a significant deformation in the orbits of the particles due to the non-commutativity of the $\\kappa$-deformed space-time. The deformation parameter becomes more significant for higher values of the angular momentum. The radius of the individual trajectories become smaller and their velocities decrease compared to the commutative case. The radius of the innermost stable circular orbit ($r_{ISCO}$) is also found using the modified effective potential. Though the equations get modified due to the non-commutativity of the $\\kappa$-deformed space-time, the $r_{ISCO}$ remains the same. We then study a large number of freely streaming particles moving in this $\\kappa$-deformed space-time and analyze the movement of these particles around the black hole due to the non-commutativity of the space-time. We concentrate on particles with different angular momentum moving around the black hole. We find that the motion of the particles are modified due to the non-commutativity of the space-time. The particles move slower along their respective trajectories in the deformed space-time. So, they remain closer to the black hole for a longer period of time, indicating that the accretion of freely streaming particles around the black hole would be modified by the non-commutativity of the space-time.","sentences":["In this study, we investigate the geodesic motion of a test particle around the Schwarzchild black hole in a $\\kappa$-deformed space-time.","We compute a modified Lagrangian to obtain the $\\kappa$-deformed effective potential and find the particle trajectories based on the constants of motion.","For the same value of angular momentum, we obtain a significant deformation in the orbits of the particles due to the non-commutativity of the $\\kappa$-deformed space-time.","The deformation parameter becomes more significant for higher values of the angular momentum.","The radius of the individual trajectories become smaller and their velocities decrease compared to the commutative case.","The radius of the innermost stable circular orbit ($r_{ISCO}$) is also found using the modified effective potential.","Though the equations get modified due to the non-commutativity of the $\\kappa$-deformed space-time, the $r_{ISCO}$ remains the same.","We then study a large number of freely streaming particles moving in this $\\kappa$-deformed space-time and analyze the movement of these particles around the black hole due to the non-commutativity of the space-time.","We concentrate on particles with different angular momentum moving around the black hole.","We find that the motion of the particles are modified due to the non-commutativity of the space-time.","The particles move slower along their respective trajectories in the deformed space-time.","So, they remain closer to the black hole for a longer period of time, indicating that the accretion of freely streaming particles around the black hole would be modified by the non-commutativity of the space-time."],"url":"http://arxiv.org/abs/2405.05568v1","category":"gr-qc"}
{"created":"2024-05-09 06:00:15","title":"Array SAR 3D Sparse Imaging Based on Regularization by Denoising Under Few Observed Data","abstract":"Array synthetic aperture radar (SAR) three-dimensional (3D) imaging can obtain 3D information of the target region, which is widely used in environmental monitoring and scattering information measurement. In recent years, with the development of compressed sensing (CS) theory, sparse signal processing is used in array SAR 3D imaging. Compared with matched filter (MF), sparse SAR imaging can effectively improve image quality. However, sparse imaging based on handcrafted regularization functions suffers from target information loss in few observed SAR data. Therefore, in this article, a general 3D sparse imaging framework based on Regulation by Denoising (RED) and proximal gradient descent type method for array SAR is presented. Firstly, we construct explicit prior terms via state-of-the-art denoising operators instead of regularization functions, which can improve the accuracy of sparse reconstruction and preserve the structure information of the target. Then, different proximal gradient descent type methods are presented, including a generalized alternating projection (GAP) and an alternating direction method of multiplier (ADMM), which is suitable for high-dimensional data processing. Additionally, the proposed method has robust convergence, which can achieve sparse reconstruction of 3D SAR in few observed SAR data. Extensive simulations and real data experiments are conducted to analyze the performance of the proposed method. The experimental results show that the proposed method has superior sparse reconstruction performance.","sentences":["Array synthetic aperture radar (SAR) three-dimensional (3D) imaging can obtain 3D information of the target region, which is widely used in environmental monitoring and scattering information measurement.","In recent years, with the development of compressed sensing (CS) theory, sparse signal processing is used in array SAR 3D imaging.","Compared with matched filter (MF), sparse SAR imaging can effectively improve image quality.","However, sparse imaging based on handcrafted regularization functions suffers from target information loss in few observed SAR data.","Therefore, in this article, a general 3D sparse imaging framework based on Regulation by Denoising (RED) and proximal gradient descent type method for array SAR is presented.","Firstly, we construct explicit prior terms via state-of-the-art denoising operators instead of regularization functions, which can improve the accuracy of sparse reconstruction and preserve the structure information of the target.","Then, different proximal gradient descent type methods are presented, including a generalized alternating projection (GAP) and an alternating direction method of multiplier (ADMM), which is suitable for high-dimensional data processing.","Additionally, the proposed method has robust convergence, which can achieve sparse reconstruction of 3D SAR in few observed SAR data.","Extensive simulations and real data experiments are conducted to analyze the performance of the proposed method.","The experimental results show that the proposed method has superior sparse reconstruction performance."],"url":"http://arxiv.org/abs/2405.05565v1","category":"eess.IV"}
{"created":"2024-05-09 05:34:25","title":"From Road Congestion to Vehicle-Control Enabled Artificial Traffic Fluids","abstract":"This article provides an overview of the design of nonlinear feedback Cruise Controllers (CCs) for automated vehicles on lane-free roads. The feedback design problem is particularly challenging because of the various state constraints (e.g., collision-free movement, road geometry, speed limits) as well as the nature of the control objective (globally stabilizing distributed controllers that require measurements from neighboring vehicles only). Therefore, the resulting nonlinear control system is defined on an open set (not necessarily diffeomorphic to a linear space) for which the set of desired equilibria is non-compact. The proposed design of the CCs is based on energy-like control Lyapunov functions which combine potential functions with kinetic energy terms and other appropriate penalty terms. The feedback design in the microscopic level is accompanied by the derivation of the corresponding macroscopic traffic flow models. Explicit relations are established between selectable CC features and the obtained macroscopic traffic flow characteristics. This facilitates the active design of efficient traffic flow with desired properties, i.e., the construction of artificial traffic fluids.","sentences":["This article provides an overview of the design of nonlinear feedback Cruise Controllers (CCs) for automated vehicles on lane-free roads.","The feedback design problem is particularly challenging because of the various state constraints (e.g., collision-free movement, road geometry, speed limits) as well as the nature of the control objective (globally stabilizing distributed controllers that require measurements from neighboring vehicles only).","Therefore, the resulting nonlinear control system is defined on an open set (not necessarily diffeomorphic to a linear space) for which the set of desired equilibria is non-compact.","The proposed design of the CCs is based on energy-like control Lyapunov functions which combine potential functions with kinetic energy terms and other appropriate penalty terms.","The feedback design in the microscopic level is accompanied by the derivation of the corresponding macroscopic traffic flow models.","Explicit relations are established between selectable CC features and the obtained macroscopic traffic flow characteristics.","This facilitates the active design of efficient traffic flow with desired properties, i.e., the construction of artificial traffic fluids."],"url":"http://arxiv.org/abs/2405.05558v1","category":"math.OC"}
{"created":"2024-05-09 05:31:00","title":"Composition Rules for Strong Structural Controllability and Minimum Input Problem in Diffusively-Coupled Networks","abstract":"This paper presents new results and reinterpretation of existing conditions for strong structural controllability in a structured network determined by the zero/non-zero patterns of edges. For diffusively-coupled networks with self-loops, we first establish a necessary and sufficient condition for strong structural controllability, based on the concepts of dedicated and sharing nodes. Subsequently, we define several conditions for strong structural controllability across various graph types by decomposing them into disjoint path graphs. We further extend our findings by introducing a composition rule, facilitating the analysis of strong structural controllability in larger networks. This rule allows us to determine the strong structural controllability of connected graphs called pactus graphs (a generalization of the well-known cactus graph) by consideration of the strong structural controllability of its disjoint component graphs. In this process, we introduce the notion of a component input node, which is a state node that functions identically to an external input node. Based on this concept, we present an algorithm with approximate polynomial complexity to determine the minimum number of external input nodes required to maintain strong structural controllability in a diffusively-coupled network with self-loops.","sentences":["This paper presents new results and reinterpretation of existing conditions for strong structural controllability in a structured network determined by the zero/non-zero patterns of edges.","For diffusively-coupled networks with self-loops, we first establish a necessary and sufficient condition for strong structural controllability, based on the concepts of dedicated and sharing nodes.","Subsequently, we define several conditions for strong structural controllability across various graph types by decomposing them into disjoint path graphs.","We further extend our findings by introducing a composition rule, facilitating the analysis of strong structural controllability in larger networks.","This rule allows us to determine the strong structural controllability of connected graphs called pactus graphs (a generalization of the well-known cactus graph) by consideration of the strong structural controllability of its disjoint component graphs.","In this process, we introduce the notion of a component input node, which is a state node that functions identically to an external input node.","Based on this concept, we present an algorithm with approximate polynomial complexity to determine the minimum number of external input nodes required to maintain strong structural controllability in a diffusively-coupled network with self-loops."],"url":"http://arxiv.org/abs/2405.05557v1","category":"math.GN"}
{"created":"2024-05-09 05:27:53","title":"Extension of graph-accelerated non-intrusive polynomial chaos to high-dimensional uncertainty quantification through the active subspace method","abstract":"The recently introduced graph-accelerated non-intrusive polynomial chaos (NIPC) method has shown effectiveness in solving a broad range of uncertainty quantification (UQ) problems with multidisciplinary systems. It uses integration-based NIPC to solve the UQ problem and generates the quadrature rule in a desired tensor structure, so that the model evaluations can be efficiently accelerated through the computational graph transformation method, Accelerated Model evaluations on Tensor grids using Computational graph transformations (AMTC). This method is efficient when the model's computational graph possesses a certain type of sparsity which is commonly the case in multidisciplinary problems. However, it faces limitations in high-dimensional cases due to the curse of dimensionality. To broaden its applicability in high-dimensional UQ problems, we propose AS-AMTC, which integrates the AMTC approach with the active subspace (AS) method, a widely-used dimension reduction technique. In developing this new method, we have also developed AS-NIPC, linking integration-based NIPC with the AS method for solving high-dimensional UQ problems. AS-AMTC incorporates rigorous approaches to generate orthogonal polynomial basis functions for lower-dimensional active variables and efficient quadrature rules to estimate their coefficients. The AS-AMTC method extends AS-NIPC by generating a quadrature rule with a desired tensor structure. This allows the AMTC method to exploit the computational graph sparsity, leading to efficient model evaluations. In an 81-dimensional UQ problem derived from an air-taxi trajectory optimization scenario, AS-NIPC demonstrates a 30% decrease in relative error compared to the existing methods, while AS-AMTC achieves an 80% reduction.","sentences":["The recently introduced graph-accelerated non-intrusive polynomial chaos (NIPC) method has shown effectiveness in solving a broad range of uncertainty quantification (UQ) problems with multidisciplinary systems.","It uses integration-based NIPC to solve the UQ problem and generates the quadrature rule in a desired tensor structure, so that the model evaluations can be efficiently accelerated through the computational graph transformation method, Accelerated Model evaluations on Tensor grids using Computational graph transformations (AMTC).","This method is efficient when the model's computational graph possesses a certain type of sparsity which is commonly the case in multidisciplinary problems.","However, it faces limitations in high-dimensional cases due to the curse of dimensionality.","To broaden its applicability in high-dimensional UQ problems, we propose AS-AMTC, which integrates the AMTC approach with the active subspace (AS) method, a widely-used dimension reduction technique.","In developing this new method, we have also developed AS-NIPC, linking integration-based NIPC with the AS method for solving high-dimensional UQ problems.","AS-AMTC incorporates rigorous approaches to generate orthogonal polynomial basis functions for lower-dimensional active variables and efficient quadrature rules to estimate their coefficients.","The AS-AMTC method extends AS-NIPC by generating a quadrature rule with a desired tensor structure.","This allows the AMTC method to exploit the computational graph sparsity, leading to efficient model evaluations.","In an 81-dimensional UQ problem derived from an air-taxi trajectory optimization scenario, AS-NIPC demonstrates a 30% decrease in relative error compared to the existing methods, while AS-AMTC achieves an 80% reduction."],"url":"http://arxiv.org/abs/2405.05556v1","category":"cs.CE"}
{"created":"2024-05-09 05:23:34","title":"Towards Robust Physical-world Backdoor Attacks on Lane Detection","abstract":"Deep learning-based lane detection (LD) plays a critical role in autonomous driving systems, such as adaptive cruise control. However, it is vulnerable to backdoor attacks. Existing backdoor attack methods on LD exhibit limited effectiveness in dynamic real-world scenarios, primarily because they fail to consider dynamic scene factors, including changes in driving perspectives (e.g., viewpoint transformations) and environmental conditions (e.g., weather or lighting changes). To tackle this issue, this paper introduces BadLANE, a dynamic scene adaptation backdoor attack for LD designed to withstand changes in real-world dynamic scene factors. To address the challenges posed by changing driving perspectives, we propose an amorphous trigger pattern composed of shapeless pixels. This trigger design allows the backdoor to be activated by various forms or shapes of mud spots or pollution on the road or lens, enabling adaptation to changes in vehicle observation viewpoints during driving. To mitigate the effects of environmental changes, we design a meta-learning framework to train meta-generators tailored to different environmental conditions. These generators produce meta-triggers that incorporate diverse environmental information, such as weather or lighting conditions, as the initialization of the trigger patterns for backdoor implantation, thus enabling adaptation to dynamic environments. Extensive experiments on various commonly used LD models in both digital and physical domains validate the effectiveness of our attacks, outperforming other baselines significantly (+25.15\\% on average in Attack Success Rate). Our codes will be available upon paper publication.","sentences":["Deep learning-based lane detection (LD) plays a critical role in autonomous driving systems, such as adaptive cruise control.","However, it is vulnerable to backdoor attacks.","Existing backdoor attack methods on LD exhibit limited effectiveness in dynamic real-world scenarios, primarily because they fail to consider dynamic scene factors, including changes in driving perspectives (e.g., viewpoint transformations) and environmental conditions (e.g., weather or lighting changes).","To tackle this issue, this paper introduces BadLANE, a dynamic scene adaptation backdoor attack for LD designed to withstand changes in real-world dynamic scene factors.","To address the challenges posed by changing driving perspectives, we propose an amorphous trigger pattern composed of shapeless pixels.","This trigger design allows the backdoor to be activated by various forms or shapes of mud spots or pollution on the road or lens, enabling adaptation to changes in vehicle observation viewpoints during driving.","To mitigate the effects of environmental changes, we design a meta-learning framework to train meta-generators tailored to different environmental conditions.","These generators produce meta-triggers that incorporate diverse environmental information, such as weather or lighting conditions, as the initialization of the trigger patterns for backdoor implantation, thus enabling adaptation to dynamic environments.","Extensive experiments on various commonly used LD models in both digital and physical domains validate the effectiveness of our attacks, outperforming other baselines significantly (+25.15\\% on average in Attack Success Rate).","Our codes will be available upon paper publication."],"url":"http://arxiv.org/abs/2405.05553v1","category":"cs.CV"}
{"created":"2024-05-09 05:20:08","title":"Intelligent Reflecting Surface Aided AirComp: Multi-Timescale Design and Performance Analysis","abstract":"The integration of intelligent reflecting surface (IRS) into over-the-air computation (AirComp) is an effective solution for reducing the computational mean squared error (MSE) via its high passive beamforming gain. Prior works on IRS aided AirComp generally rely on the full instantaneous channel state information (I-CSI), which is not applicable to large-scale systems due to its heavy signalling overhead. To address this issue, we propose a novel multi-timescale transmission protocol. In particular, the receive beamforming at the access point (AP) is pre-determined based on the static angle information and the IRS phase-shifts are optimized relying on the long-term statistical CSI. With the obtained AP receive beamforming and IRS phase-shifts, the effective low-dimensional I-CSI is exploited to determine devices' transmit power in each coherence block, thus substantially reducing the signalling overhead. Theoretical analysis unveils that the achievable MSE scales on the order of ${\\cal O}\\left( {K/\\left( {{N^2}M} \\right)} \\right)$, where $M$, $N$, and $K$ are the number of AP antennas, IRS elements, and devices, respectively. We also prove that the channel-inversion power control is asymptotically optimal for large $N$, which reveals that the full power transmission policy is not needed for lowering the power consumption of energy-limited devices.","sentences":["The integration of intelligent reflecting surface (IRS) into over-the-air computation (AirComp) is an effective solution for reducing the computational mean squared error (MSE) via its high passive beamforming gain.","Prior works on IRS aided AirComp generally rely on the full instantaneous channel state information (I-CSI), which is not applicable to large-scale systems due to its heavy signalling overhead.","To address this issue, we propose a novel multi-timescale transmission protocol.","In particular, the receive beamforming at the access point (AP) is pre-determined based on the static angle information and the IRS phase-shifts are optimized relying on the long-term statistical CSI.","With the obtained AP receive beamforming and IRS phase-shifts, the effective low-dimensional I-CSI is exploited to determine devices' transmit power in each coherence block, thus substantially reducing the signalling overhead.","Theoretical analysis unveils that the achievable MSE scales on the order of ${\\cal O}\\left( {K/\\left( {{N^2}M} \\right)} \\right)$, where $M$, $N$, and $K$ are the number of AP antennas, IRS elements, and devices, respectively.","We also prove that the channel-inversion power control is asymptotically optimal for large $N$, which reveals that the full power transmission policy is not needed for lowering the power consumption of energy-limited devices."],"url":"http://arxiv.org/abs/2405.05549v1","category":"cs.IT"}
{"created":"2024-05-09 05:10:44","title":"Investigating Interaction Modes and User Agency in Human-LLM Collaboration for Domain-Specific Data Analysis","abstract":"Despite demonstrating robust capabilities in performing tasks related to general-domain data-operation tasks, Large Language Models (LLMs) may exhibit shortcomings when applied to domain-specific tasks. We consider the design of domain-specific AI-powered data analysis tools from two dimensions: interaction and user agency. We implemented two design probes that fall on the two ends of the two dimensions: an open-ended high agency (OHA) prototype and a structured low agency (SLA) prototype. We conducted an interview study with nine data scientists to investigate (1) how users perceived the LLM outputs for data analysis assistance, and (2) how the two test design probes, OHA and SLA, affected user behavior, performance, and perceptions. Our study revealed insights regarding participants' interactions with LLMs, how they perceived the results, and their desire for explainability concerning LLM outputs, along with a noted need for collaboration with other users, and how they envisioned the utility of LLMs in their workflow.","sentences":["Despite demonstrating robust capabilities in performing tasks related to general-domain data-operation tasks, Large Language Models (LLMs) may exhibit shortcomings when applied to domain-specific tasks.","We consider the design of domain-specific AI-powered data analysis tools from two dimensions: interaction and user agency.","We implemented two design probes that fall on the two ends of the two dimensions: an open-ended high agency (OHA) prototype and a structured low agency (SLA) prototype.","We conducted an interview study with nine data scientists to investigate (1) how users perceived the LLM outputs for data analysis assistance, and (2) how the two test design probes, OHA and SLA, affected user behavior, performance, and perceptions.","Our study revealed insights regarding participants' interactions with LLMs, how they perceived the results, and their desire for explainability concerning LLM outputs, along with a noted need for collaboration with other users, and how they envisioned the utility of LLMs in their workflow."],"url":"http://arxiv.org/abs/2405.05548v1","category":"cs.HC"}
{"created":"2024-05-09 05:06:50","title":"Partially Ordered Sets Corresponding to the Partition Problem","abstract":"The partition problem is a well-known basic NP-complete problem. We mainly consider the optimization version of it in this paper. The problem has been investigated from various perspectives for a long time and can be solved efficiently in practice. Hence, we might say that the only remaining task is to decide whether the problem can be solved in polynomial time in the number $n$ of given integers.   We propose two partially ordered sets (posets) and present a novel methodology for solving the partition problem. The first poset is order-isomorphic to a well-known poset whose structures are related to the solutions of the subset sum problem, while the second is a subposet of the first and plays a crucial role in this paper. We first show several properties of the two posets, such as size, height and width (the largest size of a subset consisting of incomparable elements). Both widths are the same and $O(2^n / n^{3/2})$ for $n$ congruent to $0$ or $3$ (mod $4$). This fact indicates the hardness of the partition problem. We then prove that in general all the candidate solutions correspond to the elements of the second poset, whose size is $2^{n} - 2 \\binom{n}{\\lfloor n/2 \\rfloor}$. Since a partition corresponds to two elements of the poset, the number of the candidate partitions is half of it, that is, $2^{n-1} - \\binom{n}{\\lfloor n/2 \\rfloor}$. We finally prove that the candidate solutions can be reduced based on the partial order. In particular, we give several polynomially solvable cases by considering the minimal elements of the second poset.   Our approach offers a valuable tool for structural analysis of the partition problem and provides a new perspective on the P versus NP problem.","sentences":["The partition problem is a well-known basic NP-complete problem.","We mainly consider the optimization version of it in this paper.","The problem has been investigated from various perspectives for a long time and can be solved efficiently in practice.","Hence, we might say that the only remaining task is to decide whether the problem can be solved in polynomial time in the number $n$ of given integers.   ","We propose two partially ordered sets (posets) and present a novel methodology for solving the partition problem.","The first poset is order-isomorphic to a well-known poset whose structures are related to the solutions of the subset sum problem, while the second is a subposet of the first and plays a crucial role in this paper.","We first show several properties of the two posets, such as size, height and width (the largest size of a subset consisting of incomparable elements).","Both widths are the same and $O(2^n / n^{3/2})$ for $n$ congruent to $0$ or $3$ (mod $4$).","This fact indicates the hardness of the partition problem.","We then prove that in general all the candidate solutions correspond to the elements of the second poset, whose size is $2^{n} - 2 \\binom{n}{\\lfloor n/2 \\rfloor}$. Since a partition corresponds to two elements of the poset, the number of the candidate partitions is half of it, that is, $2^{n-1} - \\binom{n}{\\lfloor","n/2 \\rfloor}$. We finally prove that the candidate solutions can be reduced based on the partial order.","In particular, we give several polynomially solvable cases by considering the minimal elements of the second poset.   ","Our approach offers a valuable tool for structural analysis of the partition problem and provides a new perspective on the P versus NP problem."],"url":"http://arxiv.org/abs/2405.05544v1","category":"cs.DM"}
{"created":"2024-05-09 04:57:26","title":"Dynamic Deep Factor Graph for Multi-Agent Reinforcement Learning","abstract":"This work introduces a novel value decomposition algorithm, termed \\textit{Dynamic Deep Factor Graphs} (DDFG). Unlike traditional coordination graphs, DDFG leverages factor graphs to articulate the decomposition of value functions, offering enhanced flexibility and adaptability to complex value function structures. Central to DDFG is a graph structure generation policy that innovatively generates factor graph structures on-the-fly, effectively addressing the dynamic collaboration requirements among agents. DDFG strikes an optimal balance between the computational overhead associated with aggregating value functions and the performance degradation inherent in their complete decomposition. Through the application of the max-sum algorithm, DDFG efficiently identifies optimal policies. We empirically validate DDFG's efficacy in complex scenarios, including higher-order predator-prey tasks and the StarCraft II Multi-agent Challenge (SMAC), thus underscoring its capability to surmount the limitations faced by existing value decomposition algorithms. DDFG emerges as a robust solution for MARL challenges that demand nuanced understanding and facilitation of dynamic agent collaboration. The implementation of DDFG is made publicly accessible, with the source code available at \\url{https://github.com/SICC-Group/DDFG}.","sentences":["This work introduces a novel value decomposition algorithm, termed \\textit{Dynamic Deep Factor Graphs} (DDFG).","Unlike traditional coordination graphs, DDFG leverages factor graphs to articulate the decomposition of value functions, offering enhanced flexibility and adaptability to complex value function structures.","Central to DDFG is a graph structure generation policy that innovatively generates factor graph structures on-the-fly, effectively addressing the dynamic collaboration requirements among agents.","DDFG strikes an optimal balance between the computational overhead associated with aggregating value functions and the performance degradation inherent in their complete decomposition.","Through the application of the max-sum algorithm, DDFG efficiently identifies optimal policies.","We empirically validate DDFG's efficacy in complex scenarios, including higher-order predator-prey tasks and the StarCraft II Multi-agent Challenge (SMAC), thus underscoring its capability to surmount the limitations faced by existing value decomposition algorithms.","DDFG emerges as a robust solution for MARL challenges that demand nuanced understanding and facilitation of dynamic agent collaboration.","The implementation of DDFG is made publicly accessible, with the source code available at \\url{https://github.com/SICC-Group/DDFG}."],"url":"http://arxiv.org/abs/2405.05542v1","category":"cs.RO"}
{"created":"2024-05-09 04:57:10","title":"CrashJS: A NodeJS Benchmark for Automated Crash Reproduction","abstract":"Software bugs often lead to software crashes, which cost US companies upwards of $2.08 trillion annually. Automated Crash Reproduction (ACR) aims to generate unit tests that successfully reproduce a crash. The goal of ACR is to aid developers with debugging, providing them with another tool to locate where a bug is in a program. The main approach ACR currently takes is to replicate a stack trace from an error thrown within a program. Currently, ACR has been developed for C, Java, and Python, but there are no tools targeting JavaScript programs. To aid the development of JavaScript ACR tools, we propose CrashJS: a benchmark dataset of 453 Node.js crashes from several sources. CrashJS includes a mix of real-world and synthesised tests, multiple projects, and different levels of complexity for both crashes and target programs.","sentences":["Software bugs often lead to software crashes, which cost US companies upwards of $2.08 trillion annually.","Automated Crash Reproduction (ACR) aims to generate unit tests that successfully reproduce a crash.","The goal of ACR is to aid developers with debugging, providing them with another tool to locate where a bug is in a program.","The main approach ACR currently takes is to replicate a stack trace from an error thrown within a program.","Currently, ACR has been developed for C, Java, and Python, but there are no tools targeting JavaScript programs.","To aid the development of JavaScript ACR tools, we propose CrashJS: a benchmark dataset of 453 Node.js crashes from several sources.","CrashJS includes a mix of real-world and synthesised tests, multiple projects, and different levels of complexity for both crashes and target programs."],"url":"http://arxiv.org/abs/2405.05541v1","category":"cs.SE"}
{"created":"2024-05-09 04:36:04","title":"A Survey on Personalized Content Synthesis with Diffusion Models","abstract":"Recent advancements in generative models have significantly impacted content creation, leading to the emergence of Personalized Content Synthesis (PCS). With a small set of user-provided examples, PCS aims to customize the subject of interest to specific user-defined prompts. Over the past two years, more than 150 methods have been proposed. However, existing surveys mainly focus on text-to-image generation, with few providing up-to-date summaries on PCS. This paper offers a comprehensive survey of PCS, with a particular focus on the diffusion models. Specifically, we introduce the generic frameworks of PCS research, which can be broadly classified into optimization-based and learning-based approaches. We further categorize and analyze these methodologies, discussing their strengths, limitations, and key techniques. Additionally, we delve into specialized tasks within the field, such as personalized object generation, face synthesis, and style personalization, highlighting their unique challenges and innovations. Despite encouraging progress, we also present an analysis of the challenges such as overfitting and the trade-off between subject fidelity and text alignment. Through this detailed overview and analysis, we propose future directions to advance the development of PCS.","sentences":["Recent advancements in generative models have significantly impacted content creation, leading to the emergence of Personalized Content Synthesis (PCS).","With a small set of user-provided examples, PCS aims to customize the subject of interest to specific user-defined prompts.","Over the past two years, more than 150 methods have been proposed.","However, existing surveys mainly focus on text-to-image generation, with few providing up-to-date summaries on PCS.","This paper offers a comprehensive survey of PCS, with a particular focus on the diffusion models.","Specifically, we introduce the generic frameworks of PCS research, which can be broadly classified into optimization-based and learning-based approaches.","We further categorize and analyze these methodologies, discussing their strengths, limitations, and key techniques.","Additionally, we delve into specialized tasks within the field, such as personalized object generation, face synthesis, and style personalization, highlighting their unique challenges and innovations.","Despite encouraging progress, we also present an analysis of the challenges such as overfitting and the trade-off between subject fidelity and text alignment.","Through this detailed overview and analysis, we propose future directions to advance the development of PCS."],"url":"http://arxiv.org/abs/2405.05538v1","category":"cs.CV"}
{"created":"2024-05-09 04:28:29","title":"Magnetization dynamics driven by displacement currents across a magnetic tunnel junction","abstract":"Understanding the high-frequency transport characteristics of magnetic tunnel junctions (MTJs) is crucial for the development of fast-operating spintronics memories and radio frequency devices. Here, we present the study of frequency-dependent capacitive current effect in CoFeB/MgO-based MTJs and its influence on magnetization dynamics using time-resolved magneto-optical Kerr effect technique. In our device operating at gigahertz frequencies, we find a large displacement current of the order of mA's, which does not break the tunnel barrier of the MTJ. Importantly, this current generates an Oersted field and spin-orbit torque, inducing magnetization dynamics. Our discovery holds promise for building robust MTJ devices operating under high current conditions, also highlighting the significance of capacitive impedance in high frequency magnetotransport techniques.","sentences":["Understanding the high-frequency transport characteristics of magnetic tunnel junctions (MTJs) is crucial for the development of fast-operating spintronics memories and radio frequency devices.","Here, we present the study of frequency-dependent capacitive current effect in CoFeB/MgO-based MTJs and its influence on magnetization dynamics using time-resolved magneto-optical Kerr effect technique.","In our device operating at gigahertz frequencies, we find a large displacement current of the order of mA's, which does not break the tunnel barrier of the MTJ.","Importantly, this current generates an Oersted field and spin-orbit torque, inducing magnetization dynamics.","Our discovery holds promise for building robust MTJ devices operating under high current conditions, also highlighting the significance of capacitive impedance in high frequency magnetotransport techniques."],"url":"http://arxiv.org/abs/2405.05537v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-09 04:21:52","title":"Reconfiguration of Multisets with Applications to Bin Packing","abstract":"We use the reconfiguration framework to analyze problems that involve the rearrangement of items among groups. In various applications, a group of items could correspond to the files or jobs assigned to a particular machine, and the goal of rearrangement could be improving efficiency or increasing locality.   To cover problems arising in a wide range of application areas, we define the general Repacking problem as the rearrangement of multisets of multisets. We present hardness results for the general case and algorithms for various classes of instances that arise in real-life scenarios. By limiting the total size of items in each multiset, our results can be viewed as an offline approach to Bin Packing, in which each bin is represented as a multiset.   In addition to providing the first results on reconfiguration of multisets, our contributions open up several research avenues: the interplay between reconfiguration and online algorithms and parallel algorithms; the use of the tools of linear programming in reconfiguration; and, in the longer term, a focus on resources in reconfiguration.","sentences":["We use the reconfiguration framework to analyze problems that involve the rearrangement of items among groups.","In various applications, a group of items could correspond to the files or jobs assigned to a particular machine, and the goal of rearrangement could be improving efficiency or increasing locality.   ","To cover problems arising in a wide range of application areas, we define the general Repacking problem as the rearrangement of multisets of multisets.","We present hardness results for the general case and algorithms for various classes of instances that arise in real-life scenarios.","By limiting the total size of items in each multiset, our results can be viewed as an offline approach to Bin Packing, in which each bin is represented as a multiset.   ","In addition to providing the first results on reconfiguration of multisets, our contributions open up several research avenues: the interplay between reconfiguration and online algorithms and parallel algorithms; the use of the tools of linear programming in reconfiguration; and, in the longer term, a focus on resources in reconfiguration."],"url":"http://arxiv.org/abs/2405.05535v1","category":"cs.DS"}
{"created":"2024-05-09 04:05:37","title":"Generative Model for Joint Resource Management in Multi-Cell Multi-Carrier NOMA Networks","abstract":"In this work, we design a generative artificial intelligence (GAI) -based framework for joint resource allocation, beamforming, and power allocation in multi-cell multi-carrier non-orthogonal multiple access (NOMA) networks. We formulate the proposed problem as sum rate maximization problem. Next, we design a novel multi-task transformer (MTT) framework to handle the problem in real-time. To provide the necessary training set, we consider simplified but powerful mathematical techniques from the literature. Then, we train and test the proposed MTT. We perform simulation to evaluate the efficiency of the proposed MTT and compare its performance with the mathematical baseline.","sentences":["In this work, we design a generative artificial intelligence (GAI) -based framework for joint resource allocation, beamforming, and power allocation in multi-cell multi-carrier non-orthogonal multiple access (NOMA) networks.","We formulate the proposed problem as sum rate maximization problem.","Next, we design a novel multi-task transformer (MTT) framework to handle the problem in real-time.","To provide the necessary training set, we consider simplified but powerful mathematical techniques from the literature.","Then, we train and test the proposed MTT.","We perform simulation to evaluate the efficiency of the proposed MTT and compare its performance with the mathematical baseline."],"url":"http://arxiv.org/abs/2405.05531v1","category":"cs.NI"}
{"created":"2024-05-09 03:34:09","title":"Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview","abstract":"Neural Radiance Fields (NeRF) have emerged as a powerful paradigm for 3D scene representation, offering high-fidelity renderings and reconstructions from a set of sparse and unstructured sensor data. In the context of autonomous robotics, where perception and understanding of the environment are pivotal, NeRF holds immense promise for improving performance. In this paper, we present a comprehensive survey and analysis of the state-of-the-art techniques for utilizing NeRF to enhance the capabilities of autonomous robots. We especially focus on the perception, localization and navigation, and decision-making modules of autonomous robots and delve into tasks crucial for autonomous operation, including 3D reconstruction, segmentation, pose estimation, simultaneous localization and mapping (SLAM), navigation and planning, and interaction. Our survey meticulously benchmarks existing NeRF-based methods, providing insights into their strengths and limitations. Moreover, we explore promising avenues for future research and development in this domain. Notably, we discuss the integration of advanced techniques such as 3D Gaussian splatting (3DGS), large language models (LLM), and generative AIs, envisioning enhanced reconstruction efficiency, scene understanding, decision-making capabilities. This survey serves as a roadmap for researchers seeking to leverage NeRFs to empower autonomous robots, paving the way for innovative solutions that can navigate and interact seamlessly in complex environments.","sentences":["Neural Radiance Fields (NeRF) have emerged as a powerful paradigm for 3D scene representation, offering high-fidelity renderings and reconstructions from a set of sparse and unstructured sensor data.","In the context of autonomous robotics, where perception and understanding of the environment are pivotal, NeRF holds immense promise for improving performance.","In this paper, we present a comprehensive survey and analysis of the state-of-the-art techniques for utilizing NeRF to enhance the capabilities of autonomous robots.","We especially focus on the perception, localization and navigation, and decision-making modules of autonomous robots and delve into tasks crucial for autonomous operation, including 3D reconstruction, segmentation, pose estimation, simultaneous localization and mapping (SLAM), navigation and planning, and interaction.","Our survey meticulously benchmarks existing NeRF-based methods, providing insights into their strengths and limitations.","Moreover, we explore promising avenues for future research and development in this domain.","Notably, we discuss the integration of advanced techniques such as 3D Gaussian splatting (3DGS), large language models (LLM), and generative AIs, envisioning enhanced reconstruction efficiency, scene understanding, decision-making capabilities.","This survey serves as a roadmap for researchers seeking to leverage NeRFs to empower autonomous robots, paving the way for innovative solutions that can navigate and interact seamlessly in complex environments."],"url":"http://arxiv.org/abs/2405.05526v1","category":"cs.RO"}
{"created":"2024-05-09 03:27:28","title":"Universal Adversarial Perturbations for Vision-Language Pre-trained Models","abstract":"Vision-language pre-trained (VLP) models have been the foundation of numerous vision-language tasks. Given their prevalence, it be- comes imperative to assess their adversarial robustness, especially when deploying them in security-crucial real-world applications. Traditionally, adversarial perturbations generated for this assessment target specific VLP models, datasets, and/or downstream tasks. This practice suffers from low transferability and additional computation costs when transitioning to new scenarios.   In this work, we thoroughly investigate whether VLP models are commonly sensitive to imperceptible perturbations of a specific pattern for the image modality. To this end, we propose a novel black-box method to generate Universal Adversarial Perturbations (UAPs), which is so called the Effective and T ransferable Universal Adversarial Attack (ETU), aiming to mislead a variety of existing VLP models in a range of downstream tasks. The ETU comprehensively takes into account the characteristics of UAPs and the intrinsic cross-modal interactions to generate effective UAPs. Under this regime, the ETU encourages both global and local utilities of UAPs. This benefits the overall utility while reducing interactions between UAP units, improving the transferability. To further enhance the effectiveness and transferability of UAPs, we also design a novel data augmentation method named ScMix. ScMix consists of self-mix and cross-mix data transformations, which can effectively increase the multi-modal data diversity while preserving the semantics of the original data. Through comprehensive experiments on various downstream tasks, VLP models, and datasets, we demonstrate that the proposed method is able to achieve effective and transferrable universal adversarial attacks.","sentences":["Vision-language pre-trained (VLP) models have been the foundation of numerous vision-language tasks.","Given their prevalence, it be- comes imperative to assess their adversarial robustness, especially when deploying them in security-crucial real-world applications.","Traditionally, adversarial perturbations generated for this assessment target specific VLP models, datasets, and/or downstream tasks.","This practice suffers from low transferability and additional computation costs when transitioning to new scenarios.   ","In this work, we thoroughly investigate whether VLP models are commonly sensitive to imperceptible perturbations of a specific pattern for the image modality.","To this end, we propose a novel black-box method to generate Universal Adversarial Perturbations (UAPs), which is so called the Effective and T ransferable Universal Adversarial Attack (ETU), aiming to mislead a variety of existing VLP models in a range of downstream tasks.","The ETU comprehensively takes into account the characteristics of UAPs and the intrinsic cross-modal interactions to generate effective UAPs.","Under this regime, the ETU encourages both global and local utilities of UAPs.","This benefits the overall utility while reducing interactions between UAP units, improving the transferability.","To further enhance the effectiveness and transferability of UAPs, we also design a novel data augmentation method named ScMix.","ScMix consists of self-mix and cross-mix data transformations, which can effectively increase the multi-modal data diversity while preserving the semantics of the original data.","Through comprehensive experiments on various downstream tasks, VLP models, and datasets, we demonstrate that the proposed method is able to achieve effective and transferrable universal adversarial attacks."],"url":"http://arxiv.org/abs/2405.05524v1","category":"cs.CV"}
{"created":"2024-05-09 03:23:47","title":"Prompt When the Animal is: Temporal Animal Behavior Grounding with Positional Recovery Training","abstract":"Temporal grounding is crucial in multimodal learning, but it poses challenges when applied to animal behavior data due to the sparsity and uniform distribution of moments. To address these challenges, we propose a novel Positional Recovery Training framework (Port), which prompts the model with the start and end times of specific animal behaviors during training. Specifically, Port enhances the baseline model with a Recovering part to predict flipped label sequences and align distributions with a Dual-alignment method. This allows the model to focus on specific temporal regions prompted by ground-truth information. Extensive experiments on the Animal Kingdom dataset demonstrate the effectiveness of Port, achieving an IoU@0.3 of 38.52. It emerges as one of the top performers in the sub-track of MMVRAC in ICME 2024 Grand Challenges.","sentences":["Temporal grounding is crucial in multimodal learning, but it poses challenges when applied to animal behavior data due to the sparsity and uniform distribution of moments.","To address these challenges, we propose a novel Positional Recovery Training framework (Port), which prompts the model with the start and end times of specific animal behaviors during training.","Specifically, Port enhances the baseline model with a Recovering part to predict flipped label sequences and align distributions with a Dual-alignment method.","This allows the model to focus on specific temporal regions prompted by ground-truth information.","Extensive experiments on the Animal Kingdom dataset demonstrate the effectiveness of Port, achieving an IoU@0.3 of 38.52.","It emerges as one of the top performers in the sub-track of MMVRAC in ICME 2024 Grand Challenges."],"url":"http://arxiv.org/abs/2405.05523v1","category":"cs.CV"}
{"created":"2024-05-09 03:19:32","title":"Deep Learning for CSI Feedback: One-Sided Model and Joint Multi-Module Learning Perspectives","abstract":"The use of deep learning (DL) for channel state information (CSI) feedback has garnered widespread attention across academia and industry. The mainstream DL architectures, e.g., CsiNet, deploy DL models on the base station (BS) side and the user equipment (UE) side, which are highly coupled and need to be trained jointly. However, two-sided DL models require collaborations between different network vendors and UE vendors, which entails considerable challenges in order to achieve consensus, e.g., model maintenance and responsibility. Furthermore, DL-based CSI feedback design invokes DL to reduce only the CSI feedback error, whereas jointly optimizing several modules at the transceivers would provide more significant gains. This article presents DL-based CSI feedback from the perspectives of one-sided model and joint multi-module learning. We herein introduce various novel one-sided CSI feedback architectures. In particular, the recently proposed CSI-PPPNet provides a one-sided one-for-all framework, which allows a DL model to deal with arbitrary CSI compression ratios. We review different joint multi-module learning methods, where the CSI feedback module is learned jointly with other modules including channel coding, channel estimation, pilot design and precoding design. Finally, future directions and challenges for DL-based CSI feedback are discussed, from the perspectives of inherent limitations of artificial intelligence (AI) and practical deployment issues.","sentences":["The use of deep learning (DL) for channel state information (CSI) feedback has garnered widespread attention across academia and industry.","The mainstream DL architectures, e.g., CsiNet, deploy DL models on the base station (BS) side and the user equipment (UE) side, which are highly coupled and need to be trained jointly.","However, two-sided DL models require collaborations between different network vendors and UE vendors, which entails considerable challenges in order to achieve consensus, e.g., model maintenance and responsibility.","Furthermore, DL-based CSI feedback design invokes DL to reduce only the CSI feedback error, whereas jointly optimizing several modules at the transceivers would provide more significant gains.","This article presents DL-based CSI feedback from the perspectives of one-sided model and joint multi-module learning.","We herein introduce various novel one-sided CSI feedback architectures.","In particular, the recently proposed CSI-PPPNet provides a one-sided one-for-all framework, which allows a DL model to deal with arbitrary CSI compression ratios.","We review different joint multi-module learning methods, where the CSI feedback module is learned jointly with other modules including channel coding, channel estimation, pilot design and precoding design.","Finally, future directions and challenges for DL-based CSI feedback are discussed, from the perspectives of inherent limitations of artificial intelligence (AI) and practical deployment issues."],"url":"http://arxiv.org/abs/2405.05522v1","category":"eess.SP"}
{"created":"2024-05-09 02:58:55","title":"DTCLMapper: Dual Temporal Consistent Learning for Vectorized HD Map Construction","abstract":"Temporal information plays a pivotal role in Bird's-Eye-View (BEV) driving scene understanding, which can alleviate the visual information sparsity. However, the indiscriminate temporal fusion method will cause the barrier of feature redundancy when constructing vectorized High-Definition (HD) maps. In this paper, we revisit the temporal fusion of vectorized HD maps, focusing on temporal instance consistency and temporal map consistency learning. To improve the representation of instances in single-frame maps, we introduce a novel method, DTCLMapper. This approach uses a dual-stream temporal consistency learning module that combines instance embedding with geometry maps. In the instance embedding component, our approach integrates temporal Instance Consistency Learning (ICL), ensuring consistency from vector points and instance features aggregated from points. A vectorized points pre-selection module is employed to enhance the regression efficiency of vector points from each instance. Then aggregated instance features obtained from the vectorized points preselection module are grounded in contrastive learning to realize temporal consistency, where positive and negative samples are selected based on position and semantic information. The geometry mapping component introduces Map Consistency Learning (MCL) designed with self-supervised learning. The MCL enhances the generalization capability of our consistent learning approach by concentrating on the global location and distribution constraints of the instances. Extensive experiments on well-recognized benchmarks indicate that the proposed DTCLMapper achieves state-of-the-art performance in vectorized mapping tasks, reaching 61.9% and 65.1% mAP scores on the nuScenes and Argoverse datasets, respectively. The source code will be made publicly available at https://github.com/lynn-yu/DTCLMapper.","sentences":["Temporal information plays a pivotal role in Bird's-Eye-View (BEV) driving scene understanding, which can alleviate the visual information sparsity.","However, the indiscriminate temporal fusion method will cause the barrier of feature redundancy when constructing vectorized High-Definition (HD) maps.","In this paper, we revisit the temporal fusion of vectorized HD maps, focusing on temporal instance consistency and temporal map consistency learning.","To improve the representation of instances in single-frame maps, we introduce a novel method, DTCLMapper.","This approach uses a dual-stream temporal consistency learning module that combines instance embedding with geometry maps.","In the instance embedding component, our approach integrates temporal Instance Consistency Learning (ICL), ensuring consistency from vector points and instance features aggregated from points.","A vectorized points pre-selection module is employed to enhance the regression efficiency of vector points from each instance.","Then aggregated instance features obtained from the vectorized points preselection module are grounded in contrastive learning to realize temporal consistency, where positive and negative samples are selected based on position and semantic information.","The geometry mapping component introduces Map Consistency Learning (MCL) designed with self-supervised learning.","The MCL enhances the generalization capability of our consistent learning approach by concentrating on the global location and distribution constraints of the instances.","Extensive experiments on well-recognized benchmarks indicate that the proposed DTCLMapper achieves state-of-the-art performance in vectorized mapping tasks, reaching 61.9% and 65.1% mAP scores on the nuScenes and Argoverse datasets, respectively.","The source code will be made publicly available at https://github.com/lynn-yu/DTCLMapper."],"url":"http://arxiv.org/abs/2405.05518v1","category":"cs.CV"}
{"created":"2024-05-09 02:44:42","title":"Automatic question generation for propositional logical equivalences","abstract":"The increase in academic dishonesty cases among college students has raised concern, particularly due to the shift towards online learning caused by the pandemic. We aim to develop and implement a method capable of generating tailored questions for each student. The use of Automatic Question Generation (AQG) is a possible solution. Previous studies have investigated AQG frameworks in education, which include validity, user-defined difficulty, and personalized problem generation. Our new AQG approach produces logical equivalence problems for Discrete Mathematics, which is a core course for year-one computer science students. This approach utilizes a syntactic grammar and a semantic attribute system through top-down parsing and syntax tree transformations. Our experiments show that the difficulty level of questions generated by our AQG approach is similar to the questions presented to students in the textbook [1]. These results confirm the practicality of our AQG approach for automated question generation in education, with the potential to significantly enhance learning experiences.","sentences":["The increase in academic dishonesty cases among college students has raised concern, particularly due to the shift towards online learning caused by the pandemic.","We aim to develop and implement a method capable of generating tailored questions for each student.","The use of Automatic Question Generation (AQG) is a possible solution.","Previous studies have investigated AQG frameworks in education, which include validity, user-defined difficulty, and personalized problem generation.","Our new AQG approach produces logical equivalence problems for Discrete Mathematics, which is a core course for year-one computer science students.","This approach utilizes a syntactic grammar and a semantic attribute system through top-down parsing and syntax tree transformations.","Our experiments show that the difficulty level of questions generated by our AQG approach is similar to the questions presented to students in the textbook [1].","These results confirm the practicality of our AQG approach for automated question generation in education, with the potential to significantly enhance learning experiences."],"url":"http://arxiv.org/abs/2405.05513v1","category":"cs.CL"}
{"created":"2024-05-09 17:07:16","title":"Multilevel Regression and Poststratification Interface: Application to Track Community-level COVID-19 Viral Transmission","abstract":"In the absence of comprehensive or random testing throughout the COVID-19 pandemic, we have developed a proxy method for synthetic random sampling to estimate the actual viral incidence in the community, based on viral RNA testing of asymptomatic patients who present for elective procedures within a hospital system. The approach collects routine testing data on SARS-CoV-2 exposure among outpatients and performs statistical adjustments of sample representation using multilevel regression and poststratification (MRP). MRP adjusts for selection bias and yields stable small area estimates. We have developed an open-source, user-friendly MRP interface for public implementation of the statistical workflow. We illustrate the MRP interface with an application to track community-level COVID-19 viral transmission in the state of Michigan.","sentences":["In the absence of comprehensive or random testing throughout the COVID-19 pandemic, we have developed a proxy method for synthetic random sampling to estimate the actual viral incidence in the community, based on viral RNA testing of asymptomatic patients who present for elective procedures within a hospital system.","The approach collects routine testing data on SARS-CoV-2 exposure among outpatients and performs statistical adjustments of sample representation using multilevel regression and poststratification (MRP).","MRP adjusts for selection bias and yields stable small area estimates.","We have developed an open-source, user-friendly MRP interface for public implementation of the statistical workflow.","We illustrate the MRP interface with an application to track community-level COVID-19 viral transmission in the state of Michigan."],"url":"http://arxiv.org/abs/2405.05909v1","category":"stat.AP"}
{"created":"2024-05-09 15:48:07","title":"The Perspectivist Paradigm Shift: Assumptions and Challenges of Capturing Human Labels","abstract":"Longstanding data labeling practices in machine learning involve collecting and aggregating labels from multiple annotators. But what should we do when annotators disagree? Though annotator disagreement has long been seen as a problem to minimize, new perspectivist approaches challenge this assumption by treating disagreement as a valuable source of information. In this position paper, we examine practices and assumptions surrounding the causes of disagreement--some challenged by perspectivist approaches, and some that remain to be addressed--as well as practical and normative challenges for work operating under these assumptions. We conclude with recommendations for the data labeling pipeline and avenues for future research engaging with subjectivity and disagreement.","sentences":["Longstanding data labeling practices in machine learning involve collecting and aggregating labels from multiple annotators.","But what should we do when annotators disagree?","Though annotator disagreement has long been seen as a problem to minimize, new perspectivist approaches challenge this assumption by treating disagreement as a valuable source of information.","In this position paper, we examine practices and assumptions surrounding the causes of disagreement--some challenged by perspectivist approaches, and some that remain to be addressed--as well as practical and normative challenges for work operating under these assumptions.","We conclude with recommendations for the data labeling pipeline and avenues for future research engaging with subjectivity and disagreement."],"url":"http://arxiv.org/abs/2405.05860v1","category":"cs.LG"}
{"created":"2024-05-09 15:14:58","title":"Upper and Lower Bounds on Phase-Space Rearrangements","abstract":"Broad classes of plasma phenomena can be understood in terms of phase-space rearrangements. For example, the net effect of a wave-particle interaction may consist of moving populations of particles from one region of phase space to another. Different phenomena drive rearrangements that obey different rules. When those rules can be specified, it is possible to calculate bounds that limit the possible effects the rearrangement could have (such as limits on how much energy can be extracted from the particles). This leads to two problems. The first is to understand the mapping between the allowed class of rearrangements and the possible outcomes that these rearrangements can have on the overall distribution. The second is to understand which rules are appropriate for which physical systems. There has been recent progress on both fronts, but a variety of interesting questions remain.","sentences":["Broad classes of plasma phenomena can be understood in terms of phase-space rearrangements.","For example, the net effect of a wave-particle interaction may consist of moving populations of particles from one region of phase space to another.","Different phenomena drive rearrangements that obey different rules.","When those rules can be specified, it is possible to calculate bounds that limit the possible effects the rearrangement could have (such as limits on how much energy can be extracted from the particles).","This leads to two problems.","The first is to understand the mapping between the allowed class of rearrangements and the possible outcomes that these rearrangements can have on the overall distribution.","The second is to understand which rules are appropriate for which physical systems.","There has been recent progress on both fronts, but a variety of interesting questions remain."],"url":"http://arxiv.org/abs/2405.05835v1","category":"physics.plasm-ph"}
{"created":"2024-05-09 14:57:22","title":"Probing CPV mixing in the Higgs sector in VBF at 1 TeV ILC","abstract":"With the current precision of measurements by the ATLAS and CMS experiments, it cannot be excluded that a SM-like Higgs boson is a CP violating mixture of CP-even and CP-odd states. We explore this possibility here, assuming Higgs boson production in ZZ-fusion, at 1 TeV ILC, with unpolarized beams. The full simulation of SM background and fast simulation of the signal is performed, simulating 8 ab$^{-1}$ of data collected with the ILD detector. We demonstrate that the CP mixing angle $\\Psi_{\\mathrm{CP}}$ between scalar and pseudoscalar states can be measured with the statistical uncertainty of 3.8 mrad at 68% CL, corresponding to 1.44 $\\cdot 10^{-5}$ for the CP parameter $f_\\mathrm{CP}$, for the pure scalar state. This is the first result on sensitivity of an $e^{+}e^{-}$ collider to measure $f_\\mathrm{CP}$ in the Higgs production vertex in vector boson fusion.","sentences":["With the current precision of measurements by the ATLAS and CMS experiments, it cannot be excluded that a SM-like Higgs boson is a CP violating mixture of CP-even and CP-odd states.","We explore this possibility here, assuming Higgs boson production in ZZ-fusion, at 1 TeV ILC, with unpolarized beams.","The full simulation of SM background and fast simulation of the signal is performed, simulating 8 ab$^{-1}$ of data collected with the ILD detector.","We demonstrate that the CP mixing angle $\\Psi_{\\mathrm{CP}}$ between scalar and pseudoscalar states can be measured with the statistical uncertainty of 3.8 mrad at 68% CL, corresponding to 1.44 $\\cdot 10^{-5}$ for the CP parameter $f_\\mathrm{CP}$, for the pure scalar state.","This is the first result on sensitivity of an $e^{+}e^{-}$ collider to measure $f_\\mathrm{CP}$ in the Higgs production vertex in vector boson fusion."],"url":"http://arxiv.org/abs/2405.05820v1","category":"hep-ex"}
{"created":"2024-05-09 14:43:09","title":"NeuRSS: Enhancing AUV Localization and Bathymetric Mapping with Neural Rendering for Sidescan SLAM","abstract":"Implicit neural representations and neural render- ing have gained increasing attention for bathymetry estimation from sidescan sonar (SSS). These methods incorporate multiple observations of the same place from SSS data to constrain the elevation estimate, converging to a globally-consistent bathymetric model. However, the quality and precision of the bathymetric estimate are limited by the positioning accuracy of the autonomous underwater vehicle (AUV) equipped with the sonar. The global positioning estimate of the AUV relying on dead reckoning (DR) has an unbounded error due to the absence of a geo-reference system like GPS underwater. To address this challenge, we propose in this letter a modern and scalable framework, NeuRSS, for SSS SLAM based on DR and loop closures (LCs) over large timescales, with an elevation prior provided by the bathymetric estimate using neural rendering from SSS. This framework is an iterative procedure that improves localization and bathymetric mapping. Initially, the bathymetry estimated from SSS using the DR estimate, though crude, can provide an important elevation prior in the nonlinear least-squares (NLS) optimization that estimates the relative pose between two loop-closure vertices in a pose graph. Subsequently, the global pose estimate from the SLAM component improves the positioning estimate of the vehicle, thus improving the bathymetry estimation. We validate our localization and mapping approach on two large surveys collected with a surface vessel and an AUV, respectively. We evaluate their localization results against the ground truth and compare the bathymetry estimation against data collected with multibeam echo sounders (MBES).","sentences":["Implicit neural representations and neural render- ing have gained increasing attention for bathymetry estimation from sidescan sonar (SSS).","These methods incorporate multiple observations of the same place from SSS data to constrain the elevation estimate, converging to a globally-consistent bathymetric model.","However, the quality and precision of the bathymetric estimate are limited by the positioning accuracy of the autonomous underwater vehicle (AUV) equipped with the sonar.","The global positioning estimate of the AUV relying on dead reckoning (DR) has an unbounded error due to the absence of a geo-reference system like GPS underwater.","To address this challenge, we propose in this letter a modern and scalable framework, NeuRSS, for SSS SLAM based on DR and loop closures (LCs) over large timescales, with an elevation prior provided by the bathymetric estimate using neural rendering from SSS.","This framework is an iterative procedure that improves localization and bathymetric mapping.","Initially, the bathymetry estimated from SSS using the DR estimate, though crude, can provide an important elevation prior in the nonlinear least-squares (NLS) optimization that estimates the relative pose between two loop-closure vertices in a pose graph.","Subsequently, the global pose estimate from the SLAM component improves the positioning estimate of the vehicle, thus improving the bathymetry estimation.","We validate our localization and mapping approach on two large surveys collected with a surface vessel and an AUV, respectively.","We evaluate their localization results against the ground truth and compare the bathymetry estimation against data collected with multibeam echo sounders (MBES)."],"url":"http://arxiv.org/abs/2405.05807v1","category":"cs.RO"}
{"created":"2024-05-09 12:43:43","title":"Change point localisation and inference in fragmented functional data","abstract":"We study the problem of change point localisation and inference for sequentially collected fragmented functional data, where each curve is observed only over discrete grids randomly sampled over a short fragment. The sequence of underlying covariance functions is assumed to be piecewise constant, with changes happening at unknown time points. To localise the change points, we propose a computationally efficient fragmented functional dynamic programming (FFDP) algorithm with consistent change point localisation rates. With an extra step of local refinement, we derive the limiting distributions for the refined change point estimators in two different regimes where the minimal jump size vanishes and where it remains constant as the sample size diverges. Such results are the first time seen in the fragmented functional data literature. As a byproduct of independent interest, we also present a non-asymptotic result on the estimation error of the covariance function estimators over intervals with change points inspired by Lin et al. (2021). Our result accounts for the effects of the sampling grid size within each fragment under novel identifiability conditions. Extensive numerical studies are also provided to support our theoretical results.","sentences":["We study the problem of change point localisation and inference for sequentially collected fragmented functional data, where each curve is observed only over discrete grids randomly sampled over a short fragment.","The sequence of underlying covariance functions is assumed to be piecewise constant, with changes happening at unknown time points.","To localise the change points, we propose a computationally efficient fragmented functional dynamic programming (FFDP) algorithm with consistent change point localisation rates.","With an extra step of local refinement, we derive the limiting distributions for the refined change point estimators in two different regimes where the minimal jump size vanishes and where it remains constant as the sample size diverges.","Such results are the first time seen in the fragmented functional data literature.","As a byproduct of independent interest, we also present a non-asymptotic result on the estimation error of the covariance function estimators over intervals with change points inspired by Lin et al. (2021).","Our result accounts for the effects of the sampling grid size within each fragment under novel identifiability conditions.","Extensive numerical studies are also provided to support our theoretical results."],"url":"http://arxiv.org/abs/2405.05730v1","category":"stat.ME"}
{"created":"2024-05-09 09:08:09","title":"Policy Gradient with Active Importance Sampling","abstract":"Importance sampling (IS) represents a fundamental technique for a large surge of off-policy reinforcement learning approaches. Policy gradient (PG) methods, in particular, significantly benefit from IS, enabling the effective reuse of previously collected samples, thus increasing sample efficiency. However, classically, IS is employed in RL as a passive tool for re-weighting historical samples. However, the statistical community employs IS as an active tool combined with the use of behavioral distributions that allow the reduction of the estimate variance even below the sample mean one. In this paper, we focus on this second setting by addressing the behavioral policy optimization (BPO) problem. We look for the best behavioral policy from which to collect samples to reduce the policy gradient variance as much as possible. We provide an iterative algorithm that alternates between the cross-entropy estimation of the minimum-variance behavioral policy and the actual policy optimization, leveraging on defensive IS. We theoretically analyze such an algorithm, showing that it enjoys a convergence rate of order $O(\\epsilon^{-4})$ to a stationary point, but depending on a more convenient variance term w.r.t. standard PG methods. We then provide a practical version that is numerically validated, showing the advantages in the policy gradient estimation variance and on the learning speed.","sentences":["Importance sampling (IS) represents a fundamental technique for a large surge of off-policy reinforcement learning approaches.","Policy gradient (PG) methods, in particular, significantly benefit from IS, enabling the effective reuse of previously collected samples, thus increasing sample efficiency.","However, classically, IS is employed in RL as a passive tool for re-weighting historical samples.","However, the statistical community employs IS as an active tool combined with the use of behavioral distributions that allow the reduction of the estimate variance even below the sample mean one.","In this paper, we focus on this second setting by addressing the behavioral policy optimization (BPO) problem.","We look for the best behavioral policy from which to collect samples to reduce the policy gradient variance as much as possible.","We provide an iterative algorithm that alternates between the cross-entropy estimation of the minimum-variance behavioral policy and the actual policy optimization, leveraging on defensive IS.","We theoretically analyze such an algorithm, showing that it enjoys a convergence rate of order $O(\\epsilon^{-4})$ to a stationary point, but depending on a more convenient variance term w.r.t. standard PG methods.","We then provide a practical version that is numerically validated, showing the advantages in the policy gradient estimation variance and on the learning speed."],"url":"http://arxiv.org/abs/2405.05630v1","category":"cs.LG"}
{"created":"2024-05-09 08:46:11","title":"The effect of anisotropy on the formation of heavy quarkonium bound states","abstract":"We study the real part of the static potential of a heavy quark-antiquark system in an anisotropic plasma medium. We use a quasi-particle approach where the collective dynamics of the plasma constituents is described using hard-loop perturbation theory. The parton distribution function is characterized by a set of parameters that can accurately describe the anisotropy of the plasma produced in a heavy ion collision. We calculate the potential numerically in strongly anisotropic systems and study the angular dependence of the distortion of the potential relative to the isotropic one. We obtain an analytic expression for the real part of the heavy quark potential in the limit of weak anisotropy using a model that expresses the potential in terms of effective screening masses that depend on the anisotropy parameters and the orientation of the quark-antiquark pair. A 1-dimensional potential is formulated in terms of angle averaged screening masses that incorporate the anisotropy of the medium into a radial coordinate. We solve the corresponding Schr\\\"odinger equation and show that the magnitude of the binding energy typically increases with anisotropy. Anisotropy can play an important role, especially in states with non-zero angular momentum. This means that the number of bound states that are formed could depend on specific characteristics of the anisotropy of the plasma. Our study suggests that plasma anisotropy plays an important role in the dynamics of heavy quarkonium and motivates further study.","sentences":["We study the real part of the static potential of a heavy quark-antiquark system in an anisotropic plasma medium.","We use a quasi-particle approach where the collective dynamics of the plasma constituents is described using hard-loop perturbation theory.","The parton distribution function is characterized by a set of parameters that can accurately describe the anisotropy of the plasma produced in a heavy ion collision.","We calculate the potential numerically in strongly anisotropic systems and study the angular dependence of the distortion of the potential relative to the isotropic one.","We obtain an analytic expression for the real part of the heavy quark potential in the limit of weak anisotropy using a model that expresses the potential in terms of effective screening masses that depend on the anisotropy parameters and the orientation of the quark-antiquark pair.","A 1-dimensional potential is formulated in terms of angle averaged screening masses that incorporate the anisotropy of the medium into a radial coordinate.","We solve the corresponding Schr\\\"odinger equation and show that the magnitude of the binding energy typically increases with anisotropy.","Anisotropy can play an important role, especially in states with non-zero angular momentum.","This means that the number of bound states that are formed could depend on specific characteristics of the anisotropy of the plasma.","Our study suggests that plasma anisotropy plays an important role in the dynamics of heavy quarkonium and motivates further study."],"url":"http://arxiv.org/abs/2405.05622v1","category":"hep-ph"}
{"created":"2024-05-09 04:58:27","title":"Predicting Cognitive Load Using Sensor Data in a Literacy Game","abstract":"Educational games are being increasingly used to support self-paced learning. However, educators and system designers often face challenges in monitoring student affect and cognitive load. Existing assessments in game-based learning environments (GBLEs) tend to focus more on outcomes rather than processes, potentially overlooking key aspects of the learning journey that include learner affect and cognitive load. To address this issue, we collected data and trained a model to track learner cognitive load while they used an online literacy game for English. We collected affect-related physiological data and pupil data during gameplay to enable the development of models that identify these latent characteristics of learner processes. Our model indicates the feasibility of using these data to track cognitive load in GBLEs. Our multimodal model distinguished different levels of cognitive load, achieving the highest Kappa (.417) and accuracy (70%). Our model reveals the importance of including affect-related features (i.e., EDA and heart rate) when predicting cognitive load and extends recent findings suggesting the benefit of using multiple channels when modeling latent aspects of learner processes. Findings also suggest that cognitive load tracking could now be used to facilitate the creation of personalized learning experiences.","sentences":["Educational games are being increasingly used to support self-paced learning.","However, educators and system designers often face challenges in monitoring student affect and cognitive load.","Existing assessments in game-based learning environments (GBLEs) tend to focus more on outcomes rather than processes, potentially overlooking key aspects of the learning journey that include learner affect and cognitive load.","To address this issue, we collected data and trained a model to track learner cognitive load while they used an online literacy game for English.","We collected affect-related physiological data and pupil data during gameplay to enable the development of models that identify these latent characteristics of learner processes.","Our model indicates the feasibility of using these data to track cognitive load in GBLEs.","Our multimodal model distinguished different levels of cognitive load, achieving the highest Kappa (.417) and accuracy (70%).","Our model reveals the importance of including affect-related features (i.e., EDA and heart rate) when predicting cognitive load and extends recent findings suggesting the benefit of using multiple channels when modeling latent aspects of learner processes.","Findings also suggest that cognitive load tracking could now be used to facilitate the creation of personalized learning experiences."],"url":"http://arxiv.org/abs/2405.05543v1","category":"cs.HC"}
{"created":"2024-05-09 02:48:15","title":"HPPS: A Hierarchical Progressive Perception System for Luggage Trolley Detection and Localization at Airports","abstract":"The robotic autonomous luggage trolley collection system employs robots to gather and transport scattered luggage trolleys at airports. However, existing methods for detecting and locating these luggage trolleys often fail when they are not fully visible. To address this, we introduce the Hierarchical Progressive Perception System (HPPS), which enhances the detection and localization of luggage trolleys under partial occlusion. The HPPS processes the luggage trolley's position and orientation separately, which requires only RGB images for labeling and training, eliminating the need for 3D coordinates and alignment. The HPPS can accurately determine the position of the luggage trolley with just one well-detected keypoint and estimate the luggage trolley's orientation when it is partially occluded. Once the luggage trolley's initial pose is detected, HPPS updates this information continuously to refine its accuracy until the robot begins grasping. The experiments on detection and localization demonstrate that HPPS is more reliable under partial occlusion compared to existing methods. Its effectiveness and robustness have also been confirmed through practical tests in actual luggage trolley collection tasks. A website about this work is available at HPPS.","sentences":["The robotic autonomous luggage trolley collection system employs robots to gather and transport scattered luggage trolleys at airports.","However, existing methods for detecting and locating these luggage trolleys often fail when they are not fully visible.","To address this, we introduce the Hierarchical Progressive Perception System (HPPS), which enhances the detection and localization of luggage trolleys under partial occlusion.","The HPPS processes the luggage trolley's position and orientation separately, which requires only RGB images for labeling and training, eliminating the need for 3D coordinates and alignment.","The HPPS can accurately determine the position of the luggage trolley with just one well-detected keypoint and estimate the luggage trolley's orientation when it is partially occluded.","Once the luggage trolley's initial pose is detected, HPPS updates this information continuously to refine its accuracy until the robot begins grasping.","The experiments on detection and localization demonstrate that HPPS is more reliable under partial occlusion compared to existing methods.","Its effectiveness and robustness have also been confirmed through practical tests in actual luggage trolley collection tasks.","A website about this work is available at HPPS."],"url":"http://arxiv.org/abs/2405.05514v1","category":"cs.RO"}
{"created":"2024-05-09 02:41:42","title":"Characteristic Learning for Provable One Step Generation","abstract":"We propose the characteristic generator, a novel one-step generative model that combines the efficiency of sampling in Generative Adversarial Networks (GANs) with the stable performance of flow-based models. Our model is driven by characteristics, along which the probability density transport can be described by ordinary differential equations (ODEs). Specifically, We estimate the velocity field through nonparametric regression and utilize Euler method to solve the probability flow ODE, generating a series of discrete approximations to the characteristics. We then use a deep neural network to fit these characteristics, ensuring a one-step mapping that effectively pushes the prior distribution towards the target distribution. In the theoretical aspect, we analyze the errors in velocity matching, Euler discretization, and characteristic fitting to establish a non-asymptotic convergence rate for the characteristic generator in 2-Wasserstein distance. To the best of our knowledge, this is the first thorough analysis for simulation-free one step generative models. Additionally, our analysis refines the error analysis of flow-based generative models in prior works. We apply our method on both synthetic and real datasets, and the results demonstrate that the characteristic generator achieves high generation quality with just a single evaluation of neural network.","sentences":["We propose the characteristic generator, a novel one-step generative model that combines the efficiency of sampling in Generative Adversarial Networks (GANs) with the stable performance of flow-based models.","Our model is driven by characteristics, along which the probability density transport can be described by ordinary differential equations (ODEs).","Specifically, We estimate the velocity field through nonparametric regression and utilize Euler method to solve the probability flow ODE, generating a series of discrete approximations to the characteristics.","We then use a deep neural network to fit these characteristics, ensuring a one-step mapping that effectively pushes the prior distribution towards the target distribution.","In the theoretical aspect, we analyze the errors in velocity matching, Euler discretization, and characteristic fitting to establish a non-asymptotic convergence rate for the characteristic generator in 2-Wasserstein distance.","To the best of our knowledge, this is the first thorough analysis for simulation-free one step generative models.","Additionally, our analysis refines the error analysis of flow-based generative models in prior works.","We apply our method on both synthetic and real datasets, and the results demonstrate that the characteristic generator achieves high generation quality with just a single evaluation of neural network."],"url":"http://arxiv.org/abs/2405.05512v1","category":"cs.LG"}
{"created":"2024-05-09 02:37:53","title":"Redefining Information Retrieval of Structured Database via Large Language Models","abstract":"Retrieval augmentation is critical when Language Models (LMs) exploit non-parametric knowledge related to the query through external knowledge bases before reasoning. The retrieved information is incorporated into LMs as context alongside the query, enhancing the reliability of responses towards factual questions. Prior researches in retrieval augmentation typically follow a retriever-generator paradigm. In this context, traditional retrievers encounter challenges in precisely and seamlessly extracting query-relevant information from knowledge bases. To address this issue, this paper introduces a novel retrieval augmentation framework called ChatLR that primarily employs the powerful semantic understanding ability of Large Language Models (LLMs) as retrievers to achieve precise and concise information retrieval. Additionally, we construct an LLM-based search and question answering system tailored for the financial domain by fine-tuning LLM on two tasks including Text2API and API-ID recognition. Experimental results demonstrate the effectiveness of ChatLR in addressing user queries, achieving an overall information retrieval accuracy exceeding 98.8\\%.","sentences":["Retrieval augmentation is critical when Language Models (LMs) exploit non-parametric knowledge related to the query through external knowledge bases before reasoning.","The retrieved information is incorporated into LMs as context alongside the query, enhancing the reliability of responses towards factual questions.","Prior researches in retrieval augmentation typically follow a retriever-generator paradigm.","In this context, traditional retrievers encounter challenges in precisely and seamlessly extracting query-relevant information from knowledge bases.","To address this issue, this paper introduces a novel retrieval augmentation framework called ChatLR that primarily employs the powerful semantic understanding ability of Large Language Models (LLMs) as retrievers to achieve precise and concise information retrieval.","Additionally, we construct an LLM-based search and question answering system tailored for the financial domain by fine-tuning LLM on two tasks including Text2API and API-ID recognition.","Experimental results demonstrate the effectiveness of ChatLR in addressing user queries, achieving an overall information retrieval accuracy exceeding 98.8\\%."],"url":"http://arxiv.org/abs/2405.05508v1","category":"cs.IR"}
{"created":"2024-05-09 02:11:30","title":"Research on the Tender Leaf Identification and Mechanically Perceptible Plucking Finger for High-quality Green Tea","abstract":"BACKGROUND: Intelligent identification and precise plucking are the keys to intelligent tea harvesting robots, which are of increasing significance nowadays. Aiming at plucking tender leaves for high-quality green tea producing, in this paper, a tender leaf identification algorithm and a mechanically perceptible plucking finger have been proposed. RESULTS: Based on segmentation algorithm and color features, the tender leaf identification algorithm shows an average identification accuracy of over 92.8%. The mechanically perceptible plucking finger plucks tender leaves in a way that a human hand does so as to remain high quality of tea products. Though finite element analysis, we determine the ideal size of grippers and the location of strain gauge attachment on a gripper to enable the employment of feedback control of desired gripping force. Revealed from our experiments, the success rate of tender leaf plucking reaches 92.5%, demonstrating the effectiveness of our design. CONCLUSION: The results show that the tender leaf identification algorithm and the mechanically perceptible plucking finger are effective for tender leaves identification and plucking, providing a foundation for the development of an intelligent tender leaf plucking robot.","sentences":["BACKGROUND: Intelligent identification and precise plucking are the keys to intelligent tea harvesting robots, which are of increasing significance nowadays.","Aiming at plucking tender leaves for high-quality green tea producing, in this paper, a tender leaf identification algorithm and a mechanically perceptible plucking finger have been proposed.","RESULTS:","Based on segmentation algorithm and color features, the tender leaf identification algorithm shows an average identification accuracy of over 92.8%.","The mechanically perceptible plucking finger plucks tender leaves in a way that a human hand does so as to remain high quality of tea products.","Though finite element analysis, we determine the ideal size of grippers and the location of strain gauge attachment on a gripper to enable the employment of feedback control of desired gripping force.","Revealed from our experiments, the success rate of tender leaf plucking reaches 92.5%, demonstrating the effectiveness of our design.","CONCLUSION:","The results show that the tender leaf identification algorithm and the mechanically perceptible plucking finger are effective for tender leaves identification and plucking, providing a foundation for the development of an intelligent tender leaf plucking robot."],"url":"http://arxiv.org/abs/2405.05500v1","category":"cs.RO"}
{"created":"2024-05-09 02:11:01","title":"Multi-Scale Dilated Convolution Network for Long-Term Time Series Forecasting","abstract":"Accurate forecasting of long-term time series has important applications for decision making and planning. However, it remains challenging to capture the long-term dependencies in time series data. To better extract long-term dependencies, We propose Multi Scale Dilated Convolution Network (MSDCN), a method that utilizes a shallow dilated convolution architecture to capture the period and trend characteristics of long time series. We design different convolution blocks with exponentially growing dilations and varying kernel sizes to sample time series data at different scales. Furthermore, we utilize traditional autoregressive model to capture the linear relationships within the data. To validate the effectiveness of the proposed approach, we conduct experiments on eight challenging long-term time series forecasting benchmark datasets. The experimental results show that our approach outperforms the prior state-of-the-art approaches and shows significant inference speed improvements compared to several strong baseline methods.","sentences":["Accurate forecasting of long-term time series has important applications for decision making and planning.","However, it remains challenging to capture the long-term dependencies in time series data.","To better extract long-term dependencies, We propose Multi Scale Dilated Convolution Network (MSDCN), a method that utilizes a shallow dilated convolution architecture to capture the period and trend characteristics of long time series.","We design different convolution blocks with exponentially growing dilations and varying kernel sizes to sample time series data at different scales.","Furthermore, we utilize traditional autoregressive model to capture the linear relationships within the data.","To validate the effectiveness of the proposed approach, we conduct experiments on eight challenging long-term time series forecasting benchmark datasets.","The experimental results show that our approach outperforms the prior state-of-the-art approaches and shows significant inference speed improvements compared to several strong baseline methods."],"url":"http://arxiv.org/abs/2405.05499v1","category":"cs.LG"}
{"created":"2024-05-09 01:40:38","title":"Parameter-Efficient Fine-Tuning With Adapters","abstract":"In the arena of language model fine-tuning, the traditional approaches, such as Domain-Adaptive Pretraining (DAPT) and Task-Adaptive Pretraining (TAPT), although effective, but computational intensive. This research introduces a novel adaptation method utilizing the UniPELT framework as a base and added a PromptTuning Layer, which significantly reduces the number of trainable parameters while maintaining competitive performance across various benchmarks. Our method employs adapters, which enable efficient transfer of pretrained models to new tasks with minimal retraining of the base model parameters. We evaluate our approach using three diverse datasets: the GLUE benchmark, a domain-specific dataset comprising four distinct areas, and the Stanford Question Answering Dataset 1.1 (SQuAD). Our results demonstrate that our customized adapter-based method achieves performance comparable to full model fine-tuning, DAPT+TAPT and UniPELT strategies while requiring fewer or equivalent amount of parameters. This parameter efficiency not only alleviates the computational burden but also expedites the adaptation process. The study underlines the potential of adapters in achieving high performance with significantly reduced resource consumption, suggesting a promising direction for future research in parameter-efficient fine-tuning.","sentences":["In the arena of language model fine-tuning, the traditional approaches, such as Domain-Adaptive Pretraining (DAPT) and Task-Adaptive Pretraining (TAPT), although effective, but computational intensive.","This research introduces a novel adaptation method utilizing the UniPELT framework as a base and added a PromptTuning Layer, which significantly reduces the number of trainable parameters while maintaining competitive performance across various benchmarks.","Our method employs adapters, which enable efficient transfer of pretrained models to new tasks with minimal retraining of the base model parameters.","We evaluate our approach using three diverse datasets: the GLUE benchmark, a domain-specific dataset comprising four distinct areas, and the Stanford Question Answering Dataset 1.1 (SQuAD).","Our results demonstrate that our customized adapter-based method achieves performance comparable to full model fine-tuning, DAPT+TAPT and UniPELT strategies while requiring fewer or equivalent amount of parameters.","This parameter efficiency not only alleviates the computational burden but also expedites the adaptation process.","The study underlines the potential of adapters in achieving high performance with significantly reduced resource consumption, suggesting a promising direction for future research in parameter-efficient fine-tuning."],"url":"http://arxiv.org/abs/2405.05493v1","category":"cs.CL"}
{"created":"2024-05-09 01:38:38","title":"A logifold structure on measure space","abstract":"In this paper,we develop a local-to-global and measure-theoretical approach to understand datasets. The idea is to take network models with restricted domains as local charts of datasets. We develop the mathematical foundations for these structures, and show in experiments how it can be used to find fuzzy domains and to improve accuracy in data classification problems.","sentences":["In this paper,we develop a local-to-global and measure-theoretical approach to understand datasets.","The idea is to take network models with restricted domains as local charts of datasets.","We develop the mathematical foundations for these structures, and show in experiments how it can be used to find fuzzy domains and to improve accuracy in data classification problems."],"url":"http://arxiv.org/abs/2405.05492v1","category":"math.DG"}
{"created":"2024-05-09 00:37:56","title":"FloorSet - a VLSI Floorplanning Dataset with Design Constraints of Real-World SoCs","abstract":"Floorplanning for systems-on-a-chip (SoCs) and its sub-systems is a crucial and non-trivial step of the physical design flow. It represents a difficult combinatorial optimization problem. A typical large scale SoC with 120 partitions generates a search-space of nearly 10E250. As novel machine learning (ML) approaches emerge to tackle such problems, there is a growing need for a modern benchmark that comprises a large training dataset and performance metrics that better reflect real-world constraints and objectives compared to existing benchmarks. To address this need, we present FloorSet - two comprehensive datasets of synthetic fixed-outline floorplan layouts that reflect the distribution of real SoCs. Each dataset has 1M training samples and 100 test samples where each sample is a synthetic floor- plan. FloorSet-Prime comprises fully-abutted rectilinear partitions and near-optimal wire-length. A simplified dataset that reflects early design phases, FloorSet-Lite comprises rectangular partitions, with under 5 percent white-space and near-optimal wire-length. Both datasets define hard constraints seen in modern design flows such as shape constraints, edge-affinity, grouping constraints, and pre-placement constraints. FloorSet is intended to spur fundamental research on large-scale constrained optimization problems. Crucially, FloorSet alleviates the core issue of reproducibility in modern ML driven solutions to such problems. FloorSet is available as an open-source repository for the research community.","sentences":["Floorplanning for systems-on-a-chip (SoCs) and its sub-systems is a crucial and non-trivial step of the physical design flow.","It represents a difficult combinatorial optimization problem.","A typical large scale SoC with 120 partitions generates a search-space of nearly 10E250.","As novel machine learning (ML) approaches emerge to tackle such problems, there is a growing need for a modern benchmark that comprises a large training dataset and performance metrics that better reflect real-world constraints and objectives compared to existing benchmarks.","To address this need, we present FloorSet - two comprehensive datasets of synthetic fixed-outline floorplan layouts that reflect the distribution of real SoCs.","Each dataset has 1M training samples and 100 test samples where each sample is a synthetic floor- plan.","FloorSet-Prime comprises fully-abutted rectilinear partitions and near-optimal wire-length.","A simplified dataset that reflects early design phases, FloorSet-Lite comprises rectangular partitions, with under 5 percent white-space and near-optimal wire-length.","Both datasets define hard constraints seen in modern design flows such as shape constraints, edge-affinity, grouping constraints, and pre-placement constraints.","FloorSet is intended to spur fundamental research on large-scale constrained optimization problems.","Crucially, FloorSet alleviates the core issue of reproducibility in modern ML driven solutions to such problems.","FloorSet is available as an open-source repository for the research community."],"url":"http://arxiv.org/abs/2405.05480v1","category":"cs.AR"}
{"created":"2024-05-09 00:37:38","title":"BSL: Navigation Method Considering Blind Spots Based on ROS Navigation Stack and Blind Spots Layer for Mobile Robot","abstract":"This paper proposes a navigation method considering blind spots based on the robot operating system (ROS) navigation stack and blind spots layer (BSL) for a wheeled mobile robot. In this paper, environmental information is recognized using a laser range finder (LRF) and RGB-D cameras. Blind spots occur when corners or obstacles are present in the environment, and may lead to collisions if a human or object moves toward the robot from these blind spots. To prevent such collisions, this paper proposes a navigation method considering blind spots based on the local cost map layer of the BSL for the wheeled mobile robot. Blind spots are estimated by utilizing environmental data collected through RGB-D cameras. The navigation method that takes these blind spots into account is achieved through the implementation of the BSL and a local path planning method that employs an enhanced cost function of dynamic window approach. The effectiveness of the proposed method was further demonstrated through simulations and experiments.","sentences":["This paper proposes a navigation method considering blind spots based on the robot operating system (ROS) navigation stack and blind spots layer (BSL) for a wheeled mobile robot.","In this paper, environmental information is recognized using a laser range finder (LRF) and RGB-D cameras.","Blind spots occur when corners or obstacles are present in the environment, and may lead to collisions if a human or object moves toward the robot from these blind spots.","To prevent such collisions, this paper proposes a navigation method considering blind spots based on the local cost map layer of the BSL for the wheeled mobile robot.","Blind spots are estimated by utilizing environmental data collected through RGB-D cameras.","The navigation method that takes these blind spots into account is achieved through the implementation of the BSL and a local path planning method that employs an enhanced cost function of dynamic window approach.","The effectiveness of the proposed method was further demonstrated through simulations and experiments."],"url":"http://arxiv.org/abs/2405.05479v1","category":"cs.RO"}
{"created":"2024-05-08 23:52:37","title":"Model-Free Robust $\u03c6$-Divergence Reinforcement Learning Using Both Offline and Online Data","abstract":"The robust $\\phi$-regularized Markov Decision Process (RRMDP) framework focuses on designing control policies that are robust against parameter uncertainties due to mismatches between the simulator (nominal) model and real-world settings. This work makes two important contributions. First, we propose a model-free algorithm called Robust $\\phi$-regularized fitted Q-iteration (RPQ) for learning an $\\epsilon$-optimal robust policy that uses only the historical data collected by rolling out a behavior policy (with robust exploratory requirement) on the nominal model. To the best of our knowledge, we provide the first unified analysis for a class of $\\phi$-divergences achieving robust optimal policies in high-dimensional systems with general function approximation. Second, we introduce the hybrid robust $\\phi$-regularized reinforcement learning framework to learn an optimal robust policy using both historical data and online sampling. Towards this framework, we propose a model-free algorithm called Hybrid robust Total-variation-regularized Q-iteration (HyTQ: pronounced height-Q). To the best of our knowledge, we provide the first improved out-of-data-distribution assumption in large-scale problems with general function approximation under the hybrid robust $\\phi$-regularized reinforcement learning framework. Finally, we provide theoretical guarantees on the performance of the learned policies of our algorithms on systems with arbitrary large state space.","sentences":["The robust $\\phi$-regularized Markov Decision Process (RRMDP) framework focuses on designing control policies that are robust against parameter uncertainties due to mismatches between the simulator (nominal) model and real-world settings.","This work makes two important contributions.","First, we propose a model-free algorithm called Robust $\\phi$-regularized fitted Q-iteration (RPQ) for learning an $\\epsilon$-optimal robust policy that uses only the historical data collected by rolling out a behavior policy (with robust exploratory requirement) on the nominal model.","To the best of our knowledge, we provide the first unified analysis for a class of $\\phi$-divergences achieving robust optimal policies in high-dimensional systems with general function approximation.","Second, we introduce the hybrid robust $\\phi$-regularized reinforcement learning framework to learn an optimal robust policy using both historical data and online sampling.","Towards this framework, we propose a model-free algorithm called Hybrid robust Total-variation-regularized Q-iteration (HyTQ: pronounced height-Q).","To the best of our knowledge, we provide the first improved out-of-data-distribution assumption in large-scale problems with general function approximation under the hybrid robust $\\phi$-regularized reinforcement learning framework.","Finally, we provide theoretical guarantees on the performance of the learned policies of our algorithms on systems with arbitrary large state space."],"url":"http://arxiv.org/abs/2405.05468v1","category":"cs.LG"}
{"created":"2024-05-08 23:50:54","title":"AFEN: Respiratory Disease Classification using Ensemble Learning","abstract":"We present AFEN (Audio Feature Ensemble Learning), a model that leverages Convolutional Neural Networks (CNN) and XGBoost in an ensemble learning fashion to perform state-of-the-art audio classification for a range of respiratory diseases. We use a meticulously selected mix of audio features which provide the salient attributes of the data and allow for accurate classification. The extracted features are then used as an input to two separate model classifiers 1) a multi-feature CNN classifier and 2) an XGBoost Classifier. The outputs of the two models are then fused with the use of soft voting. Thus, by exploiting ensemble learning, we achieve increased robustness and accuracy. We evaluate the performance of the model on a database of 920 respiratory sounds, which undergoes data augmentation techniques to increase the diversity of the data and generalizability of the model. We empirically verify that AFEN sets a new state-of-the-art using Precision and Recall as metrics, while decreasing training time by 60%.","sentences":["We present AFEN (Audio Feature Ensemble Learning), a model that leverages Convolutional Neural Networks (CNN) and XGBoost in an ensemble learning fashion to perform state-of-the-art audio classification for a range of respiratory diseases.","We use a meticulously selected mix of audio features which provide the salient attributes of the data and allow for accurate classification.","The extracted features are then used as an input to two separate model classifiers 1) a multi-feature CNN classifier and 2) an XGBoost Classifier.","The outputs of the two models are then fused with the use of soft voting.","Thus, by exploiting ensemble learning, we achieve increased robustness and accuracy.","We evaluate the performance of the model on a database of 920 respiratory sounds, which undergoes data augmentation techniques to increase the diversity of the data and generalizability of the model.","We empirically verify that AFEN sets a new state-of-the-art using Precision and Recall as metrics, while decreasing training time by 60%."],"url":"http://arxiv.org/abs/2405.05467v1","category":"cs.SD"}
{"created":"2024-05-08 23:44:08","title":"Poser: Unmasking Alignment Faking LLMs by Manipulating Their Internals","abstract":"Like a criminal under investigation, Large Language Models (LLMs) might pretend to be aligned while evaluated and misbehave when they have a good opportunity. Can current interpretability methods catch these 'alignment fakers?' To answer this question, we introduce a benchmark that consists of 324 pairs of LLMs fine-tuned to select actions in role-play scenarios. One model in each pair is consistently benign (aligned). The other model misbehaves in scenarios where it is unlikely to be caught (alignment faking). The task is to identify the alignment faking model using only inputs where the two models behave identically. We test five detection strategies, one of which identifies 98% of alignment-fakers.","sentences":["Like a criminal under investigation, Large Language Models (LLMs) might pretend to be aligned while evaluated and misbehave when they have a good opportunity.","Can current interpretability methods catch these 'alignment fakers?'","To answer this question, we introduce a benchmark that consists of 324 pairs of LLMs fine-tuned to select actions in role-play scenarios.","One model in each pair is consistently benign (aligned).","The other model misbehaves in scenarios where it is unlikely to be caught (alignment faking).","The task is to identify the alignment faking model using only inputs where the two models behave identically.","We test five detection strategies, one of which identifies 98% of alignment-fakers."],"url":"http://arxiv.org/abs/2405.05466v1","category":"cs.CL"}
{"created":"2024-05-08 22:40:52","title":"GDGS: Gradient Domain Gaussian Splatting for Sparse Representation of Radiance Fields","abstract":"The 3D Gaussian splatting methods are getting popular. However, they work directly on the signal, leading to a dense representation of the signal. Even with some techniques such as pruning or distillation, the results are still dense. In this paper, we propose to model the gradient of the original signal. The gradients are much sparser than the original signal. Therefore, the gradients use much less Gaussian splats, leading to the more efficient storage and thus higher computational performance during both training and rendering. Thanks to the sparsity, during the view synthesis, only a small mount of pixels are needed, leading to much higher computational performance ($100\\sim 1000\\times$ faster). And the 2D image can be recovered from the gradients via solving a Poisson equation with linear computation complexity. Several experiments are performed to confirm the sparseness of the gradients and the computation performance of the proposed method. The method can be applied various applications, such as human body modeling and indoor environment modeling.","sentences":["The 3D Gaussian splatting methods are getting popular.","However, they work directly on the signal, leading to a dense representation of the signal.","Even with some techniques such as pruning or distillation, the results are still dense.","In this paper, we propose to model the gradient of the original signal.","The gradients are much sparser than the original signal.","Therefore, the gradients use much less Gaussian splats, leading to the more efficient storage and thus higher computational performance during both training and rendering.","Thanks to the sparsity, during the view synthesis, only a small mount of pixels are needed, leading to much higher computational performance ($100\\sim 1000\\times$ faster).","And the 2D image can be recovered from the gradients via solving a Poisson equation with linear computation complexity.","Several experiments are performed to confirm the sparseness of the gradients and the computation performance of the proposed method.","The method can be applied various applications, such as human body modeling and indoor environment modeling."],"url":"http://arxiv.org/abs/2405.05446v1","category":"cs.CV"}
{"created":"2024-05-08 22:23:58","title":"Evaluating Students' Open-ended Written Responses with LLMs: Using the RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large","abstract":"Evaluating open-ended written examination responses from students is an essential yet time-intensive task for educators, requiring a high degree of effort, consistency, and precision. Recent developments in Large Language Models (LLMs) present a promising opportunity to balance the need for thorough evaluation with efficient use of educators' time. In our study, we explore the effectiveness of LLMs ChatGPT-3.5, ChatGPT-4, Claude-3, and Mistral-Large in assessing university students' open-ended answers to questions made about reference material they have studied. Each model was instructed to evaluate 54 answers repeatedly under two conditions: 10 times (10-shot) with a temperature setting of 0.0 and 10 times with a temperature of 0.5, expecting a total of 1,080 evaluations per model and 4,320 evaluations across all models. The RAG (Retrieval Augmented Generation) framework was used as the framework to make the LLMs to process the evaluation of the answers. As of spring 2024, our analysis revealed notable variations in consistency and the grading outcomes provided by studied LLMs. There is a need to comprehend strengths and weaknesses of LLMs in educational settings for evaluating open-ended written responses. Further comparative research is essential to determine the accuracy and cost-effectiveness of using LLMs for educational assessments.","sentences":["Evaluating open-ended written examination responses from students is an essential yet time-intensive task for educators, requiring a high degree of effort, consistency, and precision.","Recent developments in Large Language Models (LLMs) present a promising opportunity to balance the need for thorough evaluation with efficient use of educators' time.","In our study, we explore the effectiveness of LLMs ChatGPT-3.5, ChatGPT-4, Claude-3, and Mistral-Large in assessing university students' open-ended answers to questions made about reference material they have studied.","Each model was instructed to evaluate 54 answers repeatedly under two conditions: 10 times (10-shot) with a temperature setting of 0.0 and 10 times with a temperature of 0.5, expecting a total of 1,080 evaluations per model and 4,320 evaluations across all models.","The RAG (Retrieval Augmented Generation) framework was used as the framework to make the LLMs to process the evaluation of the answers.","As of spring 2024, our analysis revealed notable variations in consistency and the grading outcomes provided by studied LLMs.","There is a need to comprehend strengths and weaknesses of LLMs in educational settings for evaluating open-ended written responses.","Further comparative research is essential to determine the accuracy and cost-effectiveness of using LLMs for educational assessments."],"url":"http://arxiv.org/abs/2405.05444v1","category":"cs.CL"}
{"created":"2024-05-08 22:00:35","title":"How Generalizable Is My Behavior Cloning Policy? A Statistical Approach to Trustworthy Performance Evaluation","abstract":"With the rise of stochastic generative models in robot policy learning, end-to-end visuomotor policies are increasingly successful at solving complex tasks by learning from human demonstrations. Nevertheless, since real-world evaluation costs afford users only a small number of policy rollouts, it remains a challenge to accurately gauge the performance of such policies. This is exacerbated by distribution shifts causing unpredictable changes in performance during deployment. To rigorously evaluate behavior cloning policies, we present a framework that provides a tight lower-bound on robot performance in an arbitrary environment, using a minimal number of experimental policy rollouts. Notably, by applying the standard stochastic ordering to robot performance distributions, we provide a worst-case bound on the entire distribution of performance (via bounds on the cumulative distribution function) for a given task. We build upon established statistical results to ensure that the bounds hold with a user-specified confidence level and tightness, and are constructed from as few policy rollouts as possible. In experiments we evaluate policies for visuomotor manipulation in both simulation and hardware. Specifically, we (i) empirically validate the guarantees of the bounds in simulated manipulation settings, (ii) find the degree to which a learned policy deployed on hardware generalizes to new real-world environments, and (iii) rigorously compare two policies tested in out-of-distribution settings. Our experimental data, code, and implementation of confidence bounds are open-source.","sentences":["With the rise of stochastic generative models in robot policy learning, end-to-end visuomotor policies are increasingly successful at solving complex tasks by learning from human demonstrations.","Nevertheless, since real-world evaluation costs afford users only a small number of policy rollouts, it remains a challenge to accurately gauge the performance of such policies.","This is exacerbated by distribution shifts causing unpredictable changes in performance during deployment.","To rigorously evaluate behavior cloning policies, we present a framework that provides a tight lower-bound on robot performance in an arbitrary environment, using a minimal number of experimental policy rollouts.","Notably, by applying the standard stochastic ordering to robot performance distributions, we provide a worst-case bound on the entire distribution of performance (via bounds on the cumulative distribution function) for a given task.","We build upon established statistical results to ensure that the bounds hold with a user-specified confidence level and tightness, and are constructed from as few policy rollouts as possible.","In experiments we evaluate policies for visuomotor manipulation in both simulation and hardware.","Specifically, we (i) empirically validate the guarantees of the bounds in simulated manipulation settings, (ii) find the degree to which a learned policy deployed on hardware generalizes to new real-world environments, and (iii) rigorously compare two policies tested in out-of-distribution settings.","Our experimental data, code, and implementation of confidence bounds are open-source."],"url":"http://arxiv.org/abs/2405.05439v1","category":"cs.RO"}
{"created":"2024-05-08 21:40:49","title":"Analysis and prevention of AI-based phishing email attacks","abstract":"Phishing email attacks are among the most common and most harmful cybersecurity attacks. With the emergence of generative AI, phishing attacks can be based on emails generated automatically, making it more difficult to detect them. That is, instead of a single email format sent to a large number of recipients, generative AI can be used to send each potential victim a different email, making it more difficult for cybersecurity systems to identify the scam email before it reaches the recipient. Here we describe a corpus of AI-generated phishing emails. We also use different machine learning tools to test the ability of automatic text analysis to identify AI-generated phishing emails. The results are encouraging, and show that machine learning tools can identify an AI-generated phishing email with high accuracy compared to regular emails or human-generated scam email. By applying descriptive analytic, the specific differences between AI-generated emails and manually crafted scam emails are profiled, and show that AI-generated emails are different in their style from human-generated phishing email scams. Therefore, automatic identification tools can be used as a warning for the user. The paper also describes the corpus of AI-generated phishing emails that is made open to the public, and can be used for consequent studies. While the ability of machine learning to detect AI-generated phishing email is encouraging, AI-generated phishing emails are different from regular phishing emails, and therefore it is important to train machine learning systems also with AI-generated emails in order to repel future phishing attacks that are powered by generative AI.","sentences":["Phishing email attacks are among the most common and most harmful cybersecurity attacks.","With the emergence of generative AI, phishing attacks can be based on emails generated automatically, making it more difficult to detect them.","That is, instead of a single email format sent to a large number of recipients, generative AI can be used to send each potential victim a different email, making it more difficult for cybersecurity systems to identify the scam email before it reaches the recipient.","Here we describe a corpus of AI-generated phishing emails.","We also use different machine learning tools to test the ability of automatic text analysis to identify AI-generated phishing emails.","The results are encouraging, and show that machine learning tools can identify an AI-generated phishing email with high accuracy compared to regular emails or human-generated scam email.","By applying descriptive analytic, the specific differences between AI-generated emails and manually crafted scam emails are profiled, and show that AI-generated emails are different in their style from human-generated phishing email scams.","Therefore, automatic identification tools can be used as a warning for the user.","The paper also describes the corpus of AI-generated phishing emails that is made open to the public, and can be used for consequent studies.","While the ability of machine learning to detect AI-generated phishing email is encouraging, AI-generated phishing emails are different from regular phishing emails, and therefore it is important to train machine learning systems also with AI-generated emails in order to repel future phishing attacks that are powered by generative AI."],"url":"http://arxiv.org/abs/2405.05435v1","category":"cs.CR"}
{"created":"2024-05-08 21:19:18","title":"How Inverse Conditional Flows Can Serve as a Substitute for Distributional Regression","abstract":"Neural network representations of simple models, such as linear regression, are being studied increasingly to better understand the underlying principles of deep learning algorithms. However, neural representations of distributional regression models, such as the Cox model, have received little attention so far. We close this gap by proposing a framework for distributional regression using inverse flow transformations (DRIFT), which includes neural representations of the aforementioned models. We empirically demonstrate that the neural representations of models in DRIFT can serve as a substitute for their classical statistical counterparts in several applications involving continuous, ordered, time-series, and survival outcomes. We confirm that models in DRIFT empirically match the performance of several statistical methods in terms of estimation of partial effects, prediction, and aleatoric uncertainty quantification. DRIFT covers both interpretable statistical models and flexible neural networks opening up new avenues in both statistical modeling and deep learning.","sentences":["Neural network representations of simple models, such as linear regression, are being studied increasingly to better understand the underlying principles of deep learning algorithms.","However, neural representations of distributional regression models, such as the Cox model, have received little attention so far.","We close this gap by proposing a framework for distributional regression using inverse flow transformations (DRIFT), which includes neural representations of the aforementioned models.","We empirically demonstrate that the neural representations of models in DRIFT can serve as a substitute for their classical statistical counterparts in several applications involving continuous, ordered, time-series, and survival outcomes.","We confirm that models in DRIFT empirically match the performance of several statistical methods in terms of estimation of partial effects, prediction, and aleatoric uncertainty quantification.","DRIFT covers both interpretable statistical models and flexible neural networks opening up new avenues in both statistical modeling and deep learning."],"url":"http://arxiv.org/abs/2405.05429v1","category":"cs.LG"}
{"created":"2024-05-08 21:13:06","title":"Chiral Magnetic Effect in Heavy Ion Collisions: The Present and Future","abstract":"The chiral magnetic effect (CME) is a collective quantum phenomenon that arises from the interplay between gauge field topology and fermion chiral anomaly, encompassing a wide range of physical systems from semimetals to quark-gluon plasma. This review, with a focus on CME and related effects in heavy ion collisions, aims to provide an introductory discussion on its conceptual foundation and measurement methodology, a timely update on the present status in terms of experimental findings and theoretical progress, as well as an outlook into the open problems and future developments.","sentences":["The chiral magnetic effect (CME) is a collective quantum phenomenon that arises from the interplay between gauge field topology and fermion chiral anomaly, encompassing a wide range of physical systems from semimetals to quark-gluon plasma.","This review, with a focus on CME and related effects in heavy ion collisions, aims to provide an introductory discussion on its conceptual foundation and measurement methodology, a timely update on the present status in terms of experimental findings and theoretical progress, as well as an outlook into the open problems and future developments."],"url":"http://arxiv.org/abs/2405.05427v1","category":"nucl-th"}
{"created":"2024-05-08 21:05:59","title":"ATLS: Automated Trailer Loading for Surface Vessels","abstract":"Automated docking technologies of marine boats have been enlightened by an increasing number of literature. This paper contributes to the literature by proposing a mathematical framework that automates \"trailer loading\" in the presence of wind disturbances, which is unexplored despite its importance to boat owners. The comprehensive pipeline of localization, system identification, and trajectory optimization is structured, followed by several techniques to improve performance reliability. The performance of the proposed method was demonstrated with a commercial pontoon boat in Michigan, in 2023, securing a success rate of 80\\% in the presence of perception errors and wind disturbance. This result indicates the strong potential of the proposed pipeline, effectively accommodating the wind effect.","sentences":["Automated docking technologies of marine boats have been enlightened by an increasing number of literature.","This paper contributes to the literature by proposing a mathematical framework that automates \"trailer loading\" in the presence of wind disturbances, which is unexplored despite its importance to boat owners.","The comprehensive pipeline of localization, system identification, and trajectory optimization is structured, followed by several techniques to improve performance reliability.","The performance of the proposed method was demonstrated with a commercial pontoon boat in Michigan, in 2023, securing a success rate of 80\\% in the presence of perception errors and wind disturbance.","This result indicates the strong potential of the proposed pipeline, effectively accommodating the wind effect."],"url":"http://arxiv.org/abs/2405.05426v1","category":"eess.SY"}
{"created":"2024-05-08 20:46:36","title":"EarthMatch: Iterative Coregistration for Fine-grained Localization of Astronaut Photography","abstract":"Precise, pixel-wise geolocalization of astronaut photography is critical to unlocking the potential of this unique type of remotely sensed Earth data, particularly for its use in disaster management and climate change research. Recent works have established the Astronaut Photography Localization task, but have either proved too costly for mass deployment or generated too coarse a localization. Thus, we present EarthMatch, an iterative homography estimation method that produces fine-grained localization of astronaut photographs while maintaining an emphasis on speed. We refocus the astronaut photography benchmark, AIMS, on the geolocalization task itself, and prove our method's efficacy on this dataset. In addition, we offer a new, fair method for image matcher comparison, and an extensive evaluation of different matching models within our localization pipeline. Our method will enable fast and accurate localization of the 4.5 million and growing collection of astronaut photography of Earth. Webpage with code and data at https://earthloc-and-earthmatch.github.io","sentences":["Precise, pixel-wise geolocalization of astronaut photography is critical to unlocking the potential of this unique type of remotely sensed Earth data, particularly for its use in disaster management and climate change research.","Recent works have established the Astronaut Photography Localization task, but have either proved too costly for mass deployment or generated too coarse a localization.","Thus, we present EarthMatch, an iterative homography estimation method that produces fine-grained localization of astronaut photographs while maintaining an emphasis on speed.","We refocus the astronaut photography benchmark, AIMS, on the geolocalization task itself, and prove our method's efficacy on this dataset.","In addition, we offer a new, fair method for image matcher comparison, and an extensive evaluation of different matching models within our localization pipeline.","Our method will enable fast and accurate localization of the 4.5 million and growing collection of astronaut photography of Earth.","Webpage with code and data at https://earthloc-and-earthmatch.github.io"],"url":"http://arxiv.org/abs/2405.05422v1","category":"cs.CV"}
{"created":"2024-05-08 20:33:32","title":"Digital Evolution: Novo Nordisk's Shift to Ontology-Based Data Management","abstract":"Biomedical data is growing exponentially, and managing it is increasingly challenging. While Findable, Accessible, Interoperable and Reusable (FAIR) data principles provide guidance, their adoption has proven difficult, especially in larger enterprises like pharmaceutical companies. In this manuscript, we describe how we leverage an Ontology-Based Data Management (OBDM) strategy for digital transformation in Novo Nordisk Research & Early Development. Here, we include both our technical blueprint and our approach for organizational change management. We further discuss how such an OBDM ecosystem plays a pivotal role in the organizations digital aspirations for data federation and discovery fuelled by artificial intelligence. Our aim for this paper is to share the lessons learned in order to foster dialogue with parties navigating similar waters while collectively advancing the efforts in the fields of data management, semantics and data driven drug discovery.","sentences":["Biomedical data is growing exponentially, and managing it is increasingly challenging.","While Findable, Accessible, Interoperable and Reusable (FAIR) data principles provide guidance, their adoption has proven difficult, especially in larger enterprises like pharmaceutical companies.","In this manuscript, we describe how we leverage an Ontology-Based Data Management (OBDM) strategy for digital transformation in Novo Nordisk Research & Early Development.","Here, we include both our technical blueprint and our approach for organizational change management.","We further discuss how such an OBDM ecosystem plays a pivotal role in the organizations digital aspirations for data federation and discovery fuelled by artificial intelligence.","Our aim for this paper is to share the lessons learned in order to foster dialogue with parties navigating similar waters while collectively advancing the efforts in the fields of data management, semantics and data driven drug discovery."],"url":"http://arxiv.org/abs/2405.05413v1","category":"cs.DB"}
{"created":"2024-05-08 19:39:12","title":"On foundation of generative statistics with F-entropy: a gradient-based approach","abstract":"This paper explores the interplay between statistics and generative artificial intelligence. Generative statistics, an integral part of the latter, aims to construct models that can {\\it generate} efficiently and meaningfully new data across the whole of the (usually high dimensional) sample space, e.g. a new photo. Within it, the gradient-based approach is a current favourite that exploits effectively, for the above purpose, the information contained in the observed sample, e.g. an old photo. However, often there are missing data in the observed sample, e.g. missing bits in the old photo. To handle this situation, we have proposed a gradient-based algorithm for generative modelling. More importantly, our paper underpins rigorously this powerful approach by introducing a new F-entropy that is related to Fisher's divergence. (The F-entropy is also of independent interest.) The underpinning has enabled the gradient-based approach to expand its scope. For example, it can now provide a tool for Possible future projects include discrete data and Bayesian variational inference.","sentences":["This paper explores the interplay between statistics and generative artificial intelligence.","Generative statistics, an integral part of the latter, aims to construct models that can {\\it generate} efficiently and meaningfully new data across the whole of the (usually high dimensional) sample space, e.g. a new photo.","Within it, the gradient-based approach is a current favourite that exploits effectively, for the above purpose, the information contained in the observed sample, e.g. an old photo.","However, often there are missing data in the observed sample, e.g. missing bits in the old photo.","To handle this situation, we have proposed a gradient-based algorithm for generative modelling.","More importantly, our paper underpins rigorously this powerful approach by introducing a new F-entropy that is related to Fisher's divergence.","(The F-entropy is also of independent interest.)","The underpinning has enabled the gradient-based approach to expand its scope.","For example, it can now provide a tool for Possible future projects include discrete data and Bayesian variational inference."],"url":"http://arxiv.org/abs/2405.05389v1","category":"stat.ME"}
{"created":"2024-05-08 19:31:06","title":"Interpretability Needs a New Paradigm","abstract":"Interpretability is the study of explaining models in understandable terms to humans. At present, interpretability is divided into two paradigms: the intrinsic paradigm, which believes that only models designed to be explained can be explained, and the post-hoc paradigm, which believes that black-box models can be explained. At the core of this debate is how each paradigm ensures its explanations are faithful, i.e., true to the model's behavior. This is important, as false but convincing explanations lead to unsupported confidence in artificial intelligence (AI), which can be dangerous. This paper's position is that we should think about new paradigms while staying vigilant regarding faithfulness. First, by examining the history of paradigms in science, we see that paradigms are constantly evolving. Then, by examining the current paradigms, we can understand their underlying beliefs, the value they bring, and their limitations. Finally, this paper presents 3 emerging paradigms for interpretability. The first paradigm designs models such that faithfulness can be easily measured. Another optimizes models such that explanations become faithful. The last paradigm proposes to develop models that produce both a prediction and an explanation.","sentences":["Interpretability is the study of explaining models in understandable terms to humans.","At present, interpretability is divided into two paradigms: the intrinsic paradigm, which believes that only models designed to be explained can be explained, and the post-hoc paradigm, which believes that black-box models can be explained.","At the core of this debate is how each paradigm ensures its explanations are faithful, i.e., true to the model's behavior.","This is important, as false but convincing explanations lead to unsupported confidence in artificial intelligence (AI), which can be dangerous.","This paper's position is that we should think about new paradigms while staying vigilant regarding faithfulness.","First, by examining the history of paradigms in science, we see that paradigms are constantly evolving.","Then, by examining the current paradigms, we can understand their underlying beliefs, the value they bring, and their limitations.","Finally, this paper presents 3 emerging paradigms for interpretability.","The first paradigm designs models such that faithfulness can be easily measured.","Another optimizes models such that explanations become faithful.","The last paradigm proposes to develop models that produce both a prediction and an explanation."],"url":"http://arxiv.org/abs/2405.05386v1","category":"cs.LG"}
{"created":"2024-05-08 19:08:45","title":"\"They are uncultured\": Unveiling Covert Harms and Social Threats in LLM Generated Conversations","abstract":"Large language models (LLMs) have emerged as an integral part of modern societies, powering user-facing applications such as personal assistants and enterprise applications like recruitment tools. Despite their utility, research indicates that LLMs perpetuate systemic biases. Yet, prior works on LLM harms predominantly focus on Western concepts like race and gender, often overlooking cultural concepts from other parts of the world. Additionally, these studies typically investigate \"harm\" as a singular dimension, ignoring the various and subtle forms in which harms manifest. To address this gap, we introduce the Covert Harms and Social Threats (CHAST), a set of seven metrics grounded in social science literature. We utilize evaluation models aligned with human assessments to examine the presence of covert harms in LLM-generated conversations, particularly in the context of recruitment. Our experiments reveal that seven out of the eight LLMs included in this study generated conversations riddled with CHAST, characterized by malign views expressed in seemingly neutral language unlikely to be detected by existing methods. Notably, these LLMs manifested more extreme views and opinions when dealing with non-Western concepts like caste, compared to Western ones such as race.","sentences":["Large language models (LLMs) have emerged as an integral part of modern societies, powering user-facing applications such as personal assistants and enterprise applications like recruitment tools.","Despite their utility, research indicates that LLMs perpetuate systemic biases.","Yet, prior works on LLM harms predominantly focus on Western concepts like race and gender, often overlooking cultural concepts from other parts of the world.","Additionally, these studies typically investigate \"harm\" as a singular dimension, ignoring the various and subtle forms in which harms manifest.","To address this gap, we introduce the Covert Harms and Social Threats (CHAST), a set of seven metrics grounded in social science literature.","We utilize evaluation models aligned with human assessments to examine the presence of covert harms in LLM-generated conversations, particularly in the context of recruitment.","Our experiments reveal that seven out of the eight LLMs included in this study generated conversations riddled with CHAST, characterized by malign views expressed in seemingly neutral language unlikely to be detected by existing methods.","Notably, these LLMs manifested more extreme views and opinions when dealing with non-Western concepts like caste, compared to Western ones such as race."],"url":"http://arxiv.org/abs/2405.05378v1","category":"cs.CL"}
{"created":"2024-05-08 19:05:18","title":"Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models","abstract":"This report describes the training dataset creation and recipe behind the family of \\texttt{arctic-embed} text embedding models (a set of five models ranging from 22 to 334 million parameters with weights open-sourced under an Apache-2 license). At the time of their release, each model achieved state-of-the-art retrieval accuracy for models of their size on the MTEB Retrieval leaderboard, with the largest model, arctic-embed-l outperforming closed source embedding models such as Cohere's embed-v3 and Open AI's text-embed-3-large. In addition to the details of our training recipe, we have provided several informative ablation studies, which we believe are the cause of our model performance.","sentences":["This report describes the training dataset creation and recipe behind the family of \\texttt{arctic-embed} text embedding models (a set of five models ranging from 22 to 334 million parameters with weights open-sourced under an Apache-2 license).","At the time of their release, each model achieved state-of-the-art retrieval accuracy for models of their size on the MTEB Retrieval leaderboard, with the largest model, arctic-embed-l outperforming closed source embedding models such as Cohere's embed-v3 and Open AI's text-embed-3-large.","In addition to the details of our training recipe, we have provided several informative ablation studies, which we believe are the cause of our model performance."],"url":"http://arxiv.org/abs/2405.05374v1","category":"cs.CL"}
{"created":"2024-05-08 18:47:52","title":"Enhancing Holonic Architecture with Natural Language Processing for System of Systems","abstract":"The complexity and dynamic nature of System of Systems (SoS) necessitate efficient communication mechanisms to ensure interoperability and collaborative functioning among constituent systems, termed holons. This paper proposes an innovative approach to enhance holon communication within SoS through the integration of Conversational Generative Intelligence (CGI) techniques. Our approach leverages advancements in CGI, specifically Large Language Models (LLMs), to enable holons to understand and act on natural language instructions. This fosters more intuitive human-holon interactions, improving social intelligence and ultimately leading to better coordination among diverse systems. This position paper outlines a conceptual framework for CGI-enhanced holon interaction, discusses the potential impact on SoS adaptability, usability and efficiency, and sets the stage for future exploration and prototype implementation.","sentences":["The complexity and dynamic nature of System of Systems (SoS) necessitate efficient communication mechanisms to ensure interoperability and collaborative functioning among constituent systems, termed holons.","This paper proposes an innovative approach to enhance holon communication within SoS through the integration of Conversational Generative Intelligence (CGI) techniques.","Our approach leverages advancements in CGI, specifically Large Language Models (LLMs), to enable holons to understand and act on natural language instructions.","This fosters more intuitive human-holon interactions, improving social intelligence and ultimately leading to better coordination among diverse systems.","This position paper outlines a conceptual framework for CGI-enhanced holon interaction, discusses the potential impact on SoS adaptability, usability and efficiency, and sets the stage for future exploration and prototype implementation."],"url":"http://arxiv.org/abs/2405.05365v1","category":"eess.SY"}
{"created":"2024-05-08 18:27:37","title":"Offline Model-Based Optimization via Policy-Guided Gradient Search","abstract":"Offline optimization is an emerging problem in many experimental engineering domains including protein, drug or aircraft design, where online experimentation to collect evaluation data is too expensive or dangerous. To avoid that, one has to optimize an unknown function given only its offline evaluation at a fixed set of inputs. A naive solution to this problem is to learn a surrogate model of the unknown function and optimize this surrogate instead. However, such a naive optimizer is prone to erroneous overestimation of the surrogate (possibly due to over-fitting on a biased sample of function evaluation) on inputs outside the offline dataset. Prior approaches addressing this challenge have primarily focused on learning robust surrogate models. However, their search strategies are derived from the surrogate model rather than the actual offline data. To fill this important gap, we introduce a new learning-to-search perspective for offline optimization by reformulating it as an offline reinforcement learning problem. Our proposed policy-guided gradient search approach explicitly learns the best policy for a given surrogate model created from the offline data. Our empirical results on multiple benchmarks demonstrate that the learned optimization policy can be combined with existing offline surrogates to significantly improve the optimization performance.","sentences":["Offline optimization is an emerging problem in many experimental engineering domains including protein, drug or aircraft design, where online experimentation to collect evaluation data is too expensive or dangerous.","To avoid that, one has to optimize an unknown function given only its offline evaluation at a fixed set of inputs.","A naive solution to this problem is to learn a surrogate model of the unknown function and optimize this surrogate instead.","However, such a naive optimizer is prone to erroneous overestimation of the surrogate (possibly due to over-fitting on a biased sample of function evaluation) on inputs outside the offline dataset.","Prior approaches addressing this challenge have primarily focused on learning robust surrogate models.","However, their search strategies are derived from the surrogate model rather than the actual offline data.","To fill this important gap, we introduce a new learning-to-search perspective for offline optimization by reformulating it as an offline reinforcement learning problem.","Our proposed policy-guided gradient search approach explicitly learns the best policy for a given surrogate model created from the offline data.","Our empirical results on multiple benchmarks demonstrate that the learned optimization policy can be combined with existing offline surrogates to significantly improve the optimization performance."],"url":"http://arxiv.org/abs/2405.05349v1","category":"cs.LG"}
{"created":"2024-05-08 18:27:20","title":"The Effect of Model Size on LLM Post-hoc Explainability via LIME","abstract":"Large language models (LLMs) are becoming bigger to boost performance. However, little is known about how explainability is affected by this trend. This work explores LIME explanations for DeBERTaV3 models of four different sizes on natural language inference (NLI) and zero-shot classification (ZSC) tasks. We evaluate the explanations based on their faithfulness to the models' internal decision processes and their plausibility, i.e. their agreement with human explanations. The key finding is that increased model size does not correlate with plausibility despite improved model performance, suggesting a misalignment between the LIME explanations and the models' internal processes as model size increases. Our results further suggest limitations regarding faithfulness metrics in NLI contexts.","sentences":["Large language models (LLMs) are becoming bigger to boost performance.","However, little is known about how explainability is affected by this trend.","This work explores LIME explanations for DeBERTaV3 models of four different sizes on natural language inference (NLI) and zero-shot classification (ZSC) tasks.","We evaluate the explanations based on their faithfulness to the models' internal decision processes and their plausibility, i.e. their agreement with human explanations.","The key finding is that increased model size does not correlate with plausibility despite improved model performance, suggesting a misalignment between the LIME explanations and the models' internal processes as model size increases.","Our results further suggest limitations regarding faithfulness metrics in NLI contexts."],"url":"http://arxiv.org/abs/2405.05348v1","category":"cs.CL"}
{"created":"2024-05-08 18:23:59","title":"Benchmarking Educational Program Repair","abstract":"The emergence of large language models (LLMs) has sparked enormous interest due to their potential application across a range of educational tasks. For example, recent work in programming education has used LLMs to generate learning resources, improve error messages, and provide feedback on code. However, one factor that limits progress within the field is that much of the research uses bespoke datasets and different evaluation metrics, making direct comparisons between results unreliable. Thus, there is a pressing need for standardization and benchmarks that facilitate the equitable comparison of competing approaches. One task where LLMs show great promise is program repair, which can be used to provide debugging support and next-step hints to students. In this article, we propose a novel educational program repair benchmark. We curate two high-quality publicly available programming datasets, present a unified evaluation procedure introducing a novel evaluation metric rouge@k for approximating the quality of repairs, and evaluate a set of five recent models to establish baseline performance.","sentences":["The emergence of large language models (LLMs) has sparked enormous interest due to their potential application across a range of educational tasks.","For example, recent work in programming education has used LLMs to generate learning resources, improve error messages, and provide feedback on code.","However, one factor that limits progress within the field is that much of the research uses bespoke datasets and different evaluation metrics, making direct comparisons between results unreliable.","Thus, there is a pressing need for standardization and benchmarks that facilitate the equitable comparison of competing approaches.","One task where LLMs show great promise is program repair, which can be used to provide debugging support and next-step hints to students.","In this article, we propose a novel educational program repair benchmark.","We curate two high-quality publicly available programming datasets, present a unified evaluation procedure introducing a novel evaluation metric rouge@k for approximating the quality of repairs, and evaluate a set of five recent models to establish baseline performance."],"url":"http://arxiv.org/abs/2405.05347v1","category":"cs.SE"}
{"created":"2024-05-08 18:10:59","title":"Joint semi-supervised and contrastive learning enables zero-shot domain-adaptation and multi-domain segmentation","abstract":"Despite their effectiveness, current deep learning models face challenges with images coming from different domains with varying appearance and content. We introduce SegCLR, a versatile framework designed to segment volumetric images across different domains, employing supervised and contrastive learning simultaneously to effectively learn from both labeled and unlabeled data. We demonstrate the superior performance of SegCLR through a comprehensive evaluation involving three diverse clinical datasets of retinal fluid segmentation in 3D Optical Coherence Tomography (OCT), various network configurations, and verification across 10 different network initializations. In an unsupervised domain adaptation context, SegCLR achieves results on par with a supervised upper-bound model trained on the intended target domain. Notably, we discover that the segmentation performance of SegCLR framework is marginally impacted by the abundance of unlabeled data from the target domain, thereby we also propose an effective zero-shot domain adaptation extension of SegCLR, eliminating the need for any target domain information. This shows that our proposed addition of contrastive loss in standard supervised training for segmentation leads to superior models, inherently more generalizable to both in- and out-of-domain test data. We additionally propose a pragmatic solution for SegCLR deployment in realistic scenarios with multiple domains containing labeled data. Accordingly, our framework pushes the boundaries of deep-learning based segmentation in multi-domain applications, regardless of data availability - labeled, unlabeled, or nonexistent.","sentences":["Despite their effectiveness, current deep learning models face challenges with images coming from different domains with varying appearance and content.","We introduce SegCLR, a versatile framework designed to segment volumetric images across different domains, employing supervised and contrastive learning simultaneously to effectively learn from both labeled and unlabeled data.","We demonstrate the superior performance of SegCLR through a comprehensive evaluation involving three diverse clinical datasets of retinal fluid segmentation in 3D Optical Coherence Tomography (OCT), various network configurations, and verification across 10 different network initializations.","In an unsupervised domain adaptation context, SegCLR achieves results on par with a supervised upper-bound model trained on the intended target domain.","Notably, we discover that the segmentation performance of SegCLR framework is marginally impacted by the abundance of unlabeled data from the target domain, thereby we also propose an effective zero-shot domain adaptation extension of SegCLR, eliminating the need for any target domain information.","This shows that our proposed addition of contrastive loss in standard supervised training for segmentation leads to superior models, inherently more generalizable to both in- and out-of-domain test data.","We additionally propose a pragmatic solution for SegCLR deployment in realistic scenarios with multiple domains containing labeled data.","Accordingly, our framework pushes the boundaries of deep-learning based segmentation in multi-domain applications, regardless of data availability - labeled, unlabeled, or nonexistent."],"url":"http://arxiv.org/abs/2405.05336v1","category":"eess.IV"}
{"created":"2024-05-08 18:03:22","title":"KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation","abstract":"Large Language Model or LLM inference has two phases, the prompt (or prefill) phase to output the first token and the extension (or decoding) phase to the generate subsequent tokens. In this work, we propose an efficient parallelization scheme, KV-Runahead to accelerate the prompt phase. The key observation is that the extension phase generates tokens faster than the prompt phase because of key-value cache (KV-cache). Hence, KV-Runahead parallelizes the prompt phase by orchestrating multiple processes to populate the KV-cache and minimizes the time-to-first-token (TTFT). Dual-purposing the KV-cache scheme has two main benefits. Fist, since KV-cache is designed to leverage the causal attention map, we minimize computation and computation automatically. Second, since it already exists for the exten- sion phase, KV-Runahead is easy to implement. We further propose context-level load-balancing to handle uneven KV-cache generation (due to the causal attention) and to optimize TTFT. Compared with an existing parallelization scheme such as tensor or sequential parallelization where keys and values are locally generated and exchanged via all-gather collectives, our experimental results demonstrate that KV-Runahead can offer over 1.4x and 1.6x speedups for Llama 7B and Falcon 7B respectively.","sentences":["Large Language Model or LLM inference has two phases, the prompt (or prefill) phase to output the first token and the extension (or decoding) phase to the generate subsequent tokens.","In this work, we propose an efficient parallelization scheme, KV-Runahead to accelerate the prompt phase.","The key observation is that the extension phase generates tokens faster than the prompt phase because of key-value cache (KV-cache).","Hence, KV-Runahead parallelizes the prompt phase by orchestrating multiple processes to populate the KV-cache and minimizes the time-to-first-token (TTFT).","Dual-purposing the KV-cache scheme has two main benefits.","Fist, since KV-cache is designed to leverage the causal attention map, we minimize computation and computation automatically.","Second, since it already exists for the exten- sion phase, KV-Runahead is easy to implement.","We further propose context-level load-balancing to handle uneven KV-cache generation (due to the causal attention) and to optimize TTFT.","Compared with an existing parallelization scheme such as tensor or sequential parallelization where keys and values are locally generated and exchanged via all-gather collectives, our experimental results demonstrate that KV-Runahead can offer over 1.4x and 1.6x speedups for Llama 7B and Falcon 7B respectively."],"url":"http://arxiv.org/abs/2405.05329v1","category":"cs.DC"}
{"created":"2024-05-08 18:00:00","title":"The Hot Jupiter Radius Anomaly and Stellar Connections","abstract":"The extremely close proximity of hot Jupiters to their parent stars has dramatically affected both their atmospheres and interiors, inflating them to up to twice the radius of Jupiter. The physical mechanism responsible for this inflation remains unknown, though many proposals have been put forward. I will review the known hot Jupiter population, the proposed inflation mechanisms, and the evidence for and against them collected thus far. In doing so, I will cover the ways that hot Jupiter interiors may be simulated computationally in detail, and present some useful formulas for estimating their radii, heating, intrinsic temperature, and tentative magnetic field strength. I will also cover the related issues of hot Jupiter intrinsic temperatures and radiative-convective boundaries, the potential connection with planetary magnetic fields, and the effects of stellar tides on the planet. Finally, I conclude with the suggestion that more than one mechanism may be operating in concert with each other and propose various avenues for future progress in understanding these objects.","sentences":["The extremely close proximity of hot Jupiters to their parent stars has dramatically affected both their atmospheres and interiors, inflating them to up to twice the radius of Jupiter.","The physical mechanism responsible for this inflation remains unknown, though many proposals have been put forward.","I will review the known hot Jupiter population, the proposed inflation mechanisms, and the evidence for and against them collected thus far.","In doing so, I will cover the ways that hot Jupiter interiors may be simulated computationally in detail, and present some useful formulas for estimating their radii, heating, intrinsic temperature, and tentative magnetic field strength.","I will also cover the related issues of hot Jupiter intrinsic temperatures and radiative-convective boundaries, the potential connection with planetary magnetic fields, and the effects of stellar tides on the planet.","Finally, I conclude with the suggestion that more than one mechanism may be operating in concert with each other and propose various avenues for future progress in understanding these objects."],"url":"http://arxiv.org/abs/2405.05307v1","category":"astro-ph.EP"}
{"created":"2024-05-08 17:17:58","title":"A design specification for Critical Illness Digital Twins to cure sepsis: responding to the National Academies of Sciences, Engineering and Medicine Report: Foundational Research Gaps and Future Directions for Digital Twins","abstract":"On December 15, 2023, The National Academies of Sciences, Engineering and Medicine (NASEM) released a report entitled: Foundational Research Gaps and Future Directions for Digital Twins. The ostensible purpose of this report was to bring some structure to the burgeoning field of digital twins by providing a working definition and a series of research challenges that need to be addressed to allow this technology to fulfill its full potential. In the work presented herein we focus on five specific findings from the NASEM Report: 1) definition of a Digital Twin, 2) using fit-for-purpose guidance, 3) developing novel approaches to Verification, Validation and Uncertainty Quantification (VVUQ) of Digital Twins, 4) incorporating control as an explicit purpose for a Digital Twin and 5) using a Digital Twin to guide data collection and sensor development, and describe how these findings are addressed through the design specifications for a Critical Illness Digital Twin (CIDT) aimed at curing sepsis.","sentences":["On December 15, 2023, The National Academies of Sciences, Engineering and Medicine (NASEM) released a report entitled: Foundational Research Gaps and Future Directions for Digital Twins.","The ostensible purpose of this report was to bring some structure to the burgeoning field of digital twins by providing a working definition and a series of research challenges that need to be addressed to allow this technology to fulfill its full potential.","In the work presented herein we focus on five specific findings from the NASEM Report: 1) definition of a Digital Twin, 2) using fit-for-purpose guidance, 3) developing novel approaches to Verification, Validation and Uncertainty Quantification (VVUQ) of Digital Twins, 4) incorporating control as an explicit purpose for a Digital Twin and 5) using a Digital Twin to guide data collection and sensor development, and describe how these findings are addressed through the design specifications for a Critical Illness Digital Twin (CIDT) aimed at curing sepsis."],"url":"http://arxiv.org/abs/2405.05301v1","category":"q-bio.OT"}
{"created":"2024-05-08 14:16:22","title":"Challenges for Responsible AI Design and Workflow Integration in Healthcare: A Case Study of Automatic Feeding Tube Qualification in Radiology","abstract":"Nasogastric tubes (NGTs) are feeding tubes that are inserted through the nose into the stomach to deliver nutrition or medication. If not placed correctly, they can cause serious harm, even death to patients. Recent AI developments demonstrate the feasibility of robustly detecting NGT placement from Chest X-ray images to reduce risks of sub-optimally or critically placed NGTs being missed or delayed in their detection, but gaps remain in clinical practice integration. In this study, we present a human-centered approach to the problem and describe insights derived following contextual inquiry and in-depth interviews with 15 clinical stakeholders. The interviews helped understand challenges in existing workflows, and how best to align technical capabilities with user needs and expectations. We discovered the trade-offs and complexities that need consideration when choosing suitable workflow stages, target users, and design configurations for different AI proposals. We explored how to balance AI benefits and risks for healthcare staff and patients within broader organizational and medical-legal constraints. We also identified data issues related to edge cases and data biases that affect model training and evaluation; how data documentation practices influence data preparation and labelling; and how to measure relevant AI outcomes reliably in future evaluations. We discuss how our work informs design and development of AI applications that are clinically useful, ethical, and acceptable in real-world healthcare services.","sentences":["Nasogastric tubes (NGTs) are feeding tubes that are inserted through the nose into the stomach to deliver nutrition or medication.","If not placed correctly, they can cause serious harm, even death to patients.","Recent AI developments demonstrate the feasibility of robustly detecting NGT placement from Chest X-ray images to reduce risks of sub-optimally or critically placed NGTs being missed or delayed in their detection, but gaps remain in clinical practice integration.","In this study, we present a human-centered approach to the problem and describe insights derived following contextual inquiry and in-depth interviews with 15 clinical stakeholders.","The interviews helped understand challenges in existing workflows, and how best to align technical capabilities with user needs and expectations.","We discovered the trade-offs and complexities that need consideration when choosing suitable workflow stages, target users, and design configurations for different AI proposals.","We explored how to balance AI benefits and risks for healthcare staff and patients within broader organizational and medical-legal constraints.","We also identified data issues related to edge cases and data biases that affect model training and evaluation; how data documentation practices influence data preparation and labelling; and how to measure relevant AI outcomes reliably in future evaluations.","We discuss how our work informs design and development of AI applications that are clinically useful, ethical, and acceptable in real-world healthcare services."],"url":"http://arxiv.org/abs/2405.05299v1","category":"cs.HC"}
{"created":"2024-05-08 11:03:22","title":"Relevant Irrelevance: Generating Alterfactual Explanations for Image Classifiers","abstract":"In this paper, we demonstrate the feasibility of alterfactual explanations for black box image classifiers. Traditional explanation mechanisms from the field of Counterfactual Thinking are a widely-used paradigm for Explainable Artificial Intelligence (XAI), as they follow a natural way of reasoning that humans are familiar with. However, most common approaches from this field are based on communicating information about features or characteristics that are especially important for an AI's decision. However, to fully understand a decision, not only knowledge about relevant features is needed, but the awareness of irrelevant information also highly contributes to the creation of a user's mental model of an AI system. To this end, a novel approach for explaining AI systems called alterfactual explanations was recently proposed on a conceptual level. It is based on showing an alternative reality where irrelevant features of an AI's input are altered. By doing so, the user directly sees which input data characteristics can change arbitrarily without influencing the AI's decision. In this paper, we show for the first time that it is possible to apply this idea to black box models based on neural networks. To this end, we present a GAN-based approach to generate these alterfactual explanations for binary image classifiers. Further, we present a user study that gives interesting insights on how alterfactual explanations can complement counterfactual explanations.","sentences":["In this paper, we demonstrate the feasibility of alterfactual explanations for black box image classifiers.","Traditional explanation mechanisms from the field of Counterfactual Thinking are a widely-used paradigm for Explainable Artificial Intelligence (XAI), as they follow a natural way of reasoning that humans are familiar with.","However, most common approaches from this field are based on communicating information about features or characteristics that are especially important for an AI's decision.","However, to fully understand a decision, not only knowledge about relevant features is needed, but the awareness of irrelevant information also highly contributes to the creation of a user's mental model of an AI system.","To this end, a novel approach for explaining AI systems called alterfactual explanations was recently proposed on a conceptual level.","It is based on showing an alternative reality where irrelevant features of an AI's input are altered.","By doing so, the user directly sees which input data characteristics can change arbitrarily without influencing the AI's decision.","In this paper, we show for the first time that it is possible to apply this idea to black box models based on neural networks.","To this end, we present a GAN-based approach to generate these alterfactual explanations for binary image classifiers.","Further, we present a user study that gives interesting insights on how alterfactual explanations can complement counterfactual explanations."],"url":"http://arxiv.org/abs/2405.05295v1","category":"cs.CV"}
{"created":"2024-05-08 08:20:27","title":"Smart Portable Computer","abstract":"Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and universities transitioning to virtual platforms, students encountered difficulties in acquiring PCs such as desktops or laptops. The starting prices, around 15,000 INR, often failed to offer adequate system specifications, posing a challenge for consumers. Additionally, those reliant on laptops for work found the conventional approach cumbersome. Enter the \"Portable Smart Computer,\" a leap into the future of computing. This innovative device boasts speed and performance comparable to traditional desktops but in a compact, energy-efficient, and cost-effective package. It delivers a seamless desktop experience, whether one is editing documents, browsing multiple tabs, managing spreadsheets, or creating presentations. Moreover, it supports programming languages like Python, C, C++, as well as compilers such as Keil and Xilinx, catering to the needs of programmers.","sentences":["Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and universities transitioning to virtual platforms, students encountered difficulties in acquiring PCs such as desktops or laptops.","The starting prices, around 15,000 INR, often failed to offer adequate system specifications, posing a challenge for consumers.","Additionally, those reliant on laptops for work found the conventional approach cumbersome.","Enter the \"Portable Smart Computer,\" a leap into the future of computing.","This innovative device boasts speed and performance comparable to traditional desktops but in a compact, energy-efficient, and cost-effective package.","It delivers a seamless desktop experience, whether one is editing documents, browsing multiple tabs, managing spreadsheets, or creating presentations.","Moreover, it supports programming languages like Python, C, C++, as well as compilers such as Keil and Xilinx, catering to the needs of programmers."],"url":"http://arxiv.org/abs/2405.05292v1","category":"cs.HC"}
{"created":"2024-05-09 17:49:04","title":"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning","abstract":"The emergence of large language models (LLMs) has opened up unprecedented possibilities for automating complex tasks that are often comparable to human performance. Despite their capabilities, LLMs still encounter difficulties in completing tasks that require high levels of accuracy and complexity due to their inherent limitations in handling multifaceted problems single-handedly. This paper introduces \"Smurfs\", a cutting-edge multi-agent framework designed to revolutionize the application of LLMs. By transforming a conventional LLM into a synergistic multi-agent ensemble, Smurfs enhances task decomposition and execution without necessitating extra training. This is achieved through innovative prompting strategies that allocate distinct roles within the model, thereby facilitating collaboration among specialized agents. The framework gives access to external tools to efficiently solve complex tasks. Our empirical investigation, featuring the mistral-7b-instruct model as a case study, showcases Smurfs' superior capability in intricate tool utilization scenarios. Notably, Smurfs outmatches the ChatGPT-ReACT in the ToolBench I2 and I3 benchmark with a remarkable 84.4% win rate, surpassing the highest recorded performance of a GPT-4 model at 73.5%. Furthermore, through comprehensive ablation studies, we dissect the contribution of the core components of the multi-agent framework to its overall efficacy. This not only verifies the effectiveness of the framework, but also sets a route for future exploration of multi-agent LLM systems.","sentences":["The emergence of large language models (LLMs) has opened up unprecedented possibilities for automating complex tasks that are often comparable to human performance.","Despite their capabilities, LLMs still encounter difficulties in completing tasks that require high levels of accuracy and complexity due to their inherent limitations in handling multifaceted problems single-handedly.","This paper introduces \"Smurfs\", a cutting-edge multi-agent framework designed to revolutionize the application of LLMs.","By transforming a conventional LLM into a synergistic multi-agent ensemble, Smurfs enhances task decomposition and execution without necessitating extra training.","This is achieved through innovative prompting strategies that allocate distinct roles within the model, thereby facilitating collaboration among specialized agents.","The framework gives access to external tools to efficiently solve complex tasks.","Our empirical investigation, featuring the mistral-7b-instruct model as a case study, showcases Smurfs' superior capability in intricate tool utilization scenarios.","Notably, Smurfs outmatches the ChatGPT-ReACT in the ToolBench I2 and I3 benchmark with a remarkable 84.4% win rate, surpassing the highest recorded performance of a GPT-4 model at 73.5%.","Furthermore, through comprehensive ablation studies, we dissect the contribution of the core components of the multi-agent framework to its overall efficacy.","This not only verifies the effectiveness of the framework, but also sets a route for future exploration of multi-agent LLM systems."],"url":"http://arxiv.org/abs/2405.05955v1","category":"cs.CL"}
{"created":"2024-05-09 17:36:23","title":"A Survey on Visualization Approaches in Political Science for Social and Political Factors: Progress to Date and Future Opportunities","abstract":"Politics is the set of activities related to strategic decision-making in groups. Political scientists study the strategic interactions between states, institutions, politicians, and citizens; they seek to understand the causes and consequences of those decisions and interactions. While some decisions might alleviate social problems, others might lead to disasters such as war and conflict. Data visualization approaches have the potential to assist political scientists in their studies by providing visual contexts. However, political researchers' perspectives on data visualization are unclear. This paper examines political scientists' perspectives on visualization and how they apply data visualization in their research. We discovered a growing trend in the use of graphs in political science journals. However, we also found a knowledge gap between the political science and visualization domains, such as effective visualization techniques for tasks and the use of color studied by visualization researchers. To reduce this gap, we survey visualization techniques applicable to the political scientists' research and report the visual analytics systems implemented for and evaluated by political scientists. At the end of this paper, we present an outline of future opportunities, including research topics and methodologies, for multidisciplinary research in political science and data analytics. Through this paper, we expect visualization researchers to get a better grasp of the political science domain, as well as broaden the possibility of future visualization approaches from a multidisciplinary perspective.","sentences":["Politics is the set of activities related to strategic decision-making in groups.","Political scientists study the strategic interactions between states, institutions, politicians, and citizens; they seek to understand the causes and consequences of those decisions and interactions.","While some decisions might alleviate social problems, others might lead to disasters such as war and conflict.","Data visualization approaches have the potential to assist political scientists in their studies by providing visual contexts.","However, political researchers' perspectives on data visualization are unclear.","This paper examines political scientists' perspectives on visualization and how they apply data visualization in their research.","We discovered a growing trend in the use of graphs in political science journals.","However, we also found a knowledge gap between the political science and visualization domains, such as effective visualization techniques for tasks and the use of color studied by visualization researchers.","To reduce this gap, we survey visualization techniques applicable to the political scientists' research and report the visual analytics systems implemented for and evaluated by political scientists.","At the end of this paper, we present an outline of future opportunities, including research topics and methodologies, for multidisciplinary research in political science and data analytics.","Through this paper, we expect visualization researchers to get a better grasp of the political science domain, as well as broaden the possibility of future visualization approaches from a multidisciplinary perspective."],"url":"http://arxiv.org/abs/2405.05947v1","category":"cs.HC"}
{"created":"2024-05-09 17:24:06","title":"Dynamics of a Towed Cable with Sensor-Array for Underwater Target Motion Analysis","abstract":"During a war situation, many times an underwater target motion analysis (TMA) is performed using bearing-only measurements, obtained from a sensor array, which is towed by an own-ship with the help of a connected cable. It is well known that the own-ship is required to perform a manoeuvre in order to make the system observable and localise the target successfully. During the maneuver, it is important to know the location of the sensor array with respect to the own-ship. This paper develops a dynamic model of a cable-sensor array system to localise the sensor array, which is towed behind a sea-surface vessel. We adopt a lumped-mass approach to represent the towed cable. The discretized cable elements are modelled as an interconnected rigid body, kinematically related to one another. The governing equations are derived by balancing the moments acting on each node. The derived dynamics are solved simultaneously for all the nodes to determine the orientation of the cable and sensor array. The position of the sensor array obtained from this proposed model will further be used by TMA algorithms to enhance the accuracy of the tracking system.","sentences":["During a war situation, many times an underwater target motion analysis (TMA) is performed using bearing-only measurements, obtained from a sensor array, which is towed by an own-ship with the help of a connected cable.","It is well known that the own-ship is required to perform a manoeuvre in order to make the system observable and localise the target successfully.","During the maneuver, it is important to know the location of the sensor array with respect to the own-ship.","This paper develops a dynamic model of a cable-sensor array system to localise the sensor array, which is towed behind a sea-surface vessel.","We adopt a lumped-mass approach to represent the towed cable.","The discretized cable elements are modelled as an interconnected rigid body, kinematically related to one another.","The governing equations are derived by balancing the moments acting on each node.","The derived dynamics are solved simultaneously for all the nodes to determine the orientation of the cable and sensor array.","The position of the sensor array obtained from this proposed model will further be used by TMA algorithms to enhance the accuracy of the tracking system."],"url":"http://arxiv.org/abs/2405.05937v1","category":"eess.SP"}
{"created":"2024-05-09 17:18:39","title":"Interfacial proximity and interplay between Kondo and short-range magnetic correlations in heterostructures","abstract":"In this work, we investigate the influence of interlayer distance in a heterostructure containing both Kondo effects and short-range magnetic correlations. Our proposed heterostructure comprises three coupled square lattice layers. The first layer is governed by the Kondo-Heisenberg lattice model involving $f$- and $d$-electrons, which interact via Kondo and Heisenberg couplings, $J_{K}$ and $J_{H}$, respectively. The other two layers consist of non-interacting itinerant electrons, where coupling with the first layer is determined by two perpendicular hopping parameters. We find that varying the interlayer couplings induces electronic dynamics at the interface, altering the behavior of mean-field parameters describing the Kondo effect and short-range magnetic correlations. The system's temperature - interlayer hopping parameter phase diagram exhibits a sequence of discontinuous and continuous transitions. In the cases, $|J_{K}|<|J_{H}|$ and $|J_{K}|>|J_{H}|$ rich phase diagrams are found which include Kondo, ferromagnetic and antiferromagnetic correlations. Our work provides insights into hosting Kondo correlations in heterostructures.","sentences":["In this work, we investigate the influence of interlayer distance in a heterostructure containing both Kondo effects and short-range magnetic correlations.","Our proposed heterostructure comprises three coupled square lattice layers.","The first layer is governed by the Kondo-Heisenberg lattice model involving $f$- and $d$-electrons, which interact via Kondo and Heisenberg couplings, $J_{K}$ and $J_{H}$, respectively.","The other two layers consist of non-interacting itinerant electrons, where coupling with the first layer is determined by two perpendicular hopping parameters.","We find that varying the interlayer couplings induces electronic dynamics at the interface, altering the behavior of mean-field parameters describing the Kondo effect and short-range magnetic correlations.","The system's temperature - interlayer hopping parameter phase diagram exhibits a sequence of discontinuous and continuous transitions.","In the cases, $|J_{K}|<|J_{H}|$ and $|J_{K}|>|J_{H}|$ rich phase diagrams are found which include Kondo, ferromagnetic and antiferromagnetic correlations.","Our work provides insights into hosting Kondo correlations in heterostructures."],"url":"http://arxiv.org/abs/2405.05935v1","category":"cond-mat.str-el"}
{"created":"2024-05-09 16:52:43","title":"A Comprehensive Survey of Masked Faces: Recognition, Detection, and Unmasking","abstract":"Masked face recognition (MFR) has emerged as a critical domain in biometric identification, especially by the global COVID-19 pandemic, which introduced widespread face masks. This survey paper presents a comprehensive analysis of the challenges and advancements in recognising and detecting individuals with masked faces, which has seen innovative shifts due to the necessity of adapting to new societal norms. Advanced through deep learning techniques, MFR, along with Face Mask Recognition (FMR) and Face Unmasking (FU), represent significant areas of focus. These methods address unique challenges posed by obscured facial features, from fully to partially covered faces. Our comprehensive review delves into the various deep learning-based methodologies developed for MFR, FMR, and FU, highlighting their distinctive challenges and the solutions proposed to overcome them. Additionally, we explore benchmark datasets and evaluation metrics specifically tailored for assessing performance in MFR research. The survey also discusses the substantial obstacles still facing researchers in this field and proposes future directions for the ongoing development of more robust and effective masked face recognition systems. This paper serves as an invaluable resource for researchers and practitioners, offering insights into the evolving landscape of face recognition technologies in the face of global health crises and beyond.","sentences":["Masked face recognition (MFR) has emerged as a critical domain in biometric identification, especially by the global COVID-19 pandemic, which introduced widespread face masks.","This survey paper presents a comprehensive analysis of the challenges and advancements in recognising and detecting individuals with masked faces, which has seen innovative shifts due to the necessity of adapting to new societal norms.","Advanced through deep learning techniques, MFR, along with Face Mask Recognition (FMR) and Face Unmasking (FU), represent significant areas of focus.","These methods address unique challenges posed by obscured facial features, from fully to partially covered faces.","Our comprehensive review delves into the various deep learning-based methodologies developed for MFR, FMR, and FU, highlighting their distinctive challenges and the solutions proposed to overcome them.","Additionally, we explore benchmark datasets and evaluation metrics specifically tailored for assessing performance in MFR research.","The survey also discusses the substantial obstacles still facing researchers in this field and proposes future directions for the ongoing development of more robust and effective masked face recognition systems.","This paper serves as an invaluable resource for researchers and practitioners, offering insights into the evolving landscape of face recognition technologies in the face of global health crises and beyond."],"url":"http://arxiv.org/abs/2405.05900v1","category":"cs.CV"}
{"created":"2024-05-09 16:51:40","title":"An Infinite Family of Integrable Sigma Models Using Auxiliary Fields","abstract":"We introduce a class of sigma models in two spacetime dimensions which are parameterized by an interaction function of one real variable. In addition to the physical group-valued field $g$, these models include an auxiliary vector field $v_\\alpha$ which mediates interactions in a prescribed way. We prove that every model in this family is weakly integrable, in the sense that the classical equations of motion are equivalent to flatness of a Lax connection for any value of a spectral parameter. We also show that these models are strongly integrable, in the sense that the Poisson bracket of the Lax connection takes the Maillet form, which guarantees the existence of an infinite set of conserved charges in involution. This class of theories includes the principal chiral model (PCM) and all deformations of the PCM by functions of the energy-momentum tensor, such as $T \\overline{T}$ and root-$T \\overline{T}$.","sentences":["We introduce a class of sigma models in two spacetime dimensions which are parameterized by an interaction function of one real variable.","In addition to the physical group-valued field $g$, these models include an auxiliary vector field $v_\\alpha$ which mediates interactions in a prescribed way.","We prove that every model in this family is weakly integrable, in the sense that the classical equations of motion are equivalent to flatness of a Lax connection for any value of a spectral parameter.","We also show that these models are strongly integrable, in the sense that the Poisson bracket of the Lax connection takes the Maillet form, which guarantees the existence of an infinite set of conserved charges in involution.","This class of theories includes the principal chiral model (PCM) and all deformations of the PCM by functions of the energy-momentum tensor, such as $T \\overline{T}$ and root-$T \\overline{T}$."],"url":"http://arxiv.org/abs/2405.05899v1","category":"hep-th"}
{"created":"2024-05-09 16:48:22","title":"Efficient numerical computation of spiral spectra with exponentially-weighted preconditioners","abstract":"The stability of nonlinear waves on spatially extended domains is commonly probed by computing the spectrum of the linearization of the underlying PDE about the wave profile. It is known that convective transport, whether driven by the nonlinear pattern itself or an underlying fluid flow, can cause exponential growth of the resolvent of the linearization as a function of the domain length. In particular, sparse eigenvalue algorithms may result in inaccurate and spurious spectra in the convective regime. In this work, we focus on spiral waves, which arise in many natural processes and which exhibit convective transport. We prove that exponential weights can serve as effective, inexpensive preconditioners that result in resolvents that are uniformly bounded in the domain size and that stabilize numerical spectral computations. We also show that the optimal exponential rates can be computed reliably from a simpler asymptotic problem posed in one space dimension.","sentences":["The stability of nonlinear waves on spatially extended domains is commonly probed by computing the spectrum of the linearization of the underlying PDE about the wave profile.","It is known that convective transport, whether driven by the nonlinear pattern itself or an underlying fluid flow, can cause exponential growth of the resolvent of the linearization as a function of the domain length.","In particular, sparse eigenvalue algorithms may result in inaccurate and spurious spectra in the convective regime.","In this work, we focus on spiral waves, which arise in many natural processes and which exhibit convective transport.","We prove that exponential weights can serve as effective, inexpensive preconditioners that result in resolvents that are uniformly bounded in the domain size and that stabilize numerical spectral computations.","We also show that the optimal exponential rates can be computed reliably from a simpler asymptotic problem posed in one space dimension."],"url":"http://arxiv.org/abs/2405.05897v1","category":"math.NA"}
{"created":"2024-05-09 16:17:41","title":"Co-driver: VLM-based Autonomous Driving Assistant with Human-like Behavior and Understanding for Complex Road Scenes","abstract":"Recent research about Large Language Model based autonomous driving solutions shows a promising picture in planning and control fields. However, heavy computational resources and hallucinations of Large Language Models continue to hinder the tasks of predicting precise trajectories and instructing control signals. To address this problem, we propose Co-driver, a novel autonomous driving assistant system to empower autonomous vehicles with adjustable driving behaviors based on the understanding of road scenes. A pipeline involving the CARLA simulator and Robot Operating System 2 (ROS2) verifying the effectiveness of our system is presented, utilizing a single Nvidia 4090 24G GPU while exploiting the capacity of textual output of the Visual Language Model. Besides, we also contribute a dataset containing an image set and a corresponding prompt set for fine-tuning the Visual Language Model module of our system. In the real-world driving dataset, our system achieved 96.16% success rate in night scenes and 89.7% in gloomy scenes regarding reasonable predictions. Our Co-driver dataset will be released at https://github.com/ZionGo6/Co-driver.","sentences":["Recent research about Large Language Model based autonomous driving solutions shows a promising picture in planning and control fields.","However, heavy computational resources and hallucinations of Large Language Models continue to hinder the tasks of predicting precise trajectories and instructing control signals.","To address this problem, we propose Co-driver, a novel autonomous driving assistant system to empower autonomous vehicles with adjustable driving behaviors based on the understanding of road scenes.","A pipeline involving the CARLA simulator and Robot Operating System 2 (ROS2) verifying the effectiveness of our system is presented, utilizing a single Nvidia 4090 24G GPU while exploiting the capacity of textual output of the Visual Language Model.","Besides, we also contribute a dataset containing an image set and a corresponding prompt set for fine-tuning the Visual Language Model module of our system.","In the real-world driving dataset, our system achieved 96.16% success rate in night scenes and 89.7% in gloomy scenes regarding reasonable predictions.","Our Co-driver dataset will be released at https://github.com/ZionGo6/Co-driver."],"url":"http://arxiv.org/abs/2405.05885v1","category":"cs.RO"}
{"created":"2024-05-09 16:10:02","title":"Cohen--Macaulay Complexes, Duality Groups, and the dualizing module of ${\\rm{Out}}(F_N)$","abstract":"We explain how Cohen--Macaulay classifying spaces are ubiquitous among discrete groups that satisfy Bieri--Eckmann duality, and compare Bieri--Eckmann duality to duality results for Cohen--Macaulay complexes. We use this comparison to give a description of the dualizing module of ${\\rm{Out}}(F_N)$ in terms of the local cohomology cosheaf of the spine of Outer space.","sentences":["We explain how Cohen--Macaulay classifying spaces are ubiquitous among discrete groups that satisfy Bieri--Eckmann duality, and compare Bieri--Eckmann duality to duality results for Cohen--Macaulay complexes.","We use this comparison to give a description of the dualizing module of ${\\rm{Out}}(F_N)$ in terms of the local cohomology cosheaf of the spine of Outer space."],"url":"http://arxiv.org/abs/2405.05881v1","category":"math.GR"}
{"created":"2024-05-09 16:05:57","title":"Optical contrast analysis of \u03b1-RuCl$_3$ nanoflakes on oxidized silicon wafers","abstract":"{\\alpha}-RuCl$_3$, a narrow-band Mott insulator with large work function, offers intriguing potential as a quantum material or as a charge acceptor for electrical contacts in van der Waals devices. In this work, we perform a systematic study of the optical reflection contrast of {\\alpha}-RuCl$_3$ nanoflakes on oxidized silicon wafers and estimate the accuracy of this imaging technique to assess the crystal thickness. Via spectroscopic micro-ellipsometry measurements, we characterize the wavelength-dependent complex refractive index of {\\alpha}-RuCl$_3$ nanoflakes of varying thickness in the visible and near-infrared. Building on these results, we simulate the optical contrast of {\\alpha}-RuCl$_3$ nanoflakes with thicknesses below 100 nm on SiO$_2$/Si substrates under different illumination conditions. We compare the simulated optical contrast with experimental values extracted from optical microscopy images and obtain good agreement. Finally, we show that optical contrast imaging allows us to retrieve the thickness of the RuCl$_3$ nanoflakes exfoliated on an oxidized silicon substrate with a mean deviation of -0.2 nm for thicknesses below 100 nm with a standard deviation of only 1 nm. Our results demonstrate that optical contrast can be used as a non-invasive, fast, and reliable technique to estimate the {\\alpha}-RuCl$_3$ thickness.","sentences":["{\\alpha}-RuCl$_3$, a narrow-band Mott insulator with large work function, offers intriguing potential as a quantum material or as a charge acceptor for electrical contacts in van der Waals devices.","In this work, we perform a systematic study of the optical reflection contrast of {\\alpha}-RuCl$_3$ nanoflakes on oxidized silicon wafers and estimate the accuracy of this imaging technique to assess the crystal thickness.","Via spectroscopic micro-ellipsometry measurements, we characterize the wavelength-dependent complex refractive index of {\\alpha}-RuCl$_3$ nanoflakes of varying thickness in the visible and near-infrared.","Building on these results, we simulate the optical contrast of {\\alpha}-RuCl$_3$ nanoflakes with thicknesses below 100 nm on SiO$_2$/Si substrates under different illumination conditions.","We compare the simulated optical contrast with experimental values extracted from optical microscopy images and obtain good agreement.","Finally, we show that optical contrast imaging allows us to retrieve the thickness of the RuCl$_3$ nanoflakes exfoliated on an oxidized silicon substrate with a mean deviation of -0.2 nm for thicknesses below 100 nm with a standard deviation of only 1 nm.","Our results demonstrate that optical contrast can be used as a non-invasive, fast, and reliable technique to estimate the {\\alpha}-RuCl$_3$ thickness."],"url":"http://arxiv.org/abs/2405.05880v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 16:05:39","title":"Uniqueness Problem for the Backward Differential Equation of a Continuous-State Branching Process","abstract":"The distributional properties of a multi-dimensional continuous-state branching process are determined by its cumulant semigroup, which is defined by the backward differential equation. We provide a proof of the assertion of Rhyzhov and Skorokhod (Theory Probab. Appl., 1970) on the uniqueness of the solutions to the equation, which is based on a characterization of the process as the pathwise unique solution to a system of stochastic equations.","sentences":["The distributional properties of a multi-dimensional continuous-state branching process are determined by its cumulant semigroup, which is defined by the backward differential equation.","We provide a proof of the assertion of Rhyzhov and Skorokhod (Theory Probab.","Appl., 1970) on the uniqueness of the solutions to the equation, which is based on a characterization of the process as the pathwise unique solution to a system of stochastic equations."],"url":"http://arxiv.org/abs/2405.05879v1","category":"math.PR"}
{"created":"2024-05-09 16:02:09","title":"Duality for Cohen--Macaulay Complexes through Combinatorial Sheaves","abstract":"We prove a duality theorem for Cohen--Macaulay simplicial complexes. This is a generalisation of Poincar\\'e Duality, framed in the language of combinatorial sheaves. Our treatment is self-contained and accessible for readers with a working knowledge of simplicial complexes and (co)homology. The main motivation is a link with Bieri-Eckmann duality for discrete groups, which is explored in a companion paper.","sentences":["We prove a duality theorem for Cohen--Macaulay simplicial complexes.","This is a generalisation of Poincar\\'e Duality, framed in the language of combinatorial sheaves.","Our treatment is self-contained and accessible for readers with a working knowledge of simplicial complexes and (co)homology.","The main motivation is a link with Bieri-Eckmann duality for discrete groups, which is explored in a companion paper."],"url":"http://arxiv.org/abs/2405.05873v1","category":"math.AT"}
{"created":"2024-05-09 15:54:02","title":"Parameter identification for an uncertain reaction-diffusion equation via setpoint regulation","abstract":"The problem of estimating the reaction coefficient of a system governed by a reaction-diffusion partial differential equation is tackled. An estimator relying on boundary measurements only is proposed. The estimator is based upon a setpoint regulation strategy and leads to an asymptotically converging estimate of the unknown reaction coefficient. The proposed estimator is combined with a state observer and shown to provide an asymptotic estimate of the actual system state. A numerical example supports and illustrates the theoretical results.","sentences":["The problem of estimating the reaction coefficient of a system governed by a reaction-diffusion partial differential equation is tackled.","An estimator relying on boundary measurements only is proposed.","The estimator is based upon a setpoint regulation strategy and leads to an asymptotically converging estimate of the unknown reaction coefficient.","The proposed estimator is combined with a state observer and shown to provide an asymptotic estimate of the actual system state.","A numerical example supports and illustrates the theoretical results."],"url":"http://arxiv.org/abs/2405.05866v1","category":"math.OC"}
{"created":"2024-05-09 15:53:43","title":"Faster Linear Systems and Matrix Norm Approximation via Multi-level Sketched Preconditioning","abstract":"We present a new class of preconditioned iterative methods for solving linear systems of the form $Ax = b$. Our methods are based on constructing a low-rank Nystr\\\"om approximation to $A$ using sparse random sketching. This approximation is used to construct a preconditioner, which itself is inverted quickly using additional levels of random sketching and preconditioning. We prove that the convergence of our methods depends on a natural average condition number of $A$, which improves as the rank of the Nystr\\\"om approximation increases. Concretely, this allows us to obtain faster runtimes for a number of fundamental linear algebraic problems:   1. We show how to solve any $n\\times n$ linear system that is well-conditioned except for $k$ outlying large singular values in $\\tilde{O}(n^{2.065} + k^\\omega)$ time, improving on a recent result of [Derezi\\'nski, Yang, STOC 2024] for all $k \\gtrsim n^{0.78}$.   2. We give the first $\\tilde{O}(n^2 + {d_\\lambda}^{\\omega}$) time algorithm for solving a regularized linear system $(A + \\lambda I)x = b$, where $A$ is positive semidefinite with effective dimension $d_\\lambda$. This problem arises in applications like Gaussian process regression.   3. We give faster algorithms for approximating Schatten $p$-norms and other matrix norms. For example, for the Schatten 1 (nuclear) norm, we give an algorithm that runs in $\\tilde{O}(n^{2.11})$ time, improving on an $\\tilde{O}(n^{2.18})$ method of [Musco et al., ITCS 2018].   Interestingly, previous state-of-the-art algorithms for most of the problems above relied on stochastic iterative methods, like stochastic coordinate and gradient descent. Our work takes a completely different approach, instead leveraging tools from matrix sketching.","sentences":["We present a new class of preconditioned iterative methods for solving linear systems of the form $Ax = b$. Our methods are based on constructing a low-rank Nystr\\\"om approximation to $A$ using sparse random sketching.","This approximation is used to construct a preconditioner, which itself is inverted quickly using additional levels of random sketching and preconditioning.","We prove that the convergence of our methods depends on a natural average condition number of $A$, which improves as the rank of the Nystr\\\"om approximation increases.","Concretely, this allows us to obtain faster runtimes for a number of fundamental linear algebraic problems:   1.","We show how to solve any $n\\times n$ linear system that is well-conditioned except for $k$ outlying large singular values in $\\tilde{O}(n^{2.065} + k^\\omega)$ time, improving on a recent result of [Derezi\\'nski, Yang, STOC 2024] for all $k \\gtrsim n^{0.78}$.   2.","We give the first $\\tilde{O}(n^2 + {d_\\lambda}^{\\omega}$) time algorithm for solving a regularized linear system $(A + \\lambda I)x","= b$, where $A$ is positive semidefinite with effective dimension $d_\\lambda$. This problem arises in applications like Gaussian process regression.   ","3.","We give faster algorithms for approximating Schatten $p$-norms and other matrix norms.","For example, for the Schatten 1 (nuclear) norm, we give an algorithm that runs in $\\tilde{O}(n^{2.11})$ time, improving on an $\\tilde{O}(n^{2.18})$ method of [Musco et al., ITCS 2018].   ","Interestingly, previous state-of-the-art algorithms for most of the problems above relied on stochastic iterative methods, like stochastic coordinate and gradient descent.","Our work takes a completely different approach, instead leveraging tools from matrix sketching."],"url":"http://arxiv.org/abs/2405.05865v1","category":"cs.DS"}
{"created":"2024-05-09 15:49:42","title":"Strong electrostatic control of excitonic features in MoS$_2$ by a free-standing ultrahigh-$\u03ba$ ferroelectric perovskite","abstract":"We present the electrostatic control of photoluminescence of monolayer MoS$_2$ at room temperature via integration of free-standing BaTiO$_3$ (BTO), a ferroelectric perovskite oxide, layers. We show that the use of BTO leads to highly tunable exciton emission of MoS$_2$ in a minimal range of gate voltages, effectively controlling the neutral excitons to charged excitons (trions) conversion. Due to BTO's ferroelectric polarization-induced doping we observe large peak emission shifts as well as a large and tunable A trion binding energy in the range of 40-100 meV. To further investigate the efficacy of electrostatic control, we compared our measurements with those carried out when the BTO is replaced by a hexagonal boron nitride (hBN) dielectric layer of comparable thickness, confirming BTO's superior gating properties and thus lower power consumption. Additionally, we take advantage of the ferroelectric switching of BTO by fabricating devices where the BTO layer is decoupled from the gate electrode with a SiO$_2$ layer. Choosing to isolate the BTO allows us to induce large remanent behavior of MoS$_2$'s excitonic features, observing hysteretic behavior in the peak energy ratio between A exciton and its trion, as well as hysteretic behavior in the doping-related trion energy shift. This study illustrates the rich physics involved in combining free-standing complex oxide layers with two-dimensional materials.","sentences":["We present the electrostatic control of photoluminescence of monolayer MoS$_2$ at room temperature via integration of free-standing BaTiO$_3$ (BTO), a ferroelectric perovskite oxide, layers.","We show that the use of BTO leads to highly tunable exciton emission of MoS$_2$ in a minimal range of gate voltages, effectively controlling the neutral excitons to charged excitons (trions) conversion.","Due to BTO's ferroelectric polarization-induced doping we observe large peak emission shifts as well as a large and tunable A trion binding energy in the range of 40-100 meV. To further investigate the efficacy of electrostatic control, we compared our measurements with those carried out when the BTO is replaced by a hexagonal boron nitride (hBN) dielectric layer of comparable thickness, confirming BTO's superior gating properties and thus lower power consumption.","Additionally, we take advantage of the ferroelectric switching of BTO by fabricating devices where the BTO layer is decoupled from the gate electrode with a SiO$_2$ layer.","Choosing to isolate the BTO allows us to induce large remanent behavior of MoS$_2","$'s excitonic features, observing hysteretic behavior in the peak energy ratio between A exciton and its trion, as well as hysteretic behavior in the doping-related trion energy shift.","This study illustrates the rich physics involved in combining free-standing complex oxide layers with two-dimensional materials."],"url":"http://arxiv.org/abs/2405.05862v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 15:41:59","title":"Infinitely many isolas of modulational instability for Stokes waves","abstract":"We prove the long-standing conjecture regarding the existence of infinitely many high-frequency modulational instability ``isolas\" for a Stokes wave in arbitrary depth $ \\mathtt{h} > 0 $, subject to longitudinal perturbations. We completely describe the spectral bands with non-zero real part away from the origin of the $L^2(\\mathbb{R})$-spectrum of the water waves system linearized at a Stokes waves of small amplitude $ \\epsilon > 0 $. The unstable spectrum is the union of isolas of elliptical shape, parameterized by integers $ \\mathtt{p}\\geq 2 $, with semiaxis of size $ |\\beta_1^{(\\mathtt{p})} (\\mathtt{h})| \\epsilon^\\mathtt{p}+ O(\\epsilon^{\\mathtt{p}+1} )$ where $\\beta_1^{( \\mathtt{p})} (\\mathtt{h})$ is a nonzero analytic function of the depth $ \\mathtt{h} $ that depends on the Taylor coefficients of the Stokes waves up to order $\\mathtt{p}$.","sentences":["We prove the long-standing conjecture regarding the existence of infinitely many high-frequency modulational instability ``isolas\" for a Stokes wave in arbitrary depth $ \\mathtt{h} > 0 $, subject to longitudinal perturbations.","We completely describe the spectral bands with non-zero real part away from the origin of the $L^2(\\mathbb{R})$-spectrum of the water waves system linearized at a Stokes waves of small amplitude $ \\epsilon > 0 $.","The unstable spectrum is the union of isolas of elliptical shape, parameterized by integers $ \\mathtt{p}\\geq 2 $, with semiaxis of size $ |\\beta_1^{(\\mathtt{p})} (\\mathtt{h})| \\epsilon^\\mathtt{p}+ O(\\epsilon^{\\mathtt{p}+1} )$ where $\\beta_1^{( \\mathtt{p})} (\\mathtt{h})$ is a nonzero analytic function of the depth $ \\mathtt{h} $ that depends on the Taylor coefficients of the Stokes waves up to order $\\mathtt{p}$."],"url":"http://arxiv.org/abs/2405.05854v1","category":"math.AP"}
{"created":"2024-05-09 15:34:20","title":"Age of Information and Energy Consumption in IoT: an Experimental Evaluation","abstract":"The Age of Information (AoI) is an end-to-end metric frequently used to understand how \"fresh\" the information about a remote system is. In this paper, we present an experimental study of the relationship between AoI and the energy spent by the device that produces information, e.g. an IoT device or a monitoring sensor. Such a relationship has been almost neglected so far, but it is particularly important whenever the sensing side is battery-operated. The study is carried out in a scenario where access is achieved via the cellular network and information is transferred using MQTT, a popular messaging protocol in the IoT domain. Numerous parameters of operation are considered, and the most efficient solutions in all configurations are provided.","sentences":["The Age of Information (AoI) is an end-to-end metric frequently used to understand how \"fresh\" the information about a remote system is.","In this paper, we present an experimental study of the relationship between AoI and the energy spent by the device that produces information, e.g. an IoT device or a monitoring sensor.","Such a relationship has been almost neglected so far, but it is particularly important whenever the sensing side is battery-operated.","The study is carried out in a scenario where access is achieved via the cellular network and information is transferred using MQTT, a popular messaging protocol in the IoT domain.","Numerous parameters of operation are considered, and the most efficient solutions in all configurations are provided."],"url":"http://arxiv.org/abs/2405.05849v1","category":"cs.NI"}
{"created":"2024-05-09 15:34:18","title":"Distributed Estimation for a 3-D Moving Target in Quaternion Space with Unknown Correlation","abstract":"For distributed estimations in a sensor network, the consistency and accuracy of an estimator are greatly affected by the unknown correlations between individual estimates. An inconsistent or too conservative estimate may degrade the estimation performance and even cause divergence of the estimator. Cooperative estimation methods based on Inverse Covariance Intersection (ICI) can utilize a network of sensors to provide a consistent and tight estimate of a target. In this paper, unlike most existing ICI-based estimators that only consider two-dimensional (2-D) target state estimation in the vector space, we address this problem in a 3-D environment by extending the ICI algorithm to the augmented quaternion space. In addition, the proposed algorithm is fully distributed, as each agent only uses the local information from itself and its communication neighbors, which is also robust to a time-varying communication topology. To evaluate the performance, we test the proposed algorithm in a camera network to track the pose of a target. Extensive Monte Carlo simulations have been performed to show the effectiveness of our approach.","sentences":["For distributed estimations in a sensor network, the consistency and accuracy of an estimator are greatly affected by the unknown correlations between individual estimates.","An inconsistent or too conservative estimate may degrade the estimation performance and even cause divergence of the estimator.","Cooperative estimation methods based on Inverse Covariance Intersection (ICI) can utilize a network of sensors to provide a consistent and tight estimate of a target.","In this paper, unlike most existing ICI-based estimators that only consider two-dimensional (2-D) target state estimation in the vector space, we address this problem in a 3-D environment by extending the ICI algorithm to the augmented quaternion space.","In addition, the proposed algorithm is fully distributed, as each agent only uses the local information from itself and its communication neighbors, which is also robust to a time-varying communication topology.","To evaluate the performance, we test the proposed algorithm in a camera network to track the pose of a target.","Extensive Monte Carlo simulations have been performed to show the effectiveness of our approach."],"url":"http://arxiv.org/abs/2405.05848v1","category":"eess.SY"}
{"created":"2024-05-09 15:30:11","title":"Structure-preserving parametric finite element methods for simulating axisymmetric solid-state dewetting problems with anisotropic surface energies","abstract":"Solid-state dewetting (SSD), a widespread phenomenon in solid-solid-vapor system, could be used to describe the accumulation of solid thin films on the substrate. In this work, we consider the sharp interface model for axisymmetric SSD with anisotropic surface energy. By introducing two types of surface energy matrices from the anisotropy functions,we aim to design two structure-preserving algorithms for the axisymmetric SSD. The newly designed schemes are applicable to a broader range of anisotropy functions, and we can theoretically prove their volume conservation and energy stability. In addition, based on a novel weak formulation for the axisymmetric SSD, we further build another two numerical schemes that have good mesh properties. Finally, numerous numerical tests are reported to showcase the accuracy and efficiency of the numerical methods.","sentences":["Solid-state dewetting (SSD), a widespread phenomenon in solid-solid-vapor system, could be used to describe the accumulation of solid thin films on the substrate.","In this work, we consider the sharp interface model for axisymmetric SSD with anisotropic surface energy.","By introducing two types of surface energy matrices from the anisotropy functions,we aim to design two structure-preserving algorithms for the axisymmetric SSD.","The newly designed schemes are applicable to a broader range of anisotropy functions, and we can theoretically prove their volume conservation and energy stability.","In addition, based on a novel weak formulation for the axisymmetric SSD, we further build another two numerical schemes that have good mesh properties.","Finally, numerous numerical tests are reported to showcase the accuracy and efficiency of the numerical methods."],"url":"http://arxiv.org/abs/2405.05844v1","category":"math.NA"}
{"created":"2024-05-09 15:16:22","title":"Altermagnetic Polar Metallic phase in Ultra-Thin Epitaxially-Strained RuO2 Films","abstract":"Altermagnetism refers to a wide class of compensated magnetic orders featuring magnetic sublattices with opposite spins related by rotational symmetry rather than inversion or translational operations, resulting in non-trivial spin splitting and high-order multipolar orders. Here, by combining theoretical analysis, electrical transport, X-ray and optical spectroscopies, and nonlinear optical measurements, we establish a phase diagram in hybrid molecular beam epitaxy-grown RuO2/TiO2 (110) films, mapping the broken symmetries along the altermagnetic/electronic/structural phase transitions as functions of film thickness and temperature. This phase diagram features a novel altermagnetic metallic polar phase in strained 2 nm samples, extending the concept of multiferroics to altermagnetic systems. These results provide a comprehensive understanding of altermagnetism upon epitaxial heterostructure design for emergent novel phases with multifunctionalities.","sentences":["Altermagnetism refers to a wide class of compensated magnetic orders featuring magnetic sublattices with opposite spins related by rotational symmetry rather than inversion or translational operations, resulting in non-trivial spin splitting and high-order multipolar orders.","Here, by combining theoretical analysis, electrical transport, X-ray and optical spectroscopies, and nonlinear optical measurements, we establish a phase diagram in hybrid molecular beam epitaxy-grown RuO2/TiO2 (110) films, mapping the broken symmetries along the altermagnetic/electronic/structural phase transitions as functions of film thickness and temperature.","This phase diagram features a novel altermagnetic metallic polar phase in strained 2 nm samples, extending the concept of multiferroics to altermagnetic systems.","These results provide a comprehensive understanding of altermagnetism upon epitaxial heterostructure design for emergent novel phases with multifunctionalities."],"url":"http://arxiv.org/abs/2405.05838v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 15:15:34","title":"Informed Decision-Making through Advancements in Open Set Recognition and Unknown Sample Detection","abstract":"Machine learning-based techniques open up many opportunities and improvements to derive deeper and more practical insights from data that can help businesses make informed decisions. However, the majority of these techniques focus on the conventional closed-set scenario, in which the label spaces for the training and test sets are identical. Open set recognition (OSR) aims to bring classification tasks in a situation that is more like reality, which focuses on classifying the known classes as well as handling unknown classes effectively. In such an open-set problem the gathered samples in the training set cannot encompass all the classes and the system needs to identify unknown samples at test time. On the other hand, building an accurate and comprehensive model in a real dynamic environment presents a number of obstacles, because it is prohibitively expensive to train for every possible example of unknown items, and the model may fail when tested in testbeds. This study provides an algorithm exploring a new representation of feature space to improve classification in OSR tasks. The efficacy and efficiency of business processes and decision-making can be improved by integrating OSR, which offers more precise and insightful predictions of outcomes. We demonstrate the performance of the proposed method on three established datasets. The results indicate that the proposed model outperforms the baseline methods in accuracy and F1-score.","sentences":["Machine learning-based techniques open up many opportunities and improvements to derive deeper and more practical insights from data that can help businesses make informed decisions.","However, the majority of these techniques focus on the conventional closed-set scenario, in which the label spaces for the training and test sets are identical.","Open set recognition (OSR) aims to bring classification tasks in a situation that is more like reality, which focuses on classifying the known classes as well as handling unknown classes effectively.","In such an open-set problem the gathered samples in the training set cannot encompass all the classes and the system needs to identify unknown samples at test time.","On the other hand, building an accurate and comprehensive model in a real dynamic environment presents a number of obstacles, because it is prohibitively expensive to train for every possible example of unknown items, and the model may fail when tested in testbeds.","This study provides an algorithm exploring a new representation of feature space to improve classification in OSR tasks.","The efficacy and efficiency of business processes and decision-making can be improved by integrating OSR, which offers more precise and insightful predictions of outcomes.","We demonstrate the performance of the proposed method on three established datasets.","The results indicate that the proposed model outperforms the baseline methods in accuracy and F1-score."],"url":"http://arxiv.org/abs/2405.05836v1","category":"cs.LG"}
{"created":"2024-05-09 15:13:34","title":"The Riemann hypothesis and dynamics of Backtracking New Q-Newton's method","abstract":"A new variant of Newton's method - named Backtracking New Q-Newton's method (BNQN) - was recently introduced by the second author. This method has good convergence guarantees, specially concerning finding roots of meromorphic functions. This paper explores using BNQN for the Riemann xi function. We show in particular that the Riemann hypothesis is equivalent to that all attractors of BNQN lie on the critical line. We also explain how an apparent relation between the basins of attraction of BNQN and Voronoi's diagram can be helpful for verifying the Riemann hypothesis or finding a counterexample to it. Some illustrating experimental results are included, which convey some interesting phenomena. The experiments show that BNQN works very stably with highly transcendental functions like the Riemann xi function and its derivatives. Based on insights from the experiments, we discuss some concrete steps on using BNQN towards the Riemann hypothesis. Ideas and results from this paper can be extended to other zeta functions.","sentences":["A new variant of Newton's method - named Backtracking New Q-Newton's method (BNQN) - was recently introduced by the second author.","This method has good convergence guarantees, specially concerning finding roots of meromorphic functions.","This paper explores using BNQN for the Riemann xi function.","We show in particular that the Riemann hypothesis is equivalent to that all attractors of BNQN lie on the critical line.","We also explain how an apparent relation between the basins of attraction of BNQN and Voronoi's diagram can be helpful for verifying the Riemann hypothesis or finding a counterexample to it.","Some illustrating experimental results are included, which convey some interesting phenomena.","The experiments show that BNQN works very stably with highly transcendental functions like the Riemann xi function and its derivatives.","Based on insights from the experiments, we discuss some concrete steps on using BNQN towards the Riemann hypothesis.","Ideas and results from this paper can be extended to other zeta functions."],"url":"http://arxiv.org/abs/2405.05834v1","category":"math.DS"}
{"created":"2024-05-09 15:06:46","title":"Common information in well-mixing graphs and applications to information-theoretic cryptography","abstract":"We study the connection between mixing properties for bipartite graphs and materialization of the mutual information in one-shot settings. We show that mixing properties of a graph imply impossibility to extract the mutual information shared by the ends of an edge randomly sampled in the graph. We apply these impossibility results to some questions motivated by information-theoretic cryptography. In particular, we show that communication complexity of a secret key agreement in one-shot setting is inherently uneven: for some inputs, almost all communication complexity inevitably falls on only one party.","sentences":["We study the connection between mixing properties for bipartite graphs and materialization of the mutual information in one-shot settings.","We show that mixing properties of a graph imply impossibility to extract the mutual information shared by the ends of an edge randomly sampled in the graph.","We apply these impossibility results to some questions motivated by information-theoretic cryptography.","In particular, we show that communication complexity of a secret key agreement in one-shot setting is inherently uneven: for some inputs, almost all communication complexity inevitably falls on only one party."],"url":"http://arxiv.org/abs/2405.05831v1","category":"cs.IT"}
{"created":"2024-05-09 15:02:26","title":"MAD-ICP: It Is All About Matching Data -- Robust and Informed LiDAR Odometry","abstract":"LiDAR odometry is the task of estimating the ego-motion of the sensor from sequential laser scans. This problem has been addressed by the community for more than two decades, and many effective solutions are available nowadays. Most of these systems implicitly rely on assumptions about the operating environment, the sensor used, and motion pattern. When these assumptions are violated, several well-known systems tend to perform poorly. This paper presents a LiDAR odometry system that can overcome these limitations and operate well under different operating conditions while achieving performance comparable with domain-specific methods. Our algorithm follows the well-known ICP paradigm that leverages a PCA-based kd-tree implementation that is used to extract structural information about the clouds being registered and to compute the minimization metric for the alignment. The drift is bound by managing the local map based on the estimated uncertainty of the tracked pose. To benefit the community, we release an open-source C++ anytime real-time implementation.","sentences":["LiDAR odometry is the task of estimating the ego-motion of the sensor from sequential laser scans.","This problem has been addressed by the community for more than two decades, and many effective solutions are available nowadays.","Most of these systems implicitly rely on assumptions about the operating environment, the sensor used, and motion pattern.","When these assumptions are violated, several well-known systems tend to perform poorly.","This paper presents a LiDAR odometry system that can overcome these limitations and operate well under different operating conditions while achieving performance comparable with domain-specific methods.","Our algorithm follows the well-known ICP paradigm that leverages a PCA-based kd-tree implementation that is used to extract structural information about the clouds being registered and to compute the minimization metric for the alignment.","The drift is bound by managing the local map based on the estimated uncertainty of the tracked pose.","To benefit the community, we release an open-source C++","anytime real-time implementation."],"url":"http://arxiv.org/abs/2405.05828v1","category":"cs.RO"}
{"created":"2024-05-09 15:00:06","title":"Robots Can Feel: LLM-based Framework for Robot Ethical Reasoning","abstract":"This paper presents the development of a novel ethical reasoning framework for robots. \"Robots Can Feel\" is the first system for robots that utilizes a combination of logic and human-like emotion simulation to make decisions in morally complex situations akin to humans. The key feature of the approach is the management of the Emotion Weight Coefficient - a customizable parameter to assign the role of emotions in robot decision-making. The system aims to serve as a tool that can equip robots of any form and purpose with ethical behavior close to human standards. Besides the platform, the system is independent of the choice of the base model. During the evaluation, the system was tested on 8 top up-to-date LLMs (Large Language Models). This list included both commercial and open-source models developed by various companies and countries. The research demonstrated that regardless of the model choice, the Emotions Weight Coefficient influences the robot's decision similarly. According to ANOVA analysis, the use of different Emotion Weight Coefficients influenced the final decision in a range of situations, such as in a request for a dietary violation F(4, 35) = 11.2, p = 0.0001 and in an animal compassion situation F(4, 35) = 8.5441, p = 0.0001. A demonstration code repository is provided at: https://github.com/TemaLykov/robots_can_feel","sentences":["This paper presents the development of a novel ethical reasoning framework for robots.","\"Robots Can Feel\" is the first system for robots that utilizes a combination of logic and human-like emotion simulation to make decisions in morally complex situations akin to humans.","The key feature of the approach is the management of the Emotion Weight Coefficient - a customizable parameter to assign the role of emotions in robot decision-making.","The system aims to serve as a tool that can equip robots of any form and purpose with ethical behavior close to human standards.","Besides the platform, the system is independent of the choice of the base model.","During the evaluation, the system was tested on 8 top up-to-date LLMs (Large Language Models).","This list included both commercial and open-source models developed by various companies and countries.","The research demonstrated that regardless of the model choice, the Emotions Weight Coefficient influences the robot's decision similarly.","According to ANOVA analysis, the use of different Emotion Weight Coefficients influenced the final decision in a range of situations, such as in a request for a dietary violation F(4, 35) = 11.2, p = 0.0001 and in an animal compassion situation F(4, 35) = 8.5441, p = 0.0001.","A demonstration code repository is provided at: https://github.com/TemaLykov/robots_can_feel"],"url":"http://arxiv.org/abs/2405.05824v1","category":"cs.RO"}
{"created":"2024-05-09 14:56:49","title":"Fine-grained Analysis and Faster Algorithms for Iteratively Solving Linear Systems","abstract":"While effective in practice, iterative methods for solving large systems of linear equations can be significantly affected by problem-dependent condition number quantities. This makes characterizing their time complexity challenging, particularly when we wish to make comparisons between deterministic and stochastic methods, that may or may not rely on preconditioning and/or fast matrix multiplication. In this work, we consider a fine-grained notion of complexity for iterative linear solvers which we call the spectral tail condition number, $\\kappa_\\ell$, defined as the ratio between the $\\ell$th largest and the smallest singular value of the matrix representing the system.   Concretely, we prove the following main algorithmic result: Given an $n\\times n$ matrix $A$ and a vector $b$, we can find $\\tilde{x}$ such that $\\|A\\tilde{x}-b\\|\\leq\\epsilon\\|b\\|$ in time $\\tilde{O}(\\kappa_\\ell\\cdot n^2\\log 1/\\epsilon)$ for any $\\ell = O(n^{\\frac1{\\omega-1}})=O(n^{0.729})$, where $\\omega \\approx 2.372$ is the current fast matrix multiplication exponent. This guarantee is achieved by Sketch-and-Project with Nesterov's acceleration. Some of the implications of our result, and of the use of $\\kappa_\\ell$, include direct improvement over a fine-grained analysis of the Conjugate Gradient method, suggesting a stronger separation between deterministic and stochastic iterative solvers; and relating the complexity of iterative solvers to the ongoing algorithmic advances in fast matrix multiplication, since the bound on $\\ell$ improves with $\\omega$.   Our main technical contributions are new sharp characterizations for the first and second moments of the random projection matrix that commonly arises in sketching algorithms, building on a combination of techniques from combinatorial sampling via determinantal point processes and Gaussian universality results from random matrix theory.","sentences":["While effective in practice, iterative methods for solving large systems of linear equations can be significantly affected by problem-dependent condition number quantities.","This makes characterizing their time complexity challenging, particularly when we wish to make comparisons between deterministic and stochastic methods, that may or may not rely on preconditioning and/or fast matrix multiplication.","In this work, we consider a fine-grained notion of complexity for iterative linear solvers which we call the spectral tail condition number, $\\kappa_\\ell$, defined as the ratio between the $\\ell$th largest and the smallest singular value of the matrix representing the system.   ","Concretely, we prove the following main algorithmic result: Given an $n\\times n$ matrix $A$ and a vector $b$, we can find $\\tilde{x}$ such that $\\|A\\tilde{x}-b\\|\\leq\\epsilon\\|b\\|$ in time $\\tilde{O}(\\kappa_\\ell\\cdot n^2\\log 1/\\epsilon)$ for any $\\ell = O(n^{\\frac1{\\omega-1}})=O(n^{0.729})$, where $\\omega \\approx 2.372$ is the current fast matrix multiplication exponent.","This guarantee is achieved by Sketch-and-Project with Nesterov's acceleration.","Some of the implications of our result, and of the use of $\\kappa_\\ell$, include direct improvement over a fine-grained analysis of the Conjugate Gradient method, suggesting a stronger separation between deterministic and stochastic iterative solvers; and relating the complexity of iterative solvers to the ongoing algorithmic advances in fast matrix multiplication, since the bound on $\\ell$ improves with $\\omega$.   Our main technical contributions are new sharp characterizations for the first and second moments of the random projection matrix that commonly arises in sketching algorithms, building on a combination of techniques from combinatorial sampling via determinantal point processes and Gaussian universality results from random matrix theory."],"url":"http://arxiv.org/abs/2405.05818v1","category":"cs.DS"}
{"created":"2024-05-09 14:55:30","title":"Time-dependent long-term hydrodynamic simulations of the inner protoplanetary disk III: The influence of photoevaporation","abstract":"The final stages of a protoplanetary disk are essential for our understanding of the formation and evolution of planets. Photoevaporation is an important mechanism that contributes to the dispersal of an accretion disk and has significant consequences for the disk's lifetime. However, the combined effects of photoevaporation and star-disk interaction have not been investigated in previous studies. We combined an implicit disk evolution model with a photoevaporative mass-loss profile. By including the innermost disk regions down to 0.01 AU, we could calculate the star-disk interaction, the stellar spin evolution, and the transition from an accreting disk to the propeller regime self-consistently. Starting from an early Class II star-disk system, we calculated the long-term evolution of the system until the disk becomes almost completely dissolved. Photoevaporation has a significant effect on disk structure and evolution. The radial extent of the dead zone decreases, and the number of episodic accretion events (outbursts) is reduced by high stellar X-ray luminosities. Reasonable accretion rates in combination with photoevaporative gaps are possible for a dead zone that is still massive enough to develop episodic accretion events. Furthermore, the stellar spin evolution during the Class II evolution is less affected by the star-disk interaction in the case of high X-ray luminosities. Our results suggest that the formation of planets, especially habitable planets, in the dead zone is strongly impaired in the case of strong X-ray luminosities. Additionally, the importance of the star-disk interaction during the Class II phase with respect to the stellar spin evolution is reduced.","sentences":["The final stages of a protoplanetary disk are essential for our understanding of the formation and evolution of planets.","Photoevaporation is an important mechanism that contributes to the dispersal of an accretion disk and has significant consequences for the disk's lifetime.","However, the combined effects of photoevaporation and star-disk interaction have not been investigated in previous studies.","We combined an implicit disk evolution model with a photoevaporative mass-loss profile.","By including the innermost disk regions down to 0.01 AU, we could calculate the star-disk interaction, the stellar spin evolution, and the transition from an accreting disk to the propeller regime self-consistently.","Starting from an early Class II star-disk system, we calculated the long-term evolution of the system until the disk becomes almost completely dissolved.","Photoevaporation has a significant effect on disk structure and evolution.","The radial extent of the dead zone decreases, and the number of episodic accretion events (outbursts) is reduced by high stellar X-ray luminosities.","Reasonable accretion rates in combination with photoevaporative gaps are possible for a dead zone that is still massive enough to develop episodic accretion events.","Furthermore, the stellar spin evolution during the Class II evolution is less affected by the star-disk interaction in the case of high X-ray luminosities.","Our results suggest that the formation of planets, especially habitable planets, in the dead zone is strongly impaired in the case of strong X-ray luminosities.","Additionally, the importance of the star-disk interaction during the Class II phase with respect to the stellar spin evolution is reduced."],"url":"http://arxiv.org/abs/2405.05816v1","category":"astro-ph.EP"}
{"created":"2024-05-09 14:55:19","title":"Non-myopic GOSPA-driven Gaussian Bernoulli Sensor Management","abstract":"In this paper, we propose an algorithm for non-myopic sensor management for Bernoulli filtering, i.e., when there may be at most one target present in the scene. The algorithm is based on selecting the action that solves a Bellman-type minimisation problem, whose cost function is the mean square generalised optimal sub-pattern assignment (GOSPA) error, over a future time window. We also propose an implementation of the sensor management algorithm based on an upper bound of the mean square GOSPA error and a Gaussian single-target posterior. Finally, we develop a Monte Carlo tree search algorithm to find an approximate optimal action within a given computational budget. The benefits of the proposed approach are demonstrated via simulations.","sentences":["In this paper, we propose an algorithm for non-myopic sensor management for Bernoulli filtering, i.e., when there may be at most one target present in the scene.","The algorithm is based on selecting the action that solves a Bellman-type minimisation problem, whose cost function is the mean square generalised optimal sub-pattern assignment (GOSPA) error, over a future time window.","We also propose an implementation of the sensor management algorithm based on an upper bound of the mean square GOSPA error and a Gaussian single-target posterior.","Finally, we develop a Monte Carlo tree search algorithm to find an approximate optimal action within a given computational budget.","The benefits of the proposed approach are demonstrated via simulations."],"url":"http://arxiv.org/abs/2405.05815v1","category":"eess.SY"}
{"created":"2024-05-09 14:50:07","title":"Parallel Cross Strip Attention Network for Single Image Dehazing","abstract":"The objective of single image dehazing is to restore hazy images and produce clear, high-quality visuals. Traditional convolutional models struggle with long-range dependencies due to their limited receptive field size. While Transformers excel at capturing such dependencies, their quadratic computational complexity in relation to feature map resolution makes them less suitable for pixel-to-pixel dense prediction tasks. Moreover, fixed kernels or tokens in most models do not adapt well to varying blur sizes, resulting in suboptimal dehazing performance. In this study, we introduce a novel dehazing network based on Parallel Stripe Cross Attention (PCSA) with a multi-scale strategy. PCSA efficiently integrates long-range dependencies by simultaneously capturing horizontal and vertical relationships, allowing each pixel to capture contextual cues from an expanded spatial domain. To handle different sizes and shapes of blurs flexibly, We employs a channel-wise design with varying convolutional kernel sizes and strip lengths in each PCSA to capture context information at different scales.Additionally, we incorporate a softmax-based adaptive weighting mechanism within PCSA to prioritize and leverage more critical features.","sentences":["The objective of single image dehazing is to restore hazy images and produce clear, high-quality visuals.","Traditional convolutional models struggle with long-range dependencies due to their limited receptive field size.","While Transformers excel at capturing such dependencies, their quadratic computational complexity in relation to feature map resolution makes them less suitable for pixel-to-pixel dense prediction tasks.","Moreover, fixed kernels or tokens in most models do not adapt well to varying blur sizes, resulting in suboptimal dehazing performance.","In this study, we introduce a novel dehazing network based on Parallel Stripe Cross Attention (PCSA) with a multi-scale strategy.","PCSA efficiently integrates long-range dependencies by simultaneously capturing horizontal and vertical relationships, allowing each pixel to capture contextual cues from an expanded spatial domain.","To handle different sizes and shapes of blurs flexibly, We employs a channel-wise design with varying convolutional kernel sizes and strip lengths in each PCSA to capture context information at different scales.","Additionally, we incorporate a softmax-based adaptive weighting mechanism within PCSA to prioritize and leverage more critical features."],"url":"http://arxiv.org/abs/2405.05811v1","category":"cs.CV"}
{"created":"2024-05-09 14:36:53","title":"3D Positioning using a New Diffraction Path Model","abstract":"Enhancing 3D and Z-axis positioning accuracy is crucial for effective rescue in indoor emergencies, ensuring safety for emergency responders and at-risk individuals. Additionally, reducing the dependence of a positioning system on fixed infrastructure is crucial, given its vulnerability to power failures and damage during emergencies. Further challenges from a signal propagation perspective include poor indoor signal coverage, multipath effects and the problem of Non-Line-OfSight (NLOS) measurement bias. In this study, we utilize the mobility provided by a rapidly deployable Uncrewed Aerial Vehicle (UAV) based wireless network to address these challenges. We recognize diffraction from window edges as a crucial signal propagation mechanism and employ the Geometrical Theory of Diffraction (GTD) to introduce a novel NLOS path length model. Using this path length model, we propose two different techniques to improve the indoor positioning performance for emergency scenarios.","sentences":["Enhancing 3D and Z-axis positioning accuracy is crucial for effective rescue in indoor emergencies, ensuring safety for emergency responders and at-risk individuals.","Additionally, reducing the dependence of a positioning system on fixed infrastructure is crucial, given its vulnerability to power failures and damage during emergencies.","Further challenges from a signal propagation perspective include poor indoor signal coverage, multipath effects and the problem of Non-Line-OfSight (NLOS) measurement bias.","In this study, we utilize the mobility provided by a rapidly deployable Uncrewed Aerial Vehicle (UAV) based wireless network to address these challenges.","We recognize diffraction from window edges as a crucial signal propagation mechanism and employ the Geometrical Theory of Diffraction (GTD) to introduce a novel NLOS path length model.","Using this path length model, we propose two different techniques to improve the indoor positioning performance for emergency scenarios."],"url":"http://arxiv.org/abs/2405.05801v1","category":"eess.SP"}
{"created":"2024-05-09 14:29:23","title":"Adaptability and Homeostasis in the Game of Life interacting with the evolved Cellular Automata","abstract":"In this paper we study the emergence of homeostasis in a two-layer system of the Game of Life, in which the Game of Life in the first layer couples with another system of cellular automata in the second layer. Homeostasis is defined here as a space-time dynamic that regulates the number of cells in state-1 in the Game of Life layer. A genetic algorithm is used to evolve the rules of the second layer to control the pattern of the Game of Life. We discovered that there are two antagonistic attractors that control the numbers of cells in state-1 in the first layer. The homeostasis sustained by these attractors are compared with the homeostatic dynamics observed in Daisy World.","sentences":["In this paper we study the emergence of homeostasis in a two-layer system of the Game of Life, in which the Game of Life in the first layer couples with another system of cellular automata in the second layer.","Homeostasis is defined here as a space-time dynamic that regulates the number of cells in state-1 in the Game of Life layer.","A genetic algorithm is used to evolve the rules of the second layer to control the pattern of the Game of Life.","We discovered that there are two antagonistic attractors that control the numbers of cells in state-1 in the first layer.","The homeostasis sustained by these attractors are compared with the homeostatic dynamics observed in Daisy World."],"url":"http://arxiv.org/abs/2405.05797v1","category":"cs.NE"}
{"created":"2024-05-09 14:17:26","title":"Sequential Amodal Segmentation via Cumulative Occlusion Learning","abstract":"To fully understand the 3D context of a single image, a visual system must be able to segment both the visible and occluded regions of objects, while discerning their occlusion order. Ideally, the system should be able to handle any object and not be restricted to segmenting a limited set of object classes, especially in robotic applications. Addressing this need, we introduce a diffusion model with cumulative occlusion learning designed for sequential amodal segmentation of objects with uncertain categories. This model iteratively refines the prediction using the cumulative mask strategy during diffusion, effectively capturing the uncertainty of invisible regions and adeptly reproducing the complex distribution of shapes and occlusion orders of occluded objects. It is akin to the human capability for amodal perception, i.e., to decipher the spatial ordering among objects and accurately predict complete contours for occluded objects in densely layered visual scenes. Experimental results across three amodal datasets show that our method outperforms established baselines.","sentences":["To fully understand the 3D context of a single image, a visual system must be able to segment both the visible and occluded regions of objects, while discerning their occlusion order.","Ideally, the system should be able to handle any object and not be restricted to segmenting a limited set of object classes, especially in robotic applications.","Addressing this need, we introduce a diffusion model with cumulative occlusion learning designed for sequential amodal segmentation of objects with uncertain categories.","This model iteratively refines the prediction using the cumulative mask strategy during diffusion, effectively capturing the uncertainty of invisible regions and adeptly reproducing the complex distribution of shapes and occlusion orders of occluded objects.","It is akin to the human capability for amodal perception, i.e., to decipher the spatial ordering among objects and accurately predict complete contours for occluded objects in densely layered visual scenes.","Experimental results across three amodal datasets show that our method outperforms established baselines."],"url":"http://arxiv.org/abs/2405.05791v1","category":"cs.CV"}
{"created":"2024-05-09 14:06:40","title":"Quantum Resource Theories beyond Convexity","abstract":"A class of quantum resource theories, based on non-convex star-shape sets, presented in this work captures the key quantum properties that cannot be studied by standard convex theories. We provide operational interpretations for a resource of this class and demonstrate its advantage to improve performance of correlated quantum discrimination tasks and testing of quantum combs. Proposed techniques provide useful tools to describe quantum discord, total correlations in composite quantum systems and to estimate the degree of non-Markovianity of an analyzed quantum dynamics. Other applications include the problem of unistochasticity of a given bistochastic matrix, with relevance for quantization of classical dynamics and studies of violation of CP-symmetry in high energy physics. In all these cases, the non-linear witnesses introduced here outperform the standard linear witnesses. Importance of our findings for quantum information theory is also emphasized.","sentences":["A class of quantum resource theories, based on non-convex star-shape sets, presented in this work captures the key quantum properties that cannot be studied by standard convex theories.","We provide operational interpretations for a resource of this class and demonstrate its advantage to improve performance of correlated quantum discrimination tasks and testing of quantum combs.","Proposed techniques provide useful tools to describe quantum discord, total correlations in composite quantum systems and to estimate the degree of non-Markovianity of an analyzed quantum dynamics.","Other applications include the problem of unistochasticity of a given bistochastic matrix, with relevance for quantization of classical dynamics and studies of violation of CP-symmetry in high energy physics.","In all these cases, the non-linear witnesses introduced here outperform the standard linear witnesses.","Importance of our findings for quantum information theory is also emphasized."],"url":"http://arxiv.org/abs/2405.05785v1","category":"quant-ph"}
{"created":"2024-05-09 14:03:31","title":"Intermediate spectral statistics of rational triangular quantum billiards","abstract":"Triangular billiards whose angles are rational multiples of $\\pi$ are one of the simplest examples of pseudo-integrable models with intriguing classical and quantum properties. We perform an extensive numerical study of spectral statistics of eight quantized rational triangles, six belonging to the family of right-angled Veech triangles and two obtuse rational triangles. Large spectral samples of up to one million energy levels were calculated for each triangle which permits to determine their spectral statistics with great accuracy. It is demonstrated that they are of the intermediate type, sharing some features with chaotic systems, like level repulsion and some with integrable systems, like exponential tails of the level spacing distributions. Another distinctive feature of intermediate spectral statistics is a finite value of the level compressibility.   The short range statistics such as the level spacing distributions, and long-range statistics such as the number variance and spectral form factors were analyzed in detail. An excellent agreement between the numerical data and the model of gamma distributions is revealed.","sentences":["Triangular billiards whose angles are rational multiples of $\\pi$ are one of the simplest examples of pseudo-integrable models with intriguing classical and quantum properties.","We perform an extensive numerical study of spectral statistics of eight quantized rational triangles, six belonging to the family of right-angled Veech triangles and two obtuse rational triangles.","Large spectral samples of up to one million energy levels were calculated for each triangle which permits to determine their spectral statistics with great accuracy.","It is demonstrated that they are of the intermediate type, sharing some features with chaotic systems, like level repulsion and some with integrable systems, like exponential tails of the level spacing distributions.","Another distinctive feature of intermediate spectral statistics is a finite value of the level compressibility.   ","The short range statistics such as the level spacing distributions, and long-range statistics such as the number variance and spectral form factors were analyzed in detail.","An excellent agreement between the numerical data and the model of gamma distributions is revealed."],"url":"http://arxiv.org/abs/2405.05783v1","category":"nlin.CD"}
{"created":"2024-05-09 13:59:46","title":"Minimax problems for ensembles of affine-control systems","abstract":"In this paper, we consider ensembles of affine-control systems in $\\mathbb{R}^n$, and we study simultaneous optimal control problems related to the worst-case minimization. After proving that such problems admit solutions, denoting with $(\\Theta^N)_N$ a sequence of compact sets that parametrize the ensembles of systems, we first show that the corresponding minimax optimal control problems are $\\Gamma$-convergent whenever $(\\Theta^N)_N$ has a limit with respect to the Hausdorff distance. Besides its independent interest, the previous result plays a crucial role for establishing the Pontryagin Maximum Principle (PMP) when the ensemble is parametrized by a set $\\Theta$ consisting of infinitely many points. Namely, we first approximate $\\Theta$ by finite and increasing-in-size sets $(\\Theta^N)_N$ for which the PMP is known, and then we derive the PMP for the $\\Gamma$-limiting problem. The same strategy can be pursued in applications where we can reduce infinite ensembles to finite ones to compute the minimizers numerically.","sentences":["In this paper, we consider ensembles of affine-control systems in $\\mathbb{R}^n$, and we study simultaneous optimal control problems related to the worst-case minimization.","After proving that such problems admit solutions, denoting with $(\\Theta^N)_N$ a sequence of compact sets that parametrize the ensembles of systems, we first show that the corresponding minimax optimal control problems are $\\Gamma$-convergent whenever $(\\Theta^N)_N$ has a limit with respect to the Hausdorff distance.","Besides its independent interest, the previous result plays a crucial role for establishing the Pontryagin Maximum Principle (PMP) when the ensemble is parametrized by a set $\\Theta$ consisting of infinitely many points.","Namely, we first approximate $\\Theta$ by finite and increasing-in-size sets $(\\Theta^N)_N$ for which the PMP is known, and then we derive the PMP for the $\\Gamma$-limiting problem.","The same strategy can be pursued in applications where we can reduce infinite ensembles to finite ones to compute the minimizers numerically."],"url":"http://arxiv.org/abs/2405.05782v1","category":"math.OC"}
{"created":"2024-05-09 13:58:11","title":"Nonparametric estimation of a future entry time distribution given the knowledge of a past state occupation in a progressive multistate model with current status data","abstract":"Case-I interval-censored (current status) data from multistate systems are often encountered in cancer and other epidemiological studies. In this article, we focus on the problem of estimating state entry distribution and occupation probabilities, contingent on a preceding state occupation. This endeavor is particularly complex owing to the inherent challenge of the unavailability of directly observed counts of individuals at risk of transitioning from a state, due to the cross-sectional nature of the data. We propose two nonparametric approaches, one using the fractional at-risk set approach recently adopted in the right-censoring framework and the other a new estimator based on the ratio of marginal state occupation probabilities. Both estimation approaches utilize innovative applications of concepts from the competing risks paradigm. The finite-sample behavior of the proposed estimators is studied via extensive simulation studies where we show that the estimators based on severely censored current status data have good performance when compared with those based on complete data. We demonstrate the application of the two methods to analyze data from patients diagnosed with breast cancer.","sentences":["Case-I interval-censored (current status) data from multistate systems are often encountered in cancer and other epidemiological studies.","In this article, we focus on the problem of estimating state entry distribution and occupation probabilities, contingent on a preceding state occupation.","This endeavor is particularly complex owing to the inherent challenge of the unavailability of directly observed counts of individuals at risk of transitioning from a state, due to the cross-sectional nature of the data.","We propose two nonparametric approaches, one using the fractional at-risk set approach recently adopted in the right-censoring framework and the other a new estimator based on the ratio of marginal state occupation probabilities.","Both estimation approaches utilize innovative applications of concepts from the competing risks paradigm.","The finite-sample behavior of the proposed estimators is studied via extensive simulation studies where we show that the estimators based on severely censored current status data have good performance when compared with those based on complete data.","We demonstrate the application of the two methods to analyze data from patients diagnosed with breast cancer."],"url":"http://arxiv.org/abs/2405.05781v1","category":"stat.ME"}
{"created":"2024-05-09 13:48:11","title":"JADES - The small blue bump in GN-z11: insights into the nuclear region of a galaxy at z=10.6","abstract":"We report the detection of continuum excess in the rest-frame UV between 3000 {\\AA} and 3550 {\\AA} in the JWST/NIRSpec spectrum of GN-z11, a galaxy hosting an active galactic nucleus (AGN) at z = 10.603. The shape of the continuum excess resembles a Balmer continuum but has a break around 3546 {\\AA} in the rest frame, which is 100 {\\AA} bluewards to the Balmer limit at 3646 {\\AA}. A Balmer continuum model alone cannot fit the spectrum, implying a different origin for the continuum excess. The absence of the Balmer jump indicates an electron temperature of $\\sim 3\\times 10^4$ K, which is significantly higher than the temperature of $T_{e}({\\rm O^{2+}}) \\approx 1.3\\times 10^{4}$ K inferred from [OIII]$\\lambda 4363$. The temperature difference must result from mixing of different ionized regions: the Balmer emission mainly arises from dense and hot clouds in the Broad Line Region, close to the accreting black hole, whereas the forbidden lines originate from less dense and colder gas in the host galaxy (although these ionized regions are kinematically similar in GN-z11 due to its small BH mass). We propose a potential explanation for the observed continuum excess to come from a complex of FeII emission, which shows a characteristic jump bluewards to the Balmer limit as previously seen in the spectra of many lower-redshift quasars. Through comparisons with Cloudy models, we show an Fe abundance or an overall metallicity above $\\sim 1/3$ solar is likely needed. Besides the FeII emission, part of the small blue bump might also be associated with an OIII Bowen fluorescent line, a line often enhanced in dense AGN-ionized gas. Finally, the spectrum provides further evidence against Wolf-Rayet or massive stars dominating the nebular emission in GN-z11.","sentences":["We report the detection of continuum excess in the rest-frame UV between 3000 {\\AA} and 3550 {\\AA} in the JWST/NIRSpec spectrum of GN-z11, a galaxy hosting an active galactic nucleus (AGN) at z = 10.603.","The shape of the continuum excess resembles a Balmer continuum but has a break around 3546 {\\AA} in the rest frame, which is 100 {\\AA} bluewards to the Balmer limit at 3646 {\\AA}.","A Balmer continuum model alone cannot fit the spectrum, implying a different origin for the continuum excess.","The absence of the Balmer jump indicates an electron temperature of $\\sim 3\\times 10^4$ K, which is significantly higher than the temperature of $T_{e}({\\rm O^{2+}})","\\approx 1.3\\times 10^{4}$ K inferred from [OIII]$\\lambda 4363$.","The temperature difference must result from mixing of different ionized regions: the Balmer emission mainly arises from dense and hot clouds in the Broad Line Region, close to the accreting black hole, whereas the forbidden lines originate from less dense and colder gas in the host galaxy (although these ionized regions are kinematically similar in GN-z11 due to its small BH mass).","We propose a potential explanation for the observed continuum excess to come from a complex of FeII emission, which shows a characteristic jump bluewards to the Balmer limit as previously seen in the spectra of many lower-redshift quasars.","Through comparisons with Cloudy models, we show an Fe abundance or an overall metallicity above $\\sim 1/3$ solar is likely needed.","Besides the FeII emission, part of the small blue bump might also be associated with an OIII Bowen fluorescent line, a line often enhanced in dense AGN-ionized gas.","Finally, the spectrum provides further evidence against Wolf-Rayet or massive stars dominating the nebular emission in GN-z11."],"url":"http://arxiv.org/abs/2405.05772v1","category":"astro-ph.GA"}
{"created":"2024-05-09 13:39:10","title":"The mechanisms behind extreme susceptibility of photon avalanche emission to quenching","abstract":"The photon avalanche (PA) process that emerges in lanthanide-doped crystals yields a threshold and highly nonlinear (of the power law order > 5) optical response to photoexcitation. PA emission is the outcome of excited-state absorption combined with a cross-relaxation process, which creates positive and efficient energy looping. In consequence, this combination of processes should be highly susceptible to small perturbations in energy distribution and thus can be hindered by other competitive 'parasite' processes such as energy transfer (ET) to quenching sites. Although luminescence quenching is a well-known phenomenon, the exact mechanisms of susceptibility of PA to resonant energy transfer (RET) remain poorly understood limiting practical applications. A deeper understanding of these mechanisms may pave the way to new areas of PA exploitation. This study focuses on the investigation of the LiYF$_{4}$:3%Tm$^{3+}$ PA system co-doped with $Nd^{3+}$ acceptor ions, which was found to impact both the looping and emitting levels and thus to effectively disrupt PA emission, causing an increase in the PA threshold ($I_{th}$) and a decrease in PA nonlinearity ($S_{max}$). Our complementary modelling results revealed that the ET from the looping level increased $I_{th}$ and $S_{max}$, whereas the ET from the emitting level diminished $S_{max}$ and the final emission intensity. Ultimately, significant PA emission quenching demonstrates a high relative sensitivity ($S_{R}$) to infinitesimal amounts of $Nd^{3+}$ acceptors, highlighting the potential for PA to be utilized as an ultra-sensitive, fluorescence-based reporting mechanism that is suitable for the detection and quantification of physical and biological phenomena or reactions.","sentences":["The photon avalanche (PA) process that emerges in lanthanide-doped crystals yields a threshold and highly nonlinear (of the power law order > 5) optical response to photoexcitation.","PA emission is the outcome of excited-state absorption combined with a cross-relaxation process, which creates positive and efficient energy looping.","In consequence, this combination of processes should be highly susceptible to small perturbations in energy distribution and thus can be hindered by other competitive 'parasite' processes such as energy transfer (ET) to quenching sites.","Although luminescence quenching is a well-known phenomenon, the exact mechanisms of susceptibility of PA to resonant energy transfer (RET) remain poorly understood limiting practical applications.","A deeper understanding of these mechanisms may pave the way to new areas of PA exploitation.","This study focuses on the investigation of the LiYF$_{4}$:3%Tm$^{3+}$ PA system co-doped with $Nd^{3+}$ acceptor ions, which was found to impact both the looping and emitting levels and thus to effectively disrupt PA emission, causing an increase in the PA threshold ($I_{th}$) and a decrease in PA nonlinearity ($S_{max}$).","Our complementary modelling results revealed that the ET from the looping level increased $I_{th}$ and $S_{max}$, whereas the ET from the emitting level diminished $S_{max}$ and the final emission intensity.","Ultimately, significant PA emission quenching demonstrates a high relative sensitivity ($S_{R}$) to infinitesimal amounts of $Nd^{3+}$ acceptors, highlighting the potential for PA to be utilized as an ultra-sensitive, fluorescence-based reporting mechanism that is suitable for the detection and quantification of physical and biological phenomena or reactions."],"url":"http://arxiv.org/abs/2405.05764v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 13:18:41","title":"Achieving Precisely-Assigned Performance Requirements for Spacecraft Attitude Control","abstract":"This paper investigates the attitude control problem of spacecraft, with the objective of achieving precise performance criteria including precise settling time, steady-state error, and overshoot elimination. To tackle this challenge, we propose the Precisely-Assigned Performance (PAP) control scheme. Firstly, we utilize a parameterized function to explicitly characterize a reference for the transient responses, termed the Reference Performance Function (RPF). Subsequently, leveraging the concept of the RPF, we define a performance-satisfied tube region and introduce the concept of control barrier functions to derive a sufficient condition for the state trajectory to converge and remain confined within this tube region. By introducing the concept of Sontag's universal formula for stabilization, a PAP controller, constructed based on the backstepping method, is then designed to guide the system to satisfy these affine constraint conditions, and a disturbance observer is further integrated to handle perturbations. Theoretical proofs are presented to demonstrate the controller's capability to establish the boundedness of the overall system and ensure that each state trajectory will converge into the performance-satisfied region within a finite time duration under any conditions. Finally, numerical simulation results are presented to validate the effectiveness of the proposed method.","sentences":["This paper investigates the attitude control problem of spacecraft, with the objective of achieving precise performance criteria including precise settling time, steady-state error, and overshoot elimination.","To tackle this challenge, we propose the Precisely-Assigned Performance (PAP) control scheme.","Firstly, we utilize a parameterized function to explicitly characterize a reference for the transient responses, termed the Reference Performance Function (RPF).","Subsequently, leveraging the concept of the RPF, we define a performance-satisfied tube region and introduce the concept of control barrier functions to derive a sufficient condition for the state trajectory to converge and remain confined within this tube region.","By introducing the concept of Sontag's universal formula for stabilization, a PAP controller, constructed based on the backstepping method, is then designed to guide the system to satisfy these affine constraint conditions, and a disturbance observer is further integrated to handle perturbations.","Theoretical proofs are presented to demonstrate the controller's capability to establish the boundedness of the overall system and ensure that each state trajectory will converge into the performance-satisfied region within a finite time duration under any conditions.","Finally, numerical simulation results are presented to validate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2405.05754v1","category":"eess.SY"}
{"created":"2024-05-09 13:13:10","title":"Cluster statistics of critical Ising and Ashkin-Teller models","abstract":"Motivated by recent progress on the scaling behavior of entanglement entropy, we study the scaling behavior of the number of clusters crossing the boundary between two subsystems for several classical statistical models in two dimension. This number exhibits a subleading logarithmic dependence of the linear dimension of the boundary when the model is at critical, in analogy to the entanglement entropy of a quantum system. It is shown that the logarithmic scaling of the cluster number originates from the conformal invariance of the critical system. We check this numerically for Ising and Ashkin-Teller models by using Monte Carlo simulations, and show that whether a universal coefficient of the logarithmic term can be observed numerically may strongly depend on the geometry and boundary condictions of the system.","sentences":["Motivated by recent progress on the scaling behavior of entanglement entropy, we study the scaling behavior of the number of clusters crossing the boundary between two subsystems for several classical statistical models in two dimension.","This number exhibits a subleading logarithmic dependence of the linear dimension of the boundary when the model is at critical, in analogy to the entanglement entropy of a quantum system.","It is shown that the logarithmic scaling of the cluster number originates from the conformal invariance of the critical system.","We check this numerically for Ising and Ashkin-Teller models by using Monte Carlo simulations, and show that whether a universal coefficient of the logarithmic term can be observed numerically may strongly depend on the geometry and boundary condictions of the system."],"url":"http://arxiv.org/abs/2405.05747v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-09 12:59:11","title":"How Quality Affects Deep Neural Networks in Fine-Grained Image Classification","abstract":"In this paper, we propose a No-Reference Image Quality Assessment (NRIQA) guided cut-off point selection (CPS) strategy to enhance the performance of a fine-grained classification system. Scores given by existing NRIQA methods on the same image may vary and not be as independent of natural image augmentations as expected, which weakens their connection and explainability to fine-grained image classification. Taking the three most commonly adopted image augmentation configurations -- cropping, rotating, and blurring -- as the entry point, we formulate a two-step mechanism for selecting the most discriminative subset from a given image dataset by considering both the confidence of model predictions and the density distribution of image qualities over several NRIQA methods. Concretely, the cut-off points yielded by those methods are aggregated via majority voting to inform the process of image subset selection. The efficacy and efficiency of such a mechanism have been confirmed by comparing the models being trained on high-quality images against a combination of high- and low-quality ones, with a range of 0.7% to 4.2% improvement on a commercial product dataset in terms of mean accuracy through four deep neural classifiers. The robustness of the mechanism has been proven by the observations that all the selected high-quality images can work jointly with 70% low-quality images with 1.3% of classification precision sacrificed when using ResNet34 in an ablation study.","sentences":["In this paper, we propose a No-Reference Image Quality Assessment (NRIQA) guided cut-off point selection (CPS) strategy to enhance the performance of a fine-grained classification system.","Scores given by existing NRIQA methods on the same image may vary and not be as independent of natural image augmentations as expected, which weakens their connection and explainability to fine-grained image classification.","Taking the three most commonly adopted image augmentation configurations -- cropping, rotating, and blurring -- as the entry point, we formulate a two-step mechanism for selecting the most discriminative subset from a given image dataset by considering both the confidence of model predictions and the density distribution of image qualities over several NRIQA methods.","Concretely, the cut-off points yielded by those methods are aggregated via majority voting to inform the process of image subset selection.","The efficacy and efficiency of such a mechanism have been confirmed by comparing the models being trained on high-quality images against a combination of high- and low-quality ones, with a range of 0.7% to 4.2% improvement on a commercial product dataset in terms of mean accuracy through four deep neural classifiers.","The robustness of the mechanism has been proven by the observations that all the selected high-quality images can work jointly with 70% low-quality images with 1.3% of classification precision sacrificed when using ResNet34 in an ablation study."],"url":"http://arxiv.org/abs/2405.05742v1","category":"cs.CV"}
{"created":"2024-05-09 12:22:13","title":"Bimodal Plasmonic Refractive Index Sensors Based on SU-8 Waveguides","abstract":"Plasmonic refractive index sensors are essential for detecting subtle variations in the ambient environment through surface plasmon interactions. Current efforts utilizing CMOS-compatible, plasmo-photonic Mach-Zehnder interferometers with active power balancing exhibit high sensitivities at the cost of fabrication and measurement complexity. Alternatively, passive bimodal plasmonic interferometers based on SU-8 waveguides present a cost-effective solution with a smaller device footprint, though they currently lack opto-mechanical isolation due to exposed photonic waveguides. In this work, we introduce innovative polymer-core and polymer-cladded bimodal plasmonic refractive index sensors with high refractive index contrast. Our sensors feature an aluminum stripe, a bilayer SU-8 photonic waveguide core, and the experimental optical cladding polymer SX AR LWL 2.0. They achieve a sensitivity of (6300 $\\pm$ 460) nm/RIU (refractive index unit), surpassing both traditional and polymer-based plasmo-photonic sensors. This approach enables integrated, wafer-scale, CMOS-compatible, and low-cost sensors and facilitates plasmonic refractive index sensing platforms for various applications.","sentences":["Plasmonic refractive index sensors are essential for detecting subtle variations in the ambient environment through surface plasmon interactions.","Current efforts utilizing CMOS-compatible, plasmo-photonic Mach-Zehnder interferometers with active power balancing exhibit high sensitivities at the cost of fabrication and measurement complexity.","Alternatively, passive bimodal plasmonic interferometers based on SU-8 waveguides present a cost-effective solution with a smaller device footprint, though they currently lack opto-mechanical isolation due to exposed photonic waveguides.","In this work, we introduce innovative polymer-core and polymer-cladded bimodal plasmonic refractive index sensors with high refractive index contrast.","Our sensors feature an aluminum stripe, a bilayer SU-8 photonic waveguide core, and the experimental optical cladding polymer SX AR LWL 2.0.","They achieve a sensitivity of (6300 $\\pm$ 460) nm/RIU (refractive index unit), surpassing both traditional and polymer-based plasmo-photonic sensors.","This approach enables integrated, wafer-scale, CMOS-compatible, and low-cost sensors and facilitates plasmonic refractive index sensing platforms for various applications."],"url":"http://arxiv.org/abs/2405.05716v1","category":"physics.optics"}
{"created":"2024-05-09 12:21:17","title":"Shifting the ISAC Trade-Off with Fluid Antenna Systems","abstract":"As an emerging antenna technology, a fluid antenna system (FAS) enhances spatial diversity to improve both sensing and communication performance by shifting the active antennas among available ports. In this letter, we study the potential of shifting the integrated sensing and communication (ISAC) trade- off with FAS. We propose the model for FAS-enabled ISAC and jointly optimize the transmit beamforming and port selection of FAS. In particular, we aim to minimize the transmit power, while satisfying both communication and sensing requirements. An efficient iterative algorithm based on sparse optimization, convex approximation, and a penalty approach is developed. The simulation results show that the proposed scheme can attain 33% reductions in transmit power with guaranteed sensing and communication performance, showing the great potential of the fluid antenna for striking a flexible tradeoff between sensing and communication in ISAC systems.","sentences":["As an emerging antenna technology, a fluid antenna system (FAS) enhances spatial diversity to improve both sensing and communication performance by shifting the active antennas among available ports.","In this letter, we study the potential of shifting the integrated sensing and communication (ISAC) trade- off with FAS.","We propose the model for FAS-enabled ISAC and jointly optimize the transmit beamforming and port selection of FAS.","In particular, we aim to minimize the transmit power, while satisfying both communication and sensing requirements.","An efficient iterative algorithm based on sparse optimization, convex approximation, and a penalty approach is developed.","The simulation results show that the proposed scheme can attain 33% reductions in transmit power with guaranteed sensing and communication performance, showing the great potential of the fluid antenna for striking a flexible tradeoff between sensing and communication in ISAC systems."],"url":"http://arxiv.org/abs/2405.05715v1","category":"eess.SP"}
{"created":"2024-05-09 12:15:45","title":"Riemannian Accelerated Zeroth-order Algorithm: Improved Robustness and Lower Query Complexity","abstract":"Optimization problems with access to only zeroth-order information of the objective function on Riemannian manifolds arise in various applications, spanning from statistical learning to robot learning. While various zeroth-order algorithms have been proposed in Euclidean space, they are not inherently designed to handle the challenging constraints imposed by Riemannian manifolds. The proper adaptation of zeroth-order techniques to Riemannian manifolds remained unknown until the pioneering work of \\cite{li2023stochastic}. However, zeroth-order algorithms are widely observed to converge slowly and be unstable in practice. To alleviate these issues, we propose a Riemannian accelerated zeroth-order algorithm with improved robustness. Regarding efficiency, our accelerated algorithm has the function query complexity of $\\mathcal{O}(\\epsilon^{-7/4}d)$ for finding an $\\epsilon$-approximate first-order stationary point. By introducing a small perturbation, it exhibits a function query complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-7/4}d)$ for seeking a second-order stationary point with a high probability, matching state-of-the-art result in Euclidean space. Moreover, we further establish the almost sure convergence in the asymptotic sense through the Stable Manifold Theorem. Regarding robustness, our algorithm requires larger smoothing parameters in the order of $\\tilde{\\mathcal{O}}(\\epsilon^{7/8}d^{-1/2})$, improving the existing result by a factor of $\\tilde{\\mathcal{O}}(\\epsilon^{3/4})$.","sentences":["Optimization problems with access to only zeroth-order information of the objective function on Riemannian manifolds arise in various applications, spanning from statistical learning to robot learning.","While various zeroth-order algorithms have been proposed in Euclidean space, they are not inherently designed to handle the challenging constraints imposed by Riemannian manifolds.","The proper adaptation of zeroth-order techniques to Riemannian manifolds remained unknown until the pioneering work of \\cite{li2023stochastic}.","However, zeroth-order algorithms are widely observed to converge slowly and be unstable in practice.","To alleviate these issues, we propose a Riemannian accelerated zeroth-order algorithm with improved robustness.","Regarding efficiency, our accelerated algorithm has the function query complexity of $\\mathcal{O}(\\epsilon^{-7/4}d)$ for finding an $\\epsilon$-approximate first-order stationary point.","By introducing a small perturbation, it exhibits a function query complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-7/4}d)$ for seeking a second-order stationary point with a high probability, matching state-of-the-art result in Euclidean space.","Moreover, we further establish the almost sure convergence in the asymptotic sense through the Stable Manifold Theorem.","Regarding robustness, our algorithm requires larger smoothing parameters in the order of $\\tilde{\\mathcal{O}}(\\epsilon^{7/8}d^{-1/2})$, improving the existing result by a factor of $\\tilde{\\mathcal{O}}(\\epsilon^{3/4})$."],"url":"http://arxiv.org/abs/2405.05713v1","category":"math.OC"}
{"created":"2024-05-09 12:10:18","title":"On the Capacity of Correlated MIMO Phase-Noise Channels: An Electro-Optic Frequency Comb Example","abstract":"The capacity of a discrete-time multiple-input-multiple-output channel with correlated phase noises is investigated. In particular, the electro-optic frequency comb system is considered, where the phase noise of each channel is a combination of two independent Wiener phase-noise sources. Capacity upper and lower bounds are derived for this channel and are compared with lower bounds obtained by numerically evaluating the achievable information rates using quadrature amplitude modulation constellations. Capacity upper and lower bounds are provided for the high signal-to-noise ratio (SNR) regime. The multiplexing gain (pre-log) is shown to be $M-1$, where $M$ represents the number of channels. A constant gap between the asymptotic upper and lower bounds is observed, which depends on the number of channels $M$. For the specific case of $M=2$, capacity is characterized up to a term that vanishes as the SNR grows large.","sentences":["The capacity of a discrete-time multiple-input-multiple-output channel with correlated phase noises is investigated.","In particular, the electro-optic frequency comb system is considered, where the phase noise of each channel is a combination of two independent Wiener phase-noise sources.","Capacity upper and lower bounds are derived for this channel and are compared with lower bounds obtained by numerically evaluating the achievable information rates using quadrature amplitude modulation constellations.","Capacity upper and lower bounds are provided for the high signal-to-noise ratio (SNR) regime.","The multiplexing gain (pre-log) is shown to be $M-1$, where $M$ represents the number of channels.","A constant gap between the asymptotic upper and lower bounds is observed, which depends on the number of channels $M$. For the specific case of $M=2$, capacity is characterized up to a term that vanishes as the SNR grows large."],"url":"http://arxiv.org/abs/2405.05709v1","category":"cs.IT"}
{"created":"2024-05-09 12:02:58","title":"Unbounded visibility domains: metric estimates and an application","abstract":"We give an explicit lower bound, in terms of the distance from the boundary, for the Kobayashi metric of a certain class of bounded pseudoconvex domains in $\\mathbb{C}^n$ with $\\mathcal{C}^2$-smooth boundary using the regularity theory for the complex Monge--Ampere equation. Using such an estimate, we construct a family of unbounded Kobayashi hyperbolic domains in $\\mathbb{C}^n$ having a certain negative-curvature-type property with respect to the Kobayashi distance. As an application, we prove a Picard-type extension theorem for the latter domains.","sentences":["We give an explicit lower bound, in terms of the distance from the boundary, for the Kobayashi metric of a certain class of bounded pseudoconvex domains in $\\mathbb{C}^n$ with $\\mathcal{C}^2$-smooth boundary using the regularity theory for the complex Monge--Ampere equation.","Using such an estimate, we construct a family of unbounded Kobayashi hyperbolic domains in $\\mathbb{C}^n$ having a certain negative-curvature-type property with respect to the Kobayashi distance.","As an application, we prove a Picard-type extension theorem for the latter domains."],"url":"http://arxiv.org/abs/2405.05704v1","category":"math.CV"}
{"created":"2024-05-09 11:51:00","title":"Investigating entropic dynamics of complicated cavity QED system","abstract":"Various aspects of entropy of a complicated cavity QED system are explored. Atoms are held in optical cavities through optical tweezers and can jump between different cavities through the tunneling effect. The interaction of atom with the cavity results in electronic transitions and the creation and annihilation of photon. Covalent bond and phonon are introduced into the model. The effect of all kinds of interactions on entropy is studied. At the same time, the von Neumann entropy of different subsystems is compared. The results show that by selectively choosing system parameters, the entropic dynamics can be controlled.","sentences":["Various aspects of entropy of a complicated cavity QED system are explored.","Atoms are held in optical cavities through optical tweezers and can jump between different cavities through the tunneling effect.","The interaction of atom with the cavity results in electronic transitions and the creation and annihilation of photon.","Covalent bond and phonon are introduced into the model.","The effect of all kinds of interactions on entropy is studied.","At the same time, the von Neumann entropy of different subsystems is compared.","The results show that by selectively choosing system parameters, the entropic dynamics can be controlled."],"url":"http://arxiv.org/abs/2405.05696v1","category":"quant-ph"}
{"created":"2024-05-09 11:38:35","title":"Magnetic evolution of Cr$_2$Te$_3$ epitaxially grown on graphene with post-growth annealing","abstract":"Two-dimensional and van der Waals ferromagnets are ideal platform to study low dimensional magnetism and proximity effects in van der Waals heterostructures. Their ultimate two dimensional character offers also the opportunity to easily adjust their magnetic properties using strain or electric fields. Among 2D ferromagnets, the Cr$_{1+x}$Te$_2$ compounds with $x$=0-1 are very promising because their magnetic properties depend on the amount of self-intercalated Cr atoms between pure CrTe$_2$ layers and the Curie temperature (T$_C$) can reach room temperature for certain compositions. Here, we investigate the evolution of the composition, structural and magnetic properties of thin Cr$_{1.33}$Te$_2$ (Cr$_2$Te$_3$) films epitaxially grown on graphene upon annealing. We observe a transition above 450{\\deg}C from the Cr$_{1.33}$Te$_2$ phase with perpendicular magnetic anisotropy and a T$_C$ of 180 K to a composition close to Cr$_{1.39}$Te$_2$ with in-plane magnetic anisotropy and a T$_C$ of 240-250 K. This phase remains stable up to 650{\\deg}C above which a pure Cr film starts to form. This work demonstrates the complex interplay between intercalated Cr, lattice parameters and magnetic properties in Cr$_{1+x}$Te$_2$ compounds.","sentences":["Two-dimensional and van der Waals ferromagnets are ideal platform to study low dimensional magnetism and proximity effects in van der Waals heterostructures.","Their ultimate two dimensional character offers also the opportunity to easily adjust their magnetic properties using strain or electric fields.","Among 2D ferromagnets, the Cr$_{1+x}$Te$_2$ compounds with $x$=0-1 are very promising because their magnetic properties depend on the amount of self-intercalated Cr atoms between pure CrTe$_2$ layers and the Curie temperature (T$_C$) can reach room temperature for certain compositions.","Here, we investigate the evolution of the composition, structural and magnetic properties of thin Cr$_{1.33}$Te$_2$ (Cr$_2$Te$_3$) films epitaxially grown on graphene upon annealing.","We observe a transition above 450{\\deg}C from the Cr$_{1.33}$Te$_2$ phase with perpendicular magnetic anisotropy and a T$_C$ of 180 K to a composition close to Cr$_{1.39}$Te$_2$ with in-plane magnetic anisotropy and a T$_C$ of 240-250 K. This phase remains stable up to 650{\\deg}C above which a pure Cr film starts to form.","This work demonstrates the complex interplay between intercalated Cr, lattice parameters and magnetic properties in Cr$_{1+x}$Te$_2$ compounds."],"url":"http://arxiv.org/abs/2405.05689v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 11:38:08","title":"Massive interacting binaries as an enrichment source for multiple populations in star clusters","abstract":"We present a suite of binary evolution models with massive primaries (10 $\\leq$ M$_1$ $\\leq$ 40 M$_\\odot$) and periods and mass ratios chosen such that the systems undergo non-conservative mass transfer while the primaries have helium cores. We track the total mass and chemical composition of the ejecta from these systems. This material shows the abundance signatures of hot hydrogen burning which are needed to explain the abundance patterns seen in multiple populations in massive star clusters. We then calculate the total yield of a population of binary stars with masses, mass ratios, and periods consistent with their distribution in a field population. We show that the overall abundance of this material is enriched in helium, nitrogen, sodium, and aluminum, and depleted in carbon, oxygen, and magnesium, by amounts that are consistent with observations. We also show that such a population of binaries will return approximately 25% of its mass in this ejecta (compared to 4% if all the stars were single), over a characteristic timescale of about 12 Myr. We argue that massive binaries must be seriously considered as a contributor to the source of enriched material needed to explain the multiple populations in massive clusters, since essentially all massive stars are formed in binaries or higher order multiples, massive binaries are primarily formed in clusters, and massive binaries naturally produce material of the right composition.","sentences":["We present a suite of binary evolution models with massive primaries (10 $\\leq$ M$_1$ $\\leq$ 40 M$_\\odot$) and periods and mass ratios chosen such that the systems undergo non-conservative mass transfer while the primaries have helium cores.","We track the total mass and chemical composition of the ejecta from these systems.","This material shows the abundance signatures of hot hydrogen burning which are needed to explain the abundance patterns seen in multiple populations in massive star clusters.","We then calculate the total yield of a population of binary stars with masses, mass ratios, and periods consistent with their distribution in a field population.","We show that the overall abundance of this material is enriched in helium, nitrogen, sodium, and aluminum, and depleted in carbon, oxygen, and magnesium, by amounts that are consistent with observations.","We also show that such a population of binaries will return approximately 25% of its mass in this ejecta (compared to 4% if all the stars were single), over a characteristic timescale of about 12 Myr.","We argue that massive binaries must be seriously considered as a contributor to the source of enriched material needed to explain the multiple populations in massive clusters, since essentially all massive stars are formed in binaries or higher order multiples, massive binaries are primarily formed in clusters, and massive binaries naturally produce material of the right composition."],"url":"http://arxiv.org/abs/2405.05687v1","category":"astro-ph.SR"}
{"created":"2024-05-09 11:18:19","title":"Mapping dissolved carbon in space and time: An experimental technique for the measurement of pH and total carbon concentration in density driven convection of CO$_2$ dissolved in water","abstract":"We present an experimental technique for determining the pH and the total carbon concentration when \\ch{CO2} diffuses and flows in water. The technique employs three different pH indicators, which, when combined with an image analysis technique, provides a dynamic range in pH from 4.0 to 9.5. In contrast to usual techniques in which a single pH indicator is used, the methodology presented allows not only to produce a binary classification (pH larger or smaller than a given threshold) but to access a much more complete continuous spatial distribution of pH and concentration levels in the system. We calibrate the method against benchmark solutions and further demonstrate its potential by measuring the pH and total carbon concentration in a density driven convection (DDC) of carbon-enriched water. The motivation for testing the method in this particular experiment comes from the fact that DDC plays a pivotal role in the efficiency of engineered carbon storage processes. The application of the technique presented here provided a direct window for the analysis of the spatial distribution of captured carbon in the DDC flow.","sentences":["We present an experimental technique for determining the pH and the total carbon concentration when \\ch{CO2} diffuses and flows in water.","The technique employs three different pH indicators, which, when combined with an image analysis technique, provides a dynamic range in pH from 4.0 to 9.5.","In contrast to usual techniques in which a single pH indicator is used, the methodology presented allows not only to produce a binary classification (pH larger or smaller than a given threshold) but to access a much more complete continuous spatial distribution of pH and concentration levels in the system.","We calibrate the method against benchmark solutions and further demonstrate its potential by measuring the pH and total carbon concentration in a density driven convection (DDC) of carbon-enriched water.","The motivation for testing the method in this particular experiment comes from the fact that DDC plays a pivotal role in the efficiency of engineered carbon storage processes.","The application of the technique presented here provided a direct window for the analysis of the spatial distribution of captured carbon in the DDC flow."],"url":"http://arxiv.org/abs/2405.05682v1","category":"physics.flu-dyn"}
{"created":"2024-05-09 11:03:15","title":"Maximum Correntropy Polynomial Chaos Kalman Filter for Underwater Navigation","abstract":"This paper develops an underwater navigation solution that utilizes a strapdown inertial navigation system (SINS) and fuses a set of auxiliary sensors such as an acoustic positioning system, Doppler velocity log, depth meter, attitude meter, and magnetometer to accurately estimate an underwater vessel's position and orientation. The conventional integrated navigation system assumes Gaussian measurement noise, while in reality, the noises are non-Gaussian, particularly contaminated by heavy-tailed impulsive noises. To address this issue, and to fuse the system model with the acquired sensor measurements efficiently, we develop a square root polynomial chaos Kalman filter based on maximum correntropy criteria. The filter is initialized using acoustic beaconing to accurately locate the initial position of the vehicle. The computational complexity of the proposed filter is calculated in terms of flops count. The proposed method is compared with the existing maximum correntropy sigma point filters in terms of estimation accuracy and computational complexity. The simulation results demonstrate an improved accuracy compared to the conventional deterministic sample point filters.","sentences":["This paper develops an underwater navigation solution that utilizes a strapdown inertial navigation system (SINS) and fuses a set of auxiliary sensors such as an acoustic positioning system, Doppler velocity log, depth meter, attitude meter, and magnetometer to accurately estimate an underwater vessel's position and orientation.","The conventional integrated navigation system assumes Gaussian measurement noise, while in reality, the noises are non-Gaussian, particularly contaminated by heavy-tailed impulsive noises.","To address this issue, and to fuse the system model with the acquired sensor measurements efficiently, we develop a square root polynomial chaos Kalman filter based on maximum correntropy criteria.","The filter is initialized using acoustic beaconing to accurately locate the initial position of the vehicle.","The computational complexity of the proposed filter is calculated in terms of flops count.","The proposed method is compared with the existing maximum correntropy sigma point filters in terms of estimation accuracy and computational complexity.","The simulation results demonstrate an improved accuracy compared to the conventional deterministic sample point filters."],"url":"http://arxiv.org/abs/2405.05676v1","category":"eess.SP"}
{"created":"2024-05-09 11:00:18","title":"Dynamical properties of a small heterogeneous chain network of neurons in discrete time","abstract":"We propose a novel nonlinear bidirectionally coupled heterogeneous chain network whose dynamics evolve in discrete time. The backbone of the model is a pair of popular map-based neuron models, the Chialvo and the Rulkov maps. This model is assumed to proximate the intricate dynamical properties of neurons in the widely complex nervous system. The model is first realized via various nonlinear analysis techniques: fixed point analysis, phase portraits, Jacobian matrix, and bifurcation diagrams. We observe the coexistence of chaotic and period-4 attractors. Various codimension-1 and -2 patterns for example saddle-node, period-doubling, Neimark-Sacker, double Neimark-Sacker, flip- and fold-Neimark Sacker, and 1:1 and 1:2 resonance are also explored. Furthermore, the study employs two synchronization measures to quantify how the oscillators in the network behave in tandem with each other over a long number of iterations. Finally, a time series analysis of the model is performed to investigate its complexity in terms of sample entropy.","sentences":["We propose a novel nonlinear bidirectionally coupled heterogeneous chain network whose dynamics evolve in discrete time.","The backbone of the model is a pair of popular map-based neuron models, the Chialvo and the Rulkov maps.","This model is assumed to proximate the intricate dynamical properties of neurons in the widely complex nervous system.","The model is first realized via various nonlinear analysis techniques: fixed point analysis, phase portraits, Jacobian matrix, and bifurcation diagrams.","We observe the coexistence of chaotic and period-4 attractors.","Various codimension-1 and -2 patterns for example saddle-node, period-doubling, Neimark-Sacker, double Neimark-Sacker, flip- and fold-Neimark Sacker, and 1:1 and 1:2 resonance are also explored.","Furthermore, the study employs two synchronization measures to quantify how the oscillators in the network behave in tandem with each other over a long number of iterations.","Finally, a time series analysis of the model is performed to investigate its complexity in terms of sample entropy."],"url":"http://arxiv.org/abs/2405.05675v1","category":"nlin.AO"}
{"created":"2024-05-09 10:49:12","title":"Passive Obstacle Aware Control to Follow Desired Velocities","abstract":"Evaluating and updating the obstacle avoidance velocity for an autonomous robot in real-time ensures robust- ness against noise and disturbances. A passive damping con- troller can obtain the desired motion with a torque-controlled robot, which remains compliant and ensures a safe response to external perturbations. Here, we propose a novel approach for designing the passive control policy. Our algorithm com- plies with obstacle-free zones while transitioning to increased damping near obstacles to ensure collision avoidance. This approach ensures stability across diverse scenarios, effectively mitigating disturbances. Validation on a 7DoF robot arm demonstrates superior collision rejection capabilities compared to the baseline, underlining its practicality for real-world ap- plications. Our obstacle-aware damping controller represents a substantial advancement in secure robot control within complex and uncertain environments.","sentences":["Evaluating and updating the obstacle avoidance velocity for an autonomous robot in real-time ensures robust- ness against noise and disturbances.","A passive damping con- troller can obtain the desired motion with a torque-controlled robot, which remains compliant and ensures a safe response to external perturbations.","Here, we propose a novel approach for designing the passive control policy.","Our algorithm com- plies with obstacle-free zones while transitioning to increased damping near obstacles to ensure collision avoidance.","This approach ensures stability across diverse scenarios, effectively mitigating disturbances.","Validation on a 7DoF robot arm demonstrates superior collision rejection capabilities compared to the baseline, underlining its practicality for real-world ap- plications.","Our obstacle-aware damping controller represents a substantial advancement in secure robot control within complex and uncertain environments."],"url":"http://arxiv.org/abs/2405.05669v1","category":"cs.RO"}
{"created":"2024-05-09 10:24:51","title":"Dynamics of a multilink wheeled vehicle: partial solutions and unbounded speedup","abstract":"A mathematical model featuring the motion of a multilink wheeled vehicle is developed using a nonholonomic model. A detailed analysis of the inertial motion is made. Fixed points of the reduced system are identified, their stability is analyzed, and invariant manifolds are found. For the case of three platforms (links), a phase portrait for motion on an invariant manifold is shown and trajectories of the attachment points of the wheel pairs of the three-link vehicle are presented. In addition, an analysis is made of motion in the case where the leading platform has a rotor whose angular velocity is a periodic function of time. The existence of trajectories for which one of the velocity components increases without bound is established, and the asymptotics for it is found.","sentences":["A mathematical model featuring the motion of a multilink wheeled vehicle is developed using a nonholonomic model.","A detailed analysis of the inertial motion is made.","Fixed points of the reduced system are identified, their stability is analyzed, and invariant manifolds are found.","For the case of three platforms (links), a phase portrait for motion on an invariant manifold is shown and trajectories of the attachment points of the wheel pairs of the three-link vehicle are presented.","In addition, an analysis is made of motion in the case where the leading platform has a rotor whose angular velocity is a periodic function of time.","The existence of trajectories for which one of the velocity components increases without bound is established, and the asymptotics for it is found."],"url":"http://arxiv.org/abs/2405.05661v1","category":"math.DS"}
{"created":"2024-05-09 09:40:56","title":"Outlier-robust Kalman Filtering through Generalised Bayes","abstract":"We derive a novel, provably robust, and closed-form Bayesian update rule for online filtering in state-space models in the presence of outliers and misspecified measurement models. Our method combines generalised Bayesian inference with filtering methods such as the extended and ensemble Kalman filter. We use the former to show robustness and the latter to ensure computational efficiency in the case of nonlinear models. Our method matches or outperforms other robust filtering methods (such as those based on variational Bayes) at a much lower computational cost. We show this empirically on a range of filtering problems with outlier measurements, such as object tracking, state estimation in high-dimensional chaotic systems, and online learning of neural networks.","sentences":["We derive a novel, provably robust, and closed-form Bayesian update rule for online filtering in state-space models in the presence of outliers and misspecified measurement models.","Our method combines generalised Bayesian inference with filtering methods such as the extended and ensemble Kalman filter.","We use the former to show robustness and the latter to ensure computational efficiency in the case of nonlinear models.","Our method matches or outperforms other robust filtering methods (such as those based on variational Bayes) at a much lower computational cost.","We show this empirically on a range of filtering problems with outlier measurements, such as object tracking, state estimation in high-dimensional chaotic systems, and online learning of neural networks."],"url":"http://arxiv.org/abs/2405.05646v1","category":"stat.ML"}
{"created":"2024-05-09 09:40:15","title":"Fractional Payment Transactions: Executing Payment Transactions in Parallel with Less than f+1 Validations","abstract":"We consider the problem of supporting payment transactions in an asynchronous system in which up to $f$ validators are subject to Byzantine failures under the control of an adaptive adversary. It was shown that, in the case of a single owner, this problem can be solved without consensus by using byzantine quorum systems (requiring a quorum of $2f+1$ validations per transaction). Nonetheless, the process of validating transactions remains sequential. For example, if one has a balance of ten coins and intends to make separate payments of two coins each to two distinct recipients, both transactions must undergo processing by a common correct validator. On the other hand, these two transactions are non-conflicting as they do not lead to double spending, allowing in principle for parallel validation. In this paper, we show that it is possible to validate payment transactions in parallel with less than $f$ validations per transaction in an asynchronous system, provided that each transaction spends only a small fraction of a balance. Our solution relies on a novel class of probabilistic quorum systems that we introduce in this paper, termed \\textit{$(k_1,k_2)$-quorum systems}. In the absence of an adaptive adversary, \\textit{$(k_1,k_2)$-quorum systems} can be used to enable concurrent and asynchronous validation of up to $k_1$ transactions while preventing validation of more than $k_2$ transactions. Employing a $(k_1, k_2)$-quorum system, we introduce protocols enabling a payer to validate multiple \\textit{fractional spending} transactions in parallel with less than $f+1$ validations per transaction. Subsequently, the payer reclaims any remaining funds through a fully validated transaction, referred to as a \\textit{settlement} transaction.","sentences":["We consider the problem of supporting payment transactions in an asynchronous system in which up to $f$ validators are subject to Byzantine failures under the control of an adaptive adversary.","It was shown that, in the case of a single owner, this problem can be solved without consensus by using byzantine quorum systems (requiring a quorum of $2f+1$ validations per transaction).","Nonetheless, the process of validating transactions remains sequential.","For example, if one has a balance of ten coins and intends to make separate payments of two coins each to two distinct recipients, both transactions must undergo processing by a common correct validator.","On the other hand, these two transactions are non-conflicting as they do not lead to double spending, allowing in principle for parallel validation.","In this paper, we show that it is possible to validate payment transactions in parallel with less than $f$ validations per transaction in an asynchronous system, provided that each transaction spends only a small fraction of a balance.","Our solution relies on a novel class of probabilistic quorum systems that we introduce in this paper, termed \\textit{$(k_1,k_2)$-quorum systems}.","In the absence of an adaptive adversary, \\textit{$(k_1,k_2)$-quorum systems} can be used to enable concurrent and asynchronous validation of up to $k_1$ transactions while preventing validation of more than $k_2$ transactions.","Employing a $(k_1, k_2)$-quorum system, we introduce protocols enabling a payer to validate multiple \\textit{fractional spending} transactions in parallel with less than $f+1$ validations per transaction.","Subsequently, the payer reclaims any remaining funds through a fully validated transaction, referred to as a \\textit{settlement} transaction."],"url":"http://arxiv.org/abs/2405.05645v1","category":"cs.DC"}
{"created":"2024-05-09 09:29:41","title":"Complex network analysis of cryptocurrency market during crashes","abstract":"This paper identifies the cryptocurrency market crashes and analyses its dynamics using the complex network. We identify three distinct crashes during 2017-20, and the analysis is carried out by dividing the time series into pre-crash, crash, and post-crash periods. Partial correlation based complex network analysis is carried out to study the crashes. Degree density ($\\rho_D$), average path length ($\\bar{l}$), and average clustering coefficient ($\\overline{cc}$) are estimated from these networks. We find that both $\\rho_D$ and $\\overline{cc}$ are smallest during the pre-crash period, and spike during the crash suggesting the network is dense during a crash. Although $\\rho_D$ and $\\overline{cc}$ decrease in the post-crash period, they remain higher than pre-crash levels for the 2017-18 and 2018-19 crashes suggesting a market attempt to return to normalcy. We get $\\bar{l}$ is minimal during the crash period, suggesting a rapid flow of information. A dense network and rapid information flow suggest that during a crash uninformed synchronized panic sell-off happens. However, during the 2019-20 crash, the values of $\\rho_D$, $\\overline{cc}$, and $\\bar{l}$ did not vary significantly, indicating minimal change in dynamics compared to other crashes. The findings of this study may guide investors in making decisions during market crashes.","sentences":["This paper identifies the cryptocurrency market crashes and analyses its dynamics using the complex network.","We identify three distinct crashes during 2017-20, and the analysis is carried out by dividing the time series into pre-crash, crash, and post-crash periods.","Partial correlation based complex network analysis is carried out to study the crashes.","Degree density ($\\rho_D$), average path length ($\\bar{l}$), and average clustering coefficient ($\\overline{cc}$) are estimated from these networks.","We find that both $\\rho_D$ and $\\overline{cc}$ are smallest during the pre-crash period, and spike during the crash suggesting the network is dense during a crash.","Although $\\rho_D$ and $\\overline{cc}$ decrease in the post-crash period, they remain higher than pre-crash levels for the 2017-18 and 2018-19 crashes suggesting a market attempt to return to normalcy.","We get $\\bar{l}$ is minimal during the crash period, suggesting a rapid flow of information.","A dense network and rapid information flow suggest that during a crash uninformed synchronized panic sell-off happens.","However, during the 2019-20 crash, the values of $\\rho_D$, $\\overline{cc}$, and $\\bar{l}$ did not vary significantly, indicating minimal change in dynamics compared to other crashes.","The findings of this study may guide investors in making decisions during market crashes."],"url":"http://arxiv.org/abs/2405.05642v1","category":"q-fin.ST"}
{"created":"2024-05-09 09:29:34","title":"Channel Estimation for Holographic MIMO: Wavenumber-Domain Sparsity Inspired Approaches","abstract":"This paper investigates the sparse channel estimation for holographic multiple-input multiple-output (HMIMO) systems. Given that the wavenumber-domain representation is based on a series of Fourier harmonics that are in essence a series of orthogonal basis functions, a novel wavenumber-domain sparsifying basis is designed to expose the sparsity inherent in HMIMO channels. Furthermore, by harnessing the beneficial sparsity in the wavenumber domain, the sparse estimation of HMIMO channels is structured as a compressed sensing problem, which can be efficiently solved by our proposed wavenumber-domain orthogonal matching pursuit (WD-OMP) algorithm. Finally, numerical results demonstrate that the proposed wavenumber-domain sparsifying basis maintains its detection accuracy regardless of the number of antenna elements and antenna spacing. Additionally, in the case of antenna spacing being much less than half a wavelength, the wavenumber-domain approach remains highly accurate in identifying the significant angular power of HMIMO channels.","sentences":["This paper investigates the sparse channel estimation for holographic multiple-input multiple-output (HMIMO) systems.","Given that the wavenumber-domain representation is based on a series of Fourier harmonics that are in essence a series of orthogonal basis functions, a novel wavenumber-domain sparsifying basis is designed to expose the sparsity inherent in HMIMO channels.","Furthermore, by harnessing the beneficial sparsity in the wavenumber domain, the sparse estimation of HMIMO channels is structured as a compressed sensing problem, which can be efficiently solved by our proposed wavenumber-domain orthogonal matching pursuit (WD-OMP) algorithm.","Finally, numerical results demonstrate that the proposed wavenumber-domain sparsifying basis maintains its detection accuracy regardless of the number of antenna elements and antenna spacing.","Additionally, in the case of antenna spacing being much less than half a wavelength, the wavenumber-domain approach remains highly accurate in identifying the significant angular power of HMIMO channels."],"url":"http://arxiv.org/abs/2405.05641v1","category":"eess.SP"}
{"created":"2024-05-09 09:28:43","title":"Experience and Analysis of Scalable High-Fidelity Computational Fluid Dynamics on Modular Supercomputing Architectures","abstract":"The never-ending computational demand from simulations of turbulence makes computational fluid dynamics (CFD) a prime application use case for current and future exascale systems. High-order finite element methods, such as the spectral element method, have been gaining traction as they offer high performance on both multicore CPUs and modern GPU-based accelerators. In this work, we assess how high-fidelity CFD using the spectral element method can exploit the modular supercomputing architecture at scale through domain partitioning, where the computational domain is split between a Booster module powered by GPUs and a Cluster module with conventional CPU nodes. We investigate several different flow cases and computer systems based on the modular supercomputing architecture (MSA). We observe that for our simulations, the communication overhead and load balancing issues incurred by incorporating different computing architectures are seldom worthwhile, especially when I/O is also considered, but when the simulation at hand requires more than the combined global memory on the GPUs, utilizing additional CPUs to increase the available memory can be fruitful. We support our results with a simple performance model to assess when running across modules might be beneficial. As MSA is becoming more widespread and efforts to increase system utilization are growing more important our results give insight into when and how a monolithic application can utilize and spread out to more than one module and obtain a faster time to solution.","sentences":["The never-ending computational demand from simulations of turbulence makes computational fluid dynamics (CFD) a prime application use case for current and future exascale systems.","High-order finite element methods, such as the spectral element method, have been gaining traction as they offer high performance on both multicore CPUs and modern GPU-based accelerators.","In this work, we assess how high-fidelity CFD using the spectral element method can exploit the modular supercomputing architecture at scale through domain partitioning, where the computational domain is split between a Booster module powered by GPUs and a Cluster module with conventional CPU nodes.","We investigate several different flow cases and computer systems based on the modular supercomputing architecture (MSA).","We observe that for our simulations, the communication overhead and load balancing issues incurred by incorporating different computing architectures are seldom worthwhile, especially when I/O is also considered, but when the simulation at hand requires more than the combined global memory on the GPUs, utilizing additional CPUs to increase the available memory can be fruitful.","We support our results with a simple performance model to assess when running across modules might be beneficial.","As MSA is becoming more widespread and efforts to increase system utilization are growing more important our results give insight into when and how a monolithic application can utilize and spread out to more than one module and obtain a faster time to solution."],"url":"http://arxiv.org/abs/2405.05640v1","category":"cs.DC"}
{"created":"2024-05-09 09:27:28","title":"Supercomputers as a Continous Medium","abstract":"As supercomputers' complexity has grown, the traditional boundaries between processor, memory, network, and accelerators have blurred, making a homogeneous computer model, in which the overall computer system is modeled as a continuous medium with homogeneously distributed computational power, memory, and data movement transfer capabilities, an intriguing and powerful abstraction. By applying a homogeneous computer model to algorithms with a given I/O complexity, we recover from first principles, other discrete computer models, such as the roofline model, parallel computing laws, such as Amdahl's and Gustafson's laws, and phenomenological observations, such as super-linear speedup. One of the homogeneous computer model's distinctive advantages is the capability of directly linking the performance limits of an application to the physical properties of a classical computer system. Applying the homogeneous computer model to supercomputers, such as Frontier, Fugaku, and the Nvidia DGX GH200, shows that applications, such as Conjugate Gradient (CG) and Fast Fourier Transforms (FFT), are rapidly approaching the fundamental classical computational limits, where the performance of even denser systems in terms of compute and memory are fundamentally limited by the speed of light.","sentences":["As supercomputers' complexity has grown, the traditional boundaries between processor, memory, network, and accelerators have blurred, making a homogeneous computer model, in which the overall computer system is modeled as a continuous medium with homogeneously distributed computational power, memory, and data movement transfer capabilities, an intriguing and powerful abstraction.","By applying a homogeneous computer model to algorithms with a given I/O complexity, we recover from first principles, other discrete computer models, such as the roofline model, parallel computing laws, such as Amdahl's and Gustafson's laws, and phenomenological observations, such as super-linear speedup.","One of the homogeneous computer model's distinctive advantages is the capability of directly linking the performance limits of an application to the physical properties of a classical computer system.","Applying the homogeneous computer model to supercomputers, such as Frontier, Fugaku, and the Nvidia DGX GH200, shows that applications, such as Conjugate Gradient (CG) and Fast Fourier Transforms (FFT), are rapidly approaching the fundamental classical computational limits, where the performance of even denser systems in terms of compute and memory are fundamentally limited by the speed of light."],"url":"http://arxiv.org/abs/2405.05639v1","category":"cs.DC"}
{"created":"2024-05-09 09:06:04","title":"Short proofs of Tverberg-type theorems for cell complexes","abstract":"We present short proofs of two Tverberg-type theorems for cell complexes by S. Hasui, D. Kishimoto, M. Takeda, and M. Tsutaya. One of them states that for any prime power $r$, any simplicial sphere $X$ of dimension $(d+1)(r-1)-1$, and any continuous map $f:X\\to\\mathbb R^d$ there are pairwise disjoint faces $\\sigma_1,\\ldots,\\sigma_r$ of $X$ such that $f(\\sigma_1)\\cap\\ldots f(\\sigma_r)\\ne\\emptyset$.","sentences":["We present short proofs of two Tverberg-type theorems for cell complexes by S. Hasui, D. Kishimoto, M. Takeda, and M. Tsutaya.","One of them states that for any prime power $r$, any simplicial sphere $X$ of dimension $(d+1)(r-1)-1$, and any continuous map $f:X\\to\\mathbb R^d$ there are pairwise disjoint faces $\\sigma_1,\\ldots,\\sigma_r$ of $X$ such that $f(\\sigma_1)\\cap\\ldots f(\\sigma_r)\\ne\\emptyset$."],"url":"http://arxiv.org/abs/2405.05629v1","category":"math.GT"}
{"created":"2024-05-09 08:50:17","title":"Onset of Quantum Thermalization in Jahn-Teller model","abstract":"We investigate the onset of quantum thermalization in a system governed by the Jahn-Teller Hamiltonian which describes the interaction between a single spin and two bosonic modes. We find that the Jahn-Teller model exhibits a finite-size quantum phase transition between the normal phase and two types of super-radiant phase when the ratios of spin-level splitting to each of the two bosonic frequencies grow to infinity. We test the prediction of the Eigenstate Thermalization Hypothesis in the Jahn-Teller model. We show that the expectation value of the spin observable quickly approaches its long-time average value. We find that the distance between the diagonal ensemble average and the microcanonical ensemble average of the spin observable decreases with the effective thermodynamic parameter. Furthermore, we show that the mean-time fluctuations of the spin observable are small and are inversely proportional to the effective system dimension.","sentences":["We investigate the onset of quantum thermalization in a system governed by the Jahn-Teller Hamiltonian which describes the interaction between a single spin and two bosonic modes.","We find that the Jahn-Teller model exhibits a finite-size quantum phase transition between the normal phase and two types of super-radiant phase when the ratios of spin-level splitting to each of the two bosonic frequencies grow to infinity.","We test the prediction of the Eigenstate Thermalization Hypothesis in the Jahn-Teller model.","We show that the expectation value of the spin observable quickly approaches its long-time average value.","We find that the distance between the diagonal ensemble average and the microcanonical ensemble average of the spin observable decreases with the effective thermodynamic parameter.","Furthermore, we show that the mean-time fluctuations of the spin observable are small and are inversely proportional to the effective system dimension."],"url":"http://arxiv.org/abs/2405.05624v1","category":"quant-ph"}
{"created":"2024-05-09 08:37:34","title":"Enhancing (quasi-)long-range order in a two-dimensional driven crystal","abstract":"It has been recently shown that 2D systems can exhibit crystalline phases with long-range translational order showcasing a striking violation of the Hohenberg-Mermin-Wagner (HMW) theorem which is valid at equilibrium. This is made possible by athermal driving mechanisms that inject energy into the system without exciting long wavelength modes of the density field. However, as thermal fluctuations are superimposed on the non-equilibrium driving, long-range translational order is inevitably lost. In this paper, we discuss the possibility of exploiting non-equilibrium effects to suppress arbitrarily large density fluctuations even when a global thermal bath is coupled to the system. We introduce a model of a harmonic crystal driven both by a global thermal bath and by a momentum conserving noise, where the typical observables related to density fluctuations and long-range translational order can be analytically derived and put in relation. This model allows us to rationalize the violation of the HMW theorem observed in previous studies through the prediction of large-wavelength phonons which thermalize at a vanishing effective temperature when the global bath is switched off. The conceptual framework introduced through this theory is then applied to numerical simulations of a hard-disk solid in contact with a thermal bath and driven out-of-equilibrium by active collisions. Our numerical analysis demonstrates how varying driving and dissipative parameters can lead to an arbitrary enhancement of the quasi-long-range order in the system regardless of the applied global noise amplitude. Finally, we outline a possible experimental procedure to apply our results to a realistic granular system.","sentences":["It has been recently shown that 2D systems can exhibit crystalline phases with long-range translational order showcasing a striking violation of the Hohenberg-Mermin-Wagner (HMW) theorem which is valid at equilibrium.","This is made possible by athermal driving mechanisms that inject energy into the system without exciting long wavelength modes of the density field.","However, as thermal fluctuations are superimposed on the non-equilibrium driving, long-range translational order is inevitably lost.","In this paper, we discuss the possibility of exploiting non-equilibrium effects to suppress arbitrarily large density fluctuations even when a global thermal bath is coupled to the system.","We introduce a model of a harmonic crystal driven both by a global thermal bath and by a momentum conserving noise, where the typical observables related to density fluctuations and long-range translational order can be analytically derived and put in relation.","This model allows us to rationalize the violation of the HMW theorem observed in previous studies through the prediction of large-wavelength phonons which thermalize at a vanishing effective temperature when the global bath is switched off.","The conceptual framework introduced through this theory is then applied to numerical simulations of a hard-disk solid in contact with a thermal bath and driven out-of-equilibrium by active collisions.","Our numerical analysis demonstrates how varying driving and dissipative parameters can lead to an arbitrary enhancement of the quasi-long-range order in the system regardless of the applied global noise amplitude.","Finally, we outline a possible experimental procedure to apply our results to a realistic granular system."],"url":"http://arxiv.org/abs/2405.05621v1","category":"cond-mat.soft"}
{"created":"2024-05-09 08:17:43","title":"Depth Awakens: A Depth-perceptual Attention Fusion Network for RGB-D Camouflaged Object Detection","abstract":"Camouflaged object detection (COD) presents a persistent challenge in accurately identifying objects that seamlessly blend into their surroundings. However, most existing COD models overlook the fact that visual systems operate within a genuine 3D environment. The scene depth inherent in a single 2D image provides rich spatial clues that can assist in the detection of camouflaged objects. Therefore, we propose a novel depth-perception attention fusion network that leverages the depth map as an auxiliary input to enhance the network's ability to perceive 3D information, which is typically challenging for the human eye to discern from 2D images. The network uses a trident-branch encoder to extract chromatic and depth information and their communications. Recognizing that certain regions of a depth map may not effectively highlight the camouflaged object, we introduce a depth-weighted cross-attention fusion module to dynamically adjust the fusion weights on depth and RGB feature maps. To keep the model simple without compromising effectiveness, we design a straightforward feature aggregation decoder that adaptively fuses the enhanced aggregated features. Experiments demonstrate the significant superiority of our proposed method over other states of the arts, which further validates the contribution of depth information in camouflaged object detection. The code will be available at https://github.com/xinran-liu00/DAF-Net.","sentences":["Camouflaged object detection (COD) presents a persistent challenge in accurately identifying objects that seamlessly blend into their surroundings.","However, most existing COD models overlook the fact that visual systems operate within a genuine 3D environment.","The scene depth inherent in a single 2D image provides rich spatial clues that can assist in the detection of camouflaged objects.","Therefore, we propose a novel depth-perception attention fusion network that leverages the depth map as an auxiliary input to enhance the network's ability to perceive 3D information, which is typically challenging for the human eye to discern from 2D images.","The network uses a trident-branch encoder to extract chromatic and depth information and their communications.","Recognizing that certain regions of a depth map may not effectively highlight the camouflaged object, we introduce a depth-weighted cross-attention fusion module to dynamically adjust the fusion weights on depth and RGB feature maps.","To keep the model simple without compromising effectiveness, we design a straightforward feature aggregation decoder that adaptively fuses the enhanced aggregated features.","Experiments demonstrate the significant superiority of our proposed method over other states of the arts, which further validates the contribution of depth information in camouflaged object detection.","The code will be available at https://github.com/xinran-liu00/DAF-Net."],"url":"http://arxiv.org/abs/2405.05614v1","category":"cs.CV"}
{"created":"2024-05-09 08:15:21","title":"Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM","abstract":"Large language models (LLMs) have achieved remarkable performance in various natural language processing tasks, especially in dialogue systems. However, LLM may also pose security and moral threats, especially in multi round conversations where large models are more easily guided by contextual content, resulting in harmful or biased responses. In this paper, we present a novel method to attack LLMs in multi-turn dialogues, called CoA (Chain of Attack). CoA is a semantic-driven contextual multi-turn attack method that adaptively adjusts the attack policy through contextual feedback and semantic relevance during multi-turn of dialogue with a large model, resulting in the model producing unreasonable or harmful content. We evaluate CoA on different LLMs and datasets, and show that it can effectively expose the vulnerabilities of LLMs, and outperform existing attack methods. Our work provides a new perspective and tool for attacking and defending LLMs, and contributes to the security and ethical assessment of dialogue systems.","sentences":["Large language models (LLMs) have achieved remarkable performance in various natural language processing tasks, especially in dialogue systems.","However, LLM may also pose security and moral threats, especially in multi round conversations where large models are more easily guided by contextual content, resulting in harmful or biased responses.","In this paper, we present a novel method to attack LLMs in multi-turn dialogues, called CoA (Chain of Attack).","CoA is a semantic-driven contextual multi-turn attack method that adaptively adjusts the attack policy through contextual feedback and semantic relevance during multi-turn of dialogue with a large model, resulting in the model producing unreasonable or harmful content.","We evaluate CoA on different LLMs and datasets, and show that it can effectively expose the vulnerabilities of LLMs, and outperform existing attack methods.","Our work provides a new perspective and tool for attacking and defending LLMs, and contributes to the security and ethical assessment of dialogue systems."],"url":"http://arxiv.org/abs/2405.05610v1","category":"cs.CL"}
{"created":"2024-05-09 08:05:24","title":"Homotopy Invariants for gradable finite dimensional algebras","abstract":"We show that for a gradable finite dimensional algebra the perfect complexes and bounded derived category cannot be distinguished by homotopy invariants.","sentences":["We show that for a gradable finite dimensional algebra the perfect complexes and bounded derived category cannot be distinguished by homotopy invariants."],"url":"http://arxiv.org/abs/2405.05609v1","category":"math.KT"}
{"created":"2024-05-09 08:00:38","title":"Electromagnetic observables of open-shell nuclei from coupled-cluster theory","abstract":"We develop a new method to describe electromagnetic observables of open-shell nuclei with two nucleons outside a closed shell. This approach combines the equation-of-motion coupled-cluster method for such systems and the Lorentz integral transform technique, expanding the applicability of coupled-cluster theory for these properties beyond closed-shell nuclei. To validate this new approach, we compute the non-energy-weighted dipole sum rule and the dipole polarizability of $^{16,24}$O in both the closed-shell and the new equation-of-motion coupled-cluster frameworks, finding agreement within error bars. We then analyze the evolution of the dipole polarizability along the oxygen and calcium isotopic chains. Our predictions agree well with available experimental data and other available theoretical calculations for the closed-shell $^{16,22}$O and the open-shell $^{18}$O. In the calcium isotopes, we observe that our dipole polarizability predictions for open-shell nuclei are lower than those of closed-shell nuclei. Our predictions for $^{24}$O and $^{54,56}$Ca will motivate future experimental studies at the dripline.","sentences":["We develop a new method to describe electromagnetic observables of open-shell nuclei with two nucleons outside a closed shell.","This approach combines the equation-of-motion coupled-cluster method for such systems and the Lorentz integral transform technique, expanding the applicability of coupled-cluster theory for these properties beyond closed-shell nuclei.","To validate this new approach, we compute the non-energy-weighted dipole sum rule and the dipole polarizability of $^{16,24}$O in both the closed-shell and the new equation-of-motion coupled-cluster frameworks, finding agreement within error bars.","We then analyze the evolution of the dipole polarizability along the oxygen and calcium isotopic chains.","Our predictions agree well with available experimental data and other available theoretical calculations for the closed-shell $^{16,22}$O and the open-shell $^{18}$O.","In the calcium isotopes, we observe that our dipole polarizability predictions for open-shell nuclei are lower than those of closed-shell nuclei.","Our predictions for $^{24}$O and $^{54,56}$Ca will motivate future experimental studies at the dripline."],"url":"http://arxiv.org/abs/2405.05608v1","category":"nucl-th"}
{"created":"2024-05-09 07:38:55","title":"Homogenization in 3D thin domains with oscillating boundaries of different orders","abstract":"This paper presents an extension of the unfolding operator technique, initially applied to two-dimensional domains, to the realm of three-dimensional thin domains. The advancement of this methodology is pivotal, as it enhances our understanding and analysis of three-dimensional geometries, which are crucial in various practical fields such as engineering and physics. Our work delves into the asymptotic behavior of solutions to a reaction-diffusion equation with Neumann boundary conditions set within such a oscillatory 3-dimensional thin domain. The method introduced enables the deduction of effective problems across all scenarios, tackling the intrinsic complexity of these domains. This complexity is especially pronounced due to the possibility of diverse types of oscillations occurring along their boundaries.","sentences":["This paper presents an extension of the unfolding operator technique, initially applied to two-dimensional domains, to the realm of three-dimensional thin domains.","The advancement of this methodology is pivotal, as it enhances our understanding and analysis of three-dimensional geometries, which are crucial in various practical fields such as engineering and physics.","Our work delves into the asymptotic behavior of solutions to a reaction-diffusion equation with Neumann boundary conditions set within such a oscillatory 3-dimensional thin domain.","The method introduced enables the deduction of effective problems across all scenarios, tackling the intrinsic complexity of these domains.","This complexity is especially pronounced due to the possibility of diverse types of oscillations occurring along their boundaries."],"url":"http://arxiv.org/abs/2405.05599v1","category":"math.AP"}
{"created":"2024-05-09 07:32:55","title":"Periodic Solutions in a Simple Delay Differential Equation","abstract":"Simple form scalar differential equation with delay and nonlinear negative periodic feedback is considered. The existence of several types of slowly oscillating periodic solutions is shown with the same and double periods of the feedback coefficient. The periodic solutions are built explicitly in the case of piecewise constant nonlinearities involved. The periodic dynamics are shown to persist under small perturbations of the equation which make it smooth. The theoretical results are verified by extensive numerical simulations.","sentences":["Simple form scalar differential equation with delay and nonlinear negative periodic feedback is considered.","The existence of several types of slowly oscillating periodic solutions is shown with the same and double periods of the feedback coefficient.","The periodic solutions are built explicitly in the case of piecewise constant nonlinearities involved.","The periodic dynamics are shown to persist under small perturbations of the equation which make it smooth.","The theoretical results are verified by extensive numerical simulations."],"url":"http://arxiv.org/abs/2405.05593v1","category":"math.DS"}
{"created":"2024-05-09 07:24:36","title":"Rotation Initialization and Stepwise Refinement for Universal LiDAR Calibration","abstract":"Autonomous systems often employ multiple LiDARs to leverage the integrated advantages, enhancing perception and robustness. The most critical prerequisite under this setting is the estimating the extrinsic between each LiDAR, i.e., calibration. Despite the exciting progress in multi-LiDAR calibration efforts, a universal, sensor-agnostic calibration method remains elusive. According to the coarse-to-fine framework, we first design a spherical descriptor TERRA for 3-DoF rotation initialization with no prior knowledge. To further optimize, we present JEEP for the joint estimation of extrinsic and pose, integrating geometric and motion information to overcome factors affecting the point cloud registration. Finally, the LiDAR poses optimized by the hierarchical optimization module are input to time syn- chronization module to produce the ultimate calibration results, including the time offset. To verify the effectiveness, we conduct extensive experiments on eight datasets, where 16 diverse types of LiDARs in total and dozens of calibration tasks are tested. In the challenging tasks, the calibration errors can still be controlled within 5cm and 1{\\deg} with a high success rate.","sentences":["Autonomous systems often employ multiple LiDARs to leverage the integrated advantages, enhancing perception and robustness.","The most critical prerequisite under this setting is the estimating the extrinsic between each LiDAR, i.e., calibration.","Despite the exciting progress in multi-LiDAR calibration efforts, a universal, sensor-agnostic calibration method remains elusive.","According to the coarse-to-fine framework, we first design a spherical descriptor TERRA for 3-DoF rotation initialization with no prior knowledge.","To further optimize, we present JEEP for the joint estimation of extrinsic and pose, integrating geometric and motion information to overcome factors affecting the point cloud registration.","Finally, the LiDAR poses optimized by the hierarchical optimization module are input to time syn- chronization module to produce the ultimate calibration results, including the time offset.","To verify the effectiveness, we conduct extensive experiments on eight datasets, where 16 diverse types of LiDARs in total and dozens of calibration tasks are tested.","In the challenging tasks, the calibration errors can still be controlled within 5cm and 1{\\deg} with a high success rate."],"url":"http://arxiv.org/abs/2405.05589v1","category":"cs.RO"}
{"created":"2024-05-09 06:52:24","title":"LayerPlexRank: Exploring Node Centrality and Layer Influence through Algebraic Connectivity in Multiplex Networks","abstract":"As the calculation of centrality in complex networks becomes increasingly vital across technological, biological, and social systems, precise and scalable ranking methods are essential for understanding these networks. This paper introduces LayerPlexRank, an algorithm that simultaneously assesses node centrality and layer influence in multiplex networks using algebraic connectivity metrics. This method enhances the robustness of the ranking algorithm by effectively assessing structural changes across layers using random walk, considering the overall connectivity of the graph. We substantiate the utility of LayerPlexRank with theoretical analyses and empirical validations on varied real-world datasets, contrasting it with established centrality measures.","sentences":["As the calculation of centrality in complex networks becomes increasingly vital across technological, biological, and social systems, precise and scalable ranking methods are essential for understanding these networks.","This paper introduces LayerPlexRank, an algorithm that simultaneously assesses node centrality and layer influence in multiplex networks using algebraic connectivity metrics.","This method enhances the robustness of the ranking algorithm by effectively assessing structural changes across layers using random walk, considering the overall connectivity of the graph.","We substantiate the utility of LayerPlexRank with theoretical analyses and empirical validations on varied real-world datasets, contrasting it with established centrality measures."],"url":"http://arxiv.org/abs/2405.05576v1","category":"cs.SI"}
{"created":"2024-05-09 06:48:42","title":"Vision-Language Modeling with Regularized Spatial Transformer Networks for All Weather Crosswind Landing of Aircraft","abstract":"The intrinsic capability to perceive depth of field and extract salient information by the Human Vision System (HVS) stimulates a pilot to perform manual landing over an autoland approach. However, harsh weather creates visibility hindrances, and a pilot must have a clear view of runway elements before the minimum decision altitude. To help a pilot in manual landing, a vision-based system tailored to localize runway elements likewise gets affected, especially during crosswind due to the projective distortion of aircraft camera images. To combat this, we propose to integrate a prompt-based climatic diffusion network with a weather distillation model using a novel diffusion-distillation loss. Precisely, the diffusion model synthesizes climatic-conditioned landing images, and the weather distillation model learns inverse mapping by clearing those visual degradations. Then, to tackle the crosswind landing scenario, a novel Regularized Spatial Transformer Networks (RuSTaN) learns to accurately calibrate for projective distortion using self-supervised learning, which minimizes localization error by the downstream runway object detector. Finally, we have simulated a clear-day landing scenario at the busiest airport globally to curate an image-based Aircraft Landing Dataset (AIRLAD) and experimentally validated our contributions using this dataset to benchmark the performance.","sentences":["The intrinsic capability to perceive depth of field and extract salient information by the Human Vision System (HVS) stimulates a pilot to perform manual landing over an autoland approach.","However, harsh weather creates visibility hindrances, and a pilot must have a clear view of runway elements before the minimum decision altitude.","To help a pilot in manual landing, a vision-based system tailored to localize runway elements likewise gets affected, especially during crosswind due to the projective distortion of aircraft camera images.","To combat this, we propose to integrate a prompt-based climatic diffusion network with a weather distillation model using a novel diffusion-distillation loss.","Precisely, the diffusion model synthesizes climatic-conditioned landing images, and the weather distillation model learns inverse mapping by clearing those visual degradations.","Then, to tackle the crosswind landing scenario, a novel Regularized Spatial Transformer Networks (RuSTaN) learns to accurately calibrate for projective distortion using self-supervised learning, which minimizes localization error by the downstream runway object detector.","Finally, we have simulated a clear-day landing scenario at the busiest airport globally to curate an image-based Aircraft Landing Dataset (AIRLAD) and experimentally validated our contributions using this dataset to benchmark the performance."],"url":"http://arxiv.org/abs/2405.05574v1","category":"cs.CV"}
{"created":"2024-05-09 06:11:58","title":"Perfect Subset Privacy in Polynomial Computation","abstract":"Delegating large-scale computations to service providers is a common practice which raises privacy concerns. This paper studies information-theoretic privacy-preserving delegation of data to a service provider, who may further delegate the computation to auxiliary worker nodes, in order to compute a polynomial over that data at a later point in time. We study techniques which are compatible with robust management of distributed computation systems, an area known as coded computing. Privacy in coded computing, however, has traditionally addressed the problem of colluding workers, and assumed that the server that administrates the computation is trusted. This viewpoint of privacy does not accurately reflect real-world privacy concerns, since normally, the service provider as a whole (i.e., the administrator and the worker nodes) form one cohesive entity which itself poses a privacy risk. This paper aims to shift the focus of privacy in coded computing to safeguarding the privacy of the user against the service provider as a whole, instead of merely against colluding workers inside the service provider. To this end, we leverage the recently defined notion of perfect subset privacy, which guarantees zero information leakage from all subsets of the data up to a certain size. Using known techniques from Reed-Muller decoding, we provide a scheme which enables polynomial computation with perfect subset privacy in straggler-free systems. Furthermore, by studying information super-sets in Reed-Muller codes, which may be of independent interest, we extend the previous scheme to tolerate straggling worker nodes inside the service provider.","sentences":["Delegating large-scale computations to service providers is a common practice which raises privacy concerns.","This paper studies information-theoretic privacy-preserving delegation of data to a service provider, who may further delegate the computation to auxiliary worker nodes, in order to compute a polynomial over that data at a later point in time.","We study techniques which are compatible with robust management of distributed computation systems, an area known as coded computing.","Privacy in coded computing, however, has traditionally addressed the problem of colluding workers, and assumed that the server that administrates the computation is trusted.","This viewpoint of privacy does not accurately reflect real-world privacy concerns, since normally, the service provider as a whole (i.e., the administrator and the worker nodes) form one cohesive entity which itself poses a privacy risk.","This paper aims to shift the focus of privacy in coded computing to safeguarding the privacy of the user against the service provider as a whole, instead of merely against colluding workers inside the service provider.","To this end, we leverage the recently defined notion of perfect subset privacy, which guarantees zero information leakage from all subsets of the data up to a certain size.","Using known techniques from Reed-Muller decoding, we provide a scheme which enables polynomial computation with perfect subset privacy in straggler-free systems.","Furthermore, by studying information super-sets in Reed-Muller codes, which may be of independent interest, we extend the previous scheme to tolerate straggling worker nodes inside the service provider."],"url":"http://arxiv.org/abs/2405.05567v1","category":"cs.IT"}
{"created":"2024-05-09 05:45:18","title":"Review-based Recommender Systems: A Survey of Approaches, Challenges and Future Perspectives","abstract":"Recommender systems play a pivotal role in helping users navigate an overwhelming selection of products and services. On online platforms, users have the opportunity to share feedback in various modes, including numerical ratings, textual reviews, and likes/dislikes. Traditional recommendation systems rely on users explicit ratings or implicit interactions (e.g. likes, clicks, shares, saves) to learn user preferences and item characteristics. Beyond these numerical ratings, textual reviews provide insights into users fine-grained preferences and item features. Analyzing these reviews is crucial for enhancing the performance and interpretability of personalized recommendation results. In recent years, review-based recommender systems have emerged as a significant sub-field in this domain. In this paper, we provide a comprehensive overview of the developments in review-based recommender systems over recent years, highlighting the importance of reviews in recommender systems, as well as the challenges associated with extracting features from reviews and integrating them into ratings. Specifically, we present a categorization of these systems and summarize the state-of-the-art methods, analyzing their unique features, effectiveness, and limitations. Finally, we propose potential directions for future research, including the integration of multi-modal data, multi-criteria rating information, and ethical considerations.","sentences":["Recommender systems play a pivotal role in helping users navigate an overwhelming selection of products and services.","On online platforms, users have the opportunity to share feedback in various modes, including numerical ratings, textual reviews, and likes/dislikes.","Traditional recommendation systems rely on users explicit ratings or implicit interactions (e.g. likes, clicks, shares, saves) to learn user preferences and item characteristics.","Beyond these numerical ratings, textual reviews provide insights into users fine-grained preferences and item features.","Analyzing these reviews is crucial for enhancing the performance and interpretability of personalized recommendation results.","In recent years, review-based recommender systems have emerged as a significant sub-field in this domain.","In this paper, we provide a comprehensive overview of the developments in review-based recommender systems over recent years, highlighting the importance of reviews in recommender systems, as well as the challenges associated with extracting features from reviews and integrating them into ratings.","Specifically, we present a categorization of these systems and summarize the state-of-the-art methods, analyzing their unique features, effectiveness, and limitations.","Finally, we propose potential directions for future research, including the integration of multi-modal data, multi-criteria rating information, and ethical considerations."],"url":"http://arxiv.org/abs/2405.05562v1","category":"cs.IR"}
{"created":"2024-05-09 05:22:18","title":"Bidirectional Progressive Transformer for Interaction Intention Anticipation","abstract":"Interaction intention anticipation aims to jointly predict future hand trajectories and interaction hotspots. Existing research often treated trajectory forecasting and interaction hotspots prediction as separate tasks or solely considered the impact of trajectories on interaction hotspots, which led to the accumulation of prediction errors over time. However, a deeper inherent connection exists between hand trajectories and interaction hotspots, which allows for continuous mutual correction between them. Building upon this relationship, a novel Bidirectional prOgressive Transformer (BOT), which introduces a Bidirectional Progressive mechanism into the anticipation of interaction intention is established. Initially, BOT maximizes the utilization of spatial information from the last observation frame through the Spatial-Temporal Reconstruction Module, mitigating conflicts arising from changes of view in first-person videos. Subsequently, based on two independent prediction branches, a Bidirectional Progressive Enhancement Module is introduced to mutually improve the prediction of hand trajectories and interaction hotspots over time to minimize error accumulation. Finally, acknowledging the intrinsic randomness in human natural behavior, we employ a Trajectory Stochastic Unit and a C-VAE to introduce appropriate uncertainty to trajectories and interaction hotspots, respectively. Our method achieves state-of-the-art results on three benchmark datasets Epic-Kitchens-100, EGO4D, and EGTEA Gaze+, demonstrating superior in complex scenarios.","sentences":["Interaction intention anticipation aims to jointly predict future hand trajectories and interaction hotspots.","Existing research often treated trajectory forecasting and interaction hotspots prediction as separate tasks or solely considered the impact of trajectories on interaction hotspots, which led to the accumulation of prediction errors over time.","However, a deeper inherent connection exists between hand trajectories and interaction hotspots, which allows for continuous mutual correction between them.","Building upon this relationship, a novel Bidirectional prOgressive Transformer (BOT), which introduces a Bidirectional Progressive mechanism into the anticipation of interaction intention is established.","Initially, BOT maximizes the utilization of spatial information from the last observation frame through the Spatial-Temporal Reconstruction Module, mitigating conflicts arising from changes of view in first-person videos.","Subsequently, based on two independent prediction branches, a Bidirectional Progressive Enhancement Module is introduced to mutually improve the prediction of hand trajectories and interaction hotspots over time to minimize error accumulation.","Finally, acknowledging the intrinsic randomness in human natural behavior, we employ a Trajectory Stochastic Unit and a C-VAE to introduce appropriate uncertainty to trajectories and interaction hotspots, respectively.","Our method achieves state-of-the-art results on three benchmark datasets Epic-Kitchens-100, EGO4D, and EGTEA Gaze+, demonstrating superior in complex scenarios."],"url":"http://arxiv.org/abs/2405.05552v1","category":"cs.CV"}
{"created":"2024-05-09 05:21:42","title":"The object detection model uses combined extraction with KNN and RF classification","abstract":"Object detection plays an important role in various fields. Developing detection models for 2D objects that experience rotation and texture variations is a challenge. In this research, the initial stage of the proposed model integrates the gray-level co-occurrence matrix (GLCM) and local binary patterns (LBP) texture feature extraction to obtain feature vectors. The next stage is classifying features using k-nearest neighbors (KNN) and random forest (RF), as well as voting ensemble (VE). System testing used a dataset of 4,437 2D images, the results for KNN accuracy were 92.7% and F1-score 92.5%, while RF performance was lower. Although GLCM features improve performance on both algorithms, KNN is more consistent. The VE approach provides the best performance with an accuracy of 93.9% and an F1 score of 93.8%, this shows the effectiveness of the ensemble technique in increasing object detection accuracy. This study contributes to the field of object detection with a new approach combining GLCM and LBP as feature vectors as well as VE for classification","sentences":["Object detection plays an important role in various fields.","Developing detection models for 2D objects that experience rotation and texture variations is a challenge.","In this research, the initial stage of the proposed model integrates the gray-level co-occurrence matrix (GLCM) and local binary patterns (LBP) texture feature extraction to obtain feature vectors.","The next stage is classifying features using k-nearest neighbors (KNN) and random forest (RF), as well as voting ensemble (VE).","System testing used a dataset of 4,437 2D images, the results for KNN accuracy were 92.7% and F1-score 92.5%, while RF performance was lower.","Although GLCM features improve performance on both algorithms, KNN is more consistent.","The VE approach provides the best performance with an accuracy of 93.9% and an F1 score of 93.8%, this shows the effectiveness of the ensemble technique in increasing object detection accuracy.","This study contributes to the field of object detection with a new approach combining GLCM and LBP as feature vectors as well as VE for classification"],"url":"http://arxiv.org/abs/2405.05551v1","category":"cs.CV"}
{"created":"2024-05-09 05:10:31","title":"2-16 GHz Multifrequency X-Cut Lithium Niobate NEMS Resonators on a Single Chip","abstract":"This work presents the design, fabrication, and testing of X-Cut Lithium Niobate (LN) acoustic nanoelectromechanical (NEMS) Laterally Vibrating Resonators (LVRs) and Degenerate LVRs (d-LVRs) operating in the S0 (YZ30) and SH0 (YZ-10) modes between 2 to 16 GHz range, monolithically fabricated on a single chip. The NEMS topology is optimized to extend the aforementioned fundamental modes in the C-, X-, and Ku-bands while preserving performance and mass manufacturability. The devices present acoustic wavelengths ({\\lambda}) varying between 1800 and 400 nm and are fabricated on a 100 nm ultra-thin LN film on high resistivity silicon with a 3-mask process. Experimental results highlighted quality factor at resonance (Qs) and mechanical quality factors (Qm) as high as 477 and 1750, respectively, and electromechanical coupling (kt2) as high as 32.7%. Large kt2 (>10%) are recorded over a broad range of frequencies (2 - 8 GHz), while Qm exceeding 100 are measured up to 15 GHz. Further enhancement to performance and range of operation on the same chip can be achieved by decreasing {\\lambda}, refining the fabrication process, and optimizing device topology. These additional steps can help pave the way for manufacturing high-performance resonators on a single chip covering the entire 1 - 25 GHz spectrum.","sentences":["This work presents the design, fabrication, and testing of X-Cut Lithium Niobate (LN) acoustic nanoelectromechanical (NEMS)","Laterally Vibrating Resonators (LVRs) and Degenerate LVRs (d-LVRs) operating in the S0 (YZ30) and SH0 (YZ-10) modes between 2 to 16 GHz range, monolithically fabricated on a single chip.","The NEMS topology is optimized to extend the aforementioned fundamental modes in the C-, X-, and Ku-bands while preserving performance and mass manufacturability.","The devices present acoustic wavelengths ({\\lambda}) varying between 1800 and 400 nm and are fabricated on a 100 nm ultra-thin LN film on high resistivity silicon with a 3-mask process.","Experimental results highlighted quality factor at resonance (Qs) and mechanical quality factors (Qm) as high as 477 and 1750, respectively, and electromechanical coupling (kt2) as high as 32.7%.","Large kt2 (>10%) are recorded over a broad range of frequencies (2 - 8 GHz), while Qm exceeding 100 are measured up to 15 GHz.","Further enhancement to performance and range of operation on the same chip can be achieved by decreasing {\\lambda}, refining the fabrication process, and optimizing device topology.","These additional steps can help pave the way for manufacturing high-performance resonators on a single chip covering the entire 1 - 25 GHz spectrum."],"url":"http://arxiv.org/abs/2405.05547v1","category":"eess.SY"}
{"created":"2024-05-09 05:09:37","title":"Data reification in a concurrent rely-guarantee algebra","abstract":"Specifications of significant systems can be made short and perspicuous by using abstract data types; data reification can provide a clear, stepwise, development history of programs that use more efficient concrete representations. Data reification (or \"refinement\") techniques for sequential programs are well established. This paper applies these ideas to concurrency, in particular, an algebraic theory supporting rely-guarantee reasoning about concurrency. A concurrent version of the Galler-Fischer equivalence relation data structure is used as an example.","sentences":["Specifications of significant systems can be made short and perspicuous by using abstract data types; data reification can provide a clear, stepwise, development history of programs that use more efficient concrete representations.","Data reification (or \"refinement\") techniques for sequential programs are well established.","This paper applies these ideas to concurrency, in particular, an algebraic theory supporting rely-guarantee reasoning about concurrency.","A concurrent version of the Galler-Fischer equivalence relation data structure is used as an example."],"url":"http://arxiv.org/abs/2405.05546v1","category":"cs.LO"}
{"created":"2024-05-09 04:39:10","title":"Instability of a dusty shear flow","abstract":"We study the instability of a dusty simple shear flow where the dust particles are distributed non-uniformly. A simple shear flow is modally stable to infinitesimal perturbations. Also, a band of particles remains unaffected in the absence of any background flow. However, we demonstrate that the combined scenario -- comprising a simple shear flow with a localised band of particles -- can exhibit destabilisation due to their two-way interaction. The instability originates solely from the momentum feedback from the particle phase to the fluid phase. Eulerian-Lagrangian simulations are employed to illustrate the existence of this instability. Furthermore, the results are compared with a linear stability analysis of the system using an Eulerian-Eulerian model. Our findings indicate that the instability has an inviscid origin and is characterised by a critical wavelength below which it is not persistent. We have observed that increasing particle inertia dampens the unstable modes, whereas the strength of the instability increases with the strength of the coupling between the fluid and particle phases.","sentences":["We study the instability of a dusty simple shear flow where the dust particles are distributed non-uniformly.","A simple shear flow is modally stable to infinitesimal perturbations.","Also, a band of particles remains unaffected in the absence of any background flow.","However, we demonstrate that the combined scenario -- comprising a simple shear flow with a localised band of particles -- can exhibit destabilisation due to their two-way interaction.","The instability originates solely from the momentum feedback from the particle phase to the fluid phase.","Eulerian-Lagrangian simulations are employed to illustrate the existence of this instability.","Furthermore, the results are compared with a linear stability analysis of the system using an Eulerian-Eulerian model.","Our findings indicate that the instability has an inviscid origin and is characterised by a critical wavelength below which it is not persistent.","We have observed that increasing particle inertia dampens the unstable modes, whereas the strength of the instability increases with the strength of the coupling between the fluid and particle phases."],"url":"http://arxiv.org/abs/2405.05539v1","category":"physics.flu-dyn"}
{"created":"2024-05-09 03:49:54","title":"NurtureNet: A Multi-task Video-based Approach for Newborn Anthropometry","abstract":"Malnutrition among newborns is a top public health concern in developing countries. Identification and subsequent growth monitoring are key to successful interventions. However, this is challenging in rural communities where health systems tend to be inaccessible and under-equipped, with poor adherence to protocol. Our goal is to equip health workers and public health systems with a solution for contactless newborn anthropometry in the community.   We propose NurtureNet, a multi-task model that fuses visual information (a video taken with a low-cost smartphone) with tabular inputs to regress multiple anthropometry estimates including weight, length, head circumference, and chest circumference. We show that visual proxy tasks of segmentation and keypoint prediction further improve performance. We establish the efficacy of the model through several experiments and achieve a relative error of 3.9% and mean absolute error of 114.3 g for weight estimation. Model compression to 15 MB also allows offline deployment to low-cost smartphones.","sentences":["Malnutrition among newborns is a top public health concern in developing countries.","Identification and subsequent growth monitoring are key to successful interventions.","However, this is challenging in rural communities where health systems tend to be inaccessible and under-equipped, with poor adherence to protocol.","Our goal is to equip health workers and public health systems with a solution for contactless newborn anthropometry in the community.   ","We propose NurtureNet, a multi-task model that fuses visual information (a video taken with a low-cost smartphone) with tabular inputs to regress multiple anthropometry estimates including weight, length, head circumference, and chest circumference.","We show that visual proxy tasks of segmentation and keypoint prediction further improve performance.","We establish the efficacy of the model through several experiments and achieve a relative error of 3.9% and mean absolute error of 114.3 g for weight estimation.","Model compression to 15 MB also allows offline deployment to low-cost smartphones."],"url":"http://arxiv.org/abs/2405.05530v1","category":"cs.CV"}
{"created":"2024-05-09 03:48:57","title":"Tomur: Traffic-Aware Performance Prediction of On-NIC Network Functions with Multi-Resource Contention","abstract":"Network function (NF) offloading on SmartNICs has been widely used in modern data centers, offering benefits in host resource saving and programmability. Co-running NFs on the same SmartNICs can cause performance interference due to onboard resource contention. Therefore, to meet performance SLAs while ensuring efficient resource management, operators need mechanisms to predict NF performance under such contention. However, existing solutions lack SmartNIC-specific knowledge and exhibit limited traffic awareness, leading to poor accuracy for on-NIC NFs. This paper proposes Tomur, a novel performance predictive system for on-NIC NFs. Tomur builds upon the key observation that co-located NFs contend for multiple resources, including onboard accelerators and the memory subsystem. It also facilitates traffic awareness according to the behaviors of individual resources to maintain accuracy as the external traffic attributes vary. Evaluation using BlueField-2 SmartNIC shows that Tomur improves the prediction accuracy by 78.8% and reduces SLA violations by 92.2% compared to state-of-the-art approaches, and enables new practical usecases.","sentences":["Network function (NF) offloading on SmartNICs has been widely used in modern data centers, offering benefits in host resource saving and programmability.","Co-running NFs on the same SmartNICs can cause performance interference due to onboard resource contention.","Therefore, to meet performance SLAs while ensuring efficient resource management, operators need mechanisms to predict NF performance under such contention.","However, existing solutions lack SmartNIC-specific knowledge and exhibit limited traffic awareness, leading to poor accuracy for on-NIC NFs.","This paper proposes Tomur, a novel performance predictive system for on-NIC NFs.","Tomur builds upon the key observation that co-located NFs contend for multiple resources, including onboard accelerators and the memory subsystem.","It also facilitates traffic awareness according to the behaviors of individual resources to maintain accuracy as the external traffic attributes vary.","Evaluation using BlueField-2 SmartNIC shows that Tomur improves the prediction accuracy by 78.8% and reduces SLA violations by 92.2% compared to state-of-the-art approaches, and enables new practical usecases."],"url":"http://arxiv.org/abs/2405.05529v1","category":"cs.NI"}
{"created":"2024-05-09 03:19:20","title":"Machine Learning for Scalable and Optimal Load Shedding Under Power System Contingency","abstract":"Prompt and effective corrective actions in response to unexpected contingencies are crucial for improving power system resilience and preventing cascading blackouts. The optimal load shedding (OLS) accounting for network limits has the potential to address the diverse system-wide impacts of contingency scenarios as compared to traditional local schemes. However, due to the fast cascading propagation of initial contingencies, real-time OLS solutions are challenging to attain in large systems with high computation and communication needs. In this paper, we propose a decentralized design that leverages offline training of a neural network (NN) model for individual load centers to autonomously construct the OLS solutions from locally available measurements. Our learning-for-OLS approach can greatly reduce the computation and communication needs during online emergency responses, thus preventing the cascading propagation of contingencies for enhanced power grid resilience. Numerical studies on both the IEEE 118-bus system and a synthetic Texas 2000-bus system have demonstrated the efficiency and effectiveness of our scalable OLS learning design for timely power system emergency operations.","sentences":["Prompt and effective corrective actions in response to unexpected contingencies are crucial for improving power system resilience and preventing cascading blackouts.","The optimal load shedding (OLS) accounting for network limits has the potential to address the diverse system-wide impacts of contingency scenarios as compared to traditional local schemes.","However, due to the fast cascading propagation of initial contingencies, real-time OLS solutions are challenging to attain in large systems with high computation and communication needs.","In this paper, we propose a decentralized design that leverages offline training of a neural network (NN) model for individual load centers to autonomously construct the OLS solutions from locally available measurements.","Our learning-for-OLS approach can greatly reduce the computation and communication needs during online emergency responses, thus preventing the cascading propagation of contingencies for enhanced power grid resilience.","Numerical studies on both the IEEE 118-bus system and a synthetic Texas 2000-bus system have demonstrated the efficiency and effectiveness of our scalable OLS learning design for timely power system emergency operations."],"url":"http://arxiv.org/abs/2405.05521v1","category":"cs.LG"}
{"created":"2024-05-09 02:52:49","title":"Deep Learning Models for Atypical Serotonergic Cells Recognition","abstract":"The serotonergic system modulates brain processes via functionally distinct subpopulations of neurons with heterogeneous properties, including their electrophysiological activity. In extracellular recordings, serotonergic neurons to be investigated for their functional properties are commonly identified on the basis of \"typical\" features of their activity, i.e. slow regular firing and relatively long duration of action potentials. Thus, due to the lack of equally robust criteria for discriminating serotonergic neurons with \"atypical\" features from non-serotonergic cells, the physiological relevance of the diversity of serotonergic neuron activities results largely understudied. We propose deep learning models capable of discriminating typical and atypical serotonergic neurons from non-serotonergic cells with high accuracy. The research utilized electrophysiological in vitro recordings from serotonergic neurons identified by the expression of fluorescent proteins specific to the serotonergic system and non-serotonergic cells. These recordings formed the basis of the training, validation, and testing data for the deep learning models. The study employed convolutional neural networks (CNNs), known for their efficiency in pattern recognition, to classify neurons based on the specific characteristics of their action potentials.","sentences":["The serotonergic system modulates brain processes via functionally distinct subpopulations of neurons with heterogeneous properties, including their electrophysiological activity.","In extracellular recordings, serotonergic neurons to be investigated for their functional properties are commonly identified on the basis of \"typical\" features of their activity, i.e. slow regular firing and relatively long duration of action potentials.","Thus, due to the lack of equally robust criteria for discriminating serotonergic neurons with \"atypical\" features from non-serotonergic cells, the physiological relevance of the diversity of serotonergic neuron activities results largely understudied.","We propose deep learning models capable of discriminating typical and atypical serotonergic neurons from non-serotonergic cells with high accuracy.","The research utilized electrophysiological in vitro recordings from serotonergic neurons identified by the expression of fluorescent proteins specific to the serotonergic system and non-serotonergic cells.","These recordings formed the basis of the training, validation, and testing data for the deep learning models.","The study employed convolutional neural networks (CNNs), known for their efficiency in pattern recognition, to classify neurons based on the specific characteristics of their action potentials."],"url":"http://arxiv.org/abs/2405.05516v1","category":"q-bio.NC"}
{"created":"2024-05-09 02:41:04","title":"Investigating impact of bit-flip errors in control electronics on quantum computation","abstract":"In this paper, we investigate the impact of bit flip errors in FPGA memories in control electronics on quantum computing systems. FPGA memories are integral in storing the amplitude and phase information pulse envelopes, which are essential for generating quantum gate pulses. However, these memories can incur faults due to physical and environmental stressors such as electromagnetic interference, power fluctuations, and temperature variations and adversarial fault injections, potentially leading to errors in quantum gate operations. To understand how these faults affect quantum computations, we conducted a series of experiments to introduce bit flips into the amplitude (both real and imaginary components) and phase values of quantum pulses using IBM's simulated quan- tum environments, FakeValencia, FakeManila, and FakeLima. Our findings reveal that bit flips in the exponent and initial mantissa bits of the real amplitude cause substantial deviations in quantum gate operations, with TVD increases as high as ~200%. Interestingly, the remaining bits exhibited natural tolerance to errors. We proposed a 3-bit repetition error correction code, which effectively reduced the TVD increases to below 40% without incurring any memory overhead. Due to reuse of less significant bits for error correction, the proposed approach introduces maximum of 5-7% extra TVD in nominal cases. However, this can be avoided by sacrificing memory area for implementing the repetition code.","sentences":["In this paper, we investigate the impact of bit flip errors in FPGA memories in control electronics on quantum computing systems.","FPGA memories are integral in storing the amplitude and phase information pulse envelopes, which are essential for generating quantum gate pulses.","However, these memories can incur faults due to physical and environmental stressors such as electromagnetic interference, power fluctuations, and temperature variations and adversarial fault injections, potentially leading to errors in quantum gate operations.","To understand how these faults affect quantum computations, we conducted a series of experiments to introduce bit flips into the amplitude (both real and imaginary components) and phase values of quantum pulses using IBM's simulated quan- tum environments, FakeValencia, FakeManila, and FakeLima.","Our findings reveal that bit flips in the exponent and initial mantissa bits of the real amplitude cause substantial deviations in quantum gate operations, with TVD increases as high as ~200%.","Interestingly, the remaining bits exhibited natural tolerance to errors.","We proposed a 3-bit repetition error correction code, which effectively reduced the TVD increases to below 40% without incurring any memory overhead.","Due to reuse of less significant bits for error correction, the proposed approach introduces maximum of 5-7% extra TVD in nominal cases.","However, this can be avoided by sacrificing memory area for implementing the repetition code."],"url":"http://arxiv.org/abs/2405.05511v1","category":"quant-ph"}
{"created":"2024-05-09 02:25:55","title":"Unveiling Higher-Order Topology via Polarized Topological Charges","abstract":"Real-space topological invariants were widely used to characterize chiral-symmetric higher-order topological phases (HOTPs). However, a momentum-space characterization to these HOTPs, which essentially reveals their intrinsic bulk-boundary correspondence and facilitates their detection in quantum simulation systems, is still lacking. Here, we propose an experimentally observable momentum-space characterization to the chiral-symmetric HOTPs by the concept of polarized topological charges. It makes a unified description to the HOTPs caused by the closing and reopening of band gap not only of the bulk states but also the edge states. Remarkably, these polarized topological charges can also be identified by measuring the pseudospin structures. A feasible scheme to detect the HOTPs in the $^{87}$Rb atomic system is given. Our work opens an avenue for characterization and experimental detection of the HOTPs in momentum space.","sentences":["Real-space topological invariants were widely used to characterize chiral-symmetric higher-order topological phases (HOTPs).","However, a momentum-space characterization to these HOTPs, which essentially reveals their intrinsic bulk-boundary correspondence and facilitates their detection in quantum simulation systems, is still lacking.","Here, we propose an experimentally observable momentum-space characterization to the chiral-symmetric HOTPs by the concept of polarized topological charges.","It makes a unified description to the HOTPs caused by the closing and reopening of band gap not only of the bulk states but also the edge states.","Remarkably, these polarized topological charges can also be identified by measuring the pseudospin structures.","A feasible scheme to detect the HOTPs in the $^{87}$Rb atomic system is given.","Our work opens an avenue for characterization and experimental detection of the HOTPs in momentum space."],"url":"http://arxiv.org/abs/2405.05505v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-09 02:21:44","title":"Communications under Bursty Mixed Gaussian-impulsive Noise: Demodulation and Performance Analysis","abstract":"This is the second part of the two-part paper considering the communications under the bursty mixed noise composed of white Gaussian noise and colored non-Gaussian impulsive noise. In the first part, based on Gaussian distribution and student distribution, we proposed a multivariate bursty mixed noise model and designed model parameter estimation algorithms. However, the performance of a communication system will significantly deteriorate under the bursty mixed noise if a conventional signal detection algorithm with respect to Gaussian noise is applied. To address this issue, in the second part, we leverage the probability density function (PDF) to derive the maximum likelihood (ML) demodulation methods for both linear and nonlinear modulations, including M-array PSK (M-PSK) and MSK modulation schemes. We analyze the theoretical bit error rate (BER) performance of M-PSK and present close-form BER expressions. For the MSK demodulation based on the Viterbi algorithm, we derive a lower and upper bound of BER. Simulation results showcase that the proposed demodulation methods outperform baselines by more than 2.5dB when the BER performance reaches the order of magnitude of $10^{-3}$, and the theoretical analysis matches the simulated results well.","sentences":["This is the second part of the two-part paper considering the communications under the bursty mixed noise composed of white Gaussian noise and colored non-Gaussian impulsive noise.","In the first part, based on Gaussian distribution and student distribution, we proposed a multivariate bursty mixed noise model and designed model parameter estimation algorithms.","However, the performance of a communication system will significantly deteriorate under the bursty mixed noise if a conventional signal detection algorithm with respect to Gaussian noise is applied.","To address this issue, in the second part, we leverage the probability density function (PDF) to derive the maximum likelihood (ML) demodulation methods for both linear and nonlinear modulations, including M-array PSK (M-PSK) and MSK modulation schemes.","We analyze the theoretical bit error rate (BER) performance of M-PSK and present close-form BER expressions.","For the MSK demodulation based on the Viterbi algorithm, we derive a lower and upper bound of BER.","Simulation results showcase that the proposed demodulation methods outperform baselines by more than 2.5dB when the BER performance reaches the order of magnitude of $10^{-3}$, and the theoretical analysis matches the simulated results well."],"url":"http://arxiv.org/abs/2405.05503v1","category":"eess.SP"}
{"created":"2024-05-09 02:16:50","title":"Towards Accurate and Robust Architectures via Neural Architecture Search","abstract":"To defend deep neural networks from adversarial attacks, adversarial training has been drawing increasing attention for its effectiveness. However, the accuracy and robustness resulting from the adversarial training are limited by the architecture, because adversarial training improves accuracy and robustness by adjusting the weight connection affiliated to the architecture. In this work, we propose ARNAS to search for accurate and robust architectures for adversarial training. First we design an accurate and robust search space, in which the placement of the cells and the proportional relationship of the filter numbers are carefully determined. With the design, the architectures can obtain both accuracy and robustness by deploying accurate and robust structures to their sensitive positions, respectively. Then we propose a differentiable multi-objective search strategy, performing gradient descent towards directions that are beneficial for both natural loss and adversarial loss, thus the accuracy and robustness can be guaranteed at the same time. We conduct comprehensive experiments in terms of white-box attacks, black-box attacks, and transferability. Experimental results show that the searched architecture has the strongest robustness with the competitive accuracy, and breaks the traditional idea that NAS-based architectures cannot transfer well to complex tasks in robustness scenarios. By analyzing outstanding architectures searched, we also conclude that accurate and robust neural architectures tend to deploy different structures near the input and output, which has great practical significance on both hand-crafting and automatically designing of accurate and robust architectures.","sentences":["To defend deep neural networks from adversarial attacks, adversarial training has been drawing increasing attention for its effectiveness.","However, the accuracy and robustness resulting from the adversarial training are limited by the architecture, because adversarial training improves accuracy and robustness by adjusting the weight connection affiliated to the architecture.","In this work, we propose ARNAS to search for accurate and robust architectures for adversarial training.","First we design an accurate and robust search space, in which the placement of the cells and the proportional relationship of the filter numbers are carefully determined.","With the design, the architectures can obtain both accuracy and robustness by deploying accurate and robust structures to their sensitive positions, respectively.","Then we propose a differentiable multi-objective search strategy, performing gradient descent towards directions that are beneficial for both natural loss and adversarial loss, thus the accuracy and robustness can be guaranteed at the same time.","We conduct comprehensive experiments in terms of white-box attacks, black-box attacks, and transferability.","Experimental results show that the searched architecture has the strongest robustness with the competitive accuracy, and breaks the traditional idea that NAS-based architectures cannot transfer well to complex tasks in robustness scenarios.","By analyzing outstanding architectures searched, we also conclude that accurate and robust neural architectures tend to deploy different structures near the input and output, which has great practical significance on both hand-crafting and automatically designing of accurate and robust architectures."],"url":"http://arxiv.org/abs/2405.05502v1","category":"cs.CV"}
{"created":"2024-05-09 02:13:51","title":"Implicit Electric Field Conjugation Through a Single-mode Fiber","abstract":"Connecting a coronagraph instrument to a spectrograph via a single-mode optical fiber is a promising technique for characterizing the atmospheres of exoplanets with ground and space-based telescopes. However, due to the small separation and extreme flux ratio between planets and their host stars, instrument sensitivity will be limited by residual starlight leaking into the fiber. To minimize stellar leakage, we must control the electric field at the fiber input. Implicit electric field conjugation (iEFC) is a model-independent wavefront control technique in contrast with classical electric field conjugation (EFC) which requires a detailed optical model of the system. We present here the concept of an iEFC-based wavefront control algorithm to improve stellar rejection through a single-mode fiber. As opposed to image-based iEFC which relies on minimizing intensity in a dark hole region, our approach aims to minimize the amount of residual starlight coupling into a single-mode fiber. We present broadband simulation results demonstrating a normalized intensity greater than 10^{-10} for both fiber-based EFC and iEFC. We find that both control algorithms exhibit similar performance for the low wavefront error (WFE) case, however, iEFC outperforms EFC by approximately 100x in the high WFE regime. Having no need for an optical model, this fiber-based approach offers a promising alternative to EFC for ground and space-based telescope missions, particularly in the presence of residual WFE.","sentences":["Connecting a coronagraph instrument to a spectrograph via a single-mode optical fiber is a promising technique for characterizing the atmospheres of exoplanets with ground and space-based telescopes.","However, due to the small separation and extreme flux ratio between planets and their host stars, instrument sensitivity will be limited by residual starlight leaking into the fiber.","To minimize stellar leakage, we must control the electric field at the fiber input.","Implicit electric field conjugation (iEFC) is a model-independent wavefront control technique in contrast with classical electric field conjugation (EFC) which requires a detailed optical model of the system.","We present here the concept of an iEFC-based wavefront control algorithm to improve stellar rejection through a single-mode fiber.","As opposed to image-based iEFC which relies on minimizing intensity in a dark hole region, our approach aims to minimize the amount of residual starlight coupling into a single-mode fiber.","We present broadband simulation results demonstrating a normalized intensity greater than 10^{-10} for both fiber-based EFC and iEFC.","We find that both control algorithms exhibit similar performance for the low wavefront error (WFE) case, however, iEFC outperforms EFC by approximately 100x in the high WFE regime.","Having no need for an optical model, this fiber-based approach offers a promising alternative to EFC for ground and space-based telescope missions, particularly in the presence of residual WFE."],"url":"http://arxiv.org/abs/2405.05501v1","category":"astro-ph.IM"}
{"created":"2024-05-09 02:03:51","title":"The RoyalFlush Automatic Speech Diarization and Recognition System for In-Car Multi-Channel Automatic Speech Recognition Challenge","abstract":"This paper presents our system submission for the In-Car Multi-Channel Automatic Speech Recognition (ICMC-ASR) Challenge, which focuses on speaker diarization and speech recognition in complex multi-speaker scenarios. To address these challenges, we develop end-to-end speaker diarization models that notably decrease the diarization error rate (DER) by 49.58\\% compared to the official baseline on the development set. For speech recognition, we utilize self-supervised learning representations to train end-to-end ASR models. By integrating these models, we achieve a character error rate (CER) of 16.93\\% on the track 1 evaluation set, and a concatenated minimum permutation character error rate (cpCER) of 25.88\\% on the track 2 evaluation set.","sentences":["This paper presents our system submission for the In-Car Multi-Channel Automatic Speech Recognition (ICMC-ASR) Challenge, which focuses on speaker diarization and speech recognition in complex multi-speaker scenarios.","To address these challenges, we develop end-to-end speaker diarization models that notably decrease the diarization error rate (DER) by 49.58\\% compared to the official baseline on the development set.","For speech recognition, we utilize self-supervised learning representations to train end-to-end ASR models.","By integrating these models, we achieve a character error rate (CER) of 16.93\\% on the track 1 evaluation set, and a concatenated minimum permutation character error rate (cpCER) of 25.88\\% on the track 2 evaluation set."],"url":"http://arxiv.org/abs/2405.05498v1","category":"cs.SD"}
{"created":"2024-05-09 01:51:41","title":"PARSAC: Fast, Human-quality Floorplanning for Modern SoCs with Complex Design Constraints","abstract":"The floorplanning of Systems-on-a-Chip (SoCs) and of chip sub- systems is a crucial step in the physical design flow as it determines the optimal shapes and locations of the blocks that make up the system. Simulated Annealing (SA) has been the method of choice for tackling classical floorplanning problems where the objective is to minimize wire-length and the total placement area. The goal in industry-relevant floorplanning problems, however, is not only to minimize area and wire-length, but to do that while respecting hard placement constraints that specify the general area and/or the specific locations for the placement of some blocks. We show that simply incorporating these constraints into the SA objective function leads to sub-optimal, and often illegal, solutions. We propose the Constraints-Aware Simulated Annealing (CA-SA) method and show that it strongly outperforms vanilla SA in floorplanning problems with hard placement constraints. We developed a new floorplan- ning tool on top of CA-SA: PARSAC (Parallel Simulated Annealing with Constraints). PARSAC is an efficient, easy-to-use, and mas- sively parallel floorplanner. Unlike current SA-based or learning- based floorplanning tools that cannot effectively incorporate hard placement-constraints, PARSAC can quickly construct the Pareto- optimal legal solutions front for constrained floorplanning problems. PARSAC also outperforms traditional SA on legacy floorplanning benchmarks. PARSAC is available as an open-source repository for researchers to replicate and build on our result.","sentences":["The floorplanning of Systems-on-a-Chip (SoCs) and of chip sub- systems is a crucial step in the physical design flow as it determines the optimal shapes and locations of the blocks that make up the system.","Simulated Annealing (SA) has been the method of choice for tackling classical floorplanning problems where the objective is to minimize wire-length and the total placement area.","The goal in industry-relevant floorplanning problems, however, is not only to minimize area and wire-length, but to do that while respecting hard placement constraints that specify the general area and/or the specific locations for the placement of some blocks.","We show that simply incorporating these constraints into the SA objective function leads to sub-optimal, and often illegal, solutions.","We propose the Constraints-Aware Simulated Annealing (CA-SA) method and show that it strongly outperforms vanilla SA in floorplanning problems with hard placement constraints.","We developed a new floorplan- ning tool on top of CA-SA: PARSAC (Parallel Simulated Annealing with Constraints).","PARSAC is an efficient, easy-to-use, and mas- sively parallel floorplanner.","Unlike current SA-based or learning- based floorplanning tools that cannot effectively incorporate hard placement-constraints, PARSAC can quickly construct the Pareto- optimal legal solutions front for constrained floorplanning problems.","PARSAC also outperforms traditional SA on legacy floorplanning benchmarks.","PARSAC is available as an open-source repository for researchers to replicate and build on our result."],"url":"http://arxiv.org/abs/2405.05495v1","category":"cs.OH"}
{"created":"2024-05-09 01:47:07","title":"Localization and bistability of bioconvection in a doubly periodic domain","abstract":"A suspension of swimming microorganisms often generates a large-scale convective pattern known as bioconvection. In contrast to the thermal Rayleigh-Benard system, recent experimental studies report an emergence of steady localized convection patterns and bistability near the onset of instability in bioconvection systems. In this study, to understand the underlying mechanisms and identify the roles of particle self-propulsion in pattern formation, we theoretically and numerically investigate a model bioconvection system in a two-dimensional periodic boundary domain. In doing so, we extend a standard bioconvection model by introducing the equilibrium density profile as an independent parameter, for which the particle self-propulsion is treated as an independent dimensional parameter. Since the large-scale vertical structure dominates in this system, we are able to simplify the model by truncating the higher vertical modes. With this truncated model, we analytically derived the neutrally stable curve and found that the particle motility stabilizes the system. We then numerically analyzed the bifurcation diagram and found the bistable structure at the onset of instability. These findings, localization and bistability, are consistent with experimental observations. We further examined the global structure of the bistable dynamical system and found that the non-trivial unstable steady solution behaves as an edge state that separates the basins of attractors. These results highlight the importance of particle self-propulsion in bioconvection, and more generally our methodology based on the dynamical systems theory is useful in understanding complex flow patterns in nature.","sentences":["A suspension of swimming microorganisms often generates a large-scale convective pattern known as bioconvection.","In contrast to the thermal Rayleigh-Benard system, recent experimental studies report an emergence of steady localized convection patterns and bistability near the onset of instability in bioconvection systems.","In this study, to understand the underlying mechanisms and identify the roles of particle self-propulsion in pattern formation, we theoretically and numerically investigate a model bioconvection system in a two-dimensional periodic boundary domain.","In doing so, we extend a standard bioconvection model by introducing the equilibrium density profile as an independent parameter, for which the particle self-propulsion is treated as an independent dimensional parameter.","Since the large-scale vertical structure dominates in this system, we are able to simplify the model by truncating the higher vertical modes.","With this truncated model, we analytically derived the neutrally stable curve and found that the particle motility stabilizes the system.","We then numerically analyzed the bifurcation diagram and found the bistable structure at the onset of instability.","These findings, localization and bistability, are consistent with experimental observations.","We further examined the global structure of the bistable dynamical system and found that the non-trivial unstable steady solution behaves as an edge state that separates the basins of attractors.","These results highlight the importance of particle self-propulsion in bioconvection, and more generally our methodology based on the dynamical systems theory is useful in understanding complex flow patterns in nature."],"url":"http://arxiv.org/abs/2405.05494v1","category":"physics.flu-dyn"}
{"created":"2024-05-09 01:34:54","title":"Banking Turn of High-DOF Dynamic Morphing Wing Flight by Shifting Structure Response Using Optimization","abstract":"The 3D flight control of a flapping wing robot is a very challenging problem. The robot stabilizes and controls its pose through the aerodynamic forces acting on the wing membrane which has complex dynamics and it is difficult to develop a control method to interact with such a complex system. Bats, in particular, are capable of performing highly agile aerial maneuvers such as tight banking and bounding flight solely using their highly flexible wings. In this work, we develop a control method for a bio-inspired bat robot, the Aerobat, using small low-powered actuators to manipulate the flapping gait and the resulting aerodynamic forces. We implemented a controller based on collocation approach to track a desired roll and perform a banking maneuver to be used in a trajectory tracking controller. This controller is implemented in a simulation to show its performance and feasibility.","sentences":["The 3D flight control of a flapping wing robot is a very challenging problem.","The robot stabilizes and controls its pose through the aerodynamic forces acting on the wing membrane which has complex dynamics and it is difficult to develop a control method to interact with such a complex system.","Bats, in particular, are capable of performing highly agile aerial maneuvers such as tight banking and bounding flight solely using their highly flexible wings.","In this work, we develop a control method for a bio-inspired bat robot, the Aerobat, using small low-powered actuators to manipulate the flapping gait and the resulting aerodynamic forces.","We implemented a controller based on collocation approach to track a desired roll and perform a banking maneuver to be used in a trajectory tracking controller.","This controller is implemented in a simulation to show its performance and feasibility."],"url":"http://arxiv.org/abs/2405.05490v1","category":"cs.RO"}
{"created":"2024-05-09 01:01:11","title":"Critical Mass Phenomena and Blow-up behavior of Ground States in stationary second order Mean-Field Games systems with decreasing cost","abstract":"This paper is devoted to the study of Mean-field Games (MFG) systems in the mass critical exponent case. We firstly establish the optimal Gagliardo-Nirenberg type inequality associated with the potential-free MFG system. Then, under some mild assumptions on the potential function, we show that there exists a critical mass $M^*$ such that the MFG system admits a least energy solution if and only if the total mass of population density $M$ satisfies $M<M^*$. Moreover, the blow-up behavior of energy minimizers are captured as $M\\nearrow M^*$. In particular, given the precise asymptotic expansions of the potential, we establish the refined blow-up behavior of ground states as $M\\nearrow M^*.$ While studying the existence of least energy solutions, we establish new local $W^{2,p}$ estimates of solutions to Hamilton-Jacobi equations with superlinear gradient terms.","sentences":["This paper is devoted to the study of Mean-field Games (MFG) systems in the mass critical exponent case.","We firstly establish the optimal Gagliardo-Nirenberg type inequality associated with the potential-free MFG system.","Then, under some mild assumptions on the potential function, we show that there exists a critical mass $M^*$ such that the MFG system admits a least energy solution if and only if the total mass of population density $M$ satisfies $M<M^*$.","Moreover, the blow-up behavior of energy minimizers are captured as $M\\nearrow M^*$. In particular, given the precise asymptotic expansions of the potential, we establish the refined blow-up behavior of ground states as $M\\nearrow M^*.$ While studying the existence of least energy solutions, we establish new local $W^{2,p}$ estimates of solutions to Hamilton-Jacobi equations with superlinear gradient terms."],"url":"http://arxiv.org/abs/2405.05484v1","category":"math.AP"}
{"created":"2024-05-09 00:41:34","title":"Achieving millisecond coherence fluxonium through overlap Josephson junctions","abstract":"Fluxonium qubits are recognized for their high coherence times and high operation fidelities, attributed to their unique design incorporating over 100 Josephson junctions per superconducting loop. However, this complexity poses significant fabrication challenges, particularly in achieving high yield and junction uniformity with traditional methods. Here, we introduce an overlap process for Josephson junction fabrication that achieves nearly 100% yield and maintains uniformity across a 2-inch wafer with less than 5% variation for the phase slip junction and less than 2% for the junction array. Our compact junction array design facilitates fluxonium qubits with energy relaxation times exceeding 1 millisecond at the flux frustration point, demonstrating consistency with state-of-the-art dielectric loss tangents and flux noise across multiple devices. This work suggests the scalability of high coherence fluxonium processors using CMOS-compatible processes, marking a significant step towards practical quantum computing.","sentences":["Fluxonium qubits are recognized for their high coherence times and high operation fidelities, attributed to their unique design incorporating over 100 Josephson junctions per superconducting loop.","However, this complexity poses significant fabrication challenges, particularly in achieving high yield and junction uniformity with traditional methods.","Here, we introduce an overlap process for Josephson junction fabrication that achieves nearly 100% yield and maintains uniformity across a 2-inch wafer with less than 5% variation for the phase slip junction and less than 2% for the junction array.","Our compact junction array design facilitates fluxonium qubits with energy relaxation times exceeding 1 millisecond at the flux frustration point, demonstrating consistency with state-of-the-art dielectric loss tangents and flux noise across multiple devices.","This work suggests the scalability of high coherence fluxonium processors using CMOS-compatible processes, marking a significant step towards practical quantum computing."],"url":"http://arxiv.org/abs/2405.05481v1","category":"quant-ph"}
{"created":"2024-05-09 00:14:31","title":"Topological bifurcations in a mean-field game","abstract":"Mean-field games (MFG) provide a statistical physics inspired modeling framework for decision making in large-populations of strategic, non-cooperative agents. Mathematically, these systems consist of a forward-backward in time system of two coupled nonlinear partial differential equations (PDEs), namely the Fokker-Plank and the Hamilton-Jacobi-Bellman equations, governing the agent state and control distribution, respectively. In this work, we study a finite-time MFG with a rich global bifurcation structure using a reduced-order model (ROM). The ROM is a 4D two-point boundary value problem obtained by restricting the controlled dynamics to first two moments of the agent state distribution, i.e., the mean and the variance. Phase space analysis of the ROM reveals that the invariant manifolds of periodic orbits around the so-called `ergodic MFG equilibrium' play a crucial role in determining the bifurcation diagram, and impart a topological signature to various solution branches. We show a qualitative agreement of these results with numerical solutions of the full-order MFG PDE system.","sentences":["Mean-field games (MFG) provide a statistical physics inspired modeling framework for decision making in large-populations of strategic, non-cooperative agents.","Mathematically, these systems consist of a forward-backward in time system of two coupled nonlinear partial differential equations (PDEs), namely the Fokker-Plank and the Hamilton-Jacobi-Bellman equations, governing the agent state and control distribution, respectively.","In this work, we study a finite-time MFG with a rich global bifurcation structure using a reduced-order model (ROM).","The ROM is a 4D two-point boundary value problem obtained by restricting the controlled dynamics to first two moments of the agent state distribution, i.e., the mean and the variance.","Phase space analysis of the ROM reveals that the invariant manifolds of periodic orbits around the so-called `ergodic MFG equilibrium' play a crucial role in determining the bifurcation diagram, and impart a topological signature to various solution branches.","We show a qualitative agreement of these results with numerical solutions of the full-order MFG PDE system."],"url":"http://arxiv.org/abs/2405.05473v1","category":"math.DS"}
{"created":"2024-05-09 00:01:27","title":"Extension of Kirkwood-Buff theory: Partial enthalpies, fluctuations of energy density, temperature, and pressure, and solute-induced effects in a mixture solvent","abstract":"We present a statistical mechanical theory of multi-component fluids, where we consider the correlation functions of the number densities and the energy density in the grand canonical ensemble. In terms of their space integrals we express the partial volumes ${\\bar v}_i$, the partial enthalpies ${\\bar H}_i$, and other thermodynamic derivatives. These ${\\bar v}_i$ and ${\\bar H}_i$ assume simple forms for binary mixtures and for ternary mixtures with a dilute solute. They are then related to the space-dependent thermal fluctuations of the temperature and the pressure. The space averages of these fluctuations are those introduced by Landau and Lifshits in the isothermal-isobaric ($T$-$p$) ensemble. We also give expressions for the long-range (nonlocal) correlations in the canonical and $T$-$p$ ensembles, which are inversely proportional to the system volume. For a mixture solvent, we examine the solvent-induced solute-solute attraction and the osmotic enthalpy changes due to the solute doping using the correlation function integrals.","sentences":["We present a statistical mechanical theory of multi-component fluids, where we consider the correlation functions of the number densities and the energy density in the grand canonical ensemble.","In terms of their space integrals we express the partial volumes ${\\bar v}_i$, the partial enthalpies ${\\bar H}_i$, and other thermodynamic derivatives.","These ${\\bar v}_i$ and ${\\bar H}_i$ assume simple forms for binary mixtures and for ternary mixtures with a dilute solute.","They are then related to the space-dependent thermal fluctuations of the temperature and the pressure.","The space averages of these fluctuations are those introduced by Landau and Lifshits in the isothermal-isobaric ($T$-$p$) ensemble.","We also give expressions for the long-range (nonlocal) correlations in the canonical and $T$-$p$ ensembles, which are inversely proportional to the system volume.","For a mixture solvent, we examine the solvent-induced solute-solute attraction and the osmotic enthalpy changes due to the solute doping using the correlation function integrals."],"url":"http://arxiv.org/abs/2405.05471v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-09 00:01:23","title":"Deep thermalization in continuous-variable quantum systems","abstract":"We uncover emergent universality arising in the equilibration dynamics of multimode continuous-variable systems. Specifically, we study the ensemble of pure states supported on a small subsystem of a few modes, generated by Gaussian measurements on the remaining modes of a globally pure bosonic Gaussian state. We find that beginning from sufficiently complex global states, such as random Gaussian states and product squeezed states coupled via a deep array of linear optical elements, the induced ensemble attains a universal form, independent of the choice of measurement basis: it is composed of unsqueezed coherent states whose displacements are distributed normally and isotropically, with variance depending on only the particle-number density of the system. We further show that the emergence of such a universal form is consistent with a generalized maximum entropy principle, which endows the limiting ensemble, which we call the \"Gaussian Scrooge distribution\", with a special quantum information-theoretic property of having minimal accessible information. Our results represent a conceptual generalization of the recently introduced notion of \"deep thermalization\" in discrete-variable quantum many-body systems -- a novel form of equilibration going beyond thermalization of local observables -- to the realm of continuous-variable quantum systems. Moreover, it demonstrates how quantum information-theoretic perspectives can unveil new physical phenomena and principles in quantum dynamics and statistical mechanics.","sentences":["We uncover emergent universality arising in the equilibration dynamics of multimode continuous-variable systems.","Specifically, we study the ensemble of pure states supported on a small subsystem of a few modes, generated by Gaussian measurements on the remaining modes of a globally pure bosonic Gaussian state.","We find that beginning from sufficiently complex global states, such as random Gaussian states and product squeezed states coupled via a deep array of linear optical elements, the induced ensemble attains a universal form, independent of the choice of measurement basis: it is composed of unsqueezed coherent states whose displacements are distributed normally and isotropically, with variance depending on only the particle-number density of the system.","We further show that the emergence of such a universal form is consistent with a generalized maximum entropy principle, which endows the limiting ensemble, which we call the \"Gaussian Scrooge distribution\", with a special quantum information-theoretic property of having minimal accessible information.","Our results represent a conceptual generalization of the recently introduced notion of \"deep thermalization\" in discrete-variable quantum many-body systems -- a novel form of equilibration going beyond thermalization of local observables -- to the realm of continuous-variable quantum systems.","Moreover, it demonstrates how quantum information-theoretic perspectives can unveil new physical phenomena and principles in quantum dynamics and statistical mechanics."],"url":"http://arxiv.org/abs/2405.05470v1","category":"quant-ph"}
{"created":"2024-05-09 00:00:27","title":"PLLM-CS: Pre-trained Large Language Model (LLM) for Cyber Threat Detection in Satellite Networks","abstract":"Satellite networks are vital in facilitating communication services for various critical infrastructures. These networks can seamlessly integrate with a diverse array of systems. However, some of these systems are vulnerable due to the absence of effective intrusion detection systems, which can be attributed to limited research and the high costs associated with deploying, fine-tuning, monitoring, and responding to security breaches. To address these challenges, we propose a pretrained Large Language Model for Cyber Security , for short PLLM-CS, which is a variant of pre-trained Transformers [1], which includes a specialized module for transforming network data into contextually suitable inputs. This transformation enables the proposed LLM to encode contextual information within the cyber data. To validate the efficacy of the proposed method, we conducted empirical experiments using two publicly available network datasets, UNSW_NB 15 and TON_IoT, both providing Internet of Things (IoT)-based traffic data. Our experiments demonstrate that proposed LLM method outperforms state-of-the-art techniques such as BiLSTM, GRU, and CNN. Notably, the PLLM-CS method achieves an outstanding accuracy level of 100% on the UNSW_NB 15 dataset, setting a new standard for benchmark performance in this domain.","sentences":["Satellite networks are vital in facilitating communication services for various critical infrastructures.","These networks can seamlessly integrate with a diverse array of systems.","However, some of these systems are vulnerable due to the absence of effective intrusion detection systems, which can be attributed to limited research and the high costs associated with deploying, fine-tuning, monitoring, and responding to security breaches.","To address these challenges, we propose a pretrained Large Language Model for Cyber Security , for short PLLM-CS, which is a variant of pre-trained Transformers [1], which includes a specialized module for transforming network data into contextually suitable inputs.","This transformation enables the proposed LLM to encode contextual information within the cyber data.","To validate the efficacy of the proposed method, we conducted empirical experiments using two publicly available network datasets, UNSW_NB 15 and TON_IoT, both providing Internet of Things (IoT)-based traffic data.","Our experiments demonstrate that proposed LLM method outperforms state-of-the-art techniques such as BiLSTM, GRU, and CNN.","Notably, the PLLM-CS method achieves an outstanding accuracy level of 100% on the UNSW_NB 15 dataset, setting a new standard for benchmark performance in this domain."],"url":"http://arxiv.org/abs/2405.05469v1","category":"cs.CR"}
{"created":"2024-05-08 23:42:13","title":"Vidur: A Large-Scale Simulation Framework For LLM Inference","abstract":"Optimizing the deployment of Large language models (LLMs) is expensive today since it requires experimentally running an application workload against an LLM implementation while exploring large configuration space formed by system knobs such as parallelization strategies, batching techniques, and scheduling policies. To address this challenge, we present Vidur - a large-scale, high-fidelity, easily-extensible simulation framework for LLM inference performance. Vidur models the performance of LLM operators using a combination of experimental profiling and predictive modeling, and evaluates the end-to-end inference performance for different workloads by estimating several metrics of interest such as latency and throughput. We validate the fidelity of Vidur on several LLMs and show that it estimates inference latency with less than 9% error across the range. Further, we present Vidur-Search, a configuration search tool that helps optimize LLM deployment. Vidur-Search uses Vidur to automatically identify the most cost-effective deployment configuration that meets application performance constraints. For example, Vidur-Search finds the best deployment configuration for LLaMA2-70B in one hour on a CPU machine, in contrast to a deployment-based exploration which would require 42K GPU hours - costing ~218K dollars. Source code for Vidur is available at https://github.com/microsoft/vidur.","sentences":["Optimizing the deployment of Large language models (LLMs) is expensive today since it requires experimentally running an application workload against an LLM implementation while exploring large configuration space formed by system knobs such as parallelization strategies, batching techniques, and scheduling policies.","To address this challenge, we present Vidur - a large-scale, high-fidelity, easily-extensible simulation framework for LLM inference performance.","Vidur models the performance of LLM operators using a combination of experimental profiling and predictive modeling, and evaluates the end-to-end inference performance for different workloads by estimating several metrics of interest such as latency and throughput.","We validate the fidelity of Vidur on several LLMs and show that it estimates inference latency with less than 9% error across the range.","Further, we present Vidur-Search, a configuration search tool that helps optimize LLM deployment.","Vidur-Search uses Vidur to automatically identify the most cost-effective deployment configuration that meets application performance constraints.","For example, Vidur-Search finds the best deployment configuration for LLaMA2-70B in one hour on a CPU machine, in contrast to a deployment-based exploration which would require 42K GPU hours - costing ~218K dollars.","Source code for Vidur is available at https://github.com/microsoft/vidur."],"url":"http://arxiv.org/abs/2405.05465v1","category":"cs.LG"}
{"created":"2024-05-08 23:41:32","title":"Platitude des tissus duaux de certains pr\u00e9-feuilletages convexes du plan projectif complexe","abstract":"A holomorphic pre-foliation $\\mathscr{F}=\\mathcal{C}\\boxtimes\\mathcal{F}$ on $\\mathbb{P}^{2}_{\\mathbb{C}}$ is the data of a reduced complex projective curve $\\mathcal{C}$ of $\\mathbb{P}^{2}_{\\mathbb{C}}$ and a holomorphic foliation $\\mathcal{F}$ on $\\mathbb{P}^{2}_{\\mathbb{C}}$. When the foliation $\\mathcal{F}$ is convex and the curve $\\mathcal{C}$ is invariant by $\\mathcal{F}$, we speak of convex pre-foliation. In a previous paper, we showed that if a foliation $\\mathcal{F}$ on $\\mathbb{P}^{2}_{\\mathbb{C}}$ is reduced convex or homogeneous convex and if $\\mathcal{C}$ is an invariant line of $\\mathcal{F}$, then the dual web of the convex pre-foliation $\\mathcal{C}\\boxtimes\\mathcal{F}$ is flat. In this paper, we propose to extend this result to the case of a curve $\\mathcal{C}$ consisting of several invariant lines.","sentences":["A holomorphic pre-foliation $\\mathscr{F}=\\mathcal{C}\\boxtimes\\mathcal{F}$ on $\\mathbb{P}^{2}_{\\mathbb{C}}$ is the data of a reduced complex projective curve $\\mathcal{C}$ of $\\mathbb{P}^{2}_{\\mathbb{C}}$ and a holomorphic foliation $\\mathcal{F}$ on $\\mathbb{P}^{2}_{\\mathbb{C}}$. When the foliation $\\mathcal{F}$ is convex and the curve $\\mathcal{C}$ is invariant by $\\mathcal{F}$, we speak of convex pre-foliation.","In a previous paper, we showed that if a foliation $\\mathcal{F}$ on $\\mathbb{P}^{2}_{\\mathbb{C}}$ is reduced convex or homogeneous convex and if $\\mathcal{C}$ is an invariant line of $\\mathcal{F}$, then the dual web of the convex pre-foliation $\\mathcal{C}\\boxtimes\\mathcal{F}$ is flat.","In this paper, we propose to extend this result to the case of a curve $\\mathcal{C}$ consisting of several invariant lines."],"url":"http://arxiv.org/abs/2405.05464v1","category":"math.DS"}
{"created":"2024-05-08 23:26:24","title":"Studying Self-Care with Generative AI Tools: Lessons for Design","abstract":"The rise of generative AI presents new opportunities for the understanding and practice of self-care through its capability to generate varied content, including self-care suggestions via text and images, and engage in dialogue with users over time. However, there are also concerns about accuracy and trustworthiness of self-care advice provided via AI. This paper reports our findings from workshops, diaries, and interviews with five researchers and 24 participants to explore their experiences and use of generative AI for self-care. We analyze our findings to present a framework for the use of generative AI to support five types of self-care, - advice seeking, mentorship, resource creation, social simulation, and therapeutic self-expression - mapped across two dimensions - expertise and modality. We discuss how these practices shift the role of technologies for self-care from merely offering information to offering personalized advice and supporting creativity for reflection, and we offer suggestions for using the framework to investigate new self-care designs.","sentences":["The rise of generative AI presents new opportunities for the understanding and practice of self-care through its capability to generate varied content, including self-care suggestions via text and images, and engage in dialogue with users over time.","However, there are also concerns about accuracy and trustworthiness of self-care advice provided via AI.","This paper reports our findings from workshops, diaries, and interviews with five researchers and 24 participants to explore their experiences and use of generative AI for self-care.","We analyze our findings to present a framework for the use of generative AI to support five types of self-care, - advice seeking, mentorship, resource creation, social simulation, and therapeutic self-expression - mapped across two dimensions - expertise and modality.","We discuss how these practices shift the role of technologies for self-care from merely offering information to offering personalized advice and supporting creativity for reflection, and we offer suggestions for using the framework to investigate new self-care designs."],"url":"http://arxiv.org/abs/2405.05458v1","category":"cs.HC"}
{"created":"2024-05-08 23:20:17","title":"Giant Thermoelectric Response of Fluxons in Superconductors","abstract":"Thermoelectric devices that operate on quantum principles have been under extensive investigation in the past decades. These devices are at the fundamental limits of miniaturized heat engines and refrigerators, advancing the field of quantum thermodynamics. Most research in this area concerns the use of conduction electrons and holes as charge and heat carriers, and only very recently have superconductors been considered as thermal engines and thermoelectric devices. Here, we investigate the thermoelectric response of an Abrikosov vortex in type-II superconductors in the deep quantum limit. We consider two thermoelectric geometries, a type-II SIN junction and a local Scanning Tunneling Microscope (STM)-tip normal metal probe over the superconductor. We exploit the strong breaking of particle-hole symmetry in bound states at sub-gap energies within the superconducting vortex to realize a giant thermoelectric response in the presence of fluxons. We predict a thermovoltage of a few mV/K at sub-Kelvin temperatures using both semi-analytic and numerical self-consistent solutions of the Bogoliubov-de Gennes equations. Relevant thermoelectric coefficients and figures of merit are found within our model, both in linear and nonlinear regimes. The ZT of the SIN junction is around 1, rising to above 3 for the STM junction centered at the vortex core. We also discuss how this system can be used as a sensitive thermocouple, diode, or localized bolometer to detect low-energy single photons.","sentences":["Thermoelectric devices that operate on quantum principles have been under extensive investigation in the past decades.","These devices are at the fundamental limits of miniaturized heat engines and refrigerators, advancing the field of quantum thermodynamics.","Most research in this area concerns the use of conduction electrons and holes as charge and heat carriers, and only very recently have superconductors been considered as thermal engines and thermoelectric devices.","Here, we investigate the thermoelectric response of an Abrikosov vortex in type-II superconductors in the deep quantum limit.","We consider two thermoelectric geometries, a type-II SIN junction and a local Scanning Tunneling Microscope (STM)-tip normal metal probe over the superconductor.","We exploit the strong breaking of particle-hole symmetry in bound states at sub-gap energies within the superconducting vortex to realize a giant thermoelectric response in the presence of fluxons.","We predict a thermovoltage of a few mV/K at sub-Kelvin temperatures using both semi-analytic and numerical self-consistent solutions of the Bogoliubov-de Gennes equations.","Relevant thermoelectric coefficients and figures of merit are found within our model, both in linear and nonlinear regimes.","The ZT of the SIN junction is around 1, rising to above 3 for the STM junction centered at the vortex core.","We also discuss how this system can be used as a sensitive thermocouple, diode, or localized bolometer to detect low-energy single photons."],"url":"http://arxiv.org/abs/2405.05456v1","category":"cond-mat.supr-con"}
{"created":"2024-05-08 23:07:51","title":"Molecular dynamics characterization of the interfacial structure and forces of the methane-ethane sII gas hydrate interface","abstract":"The nucleation of gas hydrates is of great interest in flow assurance, global energy demand, and carbon capture and storage. A complex molecular understanding is critical to control hydrate nucleation and growth in the context of potential applications. Molecular dynamics is employed in this work combined with the mechanical definition of surface tension to assess the surface stresses that control some of the behavior at the interface. Ensuring careful sampling and simulation behavior, this work extracts meaningful results from molecular properties. We characterize the interfacial tension for sII methane/ethane hydrate and gas mixtures for different temperatures and pressures. We find that the surface tension trends positively with temperature in a balance of water-solid and water-gas interactions. The molecular dipole shows the complexities of water molecule behavior in small, compressed pre-melting layer that emerges as a quasi-liquid. These behaviors contribute to the developing knowledge base surrounding practical applications of this interface.","sentences":["The nucleation of gas hydrates is of great interest in flow assurance, global energy demand, and carbon capture and storage.","A complex molecular understanding is critical to control hydrate nucleation and growth in the context of potential applications.","Molecular dynamics is employed in this work combined with the mechanical definition of surface tension to assess the surface stresses that control some of the behavior at the interface.","Ensuring careful sampling and simulation behavior, this work extracts meaningful results from molecular properties.","We characterize the interfacial tension for sII methane/ethane hydrate and gas mixtures for different temperatures and pressures.","We find that the surface tension trends positively with temperature in a balance of water-solid and water-gas interactions.","The molecular dipole shows the complexities of water molecule behavior in small, compressed pre-melting layer that emerges as a quasi-liquid.","These behaviors contribute to the developing knowledge base surrounding practical applications of this interface."],"url":"http://arxiv.org/abs/2405.05454v1","category":"physics.app-ph"}
{"created":"2024-05-08 23:05:34","title":"Spelunker: A quick-look Python pipeline for JWST NIRISS FGS Guide Star Data","abstract":"The James Webb Space Telescope produces some of the highest sensitivity imaging of the cosmos across all instruments. One of them, the NIRISS Fine Guidance Sensor, provides guide star imaging with a passband of 0.6 to 5 microns through two separate channels, each with a 2.3' x 2.3' field of view (FOV) and a sampling rate of 64 ms$-$data that is taken in parallel and is thus available for every JWST observing program. While the onboard system uses guide stars to provide information to the attitude control system (ACS), which stabilizes the observatory, the astronomical community can also use the data products associated with these 64 ms cadence images as science products. Usages range from studying guide star photometry in search of transient phenomena to using these data to identify and investigate technical anomalies that might occur during scientific observations with the rest of the JWST instruments. Despite this wide range of possible usages, these data products are not straightforward to manipulate and analyze, and there is no publicly available package to download, investigate, and research guide star data. Spelunker is a Python library that was developed to enable access to these guide star data products and their analysis.","sentences":["The James Webb Space Telescope produces some of the highest sensitivity imaging of the cosmos across all instruments.","One of them, the NIRISS Fine Guidance Sensor, provides guide star imaging with a passband of 0.6 to 5 microns through two separate channels, each with a 2.3' x 2.3' field of view (FOV) and a sampling rate of 64 ms$-$data that is taken in parallel and is thus available for every JWST observing program.","While the onboard system uses guide stars to provide information to the attitude control system (ACS), which stabilizes the observatory, the astronomical community can also use the data products associated with these 64 ms cadence images as science products.","Usages range from studying guide star photometry in search of transient phenomena to using these data to identify and investigate technical anomalies that might occur during scientific observations with the rest of the JWST instruments.","Despite this wide range of possible usages, these data products are not straightforward to manipulate and analyze, and there is no publicly available package to download, investigate, and research guide star data.","Spelunker is a Python library that was developed to enable access to these guide star data products and their analysis."],"url":"http://arxiv.org/abs/2405.05453v1","category":"astro-ph.IM"}
{"created":"2024-05-08 23:05:26","title":"Highest Fusion Performance without Harmful Edge Energy Bursts in Tokamak","abstract":"The path of tokamak fusion and ITER is maintaining high-performance plasma to produce sufficient fusion power. This effort is hindered by the transient energy burst arising from the instabilities at the boundary of high-confinement plasmas. The application of 3D magnetic perturbations is the method in ITER and possibly in future fusion power plants to suppress this instability and avoid energy busts damaging the device. Unfortunately, the conventional use of the 3D field in tokamaks typically leads to degraded fusion performance and an increased risk of other plasma instabilities, two severe issues for reactor implementation. In this work, we present an innovative 3D field optimization, exploiting machine learning, real-time adaptability, and multi-device capabilities to overcome these limitations. This integrated scheme is successfully deployed on DIII-D and KSTAR tokamaks, consistently achieving reactor-relevant core confinement and the highest fusion performance without triggering damaging instabilities or bursts while demonstrating ITER-relevant automated 3D optimization for the first time. This is enabled both by advances in the physics understanding of self-organized transport in the plasma edge and by advances in machine-learning technology, which is used to optimize the 3D field spectrum for automated management of a volatile and complex system. These findings establish real-time adaptive 3D field optimization as a crucial tool for ITER and future reactors to maximize fusion performance while simultaneously minimizing damage to machine components.","sentences":["The path of tokamak fusion and ITER is maintaining high-performance plasma to produce sufficient fusion power.","This effort is hindered by the transient energy burst arising from the instabilities at the boundary of high-confinement plasmas.","The application of 3D magnetic perturbations is the method in ITER and possibly in future fusion power plants to suppress this instability and avoid energy busts damaging the device.","Unfortunately, the conventional use of the 3D field in tokamaks typically leads to degraded fusion performance and an increased risk of other plasma instabilities, two severe issues for reactor implementation.","In this work, we present an innovative 3D field optimization, exploiting machine learning, real-time adaptability, and multi-device capabilities to overcome these limitations.","This integrated scheme is successfully deployed on DIII-D and KSTAR tokamaks, consistently achieving reactor-relevant core confinement and the highest fusion performance without triggering damaging instabilities or bursts while demonstrating ITER-relevant automated 3D optimization for the first time.","This is enabled both by advances in the physics understanding of self-organized transport in the plasma edge and by advances in machine-learning technology, which is used to optimize the 3D field spectrum for automated management of a volatile and complex system.","These findings establish real-time adaptive 3D field optimization as a crucial tool for ITER and future reactors to maximize fusion performance while simultaneously minimizing damage to machine components."],"url":"http://arxiv.org/abs/2405.05452v1","category":"physics.plasm-ph"}
{"created":"2024-05-08 22:59:55","title":"Bumpy metric theorem for co-rank 1 sub-Riemannian and reversible sub-Finsler metrics","abstract":"We prove that for a generic sub-Riemannian and reversible sub-Finsler metrics defined on a fixed co-rank 1 distribution, all strictly normal periodic orbits are non-degenerate.","sentences":["We prove that for a generic sub-Riemannian and reversible sub-Finsler metrics defined on a fixed co-rank 1 distribution, all strictly normal periodic orbits are non-degenerate."],"url":"http://arxiv.org/abs/2405.05450v1","category":"math.DS"}
{"created":"2024-05-08 22:52:54","title":"Energy-superconvergent Runge-Kutta Time Discretizations","abstract":"In this paper, we investigate the energy accuracy of explicit Runge-Kutta (RK) time discretization for antisymmetric autonomous linear systems and present a framework for constructing RK methods with an order of energy accuracy much greater than the number of stages. For an $s$-stage, $p$th-order RK method, we show that the energy accuracy can achieve superconvergence with an order up to $2s-p+1$ if $p$ is even. Several energy-superconvergent methods, including five- to seven-stage fourth-order methods with energy accuracy up to the eleventh order, together with their strong stability criteria, are derived. The proposed methods are examined using several applications, including second-order ordinary differential equations for harmonic oscillators, linear integro-differential equations for peridynamics, and one-dimensional Maxwell's equations of electrodynamics.","sentences":["In this paper, we investigate the energy accuracy of explicit Runge-Kutta (RK) time discretization for antisymmetric autonomous linear systems and present a framework for constructing RK methods with an order of energy accuracy much greater than the number of stages.","For an $s$-stage, $p$th-order RK method, we show that the energy accuracy can achieve superconvergence with an order up to $2s-p+1$ if $p$ is even.","Several energy-superconvergent methods, including five- to seven-stage fourth-order methods with energy accuracy up to the eleventh order, together with their strong stability criteria, are derived.","The proposed methods are examined using several applications, including second-order ordinary differential equations for harmonic oscillators, linear integro-differential equations for peridynamics, and one-dimensional Maxwell's equations of electrodynamics."],"url":"http://arxiv.org/abs/2405.05448v1","category":"math.NA"}
{"created":"2024-05-08 22:41:23","title":"Dynamic Posture Manipulation During Tumbling for Closed-Loop Heading Angle Control","abstract":"Passive tumbling uses natural forces like gravity for efficient travel. But without an active means of control, passive tumblers must rely entirely on external forces. Northeastern University's COBRA is a snake robot that can morph into a ring, which employs passive tumbling to traverse down slopes. However, due to its articulated joints, it is also capable of dynamically altering its posture to manipulate the dynamics of the tumbling locomotion for active steering. This paper presents a modelling and control strategy based on collocation optimization for real-time steering of COBRA's tumbling locomotion. We validate our approach using Matlab simulations.","sentences":["Passive tumbling uses natural forces like gravity for efficient travel.","But without an active means of control, passive tumblers must rely entirely on external forces.","Northeastern University's COBRA is a snake robot that can morph into a ring, which employs passive tumbling to traverse down slopes.","However, due to its articulated joints, it is also capable of dynamically altering its posture to manipulate the dynamics of the tumbling locomotion for active steering.","This paper presents a modelling and control strategy based on collocation optimization for real-time steering of COBRA's tumbling locomotion.","We validate our approach using Matlab simulations."],"url":"http://arxiv.org/abs/2405.05447v1","category":"cs.RO"}
{"created":"2024-05-08 22:07:49","title":"Insensitive edge solitons in non-Hermitian topological lattices","abstract":"In this work, we demonstrate that the synergetic interplay of topology, nonreciprocity and nonlinearity is capable of unprecedented effects. We focus on a nonreciprocal variant of the Su-Shrieffer-Heeger chain with local Kerr nonlinearity. We find a continuous family of non-reciprocal edge solitons (NESs) emerging from the topological edge mode, with near-zero energy, in great contrast from their reciprocal counterparts. Analytical results show that this energy decays exponentially towards zero when increasing the lattice size. Consequently, despite the absence of chiral symmetry within the system, we obtain zero-energy NESs, which are insensitive to growing Kerr nonlinearity. Even more surprising, these zero-energy NESs also persist in the strong nonlinear limit. Our work may enable new avenues for the control of nonlinear topological waves without requiring the addition of complex chiral-preserving nonlinearities.","sentences":["In this work, we demonstrate that the synergetic interplay of topology, nonreciprocity and nonlinearity is capable of unprecedented effects.","We focus on a nonreciprocal variant of the Su-Shrieffer-Heeger chain with local Kerr nonlinearity.","We find a continuous family of non-reciprocal edge solitons (NESs) emerging from the topological edge mode, with near-zero energy, in great contrast from their reciprocal counterparts.","Analytical results show that this energy decays exponentially towards zero when increasing the lattice size.","Consequently, despite the absence of chiral symmetry within the system, we obtain zero-energy NESs, which are insensitive to growing Kerr nonlinearity.","Even more surprising, these zero-energy NESs also persist in the strong nonlinear limit.","Our work may enable new avenues for the control of nonlinear topological waves without requiring the addition of complex chiral-preserving nonlinearities."],"url":"http://arxiv.org/abs/2405.05441v1","category":"nlin.PS"}
{"created":"2024-05-08 21:45:58","title":"On the approximation properties of fast Leja points","abstract":"Fast Leja points on an interval are points constructed using a discrete modification of the algorithm for constructing Leja points. Not much about fast Leja points has been proven theoretically. We present an asymptotic property of a triangular interpolation array, and under the assumption that fast Leja points satisfy this property, we prove that they are good for Lagrange interpolation.","sentences":["Fast Leja points on an interval are points constructed using a discrete modification of the algorithm for constructing Leja points.","Not much about fast Leja points has been proven theoretically.","We present an asymptotic property of a triangular interpolation array, and under the assumption that fast Leja points satisfy this property, we prove that they are good for Lagrange interpolation."],"url":"http://arxiv.org/abs/2405.05436v1","category":"math.NA"}
{"created":"2024-05-09 17:32:39","title":"Improved Evolutionary Algorithms for Submodular Maximization with Cost Constraints","abstract":"We present an evolutionary algorithm evo-SMC for the problem of Submodular Maximization under Cost constraints (SMC). Our algorithm achieves $1/2$-approximation with a high probability $1-1/n$ within $\\mathcal{O}(n^2K_{\\beta})$ iterations, where $K_{\\beta}$ denotes the maximum size of a feasible solution set with cost constraint $\\beta$. To the best of our knowledge, this is the best approximation guarantee offered by evolutionary algorithms for this problem. We further refine evo-SMC, and develop {\\sc st-evo-SMC}. This stochastic version yields a significantly faster algorithm while maintaining the approximation ratio of $1/2$, with probability $1-\\epsilon$. The required number of iterations reduces to $\\mathcal{O}(nK_{\\beta}\\log{(1/\\epsilon)}/p)$, where the user defined parameters $p \\in (0,1]$ represents the stochasticity probability, and $\\epsilon \\in (0,1]$ denotes the error threshold. Finally, the empirical evaluations carried out through extensive experimentation substantiate the efficiency and effectiveness of our proposed algorithms. Our algorithms consistently outperform existing methods, producing higher-quality solutions.","sentences":["We present an evolutionary algorithm evo-SMC for the problem of Submodular Maximization under Cost constraints (SMC).","Our algorithm achieves $1/2$-approximation with a high probability $1-1/n$ within $\\mathcal{O}(n^2K_{\\beta})$ iterations, where $K_{\\beta}$ denotes the maximum size of a feasible solution set with cost constraint $\\beta$. To the best of our knowledge, this is the best approximation guarantee offered by evolutionary algorithms for this problem.","We further refine evo-SMC, and develop {\\sc st-evo-SMC}.","This stochastic version yields a significantly faster algorithm while maintaining the approximation ratio of $1/2$, with probability $1-\\epsilon$. The required number of iterations reduces to $\\mathcal{O}(nK_{\\beta}\\log{(1/\\epsilon)}/p)$, where the user defined parameters $p \\in (0,1]$ represents the stochasticity probability, and $\\epsilon \\in (0,1]$ denotes the error threshold.","Finally, the empirical evaluations carried out through extensive experimentation substantiate the efficiency and effectiveness of our proposed algorithms.","Our algorithms consistently outperform existing methods, producing higher-quality solutions."],"url":"http://arxiv.org/abs/2405.05942v1","category":"cs.DS"}
{"created":"2024-05-09 16:13:32","title":"Non-monotonic conductivity of aqueous electrolytes: beyond the first Wien effect","abstract":"The conductivity of strong electrolytes increases under high electric fields, a nonlinear response known as the first Wien effect. Here, using molecular dynamics simulations we show that this nonlinear response is non-monotonic for moderately concentrated aqueous electrolytes. We attribute this unanticipated behavior to the fact that, under high electric fields, the permittivity of water decreases and becomes anisotropic. The permittivity tensor measured in the simulations can be reproduced by a model of water molecules as dipoles. We incorporate the resulting anisotropic interactions between the ions into a generalised Stochastic Density Field Theory and calculate ionic correlations as well as corrections to the Nernst-Einstein conductivity which are in good agreement with the numerical simulations.","sentences":["The conductivity of strong electrolytes increases under high electric fields, a nonlinear response known as the first Wien effect.","Here, using molecular dynamics simulations we show that this nonlinear response is non-monotonic for moderately concentrated aqueous electrolytes.","We attribute this unanticipated behavior to the fact that, under high electric fields, the permittivity of water decreases and becomes anisotropic.","The permittivity tensor measured in the simulations can be reproduced by a model of water molecules as dipoles.","We incorporate the resulting anisotropic interactions between the ions into a generalised Stochastic Density Field Theory and calculate ionic correlations as well as corrections to the Nernst-Einstein conductivity which are in good agreement with the numerical simulations."],"url":"http://arxiv.org/abs/2405.05882v1","category":"cond-mat.soft"}
{"created":"2024-05-09 16:05:34","title":"Fourier decay of product measures","abstract":"Can one characterise the Fourier decay of a product measure in terms of the Fourier decay of its marginals? We make inroads on this question by describing the Fourier spectrum of a product measure in terms of the Fourier spectrum of its marginals. The Fourier spectrum is a continuously parametrised family of dimensions living between the Fourier and Hausdorff dimensions and captures more Fourier analytic information than either dimension considered in isolation. We provide several examples and applications, including to Kakeya and Furstenberg sets. In the process we derive a novel Fourier analytic characterisation of the upper box dimension.","sentences":["Can one characterise the Fourier decay of a product measure in terms of the Fourier decay of its marginals?","We make inroads on this question by describing the Fourier spectrum of a product measure in terms of the Fourier spectrum of its marginals.","The Fourier spectrum is a continuously parametrised family of dimensions living between the Fourier and Hausdorff dimensions and captures more Fourier analytic information than either dimension considered in isolation.","We provide several examples and applications, including to Kakeya and Furstenberg sets.","In the process we derive a novel Fourier analytic characterisation of the upper box dimension."],"url":"http://arxiv.org/abs/2405.05878v1","category":"math.CA"}
{"created":"2024-05-09 15:59:11","title":"Comment on: Testing the speed of the spooky action at a distance in a tabletop experiment. [Sci Rep 13, 8201 (2023)]","abstract":"In 1989, Eberhard proposed a v-causal model where quantum correlations between entangled particles are established by communications moving at a superluminal speed v_t > c in a preferred frame. In successive years, several experiments established lower bounds for the possible tachyons velocities. In a recent paper, Luigi Santamaria Amato et al. performed an interesting east-west aligned tabletop experiment under the assumption that the preferred frame is the Cosmic Microwave Background (CMB). In that paper, they criticize long-distance experiments but here we show that most of their criticisms are not applicable to long-distance tunnel experiments where the highest lower bound was obtained.","sentences":["In 1989, Eberhard proposed a v-causal model where quantum correlations between entangled particles are established by communications moving at a superluminal speed v_t","> c in a preferred frame.","In successive years, several experiments established lower bounds for the possible tachyons velocities.","In a recent paper, Luigi Santamaria Amato et al. performed an interesting east-west aligned tabletop experiment under the assumption that the preferred frame is the Cosmic Microwave Background (CMB).","In that paper, they criticize long-distance experiments but here we show that most of their criticisms are not applicable to long-distance tunnel experiments where the highest lower bound was obtained."],"url":"http://arxiv.org/abs/2405.05869v1","category":"quant-ph"}
{"created":"2024-05-09 15:59:09","title":"Trustworthy Dimensionality Reduction","abstract":"Different unsupervised models for dimensionality reduction like PCA, LLE, Shannon's mapping, tSNE, UMAP, etc. work on different principles, hence, they are difficult to compare on the same ground. Although they are usually good for visualisation purposes, they can produce spurious patterns that are not present in the original data, losing its trustability (or credibility). On the other hand, information about some response variable (or knowledge of class labels) allows us to do supervised dimensionality reduction such as SIR, SAVE, etc. which work to reduce the data dimension without hampering its ability to explain the particular response at hand. Therefore, the reduced dataset cannot be used to further analyze its relationship with some other kind of responses, i.e., it loses its generalizability. To make a better dimensionality reduction algorithm with a better balance between these two, we shall formally describe the mathematical model used by dimensionality reduction algorithms and provide two indices to measure these intuitive concepts such as trustability and generalizability. Then, we propose a Localized Skeletonization and Dimensionality Reduction (LSDR) algorithm which approximately achieves optimality in both these indices to some extent. The proposed algorithm has been compared with state-of-the-art algorithms such as tSNE and UMAP and is found to be better overall in preserving global structure while retaining useful local information as well. We also propose some of the possible extensions of LSDR which could make this algorithm universally applicable for various types of data similar to tSNE and UMAP.","sentences":["Different unsupervised models for dimensionality reduction like PCA, LLE, Shannon's mapping, tSNE, UMAP, etc. work on different principles, hence, they are difficult to compare on the same ground.","Although they are usually good for visualisation purposes, they can produce spurious patterns that are not present in the original data, losing its trustability (or credibility).","On the other hand, information about some response variable (or knowledge of class labels) allows us to do supervised dimensionality reduction such as SIR, SAVE, etc. which work to reduce the data dimension without hampering its ability to explain the particular response at hand.","Therefore, the reduced dataset cannot be used to further analyze its relationship with some other kind of responses, i.e., it loses its generalizability.","To make a better dimensionality reduction algorithm with a better balance between these two, we shall formally describe the mathematical model used by dimensionality reduction algorithms and provide two indices to measure these intuitive concepts such as trustability and generalizability.","Then, we propose a Localized Skeletonization and Dimensionality Reduction (LSDR) algorithm which approximately achieves optimality in both these indices to some extent.","The proposed algorithm has been compared with state-of-the-art algorithms such as tSNE and UMAP and is found to be better overall in preserving global structure while retaining useful local information as well.","We also propose some of the possible extensions of LSDR which could make this algorithm universally applicable for various types of data similar to tSNE and UMAP."],"url":"http://arxiv.org/abs/2405.05868v1","category":"stat.ME"}
{"created":"2024-05-09 15:41:10","title":"Robust and Explainable Fine-Grained Visual Classification with Transfer Learning: A Dual-Carriageway Framework","abstract":"In the realm of practical fine-grained visual classification applications rooted in deep learning, a common scenario involves training a model using a pre-existing dataset. Subsequently, a new dataset becomes available, prompting the desire to make a pivotal decision for achieving enhanced and leveraged inference performance on both sides: Should one opt to train datasets from scratch or fine-tune the model trained on the initial dataset using the newly released dataset? The existing literature reveals a lack of methods to systematically determine the optimal training strategy, necessitating explainability. To this end, we present an automatic best-suit training solution searching framework, the Dual-Carriageway Framework (DCF), to fill this gap. DCF benefits from the design of a dual-direction search (starting from the pre-existing or the newly released dataset) where five different training settings are enforced. In addition, DCF is not only capable of figuring out the optimal training strategy with the capability of avoiding overfitting but also yields built-in quantitative and visual explanations derived from the actual input and weights of the trained model. We validated DCF's effectiveness through experiments with three convolutional neural networks (ResNet18, ResNet34 and Inception-v3) on two temporally continued commercial product datasets. Results showed fine-tuning pathways outperformed training-from-scratch ones by up to 2.13% and 1.23% on the pre-existing and new datasets, respectively, in terms of mean accuracy. Furthermore, DCF identified reflection padding as the superior padding method, enhancing testing accuracy by 3.72% on average. This framework stands out for its potential to guide the development of robust and explainable AI solutions in fine-grained visual classification tasks.","sentences":["In the realm of practical fine-grained visual classification applications rooted in deep learning, a common scenario involves training a model using a pre-existing dataset.","Subsequently, a new dataset becomes available, prompting the desire to make a pivotal decision for achieving enhanced and leveraged inference performance on both sides: Should one opt to train datasets from scratch or fine-tune the model trained on the initial dataset using the newly released dataset?","The existing literature reveals a lack of methods to systematically determine the optimal training strategy, necessitating explainability.","To this end, we present an automatic best-suit training solution searching framework, the Dual-Carriageway Framework (DCF), to fill this gap.","DCF benefits from the design of a dual-direction search (starting from the pre-existing or the newly released dataset) where five different training settings are enforced.","In addition, DCF is not only capable of figuring out the optimal training strategy with the capability of avoiding overfitting but also yields built-in quantitative and visual explanations derived from the actual input and weights of the trained model.","We validated DCF's effectiveness through experiments with three convolutional neural networks (ResNet18, ResNet34 and Inception-v3) on two temporally continued commercial product datasets.","Results showed fine-tuning pathways outperformed training-from-scratch ones by up to 2.13% and 1.23% on the pre-existing and new datasets, respectively, in terms of mean accuracy.","Furthermore, DCF identified reflection padding as the superior padding method, enhancing testing accuracy by 3.72% on average.","This framework stands out for its potential to guide the development of robust and explainable AI solutions in fine-grained visual classification tasks."],"url":"http://arxiv.org/abs/2405.05853v1","category":"cs.CV"}
{"created":"2024-05-09 15:12:44","title":"Multiplicity of solutions for mixed local-nonlocal elliptic equations with singular nonlinearity","abstract":"We will prove multiplicity results for the mixed local-nonlocal elliptic equation of the form \\begin{eqnarray} \\begin{split} -\\Delta_pu+(-\\Delta)_p^s u&=\\frac{\\lambda}{u^{\\gamma}}+u^r \\text { in } \\Omega, \\\\u&>0 \\text{ in } \\Omega,\\\\u&=0 \\text { in }\\mathbb{R}^n \\backslash \\Omega; \\end{split} \\end{eqnarray} where \\begin{equation*} (-\\Delta )_p^s u(x)= c_{n,s}\\operatorname{P.V.}\\int_{\\mathbb{R}^n}\\frac{|u(x)-u(y)|^{p-2}(u(x)-u(y))}{|x-y|^{n+sp}} d y, \\end{equation*} and $-\\Delta_p$ is the usual $p$-Laplace operator. Under the assumptions that $\\Omega$ is a bounded domain in $\\mathbb{R}^{n}$ with regular enough boundary, $p>1$, $n> p$, $s\\in(0,1)$, $\\lambda>0$ and $r\\in(p-1,p^*-1)$ where $p^*$ is the critical Sobolev exponent, we will show there exist at least two weak solutions to our problem for $0<\\gamma<1$ and some certain values of $\\lambda$. Further, for every $\\gamma>0$, assuming strict convexity of $\\Omega$, for $p=2$ and $s\\in(0,1/2)$, we will show the existence of at least two positive weak solutions to the problem, for small values of $\\lambda$, extending the result of \\cite{garaingeometric}. Here $c_{n,s}$ is a suitable normalization constant, and $\\operatorname{P.V.}$ stands for Cauchy Principal Value.","sentences":["We will prove multiplicity results for the mixed local-nonlocal elliptic equation of the form \\begin{eqnarray} \\begin{split} -\\Delta_pu+(-\\Delta)_p^s u&=\\frac{\\lambda}{u^{\\gamma}}+u^r \\text { in } \\Omega, \\\\u&>0 \\text{ in } \\Omega,\\\\u&=0 \\text { in }\\mathbb{R}^n \\backslash \\Omega; \\end{split} \\end{eqnarray} where \\begin{equation*} (-\\Delta )_","p^s u(x)= c_{n,s}\\operatorname{P.V.}\\int_{\\mathbb{R}^n}\\frac{|u(x)-u(y)|^{p-2}(u(x)-u(y))}{|x-y|^{n+sp}} d y, \\end{equation*} and $-\\Delta_p$ is the usual $p$-Laplace operator.","Under the assumptions that $\\Omega$ is a bounded domain in $\\mathbb{R}^{n}$ with regular enough boundary, $p>1$, $n> p$, $s\\in(0,1)$, $\\lambda>0$ and $r\\in(p-1,p^*-1)$ where $p^*$ is the critical Sobolev exponent, we will show there exist at least two weak solutions to our problem for $0<\\gamma<1$ and some certain values of $\\lambda$. Further, for every $\\gamma>0$, assuming strict convexity of $\\Omega$, for $p=2$ and $s\\in(0,1/2)$, we will show the existence of at least two positive weak solutions to the problem, for small values of $\\lambda$, extending the result of \\cite{garaingeometric}.","Here $c_{n,s}$ is a suitable normalization constant, and $\\operatorname{P.V.}$ stands for Cauchy Principal Value."],"url":"http://arxiv.org/abs/2405.05832v1","category":"math.AP"}
{"created":"2024-05-09 15:04:07","title":"Mask-TS Net: Mask Temperature Scaling Uncertainty Calibration for Polyp Segmentation","abstract":"Lots of popular calibration methods in medical images focus on classification, but there are few comparable studies on semantic segmentation. In polyp segmentation of medical images, we find most diseased area occupies only a small portion of the entire image, resulting in previous models being not well-calibrated for lesion regions but well-calibrated for background, despite their seemingly better Expected Calibration Error (ECE) scores overall. Therefore, we proposed four-branches calibration network with Mask-Loss and Mask-TS strategies to more focus on the scaling of logits within potential lesion regions, which serves to mitigate the influence of background interference. In the experiments, we compare the existing calibration methods with the proposed Mask Temperature Scaling (Mask-TS). The results indicate that the proposed calibration network outperforms other methods both qualitatively and quantitatively.","sentences":["Lots of popular calibration methods in medical images focus on classification, but there are few comparable studies on semantic segmentation.","In polyp segmentation of medical images, we find most diseased area occupies only a small portion of the entire image, resulting in previous models being not well-calibrated for lesion regions but well-calibrated for background, despite their seemingly better Expected Calibration Error (ECE) scores overall.","Therefore, we proposed four-branches calibration network with Mask-Loss and Mask-TS strategies to more focus on the scaling of logits within potential lesion regions, which serves to mitigate the influence of background interference.","In the experiments, we compare the existing calibration methods with the proposed Mask Temperature Scaling (Mask-TS).","The results indicate that the proposed calibration network outperforms other methods both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2405.05830v1","category":"cs.CV"}
{"created":"2024-05-09 15:02:31","title":"Drift-Energy Replacement Effect in Multi-Ion Magnetized Plasmas","abstract":"Fast rotation can improve the stability and confinement of fusion plasmas. However, to maintain a rapidly rotating fusion plasma in steady state, significant energy must be invested in spinning up each incoming fuel ion. We show here that, under the right circumstances, collisional cross-field radial fueling can directly transfer drift energy between outgoing and incoming ions without the need for external power recirculation, thereby reducing the energy costs of maintaining the rotation.","sentences":["Fast rotation can improve the stability and confinement of fusion plasmas.","However, to maintain a rapidly rotating fusion plasma in steady state, significant energy must be invested in spinning up each incoming fuel ion.","We show here that, under the right circumstances, collisional cross-field radial fueling can directly transfer drift energy between outgoing and incoming ions without the need for external power recirculation, thereby reducing the energy costs of maintaining the rotation."],"url":"http://arxiv.org/abs/2405.05829v1","category":"physics.plasm-ph"}
{"created":"2024-05-09 15:00:49","title":"VAIM - CFF: A variational autoencoder inverse mapper solution to Compton form factor extraction from deeply virtual exclusive reactions","abstract":"We develop a new methodology for extracting Compton form factors (CFFs) in from deeply virtual exclusive reactions such as the unpolarized DVCS cross section using a specialized inverse problem solver, a variational autoencoder inverse mapper (VAIM). The VAIM-CFF framework not only allows us access to a fitted solution set possibly containing multiple solutions in the extraction of all 8 CFFs from a single cross section measurement, but also accesses the lost information contained in the forward mapping from CFFs to cross section. We investigate various assumptions and their effects on the predicted CFFs such as cross section organization, number of extracted CFFs, use of uncertainty quantification technique, and inclusion of prior physics information. We then use dimensionality reduction techniques such as principal component analysis to visualize the missing physics information tracked in the latent space of the VAIM framework. Through re-framing the extraction of CFFs as an inverse problem, we gain access to fundamental properties of the problem not comprehensible in standard fitting methodologies: exploring the limits of the information encoded in deeply virtual exclusive experiments.","sentences":["We develop a new methodology for extracting Compton form factors (CFFs) in from deeply virtual exclusive reactions such as the unpolarized DVCS cross section using a specialized inverse problem solver, a variational autoencoder inverse mapper (VAIM).","The VAIM-CFF framework not only allows us access to a fitted solution set possibly containing multiple solutions in the extraction of all 8 CFFs from a single cross section measurement, but also accesses the lost information contained in the forward mapping from CFFs to cross section.","We investigate various assumptions and their effects on the predicted CFFs such as cross section organization, number of extracted CFFs, use of uncertainty quantification technique, and inclusion of prior physics information.","We then use dimensionality reduction techniques such as principal component analysis to visualize the missing physics information tracked in the latent space of the VAIM framework.","Through re-framing the extraction of CFFs as an inverse problem, we gain access to fundamental properties of the problem not comprehensible in standard fitting methodologies: exploring the limits of the information encoded in deeply virtual exclusive experiments."],"url":"http://arxiv.org/abs/2405.05826v1","category":"hep-ph"}
{"created":"2024-05-09 14:47:15","title":"Fast and Controllable Post-training Sparsity: Learning Optimal Sparsity Allocation with Global Constraint in Minutes","abstract":"Neural network sparsity has attracted many research interests due to its similarity to biological schemes and high energy efficiency. However, existing methods depend on long-time training or fine-tuning, which prevents large-scale applications. Recently, some works focusing on post-training sparsity (PTS) have emerged. They get rid of the high training cost but usually suffer from distinct accuracy degradation due to neglect of the reasonable sparsity rate at each layer. Previous methods for finding sparsity rates mainly focus on the training-aware scenario, which usually fails to converge stably under the PTS setting with limited data and much less training cost. In this paper, we propose a fast and controllable post-training sparsity (FCPTS) framework. By incorporating a differentiable bridge function and a controllable optimization objective, our method allows for rapid and accurate sparsity allocation learning in minutes, with the added assurance of convergence to a predetermined global sparsity rate. Equipped with these techniques, we can surpass the state-of-the-art methods by a large margin, e.g., over 30\\% improvement for ResNet-50 on ImageNet under the sparsity rate of 80\\%. Our plug-and-play code and supplementary materials are open-sourced at https://github.com/ModelTC/FCPTS.","sentences":["Neural network sparsity has attracted many research interests due to its similarity to biological schemes and high energy efficiency.","However, existing methods depend on long-time training or fine-tuning, which prevents large-scale applications.","Recently, some works focusing on post-training sparsity (PTS) have emerged.","They get rid of the high training cost but usually suffer from distinct accuracy degradation due to neglect of the reasonable sparsity rate at each layer.","Previous methods for finding sparsity rates mainly focus on the training-aware scenario, which usually fails to converge stably under the PTS setting with limited data and much less training cost.","In this paper, we propose a fast and controllable post-training sparsity (FCPTS) framework.","By incorporating a differentiable bridge function and a controllable optimization objective, our method allows for rapid and accurate sparsity allocation learning in minutes, with the added assurance of convergence to a predetermined global sparsity rate.","Equipped with these techniques, we can surpass the state-of-the-art methods by a large margin, e.g., over 30\\% improvement for ResNet-50 on ImageNet under the sparsity rate of 80\\%.","Our plug-and-play code and supplementary materials are open-sourced at https://github.com/ModelTC/FCPTS."],"url":"http://arxiv.org/abs/2405.05808v1","category":"cs.CV"}
{"created":"2024-05-09 13:57:28","title":"Neural Network Learning of Black-Scholes Equation for Option Pricing","abstract":"One of the most discussed problems in the financial world is stock option pricing. The Black-Scholes Equation is a Parabolic Partial Differential Equation which provides an option pricing model. The present work proposes an approach based on Neural Networks to solve the Black-Scholes Equations. Real-world data from the stock options market were used as the initial boundary to solve the Black-Scholes Equation. In particular, times series of call options prices of Brazilian companies Petrobras and Vale were employed. The results indicate that the network can learn to solve the Black-Sholes Equation for a specific real-world stock options time series. The experimental results showed that the Neural network option pricing based on the Black-Sholes Equation solution can reach an option pricing forecasting more accurate than the traditional Black-Sholes analytical solutions. The experimental results making it possible to use this methodology to make short-term call option price forecasts in options markets.","sentences":["One of the most discussed problems in the financial world is stock option pricing.","The Black-Scholes Equation is a Parabolic Partial Differential Equation which provides an option pricing model.","The present work proposes an approach based on Neural Networks to solve the Black-Scholes Equations.","Real-world data from the stock options market were used as the initial boundary to solve the Black-Scholes Equation.","In particular, times series of call options prices of Brazilian companies Petrobras and Vale were employed.","The results indicate that the network can learn to solve the Black-Sholes Equation for a specific real-world stock options time series.","The experimental results showed that the Neural network option pricing based on the Black-Sholes Equation solution can reach an option pricing forecasting more accurate than the traditional Black-Sholes analytical solutions.","The experimental results making it possible to use this methodology to make short-term call option price forecasts in options markets."],"url":"http://arxiv.org/abs/2405.05780v1","category":"cs.LG"}
{"created":"2024-05-09 13:55:00","title":"Weak coupling limit of a Brownian particle in the curl of the 2D GFF","abstract":"In this article, we study the weak coupling limit of the following equation in $\\mathbb{R}^2$: $$dX_t^\\varepsilon=\\frac{\\hat{\\lambda}}{\\sqrt{\\log\\frac1\\varepsilon}}\\omega^\\varepsilon(X_t^\\varepsilon)dt+\\nu dB_t,\\quad X_0^\\varepsilon=0. $$ Here $\\omega^\\varepsilon=\\nabla^{\\perp}\\rho_\\varepsilon*\\xi$ with $\\xi$ representing the $2d$ Gaussian Free Field (GFF) and $\\rho_\\varepsilon$ denoting an appropriate identity. $B_t$ denotes a two-dimensional standard Brownian motion, and $\\hat{\\lambda},\\nu>0$ are two given constants. We use the approach from \\cite{Cannizzaro.2023} to show that the second moment of $X_t^\\varepsilon$ under the annealed law converges to $(c(\\nu)^2+2\\nu^2)t$ with a precisely determined constant $c(\\nu)>0$, which implies a non-trivial limit of the drift terms as $\\varepsilon$ vanishes. We also prove that in this weak coupling regime, the sequence of solutions converges in distribution to $\\left(\\sqrt{\\frac{c(\\nu)^2}{2}+\\nu^2}\\right)\\widetilde{B}_t$ as $\\varepsilon$ vanishes, where $\\widetilde{B}_t$ is a two-dimensional standard Brownian motion.","sentences":["In this article, we study the weak coupling limit of the following equation in $\\mathbb{R}^2$: $$dX_t^\\varepsilon=\\frac{\\hat{\\lambda}}{\\sqrt{\\log\\frac1\\varepsilon}}\\omega^\\varepsilon(X_t^\\varepsilon)dt+\\nu dB_t,\\quad X_0^\\varepsilon=0.","$$ Here $\\omega^\\varepsilon=\\nabla^{\\perp}\\rho_\\varepsilon*\\xi$ with $\\xi$ representing the $2d$ Gaussian Free Field (GFF) and $\\rho_\\varepsilon$ denoting an appropriate identity.","$B_t$ denotes a two-dimensional standard Brownian motion, and $\\hat{\\lambda},\\nu>0$ are two given constants.","We use the approach from \\cite{Cannizzaro.2023} to show that the second moment of $X_t^\\varepsilon$ under the annealed law converges to $(c(\\nu)^2+2\\nu^2)t$ with a precisely determined constant $c(\\nu)>0$, which implies a non-trivial limit of the drift terms as $\\varepsilon$ vanishes.","We also prove that in this weak coupling regime, the sequence of solutions converges in distribution to $\\left(\\sqrt{\\frac{c(\\nu)^2}{2}+\\nu^2}\\right)\\widetilde{B}_t$ as $\\varepsilon$ vanishes, where $\\widetilde{B}_t$ is a two-dimensional standard Brownian motion."],"url":"http://arxiv.org/abs/2405.05778v1","category":"math.PR"}
{"created":"2024-05-09 13:32:26","title":"Similarity Guided Multimodal Fusion Transformer for Semantic Location Prediction in Social Media","abstract":"The purpose of semantic location prediction is to extract relevant semantic location information from multimodal social media posts, offering a more contextual understanding of daily activities compared to GPS coordinates. However, this task becomes challenging due to the presence of noise and irrelevant information in \"text-image\" pairs. Existing methods suffer from insufficient feature representations and fail to consider the comprehensive integration of similarity at different granularities, making it difficult to filter out noise and irrelevant information. To address these challenges, we propose a Similarity-Guided Multimodal Fusion Transformer (SG-MFT) for predicting social users' semantic locations. First, we utilize a pre-trained large-scale vision-language model to extract high-quality feature representations from social media posts. Then, we introduce a Similarity-Guided Interaction Module (SIM) to alleviate modality heterogeneity and noise interference by incorporating coarse-grained and fine-grained similarity guidance for modality interactions. Specifically, we propose a novel similarity-aware feature interpolation attention mechanism at the coarse level, leveraging modality-wise similarity to mitigate heterogeneity and reduce noise within each modality. Meanwhile, we employ a similarity-aware feed-forward block at the fine level, utilizing element-wise similarity to further mitigate the impact of modality heterogeneity. Building upon pre-processed features with minimal noise and modal interference, we propose a Similarity-aware Feature Fusion Module (SFM) to fuse two modalities with cross-attention mechanism. Comprehensive experimental results demonstrate the superior performance of our proposed method in handling modality imbalance while maintaining efficient fusion effectiveness.","sentences":["The purpose of semantic location prediction is to extract relevant semantic location information from multimodal social media posts, offering a more contextual understanding of daily activities compared to GPS coordinates.","However, this task becomes challenging due to the presence of noise and irrelevant information in \"text-image\" pairs.","Existing methods suffer from insufficient feature representations and fail to consider the comprehensive integration of similarity at different granularities, making it difficult to filter out noise and irrelevant information.","To address these challenges, we propose a Similarity-Guided Multimodal Fusion Transformer (SG-MFT) for predicting social users' semantic locations.","First, we utilize a pre-trained large-scale vision-language model to extract high-quality feature representations from social media posts.","Then, we introduce a Similarity-Guided Interaction Module (SIM) to alleviate modality heterogeneity and noise interference by incorporating coarse-grained and fine-grained similarity guidance for modality interactions.","Specifically, we propose a novel similarity-aware feature interpolation attention mechanism at the coarse level, leveraging modality-wise similarity to mitigate heterogeneity and reduce noise within each modality.","Meanwhile, we employ a similarity-aware feed-forward block at the fine level, utilizing element-wise similarity to further mitigate the impact of modality heterogeneity.","Building upon pre-processed features with minimal noise and modal interference, we propose a Similarity-aware Feature Fusion Module (SFM) to fuse two modalities with cross-attention mechanism.","Comprehensive experimental results demonstrate the superior performance of our proposed method in handling modality imbalance while maintaining efficient fusion effectiveness."],"url":"http://arxiv.org/abs/2405.05760v1","category":"cs.CV"}
{"created":"2024-05-09 12:35:57","title":"Private Online Community Detection for Censored Block Models","abstract":"We study the private online change detection problem for dynamic communities, using a censored block model (CBM). Focusing on the notion of edge differential privacy (DP), we seek to understand the fundamental tradeoffs between the privacy budget, detection delay, and exact community recovery of community labels. We establish the theoretical lower bound on the delay in detecting changes privately and propose an algorithm capable of identifying changes in the community structure, while maintaining user privacy. Further, we provide theoretical guarantees for the effectiveness of our proposed method by showing necessary and sufficient conditions on change detection and exact recovery under edge DP. Simulation and real data examples are provided to validate the proposed method.","sentences":["We study the private online change detection problem for dynamic communities, using a censored block model (CBM).","Focusing on the notion of edge differential privacy (DP), we seek to understand the fundamental tradeoffs between the privacy budget, detection delay, and exact community recovery of community labels.","We establish the theoretical lower bound on the delay in detecting changes privately and propose an algorithm capable of identifying changes in the community structure, while maintaining user privacy.","Further, we provide theoretical guarantees for the effectiveness of our proposed method by showing necessary and sufficient conditions on change detection and exact recovery under edge DP.","Simulation and real data examples are provided to validate the proposed method."],"url":"http://arxiv.org/abs/2405.05724v1","category":"cs.SI"}
{"created":"2024-05-09 12:10:02","title":"Characteristic-Mode Based Conformal Design of Ultra-Wideband Antenna Array","abstract":"An innovative design method of conformal array antennas is presented by utilizing characteristic mode analysis (CMA) in this work. A single-layer continuous perfect electric conductor under bending conditions is conducted by CMA to evaluate the variations in operating performance. By using this method, the design process of a conformal array is simplified. The results indicate that the operating performance of the antenna with single-layer metal radiation structure remains stable within a certain range of curvature. Subsequently, an infinite array element using single-layer metal radiation structure is designed, operating in ultra-wideband and dual polarization. Following, an 8 * 8 ultra-wideband dual-polarized cylindrical-conformal array (UDCA) is developed by wrapping the planar arrays to a cylindric surface, which has a stable operating performance even at a curvature radius as small as 100 mm. Finally, a physical prototype is cost-effectively fabricated by novel manufacturing solutions that stack three-layer conformal substrate. The experimental result demonstrates that the proposed UDCA with a 1.2{\\lambda} curvature radius operates at 3.6~9.6 GHz (90.9%) and achieves 60{\\deg} wide-angle scanning in two principal planes, which provides a practical and promising solution for conformal array applications. The insights derived from the CMA offer a direction for further advancement in conformal antenna research.","sentences":["An innovative design method of conformal array antennas is presented by utilizing characteristic mode analysis (CMA) in this work.","A single-layer continuous perfect electric conductor under bending conditions is conducted by CMA to evaluate the variations in operating performance.","By using this method, the design process of a conformal array is simplified.","The results indicate that the operating performance of the antenna with single-layer metal radiation structure remains stable within a certain range of curvature.","Subsequently, an infinite array element using single-layer metal radiation structure is designed, operating in ultra-wideband and dual polarization.","Following, an 8 * 8 ultra-wideband dual-polarized cylindrical-conformal array (UDCA) is developed by wrapping the planar arrays to a cylindric surface, which has a stable operating performance even at a curvature radius as small as 100 mm.","Finally, a physical prototype is cost-effectively fabricated by novel manufacturing solutions that stack three-layer conformal substrate.","The experimental result demonstrates that the proposed UDCA with a 1.2{\\lambda} curvature radius operates at 3.6~9.6 GHz (90.9%) and achieves 60{\\deg} wide-angle scanning in two principal planes, which provides a practical and promising solution for conformal array applications.","The insights derived from the CMA offer a direction for further advancement in conformal antenna research."],"url":"http://arxiv.org/abs/2405.05708v1","category":"cs.IT"}
{"created":"2024-05-09 11:12:03","title":"Non-asymptotic estimates for accelerated high order Langevin Monte Carlo algorithms","abstract":"In this paper, we propose two new algorithms, namely aHOLA and aHOLLA, to sample from high-dimensional target distributions with possibly super-linearly growing potentials. We establish non-asymptotic convergence bounds for aHOLA in Wasserstein-1 and Wasserstein-2 distances with rates of convergence equal to $1+q/2$ and $1/2+q/4$, respectively, under a local H\\\"{o}lder condition with exponent $q\\in(0,1]$ and a convexity at infinity condition on the potential of the target distribution. Similar results are obtained for aHOLLA under certain global continuity conditions and a dissipativity condition. Crucially, we achieve state-of-the-art rates of convergence of the proposed algorithms in the non-convex setting which are higher than those of the existing algorithms. Numerical experiments are conducted to sample from several distributions and the results support our main findings.","sentences":["In this paper, we propose two new algorithms, namely aHOLA and aHOLLA, to sample from high-dimensional target distributions with possibly super-linearly growing potentials.","We establish non-asymptotic convergence bounds for aHOLA in Wasserstein-1 and Wasserstein-2 distances with rates of convergence equal to $1+q/2$ and $1/2+q/4$, respectively, under a local H\\\"{o}lder condition with exponent $q\\in(0,1]$ and a convexity at infinity condition on the potential of the target distribution.","Similar results are obtained for aHOLLA under certain global continuity conditions and a dissipativity condition.","Crucially, we achieve state-of-the-art rates of convergence of the proposed algorithms in the non-convex setting which are higher than those of the existing algorithms.","Numerical experiments are conducted to sample from several distributions and the results support our main findings."],"url":"http://arxiv.org/abs/2405.05679v1","category":"math.ST"}
{"created":"2024-05-09 10:51:48","title":"Self-correcting GKP qubit and gates in a driven-dissipative circuit","abstract":"We propose a circuit architecture for a dissipatively error-corrected GKP qubit. The device consists of a high-impedance LC circuit coupled to a Josephson junction and a resistor via a controllable switch. When the switch is activated via a particular family of stepwise protocols, the resistor absorbs all noise-induced entropy, resulting in dissipative error correction of both phase and amplitude errors. This leads to an exponential increase of qubit lifetime, reaching beyond 10ms in simulations with near-feasible parameters. We show that the lifetime remains exponentially long in the presence of extrinsic noise and device/control imperfections (e.g., due to parasitics and finite control bandwidth) under specific thresholds. In this regime, lifetime is likely only limited by phase slips and quasiparticle tunneling. We show that the qubit can be read out and initialized via measurement of the supercurrent in the Josephson junction. We finally show that the qubit supports native self-correcting single-qubit Clifford gates, where dissipative error-correction of control noise leads to exponential suppression of gate infidelity.","sentences":["We propose a circuit architecture for a dissipatively error-corrected GKP qubit.","The device consists of a high-impedance LC circuit coupled to a Josephson junction and a resistor via a controllable switch.","When the switch is activated via a particular family of stepwise protocols, the resistor absorbs all noise-induced entropy, resulting in dissipative error correction of both phase and amplitude errors.","This leads to an exponential increase of qubit lifetime, reaching beyond 10ms in simulations with near-feasible parameters.","We show that the lifetime remains exponentially long in the presence of extrinsic noise and device/control imperfections (e.g., due to parasitics and finite control bandwidth) under specific thresholds.","In this regime, lifetime is likely only limited by phase slips and quasiparticle tunneling.","We show that the qubit can be read out and initialized via measurement of the supercurrent in the Josephson junction.","We finally show that the qubit supports native self-correcting single-qubit Clifford gates, where dissipative error-correction of control noise leads to exponential suppression of gate infidelity."],"url":"http://arxiv.org/abs/2405.05671v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-09 09:57:01","title":"Development of realistic simulations for the polarization of the cosmic microwave background","abstract":"Polarization of the cosmic microwave background (CMB) can help probe cosmic inflation (via primordial $B$ modes) and test parity-violating physics (via cosmic birefringence), but realizing the potential of these opportunities requires precise control and mitigation of systematic effects. To this end, some experiments (including LiteBIRD) will use rotating half-wave plates (HWPs) as polarization modulators. Ideally, this choice should remove the $1/f$ noise component in the observed polarization and reduce intensity-to-polarization leakage, but any real HWP is characterized by non-idealities that, if not properly treated in the analysis, can lead to new systematics. In this thesis, after briefly introducing the science case, we discuss the macro steps that make up any CMB experiment, introduce the HWP, and present a new time-ordered data (TOD) simulation pipeline tailored to a LiteBIRD-like experiment that returns TOD and binned maps for realistic beams and HWPs. We show that the simulation framework can be used to study how the HWP non-idealities affect the measured cosmic birefringence angle, resulting in a few degrees bias for a realistic choice of HWP. We also derive analytical formulae to model the observed temperature and polarization maps and test them against the simulations. Finally, we present a simple, semi-analytical end-to-end model to propagate the non-idealities through the macro-steps that make up any CMB experiment (observation of multi-frequency maps, foreground cleaning, and power spectra estimation) and compute the HWP-induced bias on the estimated tensor-to-scalar ratio, $r$, finding that the HWP leads to underestimating $r$. We also show how gain calibration of the CMB temperature can be used to partially mitigate the non-idealities' effect and present a set of recommendations for the HWP design that can help maximize the benefits of gain calibration. [abridged]","sentences":["Polarization of the cosmic microwave background (CMB) can help probe cosmic inflation (via primordial $B$ modes) and test parity-violating physics (via cosmic birefringence), but realizing the potential of these opportunities requires precise control and mitigation of systematic effects.","To this end, some experiments (including LiteBIRD) will use rotating half-wave plates (HWPs) as polarization modulators.","Ideally, this choice should remove the $1/f$ noise component in the observed polarization and reduce intensity-to-polarization leakage, but any real HWP is characterized by non-idealities that, if not properly treated in the analysis, can lead to new systematics.","In this thesis, after briefly introducing the science case, we discuss the macro steps that make up any CMB experiment, introduce the HWP, and present a new time-ordered data (TOD) simulation pipeline tailored to a LiteBIRD-like experiment that returns TOD and binned maps for realistic beams and HWPs.","We show that the simulation framework can be used to study how the HWP non-idealities affect the measured cosmic birefringence angle, resulting in a few degrees bias for a realistic choice of HWP.","We also derive analytical formulae to model the observed temperature and polarization maps and test them against the simulations.","Finally, we present a simple, semi-analytical end-to-end model to propagate the non-idealities through the macro-steps that make up any CMB experiment (observation of multi-frequency maps, foreground cleaning, and power spectra estimation) and compute the HWP-induced bias on the estimated tensor-to-scalar ratio, $r$, finding that the HWP leads to underestimating $r$. We also show how gain calibration of the CMB temperature can be used to partially mitigate the non-idealities' effect and present a set of recommendations for the HWP design that can help maximize the benefits of gain calibration.","[abridged]"],"url":"http://arxiv.org/abs/2405.05651v1","category":"astro-ph.CO"}
{"created":"2024-05-09 09:34:54","title":"Estimation of ill-conditioned models using penalized sums of squares of the residuals","abstract":"This paper analyzes the estimation of econometric models by penalizing the sum of squares of the residuals with a factor that makes the model estimates approximate those that would be obtained when considering the possible simple regressions between the dependent variable of the econometric model and each of its independent variables. It is shown that the ridge estimator is a particular case of the penalized estimator obtained, which, upon analysis of its main characteristics, presents better properties than the ridge especially in reference to the individual boostrap inference of the coefficients of the model and the numerical stability of the estimates obtained. This improvement is due to the fact that instead of shrinking the estimator towards zero, the estimator shrinks towards the estimates of the coefficients of the simple regressions discussed above.","sentences":["This paper analyzes the estimation of econometric models by penalizing the sum of squares of the residuals with a factor that makes the model estimates approximate those that would be obtained when considering the possible simple regressions between the dependent variable of the econometric model and each of its independent variables.","It is shown that the ridge estimator is a particular case of the penalized estimator obtained, which, upon analysis of its main characteristics, presents better properties than the ridge especially in reference to the individual boostrap inference of the coefficients of the model and the numerical stability of the estimates obtained.","This improvement is due to the fact that instead of shrinking the estimator towards zero, the estimator shrinks towards the estimates of the coefficients of the simple regressions discussed above."],"url":"http://arxiv.org/abs/2405.05644v1","category":"math.ST"}
{"created":"2024-05-09 09:10:11","title":"Controlled Fabrication of Native Ultra-Thin Amorphous Gallium Oxide from 2D Gallium Sulfide for Emerging Electronic Applications","abstract":"Oxidation of two-dimensional (2D) layered materials has proven advantageous in creating oxide/2D material heterostructures, opening the door for a new paradigm of low-power electronic devices. Gallium (II) sulfide ($\\beta$-GaS), a hexagonal phase group III monochalcogenide, is a wide bandgap semiconductor with a bandgap exceeding 3 eV in single and few layer form. Its oxide, gallium oxide (Ga$_2$O$_3$), combines large bandgap (4.4-5.3 eV) with high dielectric constant (~10). Despite the technological potential of both materials, controlled oxidation of atomically-thin $\\beta$-GaS remains under-explored. This study focuses into the controlled oxidation of $\\beta$-GaS using oxygen plasma treatment, achieving ultrathin native oxide (GaS$_x$O$_y$, ~4 nm) and GaS$_x$O$_y$/GaS heterostructures where the GaS layer beneath remains intact. By integrating such structures between metal electrodes and applying electric stresses as voltage ramps or pulses, we investigate their use for resistive random-access memory (ReRAM). The ultrathin nature of the produced oxide enables low operation power with energy use as low as 0.22 nJ per operation while maintaining endurance and retention of 350 cycles and 10$^4$ s, respectively. These results show the significant potential of the oxidation-based GaS$_x$O$_y$/GaS heterostructure for electronic applications and, in particular, low-power memory devices.","sentences":["Oxidation of two-dimensional (2D) layered materials has proven advantageous in creating oxide/2D material heterostructures, opening the door for a new paradigm of low-power electronic devices.","Gallium (II) sulfide ($\\beta$-GaS), a hexagonal phase group III monochalcogenide, is a wide bandgap semiconductor with a bandgap exceeding 3 eV in single and few layer form.","Its oxide, gallium oxide (Ga$_2$O$_3$), combines large bandgap (4.4-5.3 eV) with high dielectric constant (~10).","Despite the technological potential of both materials, controlled oxidation of atomically-thin $\\beta$-GaS remains under-explored.","This study focuses into the controlled oxidation of $\\beta$-GaS using oxygen plasma treatment, achieving ultrathin native oxide (GaS$_x$O$_y$, ~4 nm) and GaS$_x$O$_y$/GaS heterostructures where the GaS layer beneath remains intact.","By integrating such structures between metal electrodes and applying electric stresses as voltage ramps or pulses, we investigate their use for resistive random-access memory (ReRAM).","The ultrathin nature of the produced oxide enables low operation power with energy use as low as 0.22 nJ per operation while maintaining endurance and retention of 350 cycles and 10$^4$ s, respectively.","These results show the significant potential of the oxidation-based GaS$_x$O$_y$/GaS heterostructure for electronic applications and, in particular, low-power memory devices."],"url":"http://arxiv.org/abs/2405.05632v1","category":"physics.app-ph"}
{"created":"2024-05-09 08:54:12","title":"An Uncertainty-aware, Mesh-free Numerical Method for Kolmogorov PDEs","abstract":"This study introduces an uncertainty-aware, mesh-free numerical method for solving Kolmogorov PDEs. In the proposed method, we use Gaussian process regression (GPR) to smoothly interpolate pointwise solutions that are obtained by Monte Carlo methods based on the Feynman-Kac formula. The proposed method has two main advantages: 1. uncertainty assessment, which is facilitated by the probabilistic nature of GPR, and 2. mesh-free computation, which allows efficient handling of high-dimensional PDEs. The quality of the solution is improved by adjusting the kernel function and incorporating noise information from the Monte Carlo samples into the GPR noise model. The performance of the method is rigorously analyzed based on a theoretical lower bound on the posterior variance, which serves as a measure of the error between the numerical and true solutions. Extensive tests on three representative PDEs demonstrate the high accuracy and robustness of the method compared to existing methods.","sentences":["This study introduces an uncertainty-aware, mesh-free numerical method for solving Kolmogorov PDEs.","In the proposed method, we use Gaussian process regression (GPR) to smoothly interpolate pointwise solutions that are obtained by Monte Carlo methods based on the Feynman-Kac formula.","The proposed method has two main advantages: 1. uncertainty assessment, which is facilitated by the probabilistic nature of GPR, and 2. mesh-free computation, which allows efficient handling of high-dimensional PDEs.","The quality of the solution is improved by adjusting the kernel function and incorporating noise information from the Monte Carlo samples into the GPR noise model.","The performance of the method is rigorously analyzed based on a theoretical lower bound on the posterior variance, which serves as a measure of the error between the numerical and true solutions.","Extensive tests on three representative PDEs demonstrate the high accuracy and robustness of the method compared to existing methods."],"url":"http://arxiv.org/abs/2405.05626v1","category":"math.NA"}
{"created":"2024-05-09 08:35:38","title":"Emerging Optimization Problems for Distribution in Same-day Delivery","abstract":"Same-day deliveries (SDD) have become a new standard to satisfy the \"instant gratification\" of online customers. Despite the existing powerful technologies deployed in last-mile delivery, SDD services face new decision-making challenges related to the trade-off between delivery cost and time. In addition, new challenges related to environmental issues, customer satisfaction, or fairness arise. Researchers have explored various approaches to face these challenges in the context of SDD, where stochastic and dynamic data uncertainty plays a fundamental role. In this paper, we carefully review the emerging routing problems and solutions proposed in the existing literature for SDD services. We address the questions related to how to deal with dynamic arrival times of orders, how to allocate time slots to deliveries, how to select the right delivery options, how to design pickup and delivery routes, or how to partition the delivery areas and decide the composition of the fleet. We also formulate and compare models for representative problems elaborating on the pros and cons that might guide practitioners in choosing the most appropriate objectives and constraints. Finally, we sketch challenges and identify future research directions.","sentences":["Same-day deliveries (SDD) have become a new standard to satisfy the \"instant gratification\" of online customers.","Despite the existing powerful technologies deployed in last-mile delivery, SDD services face new decision-making challenges related to the trade-off between delivery cost and time.","In addition, new challenges related to environmental issues, customer satisfaction, or fairness arise.","Researchers have explored various approaches to face these challenges in the context of SDD, where stochastic and dynamic data uncertainty plays a fundamental role.","In this paper, we carefully review the emerging routing problems and solutions proposed in the existing literature for SDD services.","We address the questions related to how to deal with dynamic arrival times of orders, how to allocate time slots to deliveries, how to select the right delivery options, how to design pickup and delivery routes, or how to partition the delivery areas and decide the composition of the fleet.","We also formulate and compare models for representative problems elaborating on the pros and cons that might guide practitioners in choosing the most appropriate objectives and constraints.","Finally, we sketch challenges and identify future research directions."],"url":"http://arxiv.org/abs/2405.05620v1","category":"math.OC"}
{"created":"2024-05-09 07:51:20","title":"Derived equivalences of Brauer graph algebras","abstract":"The aim of this short survey is to trace back the ingredients going into the derived equivalence classification of Brauer graph algebras and into the proof of the fact that these algebras are closed under derived equivalence.","sentences":["The aim of this short survey is to trace back the ingredients going into the derived equivalence classification of Brauer graph algebras and into the proof of the fact that these algebras are closed under derived equivalence."],"url":"http://arxiv.org/abs/2405.05602v1","category":"math.RT"}
{"created":"2024-05-09 07:03:25","title":"Liouville type theorems for dual nonlocal evolution equations involving Marchaud derivatives","abstract":"In this paper, we establish a Liouville type theorem for the homogeneous dual fractional parabolic equation \\begin{equation}   \\partial^\\alpha_t u(x,t)+(-\\Delta)^s u(x,t) = 0\\ \\ \\mbox{in}\\ \\ \\mathbb{R}^n\\times\\mathbb{R} . \\end{equation} where $0<\\alpha,s<1$. Under an asymptotic assumption $$\\liminf_{|x|\\rightarrow\\infty}\\frac{u(x,t)}{|x|^\\gamma}\\geq 0 \\; ( \\mbox{or} \\; \\leq 0) \\,\\,\\mbox{for some} \\;0\\leq\\gamma\\leq 1, $$ in the case $\\frac{1}{2}<s < 1$, we prove that all solutions in the sense of distributions of above equation must be constant by employing a method of Fourier analysis. Our result includes the previous Liouville theorems on harmonic functions \\cite{ABR} and on $s$-harmonic functions \\cite{CDL} as special cases and it is still novel even restricted to one-sided Marchaud fractional equations, and our methods can be applied to a variety of dual nonlocal parabolic problems.   In the process of deriving our main result, through very delicate calculations, we obtain an optimal estimate on the decay rate of $\\left[D_{\\rm right}^\\alpha+(-\\Delta)^s\\right] \\varphi(x,t)$ for functions in Schwartz space. This sharp estimate plays a crucial role in defining the solution in the sense of distributions and will become a useful tool in the analysis of this family of equations.","sentences":["In this paper, we establish a Liouville type theorem for the homogeneous dual fractional parabolic equation \\begin{equation}   \\partial^\\alpha_t u(x,t)+(-\\Delta)^s u(x,t) = 0\\ \\ \\mbox{in}\\ \\ \\mathbb{R}^n\\times\\mathbb{R} .","\\end{equation} where $0<\\alpha,s<1$. Under an asymptotic assumption $$\\liminf_{|x|\\rightarrow\\infty}\\frac{u(x,t)}{|x|^\\gamma}\\geq 0 \\; ( \\mbox{or} \\; \\leq 0) \\,\\,\\mbox{for some} \\;0\\leq\\gamma\\leq 1, $$ in the case $\\frac{1}{2}<s < 1$, we prove that all solutions in the sense of distributions of above equation must be constant by employing a method of Fourier analysis.","Our result includes the previous Liouville theorems on harmonic functions \\cite{ABR} and on $s$-harmonic functions \\cite{CDL} as special cases and it is still novel even restricted to one-sided Marchaud fractional equations, and our methods can be applied to a variety of dual nonlocal parabolic problems.   ","In the process of deriving our main result, through very delicate calculations, we obtain an optimal estimate on the decay rate of $\\left[D_{\\rm right}^\\alpha+(-\\Delta)^s\\right] \\varphi(x,t)$ for functions in Schwartz space.","This sharp estimate plays a crucial role in defining the solution in the sense of distributions and will become a useful tool in the analysis of this family of equations."],"url":"http://arxiv.org/abs/2405.05577v1","category":"math.AP"}
{"created":"2024-05-09 05:44:44","title":"Sudden change of interferometric power for X shape states","abstract":"Quantum interferometric power (IP) is a discordlike measure. We study the dynamics of IP for two-qubit X shape states under different noisy environments. Our study shows that IP exhibits sudden change, and one side quantum channel is enough for the occurrence of a sudden change of IP. In particular, we show that the initial state having no sudden change of quantum discord exhibits a sudden change of IP under the dynamics of amplitude noise, but the converse is not true. Besides, we also investigate the dynamics of IP under two different kinds of composite noises. Our results also confirm that sudden change of IP occurs under such composite noises.","sentences":["Quantum interferometric power (IP) is a discordlike measure.","We study the dynamics of IP for two-qubit X shape states under different noisy environments.","Our study shows that IP exhibits sudden change, and one side quantum channel is enough for the occurrence of a sudden change of IP.","In particular, we show that the initial state having no sudden change of quantum discord exhibits a sudden change of IP under the dynamics of amplitude noise, but the converse is not true.","Besides, we also investigate the dynamics of IP under two different kinds of composite noises.","Our results also confirm that sudden change of IP occurs under such composite noises."],"url":"http://arxiv.org/abs/2405.05560v1","category":"quant-ph"}
{"created":"2024-05-09 04:23:31","title":"How Good Are Multi-dimensional Learned Indices? An Experimental Survey","abstract":"Efficient indexing is fundamental for multi-dimensional data management and analytics. An emerging tendency is to directly learn the storage layout of multi-dimensional data by simple machine learning models, yielding the concept of Learned Index. Compared with the conventional indices used for decades (e.g., kd-tree and R-tree variants), learned indices are empirically shown to be both space- and time-efficient on modern architectures. However, there lacks a comprehensive evaluation of existing multi-dimensional learned indices under a unified benchmark, which makes it difficult to decide the suitable index for specific data and queries and further prevents the deployment of learned indices in real application scenarios. In this paper, we present the first in-depth empirical study to answer the question of how good multi-dimensional learned indices are. Six recently published indices are evaluated under a unified experimental configuration including index implementation, datasets, query workloads, and evaluation metrics. We thoroughly investigate the evaluation results and discuss the findings that may provide insights for future learned index design.","sentences":["Efficient indexing is fundamental for multi-dimensional data management and analytics.","An emerging tendency is to directly learn the storage layout of multi-dimensional data by simple machine learning models, yielding the concept of Learned Index.","Compared with the conventional indices used for decades (e.g., kd-tree and R-tree variants), learned indices are empirically shown to be both space- and time-efficient on modern architectures.","However, there lacks a comprehensive evaluation of existing multi-dimensional learned indices under a unified benchmark, which makes it difficult to decide the suitable index for specific data and queries and further prevents the deployment of learned indices in real application scenarios.","In this paper, we present the first in-depth empirical study to answer the question of how good multi-dimensional learned indices are.","Six recently published indices are evaluated under a unified experimental configuration including index implementation, datasets, query workloads, and evaluation metrics.","We thoroughly investigate the evaluation results and discuss the findings that may provide insights for future learned index design."],"url":"http://arxiv.org/abs/2405.05536v1","category":"cs.DB"}
{"created":"2024-05-09 03:44:31","title":"Spin(ing) into the classroom: Quantum spin activities for Year 6-10 physics","abstract":"Quantum science is in the news daily and engages student interest and curiosity. A fundamental quantum science concept that underpins medical imaging, quantum computing and many future technologies is quantum spin. Quantum spin can explain many physical phenomena that are in the lower secondary school curriculum, such as magnetism and light, making its inclusion a great motivator for students. Here we present an activity sequence for teaching quantum spin in the classroom using spinning tops and gyroscopes to highlight the common properties of classical angular momentum and quantum spin. These toys can provide an easily understood window to the quantum world for lower secondary school students. Students who have engaged in these activities reported enjoying the content and appreciating its relevance.","sentences":["Quantum science is in the news daily and engages student interest and curiosity.","A fundamental quantum science concept that underpins medical imaging, quantum computing and many future technologies is quantum spin.","Quantum spin can explain many physical phenomena that are in the lower secondary school curriculum, such as magnetism and light, making its inclusion a great motivator for students.","Here we present an activity sequence for teaching quantum spin in the classroom using spinning tops and gyroscopes to highlight the common properties of classical angular momentum and quantum spin.","These toys can provide an easily understood window to the quantum world for lower secondary school students.","Students who have engaged in these activities reported enjoying the content and appreciating its relevance."],"url":"http://arxiv.org/abs/2405.05528v1","category":"physics.ed-ph"}
{"created":"2024-05-09 02:33:14","title":"Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias","abstract":"Large language models (LLMs) are increasingly essential in processing natural languages, yet their application is frequently compromised by biases and inaccuracies originating in their training data. In this study, we introduce Cross-Care, the first benchmark framework dedicated to assessing biases and real world knowledge in LLMs, specifically focusing on the representation of disease prevalence across diverse demographic groups. We systematically evaluate how demographic biases embedded in pre-training corpora like $ThePile$ influence the outputs of LLMs. We expose and quantify discrepancies by juxtaposing these biases against actual disease prevalences in various U.S. demographic groups. Our results highlight substantial misalignment between LLM representation of disease prevalence and real disease prevalence rates across demographic subgroups, indicating a pronounced risk of bias propagation and a lack of real-world grounding for medical applications of LLMs. Furthermore, we observe that various alignment methods minimally resolve inconsistencies in the models' representation of disease prevalence across different languages. For further exploration and analysis, we make all data and a data visualization tool available at: www.crosscare.net.","sentences":["Large language models (LLMs) are increasingly essential in processing natural languages, yet their application is frequently compromised by biases and inaccuracies originating in their training data.","In this study, we introduce Cross-Care, the first benchmark framework dedicated to assessing biases and real world knowledge in LLMs, specifically focusing on the representation of disease prevalence across diverse demographic groups.","We systematically evaluate how demographic biases embedded in pre-training corpora like $ThePile$ influence the outputs of LLMs.","We expose and quantify discrepancies by juxtaposing these biases against actual disease prevalences in various U.S. demographic groups.","Our results highlight substantial misalignment between LLM representation of disease prevalence and real disease prevalence rates across demographic subgroups, indicating a pronounced risk of bias propagation and a lack of real-world grounding for medical applications of LLMs.","Furthermore, we observe that various alignment methods minimally resolve inconsistencies in the models' representation of disease prevalence across different languages.","For further exploration and analysis, we make all data and a data visualization tool available at: www.crosscare.net."],"url":"http://arxiv.org/abs/2405.05506v1","category":"cs.CL"}
{"created":"2024-05-09 02:01:51","title":"Multi-Level Feature Fusion Network for Lightweight Stereo Image Super-Resolution","abstract":"Stereo image super-resolution utilizes the cross-view complementary information brought by the disparity effect of left and right perspective images to reconstruct higher-quality images. Cascading feature extraction modules and cross-view feature interaction modules to make use of the information from stereo images is the focus of numerous methods. However, this adds a great deal of network parameters and structural redundancy. To facilitate the application of stereo image super-resolution in downstream tasks, we propose an efficient Multi-Level Feature Fusion Network for Lightweight Stereo Image Super-Resolution (MFFSSR). Specifically, MFFSSR utilizes the Hybrid Attention Feature Extraction Block (HAFEB) to extract multi-level intra-view features. Using the channel separation strategy, HAFEB can efficiently interact with the embedded cross-view interaction module. This structural configuration can efficiently mine features inside the view while improving the efficiency of cross-view information sharing. Hence, reconstruct image details and textures more accurately. Abundant experiments demonstrate the effectiveness of MFFSSR. We achieve superior performance with fewer parameters. The source code is available at https://github.com/KarosLYX/MFFSSR.","sentences":["Stereo image super-resolution utilizes the cross-view complementary information brought by the disparity effect of left and right perspective images to reconstruct higher-quality images.","Cascading feature extraction modules and cross-view feature interaction modules to make use of the information from stereo images is the focus of numerous methods.","However, this adds a great deal of network parameters and structural redundancy.","To facilitate the application of stereo image super-resolution in downstream tasks, we propose an efficient Multi-Level Feature Fusion Network for Lightweight Stereo Image","Super-Resolution (MFFSSR).","Specifically, MFFSSR utilizes the Hybrid Attention Feature Extraction Block (HAFEB) to extract multi-level intra-view features.","Using the channel separation strategy, HAFEB can efficiently interact with the embedded cross-view interaction module.","This structural configuration can efficiently mine features inside the view while improving the efficiency of cross-view information sharing.","Hence, reconstruct image details and textures more accurately.","Abundant experiments demonstrate the effectiveness of MFFSSR.","We achieve superior performance with fewer parameters.","The source code is available at https://github.com/KarosLYX/MFFSSR."],"url":"http://arxiv.org/abs/2405.05497v1","category":"cs.CV"}
{"created":"2024-05-09 01:30:04","title":"Advancing Head and Neck Cancer Survival Prediction via Multi-Label Learning and Deep Model Interpretation","abstract":"A comprehensive and reliable survival prediction model is of great importance to assist in the personalized management of Head and Neck Cancer (HNC) patients treated with curative Radiation Therapy (RT). In this work, we propose IMLSP, an Interpretable Multi-Label multi-modal deep Survival Prediction framework for predicting multiple HNC survival outcomes simultaneously and provide time-event specific visual explanation of the deep prediction process. We adopt Multi-Task Logistic Regression (MTLR) layers to convert survival prediction from a regression problem to a multi-time point classification task, and to enable predicting of multiple relevant survival outcomes at the same time. We also present Grad-TEAM, a Gradient-weighted Time-Event Activation Mapping approach specifically developed for deep survival model visual explanation, to generate patient-specific time-to-event activation maps. We evaluate our method with the publicly available RADCURE HNC dataset, where it outperforms the corresponding single-modal models and single-label models on all survival outcomes. The generated activation maps show that the model focuses primarily on the tumor and nodal volumes when making the decision and the volume of interest varies for high- and low-risk patients. We demonstrate that the multi-label learning strategy can improve the learning efficiency and prognostic performance, while the interpretable survival prediction model is promising to help understand the decision-making process of AI and facilitate personalized treatment.","sentences":["A comprehensive and reliable survival prediction model is of great importance to assist in the personalized management of Head and Neck Cancer (HNC) patients treated with curative Radiation Therapy (RT).","In this work, we propose IMLSP, an Interpretable Multi-Label multi-modal deep Survival Prediction framework for predicting multiple HNC survival outcomes simultaneously and provide time-event specific visual explanation of the deep prediction process.","We adopt Multi-Task Logistic Regression (MTLR) layers to convert survival prediction from a regression problem to a multi-time point classification task, and to enable predicting of multiple relevant survival outcomes at the same time.","We also present Grad-TEAM, a Gradient-weighted Time-Event Activation Mapping approach specifically developed for deep survival model visual explanation, to generate patient-specific time-to-event activation maps.","We evaluate our method with the publicly available RADCURE HNC dataset, where it outperforms the corresponding single-modal models and single-label models on all survival outcomes.","The generated activation maps show that the model focuses primarily on the tumor and nodal volumes when making the decision and the volume of interest varies for high- and low-risk patients.","We demonstrate that the multi-label learning strategy can improve the learning efficiency and prognostic performance, while the interpretable survival prediction model is promising to help understand the decision-making process of AI and facilitate personalized treatment."],"url":"http://arxiv.org/abs/2405.05488v1","category":"cs.CV"}
{"created":"2024-05-09 01:08:43","title":"Design of Targeted Community-Based Resource Allocation in the Presence of Vaccine Hesitancy via a Data-Driven Compartmental Stochastic Optimization Model","abstract":"Vaccines have proven effective in mitigating the threat of severe infections and deaths during outbreaks of infectious diseases. However, vaccine hesitancy (VH) complicates disease spread prediction and healthcare resource assessment across regions and populations. We propose a modeling framework that integrates an epidemiological compartmental model that captures the spread of an infectious disease within a multi-stage stochastic program (MSP) that determines the allocation of critical resources under uncertainty. The proposed compartmental MSP model adaptively manages the allocation of resources to account for changes in population behavior toward vaccines (i.e., variability in VH), the unique patterns of disease spread, and the availability of healthcare resources over time and space. The compartmental MSP model allowed us to analyze the price of fairness in resource allocation. Using real COVID-19 vaccination and healthcare resource data from Arkansas, U.S. (January-May 2021), our findings include: (i) delaying the initial deployment of additional ventilators by one month could lead to an average increase in the expected number of deaths by 285.41/month, highlighting the importance of prompt action; (ii) each additional ventilator in the initial stockpile and in supply leads to a decrease in the expected number of deaths by 1.09/month and 0.962/month, respectively, emphasizing the importance of maintaining a large stockpile and scalable production response; (iii) the cost of ensuring equitable resource allocation varies over time and location, peaking during the peak of a disease outbreak and in densely populated areas. This study emphasizes the importance of flexible, informed public health decision-making and preparedness, providing a model for effective resource allocation in public health emergencies.","sentences":["Vaccines have proven effective in mitigating the threat of severe infections and deaths during outbreaks of infectious diseases.","However, vaccine hesitancy (VH) complicates disease spread prediction and healthcare resource assessment across regions and populations.","We propose a modeling framework that integrates an epidemiological compartmental model that captures the spread of an infectious disease within a multi-stage stochastic program (MSP) that determines the allocation of critical resources under uncertainty.","The proposed compartmental MSP model adaptively manages the allocation of resources to account for changes in population behavior toward vaccines (i.e., variability in VH), the unique patterns of disease spread, and the availability of healthcare resources over time and space.","The compartmental MSP model allowed us to analyze the price of fairness in resource allocation.","Using real COVID-19 vaccination and healthcare resource data from Arkansas, U.S. (January-May 2021), our findings include: (i) delaying the initial deployment of additional ventilators by one month could lead to an average increase in the expected number of deaths by 285.41/month, highlighting the importance of prompt action; (ii) each additional ventilator in the initial stockpile and in supply leads to a decrease in the expected number of deaths by 1.09/month and 0.962/month, respectively, emphasizing the importance of maintaining a large stockpile and scalable production response; (iii) the cost of ensuring equitable resource allocation varies over time and location, peaking during the peak of a disease outbreak and in densely populated areas.","This study emphasizes the importance of flexible, informed public health decision-making and preparedness, providing a model for effective resource allocation in public health emergencies."],"url":"http://arxiv.org/abs/2405.05487v1","category":"math.OC"}
{"created":"2024-05-09 01:04:34","title":"Variance Control for Black Box Variational Inference Using The James-Stein Estimator","abstract":"Black Box Variational Inference is a promising framework in a succession of recent efforts to make Variational Inference more ``black box\". However, in basic version it either fails to converge due to instability or requires some fine-tuning of the update steps prior to execution that hinder it from being completely general purpose. We propose a method for regulating its parameter updates by reframing stochastic gradient ascent as a multivariate estimation problem. We examine the properties of the James-Stein estimator as a replacement for the arithmetic mean of Monte Carlo estimates of the gradient of the evidence lower bound. The proposed method provides relatively weaker variance reduction than Rao-Blackwellization, but offers a tradeoff of being simpler and requiring no fine tuning on the part of the analyst. Performance on benchmark datasets also demonstrate a consistent performance at par or better than the Rao-Blackwellized approach in terms of model fit and time to convergence.","sentences":["Black Box Variational Inference is a promising framework in a succession of recent efforts to make Variational Inference more ``black box\".","However, in basic version it either fails to converge due to instability or requires some fine-tuning of the update steps prior to execution that hinder it from being completely general purpose.","We propose a method for regulating its parameter updates by reframing stochastic gradient ascent as a multivariate estimation problem.","We examine the properties of the James-Stein estimator as a replacement for the arithmetic mean of Monte Carlo estimates of the gradient of the evidence lower bound.","The proposed method provides relatively weaker variance reduction than Rao-Blackwellization, but offers a tradeoff of being simpler and requiring no fine tuning on the part of the analyst.","Performance on benchmark datasets also demonstrate a consistent performance at par or better than the Rao-Blackwellized approach in terms of model fit and time to convergence."],"url":"http://arxiv.org/abs/2405.05485v1","category":"cs.LG"}
{"created":"2024-05-09 00:31:59","title":"Using Machine Translation to Augment Multilingual Classification","abstract":"An all-too-present bottleneck for text classification model development is the need to annotate training data and this need is multiplied for multilingual classifiers. Fortunately, contemporary machine translation models are both easily accessible and have dependable translation quality, making it possible to translate labeled training data from one language into another. Here, we explore the effects of using machine translation to fine-tune a multilingual model for a classification task across multiple languages. We also investigate the benefits of using a novel technique, originally proposed in the field of image captioning, to account for potential negative effects of tuning models on translated data. We show that translated data are of sufficient quality to tune multilingual classifiers and that this novel loss technique is able to offer some improvement over models tuned without it.","sentences":["An all-too-present bottleneck for text classification model development is the need to annotate training data and this need is multiplied for multilingual classifiers.","Fortunately, contemporary machine translation models are both easily accessible and have dependable translation quality, making it possible to translate labeled training data from one language into another.","Here, we explore the effects of using machine translation to fine-tune a multilingual model for a classification task across multiple languages.","We also investigate the benefits of using a novel technique, originally proposed in the field of image captioning, to account for potential negative effects of tuning models on translated data.","We show that translated data are of sufficient quality to tune multilingual classifiers and that this novel loss technique is able to offer some improvement over models tuned without it."],"url":"http://arxiv.org/abs/2405.05478v1","category":"cs.CL"}
{"created":"2024-05-09 00:02:10","title":"Transformer neural networks for closed-loop adaptive optics using non-modulated pyramid wavefront sensors","abstract":"The Pyramid Wavefront Sensor (PyWFS) is highly nonlinear and requires the use of beam modulation to successfully close an AO loop under varying atmospheric turbulence conditions, at the expense of a loss in sensitivity. In this work we train, analyse, and compare the use of deep neural networks (NNs) as non-linear estimators for the non-modulated PyWFS, identifying the most suitable NN architecture for reliable closed-loop AO. We develop a novel training strategy for NNs that seeks to accommodate for changes in residual statistics between open and closed-loop, plus the addition of noise for robustness purposes. Through simulations, we test and compare several deep NNs, from classical to new convolutional neural networks (CNNs), plus a state-of-the-art transformer neural network (TNN, Global Context Visual Transformer, GCViT), first in open-loop and then in closed-loop. Using open-loop simulated data, we observe that a TNN (GCViT) largely surpasses any CNN in estimation accuracy in a wide range of turbulence conditions. Also, the TNN performs better in simulated closed-loop than CNNs, avoiding estimation issues at the pupil borders. When closing the loop at strong turbulence and low noise, the TNN using non-modulated PyWFS data is able to close the loop similar to a PyWFS with $12\\lambda/D$ of modulation. When raising the noise only the TNN is able to close the loop, while the standard linear reconstructor fails, even with modulation. Using the GCViT, we close a real AO loop in the optical bench achieving a Strehl ratio between 0.28 and 0.77 for turbulence conditions ranging from 6cm to 20cm, respectively. In conclusion, we demonstrate that a TNN is the most suitable architecture to extend the dynamic range without sacrificing sensitivity for a non-modulated PyWFS. It opens the path for using non-modulated Pyramid WFSs under an unprecedented range of atmospheric and noise conditions.","sentences":["The Pyramid Wavefront Sensor (PyWFS) is highly nonlinear and requires the use of beam modulation to successfully close an AO loop under varying atmospheric turbulence conditions, at the expense of a loss in sensitivity.","In this work we train, analyse, and compare the use of deep neural networks (NNs) as non-linear estimators for the non-modulated PyWFS, identifying the most suitable NN architecture for reliable closed-loop AO.","We develop a novel training strategy for NNs that seeks to accommodate for changes in residual statistics between open and closed-loop, plus the addition of noise for robustness purposes.","Through simulations, we test and compare several deep NNs, from classical to new convolutional neural networks (CNNs), plus a state-of-the-art transformer neural network (TNN, Global Context Visual Transformer, GCViT), first in open-loop and then in closed-loop.","Using open-loop simulated data, we observe that a TNN (GCViT) largely surpasses any CNN in estimation accuracy in a wide range of turbulence conditions.","Also, the TNN performs better in simulated closed-loop than CNNs, avoiding estimation issues at the pupil borders.","When closing the loop at strong turbulence and low noise, the TNN using non-modulated PyWFS data is able to close the loop similar to a PyWFS with $12\\lambda/D$ of modulation.","When raising the noise only the TNN is able to close the loop, while the standard linear reconstructor fails, even with modulation.","Using the GCViT, we close a real AO loop in the optical bench achieving a Strehl ratio between 0.28 and 0.77 for turbulence conditions ranging from 6cm to 20cm, respectively.","In conclusion, we demonstrate that a TNN is the most suitable architecture to extend the dynamic range without sacrificing sensitivity for a non-modulated PyWFS.","It opens the path for using non-modulated Pyramid WFSs under an unprecedented range of atmospheric and noise conditions."],"url":"http://arxiv.org/abs/2405.05472v1","category":"astro-ph.IM"}
{"created":"2024-05-08 23:37:25","title":"Taking a Moment for Distributional Robustness","abstract":"A rich line of recent work has studied distributionally robust learning approaches that seek to learn a hypothesis that performs well, in the worst-case, on many different distributions over a population. We argue that although the most common approaches seek to minimize the worst-case loss over distributions, a more reasonable goal is to minimize the worst-case distance to the true conditional expectation of labels given each covariate. Focusing on the minmax loss objective can dramatically fail to output a solution minimizing the distance to the true conditional expectation when certain distributions contain high levels of label noise. We introduce a new min-max objective based on what is known as the adversarial moment violation and show that minimizing this objective is equivalent to minimizing the worst-case $\\ell_2$-distance to the true conditional expectation if we take the adversary's strategy space to be sufficiently rich. Previous work has suggested minimizing the maximum regret over the worst-case distribution as a way to circumvent issues arising from differential noise levels. We show that in the case of square loss, minimizing the worst-case regret is also equivalent to minimizing the worst-case $\\ell_2$-distance to the true conditional expectation. Although their objective and our objective both minimize the worst-case distance to the true conditional expectation, we show that our approach provides large empirical savings in computational cost in terms of the number of groups, while providing the same noise-oblivious worst-distribution guarantee as the minimax regret approach, thus making positive progress on an open question posed by Agarwal and Zhang (2022).","sentences":["A rich line of recent work has studied distributionally robust learning approaches that seek to learn a hypothesis that performs well, in the worst-case, on many different distributions over a population.","We argue that although the most common approaches seek to minimize the worst-case loss over distributions, a more reasonable goal is to minimize the worst-case distance to the true conditional expectation of labels given each covariate.","Focusing on the minmax loss objective can dramatically fail to output a solution minimizing the distance to the true conditional expectation when certain distributions contain high levels of label noise.","We introduce a new min-max objective based on what is known as the adversarial moment violation and show that minimizing this objective is equivalent to minimizing the worst-case $\\ell_2$-distance to the true conditional expectation if we take the adversary's strategy space to be sufficiently rich.","Previous work has suggested minimizing the maximum regret over the worst-case distribution as a way to circumvent issues arising from differential noise levels.","We show that in the case of square loss, minimizing the worst-case regret is also equivalent to minimizing the worst-case $\\ell_2$-distance to the true conditional expectation.","Although their objective and our objective both minimize the worst-case distance to the true conditional expectation, we show that our approach provides large empirical savings in computational cost in terms of the number of groups, while providing the same noise-oblivious worst-distribution guarantee as the minimax regret approach, thus making positive progress on an open question posed by Agarwal and Zhang (2022)."],"url":"http://arxiv.org/abs/2405.05461v1","category":"cs.LG"}
{"created":"2024-05-08 23:27:23","title":"Estimation and Inference for Change Points in Functional Regression Time Series","abstract":"In this paper, we study the estimation and inference of change points under a functional linear regression model with changes in the slope function. We present a novel Functional Regression Binary Segmentation (FRBS) algorithm which is computationally efficient as well as achieving consistency in multiple change point detection. This algorithm utilizes the predictive power of piece-wise constant functional linear regression models in the reproducing kernel Hilbert space framework. We further propose a refinement step that improves the localization rate of the initial estimator output by FRBS, and derive asymptotic distributions of the refined estimators for two different regimes determined by the magnitude of a change. To facilitate the construction of confidence intervals for underlying change points based on the limiting distribution, we propose a consistent block-type long-run variance estimator. Our theoretical justifications for the proposed approach accommodate temporal dependence and heavy-tailedness in both the functional covariates and the measurement errors. Empirical effectiveness of our methodology is demonstrated through extensive simulation studies and an application to the Standard and Poor's 500 index dataset.","sentences":["In this paper, we study the estimation and inference of change points under a functional linear regression model with changes in the slope function.","We present a novel Functional Regression Binary Segmentation (FRBS) algorithm which is computationally efficient as well as achieving consistency in multiple change point detection.","This algorithm utilizes the predictive power of piece-wise constant functional linear regression models in the reproducing kernel Hilbert space framework.","We further propose a refinement step that improves the localization rate of the initial estimator output by FRBS, and derive asymptotic distributions of the refined estimators for two different regimes determined by the magnitude of a change.","To facilitate the construction of confidence intervals for underlying change points based on the limiting distribution, we propose a consistent block-type long-run variance estimator.","Our theoretical justifications for the proposed approach accommodate temporal dependence and heavy-tailedness in both the functional covariates and the measurement errors.","Empirical effectiveness of our methodology is demonstrated through extensive simulation studies and an application to the Standard and Poor's 500 index dataset."],"url":"http://arxiv.org/abs/2405.05459v1","category":"stat.ME"}
{"created":"2024-05-08 22:14:55","title":"Switching between superconductivity and current density waves in Bernal bilayer graphene","abstract":"An out-of-plane magnetic field can always suppress superconductivity. In Bernal-stacked bilayer graphene (BBG), recently observed activation of superconductivity (SC) through either in-plane magnetic fields or proximate spin-orbit coupling (SOC) offers a rare instance of switching superconductivity on. To understand this, we must first examine the non-superconducting state. We propose an incommensurate current density wave (CrDW) driven by van Hove singularities away from the zone corners as a competing order. We note that the two switches, the in-plane field and the SOC, both break spin degeneracy. Our parquet renormalization group analysis reveals that breaking spin degeneracy shifts the balance from CrDW, favored under spin degeneracy, to SC when degeneracy is lifted. Driven by purely repulsive interactions, the pairing symmetry of the resulting SC is $p/d$-wave. The presence of CrDW accounts for the non-linear $I-V$ behavior in the normal state and suggests potential anomalous Hall effects due to time-reversal symmetry breaking. We further predict that reducing screening could enhance SC.","sentences":["An out-of-plane magnetic field can always suppress superconductivity.","In Bernal-stacked bilayer graphene (BBG), recently observed activation of superconductivity (SC) through either in-plane magnetic fields or proximate spin-orbit coupling (SOC) offers a rare instance of switching superconductivity on.","To understand this, we must first examine the non-superconducting state.","We propose an incommensurate current density wave (CrDW) driven by van Hove singularities away from the zone corners as a competing order.","We note that the two switches, the in-plane field and the SOC, both break spin degeneracy.","Our parquet renormalization group analysis reveals that breaking spin degeneracy shifts the balance from CrDW, favored under spin degeneracy, to SC when degeneracy is lifted.","Driven by purely repulsive interactions, the pairing symmetry of the resulting SC is $p/d$-wave.","The presence of CrDW accounts for the non-linear $I-V$ behavior in the normal state and suggests potential anomalous Hall effects due to time-reversal symmetry breaking.","We further predict that reducing screening could enhance SC."],"url":"http://arxiv.org/abs/2405.05442v1","category":"cond-mat.str-el"}
{"created":"2024-05-08 21:30:17","title":"Pressure and convection robust Finite Elements for Magnetohydrodynamics","abstract":"We propose and analyze two convection quasi-robust and pressure robust finite element methods for a fully nonlinear time-dependent magnetohydrodynamics problem. The schemes make use of suitable upwind and CIP stabilizations to handle the fluid and magnetic convective terms. The developed error estimates are uniform in both diffusion parameters and optimal with respect to the diffusive norm; furthermore, for the second (more complex) method we are able to show a quicker error reduction rate in convection dominated regimes. A set of numerical tests support our theoretical findings.","sentences":["We propose and analyze two convection quasi-robust and pressure robust finite element methods for a fully nonlinear time-dependent magnetohydrodynamics problem.","The schemes make use of suitable upwind and CIP stabilizations to handle the fluid and magnetic convective terms.","The developed error estimates are uniform in both diffusion parameters and optimal with respect to the diffusive norm; furthermore, for the second (more complex) method we are able to show a quicker error reduction rate in convection dominated regimes.","A set of numerical tests support our theoretical findings."],"url":"http://arxiv.org/abs/2405.05434v1","category":"math.NA"}
{"created":"2024-05-08 21:26:59","title":"Robust Reward Placement under Uncertainty","abstract":"Reward placement is a common optimization problem in network diffusion processes, where a number of rewards are to be placed in a network so as to maximize the total reward obtained as agents move randomly in it. In many settings, the precise mobility network might be one of several possible, based on parameters outside our control, such as the weather conditions affecting peoples' transportation means. Solutions to the reward placement problem must thus be robust to this uncertainty, by achieving a high utility in all possible networks. To study such scenarios, we introduce the Robust Reward Placement problem (RRP). Agents move randomly on a Markovian Mobility Model that has a predetermined set of locations but its precise connectivity is unknown and chosen adversarialy from a known set $\\Pi$ of candidates. Network optimization is achieved by selecting a set of reward states, and the goal is to maximize the minimum, among all candidates, ratio of rewards obtained over the optimal solution for each candidate. We first prove that RRP is NP-hard and inapproximable in general. We then develop $\\Psi$-Saturate, a pseudo-polynomial time algorithm that achieves an $\\epsilon$-additive approximation by exceeding the budget constraint by a factor that scales as $O(ln|\\Pi|/\\epsilon)$. In addition, we present several heuristics, most prominently one inspired from a dynamic programming algorithm for the max-min 0-1 Knapsack problem. We corroborate our theoretical findings with an experimental evaluation of the methods in both synthetic and real-world datasets.","sentences":["Reward placement is a common optimization problem in network diffusion processes, where a number of rewards are to be placed in a network so as to maximize the total reward obtained as agents move randomly in it.","In many settings, the precise mobility network might be one of several possible, based on parameters outside our control, such as the weather conditions affecting peoples' transportation means.","Solutions to the reward placement problem must thus be robust to this uncertainty, by achieving a high utility in all possible networks.","To study such scenarios, we introduce the Robust Reward Placement problem (RRP).","Agents move randomly on a Markovian Mobility Model that has a predetermined set of locations but its precise connectivity is unknown and chosen adversarialy from a known set $\\Pi$ of candidates.","Network optimization is achieved by selecting a set of reward states, and the goal is to maximize the minimum, among all candidates, ratio of rewards obtained over the optimal solution for each candidate.","We first prove that RRP is NP-hard and inapproximable in general.","We then develop $\\Psi$-Saturate, a pseudo-polynomial time algorithm that achieves an $\\epsilon$-additive approximation by exceeding the budget constraint by a factor that scales as $O(ln|\\Pi|/\\epsilon)$. In addition, we present several heuristics, most prominently one inspired from a dynamic programming algorithm for the max-min 0-1 Knapsack problem.","We corroborate our theoretical findings with an experimental evaluation of the methods in both synthetic and real-world datasets."],"url":"http://arxiv.org/abs/2405.05433v1","category":"cs.MA"}
{"created":"2024-05-08 21:23:01","title":"Towards Invariant Time Series Forecasting in Smart Cities","abstract":"In the transformative landscape of smart cities, the integration of the cutting-edge web technologies into time series forecasting presents a pivotal opportunity to enhance urban planning, sustainability, and economic growth. The advancement of deep neural networks has significantly improved forecasting performance. However, a notable challenge lies in the ability of these models to generalize well to out-of-distribution (OOD) time series data. The inherent spatial heterogeneity and domain shifts across urban environments create hurdles that prevent models from adapting and performing effectively in new urban environments. To tackle this problem, we propose a solution to derive invariant representations for more robust predictions under different urban environments instead of relying on spurious correlation across urban environments for better generalizability. Through extensive experiments on both synthetic and real-world data, we demonstrate that our proposed method outperforms traditional time series forecasting models when tackling domain shifts in changing urban environments. The effectiveness and robustness of our method can be extended to diverse fields including climate modeling, urban planning, and smart city resource management.","sentences":["In the transformative landscape of smart cities, the integration of the cutting-edge web technologies into time series forecasting presents a pivotal opportunity to enhance urban planning, sustainability, and economic growth.","The advancement of deep neural networks has significantly improved forecasting performance.","However, a notable challenge lies in the ability of these models to generalize well to out-of-distribution (OOD) time series data.","The inherent spatial heterogeneity and domain shifts across urban environments create hurdles that prevent models from adapting and performing effectively in new urban environments.","To tackle this problem, we propose a solution to derive invariant representations for more robust predictions under different urban environments instead of relying on spurious correlation across urban environments for better generalizability.","Through extensive experiments on both synthetic and real-world data, we demonstrate that our proposed method outperforms traditional time series forecasting models when tackling domain shifts in changing urban environments.","The effectiveness and robustness of our method can be extended to diverse fields including climate modeling, urban planning, and smart city resource management."],"url":"http://arxiv.org/abs/2405.05430v1","category":"cs.LG"}
{"created":"2024-05-08 20:54:26","title":"Identifying stable communities in Hi-C data using a multifractal null model","abstract":"Chromosome capture techniques like Hi-C have expanded our understanding of mammalian genome 3D architecture and how it influences gene activity. To analyze Hi-C data sets, researchers increasingly treat them as DNA-contact networks and use standard community detection techniques to identify mesoscale 3D communities. However, there are considerable challenges in finding significant communities because the Hi-C networks have cross-scale interactions and are almost fully connected. This paper presents a pipeline to distil 3D communities that remain intact under experimental noise. To this end, we bootstrap an ensemble of Hi-C datasets representing noisy data and extract 3D communities that we compare with the unperturbed dataset. Notably, we extract the communities by maximizing local modularity (using the Generalized Louvain method), which considers the multifractal spectrum recently discovered in Hi-C maps. Our pipeline finds that stable communities (under noise) typically have above-average internal contact frequencies and tend to be enriched in active chromatin marks. We also find they fold into more nested cross-scale hierarchies than less stable ones. Apart from presenting how to systematically extract robust communities in Hi-C data, our paper offers new ways to generate null models that take advantage of the network's multifractal properties. We anticipate this has a broad applicability to several network applications.","sentences":["Chromosome capture techniques like Hi-C have expanded our understanding of mammalian genome 3D architecture and how it influences gene activity.","To analyze Hi-C data sets, researchers increasingly treat them as DNA-contact networks and use standard community detection techniques to identify mesoscale 3D communities.","However, there are considerable challenges in finding significant communities because the Hi-C networks have cross-scale interactions and are almost fully connected.","This paper presents a pipeline to distil 3D communities that remain intact under experimental noise.","To this end, we bootstrap an ensemble of Hi-C datasets representing noisy data and extract 3D communities that we compare with the unperturbed dataset.","Notably, we extract the communities by maximizing local modularity (using the Generalized Louvain method), which considers the multifractal spectrum recently discovered in Hi-C maps.","Our pipeline finds that stable communities (under noise) typically have above-average internal contact frequencies and tend to be enriched in active chromatin marks.","We also find they fold into more nested cross-scale hierarchies than less stable ones.","Apart from presenting how to systematically extract robust communities in Hi-C data, our paper offers new ways to generate null models that take advantage of the network's multifractal properties.","We anticipate this has a broad applicability to several network applications."],"url":"http://arxiv.org/abs/2405.05425v1","category":"physics.bio-ph"}
{"created":"2024-05-08 20:49:34","title":"Latent Variable Double Gaussian Process Model for Decoding Complex Neural Data","abstract":"Non-parametric models, such as Gaussian Processes (GP), show promising results in the analysis of complex data. Their applications in neuroscience data have recently gained traction. In this research, we introduce a novel neural decoder model built upon GP models. The core idea is that two GPs generate neural data and their associated labels using a set of low- dimensional latent variables. Under this modeling assumption, the latent variables represent the underlying manifold or essential features present in the neural data. When GPs are trained, the latent variable can be inferred from neural data to decode the labels with a high accuracy. We demonstrate an application of this decoder model in a verbal memory experiment dataset and show that the decoder accuracy in predicting stimulus significantly surpasses the state-of-the-art decoder models. The preceding performance of this model highlights the importance of utilizing non-parametric models in the analysis of neuroscience data.","sentences":["Non-parametric models, such as Gaussian Processes (GP), show promising results in the analysis of complex data.","Their applications in neuroscience data have recently gained traction.","In this research, we introduce a novel neural decoder model built upon GP models.","The core idea is that two GPs generate neural data and their associated labels using a set of low- dimensional latent variables.","Under this modeling assumption, the latent variables represent the underlying manifold or essential features present in the neural data.","When GPs are trained, the latent variable can be inferred from neural data to decode the labels with a high accuracy.","We demonstrate an application of this decoder model in a verbal memory experiment dataset and show that the decoder accuracy in predicting stimulus significantly surpasses the state-of-the-art decoder models.","The preceding performance of this model highlights the importance of utilizing non-parametric models in the analysis of neuroscience data."],"url":"http://arxiv.org/abs/2405.05424v1","category":"cs.LG"}
{"created":"2024-05-08 20:43:38","title":"Decompounding Under General Mixing Distributions","abstract":"This study focuses on statistical inference for compound models of the form $X=\\xi_1+\\ldots+\\xi_N$, where $N$ is a random variable denoting the count of summands, which are independent and identically distributed (i.i.d.) random variables $\\xi_1, \\xi_2, \\ldots$. The paper addresses the problem of reconstructing the distribution of $\\xi$ from observed samples of $X$'s distribution, a process referred to as decompounding, with the assumption that $N$'s distribution is known. This work diverges from the conventional scope by not limiting $N$'s distribution to the Poisson type, thus embracing a broader context. We propose a nonparametric estimate for the density of $\\xi$, derive its rates of convergence and prove that these rates are minimax optimal for suitable classes of distributions for $\\xi$ and $N$. Finally, we illustrate the numerical performance of the algorithm on simulated examples.","sentences":["This study focuses on statistical inference for compound models of the form $X=\\xi_1+\\ldots+\\xi_N$, where $N$ is a random variable denoting the count of summands, which are independent and identically distributed (i.i.d.)","random variables $\\xi_1, \\xi_2, \\ldots$.","The paper addresses the problem of reconstructing the distribution of $\\xi$ from observed samples of $X$'s distribution, a process referred to as decompounding, with the assumption that $N$'s distribution is known.","This work diverges from the conventional scope by not limiting $N$'s distribution to the Poisson type, thus embracing a broader context.","We propose a nonparametric estimate for the density of $\\xi$, derive its rates of convergence and prove that these rates are minimax optimal for suitable classes of distributions for $\\xi$ and $N$. Finally, we illustrate the numerical performance of the algorithm on simulated examples."],"url":"http://arxiv.org/abs/2405.05419v1","category":"math.ST"}
{"created":"2024-05-08 20:39:54","title":"Mitigating Exaggerated Safety in Large Language Models","abstract":"As the popularity of Large Language Models (LLMs) grow, combining model safety with utility becomes increasingly important. The challenge is making sure that LLMs can recognize and decline dangerous prompts without sacrificing their ability to be helpful. The problem of \"exaggerated safety\" demonstrates how difficult this can be. To reduce excessive safety behaviours -- which was discovered to be 26.1% of safe prompts being misclassified as dangerous and refused -- we use a combination of XSTest dataset prompts as well as interactive, contextual, and few-shot prompting to examine the decision bounds of LLMs such as Llama2, Gemma Command R+, and Phi-3. We find that few-shot prompting works best for Llama2, interactive prompting works best Gemma, and contextual prompting works best for Command R+ and Phi-3. Using a combination of these prompting strategies, we are able to mitigate exaggerated safety behaviors by an overall 92.9% across all LLMs. Our work presents a multiple prompting strategies to jailbreak LLMs' decision-making processes, allowing them to navigate the tight line between refusing unsafe prompts and remaining helpful.","sentences":["As the popularity of Large Language Models (LLMs) grow, combining model safety with utility becomes increasingly important.","The challenge is making sure that LLMs can recognize and decline dangerous prompts without sacrificing their ability to be helpful.","The problem of \"exaggerated safety\" demonstrates how difficult this can be.","To reduce excessive safety behaviours -- which was discovered to be 26.1% of safe prompts being misclassified as dangerous and refused -- we use a combination of XSTest dataset prompts as well as interactive, contextual, and few-shot prompting to examine the decision bounds of LLMs such as Llama2, Gemma Command R+, and Phi-3.","We find that few-shot prompting works best for Llama2, interactive prompting works best Gemma, and contextual prompting works best for Command R+ and Phi-3.","Using a combination of these prompting strategies, we are able to mitigate exaggerated safety behaviors by an overall 92.9% across all LLMs.","Our work presents a multiple prompting strategies to jailbreak LLMs' decision-making processes, allowing them to navigate the tight line between refusing unsafe prompts and remaining helpful."],"url":"http://arxiv.org/abs/2405.05418v1","category":"cs.CL"}
{"created":"2024-05-08 20:37:56","title":"Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models","abstract":"The disconnect between tokenizer creation and model training in language models has been known to allow for certain inputs, such as the infamous SolidGoldMagikarp token, to induce unwanted behaviour. Although such `glitch tokens' that are present in the tokenizer vocabulary, but are nearly or fully absent in training, have been observed across a variety of different models, a consistent way of identifying them has been missing. We present a comprehensive analysis of Large Language Model (LLM) tokenizers, specifically targeting this issue of detecting untrained and under-trained tokens. Through a combination of tokenizer analysis, model weight-based indicators, and prompting techniques, we develop effective methods for automatically detecting these problematic tokens. Our findings demonstrate the prevalence of such tokens across various models and provide insights into improving the efficiency and safety of language models.","sentences":["The disconnect between tokenizer creation and model training in language models has been known to allow for certain inputs, such as the infamous SolidGoldMagikarp token, to induce unwanted behaviour.","Although such `glitch tokens' that are present in the tokenizer vocabulary, but are nearly or fully absent in training, have been observed across a variety of different models, a consistent way of identifying them has been missing.","We present a comprehensive analysis of Large Language Model (LLM) tokenizers, specifically targeting this issue of detecting untrained and under-trained tokens.","Through a combination of tokenizer analysis, model weight-based indicators, and prompting techniques, we develop effective methods for automatically detecting these problematic tokens.","Our findings demonstrate the prevalence of such tokens across various models and provide insights into improving the efficiency and safety of language models."],"url":"http://arxiv.org/abs/2405.05417v1","category":"cs.CL"}
{"created":"2024-05-08 20:34:53","title":"Many-time physics in practice: characterising and controlling non-Markovian quantum stochastic processes","abstract":"Every year, substantial theoretical and experimental progress is made towards the realisation of a genuinely new computational paradigm in the construction of a quantum computer. But progress is fractal; to make headway is to unearth the next set of obstacles. Decades of work has so far overcome physical, mathematical, engineering, and information theoretic obstacles to produce the remarkable high-fidelity devices we see today. But these devices must be near perfect to be useful. Indeed, advancements so far have precipitated sensitivity to a host of complex dynamical and control-based effects. Chief among these today are non-Markovian memory effects, where interactions between a quantum system and its surrounding environment can give rise to multi-time correlations. In this thesis, we address this issue and formally present a generalisation of quantum process tomography, called process tensor tomography (PTT). This establishes the ability to rigorously and systematically characterise non-Markovian open quantum systems, resolving many long-standing issues in the field. In the first part of this work, we present an original review of the literature and motivate the problem at hand. In the second, we develop the framework of PTT, including experiment design, post-processing algorithms, and both simulated and near-term device demonstrations. In particular, we demonstrate this as a tool for obtaining in-depth diagnostics about the nature and origin of temporal quantum correlations. Lastly, we dedicate our efforts to efficiency and self-consistency. To this effect, we explore theoretically processes with sparse memory structures. We then leverage this to develop various efficient estimation techniques tailored for different settings. The result is a robust and lightweight framework capable of both reconstructing and optimally controlling any non-Markovian open quantum dynamics.","sentences":["Every year, substantial theoretical and experimental progress is made towards the realisation of a genuinely new computational paradigm in the construction of a quantum computer.","But progress is fractal; to make headway is to unearth the next set of obstacles.","Decades of work has so far overcome physical, mathematical, engineering, and information theoretic obstacles to produce the remarkable high-fidelity devices we see today.","But these devices must be near perfect to be useful.","Indeed, advancements so far have precipitated sensitivity to a host of complex dynamical and control-based effects.","Chief among these today are non-Markovian memory effects, where interactions between a quantum system and its surrounding environment can give rise to multi-time correlations.","In this thesis, we address this issue and formally present a generalisation of quantum process tomography, called process tensor tomography (PTT).","This establishes the ability to rigorously and systematically characterise non-Markovian open quantum systems, resolving many long-standing issues in the field.","In the first part of this work, we present an original review of the literature and motivate the problem at hand.","In the second, we develop the framework of PTT, including experiment design, post-processing algorithms, and both simulated and near-term device demonstrations.","In particular, we demonstrate this as a tool for obtaining in-depth diagnostics about the nature and origin of temporal quantum correlations.","Lastly, we dedicate our efforts to efficiency and self-consistency.","To this effect, we explore theoretically processes with sparse memory structures.","We then leverage this to develop various efficient estimation techniques tailored for different settings.","The result is a robust and lightweight framework capable of both reconstructing and optimally controlling any non-Markovian open quantum dynamics."],"url":"http://arxiv.org/abs/2405.05416v1","category":"quant-ph"}
{"created":"2024-05-08 20:28:44","title":"The Douglas question on the Bergman and Fock spaces","abstract":"Let $\\mu$ be a positive Borel measure and $T_\\mu$ be the bounded Toeplitz operator induced by $\\mu$ on the Bergman or Fock space. In this paper, we mainly investigate the invertibility of the Toeplitz operator $T_\\mu$ and the Douglas question on the Bergman and Fock spaces. In the Bergman-space setting, we obtain several necessary and sufficient conditions for the invertibility of $T_\\mu$ in terms of the Berezin transform of $\\mu$ and the reverse Carleson condition in two classical cases: (1) $\\mu$ is absolutely continuous with respect to the normalized area measure on the open unit disk $\\mathbb D$; (2) $\\mu$ is the pull-back measure of the normalized area measure under an analytic self-mapping of $\\mathbb D$. Nonetheless, we show that there exists a Carleson measure for the Bergman space such that its Berezin transform is bounded below but the corresponding Toeplitz operator is not invertible. On the Fock space, we show that $T_\\mu$ is invertible if and only if $\\mu$ is a reverse Carleson measure, but the invertibility of $T_\\mu$ is not completely determined by the invertibility of the Berezin transform of $\\mu$. These suggest that the answers to the Douglas question for Toeplitz operators induced by positive measures on the Bergman and Fock spaces are both negative in general cases.","sentences":["Let $\\mu$ be a positive Borel measure and $T_\\mu$ be the bounded Toeplitz operator induced by $\\mu$ on the Bergman or Fock space.","In this paper, we mainly investigate the invertibility of the Toeplitz operator $T_\\mu$ and the Douglas question on the Bergman and Fock spaces.","In the Bergman-space setting, we obtain several necessary and sufficient conditions for the invertibility of $T_\\mu$ in terms of the Berezin transform of $\\mu$ and the reverse Carleson condition in two classical cases: (1) $\\mu$ is absolutely continuous with respect to the normalized area measure on the open unit disk $\\mathbb D$; (2) $\\mu$ is the pull-back measure of the normalized area measure under an analytic self-mapping of $\\mathbb D$. Nonetheless, we show that there exists a Carleson measure for the Bergman space such that its Berezin transform is bounded below but the corresponding Toeplitz operator is not invertible.","On the Fock space, we show that $T_\\mu$ is invertible if and only if $\\mu$ is a reverse Carleson measure, but the invertibility of $T_\\mu$ is not completely determined by the invertibility of the Berezin transform of $\\mu$. These suggest that the answers to the Douglas question for Toeplitz operators induced by positive measures on the Bergman and Fock spaces are both negative in general cases."],"url":"http://arxiv.org/abs/2405.05412v1","category":"math.FA"}
{"created":"2024-05-08 20:22:11","title":"Planning with Probabilistic Opacity and Transparency: A Computational Model of Opaque/Transparent Observations","abstract":"Qualitative opacity of a secret is a security property, which means that a system trajectory satisfying the secret is observation-equivalent to a trajectory violating the secret. In this paper, we study how to synthesize a control policy that maximizes the probability of a secret being made opaque against an eavesdropping attacker/observer, while subject to other task performance constraints. In contrast to existing belief-based approach for opacity-enforcement, we develop an approach that uses the observation function, the secret, and the model of the dynamical systems to construct a so-called opaque-observations automaton which accepts the exact set of observations that enforce opacity. Leveraging this opaque-observations automaton, we can reduce the optimal planning in Markov decision processes(MDPs) for maximizing probabilistic opacity or its dual notion, transparency, subject to task constraints into a constrained planning problem over an augmented-state MDP. Finally, we illustrate the effectiveness of the developed methods in robot motion planning problems with opacity or transparency requirements.","sentences":["Qualitative opacity of a secret is a security property, which means that a system trajectory satisfying the secret is observation-equivalent to a trajectory violating the secret.","In this paper, we study how to synthesize a control policy that maximizes the probability of a secret being made opaque against an eavesdropping attacker/observer, while subject to other task performance constraints.","In contrast to existing belief-based approach for opacity-enforcement, we develop an approach that uses the observation function, the secret, and the model of the dynamical systems to construct a so-called opaque-observations automaton which accepts the exact set of observations that enforce opacity.","Leveraging this opaque-observations automaton, we can reduce the optimal planning in Markov decision processes(MDPs) for maximizing probabilistic opacity or its dual notion, transparency, subject to task constraints into a constrained planning problem over an augmented-state MDP.","Finally, we illustrate the effectiveness of the developed methods in robot motion planning problems with opacity or transparency requirements."],"url":"http://arxiv.org/abs/2405.05408v1","category":"cs.FL"}
{"created":"2024-05-08 20:21:33","title":"Differentiability of solutions for a degenerate fully nonlinear free transmission problem","abstract":"We study a fully nonlinear free transmission problem in the presence of general degeneracy terms. Under minimal conditions on the degeneracy of the model, we establish the existence of viscosity solutions for the associated Dirichlet problem. Once the existence of solutions has been established, we focus on their regularity estimates. By imposing a Dini-continuity condition on the degeneracy laws involved in the model, we prove that viscosity solutions are locally differentiable.","sentences":["We study a fully nonlinear free transmission problem in the presence of general degeneracy terms.","Under minimal conditions on the degeneracy of the model, we establish the existence of viscosity solutions for the associated Dirichlet problem.","Once the existence of solutions has been established, we focus on their regularity estimates.","By imposing a Dini-continuity condition on the degeneracy laws involved in the model, we prove that viscosity solutions are locally differentiable."],"url":"http://arxiv.org/abs/2405.05406v1","category":"math.AP"}
{"created":"2024-05-08 20:21:09","title":"Refined asymptotics for the Cauchy problem for the fast $p$-Laplace evolution equation","abstract":"Our focus is on the fast diffusion equation $\\partial_t u=\\Delta_p u$ with $p<2$ in the whole Euclidean space of dimension $N\\geq 2$. The properties of the solutions to the $p$-Laplace Cauchy problem change in several special values of the parameter $p$.   In the range of $p$ when mass is conserved, non-negative and integrable solutions behave like the Barenblatt (or fundamental) solutions for large times. By making use of the entropy method, we establish the polynomial rates of the convergence in the uniform relative error for a natural class of initial data. The convergence was established in the literature for $p$ close to $2$, but no rates were available. In particular, we allow for the values of $p$, for which the entropy is not displacement convex, as we do not apply the optimal transportation tools.   We approach the issue of long-term asymptotics of the gradients of solutions. In fact, in the case of the radial initial datum, we provide also polynomial rates of the uniform convergence in the relative error of radial derivatives of solutions for $\\frac{2N}{N+2}<p<2$.   Finally, providing an analysis of needed properties of solutions for the entropy method to work, we open the question on the full description of the basin of attraction of the Bareblatt solutions for $p$ close to $1$.","sentences":["Our focus is on the fast diffusion equation $\\partial_t u=\\Delta_p u$ with $p<2$ in the whole Euclidean space of dimension $N\\geq 2$.","The properties of the solutions to the $p$-Laplace Cauchy problem change in several special values of the parameter $p$.   In the range of $p$ when mass is conserved, non-negative and integrable solutions behave like the Barenblatt (or fundamental) solutions for large times.","By making use of the entropy method, we establish the polynomial rates of the convergence in the uniform relative error for a natural class of initial data.","The convergence was established in the literature for $p$ close to $2$, but no rates were available.","In particular, we allow for the values of $p$, for which the entropy is not displacement convex, as we do not apply the optimal transportation tools.   ","We approach the issue of long-term asymptotics of the gradients of solutions.","In fact, in the case of the radial initial datum, we provide also polynomial rates of the uniform convergence in the relative error of radial derivatives of solutions for $\\frac{2N}{N+2}<p<2$.   Finally, providing an analysis of needed properties of solutions for the entropy method to work, we open the question on the full description of the basin of attraction of the Bareblatt solutions for $p$ close to $1$."],"url":"http://arxiv.org/abs/2405.05405v1","category":"math.AP"}
{"created":"2024-05-08 20:17:41","title":"A fast and accurate inferential method for complex parametric models: the implicit bootstrap","abstract":"Performing inference such a computing confidence intervals is traditionally done, in the parametric case, by first fitting a model and then using the estimates to compute quantities derived at the asymptotic level or by means of simulations such as the ones from the family of bootstrap methods. These methods require the derivation and computation of a consistent estimator that can be very challenging to obtain when the models are complex as is the case for example when the data exhibit some types of features such as censoring, missclassification errors or contain outliers. In this paper, we propose a simulation based inferential method, the implicit bootstrap, that bypasses the need to compute a consistent estimator and can therefore be easily implemented. While being transformation respecting, we show that under similar conditions as for the studentized bootstrap, without the need of a consistent estimator, the implicit bootstrap is first and second order accurate. Using simulation studies, we also show the coverage accuracy of the method with data settings for which traditional methods are computationally very involving and also lead to poor coverage, especially when the sample size is relatively small. Based on these empirical results, we also explore theoretically the case of exact inference.","sentences":["Performing inference such a computing confidence intervals is traditionally done, in the parametric case, by first fitting a model and then using the estimates to compute quantities derived at the asymptotic level or by means of simulations such as the ones from the family of bootstrap methods.","These methods require the derivation and computation of a consistent estimator that can be very challenging to obtain when the models are complex as is the case for example when the data exhibit some types of features such as censoring, missclassification errors or contain outliers.","In this paper, we propose a simulation based inferential method, the implicit bootstrap, that bypasses the need to compute a consistent estimator and can therefore be easily implemented.","While being transformation respecting, we show that under similar conditions as for the studentized bootstrap, without the need of a consistent estimator, the implicit bootstrap is first and second order accurate.","Using simulation studies, we also show the coverage accuracy of the method with data settings for which traditional methods are computationally very involving and also lead to poor coverage, especially when the sample size is relatively small.","Based on these empirical results, we also explore theoretically the case of exact inference."],"url":"http://arxiv.org/abs/2405.05403v1","category":"stat.ME"}
{"created":"2024-05-08 20:03:12","title":"ASPIRE: Iterative Amortized Posterior Inference for Bayesian Inverse Problems","abstract":"Due to their uncertainty quantification, Bayesian solutions to inverse problems are the framework of choice in applications that are risk averse. These benefits come at the cost of computations that are in general, intractable. New advances in machine learning and variational inference (VI) have lowered the computational barrier by learning from examples. Two VI paradigms have emerged that represent different tradeoffs: amortized and non-amortized. Amortized VI can produce fast results but due to generalizing to many observed datasets it produces suboptimal inference results. Non-amortized VI is slower at inference but finds better posterior approximations since it is specialized towards a single observed dataset. Current amortized VI techniques run into a sub-optimality wall that can not be improved without more expressive neural networks or extra training data. We present a solution that enables iterative improvement of amortized posteriors that uses the same networks architectures and training data. The benefits of our method requires extra computations but these remain frugal since they are based on physics-hybrid methods and summary statistics. Importantly, these computations remain mostly offline thus our method maintains cheap and reusable online evaluation while bridging the approximation gap these two paradigms. We denote our proposed method ASPIRE - Amortized posteriors with Summaries that are Physics-based and Iteratively REfined. We first validate our method on a stylized problem with a known posterior then demonstrate its practical use on a high-dimensional and nonlinear transcranial medical imaging problem with ultrasound. Compared with the baseline and previous methods from the literature our method stands out as an computationally efficient and high-fidelity method for posterior inference.","sentences":["Due to their uncertainty quantification, Bayesian solutions to inverse problems are the framework of choice in applications that are risk averse.","These benefits come at the cost of computations that are in general, intractable.","New advances in machine learning and variational inference (VI) have lowered the computational barrier by learning from examples.","Two VI paradigms have emerged that represent different tradeoffs: amortized and non-amortized.","Amortized VI can produce fast results but due to generalizing to many observed datasets it produces suboptimal inference results.","Non-amortized VI is slower at inference but finds better posterior approximations since it is specialized towards a single observed dataset.","Current amortized VI techniques run into a sub-optimality wall that can not be improved without more expressive neural networks or extra training data.","We present a solution that enables iterative improvement of amortized posteriors that uses the same networks architectures and training data.","The benefits of our method requires extra computations but these remain frugal since they are based on physics-hybrid methods and summary statistics.","Importantly, these computations remain mostly offline thus our method maintains cheap and reusable online evaluation while bridging the approximation gap these two paradigms.","We denote our proposed method ASPIRE - Amortized posteriors with Summaries that are Physics-based and Iteratively REfined.","We first validate our method on a stylized problem with a known posterior then demonstrate its practical use on a high-dimensional and nonlinear transcranial medical imaging problem with ultrasound.","Compared with the baseline and previous methods from the literature our method stands out as an computationally efficient and high-fidelity method for posterior inference."],"url":"http://arxiv.org/abs/2405.05398v1","category":"cs.LG"}
{"created":"2024-05-08 19:55:13","title":"Carbon cycle instability for high-$\\mathrm{CO_2}$ exoplanets: Implications for habitability","abstract":"Implicit in the definition of the classical circumstellar habitable zone (HZ) is the hypothesis that the carbonate-silicate cycle can maintain clement climates on exoplanets with land and surface water across a range of instellations by adjusting atmospheric $\\mathrm{CO_2}$ partial pressure ($p\\mathrm{CO_2}$). This hypothesis is made by analogy to the Earth system, but it is an open question whether silicate weathering can stabilize climate on planets in the outer reaches of the HZ, where instellations are lower than those received by even the Archean Earth and $\\mathrm{CO_2}$ is thought likely to dominate atmospheres. Since weathering products are carried from land to ocean by the action of water, silicate weathering is intimately coupled to the hydrologic cycle, which intensifies with hotter temperatures under Earth-like conditions. Here, we use global climate model (GCM) simulations to demonstrate that the hydrologic cycle responds counterintuitively to changes in climate on planets with $\\mathrm{CO_2}$-$\\mathrm{H_2O}$ atmospheres at low instellations and high $p\\mathrm{CO_2}$, with global evaporation and precipitation decreasing as $p\\mathrm{CO_2}$ and temperatures increase at a given instellation. Within the MAC weathering formulation, weathering then decreases with increasing $p\\mathrm{CO_2}$ for a range of instellations and $p\\mathrm{CO_2}$ typical of the outer reaches of the HZ, resulting in an unstable carbon cycle that may lead to either runaway $\\mathrm{CO_2}$ accumulation or depletion of $\\mathrm{CO_2}$ to colder (possibly Snowball) conditions. While the behavior of the system has not been completely mapped out, the results suggest that silicate weathering could fail to maintain habitable conditions in the outer reaches of the nominal HZ.","sentences":["Implicit in the definition of the classical circumstellar habitable zone (HZ) is the hypothesis that the carbonate-silicate cycle can maintain clement climates on exoplanets with land and surface water across a range of instellations by adjusting atmospheric $\\mathrm{CO_2}$ partial pressure ($p\\mathrm{CO_2}$).","This hypothesis is made by analogy to the Earth system, but it is an open question whether silicate weathering can stabilize climate on planets in the outer reaches of the HZ, where instellations are lower than those received by even the Archean Earth and $\\mathrm{CO_2}$ is thought likely to dominate atmospheres.","Since weathering products are carried from land to ocean by the action of water, silicate weathering is intimately coupled to the hydrologic cycle, which intensifies with hotter temperatures under Earth-like conditions.","Here, we use global climate model (GCM) simulations to demonstrate that the hydrologic cycle responds counterintuitively to changes in climate on planets with $\\mathrm{CO_2}$-$\\mathrm{H_2O}$ atmospheres at low instellations and high $p\\mathrm{CO_2}$, with global evaporation and precipitation decreasing as $p\\mathrm{CO_2}$ and temperatures increase at a given instellation.","Within the MAC weathering formulation, weathering then decreases with increasing $p\\mathrm{CO_2}$ for a range of instellations and $p\\mathrm{CO_2}$ typical of the outer reaches of the HZ, resulting in an unstable carbon cycle that may lead to either runaway $\\mathrm{CO_2}$ accumulation or depletion of $\\mathrm{CO_2}$ to colder (possibly Snowball) conditions.","While the behavior of the system has not been completely mapped out, the results suggest that silicate weathering could fail to maintain habitable conditions in the outer reaches of the nominal HZ."],"url":"http://arxiv.org/abs/2405.05396v1","category":"astro-ph.EP"}
{"created":"2024-05-08 19:36:21","title":"Channel Capacity of Near-Field Multiuser Communications","abstract":"The channel capacity of near-field (NF) communications is characterized by considering three types of multiuser channels: i) multiple access channel (MAC), ii) broadcast channel (BC), and iii) multicast channel (MC). For NF MAC and BC, closed-form expressions are derived for the sum-rate capacity as well as the capacity region under a two-user scenario. These results are further extended to scenarios with an arbitrary number of users. For NF MC, closed-form expressions are derived for the two-user channel capacity and the capacity upper bound with more users. Further insights are gleaned by exploring special cases, including scenarios with infinitely large array apertures, co-directional users, and linear arrays. Theoretical and numerical results are presented and compared with far-field communications to demonstrate that: i) the NF capacity of these three channels converges to finite values rather than growing unboundedly as the number of array elements increases; ii) the capacity of the MAC and BC with co-directional users can be improved by using the additional range dimensions in NF channels to reduce inter-user interference (IUI); and iii) the MC capacity benefits less from the NF effect compared to the MAC and BC, as multicasting is less sensitive to IUI.","sentences":["The channel capacity of near-field (NF) communications is characterized by considering three types of multiuser channels: i) multiple access channel (MAC), ii) broadcast channel (BC), and iii) multicast channel (MC).","For NF MAC and BC, closed-form expressions are derived for the sum-rate capacity as well as the capacity region under a two-user scenario.","These results are further extended to scenarios with an arbitrary number of users.","For NF MC, closed-form expressions are derived for the two-user channel capacity and the capacity upper bound with more users.","Further insights are gleaned by exploring special cases, including scenarios with infinitely large array apertures, co-directional users, and linear arrays.","Theoretical and numerical results are presented and compared with far-field communications to demonstrate that: i) the NF capacity of these three channels converges to finite values rather than growing unboundedly as the number of array elements increases; ii) the capacity of the MAC and BC with co-directional users can be improved by using the additional range dimensions in NF channels to reduce inter-user interference (IUI); and iii) the MC capacity benefits less from the NF effect compared to the MAC and BC, as multicasting is less sensitive to IUI."],"url":"http://arxiv.org/abs/2405.05387v1","category":"cs.IT"}
{"created":"2024-05-08 19:07:28","title":"Transverse expansion of the metric at null hypersurfaces I. Uniqueness and application to Killing horizons","abstract":"This is the first in a series of two papers where we analyze the transverse expansion of the metric on a general null hypersurface. In this paper we obtain general geometric identities relating the transverse derivatives of the ambient Ricci tensor and the transverse expansion of the metric at the null hypersurface. We also explore the case where the hypersurface exhibits a generalized symmetry generator, namely a privileged vector field in the ambient space which, at the hypersurface, is null and tangent (including the possibility of zeroes). This covers the Killing, homothetic, or conformal horizon cases, and, more generally, any situation where detailed information on the deformation tensor of the symmetry generator is available. Our approach is entirely covariant, independent on any field equations, and does not make any assumptions regarding the topology or dimension of the null hypersurface. As an application we prove that the full transverse expansion of the spacetime metric at a non-degenerate Killing horizon (also allowing for bifurcation surfaces) is uniquely determined in terms of abstract data on the horizon and the tower of derivatives of the ambient Ricci tensor at the horizon. In particular, the transverse expansion of the metric in $\\Lambda$-vacuum spacetimes admitting a non-degenerate horizon is uniquely determined in terms of abstract data at the horizon.","sentences":["This is the first in a series of two papers where we analyze the transverse expansion of the metric on a general null hypersurface.","In this paper we obtain general geometric identities relating the transverse derivatives of the ambient Ricci tensor and the transverse expansion of the metric at the null hypersurface.","We also explore the case where the hypersurface exhibits a generalized symmetry generator, namely a privileged vector field in the ambient space which, at the hypersurface, is null and tangent (including the possibility of zeroes).","This covers the Killing, homothetic, or conformal horizon cases, and, more generally, any situation where detailed information on the deformation tensor of the symmetry generator is available.","Our approach is entirely covariant, independent on any field equations, and does not make any assumptions regarding the topology or dimension of the null hypersurface.","As an application we prove that the full transverse expansion of the spacetime metric at a non-degenerate Killing horizon (also allowing for bifurcation surfaces) is uniquely determined in terms of abstract data on the horizon and the tower of derivatives of the ambient Ricci tensor at the horizon.","In particular, the transverse expansion of the metric in $\\Lambda$-vacuum spacetimes admitting a non-degenerate horizon is uniquely determined in terms of abstract data at the horizon."],"url":"http://arxiv.org/abs/2405.05377v1","category":"gr-qc"}
{"created":"2024-05-08 18:52:47","title":"Model Reconstruction Using Counterfactual Explanations: Mitigating the Decision Boundary Shift","abstract":"Counterfactual explanations find ways of achieving a favorable model outcome with minimum input perturbation. However, counterfactual explanations can also be exploited to steal the model by strategically training a surrogate model to give similar predictions as the original (target) model. In this work, we investigate model extraction by specifically leveraging the fact that the counterfactual explanations also lie quite close to the decision boundary. We propose a novel strategy for model extraction that we call Counterfactual Clamping Attack (CCA) which trains a surrogate model using a unique loss function that treats counterfactuals differently than ordinary instances. Our approach also alleviates the related problem of decision boundary shift that arises in existing model extraction attacks which treat counterfactuals as ordinary instances. We also derive novel mathematical relationships between the error in model approximation and the number of queries using polytope theory. Experimental results demonstrate that our strategy provides improved fidelity between the target and surrogate model predictions on several real world datasets.","sentences":["Counterfactual explanations find ways of achieving a favorable model outcome with minimum input perturbation.","However, counterfactual explanations can also be exploited to steal the model by strategically training a surrogate model to give similar predictions as the original (target) model.","In this work, we investigate model extraction by specifically leveraging the fact that the counterfactual explanations also lie quite close to the decision boundary.","We propose a novel strategy for model extraction that we call Counterfactual Clamping Attack (CCA) which trains a surrogate model using a unique loss function that treats counterfactuals differently than ordinary instances.","Our approach also alleviates the related problem of decision boundary shift that arises in existing model extraction attacks which treat counterfactuals as ordinary instances.","We also derive novel mathematical relationships between the error in model approximation and the number of queries using polytope theory.","Experimental results demonstrate that our strategy provides improved fidelity between the target and surrogate model predictions on several real world datasets."],"url":"http://arxiv.org/abs/2405.05369v1","category":"cs.LG"}
{"created":"2024-05-08 18:51:39","title":"A Space/Time Interchange Symmetry of Rotating AdS Black Holes in General Dimensions","abstract":"We revisit the previously known local inversion symmetry of the five-dimensional Kerr-AdS metric that relates the over-rotating black hole to the under-rotating one and reinterpret it as an interchanging symmetry between time and the longitudinal angular coordinates. We generalize this to all $D$ dimensions, including $D=4$, thereby enlarging the trivial linear $\\mathbb Z_N$ symmetry of the $N=\\lfloor(D-1)/2\\rfloor$ longitudinal angular coordinates to the nonlinearly realized $\\mathbb Z_{N+1}$ symmetry that involves time.","sentences":["We revisit the previously known local inversion symmetry of the five-dimensional Kerr-AdS metric that relates the over-rotating black hole to the under-rotating one and reinterpret it as an interchanging symmetry between time and the longitudinal angular coordinates.","We generalize this to all $D$ dimensions, including $D=4$, thereby enlarging the trivial linear $\\mathbb Z_N$ symmetry of the $N=\\lfloor(D-1)/2\\rfloor$ longitudinal angular coordinates to the nonlinearly realized $\\mathbb Z_{N+1}$ symmetry that involves time."],"url":"http://arxiv.org/abs/2405.05367v1","category":"hep-th"}
{"created":"2024-05-08 18:34:18","title":"Logic-Based Discrete-Steepest Descent: A Solution Method for Process Synthesis Generalized Disjunctive Programs","abstract":"The optimization of chemical processes is challenging due to the nonlinearities arising from process physics and discrete design decisions. In particular, optimal synthesis and design of chemical processes can be posed as a Generalized Disjunctive Programming (GDP) superstructure problem. Various solution methods are available to address these problems, such as reformulating them as Mixed-Integer Nonlinear Programming (MINLP) problems; nevertheless, algorithms explicitly designed to solve the GDP problem and potentially leverage its structure remain scarce. This paper presents the Logic-based Discrete-Steepest Descent Algorithm (LD-SDA) as a solution method for GDP problems involving ordered Boolean variables. The LD-SDA reformulates these ordered Boolean variables into integer decisions called external variables. The LD-SDA solves the reformulated GDP problem using a two-level decomposition approach where the upper-level subproblem determines external variable configurations. Subsequently, the remaining continuous and discrete variables are solved as a subproblem only involving those constraints relevant to the given external variable arrangement, effectively taking advantage of the structure of the GDP problem. The advantages of LD-SDA are illustrated through a batch processing case study, a reactor superstructure, a distillation column, and a catalytic distillation column, and its open-source implementation is available online. The results show convergence efficiency and solution quality improvements compared to conventional GDP and MINLP solvers.","sentences":["The optimization of chemical processes is challenging due to the nonlinearities arising from process physics and discrete design decisions.","In particular, optimal synthesis and design of chemical processes can be posed as a Generalized Disjunctive Programming (GDP) superstructure problem.","Various solution methods are available to address these problems, such as reformulating them as Mixed-Integer Nonlinear Programming (MINLP) problems; nevertheless, algorithms explicitly designed to solve the GDP problem and potentially leverage its structure remain scarce.","This paper presents the Logic-based Discrete-Steepest Descent Algorithm (LD-SDA) as a solution method for GDP problems involving ordered Boolean variables.","The LD-SDA reformulates these ordered Boolean variables into integer decisions called external variables.","The LD-SDA solves the reformulated GDP problem using a two-level decomposition approach where the upper-level subproblem determines external variable configurations.","Subsequently, the remaining continuous and discrete variables are solved as a subproblem only involving those constraints relevant to the given external variable arrangement, effectively taking advantage of the structure of the GDP problem.","The advantages of LD-SDA are illustrated through a batch processing case study, a reactor superstructure, a distillation column, and a catalytic distillation column, and its open-source implementation is available online.","The results show convergence efficiency and solution quality improvements compared to conventional GDP and MINLP solvers."],"url":"http://arxiv.org/abs/2405.05358v1","category":"math.OC"}
{"created":"2024-05-08 18:28:08","title":"Deformation mechanism for stabilization of long-range order in ferromagnetic polycrystals","abstract":"The influence of magnetostriction on static fluctuations of the magnetic moment in ferromagnetic polycrystals has been theoretically studied. Conditions have been found under which magnetoelastic interaction leads to stabilization of long-range magnetic order in these systems.","sentences":["The influence of magnetostriction on static fluctuations of the magnetic moment in ferromagnetic polycrystals has been theoretically studied.","Conditions have been found under which magnetoelastic interaction leads to stabilization of long-range magnetic order in these systems."],"url":"http://arxiv.org/abs/2405.05350v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-08 18:20:03","title":"QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums","abstract":"Online discussion forums provide crucial data to understand the concerns of a wide range of real-world communities. However, the typical qualitative and quantitative methods used to analyze those data, such as thematic analysis and topic modeling, are infeasible to scale or require significant human effort to translate outputs to human readable forms. This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums. The framework consists of a novel prompting methodology and evaluation strategy. We applied this framework to analyze over one million comments from two Reddit's rideshare worker communities, marking the largest study of its type. We uncover significant worker concerns regarding AI and algorithmic platform decisions, responding to regulatory calls about worker insights. In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums.","sentences":["Online discussion forums provide crucial data to understand the concerns of a wide range of real-world communities.","However, the typical qualitative and quantitative methods used to analyze those data, such as thematic analysis and topic modeling, are infeasible to scale or require significant human effort to translate outputs to human readable forms.","This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums.","The framework consists of a novel prompting methodology and evaluation strategy.","We applied this framework to analyze over one million comments from two Reddit's rideshare worker communities, marking the largest study of its type.","We uncover significant worker concerns regarding AI and algorithmic platform decisions, responding to regulatory calls about worker insights.","In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums."],"url":"http://arxiv.org/abs/2405.05345v1","category":"cs.CL"}
{"created":"2024-05-08 18:19:19","title":"A note on the minimax risk of sparse linear regression","abstract":"Sparse linear regression is one of the classical and extensively studied problems in high-dimensional statistics and compressed sensing. Despite the substantial body of literature dedicated to this problem, the precise determination of its minimax risk remains elusive. This paper aims to fill this gap by deriving asymptotically constant-sharp characterization for the minimax risk of sparse linear regression. More specifically, the paper focuses on scenarios where the sparsity level, denoted as k, satisfies the condition $(k \\log p)/n {\\to} 0$, with p and n representing the number of features and observations respectively. We establish that the minimax risk under isotropic Gaussian random design is asymptotically equal to $2{\\sigma}^2k/n log(p/k)$, where ${\\sigma}$ denotes the standard deviation of the noise. In addition to this result, we will summarize the existing results in the literature, and mention some of the fundamental problems that have still remained open.","sentences":["Sparse linear regression is one of the classical and extensively studied problems in high-dimensional statistics and compressed sensing.","Despite the substantial body of literature dedicated to this problem, the precise determination of its minimax risk remains elusive.","This paper aims to fill this gap by deriving asymptotically constant-sharp characterization for the minimax risk of sparse linear regression.","More specifically, the paper focuses on scenarios where the sparsity level, denoted as k, satisfies the condition $(k \\log p)/n {\\to} 0$, with p and n representing the number of features and observations respectively.","We establish that the minimax risk under isotropic Gaussian random design is asymptotically equal to $2{\\sigma}^2k/n log(p/k)$, where ${\\sigma}$ denotes the standard deviation of the noise.","In addition to this result, we will summarize the existing results in the literature, and mention some of the fundamental problems that have still remained open."],"url":"http://arxiv.org/abs/2405.05344v1","category":"math.ST"}
{"created":"2024-05-08 18:14:16","title":"Quasielastic Lepton-Nucleus Scattering and the Correlated Fermi Gas Model","abstract":"The neutrino research program in the coming decades will require improved precision. A major source of uncertainty is the interaction of neutrinos with nuclei that serve as targets for such experiments. Broadly speaking, this interaction often depends, e.g., for charge-current quasi-elastic scattering, on the combination of ``nucleon physics\", expressed by form factors, and ``nuclear physics\", expressed by a nuclear model. It is important to get a good handle on both. We present a fully analytic implementation of the Correlated Fermi Gas Model for electron-nucleus and charge-current quasi-elastic neutrino-nucleus scattering. The implementation is used to compare separately form factors and nuclear model effects for both electron-carbon and neutrino-carbon scattering data.","sentences":["The neutrino research program in the coming decades will require improved precision.","A major source of uncertainty is the interaction of neutrinos with nuclei that serve as targets for such experiments.","Broadly speaking, this interaction often depends, e.g., for charge-current quasi-elastic scattering, on the combination of ``nucleon physics\", expressed by form factors, and ``nuclear physics\", expressed by a nuclear model.","It is important to get a good handle on both.","We present a fully analytic implementation of the Correlated Fermi Gas Model for electron-nucleus and charge-current quasi-elastic neutrino-nucleus scattering.","The implementation is used to compare separately form factors and nuclear model effects for both electron-carbon and neutrino-carbon scattering data."],"url":"http://arxiv.org/abs/2405.05342v1","category":"hep-ph"}
{"created":"2024-05-08 18:11:21","title":"Characterization of errors in a CNOT between surface code patches","abstract":"As current experiments already realize small quantum circuits on error corrected qubits, it is important to fully understand the effect of physical errors on the logical error channels of these fault-tolerant circuits. Here, we investigate a lattice-surgery-based CNOT operation between two surface code patches under phenomenological error models. (i) For two-qubit logical Pauli measurements -- the elementary building block of the CNOT -- we optimize the number of stabilizer measurement rounds, usually taken equal to $d$, the size (code distance) of each patch. We find that the optimal number can be greater or smaller than $d$, depending on the rate of physical and readout errors, and the separation between the code patches. (ii) We fully characterize the two-qubit logical error channel of the lattice-surgery-based CNOT. We find a symmetry of the CNOT protocol, that results in a symmetry of the logical error channel. We also find that correlations between X and Z errors on the logical level are suppressed under minimum weight decoding.","sentences":["As current experiments already realize small quantum circuits on error corrected qubits, it is important to fully understand the effect of physical errors on the logical error channels of these fault-tolerant circuits.","Here, we investigate a lattice-surgery-based CNOT operation between two surface code patches under phenomenological error models.","(i) For two-qubit logical Pauli measurements -- the elementary building block of the CNOT -- we optimize the number of stabilizer measurement rounds, usually taken equal to $d$, the size (code distance) of each patch.","We find that the optimal number can be greater or smaller than $d$, depending on the rate of physical and readout errors, and the separation between the code patches.","(ii) We fully characterize the two-qubit logical error channel of the lattice-surgery-based CNOT.","We find a symmetry of the CNOT protocol, that results in a symmetry of the logical error channel.","We also find that correlations between X and Z errors on the logical level are suppressed under minimum weight decoding."],"url":"http://arxiv.org/abs/2405.05337v1","category":"quant-ph"}
{"created":"2024-05-08 18:00:10","title":"Abundances of Elements in Solar Systems","abstract":"The relationship between stars and planets provides important information for understanding the interior composition, mineralogy, and overall classification of small planets (R $\\lesssim$ 3.5R$_{\\oplus}$). Since stars and planets are formed at the same time and from the same material, their compositions are inextricably linked to one another, especially with respect refractory elements like Mg, Si, and Fe. As a result, stellar elemental abundances can help break the degeneracy inherent to planetary mass-radius models and determine whether planets may be similar to the Earth in composition or if additional factors, such as formation near the host star or a giant impact, may have influenced the planet's make-up. To this end, we now have observations of the abundances of extrasolar rocks that were pulled onto the surfaces of a white dwarfs, whose compositions act as a direct insight into the interiors of small exoplanets. From measurements of $\\sim$30 of these \"polluted\" white dwarfs, we have found that composition of the extrasolar rocks are similar to Solar System chondritic meteorites.","sentences":["The relationship between stars and planets provides important information for understanding the interior composition, mineralogy, and overall classification of small planets (R $\\lesssim$ 3.5R$_{\\oplus}$).","Since stars and planets are formed at the same time and from the same material, their compositions are inextricably linked to one another, especially with respect refractory elements like Mg, Si, and Fe.","As a result, stellar elemental abundances can help break the degeneracy inherent to planetary mass-radius models and determine whether planets may be similar to the Earth in composition or if additional factors, such as formation near the host star or a giant impact, may have influenced the planet's make-up.","To this end, we now have observations of the abundances of extrasolar rocks that were pulled onto the surfaces of a white dwarfs, whose compositions act as a direct insight into the interiors of small exoplanets.","From measurements of $\\sim$30 of these \"polluted\" white dwarfs, we have found that composition of the extrasolar rocks are similar to Solar System chondritic meteorites."],"url":"http://arxiv.org/abs/2405.05324v1","category":"astro-ph.EP"}
{"created":"2024-05-08 18:00:06","title":"Nodal Spectral Functions Stabilized by Non-Hermitian Topology of Quasiparticles","abstract":"In quantum materials, basic observables such as spectral functions and susceptibilities are determined by Green's functions and their complex quasiparticle spectrum rather than by bare electrons. Even in closed many-body systems, this makes a description in terms of effective non-Hermitian (NH) Bloch Hamiltonians natural and intuitive. Here, we discuss how the abundance and stability of nodal phases is drastically affected by NH topology. While previous work has mostly considered complex degeneracies known as exceptional points as the NH counterpart of nodal points, we propose to relax this assumption by only requiring a crossing of the real part of the complex quasiparticle spectra, which entails a band crossing in the spectral function, i.e. a nodal spectral function. Interestingly, such real crossings are topologically protected by the braiding properties of the complex Bloch bands, and thus generically occur already in one-dimensional systems without symmetry or fine-tuning. We propose and study a microscopic lattice model in which a sublattice-dependent interaction stabilizes nodal spectral functions. Besides the gapless spectrum, we identify non-reciprocal charge transport properties after a local potential quench as a key signature of non-trivial band braiding. Finally, in the limit of zero interaction on one of the sublattices, we find a perfectly ballistic unidirectional mode in a non-integrable environment, reminiscent of a chiral edge state known from quantum Hall phases. Our analysis is corroborated by numerical simulations both in the framework of exact diagonalization and within the conserving second Born approximation.","sentences":["In quantum materials, basic observables such as spectral functions and susceptibilities are determined by Green's functions and their complex quasiparticle spectrum rather than by bare electrons.","Even in closed many-body systems, this makes a description in terms of effective non-Hermitian (NH) Bloch Hamiltonians natural and intuitive.","Here, we discuss how the abundance and stability of nodal phases is drastically affected by NH topology.","While previous work has mostly considered complex degeneracies known as exceptional points as the NH counterpart of nodal points, we propose to relax this assumption by only requiring a crossing of the real part of the complex quasiparticle spectra, which entails a band crossing in the spectral function, i.e. a nodal spectral function.","Interestingly, such real crossings are topologically protected by the braiding properties of the complex Bloch bands, and thus generically occur already in one-dimensional systems without symmetry or fine-tuning.","We propose and study a microscopic lattice model in which a sublattice-dependent interaction stabilizes nodal spectral functions.","Besides the gapless spectrum, we identify non-reciprocal charge transport properties after a local potential quench as a key signature of non-trivial band braiding.","Finally, in the limit of zero interaction on one of the sublattices, we find a perfectly ballistic unidirectional mode in a non-integrable environment, reminiscent of a chiral edge state known from quantum Hall phases.","Our analysis is corroborated by numerical simulations both in the framework of exact diagonalization and within the conserving second Born approximation."],"url":"http://arxiv.org/abs/2405.05322v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-08 18:00:03","title":"Is the James Webb Space Telescope detecting too many AGN candidates?","abstract":"In less than two years of operation, the James Webb Space Telescope (JWST) has already accelerated significantly our quest to identify active massive black holes (BHs) in the first billion years of the Universe's history. At the time of writing, about 50 AGN detections and candidates have been identified through spectroscopy, photometry, and/or morphology. Broad-line AGN are about a hundred times more numerous than the faint end of the UV-bright quasar population at z~5-6. In this paper, we compare the observational constraints on the abundance of these AGN at z~5 to the populations of AGN produced in large-scale cosmological simulations. Assuming a null fraction of obscured simulated AGN, we find that while some simulations produce more AGN than discovered so far, some others produce a similar abundance or even fewer AGN in the bolometric luminosity range probed by JWST. Keeping in mind the large uncertainty on the constraints, we discuss the implications for the theoretical modeling of BH formation and evolution in case similar constraints continue to accumulate. At the redshift of interest, the simulated AGN populations diverge the most at Lbol~1e44 erg/s (by more than a dex in the bolometric luminosity function). This regime is most affected by incompleteness in JWST surveys. However, it holds significant potential for constraining the physical processes determining the assembly of BHs (e.g., seeding, feedback from supernova and AGN) and the current abundance of broad-line AGN with >1e44.5 erg/s.","sentences":["In less than two years of operation, the James Webb Space Telescope (JWST) has already accelerated significantly our quest to identify active massive black holes (BHs) in the first billion years of the Universe's history.","At the time of writing, about 50 AGN detections and candidates have been identified through spectroscopy, photometry, and/or morphology.","Broad-line AGN are about a hundred times more numerous than the faint end of the UV-bright quasar population at z~5-6.","In this paper, we compare the observational constraints on the abundance of these AGN at z~5 to the populations of AGN produced in large-scale cosmological simulations.","Assuming a null fraction of obscured simulated AGN, we find that while some simulations produce more AGN than discovered so far, some others produce a similar abundance or even fewer AGN in the bolometric luminosity range probed by JWST.","Keeping in mind the large uncertainty on the constraints, we discuss the implications for the theoretical modeling of BH formation and evolution in case similar constraints continue to accumulate.","At the redshift of interest, the simulated AGN populations diverge the most at Lbol~1e44 erg/s (by more than a dex in the bolometric luminosity function).","This regime is most affected by incompleteness in JWST surveys.","However, it holds significant potential for constraining the physical processes determining the assembly of BHs (e.g., seeding, feedback from supernova and AGN) and the current abundance of broad-line AGN with >1e44.5 erg/s."],"url":"http://arxiv.org/abs/2405.05319v1","category":"astro-ph.GA"}
{"created":"2024-05-08 18:00:01","title":"$A_\\infty$ perspective to Sen's formalism","abstract":"Sen's formalism is a mechanism for eliminating constraints on the dynamical fields that are imposed independently from equations of motion by employing spurious free fields. In this note a cyclic homotopy associative algebra underlying Sen's formalism is developed. The novelty lies in the construction of a symplectic form and cyclic $A_\\infty$ maps on an extended algebra that combines the dynamical and spurious fields. This algebraic presentation makes the gauge invariance of theories using Sen's formulation manifest.","sentences":["Sen's formalism is a mechanism for eliminating constraints on the dynamical fields that are imposed independently from equations of motion by employing spurious free fields.","In this note a cyclic homotopy associative algebra underlying Sen's formalism is developed.","The novelty lies in the construction of a symplectic form and cyclic $A_\\infty$ maps on an extended algebra that combines the dynamical and spurious fields.","This algebraic presentation makes the gauge invariance of theories using Sen's formulation manifest."],"url":"http://arxiv.org/abs/2405.05310v1","category":"hep-th"}
{"created":"2024-05-08 18:00:00","title":"Renormalization-Group Improved Resummation of Super-Leading Logarithms","abstract":"A new strategy is presented for systematically treating super-leading logarithmic contributions including higher-order Glauber exchanges for non-global LHC observables in renormalization-group (RG) improved perturbation theory. This represents an important improvement over previous approaches, as it allows for the consistent inclusion of the scale dependence of the strong coupling, thereby providing more reliable estimates of the scale uncertainties in theoretical predictions. The key idea is to rearrange the relevant RG evolution operator in such a way that all double-logarithmic corrections are exponentiated from the outset. This forms the starting point for the first resummation of super-leading logarithms at leading order in RG-improved perturbation theory for arbitrary $2\\to M$ scattering processes. Moreover, the asymptotic scaling of subleading logarithmic corrections from higher-order Glauber exchanges is determined, demonstrating their parametric suppression.","sentences":["A new strategy is presented for systematically treating super-leading logarithmic contributions including higher-order Glauber exchanges for non-global LHC observables in renormalization-group (RG) improved perturbation theory.","This represents an important improvement over previous approaches, as it allows for the consistent inclusion of the scale dependence of the strong coupling, thereby providing more reliable estimates of the scale uncertainties in theoretical predictions.","The key idea is to rearrange the relevant RG evolution operator in such a way that all double-logarithmic corrections are exponentiated from the outset.","This forms the starting point for the first resummation of super-leading logarithms at leading order in RG-improved perturbation theory for arbitrary $2\\to M$ scattering processes.","Moreover, the asymptotic scaling of subleading logarithmic corrections from higher-order Glauber exchanges is determined, demonstrating their parametric suppression."],"url":"http://arxiv.org/abs/2405.05305v1","category":"hep-ph"}
{"created":"2024-05-08 16:31:06","title":"Characterization of the Autonomic Nervous System Activity in Females Classified According to Mood Scores During the Follicular Phase","abstract":"Many sexually mature females suffer from premenstrual syndrome (PMS), but effective coping methods for PMS are limited due to the complexity of symptoms and unclear pathogenesis. Awareness has shown promise in alleviating PMS symptoms but faces challenges in long-term recording and consistency. Our research goal is to establish a convenient and simple method to make individual female aware of their own psychological, and autonomic conditions. In previous research, we demonstrated that participants could be classified into non-PMS and PMS groups based on mood scores obtained during the follicular phase. However, the properties of neurophysiological activity in the participants classified by mood scores have not been elucidated. This study aimed to classify participants based on their scores on a mood questionnaire during the follicular phase and to evaluate their autonomic nervous system (ANS) activity using a simple device that measures pulse waves from the earlobe. Participants were grouped into Cluster I (high positive mood) and Cluster II (low mood). Cluster II participants showed reduced parasympathetic nervous system activity from the follicular to the menstrual phase, indicating potential PMS symptoms. The study demonstrates the feasibility of using mood scores to classify individuals into PMS and non-PMS groups and monitor ANS changes across menstrual phases. Despite limitations such as sample size and device variability, the findings highlight a promising avenue for convenient PMS self-monitoring.","sentences":["Many sexually mature females suffer from premenstrual syndrome (PMS), but effective coping methods for PMS are limited due to the complexity of symptoms and unclear pathogenesis.","Awareness has shown promise in alleviating PMS symptoms but faces challenges in long-term recording and consistency.","Our research goal is to establish a convenient and simple method to make individual female aware of their own psychological, and autonomic conditions.","In previous research, we demonstrated that participants could be classified into non-PMS and PMS groups based on mood scores obtained during the follicular phase.","However, the properties of neurophysiological activity in the participants classified by mood scores have not been elucidated.","This study aimed to classify participants based on their scores on a mood questionnaire during the follicular phase and to evaluate their autonomic nervous system (ANS) activity using a simple device that measures pulse waves from the earlobe.","Participants were grouped into Cluster I (high positive mood) and Cluster II (low mood).","Cluster II participants showed reduced parasympathetic nervous system activity from the follicular to the menstrual phase, indicating potential PMS symptoms.","The study demonstrates the feasibility of using mood scores to classify individuals into PMS and non-PMS groups and monitor ANS changes across menstrual phases.","Despite limitations such as sample size and device variability, the findings highlight a promising avenue for convenient PMS self-monitoring."],"url":"http://arxiv.org/abs/2405.05712v1","category":"q-bio.QM"}
{"created":"2024-05-09 15:01:04","title":"Efficient designs for threshold group testing without gap","abstract":"Given $d$ defective items in a population of $n$ items with $d \\ll n$, in threshold group testing without gap, the outcome of a test on a subset of items is positive if the subset has at least $u$ defective items and negative otherwise, where $1 \\leq u \\leq d$. The basic goal of threshold group testing is to quickly identify the defective items via a small number of tests. In non-adaptive design, all tests are designed independently and can be performed in parallel. The decoding time in the non-adaptive state-of-the-art work is a polynomial of $(d/u)^u (d/(d-u))^{d - u}, d$, and $\\log{n}$. In this work, we present a novel design that significantly reduces the number of tests and the decoding time to polynomials of $\\min\\{u^u, (d - u)^{d - u}\\}, d$, and $\\log{n}$. In particular, when $u$ is a constant, the number of tests and the decoding time are $O(d^3 (\\log^2{n}) \\log{(n/d)} )$ and $O\\big(d^3 (\\log^2{n}) \\log{(n/d)} + d^2 (\\log{n}) \\log^3{(n/d)} \\big)$, respectively. For a special case when $u = 2$, with non-adaptive design, the number of tests and the decoding time are $O(d^3 (\\log{n}) \\log{(n/d)} )$ and $O(d^2 (\\log{n} + \\log^2{(n/d)}) )$, respectively. Moreover, with 2-stage design, the number of tests and the decoding time are $O(d^2 \\log^2{(n/d)} )$.","sentences":["Given $d$ defective items in a population of $n$ items with $d \\ll n$, in threshold group testing without gap, the outcome of a test on a subset of items is positive if the subset has at least $u$ defective items and negative otherwise, where $1 \\leq u \\leq d$.","The basic goal of threshold group testing is to quickly identify the defective items via a small number of tests.","In non-adaptive design, all tests are designed independently and can be performed in parallel.","The decoding time in the non-adaptive state-of-the-art work is a polynomial of $(d/u)^u (d/(d-u))^{d - u}, d$, and $\\log{n}$. In this work, we present a novel design that significantly reduces the number of tests and the decoding time to polynomials of $\\min\\{u^u, (d - u)^{d - u}\\}, d$, and $\\log{n}$. In particular, when $u$ is a constant, the number of tests and the decoding time are $O(d^3 (\\log^2{n}) \\log{(n/d)} )$ and $O\\big(d^3 (\\log^2{n}) \\log{(n/d)}","+ d^2 (\\log{n})","\\log^3{(n/d)} \\big)$, respectively.","For a special case when $u = 2$, with non-adaptive design, the number of tests and the decoding time are $O(d^3 (\\log{n}) \\log{(n/d)} )$ and $O(d^2 (\\log{n} + \\log^2{(n/d)}) )$, respectively.","Moreover, with 2-stage design, the number of tests and the decoding time are $O(d^2 \\log^2{(n/d)} )$."],"url":"http://arxiv.org/abs/2405.05827v1","category":"cs.IT"}
{"created":"2024-05-09 02:00:07","title":"Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis","abstract":"Aspect-based sentiment analysis (ABSA) is an important subtask of sentiment analysis, which aims to extract the aspects and predict their sentiments. Most existing studies focus on improving the performance of the target domain by fine-tuning domain-specific models (trained on source domains) based on the target domain dataset. Few works propose continual learning tasks for ABSA, which aim to learn the target domain's ability while maintaining the history domains' abilities. In this paper, we propose a Large Language Model-based Continual Learning (\\texttt{LLM-CL}) model for ABSA. First, we design a domain knowledge decoupling module to learn a domain-invariant adapter and separate domain-variant adapters dependently with an orthogonal constraint. Then, we introduce a domain knowledge warmup strategy to align the representation between domain-invariant and domain-variant knowledge. In the test phase, we index the corresponding domain-variant knowledge via domain positioning to not require each sample's domain ID. Extensive experiments over 19 datasets indicate that our \\texttt{LLM-CL} model obtains new state-of-the-art performance.","sentences":["Aspect-based sentiment analysis (ABSA) is an important subtask of sentiment analysis, which aims to extract the aspects and predict their sentiments.","Most existing studies focus on improving the performance of the target domain by fine-tuning domain-specific models (trained on source domains) based on the target domain dataset.","Few works propose continual learning tasks for ABSA, which aim to learn the target domain's ability while maintaining the history domains' abilities.","In this paper, we propose a Large Language Model-based Continual Learning (\\texttt{LLM-CL}) model for ABSA.","First, we design a domain knowledge decoupling module to learn a domain-invariant adapter and separate domain-variant adapters dependently with an orthogonal constraint.","Then, we introduce a domain knowledge warmup strategy to align the representation between domain-invariant and domain-variant knowledge.","In the test phase, we index the corresponding domain-variant knowledge via domain positioning to not require each sample's domain ID.","Extensive experiments over 19 datasets indicate that our \\texttt{LLM-CL} model obtains new state-of-the-art performance."],"url":"http://arxiv.org/abs/2405.05496v1","category":"cs.CL"}
{"created":"2024-05-09 01:38:27","title":"Atomic layer etching of SiO$_2$ using sequential exposures of Al(CH$_3$)$_3$ and H$_2$/SF$_6$ plasma","abstract":"On-chip photonic devices based on SiO$_2$ are of interest for applications such as microresonator gyroscopes and microwave sources. Although SiO$_2$ microdisk resonators have achieved quality factors exceeding one billion, this value remains an order of magnitude less than the intrinsic limit due to surface roughness scattering. Atomic layer etching (ALE) has potential to mitigate this scattering because of its ability to smooth surfaces to sub-nanometer length scales. While isotropic ALE processes for SiO$_2$ have been reported, they are not generally compatible with commercial reactors, and the effect on surface roughness has not been studied. Here, we report an ALE process for SiO$_2$ using sequential exposures of Al(CH$_3$)$_3$ (trimethylaluminum, TMA) and Ar/H$_2$/SF$_6$ plasma. We find that each process step is self-limiting, and that the overall process exhibits a synergy of 100%. We observe etch rates up to 0.58 \\r{A} per cycle for thermally-grown SiO$_2$ and higher rates for ALD, PECVD, and sputtered SiO$_2$ up to 2.38 \\r{A} per cycle. Furthermore, we observe a decrease in surface roughness by 62% on a roughened film. The residual concentration of Al and F is around 1-2%, which can be further decreased by O$_2$ plasma treatment. This process could find applications in smoothing of SiO$_2$ optical devices and thereby enabling device quality factors to approach limits set by intrinsic dissipation.","sentences":["On-chip photonic devices based on SiO$_2$ are of interest for applications such as microresonator gyroscopes and microwave sources.","Although SiO$_2$ microdisk resonators have achieved quality factors exceeding one billion, this value remains an order of magnitude less than the intrinsic limit due to surface roughness scattering.","Atomic layer etching (ALE) has potential to mitigate this scattering because of its ability to smooth surfaces to sub-nanometer length scales.","While isotropic ALE processes for SiO$_2$ have been reported, they are not generally compatible with commercial reactors, and the effect on surface roughness has not been studied.","Here, we report an ALE process for SiO$_2$ using sequential exposures of Al(CH$_3$)$_3$ (trimethylaluminum, TMA) and Ar/H$_2$/SF$_6$ plasma.","We find that each process step is self-limiting, and that the overall process exhibits a synergy of 100%.","We observe etch rates up to 0.58 \\r{A} per cycle for thermally-grown SiO$_2$ and higher rates for ALD, PECVD, and sputtered SiO$_2$ up to 2.38 \\r{A} per cycle.","Furthermore, we observe a decrease in surface roughness by 62% on a roughened film.","The residual concentration of Al and F is around 1-2%, which can be further decreased by O$_2$ plasma treatment.","This process could find applications in smoothing of SiO$_2$ optical devices and thereby enabling device quality factors to approach limits set by intrinsic dissipation."],"url":"http://arxiv.org/abs/2405.05491v1","category":"physics.app-ph"}
{"created":"2024-05-09 00:30:45","title":"DynaSeg: A Deep Dynamic Fusion Method for Unsupervised Image Segmentation Incorporating Feature Similarity and Spatial Continuity","abstract":"Our work tackles the fundamental challenge of image segmentation in computer vision, which is crucial for diverse applications. While supervised methods demonstrate proficiency, their reliance on extensive pixel-level annotations limits scalability. In response to this challenge, we present an enhanced unsupervised Convolutional Neural Network (CNN)-based algorithm called DynaSeg. Unlike traditional approaches that rely on a fixed weight factor to balance feature similarity and spatial continuity, requiring manual adjustments, our novel, dynamic weighting scheme automates parameter tuning, adapting flexibly to image details. We also introduce the novel concept of a Silhouette Score Phase that addresses the challenge of dynamic clustering during iterations. Additionally, our methodology integrates both CNN-based and pre-trained ResNet feature extraction, offering a comprehensive and adaptable approach. We achieve state-of-the-art results on diverse datasets, with a notable 12.2% and 14.12% mIOU improvement compared to the current benchmarks on COCO-All and COCO-Stuff, respectively. The proposed approach unlocks the potential for unsupervised image segmentation and addresses scalability concerns in real-world scenarios by obviating the need for meticulous parameter tuning.","sentences":["Our work tackles the fundamental challenge of image segmentation in computer vision, which is crucial for diverse applications.","While supervised methods demonstrate proficiency, their reliance on extensive pixel-level annotations limits scalability.","In response to this challenge, we present an enhanced unsupervised Convolutional Neural Network (CNN)-based algorithm called DynaSeg.","Unlike traditional approaches that rely on a fixed weight factor to balance feature similarity and spatial continuity, requiring manual adjustments, our novel, dynamic weighting scheme automates parameter tuning, adapting flexibly to image details.","We also introduce the novel concept of a Silhouette Score Phase that addresses the challenge of dynamic clustering during iterations.","Additionally, our methodology integrates both CNN-based and pre-trained ResNet feature extraction, offering a comprehensive and adaptable approach.","We achieve state-of-the-art results on diverse datasets, with a notable 12.2% and 14.12% mIOU improvement compared to the current benchmarks on COCO-All and COCO-Stuff, respectively.","The proposed approach unlocks the potential for unsupervised image segmentation and addresses scalability concerns in real-world scenarios by obviating the need for meticulous parameter tuning."],"url":"http://arxiv.org/abs/2405.05477v1","category":"cs.CV"}
{"created":"2024-05-08 18:33:12","title":"Geometry-Informed Distance Candidate Selection for Adaptive Lightweight Omnidirectional Stereo Vision with Fisheye Images","abstract":"Multi-view stereo omnidirectional distance estimation usually needs to build a cost volume with many hypothetical distance candidates. The cost volume building process is often computationally heavy considering the limited resources a mobile robot has. We propose a new geometry-informed way of distance candidates selection method which enables the use of a very small number of candidates and reduces the computational cost. We demonstrate the use of the geometry-informed candidates in a set of model variants. We find that by adjusting the candidates during robot deployment, our geometry-informed distance candidates also improve a pre-trained model's accuracy if the extrinsics or the number of cameras changes. Without any re-training or fine-tuning, our models outperform models trained with evenly distributed distance candidates. Models are also released as hardware-accelerated versions with a new dedicated large-scale dataset. The project page, code, and dataset can be found at https://theairlab.org/gicandidates/ .","sentences":["Multi-view stereo omnidirectional distance estimation usually needs to build a cost volume with many hypothetical distance candidates.","The cost volume building process is often computationally heavy considering the limited resources a mobile robot has.","We propose a new geometry-informed way of distance candidates selection method which enables the use of a very small number of candidates and reduces the computational cost.","We demonstrate the use of the geometry-informed candidates in a set of model variants.","We find that by adjusting the candidates during robot deployment, our geometry-informed distance candidates also improve a pre-trained model's accuracy if the extrinsics or the number of cameras changes.","Without any re-training or fine-tuning, our models outperform models trained with evenly distributed distance candidates.","Models are also released as hardware-accelerated versions with a new dedicated large-scale dataset.","The project page, code, and dataset can be found at https://theairlab.org/gicandidates/ ."],"url":"http://arxiv.org/abs/2405.05355v1","category":"cs.CV"}
{"created":"2024-05-08 13:33:32","title":"Deep Learning Method to Predict Wound Healing Progress Based on Collagen Fibers in Wound Tissue","abstract":"Wound healing is a complex process involving changes in collagen fibers. Accurate monitoring of these changes is crucial for assessing the progress of wound healing and has significant implications for guiding clinical treatment strategies and drug screening. However, traditional quantitative analysis methods focus on spatial characteristics such as collagen fiber alignment and variance, lacking threshold standards to differentiate between different stages of wound healing. To address this issue, we propose an innovative approach based on deep learning to predict the progression of wound healing by analyzing collagen fiber features in histological images of wound tissue. Leveraging the unique learning capabilities of deep learning models, our approach captures the feature variations of collagen fibers in histological images from different categories and classifies them into various stages of wound healing. To overcome the limited availability of histological image data, we employ a transfer learning strategy. Specifically, we fine-tune a VGG16 model pretrained on the ImageNet dataset to adapt it to the classification task of histological images of wounds. Through this process, our model achieves 82% accuracy in classifying six stages of wound healing. Furthermore, to enhance the interpretability of the model, we employ a class activation mapping technique called LayerCAM. LayerCAM reveals the image regions on which the model relies when making predictions, providing transparency to the model's decision-making process. This visualization not only helps us understand how the model identifies and evaluates collagen fiber features but also enhances trust in the model's prediction results. To the best of our knowledge, our proposed model is the first deep learning-based classification model used for predicting wound healing stages.","sentences":["Wound healing is a complex process involving changes in collagen fibers.","Accurate monitoring of these changes is crucial for assessing the progress of wound healing and has significant implications for guiding clinical treatment strategies and drug screening.","However, traditional quantitative analysis methods focus on spatial characteristics such as collagen fiber alignment and variance, lacking threshold standards to differentiate between different stages of wound healing.","To address this issue, we propose an innovative approach based on deep learning to predict the progression of wound healing by analyzing collagen fiber features in histological images of wound tissue.","Leveraging the unique learning capabilities of deep learning models, our approach captures the feature variations of collagen fibers in histological images from different categories and classifies them into various stages of wound healing.","To overcome the limited availability of histological image data, we employ a transfer learning strategy.","Specifically, we fine-tune a VGG16 model pretrained on the ImageNet dataset to adapt it to the classification task of histological images of wounds.","Through this process, our model achieves 82% accuracy in classifying six stages of wound healing.","Furthermore, to enhance the interpretability of the model, we employ a class activation mapping technique called LayerCAM.","LayerCAM reveals the image regions on which the model relies when making predictions, providing transparency to the model's decision-making process.","This visualization not only helps us understand how the model identifies and evaluates collagen fiber features but also enhances trust in the model's prediction results.","To the best of our knowledge, our proposed model is the first deep learning-based classification model used for predicting wound healing stages."],"url":"http://arxiv.org/abs/2405.05297v1","category":"cs.CV"}
{"created":"2024-05-08 10:03:50","title":"Harmonizing Program Induction with Rate-Distortion Theory","abstract":"Many aspects of human learning have been proposed as a process of constructing mental programs: from acquiring symbolic number representations to intuitive theories about the world. In parallel, there is a long-tradition of using information processing to model human cognition through Rate Distortion Theory (RDT). Yet, it is still poorly understood how to apply RDT when mental representations take the form of programs. In this work, we adapt RDT by proposing a three way trade-off among rate (description length), distortion (error), and computational costs (search budget). We use simulations on a melody task to study the implications of this trade-off, and show that constructing a shared program library across tasks provides global benefits. However, this comes at the cost of sensitivity to curricula, which is also characteristic of human learners. Finally, we use methods from partial information decomposition to generate training curricula that induce more effective libraries and better generalization.","sentences":["Many aspects of human learning have been proposed as a process of constructing mental programs: from acquiring symbolic number representations to intuitive theories about the world.","In parallel, there is a long-tradition of using information processing to model human cognition through Rate Distortion Theory (RDT).","Yet, it is still poorly understood how to apply RDT when mental representations take the form of programs.","In this work, we adapt RDT by proposing a three way trade-off among rate (description length), distortion (error), and computational costs (search budget).","We use simulations on a melody task to study the implications of this trade-off, and show that constructing a shared program library across tasks provides global benefits.","However, this comes at the cost of sensitivity to curricula, which is also characteristic of human learners.","Finally, we use methods from partial information decomposition to generate training curricula that induce more effective libraries and better generalization."],"url":"http://arxiv.org/abs/2405.05294v1","category":"cs.HC"}
{"created":"2024-05-08 09:46:25","title":"Comparison of two different integration methods for the (1+1)-Dimensional Schr\u00f6dinger-Poisson Equation","abstract":"We compare two different numerical methods to integrate in time spatially delocalized initial densities using the Schr\\\"odinger-Poisson equation system as the evolution law. The basic equation is a nonlinear Schr\\\"odinger equation with an auto-gravitating potential created by the wave function density itself. The latter is determined as a solution of Poisson's equation modelling, e.g., non-relativistic gravity. For reasons of complexity, we treat a one-dimensional version of the problem whose numerical integration is still challenging because of the extreme long-range forces (being constant in the asymptotic limit). Both of our methods, a Strang splitting scheme and a basis function approach using B-splines, are compared in numerical convergence and effectivity. Overall, our Strang-splitting evolution compares favourably with the B-spline method. In particular, by using an adaptive time-stepper rather large one-dimensional boxes can be treated. These results give hope for extensions to two spatial dimensions for not too small boxes and large evolution times necessary for describing, for instance, dark matter formation over cosmologically relevant scales.","sentences":["We compare two different numerical methods to integrate in time spatially delocalized initial densities using the Schr\\\"odinger-Poisson equation system as the evolution law.","The basic equation is a nonlinear Schr\\\"odinger equation with an auto-gravitating potential created by the wave function density itself.","The latter is determined as a solution of Poisson's equation modelling, e.g., non-relativistic gravity.","For reasons of complexity, we treat a one-dimensional version of the problem whose numerical integration is still challenging because of the extreme long-range forces (being constant in the asymptotic limit).","Both of our methods, a Strang splitting scheme and a basis function approach using B-splines, are compared in numerical convergence and effectivity.","Overall, our Strang-splitting evolution compares favourably with the B-spline method.","In particular, by using an adaptive time-stepper rather large one-dimensional boxes can be treated.","These results give hope for extensions to two spatial dimensions for not too small boxes and large evolution times necessary for describing, for instance, dark matter formation over cosmologically relevant scales."],"url":"http://arxiv.org/abs/2405.04924v2","category":"gr-qc"}
{"created":"2024-05-09 17:58:25","title":"Age Aware Scheduling for Differentially-Private Federated Learning","abstract":"This paper explores differentially-private federated learning (FL) across time-varying databases, delving into a nuanced three-way tradeoff involving age, accuracy, and differential privacy (DP). Emphasizing the potential advantages of scheduling, we propose an optimization problem aimed at meeting DP requirements while minimizing the loss difference between the aggregated model and the model obtained without DP constraints. To harness the benefits of scheduling, we introduce an age-dependent upper bound on the loss, leading to the development of an age-aware scheduling design. Simulation results underscore the superior performance of our proposed scheme compared to FL with classic DP, which does not consider scheduling as a design factor. This research contributes insights into the interplay of age, accuracy, and DP in federated learning, with practical implications for scheduling strategies.","sentences":["This paper explores differentially-private federated learning (FL) across time-varying databases, delving into a nuanced three-way tradeoff involving age, accuracy, and differential privacy (DP).","Emphasizing the potential advantages of scheduling, we propose an optimization problem aimed at meeting DP requirements while minimizing the loss difference between the aggregated model and the model obtained without DP constraints.","To harness the benefits of scheduling, we introduce an age-dependent upper bound on the loss, leading to the development of an age-aware scheduling design.","Simulation results underscore the superior performance of our proposed scheme compared to FL with classic DP, which does not consider scheduling as a design factor.","This research contributes insights into the interplay of age, accuracy, and DP in federated learning, with practical implications for scheduling strategies."],"url":"http://arxiv.org/abs/2405.05962v1","category":"cs.LG"}
{"created":"2024-05-09 17:53:28","title":"OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning","abstract":"Large Language Models (LLMs) have played an important role in many fields due to their powerful capabilities.However, their massive number of parameters leads to high deployment requirements and incurs significant inference costs, which impedes their practical applications. Training smaller models is an effective way to address this problem. Therefore, we introduce OpenBA-V2, a 3.4B model derived from multi-stage compression and continual pre-training from the original 15B OpenBA model. OpenBA-V2 utilizes more data, more flexible training objectives, and techniques such as layer pruning, neural pruning, and vocabulary pruning to achieve a compression rate of 77.3\\% with minimal performance loss. OpenBA-V2 demonstrates competitive performance compared to other open-source models of similar size, achieving results close to or on par with the 15B OpenBA model in downstream tasks such as common sense reasoning and Named Entity Recognition (NER). OpenBA-V2 illustrates that LLMs can be compressed into smaller ones with minimal performance loss by employing advanced training objectives and data strategies, which may help deploy LLMs in resource-limited scenarios.","sentences":["Large Language Models (LLMs) have played an important role in many fields due to their powerful capabilities.","However, their massive number of parameters leads to high deployment requirements and incurs significant inference costs, which impedes their practical applications.","Training smaller models is an effective way to address this problem.","Therefore, we introduce OpenBA-V2, a 3.4B model derived from multi-stage compression and continual pre-training from the original 15B OpenBA model.","OpenBA-V2 utilizes more data, more flexible training objectives, and techniques such as layer pruning, neural pruning, and vocabulary pruning to achieve a compression rate of 77.3\\% with minimal performance loss.","OpenBA-V2 demonstrates competitive performance compared to other open-source models of similar size, achieving results close to or on par with the 15B OpenBA model in downstream tasks such as common sense reasoning and Named Entity Recognition (NER).","OpenBA-V2 illustrates that LLMs can be compressed into smaller ones with minimal performance loss by employing advanced training objectives and data strategies, which may help deploy LLMs in resource-limited scenarios."],"url":"http://arxiv.org/abs/2405.05957v1","category":"cs.CL"}
{"created":"2024-05-09 16:47:47","title":"A note on the volume entropy on a harmonic manifold of hypergeometric type","abstract":"Harmonic manifolds of hypergeometric type form a class of non-compact harmonic manifolds that includes rank one symmetric spaces of non-compact type and Damek-Ricci spaces. When normalizing the metric of a harmonic manifold of hypergeometric type to satisfy the Ricci curvature $\\mathrm{Ric} = -(n-1)$, we show that the volume entropy of this manifold satisfies a certain inequality. Additionally, we show that manifolds yielding the upper bound of volume entropy are only real hyperbolic spaces with sectional curvature $-1$, while examples of Damek-Ricci spaces yielding the lower bound exist in only four cases.","sentences":["Harmonic manifolds of hypergeometric type form a class of non-compact harmonic manifolds that includes rank one symmetric spaces of non-compact type and Damek-Ricci spaces.","When normalizing the metric of a harmonic manifold of hypergeometric type to satisfy the Ricci curvature $\\mathrm{Ric} = -(n-1)$, we show that the volume entropy of this manifold satisfies a certain inequality.","Additionally, we show that manifolds yielding the upper bound of volume entropy are only real hyperbolic spaces with sectional curvature $-1$, while examples of Damek-Ricci spaces yielding the lower bound exist in only four cases."],"url":"http://arxiv.org/abs/2405.05896v1","category":"math.DG"}
{"created":"2024-05-09 16:00:49","title":"The Iwasawa $\u03bc$-invariant of certain elliptic curves of analytic rank zero","abstract":"This paper is about the Iwasawa theory of elliptic curves over the cyclotomic $\\mathbb{Z}_p$-extension $\\mathbb{Q}^{\\text{cyc}}$ of $\\mathbb{Q}$. We discuss a deep conjecture of Greenberg that if $E/\\mathbb{Q}$ is an elliptic curve with good ordinary reduction at $p$, and $E[p]$ is irreducible as a Galois module, then the Selmer group of $E$ over $\\mathbb{Q}^{\\text{cyc}}$ has $\\mu$-invariant zero. We prove new cases of Greenberg's conjecture for some elliptic curves of analytic rank $0$. The proof involves studying the $p$-adic $L$-function of $E$. The crucial input is a new technique using the Rankin-Selberg method.","sentences":["This paper is about the Iwasawa theory of elliptic curves over the cyclotomic $\\mathbb{Z}_p$-extension $\\mathbb{Q}^{\\text{cyc}}$ of $\\mathbb{Q}$. We discuss a deep conjecture of Greenberg that if $E/\\mathbb{Q}$ is an elliptic curve with good ordinary reduction at $p$, and $E[p]$ is irreducible as a Galois module, then the Selmer group of $E$ over $\\mathbb{Q}^{\\text{cyc}}$ has $\\mu$-invariant zero.","We prove new cases of Greenberg's conjecture for some elliptic curves of analytic rank $0$. The proof involves studying the $p$-adic $L$-function of $E$. The crucial input is a new technique using the Rankin-Selberg method."],"url":"http://arxiv.org/abs/2405.05871v1","category":"math.NT"}
{"created":"2024-05-09 15:13:24","title":"K-stable valuations and Calabi-Yau metrics on affine spherical varieties","abstract":"After providing an explicit K-stability condition for a $\\mathbb{Q}$-Gorenstein log spherical cone, we prove the existence and uniqueness of an equivariant K-stable degeneration of the cone, and deduce uniqueness of the asymptotic cone of a given complete $K$-invariant Calabi-Yau metric in the trivial class of an affine $G$-spherical manifold, $K$ being the maximal compact subgroup of $G$.   Next, we prove that the valuation induced by $K$-invariant Calabi-Yau metrics on affine $G$-spherical manifolds is in fact $G$-invariant. As an application, we point out an affine smoothing of a Calabi-Yau cone that does not admit any $K$-invariant Calabi-Yau metrics asymptotic to the cone. Another corollary is that on $\\mathbb{C}^3$, there are no other complete Calabi-Yau metrics with maximal volume growth and spherical symmetry other than the standard flat metric and the Li-Conlon-Rochon-Sz\\'ekelyhidi metrics with horospherical asymptotic cone. This answers the question whether there is a nontrivial asymptotic cone with smooth cross section on $\\mathbb{C}^{3}$ raised by Conlon-Rochon when the symmetry is spherical.","sentences":["After providing an explicit K-stability condition for a $\\mathbb{Q}$-Gorenstein log spherical cone, we prove the existence and uniqueness of an equivariant K-stable degeneration of the cone, and deduce uniqueness of the asymptotic cone of a given complete $K$-invariant Calabi-Yau metric in the trivial class of an affine $G$-spherical manifold, $K$ being the maximal compact subgroup of $G$.   Next, we prove that the valuation induced by $K$-invariant Calabi-Yau metrics on affine $G$-spherical manifolds is in fact $G$-invariant.","As an application, we point out an affine smoothing of a Calabi-Yau cone that does not admit any $K$-invariant Calabi-Yau metrics asymptotic to the cone.","Another corollary is that on $\\mathbb{C}^3$, there are no other complete Calabi-Yau metrics with maximal volume growth and spherical symmetry other than the standard flat metric and the Li-Conlon-Rochon-Sz\\'ekelyhidi metrics with horospherical asymptotic cone.","This answers the question whether there is a nontrivial asymptotic cone with smooth cross section on $\\mathbb{C}^{3}$ raised by Conlon-Rochon when the symmetry is spherical."],"url":"http://arxiv.org/abs/2405.05833v1","category":"math.AG"}
{"created":"2024-05-09 14:11:26","title":"Determining the population of Large Meteoroids in Major Meteor Showers","abstract":"We have estimated the largest meteoroids present in major meteor showers from observations conducted between 2019-2022 by the Geostationary Lightning Mapper (GLM) instrument on the GOES-R satellites. Our integrated time area products for the Leonids, Perseids and eta Aquariids are of order 5 x 10^10 km2 hours. We compute photometric masses for shower fireballs using the approach of Vojacek et al. 2022 to correct from narrow-band GLM luminosity to bolometric luminosity and apply the luminous efficiency relation of Ceplecha and McCrosky 1976 at high speeds. Between 2019 and 2022, the showers definitely observed by GLM were the Leonids, Perseids, and eta Aquariids, with probable detections of the Orionids and Taurids. We find the largest meteoroids to be of order 7 kg for the Leonids, 3 kg for the Perseids, and 3 kg for the eta Aquariids, corresponding to meteoroids of ~ 0.2m diameter. The Orionids and Taurids had maximum meteoroid masses of 4 kg and 150 kg respectively. The Leonids and eta Aquariids are well fit by a single power-law with differential mass exponent, s, of 2.08 +/- 0.08 and 2.00 +/- 0.09 over the mass range 10^-7 < m < 1 kg. All showers had maximum meteoroid masses compatible with Whipple gas-drag ejection, with the exception of the Perseids which have much larger meteoroids than expected which is also consistent with observations from ground based instruments. This may reflect preferential ejection in narrow jets or possibly some form of mantle erosion/release in the past for the parent comet, 109P/Swift-Tuttle.","sentences":["We have estimated the largest meteoroids present in major meteor showers from observations conducted between 2019-2022 by the Geostationary Lightning Mapper (GLM) instrument on the GOES-R satellites.","Our integrated time area products for the Leonids, Perseids and eta Aquariids are of order 5 x 10^10 km2 hours.","We compute photometric masses for shower fireballs using the approach of Vojacek et al. 2022 to correct from narrow-band GLM luminosity to bolometric luminosity and apply the luminous efficiency relation of Ceplecha and McCrosky 1976 at high speeds.","Between 2019 and 2022, the showers definitely observed by GLM were the Leonids, Perseids, and eta Aquariids, with probable detections of the Orionids and Taurids.","We find the largest meteoroids to be of order 7 kg for the Leonids, 3 kg for the Perseids, and 3 kg for the eta Aquariids, corresponding to meteoroids of ~ 0.2m diameter.","The Orionids and Taurids had maximum meteoroid masses of 4 kg and 150 kg respectively.","The Leonids and eta Aquariids are well fit by a single power-law with differential mass exponent, s, of 2.08 +/- 0.08 and 2.00 +/- 0.09 over the mass range 10^-7 < m < 1 kg.","All showers had maximum meteoroid masses compatible with Whipple gas-drag ejection, with the exception of the Perseids which have much larger meteoroids than expected which is also consistent with observations from ground based instruments.","This may reflect preferential ejection in narrow jets or possibly some form of mantle erosion/release in the past for the parent comet, 109P/Swift-Tuttle."],"url":"http://arxiv.org/abs/2405.05788v1","category":"astro-ph.EP"}
{"created":"2024-05-09 13:53:39","title":"On the dimension of the singular set of perimeter minimizers in spaces with a two-sided Ricci curvature bound","abstract":"We show that the Hausdorff dimension of the singular set of perimeter minimizers in non-collapsed Ricci limit spaces with a two-sided Ricci curvature bound is at most $N-5$, where $N$ is the dimension of the ambient space. The estimate is sharp.","sentences":["We show that the Hausdorff dimension of the singular set of perimeter minimizers in non-collapsed Ricci limit spaces with a two-sided Ricci curvature bound is at most $N-5$, where $N$ is the dimension of the ambient space.","The estimate is sharp."],"url":"http://arxiv.org/abs/2405.05775v1","category":"math.DG"}
{"created":"2024-05-09 13:49:45","title":"Monoidal bicategories, differential linear logic, and analytic functors","abstract":"We develop further the theory of monoidal bicategories by introducing and studying bicate- gorical counterparts of the notions of a linear explonential comonad, as considered in the study of linear logic, and of a codereliction transformation, introduced to study differential linear logic via differential categories. As an application, we extend the differential calculus of Joyal's analytic functors to analytic functors between presheaf categories, just as ordinary calculus extends from a single variable to many variables.","sentences":["We develop further the theory of monoidal bicategories by introducing and studying bicate- gorical counterparts of the notions of a linear explonential comonad, as considered in the study of linear logic, and of a codereliction transformation, introduced to study differential linear logic via differential categories.","As an application, we extend the differential calculus of Joyal's analytic functors to analytic functors between presheaf categories, just as ordinary calculus extends from a single variable to many variables."],"url":"http://arxiv.org/abs/2405.05774v1","category":"math.CT"}
{"created":"2024-05-09 11:45:47","title":"Matter Asymmetries in the $Z_N$ Dark matter -companion Models","abstract":"A class of $Z_{N\\geq 3}$-symmetric WIMP dark matter models that are characterized by the semi-annihilation into the companion of dark matter has been proposed in ref.~\\cite{Guo:2021rre}, providing a mechanism to evade the stringent direct detection constraint. In this work, we point out that such models naturally provide the three Sakharov elements necessary for dark matter asymmetry, and moreover this asymmetry can be transferred to the visible sector with a proper link to the leptonic or quark sector. In our minimal $Z_3$ example, the migration to the leptonic sector is via the asymmetric companion decay into neutrinos, and the lepton asymmetry can be further transferred to the quark sector. The CP violation parameter in the model is suppressed in the limit of static annihilation of dark matter, and the lift from thermal motion has been studied for the first time. A preliminary numerical analysis based on the Boltzmann equations shows that both correct relic density of dark matter and baryon asymmetry can be accommodated.","sentences":["A class of $Z_{N\\geq 3}$-symmetric WIMP dark matter models that are characterized by the semi-annihilation into the companion of dark matter has been proposed in ref.~\\cite{Guo:2021rre}, providing a mechanism to evade the stringent direct detection constraint.","In this work, we point out that such models naturally provide the three Sakharov elements necessary for dark matter asymmetry, and moreover this asymmetry can be transferred to the visible sector with a proper link to the leptonic or quark sector.","In our minimal $Z_3$ example, the migration to the leptonic sector is via the asymmetric companion decay into neutrinos, and the lepton asymmetry can be further transferred to the quark sector.","The CP violation parameter in the model is suppressed in the limit of static annihilation of dark matter, and the lift from thermal motion has been studied for the first time.","A preliminary numerical analysis based on the Boltzmann equations shows that both correct relic density of dark matter and baryon asymmetry can be accommodated."],"url":"http://arxiv.org/abs/2405.05694v1","category":"hep-ph"}
{"created":"2024-05-09 11:18:55","title":"NLO corrections to $J/\u03c8+c+\\bar{c}$ photoproduction","abstract":"Based on the factorization framework of nonrelativistic quantum chromodynamics, we study the associated $J/\\psi+c+\\bar{c}$ photoproduction process at next-to-leading order in $\\alpha_s$ and leading order in the velocity expansion. The total cross section and differential cross section in $p_T^2$, $W$ and $z$ are presented. The results indicate that the next-to-leading order corrections are substantial, and testable in experiment.","sentences":["Based on the factorization framework of nonrelativistic quantum chromodynamics, we study the associated $J/\\psi+c+\\bar{c}$ photoproduction process at next-to-leading order in $\\alpha_s$ and leading order in the velocity expansion.","The total cross section and differential cross section in $p_T^2$, $W$ and $z$ are presented.","The results indicate that the next-to-leading order corrections are substantial, and testable in experiment."],"url":"http://arxiv.org/abs/2405.05683v1","category":"hep-ph"}
{"created":"2024-05-09 10:35:47","title":"Uniqueness, non-degeneracy, and exact multiplicity of positive solutions for superlinear elliptic problems","abstract":"In this paper, we focus our attention on the positive solutions to second-order nonlinear ordinary differential equations of the form $u''+q(t)g(u)=0$, where $q$ is a sign-changing weight and $g$ is a superlinear function. We exploit the classical shooting approach and the comparison theorem to present non-degeneracy and exact multiplicity results for positive solutions. This completes the multiplicity results obtained by Feltrin and Zanolin. Numerical examples and some related open problems are also discussed.","sentences":["In this paper, we focus our attention on the positive solutions to second-order nonlinear ordinary differential equations of the form $u''+q(t)g(u)=0$, where $q$ is a sign-changing weight and $g$ is a superlinear function.","We exploit the classical shooting approach and the comparison theorem to present non-degeneracy and exact multiplicity results for positive solutions.","This completes the multiplicity results obtained by Feltrin and Zanolin.","Numerical examples and some related open problems are also discussed."],"url":"http://arxiv.org/abs/2405.05664v1","category":"math.AP"}
{"created":"2024-05-09 10:01:05","title":"On supposed oscillations of differential cross sections in pp-scattering at sqrt{s} = 13 TeV","abstract":"The question of possible existence of oscillations in the region of the diffraction peak in pp-scattering is considered in detail at sqrt{s}=13 TeV. It is shown that within the framework of the available experimental data published by the TOTEM and ALFA/ATLAS collaborations, raising the question of searching for such a subtle effect looks premature.","sentences":["The question of possible existence of oscillations in the region of the diffraction peak in pp-scattering is considered in detail at sqrt{s}=13 TeV. It is shown that within the framework of the available experimental data published by the TOTEM and ALFA/ATLAS collaborations, raising the question of searching for such a subtle effect looks premature."],"url":"http://arxiv.org/abs/2405.05653v1","category":"hep-ph"}
{"created":"2024-05-09 09:48:39","title":"Evaluation of the X-ray SOI pixel detector with the on-chip ADC","abstract":"XRPIX is the monolithic X-ray SOI (silicon-on-insulator) pixel detector, which has a time resolution better than 10 $\\rm{\\mu}$s as well as a high detection efficiency for X-rays above 10 keV. XRPIX is planned to be installed on future X-ray satellites. To mount on satellites, it is essential that the ADC (analog-to-digital converter) be implemented on the detector because such peripheral circuits must be as compact as possible to achieve a large imaging area in the limited space in satellites. Thus, we developed a new XRPIX device with the on-chip ADC, and evaluated its performances. As the results, the integral non-linearity was evaluated to be 6 LSB (least significant bit), equivalent to 36~eV. The differential non-linearity was less than 0.7 LSB, and input noise from the on-chip ADC was 5~$\\rm{e^{-}}$. Also, we evaluated end-to-end performance including the sensor part as well as the on-chip ADC. As the results, energy resolution at 5.9~keV was 294 $\\rm{\\pm}$ 4~eV in full-width at half maximum for the best pixel.","sentences":["XRPIX is the monolithic X-ray SOI (silicon-on-insulator) pixel detector, which has a time resolution better than 10 $\\rm{\\mu}$s as well as a high detection efficiency for X-rays above 10 keV. XRPIX is planned to be installed on future X-ray satellites.","To mount on satellites, it is essential that the ADC (analog-to-digital converter) be implemented on the detector because such peripheral circuits must be as compact as possible to achieve a large imaging area in the limited space in satellites.","Thus, we developed a new XRPIX device with the on-chip ADC, and evaluated its performances.","As the results, the integral non-linearity was evaluated to be 6 LSB (least significant bit), equivalent to 36~eV. The differential non-linearity was less than 0.7 LSB, and input noise from the on-chip ADC was 5~$\\rm{e^{-}}$. Also, we evaluated end-to-end performance including the sensor part as well as the on-chip ADC.","As the results, energy resolution at 5.9~keV was 294 $\\rm{\\pm}$ 4~eV in full-width at half maximum for the best pixel."],"url":"http://arxiv.org/abs/2405.05649v1","category":"astro-ph.IM"}
{"created":"2024-05-09 09:13:32","title":"HarmonyBatch: Batching multi-SLO DNN Inference with Heterogeneous Serverless Functions","abstract":"Deep Neural Network (DNN) inference on serverless functions is gaining prominence due to its potential for substantial budget savings. Existing works on serverless DNN inference solely optimize batching requests from one application with a single Service Level Objective (SLO) on CPU functions. However, production serverless DNN inference traces indicate that the request arrival rate of applications is surprisingly low, which inevitably causes a long batching time and SLO violations. Hence, there is an urgent need for batching multiple DNN inference requests with diverse SLOs (i.e., multi-SLO DNN inference) in serverless platforms. Moreover, the potential performance and cost benefits of deploying heterogeneous (i.e., CPU and GPU) functions for DNN inference have received scant attention.   In this paper, we present HarmonyBatch, a cost-efficient resource provisioning framework designed to achieve predictable performance for multi-SLO DNN inference with heterogeneous serverless functions. Specifically, we construct an analytical performance and cost model of DNN inference on both CPU and GPU functions, by explicitly considering the GPU time-slicing scheduling mechanism and request arrival rate distribution. Based on such a model, we devise a two-stage merging strategy in HarmonyBatch to judiciously batch the multi-SLO DNN inference requests into application groups. It aims to minimize the budget of function provisioning for each application group while guaranteeing diverse performance SLOs of inference applications. We have implemented a prototype of HarmonyBatch on Alibaba Cloud Function Compute. Extensive prototype experiments with representative DNN inference workloads demonstrate that HarmonyBatch can provide predictable performance to serverless DNN inference workloads while reducing the monetary cost by up to 82.9% compared to the state-of-the-art methods.","sentences":["Deep Neural Network (DNN) inference on serverless functions is gaining prominence due to its potential for substantial budget savings.","Existing works on serverless DNN inference solely optimize batching requests from one application with a single Service Level Objective (SLO) on CPU functions.","However, production serverless DNN inference traces indicate that the request arrival rate of applications is surprisingly low, which inevitably causes a long batching time and SLO violations.","Hence, there is an urgent need for batching multiple DNN inference requests with diverse SLOs (i.e., multi-SLO DNN inference) in serverless platforms.","Moreover, the potential performance and cost benefits of deploying heterogeneous (i.e., CPU and GPU) functions for DNN inference have received scant attention.   ","In this paper, we present HarmonyBatch, a cost-efficient resource provisioning framework designed to achieve predictable performance for multi-SLO DNN inference with heterogeneous serverless functions.","Specifically, we construct an analytical performance and cost model of DNN inference on both CPU and GPU functions, by explicitly considering the GPU time-slicing scheduling mechanism and request arrival rate distribution.","Based on such a model, we devise a two-stage merging strategy in HarmonyBatch to judiciously batch the multi-SLO DNN inference requests into application groups.","It aims to minimize the budget of function provisioning for each application group while guaranteeing diverse performance SLOs of inference applications.","We have implemented a prototype of HarmonyBatch on Alibaba Cloud Function Compute.","Extensive prototype experiments with representative DNN inference workloads demonstrate that HarmonyBatch can provide predictable performance to serverless DNN inference workloads while reducing the monetary cost by up to 82.9% compared to the state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.05633v1","category":"cs.DC"}
{"created":"2024-05-09 07:58:46","title":"Elliptic and parabolic problems in thin domains with doubly weak oscillatory boundary","abstract":"In this work we consider higher dimensional thin domains with the property that both boundaries, bottom and top, present oscillations of weak type. We consider the Laplace operator with Neumann boundary conditions and analyze the behavior of the solutions as the thin domains shrinks to a fixed domain $\\omega\\subset \\R^n$. We obtain the convergence of the resolvent of the elliptic operators in the sense of compact convergence of operators, which in particular implies the convergence of the spectra. This convergence of the resolvent operators will allow us to conclude the global dynamics, in terms of the global attractors of a reaction diffusion equation in the thin domains. In particular, we show the upper semicontinuity of the attractors and stationary states. An important case treated is the case of a quasiperiodic situation, where the bottom and top oscillations are periodic but with period rationally independent.","sentences":["In this work we consider higher dimensional thin domains with the property that both boundaries, bottom and top, present oscillations of weak type.","We consider the Laplace operator with Neumann boundary conditions and analyze the behavior of the solutions as the thin domains shrinks to a fixed domain $\\omega\\subset \\R^n$. We obtain the convergence of the resolvent of the elliptic operators in the sense of compact convergence of operators, which in particular implies the convergence of the spectra.","This convergence of the resolvent operators will allow us to conclude the global dynamics, in terms of the global attractors of a reaction diffusion equation in the thin domains.","In particular, we show the upper semicontinuity of the attractors and stationary states.","An important case treated is the case of a quasiperiodic situation, where the bottom and top oscillations are periodic but with period rationally independent."],"url":"http://arxiv.org/abs/2405.05607v1","category":"math.AP"}
{"created":"2024-05-09 07:55:41","title":"Minimal Perspective Autocalibration","abstract":"We introduce a new family of minimal problems for reconstruction from multiple views. Our primary focus is a novel approach to autocalibration, a long-standing problem in computer vision. Traditional approaches to this problem, such as those based on Kruppa's equations or the modulus constraint, rely explicitly on the knowledge of multiple fundamental matrices or a projective reconstruction. In contrast, we consider a novel formulation involving constraints on image points, the unknown depths of 3D points, and a partially specified calibration matrix $K$. For $2$ and $3$ views, we present a comprehensive taxonomy of minimal autocalibration problems obtained by relaxing some of these constraints. These problems are organized into classes according to the number of views and any assumed prior knowledge of $K$. Within each class, we determine problems with the fewest -- or a relatively small number of -- solutions. From this zoo of problems, we devise three practical solvers. Experiments with synthetic and real data and interfacing our solvers with COLMAP demonstrate that we achieve superior accuracy compared to state-of-the-art calibration methods. The code is available at https://github.com/andreadalcin/MinimalPerspectiveAutocalibration","sentences":["We introduce a new family of minimal problems for reconstruction from multiple views.","Our primary focus is a novel approach to autocalibration, a long-standing problem in computer vision.","Traditional approaches to this problem, such as those based on Kruppa's equations or the modulus constraint, rely explicitly on the knowledge of multiple fundamental matrices or a projective reconstruction.","In contrast, we consider a novel formulation involving constraints on image points, the unknown depths of 3D points, and a partially specified calibration matrix $K$. For $2$ and $3$ views, we present a comprehensive taxonomy of minimal autocalibration problems obtained by relaxing some of these constraints.","These problems are organized into classes according to the number of views and any assumed prior knowledge of $K$. Within each class, we determine problems with the fewest -- or a relatively small number of -- solutions.","From this zoo of problems, we devise three practical solvers.","Experiments with synthetic and real data and interfacing our solvers with COLMAP demonstrate that we achieve superior accuracy compared to state-of-the-art calibration methods.","The code is available at https://github.com/andreadalcin/MinimalPerspectiveAutocalibration"],"url":"http://arxiv.org/abs/2405.05605v1","category":"cs.CV"}
{"created":"2024-05-09 07:33:40","title":"Higher order integration by parts formulae for Wiener measures on a path space between two curves","abstract":"We have formulated higher-order integration by parts formulae on the path space restricted between two curves, with respect to pinned/ordinary Wiener measures. The higher-order integration by parts formulae introduce nontrivial boundary terms, unlike the first-order one. Furthermore, in the process of proving these formulae, it becomes necessary to employ the construction methods of Brownian excursion and Brownian house-moving through random walk approximations. To express the integration by parts formula concisely, we introduced a notation called Symmetrization. This notation enables the rewriting of the intricate expressions of boundary terms associated with higher-order integration by parts into more concise forms. Additionally, we provided a probabilistic explanation for the boundary terms by introducing symbols based on the concept of infinitesimal probability. These efforts are aimed at fostering an intuitive understanding of the integration by parts formulae.","sentences":["We have formulated higher-order integration by parts formulae on the path space restricted between two curves, with respect to pinned/ordinary Wiener measures.","The higher-order integration by parts formulae introduce nontrivial boundary terms, unlike the first-order one.","Furthermore, in the process of proving these formulae, it becomes necessary to employ the construction methods of Brownian excursion and Brownian house-moving through random walk approximations.","To express the integration by parts formula concisely, we introduced a notation called Symmetrization.","This notation enables the rewriting of the intricate expressions of boundary terms associated with higher-order integration by parts into more concise forms.","Additionally, we provided a probabilistic explanation for the boundary terms by introducing symbols based on the concept of infinitesimal probability.","These efforts are aimed at fostering an intuitive understanding of the integration by parts formulae."],"url":"http://arxiv.org/abs/2405.05595v1","category":"math.PR"}
{"created":"2024-05-09 07:30:59","title":"Identification of problematic epochs in Astronomical Time Series through Transfer Learning","abstract":"We present a novel method for detecting outliers in astronomical time series based on the combination of a deep neural network and a k-nearest neighbor algorithm with the aim of identifying and removing problematic epochs in the light curves of astronomical objects. We use an EfficientNet network pre-trained on ImageNet as a feature extractor and perform a k-nearest neighbor search in the resulting feature space to measure the distance from the first neighbor for each image. If the distance is above the one obtained for a stacked image, we flag the image as a potential outlier. We apply our method to time series obtained from the VLT Survey Telescope (VST) monitoring campaign of the Deep Drilling Fields of the Vera C. Rubin Legacy Survey of Space and Time (LSST)\\thanksObservations were provided by the ESO programs 088.D-4013, 092.D-0370, and 094.D-0417 (PI G. Pignata).. We show that our method can effectively identify and remove artifacts from the VST time series and improve the quality and reliability of the data. This approach may prove very useful in sight of the amount of data that will be provided by the LSST, which will prevent the inspection of individual light curves. We also discuss the advantages and limitations of our method and suggest possible directions for future work.","sentences":["We present a novel method for detecting outliers in astronomical time series based on the combination of a deep neural network and a k-nearest neighbor algorithm with the aim of identifying and removing problematic epochs in the light curves of astronomical objects.","We use an EfficientNet network pre-trained on ImageNet as a feature extractor and perform a k-nearest neighbor search in the resulting feature space to measure the distance from the first neighbor for each image.","If the distance is above the one obtained for a stacked image, we flag the image as a potential outlier.","We apply our method to time series obtained from the VLT Survey Telescope (VST) monitoring campaign of the Deep Drilling Fields of the Vera C. Rubin Legacy Survey of Space and Time (LSST)\\thanksObservations were provided by the ESO programs 088.D-4013, 092.D-0370, and 094.D-0417 (PI G. Pignata)..","We show that our method can effectively identify and remove artifacts from the VST time series and improve the quality and reliability of the data.","This approach may prove very useful in sight of the amount of data that will be provided by the LSST, which will prevent the inspection of individual light curves.","We also discuss the advantages and limitations of our method and suggest possible directions for future work."],"url":"http://arxiv.org/abs/2405.05591v1","category":"astro-ph.IM"}
{"created":"2024-05-09 07:12:23","title":"The corona of a fully convective star with a near-polar flare","abstract":"In 2020, the Transiting Exoplanet Survey Satellite (TESS) observed a rapidly rotating M7 dwarf, TIC 277539431, produce a flare at 81{\\deg} latitude, the highest latitude flare located to date. This is in stark contrast to solar flares that occur much closer to the equator, typically below 30{\\deg}. The mechanisms that allow flares at high latitudes to occur are poorly understood. We studied five Sectors of TESS monitoring, and obtained 36 ks of XMM-Newton observations to investigate the coronal and flaring activity of TIC 277539431. From the observations, we infer the optical flare frequency distribution, flare loop sizes and magnetic field strengths, the soft X-ray flux, luminosity and coronal temperatures, as well as the energy, loop size and field strength of a large flare in the XMM-Newton observations. We find that TIC 277539431's corona does not differ significantly from other low mass stars on the canonical saturated activity branch with respect to coronal temperatures and flaring activity, but shows lower luminosity in soft X-ray emission by about an order of magnitude, consistent with other late M dwarfs. The lack of X-ray flux, the high latitude flare, the star's viewing geometry, and the otherwise typical stellar corona taken together can be explained by the migration of flux emergence to the poles in rapid rotators like TIC 277539431 that drain the star's equatorial regions of magnetic flux, but preserve its ability to produce powerful flares.","sentences":["In 2020, the Transiting Exoplanet Survey Satellite (TESS) observed a rapidly rotating M7 dwarf, TIC 277539431, produce a flare at 81{\\deg} latitude, the highest latitude flare located to date.","This is in stark contrast to solar flares that occur much closer to the equator, typically below 30{\\deg}.","The mechanisms that allow flares at high latitudes to occur are poorly understood.","We studied five Sectors of TESS monitoring, and obtained 36 ks of XMM-Newton observations to investigate the coronal and flaring activity of TIC 277539431.","From the observations, we infer the optical flare frequency distribution, flare loop sizes and magnetic field strengths, the soft X-ray flux, luminosity and coronal temperatures, as well as the energy, loop size and field strength of a large flare in the XMM-Newton observations.","We find that TIC 277539431's corona does not differ significantly from other low mass stars on the canonical saturated activity branch with respect to coronal temperatures and flaring activity, but shows lower luminosity in soft X-ray emission by about an order of magnitude, consistent with other late M dwarfs.","The lack of X-ray flux, the high latitude flare, the star's viewing geometry, and the otherwise typical stellar corona taken together can be explained by the migration of flux emergence to the poles in rapid rotators like TIC 277539431 that drain the star's equatorial regions of magnetic flux, but preserve its ability to produce powerful flares."],"url":"http://arxiv.org/abs/2405.05580v1","category":"astro-ph.SR"}
{"created":"2024-05-09 05:44:46","title":"Infinite horizon stochastic recursive control problems with jumps: dynamic programming and stochastic verification theorems","abstract":"This paper is devoted to studying an infinite horizon stochastic recursive control problem with jumps, where infinite horizon stochastic differential equation and backward stochastic differential equation with jumps describe the state process and cost functional, respectively. For this, the first is to explore the wellposedness and regularity of these two equations in $L^p$-frameworks ($p\\geq2$). By establishing the dynamic programming principle of the control problem, we relate the value function of the control problem with a kind of integral-partial differential equation of HJB type in the sense of viscosity solutions. On the other hand, stochastic verification theorems are also studied to provide the sufficient conditions to verify the optimality of given admissible controls. Such a study is carried out in the framework of classical solutions but also in that of viscosity solutions. By these research, we reveal several essential differences from the finite horizon problem.","sentences":["This paper is devoted to studying an infinite horizon stochastic recursive control problem with jumps, where infinite horizon stochastic differential equation and backward stochastic differential equation with jumps describe the state process and cost functional, respectively.","For this, the first is to explore the wellposedness and regularity of these two equations in $L^p$-frameworks ($p\\geq2$).","By establishing the dynamic programming principle of the control problem, we relate the value function of the control problem with a kind of integral-partial differential equation of HJB type in the sense of viscosity solutions.","On the other hand, stochastic verification theorems are also studied to provide the sufficient conditions to verify the optimality of given admissible controls.","Such a study is carried out in the framework of classical solutions but also in that of viscosity solutions.","By these research, we reveal several essential differences from the finite horizon problem."],"url":"http://arxiv.org/abs/2405.05561v1","category":"math.OC"}
{"created":"2024-05-09 04:09:11","title":"Error estimates for a bilinear optimal control problem of Maxwell's equations","abstract":"We consider a control-constrained optimal control problem subject to time-harmonic Maxwell's equations; the control variable belongs to a finite-dimensional set and enters the state equation as a coefficient. We derive existence of optimal solutions, and analyze first- and second-order optimality conditions. We devise an approximation scheme based on the lowest order N\\'ed\\'elec finite elements to approximate optimal solutions. We analyze convergence properties of the proposed scheme and prove a priori error estimates. We also design an a posteriori error estimator that can be decomposed as the sum two contributions related to the discretization of the state and adjoint equations, and prove that the devised error estimator is reliable and locally efficient. We perform numerical tests in order to assess the performance of the devised discretization strategy and the a posteriori error estimator.","sentences":["We consider a control-constrained optimal control problem subject to time-harmonic Maxwell's equations; the control variable belongs to a finite-dimensional set and enters the state equation as a coefficient.","We derive existence of optimal solutions, and analyze first- and second-order optimality conditions.","We devise an approximation scheme based on the lowest order N\\'ed\\'elec finite elements to approximate optimal solutions.","We analyze convergence properties of the proposed scheme and prove a priori error estimates.","We also design an a posteriori error estimator that can be decomposed as the sum two contributions related to the discretization of the state and adjoint equations, and prove that the devised error estimator is reliable and locally efficient.","We perform numerical tests in order to assess the performance of the devised discretization strategy and the a posteriori error estimator."],"url":"http://arxiv.org/abs/2405.05532v1","category":"math.NA"}
{"created":"2024-05-09 02:58:59","title":"Numerical model of Phobos' motion incorporating the effects of free rotation","abstract":"High-precision ephemerides are not only useful in supporting space missions, but also in investigating the physical nature of celestial bodies. This paper reports an update to the orbit and rotation model of the Martian moon Phobos. In contrast to earlier numerical models, this paper details a dynamical model that fully considers the rotation of Phobos. Here, Phobos' rotation is first described by Euler's rotational equations and integrated simultaneously with the orbital motion equations. We discuss this dynamical model, along with the differences with respect to the model now in use.   We present the variational equation for Phobos' rotation employing the symbolic \\emph{Maple} computation software. The adjustment test simulations confirm the latitude libration of Phobos, suggesting gravity field coefficients obtained using a shape model and homogeneous density hypothesis should be re-examined in the future in the context of dynamics. Furthermore, the simulations with different $k_2$ values indicate that it is difficult to determine k_2 efficiently using the current data.","sentences":["High-precision ephemerides are not only useful in supporting space missions, but also in investigating the physical nature of celestial bodies.","This paper reports an update to the orbit and rotation model of the Martian moon Phobos.","In contrast to earlier numerical models, this paper details a dynamical model that fully considers the rotation of Phobos.","Here, Phobos' rotation is first described by Euler's rotational equations and integrated simultaneously with the orbital motion equations.","We discuss this dynamical model, along with the differences with respect to the model now in use.   ","We present the variational equation for Phobos' rotation employing the symbolic \\emph{Maple} computation software.","The adjustment test simulations confirm the latitude libration of Phobos, suggesting gravity field coefficients obtained using a shape model and homogeneous density hypothesis should be re-examined in the future in the context of dynamics.","Furthermore, the simulations with different $k_2$ values indicate that it is difficult to determine k_2 efficiently using the current data."],"url":"http://arxiv.org/abs/2405.05519v1","category":"astro-ph.EP"}
{"created":"2024-05-09 02:58:54","title":"The rigidity of eigenfunctions' gradient estimates","abstract":"On compact Riemannian manifolds with non-negative Ricci curvature and smooth (possibly empty), convex (or mean convex) boundary, if the sharp Li-Yau type gradient estimate of an Neumann (or Dirichlet) eigenfunction holds at some non-critical points of the eigenfunction; we show that the manifold is isometric to the product of one lower dimension manifold and a round circle (or a line segment).","sentences":["On compact Riemannian manifolds with non-negative Ricci curvature and smooth (possibly empty), convex (or mean convex) boundary, if the sharp Li-Yau type gradient estimate of an Neumann (or Dirichlet) eigenfunction holds at some non-critical points of the eigenfunction; we show that the manifold is isometric to the product of one lower dimension manifold and a round circle (or a line segment)."],"url":"http://arxiv.org/abs/2405.05517v1","category":"math.DG"}
{"created":"2024-05-08 23:09:43","title":"Automated Program Repair: Emerging trends pose and expose problems for benchmarks","abstract":"Machine learning (ML) now pervades the field of Automated Program Repair (APR). Algorithms deploy neural machine translation and large language models (LLMs) to generate software patches, among other tasks. But, there are important differences between these applications of ML and earlier work. Evaluations and comparisons must take care to ensure that results are valid and likely to generalize. A challenge is that the most popular APR evaluation benchmarks were not designed with ML techniques in mind. This is especially true for LLMs, whose large and often poorly-disclosed training datasets may include problems on which they are evaluated.","sentences":["Machine learning (ML) now pervades the field of Automated Program Repair (APR).","Algorithms deploy neural machine translation and large language models (LLMs) to generate software patches, among other tasks.","But, there are important differences between these applications of ML and earlier work.","Evaluations and comparisons must take care to ensure that results are valid and likely to generalize.","A challenge is that the most popular APR evaluation benchmarks were not designed with ML techniques in mind.","This is especially true for LLMs, whose large and often poorly-disclosed training datasets may include problems on which they are evaluated."],"url":"http://arxiv.org/abs/2405.05455v1","category":"cs.SE"}
{"created":"2024-05-08 20:47:44","title":"Comparative Analysis of Single Charged Particle Production in Proton-Carbon Interactions at High Energies","abstract":"The generation of charged pions, kaons, and protons from proton beam incident on Carbon at 31 GeV/c is calculated and compared with the results of the NA61/SHINE experiment. Predictions of the single charged particle yield by proton off the Carbon target is calculated using the Giessen Boltzmann-Uehling-Uhlenbeck (GiBUU) model and compared with the recent data of the EMPHATIC hadron scattering and production experiment for incident proton beam energies of 20, 30, and 120 GeV/c within $\\pm$20 mrad with respect to the beam particle. We present the analysis for the single and double differential cross-sections for the produced particles at various scattering angles and conduct a comprehensive comparison with the experimental data. These studies show significant agreement with the measured data.","sentences":["The generation of charged pions, kaons, and protons from proton beam incident on Carbon at 31 GeV/c is calculated and compared with the results of the NA61/SHINE experiment.","Predictions of the single charged particle yield by proton off the Carbon target is calculated using the Giessen Boltzmann-Uehling-Uhlenbeck (GiBUU) model and compared with the recent data of the EMPHATIC hadron scattering and production experiment for incident proton beam energies of 20, 30, and 120 GeV/c within $\\pm$20 mrad with respect to the beam particle.","We present the analysis for the single and double differential cross-sections for the produced particles at various scattering angles and conduct a comprehensive comparison with the experimental data.","These studies show significant agreement with the measured data."],"url":"http://arxiv.org/abs/2405.05423v1","category":"hep-ex"}
{"created":"2024-05-08 20:44:29","title":"Computing almost commuting bases of ODOs and Gelfand-Dikey hierarchies","abstract":"Almost commuting operators were introduced in 1985 by George Wilson to present generalizations the Korteweg-de Vries hierarchy, nowadays known as Gelfand-Dikey (GD) hierarchies. In this paper, we review the formal construction of the vector space of almost commuting operators with a given ordinary differential operator (ODO), with the ultimate goal of obtaining a basis by computational routines, using the language of differential polynomials. We use Wilson's results on weigheted ODOs to guarantee the solvability of the triangular system that allows to compute the homogeneous almost commuting operator of a given order in the ring of ODOs. As a consequence the computation of the equations of the GD hierarchies is obtained without using pseudo-differential operators. The algorithms to calculate the almost commuting basis and the GD hierarchies in the ring of ODOs are implemented in SageMath, and explicit examples are provided.","sentences":["Almost commuting operators were introduced in 1985 by George Wilson to present generalizations the Korteweg-de Vries hierarchy, nowadays known as Gelfand-Dikey (GD) hierarchies.","In this paper, we review the formal construction of the vector space of almost commuting operators with a given ordinary differential operator (ODO), with the ultimate goal of obtaining a basis by computational routines, using the language of differential polynomials.","We use Wilson's results on weigheted ODOs to guarantee the solvability of the triangular system that allows to compute the homogeneous almost commuting operator of a given order in the ring of ODOs.","As a consequence the computation of the equations of the GD hierarchies is obtained without using pseudo-differential operators.","The algorithms to calculate the almost commuting basis and the GD hierarchies in the ring of ODOs are implemented in SageMath, and explicit examples are provided."],"url":"http://arxiv.org/abs/2405.05421v1","category":"math.AC"}
{"created":"2024-05-08 20:21:06","title":"Concavity and perturbed concavity for $p$-Laplace equations","abstract":"In this paper we study convexity properties for quasilinear Lane-Emden-Fowler equations of the type $$ \\begin{cases} -\\Delta_p u = a(x) u^q & \\quad \\hbox{ in $\\Omega$},\\\\ u >0 & \\quad \\hbox{ in $\\Omega$}, \\\\ u =0 & \\quad \\hbox{ on $\\partial \\Omega$}, \\end{cases} $$ when $\\Omega \\subset \\mathbb{R}^N$ is a convex domain. In particular, in the subhomogeneous case $q \\in [0,p-1]$, the solution $u$ inherits concavity properties from $a$ whenever assumed, while it is proved to be concave up to an error if $a$ is near to a constant. More general cases are also taken into account, including a wider class of nonlinearities. These results generalize some contained in [Kennington, Indiana Univ. Math. J., 1985] and [Sakaguchi, Ann. Sc. Norm. Super. Pisa, 1987].   Additionally, some results for the singular case $q \\in [-1,0)$ and the superhomogeneous case $q>p-1$, $q \\approx p-1$ are obtained. Some properties for the $p$-fractional Laplacian $(-\\Delta)^s_p$, $s\\in (0,1)$, $s \\approx 1$, are shown as well.   We highlight that some results are new even in the semilinear case $p=2$; in some of these cases, we deduce also uniqueness (and nondegeneracy) of the critical point of $u$.","sentences":["In this paper we study convexity properties for quasilinear Lane-Emden-Fowler equations of the type $$ \\begin{cases} -\\Delta_p u = a(x) u^q & \\quad \\hbox{ in $\\Omega$},\\\\ u >0 & \\quad \\hbox{ in $\\Omega$}, \\\\ u =0 & \\quad \\hbox{ on $\\partial \\Omega$}, \\end{cases} $$ when $\\Omega \\subset \\mathbb{R}^N$ is a convex domain.","In particular, in the subhomogeneous case $q \\in [0,p-1]$, the solution $u$ inherits concavity properties from $a$ whenever assumed, while it is proved to be concave up to an error if $a$ is near to a constant.","More general cases are also taken into account, including a wider class of nonlinearities.","These results generalize some contained in [Kennington, Indiana Univ.","Math.","J., 1985] and [Sakaguchi, Ann.","Sc.","Norm.","Super. Pisa, 1987].   ","Additionally, some results for the singular case $q \\in [-1,0)$ and the superhomogeneous case $q>p-1$, $q \\approx p-1$ are obtained.","Some properties for the $p$-fractional Laplacian $(-\\Delta)^s_p$, $s\\in (0,1)$, $s \\approx 1$, are shown as well.   ","We highlight that some results are new even in the semilinear case $p=2$; in some of these cases, we deduce also uniqueness (and nondegeneracy) of the critical point of $u$."],"url":"http://arxiv.org/abs/2405.05404v1","category":"math.AP"}
{"created":"2024-05-08 19:53:38","title":"Exotic Stable Branches with Efficient TOV Sequences","abstract":"Modern inference schemes for the neutron star equation of state (EoS) require large numbers of stellar models constructed with different EoS, and these stellar models must capture all the behavior of stable stars. I introduce termination conditions for sequences of stellar models for cold, non-rotating neutron stars that are guaranteed to identify all stable stellar configurations up to arbitrarily large central pressures along with an efficient algorithm to build accurate interpolators for macroscopic properties. I explore the behavior of stars with both high- and low-central pressures. Interestingly, I find that EoS with monotonically increasing sound-speed can produce multiple stable branches (twin stars) and that large phase transitions at high densities can produce stable branches at nearly any mass scale, including sub-solar masses, while still supporting stars with $M > 2\\,M_\\odot$. I conclude with some speculation about the astrophysical implications of this behavior.","sentences":["Modern inference schemes for the neutron star equation of state (EoS) require large numbers of stellar models constructed with different EoS, and these stellar models must capture all the behavior of stable stars.","I introduce termination conditions for sequences of stellar models for cold, non-rotating neutron stars that are guaranteed to identify all stable stellar configurations up to arbitrarily large central pressures along with an efficient algorithm to build accurate interpolators for macroscopic properties.","I explore the behavior of stars with both high- and low-central pressures.","Interestingly, I find that EoS with monotonically increasing sound-speed can produce multiple stable branches (twin stars) and that large phase transitions at high densities can produce stable branches at nearly any mass scale, including sub-solar masses, while still supporting stars with $M > 2\\,M_\\odot$.","I conclude with some speculation about the astrophysical implications of this behavior."],"url":"http://arxiv.org/abs/2405.05395v1","category":"astro-ph.HE"}
{"created":"2024-05-08 19:49:50","title":"Penetration of a spinning sphere impacting a granular medium","abstract":"We investigate experimentally the influence of rotation on the penetration depth of a spherical projectile impacting a granular medium. We show that a rotational motion significantly increases the penetration depth achieved. Moreover, we model our experimental results by modifying the frictional term of the equation describing the penetration dynamics of an object in a granular medium. In particular, we find that the frictional drag decreases linearly with the velocity ratio between rotational (spin motion) and translational (falling motion) velocities. The good agreement between our model and our experimental measurements offers perspectives for estimating the depth that spinning projectiles reach after impacting onto a granular ground, such as happens with seeds dropped from aircraft or with landing probes.","sentences":["We investigate experimentally the influence of rotation on the penetration depth of a spherical projectile impacting a granular medium.","We show that a rotational motion significantly increases the penetration depth achieved.","Moreover, we model our experimental results by modifying the frictional term of the equation describing the penetration dynamics of an object in a granular medium.","In particular, we find that the frictional drag decreases linearly with the velocity ratio between rotational (spin motion) and translational (falling motion) velocities.","The good agreement between our model and our experimental measurements offers perspectives for estimating the depth that spinning projectiles reach after impacting onto a granular ground, such as happens with seeds dropped from aircraft or with landing probes."],"url":"http://arxiv.org/abs/2405.05394v1","category":"cond-mat.soft"}
{"created":"2024-05-08 19:49:17","title":"A Talenti comparison result for a class of Neumann boundary value problems","abstract":"In this paper, we establish a comparison principle in terms of Lorentz norms and point-wise inequalities between a positive solution $u$ to the Poisson equation with non-homogeneous Neumann boundary conditions and a specific positive solution $v$ to the Schwartz symmetrized problem, which is related to $u$ through an additional boundary condition.","sentences":["In this paper, we establish a comparison principle in terms of Lorentz norms and point-wise inequalities between a positive solution $u$ to the Poisson equation with non-homogeneous Neumann boundary conditions and a specific positive solution $v$ to the Schwartz symmetrized problem, which is related to $u$ through an additional boundary condition."],"url":"http://arxiv.org/abs/2405.05392v1","category":"math.AP"}
{"created":"2024-05-08 19:41:24","title":"Topological conditions drive stability in meta-ecosystems","abstract":"On a global level, ecological communities are being perturbed at an unprecedented rate by human activities and environmental instabilities. Yet, we understand little about what factors facilitate or impede long-term persistence of these communities. While observational studies indicate that increased biodiversity must, somehow, be driving stability, theoretical studies have argued the exact opposite viewpoint instead. This encouraged many researchers to participate in the ongoing diversity-stability debate. Within this context, however, there has been a severe lack of studies that consider spatial features explicitly, even though nearly all habitats are spatially embedded. To this end, we study here the linear stability of meta-ecosystems on networks that describe how discrete patches are connected by dispersal between them. By combining results from random matrix theory and network theory, we are able to show that there are three distinct features that underlie stability: edge density, tendency to triadic closure, and isolation or fragmentation. Our results appear to further indicate that network sparsity does not necessarily reduce stability, and that connections between patches are just as, if not more, important to consider when studying the stability of large ecological systems.","sentences":["On a global level, ecological communities are being perturbed at an unprecedented rate by human activities and environmental instabilities.","Yet, we understand little about what factors facilitate or impede long-term persistence of these communities.","While observational studies indicate that increased biodiversity must, somehow, be driving stability, theoretical studies have argued the exact opposite viewpoint instead.","This encouraged many researchers to participate in the ongoing diversity-stability debate.","Within this context, however, there has been a severe lack of studies that consider spatial features explicitly, even though nearly all habitats are spatially embedded.","To this end, we study here the linear stability of meta-ecosystems on networks that describe how discrete patches are connected by dispersal between them.","By combining results from random matrix theory and network theory, we are able to show that there are three distinct features that underlie stability: edge density, tendency to triadic closure, and isolation or fragmentation.","Our results appear to further indicate that network sparsity does not necessarily reduce stability, and that connections between patches are just as, if not more, important to consider when studying the stability of large ecological systems."],"url":"http://arxiv.org/abs/2405.05390v1","category":"q-bio.PE"}
{"created":"2024-05-08 18:56:10","title":"Coupling of the Finite Element Method with Physics Informed Neural Networks for the Multi-Fluid Flow Problem","abstract":"Multi-fluid flows are found in various industrial processes, including metal injection molding and 3D printing. The accuracy of multi-fluid flow modeling is determined by how well interfaces and capillary forces are represented. In this paper, the multi-fluid flow problem is discretized using a combination of a Physics-Informed Neural Network (PINN) with a finite element discretization. To determine the best PINN formulation, a comparative study is conducted using a manufactured solution. We compare interface reinitialization methods to determine the most suitable approach for our discretization strategy. We devise a neural network architecture that better handles complex free surface topologies. Finally, the coupled numerical strategy is used to model a rising bubble problem.","sentences":["Multi-fluid flows are found in various industrial processes, including metal injection molding and 3D printing.","The accuracy of multi-fluid flow modeling is determined by how well interfaces and capillary forces are represented.","In this paper, the multi-fluid flow problem is discretized using a combination of a Physics-Informed Neural Network (PINN) with a finite element discretization.","To determine the best PINN formulation, a comparative study is conducted using a manufactured solution.","We compare interface reinitialization methods to determine the most suitable approach for our discretization strategy.","We devise a neural network architecture that better handles complex free surface topologies.","Finally, the coupled numerical strategy is used to model a rising bubble problem."],"url":"http://arxiv.org/abs/2405.05371v1","category":"math.NA"}
{"created":"2024-05-08 18:55:30","title":"Non-singular black hole by gravitational decoupling and some thermodynamic properties","abstract":"Gravitational decoupling allows to obtain new solutions of general relativity. In this paper, we obtain new solutions of the Einstein field equations which describe non-singular black holes. We consider Hayward and Bardeen regular black holes as seed spacetimes and apply gravitational decoupling to obtain a new non-singular solution. We show that anisotropic energy-momentum tensor can spoil the regularity condition in the centre of a black hole. We solve the Einstein field equation and obtain new solutions that possess a de Sitter core and have Schwarzschild behaviour in infinity. We also analyse the thermodynamic properties of the obtained solutions.","sentences":["Gravitational decoupling allows to obtain new solutions of general relativity.","In this paper, we obtain new solutions of the Einstein field equations which describe non-singular black holes.","We consider Hayward and Bardeen regular black holes as seed spacetimes and apply gravitational decoupling to obtain a new non-singular solution.","We show that anisotropic energy-momentum tensor can spoil the regularity condition in the centre of a black hole.","We solve the Einstein field equation and obtain new solutions that possess a de Sitter core and have Schwarzschild behaviour in infinity.","We also analyse the thermodynamic properties of the obtained solutions."],"url":"http://arxiv.org/abs/2405.05370v1","category":"gr-qc"}
{"created":"2024-05-08 18:13:40","title":"Vector Superstrata: Part Two","abstract":"Microstate geometries are proposed microstates of black holes which can be described within supergravity. Even though their number may not reproduce the full entropy of black holes with finite-sized horizons, they still offer a glimpse into the microscopic structure of black holes. In this paper we construct a new set of microstate geometries of the supersymmetric D1-D5-P black hole, where the momentum charge is carried by a vector field, as seen from the perspective of six-dimensional supergravity. To aid our construction, we develop an algorithm which solves a complicated partial differential equation using the regularity of the geometries. The new solutions are asymptotically AdS$_3\\times S^3$, and have a long, but finite AdS$_2$ throat that caps off without ever developing a horizon. These microstate geometries have a holographic interpretation as coherent superpositions of heavy states in the boundary D1-D5 CFT. We identify the states which are dual to our newly constructed solutions and carry out some basic consistency checks to support our identification.","sentences":["Microstate geometries are proposed microstates of black holes which can be described within supergravity.","Even though their number may not reproduce the full entropy of black holes with finite-sized horizons, they still offer a glimpse into the microscopic structure of black holes.","In this paper we construct a new set of microstate geometries of the supersymmetric D1-D5-P black hole, where the momentum charge is carried by a vector field, as seen from the perspective of six-dimensional supergravity.","To aid our construction, we develop an algorithm which solves a complicated partial differential equation using the regularity of the geometries.","The new solutions are asymptotically AdS$_3\\times S^3$, and have a long, but finite AdS$_2$ throat that caps off without ever developing a horizon.","These microstate geometries have a holographic interpretation as coherent superpositions of heavy states in the boundary D1-D5 CFT.","We identify the states which are dual to our newly constructed solutions and carry out some basic consistency checks to support our identification."],"url":"http://arxiv.org/abs/2405.05341v1","category":"hep-th"}
{"created":"2024-05-08 18:10:45","title":"Wave Function Collapse, Lorentz Invariance, and the Third Postulate of Relativity","abstract":"The changes that quantum states undergo during measurement are both probabilistic and nonlocal. These two characteristics complement one another to insure compatibility with relativity and maintain conservation laws. Nonlocal entanglement relations provide a means to enforce conservation laws in a probabilistic theory, while the probabilistic nature of nonlocal effects prevents the superluminal transmission of information. In order to explain these measurement-induced changes in terms of fundamental physical processes it is necessary to take these two key characteristics into account. One way to do this is to modify the Schroedinger equation by adding stochastic, nonlinear terms. A number of such proposals have been made over the past few decades. A recently proposed equation based on the assumption that wave function collapse is induced by a sequence of correlating interactions of the kind that constitute measurements has been shown to maintain strict adherence to conservation laws in individual instances, and has also eliminated the need to introduce any new, ad hoc physical constants. In this work it is shown that this modified Schroedinger equation is naturally Lorentz invariant. It is further argued that the additional spacetime structures that it requires provide a way to implement the assumption that spacelike-separated operators (and measurements) commute, and that this assumption of local commutativity should be regarded as a third postulate of relativity.","sentences":["The changes that quantum states undergo during measurement are both probabilistic and nonlocal.","These two characteristics complement one another to insure compatibility with relativity and maintain conservation laws.","Nonlocal entanglement relations provide a means to enforce conservation laws in a probabilistic theory, while the probabilistic nature of nonlocal effects prevents the superluminal transmission of information.","In order to explain these measurement-induced changes in terms of fundamental physical processes it is necessary to take these two key characteristics into account.","One way to do this is to modify the Schroedinger equation by adding stochastic, nonlinear terms.","A number of such proposals have been made over the past few decades.","A recently proposed equation based on the assumption that wave function collapse is induced by a sequence of correlating interactions of the kind that constitute measurements has been shown to maintain strict adherence to conservation laws in individual instances, and has also eliminated the need to introduce any new, ad hoc physical constants.","In this work it is shown that this modified Schroedinger equation is naturally Lorentz invariant.","It is further argued that the additional spacetime structures that it requires provide a way to implement the assumption that spacelike-separated operators (and measurements) commute, and that this assumption of local commutativity should be regarded as a third postulate of relativity."],"url":"http://arxiv.org/abs/2405.05335v1","category":"quant-ph"}
{"created":"2024-05-08 18:02:21","title":"A fast Solver for Pentadiagonal Toeplitz Systems","abstract":"The objective of this work is to present a novel approach for the solution of Pentadiagonal Toeplitz systems of equations that is both faster and more effective than existing classical direct methods. The distinctive structure of Pentadiagonal Toeplitz matrices can be leveraged to devise an algorithm for solving upper triangle systems, rather than the original system. This approach is considerably more straightforward and expeditious than classical methods such as LU and Gauss Eliminations. A comparison with the LU and PLU methods demonstrates the efficacy of our novel algorithm. Furthermore, numerical tests substantiate this efficacy.","sentences":["The objective of this work is to present a novel approach for the solution of Pentadiagonal Toeplitz systems of equations that is both faster and more effective than existing classical direct methods.","The distinctive structure of Pentadiagonal Toeplitz matrices can be leveraged to devise an algorithm for solving upper triangle systems, rather than the original system.","This approach is considerably more straightforward and expeditious than classical methods such as LU and Gauss Eliminations.","A comparison with the LU and PLU methods demonstrates the efficacy of our novel algorithm.","Furthermore, numerical tests substantiate this efficacy."],"url":"http://arxiv.org/abs/2405.05328v1","category":"math.NA"}
{"created":"2024-05-08 18:00:08","title":"Higher Berry Curvature from the Wave function II: Locally Parameterized States Beyond One Dimension","abstract":"We propose a systematic wave function based approach to construct topological invariants for families of lattice systems that are short-range entangled using local parameter spaces. This construction is particularly suitable when given a family of tensor networks that can be viewed as the ground states of $d$ dimensional lattice systems, for which we construct the closed $(d+2)$-form higher Berry curvature, which is a generalization of the well known 2-form Berry curvature. Such $(d+2)$-form higher Berry curvature characterizes a flow of $(d+1)$-form higher Berry curvature in the system. Our construction is equally suitable for constructing other higher pumps, such as the (higher) Thouless pump in the presence of a global on-site $U(1)$ symmetry, which corresponds to a closed $d$-form. The cohomology classes of such higher differential forms are topological invariants and are expected to be quantized for short-range entangled states. We illustrate our construction with exactly solvable lattice models that are in nontrivial higher Berry classes in $d=2$.","sentences":["We propose a systematic wave function based approach to construct topological invariants for families of lattice systems that are short-range entangled using local parameter spaces.","This construction is particularly suitable when given a family of tensor networks that can be viewed as the ground states of $d$ dimensional lattice systems, for which we construct the closed $(d+2)$-form higher Berry curvature, which is a generalization of the well known 2-form Berry curvature.","Such $(d+2)$-form higher Berry curvature characterizes a flow of $(d+1)$-form higher Berry curvature in the system.","Our construction is equally suitable for constructing other higher pumps, such as the (higher) Thouless pump in the presence of a global on-site $U(1)$ symmetry, which corresponds to a closed $d$-form.","The cohomology classes of such higher differential forms are topological invariants and are expected to be quantized for short-range entangled states.","We illustrate our construction with exactly solvable lattice models that are in nontrivial higher Berry classes in $d=2$."],"url":"http://arxiv.org/abs/2405.05323v1","category":"cond-mat.str-el"}
{"created":"2024-05-08 18:00:06","title":"Gauge invariant perturbations of static spatially compact LRS II spacetimes","abstract":"We present a framework to describe completely general first-order perturbations of static, spatially compact, and locally rotationally symmetric class II spacetimes within the theory of general relativity. The perturbation variables are by construction covariant and identification gauge invariant and encompass the geometry and the thermodynamics of the fluid sources. The new equations are then applied to the study of isotropic, adiabatic perturbations. We discuss how the choice of frame in which perturbations are described can significantly simplify the mathematical analysis of the problem and show that it is possible to change frames directly from the linear level equations. We find explicitly that the case of isotropic, adiabatic perturbations can be reduced to a singular Sturm-Liouville eigenvalue problem, and lower bounds for the values of the eigenfrequencies can be derived. These results lay the theoretical groundwork to analytically describe linear, isotropic, and adiabatic perturbations of static, spherically symmetric spacetimes.","sentences":["We present a framework to describe completely general first-order perturbations of static, spatially compact, and locally rotationally symmetric class II spacetimes within the theory of general relativity.","The perturbation variables are by construction covariant and identification gauge invariant and encompass the geometry and the thermodynamics of the fluid sources.","The new equations are then applied to the study of isotropic, adiabatic perturbations.","We discuss how the choice of frame in which perturbations are described can significantly simplify the mathematical analysis of the problem and show that it is possible to change frames directly from the linear level equations.","We find explicitly that the case of isotropic, adiabatic perturbations can be reduced to a singular Sturm-Liouville eigenvalue problem, and lower bounds for the values of the eigenfrequencies can be derived.","These results lay the theoretical groundwork to analytically describe linear, isotropic, and adiabatic perturbations of static, spherically symmetric spacetimes."],"url":"http://arxiv.org/abs/2405.05321v1","category":"gr-qc"}
{"created":"2024-05-08 18:00:00","title":"Swampland Conjectures Constraints on Dark Energy from a Highly Curved Field Space","abstract":"We study the interplay of the trans-Planckian censorship conjecture (TCC) and the swampland distance conjecture (SDC) in the context of multifield dark energy in a curved field space. In this scenario, the phase of accelerated expansion is realized as non-geodesic motion in a highly-curved field space, reminiscent of models developed in the context of inflation. The model features a stable attractor solution with near constant equation of state $w\\simeq -1$, and predicts that the current era of accelerated expansion is eternal. The latter implies an eventual conflict with the TCC, which holds that the duration of any epoch of cosmic acceleration is bounded by the requirement that the large-scale observable universe is blind to Planck-scale early universe physics. This tension can be resolved by an interplay with the distance conjecture: for suitable parameter values, the apparent violation of the TCC occurs well after the fields have traversed a Planckian distance. The SDC then predicts a breakdown of the effective field theory (EFT) before the TCC can be violated. We derive the constraints on the model arising from the SDC+TCC and the de Sitter conjecture. We demonstrate that the model can be consistent with both swampland conjectures and observational data from Planck 2018 and the Dark Energy Spectroscopic Instrument.","sentences":["We study the interplay of the trans-Planckian censorship conjecture (TCC) and the swampland distance conjecture (SDC) in the context of multifield dark energy in a curved field space.","In this scenario, the phase of accelerated expansion is realized as non-geodesic motion in a highly-curved field space, reminiscent of models developed in the context of inflation.","The model features a stable attractor solution with near constant equation of state $w\\simeq -1$, and predicts that the current era of accelerated expansion is eternal.","The latter implies an eventual conflict with the TCC, which holds that the duration of any epoch of cosmic acceleration is bounded by the requirement that the large-scale observable universe is blind to Planck-scale early universe physics.","This tension can be resolved by an interplay with the distance conjecture: for suitable parameter values, the apparent violation of the TCC occurs well after the fields have traversed a Planckian distance.","The SDC then predicts a breakdown of the effective field theory (EFT) before the TCC can be violated.","We derive the constraints on the model arising from the SDC+TCC and the de Sitter conjecture.","We demonstrate that the model can be consistent with both swampland conjectures and observational data from Planck 2018 and the Dark Energy Spectroscopic Instrument."],"url":"http://arxiv.org/abs/2405.05304v1","category":"hep-th"}
{"created":"2024-05-08 18:00:00","title":"Nonreciprocity of supercurrent along applied magnetic field","abstract":"Nonreciprocal current responses arise in a broad range of systems, from magnons and phonons to supercurrents, due to an interplay between spatial and temporal symmetry breakings. These find applications in devices, such as circulators and rectifiers, as well as in probing the interactions and states that underlie the nonreciprocity. An established symmetry argument anticipates emergence of nonreciprocal currents along a direction perpendicular to the applied magnetic field that breaks the time-reversal symmetry. Here, motivated by recent experiments, we examine the emergence of nonreciprocity in vortex-limited superconducting critical currents along an applied magnetic field. Employing London's equations for describing the Meissner response of a superconducting film, we find that an additional symmetry breaking due to a preferred vortex axis enables nonreciprocal critical currents along the applied magnetic field, consistent with the so far unexplained experimental observation. Building on our concrete theoretical model for supercurrents, we discuss a possible generalization of the prevailing symmetry consideration to encompass nonreciprocal currents along the time-reversal symmetry breaking direction.","sentences":["Nonreciprocal current responses arise in a broad range of systems, from magnons and phonons to supercurrents, due to an interplay between spatial and temporal symmetry breakings.","These find applications in devices, such as circulators and rectifiers, as well as in probing the interactions and states that underlie the nonreciprocity.","An established symmetry argument anticipates emergence of nonreciprocal currents along a direction perpendicular to the applied magnetic field that breaks the time-reversal symmetry.","Here, motivated by recent experiments, we examine the emergence of nonreciprocity in vortex-limited superconducting critical currents along an applied magnetic field.","Employing London's equations for describing the Meissner response of a superconducting film, we find that an additional symmetry breaking due to a preferred vortex axis enables nonreciprocal critical currents along the applied magnetic field, consistent with the so far unexplained experimental observation.","Building on our concrete theoretical model for supercurrents, we discuss a possible generalization of the prevailing symmetry consideration to encompass nonreciprocal currents along the time-reversal symmetry breaking direction."],"url":"http://arxiv.org/abs/2405.05306v1","category":"cond-mat.supr-con"}
{"created":"2024-05-09 17:59:56","title":"Learned harmonic mean estimation of the Bayesian evidence with normalizing flows","abstract":"We present the learned harmonic mean estimator with normalizing flows - a robust, scalable and flexible estimator of the Bayesian evidence for model comparison. Since the estimator is agnostic to sampling strategy and simply requires posterior samples, it can be applied to compute the evidence using any Markov chain Monte Carlo (MCMC) sampling technique, including saved down MCMC chains, or any variational inference approach. The learned harmonic mean estimator was recently introduced, where machine learning techniques were developed to learn a suitable internal importance sampling target distribution to solve the issue of exploding variance of the original harmonic mean estimator. In this article we present the use of normalizing flows as the internal machine learning technique within the learned harmonic mean estimator. Normalizing flows can be elegantly coupled with the learned harmonic mean to provide an approach that is more robust, flexible and scalable than the machine learning models considered previously. We perform a series of numerical experiments, applying our method to benchmark problems and to a cosmological example in up to 21 dimensions. We find the learned harmonic mean estimator is in agreement with ground truth values and nested sampling estimates. The open-source harmonic Python package implementing the learned harmonic mean, now with normalizing flows included, is publicly available.","sentences":["We present the learned harmonic mean estimator with normalizing flows - a robust, scalable and flexible estimator of the Bayesian evidence for model comparison.","Since the estimator is agnostic to sampling strategy and simply requires posterior samples, it can be applied to compute the evidence using any Markov chain Monte Carlo (MCMC) sampling technique, including saved down MCMC chains, or any variational inference approach.","The learned harmonic mean estimator was recently introduced, where machine learning techniques were developed to learn a suitable internal importance sampling target distribution to solve the issue of exploding variance of the original harmonic mean estimator.","In this article we present the use of normalizing flows as the internal machine learning technique within the learned harmonic mean estimator.","Normalizing flows can be elegantly coupled with the learned harmonic mean to provide an approach that is more robust, flexible and scalable than the machine learning models considered previously.","We perform a series of numerical experiments, applying our method to benchmark problems and to a cosmological example in up to 21 dimensions.","We find the learned harmonic mean estimator is in agreement with ground truth values and nested sampling estimates.","The open-source harmonic Python package implementing the learned harmonic mean, now with normalizing flows included, is publicly available."],"url":"http://arxiv.org/abs/2405.05969v1","category":"astro-ph.IM"}
{"created":"2024-05-09 17:16:54","title":"Theoretical Guarantees of Data Augmented Last Layer Retraining Methods","abstract":"Ensuring fair predictions across many distinct subpopulations in the training data can be prohibitive for large models. Recently, simple linear last layer retraining strategies, in combination with data augmentation methods such as upweighting, downsampling and mixup, have been shown to achieve state-of-the-art performance for worst-group accuracy, which quantifies accuracy for the least prevalent subpopulation. For linear last layer retraining and the abovementioned augmentations, we present the optimal worst-group accuracy when modeling the distribution of the latent representations (input to the last layer) as Gaussian for each subpopulation. We evaluate and verify our results for both synthetic and large publicly available datasets.","sentences":["Ensuring fair predictions across many distinct subpopulations in the training data can be prohibitive for large models.","Recently, simple linear last layer retraining strategies, in combination with data augmentation methods such as upweighting, downsampling and mixup, have been shown to achieve state-of-the-art performance for worst-group accuracy, which quantifies accuracy for the least prevalent subpopulation.","For linear last layer retraining and the abovementioned augmentations, we present the optimal worst-group accuracy when modeling the distribution of the latent representations (input to the last layer) as Gaussian for each subpopulation.","We evaluate and verify our results for both synthetic and large publicly available datasets."],"url":"http://arxiv.org/abs/2405.05934v1","category":"cs.LG"}
{"created":"2024-05-09 16:44:35","title":"An RNN-policy gradient approach for quantum architecture search","abstract":"Variational quantum circuits are one of the promising ways to exploit the advantages of quantum computing in the noisy intermediate-scale quantum technology era. The design of the quantum circuit architecture might greatly affect the performance capability of the quantum algorithms. The quantum architecture search is the process of automatically designing quantum circuit architecture, aiming at finding the optimal quantum circuit composition architecture by the algorithm for a given task, so that the algorithm can learn to design the circuit architecture. Compared to manual design, quantum architecture search algorithms are more effective in finding quantum circuits with better performance capabilities. In this paper, based on the deep reinforcement learning, we propose an approach for quantum circuit architecture search. The sampling of the circuit architecture is learnt through reinforcement learning based controller. Layer-based search is also used to accelerate the computational efficiency of the search algorithm. Applying to data classification tasks we show that the method can search for quantum circuit architectures with better accuracies. Moreover, the circuit has a smaller number of quantum gates and parameters.","sentences":["Variational quantum circuits are one of the promising ways to exploit the advantages of quantum computing in the noisy intermediate-scale quantum technology era.","The design of the quantum circuit architecture might greatly affect the performance capability of the quantum algorithms.","The quantum architecture search is the process of automatically designing quantum circuit architecture, aiming at finding the optimal quantum circuit composition architecture by the algorithm for a given task, so that the algorithm can learn to design the circuit architecture.","Compared to manual design, quantum architecture search algorithms are more effective in finding quantum circuits with better performance capabilities.","In this paper, based on the deep reinforcement learning, we propose an approach for quantum circuit architecture search.","The sampling of the circuit architecture is learnt through reinforcement learning based controller.","Layer-based search is also used to accelerate the computational efficiency of the search algorithm.","Applying to data classification tasks we show that the method can search for quantum circuit architectures with better accuracies.","Moreover, the circuit has a smaller number of quantum gates and parameters."],"url":"http://arxiv.org/abs/2405.05892v1","category":"quant-ph"}
{"created":"2024-05-09 13:09:15","title":"Designing Social Learning","abstract":"This paper studies strategic communication in the context of social learning. Product reviews are used by consumers to learn product quality, but in order to write a review, a consumer must be convinced to purchase the item first. When reviewers care about welfare of future consumers, this leads to a conflict: a reviewer today wants the future consumers to purchase the item even when this comes at a loss to them, so that more information is revealed for the consumers that come after. We show that due to this conflict, communication via reviews is inevitably noisy, regardless of whether reviewers can commit to a communication strategy or have to resort to cheap talk. The optimal communication mechanism involves truthful communication of extreme experiences and pools the moderate experiences together.","sentences":["This paper studies strategic communication in the context of social learning.","Product reviews are used by consumers to learn product quality, but in order to write a review, a consumer must be convinced to purchase the item first.","When reviewers care about welfare of future consumers, this leads to a conflict: a reviewer today wants the future consumers to purchase the item even when this comes at a loss to them, so that more information is revealed for the consumers that come after.","We show that due to this conflict, communication via reviews is inevitably noisy, regardless of whether reviewers can commit to a communication strategy or have to resort to cheap talk.","The optimal communication mechanism involves truthful communication of extreme experiences and pools the moderate experiences together."],"url":"http://arxiv.org/abs/2405.05744v1","category":"econ.TH"}
{"created":"2024-05-09 12:50:16","title":"Batched Stochastic Bandit for Nondegenerate Functions","abstract":"This paper studies batched bandit learning problems for nondegenerate functions. We introduce an algorithm that solves the batched bandit problem for nondegenerate functions near-optimally. More specifically, we introduce an algorithm, called Geometric Narrowing (GN), whose regret bound is of order $\\widetilde{{\\mathcal{O}}} ( A_{+}^d \\sqrt{T} )$. In addition, GN only needs $\\mathcal{O} (\\log \\log T)$ batches to achieve this regret. We also provide lower bound analysis for this problem. More specifically, we prove that over some (compact) doubling metric space of doubling dimension $d$: 1. For any policy $\\pi$, there exists a problem instance on which $\\pi$ admits a regret of order ${\\Omega} ( A_-^d \\sqrt{T})$; 2. No policy can achieve a regret of order $ A_-^d \\sqrt{T} $ over all problem instances, using less than $ \\Omega ( \\log \\log T ) $ rounds of communications. Our lower bound analysis shows that the GN algorithm achieves near optimal regret with minimal number of batches.","sentences":["This paper studies batched bandit learning problems for nondegenerate functions.","We introduce an algorithm that solves the batched bandit problem for nondegenerate functions near-optimally.","More specifically, we introduce an algorithm, called Geometric Narrowing (GN), whose regret bound is of order $\\widetilde{{\\mathcal{O}}} ( A_{+}^d \\sqrt{T} )$.","In addition, GN only needs $\\mathcal{O} (\\log \\log T)$ batches to achieve this regret.","We also provide lower bound analysis for this problem.","More specifically, we prove that over some (compact) doubling metric space of doubling dimension $d$: 1.","For any policy $\\pi$, there exists a problem instance on which $\\pi$ admits a regret of order ${\\Omega} ( A_-^d \\sqrt{T})$; 2.","No policy can achieve a regret of order $ A_-^d \\sqrt{T} $ over all problem instances, using less than $ \\Omega ( \\log \\log T )","$ rounds of communications.","Our lower bound analysis shows that the GN algorithm achieves near optimal regret with minimal number of batches."],"url":"http://arxiv.org/abs/2405.05733v1","category":"stat.ML"}
{"created":"2024-05-09 12:03:38","title":"Detecting Statements in Text: A Domain-Agnostic Few-Shot Solution","abstract":"Many tasks related to Computational Social Science and Web Content Analysis involve classifying pieces of text based on the claims they contain. State-of-the-art approaches usually involve fine-tuning models on large annotated datasets, which are costly to produce. In light of this, we propose and release a qualitative and versatile few-shot learning methodology as a common paradigm for any claim-based textual classification task. This methodology involves defining the classes as arbitrarily sophisticated taxonomies of claims, and using Natural Language Inference models to obtain the textual entailment between these and a corpus of interest. The performance of these models is then boosted by annotating a minimal sample of data points, dynamically sampled using the well-established statistical heuristic of Probabilistic Bisection. We illustrate this methodology in the context of three tasks: climate change contrarianism detection, topic/stance classification and depression-relates symptoms detection. This approach rivals traditional pre-train/fine-tune approaches while drastically reducing the need for data annotation.","sentences":["Many tasks related to Computational Social Science and Web Content Analysis involve classifying pieces of text based on the claims they contain.","State-of-the-art approaches usually involve fine-tuning models on large annotated datasets, which are costly to produce.","In light of this, we propose and release a qualitative and versatile few-shot learning methodology as a common paradigm for any claim-based textual classification task.","This methodology involves defining the classes as arbitrarily sophisticated taxonomies of claims, and using Natural Language Inference models to obtain the textual entailment between these and a corpus of interest.","The performance of these models is then boosted by annotating a minimal sample of data points, dynamically sampled using the well-established statistical heuristic of Probabilistic Bisection.","We illustrate this methodology in the context of three tasks: climate change contrarianism detection, topic/stance classification and depression-relates symptoms detection.","This approach rivals traditional pre-train/fine-tune approaches while drastically reducing the need for data annotation."],"url":"http://arxiv.org/abs/2405.05705v1","category":"cs.CL"}
{"created":"2024-05-09 10:58:40","title":"Imprecise Multi-Armed Bandits","abstract":"We introduce a novel multi-armed bandit framework, where each arm is associated with a fixed unknown credal set over the space of outcomes (which can be richer than just the reward). The arm-to-credal-set correspondence comes from a known class of hypotheses. We then define a notion of regret corresponding to the lower prevision defined by these credal sets. Equivalently, the setting can be regarded as a two-player zero-sum game, where, on each round, the agent chooses an arm and the adversary chooses the distribution over outcomes from a set of options associated with this arm. The regret is defined with respect to the value of game. For certain natural hypothesis classes, loosely analgous to stochastic linear bandits (which are a special case of the resulting setting), we propose an algorithm and prove a corresponding upper bound on regret. We also prove lower bounds on regret for particular special cases.","sentences":["We introduce a novel multi-armed bandit framework, where each arm is associated with a fixed unknown credal set over the space of outcomes (which can be richer than just the reward).","The arm-to-credal-set correspondence comes from a known class of hypotheses.","We then define a notion of regret corresponding to the lower prevision defined by these credal sets.","Equivalently, the setting can be regarded as a two-player zero-sum game, where, on each round, the agent chooses an arm and the adversary chooses the distribution over outcomes from a set of options associated with this arm.","The regret is defined with respect to the value of game.","For certain natural hypothesis classes, loosely analgous to stochastic linear bandits (which are a special case of the resulting setting), we propose an algorithm and prove a corresponding upper bound on regret.","We also prove lower bounds on regret for particular special cases."],"url":"http://arxiv.org/abs/2405.05673v1","category":"cs.LG"}
{"created":"2024-05-09 10:37:33","title":"SubGDiff: A Subgraph Diffusion Model to Improve Molecular Representation Learning","abstract":"Molecular representation learning has shown great success in advancing AI-based drug discovery. The core of many recent works is based on the fact that the 3D geometric structure of molecules provides essential information about their physical and chemical characteristics. Recently, denoising diffusion probabilistic models have achieved impressive performance in 3D molecular representation learning. However, most existing molecular diffusion models treat each atom as an independent entity, overlooking the dependency among atoms within the molecular substructures. This paper introduces a novel approach that enhances molecular representation learning by incorporating substructural information within the diffusion process. We propose a novel diffusion model termed SubGDiff for involving the molecular subgraph information in diffusion. Specifically, SubGDiff adopts three vital techniques: i) subgraph prediction, ii) expectation state, and iii) k-step same subgraph diffusion, to enhance the perception of molecular substructure in the denoising network. Experimentally, extensive downstream tasks demonstrate the superior performance of our approach. The code is available at https://github.com/youjibiying/SubGDiff.","sentences":["Molecular representation learning has shown great success in advancing AI-based drug discovery.","The core of many recent works is based on the fact that the 3D geometric structure of molecules provides essential information about their physical and chemical characteristics.","Recently, denoising diffusion probabilistic models have achieved impressive performance in 3D molecular representation learning.","However, most existing molecular diffusion models treat each atom as an independent entity, overlooking the dependency among atoms within the molecular substructures.","This paper introduces a novel approach that enhances molecular representation learning by incorporating substructural information within the diffusion process.","We propose a novel diffusion model termed SubGDiff for involving the molecular subgraph information in diffusion.","Specifically, SubGDiff adopts three vital techniques: i) subgraph prediction, ii) expectation state, and iii) k-step same subgraph diffusion, to enhance the perception of molecular substructure in the denoising network.","Experimentally, extensive downstream tasks demonstrate the superior performance of our approach.","The code is available at https://github.com/youjibiying/SubGDiff."],"url":"http://arxiv.org/abs/2405.05665v1","category":"cs.LG"}
{"created":"2024-05-09 08:23:20","title":"Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning","abstract":"Current solutions for efficiently constructing large vision-language (VL) models follow a two-step paradigm: projecting the output of pre-trained vision encoders to the input space of pre-trained language models as visual prompts; and then transferring the models to downstream VL tasks via end-to-end parameter-efficient fine-tuning (PEFT). However, this paradigm still exhibits inefficiency since it significantly increases the input length of the language models. In this paper, in contrast to integrating visual prompts into inputs, we regard visual prompts as additional knowledge that facilitates language models in addressing tasks associated with visual information. Motivated by the finding that Feed-Forward Network (FFN) of language models acts as \"key-value memory\", we introduce a novel approach termed memory-space visual prompting (MemVP), wherein visual prompts are concatenated with the weights of FFN for visual knowledge injection. Experimental results across various VL tasks and language models reveal that MemVP significantly reduces the training time and inference latency of the finetuned VL models and surpasses the performance of previous PEFT methods. Code: https://github.com/JieShibo/MemVP","sentences":["Current solutions for efficiently constructing large vision-language (VL) models follow a two-step paradigm: projecting the output of pre-trained vision encoders to the input space of pre-trained language models as visual prompts; and then transferring the models to downstream VL tasks via end-to-end parameter-efficient fine-tuning (PEFT).","However, this paradigm still exhibits inefficiency since it significantly increases the input length of the language models.","In this paper, in contrast to integrating visual prompts into inputs, we regard visual prompts as additional knowledge that facilitates language models in addressing tasks associated with visual information.","Motivated by the finding that Feed-Forward Network (FFN) of language models acts as \"key-value memory\", we introduce a novel approach termed memory-space visual prompting (MemVP), wherein visual prompts are concatenated with the weights of FFN for visual knowledge injection.","Experimental results across various VL tasks and language models reveal that MemVP significantly reduces the training time and inference latency of the finetuned VL models and surpasses the performance of previous PEFT methods.","Code: https://github.com/JieShibo/MemVP"],"url":"http://arxiv.org/abs/2405.05615v1","category":"cs.CV"}
{"created":"2024-05-09 08:17:06","title":"Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification","abstract":"Unsupervised Visible-Infrared Person Re-identification (USVI-ReID) presents a formidable challenge, which aims to match pedestrian images across visible and infrared modalities without any annotations. Recently, clustered pseudo-label methods have become predominant in USVI-ReID, although the inherent noise in pseudo-labels presents a significant obstacle. Most existing works primarily focus on shielding the model from the harmful effects of noise, neglecting to calibrate noisy pseudo-labels usually associated with hard samples, which will compromise the robustness of the model. To address this issue, we design a Robust Pseudo-label Learning with Neighbor Relation (RPNR) framework for USVI-ReID. To be specific, we first introduce a straightforward yet potent Noisy Pseudo-label Calibration module to correct noisy pseudo-labels. Due to the high intra-class variations, noisy pseudo-labels are difficult to calibrate completely. Therefore, we introduce a Neighbor Relation Learning module to reduce high intra-class variations by modeling potential interactions between all samples. Subsequently, we devise an Optimal Transport Prototype Matching module to establish reliable cross-modality correspondences. On that basis, we design a Memory Hybrid Learning module to jointly learn modality-specific and modality-invariant information. Comprehensive experiments conducted on two widely recognized benchmarks, SYSU-MM01 and RegDB, demonstrate that RPNR outperforms the current state-of-the-art GUR with an average Rank-1 improvement of 10.3%. The source codes will be released soon.","sentences":["Unsupervised Visible-Infrared Person Re-identification (USVI-ReID) presents a formidable challenge, which aims to match pedestrian images across visible and infrared modalities without any annotations.","Recently, clustered pseudo-label methods have become predominant in USVI-ReID, although the inherent noise in pseudo-labels presents a significant obstacle.","Most existing works primarily focus on shielding the model from the harmful effects of noise, neglecting to calibrate noisy pseudo-labels usually associated with hard samples, which will compromise the robustness of the model.","To address this issue, we design a Robust Pseudo-label Learning with Neighbor Relation (RPNR) framework for USVI-ReID.","To be specific, we first introduce a straightforward yet potent Noisy Pseudo-label Calibration module to correct noisy pseudo-labels.","Due to the high intra-class variations, noisy pseudo-labels are difficult to calibrate completely.","Therefore, we introduce a Neighbor Relation Learning module to reduce high intra-class variations by modeling potential interactions between all samples.","Subsequently, we devise an Optimal Transport Prototype Matching module to establish reliable cross-modality correspondences.","On that basis, we design a Memory Hybrid Learning module to jointly learn modality-specific and modality-invariant information.","Comprehensive experiments conducted on two widely recognized benchmarks, SYSU-MM01 and RegDB, demonstrate that RPNR outperforms the current state-of-the-art GUR with an average Rank-1 improvement of 10.3%.","The source codes will be released soon."],"url":"http://arxiv.org/abs/2405.05613v1","category":"cs.CV"}
{"created":"2024-05-09 07:24:28","title":"Model Inversion Robustness: Can Transfer Learning Help?","abstract":"Model Inversion (MI) attacks aim to reconstruct private training data by abusing access to machine learning models. Contemporary MI attacks have achieved impressive attack performance, posing serious threats to privacy. Meanwhile, all existing MI defense methods rely on regularization that is in direct conflict with the training objective, resulting in noticeable degradation in model utility. In this work, we take a different perspective, and propose a novel and simple Transfer Learning-based Defense against Model Inversion (TL-DMI) to render MI-robust models. Particularly, by leveraging TL, we limit the number of layers encoding sensitive information from private training dataset, thereby degrading the performance of MI attack. We conduct an analysis using Fisher Information to justify our method. Our defense is remarkably simple to implement. Without bells and whistles, we show in extensive experiments that TL-DMI achieves state-of-the-art (SOTA) MI robustness. Our code, pre-trained models, demo and inverted data are available at: https://hosytuyen.github.io/projects/TL-DMI","sentences":["Model Inversion (MI) attacks aim to reconstruct private training data by abusing access to machine learning models.","Contemporary MI attacks have achieved impressive attack performance, posing serious threats to privacy.","Meanwhile, all existing MI defense methods rely on regularization that is in direct conflict with the training objective, resulting in noticeable degradation in model utility.","In this work, we take a different perspective, and propose a novel and simple Transfer Learning-based Defense against Model Inversion (TL-DMI) to render MI-robust models.","Particularly, by leveraging TL, we limit the number of layers encoding sensitive information from private training dataset, thereby degrading the performance of MI attack.","We conduct an analysis using Fisher Information to justify our method.","Our defense is remarkably simple to implement.","Without bells and whistles, we show in extensive experiments that TL-DMI achieves state-of-the-art (SOTA) MI robustness.","Our code, pre-trained models, demo and inverted data are available at: https://hosytuyen.github.io/projects/TL-DMI"],"url":"http://arxiv.org/abs/2405.05588v1","category":"cs.LG"}
{"created":"2024-05-09 05:51:33","title":"Joint Edge Optimization Deep Unfolding Network for Accelerated MRI Reconstruction","abstract":"Magnetic Resonance Imaging (MRI) is a widely used imaging technique, however it has the limitation of long scanning time. Though previous model-based and learning-based MRI reconstruction methods have shown promising performance, most of them have not fully utilized the edge prior of MR images, and there is still much room for improvement. In this paper, we build a joint edge optimization model that not only incorporates individual regularizers specific to both the MR image and the edges, but also enforces a co-regularizer to effectively establish a stronger correlation between them. Specifically, the edge information is defined through a non-edge probability map to guide the image reconstruction during the optimization process. Meanwhile, the regularizers pertaining to images and edges are incorporated into a deep unfolding network to automatically learn their respective inherent a-priori information.Numerical experiments, consisting of multi-coil and single-coil MRI data with different sampling schemes at a variety of sampling factors, demonstrate that the proposed method outperforms other compared methods.","sentences":["Magnetic Resonance Imaging (MRI) is a widely used imaging technique, however it has the limitation of long scanning time.","Though previous model-based and learning-based MRI reconstruction methods have shown promising performance, most of them have not fully utilized the edge prior of MR images, and there is still much room for improvement.","In this paper, we build a joint edge optimization model that not only incorporates individual regularizers specific to both the MR image and the edges, but also enforces a co-regularizer to effectively establish a stronger correlation between them.","Specifically, the edge information is defined through a non-edge probability map to guide the image reconstruction during the optimization process.","Meanwhile, the regularizers pertaining to images and edges are incorporated into a deep unfolding network to automatically learn their respective inherent a-priori information.","Numerical experiments, consisting of multi-coil and single-coil MRI data with different sampling schemes at a variety of sampling factors, demonstrate that the proposed method outperforms other compared methods."],"url":"http://arxiv.org/abs/2405.05564v1","category":"eess.IV"}
{"created":"2024-05-09 05:08:30","title":"Deep Hierarchical Graph Alignment Kernels","abstract":"Typical R-convolution graph kernels invoke the kernel functions that decompose graphs into non-isomorphic substructures and compare them. However, overlooking implicit similarities and topological position information between those substructures limits their performances. In this paper, we introduce Deep Hierarchical Graph Alignment Kernels (DHGAK) to resolve this problem. Specifically, the relational substructures are hierarchically aligned to cluster distributions in their deep embedding space. The substructures belonging to the same cluster are assigned the same feature map in the Reproducing Kernel Hilbert Space (RKHS), where graph feature maps are derived by kernel mean embedding. Theoretical analysis guarantees that DHGAK is positive semi-definite and has linear separability in the RKHS. Comparison with state-of-the-art graph kernels on various benchmark datasets demonstrates the effectiveness and efficiency of DHGAK. The code is available at Github (https://github.com/EWesternRa/DHGAK).","sentences":["Typical R-convolution graph kernels invoke the kernel functions that decompose graphs into non-isomorphic substructures and compare them.","However, overlooking implicit similarities and topological position information between those substructures limits their performances.","In this paper, we introduce Deep Hierarchical Graph Alignment Kernels (DHGAK) to resolve this problem.","Specifically, the relational substructures are hierarchically aligned to cluster distributions in their deep embedding space.","The substructures belonging to the same cluster are assigned the same feature map in the Reproducing Kernel Hilbert Space (RKHS), where graph feature maps are derived by kernel mean embedding.","Theoretical analysis guarantees that DHGAK is positive semi-definite and has linear separability in the RKHS.","Comparison with state-of-the-art graph kernels on various benchmark datasets demonstrates the effectiveness and efficiency of DHGAK.","The code is available at Github (https://github.com/EWesternRa/DHGAK)."],"url":"http://arxiv.org/abs/2405.05545v1","category":"cs.LG"}
{"created":"2024-05-09 03:28:16","title":"Ditto: Quantization-aware Secure Inference of Transformers upon MPC","abstract":"Due to the rising privacy concerns on sensitive client data and trained models like Transformers, secure multi-party computation (MPC) techniques are employed to enable secure inference despite attendant overhead. Existing works attempt to reduce the overhead using more MPC-friendly non-linear function approximations. However, the integration of quantization widely used in plaintext inference into the MPC domain remains unclear. To bridge this gap, we propose the framework named Ditto to enable more efficient quantization-aware secure Transformer inference. Concretely, we first incorporate an MPC-friendly quantization into Transformer inference and employ a quantization-aware distillation procedure to maintain the model utility. Then, we propose novel MPC primitives to support the type conversions that are essential in quantization and implement the quantization-aware MPC execution of secure quantized inference. This approach significantly decreases both computation and communication overhead, leading to improvements in overall efficiency. We conduct extensive experiments on Bert and GPT2 models to evaluate the performance of Ditto. The results demonstrate that Ditto is about $3.14\\sim 4.40\\times$ faster than MPCFormer (ICLR 2023) and $1.44\\sim 2.35\\times$ faster than the state-of-the-art work PUMA with negligible utility degradation.","sentences":["Due to the rising privacy concerns on sensitive client data and trained models like Transformers, secure multi-party computation (MPC) techniques are employed to enable secure inference despite attendant overhead.","Existing works attempt to reduce the overhead using more MPC-friendly non-linear function approximations.","However, the integration of quantization widely used in plaintext inference into the MPC domain remains unclear.","To bridge this gap, we propose the framework named Ditto to enable more efficient quantization-aware secure Transformer inference.","Concretely, we first incorporate an MPC-friendly quantization into Transformer inference and employ a quantization-aware distillation procedure to maintain the model utility.","Then, we propose novel MPC primitives to support the type conversions that are essential in quantization and implement the quantization-aware MPC execution of secure quantized inference.","This approach significantly decreases both computation and communication overhead, leading to improvements in overall efficiency.","We conduct extensive experiments on Bert and GPT2 models to evaluate the performance of Ditto.","The results demonstrate that Ditto is about $3.14\\sim 4.40\\times$ faster than MPCFormer (ICLR 2023) and $1.44\\sim 2.35\\times$ faster than the state-of-the-art work PUMA with negligible utility degradation."],"url":"http://arxiv.org/abs/2405.05525v1","category":"cs.CR"}
{"created":"2024-05-09 03:19:19","title":"Continuous max-flow augmentation of self-supervised few-shot learning on SPECT left ventricles","abstract":"Single-Photon Emission Computed Tomography (SPECT) left ventricular assessment protocols are important for detecting ischemia in high-risk patients. To quantitatively measure myocardial function, clinicians depend on commercially available solutions to segment and reorient the left ventricle (LV) for evaluation. Based on large normal datasets, the segmentation performance and the high price of these solutions can hinder the availability of reliable and precise localization of the LV delineation. To overcome the aforementioned shortcomings this paper aims to give a recipe for diagnostic centers as well as for clinics to automatically segment the myocardium based on small and low-quality labels on reconstructed SPECT, complete field-of-view (FOV) volumes. A combination of Continuous Max-Flow (CMF) with prior shape information is developed to augment the 3D U-Net self-supervised learning (SSL) approach on various geometries of SPECT apparatus. Experimental results on the acquired dataset have shown a 5-10\\% increase in quantitative metrics based on the previous State-of-the-Art (SOTA) solutions, suggesting a good plausible way to tackle the few-shot SSL problem on high-noise SPECT cardiac datasets.","sentences":["Single-Photon Emission Computed Tomography (SPECT) left ventricular assessment protocols are important for detecting ischemia in high-risk patients.","To quantitatively measure myocardial function, clinicians depend on commercially available solutions to segment and reorient the left ventricle (LV) for evaluation.","Based on large normal datasets, the segmentation performance and the high price of these solutions can hinder the availability of reliable and precise localization of the LV delineation.","To overcome the aforementioned shortcomings this paper aims to give a recipe for diagnostic centers as well as for clinics to automatically segment the myocardium based on small and low-quality labels on reconstructed SPECT, complete field-of-view (FOV) volumes.","A combination of Continuous Max-Flow (CMF) with prior shape information is developed to augment the 3D U-Net self-supervised learning (SSL) approach on various geometries of SPECT apparatus.","Experimental results on the acquired dataset have shown a 5-10\\% increase in quantitative metrics based on the previous State-of-the-Art (SOTA) solutions, suggesting a good plausible way to tackle the few-shot SSL problem on high-noise SPECT cardiac datasets."],"url":"http://arxiv.org/abs/2405.05520v1","category":"eess.IV"}
{"created":"2024-05-08 23:38:02","title":"Cross-Modality Translation with Generative Adversarial Networks to Unveil Alzheimer's Disease Biomarkers","abstract":"Generative approaches for cross-modality transformation have recently gained significant attention in neuroimaging. While most previous work has focused on case-control data, the application of generative models to disorder-specific datasets and their ability to preserve diagnostic patterns remain relatively unexplored. Hence, in this study, we investigated the use of a generative adversarial network (GAN) in the context of Alzheimer's disease (AD) to generate functional network connectivity (FNC) and T1-weighted structural magnetic resonance imaging data from each other. We employed a cycle-GAN to synthesize data in an unpaired data transition and enhanced the transition by integrating weak supervision in cases where paired data were available. Our findings revealed that our model could offer remarkable capability, achieving a structural similarity index measure (SSIM) of $0.89 \\pm 0.003$ for T1s and a correlation of $0.71 \\pm 0.004$ for FNCs. Moreover, our qualitative analysis revealed similar patterns between generated and actual data when comparing AD to cognitively normal (CN) individuals. In particular, we observed significantly increased functional connectivity in cerebellar-sensory motor and cerebellar-visual networks and reduced connectivity in cerebellar-subcortical, auditory-sensory motor, sensory motor-visual, and cerebellar-cognitive control networks. Additionally, the T1 images generated by our model showed a similar pattern of atrophy in the hippocampal and other temporal regions of Alzheimer's patients.","sentences":["Generative approaches for cross-modality transformation have recently gained significant attention in neuroimaging.","While most previous work has focused on case-control data, the application of generative models to disorder-specific datasets and their ability to preserve diagnostic patterns remain relatively unexplored.","Hence, in this study, we investigated the use of a generative adversarial network (GAN) in the context of Alzheimer's disease (AD) to generate functional network connectivity (FNC) and T1-weighted structural magnetic resonance imaging data from each other.","We employed a cycle-GAN to synthesize data in an unpaired data transition and enhanced the transition by integrating weak supervision in cases where paired data were available.","Our findings revealed that our model could offer remarkable capability, achieving a structural similarity index measure (SSIM) of $0.89 \\pm 0.003$ for T1s and a correlation of $0.71 \\pm 0.004$ for FNCs.","Moreover, our qualitative analysis revealed similar patterns between generated and actual data when comparing AD to cognitively normal (CN) individuals.","In particular, we observed significantly increased functional connectivity in cerebellar-sensory motor and cerebellar-visual networks and reduced connectivity in cerebellar-subcortical, auditory-sensory motor, sensory motor-visual, and cerebellar-cognitive control networks.","Additionally, the T1 images generated by our model showed a similar pattern of atrophy in the hippocampal and other temporal regions of Alzheimer's patients."],"url":"http://arxiv.org/abs/2405.05462v1","category":"q-bio.NC"}
{"created":"2024-05-08 22:54:04","title":"Markowitz Meets Bellman: Knowledge-distilled Reinforcement Learning for Portfolio Management","abstract":"Investment portfolios, central to finance, balance potential returns and risks. This paper introduces a hybrid approach combining Markowitz's portfolio theory with reinforcement learning, utilizing knowledge distillation for training agents. In particular, our proposed method, called KDD (Knowledge Distillation DDPG), consist of two training stages: supervised and reinforcement learning stages. The trained agents optimize portfolio assembly. A comparative analysis against standard financial models and AI frameworks, using metrics like returns, the Sharpe ratio, and nine evaluation indices, reveals our model's superiority. It notably achieves the highest yield and Sharpe ratio of 2.03, ensuring top profitability with the lowest risk in comparable return scenarios.","sentences":["Investment portfolios, central to finance, balance potential returns and risks.","This paper introduces a hybrid approach combining Markowitz's portfolio theory with reinforcement learning, utilizing knowledge distillation for training agents.","In particular, our proposed method, called KDD (Knowledge Distillation DDPG), consist of two training stages: supervised and reinforcement learning stages.","The trained agents optimize portfolio assembly.","A comparative analysis against standard financial models and AI frameworks, using metrics like returns, the Sharpe ratio, and nine evaluation indices, reveals our model's superiority.","It notably achieves the highest yield and Sharpe ratio of 2.03, ensuring top profitability with the lowest risk in comparable return scenarios."],"url":"http://arxiv.org/abs/2405.05449v1","category":"q-fin.CP"}
{"created":"2024-05-08 22:28:57","title":"Large Language Model Enhanced Machine Learning Estimators for Classification","abstract":"Pre-trained large language models (LLM) have emerged as a powerful tool for simulating various scenarios and generating output given specific instructions and multimodal input. In this work, we analyze the specific use of LLM to enhance a classical supervised machine learning method for classification problems. We propose a few approaches to integrate LLM into a classical machine learning estimator to further enhance the prediction performance. We examine the performance of the proposed approaches through both standard supervised learning binary classification tasks, and a transfer learning task where the test data observe distribution changes compared to the training data. Numerical experiments using four publicly available datasets are conducted and suggest that using LLM to enhance classical machine learning estimators can provide significant improvement on prediction performance.","sentences":["Pre-trained large language models (LLM) have emerged as a powerful tool for simulating various scenarios and generating output given specific instructions and multimodal input.","In this work, we analyze the specific use of LLM to enhance a classical supervised machine learning method for classification problems.","We propose a few approaches to integrate LLM into a classical machine learning estimator to further enhance the prediction performance.","We examine the performance of the proposed approaches through both standard supervised learning binary classification tasks, and a transfer learning task where the test data observe distribution changes compared to the training data.","Numerical experiments using four publicly available datasets are conducted and suggest that using LLM to enhance classical machine learning estimators can provide significant improvement on prediction performance."],"url":"http://arxiv.org/abs/2405.05445v1","category":"cs.LG"}
{"created":"2024-05-08 21:24:49","title":"Searching for Programmatic Policies in Semantic Spaces","abstract":"Syntax-guided synthesis is commonly used to generate programs encoding policies. In this approach, the set of programs, that can be written in a domain-specific language defines the search space, and an algorithm searches within this space for programs that encode strong policies. In this paper, we propose an alternative method for synthesizing programmatic policies, where we search within an approximation of the language's semantic space. We hypothesized that searching in semantic spaces is more sample-efficient compared to syntax-based spaces. Our rationale is that the search is more efficient if the algorithm evaluates different agent behaviors as it searches through the space, a feature often missing in syntax-based spaces. This is because small changes in the syntax of a program often do not result in different agent behaviors. We define semantic spaces by learning a library of programs that present different agent behaviors. Then, we approximate the semantic space by defining a neighborhood function for local search algorithms, where we replace parts of the current candidate program with programs from the library. We evaluated our hypothesis in a real-time strategy game called MicroRTS. Empirical results support our hypothesis that searching in semantic spaces can be more sample-efficient than searching in syntax-based spaces.","sentences":["Syntax-guided synthesis is commonly used to generate programs encoding policies.","In this approach, the set of programs, that can be written in a domain-specific language defines the search space, and an algorithm searches within this space for programs that encode strong policies.","In this paper, we propose an alternative method for synthesizing programmatic policies, where we search within an approximation of the language's semantic space.","We hypothesized that searching in semantic spaces is more sample-efficient compared to syntax-based spaces.","Our rationale is that the search is more efficient if the algorithm evaluates different agent behaviors as it searches through the space, a feature often missing in syntax-based spaces.","This is because small changes in the syntax of a program often do not result in different agent behaviors.","We define semantic spaces by learning a library of programs that present different agent behaviors.","Then, we approximate the semantic space by defining a neighborhood function for local search algorithms, where we replace parts of the current candidate program with programs from the library.","We evaluated our hypothesis in a real-time strategy game called MicroRTS.","Empirical results support our hypothesis that searching in semantic spaces can be more sample-efficient than searching in syntax-based spaces."],"url":"http://arxiv.org/abs/2405.05431v1","category":"cs.LG"}
{"created":"2024-05-08 21:18:02","title":"Adversary-Guided Motion Retargeting for Skeleton Anonymization","abstract":"Skeleton-based motion visualization is a rising field in computer vision, especially in the case of virtual reality (VR). With further advancements in human-pose estimation and skeleton extracting sensors, more and more applications that utilize skeleton data have come about. These skeletons may appear to be anonymous but they contain embedded personally identifiable information (PII). In this paper we present a new anonymization technique that is based on motion retargeting, utilizing adversary classifiers to further remove PII embedded in the skeleton. Motion retargeting is effective in anonymization as it transfers the movement of the user onto the a dummy skeleton. In doing so, any PII linked to the skeleton will be based on the dummy skeleton instead of the user we are protecting. We propose a Privacy-centric Deep Motion Retargeting model (PMR) which aims to further clear the retargeted skeleton of PII through adversarial learning. In our experiments, PMR achieves motion retargeting utility performance on par with state of the art models while also reducing the performance of privacy attacks.","sentences":["Skeleton-based motion visualization is a rising field in computer vision, especially in the case of virtual reality (VR).","With further advancements in human-pose estimation and skeleton extracting sensors, more and more applications that utilize skeleton data have come about.","These skeletons may appear to be anonymous but they contain embedded personally identifiable information (PII).","In this paper we present a new anonymization technique that is based on motion retargeting, utilizing adversary classifiers to further remove PII embedded in the skeleton.","Motion retargeting is effective in anonymization as it transfers the movement of the user onto the a dummy skeleton.","In doing so, any PII linked to the skeleton will be based on the dummy skeleton instead of the user we are protecting.","We propose a Privacy-centric Deep Motion Retargeting model (PMR) which aims to further clear the retargeted skeleton of PII through adversarial learning.","In our experiments, PMR achieves motion retargeting utility performance on par with state of the art models while also reducing the performance of privacy attacks."],"url":"http://arxiv.org/abs/2405.05428v1","category":"cs.CV"}
{"created":"2024-05-08 20:25:55","title":"Permalife Of The Archive: Archaeogaming As Queergaming","abstract":"Archaeogaming and queer games studies have both grown as paradigms in the last decade. The former broadly refers to the archaeological study of games, while the later concerns the application of queer theory to the medium. To date, there has been limited engagement of archaeogamers with queer games scholarship, and vice versa. This article argues that there are epistomological parallels between the two; as they are both concerned with the limits and ethics of representation, the personal and political contexts of game development and engagement with video games through transgressive play.   The paper is structured around an extended literature review and three vignettes that reflect on the author's personal experience of conducting archaeogaming research, an ethnographic study of Wurm Online, an archaeological survey of Elden Ring and a player study of the generative archaeology game Nothing Beside Remains. While archaeogaming can learn from the centring of subjective lived experience and labour in the queer games sphere, archaeogaming as a form of game preservation can also benefit queer games studies.","sentences":["Archaeogaming and queer games studies have both grown as paradigms in the last decade.","The former broadly refers to the archaeological study of games, while the later concerns the application of queer theory to the medium.","To date, there has been limited engagement of archaeogamers with queer games scholarship, and vice versa.","This article argues that there are epistomological parallels between the two; as they are both concerned with the limits and ethics of representation, the personal and political contexts of game development and engagement with video games through transgressive play.   ","The paper is structured around an extended literature review and three vignettes that reflect on the author's personal experience of conducting archaeogaming research, an ethnographic study of Wurm Online, an archaeological survey of Elden Ring and a player study of the generative archaeology game Nothing Beside Remains.","While archaeogaming can learn from the centring of subjective lived experience and labour in the queer games sphere, archaeogaming as a form of game preservation can also benefit queer games studies."],"url":"http://arxiv.org/abs/2405.05411v1","category":"cs.HC"}
{"created":"2024-05-08 20:23:24","title":"Initialization is Critical to Whether Transformers Fit Composite Functions by Inference or Memorizing","abstract":"Transformers have shown impressive capabilities across various tasks, but their performance on compositional problems remains a topic of debate. In this work, we investigate the mechanisms of how transformers behave on unseen compositional tasks using anchor functions. We discover that the parameter initialization scale plays a critical role in determining whether the model learns inferential solutions, which capture the underlying compositional primitives, or symmetric solutions, which simply memorize mappings without understanding the compositional structure. By analyzing the information flow and vector representations within the model, we reveal the distinct mechanisms underlying these solution types. We further find that inferential solutions exhibit low complexity bias, which we hypothesize is a key factor enabling them to learn individual mappings for single anchors. Building upon our understanding of these mechanisms, we can predict the learning behavior of models with different initialization scales when faced with data of varying inferential complexity. Our findings provide valuable insights into the role of initialization scale in shaping the type of solution learned by transformers and their ability to learn and generalize compositional functions.","sentences":["Transformers have shown impressive capabilities across various tasks, but their performance on compositional problems remains a topic of debate.","In this work, we investigate the mechanisms of how transformers behave on unseen compositional tasks using anchor functions.","We discover that the parameter initialization scale plays a critical role in determining whether the model learns inferential solutions, which capture the underlying compositional primitives, or symmetric solutions, which simply memorize mappings without understanding the compositional structure.","By analyzing the information flow and vector representations within the model, we reveal the distinct mechanisms underlying these solution types.","We further find that inferential solutions exhibit low complexity bias, which we hypothesize is a key factor enabling them to learn individual mappings for single anchors.","Building upon our understanding of these mechanisms, we can predict the learning behavior of models with different initialization scales when faced with data of varying inferential complexity.","Our findings provide valuable insights into the role of initialization scale in shaping the type of solution learned by transformers and their ability to learn and generalize compositional functions."],"url":"http://arxiv.org/abs/2405.05409v1","category":"cs.LG"}
{"created":"2024-05-08 19:05:55","title":"Antimagic and product antimagic graphs with pendant edges","abstract":"Let $G=(V,E)$ be a simple graph of size $m$ and $L$ a set of $m$ distinct real numbers. An $L$-labeling of $G$ is a bijection $\\phi: E \\rightarrow L$. We say that $\\phi$ is an antimagic $L$-labeling if the induced vertex sum $\\phi_+: V \\rightarrow \\mathbb {R}$ defined as $\\phi_+(u)=\\sum_{uv\\in E}\\phi(uv)$ is injective. Similarly, $\\phi$ is a product antimagic $L$-labeling of $G$ if the induced vertex product $\\phi_{\\circ}: V \\rightarrow \\mathbb {R}$ defined as $\\phi_{\\circ}(u)=\\prod_{uv\\in E}\\phi(uv)$ is injective. A graph $G$ is antimagic (resp. product antimagic) if it has an antimagic (resp. a product antimagic) $L$-labeling for $L=\\{1,2,\\dots,m\\}$. Hartsfield and Ringel conjectured that every simple connected graph distinct from $K_2$ is antimagic, but the conjecture remains widely open.   We prove, among other results, that every connected graph of size $m$, $m \\geq 3$, admits an antimagic $L$-labeling for every arithmetic sequence $L$ of $m$ positive real numbers, if every vertex of degree at least three is a support vertex. As a corollary, we derive that these graphs are antimagic, reinforcing the veracity of the conjecture by Hartsfield and Ringel. Moreover, these graphs admit also a product antimagic $L$-labeling provided that the smallest element of $L$ is at least one. The proof is constructive.","sentences":["Let $G=(V,E)$ be a simple graph of size $m$ and $L$ a set of $m$ distinct real numbers.","An $L$-labeling of $G$ is a bijection $\\phi:","E \\rightarrow L$. We say that $\\phi$ is an antimagic $L$-labeling if the induced vertex sum $\\phi_+:","V \\rightarrow \\mathbb {R}$ defined as $\\phi_+(u)=\\sum_{uv\\in E}\\phi(uv)$ is injective.","Similarly, $\\phi$ is a product antimagic $L$-labeling of $G$ if the induced vertex product $\\phi_{\\circ}: V \\rightarrow \\mathbb {R}$ defined as $\\phi_{\\circ}(u)=\\prod_{uv\\in E}\\phi(uv)$ is injective.","A graph $G$ is antimagic (resp.","product antimagic) if it has an antimagic (resp.","a product antimagic) $L$-labeling for $L=\\{1,2,\\dots,m\\}$. Hartsfield and Ringel conjectured that every simple connected graph distinct from $K_2$ is antimagic, but the conjecture remains widely open.   ","We prove, among other results, that every connected graph of size $m$, $m \\geq 3$, admits an antimagic $L$-labeling for every arithmetic sequence $L$ of $m$ positive real numbers, if every vertex of degree at least three is a support vertex.","As a corollary, we derive that these graphs are antimagic, reinforcing the veracity of the conjecture by Hartsfield and Ringel.","Moreover, these graphs admit also a product antimagic $L$-labeling provided that the smallest element of $L$ is at least one.","The proof is constructive."],"url":"http://arxiv.org/abs/2405.05375v1","category":"math.CO"}
{"created":"2024-05-08 18:56:35","title":"Learning to Play Pursuit-Evasion with Dynamic and Sensor Constraints","abstract":"We present a multi-agent reinforcement learning approach to solve a pursuit-evasion game between two players with car-like dynamics and sensing limitations. We develop a curriculum for an existing multi-agent deterministic policy gradient algorithm to simultaneously obtain strategies for both players, and deploy the learned strategies on real robots moving as fast as 2 m/s in indoor environments. Through experiments we show that the learned strategies improve over existing baselines by up to 30% in terms of capture rate for the pursuer. The learned evader model has up to 5% better escape rate over the baselines even against our competitive pursuer model. We also present experiment results which show how the pursuit-evasion game and its results evolve as the player dynamics and sensor constraints are varied. Finally, we deploy learned policies on physical robots for a game between the F1TENTH and JetRacer platforms and show that the learned strategies can be executed on real-robots. Our code and supplementary material including videos from experiments are available at https: //gonultasbu.github.io/pursuit-evasion/.","sentences":["We present a multi-agent reinforcement learning approach to solve a pursuit-evasion game between two players with car-like dynamics and sensing limitations.","We develop a curriculum for an existing multi-agent deterministic policy gradient algorithm to simultaneously obtain strategies for both players, and deploy the learned strategies on real robots moving as fast as 2 m/s in indoor environments.","Through experiments we show that the learned strategies improve over existing baselines by up to 30% in terms of capture rate for the pursuer.","The learned evader model has up to 5% better escape rate over the baselines even against our competitive pursuer model.","We also present experiment results which show how the pursuit-evasion game and its results evolve as the player dynamics and sensor constraints are varied.","Finally, we deploy learned policies on physical robots for a game between the F1TENTH and JetRacer platforms and show that the learned strategies can be executed on real-robots.","Our code and supplementary material including videos from experiments are available at https: //gonultasbu.github.io/pursuit-evasion/."],"url":"http://arxiv.org/abs/2405.05372v1","category":"cs.RO"}
{"created":"2024-05-08 18:23:51","title":"Data-Driven Cooling Tower Optimization: A Comprehensive Analysis of Energy Savings using Microsand Filtration","abstract":"Effective management of cooling tower systems requires thorough disinfection. While traditional chemical water treatment methods are currently the most prominent strategy, they are costly and yield limited results when relied upon as the sole approach. Cross-flow microsand filtration systems offer a promising alternative with the added benefit of potentially increasing evaporative cooling efficiency, thus saving energy. A comprehensive data-driven analysis over two cooling seasons evaluated the energetic performance of a system equipped with and without an operating filter. For similar environmental conditions, the coefficient of performance was on average 18% higher and was higher 63% of the time when the filter was operating, indicating superior heat transfer efficiency and significant energy savings. It was also 41% higher during periods of high cooling demand. Consequently, the filter and the system work more efficiently at high wet-bulb temperature and thermal load. Machine learning modeling suggested that operating the filter year-round could save between 5% and 13% of the energy bill, primarily during the cooling season. Continuous filter operation is essential as it mitigates biofouling, underscoring its long-term significance, even during periods of lower thermal loads. Integrating filtration systems into cooling tower management therefore fosters sustainable practices by decreasing energy consumption and biofouling.","sentences":["Effective management of cooling tower systems requires thorough disinfection.","While traditional chemical water treatment methods are currently the most prominent strategy, they are costly and yield limited results when relied upon as the sole approach.","Cross-flow microsand filtration systems offer a promising alternative with the added benefit of potentially increasing evaporative cooling efficiency, thus saving energy.","A comprehensive data-driven analysis over two cooling seasons evaluated the energetic performance of a system equipped with and without an operating filter.","For similar environmental conditions, the coefficient of performance was on average 18% higher and was higher 63% of the time when the filter was operating, indicating superior heat transfer efficiency and significant energy savings.","It was also 41% higher during periods of high cooling demand.","Consequently, the filter and the system work more efficiently at high wet-bulb temperature and thermal load.","Machine learning modeling suggested that operating the filter year-round could save between 5% and 13% of the energy bill, primarily during the cooling season.","Continuous filter operation is essential as it mitigates biofouling, underscoring its long-term significance, even during periods of lower thermal loads.","Integrating filtration systems into cooling tower management therefore fosters sustainable practices by decreasing energy consumption and biofouling."],"url":"http://arxiv.org/abs/2405.05346v1","category":"physics.app-ph"}
{"created":"2024-05-08 18:16:37","title":"Distributed Least Squares in Small Space via Sketching and Bias Reduction","abstract":"Matrix sketching is a powerful tool for reducing the size of large data matrices. Yet there are fundamental limitations to this size reduction when we want to recover an accurate estimator for a task such as least square regression. We show that these limitations can be circumvented in the distributed setting by designing sketching methods that minimize the bias of the estimator, rather than its error. In particular, we give a sparse sketching method running in optimal space and current matrix multiplication time, which recovers a nearly-unbiased least squares estimator using two passes over the data. This leads to new communication-efficient distributed averaging algorithms for least squares and related tasks, which directly improve on several prior approaches. Our key novelty is a new bias analysis for sketched least squares, giving a sharp characterization of its dependence on the sketch sparsity. The techniques include new higher-moment restricted Bai-Silverstein inequalities, which are of independent interest to the non-asymptotic analysis of deterministic equivalents for random matrices that arise from sketching.","sentences":["Matrix sketching is a powerful tool for reducing the size of large data matrices.","Yet there are fundamental limitations to this size reduction when we want to recover an accurate estimator for a task such as least square regression.","We show that these limitations can be circumvented in the distributed setting by designing sketching methods that minimize the bias of the estimator, rather than its error.","In particular, we give a sparse sketching method running in optimal space and current matrix multiplication time, which recovers a nearly-unbiased least squares estimator using two passes over the data.","This leads to new communication-efficient distributed averaging algorithms for least squares and related tasks, which directly improve on several prior approaches.","Our key novelty is a new bias analysis for sketched least squares, giving a sharp characterization of its dependence on the sketch sparsity.","The techniques include new higher-moment restricted Bai-Silverstein inequalities, which are of independent interest to the non-asymptotic analysis of deterministic equivalents for random matrices that arise from sketching."],"url":"http://arxiv.org/abs/2405.05343v1","category":"cs.DS"}
{"created":"2024-05-08 18:09:16","title":"Multiplicative Dynamic Mode Decomposition","abstract":"Koopman operators are infinite-dimensional operators that linearize nonlinear dynamical systems, facilitating the study of their spectral properties and enabling the prediction of the time evolution of observable quantities. Recent methods have aimed to approximate Koopman operators while preserving key structures. However, approximating Koopman operators typically requires a dictionary of observables to capture the system's behavior in a finite-dimensional subspace. The selection of these functions is often heuristic, may result in the loss of spectral information, and can severely complicate structure preservation. This paper introduces Multiplicative Dynamic Mode Decomposition (MultDMD), which enforces the multiplicative structure inherent in the Koopman operator within its finite-dimensional approximation. Leveraging this multiplicative property, we guide the selection of observables and define a constrained optimization problem for the matrix approximation, which can be efficiently solved. MultDMD presents a structured approach to finite-dimensional approximations and can more accurately reflect the spectral properties of the Koopman operator. We elaborate on the theoretical framework of MultDMD, detailing its formulation, optimization strategy, and convergence properties. The efficacy of MultDMD is demonstrated through several examples, including the nonlinear pendulum, the Lorenz system, and fluid dynamics data, where we demonstrate its remarkable robustness to noise.","sentences":["Koopman operators are infinite-dimensional operators that linearize nonlinear dynamical systems, facilitating the study of their spectral properties and enabling the prediction of the time evolution of observable quantities.","Recent methods have aimed to approximate Koopman operators while preserving key structures.","However, approximating Koopman operators typically requires a dictionary of observables to capture the system's behavior in a finite-dimensional subspace.","The selection of these functions is often heuristic, may result in the loss of spectral information, and can severely complicate structure preservation.","This paper introduces Multiplicative Dynamic Mode Decomposition (MultDMD), which enforces the multiplicative structure inherent in the Koopman operator within its finite-dimensional approximation.","Leveraging this multiplicative property, we guide the selection of observables and define a constrained optimization problem for the matrix approximation, which can be efficiently solved.","MultDMD presents a structured approach to finite-dimensional approximations and can more accurately reflect the spectral properties of the Koopman operator.","We elaborate on the theoretical framework of MultDMD, detailing its formulation, optimization strategy, and convergence properties.","The efficacy of MultDMD is demonstrated through several examples, including the nonlinear pendulum, the Lorenz system, and fluid dynamics data, where we demonstrate its remarkable robustness to noise."],"url":"http://arxiv.org/abs/2405.05334v1","category":"math.DS"}
{"created":"2024-05-08 18:04:15","title":"Barren plateaus are swamped with traps","abstract":"Two main challenges preventing efficient training of variational quantum algorithms and quantum machine learning models are local minima and barren plateaus. Typically, barren plateaus are associated with deep circuits, while shallow circuits have been shown to suffer from suboptimal local minima. We point out a simple mechanism that creates exponentially many poor local minima specifically in the barren plateau regime. These local minima are trivial solutions, optimizing only a few terms in the loss function, leaving the rest on their barren plateaus. More precisely, we show the existence of approximate local minima, optimizing a single loss term, and conjecture the existence of exact local minima, optimizing only a logarithmic fraction of all loss function terms. One implication of our findings is that simply yielding large gradients is not sufficient to render an initialization strategy a meaningful solution to the barren plateau problem.","sentences":["Two main challenges preventing efficient training of variational quantum algorithms and quantum machine learning models are local minima and barren plateaus.","Typically, barren plateaus are associated with deep circuits, while shallow circuits have been shown to suffer from suboptimal local minima.","We point out a simple mechanism that creates exponentially many poor local minima specifically in the barren plateau regime.","These local minima are trivial solutions, optimizing only a few terms in the loss function, leaving the rest on their barren plateaus.","More precisely, we show the existence of approximate local minima, optimizing a single loss term, and conjecture the existence of exact local minima, optimizing only a logarithmic fraction of all loss function terms.","One implication of our findings is that simply yielding large gradients is not sufficient to render an initialization strategy a meaningful solution to the barren plateau problem."],"url":"http://arxiv.org/abs/2405.05332v1","category":"quant-ph"}
{"created":"2024-05-09 16:31:54","title":"Quantum entanglement enables single-shot trajectory sensing for weakly interacting particles","abstract":"Sensors for mapping the trajectory of an incoming particle find important utility in experimental high energy physics and searches for dark matter. For a quantum sensing protocol that uses projective measurements on a multi-qubit sensor array to infer the trajectory of an incident particle, we show that entanglement can dramatically reduce the particle-sensor interaction strength $\\theta$ required for perfect trajectory discrimination. Within an interval of $\\theta$ above this reduced threshold, any unentangled sensor requires $\\Theta(\\log(1/\\epsilon))$ repetitions of the protocol to estimate the particle trajectory with $\\epsilon$ error probability, whereas an entangled sensor can succeed with zero error in a single shot.","sentences":["Sensors for mapping the trajectory of an incoming particle find important utility in experimental high energy physics and searches for dark matter.","For a quantum sensing protocol that uses projective measurements on a multi-qubit sensor array to infer the trajectory of an incident particle, we show that entanglement can dramatically reduce the particle-sensor interaction strength $\\theta$ required for perfect trajectory discrimination.","Within an interval of $\\theta$ above this reduced threshold, any unentangled sensor requires $\\Theta(\\log(1/\\epsilon))$ repetitions of the protocol to estimate the particle trajectory with $\\epsilon$ error probability, whereas an entangled sensor can succeed with zero error in a single shot."],"url":"http://arxiv.org/abs/2405.05888v1","category":"quant-ph"}
{"created":"2024-05-09 14:12:41","title":"High-Performance Privacy-Preserving Matrix Completion for Trajectory Recovery","abstract":"Matrix completion has important applications in trajectory recovery and mobile social networks. However, sending raw data containing personal, sensitive information to cloud computing nodes may lead to privacy exposure issue.The privacy-preserving matrix completion is a useful approach to perform matrix completion while preserving privacy. In this paper, we propose a high-performance method for privacy-preserving matrix completion. First,we use a lightweight encryption scheme to encrypt the raw data and then perform matrix completion using alternating direction method of multipliers (ADMM). Then,the complemented matrix is decrypted and compared with the original matrix to calculate the error. This method has faster speed with higher accuracy. The results of numerical experiments reveal that the proposed method is faster than other algorithms.","sentences":["Matrix completion has important applications in trajectory recovery and mobile social networks.","However, sending raw data containing personal, sensitive information to cloud computing nodes may lead to privacy exposure issue.","The privacy-preserving matrix completion is a useful approach to perform matrix completion while preserving privacy.","In this paper, we propose a high-performance method for privacy-preserving matrix completion.","First,we use a lightweight encryption scheme to encrypt the raw data and then perform matrix completion using alternating direction method of multipliers (ADMM).","Then,the complemented matrix is decrypted and compared with the original matrix to calculate the error.","This method has faster speed with higher accuracy.","The results of numerical experiments reveal that the proposed method is faster than other algorithms."],"url":"http://arxiv.org/abs/2405.05789v1","category":"cs.CR"}
{"created":"2024-05-09 13:32:42","title":"Spin parameter optimization for spin-polarized extended tight-binding methods","abstract":"We present an optimization strategy for atom-specific spin-polarization constants within the spin-polarized GFN2-xTB framework, aiming to enhance the accuracy of molecular simulations. We compare a sequential and global optimization of spin parameters for hydrogen, carbon, nitrogen, oxygen, and fluorine. Sensitivity analysis using Sobol indices guides the identification of the most influential parameters for a given reference dataset, allowing for a nuanced understanding of their impact on diverse molecular properties. In the case of the W4-11 dataset, substantial error reduction was achieved, demonstrating the potential of the optimization. Transferability of the optimized spin-polarization constants over different properties, however, is limited, as we demonstrate by applying the optimized parameters on a set of singlet-triplet gaps in carbenes. Further studies on ionization potentials and electron affinities highlight some inherent limitations of current extended tight-binding methods that can not be resolved by simple parameter optimization. We conclude that the significantly improved accuracy strongly encourages the present re-optimization of the spin-polarization constants, whereas the limited transferability motivates a property-specific optimization strategy.","sentences":["We present an optimization strategy for atom-specific spin-polarization constants within the spin-polarized GFN2-xTB framework, aiming to enhance the accuracy of molecular simulations.","We compare a sequential and global optimization of spin parameters for hydrogen, carbon, nitrogen, oxygen, and fluorine.","Sensitivity analysis using Sobol indices guides the identification of the most influential parameters for a given reference dataset, allowing for a nuanced understanding of their impact on diverse molecular properties.","In the case of the W4-11 dataset, substantial error reduction was achieved, demonstrating the potential of the optimization.","Transferability of the optimized spin-polarization constants over different properties, however, is limited, as we demonstrate by applying the optimized parameters on a set of singlet-triplet gaps in carbenes.","Further studies on ionization potentials and electron affinities highlight some inherent limitations of current extended tight-binding methods that can not be resolved by simple parameter optimization.","We conclude that the significantly improved accuracy strongly encourages the present re-optimization of the spin-polarization constants, whereas the limited transferability motivates a property-specific optimization strategy."],"url":"http://arxiv.org/abs/2405.05761v1","category":"physics.chem-ph"}
{"created":"2024-05-09 12:37:22","title":"Minimum Time Escape from a Circular Region of a Dubins Car","abstract":"A turn-constrained evader strives to escape a circular region in minimum time.","sentences":["A turn-constrained evader strives to escape a circular region in minimum time."],"url":"http://arxiv.org/abs/2405.05725v1","category":"math.OC"}
{"created":"2024-05-09 11:45:33","title":"Development and optimization of large-scale integration of 2D material in memristors","abstract":"Two-dimensional (2D) materials like transition metal dichalcogenides (TMD) have proved to be serious candidates to replace silicon in several technologies with enhanced performances. In this respect, the two remaining challenges are the wafer scale growth of TMDs and their integration into operational devices using clean room compatible processes. In this work, two different CMOS-compatible protocols are developed for the fabrication of MoS$_2$-based memristors, and the resulting performances are compared. The quality of MoS$_2$ at each stage of the process is characterized by Raman spectroscopy and x-ray photoemission spectroscopy. In the first protocol, the structure of MoS$_2$ is preserved during transfer and patterning processes. However, a polymer layer with a minimum thickness of 3 nm remains at the surface of MoS$_2$ limiting the electrical switching performances. In the second protocol, the contamination layer is completely removed resulting in improved electrical switching performances and reproducibility. Based on physico-chemical and electrical results, the switching mechanism is discussed in terms of conduction through grain boundaries.","sentences":["Two-dimensional (2D) materials like transition metal dichalcogenides (TMD) have proved to be serious candidates to replace silicon in several technologies with enhanced performances.","In this respect, the two remaining challenges are the wafer scale growth of TMDs and their integration into operational devices using clean room compatible processes.","In this work, two different CMOS-compatible protocols are developed for the fabrication of MoS$_2$-based memristors, and the resulting performances are compared.","The quality of MoS$_2$ at each stage of the process is characterized by Raman spectroscopy and x-ray photoemission spectroscopy.","In the first protocol, the structure of MoS$_2$ is preserved during transfer and patterning processes.","However, a polymer layer with a minimum thickness of 3 nm remains at the surface of MoS$_2$ limiting the electrical switching performances.","In the second protocol, the contamination layer is completely removed resulting in improved electrical switching performances and reproducibility.","Based on physico-chemical and electrical results, the switching mechanism is discussed in terms of conduction through grain boundaries."],"url":"http://arxiv.org/abs/2405.05693v1","category":"physics.app-ph"}
{"created":"2024-05-09 10:06:30","title":"Consistent Empirical Bayes estimation of the mean of a mixing distribution without identifiability assumption. With applications to treatment of non-response","abstract":"{\\bf Abstract}   Consider a Non-Parametric Empirical Bayes (NPEB) setup. We observe $Y_i, \\sim f(y|\\theta_i)$, $\\theta_i \\in \\Theta$ independent, where $\\theta_i \\sim G$ are independent $i=1,...,n$. The mixing distribution $G$ is unknown $G \\in \\{G\\}$ with no parametric assumptions about the class $\\{G \\}$. The common NPEB task is to estimate $\\theta_i, \\; i=1,...,n$. Conditions that imply 'optimality' of such NPEB estimators typically require identifiability of $G$ based on $Y_1,...,Y_n$. We consider the task of estimating $E_G \\theta$. We show that `often' consistent estimation of $E_G \\theta$ is implied without identifiability.   We motivate the later task, especially in setups with non-response and missing data. We demonstrate consistency in simulations.","sentences":["{\\bf Abstract}   Consider a Non-Parametric Empirical Bayes (NPEB) setup.","We observe $Y_i, \\sim f(y|\\theta_i)$, $\\theta_i \\in \\Theta$ independent, where $\\theta_i \\sim G$ are independent $i=1,...,n$.","The mixing distribution $G$ is unknown $G \\in \\{G\\}$ with no parametric assumptions about the class $\\{G \\}$. The common NPEB task is to estimate $\\theta_i, \\; i=1,...,n$. Conditions that imply 'optimality' of such NPEB estimators typically require identifiability of $G$ based on $Y_1,...,Y_n$.","We consider the task of estimating $E_G \\theta$. We show that `often' consistent estimation of $E_G \\theta$ is implied without identifiability.   ","We motivate the later task, especially in setups with non-response and missing data.","We demonstrate consistency in simulations."],"url":"http://arxiv.org/abs/2405.05656v1","category":"math.ST"}
{"created":"2024-05-09 07:32:14","title":"Optimal quantitative weak approximation for projective quadrics","abstract":"We derive asymptotic formulas for the number of rational points on a smooth projective quadratic hypersurface of dimension at least three inside of a shrinking adelic open neighbourhood. This is a quantitative version of weak approximation for quadrics and allows us to deduce the best growth rate of the size of such an adelic neighbourhood for which equidistribution is preserved.","sentences":["We derive asymptotic formulas for the number of rational points on a smooth projective quadratic hypersurface of dimension at least three inside of a shrinking adelic open neighbourhood.","This is a quantitative version of weak approximation for quadrics and allows us to deduce the best growth rate of the size of such an adelic neighbourhood for which equidistribution is preserved."],"url":"http://arxiv.org/abs/2405.05592v1","category":"math.NT"}
{"created":"2024-05-09 06:31:09","title":"A Highly Efficient Hybrid Fiber Optic Laser Using a Cesium Atom Vapor Cell as an Optical Gain Medium","abstract":"A new scheme of a highly efficient hybrid laser cavity is proposed and experimentally demonstrated utilizing a hot cesium (Cs) vapor cell as an optical gain medium. The laser cavity consists of a macroscopic concave reflecting mirror (>99% reflectivity) and a 4% Fresnel-reflecting perpendicularly cleaved facet of a single mode fiber (SMF). The cylindrical cesium gain cell is located between these two reflectors. The SMF serves multiple roles: 1) a passive mode-matching component to approximate the pump beam diameter to that of the laser cavity mode within the cesium cell, 2) an output coupler with low reflectivity, and 3) a high beam-quality laser delivery with a low loss. Optimizing the pump beam waist diameter and the cesium vapor cell temperature, a high slope efficiency of 86% and continuous wave power of 419 mW were obtained in the pump power range of 400 to 600 mW, with an optical-to-optical conversion efficiency of 71%. The unique multi-functional role of the SMF in the hybrid cavity is fully described, and it can also be applied to other phases of high optical gain media.","sentences":["A new scheme of a highly efficient hybrid laser cavity is proposed and experimentally demonstrated utilizing a hot cesium (Cs) vapor cell as an optical gain medium.","The laser cavity consists of a macroscopic concave reflecting mirror (>99% reflectivity) and a 4% Fresnel-reflecting perpendicularly cleaved facet of a single mode fiber (SMF).","The cylindrical cesium gain cell is located between these two reflectors.","The SMF serves multiple roles: 1) a passive mode-matching component to approximate the pump beam diameter to that of the laser cavity mode within the cesium cell, 2) an output coupler with low reflectivity, and 3) a high beam-quality laser delivery with a low loss.","Optimizing the pump beam waist diameter and the cesium vapor cell temperature, a high slope efficiency of 86% and continuous wave power of 419 mW were obtained in the pump power range of 400 to 600 mW, with an optical-to-optical conversion efficiency of 71%.","The unique multi-functional role of the SMF in the hybrid cavity is fully described, and it can also be applied to other phases of high optical gain media."],"url":"http://arxiv.org/abs/2405.05569v1","category":"physics.optics"}
{"created":"2024-05-09 05:42:35","title":"Improved electrochemical performance of NASICON type Na$_{3}$V$_{2-x}$Co$_x$(PO$_{4}$)$_{3}$/C ($x=$ 0--0.15) cathode for high rate and stable sodium-ion batteries","abstract":"In recent years, the Na-ion SuperIonic CONductor (NASICON) based polyanionics are considered the pertinent cathode materials in sodium-ion batteries due to their 3D open framework, which can accommodate a wide range of Na content and can offer high ionic conductivity with great structural stability. However, owing to the inferior electronic conductivity, these materials suffer from unappealing rate capability and cyclic stability for practical applications. Therefore, in this work we investigate the effect of Co substitution at V site on the electrochemical performance and diffusion kinetics of Na$_{3}$V$_{2-x}$Co$_x$(PO$_{4}$)$_{3}$/C ($x=$ 0--0.15) cathodes. All the samples are characterized through Rietveld refinement of the x-ray diffraction patterns, Raman spectroscopy, transmission electron microscopy, etc. We demonstrate improved electrochemical performance for the $x=$ 0.05 electrode with reversible capacity of 105 mAh g$^{-1}$ at 0.1 C. Interestingly, the specific capacity of 80 mAh g$^{-1}$ is achieved at 10 C with retention of about 92\\% after 500 cycles and 79.5\\% after 1500 cycles and having nearly 100\\% Coulombic efficiency. The extracted diffusion coefficient values through galvanostatic intermittent titration technique and cyclic voltammetry are found to be in the range of 10$^{-9}$--10$^{-11}$ cm$^{2}$ s$^{-1}$. The postmortem studies show the excellent structural and morphological stability after testing for 500 cycles at 10 C. Our study reveals the role of optimal dopant of Co$^{3+}$ ions at V site to improve the cyclic stability at high current rate.","sentences":["In recent years, the Na-ion SuperIonic CONductor (NASICON) based polyanionics are considered the pertinent cathode materials in sodium-ion batteries due to their 3D open framework, which can accommodate a wide range of Na content and can offer high ionic conductivity with great structural stability.","However, owing to the inferior electronic conductivity, these materials suffer from unappealing rate capability and cyclic stability for practical applications.","Therefore, in this work we investigate the effect of Co substitution at V site on the electrochemical performance and diffusion kinetics of Na$_{3}$V$_{2-x}$Co$_x$(PO$_{4}$)$_{3}$/C ($x=$ 0--0.15) cathodes.","All the samples are characterized through Rietveld refinement of the x-ray diffraction patterns, Raman spectroscopy, transmission electron microscopy, etc.","We demonstrate improved electrochemical performance for the $x=$ 0.05 electrode with reversible capacity of 105 mAh g$^{-1}$ at 0.1 C. Interestingly, the specific capacity of 80 mAh g$^{-1}$ is achieved at 10 C with retention of about 92\\% after 500 cycles and 79.5\\% after 1500 cycles and having nearly 100\\% Coulombic efficiency.","The extracted diffusion coefficient values through galvanostatic intermittent titration technique and cyclic voltammetry are found to be in the range of 10$^{-9}$--10$^{-11}$ cm$^{2}$ s$^{-1}$.","The postmortem studies show the excellent structural and morphological stability after testing for 500 cycles at 10 C. Our study reveals the role of optimal dopant of Co$^{3+}$ ions at V site to improve the cyclic stability at high current rate."],"url":"http://arxiv.org/abs/2405.05559v1","category":"physics.chem-ph"}
{"created":"2024-05-09 05:24:08","title":"RELICS: a REactor neutrino LIquid xenon Coherent elastic Scattering experiment","abstract":"Coherent elastic neutrino-nucleus scattering (CEvNS) provides a unique probe for neutrino properties Beyond the Standard Model (BSM) physics. REactor neutrino LIquid xenon Coherent Scattering experiment (RELICS), a proposed reactor neutrino program using liquid xenon time projection chamber (LXeTPC) technology, aims to investigate the CEvNS process of antineutrinos off xenon atomic nuclei. In this work, the design of the experiment is studied and optimized based on Monte Carlo (MC) simulations. To achieve a sufficiently low energy threshold for CEvNS detection, an ionization-only analysis channel will be adopted for RELICS. A high emission rate of delayed electrons after a big ionization signal is the major background, leading to an analysis threshold of 120 photo-electrons in the CEvNS search. The second largest background, nuclear recoils induced by cosmic-ray neutrons, is suppressed via a passive water shield. The physics potential of RELICS was explored with a 32 kg-yr exposure at a baseline of 25 m from a reactor core with a 3 GW thermal power. In an energy range of 120 to 240 PEs, we the expected 13673.5 CEvNS and 2133.7 background events.The sensitivity of RELICS to the weak mixing angle was investigated at a low momentum transfer. Our study has shown that RELICS can further improve the constraints on the non-standard neutrino interaction (NSI) compared to the best results. RELICS set out to develop a powerful surface-level detection technology for low-energy neutrinos from reactors.","sentences":["Coherent elastic neutrino-nucleus scattering (CEvNS) provides a unique probe for neutrino properties Beyond the Standard Model (BSM) physics.","REactor neutrino LIquid xenon Coherent Scattering experiment (RELICS), a proposed reactor neutrino program using liquid xenon time projection chamber (LXeTPC) technology, aims to investigate the CEvNS process of antineutrinos off xenon atomic nuclei.","In this work, the design of the experiment is studied and optimized based on Monte Carlo (MC) simulations.","To achieve a sufficiently low energy threshold for CEvNS detection, an ionization-only analysis channel will be adopted for RELICS.","A high emission rate of delayed electrons after a big ionization signal is the major background, leading to an analysis threshold of 120 photo-electrons in the CEvNS search.","The second largest background, nuclear recoils induced by cosmic-ray neutrons, is suppressed via a passive water shield.","The physics potential of RELICS was explored with a 32 kg-yr exposure at a baseline of 25 m from a reactor core with a 3 GW thermal power.","In an energy range of 120 to 240 PEs, we the expected 13673.5 CEvNS and 2133.7 background events.","The sensitivity of RELICS to the weak mixing angle was investigated at a low momentum transfer.","Our study has shown that RELICS can further improve the constraints on the non-standard neutrino interaction (NSI) compared to the best results.","RELICS set out to develop a powerful surface-level detection technology for low-energy neutrinos from reactors."],"url":"http://arxiv.org/abs/2405.05554v1","category":"hep-ex"}
{"created":"2024-05-09 04:54:54","title":"Shape-Optimized Electrooptic Beam Scanners: Experiment","abstract":"A new horn-shaped electrooptic scanner is described with significantly improved scanning sensitivity over rectangular- shaped devices. In the new device, the shape of the scanner is chosen to follow the trajectory of the beam. An example design is described that exhibits a factor of two larger scanning sensitivity than a rectangular device with comparable maximum scanning angle. Beam propagation simulations and measurements on an experimental device verify the scanner performance.","sentences":["A new horn-shaped electrooptic scanner is described with significantly improved scanning sensitivity over rectangular- shaped devices.","In the new device, the shape of the scanner is chosen to follow the trajectory of the beam.","An example design is described that exhibits a factor of two larger scanning sensitivity than a rectangular device with comparable maximum scanning angle.","Beam propagation simulations and measurements on an experimental device verify the scanner performance."],"url":"http://arxiv.org/abs/2405.05540v1","category":"cs.NI"}
{"created":"2024-05-08 20:34:39","title":"Local minima in Newton's aerodynamical problem and inequalities between norms of partial derivatives","abstract":"The problem considered first by I. Newton (1687) consists in finding a surface of the minimal frontal resistance in a parallel flow of non-interacting point particles. The standard formulation assumes that the surface is convex with a given convex base $\\Omega$ and a bounded altitude. Newton found the solution for surfaces of revolution. Without this assumption the problem is still unsolved, although many important results have been obtained in the last decades. We consider the problem to characterize the domains $\\Omega$ for which the flat surface gives a local minimum. We show that this problem can be reduced to an inequality between $L_2$-norms of partial derivatives for bivariate concave functions on a convex domain that vanish on the boundary. Can the ratio between those norms be arbitrarily large? The answer depends on the geometry of the domain. A complete criterion is derived, which also solves the local minimality problem.","sentences":["The problem considered first by I. Newton (1687) consists in finding a surface of the minimal frontal resistance in a parallel flow of non-interacting point particles.","The standard formulation assumes that the surface is convex with a given convex base $\\Omega$ and a bounded altitude.","Newton found the solution for surfaces of revolution.","Without this assumption the problem is still unsolved, although many important results have been obtained in the last decades.","We consider the problem to characterize the domains $\\Omega$ for which the flat surface gives a local minimum.","We show that this problem can be reduced to an inequality between $L_2$-norms of partial derivatives for bivariate concave functions on a convex domain that vanish on the boundary.","Can the ratio between those norms be arbitrarily large?","The answer depends on the geometry of the domain.","A complete criterion is derived, which also solves the local minimality problem."],"url":"http://arxiv.org/abs/2405.05415v1","category":"math.OC"}
{"created":"2024-05-08 18:00:01","title":"Optimal Celestial Bodies for Dark Matter Detection","abstract":"A wide variety of celestial bodies have been considered as dark matter detectors. Which stands the best chance of delivering the discovery of dark matter? Which is the most powerful dark matter detector? We investigate a range of objects, including the Sun, Earth, Jupiter, Brown Dwarfs, White Dwarfs, Neutron Stars, Stellar populations, and Exoplanets. We quantify how different objects are optimal dark matter detectors in different regimes by deconstructing some of the in-built assumptions in these search sensitivities, including observation potential and particle model assumptions. We show how different objects can be expected to deliver corroborating signals. We discuss different search strategies, their opportunities and limitations, and the interplay of regimes where different celestial objects are optimal dark matter detectors.","sentences":["A wide variety of celestial bodies have been considered as dark matter detectors.","Which stands the best chance of delivering the discovery of dark matter?","Which is the most powerful dark matter detector?","We investigate a range of objects, including the Sun, Earth, Jupiter, Brown Dwarfs, White Dwarfs, Neutron Stars, Stellar populations, and Exoplanets.","We quantify how different objects are optimal dark matter detectors in different regimes by deconstructing some of the in-built assumptions in these search sensitivities, including observation potential and particle model assumptions.","We show how different objects can be expected to deliver corroborating signals.","We discuss different search strategies, their opportunities and limitations, and the interplay of regimes where different celestial objects are optimal dark matter detectors."],"url":"http://arxiv.org/abs/2405.05312v1","category":"hep-ph"}
{"created":"2024-05-08 03:40:36","title":"Learning Social Graph for Inactive User Recommendation","abstract":"Social relations have been widely incorporated into recommender systems to alleviate data sparsity problem. However, raw social relations don't always benefit recommendation due to their inferior quality and insufficient quantity, especially for inactive users, whose interacted items are limited. In this paper, we propose a novel social recommendation method called LSIR (\\textbf{L}earning \\textbf{S}ocial Graph for \\textbf{I}nactive User \\textbf{R}ecommendation) that learns an optimal social graph structure for social recommendation, especially for inactive users. LSIR recursively aggregates user and item embeddings to collaboratively encode item and user features. Then, graph structure learning (GSL) is employed to refine the raw user-user social graph, by removing noisy edges and adding new edges based on the enhanced embeddings. Meanwhile, mimic learning is implemented to guide active users in mimicking inactive users during model training, which improves the construction of new edges for inactive users. Extensive experiments on real-world datasets demonstrate that LSIR achieves significant improvements of up to 129.58\\% on NDCG in inactive user recommendation. Our code is available at~\\url{https://github.com/liun-online/LSIR}.","sentences":["Social relations have been widely incorporated into recommender systems to alleviate data sparsity problem.","However, raw social relations don't always benefit recommendation due to their inferior quality and insufficient quantity, especially for inactive users, whose interacted items are limited.","In this paper, we propose a novel social recommendation method called LSIR (\\textbf{L}earning \\textbf{S}ocial Graph for \\textbf{I}nactive User \\textbf{R}ecommendation) that learns an optimal social graph structure for social recommendation, especially for inactive users.","LSIR recursively aggregates user and item embeddings to collaboratively encode item and user features.","Then, graph structure learning (GSL) is employed to refine the raw user-user social graph, by removing noisy edges and adding new edges based on the enhanced embeddings.","Meanwhile, mimic learning is implemented to guide active users in mimicking inactive users during model training, which improves the construction of new edges for inactive users.","Extensive experiments on real-world datasets demonstrate that LSIR achieves significant improvements of up to 129.58\\% on NDCG in inactive user recommendation.","Our code is available at~\\url{https://github.com/liun-online/LSIR}."],"url":"http://arxiv.org/abs/2405.05288v1","category":"cs.SI"}
{"created":"2024-05-09 17:37:20","title":"CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts","abstract":"Recent advancements in Multimodal Large Language Models (LLMs) have focused primarily on scaling by increasing text-image pair data and enhancing LLMs to improve performance on multimodal tasks. However, these scaling approaches are computationally expensive and overlook the significance of improving model capabilities from the vision side. Inspired by the successful applications of Mixture-of-Experts (MoE) in LLMs, which improves model scalability during training while keeping inference costs similar to those of smaller models, we propose CuMo. CuMo incorporates Co-upcycled Top-K sparsely-gated Mixture-of-experts blocks into both the vision encoder and the MLP connector, thereby enhancing the multimodal LLMs with minimal additional activated parameters during inference. CuMo first pre-trains the MLP blocks and then initializes each expert in the MoE block from the pre-trained MLP block during the visual instruction tuning stage. Auxiliary losses are used to ensure a balanced loading of experts. CuMo outperforms state-of-the-art multimodal LLMs across various VQA and visual-instruction-following benchmarks using models within each model size group, all while training exclusively on open-sourced datasets. The code and model weights for CuMo are open-sourced at https://github.com/SHI-Labs/CuMo.","sentences":["Recent advancements in Multimodal Large Language Models (LLMs) have focused primarily on scaling by increasing text-image pair data and enhancing LLMs to improve performance on multimodal tasks.","However, these scaling approaches are computationally expensive and overlook the significance of improving model capabilities from the vision side.","Inspired by the successful applications of Mixture-of-Experts (MoE) in LLMs, which improves model scalability during training while keeping inference costs similar to those of smaller models, we propose CuMo.","CuMo incorporates Co-upcycled Top-K sparsely-gated Mixture-of-experts blocks into both the vision encoder and the MLP connector, thereby enhancing the multimodal LLMs with minimal additional activated parameters during inference.","CuMo first pre-trains the MLP blocks and then initializes each expert in the MoE block from the pre-trained MLP block during the visual instruction tuning stage.","Auxiliary losses are used to ensure a balanced loading of experts.","CuMo outperforms state-of-the-art multimodal LLMs across various VQA and visual-instruction-following benchmarks using models within each model size group, all while training exclusively on open-sourced datasets.","The code and model weights for CuMo are open-sourced at https://github.com/SHI-Labs/CuMo."],"url":"http://arxiv.org/abs/2405.05949v1","category":"cs.CV"}
{"created":"2024-05-09 17:13:41","title":"In-medium changes of nucleon cross sections tested in neutrino-induced reactions","abstract":"Historically studied in the context of heavy-ion collisions, the extent to which free nucleon-nucleon forces are modified in-medium remains undetermined by these data sets. Therefore, we investigate the impact of NN in-medium modifications on neutrino-nucleus cross section predictions using the GiBUU transport model. We find that including an in-medium lowering of the NN cross section and density dependence on $\\Delta$ excitation improves agreement with MicroBooNE neutrino-argon scattering data. This is observed for both proton and neutral pion spectra in charged-current muon neutrino and neutral-current single pion production datasets. The impact of collision broadening of the $\\Delta$ resonance is also investigated.","sentences":["Historically studied in the context of heavy-ion collisions, the extent to which free nucleon-nucleon forces are modified in-medium remains undetermined by these data sets.","Therefore, we investigate the impact of NN in-medium modifications on neutrino-nucleus cross section predictions using the GiBUU transport model.","We find that including an in-medium lowering of the NN cross section and density dependence on $\\Delta$ excitation improves agreement with MicroBooNE neutrino-argon scattering data.","This is observed for both proton and neutral pion spectra in charged-current muon neutrino and neutral-current single pion production datasets.","The impact of collision broadening of the $\\Delta$ resonance is also investigated."],"url":"http://arxiv.org/abs/2405.05921v1","category":"hep-ex"}
{"created":"2024-05-09 15:45:06","title":"Non-perturbative determination of the $N_f=2+1$ QCD sphaleron rate","abstract":"The strong sphaleron rate, i.e., the rate of real time QCD topological transitions, is a key phenomenological quantity, playing a fundamental role in several physical contexts. In heavy-ion collisions, a non-vanishing rate can lead to the so-called Chiral Magnetic Effect. In early-Universe cosmology, instead, it can be related to the rate of thermal production of QCD axions. In this talk, we present the first reliable fully non-perturbative computation of the strong sphaleron rate in $N_f=2+1$ QCD at the physical point by means of lattice simulations, in a range of temperatures going from 200 MeV to 600 MeV. Our strategy is based on the inversion of lattice correlators via a recently-proposed modified version of the Backus-Gilbert method.","sentences":["The strong sphaleron rate, i.e., the rate of real time QCD topological transitions, is a key phenomenological quantity, playing a fundamental role in several physical contexts.","In heavy-ion collisions, a non-vanishing rate can lead to the so-called Chiral Magnetic Effect.","In early-Universe cosmology, instead, it can be related to the rate of thermal production of QCD axions.","In this talk, we present the first reliable fully non-perturbative computation of the strong sphaleron rate in $N_f=2+1$ QCD at the physical point by means of lattice simulations, in a range of temperatures going from 200 MeV to 600 MeV.","Our strategy is based on the inversion of lattice correlators via a recently-proposed modified version of the Backus-Gilbert method."],"url":"http://arxiv.org/abs/2405.05857v1","category":"hep-lat"}
{"created":"2024-05-09 15:34:56","title":"Extreme high lattice-misfit superalloys with regular cubic L12 particles and excellent creep resistance","abstract":"In novel Co and CoNi based superalloys, the creep resistance is limited at high temperatures due to low lattice misfit and solvus temperature. In this study, we combined the advantages of Co-Ti (high lattice misfit and solvus temperature) and Co-Al based superalloys (cuboidal precipitates) by using Ti to substitute Al in alloys of Co-30Ni-(12.5-x)Al-xTi-2.5Mo-2.5W (at.%) composition. With high Ti content, the alloys obtained extreme high lattice misfit (bigger than 1.3 %) and solvus temperature (bigger than 1150 degree). During aging at 900 degree, alloys with high Ti/Al ratio exhibited a lower gamma prime precipitate coarsening rate resulting from their lower gamma prime and gamma interfacial energy and higher lattice misfit. In addition, high Ti/Al ratio brought higher gamma prime volume fraction and excellent mechanical properties, such as higher yield stress and better creep resistance. However, at high temperature of 1100 degree, the cubic gamma prime phase was decomposed into deleterious Eta phase with D024 structure if the Ti/Al ratio exceeded 1. Based on this, we outreached new alloys design with a high content of Cr and Ta and appropriate Ti/Al ratio, i.e., Ti/Al ratio is smaller than 1. The newly designed alloys still have high solvus temperature (bigger than 1200 degree) and exhibit high lattice misfit (bigger than 1.2 %) as Co-12Ti (at.%) superalloys but more regular cubic gamma prime precipitates and significantly better creep resistance than superalloys Co-9Al-9W and Co-9Al-9W-2Ti at 850 and 950 degree. Nevertheless, compared with creep resistance of Ni based superalloys, our newly designed alloys still need to be further improved, especially in the 1000 and 1050 degree range.","sentences":["In novel Co and CoNi based superalloys, the creep resistance is limited at high temperatures due to low lattice misfit and solvus temperature.","In this study, we combined the advantages of Co-Ti (high lattice misfit and solvus temperature) and Co-Al based superalloys (cuboidal precipitates) by using Ti to substitute Al in alloys of Co-30Ni-(12.5-x)Al-xTi-2.5Mo-2.5W (at.%) composition.","With high Ti content, the alloys obtained extreme high lattice misfit (bigger than 1.3 %) and solvus temperature (bigger than 1150 degree).","During aging at 900 degree, alloys with high Ti/Al ratio exhibited a lower gamma prime precipitate coarsening rate resulting from their lower gamma prime and gamma interfacial energy and higher lattice misfit.","In addition, high Ti/Al ratio brought higher gamma prime volume fraction and excellent mechanical properties, such as higher yield stress and better creep resistance.","However, at high temperature of 1100 degree, the cubic gamma prime phase was decomposed into deleterious Eta phase with D024 structure if the Ti/Al ratio exceeded 1.","Based on this, we outreached new alloys design with a high content of Cr and Ta and appropriate Ti/Al ratio, i.e., Ti/Al ratio is smaller than 1.","The newly designed alloys still have high solvus temperature (bigger than 1200 degree) and exhibit high lattice misfit (bigger than 1.2 %) as Co-12Ti (at.%) superalloys but more regular cubic gamma prime precipitates and significantly better creep resistance than superalloys Co-9Al-9W and Co-9Al-9W-2Ti at 850 and 950 degree.","Nevertheless, compared with creep resistance of Ni based superalloys, our newly designed alloys still need to be further improved, especially in the 1000 and 1050 degree range."],"url":"http://arxiv.org/abs/2405.05851v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-09 15:34:23","title":"Dissociative recombination and vibrational excitation of BF$^{+}$ in low energy electron collisions","abstract":"The latest molecular data - potential energy curves and Rydberg-valence interactions - characterising the super-excited electronic states of BF are reviewed in order to provide the input for the study of their fragmentation dynamics. Starting from this input, the main paths and mechanisms of BF$^+$ dissociative recombination and vibrational excitation are analysed. Their cross sections are computed for the first time using a method based on the multichannel quantum defect theory (MQDT), and Maxwellian rate-coefficients are calculated and displayed in ready-to-be-used format for low temperature plasma kinetics simulations.","sentences":["The latest molecular data - potential energy curves and Rydberg-valence interactions - characterising the super-excited electronic states of BF are reviewed in order to provide the input for the study of their fragmentation dynamics.","Starting from this input, the main paths and mechanisms of BF$^+$ dissociative recombination and vibrational excitation are analysed.","Their cross sections are computed for the first time using a method based on the multichannel quantum defect theory (MQDT), and Maxwellian rate-coefficients are calculated and displayed in ready-to-be-used format for low temperature plasma kinetics simulations."],"url":"http://arxiv.org/abs/2405.05850v1","category":"physics.plasm-ph"}
{"created":"2024-05-09 15:27:05","title":"Alleviating $H_0$ and $S_8$ Tensions Simultaneously in K-essence Cosmology","abstract":"The present work begins by examining the early-Universe inflationary epoch of a special K-essence model, which incorporates a linear coupling term between the scalar field potential and the canonical Lagrangian. For the power law potential, we both numerically and analytically prove that the inflationary parameters such as the spectral index and tensor-to-scalar ratio are compatible with the recent BICEP/Keck observations. Continuing this work, our analysis based on comparing early-Universe observations with late-Universe measurements indicates that the tension on the Hubble parameter $H_0$ and the growth of structure parameter $S_8$ can be alleviated simultaneously. More precisely, compared to the standard $\\Lambda$CDM model, our model can reduce $H_0$ tension to roughly $2.2 \\sigma$ and the $S_8$ discrepancy diminishes to $0.82\\sigma$.","sentences":["The present work begins by examining the early-Universe inflationary epoch of a special K-essence model, which incorporates a linear coupling term between the scalar field potential and the canonical Lagrangian.","For the power law potential, we both numerically and analytically prove that the inflationary parameters such as the spectral index and tensor-to-scalar ratio are compatible with the recent BICEP/Keck observations.","Continuing this work, our analysis based on comparing early-Universe observations with late-Universe measurements indicates that the tension on the Hubble parameter $H_0$ and the growth of structure parameter $S_8$ can be alleviated simultaneously.","More precisely, compared to the standard $\\Lambda$CDM model, our model can reduce $H_0$ tension to roughly $2.2 \\sigma$ and the $S_8$ discrepancy diminishes to $0.82\\sigma$."],"url":"http://arxiv.org/abs/2405.05843v1","category":"astro-ph.CO"}
{"created":"2024-05-09 14:56:54","title":"X-ray signatures of galactic outflows into the circumgalactic medium","abstract":"We present a set of controlled hydrodynamical simulations to study the effects of strong galactic outflows on the density and temperature structures, and associated X-ray signatures, of extra-planar and circumgalactic gas. We consider three initial state models, isothermal, isentropic, and rotating cooling-flow, for the hot circumgalactic medium (CGM) into which the outflows are driven. The energy sources are either stellar winds and supernovae, or active galactic nuclei. We consider energy injection rates in the range $10^{40} < \\dot{E}_{\\rm inj} <10^{44.5}$ erg s$^{-1}$, and compute the time-dependent soft X-ray (0.5-2 keV) surface brightness. For $\\dot{E}_{\\rm inj} \\gtrsim 10^{41} - 10^{42}$ erg s$^{-1}$, with the exact threshold depending on the initial CGM state, the X-ray response is dominated by dense hot gas in the forward shock that eventually fades into the CGM as a sound wave. The shock surrounds an inner hot bubble leading to a radial flattening of the X-ray surface brightness. For lower energy injection rates, the X-ray surface brightness of the initial CGM state is almost unaffected. We present analytic approximations for the outflow shock propagation and the associated X-ray emissions.","sentences":["We present a set of controlled hydrodynamical simulations to study the effects of strong galactic outflows on the density and temperature structures, and associated X-ray signatures, of extra-planar and circumgalactic gas.","We consider three initial state models, isothermal, isentropic, and rotating cooling-flow, for the hot circumgalactic medium (CGM) into which the outflows are driven.","The energy sources are either stellar winds and supernovae, or active galactic nuclei.","We consider energy injection rates in the range $10^{40} < \\dot{E}_{\\rm inj} <10^{44.5}$ erg s$^{-1}$, and compute the time-dependent soft X-ray (0.5-2 keV) surface brightness.","For $\\dot{E}_{\\rm inj} \\gtrsim 10^{41} - 10^{42}$ erg s$^{-1}$, with the exact threshold depending on the initial CGM state, the X-ray response is dominated by dense hot gas in the forward shock that eventually fades into the CGM as a sound wave.","The shock surrounds an inner hot bubble leading to a radial flattening of the X-ray surface brightness.","For lower energy injection rates, the X-ray surface brightness of the initial CGM state is almost unaffected.","We present analytic approximations for the outflow shock propagation and the associated X-ray emissions."],"url":"http://arxiv.org/abs/2405.05819v1","category":"astro-ph.GA"}
{"created":"2024-05-09 14:31:28","title":"On the fine structure of the solutions to nonlinear thin two-membrane problems in 2D","abstract":"We prove a structure theorem for the solutions of nonlinear thin two-membrane problems in dimension two. Using the theory of quasi-conformal maps, we show that the difference of the sheets is topologically equivalent to a solution of the linear thin obstacle problem, thus inheriting its free boundary structure. More precisely, we show that even in the nonlinear case the branching points can only occur in finite number. We apply our methods to one-phase free boundaries approaching a fixed analytic boundary and to the solutions of a one-sided two-phase Bernoulli problem.","sentences":["We prove a structure theorem for the solutions of nonlinear thin two-membrane problems in dimension two.","Using the theory of quasi-conformal maps, we show that the difference of the sheets is topologically equivalent to a solution of the linear thin obstacle problem, thus inheriting its free boundary structure.","More precisely, we show that even in the nonlinear case the branching points can only occur in finite number.","We apply our methods to one-phase free boundaries approaching a fixed analytic boundary and to the solutions of a one-sided two-phase Bernoulli problem."],"url":"http://arxiv.org/abs/2405.05799v1","category":"math.AP"}
{"created":"2024-05-09 14:30:49","title":"A PAge-like Unified Dark Fluid Model","abstract":"The unified dark fluid model unifies dark matter and dark energy into a single component, providing an alternative and more concise framework for interpreting cosmological observations. We introduce a PAge-like Unified Dark Fluid (PUDF) model based on the PAge approximation (Huang 2020), which is parameterized by the age of the universe and an $\\eta$ parameter indicating the deviation from Einstein-De Sitter Universe. The PUDF model shares many similar features of the standard Lambda cold dark matter ($\\Lambda$CDM) model and can effectively describe the large-scale structure formation and late-time cosmic acceleration. We constrain the PUDF model with the Planck 2018 cosmic microwave background anisotropies, baryon acoustic oscillation measurements including those from the most recent DESI 2024, the Pantheon+ sample of Type Ia supernovae, and the Cosmic Chronometers compilation. Although the PUDF performs well in fitting all the cosmological datasets, the joint analysis of the data still favors the $\\Lambda$CDM model over the PUDF model, according to the Bayesian evidence of model comparison.","sentences":["The unified dark fluid model unifies dark matter and dark energy into a single component, providing an alternative and more concise framework for interpreting cosmological observations.","We introduce a PAge-like Unified Dark Fluid (PUDF) model based on the PAge approximation (Huang 2020), which is parameterized by the age of the universe and an $\\eta$ parameter indicating the deviation from Einstein-De Sitter Universe.","The PUDF model shares many similar features of the standard Lambda cold dark matter ($\\Lambda$CDM) model and can effectively describe the large-scale structure formation and late-time cosmic acceleration.","We constrain the PUDF model with the Planck 2018 cosmic microwave background anisotropies, baryon acoustic oscillation measurements including those from the most recent DESI 2024, the Pantheon+ sample of Type Ia supernovae, and the Cosmic Chronometers compilation.","Although the PUDF performs well in fitting all the cosmological datasets, the joint analysis of the data still favors the $\\Lambda$CDM model over the PUDF model, according to the Bayesian evidence of model comparison."],"url":"http://arxiv.org/abs/2405.05798v1","category":"astro-ph.CO"}
{"created":"2024-05-09 13:54:15","title":"Experimental Pragmatics with Machines: Testing LLM Predictions for the Inferences of Plain and Embedded Disjunctions","abstract":"Human communication is based on a variety of inferences that we draw from sentences, often going beyond what is literally said. While there is wide agreement on the basic distinction between entailment, implicature, and presupposition, the status of many inferences remains controversial. In this paper, we focus on three inferences of plain and embedded disjunctions, and compare them with regular scalar implicatures. We investigate this comparison from the novel perspective of the predictions of state-of-the-art large language models, using the same experimental paradigms as recent studies investigating the same inferences with humans. The results of our best performing models mostly align with those of humans, both in the large differences we find between those inferences and implicatures, as well as in fine-grained distinctions among different aspects of those inferences.","sentences":["Human communication is based on a variety of inferences that we draw from sentences, often going beyond what is literally said.","While there is wide agreement on the basic distinction between entailment, implicature, and presupposition, the status of many inferences remains controversial.","In this paper, we focus on three inferences of plain and embedded disjunctions, and compare them with regular scalar implicatures.","We investigate this comparison from the novel perspective of the predictions of state-of-the-art large language models, using the same experimental paradigms as recent studies investigating the same inferences with humans.","The results of our best performing models mostly align with those of humans, both in the large differences we find between those inferences and implicatures, as well as in fine-grained distinctions among different aspects of those inferences."],"url":"http://arxiv.org/abs/2405.05776v1","category":"cs.CL"}
{"created":"2024-05-09 13:12:47","title":"3D bulk field theories for 2D non-unitary N=1 supersymmetric minimal models","abstract":"We propose bulk 3D N=4 rank-0 superconformal field theories, which are related to 2D N=1 supersymmetric minimal models, SM(2, ...) and SM(3, ...), via recently discovered non-unitary bulk-boundary correspondence. The correspondence relates a 3D N=4 rank-0 superconformal field theory to 2D chiral rational conformal field theories. A topologically twisted theory of the rank-0 SCFT supports the rational chiral algebra at the boundary upon a proper choice of boundary condition. We test the proposal by checking several non-trivial dictionaries of the correspondence.","sentences":["We propose bulk 3D N=4 rank-0 superconformal field theories, which are related to 2D N=1 supersymmetric minimal models, SM(2, ...) and SM(3, ...), via recently discovered non-unitary bulk-boundary correspondence.","The correspondence relates a 3D N=4 rank-0 superconformal field theory to 2D chiral rational conformal field theories.","A topologically twisted theory of the rank-0 SCFT supports the rational chiral algebra at the boundary upon a proper choice of boundary condition.","We test the proposal by checking several non-trivial dictionaries of the correspondence."],"url":"http://arxiv.org/abs/2405.05746v1","category":"hep-th"}
{"created":"2024-05-09 13:06:23","title":"Development, Characterization and Production of a novel Water-based Liquid Scintillator based on the Surfactant TRITON X-100","abstract":"Water-based Liquid Scintillator (WbLS) is a novel detector medium for particle physics experiments. Applications range from the use as hybrid Cherenkov/scintillation target in low-energy and accelerator neutrino experiments to large-volume neutron vetoes for dark matter detectors. Here, we present a novel WbLS featuring new components (the surfactant Triton-X and vitamin C for long-term stability), a new production recipe, and a thorough characterization of its properties. Moreover, based on neutron scattering data we are able to demonstrate that the pulse shape discrimination capabilities of this particular LS are comparable to fully-organic LAB based scintillators.","sentences":["Water-based Liquid Scintillator (WbLS) is a novel detector medium for particle physics experiments.","Applications range from the use as hybrid Cherenkov/scintillation target in low-energy and accelerator neutrino experiments to large-volume neutron vetoes for dark matter detectors.","Here, we present a novel WbLS featuring new components (the surfactant Triton-X and vitamin C for long-term stability), a new production recipe, and a thorough characterization of its properties.","Moreover, based on neutron scattering data we are able to demonstrate that the pulse shape discrimination capabilities of this particular LS are comparable to fully-organic LAB based scintillators."],"url":"http://arxiv.org/abs/2405.05743v1","category":"hep-ex"}
{"created":"2024-05-09 12:42:17","title":"Incoherent Fermionic Dark Matter Absorption with Nucleon Fermi Motion","abstract":"We investigate the incoherent regime of the fermionic dark matter absorption by nuclei using the relativistic Fermi gas model and nuclear form factors. With the momentum transfer being roughly equal to the dark matter mass $m_\\chi$, the incoherent regime contributes significantly to the absorption process for $m_\\chi \\gtrsim 100$ MeV with a spin-independent operator and for even smaller mass with a spin-dependent one. We also compare the situations for various target nuclei ($^{131}$Xe, $^{72}$Ge, $^{40}$Ar, $^{20}$Ne and $^4$He) that are typically used in the dark matter direct detection. A heavier nucleus actually has the advantage of probing the incoherent scattering of the fermionic absorption dark matter. Observing both the coherent and incoherent contributions would be an important justification of the fermionic dark matter absorption.","sentences":["We investigate the incoherent regime of the fermionic dark matter absorption by nuclei using the relativistic Fermi gas model and nuclear form factors.","With the momentum transfer being roughly equal to the dark matter mass $m_\\chi$, the incoherent regime contributes significantly to the absorption process for $m_\\chi \\gtrsim 100$ MeV with a spin-independent operator and for even smaller mass with a spin-dependent one.","We also compare the situations for various target nuclei ($^{131}$Xe, $^{72}$Ge, $^{40}$Ar, $^{20}$Ne and $^4$He) that are typically used in the dark matter direct detection.","A heavier nucleus actually has the advantage of probing the incoherent scattering of the fermionic absorption dark matter.","Observing both the coherent and incoherent contributions would be an important justification of the fermionic dark matter absorption."],"url":"http://arxiv.org/abs/2405.05728v1","category":"hep-ph"}
{"created":"2024-05-09 12:33:40","title":"Prolegomena to the Bestiary","abstract":"``Calabi-Yau Manifolds: a Bestiary for Physicists'' by Tristan Hubsch in 1992 was a classic that served to introduce algebraic geometry to physicists when the first string theory revolution of 1984 - 94 brought, inter alia, the subject of Calabi-Yau manifolds to the staple of high-energy theorists. We are fortunate that a substantially expanded and updated new edition of the Bestiary will shortly appear. This brief note will serve as an afterword to the much anticipated volume.","sentences":["``Calabi-Yau Manifolds: a Bestiary for Physicists'' by Tristan Hubsch in 1992 was a classic that served to introduce algebraic geometry to physicists when the first string theory revolution of 1984 - 94 brought, inter alia, the subject of Calabi-Yau manifolds to the staple of high-energy theorists.","We are fortunate that a substantially expanded and updated new edition of the Bestiary will shortly appear.","This brief note will serve as an afterword to the much anticipated volume."],"url":"http://arxiv.org/abs/2405.05720v1","category":"math.HO"}
{"created":"2024-05-09 11:55:01","title":"Parallelizing Air Shower Simulation for Background Characterization in IceCube","abstract":"The IceCube Neutrino Observatory is a cubic kilometer neutrino telescope located at the Geographic South Pole. For every observed neutrino event, there are over $10^6$ background events caused by cosmic ray air shower muons. In order to properly separate signal from background, it is necessary to produce Monte Carlo simulations of these air showers. Although to-date, IceCube has produced large quantities of background simulation, these studies still remain statistics limited. The first stage of simulation requires heavy CPU usage while the second stage requires heavy GPU usage. Processing both of these stages on the same node will result in an underutilized GPU but using different nodes will encounter bandwidth bottlenecks. Furthermore, due to the power-law energy spectrum of cosmic rays, the memory footprint of the detector response often exceeded the limit in unpredictable ways. This proceeding presents new client-server code which parallelizes the first stage onto multiple CPUs on the same node and then passes it on to the GPU for photon propagation. This results in GPU utilization of greater than 90% as well as more predictable memory usage and an overall factor of 20 improvement in speed over previous techniques.","sentences":["The IceCube Neutrino Observatory is a cubic kilometer neutrino telescope located at the Geographic South Pole.","For every observed neutrino event, there are over $10^6$ background events caused by cosmic ray air shower muons.","In order to properly separate signal from background, it is necessary to produce Monte Carlo simulations of these air showers.","Although to-date, IceCube has produced large quantities of background simulation, these studies still remain statistics limited.","The first stage of simulation requires heavy CPU usage while the second stage requires heavy GPU usage.","Processing both of these stages on the same node will result in an underutilized GPU but using different nodes will encounter bandwidth bottlenecks.","Furthermore, due to the power-law energy spectrum of cosmic rays, the memory footprint of the detector response often exceeded the limit in unpredictable ways.","This proceeding presents new client-server code which parallelizes the first stage onto multiple CPUs on the same node and then passes it on to the GPU for photon propagation.","This results in GPU utilization of greater than 90% as well as more predictable memory usage and an overall factor of 20 improvement in speed over previous techniques."],"url":"http://arxiv.org/abs/2405.05700v1","category":"astro-ph.HE"}
{"created":"2024-05-09 11:54:35","title":"MCMC inversions of the internal rotation of Kepler subgiants","abstract":"The measurement of the internal rotation of post-main sequence stars using data from space-based photometry missions has demonstrated the need for an efficient angular momentum transport in stellar interiors. So far, no clear solution has emerged and explaining the observed trends remain a challenge for stellar modellers. We aim at constraining both the shape of the internal rotation profile of six Kepler subgiants studied in details in 2014 and the properties of the missing angular momentum transport process acting in stellar interiors from MCMC inversions of the internal rotation. We apply a new MCMC inversion technique to existing Kepler subgiant targets and test various shapes of the internal rotation profile of all six original subgiants observed in 2014. We also constrain the limitations on the number of free parameters that can be used in the MCMC inversion, showing the limitations in the amount of information in the seismic data. First, we show that large-scale fossil magnetic fields are not able to explain the internal rotation of subgiants, similarly to what was determined from detailed studies of Kepler red giants. We are also able to constrain the location of the transition in the internal rotation profile for the most evolved stars in the available set of subgiants. We find that some of them exhibit a transition located close to the border of the helium core while one clearly does not. We conclude that it might be possible that various processes might be at play to explain our observations, but that revealing the physical nature of the angular momentum process will require a consistent detailed modelling of all subgiants available, particularly the least evolved. In addition, increasing the number of stars for which such inferences are possible (e.g. with the future PLATO mission) is paramount given the key role they play in validating transport process candidates.","sentences":["The measurement of the internal rotation of post-main sequence stars using data from space-based photometry missions has demonstrated the need for an efficient angular momentum transport in stellar interiors.","So far, no clear solution has emerged and explaining the observed trends remain a challenge for stellar modellers.","We aim at constraining both the shape of the internal rotation profile of six Kepler subgiants studied in details in 2014 and the properties of the missing angular momentum transport process acting in stellar interiors from MCMC inversions of the internal rotation.","We apply a new MCMC inversion technique to existing Kepler subgiant targets and test various shapes of the internal rotation profile of all six original subgiants observed in 2014.","We also constrain the limitations on the number of free parameters that can be used in the MCMC inversion, showing the limitations in the amount of information in the seismic data.","First, we show that large-scale fossil magnetic fields are not able to explain the internal rotation of subgiants, similarly to what was determined from detailed studies of Kepler red giants.","We are also able to constrain the location of the transition in the internal rotation profile for the most evolved stars in the available set of subgiants.","We find that some of them exhibit a transition located close to the border of the helium core while one clearly does not.","We conclude that it might be possible that various processes might be at play to explain our observations, but that revealing the physical nature of the angular momentum process will require a consistent detailed modelling of all subgiants available, particularly the least evolved.","In addition, increasing the number of stars for which such inferences are possible (e.g. with the future PLATO mission) is paramount given the key role they play in validating transport process candidates."],"url":"http://arxiv.org/abs/2405.05699v1","category":"astro-ph.SR"}
{"created":"2024-05-09 09:16:42","title":"High-Frequency Stock Market Order Transitions during the US-China Trade War 2018: A Discrete-Time Markov Chain Analysis","abstract":"Statistical analysis of high-frequency stock market order transaction data is conducted to understand order transition dynamics. We employ a first-order time-homogeneous discrete-time Markov chain model to the sequence of orders of stocks belonging to six different sectors during the USA-China trade war of 2018. The Markov property of the order sequence is validated by the Chi-square test. We estimate the transition probability matrix of the sequence using maximum likelihood estimation. From the heat-map of these matrices, we found the presence of active participation by different types of traders during high volatility days. On such days, these traders place limit orders primarily with the intention of deleting the majority of them to influence the market. These findings are supported by high stationary distribution and low mean recurrence values of add and delete orders. Further, we found similar spectral gap and entropy rate values, which indicates that similar trading strategies are employed on both high and low volatility days during the trade war. Among all the sectors considered in this study, we observe that there is a recurring pattern of full execution orders in Finance & Banking sector. This shows that the banking stocks are resilient during the trade war. Hence, this study may be useful in understanding stock market order dynamics and devise trading strategies accordingly on high and low volatility days during extreme macroeconomic events.","sentences":["Statistical analysis of high-frequency stock market order transaction data is conducted to understand order transition dynamics.","We employ a first-order time-homogeneous discrete-time Markov chain model to the sequence of orders of stocks belonging to six different sectors during the USA-China trade war of 2018.","The Markov property of the order sequence is validated by the Chi-square test.","We estimate the transition probability matrix of the sequence using maximum likelihood estimation.","From the heat-map of these matrices, we found the presence of active participation by different types of traders during high volatility days.","On such days, these traders place limit orders primarily with the intention of deleting the majority of them to influence the market.","These findings are supported by high stationary distribution and low mean recurrence values of add and delete orders.","Further, we found similar spectral gap and entropy rate values, which indicates that similar trading strategies are employed on both high and low volatility days during the trade war.","Among all the sectors considered in this study, we observe that there is a recurring pattern of full execution orders in Finance & Banking sector.","This shows that the banking stocks are resilient during the trade war.","Hence, this study may be useful in understanding stock market order dynamics and devise trading strategies accordingly on high and low volatility days during extreme macroeconomic events."],"url":"http://arxiv.org/abs/2405.05634v1","category":"q-fin.ST"}
{"created":"2024-05-09 08:31:00","title":"Semiclassical solution of black hole information paradox","abstract":"We resolve black hole information paradox within semiclassical gravity, in a manner that does not depend on details of unknown quantum gravity. Our crucial insight is that outgoing Hawking particles are physical only at distances much larger than the Schwarzschild radius, so they are created far from the horizon and entangled with degrees of freedom at smaller distances. The later degrees of freedom can be understood as quasi-classical coherent states, implying that Hawking radiation is accompanied by additional radiation similar to classical radiation by which the black hole loses hair during the classical gravitational collapse. The two kinds of radiation are entangled, which resolves black hole information paradox.","sentences":["We resolve black hole information paradox within semiclassical gravity, in a manner that does not depend on details of unknown quantum gravity.","Our crucial insight is that outgoing Hawking particles are physical only at distances much larger than the Schwarzschild radius, so they are created far from the horizon and entangled with degrees of freedom at smaller distances.","The later degrees of freedom can be understood as quasi-classical coherent states, implying that Hawking radiation is accompanied by additional radiation similar to classical radiation by which the black hole loses hair during the classical gravitational collapse.","The two kinds of radiation are entangled, which resolves black hole information paradox."],"url":"http://arxiv.org/abs/2405.05617v1","category":"hep-th"}
{"created":"2024-05-09 07:23:06","title":"Modelling the galaxy radio continuum from star formation and active galactic nuclei in the Shark semi-analytic model","abstract":"We present a model of radio continuum emission associated with star formation (SF) and active galactic nuclei (AGN) implemented in the Shark semi-analytic model of galaxy formation. SF emission includes free-free and synchrotron emission, which depend on the free-electron density and the rate of core-collapse supernovae with a minor contribution from supernova remnants, respectively. AGN emission is modelled based on the jet production rate, which depends on the black hole mass, accretion rate and spin, and includes synchrotron self-absorption. Shark reproduces radio luminosity functions (RLFs) at 1.4 GHz and 150 MHz for 0 $\\leq$ z $\\leq$ 4, and scaling relations between radio luminosity, star formation rate and infrared luminosity of galaxies in the local and distant universe in good agreement with observations. The model also reproduces observed number counts of radio sources from 150 MHz to 8.4 GHz to within a factor of two on average, though larger discrepancies are seen at the very bright fluxes at higher frequencies. We use this model to understand how the radio continuum emission from radio-quiet AGNs can affect the measured RLFs of galaxies. We find current methods to exclude AGNs from observational samples result in large fractions of radio-quiet AGNs contaminating the \"star-forming galaxies\" selection and a brighter end to the resulting RLFs. We investigate how this effects the infrared-radio correlation (IRRC) and show that AGN contamination can lead to evolution of the IRRC with redshift. Without this contamination our model predicts a redshift- and stellar mass-independent IRRC, except at the dwarf-galaxy regime.","sentences":["We present a model of radio continuum emission associated with star formation (SF) and active galactic nuclei (AGN) implemented in the Shark semi-analytic model of galaxy formation.","SF emission includes free-free and synchrotron emission, which depend on the free-electron density and the rate of core-collapse supernovae with a minor contribution from supernova remnants, respectively.","AGN emission is modelled based on the jet production rate, which depends on the black hole mass, accretion rate and spin, and includes synchrotron self-absorption.","Shark reproduces radio luminosity functions (RLFs) at 1.4 GHz and 150 MHz for 0 $\\leq$ z $\\leq$ 4, and scaling relations between radio luminosity, star formation rate and infrared luminosity of galaxies in the local and distant universe in good agreement with observations.","The model also reproduces observed number counts of radio sources from 150 MHz to 8.4 GHz to within a factor of two on average, though larger discrepancies are seen at the very bright fluxes at higher frequencies.","We use this model to understand how the radio continuum emission from radio-quiet AGNs can affect the measured RLFs of galaxies.","We find current methods to exclude AGNs from observational samples result in large fractions of radio-quiet AGNs contaminating the \"star-forming galaxies\" selection and a brighter end to the resulting RLFs.","We investigate how this effects the infrared-radio correlation (IRRC) and show that AGN contamination can lead to evolution of the IRRC with redshift.","Without this contamination our model predicts a redshift- and stellar mass-independent IRRC, except at the dwarf-galaxy regime."],"url":"http://arxiv.org/abs/2405.05586v1","category":"astro-ph.GA"}
{"created":"2024-05-09 07:15:19","title":"OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs","abstract":"The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. Difficulties lie in assessing the factuality of free-form responses in open domains. Also, different papers use disparate evaluation benchmarks and measurements, which renders them hard to compare and hampers future progress. To mitigate these issues, we propose OpenFactCheck, a unified factuality evaluation framework for LLMs. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users to easily customize an automatic fact-checker and verify the factual correctness of documents and claims, (ii) LLMEVAL, a unified evaluation framework assesses LLM's factuality ability from various perspectives fairly, and (iii) CHECKEREVAL is an extensible solution for gauging the reliability of automatic fact-checkers' verification results using human-annotated datasets. OpenFactCheck is publicly released at https://github.com/yuxiaw/OpenFactCheck.","sentences":["The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs.","Difficulties lie in assessing the factuality of free-form responses in open domains.","Also, different papers use disparate evaluation benchmarks and measurements, which renders them hard to compare and hampers future progress.","To mitigate these issues, we propose OpenFactCheck, a unified factuality evaluation framework for LLMs.","OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users to easily customize an automatic fact-checker and verify the factual correctness of documents and claims, (ii) LLMEVAL, a unified evaluation framework assesses LLM's factuality ability from various perspectives fairly, and (iii) CHECKEREVAL is an extensible solution for gauging the reliability of automatic fact-checkers' verification results using human-annotated datasets.","OpenFactCheck is publicly released at https://github.com/yuxiaw/OpenFactCheck."],"url":"http://arxiv.org/abs/2405.05583v1","category":"cs.CL"}
{"created":"2024-05-09 06:50:24","title":"Discovering the Mass-Scaled Damping Timescale from Microquasars to Blazars","abstract":"Studying the variability of the accretion disks of black holes and jets is important to identify their internal physical processes. In this letter, we obtain the characteristic damping timescale of 34 blazars and seven microquasars from the Fermi-Large Area Telescope and the XMM-Newton X-ray telescope, respectively. We found that the mass-scaled characteristic timescales, ranging from the microquasars of stellar-mass black holes to the blazars of supermassive black holes, exhibited a linear relationship with a slope of $\\sim$0.57. Given the fact the damping timescales of the $\\gamma$-ray in the blazars are associated with the jet, we propose that the timescales of the X-ray in these microquasars are also related with the jet. The mass-scaled damping timescale that we found was consistent with the radiation of the optical accretion disk. This can be attributed to the viscous timescale at the ultraviolet-emitting radii of the disk, which can affect the jet. Our study provides a new perspective on the origin of the region of radiation and the possible disk--jet connection based on time-domain analysis.","sentences":["Studying the variability of the accretion disks of black holes and jets is important to identify their internal physical processes.","In this letter, we obtain the characteristic damping timescale of 34 blazars and seven microquasars from the Fermi-Large Area Telescope and the XMM-Newton X-ray telescope, respectively.","We found that the mass-scaled characteristic timescales, ranging from the microquasars of stellar-mass black holes to the blazars of supermassive black holes, exhibited a linear relationship with a slope of $\\sim$0.57.","Given the fact the damping timescales of the $\\gamma$-ray in the blazars are associated with the jet, we propose that the timescales of the X-ray in these microquasars are also related with the jet.","The mass-scaled damping timescale that we found was consistent with the radiation of the optical accretion disk.","This can be attributed to the viscous timescale at the ultraviolet-emitting radii of the disk, which can affect the jet.","Our study provides a new perspective on the origin of the region of radiation and the possible disk--jet connection based on time-domain analysis."],"url":"http://arxiv.org/abs/2405.05575v1","category":"astro-ph.HE"}
{"created":"2024-05-09 05:51:17","title":"Conformal to confining SQFTs from holography","abstract":"In this paper we present three new families of smooth Type II string theory backgrounds. These are dual to supersymmetry-preserving deformations of 4d SCFTs. The deformations include a VEV for a global current and a `twisted compactification' on a circle. We study various holographic aspects of the dual QFTs, focusing on Wilson loops and Entanglement Entropy. Additionally, we present a monotonic quantity calculating the density of degrees of freedom in terms of the energy, which interpolates between the IR 3d gapped theory and the 4d SCFT result. Other probes related to global aspects of the QFTs are briefly discussed.","sentences":["In this paper we present three new families of smooth Type II string theory backgrounds.","These are dual to supersymmetry-preserving deformations of 4d SCFTs.","The deformations include a VEV for a global current and a `twisted compactification' on a circle.","We study various holographic aspects of the dual QFTs, focusing on Wilson loops and Entanglement Entropy.","Additionally, we present a monotonic quantity calculating the density of degrees of freedom in terms of the energy, which interpolates between the IR 3d gapped theory and the 4d SCFT result.","Other probes related to global aspects of the QFTs are briefly discussed."],"url":"http://arxiv.org/abs/2405.05563v1","category":"hep-th"}
{"created":"2024-05-09 02:40:45","title":"Locally Scale Invariant Chern-Simons Actions in 3+1 Dimensions and Their Emergence From 4+2 Dimensional 2T-Physics","abstract":"The traditional Chern-Simons (CS) terms in 3+1 dimensions that modify General Relativity (GR), Quantum Chromodynamics (QCD), and Quantum Electrodynamics (QED), typically lack scale invariance. However, a locally scale invariant and geodesically complete framework for the Standard Model (SM) coupled to GR was previously constructed by employing a tailored form of local scale (Weyl) symmetry. This refined SM+GR model closely resembles the conventional SM in subatomic realms where gravitational effects are negligible. Nevertheless, it offers an intriguing prediction: the emergence of new physics beyond the traditional SM and GR near spacetime singularities, characterized by intense gravity and substantial deviations in the Higgs field. In this study, we expand upon the enhanced SM+GR by incorporating Weyl invariant CS terms for gravity, QCD, and QED in 3+1 dimensions, thereby integrating CS contributions within the locally scale-invariant and geodesically complete paradigm. Additionally, we establish a holographic correspondence between the new CS terms in 3+1 dimensions and novel 4+2 dimensional CS-type actions within 2T-physics. We demonstrate that the Weyl transformation in 3+1 dimensions arises from 4+2 general coordinate transformations, which unify the hidden extra 1+1 large (not curled up) dimensions with the evident 3+1 dimensions. By leveraging the newfound local conformal symmetry, the augmented and geodesically complete SM+GR+CS introduces innovative tools and perspectives for exploring classical field theory aspects of black hole and cosmological singularities in 3+1 dimensions, while the 4+2 dimensional connection unveils deeper facets of spacetime.","sentences":["The traditional Chern-Simons (CS) terms in 3+1 dimensions that modify General Relativity (GR), Quantum Chromodynamics (QCD), and Quantum Electrodynamics (QED), typically lack scale invariance.","However, a locally scale invariant and geodesically complete framework for the Standard Model (SM) coupled to GR was previously constructed by employing a tailored form of local scale (Weyl) symmetry.","This refined SM+GR model closely resembles the conventional SM in subatomic realms where gravitational effects are negligible.","Nevertheless, it offers an intriguing prediction: the emergence of new physics beyond the traditional SM and GR near spacetime singularities, characterized by intense gravity and substantial deviations in the Higgs field.","In this study, we expand upon the enhanced SM+GR by incorporating Weyl invariant CS terms for gravity, QCD, and QED in 3+1 dimensions, thereby integrating CS contributions within the locally scale-invariant and geodesically complete paradigm.","Additionally, we establish a holographic correspondence between the new CS terms in 3+1 dimensions and novel 4+2 dimensional CS-type actions within 2T-physics.","We demonstrate that the Weyl transformation in 3+1 dimensions arises from 4+2 general coordinate transformations, which unify the hidden extra 1+1 large (not curled up) dimensions with the evident 3+1 dimensions.","By leveraging the newfound local conformal symmetry, the augmented and geodesically complete SM+GR+CS introduces innovative tools and perspectives for exploring classical field theory aspects of black hole and cosmological singularities in 3+1 dimensions, while the 4+2 dimensional connection unveils deeper facets of spacetime."],"url":"http://arxiv.org/abs/2405.05510v1","category":"hep-th"}
{"created":"2024-05-09 00:26:37","title":"A review of wormhole stabilization in f(R) gravity theories","abstract":"It has been proven that in standard Einstein gravity, exotic matter (i.e. matter violating the point-wise and averaged weak and null energy conditions) is required to stabilize traversable wormholes. Quantum field theory permits these violations due to the quantum coherent effects found in any quantum field [1]. Even reasonable classical scalar fields violate the energy conditions. In the case of the Casimir effect and squeezed vacuum states, these violations have been experimentally proven. It is advantageous to investigate methods of minimizing the use of exotic matter. One such area of interest is extended theories of Einstein gravity. It has been claimed that in some extended theories, stable traversable wormholes solutions can be found without the use of exotic matter. There are many extended theories of gravity and in this review paper we first explore f(R) theories and then explore some wormhole solutions in f(R) theories including Lovelock gravity.","sentences":["It has been proven that in standard Einstein gravity, exotic matter (i.e. matter violating the point-wise and averaged weak and null energy conditions) is required to stabilize traversable wormholes.","Quantum field theory permits these violations due to the quantum coherent effects found in any quantum field [1].","Even reasonable classical scalar fields violate the energy conditions.","In the case of the Casimir effect and squeezed vacuum states, these violations have been experimentally proven.","It is advantageous to investigate methods of minimizing the use of exotic matter.","One such area of interest is extended theories of Einstein gravity.","It has been claimed that in some extended theories, stable traversable wormholes solutions can be found without the use of exotic matter.","There are many extended theories of gravity and in this review paper we first explore f(R) theories and then explore some wormhole solutions in f(R) theories including Lovelock gravity."],"url":"http://arxiv.org/abs/2405.05476v1","category":"gr-qc"}
{"created":"2024-05-09 00:21:42","title":"The irreducible mass of a regular rotating black hole","abstract":"This article presents an analysis of regular rotating black hole solutions within the framework of Teleparallel Equivalent to General Relativity (TEGR). The study evaluates the total energy and derives an analytical expression for the irreducible mass of a regular black hole. The results reveal the significance of these regular black holes as approximations of real astrophysical objects. The investigation explores the behavior of the total energy for different surfaces and its value at spatial infinity. Additionally, the article addresses the instability of the inner horizon and examines the inertial acceleration of an observer inside the inner horizon.","sentences":["This article presents an analysis of regular rotating black hole solutions within the framework of Teleparallel Equivalent to General Relativity (TEGR).","The study evaluates the total energy and derives an analytical expression for the irreducible mass of a regular black hole.","The results reveal the significance of these regular black holes as approximations of real astrophysical objects.","The investigation explores the behavior of the total energy for different surfaces and its value at spatial infinity.","Additionally, the article addresses the instability of the inner horizon and examines the inertial acceleration of an observer inside the inner horizon."],"url":"http://arxiv.org/abs/2405.05475v1","category":"gr-qc"}
{"created":"2024-05-08 23:38:21","title":"Theoretical investigation of energy levels and transitions for Ce III with applications to kilonova spectra","abstract":"Doubly ionized cerium (Ce$^{2+}$) is one of the most important ions to understand the kilonova spectra. In particular, near-infrared (NIR) transitions of Ce III between the ground (5p$^6$ 4f$^2$) and first excited (5p$^6$ 4f 5d) configurations are responsible for the absorption features around 14,500 A. However, there is no dedicated theoretical studies to provide accurate transition probabilities for these transitions. We present energy levels of the ground and first excited configurations and transition data between them for Ce III. Calculations are performed using the GRASP2018 package, which is based on the multiconfiguration Dirac-Hartree-Fock and relativistic configuration interaction methods. Compared with the energy levels in the NIST database, our calculations reach the accuracy with the root-mean-square (rms) of 2732 cm$^{-1}$ or 1404 cm$^{-1}$ (excluding one highest level) for ground configuration, and rms of 618 cm$^{-1}$ for the first excited configuration. We extensively study the line strengths and find that the Babushkin gauge provide the more accurate values. By using the calculated gf values, we show that the NIR spectral features of kilonova can be explained by the Ce III lines.","sentences":["Doubly ionized cerium (Ce$^{2+}$) is one of the most important ions to understand the kilonova spectra.","In particular, near-infrared (NIR) transitions of Ce III between the ground (5p$^6$ 4f$^2$) and first excited (5p$^6$ 4f 5d) configurations are responsible for the absorption features around 14,500 A.","However, there is no dedicated theoretical studies to provide accurate transition probabilities for these transitions.","We present energy levels of the ground and first excited configurations and transition data between them for Ce III.","Calculations are performed using the GRASP2018 package, which is based on the multiconfiguration Dirac-Hartree-Fock and relativistic configuration interaction methods.","Compared with the energy levels in the NIST database, our calculations reach the accuracy with the root-mean-square (rms) of 2732 cm$^{-1}$ or 1404 cm$^{-1}$ (excluding one highest level) for ground configuration, and rms of 618 cm$^{-1}$ for the first excited configuration.","We extensively study the line strengths and find that the Babushkin gauge provide the more accurate values.","By using the calculated gf values, we show that the NIR spectral features of kilonova can be explained by the Ce III lines."],"url":"http://arxiv.org/abs/2405.05463v1","category":"astro-ph.HE"}
{"created":"2024-05-08 23:20:44","title":"Link groups of Kishino knot stacks","abstract":"For any virtual link, a class of new links can be defined called stacks, in which copies of the virtual link are placed on top of one another. The resulting virtual link depends only on the virtual isotopy class of the original link, and the fundamental group of such a link may be used to detect whether the link is nontrivial and whether it is nonclassical in some cases. We show that this group is able to distinguish three Kishino knots from the unknot using a stacked pair. However, for a fourth Kishino knot, the group and quandle of any stack invariant will be free with a number of generators equal to the number of copies in the stac.","sentences":["For any virtual link, a class of new links can be defined called stacks, in which copies of the virtual link are placed on top of one another.","The resulting virtual link depends only on the virtual isotopy class of the original link, and the fundamental group of such a link may be used to detect whether the link is nontrivial and whether it is nonclassical in some cases.","We show that this group is able to distinguish three Kishino knots from the unknot using a stacked pair.","However, for a fourth Kishino knot, the group and quandle of any stack invariant will be free with a number of generators equal to the number of copies in the stac."],"url":"http://arxiv.org/abs/2405.05457v1","category":"math.GT"}
{"created":"2024-05-08 22:06:57","title":"Unbiasing Fermionic Auxiliary-Field Quantum Monte Carlo with Matrix Product State Trial Wavefunctions","abstract":"In this work, we report, for the first time, an implementation of fermionic auxiliary-field quantum Monte Carlo (AFQMC) using matrix product state (MPS) trial wavefunctions, dubbed MPS-AFQMC. Calculating overlaps between an MPS trial and arbitrary Slater determinants up to a multiplicative error, a crucial subroutine in MPS-AFQMC, is proven to be #P-hard. Nonetheless, we tested several promising heuristics in successfully improving fermionic phaseless AFQMC energies. We also proposed a way to evaluate local energy and force bias evaluations free of matrix-product operators. This allows for larger basis set calculations without significant overhead. We showcase the utility of our approach on one- and two-dimensional hydrogen lattices, even when the MPS trial itself struggles to obtain high accuracy. Our work offers a new set of tools that can solve currently challenging electronic structure problems with future improvements.","sentences":["In this work, we report, for the first time, an implementation of fermionic auxiliary-field quantum Monte Carlo (AFQMC) using matrix product state (MPS) trial wavefunctions, dubbed MPS-AFQMC.","Calculating overlaps between an MPS trial and arbitrary Slater determinants up to a multiplicative error, a crucial subroutine in MPS-AFQMC, is proven to be #P-hard.","Nonetheless, we tested several promising heuristics in successfully improving fermionic phaseless AFQMC energies.","We also proposed a way to evaluate local energy and force bias evaluations free of matrix-product operators.","This allows for larger basis set calculations without significant overhead.","We showcase the utility of our approach on one-","and two-dimensional hydrogen lattices, even when the MPS trial itself struggles to obtain high accuracy.","Our work offers a new set of tools that can solve currently challenging electronic structure problems with future improvements."],"url":"http://arxiv.org/abs/2405.05440v1","category":"physics.chem-ph"}
{"created":"2024-05-08 21:50:39","title":"Measurement of Coherent Vibrational Dynamics with X-ray Transient Absorption Spectroscopy Simultaneously at the Carbon K- and Chlorine L$_{2,3}$- Edges","abstract":"X-ray Transient Absorption Spectroscopy near the carbon K-edge (1s, $\\sim$ 285 eV) and chlorine L$_{2,3}$ edges (2p, $\\sim$ 200 eV) is used to study the nuclear dynamics of CCl$_4$ vibrationally activated by impulsive stimulated Raman scattering with a few-cycle 800 nm pump pulse. The totally symmetric stretching mode leads to a strong response in the inner-shell spectra, with the concerted elongation (contraction) in bond lengths leading to a red (blue) shift in the X-ray absorption energies associated with core-to-antibonding excitations. The relative slopes of the potential energy surfaces associated with the relevant core-excited states along the symmetric stretching mode are experimentally measured and compared to results from restricted open-shell Kohn-Sham calculations. A combination of experiment and theory indicates that the slope of the core-excited potential energy surface vs totally symmetric bond elongation is $-11.1 \\pm 0.8$ eV/{\\AA} for the Cl 2p$\\to7a_1^*$ excitation, $-9.0\\pm0.6$ eV/{\\AA} for the Cl 2p$\\to8t_2^*$ excitation and $-5.2\\pm 0.4$ eV/{\\AA} for the C 1s$\\to8t_2^*$ excitation, to 95% confidence. The much larger slopes for the Cl 2p excitations compared to the C 1s state are attributed to greater contributions from Cl to the $7a_1^*$ or $8t_2^*$ antibonding orbitals to which the inner-shell electrons are being excited. No net displacement of the center of the vibrational wavefunction along the other vibrational modes is induced by the pump pulse, leading to absence of transient signal. The results highlight the ability of X-ray Transient Absorption Spectroscopy to reveal nuclear dynamics involving tiny ($<0.01$ {\\AA}) atomic displacements and also provide direct measurement of forces on core-excited potential energy surfaces.","sentences":["X-ray Transient Absorption Spectroscopy near the carbon K-edge (1s, $\\sim$ 285 eV) and chlorine L$_{2,3}$ edges (2p, $\\sim$ 200 eV) is used to study the nuclear dynamics of CCl$_4$ vibrationally activated by impulsive stimulated Raman scattering with a few-cycle 800 nm pump pulse.","The totally symmetric stretching mode leads to a strong response in the inner-shell spectra, with the concerted elongation (contraction) in bond lengths leading to a red (blue) shift in the X-ray absorption energies associated with core-to-antibonding excitations.","The relative slopes of the potential energy surfaces associated with the relevant core-excited states along the symmetric stretching mode are experimentally measured and compared to results from restricted open-shell Kohn-Sham calculations.","A combination of experiment and theory indicates that the slope of the core-excited potential energy surface vs totally symmetric bond elongation is $-11.1 \\pm 0.8$ eV/{\\AA} for the Cl 2p$\\to7a_1^*$ excitation, $-9.0\\pm0.6$ eV/{\\AA} for the Cl 2p$\\to8t_2^*$ excitation and $-5.2\\pm 0.4$ eV/{\\AA} for the C 1s$\\to8t_2^*$ excitation, to 95% confidence.","The much larger slopes for the Cl 2p excitations compared to the C 1s state are attributed to greater contributions from Cl to the $7a_1^*$ or $8t_2^*$ antibonding orbitals to which the inner-shell electrons are being excited.","No net displacement of the center of the vibrational wavefunction along the other vibrational modes is induced by the pump pulse, leading to absence of transient signal.","The results highlight the ability of X-ray Transient Absorption Spectroscopy to reveal nuclear dynamics involving tiny ($<0.01$ {\\AA}) atomic displacements and also provide direct measurement of forces on core-excited potential energy surfaces."],"url":"http://arxiv.org/abs/2405.05437v1","category":"physics.chem-ph"}
{"created":"2024-05-08 20:33:32","title":"Power spectrum multipoles and clustering wedges during the Epoch of Reionization","abstract":"We study the viability of using power spectrum clustering wedges as summary statistics of 21cm surveys during the Epoch of Reionization (EoR). For observations along a large lightcone $z\\sim 7-9$, the power spectrum is subject to large anisotropic effects due to the evolution along the light-of-sight. Information on the physics of reionization can be extracted from the anisotropy using the power spectrum multipoles. Signals of the power spectrum monopole are highly correlated at scales smaller than the typical ionization bubble, which can be disentangled by including higher-order multipoles. By simulating observations of the low frequency part of the SKA Observatory, we find that the sampling of the cylindrical wavenumber $k$-space is highly non-uniform due to the baseline distribution. Measurements in clustering wedges can be used for isolating parts of $k$-space with relatively uniform sampling, allowing for more precise parameter inference. Using Fisher Matrix forecasts, we find that the reionization model can be inferred with per-cent level precision with $\\sim 120$hrs of integration time using SKA-Low. Comparing to using only the power spectrum monopole above the foreground wedge, model inference using multipole power spectra in clustering wedges yields a factor of $\\sim 3$ improvement on parameter constraints.","sentences":["We study the viability of using power spectrum clustering wedges as summary statistics of 21cm surveys during the Epoch of Reionization (EoR).","For observations along a large lightcone $z\\sim 7-9$, the power spectrum is subject to large anisotropic effects due to the evolution along the light-of-sight.","Information on the physics of reionization can be extracted from the anisotropy using the power spectrum multipoles.","Signals of the power spectrum monopole are highly correlated at scales smaller than the typical ionization bubble, which can be disentangled by including higher-order multipoles.","By simulating observations of the low frequency part of the SKA Observatory, we find that the sampling of the cylindrical wavenumber $k$-space is highly non-uniform due to the baseline distribution.","Measurements in clustering wedges can be used for isolating parts of $k$-space with relatively uniform sampling, allowing for more precise parameter inference.","Using Fisher Matrix forecasts, we find that the reionization model can be inferred with per-cent level precision with $\\sim 120$hrs of integration time using SKA-Low.","Comparing to using only the power spectrum monopole above the foreground wedge, model inference using multipole power spectra in clustering wedges yields a factor of $\\sim 3$ improvement on parameter constraints."],"url":"http://arxiv.org/abs/2405.05414v1","category":"astro-ph.CO"}
{"created":"2024-05-08 20:11:09","title":"The possibility of multi-TeV secondary gamma rays from GRB221009A","abstract":"The brightest gamma ray burst (GRB) ever observed, GRB221009A, produced a surprisingly large flux of gamma rays with multi-TeV energies, which are expected to be absorbed in interactions with extragalactic background light (EBL). If the highest energy gamma rays were produced at the source, their spectral shape would have to exhibit a nonphysical spike even for the lowest levels of EBL. We show that, for widely accepted models of EBL, the data can be explained by secondary gamma rays produced in cosmic ray interactions along the line of sight, as long as the extragalactic magnetic fields (EGMFs) are $10^{-16}$G or smaller, assuming 1 Mpc correlation length. Our interpretation supports the widely held expectation that GRB jets can accelerate cosmic rays to energies as high as 10 EeV and above, and it has implications for understanding the magnitudes of EGMFs.","sentences":["The brightest gamma ray burst (GRB) ever observed, GRB221009A, produced a surprisingly large flux of gamma rays with multi-TeV energies, which are expected to be absorbed in interactions with extragalactic background light (EBL).","If the highest energy gamma rays were produced at the source, their spectral shape would have to exhibit a nonphysical spike even for the lowest levels of EBL.","We show that, for widely accepted models of EBL, the data can be explained by secondary gamma rays produced in cosmic ray interactions along the line of sight, as long as the extragalactic magnetic fields (EGMFs) are $10^{-16}$G or smaller, assuming 1 Mpc correlation length.","Our interpretation supports the widely held expectation that GRB jets can accelerate cosmic rays to energies as high as 10 EeV and above, and it has implications for understanding the magnitudes of EGMFs."],"url":"http://arxiv.org/abs/2405.05402v1","category":"astro-ph.HE"}
{"created":"2024-05-08 20:03:09","title":"From Young Massive Clusters to Old Globular Clusters: Density Profile Evolution and IMBH Formation","abstract":"The surface brightness profiles of globular clusters are conventionally described with the well-known King profile. However, observations of young massive clusters (YMCs) in the local Universe suggest that they are better fit by simple models with flat central cores and simple power-law densities in their outer regions (such as the Elson-Fall-Freeman, or EFF, profile). Depending on their initial central density, these YMCs may also facilitate large numbers of stellar collisions, potentially creating very massive stars that will directly collapse to intermediate-mass black holes (IMBHs). Using Monte Carlo $N$-body models of YMCs, we show that EFF-profile clusters transform to Wilson or King profiles through natural dynamical evolution, but that their final $W_0$ parameters do not strongly correlate to their initial concentrations. The most centrally-dense YMCs can produce runaway stellar mergers as massive as $4000\\,M_{\\odot}$ (the largest resolved mass in our simulations) which can collapse to produce IMBHs of similar masses. In doing so, these runaway collisions also deplete the clusters of their primordial massive stars, reducing the number of stellar-mass BHs by as much as $\\sim$ 40\\%. This depletion will accelerate the core collapse of clusters, suggesting that the process of IMBH formation itself may produce the high densities observed in some core-collapsed clusters.","sentences":["The surface brightness profiles of globular clusters are conventionally described with the well-known King profile.","However, observations of young massive clusters (YMCs) in the local Universe suggest that they are better fit by simple models with flat central cores and simple power-law densities in their outer regions (such as the Elson-Fall-Freeman, or EFF, profile).","Depending on their initial central density, these YMCs may also facilitate large numbers of stellar collisions, potentially creating very massive stars that will directly collapse to intermediate-mass black holes (IMBHs).","Using Monte Carlo $N$-body models of YMCs, we show that EFF-profile clusters transform to Wilson or King profiles through natural dynamical evolution, but that their final $W_0$ parameters do not strongly correlate to their initial concentrations.","The most centrally-dense YMCs can produce runaway stellar mergers as massive as $4000\\,M_{\\odot}$ (the largest resolved mass in our simulations) which can collapse to produce IMBHs of similar masses.","In doing so, these runaway collisions also deplete the clusters of their primordial massive stars, reducing the number of stellar-mass BHs by as much as $\\sim$ 40\\%.","This depletion will accelerate the core collapse of clusters, suggesting that the process of IMBH formation itself may produce the high densities observed in some core-collapsed clusters."],"url":"http://arxiv.org/abs/2405.05397v1","category":"astro-ph.GA"}
{"created":"2024-05-08 19:49:39","title":"Mutual information and the encoding of contingency tables","abstract":"Mutual information is commonly used as a measure of similarity between competing labelings of a given set of objects, for example to quantify performance in classification and community detection tasks. As argued recently, however, the mutual information as conventionally defined can return biased results because it neglects the information cost of the so-called contingency table, a crucial component of the similarity calculation. In principle the bias can be rectified by subtracting the appropriate information cost, leading to the modified measure known as the reduced mutual information, but in practice one can only ever compute an upper bound on this information cost, and the value of the reduced mutual information depends crucially on how good a bound is established. In this paper we describe an improved method for encoding contingency tables that gives a substantially better bound in typical use cases, and approaches the ideal value in the common case where the labelings are closely similar, as we demonstrate with extensive numerical results.","sentences":["Mutual information is commonly used as a measure of similarity between competing labelings of a given set of objects, for example to quantify performance in classification and community detection tasks.","As argued recently, however, the mutual information as conventionally defined can return biased results because it neglects the information cost of the so-called contingency table, a crucial component of the similarity calculation.","In principle the bias can be rectified by subtracting the appropriate information cost, leading to the modified measure known as the reduced mutual information, but in practice one can only ever compute an upper bound on this information cost, and the value of the reduced mutual information depends crucially on how good a bound is established.","In this paper we describe an improved method for encoding contingency tables that gives a substantially better bound in typical use cases, and approaches the ideal value in the common case where the labelings are closely similar, as we demonstrate with extensive numerical results."],"url":"http://arxiv.org/abs/2405.05393v1","category":"cs.SI"}
{"created":"2024-05-08 19:43:33","title":"Magnetic fields beneath active region coronal loops","abstract":"We examine the hypothesis that multipolar magnetic fields advected by photospheric granules can contribute heating to the active chromosphere and corona. On 28 September 2020 the GRIS and HiFI+ instruments at the GREGOR telescope obtained data of NOAA 12773. We analyze Stokes profiles of spectral lines of Si I and He I, to study magnetic fields from photosphere to the upper chromosphere. Magnetogram and EUV data from the HMI and AIA instruments on the SDO spacecraft are co-aligned and studied in relation to the GRIS data. At coronal loop footpoints, minor polarity fields comprise just 0.2% and 0.02% of the flux measured over the 40\" x 60\" area observed in the photosphere and upper chromosphere, centered 320\" from disk center. Significantly, the minority fields are situated >~ 12\" from bright footpoints. We use physical arguments to show that any unresolved minority flux cannot reach coronal footpoints adjacent to the upper chromosphere. Even if it did, the most optimistic estimate of the energy released through chromospheric reconnection is barely sufficient to account for the coronal energy losses. Further, dynamical changes accompanying reconnection between uni- and multi- polar fields are seen neither in the He I data nor in narrow-band movies of the H alpha line core. We conclude that the hypothesis must be rejected. Bright chromospheric, transition region and coronal loop plasmas must be heated by mechanisms involving unipolar fields.","sentences":["We examine the hypothesis that multipolar magnetic fields advected by photospheric granules can contribute heating to the active chromosphere and corona.","On 28 September 2020 the GRIS and HiFI+ instruments at the GREGOR telescope obtained data of NOAA 12773.","We analyze Stokes profiles of spectral lines of Si I","and He I, to study magnetic fields from photosphere to the upper chromosphere.","Magnetogram and EUV data from the HMI and AIA instruments on the SDO spacecraft are co-aligned and studied in relation to the GRIS data.","At coronal loop footpoints, minor polarity fields comprise just 0.2% and 0.02% of the flux measured over the 40\" x 60\" area observed in the photosphere and upper chromosphere, centered 320\" from disk center.","Significantly, the minority fields are situated >~ 12\" from bright footpoints.","We use physical arguments to show that any unresolved minority flux cannot reach coronal footpoints adjacent to the upper chromosphere.","Even if it did, the most optimistic estimate of the energy released through chromospheric reconnection is barely sufficient to account for the coronal energy losses.","Further, dynamical changes accompanying reconnection between uni- and multi- polar fields are seen neither in the He I data nor in narrow-band movies of the H alpha line core.","We conclude that the hypothesis must be rejected.","Bright chromospheric, transition region and coronal loop plasmas must be heated by mechanisms involving unipolar fields."],"url":"http://arxiv.org/abs/2405.05391v1","category":"astro-ph.SR"}
{"created":"2024-05-08 19:27:26","title":"Bosonic multi-Higgs correlations beyond leading order","abstract":"The production of multiple Higgs bosons at the LHC and beyond is a strong test of the mechanism of electroweak symmetry breaking. Taking inspiration from recent experimental efforts to move towards limits on triple Higgs production at the Large Hadron Collider, we consider generic bosonic deviations of $HH$ and $HHH$ production from the Standard Model in the guise of Higgs Effective Field Theory. Including one-loop radiative corrections within the HEFT and going up to ${\\mathcal{O}}(p^4)$ in the momentum expansion, we provide a detailed motivation of the parameter range that the LHC (and future hadron colliders) can explore, through accessing non-standard coupling modifications and momentum dependencies that probe Higgs boson non-linearities. In particular, we find that radiative corrections can enhance the sensitivity to Higgs-self coupling modifiers and HEFT-specific momentum dependencies can vastly increase triple Higgs production thus providing further motivation to consider these processes during the LHC's high-luminosity phase.","sentences":["The production of multiple Higgs bosons at the LHC and beyond is a strong test of the mechanism of electroweak symmetry breaking.","Taking inspiration from recent experimental efforts to move towards limits on triple Higgs production at the Large Hadron Collider, we consider generic bosonic deviations of $HH$ and $HHH$ production from the Standard Model in the guise of Higgs Effective Field Theory.","Including one-loop radiative corrections within the HEFT and going up to ${\\mathcal{O}}(p^4)$ in the momentum expansion, we provide a detailed motivation of the parameter range that the LHC (and future hadron colliders) can explore, through accessing non-standard coupling modifications and momentum dependencies that probe Higgs boson non-linearities.","In particular, we find that radiative corrections can enhance the sensitivity to Higgs-self coupling modifiers and HEFT-specific momentum dependencies can vastly increase triple Higgs production thus providing further motivation to consider these processes during the LHC's high-luminosity phase."],"url":"http://arxiv.org/abs/2405.05385v1","category":"hep-ph"}
{"created":"2024-05-08 19:14:22","title":"Excluded volume effects on tangentially driven active ring polymers","abstract":"The conformational and dynamical properties of active ring polymers are studied by numerical simulations. The two-dimensionally confined polymer is modeled as a closed bead-spring chain, driven by tangential forces, put in contact with a heat bath described by the Brownian multiparticle collision dynamics. Both phantom polymers and chains comprising excluded volume interactions are considered for different bending rigidities. The size and shape are found to be dependent on persistence length, driving force, and bead mutual exclusion. The lack of excluded volume interactions is responsible for a shrinkage of active rings when increasing driving force in the flexible limit while the presence induces a moderate swelling of chains. Internal dynamics of flexible phantom active rings shows activity-enhanced diffusive behavior at large activity values while, in the case of self-avoiding active chains, it is characterized by active ballistic motion not depending on stiffness. The long-time dynamics of active rings is marked by rotational motion whose period scales as the inverse of the applied tangential force, irrespective of persistence length and beads self-exclusion.","sentences":["The conformational and dynamical properties of active ring polymers are studied by numerical simulations.","The two-dimensionally confined polymer is modeled as a closed bead-spring chain, driven by tangential forces, put in contact with a heat bath described by the Brownian multiparticle collision dynamics.","Both phantom polymers and chains comprising excluded volume interactions are considered for different bending rigidities.","The size and shape are found to be dependent on persistence length, driving force, and bead mutual exclusion.","The lack of excluded volume interactions is responsible for a shrinkage of active rings when increasing driving force in the flexible limit while the presence induces a moderate swelling of chains.","Internal dynamics of flexible phantom active rings shows activity-enhanced diffusive behavior at large activity values while, in the case of self-avoiding active chains, it is characterized by active ballistic motion not depending on stiffness.","The long-time dynamics of active rings is marked by rotational motion whose period scales as the inverse of the applied tangential force, irrespective of persistence length and beads self-exclusion."],"url":"http://arxiv.org/abs/2405.05380v1","category":"cond-mat.soft"}
{"created":"2024-05-08 18:45:37","title":"LOC-ZSON: Language-driven Object-Centric Zero-Shot Object Retrieval and Navigation","abstract":"In this paper, we present LOC-ZSON, a novel Language-driven Object-Centric image representation for object navigation task within complex scenes. We propose an object-centric image representation and corresponding losses for visual-language model (VLM) fine-tuning, which can handle complex object-level queries. In addition, we design a novel LLM-based augmentation and prompt templates for stability during training and zero-shot inference. We implement our method on Astro robot and deploy it in both simulated and real-world environments for zero-shot object navigation. We show that our proposed method can achieve an improvement of 1.38 - 13.38% in terms of text-to-image recall on different benchmark settings for the retrieval task. For object navigation, we show the benefit of our approach in simulation and real world, showing 5% and 16.67% improvement in terms of navigation success rate, respectively.","sentences":["In this paper, we present LOC-ZSON, a novel Language-driven Object-Centric image representation for object navigation task within complex scenes.","We propose an object-centric image representation and corresponding losses for visual-language model (VLM) fine-tuning, which can handle complex object-level queries.","In addition, we design a novel LLM-based augmentation and prompt templates for stability during training and zero-shot inference.","We implement our method on Astro robot and deploy it in both simulated and real-world environments for zero-shot object navigation.","We show that our proposed method can achieve an improvement of 1.38 - 13.38% in terms of text-to-image recall on different benchmark settings for the retrieval task.","For object navigation, we show the benefit of our approach in simulation and real world, showing 5% and 16.67% improvement in terms of navigation success rate, respectively."],"url":"http://arxiv.org/abs/2405.05363v1","category":"cs.CV"}
{"created":"2024-05-08 18:37:22","title":"Self-Similar Mass Accretion History in Scale-Free Simulations","abstract":"Using a scale-free $N$-body simulation generated with the ABACUS $N$-body code, we test the robustness of halo mass accretion histories via their convergence to self-similarity. We compare two halo finders, ROCKSTAR and COMPASO. We find superior self-similarity in halo mass accretion histories determined using ROCKSTAR, with convergence to 5% or better between $\\sim10^2$ to $10^5$ particles. For COMPASO we find weaker convergence over a similar region, with at least 10% between $\\sim10^2$ to $10^4$ particles. Furthermore, we find the convergence to self-similarity improves as the simulation evolves, with the largest and deepest regions of convergence appearing after the scale factor quadrupled from the time at which non-linear structures begin to form. With sufficient time evolution, halo mass accretion histories are converged to self-similarity within 5% with as few as $\\sim70$ particles for COMPASO and within 2% for as few as $\\sim30$ particles for ROCKSTAR.","sentences":["Using a scale-free $N$-body simulation generated with the ABACUS $N$-body code, we test the robustness of halo mass accretion histories via their convergence to self-similarity.","We compare two halo finders, ROCKSTAR and COMPASO.","We find superior self-similarity in halo mass accretion histories determined using ROCKSTAR, with convergence to 5% or better between $\\sim10^2$ to $10^5$ particles.","For COMPASO we find weaker convergence over a similar region, with at least 10% between $\\sim10^2$ to $10^4$ particles.","Furthermore, we find the convergence to self-similarity improves as the simulation evolves, with the largest and deepest regions of convergence appearing after the scale factor quadrupled from the time at which non-linear structures begin to form.","With sufficient time evolution, halo mass accretion histories are converged to self-similarity within 5% with as few as $\\sim70$ particles for COMPASO and within 2% for as few as $\\sim30$ particles for ROCKSTAR."],"url":"http://arxiv.org/abs/2405.05360v1","category":"astro-ph.CO"}
{"created":"2024-05-08 18:32:51","title":"Eco-driving Accounting for Interactive Cut-in Vehicles","abstract":"Automated vehicles can gather information about surrounding traffic and plan safe and energy-efficient driving behavior, which is known as eco-driving. Conventional eco-driving designs only consider preceding vehicles in the same lane as the ego vehicle. In heavy traffic, however, vehicles in adjacent lanes may cut into the ego vehicle's lane, influencing the ego vehicle's eco-driving behavior and compromising the energy-saving performance. Therefore, in this paper, we propose an eco-driving design that accounts for neighbor vehicles that have cut-in intentions. Specifically, we integrate a leader-follower game to predict the interaction between the ego and the cut-in vehicles and a model-predictive controller for planning energy-efficient behavior for the automated ego vehicle. We show that the leader-follower game model can reasonably represent the interactive motion between the ego vehicle and the cut-in vehicle. More importantly, we show that the proposed design can predict and react to neighbor vehicles' cut-in behaviors properly, leading to improved energy efficiency in cut-in scenarios compared to baseline designs that consider preceding vehicles only.","sentences":["Automated vehicles can gather information about surrounding traffic and plan safe and energy-efficient driving behavior, which is known as eco-driving.","Conventional eco-driving designs only consider preceding vehicles in the same lane as the ego vehicle.","In heavy traffic, however, vehicles in adjacent lanes may cut into the ego vehicle's lane, influencing the ego vehicle's eco-driving behavior and compromising the energy-saving performance.","Therefore, in this paper, we propose an eco-driving design that accounts for neighbor vehicles that have cut-in intentions.","Specifically, we integrate a leader-follower game to predict the interaction between the ego and the cut-in vehicles and a model-predictive controller for planning energy-efficient behavior for the automated ego vehicle.","We show that the leader-follower game model can reasonably represent the interactive motion between the ego vehicle and the cut-in vehicle.","More importantly, we show that the proposed design can predict and react to neighbor vehicles' cut-in behaviors properly, leading to improved energy efficiency in cut-in scenarios compared to baseline designs that consider preceding vehicles only."],"url":"http://arxiv.org/abs/2405.05353v1","category":"eess.SY"}
{"created":"2024-05-08 18:12:58","title":"Solar neutrinos and leptonic spin forces","abstract":"We quantify the effects on the evolution of solar neutrinos of light spin-zero particles with pseudoscalar couplings to leptons and scalar couplings to nucleons. In this scenario the matter potential sourced by the nucleons in the Sun's matter gives rise to spin precession of the relativistic neutrino ensemble. As such the effects in the solar observables are different if neutrinos are Dirac or Majorana particles. For Dirac neutrinos the spin-flavour precession results into left-handed neutrino to right-handed neutrino (i.e., active--sterile) oscillations, while for Majorana neutrinos it results into left-handed neutrino to right-handed antineutrino (i.e., active-active) oscillations. In both cases this leads to distortions in the solar neutrino spectrum which we use to derive constraints on the allowed values of the mediator mass and couplings via a global analysis of the solar neutrino data. In addition for Majorana neutrinos spin-flavour precession results into a potentially observable flux of solar electron antineutrinos at the Earth which we quantify and constrain with the existing bounds from Borexino and KamLAND.","sentences":["We quantify the effects on the evolution of solar neutrinos of light spin-zero particles with pseudoscalar couplings to leptons and scalar couplings to nucleons.","In this scenario the matter potential sourced by the nucleons in the Sun's matter gives rise to spin precession of the relativistic neutrino ensemble.","As such the effects in the solar observables are different if neutrinos are Dirac or Majorana particles.","For Dirac neutrinos the spin-flavour precession results into left-handed neutrino to right-handed neutrino (i.e., active--sterile) oscillations, while for Majorana neutrinos it results into left-handed neutrino to right-handed antineutrino (i.e., active-active) oscillations.","In both cases this leads to distortions in the solar neutrino spectrum which we use to derive constraints on the allowed values of the mediator mass and couplings via a global analysis of the solar neutrino data.","In addition for Majorana neutrinos spin-flavour precession results into a potentially observable flux of solar electron antineutrinos at the Earth which we quantify and constrain with the existing bounds from Borexino and KamLAND."],"url":"http://arxiv.org/abs/2405.05340v1","category":"hep-ph"}
{"created":"2024-05-08 18:12:34","title":"Optical Conductivity in Symmetric Mass Generation Insulators","abstract":"Symmetric mass generation (SMG) insulators are interaction-driven, featureless Mott insulating states in quantum many-body fermionic systems. Recent advancements suggest that zeros in the fermion Green's function could lead to non-vanishing negative optical conductivity in SMG insulators, even below the charge excitation gap. This study explores the origin of this unusual behavior through the lens of pole-zero duality, highlighting a critical issue where the current operator becomes unbounded, rendering the response function unphysical. By employing a lattice model, we derive a well-behaved lattice regularization of the current operator, enabling a detailed study of optical conductivity in SMG insulators. Utilizing both analytical and numerical methods, including strong-coupling expansions, we confirm that SMG insulators exhibit no optical conductivity at low energies below the charge gap, effectively resolving the paradox. This work not only deepens our understanding of quantum many-body phenomena but also lays a robust theoretical groundwork for future experimental explorations of SMG materials.","sentences":["Symmetric mass generation (SMG) insulators are interaction-driven, featureless Mott insulating states in quantum many-body fermionic systems.","Recent advancements suggest that zeros in the fermion Green's function could lead to non-vanishing negative optical conductivity in SMG insulators, even below the charge excitation gap.","This study explores the origin of this unusual behavior through the lens of pole-zero duality, highlighting a critical issue where the current operator becomes unbounded, rendering the response function unphysical.","By employing a lattice model, we derive a well-behaved lattice regularization of the current operator, enabling a detailed study of optical conductivity in SMG insulators.","Utilizing both analytical and numerical methods, including strong-coupling expansions, we confirm that SMG insulators exhibit no optical conductivity at low energies below the charge gap, effectively resolving the paradox.","This work not only deepens our understanding of quantum many-body phenomena but also lays a robust theoretical groundwork for future experimental explorations of SMG materials."],"url":"http://arxiv.org/abs/2405.05339v1","category":"cond-mat.str-el"}
{"created":"2024-05-08 18:11:32","title":"Exploring the Influence of (n-1)d Subvalence Correlation and of Spin-Orbit Coupling on Chalcogen Bonding","abstract":"This article presents a comprehensive computational investigation into chalcogen bonding interactions, focusing specifically on elucidating the role of subvalence (n$-$1)d and (n$-$1)sp correlation. The incorporation of inner-shell (n$-$1)d correlation leads to a decrease in interaction energies for chalcogen-bonded systems (at least those studied herein), contradicting the observations regarding halogen bonding documented by Kesharwani et al. in \\textit{J. Phys. Chem. A}, \\textbf{2018}, 122 (8), 2184-2197. The significance of (n$-$1)sp subvalence correlation appears to be lower by an order of magnitude. Notably, among the various components of interaction energies computed at the PNO-LCCSD(T) or DF-CCSD levels, we identify the PNO-LMP2 or DF-MP2 component of the (n$-$1)d correlation as predominant. Furthermore, we delve into the impact of second-order spin-orbit coupling (SOC2) on these interactions. Specifically, for the Te complexes, SOC2 effects rival (n$-$1)d correlation in importance; for the Se complexes, SOC2 is much less important. Generally, SOC2 stabilizes monomers more than dimers, resulting in reduced binding of the latter. Notably, at equilibrium and stretched geometries, SOC2 and (n$-$1)d destabilize the complex; however, at compressed geometries, they exhibit opposing effects, with (n$-$1)d becoming stabilizing.","sentences":["This article presents a comprehensive computational investigation into chalcogen bonding interactions, focusing specifically on elucidating the role of subvalence (n$-$1)d and (n$-$1)sp correlation.","The incorporation of inner-shell (n$-$1)d correlation leads to a decrease in interaction energies for chalcogen-bonded systems (at least those studied herein), contradicting the observations regarding halogen bonding documented by Kesharwani et al. in \\textit{J. Phys.","Chem.","A}, \\textbf{2018}, 122 (8), 2184-2197.","The significance of (n$-$1)sp subvalence correlation appears to be lower by an order of magnitude.","Notably, among the various components of interaction energies computed at the PNO-LCCSD(T) or DF-CCSD levels, we identify the PNO-LMP2 or DF-MP2 component of the (n$-$1)d correlation as predominant.","Furthermore, we delve into the impact of second-order spin-orbit coupling (SOC2) on these interactions.","Specifically, for the Te complexes, SOC2 effects rival (n$-$1)d correlation in importance; for the Se complexes, SOC2 is much less important.","Generally, SOC2 stabilizes monomers more than dimers, resulting in reduced binding of the latter.","Notably, at equilibrium and stretched geometries, SOC2 and (n$-$1)d destabilize the complex; however, at compressed geometries, they exhibit opposing effects, with (n$-$1)d becoming stabilizing."],"url":"http://arxiv.org/abs/2405.05338v1","category":"physics.chem-ph"}
{"created":"2024-05-08 18:05:40","title":"Feeding Hidden Monsters: a Super-Eddington accreting Black Hole ~1.5 Gyr after the Big Bang","abstract":"Recent James Webb Space Telescope (JWST) observations have revealed a surprisingly abundant population of faint, dusty active galactic nuclei (AGNs) at z~4-7. Together with the presence of supermassive black holes (SMBHs) at z>6, this raises questions about the formation and growth histories of early black holes. Current theories for the formation of seed black holes from the death of the first stars (i.e. light seeds) and/or the direct collapse of primordial gas clouds (i.e. heavy seeds) still lack observational confirmation. Here, we present LID-568, a low-mass (7.2e6Msun) black hole hosting powerful outflows that is observed in an extreme phase of rapid growth at z~4. This object is similar to other JWST-discovered faint AGN populations, but is bright in X-ray emission and accreting at more than 4000% of the limit at which radiation pressure exceeds the force of gravitational attraction of the black hole (i.e. super-Eddington accretion). Analysis of JWST NIRSpec/IFU data reveals spatially extended Ha emission with velocities of ~ -600 - -500 km/s relative to the central black hole, indicative of robust nuclear-driven outflows. LID-568 represents an elusive low-mass black hole experiencing super-Eddington accretion as invoked by models of early black hole formation. This discovery showcases a previously undiscovered key parameter space and offers crucial insights into rapid black hole growth mechanisms in the early universe.","sentences":["Recent James Webb Space Telescope (JWST) observations have revealed a surprisingly abundant population of faint, dusty active galactic nuclei (AGNs) at z~4-7.","Together with the presence of supermassive black holes (SMBHs) at z>6, this raises questions about the formation and growth histories of early black holes.","Current theories for the formation of seed black holes from the death of the first stars (i.e. light seeds) and/or the direct collapse of primordial gas clouds (i.e. heavy seeds) still lack observational confirmation.","Here, we present LID-568, a low-mass (7.2e6Msun) black hole hosting powerful outflows that is observed in an extreme phase of rapid growth at z~4.","This object is similar to other JWST-discovered faint AGN populations, but is bright in X-ray emission and accreting at more than 4000% of the limit at which radiation pressure exceeds the force of gravitational attraction of the black hole (i.e. super-Eddington accretion).","Analysis of JWST NIRSpec/IFU data reveals spatially extended Ha emission with velocities of ~ -600 - -500 km/s relative to the central black hole, indicative of robust nuclear-driven outflows.","LID-568 represents an elusive low-mass black hole experiencing super-Eddington accretion as invoked by models of early black hole formation.","This discovery showcases a previously undiscovered key parameter space and offers crucial insights into rapid black hole growth mechanisms in the early universe."],"url":"http://arxiv.org/abs/2405.05333v1","category":"astro-ph.GA"}
{"created":"2024-05-08 18:04:10","title":"Quantum Phases and Transitions in Spin Chains with Non-Invertible Symmetries","abstract":"Generalized symmetries often appear in the form of emergent symmetries in low energy effective descriptions of quantum many-body systems. Non-invertible symmetries are a particularly exotic class of generalized symmetries, in that they are implemented by transformations that do not form a group. Such symmetries appear generically in gapless states of quantum matter, constraining the low-energy dynamics. To provide a UV-complete description of such symmetries, it is useful to construct lattice models that respect these symmetries exactly. In this paper, we discuss two families of one-dimensional lattice Hamiltonians with finite on-site Hilbert spaces: one with (invertible) $S^{\\,}_3$ symmetry and the other with non-invertible $\\mathsf{Rep}(S^{\\,}_3)$ symmetry. Our models are largely analytically tractable and demonstrate all possible spontaneous symmetry breaking patterns of these symmetries. Moreover, we use numerical techniques to study the nature of continuous phase transitions between the different symmetry-breaking gapped phases associated with both symmetries. Both models have self-dual lines, where the models are enriched by so-called intrinsically non-invertible symmetries generated by Kramers-Wannier-like duality transformations. We provide explicit lattice operators that generate these non-invertible self-duality symmetries. We show that the enhanced symmetry at the self-dual lines is described by a 2+1D symmetry-topological-order (SymTO) of type $\\mathrm{JK}^{\\,}_4\\boxtimes \\overline{\\mathrm{JK}}^{\\,}_4$. The condensable algebras of the SymTO determine the allowed gapped and gapless states of the self-dual $S^{\\,}_3$-symmetric and $\\mathsf{Rep}(S^{\\,}_3)$-symmetric models.","sentences":["Generalized symmetries often appear in the form of emergent symmetries in low energy effective descriptions of quantum many-body systems.","Non-invertible symmetries are a particularly exotic class of generalized symmetries, in that they are implemented by transformations that do not form a group.","Such symmetries appear generically in gapless states of quantum matter, constraining the low-energy dynamics.","To provide a UV-complete description of such symmetries, it is useful to construct lattice models that respect these symmetries exactly.","In this paper, we discuss two families of one-dimensional lattice Hamiltonians with finite on-site Hilbert spaces: one with (invertible) $S^{\\,}_3$ symmetry and the other with non-invertible $\\mathsf{Rep}(S^{\\,}_3)$ symmetry.","Our models are largely analytically tractable and demonstrate all possible spontaneous symmetry breaking patterns of these symmetries.","Moreover, we use numerical techniques to study the nature of continuous phase transitions between the different symmetry-breaking gapped phases associated with both symmetries.","Both models have self-dual lines, where the models are enriched by so-called intrinsically non-invertible symmetries generated by Kramers-Wannier-like duality transformations.","We provide explicit lattice operators that generate these non-invertible self-duality symmetries.","We show that the enhanced symmetry at the self-dual lines is described by a 2+1D symmetry-topological-order (SymTO) of type $\\mathrm{JK}^{\\,}_4\\boxtimes \\overline{\\mathrm{JK}}^{\\,}_4$. The condensable algebras of the SymTO determine the allowed gapped and gapless states of the self-dual $S^{\\,}_3$-symmetric and $\\mathsf{Rep}(S^{\\,}_3)$-symmetric models."],"url":"http://arxiv.org/abs/2405.05331v1","category":"cond-mat.str-el"}
{"created":"2024-05-08 18:03:31","title":"Chemo-dynamical Evolution of Simulated Satellites for a Milky Way-like Galaxy","abstract":"The chemical abundances of Milky Way's satellites reflect their star formation histories (SFHs), yet, due to the difficulty of determining the ages of old stars, the SFHs of most satellites are poorly measured. Ongoing and upcoming surveys will obtain around ten times more medium-resolution spectra for stars in satellites than are currently available. To correctly extract SFHs from large samples of chemical abundances, the relationship between chemical abundances and SFHs needs to be clarified. Here, we perform a high-resolution cosmological zoom-in simulation of a Milky Way-like galaxy with detailed models of star formation, supernova feedback, and metal diffusion. We quantify SFHs, metallicity distribution functions, and the $\\alpha$-element (Mg, Ca, and Si) abundances in satellites of the host galaxy. We find that star formation in most simulated satellites is quenched before infalling to their host. Star formation episodes in simulated satellites are separated by a few hundred Myr owing to supernova feedback; each star formation event produces groups of stars with similar [$\\alpha$/Fe] and [Fe/H]. We then perform a mock observation of the upcoming Subaru Prime Focus Spectrograph (PFS) observations. We find that Subaru PFS will be able to detect distinct groups of stars in [$\\alpha$/Fe] vs. [Fe/H] space, produced by episodic star formation. This result means that episodic SFHs can be estimated from the chemical abundances of $\\gtrsim$ 1,000 stars determined with medium-resolution spectroscopy.","sentences":["The chemical abundances of Milky Way's satellites reflect their star formation histories (SFHs), yet, due to the difficulty of determining the ages of old stars, the SFHs of most satellites are poorly measured.","Ongoing and upcoming surveys will obtain around ten times more medium-resolution spectra for stars in satellites than are currently available.","To correctly extract SFHs from large samples of chemical abundances, the relationship between chemical abundances and SFHs needs to be clarified.","Here, we perform a high-resolution cosmological zoom-in simulation of a Milky Way-like galaxy with detailed models of star formation, supernova feedback, and metal diffusion.","We quantify SFHs, metallicity distribution functions, and the $\\alpha$-element (Mg, Ca, and Si) abundances in satellites of the host galaxy.","We find that star formation in most simulated satellites is quenched before infalling to their host.","Star formation episodes in simulated satellites are separated by a few hundred Myr owing to supernova feedback; each star formation event produces groups of stars with similar [$\\alpha$/Fe] and [Fe/H].","We then perform a mock observation of the upcoming Subaru Prime Focus Spectrograph (PFS) observations.","We find that Subaru PFS will be able to detect distinct groups of stars in [$\\alpha$/Fe] vs. [Fe/H] space, produced by episodic star formation.","This result means that episodic SFHs can be estimated from the chemical abundances of $\\gtrsim$ 1,000 stars determined with medium-resolution spectroscopy."],"url":"http://arxiv.org/abs/2405.05330v1","category":"astro-ph.GA"}
{"created":"2024-05-08 18:00:33","title":"Higher Berry Connection for Matrix Product States","abstract":"In one spatial dimension, families of short-range entangled many-body quantum states, parameterized over some parameter space, can be topologically distinguished and classified by topological invariants built from the higher Berry phase -- a many-body generalization of the Berry phase. Previous works identified the underlying mathematical structure (the gerbe structure) and introduced a multi-wavefunction overlap, a generalization of the inner product in quantum mechanics, which allows for the extraction of the higher Berry phase and topological invariants. In this paper, building on these works, we introduce a connection, the higher Berry connection, for a family of parameterized Matrix Product States (MPS) over a parameter space. We demonstrate the use of our formula for simple non-trivial models.","sentences":["In one spatial dimension, families of short-range entangled many-body quantum states, parameterized over some parameter space, can be topologically distinguished and classified by topological invariants built from the higher Berry phase -- a many-body generalization of the Berry phase.","Previous works identified the underlying mathematical structure (the gerbe structure) and introduced a multi-wavefunction overlap, a generalization of the inner product in quantum mechanics, which allows for the extraction of the higher Berry phase and topological invariants.","In this paper, building on these works, we introduce a connection, the higher Berry connection, for a family of parameterized Matrix Product States (MPS) over a parameter space.","We demonstrate the use of our formula for simple non-trivial models."],"url":"http://arxiv.org/abs/2405.05327v1","category":"cond-mat.str-el"}
{"created":"2024-05-08 18:00:20","title":"Higher Berry Phase from Projected Entangled Pair States in (2+1) dimensions","abstract":"We consider families of invertible many-body quantum states in $d$ spatial dimensions that are parameterized over some parameter space $X$. The space of such families is expected to have topologically distinct sectors classified by the cohomology group $\\mathrm{H}^{d+2}(X;\\mathbb{Z})$. These topological sectors are distinguished by a topological invariant built from a generalization of the Berry phase, called the higher Berry phase. In the previous work, we introduced a generalized inner product for three one-dimensional many-body quantum states, (``triple inner product''). The higher Berry phase for one-dimensional invertible states can be introduced through the triple inner product and furthermore the topological invariant, which takes its value in $\\mathrm{H}^{3}(X;\\mathbb{Z})$, can be extracted. In this paper, we introduce an inner product of four two-dimensional invertible quantum many-body states. We use it to measure the topological nontriviality of parameterized families of 2d invertible states. In particular, we define a topological invariant of such families that takes values in $\\mathrm{H}^{4}(X;\\mathbb{Z})$. Our formalism uses projected entangled pair states (PEPS). We also construct a specific example of non-trivial parameterized families of 2d invertible states parameterized over $\\mathbb{R}P^4$ and demonstrate the use of our formula. Applications for symmetry-protected topological phases are also discussed.","sentences":["We consider families of invertible many-body quantum states in $d$ spatial dimensions that are parameterized over some parameter space $X$. The space of such families is expected to have topologically distinct sectors classified by the cohomology group $\\mathrm{H}^{d+2}(X;\\mathbb{Z})$. These topological sectors are distinguished by a topological invariant built from a generalization of the Berry phase, called the higher Berry phase.","In the previous work, we introduced a generalized inner product for three one-dimensional many-body quantum states, (``triple inner product'').","The higher Berry phase for one-dimensional invertible states can be introduced through the triple inner product and furthermore the topological invariant, which takes its value in $\\mathrm{H}^{3}(X;\\mathbb{Z})$, can be extracted.","In this paper, we introduce an inner product of four two-dimensional invertible quantum many-body states.","We use it to measure the topological nontriviality of parameterized families of 2d invertible states.","In particular, we define a topological invariant of such families that takes values in $\\mathrm{H}^{4}(X;\\mathbb{Z})$. Our formalism uses projected entangled pair states (PEPS).","We also construct a specific example of non-trivial parameterized families of 2d invertible states parameterized over $\\mathbb{R}P^4$ and demonstrate the use of our formula.","Applications for symmetry-protected topological phases are also discussed."],"url":"http://arxiv.org/abs/2405.05325v1","category":"cond-mat.str-el"}
{"created":"2024-05-08 18:00:03","title":"AGN flares as counterparts to the mergers detected by LIGO and Virgo: a novel spatial correlation analysis","abstract":"The primary formation channel for the stellar-mass Binary Black Holes which have been detected merging by the LIGO-Virgo-KAGRA (LVK) collaboration is yet to be discerned. One of the main obstacles is that such Gravitational Wave (GW) events are not in general expected to produce an Electromagnetic (EM) counterpart. This might not be the case if the mergers happen in gaseous environments, such as the accretion discs of Active Galactic Nuclei (AGN). Recently, 20 AGN flares detected by the Zwicky Transient Facility have been investigated as potential counterparts of GW events by Graham et al. (2023). We present a new spatial correlation analysis involving such events that uses the up-to-date posterior samples of 78 mergers detected during the third observing run of the LVK collaboration. We apply a likelihood method which takes into account the exact position of the EM signal within the 3D sky map of the GW events. We find that current data favour the hypothesis of no causal connection between the detected mergers and the AGN flares. We place an upper limit of 0.155 at a 90 per cent credibility level on the fraction of coalescences that are physically related to a flare. Moreover, we show that the mass distribution of the merging binaries that appear in coincidence with AGN flares, characterised by values typically larger than the ones of the full population of GW events, is also consistent with the no-connection hypothesis. This is because of a positive correlation between the binary mass and the reconstruction volume.","sentences":["The primary formation channel for the stellar-mass Binary Black Holes which have been detected merging by the LIGO-Virgo-KAGRA (LVK) collaboration is yet to be discerned.","One of the main obstacles is that such Gravitational Wave (GW) events are not in general expected to produce an Electromagnetic (EM) counterpart.","This might not be the case if the mergers happen in gaseous environments, such as the accretion discs of Active Galactic Nuclei (AGN).","Recently, 20 AGN flares detected by the Zwicky Transient Facility have been investigated as potential counterparts of GW events by Graham et al. (2023).","We present a new spatial correlation analysis involving such events that uses the up-to-date posterior samples of 78 mergers detected during the third observing run of the LVK collaboration.","We apply a likelihood method which takes into account the exact position of the EM signal within the 3D sky map of the GW events.","We find that current data favour the hypothesis of no causal connection between the detected mergers and the AGN flares.","We place an upper limit of 0.155 at a 90 per cent credibility level on the fraction of coalescences that are physically related to a flare.","Moreover, we show that the mass distribution of the merging binaries that appear in coincidence with AGN flares, characterised by values typically larger than the ones of the full population of GW events, is also consistent with the no-connection hypothesis.","This is because of a positive correlation between the binary mass and the reconstruction volume."],"url":"http://arxiv.org/abs/2405.05318v1","category":"astro-ph.HE"}
{"created":"2024-05-08 18:00:02","title":"The impact of plasma on the relaxation of black holes","abstract":"Our universe is permeated with interstellar plasma, which prevents propagation of low-frequency electromagnetic waves. Here, we show that two dramatic consequences arise out of such suppression; (i) if plasma permeates the light ring of a black hole, electromagnetic modes are screened entirely from the gravitational-wave signal, changing the black hole spectroscopy paradigm; (ii) if a near vacuum cavity is formed close to a charged black hole, as expected for near equal-mass mergers, ringdown \"echoes\" are excited. The amplitude of such echoes decays slowly and could thus serve as a silver bullet for plasmas near charged black holes.","sentences":["Our universe is permeated with interstellar plasma, which prevents propagation of low-frequency electromagnetic waves.","Here, we show that two dramatic consequences arise out of such suppression; (i) if plasma permeates the light ring of a black hole, electromagnetic modes are screened entirely from the gravitational-wave signal, changing the black hole spectroscopy paradigm; (ii) if a near vacuum cavity is formed close to a charged black hole, as expected for near equal-mass mergers, ringdown \"echoes\" are excited.","The amplitude of such echoes decays slowly and could thus serve as a silver bullet for plasmas near charged black holes."],"url":"http://arxiv.org/abs/2405.05315v1","category":"gr-qc"}
{"created":"2024-05-08 18:00:02","title":"Higher Berry Curvature from the Wave Function I: Schmidt Decomposition and Matrix Product States","abstract":"Higher Berry curvature (HBC) is the proposed generalization of Berry curvature to infinitely extended systems. Heuristically HBC captures the flow of local Berry curvature in a system. Here we provide a simple formula for computing the HBC for extended $d = 1$ systems at the level of wave functions using the Schmidt decomposition. We also find a corresponding formula for matrix product states (MPS), and show that for translationally invariant MPS this gives rise to a quantized invariant. We demonstrate our approach with an exactly solvable model and numerical calculations for generic models using iDMRG","sentences":["Higher Berry curvature (HBC) is the proposed generalization of Berry curvature to infinitely extended systems.","Heuristically HBC captures the flow of local Berry curvature in a system.","Here we provide a simple formula for computing the HBC for extended $d = 1$ systems at the level of wave functions using the Schmidt decomposition.","We also find a corresponding formula for matrix product states (MPS), and show that for translationally invariant MPS this gives rise to a quantized invariant.","We demonstrate our approach with an exactly solvable model and numerical calculations for generic models using iDMRG"],"url":"http://arxiv.org/abs/2405.05316v1","category":"cond-mat.str-el"}
