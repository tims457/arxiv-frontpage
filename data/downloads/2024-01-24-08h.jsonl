{"created":"2024-01-23 18:59:59","title":"Zero-Shot Learning for the Primitives of 3D Affordance in General Objects","abstract":"One of the major challenges in AI is teaching machines to precisely respond and utilize environmental functionalities, thereby achieving the affordance awareness that humans possess. Despite its importance, the field has been lagging in terms of learning, especially in 3D, as annotating affordance accompanies a laborious process due to the numerous variations of human-object interaction. The low availability of affordance data limits the learning in terms of generalization for object categories, and also simplifies the representation of affordance, capturing only a fraction of the affordance. To overcome these challenges, we propose a novel, self-supervised method to generate the 3D affordance examples given only a 3D object, without any manual annotations. The method starts by capturing the 3D object into images and creating 2D affordance images by inserting humans into the image via inpainting diffusion models, where we present the Adaptive Mask algorithm to enable human insertion without altering the original details of the object. The method consequently lifts inserted humans back to 3D to create 3D human-object pairs, where the depth ambiguity is resolved within a depth optimization framework that utilizes pre-generated human postures from multiple viewpoints. We also provide a novel affordance representation defined on relative orientations and proximity between dense human and object points, that can be easily aggregated from any 3D HOI datasets. The proposed representation serves as a primitive that can be manifested to conventional affordance representations via simple transformations, ranging from physically exerted affordances to nonphysical ones. We demonstrate the efficacy of our method and representation by generating the 3D affordance samples and deriving high-quality affordance examples from the representation, including contact, orientation, and spatial occupancies.","sentences":["One of the major challenges in AI is teaching machines to precisely respond and utilize environmental functionalities, thereby achieving the affordance awareness that humans possess.","Despite its importance, the field has been lagging in terms of learning, especially in 3D, as annotating affordance accompanies a laborious process due to the numerous variations of human-object interaction.","The low availability of affordance data limits the learning in terms of generalization for object categories, and also simplifies the representation of affordance, capturing only a fraction of the affordance.","To overcome these challenges, we propose a novel, self-supervised method to generate the 3D affordance examples given only a 3D object, without any manual annotations.","The method starts by capturing the 3D object into images and creating 2D affordance images by inserting humans into the image via inpainting diffusion models, where we present the Adaptive Mask algorithm to enable human insertion without altering the original details of the object.","The method consequently lifts inserted humans back to 3D to create 3D human-object pairs, where the depth ambiguity is resolved within a depth optimization framework that utilizes pre-generated human postures from multiple viewpoints.","We also provide a novel affordance representation defined on relative orientations and proximity between dense human and object points, that can be easily aggregated from any 3D HOI datasets.","The proposed representation serves as a primitive that can be manifested to conventional affordance representations via simple transformations, ranging from physically exerted affordances to nonphysical ones.","We demonstrate the efficacy of our method and representation by generating the 3D affordance samples and deriving high-quality affordance examples from the representation, including contact, orientation, and spatial occupancies."],"url":"http://arxiv.org/abs/2401.12978v1","category":"cs.CV"}
{"created":"2024-01-23 18:59:59","title":"GALA: Generating Animatable Layered Assets from a Single Scan","abstract":"We present GALA, a framework that takes as input a single-layer clothed 3D human mesh and decomposes it into complete multi-layered 3D assets. The outputs can then be combined with other assets to create novel clothed human avatars with any pose. Existing reconstruction approaches often treat clothed humans as a single-layer of geometry and overlook the inherent compositionality of humans with hairstyles, clothing, and accessories, thereby limiting the utility of the meshes for downstream applications. Decomposing a single-layer mesh into separate layers is a challenging task because it requires the synthesis of plausible geometry and texture for the severely occluded regions. Moreover, even with successful decomposition, meshes are not normalized in terms of poses and body shapes, failing coherent composition with novel identities and poses. To address these challenges, we propose to leverage the general knowledge of a pretrained 2D diffusion model as geometry and appearance prior for humans and other assets. We first separate the input mesh using the 3D surface segmentation extracted from multi-view 2D segmentations. Then we synthesize the missing geometry of different layers in both posed and canonical spaces using a novel pose-guided Score Distillation Sampling (SDS) loss. Once we complete inpainting high-fidelity 3D geometry, we also apply the same SDS loss to its texture to obtain the complete appearance including the initially occluded regions. Through a series of decomposition steps, we obtain multiple layers of 3D assets in a shared canonical space normalized in terms of poses and human shapes, hence supporting effortless composition to novel identities and reanimation with novel poses. Our experiments demonstrate the effectiveness of our approach for decomposition, canonicalization, and composition tasks compared to existing solutions.","sentences":["We present GALA, a framework that takes as input a single-layer clothed 3D human mesh and decomposes it into complete multi-layered 3D assets.","The outputs can then be combined with other assets to create novel clothed human avatars with any pose.","Existing reconstruction approaches often treat clothed humans as a single-layer of geometry and overlook the inherent compositionality of humans with hairstyles, clothing, and accessories, thereby limiting the utility of the meshes for downstream applications.","Decomposing a single-layer mesh into separate layers is a challenging task because it requires the synthesis of plausible geometry and texture for the severely occluded regions.","Moreover, even with successful decomposition, meshes are not normalized in terms of poses and body shapes, failing coherent composition with novel identities and poses.","To address these challenges, we propose to leverage the general knowledge of a pretrained 2D diffusion model as geometry and appearance prior for humans and other assets.","We first separate the input mesh using the 3D surface segmentation extracted from multi-view 2D segmentations.","Then we synthesize the missing geometry of different layers in both posed and canonical spaces using a novel pose-guided Score Distillation Sampling (SDS) loss.","Once we complete inpainting high-fidelity 3D geometry, we also apply the same SDS loss to its texture to obtain the complete appearance including the initially occluded regions.","Through a series of decomposition steps, we obtain multiple layers of 3D assets in a shared canonical space normalized in terms of poses and human shapes, hence supporting effortless composition to novel identities and reanimation with novel poses.","Our experiments demonstrate the effectiveness of our approach for decomposition, canonicalization, and composition tasks compared to existing solutions."],"url":"http://arxiv.org/abs/2401.12979v1","category":"cs.CV"}
{"created":"2024-01-23 18:59:56","title":"IRIS: Inverse Rendering of Indoor Scenes from Low Dynamic Range Images","abstract":"While numerous 3D reconstruction and novel-view synthesis methods allow for photorealistic rendering of a scene from multi-view images easily captured with consumer cameras, they bake illumination in their representations and fall short of supporting advanced applications like material editing, relighting, and virtual object insertion. The reconstruction of physically based material properties and lighting via inverse rendering promises to enable such applications.   However, most inverse rendering techniques require high dynamic range (HDR) images as input, a setting that is inaccessible to most users. We present a method that recovers the physically based material properties and spatially-varying HDR lighting of a scene from multi-view, low-dynamic-range (LDR) images. We model the LDR image formation process in our inverse rendering pipeline and propose a novel optimization strategy for material, lighting, and a camera response model. We evaluate our approach with synthetic and real scenes compared to the state-of-the-art inverse rendering methods that take either LDR or HDR input. Our method outperforms existing methods taking LDR images as input, and allows for highly realistic relighting and object insertion.","sentences":["While numerous 3D reconstruction and novel-view synthesis methods allow for photorealistic rendering of a scene from multi-view images easily captured with consumer cameras, they bake illumination in their representations and fall short of supporting advanced applications like material editing, relighting, and virtual object insertion.","The reconstruction of physically based material properties and lighting via inverse rendering promises to enable such applications.   ","However, most inverse rendering techniques require high dynamic range (HDR) images as input, a setting that is inaccessible to most users.","We present a method that recovers the physically based material properties and spatially-varying HDR lighting of a scene from multi-view, low-dynamic-range (LDR) images.","We model the LDR image formation process in our inverse rendering pipeline and propose a novel optimization strategy for material, lighting, and a camera response model.","We evaluate our approach with synthetic and real scenes compared to the state-of-the-art inverse rendering methods that take either LDR or HDR input.","Our method outperforms existing methods taking LDR images as input, and allows for highly realistic relighting and object insertion."],"url":"http://arxiv.org/abs/2401.12977v1","category":"cs.CV"}
{"created":"2024-01-23 18:59:43","title":"HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments","abstract":"Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios, including fire, flood, and wind, and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents' decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.","sentences":["Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world.","Typically, these environments remain unchanged unless agents interact with them.","However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly.","To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations.","HAZARD consists of three unexpected disaster scenarios, including fire, flood, and wind, and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making.","This benchmark enables us to evaluate autonomous agents' decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments.","As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks.","HAZARD is available at https://vis-www.cs.umass.edu/hazard/."],"url":"http://arxiv.org/abs/2401.12975v1","category":"cs.CV"}
{"created":"2024-01-23 18:59:21","title":"In-Context Language Learning: Arhitectures and Algorithms","abstract":"Large-scale neural language models exhibit a remarkable capacity for in-context learning (ICL): they can infer novel functions from datasets provided as input. Most of our current understanding of when and how ICL arises comes from LMs trained on extremely simple learning problems like linear regression and associative recall. There remains a significant gap between these model problems and the \"real\" ICL exhibited by LMs trained on large text corpora, which involves not just retrieval and function approximation but free-form generation of language and other structured outputs. In this paper, we study ICL through the lens of a new family of model problems we term in context language learning (ICLL). In ICLL, LMs are presented with a set of strings from a formal language, and must generate additional strings from the same language. We focus on in-context learning of regular languages generated by random finite automata. We evaluate a diverse set of neural sequence models (including several RNNs, Transformers, and state-space model variants) on regular ICLL tasks, aiming to answer three questions: (1) Which model classes are empirically capable of ICLL? (2) What algorithmic solutions do successful models implement to perform ICLL? (3) What architectural changes can improve ICLL in less performant models? We first show that Transformers significantly outperform neural sequence models with recurrent or convolutional representations on ICLL tasks. Next, we provide evidence that their ability to do so relies on specialized \"n-gram heads\" (higher-order variants of induction heads) that compute input-conditional next-token distributions. Finally, we show that hard-wiring these heads into recurrent and convolutional models improves performance not just on ICLL, but natural language modeling -- improving the perplexity of 340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset.","sentences":["Large-scale neural language models exhibit a remarkable capacity for in-context learning (ICL): they can infer novel functions from datasets provided as input.","Most of our current understanding of when and how ICL arises comes from LMs trained on extremely simple learning problems like linear regression and associative recall.","There remains a significant gap between these model problems and the \"real\" ICL exhibited by LMs trained on large text corpora, which involves not just retrieval and function approximation but free-form generation of language and other structured outputs.","In this paper, we study ICL through the lens of a new family of model problems we term in context language learning (ICLL).","In ICLL, LMs are presented with a set of strings from a formal language, and must generate additional strings from the same language.","We focus on in-context learning of regular languages generated by random finite automata.","We evaluate a diverse set of neural sequence models (including several RNNs, Transformers, and state-space model variants) on regular ICLL tasks, aiming to answer three questions: (1) Which model classes are empirically capable of ICLL?","(2) What algorithmic solutions do successful models implement to perform ICLL?","(3) What architectural changes can improve ICLL in less performant models?","We first show that Transformers significantly outperform neural sequence models with recurrent or convolutional representations on ICLL tasks.","Next, we provide evidence that their ability to do so relies on specialized \"n-gram heads\" (higher-order variants of induction heads) that compute input-conditional next-token distributions.","Finally, we show that hard-wiring these heads into recurrent and convolutional models improves performance not just on ICLL, but natural language modeling -- improving the perplexity of 340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset."],"url":"http://arxiv.org/abs/2401.12973v1","category":"cs.CL"}
{"created":"2024-01-23 18:58:35","title":"On the Efficacy of Text-Based Input Modalities for Action Anticipation","abstract":"Although the task of anticipating future actions is highly uncertain, information from additional modalities help to narrow down plausible action choices. Each modality provides different environmental context for the model to learn from. While previous multi-modal methods leverage information from modalities such as video and audio, we primarily explore how text inputs for actions and objects can also enable more accurate action anticipation. Therefore, we propose a Multi-modal Anticipative Transformer (MAT), an attention-based video transformer architecture that jointly learns from multi-modal features and text captions. We train our model in two-stages, where the model first learns to predict actions in the video clip by aligning with captions, and during the second stage, we fine-tune the model to predict future actions. Compared to existing methods, MAT has the advantage of learning additional environmental context from two kinds of text inputs: action descriptions during the pre-training stage, and the text inputs for detected objects and actions during modality feature fusion. Through extensive experiments, we evaluate the effectiveness of the pre-training stage, and show that our model outperforms previous methods on all datasets. In addition, we examine the impact of object and action information obtained via text and perform extensive ablations. We evaluate the performance on on three datasets: EpicKitchens-100, EpicKitchens-55 and EGTEA GAZE+; and show that text descriptions do indeed aid in more effective action anticipation.","sentences":["Although the task of anticipating future actions is highly uncertain, information from additional modalities help to narrow down plausible action choices.","Each modality provides different environmental context for the model to learn from.","While previous multi-modal methods leverage information from modalities such as video and audio, we primarily explore how text inputs for actions and objects can also enable more accurate action anticipation.","Therefore, we propose a Multi-modal Anticipative Transformer (MAT), an attention-based video transformer architecture that jointly learns from multi-modal features and text captions.","We train our model in two-stages, where the model first learns to predict actions in the video clip by aligning with captions, and during the second stage, we fine-tune the model to predict future actions.","Compared to existing methods, MAT has the advantage of learning additional environmental context from two kinds of text inputs: action descriptions during the pre-training stage, and the text inputs for detected objects and actions during modality feature fusion.","Through extensive experiments, we evaluate the effectiveness of the pre-training stage, and show that our model outperforms previous methods on all datasets.","In addition, we examine the impact of object and action information obtained via text and perform extensive ablations.","We evaluate the performance on on three datasets: EpicKitchens-100, EpicKitchens-55 and EGTEA GAZE+; and show that text descriptions do indeed aid in more effective action anticipation."],"url":"http://arxiv.org/abs/2401.12972v1","category":"cs.CV"}
{"created":"2024-01-23 18:57:53","title":"Raidar: geneRative AI Detection viA Rewriting","abstract":"We find that large language models (LLMs) are more likely to modify human-written text than AI-generated text when tasked with rewriting. This tendency arises because LLMs often perceive AI-generated text as high-quality, leading to fewer modifications. We introduce a method to detect AI-generated content by prompting LLMs to rewrite text and calculating the editing distance of the output. We dubbed our geneRative AI Detection viA Rewriting method Raidar. Raidar significantly improves the F1 detection scores of existing AI content detection models -- both academic and commercial -- across various domains, including News, creative writing, student essays, code, Yelp reviews, and arXiv papers, with gains of up to 29 points. Operating solely on word symbols without high-dimensional features, our method is compatible with black box LLMs, and is inherently robust on new content. Our results illustrate the unique imprint of machine-generated text through the lens of the machines themselves.","sentences":["We find that large language models (LLMs) are more likely to modify human-written text than AI-generated text when tasked with rewriting.","This tendency arises because LLMs often perceive AI-generated text as high-quality, leading to fewer modifications.","We introduce a method to detect AI-generated content by prompting LLMs to rewrite text and calculating the editing distance of the output.","We dubbed our geneRative AI Detection viA Rewriting method Raidar.","Raidar significantly improves the F1 detection scores of existing AI content detection models -- both academic and commercial -- across various domains, including News, creative writing, student essays, code, Yelp reviews, and arXiv papers, with gains of up to 29 points.","Operating solely on word symbols without high-dimensional features, our method is compatible with black box LLMs, and is inherently robust on new content.","Our results illustrate the unique imprint of machine-generated text through the lens of the machines themselves."],"url":"http://arxiv.org/abs/2401.12970v1","category":"cs.CL"}
{"created":"2024-01-23 18:53:34","title":"The classical limit of Quantum Max-Cut","abstract":"It is well-known in physics that the limit of large quantum spin $S$ should be understood as a semiclassical limit. This raises the question of whether such emergent classicality facilitates the approximation of computationally hard quantum optimization problems, such as the local Hamiltonian problem. We demonstrate this explicitly for spin-$S$ generalizations of Quantum Max-Cut ($\\mathrm{QMaxCut}_S$), equivalent to the problem of finding the ground state energy of an arbitrary spin-$S$ quantum Heisenberg antiferromagnet ($\\mathrm{AFH}_S$). We prove that approximating the value of $\\mathrm{AFH}_S$ to inverse polynomial accuracy is QMA-complete for all $S$, extending previous results for $S=1/2$. We also present two distinct families of classical approximation algorithms for $\\mathrm{QMaxCut}_S$ based on rounding the output of a semidefinite program to a product of Bloch coherent states. The approximation ratios for both our proposed algorithms strictly increase with $S$ and converge to the Bri\\\"et-Oliveira-Vallentin approximation ratio $\\alpha_{\\mathrm{BOV}} \\approx 0.956$ from below as $S \\to \\infty$.","sentences":["It is well-known in physics that the limit of large quantum spin $S$ should be understood as a semiclassical limit.","This raises the question of whether such emergent classicality facilitates the approximation of computationally hard quantum optimization problems, such as the local Hamiltonian problem.","We demonstrate this explicitly for spin-$S$ generalizations of Quantum Max-Cut ($\\mathrm{QMaxCut}_S$), equivalent to the problem of finding the ground state energy of an arbitrary spin-$S$ quantum Heisenberg antiferromagnet ($\\mathrm{AFH}_S$).","We prove that approximating the value of $\\mathrm{AFH}_S$ to inverse polynomial accuracy is QMA-complete for all $S$, extending previous results for $S=1/2$. We also present two distinct families of classical approximation algorithms for $\\mathrm{QMaxCut}_S$ based on rounding the output of a semidefinite program to a product of Bloch coherent states.","The approximation ratios for both our proposed algorithms strictly increase with $S$ and converge to the Bri\\\"et-Oliveira-Vallentin approximation ratio $\\alpha_{\\mathrm{BOV}} \\approx 0.956$ from below as $S \\to \\infty$."],"url":"http://arxiv.org/abs/2401.12968v1","category":"quant-ph"}
{"created":"2024-01-23 18:50:59","title":"Workspace Optimization Techniques to Improve Prediction of Human Motion During Human-Robot Collaboration","abstract":"Understanding human intentions is critical for safe and effective human-robot collaboration. While state of the art methods for human goal prediction utilize learned models to account for the uncertainty of human motion data, that data is inherently stochastic and high variance, hindering those models' utility for interactions requiring coordination, including safety-critical or close-proximity tasks. Our key insight is that robot teammates can deliberately configure shared workspaces prior to interaction in order to reduce the variance in human motion, realizing classifier-agnostic improvements in goal prediction. In this work, we present an algorithmic approach for a robot to arrange physical objects and project \"virtual obstacles\" using augmented reality in shared human-robot workspaces, optimizing for human legibility over a given set of tasks. We compare our approach against other workspace arrangement strategies using two human-subjects studies, one in a virtual 2D navigation domain and the other in a live tabletop manipulation domain involving a robotic manipulator arm. We evaluate the accuracy of human motion prediction models learned from each condition, demonstrating that our workspace optimization technique with virtual obstacles leads to higher robot prediction accuracy using less training data.","sentences":["Understanding human intentions is critical for safe and effective human-robot collaboration.","While state of the art methods for human goal prediction utilize learned models to account for the uncertainty of human motion data, that data is inherently stochastic and high variance, hindering those models' utility for interactions requiring coordination, including safety-critical or close-proximity tasks.","Our key insight is that robot teammates can deliberately configure shared workspaces prior to interaction in order to reduce the variance in human motion, realizing classifier-agnostic improvements in goal prediction.","In this work, we present an algorithmic approach for a robot to arrange physical objects and project \"virtual obstacles\" using augmented reality in shared human-robot workspaces, optimizing for human legibility over a given set of tasks.","We compare our approach against other workspace arrangement strategies using two human-subjects studies, one in a virtual 2D navigation domain and the other in a live tabletop manipulation domain involving a robotic manipulator arm.","We evaluate the accuracy of human motion prediction models learned from each condition, demonstrating that our workspace optimization technique with virtual obstacles leads to higher robot prediction accuracy using less training data."],"url":"http://arxiv.org/abs/2401.12965v1","category":"cs.RO"}
{"created":"2024-01-23 18:45:54","title":"AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents","abstract":"Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies. We experimentally show that such \"in-the-wild\" data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences.","sentences":["Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks.","However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world.","In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision.","AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots.","Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning.","We demonstrate AutoRT proposing instructions to over 20 robots across multiple buildings and collecting 77k real robot episodes via both teleoperation and autonomous robot policies.","We experimentally show that such \"in-the-wild\" data collected by AutoRT is significantly more diverse, and that AutoRT's use of LLMs allows for instruction following data collection robots that can align to human preferences."],"url":"http://arxiv.org/abs/2401.12963v1","category":"cs.RO"}
{"created":"2024-01-23 18:45:47","title":"Minimizing the Age of Two Heterogeneous Sources With Packet Drops Via Cyclic Schedulers","abstract":"In a communication setting where multiple sources share a single channel to provide status updates to a remote monitor, source transmissions need to be scheduled appropriately to maintain timely communication between each of the sources and the monitor. We consider age-agnostic scheduling policies which are advantageous due to their simplicity of implementation. Further, we focus on a special class of age-agnostic policies, called cyclic schedulers, where each source is scheduled based on a fixed cyclic pattern. We use weighted average age of information (AoI) to quantify the timeliness of communication. We develop a Markov chain formulation to compute the exact mean AoI for the case of two-source cyclic schedulers. Based on the obtained age expression, we develop an algorithm that generates near-optimal cyclic schedulers to minimize the weighted average AoI for two heterogeneous sources, in the presence of channel errors.","sentences":["In a communication setting where multiple sources share a single channel to provide status updates to a remote monitor, source transmissions need to be scheduled appropriately to maintain timely communication between each of the sources and the monitor.","We consider age-agnostic scheduling policies which are advantageous due to their simplicity of implementation.","Further, we focus on a special class of age-agnostic policies, called cyclic schedulers, where each source is scheduled based on a fixed cyclic pattern.","We use weighted average age of information (AoI) to quantify the timeliness of communication.","We develop a Markov chain formulation to compute the exact mean AoI for the case of two-source cyclic schedulers.","Based on the obtained age expression, we develop an algorithm that generates near-optimal cyclic schedulers to minimize the weighted average AoI for two heterogeneous sources, in the presence of channel errors."],"url":"http://arxiv.org/abs/2401.12962v1","category":"cs.IT"}
{"created":"2024-01-23 18:45:27","title":"Chatterbox: Robust Transport for LLM Token Streaming under Unstable Network","abstract":"To render each generated token in real time, the LLM server generates response tokens one by one and streams each generated token (or group of a few tokens) through the network to the user right after it is generated, which we refer to as LLM token streaming. However, under unstable network conditions, the LLM token streaming experience could suffer greatly from stalls since one packet loss could block the rendering of tokens contained in subsequent packets even if they arrive on time. With a real-world measurement study, we show that current applications including ChatGPT, Claude, and Bard all suffer from increased stall under unstable network.   For this emerging token streaming problem in LLM Chatbots, we propose a novel transport layer scheme, called Chatterbox, which puts new generated tokens as well as currently unacknowledged tokens in the next outgoing packet. This ensures that each packet contains some new tokens and can be independently rendered when received, thus avoiding aforementioned stalls caused by missing packets. Through simulation under various network conditions, we show Chatterbox reduces stall ratio (proportion of token rendering wait time) by 71.0% compared to the token streaming method commonly used by real chatbot applications and by 31.6% compared to a custom packet duplication scheme. By tailoring Chatterbox to fit the token-by-token generation of LLM, we enable the Chatbots to respond like an eloquent speaker for users to better enjoy pervasive AI.","sentences":["To render each generated token in real time, the LLM server generates response tokens one by one and streams each generated token (or group of a few tokens) through the network to the user right after it is generated, which we refer to as LLM token streaming.","However, under unstable network conditions, the LLM token streaming experience could suffer greatly from stalls since one packet loss could block the rendering of tokens contained in subsequent packets even if they arrive on time.","With a real-world measurement study, we show that current applications including ChatGPT, Claude, and Bard all suffer from increased stall under unstable network.   ","For this emerging token streaming problem in LLM Chatbots, we propose a novel transport layer scheme, called Chatterbox, which puts new generated tokens as well as currently unacknowledged tokens in the next outgoing packet.","This ensures that each packet contains some new tokens and can be independently rendered when received, thus avoiding aforementioned stalls caused by missing packets.","Through simulation under various network conditions, we show Chatterbox reduces stall ratio (proportion of token rendering wait time) by 71.0% compared to the token streaming method commonly used by real chatbot applications and by 31.6% compared to a custom packet duplication scheme.","By tailoring Chatterbox to fit the token-by-token generation of LLM, we enable the Chatbots to respond like an eloquent speaker for users to better enjoy pervasive AI."],"url":"http://arxiv.org/abs/2401.12961v1","category":"cs.NI"}
{"created":"2024-01-23 18:45:17","title":"Framework for the Quantum Mechanical Sum of Possibilities and Meaning for Field Theory and Gravity","abstract":"In quantum mechanics, the measureable quantities of a given theory are predicted by performing a weighted sum over possibilities. We show how to arrange the possibilities into bundles such that the associated subsums can be viewed as well-defined theories on their own right. These bundles are submani$\\textit{folds}$ of $\\textit{possi}$bilities which we call possifolds. We collect and prove some basic facts about possifolds. Especially, we show that possifolds are ensembles of what in a certain broadly defined sense that we explain can be regarded as soliton excitations (soliton-possifold correspondence). We provide an outlook on some applications. Among other things, we illustrate the use of the developed framework for the example of the Lieb-Liniger model. It describes non-relativistic bosons with an attractive interaction. We derive a dual theory describing the lowest-lying energy excitation modes. While the standard Bogoliubov-approximation breaks down at the critical point, our derived summation prescription stays regular. In the Bogoliubov-limit we observe the summation to possess an enhanced symmetry at this point while the summation cannot be ignored there. We finally provide a glimpse on the restrictions black hole physics implies in this context for the gravitational path integral.","sentences":["In quantum mechanics, the measureable quantities of a given theory are predicted by performing a weighted sum over possibilities.","We show how to arrange the possibilities into bundles such that the associated subsums can be viewed as well-defined theories on their own right.","These bundles are submani$\\textit{folds}$ of $\\textit{possi}$bilities which we call possifolds.","We collect and prove some basic facts about possifolds.","Especially, we show that possifolds are ensembles of what in a certain broadly defined sense that we explain can be regarded as soliton excitations (soliton-possifold correspondence).","We provide an outlook on some applications.","Among other things, we illustrate the use of the developed framework for the example of the Lieb-Liniger model.","It describes non-relativistic bosons with an attractive interaction.","We derive a dual theory describing the lowest-lying energy excitation modes.","While the standard Bogoliubov-approximation breaks down at the critical point, our derived summation prescription stays regular.","In the Bogoliubov-limit we observe the summation to possess an enhanced symmetry at this point while the summation cannot be ignored there.","We finally provide a glimpse on the restrictions black hole physics implies in this context for the gravitational path integral."],"url":"http://arxiv.org/abs/2401.12960v1","category":"hep-th"}
{"created":"2024-01-23 18:45:11","title":"Understanding Emojis :) in Useful Code Review Comments","abstract":"Emojis and emoticons serve as non-verbal cues and are increasingly prevalent across various platforms, including Modern Code Review. These cues often carry emotive or instructive weight for developers. Our study dives into the utility of Code Review comments (CR comments) by scrutinizing the sentiments and semantics conveyed by emojis within these comments. To assess the usefulness of CR comments, we augment traditional 'textual' features and pre-trained embeddings with 'emoji-specific' features and pre-trained embeddings. To fortify our inquiry, we expand an existing dataset with emoji annotations, guided by existing research on GitHub emoji usage, and re-evaluate the CR comments accordingly. Our models, which incorporate textual and emoji-based sentiment features and semantic understandings of emojis, substantially outperform baseline metrics. The often-overlooked emoji elements in CR comments emerge as key indicators of usefulness, suggesting that these symbols carry significant weight.","sentences":["Emojis and emoticons serve as non-verbal cues and are increasingly prevalent across various platforms, including Modern Code Review.","These cues often carry emotive or instructive weight for developers.","Our study dives into the utility of Code Review comments (CR comments) by scrutinizing the sentiments and semantics conveyed by emojis within these comments.","To assess the usefulness of CR comments, we augment traditional 'textual' features and pre-trained embeddings with 'emoji-specific' features and pre-trained embeddings.","To fortify our inquiry, we expand an existing dataset with emoji annotations, guided by existing research on GitHub emoji usage, and re-evaluate the CR comments accordingly.","Our models, which incorporate textual and emoji-based sentiment features and semantic understandings of emojis, substantially outperform baseline metrics.","The often-overlooked emoji elements in CR comments emerge as key indicators of usefulness, suggesting that these symbols carry significant weight."],"url":"http://arxiv.org/abs/2401.12959v1","category":"cs.SE"}
{"created":"2024-01-23 18:37:12","title":"Non-Gaussianity consistency relations and their consequences for the peaks","abstract":"Strong deviations from scale invariance and the appearance of high peaks in the primordial power spectrum have been extensively studied for generating primordial black holes (PBHs) or gravitational waves (GWs). It is also well-known that the effect of non-linearities can be significant in both phenomena. In this paper, we advocate the existence of a general single-field consistency relation that relates the amplitude of non-Gaussianity in the squeezed limit $f_{\\text{NL}}$ to the power spectrum and remains valid when almost all other consistency relations are violated. In particular, it is suitable for studying scenarios where scale invariance is strongly violated. We discuss the general and model-independent consequences of the consistency relation on the behavior of $f_{\\text{NL}}$ at different scales. Specifically, we study the size, sign and slope of $f_{\\text{NL}}$ at the scales where the power spectrum peaks and argue that generally the peaks of $f_{\\text{NL}}$ and the power spectrum occur at different scales. As an implication of our results, we argue that non-linearities can shift or extend the range of scales responsible for the production of PBHs or GWs, relative to the window as determined by the largest peak of the power spectrum, and may also open up new windows for both phenomena.","sentences":["Strong deviations from scale invariance and the appearance of high peaks in the primordial power spectrum have been extensively studied for generating primordial black holes (PBHs) or gravitational waves (GWs).","It is also well-known that the effect of non-linearities can be significant in both phenomena.","In this paper, we advocate the existence of a general single-field consistency relation that relates the amplitude of non-Gaussianity in the squeezed limit $f_{\\text{NL}}$ to the power spectrum and remains valid when almost all other consistency relations are violated.","In particular, it is suitable for studying scenarios where scale invariance is strongly violated.","We discuss the general and model-independent consequences of the consistency relation on the behavior of $f_{\\text{NL}}$ at different scales.","Specifically, we study the size, sign and slope of $f_{\\text{NL}}$ at the scales where the power spectrum peaks and argue that generally the peaks of $f_{\\text{NL}}$ and the power spectrum occur at different scales.","As an implication of our results, we argue that non-linearities can shift or extend the range of scales responsible for the production of PBHs or GWs, relative to the window as determined by the largest peak of the power spectrum, and may also open up new windows for both phenomena."],"url":"http://arxiv.org/abs/2401.12958v1","category":"astro-ph.CO"}
{"created":"2024-01-23 18:32:15","title":"Symmetry Duality: Exploring Exotic Oscillators And Dissipative Dynamics Through The Glass Of Newton-Hooke","abstract":"Motivated by the symmetry in the non-relativistic limit of anti-de Sitter geometry, we employ planar dynamical models featuring exotic (deformed) harmonic oscillators, presented through direct and indirect Lagrangian representations. The latter introduces Bateman dissipative oscillator system. Analyzing these dynamic systems with a first-order Lagrangian scheme, our phase-space-based approach utilizes the moment map components to reveal the underlying symmetry algebra. This obtained algebra, interpreted as an extended version of Newton-Hooke (NH) cosmological symmetry algebras, has the potential to cast an augmented non-relativistic shadow over the expanding universe, offering an insightful perspective on extended NH spacetime in 2+1 dimensions through our dynamical realizations.","sentences":["Motivated by the symmetry in the non-relativistic limit of anti-de Sitter geometry, we employ planar dynamical models featuring exotic (deformed) harmonic oscillators, presented through direct and indirect Lagrangian representations.","The latter introduces Bateman dissipative oscillator system.","Analyzing these dynamic systems with a first-order Lagrangian scheme, our phase-space-based approach utilizes the moment map components to reveal the underlying symmetry algebra.","This obtained algebra, interpreted as an extended version of Newton-Hooke (NH) cosmological symmetry algebras, has the potential to cast an augmented non-relativistic shadow over the expanding universe, offering an insightful perspective on extended NH spacetime in 2+1 dimensions through our dynamical realizations."],"url":"http://arxiv.org/abs/2401.12957v1","category":"hep-th"}
{"created":"2024-01-23 18:28:07","title":"Examining the Role of Peer Acknowledgements on Social Annotations: Unraveling the Psychological Underpinnings","abstract":"This study explores the impact of peer acknowledgement on learner engagement and implicit psychological attributes in written annotations on an online social reading platform. Participants included 91 undergraduates from a large North American University. Using log file data, we analyzed the relationship between learners' received peer acknowledgement and their subsequent annotation behaviours using cross-lag regression. Higher peer acknowledgements correlate with increased initiation of annotations and responses to peer annotations. By applying text mining techniques and calculating Shapley values to analyze 1,969 social annotation entries, we identified prominent psychological themes within three dimensions (i.e., affect, cognition, and motivation) that foster peer acknowledgment in digital social annotation. These themes include positive affect, openness to learning and discussion, and expression of motivation. The findings assist educators in improving online learning communities and provide guidance to technology developers in designing effective prompts, drawing from both implicit psychological cues and explicit learning behaviours.","sentences":["This study explores the impact of peer acknowledgement on learner engagement and implicit psychological attributes in written annotations on an online social reading platform.","Participants included 91 undergraduates from a large North American University.","Using log file data, we analyzed the relationship between learners' received peer acknowledgement and their subsequent annotation behaviours using cross-lag regression.","Higher peer acknowledgements correlate with increased initiation of annotations and responses to peer annotations.","By applying text mining techniques and calculating Shapley values to analyze 1,969 social annotation entries, we identified prominent psychological themes within three dimensions (i.e., affect, cognition, and motivation) that foster peer acknowledgment in digital social annotation.","These themes include positive affect, openness to learning and discussion, and expression of motivation.","The findings assist educators in improving online learning communities and provide guidance to technology developers in designing effective prompts, drawing from both implicit psychological cues and explicit learning behaviours."],"url":"http://arxiv.org/abs/2401.12956v1","category":"cs.HC"}
{"created":"2024-01-23 18:22:19","title":"Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding","abstract":"We introduce meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to break down complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct \"expert\" instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly enhancing its performance across a wide array of tasks. The zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction by obviating the need for detailed, task-specific instructions. Furthermore, our research demonstrates the seamless integration of external tools, such as a Python interpreter, into the meta-prompting framework, thereby broadening its applicability and utility. Through rigorous experimentation with GPT-4, we establish the superiority of meta-prompting over conventional scaffolding methods: When averaged across all tasks, including the Game of 24, Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented with a Python interpreter functionality, surpasses standard prompting by 17.1%, expert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.","sentences":["We introduce meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs).","This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries.","By employing high-level instructions, meta-prompting guides the LM to break down complex tasks into smaller, more manageable subtasks.","These subtasks are then handled by distinct \"expert\" instances of the same LM, each operating under specific, tailored instructions.","Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models.","It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result.","This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly enhancing its performance across a wide array of tasks.","The zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction by obviating the need for detailed, task-specific instructions.","Furthermore, our research demonstrates the seamless integration of external tools, such as a Python interpreter, into the meta-prompting framework, thereby broadening its applicability and utility.","Through rigorous experimentation with GPT-4, we establish the superiority of meta-prompting over conventional scaffolding methods: When averaged across all tasks, including the Game of 24, Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented with a Python interpreter functionality, surpasses standard prompting by 17.1%, expert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%."],"url":"http://arxiv.org/abs/2401.12954v1","category":"cs.CL"}
{"created":"2024-01-23 18:15:58","title":"Bayesian Semi-structured Subspace Inference","abstract":"Semi-structured regression models enable the joint modeling of interpretable structured and complex unstructured feature effects. The structured model part is inspired by statistical models and can be used to infer the input-output relationship for features of particular importance. The complex unstructured part defines an arbitrary deep neural network and thereby provides enough flexibility to achieve competitive prediction performance. While these models can also account for aleatoric uncertainty, there is still a lack of work on accounting for epistemic uncertainty. In this paper, we address this problem by presenting a Bayesian approximation for semi-structured regression models using subspace inference. To this end, we extend subspace inference for joint posterior sampling from a full parameter space for structured effects and a subspace for unstructured effects. Apart from this hybrid sampling scheme, our method allows for tunable complexity of the subspace and can capture multiple minima in the loss landscape. Numerical experiments validate our approach's efficacy in recovering structured effect parameter posteriors in semi-structured models and approaching the full-space posterior distribution of MCMC for increasing subspace dimension. Further, our approach exhibits competitive predictive performance across simulated and real-world datasets.","sentences":["Semi-structured regression models enable the joint modeling of interpretable structured and complex unstructured feature effects.","The structured model part is inspired by statistical models and can be used to infer the input-output relationship for features of particular importance.","The complex unstructured part defines an arbitrary deep neural network and thereby provides enough flexibility to achieve competitive prediction performance.","While these models can also account for aleatoric uncertainty, there is still a lack of work on accounting for epistemic uncertainty.","In this paper, we address this problem by presenting a Bayesian approximation for semi-structured regression models using subspace inference.","To this end, we extend subspace inference for joint posterior sampling from a full parameter space for structured effects and a subspace for unstructured effects.","Apart from this hybrid sampling scheme, our method allows for tunable complexity of the subspace and can capture multiple minima in the loss landscape.","Numerical experiments validate our approach's efficacy in recovering structured effect parameter posteriors in semi-structured models and approaching the full-space posterior distribution of MCMC for increasing subspace dimension.","Further, our approach exhibits competitive predictive performance across simulated and real-world datasets."],"url":"http://arxiv.org/abs/2401.12950v1","category":"cs.LG"}
{"created":"2024-01-23 18:07:38","title":"Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion","abstract":"This paper investigates the ability of transformer-based models to learn structural recursion from examples. Recursion is a universal concept in both natural and formal languages. Structural recursion is central to the programming language and formal mathematics tasks where symbolic tools currently excel beyond neural models, such as inferring semantic relations between datatypes and emulating program behavior. We introduce a general framework that nicely connects the abstract concepts of structural recursion in the programming language domain to concrete sequence modeling problems and learned models' behavior. The framework includes a representation that captures the general \\textit{syntax} of structural recursion, coupled with two different frameworks for understanding their \\textit{semantics} -- one that is more natural from a programming languages perspective and one that helps bridge that perspective with a mechanistic understanding of the underlying transformer architecture.   With our framework as a powerful conceptual tool, we identify different issues under various set-ups. The models trained to emulate recursive computations cannot fully capture the recursion yet instead fit short-cut algorithms and thus cannot solve certain edge cases that are under-represented in the training distribution. In addition, it is difficult for state-of-the-art large language models (LLMs) to mine recursive rules from in-context demonstrations. Meanwhile, these LLMs fail in interesting ways when emulating reduction (step-wise computation) of the recursive function.","sentences":["This paper investigates the ability of transformer-based models to learn structural recursion from examples.","Recursion is a universal concept in both natural and formal languages.","Structural recursion is central to the programming language and formal mathematics tasks where symbolic tools currently excel beyond neural models, such as inferring semantic relations between datatypes and emulating program behavior.","We introduce a general framework that nicely connects the abstract concepts of structural recursion in the programming language domain to concrete sequence modeling problems and learned models' behavior.","The framework includes a representation that captures the general \\textit{syntax} of structural recursion, coupled with two different frameworks for understanding their \\textit{semantics} -- one that is more natural from a programming languages perspective and one that helps bridge that perspective with a mechanistic understanding of the underlying transformer architecture.   ","With our framework as a powerful conceptual tool, we identify different issues under various set-ups.","The models trained to emulate recursive computations cannot fully capture the recursion yet instead fit short-cut algorithms and thus cannot solve certain edge cases that are under-represented in the training distribution.","In addition, it is difficult for state-of-the-art large language models (LLMs) to mine recursive rules from in-context demonstrations.","Meanwhile, these LLMs fail in interesting ways when emulating reduction (step-wise computation) of the recursive function."],"url":"http://arxiv.org/abs/2401.12947v1","category":"cs.CL"}
{"created":"2024-01-23 18:07:07","title":"Coverage Axis++: Efficient Inner Point Selection for 3D Shape Skeletonization","abstract":"We introduce Coverage Axis++, a novel and efficient approach to 3D shape skeletonization. The current state-of-the-art approaches for this task often rely on the watertightness of the input or suffer from substantial computational costs, thereby limiting their practicality. To address this challenge, Coverage Axis++ proposes a heuristic algorithm to select skeletal points, offering a high-accuracy approximation of the Medial Axis Transform (MAT) while significantly mitigating computational intensity for various shape representations. We introduce a simple yet effective strategy that considers both shape coverage and uniformity to derive skeletal points. The selection procedure enforces consistency with the shape structure while favoring the dominant medial balls, which thus introduces a compact underlying shape representation in terms of MAT. As a result, Coverage Axis++ allows for skeletonization for various shape representations (e.g., water-tight meshes, triangle soups, point clouds), specification of the number of skeletal points, few hyperparameters, and highly efficient computation with improved reconstruction accuracy. Extensive experiments across a wide range of 3D shapes validate the efficiency and effectiveness of Coverage Axis++. The code will be publicly available once the paper is published.","sentences":["We introduce Coverage Axis++, a novel and efficient approach to 3D shape skeletonization.","The current state-of-the-art approaches for this task often rely on the watertightness of the input or suffer from substantial computational costs, thereby limiting their practicality.","To address this challenge, Coverage Axis++ proposes a heuristic algorithm to select skeletal points, offering a high-accuracy approximation of the Medial Axis Transform (MAT) while significantly mitigating computational intensity for various shape representations.","We introduce a simple yet effective strategy that considers both shape coverage and uniformity to derive skeletal points.","The selection procedure enforces consistency with the shape structure while favoring the dominant medial balls, which thus introduces a compact underlying shape representation in terms of MAT.","As a result, Coverage Axis++ allows for skeletonization for various shape representations (e.g., water-tight meshes, triangle soups, point clouds), specification of the number of skeletal points, few hyperparameters, and highly efficient computation with improved reconstruction accuracy.","Extensive experiments across a wide range of 3D shapes validate the efficiency and effectiveness of Coverage Axis++.","The code will be publicly available once the paper is published."],"url":"http://arxiv.org/abs/2401.12946v1","category":"cs.CV"}
{"created":"2024-01-23 18:05:25","title":"Lumiere: A Space-Time Diffusion Model for Video Generation","abstract":"We introduce Lumiere -- a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion -- a pivotal challenge in video synthesis. To this end, we introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution -- an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, our model learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. We demonstrate state-of-the-art text-to-video generation results, and show that our design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation.","sentences":["We introduce Lumiere -- a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion -- a pivotal challenge in video synthesis.","To this end, we introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model.","This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution -- an approach that inherently makes global temporal consistency difficult to achieve.","By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, our model learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales.","We demonstrate state-of-the-art text-to-video generation results, and show that our design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation."],"url":"http://arxiv.org/abs/2401.12945v1","category":"cs.CV"}
{"created":"2024-01-23 17:58:55","title":"Nonlinear dynamics in neuromorphic photonic networks: physical simulation in Verilog-A","abstract":"Advances in silicon photonics technology have enabled the field of neuromorphic photonics, where analog neuron-like processing elements are implemented in silicon photonics technology. Accurate and scalable simulation tools for photonic integrated circuits are critical for designing neuromorphic photonic circuits. This is especially important when designing networks with recurrent connections, where the dynamics of the system may give rise to unstable and oscillatory solutions which need to be accurately modelled. These tools must simultaneously simulate the analog electronics and the multi-channel (wavelength-division-multiplexed) photonics contained in a photonic neuron to accurately predict on-chip behaviour. In this paper, we utilize a Verilog-A model of the photonic neural network to investigate the dynamics of recurrent integrated circuits. We begin by reviewing the theory of continuous-time recurrent neural networks as dynamical systems and the relation of these dynamics to important physical features of photonic neurons such as cascadability. We then present the neural dynamics of systems of one and two neurons in the simulated Verilog-A circuit, which are compared to the expected dynamics of the abstract CTRNN model. Due to the presence of parasitic circuit elements in the Verilog-A simulation, it is seen that there is a topological equivalence, but not an exact isomorphism, between the theoretical model and the simulated model. The implications of these discrepancies for the design of neuromorphic photonic circuits are discussed. Our findings pave the way for the practical implementation of large-scale silicon photonic recurrent neural networks.","sentences":["Advances in silicon photonics technology have enabled the field of neuromorphic photonics, where analog neuron-like processing elements are implemented in silicon photonics technology.","Accurate and scalable simulation tools for photonic integrated circuits are critical for designing neuromorphic photonic circuits.","This is especially important when designing networks with recurrent connections, where the dynamics of the system may give rise to unstable and oscillatory solutions which need to be accurately modelled.","These tools must simultaneously simulate the analog electronics and the multi-channel (wavelength-division-multiplexed) photonics contained in a photonic neuron to accurately predict on-chip behaviour.","In this paper, we utilize a Verilog-A model of the photonic neural network to investigate the dynamics of recurrent integrated circuits.","We begin by reviewing the theory of continuous-time recurrent neural networks as dynamical systems and the relation of these dynamics to important physical features of photonic neurons such as cascadability.","We then present the neural dynamics of systems of one and two neurons in the simulated Verilog-A circuit, which are compared to the expected dynamics of the abstract CTRNN model.","Due to the presence of parasitic circuit elements in the Verilog-A simulation, it is seen that there is a topological equivalence, but not an exact isomorphism, between the theoretical model and the simulated model.","The implications of these discrepancies for the design of neuromorphic photonic circuits are discussed.","Our findings pave the way for the practical implementation of large-scale silicon photonic recurrent neural networks."],"url":"http://arxiv.org/abs/2401.12942v1","category":"cs.ET"}
{"created":"2024-01-23 17:58:38","title":"Multicultural Name Recognition For Previously Unseen Names","abstract":"State of the art Named Entity Recognition (NER) models have achieved an impressive ability to extract common phrases from text that belong to labels such as location, organization, time, and person. However, typical NER systems that rely on having seen a specific entity in their training data in order to label an entity perform poorly on rare or unseen entities ta in order to label an entity perform poorly on rare or unseen entities (Derczynski et al., 2017). This paper attempts to improve recognition of person names, a diverse category that can grow any time someone is born or changes their name. In order for downstream tasks to not exhibit bias based on cultural background, a model should perform well on names from a variety of backgrounds. In this paper I experiment with the training data and input structure of an English Bi-LSTM name recognition model. I look at names from 103 countries to compare how well the model performs on names from different cultures, specifically in the context of a downstream task where extracted names will be matched to information on file. I find that a model with combined character and word input outperforms word-only models and may improve on accuracy compared to classical NER models that are not geared toward identifying unseen entity values.","sentences":["State of the art Named Entity Recognition (NER) models have achieved an impressive ability to extract common phrases from text that belong to labels such as location, organization, time, and person.","However, typical NER systems that rely on having seen a specific entity in their training data in order to label an entity perform poorly on rare or unseen entities ta in order to label an entity perform poorly on rare or unseen entities (Derczynski et al., 2017).","This paper attempts to improve recognition of person names, a diverse category that can grow any time someone is born or changes their name.","In order for downstream tasks to not exhibit bias based on cultural background, a model should perform well on names from a variety of backgrounds.","In this paper I experiment with the training data and input structure of an English Bi-LSTM name recognition model.","I look at names from 103 countries to compare how well the model performs on names from different cultures, specifically in the context of a downstream task where extracted names will be matched to information on file.","I find that a model with combined character and word input outperforms word-only models and may improve on accuracy compared to classical NER models that are not geared toward identifying unseen entity values."],"url":"http://arxiv.org/abs/2401.12941v1","category":"cs.CL"}
{"created":"2024-01-23 17:56:38","title":"Testing space-time non-commutativity with TianQin","abstract":"The direct detection of gravitational waves offers a powerful tool to explore the nature of gravity and the structure of space-time. This paper focuses on the capabilities of space-based gravitational wave detectors in testing space-time non-commutativity. Our findings indicate that TianQin has the potential to impose constraints on the non-commutative scale at a sub-Planckian level using massive black hole binaries. Additionally, we have developed a pipeline tailored to this specific topic.","sentences":["The direct detection of gravitational waves offers a powerful tool to explore the nature of gravity and the structure of space-time.","This paper focuses on the capabilities of space-based gravitational wave detectors in testing space-time non-commutativity.","Our findings indicate that TianQin has the potential to impose constraints on the non-commutative scale at a sub-Planckian level using massive black hole binaries.","Additionally, we have developed a pipeline tailored to this specific topic."],"url":"http://arxiv.org/abs/2401.12940v1","category":"gr-qc"}
{"created":"2024-01-23 17:42:17","title":"Reward-Relevance-Filtered Linear Offline Reinforcement Learning","abstract":"This paper studies offline reinforcement learning with linear function approximation in a setting with decision-theoretic, but not estimation sparsity. The structural restrictions of the data-generating process presume that the transitions factor into a sparse component that affects the reward and could affect additional exogenous dynamics that do not affect the reward. Although the minimally sufficient adjustment set for estimation of full-state transition properties depends on the whole state, the optimal policy and therefore state-action value function depends only on the sparse component: we call this causal/decision-theoretic sparsity. We develop a method for reward-filtering the estimation of the state-action value function to the sparse component by a modification of thresholded lasso in least-squares policy evaluation. We provide theoretical guarantees for our reward-filtered linear fitted-Q-iteration, with sample complexity depending only on the size of the sparse component.","sentences":["This paper studies offline reinforcement learning with linear function approximation in a setting with decision-theoretic, but not estimation sparsity.","The structural restrictions of the data-generating process presume that the transitions factor into a sparse component that affects the reward and could affect additional exogenous dynamics that do not affect the reward.","Although the minimally sufficient adjustment set for estimation of full-state transition properties depends on the whole state, the optimal policy and therefore state-action value function depends only on the sparse component: we call this causal/decision-theoretic sparsity.","We develop a method for reward-filtering the estimation of the state-action value function to the sparse component by a modification of thresholded lasso in least-squares policy evaluation.","We provide theoretical guarantees for our reward-filtered linear fitted-Q-iteration, with sample complexity depending only on the size of the sparse component."],"url":"http://arxiv.org/abs/2401.12934v1","category":"stat.ML"}
{"created":"2024-01-23 17:33:41","title":"pyAKI - An Open Source Solution to Automated KDIGO classification","abstract":"Acute Kidney Injury (AKI) is a frequent complication in critically ill patients, affecting up to 50% of patients in the intensive care units. The lack of standardized and open-source tools for applying the Kidney Disease Improving Global Outcomes (KDIGO) criteria to time series data has a negative impact on workload and study quality. This project introduces pyAKI, an open-source pipeline addressing this gap by providing a comprehensive solution for consistent KDIGO criteria implementation.   The pyAKI pipeline was developed and validated using a subset of the Medical Information Mart for Intensive Care (MIMIC)-IV database, a commonly used database in critical care research. We defined a standardized data model in order to ensure reproducibility. Validation against expert annotations demonstrated pyAKI's robust performance in implementing KDIGO criteria. Comparative analysis revealed its ability to surpass the quality of human labels.   This work introduces pyAKI as an open-source solution for implementing the KDIGO criteria for AKI diagnosis using time series data with high accuracy and performance.","sentences":["Acute Kidney Injury (AKI) is a frequent complication in critically ill patients, affecting up to 50% of patients in the intensive care units.","The lack of standardized and open-source tools for applying the Kidney Disease Improving Global Outcomes (KDIGO) criteria to time series data has a negative impact on workload and study quality.","This project introduces pyAKI, an open-source pipeline addressing this gap by providing a comprehensive solution for consistent KDIGO criteria implementation.   ","The pyAKI pipeline was developed and validated using a subset of the Medical Information Mart for Intensive Care (MIMIC)-IV database, a commonly used database in critical care research.","We defined a standardized data model in order to ensure reproducibility.","Validation against expert annotations demonstrated pyAKI's robust performance in implementing KDIGO criteria.","Comparative analysis revealed its ability to surpass the quality of human labels.   ","This work introduces pyAKI as an open-source solution for implementing the KDIGO criteria for AKI diagnosis using time series data with high accuracy and performance."],"url":"http://arxiv.org/abs/2401.12930v1","category":"cs.LG"}
{"created":"2024-01-23 17:23:58","title":"The fiber bundle structure of General Relativity in Ashtekar variables","abstract":"In this review, we aim to analyze the mathematical interpretation of the Ashtekar-Barbero-Immirzi formulation of General Relativity. Along with a brief introduction to the necessary mathematical structures and tools, we illustrate some relevant physical theory quantities as geometrical objects within the framework of principal bundle theory.","sentences":["In this review, we aim to analyze the mathematical interpretation of the Ashtekar-Barbero-Immirzi formulation of General Relativity.","Along with a brief introduction to the necessary mathematical structures and tools, we illustrate some relevant physical theory quantities as geometrical objects within the framework of principal bundle theory."],"url":"http://arxiv.org/abs/2401.12927v1","category":"gr-qc"}
{"created":"2024-01-23 17:22:00","title":"DsDm: Model-Aware Dataset Selection with Datamodels","abstract":"When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with \"high quality\" data sources may not increase (and can even hurt) performance compared to randomly selecting data.   To develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance. This framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks. Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously unseen tasks. Specifically, choosing target tasks representative of standard LM problems and evaluating on diverse held-out benchmarks, our selected datasets provide a 2x compute multiplier over baseline methods.","sentences":["When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality.","Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior.","However, in practice the opposite can often happen: we find that selecting according to similarity with \"high quality\" data sources may not increase (and can even hurt) performance compared to randomly selecting data.   ","To develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance.","This framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks.","Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously unseen tasks.","Specifically, choosing target tasks representative of standard LM problems and evaluating on diverse held-out benchmarks, our selected datasets provide a 2x compute multiplier over baseline methods."],"url":"http://arxiv.org/abs/2401.12926v1","category":"cs.LG"}
{"created":"2024-01-23 17:21:43","title":"Emotion-Aware Contrastive Adaptation Network for Source-Free Cross-Corpus Speech Emotion Recognition","abstract":"Cross-corpus speech emotion recognition (SER) aims to transfer emotional knowledge from a labeled source corpus to an unlabeled corpus. However, prior methods require access to source data during adaptation, which is unattainable in real-life scenarios due to data privacy protection concerns. This paper tackles a more practical task, namely source-free cross-corpus SER, where a pre-trained source model is adapted to the target domain without access to source data. To address the problem, we propose a novel method called emotion-aware contrastive adaptation network (ECAN). The core idea is to capture local neighborhood information between samples while considering the global class-level adaptation. Specifically, we propose a nearest neighbor contrastive learning to promote local emotion consistency among features of highly similar samples. Furthermore, relying solely on nearest neighborhoods may lead to ambiguous boundaries between clusters. Thus, we incorporate supervised contrastive learning to encourage greater separation between clusters representing different emotions, thereby facilitating improved class-level adaptation. Extensive experiments indicate that our proposed ECAN significantly outperforms state-of-the-art methods under the source-free cross-corpus SER setting on several speech emotion corpora.","sentences":["Cross-corpus speech emotion recognition (SER) aims to transfer emotional knowledge from a labeled source corpus to an unlabeled corpus.","However, prior methods require access to source data during adaptation, which is unattainable in real-life scenarios due to data privacy protection concerns.","This paper tackles a more practical task, namely source-free cross-corpus SER, where a pre-trained source model is adapted to the target domain without access to source data.","To address the problem, we propose a novel method called emotion-aware contrastive adaptation network (ECAN).","The core idea is to capture local neighborhood information between samples while considering the global class-level adaptation.","Specifically, we propose a nearest neighbor contrastive learning to promote local emotion consistency among features of highly similar samples.","Furthermore, relying solely on nearest neighborhoods may lead to ambiguous boundaries between clusters.","Thus, we incorporate supervised contrastive learning to encourage greater separation between clusters representing different emotions, thereby facilitating improved class-level adaptation.","Extensive experiments indicate that our proposed ECAN significantly outperforms state-of-the-art methods under the source-free cross-corpus SER setting on several speech emotion corpora."],"url":"http://arxiv.org/abs/2401.12925v1","category":"cs.SD"}
{"created":"2024-01-23 17:20:52","title":"Performance Analysis of Support Vector Machine (SVM) on Challenging Datasets for Forest Fire Detection","abstract":"This article delves into the analysis of performance and utilization of Support Vector Machines (SVMs) for the critical task of forest fire detection using image datasets. With the increasing threat of forest fires to ecosystems and human settlements, the need for rapid and accurate detection systems is of utmost importance. SVMs, renowned for their strong classification capabilities, exhibit proficiency in recognizing patterns associated with fire within images. By training on labeled data, SVMs acquire the ability to identify distinctive attributes associated with fire, such as flames, smoke, or alterations in the visual characteristics of the forest area. The document thoroughly examines the use of SVMs, covering crucial elements like data preprocessing, feature extraction, and model training. It rigorously evaluates parameters such as accuracy, efficiency, and practical applicability. The knowledge gained from this study aids in the development of efficient forest fire detection systems, enabling prompt responses and improving disaster management. Moreover, the correlation between SVM accuracy and the difficulties presented by high-dimensional datasets is carefully investigated, demonstrated through a revealing case study. The relationship between accuracy scores and the different resolutions used for resizing the training datasets has also been discussed in this article. These comprehensive studies result in a definitive overview of the difficulties faced and the potential sectors requiring further improvement and focus.","sentences":["This article delves into the analysis of performance and utilization of Support Vector Machines (SVMs) for the critical task of forest fire detection using image datasets.","With the increasing threat of forest fires to ecosystems and human settlements, the need for rapid and accurate detection systems is of utmost importance.","SVMs, renowned for their strong classification capabilities, exhibit proficiency in recognizing patterns associated with fire within images.","By training on labeled data, SVMs acquire the ability to identify distinctive attributes associated with fire, such as flames, smoke, or alterations in the visual characteristics of the forest area.","The document thoroughly examines the use of SVMs, covering crucial elements like data preprocessing, feature extraction, and model training.","It rigorously evaluates parameters such as accuracy, efficiency, and practical applicability.","The knowledge gained from this study aids in the development of efficient forest fire detection systems, enabling prompt responses and improving disaster management.","Moreover, the correlation between SVM accuracy and the difficulties presented by high-dimensional datasets is carefully investigated, demonstrated through a revealing case study.","The relationship between accuracy scores and the different resolutions used for resizing the training datasets has also been discussed in this article.","These comprehensive studies result in a definitive overview of the difficulties faced and the potential sectors requiring further improvement and focus."],"url":"http://arxiv.org/abs/2401.12924v1","category":"stat.ML"}
{"created":"2024-01-23 17:20:48","title":"Deep multitask neural networks for solving some stochastic optimal control problems","abstract":"Most existing neural network-based approaches for solving stochastic optimal control problems using the associated backward dynamic programming principle rely on the ability to simulate the underlying state variables. However, in some problems, this simulation is infeasible, leading to the discretization of state variable space and the need to train one neural network for each data point. This approach becomes computationally inefficient when dealing with large state variable spaces. In this paper, we consider a class of this type of stochastic optimal control problems and introduce an effective solution employing multitask neural networks. To train our multitask neural network, we introduce a novel scheme that dynamically balances the learning across tasks. Through numerical experiments on real-world derivatives pricing problems, we prove that our method outperforms state-of-the-art approaches.","sentences":["Most existing neural network-based approaches for solving stochastic optimal control problems using the associated backward dynamic programming principle rely on the ability to simulate the underlying state variables.","However, in some problems, this simulation is infeasible, leading to the discretization of state variable space and the need to train one neural network for each data point.","This approach becomes computationally inefficient when dealing with large state variable spaces.","In this paper, we consider a class of this type of stochastic optimal control problems and introduce an effective solution employing multitask neural networks.","To train our multitask neural network, we introduce a novel scheme that dynamically balances the learning across tasks.","Through numerical experiments on real-world derivatives pricing problems, we prove that our method outperforms state-of-the-art approaches."],"url":"http://arxiv.org/abs/2401.12923v1","category":"stat.ML"}
{"created":"2024-01-23 17:14:01","title":"Truck Parking Usage Prediction with Decomposed Graph Neural Networks","abstract":"Truck parking on freight corridors faces various challenges, such as insufficient parking spaces and compliance with Hour-of-Service (HOS) regulations. These constraints often result in unauthorized parking practices, causing safety concerns. To enhance the safety of freight operations, providing accurate parking usage prediction proves to be a cost-effective solution. Despite the existing research demonstrating satisfactory accuracy for predicting individual truck parking site usage, few approaches have been proposed for predicting usage with spatial dependencies of multiple truck parking sites. We present the Regional Temporal Graph Neural Network (RegT-GCN) as a predictive framework for assessing parking usage across the entire state to provide better truck parking information and mitigate unauthorized parking. The framework leverages the topological structures of truck parking site distributions and historical parking data to predict occupancy rates across a state. To achieve this, we introduce a Regional Decomposition approach, which effectively captures the geographical characteristics. We also introduce the spatial module working efficiently with the temporal module. Evaluation results demonstrate that the proposed model surpasses other baseline models, improving the performance by more than $20\\%$ compared with the original model. The proposed model allows truck parking sites' percipience of the topological structures and provides higher performance.","sentences":["Truck parking on freight corridors faces various challenges, such as insufficient parking spaces and compliance with Hour-of-Service (HOS) regulations.","These constraints often result in unauthorized parking practices, causing safety concerns.","To enhance the safety of freight operations, providing accurate parking usage prediction proves to be a cost-effective solution.","Despite the existing research demonstrating satisfactory accuracy for predicting individual truck parking site usage, few approaches have been proposed for predicting usage with spatial dependencies of multiple truck parking sites.","We present the Regional Temporal Graph Neural Network (RegT-GCN) as a predictive framework for assessing parking usage across the entire state to provide better truck parking information and mitigate unauthorized parking.","The framework leverages the topological structures of truck parking site distributions and historical parking data to predict occupancy rates across a state.","To achieve this, we introduce a Regional Decomposition approach, which effectively captures the geographical characteristics.","We also introduce the spatial module working efficiently with the temporal module.","Evaluation results demonstrate that the proposed model surpasses other baseline models, improving the performance by more than $20\\%$ compared with the original model.","The proposed model allows truck parking sites' percipience of the topological structures and provides higher performance."],"url":"http://arxiv.org/abs/2401.12920v1","category":"cs.AI"}
{"created":"2024-01-23 17:09:25","title":"Active Inference as a Model of Agency","abstract":"Is there a canonical way to think of agency beyond reward maximisation? In this paper, we show that any type of behaviour complying with physically sound assumptions about how macroscopic biological agents interact with the world canonically integrates exploration and exploitation in the sense of minimising risk and ambiguity about states of the world. This description, known as active inference, refines the free energy principle, a popular descriptive framework for action and perception originating in neuroscience. Active inference provides a normative Bayesian framework to simulate and model agency that is widely used in behavioural neuroscience, reinforcement learning (RL) and robotics. The usefulness of active inference for RL is three-fold. \\emph{a}) Active inference provides a principled solution to the exploration-exploitation dilemma that usefully simulates biological agency. \\emph{b}) It provides an explainable recipe to simulate behaviour, whence behaviour follows as an explainable mixture of exploration and exploitation under a generative world model, and all differences in behaviour are explicit in differences in world model. \\emph{c}) This framework is universal in the sense that it is theoretically possible to rewrite any RL algorithm conforming to the descriptive assumptions of active inference as an active inference algorithm. Thus, active inference can be used as a tool to uncover and compare the commitments and assumptions of more specific models of agency.","sentences":["Is there a canonical way to think of agency beyond reward maximisation?","In this paper, we show that any type of behaviour complying with physically sound assumptions about how macroscopic biological agents interact with the world canonically integrates exploration and exploitation in the sense of minimising risk and ambiguity about states of the world.","This description, known as active inference, refines the free energy principle, a popular descriptive framework for action and perception originating in neuroscience.","Active inference provides a normative Bayesian framework to simulate and model agency that is widely used in behavioural neuroscience, reinforcement learning (RL) and robotics.","The usefulness of active inference for RL is three-fold.","\\emph{a})","Active inference provides a principled solution to the exploration-exploitation dilemma that usefully simulates biological agency.","\\emph{b})","It provides an explainable recipe to simulate behaviour, whence behaviour follows as an explainable mixture of exploration and exploitation under a generative world model, and all differences in behaviour are explicit in differences in world model.","\\emph{c})","This framework is universal in the sense that it is theoretically possible to rewrite any RL algorithm conforming to the descriptive assumptions of active inference as an active inference algorithm.","Thus, active inference can be used as a tool to uncover and compare the commitments and assumptions of more specific models of agency."],"url":"http://arxiv.org/abs/2401.12917v1","category":"cs.AI"}
{"created":"2024-01-23 17:07:18","title":"Red Teaming Visual Language Models","abstract":"VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM test set, 13% in MM-Hal, and without noticeable decline in MM-Bench, overpassing other LLaVA-based models with regular alignment data. This reveals that current open-sourced VLMs still lack red teaming alignment. Our code and datasets will be open-source.","sentences":["VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs.","Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question.","To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness).","Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects.","Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM test set, 13% in MM-Hal, and without noticeable decline in MM-Bench, overpassing other LLaVA-based models with regular alignment data.","This reveals that current open-sourced VLMs still lack red teaming alignment.","Our code and datasets will be open-source."],"url":"http://arxiv.org/abs/2401.12915v1","category":"cs.AI"}
{"created":"2024-01-23 17:06:13","title":"Advancing Glitch Classification in Gravity Spy: Multi-view Fusion with Attention-based Machine Learning for Advanced LIGO's Fourth Observing Run","abstract":"The first successful detection of gravitational waves by ground-based observatories, such as the Laser Interferometer Gravitational-Wave Observatory (LIGO), marked a revolutionary breakthrough in our comprehension of the Universe. However, due to the unprecedented sensitivity required to make such observations, gravitational-wave detectors also capture disruptive noise sources called glitches, potentially masking or appearing as gravitational-wave signals themselves. To address this problem, a community-science project, Gravity Spy, incorporates human insight and machine learning to classify glitches in LIGO data. The machine learning classifier, integrated into the project since 2017, has evolved over time to accommodate increasing numbers of glitch classes. Despite its success, limitations have arisen in the ongoing LIGO fourth observing run (O4) due to its architecture's simplicity, which led to poor generalization and inability to handle multi-time window inputs effectively. We propose an advanced classifier for O4 glitches. Our contributions include evaluating fusion strategies for multi-time window inputs, using label smoothing to counter noisy labels, and enhancing interpretability through attention module-generated weights. This development seeks to enhance glitch classification, aiding in the ongoing exploration of gravitational-wave phenomena.","sentences":["The first successful detection of gravitational waves by ground-based observatories, such as the Laser Interferometer Gravitational-Wave Observatory (LIGO), marked a revolutionary breakthrough in our comprehension of the Universe.","However, due to the unprecedented sensitivity required to make such observations, gravitational-wave detectors also capture disruptive noise sources called glitches, potentially masking or appearing as gravitational-wave signals themselves.","To address this problem, a community-science project, Gravity Spy, incorporates human insight and machine learning to classify glitches in LIGO data.","The machine learning classifier, integrated into the project since 2017, has evolved over time to accommodate increasing numbers of glitch classes.","Despite its success, limitations have arisen in the ongoing LIGO fourth observing run (O4) due to its architecture's simplicity, which led to poor generalization and inability to handle multi-time window inputs effectively.","We propose an advanced classifier for O4 glitches.","Our contributions include evaluating fusion strategies for multi-time window inputs, using label smoothing to counter noisy labels, and enhancing interpretability through attention module-generated weights.","This development seeks to enhance glitch classification, aiding in the ongoing exploration of gravitational-wave phenomena."],"url":"http://arxiv.org/abs/2401.12913v1","category":"gr-qc"}
{"created":"2024-01-23 17:06:13","title":"Emergent Communication Protocol Learning for Task Offloading in Industrial Internet of Things","abstract":"In this paper, we leverage a multi-agent reinforcement learning (MARL) framework to jointly learn a computation offloading decision and multichannel access policy with corresponding signaling. Specifically, the base station and industrial Internet of Things mobile devices are reinforcement learning agents that need to cooperate to execute their computation tasks within a deadline constraint. We adopt an emergent communication protocol learning framework to solve this problem. The numerical results illustrate the effectiveness of emergent communication in improving the channel access success rate and the number of successfully computed tasks compared to contention-based, contention-free, and no-communication approaches. Moreover, the proposed task offloading policy outperforms remote and local computation baselines.","sentences":["In this paper, we leverage a multi-agent reinforcement learning (MARL) framework to jointly learn a computation offloading decision and multichannel access policy with corresponding signaling.","Specifically, the base station and industrial Internet of Things mobile devices are reinforcement learning agents that need to cooperate to execute their computation tasks within a deadline constraint.","We adopt an emergent communication protocol learning framework to solve this problem.","The numerical results illustrate the effectiveness of emergent communication in improving the channel access success rate and the number of successfully computed tasks compared to contention-based, contention-free, and no-communication approaches.","Moreover, the proposed task offloading policy outperforms remote and local computation baselines."],"url":"http://arxiv.org/abs/2401.12914v1","category":"cs.IT"}
{"created":"2024-01-23 17:03:40","title":"Searches for Compact Binary Coalescence Events Using Neural Networks in LIGO/Virgo Third Observation Period","abstract":"We present the results on the search for the coalescence of compact binary mergers using convolutional neural networks and the LIGO/Virgo data for the O3 observation period. Two-dimensional images in time and frequency are used as input. The analysis is performed in three separate mass regions covering the range for the masses in the binary system from 0.2 to 100 solar masses, excluding very asymmetric mass configurations. We explore neural networks trained with input information from pairs of interferometers or all three interferometers together, concluding that the use of the maximum information available leads to an improved performance. A scan over the O3 data set, using the convolutional neural networks, is performed with different fake rate thresholds for claiming detection of at most one event per year or at most one event per week. The latter would correspond to a loose online selection still leading to affordable fake alarm rates. The efficiency of the neutral networks to detect the O3 catalog events is discussed. In the case of a fake rate threshold of at most one event per week, the scan leads to the detection of about 50% of the O3 catalog events. Once the search is limited to the catalog events within the mass range used for neural networks training, the detection efficiency increases up to 70%. A further improvement in the search efficiency, using the same kind of algorithms, will require the implementation of new criteria for the suppression of detector glitches.","sentences":["We present the results on the search for the coalescence of compact binary mergers using convolutional neural networks and the LIGO/Virgo data for the O3 observation period.","Two-dimensional images in time and frequency are used as input.","The analysis is performed in three separate mass regions covering the range for the masses in the binary system from 0.2 to 100 solar masses, excluding very asymmetric mass configurations.","We explore neural networks trained with input information from pairs of interferometers or all three interferometers together, concluding that the use of the maximum information available leads to an improved performance.","A scan over the O3 data set, using the convolutional neural networks, is performed with different fake rate thresholds for claiming detection of at most one event per year or at most one event per week.","The latter would correspond to a loose online selection still leading to affordable fake alarm rates.","The efficiency of the neutral networks to detect the O3 catalog events is discussed.","In the case of a fake rate threshold of at most one event per week, the scan leads to the detection of about 50% of the O3 catalog events.","Once the search is limited to the catalog events within the mass range used for neural networks training, the detection efficiency increases up to 70%.","A further improvement in the search efficiency, using the same kind of algorithms, will require the implementation of new criteria for the suppression of detector glitches."],"url":"http://arxiv.org/abs/2401.12912v1","category":"gr-qc"}
{"created":"2024-01-23 16:48:59","title":"Unbounded quantum advantage in communication complexity measured by distinguishability","abstract":"Communication complexity is a pivotal element in information science, with quantum theory presenting a significant edge over classical approaches. The standard quantification of one-way communication complexity relies on the minimal dimension of the systems that the sender communicates to accomplish the designated task. In this study, we adopt a novel perspective, measuring the complexity of the communication by the distinguishability of the sender's input without constraining the dimension of the communicated systems. This measure becomes especially pertinent when maintaining the confidentiality of the sender's input is essential. After establishing the generic framework, we focus on two important categories of communication complexity tasks - the general version of random access codes and equality problems defined by graphs. We derive lower bounds on the distinguishability of the sender's input as a function of the success metric of these tasks in classical communication. Notably, we show that the ratio between the distinguishability in classical and quantum communication to achieve the same success metric escalates with the complexity of these tasks, reaching arbitrarily large values. Besides, we demonstrate the quantum advantage by employing qubits in solving equality problems associated with odd-cycle graphs. Furthermore, we derive lower bounds on distinguishability for another class of communication tasks, namely, pair-distinguishability tasks, and present several instances of the quantum advantage.","sentences":["Communication complexity is a pivotal element in information science, with quantum theory presenting a significant edge over classical approaches.","The standard quantification of one-way communication complexity relies on the minimal dimension of the systems that the sender communicates to accomplish the designated task.","In this study, we adopt a novel perspective, measuring the complexity of the communication by the distinguishability of the sender's input without constraining the dimension of the communicated systems.","This measure becomes especially pertinent when maintaining the confidentiality of the sender's input is essential.","After establishing the generic framework, we focus on two important categories of communication complexity tasks - the general version of random access codes and equality problems defined by graphs.","We derive lower bounds on the distinguishability of the sender's input as a function of the success metric of these tasks in classical communication.","Notably, we show that the ratio between the distinguishability in classical and quantum communication to achieve the same success metric escalates with the complexity of these tasks, reaching arbitrarily large values.","Besides, we demonstrate the quantum advantage by employing qubits in solving equality problems associated with odd-cycle graphs.","Furthermore, we derive lower bounds on distinguishability for another class of communication tasks, namely, pair-distinguishability tasks, and present several instances of the quantum advantage."],"url":"http://arxiv.org/abs/2401.12903v1","category":"quant-ph"}
{"created":"2024-01-23 16:48:18","title":"Facing the Elephant in the Room: Visual Prompt Tuning or Full Finetuning?","abstract":"As the scale of vision models continues to grow, the emergence of Visual Prompt Tuning (VPT) as a parameter-efficient transfer learning technique has gained attention due to its superior performance compared to traditional full-finetuning. However, the conditions favoring VPT (the ``when\") and the underlying rationale (the ``why\") remain unclear. In this paper, we conduct a comprehensive analysis across 19 distinct datasets and tasks. To understand the ``when\" aspect, we identify the scenarios where VPT proves favorable by two dimensions: task objectives and data distributions. We find that VPT is preferrable when there is 1) a substantial disparity between the original and the downstream task objectives (e.g., transitioning from classification to counting), or 2) a similarity in data distributions between the two tasks (e.g., both involve natural images). In exploring the ``why\" dimension, our results indicate VPT's success cannot be attributed solely to overfitting and optimization considerations. The unique way VPT preserves original features and adds parameters appears to be a pivotal factor. Our study provides insights into VPT's mechanisms, and offers guidance for its optimal utilization.","sentences":["As the scale of vision models continues to grow, the emergence of Visual Prompt Tuning (VPT) as a parameter-efficient transfer learning technique has gained attention due to its superior performance compared to traditional full-finetuning.","However, the conditions favoring VPT (the ``when\") and the underlying rationale (the ``why\") remain unclear.","In this paper, we conduct a comprehensive analysis across 19 distinct datasets and tasks.","To understand the ``when\" aspect, we identify the scenarios where VPT proves favorable by two dimensions: task objectives and data distributions.","We find that VPT is preferrable when there is 1) a substantial disparity between the original and the downstream task objectives (e.g., transitioning from classification to counting), or 2) a similarity in data distributions between the two tasks (e.g., both involve natural images).","In exploring the ``why\" dimension, our results indicate VPT's success cannot be attributed solely to overfitting and optimization considerations.","The unique way VPT preserves original features and adds parameters appears to be a pivotal factor.","Our study provides insights into VPT's mechanisms, and offers guidance for its optimal utilization."],"url":"http://arxiv.org/abs/2401.12902v1","category":"cs.CV"}
{"created":"2024-01-23 16:48:06","title":"Secure Spatial Signal Design for ISAC in a Cell-Free MIMO Network","abstract":"In this paper, we study a cell-free multiple-input multiple-output network equipped with integrated sensing and communication (ISAC) access points (APs). The distributed APs are used to jointly serve the communication needs of user equipments (UEs) while sensing a target, assumed to be an eavesdropper (Eve). To increase the system's robustness towards said Eve, we develop an ISAC waveform model that includes artificial noise (AN) aimed at degrading the Eve channel quality. The central processing unit receives the observations from each AP and calculates the optimal precoding and AN covariance matrices by solving a semi-definite relaxation of a constrained Cramer-Rao bound (CRB) minimization problem. Simulation results highlight an underlying trade-off between sensing and communication performances: in particular, the UEs signal-to-noise and interference ratio and the maximum Eve's signal to noise ratio are directly proportional to the CRB. Furthermore, the optimal AN covariance matrix is rank-1 and has a peak in the eve's direction, leading to a surprising inverse-proportionality between the UEs-Eve distance and optimal-CRB magnitude.","sentences":["In this paper, we study a cell-free multiple-input multiple-output network equipped with integrated sensing and communication (ISAC) access points (APs).","The distributed APs are used to jointly serve the communication needs of user equipments (UEs) while sensing a target, assumed to be an eavesdropper (Eve).","To increase the system's robustness towards said Eve, we develop an ISAC waveform model that includes artificial noise (AN) aimed at degrading the Eve channel quality.","The central processing unit receives the observations from each AP and calculates the optimal precoding and AN covariance matrices by solving a semi-definite relaxation of a constrained Cramer-Rao bound (CRB) minimization problem.","Simulation results highlight an underlying trade-off between sensing and communication performances: in particular, the UEs signal-to-noise and interference ratio and the maximum Eve's signal to noise ratio are directly proportional to the CRB.","Furthermore, the optimal AN covariance matrix is rank-1 and has a peak in the eve's direction, leading to a surprising inverse-proportionality between the UEs-Eve distance and optimal-CRB magnitude."],"url":"http://arxiv.org/abs/2401.12901v1","category":"cs.IT"}
{"created":"2024-01-23 16:40:47","title":"PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar Creation with 3D Gaussian Splatting","abstract":"Despite much progress, creating real-time high-fidelity head avatar is still difficult and existing methods have to trade-off between speed and quality. 3DMM based methods often fail to model non-facial structures such as eyeglasses and hairstyles, while neural implicit models suffer from deformation inflexibility and rendering inefficiency.   Although 3D Gaussian has been demonstrated to possess promising capability for geometry representation and radiance field reconstruction, applying 3D Gaussian in head avatar creation remains a major challenge since it is difficult for 3D Gaussian to model the head shape variations caused by changing poses and expressions. In this paper, we introduce PSAvatar, a novel framework for animatable head avatar creation that utilizes discrete geometric primitive to create a parametric morphable shape model and employs 3D Gaussian for fine detail representation and high fidelity rendering. The parametric morphable shape model is a Point-based Morphable Shape Model (PMSM) which uses points instead of meshes for 3D representation to achieve enhanced representation flexibility. The PMSM first converts the FLAME mesh to points by sampling on the surfaces as well as off the meshes to enable the reconstruction of not only surface-like structures but also complex geometries such as eyeglasses and hairstyles. By aligning these points with the head shape in an analysis-by-synthesis manner, the PMSM makes it possible to utilize 3D Gaussian for fine detail representation and appearance modeling, thus enabling the creation of high-fidelity avatars. We show that PSAvatar can reconstruct high-fidelity head avatars of a variety of subjects and the avatars can be animated in real-time ($\\ge$ 25 fps at a resolution of 512 x 512 )","sentences":["Despite much progress, creating real-time high-fidelity head avatar is still difficult and existing methods have to trade-off between speed and quality.","3DMM based methods often fail to model non-facial structures such as eyeglasses and hairstyles, while neural implicit models suffer from deformation inflexibility and rendering inefficiency.   ","Although 3D Gaussian has been demonstrated to possess promising capability for geometry representation and radiance field reconstruction, applying 3D Gaussian in head avatar creation remains a major challenge since it is difficult for 3D Gaussian to model the head shape variations caused by changing poses and expressions.","In this paper, we introduce PSAvatar, a novel framework for animatable head avatar creation that utilizes discrete geometric primitive to create a parametric morphable shape model and employs 3D Gaussian for fine detail representation and high fidelity rendering.","The parametric morphable shape model is a Point-based Morphable Shape Model (PMSM) which uses points instead of meshes for 3D representation to achieve enhanced representation flexibility.","The PMSM first converts the FLAME mesh to points by sampling on the surfaces as well as off the meshes to enable the reconstruction of not only surface-like structures but also complex geometries such as eyeglasses and hairstyles.","By aligning these points with the head shape in an analysis-by-synthesis manner, the PMSM makes it possible to utilize 3D Gaussian for fine detail representation and appearance modeling, thus enabling the creation of high-fidelity avatars.","We show that PSAvatar can reconstruct high-fidelity head avatars of a variety of subjects and the avatars can be animated in real-time ($\\ge$ 25 fps at a resolution of 512 x 512 )"],"url":"http://arxiv.org/abs/2401.12900v1","category":"cs.GR"}
{"created":"2024-01-23 16:38:36","title":"How Chaotic is the Dynamics Induced by a Hermitian Matrix?","abstract":"Given an arbitrary \\(V \\times V\\) Hermitian matrix, considered as a finite discrete quantum Hamiltonian, we use methods from graph and ergodic theories to construct a corresponding stochastic classical dynamics on an appropriate discrete phase space. It consists of the directed edges of a graph with \\(V\\) vertices that are in one-to-one correspondence with the non-vanishing off-diagonal elements of \\(H\\). The classical dynamics is a stochastic variant of a Poincar\\'e map at an energy \\(E\\) and an alternative to standard quantum-classical correspondence based on a classical limit \\(\\hbar \\to 0\\). Most importantly it can be constructed where no such limit exists. Using standard methods from ergodic theory we then proceed to define an expression for the Lyapunov exponent \\(\\Lambda(E)\\) of the classical map. It measures the rate of separation of stochastic classical trajectories in phase space. We suggest to use this Lyapunov exponent to quantify the amount of chaos in a finite quantum system.","sentences":["Given an arbitrary \\(V \\times V\\) Hermitian matrix, considered as a finite discrete quantum Hamiltonian, we use methods from graph and ergodic theories to construct a corresponding stochastic classical dynamics on an appropriate discrete phase space.","It consists of the directed edges of a graph with \\(V\\) vertices that are in one-to-one correspondence with the non-vanishing off-diagonal elements of \\(H\\).","The classical dynamics is a stochastic variant of a Poincar\\'e map at an energy \\(E\\) and an alternative to standard quantum-classical correspondence based on a classical limit \\(\\hbar \\to 0\\).","Most importantly it can be constructed where no such limit exists.","Using standard methods from ergodic theory we then proceed to define an expression for the Lyapunov exponent \\(\\Lambda(E)\\) of the classical map.","It measures the rate of separation of stochastic classical trajectories in phase space.","We suggest to use this Lyapunov exponent to quantify the amount of chaos in a finite quantum system."],"url":"http://arxiv.org/abs/2401.12898v1","category":"quant-ph"}
{"created":"2024-01-23 16:36:58","title":"ESC: Edge-attributed Skyline Community Search in Large-scale Bipartite Graphs","abstract":"Due to the ability of modeling relationships between two different types of entities, bipartite graphs are naturally employed in many real-world applications. Community Search in bipartite graphs is a fundamental problem and has gained much attention. However, existing studies focus on measuring the structural cohesiveness between two sets of vertices, while either completely ignoring the edge attributes or only considering one-dimensional importance in forming communities. In this paper, we introduce a novel community model, named edge-attributed skyline community (ESC), which not only preserves the structural cohesiveness but unravels the inherent dominance brought about by multi-dimensional attributes on the edges of bipartite graphs. To search the ESCs, we develop an elegant peeling algorithm by iteratively deleting edges with the minimum attribute in each dimension. In addition, we also devise a more efficient expanding algorithm to further reduce the search space and speed up the filtering of unpromising vertices, where a upper bound is proposed and proven. Extensive experiments on real-world large-scale datasets demonstrate the efficiency, effectiveness, and scalability of the proposed ESC search algorithms. A case study was conducted to compare with existing community models, substantiating that our approach facilitates the precision and diversity of results.","sentences":["Due to the ability of modeling relationships between two different types of entities, bipartite graphs are naturally employed in many real-world applications.","Community Search in bipartite graphs is a fundamental problem and has gained much attention.","However, existing studies focus on measuring the structural cohesiveness between two sets of vertices, while either completely ignoring the edge attributes or only considering one-dimensional importance in forming communities.","In this paper, we introduce a novel community model, named edge-attributed skyline community (ESC), which not only preserves the structural cohesiveness but unravels the inherent dominance brought about by multi-dimensional attributes on the edges of bipartite graphs.","To search the ESCs, we develop an elegant peeling algorithm by iteratively deleting edges with the minimum attribute in each dimension.","In addition, we also devise a more efficient expanding algorithm to further reduce the search space and speed up the filtering of unpromising vertices, where a upper bound is proposed and proven.","Extensive experiments on real-world large-scale datasets demonstrate the efficiency, effectiveness, and scalability of the proposed ESC search algorithms.","A case study was conducted to compare with existing community models, substantiating that our approach facilitates the precision and diversity of results."],"url":"http://arxiv.org/abs/2401.12895v1","category":"cs.SI"}
{"created":"2024-01-23 16:31:51","title":"Fully dynamic G3W2 self-energy for finite systems: Formulas and benchmark","abstract":"Over the years, Hedin's $GW$ self-energy has been proven to be a rather accurate and simple approximation to evaluate electronic quasiparticle energies in solids and in molecules. Attempts to improve over the simple $GW$ approximation, the so-called vertex corrections, have been constantly proposed in the literature. Here, we derive, analyze, and benchmark the complete second-order term in the screened Coulomb interaction $W$ for finite systems. This self-energy named $G3W2$ contains all the possible time orderings that combine 3 Green's functions $G$ and 2 dynamic $W$. We present the analytic formula and its imaginary frequency counterpart, the latter allowing us to treat larger molecules. The accuracy of the $G3W2$ self-energy is evaluated on well-established benchmarks (GW100, Acceptor 24 and Core 65) for valence and core quasiparticle energies. Its link with the simpler static approximation, named SOSEX for static screened second-order exchange, is analyzed, which leads us to propose a more consistent approximation named 2SOSEX. In the end, we find that neither the $G3W2$ self-energy nor any of the investigated approximations to it improve over one-shot $G_0W_0$ with a good starting point. Only quasi-particle self-consistent $GW$ HOMO energies are slightly improved by addition of the $G3W2$ self-energy correction. We show that this is due to the self-consistent update of the screened Coulomb interaction leading to an overall sign change of the vertex correction to the frontier quasiparticle energies.","sentences":["Over the years, Hedin's $GW$ self-energy has been proven to be a rather accurate and simple approximation to evaluate electronic quasiparticle energies in solids and in molecules.","Attempts to improve over the simple $GW$ approximation, the so-called vertex corrections, have been constantly proposed in the literature.","Here, we derive, analyze, and benchmark the complete second-order term in the screened Coulomb interaction $W$ for finite systems.","This self-energy named $G3W2$ contains all the possible time orderings that combine 3 Green's functions $G$ and 2 dynamic $W$. We present the analytic formula and its imaginary frequency counterpart, the latter allowing us to treat larger molecules.","The accuracy of the $G3W2$ self-energy is evaluated on well-established benchmarks (GW100, Acceptor 24 and Core 65) for valence and core quasiparticle energies.","Its link with the simpler static approximation, named SOSEX for static screened second-order exchange, is analyzed, which leads us to propose a more consistent approximation named 2SOSEX.","In the end, we find that neither the $G3W2$ self-energy nor any of the investigated approximations to it improve over one-shot $G_0W_0$ with a good starting point.","Only quasi-particle self-consistent $GW$ HOMO energies are slightly improved by addition of the $G3W2$ self-energy correction.","We show that this is due to the self-consistent update of the screened Coulomb interaction leading to an overall sign change of the vertex correction to the frontier quasiparticle energies."],"url":"http://arxiv.org/abs/2401.12892v1","category":"physics.comp-ph"}
{"created":"2024-01-23 16:29:10","title":"Is PSR J0514$-$4002E in a PBH-NS binary?","abstract":"Recent pulsar timing observations with MeerKAT on the eccentric binary millisecond pulsar, PSR J0514$-$4002E, reveal a companion with a mass (between $2.09\\, M_\\odot$ and $2.71\\, M_\\odot$) in the mass gap, challenging conventional astrophysical scenarios for black hole formation. In this letter, we propose an alternative explanation: PSR J0514$-$4002E exists in a PBH-NS binary, with the companion potentially being a primordial black hole formed during the early Universe's first-order phase transition. The associated stochastic gravitational-wave background generated during this phase transition can explain the detected signal from the pulsar timing array and the abundance of primordial black holes is consistent with constraints from LIGO-Virgo-KAGRA.","sentences":["Recent pulsar timing observations with MeerKAT on the eccentric binary millisecond pulsar, PSR J0514$-$4002E, reveal a companion with a mass (between $2.09\\, M_\\odot$ and $2.71\\, M_\\odot$) in the mass gap, challenging conventional astrophysical scenarios for black hole formation.","In this letter, we propose an alternative explanation: PSR J0514$-$4002E exists in a PBH-NS binary, with the companion potentially being a primordial black hole formed during the early Universe's first-order phase transition.","The associated stochastic gravitational-wave background generated during this phase transition can explain the detected signal from the pulsar timing array and the abundance of primordial black holes is consistent with constraints from LIGO-Virgo-KAGRA."],"url":"http://arxiv.org/abs/2401.12889v1","category":"astro-ph.HE"}
{"created":"2024-01-23 16:28:30","title":"Data-Centric Evolution in Autonomous Driving: A Comprehensive Survey of Big Data System, Data Mining, and Closed-Loop Technologies","abstract":"The aspiration of the next generation's autonomous driving (AD) technology relies on the dedicated integration and interaction among intelligent perception, prediction, planning, and low-level control. There has been a huge bottleneck regarding the upper bound of autonomous driving algorithm performance, a consensus from academia and industry believes that the key to surmount the bottleneck lies in data-centric autonomous driving technology. Recent advancement in AD simulation, closed-loop model training, and AD big data engine have gained some valuable experience. However, there is a lack of systematic knowledge and deep understanding regarding how to build efficient data-centric AD technology for AD algorithm self-evolution and better AD big data accumulation. To fill in the identified research gaps, this article will closely focus on reviewing the state-of-the-art data-driven autonomous driving technologies, with an emphasis on the comprehensive taxonomy of autonomous driving datasets characterized by milestone generations, key features, data acquisition settings, etc. Furthermore, we provide a systematic review of the existing benchmark closed-loop AD big data pipelines from the industrial frontier, including the procedure of closed-loop frameworks, key technologies, and empirical studies. Finally, the future directions, potential applications, limitations and concerns are discussed to arouse efforts from both academia and industry for promoting the further development of autonomous driving.","sentences":["The aspiration of the next generation's autonomous driving (AD) technology relies on the dedicated integration and interaction among intelligent perception, prediction, planning, and low-level control.","There has been a huge bottleneck regarding the upper bound of autonomous driving algorithm performance, a consensus from academia and industry believes that the key to surmount the bottleneck lies in data-centric autonomous driving technology.","Recent advancement in AD simulation, closed-loop model training, and AD big data engine have gained some valuable experience.","However, there is a lack of systematic knowledge and deep understanding regarding how to build efficient data-centric AD technology for AD algorithm self-evolution and better AD big data accumulation.","To fill in the identified research gaps, this article will closely focus on reviewing the state-of-the-art data-driven autonomous driving technologies, with an emphasis on the comprehensive taxonomy of autonomous driving datasets characterized by milestone generations, key features, data acquisition settings, etc.","Furthermore, we provide a systematic review of the existing benchmark closed-loop AD big data pipelines from the industrial frontier, including the procedure of closed-loop frameworks, key technologies, and empirical studies.","Finally, the future directions, potential applications, limitations and concerns are discussed to arouse efforts from both academia and industry for promoting the further development of autonomous driving."],"url":"http://arxiv.org/abs/2401.12888v1","category":"cs.RO"}
{"created":"2024-01-23 16:22:50","title":"Model-Free $\u03b4$-Policy Iteration Based on Damped Newton Method for Nonlinear Continuous-Time H$\\infty$ Tracking Control","abstract":"This paper presents a {\\delta}-PI algorithm which is based on damped Newton method for the H{\\infty} tracking control problem of unknown continuous-time nonlinear system. A discounted performance function and an augmented system are used to get the tracking Hamilton-Jacobi-Isaac (HJI) equation. Tracking HJI equation is a nonlinear partial differential equation, traditional reinforcement learning methods for solving the tracking HJI equation are mostly based on the Newton method, which usually only satisfies local convergence and needs a good initial guess. Based upon the damped Newton iteration operator equation, a generalized tracking Bellman equation is derived firstly. The {\\delta}-PI algorithm can seek the optimal solution of the tracking HJI equation by iteratively solving the generalized tracking Bellman equation. On-policy learning and off-policy learning {\\delta}-PI reinforcement learning methods are provided, respectively. Off-policy version {\\delta}-PI algorithm is a model-free algorithm which can be performed without making use of a priori knowledge of the system dynamics. NN-based implementation scheme for the off-policy {\\delta}-PI algorithms is shown. The suitability of the model-free {\\delta}-PI algorithm is illustrated with a nonlinear system simulation.","sentences":["This paper presents a {\\delta}-PI algorithm which is based on damped Newton method for the H{\\infty} tracking control problem of unknown continuous-time nonlinear system.","A discounted performance function and an augmented system are used to get the tracking Hamilton-Jacobi-Isaac (HJI) equation.","Tracking HJI equation is a nonlinear partial differential equation, traditional reinforcement learning methods for solving the tracking HJI equation are mostly based on the Newton method, which usually only satisfies local convergence and needs a good initial guess.","Based upon the damped Newton iteration operator equation, a generalized tracking Bellman equation is derived firstly.","The {\\delta}-PI algorithm can seek the optimal solution of the tracking HJI equation by iteratively solving the generalized tracking Bellman equation.","On-policy learning and off-policy learning {\\delta}-PI reinforcement learning methods are provided, respectively.","Off-policy version {\\delta}-PI algorithm is a model-free algorithm which can be performed without making use of a priori knowledge of the system dynamics.","NN-based implementation scheme for the off-policy {\\delta}-PI algorithms is shown.","The suitability of the model-free {\\delta}-PI algorithm is illustrated with a nonlinear system simulation."],"url":"http://arxiv.org/abs/2401.12882v1","category":"cs.LG"}
{"created":"2024-01-23 16:20:20","title":"Computing Diameter+2 in Truly Subquadratic Time for Unit-Disk Graphs","abstract":"Finding the diameter of a graph in general cannot be done in truly subquadratic assuming the Strong Exponential Time Hypothesis (SETH), even when the underlying graph is unweighted and sparse. When restricting to concrete classes of graphs and assuming SETH, planar graphs and minor-free graphs admit truly subquadratic algorithms, while geometric intersection graphs of unit balls, congruent equilateral triangles, and unit segments do not. Unit-disk graphs are one of the major open cases where the complexity of diameter computation remains unknown. More generally, it is conjectured that a truly subquadratic time algorithm exists for pseudo-disk graphs.   In this paper, we show a truly subquadratic algorithm of running time $\\tilde{O}(n^{2-1/18})$, for finding the diameter in a unit-disk graph, whose output differs from the optimal solution by at most 2. This is the first algorithm that provides an additive guarantee in distortion, independent of the size or the diameter of the graph. Our algorithm requires two important technical elements. First, we show that for the intersection graph of pseudo-disks, the graph VC-dimension, either of $k$-hop balls or the distance encoding vectors, is 4. This contracts to the VC dimension of the pseudo-disks themselves as geometric ranges (which is known to be 3). Second, we introduce a clique-based $r$-clustering for geometric intersection graphs, which is an analog of the $r$-division construction for planar graphs. We also showcase the new techniques by establishing new results for distance oracles for unit-disk graphs with subquadratic storage and $O(1)$ query time. The results naturally extend to unit $L_1$ or $L_\\infty$-disks and fat pseudo-disks of similar size. Last, if the pseudo-disks additionally have bounded ply, we have a truly subquadratic algorithm to find the exact diameter.","sentences":["Finding the diameter of a graph in general cannot be done in truly subquadratic assuming the Strong Exponential Time Hypothesis (SETH), even when the underlying graph is unweighted and sparse.","When restricting to concrete classes of graphs and assuming SETH, planar graphs and minor-free graphs admit truly subquadratic algorithms, while geometric intersection graphs of unit balls, congruent equilateral triangles, and unit segments do not.","Unit-disk graphs are one of the major open cases where the complexity of diameter computation remains unknown.","More generally, it is conjectured that a truly subquadratic time algorithm exists for pseudo-disk graphs.   ","In this paper, we show a truly subquadratic algorithm of running time $\\tilde{O}(n^{2-1/18})$, for finding the diameter in a unit-disk graph, whose output differs from the optimal solution by at most 2.","This is the first algorithm that provides an additive guarantee in distortion, independent of the size or the diameter of the graph.","Our algorithm requires two important technical elements.","First, we show that for the intersection graph of pseudo-disks, the graph VC-dimension, either of $k$-hop balls or the distance encoding vectors, is 4.","This contracts to the VC dimension of the pseudo-disks themselves as geometric ranges (which is known to be 3).","Second, we introduce a clique-based $r$-clustering for geometric intersection graphs, which is an analog of the $r$-division construction for planar graphs.","We also showcase the new techniques by establishing new results for distance oracles for unit-disk graphs with subquadratic storage and $O(1)$ query time.","The results naturally extend to unit $L_1$ or $L_\\infty$-disks and fat pseudo-disks of similar size.","Last, if the pseudo-disks additionally have bounded ply, we have a truly subquadratic algorithm to find the exact diameter."],"url":"http://arxiv.org/abs/2401.12881v1","category":"cs.DS"}
{"created":"2024-01-23 16:13:20","title":"Optimal compilation of parametrised quantum circuits","abstract":"Parametrised quantum circuits contain phase gates whose phase is determined by a classical algorithm prior to running the circuit on a quantum device. Such circuits are used in variational algorithms like QAOA and VQE. In order for these algorithms to be as efficient as possible it is important that we use the fewest number of parameters. We show that, while the general problem of minimising the number of parameters is NP-hard, when we restrict to circuits that are Clifford apart from parametrised phase gates and where each parameter is used just once, we can efficiently find the optimal parameter count. We show that when parameter transformations are required to be sufficiently well-behaved that the only rewrites that reduce parameters correspond to simple 'fusions'. Using this we find that a previous circuit optimisation strategy by some of the authors [Kissinger, van de Wetering. PRA (2019)] finds the optimal number of parameters. Our proof uses the ZX-calculus. We also prove that the standard rewrite rules of the ZX-calculus suffice to prove any equality between parametrised Clifford circuits.","sentences":["Parametrised quantum circuits contain phase gates whose phase is determined by a classical algorithm prior to running the circuit on a quantum device.","Such circuits are used in variational algorithms like QAOA and VQE.","In order for these algorithms to be as efficient as possible it is important that we use the fewest number of parameters.","We show that, while the general problem of minimising the number of parameters is NP-hard, when we restrict to circuits that are Clifford apart from parametrised phase gates and where each parameter is used just once, we can efficiently find the optimal parameter count.","We show that when parameter transformations are required to be sufficiently well-behaved that the only rewrites that reduce parameters correspond to simple 'fusions'.","Using this we find that a previous circuit optimisation strategy by some of the authors [Kissinger, van de Wetering.","PRA (2019)] finds the optimal number of parameters.","Our proof uses the ZX-calculus.","We also prove that the standard rewrite rules of the ZX-calculus suffice to prove any equality between parametrised Clifford circuits."],"url":"http://arxiv.org/abs/2401.12877v1","category":"quant-ph"}
{"created":"2024-01-23 16:09:53","title":"From Understanding to Utilization: A Survey on Explainability for Large Language Models","abstract":"This survey paper delves into the burgeoning field of explainability for Large Language Models (LLMs), a critical yet challenging aspect of natural language processing. With LLMs playing a pivotal role in various applications, their \"black-box\" nature raises concerns about transparency and ethical use. This paper emphasizes the necessity for enhanced explainability in LLMs, addressing both the general public's trust and the technical community's need for a deeper understanding of these models. We concentrate on pre-trained Transformer-based LLMs, such as LLaMA, which present unique interpretability challenges due to their scale and complexity. Our review categorizes existing explainability methods and discusses their application in improving model transparency and reliability. We also discuss representative evaluation methods, highlighting their strengths and limitations. The goal of this survey is to bridge the gap between theoretical understanding and practical application, offering insights for future research and development in the field of LLM explainability.","sentences":["This survey paper delves into the burgeoning field of explainability for Large Language Models (LLMs), a critical yet challenging aspect of natural language processing.","With LLMs playing a pivotal role in various applications, their \"black-box\" nature raises concerns about transparency and ethical use.","This paper emphasizes the necessity for enhanced explainability in LLMs, addressing both the general public's trust and the technical community's need for a deeper understanding of these models.","We concentrate on pre-trained Transformer-based LLMs, such as LLaMA, which present unique interpretability challenges due to their scale and complexity.","Our review categorizes existing explainability methods and discusses their application in improving model transparency and reliability.","We also discuss representative evaluation methods, highlighting their strengths and limitations.","The goal of this survey is to bridge the gap between theoretical understanding and practical application, offering insights for future research and development in the field of LLM explainability."],"url":"http://arxiv.org/abs/2401.12874v1","category":"cs.CL"}
{"created":"2024-01-23 16:07:43","title":"Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model","abstract":"Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality. Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years. In this work, we investigate the potential of employing the QE model as the reward model (the QE-based reward model) to predict human preferences for feedback training. We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines. We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation. To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the QE-based rewards for the detected incorrect translations. Experimental results show that the proposed QE-based feedback training achieves consistent and significant improvements across various settings, further verified through human preference studies. Our subsequent analysis demonstrates the high data efficiency of the proposed QE-based feedback training: the proposed approach using a small amount of monolingual data can outperform systems using larger parallel corpora.","sentences":["Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality.","Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years.","In this work, we investigate the potential of employing the QE model as the reward model (the QE-based reward model) to predict human preferences for feedback training.","We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines.","We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation.","To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the QE-based rewards for the detected incorrect translations.","Experimental results show that the proposed QE-based feedback training achieves consistent and significant improvements across various settings, further verified through human preference studies.","Our subsequent analysis demonstrates the high data efficiency of the proposed QE-based feedback training: the proposed approach using a small amount of monolingual data can outperform systems using larger parallel corpora."],"url":"http://arxiv.org/abs/2401.12873v1","category":"cs.CL"}
{"created":"2024-01-23 16:05:02","title":"FocusFlow: 3D Gaze-Depth Interaction in Virtual Reality Leveraging Active Visual Depth Manipulation","abstract":"Gaze interaction presents a promising avenue in Virtual Reality (VR) due to its intuitive and efficient user experience. Yet, the depth control inherent in our visual system remains underutilized in current methods. In this study, we introduce FocusFlow, a hands-free interaction method that capitalizes on human visual depth perception within the 3D scenes of Virtual Reality. We first develop a binocular visual depth detection algorithm to understand eye input characteristics. We then propose a layer-based user interface and introduce the concept of 'Virtual Window' that offers an intuitive and robust gaze-depth VR interaction, despite the constraints of visual depth accuracy and precision spatially at further distances. Finally, to help novice users actively manipulate their visual depth, we propose two learning strategies that use different visual cues to help users master visual depth control. Our user studies on 24 participants demonstrate the usability of our proposed virtual window concept as a gaze-depth interaction method. In addition, our findings reveal that the user experience can be enhanced through an effective learning process with adaptive visual cues, helping users to develop muscle memory for this brand-new input mechanism. We conclude the paper by discussing strategies to optimize learning and potential research topics of gaze-depth interaction.","sentences":["Gaze interaction presents a promising avenue in Virtual Reality (VR) due to its intuitive and efficient user experience.","Yet, the depth control inherent in our visual system remains underutilized in current methods.","In this study, we introduce FocusFlow, a hands-free interaction method that capitalizes on human visual depth perception within the 3D scenes of Virtual Reality.","We first develop a binocular visual depth detection algorithm to understand eye input characteristics.","We then propose a layer-based user interface and introduce the concept of 'Virtual Window' that offers an intuitive and robust gaze-depth VR interaction, despite the constraints of visual depth accuracy and precision spatially at further distances.","Finally, to help novice users actively manipulate their visual depth, we propose two learning strategies that use different visual cues to help users master visual depth control.","Our user studies on 24 participants demonstrate the usability of our proposed virtual window concept as a gaze-depth interaction method.","In addition, our findings reveal that the user experience can be enhanced through an effective learning process with adaptive visual cues, helping users to develop muscle memory for this brand-new input mechanism.","We conclude the paper by discussing strategies to optimize learning and potential research topics of gaze-depth interaction."],"url":"http://arxiv.org/abs/2401.12872v1","category":"cs.HC"}
{"created":"2024-01-23 16:04:19","title":"Unlocking the Potential: Multi-task Deep Learning for Spaceborne Quantitative Monitoring of Fugitive Methane Plumes","abstract":"With the intensification of global warming, the monitoring of methane emission and detection of gas plumes from landfills have increasingly received attention. We decompose methane emission monitoring into three sub-tasks: methane concentration inversion, plume segmentation, and emission rate estimation. Conventional algorithms have limitations: methane concentration inversion usually uses the matched filter, which is sensitive to global spectrum distribution and contains a large amount of noises. There is limited research on plume segmentation, with many studies resorting to manual segmentation that is likely to be subjective. The estimation of methane emission rate often utilizes IME algorithm, which relies on obtaining meteorological measurement data. Using the WENT landfill site in Hong Kong and PRISMA hyperspectral satellite imagery, we propose a new deep learning-based framework for quantitative monitoring of methane emissions from remote sensing images based on physical simulation. We generate simulated methane plumes using large eddy simulation (LES) and different concentration maps of fugitive emission using the radiative transfer equation (RTE), while combining augmentation techniques to create a simulated PRISMA dataset. We train a U-Net network for methane concentration inversion, a Mask R-CNN network for methane plume segmentation, and a ResNet-50 network for methane emission rate estimation. All three deep networks achieve higher validation accuracy compared to conventional algorithms. We further respectively combine the first two sub-tasks and the last two sub-tasks to design the multi-task learning models - MTL-01 and MTL-02, both of which achieve higher accuracy than single-task models. Our research serves as a demonstration of applying multi-task deep learning to quantitative methane monitoring and can be extended to a broad range of methane monitoring tasks.","sentences":["With the intensification of global warming, the monitoring of methane emission and detection of gas plumes from landfills have increasingly received attention.","We decompose methane emission monitoring into three sub-tasks: methane concentration inversion, plume segmentation, and emission rate estimation.","Conventional algorithms have limitations: methane concentration inversion usually uses the matched filter, which is sensitive to global spectrum distribution and contains a large amount of noises.","There is limited research on plume segmentation, with many studies resorting to manual segmentation that is likely to be subjective.","The estimation of methane emission rate often utilizes IME algorithm, which relies on obtaining meteorological measurement data.","Using the WENT landfill site in Hong Kong and PRISMA hyperspectral satellite imagery, we propose a new deep learning-based framework for quantitative monitoring of methane emissions from remote sensing images based on physical simulation.","We generate simulated methane plumes using large eddy simulation (LES) and different concentration maps of fugitive emission using the radiative transfer equation (RTE), while combining augmentation techniques to create a simulated PRISMA dataset.","We train a U-Net network for methane concentration inversion, a Mask R-CNN network for methane plume segmentation, and a ResNet-50 network for methane emission rate estimation.","All three deep networks achieve higher validation accuracy compared to conventional algorithms.","We further respectively combine the first two sub-tasks and the last two sub-tasks to design the multi-task learning models - MTL-01 and MTL-02, both of which achieve higher accuracy than single-task models.","Our research serves as a demonstration of applying multi-task deep learning to quantitative methane monitoring and can be extended to a broad range of methane monitoring tasks."],"url":"http://arxiv.org/abs/2401.12870v1","category":"cs.CV"}
{"created":"2024-01-23 16:03:17","title":"TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks","abstract":"Language models (LMs) can solve tasks such as answering questions about tables or images by writing programs. However, using primitive functions often leads to verbose and error-prone programs, and higher-level functions require expert design. To enable better solutions without human labor, we ask code LMs to curate reusable high-level functions, and use them to write solutions. We present TROVE, a training-free method of inducing a verifiable and efficient toolbox of functions, by generating via using, growing, and periodically trimming the toolbox. On 11 datasets from math, table question answering, and image reasoning tasks, TROVE consistently yields simpler solutions with higher accuracy than baselines using CODELLAMA and previous methods using GPT, while using 79-98% smaller toolboxes. TROVE further enables 31% faster and 13% more accurate human verification than baselines. With the same pipeline, it creates diverse functions for varied tasks and datasets, providing insights into their individual characteristics.","sentences":["Language models (LMs) can solve tasks such as answering questions about tables or images by writing programs.","However, using primitive functions often leads to verbose and error-prone programs, and higher-level functions require expert design.","To enable better solutions without human labor, we ask code LMs to curate reusable high-level functions, and use them to write solutions.","We present TROVE, a training-free method of inducing a verifiable and efficient toolbox of functions, by generating via using, growing, and periodically trimming the toolbox.","On 11 datasets from math, table question answering, and image reasoning tasks, TROVE consistently yields simpler solutions with higher accuracy than baselines using CODELLAMA and previous methods using GPT, while using 79-98% smaller toolboxes.","TROVE further enables 31% faster and 13% more accurate human verification than baselines.","With the same pipeline, it creates diverse functions for varied tasks and datasets, providing insights into their individual characteristics."],"url":"http://arxiv.org/abs/2401.12869v1","category":"cs.AI"}
{"created":"2024-01-23 16:00:45","title":"Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported Coordination of Mobile Crowdsourcing","abstract":"Mobile crowdsourcing refers to systems where the completion of tasks necessarily requires physical movement of crowdworkers in an on-demand workforce. Evidence suggests that in such systems, tasks often get assigned to crowdworkers who struggle to complete those tasks successfully, resulting in high failure rates and low service quality. A promising solution to ensure higher quality of service is to continuously adapt the assignment and respond to failure-causing events by transferring tasks to better-suited workers who use different routes or vehicles. However, implementing task transfers in mobile crowdsourcing is difficult because workers are autonomous and may reject transfer requests. Moreover, task outcomes are uncertain and need to be predicted. In this paper, we propose different mechanisms to achieve outcome prediction and task coordination in mobile crowdsourcing. First, we analyze different data stream learning approaches for the prediction of task outcomes. Second, based on the suggested prediction model, we propose and evaluate two different approaches for task coordination with different degrees of autonomy: an opportunistic approach for crowdshipping with collaborative, but non-autonomous workers, and a market-based model with autonomous workers for crowdsensing.","sentences":["Mobile crowdsourcing refers to systems where the completion of tasks necessarily requires physical movement of crowdworkers in an on-demand workforce.","Evidence suggests that in such systems, tasks often get assigned to crowdworkers who struggle to complete those tasks successfully, resulting in high failure rates and low service quality.","A promising solution to ensure higher quality of service is to continuously adapt the assignment and respond to failure-causing events by transferring tasks to better-suited workers who use different routes or vehicles.","However, implementing task transfers in mobile crowdsourcing is difficult because workers are autonomous and may reject transfer requests.","Moreover, task outcomes are uncertain and need to be predicted.","In this paper, we propose different mechanisms to achieve outcome prediction and task coordination in mobile crowdsourcing.","First, we analyze different data stream learning approaches for the prediction of task outcomes.","Second, based on the suggested prediction model, we propose and evaluate two different approaches for task coordination with different degrees of autonomy: an opportunistic approach for crowdshipping with collaborative, but non-autonomous workers, and a market-based model with autonomous workers for crowdsensing."],"url":"http://arxiv.org/abs/2401.12866v1","category":"cs.AI"}
{"created":"2024-01-23 15:56:12","title":"Threshold Quantum State Tomography","abstract":"Quantum state tomography (QST) aims at reconstructing the state of a quantum system. However in conventional QST the number of measurements scales exponentially with the number of qubits. Here we propose a QST protocol, in which the introduction of a threshold allows one to drastically reduce the number of measurements required for the reconstruction of the state density matrix without compromising the result accuracy. In addition, one can also use the same approach to reconstruct an approximated density matrix depending on the available resources. We experimentally demonstrate this protocol by performing the tomography of states up to 7 qubits. We show that our approach can lead to the same accuracy of QST even when the number of measurements is reduced by more than two orders of magnitudes.","sentences":["Quantum state tomography (QST) aims at reconstructing the state of a quantum system.","However in conventional QST the number of measurements scales exponentially with the number of qubits.","Here we propose a QST protocol, in which the introduction of a threshold allows one to drastically reduce the number of measurements required for the reconstruction of the state density matrix without compromising the result accuracy.","In addition, one can also use the same approach to reconstruct an approximated density matrix depending on the available resources.","We experimentally demonstrate this protocol by performing the tomography of states up to 7 qubits.","We show that our approach can lead to the same accuracy of QST even when the number of measurements is reduced by more than two orders of magnitudes."],"url":"http://arxiv.org/abs/2401.12864v1","category":"quant-ph"}
{"created":"2024-01-23 15:56:11","title":"KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning","abstract":"Large Language Models (LLMs) have demonstrated impressive performance in natural language processing tasks by leveraging chain of thought (CoT) that enables step-by-step thinking. Extending LLMs with multimodal capabilities is the recent interest, but incurs computational cost and requires substantial hardware resources. To address these challenges, we propose KAM-CoT a framework that integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities for a comprehensive understanding of multimodal tasks. KAM-CoT adopts a two-stage training process with KG grounding to generate effective rationales and answers. By incorporating external knowledge from KGs during reasoning, the model gains a deeper contextual understanding reducing hallucinations and enhancing the quality of answers. This knowledge-augmented CoT reasoning empowers the model to handle questions requiring external context, providing more informed answers. Experimental findings show KAM-CoT outperforms the state-of-the-art methods. On the ScienceQA dataset, we achieve an average accuracy of 93.87%, surpassing GPT-3.5 (75.17%) by 18% and GPT-4 (83.99%) by 10%. Remarkably, KAM-CoT achieves these results with only 280M trainable parameters at a time, demonstrating its cost-efficiency and effectiveness.","sentences":["Large Language Models (LLMs) have demonstrated impressive performance in natural language processing tasks by leveraging chain of thought (CoT) that enables step-by-step thinking.","Extending LLMs with multimodal capabilities is the recent interest, but incurs computational cost and requires substantial hardware resources.","To address these challenges, we propose KAM-CoT a framework that integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities for a comprehensive understanding of multimodal tasks.","KAM-CoT adopts a two-stage training process with KG grounding to generate effective rationales and answers.","By incorporating external knowledge from KGs during reasoning, the model gains a deeper contextual understanding reducing hallucinations and enhancing the quality of answers.","This knowledge-augmented CoT reasoning empowers the model to handle questions requiring external context, providing more informed answers.","Experimental findings show KAM-CoT outperforms the state-of-the-art methods.","On the ScienceQA dataset, we achieve an average accuracy of 93.87%, surpassing GPT-3.5 (75.17%) by 18% and GPT-4 (83.99%) by 10%.","Remarkably, KAM-CoT achieves these results with only 280M trainable parameters at a time, demonstrating its cost-efficiency and effectiveness."],"url":"http://arxiv.org/abs/2401.12863v1","category":"cs.CL"}
{"created":"2024-01-23 15:52:57","title":"FedRSU: Federated Learning for Scene Flow Estimation on Roadside Units","abstract":"Roadside unit (RSU) can significantly improve the safety and robustness of autonomous vehicles through Vehicle-to-Everything (V2X) communication. Currently, the usage of a single RSU mainly focuses on real-time inference and V2X collaboration, while neglecting the potential value of the high-quality data collected by RSU sensors. Integrating the vast amounts of data from numerous RSUs can provide a rich source of data for model training. However, the absence of ground truth annotations and the difficulty of transmitting enormous volumes of data are two inevitable barriers to fully exploiting this hidden value. In this paper, we introduce FedRSU, an innovative federated learning framework for self-supervised scene flow estimation. In FedRSU, we present a recurrent self-supervision training paradigm, where for each RSU, the scene flow prediction of points at every timestamp can be supervised by its subsequent future multi-modality observation. Another key component of FedRSU is federated learning, where multiple devices collaboratively train an ML model while keeping the training data local and private. With the power of the recurrent self-supervised learning paradigm, FL is able to leverage innumerable underutilized data from RSU. To verify the FedRSU framework, we construct a large-scale multi-modality dataset RSU-SF. The dataset consists of 17 RSU clients, covering various scenarios, modalities, and sensor settings. Based on RSU-SF, we show that FedRSU can greatly improve model performance in ITS and provide a comprehensive benchmark under diverse FL scenarios. To the best of our knowledge, we provide the first real-world LiDAR-camera multi-modal dataset and benchmark for the FL community.","sentences":["Roadside unit (RSU) can significantly improve the safety and robustness of autonomous vehicles through Vehicle-to-Everything (V2X) communication.","Currently, the usage of a single RSU mainly focuses on real-time inference and V2X collaboration, while neglecting the potential value of the high-quality data collected by RSU sensors.","Integrating the vast amounts of data from numerous RSUs can provide a rich source of data for model training.","However, the absence of ground truth annotations and the difficulty of transmitting enormous volumes of data are two inevitable barriers to fully exploiting this hidden value.","In this paper, we introduce FedRSU, an innovative federated learning framework for self-supervised scene flow estimation.","In FedRSU, we present a recurrent self-supervision training paradigm, where for each RSU, the scene flow prediction of points at every timestamp can be supervised by its subsequent future multi-modality observation.","Another key component of FedRSU is federated learning, where multiple devices collaboratively train an ML model while keeping the training data local and private.","With the power of the recurrent self-supervised learning paradigm, FL is able to leverage innumerable underutilized data from RSU.","To verify the FedRSU framework, we construct a large-scale multi-modality dataset RSU-SF.","The dataset consists of 17 RSU clients, covering various scenarios, modalities, and sensor settings.","Based on RSU-SF, we show that FedRSU can greatly improve model performance in ITS and provide a comprehensive benchmark under diverse FL scenarios.","To the best of our knowledge, we provide the first real-world LiDAR-camera multi-modal dataset and benchmark for the FL community."],"url":"http://arxiv.org/abs/2401.12862v1","category":"cs.CV"}
{"created":"2024-01-23 15:52:35","title":"Secure Communication with Unreliable Entanglement Assistance","abstract":"Secure communication is considered with unreliable entanglement assistance, where the adversary may intercept the legitimate receiver's entanglement resource before communication takes place. The communication setting of unreliable assistance, without security aspects, was originally motivated by the extreme photon loss in practical communication systems. The operational principle is to adapt the transmission rate to the availability of entanglement assistance, without resorting to feedback and repetition. Here, we require secrecy as well. An achievable secrecy rate region is derived for general quantum wiretap channels, and a multi-letter secrecy capacity formula for the special class of degraded channels.","sentences":["Secure communication is considered with unreliable entanglement assistance, where the adversary may intercept the legitimate receiver's entanglement resource before communication takes place.","The communication setting of unreliable assistance, without security aspects, was originally motivated by the extreme photon loss in practical communication systems.","The operational principle is to adapt the transmission rate to the availability of entanglement assistance, without resorting to feedback and repetition.","Here, we require secrecy as well.","An achievable secrecy rate region is derived for general quantum wiretap channels, and a multi-letter secrecy capacity formula for the special class of degraded channels."],"url":"http://arxiv.org/abs/2401.12861v1","category":"quant-ph"}
{"created":"2024-01-23 15:45:31","title":"Simultaneous exercise recognition and evaluation in prescribed routines: Approach to virtual coaches","abstract":"Home-based physical therapies are effective if the prescribed exercises are correctly executed and patients adhere to these routines. This is specially important for older adults who can easily forget the guidelines from therapists. Inertial Measurement Units (IMUs) are commonly used for tracking exercise execution giving information of patients' motion data. In this work, we propose the use of Machine Learning techniques to recognize which exercise is being carried out and to assess if the recognized exercise is properly executed by using data from four IMUs placed on the person limbs. To the best of our knowledge, both tasks have never been addressed together as a unique complex task before. However, their combination is needed for the complete characterization of the performance of physical therapies. We evaluate the performance of six machine learning classifiers in three contexts: recognition and evaluation in a single classifier, recognition of correct exercises, excluding the wrongly performed exercises, and a two-stage approach that first recognizes the exercise and then evaluates it. We apply our proposal to a set of 8 exercises of the upper-and lower-limbs designed for maintaining elderly people health status. To do so, the motion of volunteers were monitored with 4 IMUs. We obtain accuracies of 88.4 \\% and the 91.4 \\% in the two initial scenarios. In the third one, the recognition provides an accuracy of 96.2 \\%, whereas the exercise evaluation varies between 93.6 \\% and 100.0 \\%. This work proves the feasibility of IMUs for a complete monitoring of physical therapies in which we can get information of which exercise is being performed and its quality, as a basis for designing virtual coaches.","sentences":["Home-based physical therapies are effective if the prescribed exercises are correctly executed and patients adhere to these routines.","This is specially important for older adults who can easily forget the guidelines from therapists.","Inertial Measurement Units (IMUs) are commonly used for tracking exercise execution giving information of patients' motion data.","In this work, we propose the use of Machine Learning techniques to recognize which exercise is being carried out and to assess if the recognized exercise is properly executed by using data from four IMUs placed on the person limbs.","To the best of our knowledge, both tasks have never been addressed together as a unique complex task before.","However, their combination is needed for the complete characterization of the performance of physical therapies.","We evaluate the performance of six machine learning classifiers in three contexts: recognition and evaluation in a single classifier, recognition of correct exercises, excluding the wrongly performed exercises, and a two-stage approach that first recognizes the exercise and then evaluates it.","We apply our proposal to a set of 8 exercises of the upper-and lower-limbs designed for maintaining elderly people health status.","To do so, the motion of volunteers were monitored with 4 IMUs.","We obtain accuracies of 88.4 \\% and the 91.4 \\% in the two initial scenarios.","In the third one, the recognition provides an accuracy of 96.2 \\%, whereas the exercise evaluation varies between 93.6 \\% and 100.0 \\%.","This work proves the feasibility of IMUs for a complete monitoring of physical therapies in which we can get information of which exercise is being performed and its quality, as a basis for designing virtual coaches."],"url":"http://arxiv.org/abs/2401.12857v1","category":"cs.HC"}
{"created":"2024-01-23 15:43:45","title":"On the relevance of quantum corrections to the matter stress-energy tensor in eternally expanding universes","abstract":"We study a toy-model of continuous infinite expansion of space-time with the flat start. We use as the gravitational background a conformaly flat metric with an exponentially growing factor in conformal time. We aim to clarify some properties of quantum fields in such a gravitational background. In particular, we calculate one-loop corrections to the Keldysh propagator to verify the fact of secular growth of the occupation number and anomalous quantum average in the massless scalar field theory with selfinteractions. We perform the calculation in arbitrary dimensions with the use of the Schwinger-Keldysh technique. We get a secular growth which is not of a kinetic type. We provide some results for the case of generic interaction $\\frac{\\lambda}{b!}\\phi^b$.","sentences":["We study a toy-model of continuous infinite expansion of space-time with the flat start.","We use as the gravitational background a conformaly flat metric with an exponentially growing factor in conformal time.","We aim to clarify some properties of quantum fields in such a gravitational background.","In particular, we calculate one-loop corrections to the Keldysh propagator to verify the fact of secular growth of the occupation number and anomalous quantum average in the massless scalar field theory with selfinteractions.","We perform the calculation in arbitrary dimensions with the use of the Schwinger-Keldysh technique.","We get a secular growth which is not of a kinetic type.","We provide some results for the case of generic interaction $\\frac{\\lambda}{b!}\\phi^b$."],"url":"http://arxiv.org/abs/2401.12855v1","category":"hep-th"}
{"created":"2024-01-23 15:42:55","title":"Optimization and Stabilization of Functional Renormalization Group Flows","abstract":"We revisit optimization of functional renormalization group flows by analyzing regularized loop integrals. This leads us to a principle, the Principle of Strongest Singularity, and a corresponding order relation which allows to order existing regularization schemes with respect to the stability of renormalization group flows. Moreover, the order relation can be used to construct new regulators in a systematic fashion. For studies of critical behavior, which require to follow renormalization group flows down to the deep infrared regime, such new regulators may turn out to be particularly useful. The general application of this principle is demonstrated with the aid of a scalar field theory which is solved over a wide range of scales with novel methods borrowed from numerical fluid dynamics.","sentences":["We revisit optimization of functional renormalization group flows by analyzing regularized loop integrals.","This leads us to a principle, the Principle of Strongest Singularity, and a corresponding order relation which allows to order existing regularization schemes with respect to the stability of renormalization group flows.","Moreover, the order relation can be used to construct new regulators in a systematic fashion.","For studies of critical behavior, which require to follow renormalization group flows down to the deep infrared regime, such new regulators may turn out to be particularly useful.","The general application of this principle is demonstrated with the aid of a scalar field theory which is solved over a wide range of scales with novel methods borrowed from numerical fluid dynamics."],"url":"http://arxiv.org/abs/2401.12854v1","category":"hep-ph"}
{"created":"2024-01-23 15:41:27","title":"Hyper-Realist Rendering: A Theoretical Framework","abstract":"This is the first paper in a series on hyper-realist rendering. In this paper, we introduce the concept of hyper-realist rendering and present a theoretical framework to obtain hyper-realist images. We are using the term Hyper-realism as an umbrella word that captures all types of visual artifacts that can evoke an impression of reality. The hyper-realist artifacts are visual representations that are not necessarily created by following logical and physical principles and can still be perceived as representations of reality. This idea stems from the principles of representational arts, which attain visually acceptable renderings of scenes without implementing strict physical laws of optics and materials. The objective of this work is to demonstrate that it is possible to obtain visually acceptable illusions of reality by employing such artistic approaches. With representational art methods, we can even obtain an alternate illusion of reality that looks more real even when it is not real. This paper demonstrates that it is common to create illusions of reality in visual arts with examples of paintings by representational artists. We propose an approach to obtain expressive local and global illuminations to obtain these stylistic illusions with a set of well-defined and formal methods.","sentences":["This is the first paper in a series on hyper-realist rendering.","In this paper, we introduce the concept of hyper-realist rendering and present a theoretical framework to obtain hyper-realist images.","We are using the term Hyper-realism as an umbrella word that captures all types of visual artifacts that can evoke an impression of reality.","The hyper-realist artifacts are visual representations that are not necessarily created by following logical and physical principles and can still be perceived as representations of reality.","This idea stems from the principles of representational arts, which attain visually acceptable renderings of scenes without implementing strict physical laws of optics and materials.","The objective of this work is to demonstrate that it is possible to obtain visually acceptable illusions of reality by employing such artistic approaches.","With representational art methods, we can even obtain an alternate illusion of reality that looks more real even when it is not real.","This paper demonstrates that it is common to create illusions of reality in visual arts with examples of paintings by representational artists.","We propose an approach to obtain expressive local and global illuminations to obtain these stylistic illusions with a set of well-defined and formal methods."],"url":"http://arxiv.org/abs/2401.12853v1","category":"cs.GR"}
{"created":"2024-01-23 15:39:18","title":"Control-Aware Trajectory Predictions for Communication-Efficient Drone Swarm Coordination in Cluttered Environments","abstract":"Swarms of Unmanned Aerial Vehicles (UAV) have demonstrated enormous potential in many industrial and commercial applications. However, before deploying UAVs in the real world, it is essential to ensure they can operate safely in complex environments, especially with limited communication capabilities. To address this challenge, we propose a control-aware learning-based trajectory prediction algorithm that can enable communication-efficient UAV swarm control in a cluttered environment. Specifically, our proposed algorithm can enable each UAV to predict the planned trajectories of its neighbors in scenarios with various levels of communication capabilities. The predicted planned trajectories will serve as input to a distributed model predictive control (DMPC) approach. The proposed algorithm combines (1) a trajectory compression and reconstruction model based on Variational Auto-Encoder, (2) a trajectory prediction model based on EvolveGCN, a graph convolutional network (GCN) that can handle dynamic graphs, and (3) a KKT-informed training approach that applies the Karush-Kuhn-Tucker (KKT) conditions in the training process to encode DMPC information into the trained neural network. We evaluate our proposed algorithm in a funnel-like environment. Results show that the proposed algorithm outperforms state-of-the-art benchmarks, providing close-to-optimal control performance and robustness to limited communication capabilities and measurement noises.","sentences":["Swarms of Unmanned Aerial Vehicles (UAV) have demonstrated enormous potential in many industrial and commercial applications.","However, before deploying UAVs in the real world, it is essential to ensure they can operate safely in complex environments, especially with limited communication capabilities.","To address this challenge, we propose a control-aware learning-based trajectory prediction algorithm that can enable communication-efficient UAV swarm control in a cluttered environment.","Specifically, our proposed algorithm can enable each UAV to predict the planned trajectories of its neighbors in scenarios with various levels of communication capabilities.","The predicted planned trajectories will serve as input to a distributed model predictive control (DMPC) approach.","The proposed algorithm combines (1) a trajectory compression and reconstruction model based on Variational Auto-Encoder, (2) a trajectory prediction model based on EvolveGCN, a graph convolutional network (GCN) that can handle dynamic graphs, and (3) a KKT-informed training approach that applies the Karush-Kuhn-Tucker (KKT) conditions in the training process to encode DMPC information into the trained neural network.","We evaluate our proposed algorithm in a funnel-like environment.","Results show that the proposed algorithm outperforms state-of-the-art benchmarks, providing close-to-optimal control performance and robustness to limited communication capabilities and measurement noises."],"url":"http://arxiv.org/abs/2401.12852v1","category":"cs.RO"}
{"created":"2024-01-23 15:35:50","title":"Classification of grapevine varieties using UAV hyperspectral imaging","abstract":"The classification of different grapevine varieties is a relevant phenotyping task in Precision Viticulture since it enables estimating the growth of vineyard rows dedicated to different varieties, among other applications concerning the wine industry. This task can be performed with destructive methods that require time-consuming tasks, including data collection and analysis in the laboratory. However, Unmanned Aerial Vehicles (UAV) provide a more efficient and less prohibitive approach to collecting hyperspectral data, despite acquiring noisier data. Therefore, the first task is the processing of these data to correct and downsample large amounts of data. In addition, the hyperspectral signatures of grape varieties are very similar. In this work, a Convolutional Neural Network (CNN) is proposed for classifying seventeen varieties of red and white grape variants. Rather than classifying single samples, these are processed together with their neighbourhood. Hence, the extraction of spatial and spectral features is addressed with 1) a spatial attention layer and 2) Inception blocks. The pipeline goes from processing to dataset elaboration, finishing with the training phase. The fitted model is evaluated in terms of response time, accuracy and data separability, and compared with other state-of-the-art CNNs for classifying hyperspectral data. Our network was proven to be much more lightweight with a reduced number of input bands, a lower number of trainable weights and therefore, reduced training time. Despite this, the evaluated metrics showed much better results for our network (~99% overall accuracy), in comparison with previous works barely achieving 81% OA.","sentences":["The classification of different grapevine varieties is a relevant phenotyping task in Precision Viticulture since it enables estimating the growth of vineyard rows dedicated to different varieties, among other applications concerning the wine industry.","This task can be performed with destructive methods that require time-consuming tasks, including data collection and analysis in the laboratory.","However, Unmanned Aerial Vehicles (UAV) provide a more efficient and less prohibitive approach to collecting hyperspectral data, despite acquiring noisier data.","Therefore, the first task is the processing of these data to correct and downsample large amounts of data.","In addition, the hyperspectral signatures of grape varieties are very similar.","In this work, a Convolutional Neural Network (CNN) is proposed for classifying seventeen varieties of red and white grape variants.","Rather than classifying single samples, these are processed together with their neighbourhood.","Hence, the extraction of spatial and spectral features is addressed with 1) a spatial attention layer and 2) Inception blocks.","The pipeline goes from processing to dataset elaboration, finishing with the training phase.","The fitted model is evaluated in terms of response time, accuracy and data separability, and compared with other state-of-the-art CNNs for classifying hyperspectral data.","Our network was proven to be much more lightweight with a reduced number of input bands, a lower number of trainable weights and therefore, reduced training time.","Despite this, the evaluated metrics showed much better results for our network (~99% overall accuracy), in comparison with previous works barely achieving 81% OA."],"url":"http://arxiv.org/abs/2401.12851v1","category":"cs.CV"}
{"created":"2024-01-23 15:35:44","title":"Overlap-aware End-to-End Supervised Hierarchical Graph Clustering for Speaker Diarization","abstract":"Speaker diarization, the task of segmenting an audio recording based on speaker identity, constitutes an important speech pre-processing step for several downstream applications. The conventional approach to diarization involves multiple steps of embedding extraction and clustering, which are often optimized in an isolated fashion. While end-to-end diarization systems attempt to learn a single model for the task, they are often cumbersome to train and require large supervised datasets. In this paper, we propose an end-to-end supervised hierarchical clustering algorithm based on graph neural networks (GNN), called End-to-end Supervised HierARchical Clustering (E-SHARC). The E-SHARC approach uses front-end mel-filterbank features as input and jointly learns an embedding extractor and the GNN clustering module, performing representation learning, metric learning, and clustering with end-to-end optimization. Further, with additional inputs from an external overlap detector, the E-SHARC approach is capable of predicting the speakers in the overlapping speech regions. The experimental evaluation on several benchmark datasets like AMI, VoxConverse and DISPLACE, illustrates that the proposed E-SHARC framework improves significantly over the state-of-art diarization systems.","sentences":["Speaker diarization, the task of segmenting an audio recording based on speaker identity, constitutes an important speech pre-processing step for several downstream applications.","The conventional approach to diarization involves multiple steps of embedding extraction and clustering, which are often optimized in an isolated fashion.","While end-to-end diarization systems attempt to learn a single model for the task, they are often cumbersome to train and require large supervised datasets.","In this paper, we propose an end-to-end supervised hierarchical clustering algorithm based on graph neural networks (GNN), called End-to-end Supervised HierARchical Clustering (E-SHARC).","The E-SHARC approach uses front-end mel-filterbank features as input and jointly learns an embedding extractor and the GNN clustering module, performing representation learning, metric learning, and clustering with end-to-end optimization.","Further, with additional inputs from an external overlap detector, the E-SHARC approach is capable of predicting the speakers in the overlapping speech regions.","The experimental evaluation on several benchmark datasets like AMI, VoxConverse and DISPLACE, illustrates that the proposed E-SHARC framework improves significantly over the state-of-art diarization systems."],"url":"http://arxiv.org/abs/2401.12850v1","category":"eess.AS"}
{"created":"2024-01-23 15:33:30","title":"Learning safety critics via a non-contractive binary bellman operator","abstract":"The inability to naturally enforce safety in Reinforcement Learning (RL), with limited failures, is a core challenge impeding its use in real-world applications. One notion of safety of vast practical relevance is the ability to avoid (unsafe) regions of the state space. Though such a safety goal can be captured by an action-value-like function, a.k.a. safety critics, the associated operator lacks the desired contraction and uniqueness properties that the classical Bellman operator enjoys. In this work, we overcome the non-contractiveness of safety critic operators by leveraging that safety is a binary property. To that end, we study the properties of the binary safety critic associated with a deterministic dynamical system that seeks to avoid reaching an unsafe region. We formulate the corresponding binary Bellman equation (B2E) for safety and study its properties. While the resulting operator is still non-contractive, we fully characterize its fixed points representing--except for a spurious solution--maximal persistently safe regions of the state space that can always avoid failure. We provide an algorithm that, by design, leverages axiomatic knowledge of safe data to avoid spurious fixed points.","sentences":["The inability to naturally enforce safety in Reinforcement Learning (RL), with limited failures, is a core challenge impeding its use in real-world applications.","One notion of safety of vast practical relevance is the ability to avoid (unsafe) regions of the state space.","Though such a safety goal can be captured by an action-value-like function, a.k.a. safety critics, the associated operator lacks the desired contraction and uniqueness properties that the classical Bellman operator enjoys.","In this work, we overcome the non-contractiveness of safety critic operators by leveraging that safety is a binary property.","To that end, we study the properties of the binary safety critic associated with a deterministic dynamical system that seeks to avoid reaching an unsafe region.","We formulate the corresponding binary Bellman equation (B2E) for safety and study its properties.","While the resulting operator is still non-contractive, we fully characterize its fixed points representing--except for a spurious solution--maximal persistently safe regions of the state space that can always avoid failure.","We provide an algorithm that, by design, leverages axiomatic knowledge of safe data to avoid spurious fixed points."],"url":"http://arxiv.org/abs/2401.12849v1","category":"cs.LG"}
{"created":"2024-01-23 15:32:33","title":"Optimal Evasion from a Sensing-Limited Pursuer","abstract":"This paper investigates a partial-information pursuit evasion game in which the Pursuer has a limited-range sensor to detect the Evader. Given a fixed final time, we derive the optimal evasion strategy for the Evader to maximize its distance from the pursuer at the end. Our analysis reveals that in certain parametric regimes, the optimal Evasion strategy involves a 'risky' maneuver, where the Evader's trajectory comes extremely close to the pursuer's sensing boundary before moving behind the Pursuer. Additionally, we explore a special case in which the Pursuer can choose the final time. In this scenario, we determine a (Nash) equilibrium pair for both the final time and the evasion strategy.","sentences":["This paper investigates a partial-information pursuit evasion game in which the Pursuer has a limited-range sensor to detect the Evader.","Given a fixed final time, we derive the optimal evasion strategy for the Evader to maximize its distance from the pursuer at the end.","Our analysis reveals that in certain parametric regimes, the optimal Evasion strategy involves a 'risky' maneuver, where the Evader's trajectory comes extremely close to the pursuer's sensing boundary before moving behind the Pursuer.","Additionally, we explore a special case in which the Pursuer can choose the final time.","In this scenario, we determine a (Nash) equilibrium pair for both the final time and the evasion strategy."],"url":"http://arxiv.org/abs/2401.12848v1","category":"cs.GT"}
{"created":"2024-01-23 15:29:26","title":"How well can large language models explain business processes?","abstract":"Large Language Models (LLMs) are likely to play a prominent role in future AI-augmented business process management systems (ABPMSs) catering functionalities across all system lifecycle stages. One such system's functionality is Situation-Aware eXplainability (SAX), which relates to generating causally sound and yet human-interpretable explanations that take into account the process context in which the explained condition occurred. In this paper, we present the SAX4BPM framework developed to generate SAX explanations. The SAX4BPM suite consists of a set of services and a central knowledge repository. The functionality of these services is to elicit the various knowledge ingredients that underlie SAX explanations. A key innovative component among these ingredients is the causal process execution view. In this work, we integrate the framework with an LLM to leverage its power to synthesize the various input ingredients for the sake of improved SAX explanations. Since the use of LLMs for SAX is also accompanied by a certain degree of doubt related to its capacity to adequately fulfill SAX along with its tendency for hallucination and lack of inherent capacity to reason, we pursued a methodological evaluation of the quality of the generated explanations. To this aim, we developed a designated scale and conducted a rigorous user study. Our findings show that the input presented to the LLMs aided with the guard-railing of its performance, yielding SAX explanations having better-perceived fidelity. This improvement is moderated by the perception of trust and curiosity. More so, this improvement comes at the cost of the perceived interpretability of the explanation.","sentences":["Large Language Models (LLMs) are likely to play a prominent role in future AI-augmented business process management systems (ABPMSs) catering functionalities across all system lifecycle stages.","One such system's functionality is Situation-Aware eXplainability (SAX), which relates to generating causally sound and yet human-interpretable explanations that take into account the process context in which the explained condition occurred.","In this paper, we present the SAX4BPM framework developed to generate SAX explanations.","The SAX4BPM suite consists of a set of services and a central knowledge repository.","The functionality of these services is to elicit the various knowledge ingredients that underlie SAX explanations.","A key innovative component among these ingredients is the causal process execution view.","In this work, we integrate the framework with an LLM to leverage its power to synthesize the various input ingredients for the sake of improved SAX explanations.","Since the use of LLMs for SAX is also accompanied by a certain degree of doubt related to its capacity to adequately fulfill SAX along with its tendency for hallucination and lack of inherent capacity to reason, we pursued a methodological evaluation of the quality of the generated explanations.","To this aim, we developed a designated scale and conducted a rigorous user study.","Our findings show that the input presented to the LLMs aided with the guard-railing of its performance, yielding SAX explanations having better-perceived fidelity.","This improvement is moderated by the perception of trust and curiosity.","More so, this improvement comes at the cost of the perceived interpretability of the explanation."],"url":"http://arxiv.org/abs/2401.12846v1","category":"cs.AI"}
{"created":"2024-01-23 15:26:28","title":"Gelation and localization in multicomponent coagulation with multiplicative kernel through branching processes","abstract":"The multicomponent coagulation equation is a generalisation of the Smoluchowski coagulation equation in which size of a particle is described by a vector. As with the original Smoluchowski equation, the multicomponent coagulation equation features gelation when supplied with a multiplicative kernel. Additionally, a new type of behaviour called localization is observed due to the multivariate nature of the particle size distribution. Here we extend and apply the branching process representation technique, which we introduced to study differential equations in our previous work, to find a concise probabilistic solution of the multicomponent coagulation equation supplied with monodisperse initial conditions and provide short proofs for the gelation time and localization.","sentences":["The multicomponent coagulation equation is a generalisation of the Smoluchowski coagulation equation in which size of a particle is described by a vector.","As with the original Smoluchowski equation, the multicomponent coagulation equation features gelation when supplied with a multiplicative kernel.","Additionally, a new type of behaviour called localization is observed due to the multivariate nature of the particle size distribution.","Here we extend and apply the branching process representation technique, which we introduced to study differential equations in our previous work, to find a concise probabilistic solution of the multicomponent coagulation equation supplied with monodisperse initial conditions and provide short proofs for the gelation time and localization."],"url":"http://arxiv.org/abs/2401.12844v1","category":"math-ph"}
{"created":"2024-01-23 15:25:21","title":"An embedding-based distance for temporal graphs","abstract":"We define a distance between temporal graphs based on graph embeddings built using time-respecting random walks. We study both the case of matched graphs, when there exists a known relation between the nodes, and the unmatched case, when such a relation is unavailable and the graphs may be of different sizes. We illustrate the interest of our distance definition, using both real and synthetic temporal network data, by showing its ability to discriminate between graphs with different structural and temporal properties. Leveraging state-of-the-art machine learning techniques, we propose an efficient implementation of distance computation that is viable for large-scale temporal graphs.","sentences":["We define a distance between temporal graphs based on graph embeddings built using time-respecting random walks.","We study both the case of matched graphs, when there exists a known relation between the nodes, and the unmatched case, when such a relation is unavailable and the graphs may be of different sizes.","We illustrate the interest of our distance definition, using both real and synthetic temporal network data, by showing its ability to discriminate between graphs with different structural and temporal properties.","Leveraging state-of-the-art machine learning techniques, we propose an efficient implementation of distance computation that is viable for large-scale temporal graphs."],"url":"http://arxiv.org/abs/2401.12843v1","category":"cs.SI"}
{"created":"2024-01-23 15:23:13","title":"Iterated Relevance Matrix Analysis (IRMA) for the identification of class-discriminative subspaces","abstract":"We introduce and investigate the iterated application of Generalized Matrix Learning Vector Quantizaton for the analysis of feature relevances in classification problems, as well as for the construction of class-discriminative subspaces. The suggested Iterated Relevance Matrix Analysis (IRMA) identifies a linear subspace representing the classification specific information of the considered data sets using Generalized Matrix Learning Vector Quantization (GMLVQ). By iteratively determining a new discriminative subspace while projecting out all previously identified ones, a combined subspace carrying all class-specific information can be found. This facilitates a detailed analysis of feature relevances, and enables improved low-dimensional representations and visualizations of labeled data sets. Additionally, the IRMA-based class-discriminative subspace can be used for dimensionality reduction and the training of robust classifiers with potentially improved performance.","sentences":["We introduce and investigate the iterated application of Generalized Matrix Learning Vector Quantizaton for the analysis of feature relevances in classification problems, as well as for the construction of class-discriminative subspaces.","The suggested Iterated Relevance Matrix Analysis (IRMA) identifies a linear subspace representing the classification specific information of the considered data sets using Generalized Matrix Learning Vector Quantization (GMLVQ).","By iteratively determining a new discriminative subspace while projecting out all previously identified ones, a combined subspace carrying all class-specific information can be found.","This facilitates a detailed analysis of feature relevances, and enables improved low-dimensional representations and visualizations of labeled data sets.","Additionally, the IRMA-based class-discriminative subspace can be used for dimensionality reduction and the training of robust classifiers with potentially improved performance."],"url":"http://arxiv.org/abs/2401.12842v1","category":"cs.LG"}
{"created":"2024-01-23 15:18:20","title":"SGTR+: End-to-end Scene Graph Generation with Transformer","abstract":"Scene Graph Generation (SGG) remains a challenging visual understanding task due to its compositional property. Most previous works adopt a bottom-up, two-stage or point-based, one-stage approach, which often suffers from high time complexity or suboptimal designs. In this work, we propose a novel SGG method to address the aforementioned issues, formulating the task as a bipartite graph construction problem. To address the issues above, we create a transformer-based end-to-end framework to generate the entity and entity-aware predicate proposal set, and infer directed edges to form relation triplets. Moreover, we design a graph assembling module to infer the connectivity of the bipartite scene graph based on our entity-aware structure, enabling us to generate the scene graph in an end-to-end manner. Based on bipartite graph assembling paradigm, we further propose a new technical design to address the efficacy of entity-aware modeling and optimization stability of graph assembling. Equipped with the enhanced entity-aware design, our method achieves optimal performance and time-complexity. Extensive experimental results show that our design is able to achieve the state-of-the-art or comparable performance on three challenging benchmarks, surpassing most of the existing approaches and enjoying higher efficiency in inference. Code is available: https://github.com/Scarecrow0/SGTR","sentences":["Scene Graph Generation (SGG) remains a challenging visual understanding task due to its compositional property.","Most previous works adopt a bottom-up, two-stage or point-based, one-stage approach, which often suffers from high time complexity or suboptimal designs.","In this work, we propose a novel SGG method to address the aforementioned issues, formulating the task as a bipartite graph construction problem.","To address the issues above, we create a transformer-based end-to-end framework to generate the entity and entity-aware predicate proposal set, and infer directed edges to form relation triplets.","Moreover, we design a graph assembling module to infer the connectivity of the bipartite scene graph based on our entity-aware structure, enabling us to generate the scene graph in an end-to-end manner.","Based on bipartite graph assembling paradigm, we further propose a new technical design to address the efficacy of entity-aware modeling and optimization stability of graph assembling.","Equipped with the enhanced entity-aware design, our method achieves optimal performance and time-complexity.","Extensive experimental results show that our design is able to achieve the state-of-the-art or comparable performance on three challenging benchmarks, surpassing most of the existing approaches and enjoying higher efficiency in inference.","Code is available: https://github.com/Scarecrow0/SGTR"],"url":"http://arxiv.org/abs/2401.12835v1","category":"cs.CV"}
{"created":"2024-01-23 15:07:49","title":"Enhancing Next Destination Prediction: A Novel LSTM Approach Using Real-World Airline Data","abstract":"In the modern transportation industry, accurate prediction of travelers' next destinations brings multiple benefits to companies, such as customer satisfaction and targeted marketing. This study focuses on developing a precise model that captures the sequential patterns and dependencies in travel data, enabling accurate predictions of individual travelers' future destinations. To achieve this, a novel model architecture with a sliding window approach based on Long Short-Term Memory (LSTM) is proposed for destination prediction in the transportation industry. The experimental results highlight satisfactory performance and high scores achieved by the proposed model across different data sizes and performance metrics. This research contributes to advancing destination prediction methods, empowering companies to deliver personalized recommendations and optimize customer experiences in the dynamic travel landscape.","sentences":["In the modern transportation industry, accurate prediction of travelers' next destinations brings multiple benefits to companies, such as customer satisfaction and targeted marketing.","This study focuses on developing a precise model that captures the sequential patterns and dependencies in travel data, enabling accurate predictions of individual travelers' future destinations.","To achieve this, a novel model architecture with a sliding window approach based on Long Short-Term Memory (LSTM) is proposed for destination prediction in the transportation industry.","The experimental results highlight satisfactory performance and high scores achieved by the proposed model across different data sizes and performance metrics.","This research contributes to advancing destination prediction methods, empowering companies to deliver personalized recommendations and optimize customer experiences in the dynamic travel landscape."],"url":"http://arxiv.org/abs/2401.12830v1","category":"cs.LG"}
{"created":"2024-01-23 15:01:49","title":"Digital Twin-Based Network Management for Better QoE in Multicast Short Video Streaming","abstract":"Multicast short video streaming can enhance bandwidth utilization by enabling simultaneous video transmission to multiple users over shared wireless channels. The existing network management schemes mainly rely on the sequential buffering principle and general quality of experience (QoE) model, which may deteriorate QoE when users' swipe behaviors exhibit distinct spatiotemporal variation. In this paper, we propose a digital twin (DT)-based network management scheme to enhance QoE. Firstly, user status emulated by the DT is utilized to estimate the transmission capabilities and watching probability distributions of sub-multicast groups (SMGs) for an adaptive segment buffering. The SMGs' buffers are aligned to the unique virtual buffers managed by the DT for a fine-grained buffer update. Then, a multicast QoE model consisting of rebuffering time, video quality, and quality variation is developed, by considering the mutual influence of segment buffering among SMGs. Finally, a joint optimization problem of segment version selection and slot division is formulated to maximize QoE. To efficiently solve the problem, a data-model-driven algorithm is proposed by integrating a convex optimization method and a deep reinforcement learning algorithm. Simulation results based on the real-world dataset demonstrate that the proposed DT-based network management scheme outperforms benchmark schemes in terms of QoE improvement.","sentences":["Multicast short video streaming can enhance bandwidth utilization by enabling simultaneous video transmission to multiple users over shared wireless channels.","The existing network management schemes mainly rely on the sequential buffering principle and general quality of experience (QoE) model, which may deteriorate QoE when users' swipe behaviors exhibit distinct spatiotemporal variation.","In this paper, we propose a digital twin (DT)-based network management scheme to enhance QoE.","Firstly, user status emulated by the DT is utilized to estimate the transmission capabilities and watching probability distributions of sub-multicast groups (SMGs) for an adaptive segment buffering.","The SMGs' buffers are aligned to the unique virtual buffers managed by the DT for a fine-grained buffer update.","Then, a multicast QoE model consisting of rebuffering time, video quality, and quality variation is developed, by considering the mutual influence of segment buffering among SMGs.","Finally, a joint optimization problem of segment version selection and slot division is formulated to maximize QoE. To efficiently solve the problem, a data-model-driven algorithm is proposed by integrating a convex optimization method and a deep reinforcement learning algorithm.","Simulation results based on the real-world dataset demonstrate that the proposed DT-based network management scheme outperforms benchmark schemes in terms of QoE improvement."],"url":"http://arxiv.org/abs/2401.12826v1","category":"cs.NI"}
{"created":"2024-01-23 14:59:46","title":"MAPPING: Debiasing Graph Neural Networks for Fair Node Classification with Limited Sensitive Information Leakage","abstract":"Despite remarkable success in diverse web-based applications, Graph Neural Networks(GNNs) inherit and further exacerbate historical discrimination and social stereotypes, which critically hinder their deployments in high-stake domains such as online clinical diagnosis, financial crediting, etc. However, current fairness research that primarily craft on i.i.d data, cannot be trivially replicated to non-i.i.d. graph structures with topological dependence among samples. Existing fair graph learning typically favors pairwise constraints to achieve fairness but fails to cast off dimensional limitations and generalize them into multiple sensitive attributes; besides, most studies focus on in-processing techniques to enforce and calibrate fairness, constructing a model-agnostic debiasing GNN framework at the pre-processing stage to prevent downstream misuses and improve training reliability is still largely under-explored. Furthermore, previous work on GNNs tend to enhance either fairness or privacy individually but few probe into their interplays. In this paper, we propose a novel model-agnostic debiasing framework named MAPPING (\\underline{M}asking \\underline{A}nd \\underline{P}runing and Message-\\underline{P}assing train\\underline{ING}) for fair node classification, in which we adopt the distance covariance($dCov$)-based fairness constraints to simultaneously reduce feature and topology biases in arbitrary dimensions, and combine them with adversarial debiasing to confine the risks of attribute inference attacks. Experiments on real-world datasets with different GNN variants demonstrate the effectiveness and flexibility of MAPPING. Our results show that MAPPING can achieve better trade-offs between utility and fairness, and mitigate privacy risks of sensitive information leakage.","sentences":["Despite remarkable success in diverse web-based applications, Graph Neural Networks(GNNs) inherit and further exacerbate historical discrimination and social stereotypes, which critically hinder their deployments in high-stake domains such as online clinical diagnosis, financial crediting, etc.","However, current fairness research that primarily craft on i.i.d data, cannot be trivially replicated to non-i.i.d. graph structures with topological dependence among samples.","Existing fair graph learning typically favors pairwise constraints to achieve fairness but fails to cast off dimensional limitations and generalize them into multiple sensitive attributes; besides, most studies focus on in-processing techniques to enforce and calibrate fairness, constructing a model-agnostic debiasing GNN framework at the pre-processing stage to prevent downstream misuses and improve training reliability is still largely under-explored.","Furthermore, previous work on GNNs tend to enhance either fairness or privacy individually but few probe into their interplays.","In this paper, we propose a novel model-agnostic debiasing framework named MAPPING (\\underline{M}asking \\underline{A}nd \\underline{P}runing and Message-\\underline{P}assing train\\underline{ING}) for fair node classification, in which we adopt the distance covariance($dCov$)-based fairness constraints to simultaneously reduce feature and topology biases in arbitrary dimensions, and combine them with adversarial debiasing to confine the risks of attribute inference attacks.","Experiments on real-world datasets with different GNN variants demonstrate the effectiveness and flexibility of MAPPING.","Our results show that MAPPING can achieve better trade-offs between utility and fairness, and mitigate privacy risks of sensitive information leakage."],"url":"http://arxiv.org/abs/2401.12824v1","category":"cs.LG"}
{"created":"2024-01-23 14:55:46","title":"Deep Learning Based Simulators for the Phosphorus Removal Process Control in Wastewater Treatment via Deep Reinforcement Learning Algorithms","abstract":"Phosphorus removal is vital in wastewater treatment to reduce reliance on limited resources. Deep reinforcement learning (DRL) is a machine learning technique that can optimize complex and nonlinear systems, including the processes in wastewater treatment plants, by learning control policies through trial and error. However, applying DRL to chemical and biological processes is challenging due to the need for accurate simulators. This study trained six models to identify the phosphorus removal process and used them to create a simulator for the DRL environment. Although the models achieved high accuracy (>97%), uncertainty and incorrect prediction behavior limited their performance as simulators over longer horizons. Compounding errors in the models' predictions were identified as one of the causes of this problem. This approach for improving process control involves creating simulation environments for DRL algorithms, using data from supervisory control and data acquisition (SCADA) systems with a sufficient historical horizon without complex system modeling or parameter estimation.","sentences":["Phosphorus removal is vital in wastewater treatment to reduce reliance on limited resources.","Deep reinforcement learning (DRL) is a machine learning technique that can optimize complex and nonlinear systems, including the processes in wastewater treatment plants, by learning control policies through trial and error.","However, applying DRL to chemical and biological processes is challenging due to the need for accurate simulators.","This study trained six models to identify the phosphorus removal process and used them to create a simulator for the DRL environment.","Although the models achieved high accuracy (>97%), uncertainty and incorrect prediction behavior limited their performance as simulators over longer horizons.","Compounding errors in the models' predictions were identified as one of the causes of this problem.","This approach for improving process control involves creating simulation environments for DRL algorithms, using data from supervisory control and data acquisition (SCADA) systems with a sufficient historical horizon without complex system modeling or parameter estimation."],"url":"http://arxiv.org/abs/2401.12822v1","category":"eess.SY"}
{"created":"2024-01-23 14:53:32","title":"DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained Self-supervised Vision Transformer","abstract":"Successive proposals of several self-supervised training schemes continue to emerge, taking one step closer to developing a universal foundation model. In this process, the unsupervised downstream tasks are recognized as one of the evaluation methods to validate the quality of visual features learned with a self-supervised training scheme. However, unsupervised dense semantic segmentation has not been explored as a downstream task, which can utilize and evaluate the quality of semantic information introduced in patch-level feature representations during self-supervised training of a vision transformer. Therefore, this paper proposes a novel data-driven approach for unsupervised semantic segmentation (DatUS^2) as a downstream task. DatUS^2 generates semantically consistent and dense pseudo annotate segmentation masks for the unlabeled image dataset without using any visual-prior or synchronized data. We compare these pseudo-annotated segmentation masks with ground truth masks for evaluating recent self-supervised training schemes to learn shared semantic properties at the patch level and discriminative semantic properties at the segment level. Finally, we evaluate existing state-of-the-art self-supervised training schemes with our proposed downstream task, i.e., DatUS^2. Also, the best version of DatUS^2 outperforms the existing state-of-the-art method for the unsupervised dense semantic segmentation task with 15.02% MiOU and 21.47% Pixel accuracy on the SUIM dataset. It also achieves a competitive level of accuracy for a large-scale and complex dataset, i.e., the COCO dataset.","sentences":["Successive proposals of several self-supervised training schemes continue to emerge, taking one step closer to developing a universal foundation model.","In this process, the unsupervised downstream tasks are recognized as one of the evaluation methods to validate the quality of visual features learned with a self-supervised training scheme.","However, unsupervised dense semantic segmentation has not been explored as a downstream task, which can utilize and evaluate the quality of semantic information introduced in patch-level feature representations during self-supervised training of a vision transformer.","Therefore, this paper proposes a novel data-driven approach for unsupervised semantic segmentation (DatUS^2) as a downstream task.","DatUS^2 generates semantically consistent and dense pseudo annotate segmentation masks for the unlabeled image dataset without using any visual-prior or synchronized data.","We compare these pseudo-annotated segmentation masks with ground truth masks for evaluating recent self-supervised training schemes to learn shared semantic properties at the patch level and discriminative semantic properties at the segment level.","Finally, we evaluate existing state-of-the-art self-supervised training schemes with our proposed downstream task, i.e., DatUS^2.","Also, the best version of DatUS^2 outperforms the existing state-of-the-art method for the unsupervised dense semantic segmentation task with 15.02% MiOU and 21.47% Pixel accuracy on the SUIM dataset.","It also achieves a competitive level of accuracy for a large-scale and complex dataset, i.e., the COCO dataset."],"url":"http://arxiv.org/abs/2401.12820v1","category":"cs.CV"}
{"created":"2024-01-23 14:53:20","title":"Dynamic Layer Tying for Parameter-Efficient Transformers","abstract":"In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer $i$ independently or to copy the weights of a previous layer $j<i$. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.","sentences":["In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together.","Every few iterations, the RL agent is asked whether to train each layer $i$ independently or to copy the weights of a previous layer $j<i$.","This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique.","Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters.","In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method."],"url":"http://arxiv.org/abs/2401.12819v1","category":"cs.LG"}
{"created":"2024-01-23 14:53:00","title":"Binomial Channel: On the Capacity-Achieving Distribution and Bounds on the Capacity","abstract":"This work considers a binomial noise channel. The paper can be roughly divided into two parts. The first part is concerned with the properties of the capacity-achieving distribution. In particular, for the binomial channel, it is not known if the capacity-achieving distribution is unique since the output space is finite (i.e., supported on integers $0, \\ldots, n)$ and the input space is infinite (i.e., supported on the interval $[0,1]$), and there are multiple distributions that induce the same output distribution. This paper shows that the capacity-achieving distribution is unique by appealing to the total positivity property of the binomial kernel. In addition, we provide upper and lower bounds on the cardinality of the support of the capacity-achieving distribution. Specifically, an upper bound of order $ \\frac{n}{2}$ is shown, which improves on the previous upper bound of order $n$ due to Witsenhausen. Moreover, a lower bound of order $\\sqrt{n}$ is shown. Finally, additional information about the locations and probability values of the support points is established.   The second part of the paper focuses on deriving upper and lower bounds on capacity. In particular, firm bounds are established for all $n$ that show that the capacity scales as $\\frac{1}{2} \\log(n)$.","sentences":["This work considers a binomial noise channel.","The paper can be roughly divided into two parts.","The first part is concerned with the properties of the capacity-achieving distribution.","In particular, for the binomial channel, it is not known if the capacity-achieving distribution is unique since the output space is finite (i.e., supported on integers $0, \\ldots, n)$ and the input space is infinite (i.e., supported on the interval $[0,1]$), and there are multiple distributions that induce the same output distribution.","This paper shows that the capacity-achieving distribution is unique by appealing to the total positivity property of the binomial kernel.","In addition, we provide upper and lower bounds on the cardinality of the support of the capacity-achieving distribution.","Specifically, an upper bound of order $ \\frac{n}{2}$ is shown, which improves on the previous upper bound of order $n$ due to Witsenhausen.","Moreover, a lower bound of order $\\sqrt{n}$ is shown.","Finally, additional information about the locations and probability values of the support points is established.   ","The second part of the paper focuses on deriving upper and lower bounds on capacity.","In particular, firm bounds are established for all $n$ that show that the capacity scales as $\\frac{1}{2} \\log(n)$."],"url":"http://arxiv.org/abs/2401.12818v1","category":"cs.IT"}
{"created":"2024-01-23 14:51:42","title":"Ray-Singer Torsion, Topological Strings and Black Holes","abstract":"Genus one amplitude for topological strings on Calabi-Yau 3-folds can be computed using mirror symmetry: The partition function at genus one gets mapped to a holomorphic version of Ray-Singer torsion on the mirror Calabi-Yau. On the other hand it can be shown by a physical argument that this gives a curvature squared correction term to the gravitational action. This in paticular leads to an effective quantum gravity cutoff known as the species scale, which varies over moduli space of Calabi-Yau manifolds. This resolves some of the puzzles associated to the entropy of small black holes when there are a large number of light species of particles. Thus Ray-Singer torsion, via its connection to topological strings at genus one, provides a measure of light degrees of freedom of four dimensional N=2 supergravity theories. Based on a talk given on May 12th, 2023 at the Singer Memorial Conference, MIT.","sentences":["Genus one amplitude for topological strings on Calabi-Yau 3-folds can be computed using mirror symmetry:","The partition function at genus one gets mapped to a holomorphic version of Ray-Singer torsion on the mirror Calabi-Yau.","On the other hand it can be shown by a physical argument that this gives a curvature squared correction term to the gravitational action.","This in paticular leads to an effective quantum gravity cutoff known as the species scale, which varies over moduli space of Calabi-Yau manifolds.","This resolves some of the puzzles associated to the entropy of small black holes when there are a large number of light species of particles.","Thus Ray-Singer torsion, via its connection to topological strings at genus one, provides a measure of light degrees of freedom of four dimensional N=2 supergravity theories.","Based on a talk given on May 12th, 2023 at the Singer Memorial Conference, MIT."],"url":"http://arxiv.org/abs/2401.12816v1","category":"hep-th"}
{"created":"2024-01-23 14:51:39","title":"COREC: Concurrent Non-Blocking Single-Queue Receive Driver for Low Latency Networking","abstract":"Existing network stacks tackle performance and scalability aspects by relying on multiple receive queues. However, at software level, each queue is processed by a single thread, which prevents simultaneous work on the same queue and limits performance in terms of tail latency. To overcome this limitation, we introduce COREC, the first software implementation of a concurrent non-blocking single-queue receive driver. By sharing a single queue among multiple threads, workload distribution is improved, leading to a work-conserving policy for network stacks. On the technical side, instead of relying on traditional critical sections - which would sequentialize the operations by threads - COREC coordinates the threads that concurrently access the same receive queue in non-blocking manner via atomic machine instructions from the Read-Modify-Write (RMW) class. These instructions allow threads to access and update memory locations atomically, based on specific conditions, such as the matching of a target value selected by the thread. Also, they enable making any update globally visible in the memory hierarchy, bypassing interference on memory consistency caused by the CPU store buffers. Extensive evaluation results demonstrate that the possible additional reordering, which our approach may occasionally cause, is non-critical and has minimal impact on performance, even in the worst-case scenario of a single large TCP flow, with performance impairments accounting to at most 2-3 percent. Conversely, substantial latency gains are achieved when handling UDP traffic, real-world traffic mix, and multiple shorter TCP flows.","sentences":["Existing network stacks tackle performance and scalability aspects by relying on multiple receive queues.","However, at software level, each queue is processed by a single thread, which prevents simultaneous work on the same queue and limits performance in terms of tail latency.","To overcome this limitation, we introduce COREC, the first software implementation of a concurrent non-blocking single-queue receive driver.","By sharing a single queue among multiple threads, workload distribution is improved, leading to a work-conserving policy for network stacks.","On the technical side, instead of relying on traditional critical sections - which would sequentialize the operations by threads - COREC coordinates the threads that concurrently access the same receive queue in non-blocking manner via atomic machine instructions from the Read-Modify-Write (RMW) class.","These instructions allow threads to access and update memory locations atomically, based on specific conditions, such as the matching of a target value selected by the thread.","Also, they enable making any update globally visible in the memory hierarchy, bypassing interference on memory consistency caused by the CPU store buffers.","Extensive evaluation results demonstrate that the possible additional reordering, which our approach may occasionally cause, is non-critical and has minimal impact on performance, even in the worst-case scenario of a single large TCP flow, with performance impairments accounting to at most 2-3 percent.","Conversely, substantial latency gains are achieved when handling UDP traffic, real-world traffic mix, and multiple shorter TCP flows."],"url":"http://arxiv.org/abs/2401.12815v1","category":"cs.NI"}
{"created":"2024-01-23 14:50:46","title":"$b$-Hurwitz numbers from Whittaker vectors for $\\mathcal{W}$-algebras","abstract":"We show that $b$-Hurwitz numbers with a rational weight are obtained by taking an explicit limit of a Whittaker vector for the $\\mathcal{W}$-algebra of type $A$. Our result is a vast generalization of several previous results that treated the monotone case, and the cases of quadratic and cubic polynomial weights. It also provides an interpretation of the associated Whittaker vector in terms of generalized branched coverings that might be of independent interest. Our result is new even in the special case $b=0$ that corresponds to classical hypergeometric Hurwitz numbers, and implies that they are governed by the topological recursion of Eynard-Orantin. This gives an independent proof of the recent result of Bychkov-Dunin-Barkowski-Kazarian-Shadrin.","sentences":["We show that $b$-Hurwitz numbers with a rational weight are obtained by taking an explicit limit of a Whittaker vector for the $\\mathcal{W}$-algebra of type $A$.","Our result is a vast generalization of several previous results that treated the monotone case, and the cases of quadratic and cubic polynomial weights.","It also provides an interpretation of the associated Whittaker vector in terms of generalized branched coverings that might be of independent interest.","Our result is new even in the special case $b=0$ that corresponds to classical hypergeometric Hurwitz numbers, and implies that they are governed by the topological recursion of Eynard-Orantin.","This gives an independent proof of the recent result of Bychkov-Dunin-Barkowski-Kazarian-Shadrin."],"url":"http://arxiv.org/abs/2401.12814v1","category":"math.AG"}
{"created":"2024-01-23 14:50:44","title":"Bayesian parameter estimation of massive black hole binaries with TianQin-LISA","abstract":"This paper analyses the impact of various parameter changes on the estimation of parameters for massive black hole binary (MBHB) systems using a Bayesian inference technique. Several designed MBHB systems were chosen for comparison with a fiducial system to explore the influence of parameters such as sky location, inclination angle, anti-spin, large mass ratio and light mass. And the two reported MBHB candidates named OJ287 and Tick-Tock are also considered. The study found that the network of TianQin and LISA can break certain degeneracies among different parameters, improving the estimation of parameters, particularly for extrinsic parameters. Meanwhile, the degeneracies between different intrinsic parameters are highly sensitive to the value of the parameters. Additionally, the small inclination angles and limited detection of the inspiral phase can introduce significant bias in the estimation of parameters. The presence of instrument noise will also introduce bias and worsen the precision. The paper concludes that the network of TianQin and LISA can significantly improve the estimation of extrinsic parameters by about one order of magnitude while yielding slight improvements in the intrinsic parameters. Moreover, parameter estimation can still be subject to biases even with a sufficiently high signal-to-noise ratio if the detected signal does not encompass all stages of the inspiral, merger, and ringdown.","sentences":["This paper analyses the impact of various parameter changes on the estimation of parameters for massive black hole binary (MBHB) systems using a Bayesian inference technique.","Several designed MBHB systems were chosen for comparison with a fiducial system to explore the influence of parameters such as sky location, inclination angle, anti-spin, large mass ratio and light mass.","And the two reported MBHB candidates named OJ287 and Tick-Tock are also considered.","The study found that the network of TianQin and LISA can break certain degeneracies among different parameters, improving the estimation of parameters, particularly for extrinsic parameters.","Meanwhile, the degeneracies between different intrinsic parameters are highly sensitive to the value of the parameters.","Additionally, the small inclination angles and limited detection of the inspiral phase can introduce significant bias in the estimation of parameters.","The presence of instrument noise will also introduce bias and worsen the precision.","The paper concludes that the network of TianQin and LISA can significantly improve the estimation of extrinsic parameters by about one order of magnitude while yielding slight improvements in the intrinsic parameters.","Moreover, parameter estimation can still be subject to biases even with a sufficiently high signal-to-noise ratio if the detected signal does not encompass all stages of the inspiral, merger, and ringdown."],"url":"http://arxiv.org/abs/2401.12813v1","category":"astro-ph.GA"}
{"created":"2024-01-23 14:45:22","title":"A Robot Expressing Emotions Through Gestures: Everyone Outside of Italy Would Understand this?","abstract":"In the context of our research activities on affective computing and human-robot interaction we are working on both the recognition of human's emotions and the expression of emotions by robots. In our vision, robots will be increasingly present in schools, factories, and homes, and their empathetic behavior may foster their acceptance. In particular, in one of our research, we sought to replicate gestures associated with specific emotions on a social robot, NAO. Our focus was on Ekman's six primary emotions, along with five emotions selected from Plutchik's wheel of emotions. In our opinion the cultural component linked to the expression of emotions through gestures certainly influenced both us and the participants. Thus, we would like to investigate the influence of our culture in the gestural expression of emotion.","sentences":["In the context of our research activities on affective computing and human-robot interaction we are working on both the recognition of human's emotions and the expression of emotions by robots.","In our vision, robots will be increasingly present in schools, factories, and homes, and their empathetic behavior may foster their acceptance.","In particular, in one of our research, we sought to replicate gestures associated with specific emotions on a social robot, NAO.","Our focus was on Ekman's six primary emotions, along with five emotions selected from Plutchik's wheel of emotions.","In our opinion the cultural component linked to the expression of emotions through gestures certainly influenced both us and the participants.","Thus, we would like to investigate the influence of our culture in the gestural expression of emotion."],"url":"http://arxiv.org/abs/2401.12808v1","category":"cs.HC"}
{"created":"2024-01-23 14:37:51","title":"Binary structured physics-informed neural networks for solving equations with rapidly changing solutions","abstract":"Physics-informed neural networks (PINNs), rooted in deep learning, have emerged as a promising approach for solving partial differential equations (PDEs). By embedding the physical information described by PDEs into feedforward neural networks, PINNs are trained as surrogate models to approximate solutions without the need for label data. Nevertheless, even though PINNs have shown remarkable performance, they can face difficulties, especially when dealing with equations featuring rapidly changing solutions. These difficulties encompass slow convergence, susceptibility to becoming trapped in local minima, and reduced solution accuracy. To address these issues, we propose a binary structured physics-informed neural network (BsPINN) framework, which employs binary structured neural network (BsNN) as the neural network component. By leveraging a binary structure that reduces inter-neuron connections compared to fully connected neural networks, BsPINNs excel in capturing the local features of solutions more effectively and efficiently. These features are particularly crucial for learning the rapidly changing in the nature of solutions. In a series of numerical experiments solving Burgers equation, Euler equation, Helmholtz equation, and high-dimension Poisson equation, BsPINNs exhibit superior convergence speed and heightened accuracy compared to PINNs. From these experiments, we discover that BsPINNs resolve the issues caused by increased hidden layers in PINNs resulting in over-smoothing, and prevent the decline in accuracy due to non-smoothness of PDEs solutions.","sentences":["Physics-informed neural networks (PINNs), rooted in deep learning, have emerged as a promising approach for solving partial differential equations (PDEs).","By embedding the physical information described by PDEs into feedforward neural networks, PINNs are trained as surrogate models to approximate solutions without the need for label data.","Nevertheless, even though PINNs have shown remarkable performance, they can face difficulties, especially when dealing with equations featuring rapidly changing solutions.","These difficulties encompass slow convergence, susceptibility to becoming trapped in local minima, and reduced solution accuracy.","To address these issues, we propose a binary structured physics-informed neural network (BsPINN) framework, which employs binary structured neural network (BsNN) as the neural network component.","By leveraging a binary structure that reduces inter-neuron connections compared to fully connected neural networks, BsPINNs excel in capturing the local features of solutions more effectively and efficiently.","These features are particularly crucial for learning the rapidly changing in the nature of solutions.","In a series of numerical experiments solving Burgers equation, Euler equation, Helmholtz equation, and high-dimension Poisson equation, BsPINNs exhibit superior convergence speed and heightened accuracy compared to PINNs.","From these experiments, we discover that BsPINNs resolve the issues caused by increased hidden layers in PINNs resulting in over-smoothing, and prevent the decline in accuracy due to non-smoothness of PDEs solutions."],"url":"http://arxiv.org/abs/2401.12806v1","category":"cs.LG"}
{"created":"2024-01-23 14:31:12","title":"Gradient Flow of Energy: A General and Efficient Approach for Entity Alignment Decoding","abstract":"Entity alignment (EA), a pivotal process in integrating multi-source Knowledge Graphs (KGs), seeks to identify equivalent entity pairs across these graphs. Most existing approaches regard EA as a graph representation learning task, concentrating on enhancing graph encoders. However, the decoding process in EA - essential for effective operation and alignment accuracy - has received limited attention and remains tailored to specific datasets and model architectures, necessitating both entity and additional explicit relation embeddings. This specificity limits its applicability, particularly in GNN-based models. To address this gap, we introduce a novel, generalized, and efficient decoding approach for EA, relying solely on entity embeddings. Our method optimizes the decoding process by minimizing Dirichlet energy, leading to the gradient flow within the graph, to promote graph homophily. The discretization of the gradient flow produces a fast and scalable approach, termed Triple Feature Propagation (TFP). TFP innovatively channels gradient flow through three views: entity-to-entity, entity-to-relation, and relation-to-entity. This generalized gradient flow enables TFP to harness the multi-view structural information of KGs. Rigorous experimentation on diverse real-world datasets demonstrates that our approach significantly enhances various EA methods. Notably, the approach achieves these advancements with less than 6 seconds of additional computational time, establishing a new benchmark in efficiency and adaptability for future EA methods.","sentences":["Entity alignment (EA), a pivotal process in integrating multi-source Knowledge Graphs (KGs), seeks to identify equivalent entity pairs across these graphs.","Most existing approaches regard EA as a graph representation learning task, concentrating on enhancing graph encoders.","However, the decoding process in EA - essential for effective operation and alignment accuracy - has received limited attention and remains tailored to specific datasets and model architectures, necessitating both entity and additional explicit relation embeddings.","This specificity limits its applicability, particularly in GNN-based models.","To address this gap, we introduce a novel, generalized, and efficient decoding approach for EA, relying solely on entity embeddings.","Our method optimizes the decoding process by minimizing Dirichlet energy, leading to the gradient flow within the graph, to promote graph homophily.","The discretization of the gradient flow produces a fast and scalable approach, termed Triple Feature Propagation (TFP).","TFP innovatively channels gradient flow through three views: entity-to-entity, entity-to-relation, and relation-to-entity.","This generalized gradient flow enables TFP to harness the multi-view structural information of KGs.","Rigorous experimentation on diverse real-world datasets demonstrates that our approach significantly enhances various EA methods.","Notably, the approach achieves these advancements with less than 6 seconds of additional computational time, establishing a new benchmark in efficiency and adaptability for future EA methods."],"url":"http://arxiv.org/abs/2401.12798v1","category":"cs.IR"}
{"created":"2024-01-23 14:29:17","title":"Benchmarking LLMs via Uncertainty Quantification","abstract":"The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves eight LLMs (LLM series) spanning five representative natural language processing tasks. Additionally, we introduce an uncertainty-aware evaluation metric, UAcc, which takes into account both prediction accuracy and prediction uncertainty. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. By taking uncertainty into account, our new UAcc metric can either amplify or diminish the relative improvement of one LLM over another and may even change the relative ranking of two LLMs. These results underscore the significance of incorporating uncertainty in the evaluation of LLMs.","sentences":["The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods.","However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs.","To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification.","Our examination involves eight LLMs (LLM series) spanning five representative natural language processing tasks.","Additionally, we introduce an uncertainty-aware evaluation metric, UAcc, which takes into account both prediction accuracy and prediction uncertainty.","Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs.","By taking uncertainty into account, our new UAcc metric can either amplify or diminish the relative improvement of one LLM over another and may even change the relative ranking of two LLMs.","These results underscore the significance of incorporating uncertainty in the evaluation of LLMs."],"url":"http://arxiv.org/abs/2401.12794v1","category":"cs.CL"}
{"created":"2024-01-23 14:25:51","title":"Extremal Tsirelson inequalities","abstract":"It is well-known that the set of statistics that can be observed in a Bell-type experiment is limited by quantum theory. Unfortunately, tools are missing to identify the precise boundary of this set. Here, we propose to study the set of quantum statistics from a dual perspective. By considering all Bell expressions saturated by a given realization, we show that the CHSH expression can be decomposed in terms of extremal Tsirelson inequalities that we identify. This brings novel insight into the geometry of the quantum set in the (2,2,2) scenario. Furthermore, this allows us to identify all the Bell expressions that are able to self-test the Tsirelson realization.","sentences":["It is well-known that the set of statistics that can be observed in a Bell-type experiment is limited by quantum theory.","Unfortunately, tools are missing to identify the precise boundary of this set.","Here, we propose to study the set of quantum statistics from a dual perspective.","By considering all Bell expressions saturated by a given realization, we show that the CHSH expression can be decomposed in terms of extremal Tsirelson inequalities that we identify.","This brings novel insight into the geometry of the quantum set in the (2,2,2) scenario.","Furthermore, this allows us to identify all the Bell expressions that are able to self-test the Tsirelson realization."],"url":"http://arxiv.org/abs/2401.12791v1","category":"quant-ph"}
{"created":"2024-01-23 14:25:43","title":"MORPH: Towards Automated Concept Drift Adaptation for Malware Detection","abstract":"Concept drift is a significant challenge for malware detection, as the performance of trained machine learning models degrades over time, rendering them impractical. While prior research in malware concept drift adaptation has primarily focused on active learning, which involves selecting representative samples to update the model, self-training has emerged as a promising approach to mitigate concept drift. Self-training involves retraining the model using pseudo labels to adapt to shifting data distributions. In this research, we propose MORPH -- an effective pseudo-label-based concept drift adaptation method specifically designed for neural networks. Through extensive experimental analysis of Android and Windows malware datasets, we demonstrate the efficacy of our approach in mitigating the impact of concept drift. Our method offers the advantage of reducing annotation efforts when combined with active learning. Furthermore, our method significantly improves over existing works in automated concept drift adaptation for malware detection.","sentences":["Concept drift is a significant challenge for malware detection, as the performance of trained machine learning models degrades over time, rendering them impractical.","While prior research in malware concept drift adaptation has primarily focused on active learning, which involves selecting representative samples to update the model, self-training has emerged as a promising approach to mitigate concept drift.","Self-training involves retraining the model using pseudo labels to adapt to shifting data distributions.","In this research, we propose MORPH -- an effective pseudo-label-based concept drift adaptation method specifically designed for neural networks.","Through extensive experimental analysis of Android and Windows malware datasets, we demonstrate the efficacy of our approach in mitigating the impact of concept drift.","Our method offers the advantage of reducing annotation efforts when combined with active learning.","Furthermore, our method significantly improves over existing works in automated concept drift adaptation for malware detection."],"url":"http://arxiv.org/abs/2401.12790v1","category":"cs.LG"}
{"created":"2024-01-23 14:19:01","title":"Multilingual and Fully Non-Autoregressive ASR with Large Language Model Fusion: A Comprehensive Study","abstract":"In the era of large models, the autoregressive nature of decoding often results in latency serving as a significant bottleneck. We propose a non-autoregressive LM-fused ASR system that effectively leverages the parallelization capabilities of accelerator hardware. Our approach combines the Universal Speech Model (USM) and the PaLM 2 language model in per-segment scoring mode, achieving an average relative WER improvement across all languages of 10.8% on FLEURS and 3.6% on YouTube captioning. Furthermore, our comprehensive ablation study analyzes key parameters such as LLM size, context length, vocabulary size, fusion methodology. For instance, we explore the impact of LLM size ranging from 128M to 340B parameters on ASR performance. This study provides valuable insights into the factors influencing the effectiveness of practical large-scale LM-fused speech recognition systems.","sentences":["In the era of large models, the autoregressive nature of decoding often results in latency serving as a significant bottleneck.","We propose a non-autoregressive LM-fused ASR system that effectively leverages the parallelization capabilities of accelerator hardware.","Our approach combines the Universal Speech Model (USM) and the PaLM 2 language model in per-segment scoring mode, achieving an average relative WER improvement across all languages of 10.8% on FLEURS and 3.6% on YouTube captioning.","Furthermore, our comprehensive ablation study analyzes key parameters such as LLM size, context length, vocabulary size, fusion methodology.","For instance, we explore the impact of LLM size ranging from 128M to 340B parameters on ASR performance.","This study provides valuable insights into the factors influencing the effectiveness of practical large-scale LM-fused speech recognition systems."],"url":"http://arxiv.org/abs/2401.12789v1","category":"cs.CL"}
{"created":"2024-01-23 14:14:04","title":"Extended imaginary gauge transformation in a general nonreciprocal lattice","abstract":"Imaginary gauge transformation (IGT) provides a clear understanding of the non-Hermitian skin effect by transforming the non-Hermitian Hamiltonians with real spectra into Hermitian ones. In this work, we extend this approach to the complex spectrum regime in a general nonreciprocal lattice model. We unveil the validity of IGT hinges on a class of pseudo-Hermitian symmetry. The generalized Brillouin zone of Hamiltonian respect such pseudo-Hermiticity is demonstrated to be a circle, which enables easy access to the continuum bands, localization length of skin modes, and relevant topological numbers. Furthermore, we investigate the applicability of IGT and the underlying pseudo-Hermiticity beyond nearest-neighbour hopping, offering a graphical interpretation. Our theoretical framework is applied to establish bulk-boundary correspondence in the nonreciprocal trimer Su-Schrieffer-Heeger model and analyze the localization behaviors of skin modes in the two-dimensional Hatano-Nelson model.","sentences":["Imaginary gauge transformation (IGT) provides a clear understanding of the non-Hermitian skin effect by transforming the non-Hermitian Hamiltonians with real spectra into Hermitian ones.","In this work, we extend this approach to the complex spectrum regime in a general nonreciprocal lattice model.","We unveil the validity of IGT hinges on a class of pseudo-Hermitian symmetry.","The generalized Brillouin zone of Hamiltonian respect such pseudo-Hermiticity is demonstrated to be a circle, which enables easy access to the continuum bands, localization length of skin modes, and relevant topological numbers.","Furthermore, we investigate the applicability of IGT and the underlying pseudo-Hermiticity beyond nearest-neighbour hopping, offering a graphical interpretation.","Our theoretical framework is applied to establish bulk-boundary correspondence in the nonreciprocal trimer Su-Schrieffer-Heeger model and analyze the localization behaviors of skin modes in the two-dimensional Hatano-Nelson model."],"url":"http://arxiv.org/abs/2401.12785v1","category":"quant-ph"}
{"created":"2024-01-23 14:11:29","title":"A Review of Deep Learning Methods for Photoplethysmography Data","abstract":"Photoplethysmography (PPG) is a highly promising device due to its advantages in portability, user-friendly operation, and non-invasive capabilities to measure a wide range of physiological information. Recent advancements in deep learning have demonstrated remarkable outcomes by leveraging PPG signals for tasks related to personal health management and other multifaceted applications. In this review, we systematically reviewed papers that applied deep learning models to process PPG data between January 1st of 2017 and July 31st of 2023 from Google Scholar, PubMed and Dimensions. Each paper is analyzed from three key perspectives: tasks, models, and data. We finally extracted 193 papers where different deep learning frameworks were used to process PPG signals. Based on the tasks addressed in these papers, we categorized them into two major groups: medical-related, and non-medical-related. The medical-related tasks were further divided into seven subgroups, including blood pressure analysis, cardiovascular monitoring and diagnosis, sleep health, mental health, respiratory monitoring and analysis, blood glucose analysis, as well as others. The non-medical-related tasks were divided into four subgroups, which encompass signal processing, biometric identification, electrocardiogram reconstruction, and human activity recognition. In conclusion, significant progress has been made in the field of using deep learning methods to process PPG data recently. This allows for a more thorough exploration and utilization of the information contained in PPG signals. However, challenges remain, such as limited quantity and quality of publicly available databases, a lack of effective validation in real-world scenarios, and concerns about the interpretability, scalability, and complexity of deep learning models. Moreover, there are still emerging research areas that require further investigation.","sentences":["Photoplethysmography (PPG) is a highly promising device due to its advantages in portability, user-friendly operation, and non-invasive capabilities to measure a wide range of physiological information.","Recent advancements in deep learning have demonstrated remarkable outcomes by leveraging PPG signals for tasks related to personal health management and other multifaceted applications.","In this review, we systematically reviewed papers that applied deep learning models to process PPG data between January 1st of 2017 and July 31st of 2023 from Google Scholar, PubMed and Dimensions.","Each paper is analyzed from three key perspectives: tasks, models, and data.","We finally extracted 193 papers where different deep learning frameworks were used to process PPG signals.","Based on the tasks addressed in these papers, we categorized them into two major groups: medical-related, and non-medical-related.","The medical-related tasks were further divided into seven subgroups, including blood pressure analysis, cardiovascular monitoring and diagnosis, sleep health, mental health, respiratory monitoring and analysis, blood glucose analysis, as well as others.","The non-medical-related tasks were divided into four subgroups, which encompass signal processing, biometric identification, electrocardiogram reconstruction, and human activity recognition.","In conclusion, significant progress has been made in the field of using deep learning methods to process PPG data recently.","This allows for a more thorough exploration and utilization of the information contained in PPG signals.","However, challenges remain, such as limited quantity and quality of publicly available databases, a lack of effective validation in real-world scenarios, and concerns about the interpretability, scalability, and complexity of deep learning models.","Moreover, there are still emerging research areas that require further investigation."],"url":"http://arxiv.org/abs/2401.12783v1","category":"cs.AI"}
{"created":"2024-01-23 14:06:08","title":"DeepRicci: Self-supervised Graph Structure-Feature Co-Refinement for Alleviating Over-squashing","abstract":"Graph Neural Networks (GNNs) have shown great power for learning and mining on graphs, and Graph Structure Learning (GSL) plays an important role in boosting GNNs with a refined graph. In the literature, most GSL solutions either primarily focus on structure refinement with task-specific supervision (i.e., node classification), or overlook the inherent weakness of GNNs themselves (e.g., over-squashing), resulting in suboptimal performance despite sophisticated designs. In light of these limitations, we propose to study self-supervised graph structure-feature co-refinement for effectively alleviating the issue of over-squashing in typical GNNs. In this paper, we take a fundamentally different perspective of the Ricci curvature in Riemannian geometry, in which we encounter the challenges of modeling, utilizing and computing Ricci curvature. To tackle these challenges, we present a self-supervised Riemannian model, DeepRicci. Specifically, we introduce a latent Riemannian space of heterogeneous curvatures to model various Ricci curvatures, and propose a gyrovector feature mapping to utilize Ricci curvature for typical GNNs. Thereafter, we refine node features by geometric contrastive learning among different geometric views, and simultaneously refine graph structure by backward Ricci flow based on a novel formulation of differentiable Ricci curvature. Finally, extensive experiments on public datasets show the superiority of DeepRicci, and the connection between backward Ricci flow and over-squashing. Codes of our work are given in https://github.com/RiemanGraph/.","sentences":["Graph Neural Networks (GNNs) have shown great power for learning and mining on graphs, and Graph Structure Learning (GSL) plays an important role in boosting GNNs with a refined graph.","In the literature, most GSL solutions either primarily focus on structure refinement with task-specific supervision (i.e., node classification), or overlook the inherent weakness of GNNs themselves (e.g., over-squashing), resulting in suboptimal performance despite sophisticated designs.","In light of these limitations, we propose to study self-supervised graph structure-feature co-refinement for effectively alleviating the issue of over-squashing in typical GNNs.","In this paper, we take a fundamentally different perspective of the Ricci curvature in Riemannian geometry, in which we encounter the challenges of modeling, utilizing and computing Ricci curvature.","To tackle these challenges, we present a self-supervised Riemannian model, DeepRicci.","Specifically, we introduce a latent Riemannian space of heterogeneous curvatures to model various Ricci curvatures, and propose a gyrovector feature mapping to utilize Ricci curvature for typical GNNs.","Thereafter, we refine node features by geometric contrastive learning among different geometric views, and simultaneously refine graph structure by backward Ricci flow based on a novel formulation of differentiable Ricci curvature.","Finally, extensive experiments on public datasets show the superiority of DeepRicci, and the connection between backward Ricci flow and over-squashing.","Codes of our work are given in https://github.com/RiemanGraph/."],"url":"http://arxiv.org/abs/2401.12780v1","category":"cs.LG"}
{"created":"2024-01-23 14:05:29","title":"Singular space-times with bounded algebraic curvature scalars","abstract":"We show that the absence of unbounded algebraic curvature invariants constructed from polynomials of the Riemann tensor cannot guarantee the absence of strong singularities. As a consequence, it is not sufficient to rely solely on the analysis of such scalars to assess the regularity of a given space-time. This conclusion follows from the analysis of incomplete geodesics within the internal region of asymmetric wormholes supported by scalar matter which arise in two distinct metric-affine gravity theories. These wormholes have bounded algebraic curvature scalars everywhere, which highlights that their finiteness does not prevent the emergence of pathologies (singularities) in the geodesic structure of space-time. By analyzing the tidal forces in the internal wormhole region, we find that the angular components are unbounded along incomplete radial time-like geodesics. The strength of the singularity is determined by the evolution of Jacobi fields along such geodesics, finding that it is of strong type, as volume elements are torn apart as the singularity is approached. Lastly, and for completeness, we consider the wormhole of the quadratic Palatini theory and present an analysis of the tidal forces in the entire space-time.","sentences":["We show that the absence of unbounded algebraic curvature invariants constructed from polynomials of the Riemann tensor cannot guarantee the absence of strong singularities.","As a consequence, it is not sufficient to rely solely on the analysis of such scalars to assess the regularity of a given space-time.","This conclusion follows from the analysis of incomplete geodesics within the internal region of asymmetric wormholes supported by scalar matter which arise in two distinct metric-affine gravity theories.","These wormholes have bounded algebraic curvature scalars everywhere, which highlights that their finiteness does not prevent the emergence of pathologies (singularities) in the geodesic structure of space-time.","By analyzing the tidal forces in the internal wormhole region, we find that the angular components are unbounded along incomplete radial time-like geodesics.","The strength of the singularity is determined by the evolution of Jacobi fields along such geodesics, finding that it is of strong type, as volume elements are torn apart as the singularity is approached.","Lastly, and for completeness, we consider the wormhole of the quadratic Palatini theory and present an analysis of the tidal forces in the entire space-time."],"url":"http://arxiv.org/abs/2401.12779v1","category":"gr-qc"}
{"created":"2024-01-23 14:01:27","title":"On $p$-adic Hurwitz-type spectral zeta functions","abstract":"Let $\\left\\{E_n\\right\\}_{n=1}^{\\infty}$ be the set of energy levels corresponding to a Hamiltonian $H$.   Denote by $$\\lambda_{0}=0~~\\textrm{and}~~\\lambda_{n}=E_{n}$$   for $n\\in\\mathbb N.$ In this paper, we shall construct and investigate the $p$-adic counterparts of the Hurwitz-type spectral zeta function \\begin{equation} \\zeta^{H}(s,\\lambda)=\\sum_{n=0}^{\\infty}\\frac{1}{(\\lambda_{n}+\\lambda)^{s}} \\end{equation} and its alternating form \\begin{equation} \\zeta_{E}^{H}(s,\\lambda)=2\\sum_{n=0}^{\\infty}\\frac{(-1)^{n}}{(\\lambda_{n}+\\lambda)^{s}} \\end{equation} in a parallel way.","sentences":["Let $\\left\\{E_n\\right\\}_{n=1}^{\\infty}$ be the set of energy levels corresponding to a Hamiltonian $H$.   Denote by $$\\lambda_{0}=0~~\\textrm{and}~~\\lambda_{n}=E_{n}$$   for $n\\in\\mathbb N.$","In this paper, we shall construct and investigate the $p$-adic counterparts of the Hurwitz-type spectral zeta function \\begin{equation} \\zeta^{H}(s,\\lambda)=\\sum_{n=0}^{\\infty}\\frac{1}{(\\lambda_{n}+\\lambda)^{s}} \\end{equation} and its alternating form \\begin{equation} \\zeta_{E}^{H}(s,\\lambda)=2\\sum_{n=0}^{\\infty}\\frac{(-1)^{n}}{(\\lambda_{n}+\\lambda)^{s}} \\end{equation} in a parallel way."],"url":"http://arxiv.org/abs/2401.12775v1","category":"math.NT"}
{"created":"2024-01-23 13:57:50","title":"Deep Learning-based Intraoperative MRI Reconstruction","abstract":"Purpose: To evaluate the quality of deep learning reconstruction for prospectively accelerated intraoperative magnetic resonance imaging (iMRI) during resective brain tumor surgery.   Materials and Methods: Accelerated iMRI was performed during brain surgery using dual surface coils positioned around the area of resection. A deep learning (DL) model was trained on the fastMRI neuro dataset to mimic the data from the iMRI protocol. Evaluation was performed on imaging material from 40 patients imaged between 01.11.2021 - 01.06.2023 that underwent iMRI during tumor resection surgery. A comparative analysis was conducted between the conventional compressed sense (CS) method and the trained DL reconstruction method. Blinded evaluation of multiple image quality metrics was performed by two working neuro-radiologists and a working neurosurgeon on a 1 to 5 Likert scale (1=non diagnostic, 2=poor, 3=acceptable, 4=good, 5=excellent), and the favored reconstruction variant.   Results: The DL reconstruction was strongly favored or favored over the CS reconstruction for 33/40, 39/40, and 8/40 of cases for reader 1, 2, and 3, respectively. Two of three readers consistently assigned higher ratings for the DL reconstructions, and the DL reconstructions had a higher score than their respective CS counterparts for 72%, 72%, and 14% of the cases for reader 1, 2, and 3, respectively. Still, the DL reconstructions exhibited shortcomings such as a striping artifact and reduced signal.   Conclusion: DL shows promise to allow for high-quality reconstructions of intraoperative MRI with equal to or improved perceived spatial resolution, signal-to-noise ratio, diagnostic confidence, diagnostic conspicuity, and spatial resolution compared to compressed sense.","sentences":["Purpose: To evaluate the quality of deep learning reconstruction for prospectively accelerated intraoperative magnetic resonance imaging (iMRI) during resective brain tumor surgery.   ","Materials and Methods: Accelerated iMRI was performed during brain surgery using dual surface coils positioned around the area of resection.","A deep learning (DL) model was trained on the fastMRI neuro dataset to mimic the data from the iMRI protocol.","Evaluation was performed on imaging material from 40 patients imaged between 01.11.2021 - 01.06.2023 that underwent iMRI during tumor resection surgery.","A comparative analysis was conducted between the conventional compressed sense (CS) method and the trained DL reconstruction method.","Blinded evaluation of multiple image quality metrics was performed by two working neuro-radiologists and a working neurosurgeon on a 1 to 5 Likert scale (1=non diagnostic, 2=poor,","3=acceptable, 4=good, 5=excellent), and the favored reconstruction variant.   ","Results:","The DL reconstruction was strongly favored or favored over the CS reconstruction for 33/40, 39/40, and 8/40 of cases for reader 1, 2, and 3, respectively.","Two of three readers consistently assigned higher ratings for the DL reconstructions, and the DL reconstructions had a higher score than their respective CS counterparts for 72%, 72%, and 14% of the cases for reader 1, 2, and 3, respectively.","Still, the DL reconstructions exhibited shortcomings such as a striping artifact and reduced signal.   ","Conclusion: DL shows promise to allow for high-quality reconstructions of intraoperative MRI with equal to or improved perceived spatial resolution, signal-to-noise ratio, diagnostic confidence, diagnostic conspicuity, and spatial resolution compared to compressed sense."],"url":"http://arxiv.org/abs/2401.12771v1","category":"eess.IV"}
{"created":"2024-01-23 13:48:49","title":"What Can Self-Admitted Technical Debt Tell Us About Security? A Mixed-Methods Study","abstract":"Self-Admitted Technical Debt (SATD) encompasses a wide array of sub-optimal design and implementation choices reported in software artefacts (e.g., code comments and commit messages) by developers themselves. Such reports have been central to the study of software maintenance and evolution over the last decades. However, they can also be deemed as dreadful sources of information on potentially exploitable vulnerabilities and security flaws. This work investigates the security implications of SATD from a technical and developer-centred perspective. On the one hand, it analyses whether security pointers disclosed inside SATD sources can be used to characterise vulnerabilities in Open-Source Software (OSS) projects and repositories. On the other hand, it delves into developers' perspectives regarding the motivations behind this practice, its prevalence, and its potential negative consequences. We followed a mixed-methods approach consisting of (i) the analysis of a preexisting dataset containing 94,455 SATD instances and (ii) an online survey with 222 OSS practitioners. We gathered 201 SATD instances through the dataset analysis and mapped them to different Common Weakness Enumeration (CWE) identifiers. Overall, 25 different types of CWEs were spotted across commit messages, pull requests, code comments, and issue sections, from which 8 appear among MITRE's Top-25 most dangerous ones. The survey shows that software practitioners often place security pointers across SATD artefacts to promote a security culture among their peers and help them spot flaky code sections, among other motives. However, they also consider such a practice risky as it may facilitate vulnerability exploits. Our findings suggest that preserving the contextual integrity of security pointers disseminated across SATD artefacts is critical to safeguard both commercial and OSS solutions against zero-day attacks.","sentences":["Self-Admitted Technical Debt (SATD) encompasses a wide array of sub-optimal design and implementation choices reported in software artefacts (e.g., code comments and commit messages) by developers themselves.","Such reports have been central to the study of software maintenance and evolution over the last decades.","However, they can also be deemed as dreadful sources of information on potentially exploitable vulnerabilities and security flaws.","This work investigates the security implications of SATD from a technical and developer-centred perspective.","On the one hand, it analyses whether security pointers disclosed inside SATD sources can be used to characterise vulnerabilities in Open-Source Software (OSS) projects and repositories.","On the other hand, it delves into developers' perspectives regarding the motivations behind this practice, its prevalence, and its potential negative consequences.","We followed a mixed-methods approach consisting of (i) the analysis of a preexisting dataset containing 94,455 SATD instances and (ii) an online survey with 222 OSS practitioners.","We gathered 201 SATD instances through the dataset analysis and mapped them to different Common Weakness Enumeration (CWE) identifiers.","Overall, 25 different types of CWEs were spotted across commit messages, pull requests, code comments, and issue sections, from which 8 appear among MITRE's Top-25 most dangerous ones.","The survey shows that software practitioners often place security pointers across SATD artefacts to promote a security culture among their peers and help them spot flaky code sections, among other motives.","However, they also consider such a practice risky as it may facilitate vulnerability exploits.","Our findings suggest that preserving the contextual integrity of security pointers disseminated across SATD artefacts is critical to safeguard both commercial and OSS solutions against zero-day attacks."],"url":"http://arxiv.org/abs/2401.12768v1","category":"cs.SE"}
{"created":"2024-01-23 13:44:15","title":"Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving $\\mathcal{O}(1/k)$ Finite-Sample Complexity","abstract":"This paper proposes to develop a new variant of the two-time-scale stochastic approximation to find the roots of two coupled nonlinear operators, assuming only noisy samples of these operators can be observed. Our key idea is to leverage the classic Ruppert-Polyak averaging technique to dynamically estimate the operators through their samples. The estimated values of these averaging steps will then be used in the two-time-scale stochastic approximation updates to find the desired solution. Our main theoretical result is to show that under the strongly monotone condition of the underlying nonlinear operators the mean-squared errors of the iterates generated by the proposed method converge to zero at an optimal rate $\\mathcal{O}(1/k)$, where $k$ is the number of iterations. Our result significantly improves the existing result of two-time-scale stochastic approximation, where the best known finite-time convergence rate is $\\mathcal{O}(1/k^{2/3})$.","sentences":["This paper proposes to develop a new variant of the two-time-scale stochastic approximation to find the roots of two coupled nonlinear operators, assuming only noisy samples of these operators can be observed.","Our key idea is to leverage the classic Ruppert-Polyak averaging technique to dynamically estimate the operators through their samples.","The estimated values of these averaging steps will then be used in the two-time-scale stochastic approximation updates to find the desired solution.","Our main theoretical result is to show that under the strongly monotone condition of the underlying nonlinear operators the mean-squared errors of the iterates generated by the proposed method converge to zero at an optimal rate $\\mathcal{O}(1/k)$, where $k$ is the number of iterations.","Our result significantly improves the existing result of two-time-scale stochastic approximation, where the best known finite-time convergence rate is $\\mathcal{O}(1/k^{2/3})$."],"url":"http://arxiv.org/abs/2401.12764v1","category":"math.OC"}
{"created":"2024-01-23 13:44:05","title":"The State-Dependent Channel with a Rate-Limited Cribbing Helper","abstract":"The capacity of a memoryless state-dependent channel is derived for a setting in which the encoder is provided with rate-limited assistance from a cribbing helper that observes the state sequence causally and the past channel inputs strictly-causally. Said cribbing may increase capacity but not to the level achievable by a message-cognizant helper.","sentences":["The capacity of a memoryless state-dependent channel is derived for a setting in which the encoder is provided with rate-limited assistance from a cribbing helper that observes the state sequence causally and the past channel inputs strictly-causally.","Said cribbing may increase capacity but not to the level achievable by a message-cognizant helper."],"url":"http://arxiv.org/abs/2401.12763v1","category":"cs.IT"}
{"created":"2024-01-23 13:43:17","title":"MUSES: The Multi-Sensor Semantic Perception Dataset for Driving under Uncertainty","abstract":"Achieving level-5 driving automation in autonomous vehicles necessitates a robust semantic visual perception system capable of parsing data from different sensors across diverse conditions. However, existing semantic perception datasets often lack important non-camera modalities typically used in autonomous vehicles, or they do not exploit such modalities to aid and improve semantic annotations in challenging conditions. To address this, we introduce MUSES, the MUlti-SEnsor Semantic perception dataset for driving in adverse conditions under increased uncertainty. MUSES includes synchronized multimodal recordings with 2D panoptic annotations for 2500 images captured under diverse weather and illumination. The dataset integrates a frame camera, a lidar, a radar, an event camera, and an IMU/GNSS sensor. Our new two-stage panoptic annotation protocol captures both class-level and instance-level uncertainty in the ground truth and enables the novel task of uncertainty-aware panoptic segmentation we introduce, along with standard semantic and panoptic segmentation. MUSES proves both effective for training and challenging for evaluating models under diverse visual conditions, and it opens new avenues for research in multimodal and uncertainty-aware dense semantic perception. Our dataset and benchmark will be made publicly available.","sentences":["Achieving level-5 driving automation in autonomous vehicles necessitates a robust semantic visual perception system capable of parsing data from different sensors across diverse conditions.","However, existing semantic perception datasets often lack important non-camera modalities typically used in autonomous vehicles, or they do not exploit such modalities to aid and improve semantic annotations in challenging conditions.","To address this, we introduce MUSES, the MUlti-SEnsor Semantic perception dataset for driving in adverse conditions under increased uncertainty.","MUSES includes synchronized multimodal recordings with 2D panoptic annotations for 2500 images captured under diverse weather and illumination.","The dataset integrates a frame camera, a lidar, a radar, an event camera, and an IMU/GNSS sensor.","Our new two-stage panoptic annotation protocol captures both class-level and instance-level uncertainty in the ground truth and enables the novel task of uncertainty-aware panoptic segmentation we introduce, along with standard semantic and panoptic segmentation.","MUSES proves both effective for training and challenging for evaluating models under diverse visual conditions, and it opens new avenues for research in multimodal and uncertainty-aware dense semantic perception.","Our dataset and benchmark will be made publicly available."],"url":"http://arxiv.org/abs/2401.12761v1","category":"cs.CV"}
{"created":"2024-01-23 13:35:47","title":"What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition","abstract":"The knowledge encapsulated in a model is the core factor determining its final performance on downstream tasks. Much research in NLP has focused on efficient methods for storing and adapting different types of knowledge, e.g., in dedicated modularized structures, and on how to effectively combine these, e.g., by learning additional parameters. However, given the many possible options, a thorough understanding of the mechanisms involved in these compositions is missing, and hence it remains unclear which strategies to utilize. To address this research gap, we propose a novel framework for zero-shot module composition, which encompasses existing and some novel variations for selecting, weighting, and combining parameter modules under a single unified notion. Focusing on the scenario of domain knowledge and adapter layers, our framework provides a systematic unification of concepts, allowing us to conduct the first comprehensive benchmarking study of various zero-shot knowledge composition strategies. In particular, we test two module combination methods and five selection and weighting strategies for their effectiveness and efficiency in an extensive experimental setup. Our results highlight the efficacy of ensembling but also hint at the power of simple though often-ignored weighting methods. Further in-depth analyses allow us to understand the role of weighting vs. top-k selection, and show that, to a certain extent, the performance of adapter composition can even be predicted.","sentences":["The knowledge encapsulated in a model is the core factor determining its final performance on downstream tasks.","Much research in NLP has focused on efficient methods for storing and adapting different types of knowledge, e.g., in dedicated modularized structures, and on how to effectively combine these, e.g., by learning additional parameters.","However, given the many possible options, a thorough understanding of the mechanisms involved in these compositions is missing, and hence it remains unclear which strategies to utilize.","To address this research gap, we propose a novel framework for zero-shot module composition, which encompasses existing and some novel variations for selecting, weighting, and combining parameter modules under a single unified notion.","Focusing on the scenario of domain knowledge and adapter layers, our framework provides a systematic unification of concepts, allowing us to conduct the first comprehensive benchmarking study of various zero-shot knowledge composition strategies.","In particular, we test two module combination methods and five selection and weighting strategies for their effectiveness and efficiency in an extensive experimental setup.","Our results highlight the efficacy of ensembling but also hint at the power of simple though often-ignored weighting methods.","Further in-depth analyses allow us to understand the role of weighting vs. top-k selection, and show that, to a certain extent, the performance of adapter composition can even be predicted."],"url":"http://arxiv.org/abs/2401.12756v1","category":"cs.CL"}
{"created":"2024-01-23 13:35:16","title":"Towards Risk Analysis of the Impact of AI on the Deliberate Biological Threat Landscape","abstract":"The perception that the convergence of biological engineering and artificial intelligence (AI) could enable increased biorisk has recently drawn attention to the governance of biotechnology and artificial intelligence. The 2023 Executive Order, Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence, requires an assessment of how artificial intelligence can increase biorisk. Within this perspective, we present a simplistic framework for evaluating biorisk and demonstrate how this framework falls short in achieving actionable outcomes for a biorisk manager. We then suggest a potential path forward that builds upon existing risk characterization work and justify why characterization efforts of AI-enabled tools for engineering biology is needed.","sentences":["The perception that the convergence of biological engineering and artificial intelligence (AI) could enable increased biorisk has recently drawn attention to the governance of biotechnology and artificial intelligence.","The 2023 Executive Order, Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence, requires an assessment of how artificial intelligence can increase biorisk.","Within this perspective, we present a simplistic framework for evaluating biorisk and demonstrate how this framework falls short in achieving actionable outcomes for a biorisk manager.","We then suggest a potential path forward that builds upon existing risk characterization work and justify why characterization efforts of AI-enabled tools for engineering biology is needed."],"url":"http://arxiv.org/abs/2401.12755v1","category":"cs.CY"}
{"created":"2024-01-23 13:30:43","title":"PSDF: Prior-Driven Neural Implicit Surface Learning for Multi-view Reconstruction","abstract":"Surface reconstruction has traditionally relied on the Multi-View Stereo (MVS)-based pipeline, which often suffers from noisy and incomplete geometry. This is due to that although MVS has been proven to be an effective way to recover the geometry of the scenes, especially for locally detailed areas with rich textures, it struggles to deal with areas with low texture and large variations of illumination where the photometric consistency is unreliable. Recently, Neural Implicit Surface Reconstruction (NISR) combines surface rendering and volume rendering techniques and bypasses the MVS as an intermediate step, which has emerged as a promising alternative to overcome the limitations of traditional pipelines. While NISR has shown impressive results on simple scenes, it remains challenging to recover delicate geometry from uncontrolled real-world scenes which is caused by its underconstrained optimization. To this end, the framework PSDF is proposed which resorts to external geometric priors from a pretrained MVS network and internal geometric priors inherent in the NISR model to facilitate high-quality neural implicit surface learning. Specifically, the visibility-aware feature consistency loss and depth prior-assisted sampling based on external geometric priors are introduced. These proposals provide powerfully geometric consistency constraints and aid in locating surface intersection points, thereby significantly improving the accuracy and delicate reconstruction of NISR. Meanwhile, the internal prior-guided importance rendering is presented to enhance the fidelity of the reconstructed surface mesh by mitigating the biased rendering issue in NISR. Extensive experiments on the Tanks and Temples dataset show that PSDF achieves state-of-the-art performance on complex uncontrolled scenes.","sentences":["Surface reconstruction has traditionally relied on the Multi-View Stereo (MVS)-based pipeline, which often suffers from noisy and incomplete geometry.","This is due to that although MVS has been proven to be an effective way to recover the geometry of the scenes, especially for locally detailed areas with rich textures, it struggles to deal with areas with low texture and large variations of illumination where the photometric consistency is unreliable.","Recently, Neural Implicit Surface Reconstruction (NISR) combines surface rendering and volume rendering techniques and bypasses the MVS as an intermediate step, which has emerged as a promising alternative to overcome the limitations of traditional pipelines.","While NISR has shown impressive results on simple scenes, it remains challenging to recover delicate geometry from uncontrolled real-world scenes which is caused by its underconstrained optimization.","To this end, the framework PSDF is proposed which resorts to external geometric priors from a pretrained MVS network and internal geometric priors inherent in the NISR model to facilitate high-quality neural implicit surface learning.","Specifically, the visibility-aware feature consistency loss and depth prior-assisted sampling based on external geometric priors are introduced.","These proposals provide powerfully geometric consistency constraints and aid in locating surface intersection points, thereby significantly improving the accuracy and delicate reconstruction of NISR.","Meanwhile, the internal prior-guided importance rendering is presented to enhance the fidelity of the reconstructed surface mesh by mitigating the biased rendering issue in NISR.","Extensive experiments on the Tanks and Temples dataset show that PSDF achieves state-of-the-art performance on complex uncontrolled scenes."],"url":"http://arxiv.org/abs/2401.12751v1","category":"cs.CV"}
{"created":"2024-01-23 13:23:59","title":"On the Utility of Probing Trajectories for Algorithm-Selection","abstract":"Machine-learning approaches to algorithm-selection typically take data describing an instance as input. Input data can take the form of features derived from the instance description or fitness landscape, or can be a direct representation of the instance itself, i.e. an image or textual description. Regardless of the choice of input, there is an implicit assumption that instances that are similar will elicit similar performance from algorithm, and that a model is capable of learning this relationship. We argue that viewing algorithm-selection purely from an instance perspective can be misleading as it fails to account for how an algorithm `views' similarity between instances. We propose a novel `algorithm-centric' method for describing instances that can be used to train models for algorithm-selection: specifically, we use short probing trajectories calculated by applying a solver to an instance for a very short period of time. The approach is demonstrated to be promising, providing comparable or better results to computationally expensive landscape-based feature-based approaches. Furthermore, projecting the trajectories into a 2-dimensional space illustrates that functions that are similar from an algorithm-perspective do not necessarily correspond to the accepted categorisation of these functions from a human perspective.","sentences":["Machine-learning approaches to algorithm-selection typically take data describing an instance as input.","Input data can take the form of features derived from the instance description or fitness landscape, or can be a direct representation of the instance itself, i.e. an image or textual description.","Regardless of the choice of input, there is an implicit assumption that instances that are similar will elicit similar performance from algorithm, and that a model is capable of learning this relationship.","We argue that viewing algorithm-selection purely from an instance perspective can be misleading as it fails to account for how an algorithm `views' similarity between instances.","We propose a novel `algorithm-centric' method for describing instances that can be used to train models for algorithm-selection: specifically, we use short probing trajectories calculated by applying a solver to an instance for a very short period of time.","The approach is demonstrated to be promising, providing comparable or better results to computationally expensive landscape-based feature-based approaches.","Furthermore, projecting the trajectories into a 2-dimensional space illustrates that functions that are similar from an algorithm-perspective do not necessarily correspond to the accepted categorisation of these functions from a human perspective."],"url":"http://arxiv.org/abs/2401.12745v1","category":"cs.LG"}
{"created":"2024-01-23 13:22:29","title":"Monadic Intersection Types, Relationally (Extended Version)","abstract":"We extend intersection types to a computational $\\lambda$-calculus with algebraic operations \\`a la Plotkin and Power. We achieve this by considering monadic intersections, whereby computational effects appear not only in the operational semantics, but also in the type system. Since in the effectful setting termination is not anymore the only property of interest, we want to analyze the interactive behavior of typed programs with the environment. Indeed, our type system is able to characterize the natural notion of observation, both in the finite and in the infinitary setting, and for a wide class of effects, such as output, cost, pure and probabilistic nondeterminism, and combinations thereof. The main technical tool is a novel combination of syntactic techniques with abstract relational reasoning, which allows us to lift all the required notions, e.g. of typability and logical relation, to the monadic setting.","sentences":["We extend intersection types to a computational $\\lambda$-calculus with algebraic operations \\`a la Plotkin and Power.","We achieve this by considering monadic intersections, whereby computational effects appear not only in the operational semantics, but also in the type system.","Since in the effectful setting termination is not anymore the only property of interest, we want to analyze the interactive behavior of typed programs with the environment.","Indeed, our type system is able to characterize the natural notion of observation, both in the finite and in the infinitary setting, and for a wide class of effects, such as output, cost, pure and probabilistic nondeterminism, and combinations thereof.","The main technical tool is a novel combination of syntactic techniques with abstract relational reasoning, which allows us to lift all the required notions, e.g. of typability and logical relation, to the monadic setting."],"url":"http://arxiv.org/abs/2401.12744v1","category":"cs.PL"}
{"created":"2024-01-23 13:20:57","title":"Correlation-Embedded Transformer Tracking: A Single-Branch Framework","abstract":"Developing robust and discriminative appearance models has been a long-standing research challenge in visual object tracking. In the prevalent Siamese-based paradigm, the features extracted by the Siamese-like networks are often insufficient to model the tracked targets and distractor objects, thereby hindering them from being robust and discriminative simultaneously. While most Siamese trackers focus on designing robust correlation operations, we propose a novel single-branch tracking framework inspired by the transformer. Unlike the Siamese-like feature extraction, our tracker deeply embeds cross-image feature correlation in multiple layers of the feature network. By extensively matching the features of the two images through multiple layers, it can suppress non-target features, resulting in target-aware feature extraction. The output features can be directly used for predicting target locations without additional correlation steps. Thus, we reformulate the two-branch Siamese tracking as a conceptually simple, fully transformer-based Single-Branch Tracking pipeline, dubbed SBT. After conducting an in-depth analysis of the SBT baseline, we summarize many effective design principles and propose an improved tracker dubbed SuperSBT. SuperSBT adopts a hierarchical architecture with a local modeling layer to enhance shallow-level features. A unified relation modeling is proposed to remove complex handcrafted layer pattern designs. SuperSBT is further improved by masked image modeling pre-training, integrating temporal modeling, and equipping with dedicated prediction heads. Thus, SuperSBT outperforms the SBT baseline by 4.7%,3.0%, and 4.5% AUC scores in LaSOT, TrackingNet, and GOT-10K. Notably, SuperSBT greatly raises the speed of SBT from 37 FPS to 81 FPS. Extensive experiments show that our method achieves superior results on eight VOT benchmarks.","sentences":["Developing robust and discriminative appearance models has been a long-standing research challenge in visual object tracking.","In the prevalent Siamese-based paradigm, the features extracted by the Siamese-like networks are often insufficient to model the tracked targets and distractor objects, thereby hindering them from being robust and discriminative simultaneously.","While most Siamese trackers focus on designing robust correlation operations, we propose a novel single-branch tracking framework inspired by the transformer.","Unlike the Siamese-like feature extraction, our tracker deeply embeds cross-image feature correlation in multiple layers of the feature network.","By extensively matching the features of the two images through multiple layers, it can suppress non-target features, resulting in target-aware feature extraction.","The output features can be directly used for predicting target locations without additional correlation steps.","Thus, we reformulate the two-branch Siamese tracking as a conceptually simple, fully transformer-based Single-Branch Tracking pipeline, dubbed SBT.","After conducting an in-depth analysis of the SBT baseline, we summarize many effective design principles and propose an improved tracker dubbed SuperSBT.","SuperSBT adopts a hierarchical architecture with a local modeling layer to enhance shallow-level features.","A unified relation modeling is proposed to remove complex handcrafted layer pattern designs.","SuperSBT is further improved by masked image modeling pre-training, integrating temporal modeling, and equipping with dedicated prediction heads.","Thus, SuperSBT outperforms the SBT baseline by 4.7%,3.0%, and 4.5% AUC scores in LaSOT, TrackingNet, and GOT-10K. Notably, SuperSBT greatly raises the speed of SBT from 37 FPS to 81 FPS.","Extensive experiments show that our method achieves superior results on eight VOT benchmarks."],"url":"http://arxiv.org/abs/2401.12743v1","category":"cs.CV"}
{"created":"2024-01-23 13:15:43","title":"Decoding University Hierarchy and Prestige in China through Domestic Ph.D. Hiring Network","abstract":"The academic job market for fresh Ph.D. students to pursue postdoctoral and junior faculty positions plays a crucial role in shaping the future orientations, developments, and status of the global academic system. In this work, we focus on the domestic Ph.D. hiring network among universities in China by exploring the doctoral education and academic employment of nearly 28,000 scientists across all Ph.D.-granting Chinese universities over three decades. We employ the minimum violation rankings algorithm to decode the rankings for universities based on the Ph.D. hiring network, which offers a deep understanding of the structure and dynamics within the network. Our results uncover a consistent, highly structured hierarchy within this hiring network, indicating the imbalances wherein a limited number of universities serve as the main sources of fresh Ph.D. across diverse disciplines. Furthermore, over time, it has become increasingly challenging for Chinese Ph.D. graduates to secure positions at institutions more prestigious than their alma maters. This study quantitatively captures the evolving structure of talent circulation in the domestic environment, providing valuable insights to enhance the organization, diversity, and talent distribution in China's academic enterprise.","sentences":["The academic job market for fresh Ph.D. students to pursue postdoctoral and junior faculty positions plays a crucial role in shaping the future orientations, developments, and status of the global academic system.","In this work, we focus on the domestic Ph.D. hiring network among universities in China by exploring the doctoral education and academic employment of nearly 28,000 scientists across all Ph.D.-granting Chinese universities over three decades.","We employ the minimum violation rankings algorithm to decode the rankings for universities based on the Ph.D. hiring network, which offers a deep understanding of the structure and dynamics within the network.","Our results uncover a consistent, highly structured hierarchy within this hiring network, indicating the imbalances wherein a limited number of universities serve as the main sources of fresh Ph.D. across diverse disciplines.","Furthermore, over time, it has become increasingly challenging for Chinese Ph.D. graduates to secure positions at institutions more prestigious than their alma maters.","This study quantitatively captures the evolving structure of talent circulation in the domestic environment, providing valuable insights to enhance the organization, diversity, and talent distribution in China's academic enterprise."],"url":"http://arxiv.org/abs/2401.12739v1","category":"cs.DL"}
{"created":"2024-01-23 13:13:45","title":"Shift-ConvNets: Small Convolutional Kernel with Large Kernel Effects","abstract":"Recent studies reveal that the remarkable performance of Vision transformers (ViTs) benefits from large receptive fields. For this reason, the large convolutional kernel design becomes an ideal solution to make Convolutional Neural Networks (CNNs) great again. However, the typical large convolutional kernels turn out to be hardware-unfriendly operators, resulting in discount compatibility of various hardware platforms. Thus, it is unwise to simply enlarge the convolutional kernel size. In this paper, we reveal that small convolutional kernels and convolution operations can achieve the closing effects of large kernel sizes. Then, we propose a shift-wise operator that ensures the CNNs capture long-range dependencies with the help of the sparse mechanism, while remaining hardware-friendly. Experimental results show that our shift-wise operator significantly improves the accuracy of a regular CNN while markedly reducing computational requirements. On the ImageNet-1k, our shift-wise enhanced CNN model outperforms the state-of-the-art models. Code & models at https://github.com/lidc54/shift-wiseConv.","sentences":["Recent studies reveal that the remarkable performance of Vision transformers (ViTs) benefits from large receptive fields.","For this reason, the large convolutional kernel design becomes an ideal solution to make Convolutional Neural Networks (CNNs) great again.","However, the typical large convolutional kernels turn out to be hardware-unfriendly operators, resulting in discount compatibility of various hardware platforms.","Thus, it is unwise to simply enlarge the convolutional kernel size.","In this paper, we reveal that small convolutional kernels and convolution operations can achieve the closing effects of large kernel sizes.","Then, we propose a shift-wise operator that ensures the CNNs capture long-range dependencies with the help of the sparse mechanism, while remaining hardware-friendly.","Experimental results show that our shift-wise operator significantly improves the accuracy of a regular CNN while markedly reducing computational requirements.","On the ImageNet-1k, our shift-wise enhanced CNN model outperforms the state-of-the-art models.","Code & models at https://github.com/lidc54/shift-wiseConv."],"url":"http://arxiv.org/abs/2401.12736v1","category":"cs.CV"}
{"created":"2024-01-23 13:11:05","title":"TNANet: A Temporal-Noise-Aware Neural Network for Suicidal Ideation Prediction with Noisy Physiological Data","abstract":"The robust generalization of deep learning models in the presence of inherent noise remains a significant challenge, especially when labels are subjective and noise is indiscernible in natural settings. This problem is particularly pronounced in many practical applications. In this paper, we address a special and important scenario of monitoring suicidal ideation, where time-series data, such as photoplethysmography (PPG), is susceptible to such noise. Current methods predominantly focus on image and text data or address artificially introduced noise, neglecting the complexities of natural noise in time-series analysis. To tackle this, we introduce a novel neural network model tailored for analyzing noisy physiological time-series data, named TNANet, which merges advanced encoding techniques with confidence learning, enhancing prediction accuracy. Another contribution of our work is the collection of a specialized dataset of PPG signals derived from real-world environments for suicidal ideation prediction. Employing this dataset, our TNANet achieves the prediction accuracy of 63.33% in a binary classification task, outperforming state-of-the-art models. Furthermore, comprehensive evaluations were conducted on three other well-known public datasets with artificially introduced noise to rigorously test the TNANet's capabilities. These tests consistently demonstrated TNANet's superior performance by achieving an accuracy improvement of more than 10% compared to baseline methods.","sentences":["The robust generalization of deep learning models in the presence of inherent noise remains a significant challenge, especially when labels are subjective and noise is indiscernible in natural settings.","This problem is particularly pronounced in many practical applications.","In this paper, we address a special and important scenario of monitoring suicidal ideation, where time-series data, such as photoplethysmography (PPG), is susceptible to such noise.","Current methods predominantly focus on image and text data or address artificially introduced noise, neglecting the complexities of natural noise in time-series analysis.","To tackle this, we introduce a novel neural network model tailored for analyzing noisy physiological time-series data, named TNANet, which merges advanced encoding techniques with confidence learning, enhancing prediction accuracy.","Another contribution of our work is the collection of a specialized dataset of PPG signals derived from real-world environments for suicidal ideation prediction.","Employing this dataset, our TNANet achieves the prediction accuracy of 63.33% in a binary classification task, outperforming state-of-the-art models.","Furthermore, comprehensive evaluations were conducted on three other well-known public datasets with artificially introduced noise to rigorously test the TNANet's capabilities.","These tests consistently demonstrated TNANet's superior performance by achieving an accuracy improvement of more than 10% compared to baseline methods."],"url":"http://arxiv.org/abs/2401.12733v1","category":"cs.CY"}
{"created":"2024-01-23 13:06:19","title":"CDRNP: Cross-Domain Recommendation to Cold-Start Users via Neural Process","abstract":"Cross-domain recommendation (CDR) has been proven as a promising way to tackle the user cold-start problem, which aims to make recommendations for users in the target domain by transferring the user preference derived from the source domain. Traditional CDR studies follow the embedding and mapping (EMCDR) paradigm, which transfers user representations from the source to target domain by learning a user-shared mapping function, neglecting the user-specific preference. Recent CDR studies attempt to learn user-specific mapping functions in meta-learning paradigm, which regards each user's CDR as an individual task, but neglects the preference correlations among users, limiting the beneficial information for user representations. Moreover, both of the paradigms neglect the explicit user-item interactions from both domains during the mapping process. To address the above issues, this paper proposes a novel CDR framework with neural process (NP), termed as CDRNP. Particularly, it develops the meta-learning paradigm to leverage user-specific preference, and further introduces a stochastic process by NP to capture the preference correlations among the overlapping and cold-start users, thus generating more powerful mapping functions by mapping the user-specific preference and common preference correlations to a predictive probability distribution. In addition, we also introduce a preference remainer to enhance the common preference from the overlapping users, and finally devises an adaptive conditional decoder with preference modulation to make prediction for cold-start users with items in the target domain. Experimental results demonstrate that CDRNP outperforms previous SOTA methods in three real-world CDR scenarios.","sentences":["Cross-domain recommendation (CDR) has been proven as a promising way to tackle the user cold-start problem, which aims to make recommendations for users in the target domain by transferring the user preference derived from the source domain.","Traditional CDR studies follow the embedding and mapping (EMCDR) paradigm, which transfers user representations from the source to target domain by learning a user-shared mapping function, neglecting the user-specific preference.","Recent CDR studies attempt to learn user-specific mapping functions in meta-learning paradigm, which regards each user's CDR as an individual task, but neglects the preference correlations among users, limiting the beneficial information for user representations.","Moreover, both of the paradigms neglect the explicit user-item interactions from both domains during the mapping process.","To address the above issues, this paper proposes a novel CDR framework with neural process (NP), termed as CDRNP.","Particularly, it develops the meta-learning paradigm to leverage user-specific preference, and further introduces a stochastic process by NP to capture the preference correlations among the overlapping and cold-start users, thus generating more powerful mapping functions by mapping the user-specific preference and common preference correlations to a predictive probability distribution.","In addition, we also introduce a preference remainer to enhance the common preference from the overlapping users, and finally devises an adaptive conditional decoder with preference modulation to make prediction for cold-start users with items in the target domain.","Experimental results demonstrate that CDRNP outperforms previous SOTA methods in three real-world CDR scenarios."],"url":"http://arxiv.org/abs/2401.12732v1","category":"cs.IR"}
{"created":"2024-01-23 13:04:02","title":"The Distributional Uncertainty of the SHAP score in Explainable Machine Learning","abstract":"Attribution scores reflect how important the feature values in an input entity are for the output of a machine learning model. One of the most popular attribution scores is the SHAP score, which is an instantiation of the general Shapley value used in coalition game theory. The definition of this score relies on a probability distribution on the entity population. Since the exact distribution is generally unknown, it needs to be assigned subjectively or be estimated from data, which may lead to misleading feature scores. In this paper, we propose a principled framework for reasoning on SHAP scores under unknown entity population distributions. In our framework, we consider an uncertainty region that contains the potential distributions, and the SHAP score of a feature becomes a function defined over this region. We study the basic problems of finding maxima and minima of this function, which allows us to determine tight ranges for the SHAP scores of all features. In particular, we pinpoint the complexity of these problems, and other related ones, showing them to be NP-complete. Finally, we present experiments on a real-world dataset, showing that our framework may contribute to a more robust feature scoring.","sentences":["Attribution scores reflect how important the feature values in an input entity are for the output of a machine learning model.","One of the most popular attribution scores is the SHAP score, which is an instantiation of the general Shapley value used in coalition game theory.","The definition of this score relies on a probability distribution on the entity population.","Since the exact distribution is generally unknown, it needs to be assigned subjectively or be estimated from data, which may lead to misleading feature scores.","In this paper, we propose a principled framework for reasoning on SHAP scores under unknown entity population distributions.","In our framework, we consider an uncertainty region that contains the potential distributions, and the SHAP score of a feature becomes a function defined over this region.","We study the basic problems of finding maxima and minima of this function, which allows us to determine tight ranges for the SHAP scores of all features.","In particular, we pinpoint the complexity of these problems, and other related ones, showing them to be NP-complete.","Finally, we present experiments on a real-world dataset, showing that our framework may contribute to a more robust feature scoring."],"url":"http://arxiv.org/abs/2401.12731v1","category":"cs.AI"}
{"created":"2024-01-23 13:02:11","title":"Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios","abstract":"Object Detection (OD) has proven to be a significant computer vision method in extracting localized class information and has multiple applications in the industry. Although many of the state-of-the-art (SOTA) OD models perform well on medium and large sized objects, they seem to under perform on small objects. In most of the industrial use cases, it is difficult to collect and annotate data for small objects, as it is time-consuming and prone to human errors. Additionally, those datasets are likely to be unbalanced and often result in an inefficient model convergence. To tackle this challenge, this study presents a novel approach that injects additional data points to improve the performance of the OD models. Using synthetic data generation, the difficulties in data collection and annotations for small object data points can be minimized and to create a dataset with balanced distribution. This paper discusses the effects of a simple proportional class-balancing technique, to enable better anchor matching of the OD models. A comparison was carried out on the performances of the SOTA OD models: YOLOv5, YOLOv7 and SSD, for combinations of real and synthetic datasets within an industrial use case.","sentences":["Object Detection (OD) has proven to be a significant computer vision method in extracting localized class information and has multiple applications in the industry.","Although many of the state-of-the-art (SOTA) OD models perform well on medium and large sized objects, they seem to under perform on small objects.","In most of the industrial use cases, it is difficult to collect and annotate data for small objects, as it is time-consuming and prone to human errors.","Additionally, those datasets are likely to be unbalanced and often result in an inefficient model convergence.","To tackle this challenge, this study presents a novel approach that injects additional data points to improve the performance of the OD models.","Using synthetic data generation, the difficulties in data collection and annotations for small object data points can be minimized and to create a dataset with balanced distribution.","This paper discusses the effects of a simple proportional class-balancing technique, to enable better anchor matching of the OD models.","A comparison was carried out on the performances of the SOTA OD models: YOLOv5, YOLOv7 and SSD, for combinations of real and synthetic datasets within an industrial use case."],"url":"http://arxiv.org/abs/2401.12729v1","category":"cs.CV"}
{"created":"2024-01-23 12:58:41","title":"On A Proof of the ADKMV Conjecture","abstract":"We present a mathematical proof of a conjectural formula due to Aganagic, Dijkgraaf, Klemm, Mari\\~no and Vafa, expressing the topological vertex as a Bogoliubov transform of the fermionic vacuum. In our proof we introduce a boson-fermionic field assignment which generalizes the well-known boson-fermion correspondence. The proof also works for the generalization to the framed topological vertex made by Deng and Zhou. As a consequence, partition functions of toric Calabi-Yau threefolds are related to tau-functions of multi-component KP hierarchy.","sentences":["We present a mathematical proof of a conjectural formula due to Aganagic, Dijkgraaf, Klemm, Mari\\~no and Vafa, expressing the topological vertex as a Bogoliubov transform of the fermionic vacuum.","In our proof we introduce a boson-fermionic field assignment which generalizes the well-known boson-fermion correspondence.","The proof also works for the generalization to the framed topological vertex made by Deng and Zhou.","As a consequence, partition functions of toric Calabi-Yau threefolds are related to tau-functions of multi-component KP hierarchy."],"url":"http://arxiv.org/abs/2401.12726v1","category":"math-ph"}
{"created":"2024-01-23 12:53:02","title":"A Multi-scale Yarn Appearance Model with Fiber Details","abstract":"Rendering realistic cloth has always been a challenge due to its intricate structure. Cloth is made up of fibers, plies, and yarns, and previous curved-based models, while detailed, were computationally expensive and inflexible for large cloth. To address this, we propose a simplified approach.   We introduce a geometric aggregation technique that reduces ray-tracing computation by using fewer curves, focusing only on yarn curves. Our model generates ply and fiber shapes implicitly, compensating for the lack of explicit geometry with a novel shadowing component. We also present a shading model that simplifies light interactions among fibers by categorizing them into four components, accurately capturing specular and scattered light in both forward and backward directions.   To render large cloth efficiently, we propose a multi-scale solution based on pixel coverage. Our yarn shading model outperforms previous methods, achieving rendering speeds 3-5 times faster with less memory in near-field views. Additionally, our multi-scale solution offers a 20% speed boost for distant cloth observation.","sentences":["Rendering realistic cloth has always been a challenge due to its intricate structure.","Cloth is made up of fibers, plies, and yarns, and previous curved-based models, while detailed, were computationally expensive and inflexible for large cloth.","To address this, we propose a simplified approach.   ","We introduce a geometric aggregation technique that reduces ray-tracing computation by using fewer curves, focusing only on yarn curves.","Our model generates ply and fiber shapes implicitly, compensating for the lack of explicit geometry with a novel shadowing component.","We also present a shading model that simplifies light interactions among fibers by categorizing them into four components, accurately capturing specular and scattered light in both forward and backward directions.   ","To render large cloth efficiently, we propose a multi-scale solution based on pixel coverage.","Our yarn shading model outperforms previous methods, achieving rendering speeds 3-5 times faster with less memory in near-field views.","Additionally, our multi-scale solution offers a 20% speed boost for distant cloth observation."],"url":"http://arxiv.org/abs/2401.12724v1","category":"cs.GR"}
{"created":"2024-01-23 12:49:49","title":"A regular MOG black hole's impact on shadows and gravitational weak lensing in the presence of quintessence field","abstract":"We investigate the impact of the modified gravity (MOG) field and the quintessence scalar field on horizon evolution, black hole (BH) shadow and the weak gravitational lensing around a static spherically symmetric BH. We first begin to write the BH metric associated with the MOG parameter and quintessence scalar field. We then determine the BH shadow and obtain numerical solutions for the photon sphere and shadow radius. We show that the MOG ($\\alpha$) and the quintessence ($c$) parameters have a significant impact on BH shadow and photon sphere. Based on the analysis, we further show that the combined effects of the MOG parameter and quintessential field can increase the values of BH shadow and photon sphere radii. We also obtain constraints on the BH parameters by applying the observational data of Sgr A$^{\\star}$ and M87$^{\\star}$. Finally, we consider the weak deflection angle of BH within the context of the Gauss-Bonnet theorem (GBT) and show that the combined effects of the MOG and quintessence parameters do make the value of the deflection angle grow, referring to remarkable property being in well agreement with the physical meaning of both parameters that can maintain the strong gravitational field in the surrounding environment of BH.","sentences":["We investigate the impact of the modified gravity (MOG) field and the quintessence scalar field on horizon evolution, black hole (BH) shadow and the weak gravitational lensing around a static spherically symmetric BH.","We first begin to write the BH metric associated with the MOG parameter and quintessence scalar field.","We then determine the BH shadow and obtain numerical solutions for the photon sphere and shadow radius.","We show that the MOG ($\\alpha$) and the quintessence ($c$) parameters have a significant impact on BH shadow and photon sphere.","Based on the analysis, we further show that the combined effects of the MOG parameter and quintessential field can increase the values of BH shadow and photon sphere radii.","We also obtain constraints on the BH parameters by applying the observational data of Sgr A$^{\\star}$ and M87$^{\\star}$.","Finally, we consider the weak deflection angle of BH within the context of the Gauss-Bonnet theorem (GBT) and show that the combined effects of the MOG and quintessence parameters do make the value of the deflection angle grow, referring to remarkable property being in well agreement with the physical meaning of both parameters that can maintain the strong gravitational field in the surrounding environment of BH."],"url":"http://arxiv.org/abs/2401.12723v1","category":"gr-qc"}
{"created":"2024-01-23 12:48:27","title":"Falcon: Fair Active Learning using Multi-armed Bandits","abstract":"Biased data can lead to unfair machine learning models, highlighting the importance of embedding fairness at the beginning of data analysis, particularly during dataset curation and labeling. In response, we propose Falcon, a scalable fair active learning framework. Falcon adopts a data-centric approach that improves machine learning model fairness via strategic sample selection. Given a user-specified group fairness measure, Falcon identifies samples from \"target groups\" (e.g., (attribute=female, label=positive)) that are the most informative for improving fairness. However, a challenge arises since these target groups are defined using ground truth labels that are not available during sample selection. To handle this, we propose a novel trial-and-error method, where we postpone using a sample if the predicted label is different from the expected one and falls outside the target group. We also observe the trade-off that selecting more informative samples results in higher likelihood of postponing due to undesired label prediction, and the optimal balance varies per dataset. We capture the trade-off between informativeness and postpone rate as policies and propose to automatically select the best policy using adversarial multi-armed bandit methods, given their computational efficiency and theoretical guarantees. Experiments show that Falcon significantly outperforms existing fair active learning approaches in terms of fairness and accuracy and is more efficient. In particular, only Falcon supports a proper trade-off between accuracy and fairness where its maximum fairness score is 1.8-4.5x higher than the second-best results.","sentences":["Biased data can lead to unfair machine learning models, highlighting the importance of embedding fairness at the beginning of data analysis, particularly during dataset curation and labeling.","In response, we propose Falcon, a scalable fair active learning framework.","Falcon adopts a data-centric approach that improves machine learning model fairness via strategic sample selection.","Given a user-specified group fairness measure, Falcon identifies samples from \"target groups\" (e.g., (attribute=female, label=positive))","that are the most informative for improving fairness.","However, a challenge arises since these target groups are defined using ground truth labels that are not available during sample selection.","To handle this, we propose a novel trial-and-error method, where we postpone using a sample if the predicted label is different from the expected one and falls outside the target group.","We also observe the trade-off that selecting more informative samples results in higher likelihood of postponing due to undesired label prediction, and the optimal balance varies per dataset.","We capture the trade-off between informativeness and postpone rate as policies and propose to automatically select the best policy using adversarial multi-armed bandit methods, given their computational efficiency and theoretical guarantees.","Experiments show that Falcon significantly outperforms existing fair active learning approaches in terms of fairness and accuracy and is more efficient.","In particular, only Falcon supports a proper trade-off between accuracy and fairness where its maximum fairness score is 1.8-4.5x higher than the second-best results."],"url":"http://arxiv.org/abs/2401.12722v1","category":"cs.LG"}
{"created":"2024-01-23 12:41:03","title":"A Comprehensive View of the Biases of Toxicity and Sentiment Analysis Methods Towards Utterances with African American English Expressions","abstract":"Language is a dynamic aspect of our culture that changes when expressed in different technologies/communities. Online social networks have enabled the diffusion and evolution of different dialects, including African American English (AAE). However, this increased usage is not without barriers. One particular barrier is how sentiment (Vader, TextBlob, and Flair) and toxicity (Google's Perspective and the open-source Detoxify) methods present biases towards utterances with AAE expressions. Consider Google's Perspective to understand bias. Here, an utterance such as ``All n*ggers deserve to die respectfully. The police murder us.'' it reaches a higher toxicity than ``African-Americans deserve to die respectfully. The police murder us.''. This score difference likely arises because the tool cannot understand the re-appropriation of the term ``n*gger''. One explanation for this bias is that AI models are trained on limited datasets, and using such a term in training data is more likely to appear in a toxic utterance. While this may be plausible, the tool will make mistakes regardless. Here, we study bias on two Web-based (YouTube and Twitter) datasets and two spoken English datasets. Our analysis shows how most models present biases towards AAE in most settings. We isolate the impact of AAE expression usage via linguistic control features from the Linguistic Inquiry and Word Count (LIWC) software, grammatical control features extracted via Part-of-Speech (PoS) tagging from Natural Language Processing (NLP) models, and the semantic of utterances by comparing sentence embeddings from recent language models. We present consistent results on how a heavy usage of AAE expressions may cause the speaker to be considered substantially more toxic, even when speaking about nearly the same subject. Our study complements similar analyses focusing on small datasets and/or one method only.","sentences":["Language is a dynamic aspect of our culture that changes when expressed in different technologies/communities.","Online social networks have enabled the diffusion and evolution of different dialects, including African American English (AAE).","However, this increased usage is not without barriers.","One particular barrier is how sentiment (Vader, TextBlob, and Flair) and toxicity (Google's Perspective and the open-source Detoxify) methods present biases towards utterances with AAE expressions.","Consider Google's Perspective to understand bias.","Here, an utterance such as ``All n*ggers deserve to die respectfully.","The police murder us.''","it reaches a higher toxicity than ``African-Americans deserve to die respectfully.","The police murder us.''.","This score difference likely arises because the tool cannot understand the re-appropriation of the term ``n*gger''.","One explanation for this bias is that AI models are trained on limited datasets, and using such a term in training data is more likely to appear in a toxic utterance.","While this may be plausible, the tool will make mistakes regardless.","Here, we study bias on two Web-based (YouTube and Twitter) datasets and two spoken English datasets.","Our analysis shows how most models present biases towards AAE in most settings.","We isolate the impact of AAE expression usage via linguistic control features from the Linguistic Inquiry and Word Count (LIWC) software, grammatical control features extracted via Part-of-Speech (PoS) tagging from Natural Language Processing (NLP) models, and the semantic of utterances by comparing sentence embeddings from recent language models.","We present consistent results on how a heavy usage of AAE expressions may cause the speaker to be considered substantially more toxic, even when speaking about nearly the same subject.","Our study complements similar analyses focusing on small datasets and/or one method only."],"url":"http://arxiv.org/abs/2401.12720v1","category":"cs.CL"}
{"created":"2024-01-23 12:39:37","title":"Device-independent quantum state discrimination","abstract":"Quantum state discrimination depicts the general progress of extracting classical information from quantum systems. We show that quantum state discrimination can be realized in a device-independent scenario using tools of self-testing results. That is, the states can be discriminated credibly with the untrusted experiment devices by the correspondence between quantum correlations and states. In detail, we show that two arbitrary states can be discriminated in a device-independent manner when they are not conjugate with each other, while other states can be discriminated measurement-device-independently. To fulfill the device-independent requirement, the measurements are restricted on Pauli observables. The influence of this restriction is acceptable based on the guessing probability analysis for minimum error discrimination.","sentences":["Quantum state discrimination depicts the general progress of extracting classical information from quantum systems.","We show that quantum state discrimination can be realized in a device-independent scenario using tools of self-testing results.","That is, the states can be discriminated credibly with the untrusted experiment devices by the correspondence between quantum correlations and states.","In detail, we show that two arbitrary states can be discriminated in a device-independent manner when they are not conjugate with each other, while other states can be discriminated measurement-device-independently.","To fulfill the device-independent requirement, the measurements are restricted on Pauli observables.","The influence of this restriction is acceptable based on the guessing probability analysis for minimum error discrimination."],"url":"http://arxiv.org/abs/2401.12719v1","category":"quant-ph"}
{"created":"2024-01-23 12:39:15","title":"Gas trap prediction from 3D seismic and well test data using machine learning","abstract":"The aim of this work is to create and apply a methodological approach for predicting gas traps from 3D seismic data and gas well testing. The paper formalizes the approach to creating a training dataset by selecting volumes with established gas saturation and filtration properties within the seismic wavefield. The training dataset thus created is used in a process stack of sequential application of data processing methods and ensemble machine learning algorithms. As a result, a cube of calibrated probabilities of belonging of the study space to gas reservoirs was obtained. The high efficiency of this approach is shown on a delayed test sample of three wells (blind wells). The final value of the gas reservoir prediction quality metric f1 score was 0.893846.","sentences":["The aim of this work is to create and apply a methodological approach for predicting gas traps from 3D seismic data and gas well testing.","The paper formalizes the approach to creating a training dataset by selecting volumes with established gas saturation and filtration properties within the seismic wavefield.","The training dataset thus created is used in a process stack of sequential application of data processing methods and ensemble machine learning algorithms.","As a result, a cube of calibrated probabilities of belonging of the study space to gas reservoirs was obtained.","The high efficiency of this approach is shown on a delayed test sample of three wells (blind wells).","The final value of the gas reservoir prediction quality metric f1 score was 0.893846."],"url":"http://arxiv.org/abs/2401.12717v1","category":"physics.geo-ph"}
{"created":"2024-01-23 12:33:56","title":"On positively divisible non-Markovian processes","abstract":"There are some positively divisible non-Markovian processes whose transition matrices satisfy the Chapman-Kolmogorov equation. These processes should also satisfy the Kolmogorov consistency conditions, an essential requirement for a process to be classified as a stochastic process. Combining the Kolmogorov consistency conditions with the Chapman-Kolmogorov equation, we derive a necessary condition for positively divisible stochastic processes on a finite sample space. This necessary condition enables a systematic approach to the manipulation of certain Markov processes in order to obtain a positively divisible non-Markovian process. We illustrate this idea by an example and, in addition, analyze a classic example given by Feller in the light of our approach.","sentences":["There are some positively divisible non-Markovian processes whose transition matrices satisfy the Chapman-Kolmogorov equation.","These processes should also satisfy the Kolmogorov consistency conditions, an essential requirement for a process to be classified as a stochastic process.","Combining the Kolmogorov consistency conditions with the Chapman-Kolmogorov equation, we derive a necessary condition for positively divisible stochastic processes on a finite sample space.","This necessary condition enables a systematic approach to the manipulation of certain Markov processes in order to obtain a positively divisible non-Markovian process.","We illustrate this idea by an example and, in addition, analyze a classic example given by Feller in the light of our approach."],"url":"http://arxiv.org/abs/2401.12715v1","category":"math.PR"}
{"created":"2024-01-23 12:29:42","title":"Evaluation of large language models for assessing code maintainability","abstract":"Increased availability of open-source software repositories and recent advances in code analysis using large language models (LLMs) has triggered a wave of new work to automate software engineering tasks that were previously very difficult to automate. In this paper, we investigate a recent line of work that hypothesises that comparing the probability of code generated by LLMs with the probability the current code would have had can indicate potential quality problems. We investigate the association between the cross-entropy of code generated by ten different models (based on GPT2 and Llama2) and the following quality aspects: readability, understandability, complexity, modularisation, and overall maintainability assessed by experts and available in an benchmark dataset. Our results show that, controlling for the number of logical lines of codes (LLOC), cross-entropy computed by LLMs is indeed a predictor of maintainability on a class level (the higher the cross-entropy the lower the maintainability). However, this relation is reversed when one does not control for LLOC (e.g., comparing small classes with longer ones). Furthermore, while the complexity of LLMs affects the range of cross-entropy (smaller models tend to have a wider range of cross-entropy), this plays a significant role in predicting maintainability aspects. Our study limits itself on ten different pretrained models (based on GPT2 and Llama2) and on maintainability aspects collected by Schnappinger et al. When controlling for logical lines of code (LLOC), cross-entropy is a predictor of maintainability. However, while related work has shown the potential usefulness of cross-entropy at the level of tokens or short sequences, at the class level this criterion alone may prove insufficient to predict maintainability and further research is needed to make best use of this information in practice.","sentences":["Increased availability of open-source software repositories and recent advances in code analysis using large language models (LLMs) has triggered a wave of new work to automate software engineering tasks that were previously very difficult to automate.","In this paper, we investigate a recent line of work that hypothesises that comparing the probability of code generated by LLMs with the probability the current code would have had can indicate potential quality problems.","We investigate the association between the cross-entropy of code generated by ten different models (based on GPT2 and Llama2) and the following quality aspects: readability, understandability, complexity, modularisation, and overall maintainability assessed by experts and available in an benchmark dataset.","Our results show that, controlling for the number of logical lines of codes (LLOC), cross-entropy computed by LLMs is indeed a predictor of maintainability on a class level (the higher the cross-entropy the lower the maintainability).","However, this relation is reversed when one does not control for LLOC (e.g., comparing small classes with longer ones).","Furthermore, while the complexity of LLMs affects the range of cross-entropy (smaller models tend to have a wider range of cross-entropy), this plays a significant role in predicting maintainability aspects.","Our study limits itself on ten different pretrained models (based on GPT2 and Llama2) and on maintainability aspects collected by Schnappinger et al.","When controlling for logical lines of code (LLOC), cross-entropy is a predictor of maintainability.","However, while related work has shown the potential usefulness of cross-entropy at the level of tokens or short sequences, at the class level this criterion alone may prove insufficient to predict maintainability and further research is needed to make best use of this information in practice."],"url":"http://arxiv.org/abs/2401.12714v1","category":"cs.SE"}
{"created":"2024-01-23 12:29:37","title":"Generating Unsupervised Abstractive Explanations for Rumour Verification","abstract":"The task of rumour verification in social media concerns assessing the veracity of a claim on the basis of conversation threads that result from it. While previous work has focused on predicting a veracity label, here we reformulate the task to generate model-centric, free-text explanations of a rumour's veracity. We follow an unsupervised approach by first utilising post-hoc explainability methods to score the most important posts within a thread and then we use these posts to generate informative explanatory summaries by employing template-guided summarisation. To evaluate the informativeness of the explanatory summaries, we exploit the few-shot learning capabilities of a large language model (LLM). Our experiments show that LLMs can have similar agreement to humans in evaluating summaries. Importantly, we show that explanatory abstractive summaries are more informative and better reflect the predicted rumour veracity than just using the highest ranking posts in the thread.","sentences":["The task of rumour verification in social media concerns assessing the veracity of a claim on the basis of conversation threads that result from it.","While previous work has focused on predicting a veracity label, here we reformulate the task to generate model-centric, free-text explanations of a rumour's veracity.","We follow an unsupervised approach by first utilising post-hoc explainability methods to score the most important posts within a thread and then we use these posts to generate informative explanatory summaries by employing template-guided summarisation.","To evaluate the informativeness of the explanatory summaries, we exploit the few-shot learning capabilities of a large language model (LLM).","Our experiments show that LLMs can have similar agreement to humans in evaluating summaries.","Importantly, we show that explanatory abstractive summaries are more informative and better reflect the predicted rumour veracity than just using the highest ranking posts in the thread."],"url":"http://arxiv.org/abs/2401.12713v1","category":"cs.CL"}
{"created":"2024-01-23 12:20:17","title":"When Redundancy Matters: Machine Teaching of Representations","abstract":"In traditional machine teaching, a teacher wants to teach a concept to a learner, by means of a finite set of examples, the witness set. But concepts can have many equivalent representations. This redundancy strongly affects the search space, to the extent that teacher and learner may not be able to easily determine the equivalence class of each representation. In this common situation, instead of teaching concepts, we explore the idea of teaching representations. We work with several teaching schemas that exploit representation and witness size (Eager, Greedy and Optimal) and analyze the gains in teaching effectiveness for some representational languages (DNF expressions and Turing-complete P3 programs). Our theoretical and experimental results indicate that there are various types of redundancy, handled better by the Greedy schema introduced here than by the Eager schema, although both can be arbitrarily far away from the Optimal. For P3 programs we found that witness sets are usually smaller than the programs they identify, which is an illuminating justification of why machine teaching from examples makes sense at all.","sentences":["In traditional machine teaching, a teacher wants to teach a concept to a learner, by means of a finite set of examples, the witness set.","But concepts can have many equivalent representations.","This redundancy strongly affects the search space, to the extent that teacher and learner may not be able to easily determine the equivalence class of each representation.","In this common situation, instead of teaching concepts, we explore the idea of teaching representations.","We work with several teaching schemas that exploit representation and witness size (Eager, Greedy and Optimal) and analyze the gains in teaching effectiveness for some representational languages (DNF expressions and Turing-complete P3 programs).","Our theoretical and experimental results indicate that there are various types of redundancy, handled better by the Greedy schema introduced here than by the Eager schema, although both can be arbitrarily far away from the Optimal.","For P3 programs we found that witness sets are usually smaller than the programs they identify, which is an illuminating justification of why machine teaching from examples makes sense at all."],"url":"http://arxiv.org/abs/2401.12711v1","category":"cs.LG"}
{"created":"2024-01-23 12:19:48","title":"New spectral-parameter dependent solutions of the Yang-Baxter equation","abstract":"The Yang-Baxter Equation (YBE) plays a crucial role for studying integrable many-body quantum systems. Many known YBE solutions provide various examples ranging from quantum spin chains to superconducting systems. Models of solvable statistical mechanics and their avatars are also based on YBE. Therefore, new solutions of the YBE could be used to construct new interesting 1D quantum or 2D classical systems with many other far-reaching applications. In this work, we attempt to find (almost) exhaustive set of solutions for the YBE in the lowest dimensions corresponding to a two-qubit case. We develop an algorithm, which can potentially be used for generating new higher-dimensional solutions of the YBE.","sentences":["The Yang-Baxter Equation (YBE) plays a crucial role for studying integrable many-body quantum systems.","Many known YBE solutions provide various examples ranging from quantum spin chains to superconducting systems.","Models of solvable statistical mechanics and their avatars are also based on YBE.","Therefore, new solutions of the YBE could be used to construct new interesting 1D quantum or 2D classical systems with many other far-reaching applications.","In this work, we attempt to find (almost) exhaustive set of solutions for the YBE in the lowest dimensions corresponding to a two-qubit case.","We develop an algorithm, which can potentially be used for generating new higher-dimensional solutions of the YBE."],"url":"http://arxiv.org/abs/2401.12710v1","category":"quant-ph"}
{"created":"2024-01-23 12:15:47","title":"Deep Neural Network Benchmarks for Selective Classification","abstract":"With the increasing deployment of machine learning models in many socially-sensitive tasks, there is a growing demand for reliable and trustworthy predictions. One way to accomplish these requirements is to allow a model to abstain from making a prediction when there is a high risk of making an error. This requires adding a selection mechanism to the model, which selects those examples for which the model will provide a prediction. The selective classification framework aims to design a mechanism that balances the fraction of rejected predictions (i.e., the proportion of examples for which the model does not make a prediction) versus the improvement in predictive performance on the selected predictions. Multiple selective classification frameworks exist, most of which rely on deep neural network architectures. However, the empirical evaluation of the existing approaches is still limited to partial comparisons among methods and settings, providing practitioners with little insight into their relative merits. We fill this gap by benchmarking 18 baselines on a diverse set of 44 datasets that includes both image and tabular data. Moreover, there is a mix of binary and multiclass tasks. We evaluate these approaches using several criteria, including selective error rate, empirical coverage, distribution of rejected instance's classes, and performance on out-of-distribution instances. The results indicate that there is not a single clear winner among the surveyed baselines, and the best method depends on the users' objectives.","sentences":["With the increasing deployment of machine learning models in many socially-sensitive tasks, there is a growing demand for reliable and trustworthy predictions.","One way to accomplish these requirements is to allow a model to abstain from making a prediction when there is a high risk of making an error.","This requires adding a selection mechanism to the model, which selects those examples for which the model will provide a prediction.","The selective classification framework aims to design a mechanism that balances the fraction of rejected predictions (i.e., the proportion of examples for which the model does not make a prediction) versus the improvement in predictive performance on the selected predictions.","Multiple selective classification frameworks exist, most of which rely on deep neural network architectures.","However, the empirical evaluation of the existing approaches is still limited to partial comparisons among methods and settings, providing practitioners with little insight into their relative merits.","We fill this gap by benchmarking 18 baselines on a diverse set of 44 datasets that includes both image and tabular data.","Moreover, there is a mix of binary and multiclass tasks.","We evaluate these approaches using several criteria, including selective error rate, empirical coverage, distribution of rejected instance's classes, and performance on out-of-distribution instances.","The results indicate that there is not a single clear winner among the surveyed baselines, and the best method depends on the users' objectives."],"url":"http://arxiv.org/abs/2401.12708v1","category":"cs.LG"}
{"created":"2024-01-23 12:13:44","title":"An ion trap design for a space-deployable strontium-ion optical clock","abstract":"Optical atomic clocks demonstrate a better stability and lower systematic uncertainty than the highest performance microwave atomic clocks. However, the best performing optical clocks have a large footprint in a laboratory environment and require specialist skills to maintain continuous operation. Growing and evolving needs across several sectors are increasing the demand for compact robust and portable devices at this capability level. In this paper we discuss the design of a physics package for a compact laser-cooled 88Sr+ optical clock that would, with further development, be suitable for space deployment. We review the design parameters to target a relative frequency uncertainty at the low parts in 10^18 with this system. We then explain the results of finite element modelling to simulate the response of the ion trap and vacuum chamber to vibration, shock and thermal conditions expected during launch and space deployment. Additionally, an electrostatic model has been developed to investigate the relationship between the ion trap geometrical tolerances and the trapping efficiency. We present the results from these analyses that have led to the design of a more robust prototype ready for experimental testing.","sentences":["Optical atomic clocks demonstrate a better stability and lower systematic uncertainty than the highest performance microwave atomic clocks.","However, the best performing optical clocks have a large footprint in a laboratory environment and require specialist skills to maintain continuous operation.","Growing and evolving needs across several sectors are increasing the demand for compact robust and portable devices at this capability level.","In this paper we discuss the design of a physics package for a compact laser-cooled 88Sr+ optical clock that would, with further development, be suitable for space deployment.","We review the design parameters to target a relative frequency uncertainty at the low parts in 10^18 with this system.","We then explain the results of finite element modelling to simulate the response of the ion trap and vacuum chamber to vibration, shock and thermal conditions expected during launch and space deployment.","Additionally, an electrostatic model has been developed to investigate the relationship between the ion trap geometrical tolerances and the trapping efficiency.","We present the results from these analyses that have led to the design of a more robust prototype ready for experimental testing."],"url":"http://arxiv.org/abs/2401.12706v1","category":"physics.atom-ph"}
{"created":"2024-01-23 12:13:05","title":"Quandles as pre-Lie skew braces, set-theoretic Hopf algebras & universal R-matrices","abstract":"We present connections between left non-degenerate solutions of set-theoretic Yang-Baxter equation and left shelves using certain maps called Drinfel'd homomorphisms. We further generalise the notion of affine quandle, by using heap endomorphisms and metahomomorphisms, and identify the Yang-Baxter algebra for solutions of the braid equation associated to a given quandle. We introduce the notion of the pre-Lie skew brace and identify certain affine quandles that give rise to pre-Lie skew braces. Generalisations of the braiding of a group, associated to set-theoretic solutions of the braid equation is also presented. These generalized structures encode part of the underlying Hopf algebra. Indeed, we also introduce the quasi-triangular Hopf algebras and the universal R-matrices for quandle algebras and for set-theoretic Yang-Baxter algebras. In fact, we obtain the universal R-matrix for the set-theoretic Yang-Baxter algebras after identifying the associated admissible Drinfel'd twist. Generic set-theoretic solutions coming from heap endomorphisms are also identified.","sentences":["We present connections between left non-degenerate solutions of set-theoretic Yang-Baxter equation and left shelves using certain maps called Drinfel'd homomorphisms.","We further generalise the notion of affine quandle, by using heap endomorphisms and metahomomorphisms, and identify the Yang-Baxter algebra for solutions of the braid equation associated to a given quandle.","We introduce the notion of the pre-Lie skew brace and identify certain affine quandles that give rise to pre-Lie skew braces.","Generalisations of the braiding of a group, associated to set-theoretic solutions of the braid equation is also presented.","These generalized structures encode part of the underlying Hopf algebra.","Indeed, we also introduce the quasi-triangular Hopf algebras and the universal R-matrices for quandle algebras and for set-theoretic Yang-Baxter algebras.","In fact, we obtain the universal R-matrix for the set-theoretic Yang-Baxter algebras after identifying the associated admissible Drinfel'd twist.","Generic set-theoretic solutions coming from heap endomorphisms are also identified."],"url":"http://arxiv.org/abs/2401.12704v1","category":"math.QA"}
{"created":"2024-01-23 12:11:16","title":"Small Test Suites for Active Automata Learning","abstract":"A bottleneck in modern active automata learning is to test whether a hypothesized Mealy machine correctly describes the system under learning. The search space for possible counterexamples is given by so-called test suites, consisting of input sequences that have to be checked to decide whether a counterexample exists. This paper shows that significantly smaller test suites suffice under reasonable assumptions on the structure of the black box. These smaller test suites help to refute false hypotheses during active automata learning, even when the assumptions do not hold. We combine multiple test suites using a multi-armed bandit setup that adaptively selects a test suite. An extensive empirical evaluation shows the efficacy of our approach. For small to medium-sized models, the performance gain is limited. However, the approach allows learning models from large, industrial case studies that were beyond the reach of known methods.","sentences":["A bottleneck in modern active automata learning is to test whether a hypothesized Mealy machine correctly describes the system under learning.","The search space for possible counterexamples is given by so-called test suites, consisting of input sequences that have to be checked to decide whether a counterexample exists.","This paper shows that significantly smaller test suites suffice under reasonable assumptions on the structure of the black box.","These smaller test suites help to refute false hypotheses during active automata learning, even when the assumptions do not hold.","We combine multiple test suites using a multi-armed bandit setup that adaptively selects a test suite.","An extensive empirical evaluation shows the efficacy of our approach.","For small to medium-sized models, the performance gain is limited.","However, the approach allows learning models from large, industrial case studies that were beyond the reach of known methods."],"url":"http://arxiv.org/abs/2401.12703v1","category":"cs.LO"}
{"created":"2024-01-23 12:07:20","title":"Securing Recommender System via Cooperative Training","abstract":"Recommender systems are often susceptible to well-crafted fake profiles, leading to biased recommendations. Among existing defense methods, data-processing-based methods inevitably exclude normal samples, while model-based methods struggle to enjoy both generalization and robustness. To this end, we suggest integrating data processing and the robust model to propose a general framework, Triple Cooperative Defense (TCD), which employs three cooperative models that mutually enhance data and thereby improve recommendation robustness. Furthermore, Considering that existing attacks struggle to balance bi-level optimization and efficiency, we revisit poisoning attacks in recommender systems and introduce an efficient attack strategy, Co-training Attack (Co-Attack), which cooperatively optimizes the attack optimization and model training, considering the bi-level setting while maintaining attack efficiency. Moreover, we reveal a potential reason for the insufficient threat of existing attacks is their default assumption of optimizing attacks in undefended scenarios. This overly optimistic setting limits the potential of attacks. Consequently, we put forth a Game-based Co-training Attack (GCoAttack), which frames the proposed CoAttack and TCD as a game-theoretic process, thoroughly exploring CoAttack's attack potential in the cooperative training of attack and defense. Extensive experiments on three real datasets demonstrate TCD's superiority in enhancing model robustness. Additionally, we verify that the two proposed attack strategies significantly outperform existing attacks, with game-based GCoAttack posing a greater poisoning threat than CoAttack.","sentences":["Recommender systems are often susceptible to well-crafted fake profiles, leading to biased recommendations.","Among existing defense methods, data-processing-based methods inevitably exclude normal samples, while model-based methods struggle to enjoy both generalization and robustness.","To this end, we suggest integrating data processing and the robust model to propose a general framework, Triple Cooperative Defense (TCD), which employs three cooperative models that mutually enhance data and thereby improve recommendation robustness.","Furthermore, Considering that existing attacks struggle to balance bi-level optimization and efficiency, we revisit poisoning attacks in recommender systems and introduce an efficient attack strategy, Co-training Attack (Co-Attack), which cooperatively optimizes the attack optimization and model training, considering the bi-level setting while maintaining attack efficiency.","Moreover, we reveal a potential reason for the insufficient threat of existing attacks is their default assumption of optimizing attacks in undefended scenarios.","This overly optimistic setting limits the potential of attacks.","Consequently, we put forth a Game-based Co-training Attack (GCoAttack), which frames the proposed CoAttack and TCD as a game-theoretic process, thoroughly exploring CoAttack's attack potential in the cooperative training of attack and defense.","Extensive experiments on three real datasets demonstrate TCD's superiority in enhancing model robustness.","Additionally, we verify that the two proposed attack strategies significantly outperform existing attacks, with game-based GCoAttack posing a greater poisoning threat than CoAttack."],"url":"http://arxiv.org/abs/2401.12700v1","category":"cs.AI"}
{"created":"2024-01-23 12:06:52","title":"A lightweight decentralized service placement policy for performance optimization in fog computing","abstract":"A decentralized optimization policy for service placement in fog computing is presented. The optimization is addressed to place most popular services as closer to the users as possible. The experimental validation is done in the iFogSim simulator and by comparing our algorithm with the simulator's built-in policy. The simulation is characterized by modeling a microservice-based application for different experiment sizes. Results showed that our decentralized algorithm places most popular services closer to users, improving network usage and service latency of the most requested applications, at the expense of a latency increment for the less requested services and a greater number of service migrations.","sentences":["A decentralized optimization policy for service placement in fog computing is presented.","The optimization is addressed to place most popular services as closer to the users as possible.","The experimental validation is done in the iFogSim simulator and by comparing our algorithm with the simulator's built-in policy.","The simulation is characterized by modeling a microservice-based application for different experiment sizes.","Results showed that our decentralized algorithm places most popular services closer to users, improving network usage and service latency of the most requested applications, at the expense of a latency increment for the less requested services and a greater number of service migrations."],"url":"http://arxiv.org/abs/2401.12699v1","category":"cs.NI"}
{"created":"2024-01-23 12:02:46","title":"Genetic Algorithm for Multi-Objective Optimization of Container Allocation in Cloud Architecture","abstract":"The use of containers in cloud architectures has become widespread because of advantages such as limited overhead, easier and faster deployment and higher portability. Moreover, they are a suitable architectural solution for deployment of applications created using a microservices development pattern. Despite the large number of solutions and implementations, open issues have not been addressed in container automation and management. Container resource allocation influences system performance and resource consumption so it is a key factor for cloud providers. We propose a genetic algorithm approach, using the Non-dominated Sorting Genetic Algorithm-II (NSGA-II), to optimize container allocation and elasticity management due to the good results obtained with this algorithm in other resource management optimization problems in cloud architectures. The optimization has been focused on a tight use of the resources and a reduction of the network overhead and system failure rate. A model for cloud cluster, containers, microservices and four optimization objectives is presented. Experimental results have shown that our approach is a suitable solution to address the problem of container allocation and elasticity and it obtains better objectives values than the container management policies implemented in Kubernetes.","sentences":["The use of containers in cloud architectures has become widespread because of advantages such as limited overhead, easier and faster deployment and higher portability.","Moreover, they are a suitable architectural solution for deployment of applications created using a microservices development pattern.","Despite the large number of solutions and implementations, open issues have not been addressed in container automation and management.","Container resource allocation influences system performance and resource consumption so it is a key factor for cloud providers.","We propose a genetic algorithm approach, using the Non-dominated Sorting Genetic Algorithm-II (NSGA-II), to optimize container allocation and elasticity management due to the good results obtained with this algorithm in other resource management optimization problems in cloud architectures.","The optimization has been focused on a tight use of the resources and a reduction of the network overhead and system failure rate.","A model for cloud cluster, containers, microservices and four optimization objectives is presented.","Experimental results have shown that our approach is a suitable solution to address the problem of container allocation and elasticity and it obtains better objectives values than the container management policies implemented in Kubernetes."],"url":"http://arxiv.org/abs/2401.12698v1","category":"cs.NI"}
{"created":"2024-01-23 11:58:08","title":"Pragmatic Communication in Multi-Agent Collaborative Perception","abstract":"Collaborative perception allows each agent to enhance its perceptual abilities by exchanging messages with others. It inherently results in a trade-off between perception ability and communication costs. Previous works transmit complete full-frame high-dimensional feature maps among agents, resulting in substantial communication costs. To promote communication efficiency, we propose only transmitting the information needed for the collaborator's downstream task. This pragmatic communication strategy focuses on three key aspects: i) pragmatic message selection, which selects task-critical parts from the complete data, resulting in spatially and temporally sparse feature vectors; ii) pragmatic message representation, which achieves pragmatic approximation of high-dimensional feature vectors with a task-adaptive dictionary, enabling communicating with integer indices; iii) pragmatic collaborator selection, which identifies beneficial collaborators, pruning unnecessary communication links. Following this strategy, we first formulate a mathematical optimization framework for the perception-communication trade-off and then propose PragComm, a multi-agent collaborative perception system with two key components: i) single-agent detection and tracking and ii) pragmatic collaboration. The proposed PragComm promotes pragmatic communication and adapts to a wide range of communication conditions. We evaluate PragComm for both collaborative 3D object detection and tracking tasks in both real-world, V2V4Real, and simulation datasets, OPV2V and V2X-SIM2.0. PragComm consistently outperforms previous methods with more than 32.7K times lower communication volume on OPV2V. Code is available at github.com/PhyllisH/PragComm.","sentences":["Collaborative perception allows each agent to enhance its perceptual abilities by exchanging messages with others.","It inherently results in a trade-off between perception ability and communication costs.","Previous works transmit complete full-frame high-dimensional feature maps among agents, resulting in substantial communication costs.","To promote communication efficiency, we propose only transmitting the information needed for the collaborator's downstream task.","This pragmatic communication strategy focuses on three key aspects: i) pragmatic message selection, which selects task-critical parts from the complete data, resulting in spatially and temporally sparse feature vectors; ii) pragmatic message representation, which achieves pragmatic approximation of high-dimensional feature vectors with a task-adaptive dictionary, enabling communicating with integer indices; iii) pragmatic collaborator selection, which identifies beneficial collaborators, pruning unnecessary communication links.","Following this strategy, we first formulate a mathematical optimization framework for the perception-communication trade-off and then propose PragComm, a multi-agent collaborative perception system with two key components: i) single-agent detection and tracking and ii) pragmatic collaboration.","The proposed PragComm promotes pragmatic communication and adapts to a wide range of communication conditions.","We evaluate PragComm for both collaborative 3D object detection and tracking tasks in both real-world, V2V4Real, and simulation datasets, OPV2V and V2X-SIM2.0.","PragComm consistently outperforms previous methods with more than 32.7K times lower communication volume on OPV2V. Code is available at github.com/PhyllisH/PragComm."],"url":"http://arxiv.org/abs/2401.12694v1","category":"cs.CV"}
{"created":"2024-01-23 11:55:40","title":"Availability-aware Service Placement Policy in Fog Computing Based on Graph Partitions","abstract":"This paper presents a policy for service placement of fog applications inspired on complex networks and graph theory. We propose a twofold partition process based on communities for the partition of the fog devices and based on transitive closures for the application services partition. The allocation of the services is performed sequentially by, firstly, mapping applications to device communities and, secondly, mapping service transitive closures to fog devices in the community. The underlying idea is to place as many inter-related services as possible in the most nearby devices to the users. The optimization objectives are the availability of the applications and the Quality of Service (QoS) of the system, measured as the number of requests that are executed before the application deadlines. We compared our solution with an Integer Linear Programming approach, and the simulation results showed that our proposal obtains higher QoS and availability when fails in the nodes are considered.","sentences":["This paper presents a policy for service placement of fog applications inspired on complex networks and graph theory.","We propose a twofold partition process based on communities for the partition of the fog devices and based on transitive closures for the application services partition.","The allocation of the services is performed sequentially by, firstly, mapping applications to device communities and, secondly, mapping service transitive closures to fog devices in the community.","The underlying idea is to place as many inter-related services as possible in the most nearby devices to the users.","The optimization objectives are the availability of the applications and the Quality of Service (QoS) of the system, measured as the number of requests that are executed before the application deadlines.","We compared our solution with an Integer Linear Programming approach, and the simulation results showed that our proposal obtains higher QoS and availability when fails in the nodes are considered."],"url":"http://arxiv.org/abs/2401.12690v1","category":"cs.NI"}
{"created":"2024-01-23 11:54:09","title":"Energy-based Automated Model Evaluation","abstract":"The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive experiments across modalities, datasets and different architectural backbones to validate MDE's validity, together with its superiority compared with prior approaches. We also prove MDE's versatility by showing its seamless integration with large-scale models, and easy adaption to learning scenarios with noisy- or imbalanced- labels.","sentences":["The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications.","The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels.","Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost.","In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective.","The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning.","We further provide our theoretical insights by connecting the MDE with the classification loss.","We provide extensive experiments across modalities, datasets and different architectural backbones to validate MDE's validity, together with its superiority compared with prior approaches.","We also prove MDE's versatility by showing its seamless integration with large-scale models, and easy adaption to learning scenarios with noisy- or imbalanced- labels."],"url":"http://arxiv.org/abs/2401.12689v1","category":"cs.LG"}
{"created":"2024-01-23 11:52:25","title":"DVL Calibration using Data-driven Methods","abstract":"Autonomous underwater vehicles (AUVs) are used in a wide range of underwater applications, ranging from seafloor mapping to industrial operations. While underwater, the AUV navigation solution commonly relies on the fusion between inertial sensors and Doppler velocity logs (DVL). To achieve accurate DVL measurements a calibration procedure should be conducted before the mission begins. Model-based calibration approaches include filtering approaches utilizing global navigation satellite system signals. In this paper, we propose an end-to-end deep-learning framework for the calibration procedure. Using stimulative data, we show that our proposed approach outperforms model-based approaches by 35% in accuracy and 80% in the required calibration time.","sentences":["Autonomous underwater vehicles (AUVs) are used in a wide range of underwater applications, ranging from seafloor mapping to industrial operations.","While underwater, the AUV navigation solution commonly relies on the fusion between inertial sensors and Doppler velocity logs (DVL).","To achieve accurate DVL measurements a calibration procedure should be conducted before the mission begins.","Model-based calibration approaches include filtering approaches utilizing global navigation satellite system signals.","In this paper, we propose an end-to-end deep-learning framework for the calibration procedure.","Using stimulative data, we show that our proposed approach outperforms model-based approaches by 35% in accuracy and 80% in the required calibration time."],"url":"http://arxiv.org/abs/2401.12687v1","category":"cs.RO"}
{"created":"2024-01-23 11:52:00","title":"Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach","abstract":"Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.","sentences":["Learning the behavior of large agent populations is an important task for numerous research areas.","Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees.","Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents.","Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs.","Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies.","Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes.","Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property.","Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs.","To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup.","This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery.","After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks.","This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods."],"url":"http://arxiv.org/abs/2401.12686v1","category":"cs.MA"}
{"created":"2024-01-23 11:46:52","title":"LLpowershap: Logistic Loss-based Automated Shapley Values Feature Selection Method","abstract":"Shapley values have been used extensively in machine learning, not only to explain black box machine learning models, but among other tasks, also to conduct model debugging, sensitivity and fairness analyses and to select important features for robust modelling and for further follow-up analyses. Shapley values satisfy certain axioms that promote fairness in distributing contributions of features toward prediction or reducing error, after accounting for non-linear relationships and interactions when complex machine learning models are employed. Recently, a number of feature selection methods utilising Shapley values have been introduced. Here, we present a novel feature selection method, LLpowershap, which makes use of loss-based Shapley values to identify informative features with minimal noise among the selected sets of features. Our simulation results show that LLpowershap not only identifies higher number of informative features but outputs fewer noise features compared to other state-of-the-art feature selection methods. Benchmarking results on four real-world datasets demonstrate higher or at par predictive performance of LLpowershap compared to other Shapley based wrapper methods, or filter methods.","sentences":["Shapley values have been used extensively in machine learning, not only to explain black box machine learning models, but among other tasks, also to conduct model debugging, sensitivity and fairness analyses and to select important features for robust modelling and for further follow-up analyses.","Shapley values satisfy certain axioms that promote fairness in distributing contributions of features toward prediction or reducing error, after accounting for non-linear relationships and interactions when complex machine learning models are employed.","Recently, a number of feature selection methods utilising Shapley values have been introduced.","Here, we present a novel feature selection method, LLpowershap, which makes use of loss-based Shapley values to identify informative features with minimal noise among the selected sets of features.","Our simulation results show that LLpowershap not only identifies higher number of informative features but outputs fewer noise features compared to other state-of-the-art feature selection methods.","Benchmarking results on four real-world datasets demonstrate higher or at par predictive performance of LLpowershap compared to other Shapley based wrapper methods, or filter methods."],"url":"http://arxiv.org/abs/2401.12683v1","category":"cs.LG"}
{"created":"2024-01-23 11:46:31","title":"Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical Learning","abstract":"Kriging aims at estimating the attributes of unsampled geo-locations from observations in the spatial vicinity or physical connections, which helps mitigate skewed monitoring caused by under-deployed sensors. Existing works assume that neighbors' information offers the basis for estimating the attributes of the unobserved target while ignoring non-neighbors. However, non-neighbors could also offer constructive information, and neighbors could also be misleading. To this end, we propose ``Contrastive-Prototypical'' self-supervised learning for Kriging (KCP) to refine valuable information from neighbors and recycle the one from non-neighbors. As a pre-trained paradigm, we conduct the Kriging task from a new perspective of representation: we aim to first learn robust and general representations and then recover attributes from representations. A neighboring contrastive module is designed that coarsely learns the representations by narrowing the representation distance between the target and its neighbors while pushing away the non-neighbors. In parallel, a prototypical module is introduced to identify similar representations via exchanged prediction, thus refining the misleading neighbors and recycling the useful non-neighbors from the neighboring contrast component. As a result, not all the neighbors and some of the non-neighbors will be used to infer the target. To encourage the two modules above to learn general and robust representations, we design an adaptive augmentation module that incorporates data-driven attribute augmentation and centrality-based topology augmentation over the spatiotemporal Kriging graph data. Extensive experiments on real-world datasets demonstrate the superior performance of KCP compared to its peers with 6% improvements and exceptional transferability and robustness. The code is available at https://github.com/bonaldli/KCP","sentences":["Kriging aims at estimating the attributes of unsampled geo-locations from observations in the spatial vicinity or physical connections, which helps mitigate skewed monitoring caused by under-deployed sensors.","Existing works assume that neighbors' information offers the basis for estimating the attributes of the unobserved target while ignoring non-neighbors.","However, non-neighbors could also offer constructive information, and neighbors could also be misleading.","To this end, we propose ``Contrastive-Prototypical'' self-supervised learning for Kriging (KCP) to refine valuable information from neighbors and recycle the one from non-neighbors.","As a pre-trained paradigm, we conduct the Kriging task from a new perspective of representation: we aim to first learn robust and general representations and then recover attributes from representations.","A neighboring contrastive module is designed that coarsely learns the representations by narrowing the representation distance between the target and its neighbors while pushing away the non-neighbors.","In parallel, a prototypical module is introduced to identify similar representations via exchanged prediction, thus refining the misleading neighbors and recycling the useful non-neighbors from the neighboring contrast component.","As a result, not all the neighbors and some of the non-neighbors will be used to infer the target.","To encourage the two modules above to learn general and robust representations, we design an adaptive augmentation module that incorporates data-driven attribute augmentation and centrality-based topology augmentation over the spatiotemporal Kriging graph data.","Extensive experiments on real-world datasets demonstrate the superior performance of KCP compared to its peers with 6% improvements and exceptional transferability and robustness.","The code is available at https://github.com/bonaldli/KCP"],"url":"http://arxiv.org/abs/2401.12681v1","category":"cs.LG"}
